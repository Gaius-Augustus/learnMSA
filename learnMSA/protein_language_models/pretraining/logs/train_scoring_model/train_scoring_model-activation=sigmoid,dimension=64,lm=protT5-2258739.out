Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=100000, mem_mib=95368, disk_mb=1000, disk_mib=954, gpu=1
Select jobs to execute...

[Wed Nov 22 18:40:33 2023]
rule train_scoring_model:
    output: outputs/protT5_64_sigmoid.out
    jobid: 0
    reason: Missing output files: outputs/protT5_64_sigmoid.out
    wildcards: lm=protT5, dimension=64, activation=sigmoid
    threads: 4
    resources: mem_mb=100000, mem_mib=95368, disk_mb=1000, disk_mib=954, tmpdir=/tmp, partition=vision, gpu=1, runtime=4320


        cd .. && python3 PretrainScoringModel.py --lm protT5 --dim 64             --activation sigmoid --lr 0.1 > train_scoring_models/outputs/protT5_64_sigmoid.out
        
2023-11-22 18:40:34.095276: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-22 18:41:22.026617: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-22 18:41:23.083085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78713 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:01:00.0, compute capability: 8.0
2023-11-22 18:41:26.024293: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
[Thu Nov 23 05:27:01 2023]
Finished job 0.
1 of 1 steps (100%) done
