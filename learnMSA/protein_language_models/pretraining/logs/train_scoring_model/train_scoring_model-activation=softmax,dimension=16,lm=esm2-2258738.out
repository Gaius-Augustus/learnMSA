Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=100000, mem_mib=95368, disk_mb=1000, disk_mib=954, gpu=1
Select jobs to execute...

[Wed Nov 22 18:40:38 2023]
rule train_scoring_model:
    output: outputs/esm2_16_softmax.out
    jobid: 0
    reason: Missing output files: outputs/esm2_16_softmax.out
    wildcards: lm=esm2, dimension=16, activation=softmax
    threads: 4
    resources: mem_mb=100000, mem_mib=95368, disk_mb=1000, disk_mib=954, tmpdir=/tmp, partition=vision, gpu=1, runtime=4320


        cd .. && python3 PretrainScoringModel.py --lm esm2 --dim 16             --activation softmax --lr 0.1 > train_scoring_models/outputs/esm2_16_softmax.out
        
2023-11-22 18:40:38.732475: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-22 18:41:06.573279: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-22 18:41:09.600227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78536 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:c1:00.0, compute capability: 8.0
2023-11-22 18:41:12.255575: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
[Thu Nov 23 14:03:50 2023]
Finished job 0.
1 of 1 steps (100%) done
