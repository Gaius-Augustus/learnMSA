Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 4
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=100000, mem_mib=95368, disk_mb=1000, disk_mib=954, gpu=1
Select jobs to execute...

[Thu Nov 23 05:38:35 2023]
rule train_scoring_model:
    output: outputs/esm2_32_softmax.out
    jobid: 0
    reason: Missing output files: outputs/esm2_32_softmax.out
    wildcards: lm=esm2, dimension=32, activation=softmax
    threads: 4
    resources: mem_mb=100000, mem_mib=95368, disk_mb=1000, disk_mib=954, tmpdir=/tmp, partition=vision, gpu=1, runtime=4320


        cd .. && python3 PretrainScoringModel.py --lm esm2 --dim 32             --activation softmax --lr 0.1 > train_scoring_models/outputs/esm2_32_softmax.out
        
2023-11-23 05:38:35.555758: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-23 05:39:57.927816: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-23 05:50:42.858917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78536 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-80GB, pci bus id: 0000:01:00.0, compute capability: 8.0
2023-11-23 05:56:34.190160: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
[Fri Nov 24 01:21:16 2023]
Finished job 0.
1 of 1 steps (100%) done
