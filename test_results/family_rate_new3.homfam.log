Training of 5 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2034d4ecd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2034d4e3a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2034d4e790>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f206c311dc0>
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 554.4358 - loglik: -5.4963e+02 - logprior: -4.8107e+00
Epoch 2/10
25/25 - 6s - loss: 441.8111 - loglik: -4.4007e+02 - logprior: -1.7375e+00
Epoch 3/10
25/25 - 6s - loss: 416.1068 - loglik: -4.1412e+02 - logprior: -1.9887e+00
Epoch 4/10
25/25 - 6s - loss: 411.8868 - loglik: -4.1003e+02 - logprior: -1.8613e+00
Epoch 5/10
25/25 - 6s - loss: 410.7256 - loglik: -4.0891e+02 - logprior: -1.8135e+00
Epoch 6/10
25/25 - 6s - loss: 409.6766 - loglik: -4.0786e+02 - logprior: -1.8162e+00
Epoch 7/10
25/25 - 6s - loss: 410.3063 - loglik: -4.0850e+02 - logprior: -1.8068e+00
Fitted a model with MAP estimate = -408.7925
expansions: [(9, 3), (10, 1), (13, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 3), (36, 2), (47, 2), (49, 1), (57, 1), (62, 2), (81, 1), (84, 1), (85, 2), (88, 1), (91, 2), (93, 1), (96, 1), (97, 1), (98, 1), (101, 1), (112, 1), (115, 1), (121, 1), (122, 2), (125, 2), (136, 1), (139, 2), (142, 1), (158, 1), (160, 2), (161, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 406.3107 - loglik: -3.9950e+02 - logprior: -6.8132e+00
Epoch 2/2
25/25 - 8s - loss: 391.1140 - loglik: -3.8860e+02 - logprior: -2.5125e+00
Fitted a model with MAP estimate = -387.9973
expansions: [(0, 3)]
discards: [  0   9  44  45  63  82 157 205]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 396.5046 - loglik: -3.9214e+02 - logprior: -4.3601e+00
Epoch 2/2
25/25 - 7s - loss: 387.2063 - loglik: -3.8672e+02 - logprior: -4.8466e-01
Fitted a model with MAP estimate = -387.1337
expansions: []
discards: [  0   2 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 211 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 398.5417 - loglik: -3.9216e+02 - logprior: -6.3786e+00
Epoch 2/10
25/25 - 7s - loss: 389.5102 - loglik: -3.8820e+02 - logprior: -1.3117e+00
Epoch 3/10
25/25 - 7s - loss: 386.7466 - loglik: -3.8704e+02 - logprior: 0.2953
Epoch 4/10
25/25 - 7s - loss: 386.1614 - loglik: -3.8651e+02 - logprior: 0.3444
Epoch 5/10
25/25 - 7s - loss: 385.4081 - loglik: -3.8583e+02 - logprior: 0.4177
Epoch 6/10
25/25 - 7s - loss: 383.2908 - loglik: -3.8381e+02 - logprior: 0.5187
Epoch 7/10
25/25 - 7s - loss: 383.5493 - loglik: -3.8415e+02 - logprior: 0.6017
Fitted a model with MAP estimate = -383.2365
Time for alignment: 160.9451
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 553.8831 - loglik: -5.4909e+02 - logprior: -4.7936e+00
Epoch 2/10
25/25 - 6s - loss: 439.5423 - loglik: -4.3775e+02 - logprior: -1.7950e+00
Epoch 3/10
25/25 - 6s - loss: 414.4747 - loglik: -4.1234e+02 - logprior: -2.1391e+00
Epoch 4/10
25/25 - 6s - loss: 411.0310 - loglik: -4.0908e+02 - logprior: -1.9539e+00
Epoch 5/10
25/25 - 6s - loss: 409.9877 - loglik: -4.0806e+02 - logprior: -1.9240e+00
Epoch 6/10
25/25 - 6s - loss: 410.8033 - loglik: -4.0888e+02 - logprior: -1.9245e+00
Fitted a model with MAP estimate = -408.8350
expansions: [(3, 1), (4, 1), (7, 1), (12, 1), (30, 1), (31, 1), (32, 1), (33, 2), (34, 1), (35, 2), (36, 2), (46, 2), (57, 1), (62, 1), (63, 1), (82, 1), (84, 1), (85, 3), (91, 2), (93, 1), (97, 1), (99, 1), (102, 1), (113, 1), (118, 1), (122, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (142, 1), (143, 1), (163, 1), (166, 1), (167, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 405.9729 - loglik: -3.9922e+02 - logprior: -6.7495e+00
Epoch 2/2
25/25 - 8s - loss: 392.6832 - loglik: -3.9028e+02 - logprior: -2.4039e+00
Fitted a model with MAP estimate = -389.3615
expansions: [(0, 3)]
discards: [  0  42  45  61 105 155 175]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 395.0375 - loglik: -3.9064e+02 - logprior: -4.3937e+00
Epoch 2/2
25/25 - 7s - loss: 389.4785 - loglik: -3.8897e+02 - logprior: -5.1130e-01
Fitted a model with MAP estimate = -387.5318
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 211 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 397.5812 - loglik: -3.9120e+02 - logprior: -6.3766e+00
Epoch 2/10
25/25 - 7s - loss: 390.5847 - loglik: -3.8926e+02 - logprior: -1.3259e+00
Epoch 3/10
25/25 - 7s - loss: 387.6707 - loglik: -3.8793e+02 - logprior: 0.2581
Epoch 4/10
25/25 - 7s - loss: 385.5674 - loglik: -3.8587e+02 - logprior: 0.3063
Epoch 5/10
25/25 - 7s - loss: 385.6241 - loglik: -3.8602e+02 - logprior: 0.3938
Fitted a model with MAP estimate = -384.1905
Time for alignment: 139.6425
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 554.3586 - loglik: -5.4955e+02 - logprior: -4.8111e+00
Epoch 2/10
25/25 - 6s - loss: 441.0966 - loglik: -4.3935e+02 - logprior: -1.7433e+00
Epoch 3/10
25/25 - 6s - loss: 413.9293 - loglik: -4.1193e+02 - logprior: -1.9962e+00
Epoch 4/10
25/25 - 6s - loss: 412.8291 - loglik: -4.1100e+02 - logprior: -1.8269e+00
Epoch 5/10
25/25 - 6s - loss: 409.2359 - loglik: -4.0744e+02 - logprior: -1.7934e+00
Epoch 6/10
25/25 - 6s - loss: 407.7140 - loglik: -4.0593e+02 - logprior: -1.7868e+00
Epoch 7/10
25/25 - 6s - loss: 407.9638 - loglik: -4.0619e+02 - logprior: -1.7764e+00
Fitted a model with MAP estimate = -407.4901
expansions: [(9, 3), (10, 1), (12, 1), (30, 1), (31, 1), (32, 1), (33, 3), (34, 2), (35, 1), (46, 3), (57, 1), (62, 2), (82, 1), (84, 1), (85, 4), (86, 1), (90, 1), (93, 1), (96, 1), (99, 1), (102, 1), (112, 2), (122, 2), (123, 2), (125, 1), (126, 2), (136, 1), (139, 1), (142, 1), (143, 1), (161, 2), (166, 1), (167, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 405.6428 - loglik: -3.9885e+02 - logprior: -6.7952e+00
Epoch 2/2
25/25 - 8s - loss: 390.8744 - loglik: -3.8833e+02 - logprior: -2.5429e+00
Fitted a model with MAP estimate = -387.9480
expansions: [(0, 3)]
discards: [  0   9  44  61  81 106 107 156 158 206]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 395.4072 - loglik: -3.9107e+02 - logprior: -4.3376e+00
Epoch 2/2
25/25 - 7s - loss: 388.8196 - loglik: -3.8833e+02 - logprior: -4.8578e-01
Fitted a model with MAP estimate = -387.1125
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 211 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 396.8203 - loglik: -3.9048e+02 - logprior: -6.3400e+00
Epoch 2/10
25/25 - 7s - loss: 390.9257 - loglik: -3.8960e+02 - logprior: -1.3229e+00
Epoch 3/10
25/25 - 7s - loss: 385.6898 - loglik: -3.8601e+02 - logprior: 0.3200
Epoch 4/10
25/25 - 7s - loss: 386.8779 - loglik: -3.8724e+02 - logprior: 0.3584
Fitted a model with MAP estimate = -384.5633
Time for alignment: 138.4009
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 555.5464 - loglik: -5.5074e+02 - logprior: -4.8022e+00
Epoch 2/10
25/25 - 6s - loss: 441.6423 - loglik: -4.3986e+02 - logprior: -1.7829e+00
Epoch 3/10
25/25 - 6s - loss: 415.0402 - loglik: -4.1298e+02 - logprior: -2.0590e+00
Epoch 4/10
25/25 - 6s - loss: 412.3907 - loglik: -4.1050e+02 - logprior: -1.8919e+00
Epoch 5/10
25/25 - 6s - loss: 410.3307 - loglik: -4.0846e+02 - logprior: -1.8749e+00
Epoch 6/10
25/25 - 6s - loss: 408.9341 - loglik: -4.0706e+02 - logprior: -1.8712e+00
Epoch 7/10
25/25 - 6s - loss: 408.0414 - loglik: -4.0616e+02 - logprior: -1.8798e+00
Epoch 8/10
25/25 - 6s - loss: 409.0567 - loglik: -4.0716e+02 - logprior: -1.8961e+00
Fitted a model with MAP estimate = -407.8706
expansions: [(9, 3), (10, 1), (12, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 2), (35, 2), (36, 2), (46, 2), (55, 1), (57, 1), (62, 2), (76, 1), (84, 1), (85, 3), (87, 1), (90, 1), (93, 1), (96, 1), (99, 1), (102, 1), (113, 2), (120, 1), (122, 1), (125, 2), (126, 2), (136, 1), (139, 1), (143, 1), (149, 1), (161, 2), (166, 1), (167, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 407.3238 - loglik: -4.0051e+02 - logprior: -6.8149e+00
Epoch 2/2
25/25 - 8s - loss: 391.0473 - loglik: -3.8850e+02 - logprior: -2.5440e+00
Fitted a model with MAP estimate = -388.5612
expansions: [(0, 3)]
discards: [  0   9  42  46  62  82 107 161 205]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 396.0231 - loglik: -3.9166e+02 - logprior: -4.3591e+00
Epoch 2/2
25/25 - 7s - loss: 388.6719 - loglik: -3.8822e+02 - logprior: -4.5323e-01
Fitted a model with MAP estimate = -387.1497
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 211 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 396.2704 - loglik: -3.8995e+02 - logprior: -6.3212e+00
Epoch 2/10
25/25 - 7s - loss: 391.0381 - loglik: -3.8986e+02 - logprior: -1.1767e+00
Epoch 3/10
25/25 - 7s - loss: 387.6320 - loglik: -3.8796e+02 - logprior: 0.3306
Epoch 4/10
25/25 - 7s - loss: 385.0642 - loglik: -3.8546e+02 - logprior: 0.3926
Epoch 5/10
25/25 - 7s - loss: 384.0392 - loglik: -3.8453e+02 - logprior: 0.4864
Epoch 6/10
25/25 - 7s - loss: 384.2260 - loglik: -3.8481e+02 - logprior: 0.5813
Fitted a model with MAP estimate = -383.1740
Time for alignment: 159.2835
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 8s - loss: 555.6718 - loglik: -5.5086e+02 - logprior: -4.8084e+00
Epoch 2/10
25/25 - 6s - loss: 440.3037 - loglik: -4.3859e+02 - logprior: -1.7124e+00
Epoch 3/10
25/25 - 6s - loss: 414.7062 - loglik: -4.1270e+02 - logprior: -2.0027e+00
Epoch 4/10
25/25 - 6s - loss: 412.7867 - loglik: -4.1098e+02 - logprior: -1.8082e+00
Epoch 5/10
25/25 - 6s - loss: 410.7255 - loglik: -4.0894e+02 - logprior: -1.7833e+00
Epoch 6/10
25/25 - 6s - loss: 409.2851 - loglik: -4.0749e+02 - logprior: -1.7907e+00
Epoch 7/10
25/25 - 6s - loss: 409.9612 - loglik: -4.0818e+02 - logprior: -1.7844e+00
Fitted a model with MAP estimate = -408.8221
expansions: [(9, 3), (10, 1), (12, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 2), (35, 2), (36, 2), (47, 2), (58, 1), (62, 1), (63, 1), (82, 1), (84, 1), (85, 3), (91, 1), (92, 1), (97, 1), (99, 1), (102, 1), (113, 1), (118, 1), (122, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 3), (143, 1), (161, 2), (166, 1), (167, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 406.9604 - loglik: -4.0008e+02 - logprior: -6.8771e+00
Epoch 2/2
25/25 - 8s - loss: 392.3600 - loglik: -3.8977e+02 - logprior: -2.5874e+00
Fitted a model with MAP estimate = -389.2048
expansions: [(0, 3), (119, 1)]
discards: [  0   9  42  46  62 175 179 205]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 395.6090 - loglik: -3.9124e+02 - logprior: -4.3712e+00
Epoch 2/2
25/25 - 7s - loss: 388.6925 - loglik: -3.8823e+02 - logprior: -4.6400e-01
Fitted a model with MAP estimate = -387.0597
expansions: []
discards: [  0   2 105 154]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 211 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 397.3484 - loglik: -3.9099e+02 - logprior: -6.3604e+00
Epoch 2/10
25/25 - 7s - loss: 390.5367 - loglik: -3.8920e+02 - logprior: -1.3415e+00
Epoch 3/10
25/25 - 7s - loss: 389.2607 - loglik: -3.8956e+02 - logprior: 0.2982
Epoch 4/10
25/25 - 7s - loss: 384.4228 - loglik: -3.8477e+02 - logprior: 0.3425
Epoch 5/10
25/25 - 7s - loss: 384.1810 - loglik: -3.8461e+02 - logprior: 0.4267
Epoch 6/10
25/25 - 7s - loss: 385.4287 - loglik: -3.8596e+02 - logprior: 0.5290
Fitted a model with MAP estimate = -383.5250
Time for alignment: 153.3114
Computed alignments with likelihoods: ['-383.2365', '-384.1905', '-384.5633', '-383.1740', '-383.5250']
Best model has likelihood: -383.1740  (prior= 0.5925 )
time for generating output: 0.2247
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9266470732304678
Training of 5 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe00af730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201a8e6c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201b72dca0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2034cba790>
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 408.1875 - loglik: -3.8812e+02 - logprior: -2.0064e+01
Epoch 2/10
10/10 - 1s - loss: 348.5830 - loglik: -3.4402e+02 - logprior: -4.5587e+00
Epoch 3/10
10/10 - 1s - loss: 296.9880 - loglik: -2.9458e+02 - logprior: -2.4128e+00
Epoch 4/10
10/10 - 1s - loss: 264.7411 - loglik: -2.6267e+02 - logprior: -2.0715e+00
Epoch 5/10
10/10 - 1s - loss: 252.1956 - loglik: -2.5035e+02 - logprior: -1.8411e+00
Epoch 6/10
10/10 - 1s - loss: 246.1500 - loglik: -2.4453e+02 - logprior: -1.6186e+00
Epoch 7/10
10/10 - 1s - loss: 244.1038 - loglik: -2.4260e+02 - logprior: -1.4995e+00
Epoch 8/10
10/10 - 1s - loss: 242.8093 - loglik: -2.4140e+02 - logprior: -1.4093e+00
Epoch 9/10
10/10 - 1s - loss: 241.3562 - loglik: -2.4005e+02 - logprior: -1.3082e+00
Epoch 10/10
10/10 - 1s - loss: 241.3249 - loglik: -2.4004e+02 - logprior: -1.2874e+00
Fitted a model with MAP estimate = -240.7948
expansions: [(0, 3), (7, 1), (12, 1), (14, 2), (15, 1), (22, 1), (26, 1), (30, 1), (31, 1), (44, 1), (47, 1), (62, 1), (70, 1), (73, 1), (75, 2), (76, 1), (77, 2), (78, 1), (105, 5), (106, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 259.6371 - loglik: -2.3332e+02 - logprior: -2.6312e+01
Epoch 2/2
10/10 - 2s - loss: 222.5005 - loglik: -2.1535e+02 - logprior: -7.1493e+00
Fitted a model with MAP estimate = -215.2759
expansions: []
discards: [  0 134]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 236.9319 - loglik: -2.1406e+02 - logprior: -2.2874e+01
Epoch 2/2
10/10 - 2s - loss: 219.7500 - loglik: -2.1130e+02 - logprior: -8.4501e+00
Fitted a model with MAP estimate = -216.3989
expansions: [(131, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 233.1770 - loglik: -2.1196e+02 - logprior: -2.1221e+01
Epoch 2/10
10/10 - 2s - loss: 215.5125 - loglik: -2.1016e+02 - logprior: -5.3514e+00
Epoch 3/10
10/10 - 2s - loss: 210.0383 - loglik: -2.0876e+02 - logprior: -1.2756e+00
Epoch 4/10
10/10 - 2s - loss: 208.3374 - loglik: -2.0828e+02 - logprior: -6.2158e-02
Epoch 5/10
10/10 - 2s - loss: 207.0723 - loglik: -2.0768e+02 - logprior: 0.6073
Epoch 6/10
10/10 - 2s - loss: 206.6301 - loglik: -2.0774e+02 - logprior: 1.1143
Epoch 7/10
10/10 - 2s - loss: 207.2713 - loglik: -2.0863e+02 - logprior: 1.3603
Fitted a model with MAP estimate = -206.3567
Time for alignment: 52.0152
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 408.3853 - loglik: -3.8832e+02 - logprior: -2.0063e+01
Epoch 2/10
10/10 - 1s - loss: 348.6509 - loglik: -3.4410e+02 - logprior: -4.5533e+00
Epoch 3/10
10/10 - 1s - loss: 295.2260 - loglik: -2.9282e+02 - logprior: -2.4056e+00
Epoch 4/10
10/10 - 1s - loss: 261.6620 - loglik: -2.5964e+02 - logprior: -2.0187e+00
Epoch 5/10
10/10 - 1s - loss: 249.7678 - loglik: -2.4807e+02 - logprior: -1.6934e+00
Epoch 6/10
10/10 - 1s - loss: 244.8869 - loglik: -2.4341e+02 - logprior: -1.4768e+00
Epoch 7/10
10/10 - 1s - loss: 242.4868 - loglik: -2.4109e+02 - logprior: -1.3995e+00
Epoch 8/10
10/10 - 1s - loss: 241.3441 - loglik: -2.4002e+02 - logprior: -1.3254e+00
Epoch 9/10
10/10 - 1s - loss: 240.4402 - loglik: -2.3923e+02 - logprior: -1.2096e+00
Epoch 10/10
10/10 - 1s - loss: 239.9478 - loglik: -2.3876e+02 - logprior: -1.1905e+00
Fitted a model with MAP estimate = -239.7148
expansions: [(0, 5), (14, 2), (15, 1), (18, 1), (26, 1), (27, 1), (31, 1), (47, 1), (49, 1), (62, 1), (70, 1), (71, 1), (75, 2), (76, 1), (77, 2), (78, 1), (93, 1), (104, 3), (105, 2), (106, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 257.3797 - loglik: -2.3119e+02 - logprior: -2.6189e+01
Epoch 2/2
10/10 - 2s - loss: 219.8980 - loglik: -2.1292e+02 - logprior: -6.9787e+00
Fitted a model with MAP estimate = -213.7810
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 236.1430 - loglik: -2.1339e+02 - logprior: -2.2751e+01
Epoch 2/2
10/10 - 2s - loss: 218.8139 - loglik: -2.1047e+02 - logprior: -8.3478e+00
Fitted a model with MAP estimate = -215.8021
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 231.8334 - loglik: -2.1064e+02 - logprior: -2.1197e+01
Epoch 2/10
10/10 - 2s - loss: 214.0823 - loglik: -2.0869e+02 - logprior: -5.3894e+00
Epoch 3/10
10/10 - 2s - loss: 209.1517 - loglik: -2.0781e+02 - logprior: -1.3421e+00
Epoch 4/10
10/10 - 2s - loss: 207.8282 - loglik: -2.0770e+02 - logprior: -1.3035e-01
Epoch 5/10
10/10 - 2s - loss: 206.7932 - loglik: -2.0740e+02 - logprior: 0.6118
Epoch 6/10
10/10 - 2s - loss: 206.0258 - loglik: -2.0711e+02 - logprior: 1.0863
Epoch 7/10
10/10 - 2s - loss: 205.6671 - loglik: -2.0697e+02 - logprior: 1.3077
Epoch 8/10
10/10 - 2s - loss: 206.2721 - loglik: -2.0774e+02 - logprior: 1.4686
Fitted a model with MAP estimate = -205.3483
Time for alignment: 53.9968
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 408.2484 - loglik: -3.8818e+02 - logprior: -2.0063e+01
Epoch 2/10
10/10 - 1s - loss: 348.2720 - loglik: -3.4371e+02 - logprior: -4.5646e+00
Epoch 3/10
10/10 - 1s - loss: 296.6688 - loglik: -2.9417e+02 - logprior: -2.4972e+00
Epoch 4/10
10/10 - 1s - loss: 264.6783 - loglik: -2.6251e+02 - logprior: -2.1702e+00
Epoch 5/10
10/10 - 1s - loss: 252.1508 - loglik: -2.5036e+02 - logprior: -1.7860e+00
Epoch 6/10
10/10 - 1s - loss: 246.4481 - loglik: -2.4502e+02 - logprior: -1.4328e+00
Epoch 7/10
10/10 - 1s - loss: 243.8486 - loglik: -2.4255e+02 - logprior: -1.2948e+00
Epoch 8/10
10/10 - 1s - loss: 242.0416 - loglik: -2.4083e+02 - logprior: -1.2080e+00
Epoch 9/10
10/10 - 1s - loss: 241.5374 - loglik: -2.4043e+02 - logprior: -1.1034e+00
Epoch 10/10
10/10 - 1s - loss: 241.0271 - loglik: -2.3990e+02 - logprior: -1.1237e+00
Fitted a model with MAP estimate = -240.4267
expansions: [(0, 3), (7, 1), (14, 3), (15, 1), (18, 1), (26, 2), (27, 1), (31, 1), (47, 1), (49, 1), (62, 1), (70, 1), (73, 1), (76, 1), (77, 4), (78, 1), (105, 5), (106, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 259.7139 - loglik: -2.3342e+02 - logprior: -2.6296e+01
Epoch 2/2
10/10 - 2s - loss: 222.4718 - loglik: -2.1528e+02 - logprior: -7.1893e+00
Fitted a model with MAP estimate = -215.6426
expansions: [(133, 1)]
discards: [  0 135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 236.1387 - loglik: -2.1330e+02 - logprior: -2.2843e+01
Epoch 2/2
10/10 - 2s - loss: 219.9086 - loglik: -2.1150e+02 - logprior: -8.4099e+00
Fitted a model with MAP estimate = -215.9771
expansions: []
discards: [ 0 35]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 233.3189 - loglik: -2.1213e+02 - logprior: -2.1187e+01
Epoch 2/10
10/10 - 2s - loss: 214.7979 - loglik: -2.0949e+02 - logprior: -5.3071e+00
Epoch 3/10
10/10 - 2s - loss: 209.7501 - loglik: -2.0849e+02 - logprior: -1.2619e+00
Epoch 4/10
10/10 - 2s - loss: 208.6712 - loglik: -2.0860e+02 - logprior: -6.6637e-02
Epoch 5/10
10/10 - 2s - loss: 207.5046 - loglik: -2.0813e+02 - logprior: 0.6264
Epoch 6/10
10/10 - 2s - loss: 206.7354 - loglik: -2.0787e+02 - logprior: 1.1354
Epoch 7/10
10/10 - 2s - loss: 206.7979 - loglik: -2.0816e+02 - logprior: 1.3633
Fitted a model with MAP estimate = -206.3903
Time for alignment: 51.9002
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 408.1740 - loglik: -3.8811e+02 - logprior: -2.0063e+01
Epoch 2/10
10/10 - 1s - loss: 349.3680 - loglik: -3.4480e+02 - logprior: -4.5657e+00
Epoch 3/10
10/10 - 1s - loss: 296.5152 - loglik: -2.9402e+02 - logprior: -2.4916e+00
Epoch 4/10
10/10 - 1s - loss: 265.1167 - loglik: -2.6285e+02 - logprior: -2.2641e+00
Epoch 5/10
10/10 - 1s - loss: 250.9492 - loglik: -2.4897e+02 - logprior: -1.9759e+00
Epoch 6/10
10/10 - 1s - loss: 246.1054 - loglik: -2.4442e+02 - logprior: -1.6900e+00
Epoch 7/10
10/10 - 1s - loss: 243.3437 - loglik: -2.4177e+02 - logprior: -1.5775e+00
Epoch 8/10
10/10 - 1s - loss: 242.4679 - loglik: -2.4096e+02 - logprior: -1.5030e+00
Epoch 9/10
10/10 - 1s - loss: 240.9264 - loglik: -2.3948e+02 - logprior: -1.4499e+00
Epoch 10/10
10/10 - 1s - loss: 240.4617 - loglik: -2.3899e+02 - logprior: -1.4681e+00
Fitted a model with MAP estimate = -240.1192
expansions: [(0, 3), (7, 1), (12, 1), (14, 3), (15, 1), (22, 1), (26, 1), (30, 1), (31, 1), (44, 1), (46, 1), (48, 1), (70, 1), (73, 1), (75, 2), (76, 1), (77, 2), (78, 1), (93, 1), (104, 3), (105, 2), (106, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 258.7987 - loglik: -2.3243e+02 - logprior: -2.6367e+01
Epoch 2/2
10/10 - 2s - loss: 220.4563 - loglik: -2.1332e+02 - logprior: -7.1360e+00
Fitted a model with MAP estimate = -214.1644
expansions: []
discards: [ 0 20]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 235.6880 - loglik: -2.1293e+02 - logprior: -2.2757e+01
Epoch 2/2
10/10 - 2s - loss: 219.1299 - loglik: -2.1081e+02 - logprior: -8.3219e+00
Fitted a model with MAP estimate = -215.8039
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 232.4900 - loglik: -2.1128e+02 - logprior: -2.1207e+01
Epoch 2/10
10/10 - 2s - loss: 215.6300 - loglik: -2.1029e+02 - logprior: -5.3427e+00
Epoch 3/10
10/10 - 2s - loss: 210.2188 - loglik: -2.0895e+02 - logprior: -1.2690e+00
Epoch 4/10
10/10 - 2s - loss: 208.0024 - loglik: -2.0795e+02 - logprior: -5.5026e-02
Epoch 5/10
10/10 - 2s - loss: 207.2275 - loglik: -2.0787e+02 - logprior: 0.6421
Epoch 6/10
10/10 - 2s - loss: 207.3402 - loglik: -2.0850e+02 - logprior: 1.1549
Fitted a model with MAP estimate = -206.5888
Time for alignment: 50.0194
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 407.8790 - loglik: -3.8781e+02 - logprior: -2.0065e+01
Epoch 2/10
10/10 - 1s - loss: 348.6020 - loglik: -3.4404e+02 - logprior: -4.5661e+00
Epoch 3/10
10/10 - 1s - loss: 296.6147 - loglik: -2.9411e+02 - logprior: -2.5046e+00
Epoch 4/10
10/10 - 1s - loss: 265.1931 - loglik: -2.6290e+02 - logprior: -2.2927e+00
Epoch 5/10
10/10 - 1s - loss: 252.9137 - loglik: -2.5081e+02 - logprior: -2.1029e+00
Epoch 6/10
10/10 - 1s - loss: 246.2041 - loglik: -2.4432e+02 - logprior: -1.8888e+00
Epoch 7/10
10/10 - 1s - loss: 243.9936 - loglik: -2.4218e+02 - logprior: -1.8150e+00
Epoch 8/10
10/10 - 1s - loss: 241.4343 - loglik: -2.3973e+02 - logprior: -1.7045e+00
Epoch 9/10
10/10 - 1s - loss: 241.0435 - loglik: -2.3949e+02 - logprior: -1.5487e+00
Epoch 10/10
10/10 - 1s - loss: 239.9878 - loglik: -2.3846e+02 - logprior: -1.5292e+00
Fitted a model with MAP estimate = -240.1412
expansions: [(0, 3), (12, 1), (14, 3), (15, 1), (24, 1), (26, 2), (27, 1), (31, 1), (44, 1), (46, 1), (52, 1), (70, 1), (73, 1), (76, 2), (77, 1), (78, 1), (79, 1), (81, 1), (93, 1), (104, 3), (105, 2), (106, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 258.8771 - loglik: -2.3251e+02 - logprior: -2.6365e+01
Epoch 2/2
10/10 - 2s - loss: 221.2506 - loglik: -2.1418e+02 - logprior: -7.0662e+00
Fitted a model with MAP estimate = -214.4875
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 235.6459 - loglik: -2.1288e+02 - logprior: -2.2766e+01
Epoch 2/2
10/10 - 2s - loss: 219.7067 - loglik: -2.1135e+02 - logprior: -8.3615e+00
Fitted a model with MAP estimate = -215.9464
expansions: []
discards: [35]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 231.2345 - loglik: -2.1016e+02 - logprior: -2.1079e+01
Epoch 2/10
10/10 - 2s - loss: 214.9292 - loglik: -2.0973e+02 - logprior: -5.2023e+00
Epoch 3/10
10/10 - 2s - loss: 209.3817 - loglik: -2.0813e+02 - logprior: -1.2507e+00
Epoch 4/10
10/10 - 2s - loss: 208.2615 - loglik: -2.0819e+02 - logprior: -6.9190e-02
Epoch 5/10
10/10 - 2s - loss: 205.8796 - loglik: -2.0652e+02 - logprior: 0.6375
Epoch 6/10
10/10 - 2s - loss: 206.7808 - loglik: -2.0791e+02 - logprior: 1.1319
Fitted a model with MAP estimate = -205.9242
Time for alignment: 50.4032
Computed alignments with likelihoods: ['-206.3567', '-205.3483', '-206.3903', '-206.5888', '-205.9242']
Best model has likelihood: -205.3483  (prior= 1.5544 )
time for generating output: 0.1721
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.8996587030716724
Training of 5 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa3d65460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe825c0a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe81e8f70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2034cba790>
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 317.9288 - loglik: -2.4950e+02 - logprior: -6.8427e+01
Epoch 2/10
10/10 - 2s - loss: 238.6539 - loglik: -2.2143e+02 - logprior: -1.7222e+01
Epoch 3/10
10/10 - 2s - loss: 201.5856 - loglik: -1.9405e+02 - logprior: -7.5374e+00
Epoch 4/10
10/10 - 1s - loss: 184.1385 - loglik: -1.7996e+02 - logprior: -4.1800e+00
Epoch 5/10
10/10 - 1s - loss: 177.5663 - loglik: -1.7547e+02 - logprior: -2.0945e+00
Epoch 6/10
10/10 - 2s - loss: 174.4929 - loglik: -1.7350e+02 - logprior: -9.9117e-01
Epoch 7/10
10/10 - 2s - loss: 172.9596 - loglik: -1.7255e+02 - logprior: -4.0964e-01
Epoch 8/10
10/10 - 2s - loss: 172.1431 - loglik: -1.7215e+02 - logprior: 0.0119
Epoch 9/10
10/10 - 1s - loss: 171.6427 - loglik: -1.7195e+02 - logprior: 0.3115
Epoch 10/10
10/10 - 1s - loss: 171.3045 - loglik: -1.7184e+02 - logprior: 0.5321
Fitted a model with MAP estimate = -171.1463
expansions: [(10, 3), (11, 1), (14, 2), (26, 2), (27, 1), (28, 3), (39, 1), (52, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.9452 - loglik: -1.7551e+02 - logprior: -7.6439e+01
Epoch 2/2
10/10 - 2s - loss: 195.3681 - loglik: -1.6471e+02 - logprior: -3.0658e+01
Fitted a model with MAP estimate = -185.8174
expansions: []
discards: [31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 235.5001 - loglik: -1.6242e+02 - logprior: -7.3085e+01
Epoch 2/2
10/10 - 2s - loss: 180.2070 - loglik: -1.5927e+02 - logprior: -2.0940e+01
Fitted a model with MAP estimate = -168.8361
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 219.6631 - loglik: -1.5854e+02 - logprior: -6.1119e+01
Epoch 2/10
10/10 - 2s - loss: 172.2466 - loglik: -1.5758e+02 - logprior: -1.4664e+01
Epoch 3/10
10/10 - 2s - loss: 162.4178 - loglik: -1.5743e+02 - logprior: -4.9888e+00
Epoch 4/10
10/10 - 2s - loss: 158.2911 - loglik: -1.5734e+02 - logprior: -9.5531e-01
Epoch 5/10
10/10 - 2s - loss: 156.0428 - loglik: -1.5725e+02 - logprior: 1.2054
Epoch 6/10
10/10 - 2s - loss: 154.7811 - loglik: -1.5721e+02 - logprior: 2.4327
Epoch 7/10
10/10 - 2s - loss: 154.0263 - loglik: -1.5726e+02 - logprior: 3.2333
Epoch 8/10
10/10 - 2s - loss: 153.4769 - loglik: -1.5733e+02 - logprior: 3.8550
Epoch 9/10
10/10 - 2s - loss: 153.0257 - loglik: -1.5739e+02 - logprior: 4.3676
Epoch 10/10
10/10 - 2s - loss: 152.6620 - loglik: -1.5744e+02 - logprior: 4.7778
Fitted a model with MAP estimate = -152.4601
Time for alignment: 52.6153
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 317.9482 - loglik: -2.4952e+02 - logprior: -6.8427e+01
Epoch 2/10
10/10 - 1s - loss: 238.6330 - loglik: -2.2141e+02 - logprior: -1.7222e+01
Epoch 3/10
10/10 - 1s - loss: 201.3332 - loglik: -1.9380e+02 - logprior: -7.5287e+00
Epoch 4/10
10/10 - 1s - loss: 184.1501 - loglik: -1.7999e+02 - logprior: -4.1624e+00
Epoch 5/10
10/10 - 1s - loss: 177.7634 - loglik: -1.7568e+02 - logprior: -2.0833e+00
Epoch 6/10
10/10 - 1s - loss: 174.5407 - loglik: -1.7355e+02 - logprior: -9.8752e-01
Epoch 7/10
10/10 - 1s - loss: 173.0150 - loglik: -1.7261e+02 - logprior: -4.0661e-01
Epoch 8/10
10/10 - 1s - loss: 172.1940 - loglik: -1.7219e+02 - logprior: -2.7872e-03
Epoch 9/10
10/10 - 1s - loss: 171.6949 - loglik: -1.7199e+02 - logprior: 0.2962
Epoch 10/10
10/10 - 2s - loss: 171.3379 - loglik: -1.7186e+02 - logprior: 0.5267
Fitted a model with MAP estimate = -171.1710
expansions: [(10, 3), (11, 1), (14, 2), (26, 2), (27, 1), (28, 3), (39, 1), (52, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 251.6895 - loglik: -1.7524e+02 - logprior: -7.6445e+01
Epoch 2/2
10/10 - 2s - loss: 195.2315 - loglik: -1.6457e+02 - logprior: -3.0665e+01
Fitted a model with MAP estimate = -185.6890
expansions: []
discards: [31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 235.3580 - loglik: -1.6228e+02 - logprior: -7.3080e+01
Epoch 2/2
10/10 - 2s - loss: 180.0536 - loglik: -1.5910e+02 - logprior: -2.0953e+01
Fitted a model with MAP estimate = -168.7384
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 219.5403 - loglik: -1.5840e+02 - logprior: -6.1136e+01
Epoch 2/10
10/10 - 1s - loss: 172.1916 - loglik: -1.5752e+02 - logprior: -1.4669e+01
Epoch 3/10
10/10 - 2s - loss: 162.3756 - loglik: -1.5738e+02 - logprior: -4.9974e+00
Epoch 4/10
10/10 - 2s - loss: 158.2763 - loglik: -1.5732e+02 - logprior: -9.5692e-01
Epoch 5/10
10/10 - 2s - loss: 156.0355 - loglik: -1.5722e+02 - logprior: 1.1875
Epoch 6/10
10/10 - 2s - loss: 154.7784 - loglik: -1.5720e+02 - logprior: 2.4207
Epoch 7/10
10/10 - 2s - loss: 154.0359 - loglik: -1.5726e+02 - logprior: 3.2262
Epoch 8/10
10/10 - 2s - loss: 153.5199 - loglik: -1.5737e+02 - logprior: 3.8517
Epoch 9/10
10/10 - 2s - loss: 153.0805 - loglik: -1.5745e+02 - logprior: 4.3700
Epoch 10/10
10/10 - 2s - loss: 152.7051 - loglik: -1.5749e+02 - logprior: 4.7825
Fitted a model with MAP estimate = -152.4965
Time for alignment: 51.8111
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.9529 - loglik: -2.4953e+02 - logprior: -6.8426e+01
Epoch 2/10
10/10 - 1s - loss: 238.7170 - loglik: -2.2150e+02 - logprior: -1.7219e+01
Epoch 3/10
10/10 - 2s - loss: 201.9090 - loglik: -1.9439e+02 - logprior: -7.5201e+00
Epoch 4/10
10/10 - 2s - loss: 184.2867 - loglik: -1.8013e+02 - logprior: -4.1614e+00
Epoch 5/10
10/10 - 1s - loss: 177.5803 - loglik: -1.7545e+02 - logprior: -2.1267e+00
Epoch 6/10
10/10 - 2s - loss: 174.5635 - loglik: -1.7356e+02 - logprior: -1.0001e+00
Epoch 7/10
10/10 - 1s - loss: 173.0867 - loglik: -1.7268e+02 - logprior: -4.0692e-01
Epoch 8/10
10/10 - 1s - loss: 172.3436 - loglik: -1.7237e+02 - logprior: 0.0310
Epoch 9/10
10/10 - 2s - loss: 171.8742 - loglik: -1.7220e+02 - logprior: 0.3234
Epoch 10/10
10/10 - 1s - loss: 171.5472 - loglik: -1.7210e+02 - logprior: 0.5492
Fitted a model with MAP estimate = -171.3793
expansions: [(10, 3), (11, 1), (14, 2), (26, 2), (27, 1), (28, 2), (39, 1), (52, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.8358 - loglik: -1.7534e+02 - logprior: -7.6498e+01
Epoch 2/2
10/10 - 2s - loss: 195.6176 - loglik: -1.6496e+02 - logprior: -3.0653e+01
Fitted a model with MAP estimate = -186.0076
expansions: []
discards: [31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 236.2219 - loglik: -1.6316e+02 - logprior: -7.3065e+01
Epoch 2/2
10/10 - 2s - loss: 180.9485 - loglik: -1.6001e+02 - logprior: -2.0937e+01
Fitted a model with MAP estimate = -169.7197
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 220.6618 - loglik: -1.5946e+02 - logprior: -6.1206e+01
Epoch 2/10
10/10 - 2s - loss: 173.3458 - loglik: -1.5856e+02 - logprior: -1.4789e+01
Epoch 3/10
10/10 - 2s - loss: 163.6357 - loglik: -1.5851e+02 - logprior: -5.1254e+00
Epoch 4/10
10/10 - 2s - loss: 159.5610 - loglik: -1.5847e+02 - logprior: -1.0933e+00
Epoch 5/10
10/10 - 2s - loss: 157.3046 - loglik: -1.5837e+02 - logprior: 1.0658
Epoch 6/10
10/10 - 2s - loss: 156.0490 - loglik: -1.5834e+02 - logprior: 2.2893
Epoch 7/10
10/10 - 2s - loss: 155.2946 - loglik: -1.5838e+02 - logprior: 3.0843
Epoch 8/10
10/10 - 2s - loss: 154.7489 - loglik: -1.5845e+02 - logprior: 3.7041
Epoch 9/10
10/10 - 2s - loss: 154.3449 - loglik: -1.5856e+02 - logprior: 4.2133
Epoch 10/10
10/10 - 2s - loss: 153.9905 - loglik: -1.5862e+02 - logprior: 4.6300
Fitted a model with MAP estimate = -153.7794
Time for alignment: 51.5262
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.9717 - loglik: -2.4954e+02 - logprior: -6.8427e+01
Epoch 2/10
10/10 - 2s - loss: 238.9415 - loglik: -2.2172e+02 - logprior: -1.7220e+01
Epoch 3/10
10/10 - 1s - loss: 202.7038 - loglik: -1.9517e+02 - logprior: -7.5306e+00
Epoch 4/10
10/10 - 2s - loss: 185.2752 - loglik: -1.8110e+02 - logprior: -4.1797e+00
Epoch 5/10
10/10 - 1s - loss: 178.2189 - loglik: -1.7612e+02 - logprior: -2.1029e+00
Epoch 6/10
10/10 - 2s - loss: 175.0942 - loglik: -1.7411e+02 - logprior: -9.8517e-01
Epoch 7/10
10/10 - 1s - loss: 173.3924 - loglik: -1.7301e+02 - logprior: -3.8726e-01
Epoch 8/10
10/10 - 1s - loss: 172.5323 - loglik: -1.7256e+02 - logprior: 0.0288
Epoch 9/10
10/10 - 2s - loss: 172.0486 - loglik: -1.7236e+02 - logprior: 0.3134
Epoch 10/10
10/10 - 2s - loss: 171.7150 - loglik: -1.7223e+02 - logprior: 0.5196
Fitted a model with MAP estimate = -171.5474
expansions: [(10, 2), (11, 1), (14, 2), (26, 1), (27, 2), (28, 3), (39, 1), (52, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 252.5219 - loglik: -1.7600e+02 - logprior: -7.6522e+01
Epoch 2/2
10/10 - 2s - loss: 196.4527 - loglik: -1.6580e+02 - logprior: -3.0654e+01
Fitted a model with MAP estimate = -186.9266
expansions: []
discards: [17 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 237.7565 - loglik: -1.6466e+02 - logprior: -7.3093e+01
Epoch 2/2
10/10 - 2s - loss: 182.3600 - loglik: -1.6138e+02 - logprior: -2.0979e+01
Fitted a model with MAP estimate = -171.1925
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 222.1287 - loglik: -1.6084e+02 - logprior: -6.1285e+01
Epoch 2/10
10/10 - 2s - loss: 174.8198 - loglik: -1.5994e+02 - logprior: -1.4879e+01
Epoch 3/10
10/10 - 2s - loss: 165.0138 - loglik: -1.5978e+02 - logprior: -5.2299e+00
Epoch 4/10
10/10 - 2s - loss: 160.8941 - loglik: -1.5967e+02 - logprior: -1.2197e+00
Epoch 5/10
10/10 - 2s - loss: 158.6428 - loglik: -1.5959e+02 - logprior: 0.9475
Epoch 6/10
10/10 - 2s - loss: 157.3489 - loglik: -1.5953e+02 - logprior: 2.1801
Epoch 7/10
10/10 - 2s - loss: 156.5716 - loglik: -1.5955e+02 - logprior: 2.9775
Epoch 8/10
10/10 - 2s - loss: 156.0395 - loglik: -1.5964e+02 - logprior: 3.5965
Epoch 9/10
10/10 - 2s - loss: 155.6283 - loglik: -1.5974e+02 - logprior: 4.1088
Epoch 10/10
10/10 - 2s - loss: 155.2860 - loglik: -1.5980e+02 - logprior: 4.5136
Fitted a model with MAP estimate = -155.0846
Time for alignment: 52.0443
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.9520 - loglik: -2.4953e+02 - logprior: -6.8425e+01
Epoch 2/10
10/10 - 2s - loss: 238.6188 - loglik: -2.2140e+02 - logprior: -1.7219e+01
Epoch 3/10
10/10 - 2s - loss: 201.4874 - loglik: -1.9396e+02 - logprior: -7.5319e+00
Epoch 4/10
10/10 - 2s - loss: 183.8941 - loglik: -1.7971e+02 - logprior: -4.1794e+00
Epoch 5/10
10/10 - 2s - loss: 177.2385 - loglik: -1.7511e+02 - logprior: -2.1256e+00
Epoch 6/10
10/10 - 2s - loss: 174.3522 - loglik: -1.7332e+02 - logprior: -1.0274e+00
Epoch 7/10
10/10 - 1s - loss: 172.9093 - loglik: -1.7249e+02 - logprior: -4.1896e-01
Epoch 8/10
10/10 - 1s - loss: 172.1543 - loglik: -1.7216e+02 - logprior: 0.0055
Epoch 9/10
10/10 - 2s - loss: 171.6571 - loglik: -1.7197e+02 - logprior: 0.3085
Epoch 10/10
10/10 - 2s - loss: 171.3155 - loglik: -1.7185e+02 - logprior: 0.5307
Fitted a model with MAP estimate = -171.1518
expansions: [(10, 3), (11, 1), (14, 2), (26, 2), (27, 1), (28, 3), (39, 1), (52, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.5616 - loglik: -1.7512e+02 - logprior: -7.6444e+01
Epoch 2/2
10/10 - 2s - loss: 194.8351 - loglik: -1.6414e+02 - logprior: -3.0697e+01
Fitted a model with MAP estimate = -185.1624
expansions: []
discards: [13 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 235.1825 - loglik: -1.6218e+02 - logprior: -7.3005e+01
Epoch 2/2
10/10 - 2s - loss: 179.9467 - loglik: -1.5909e+02 - logprior: -2.0852e+01
Fitted a model with MAP estimate = -168.6606
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 219.4244 - loglik: -1.5834e+02 - logprior: -6.1087e+01
Epoch 2/10
10/10 - 2s - loss: 172.1153 - loglik: -1.5747e+02 - logprior: -1.4646e+01
Epoch 3/10
10/10 - 2s - loss: 162.2766 - loglik: -1.5733e+02 - logprior: -4.9482e+00
Epoch 4/10
10/10 - 2s - loss: 158.1414 - loglik: -1.5724e+02 - logprior: -8.9964e-01
Epoch 5/10
10/10 - 2s - loss: 155.8485 - loglik: -1.5712e+02 - logprior: 1.2753
Epoch 6/10
10/10 - 2s - loss: 154.5568 - loglik: -1.5705e+02 - logprior: 2.4936
Epoch 7/10
10/10 - 2s - loss: 153.7847 - loglik: -1.5708e+02 - logprior: 3.2934
Epoch 8/10
10/10 - 2s - loss: 153.2599 - loglik: -1.5718e+02 - logprior: 3.9212
Epoch 9/10
10/10 - 2s - loss: 152.8386 - loglik: -1.5728e+02 - logprior: 4.4382
Epoch 10/10
10/10 - 2s - loss: 152.4916 - loglik: -1.5734e+02 - logprior: 4.8454
Fitted a model with MAP estimate = -152.2967
Time for alignment: 53.3182
Computed alignments with likelihoods: ['-152.4601', '-152.4965', '-153.7794', '-155.0846', '-152.2967']
Best model has likelihood: -152.2967  (prior= 5.0343 )
time for generating output: 0.2155
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.2967251075090969
Training of 5 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202c477bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe815f610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff92f8130>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fe83ee1f0>
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 764.3759 - loglik: -7.5648e+02 - logprior: -7.8947e+00
Epoch 2/10
20/20 - 9s - loss: 702.5048 - loglik: -7.0282e+02 - logprior: 0.3143
Epoch 3/10
20/20 - 9s - loss: 665.2828 - loglik: -6.6538e+02 - logprior: 0.0988
Epoch 4/10
20/20 - 9s - loss: 652.8480 - loglik: -6.5307e+02 - logprior: 0.2237
Epoch 5/10
20/20 - 9s - loss: 651.9108 - loglik: -6.5225e+02 - logprior: 0.3425
Epoch 6/10
20/20 - 9s - loss: 650.9750 - loglik: -6.5144e+02 - logprior: 0.4662
Epoch 7/10
20/20 - 9s - loss: 651.7239 - loglik: -6.5229e+02 - logprior: 0.5656
Fitted a model with MAP estimate = -649.1987
expansions: [(0, 7), (11, 3), (43, 4), (48, 2), (50, 3), (51, 1), (80, 5), (86, 1), (91, 1), (102, 1), (115, 2), (117, 1), (132, 1), (153, 2), (198, 4)]
discards: [  1   2   5   6 217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 257 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 668.2317 - loglik: -6.5664e+02 - logprior: -1.1596e+01
Epoch 2/2
20/20 - 10s - loss: 645.5006 - loglik: -6.4464e+02 - logprior: -8.5553e-01
Fitted a model with MAP estimate = -643.1620
expansions: [(0, 5), (104, 4)]
discards: [  2   7   8   9  25  59  63 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 258 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 658.2816 - loglik: -6.4638e+02 - logprior: -1.1901e+01
Epoch 2/2
20/20 - 11s - loss: 643.1627 - loglik: -6.4237e+02 - logprior: -7.8969e-01
Fitted a model with MAP estimate = -640.7262
expansions: [(0, 4), (17, 1), (99, 4), (104, 1)]
discards: [  6   7   8   9  10  11 183]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 656.2408 - loglik: -6.4509e+02 - logprior: -1.1153e+01
Epoch 2/10
20/20 - 11s - loss: 639.4893 - loglik: -6.3862e+02 - logprior: -8.6813e-01
Epoch 3/10
20/20 - 11s - loss: 639.7385 - loglik: -6.4134e+02 - logprior: 1.6031
Fitted a model with MAP estimate = -636.6717
Time for alignment: 181.1879
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 764.3873 - loglik: -7.5649e+02 - logprior: -7.8987e+00
Epoch 2/10
20/20 - 9s - loss: 703.7399 - loglik: -7.0400e+02 - logprior: 0.2555
Epoch 3/10
20/20 - 9s - loss: 668.9886 - loglik: -6.6892e+02 - logprior: -7.3028e-02
Epoch 4/10
20/20 - 9s - loss: 659.0245 - loglik: -6.5904e+02 - logprior: 0.0111
Epoch 5/10
20/20 - 9s - loss: 656.8401 - loglik: -6.5690e+02 - logprior: 0.0560
Epoch 6/10
20/20 - 9s - loss: 656.7894 - loglik: -6.5696e+02 - logprior: 0.1752
Epoch 7/10
20/20 - 9s - loss: 653.8784 - loglik: -6.5412e+02 - logprior: 0.2448
Epoch 8/10
20/20 - 9s - loss: 653.0807 - loglik: -6.5340e+02 - logprior: 0.3200
Epoch 9/10
20/20 - 9s - loss: 653.5600 - loglik: -6.5394e+02 - logprior: 0.3752
Fitted a model with MAP estimate = -652.8700
expansions: [(0, 7), (11, 2), (12, 1), (28, 1), (43, 1), (52, 1), (53, 1), (75, 1), (84, 9), (87, 1), (92, 1), (100, 1), (112, 2), (117, 2), (121, 1), (122, 1), (170, 9), (175, 1), (198, 4), (201, 1), (208, 1)]
discards: [  1   2   5   6 217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 268 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 669.7079 - loglik: -6.5821e+02 - logprior: -1.1499e+01
Epoch 2/2
20/20 - 11s - loss: 646.3258 - loglik: -6.4555e+02 - logprior: -7.7363e-01
Fitted a model with MAP estimate = -642.5266
expansions: [(0, 5), (17, 1), (56, 1)]
discards: [  7   8   9 199 200 201 237 238 239]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 659.2366 - loglik: -6.4726e+02 - logprior: -1.1972e+01
Epoch 2/2
20/20 - 11s - loss: 642.0378 - loglik: -6.4138e+02 - logprior: -6.6149e-01
Fitted a model with MAP estimate = -639.7497
expansions: [(0, 4), (101, 3), (238, 3)]
discards: [2 6 7]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 273 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 654.3656 - loglik: -6.4337e+02 - logprior: -1.0994e+01
Epoch 2/10
20/20 - 12s - loss: 641.1559 - loglik: -6.4028e+02 - logprior: -8.7974e-01
Epoch 3/10
20/20 - 12s - loss: 636.0645 - loglik: -6.3774e+02 - logprior: 1.6798
Epoch 4/10
20/20 - 12s - loss: 635.0723 - loglik: -6.3737e+02 - logprior: 2.3005
Epoch 5/10
20/20 - 12s - loss: 633.6534 - loglik: -6.3620e+02 - logprior: 2.5456
Epoch 6/10
20/20 - 11s - loss: 630.9147 - loglik: -6.3363e+02 - logprior: 2.7137
Epoch 7/10
20/20 - 12s - loss: 633.5973 - loglik: -6.3649e+02 - logprior: 2.8914
Fitted a model with MAP estimate = -631.5977
Time for alignment: 250.5912
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 764.9835 - loglik: -7.5709e+02 - logprior: -7.8919e+00
Epoch 2/10
20/20 - 9s - loss: 698.1597 - loglik: -6.9844e+02 - logprior: 0.2839
Epoch 3/10
20/20 - 9s - loss: 664.0924 - loglik: -6.6419e+02 - logprior: 0.0974
Epoch 4/10
20/20 - 9s - loss: 654.6672 - loglik: -6.5498e+02 - logprior: 0.3154
Epoch 5/10
20/20 - 9s - loss: 652.8384 - loglik: -6.5324e+02 - logprior: 0.4010
Epoch 6/10
20/20 - 9s - loss: 653.4847 - loglik: -6.5400e+02 - logprior: 0.5176
Fitted a model with MAP estimate = -650.4485
expansions: [(0, 6), (11, 3), (45, 2), (50, 6), (81, 4), (87, 1), (92, 1), (103, 1), (116, 4), (146, 1), (170, 9), (179, 1), (198, 3), (200, 1), (207, 1), (208, 1)]
discards: [  1   2   3 217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 265 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 667.3192 - loglik: -6.5588e+02 - logprior: -1.1439e+01
Epoch 2/2
20/20 - 11s - loss: 644.7153 - loglik: -6.4400e+02 - logprior: -7.1504e-01
Fitted a model with MAP estimate = -641.9661
expansions: [(0, 5), (102, 4), (142, 1), (175, 1)]
discards: [  1   2   3   4   5   8   9  52 196 197 198]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 265 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 657.7740 - loglik: -6.4615e+02 - logprior: -1.1627e+01
Epoch 2/2
20/20 - 11s - loss: 641.9974 - loglik: -6.4121e+02 - logprior: -7.8851e-01
Fitted a model with MAP estimate = -639.8793
expansions: [(0, 4), (24, 3), (92, 1), (96, 3)]
discards: [0 1 2 3 4 6 7]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 269 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 651.1141 - loglik: -6.4254e+02 - logprior: -8.5781e+00
Epoch 2/10
20/20 - 11s - loss: 636.6667 - loglik: -6.3723e+02 - logprior: 0.5585
Epoch 3/10
20/20 - 11s - loss: 640.1286 - loglik: -6.4176e+02 - logprior: 1.6350
Fitted a model with MAP estimate = -635.3834
Time for alignment: 176.4679
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 764.2070 - loglik: -7.5629e+02 - logprior: -7.9185e+00
Epoch 2/10
20/20 - 9s - loss: 702.7855 - loglik: -7.0305e+02 - logprior: 0.2625
Epoch 3/10
20/20 - 9s - loss: 665.1385 - loglik: -6.6512e+02 - logprior: -1.9526e-02
Epoch 4/10
20/20 - 9s - loss: 654.6370 - loglik: -6.5480e+02 - logprior: 0.1619
Epoch 5/10
20/20 - 9s - loss: 657.6019 - loglik: -6.5787e+02 - logprior: 0.2645
Fitted a model with MAP estimate = -651.9385
expansions: [(0, 6), (9, 4), (28, 2), (43, 2), (51, 4), (81, 4), (92, 1), (100, 1), (102, 1), (115, 2), (117, 1), (128, 1), (153, 2), (198, 4), (200, 1), (207, 1), (208, 1)]
discards: [  1 217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 260 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 665.5331 - loglik: -6.5427e+02 - logprior: -1.1266e+01
Epoch 2/2
20/20 - 11s - loss: 646.4151 - loglik: -6.4550e+02 - logprior: -9.1673e-01
Fitted a model with MAP estimate = -641.7723
expansions: [(0, 5), (102, 3), (104, 3)]
discards: [  1  10  11  12  13  14  15  16  17  18  19  20  38  65 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 256 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 658.9149 - loglik: -6.4722e+02 - logprior: -1.1692e+01
Epoch 2/2
20/20 - 11s - loss: 643.1558 - loglik: -6.4234e+02 - logprior: -8.1678e-01
Fitted a model with MAP estimate = -640.1291
expansions: [(0, 4), (11, 1), (99, 1)]
discards: [12 94]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 260 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 655.3741 - loglik: -6.4388e+02 - logprior: -1.1497e+01
Epoch 2/10
20/20 - 11s - loss: 642.3632 - loglik: -6.4119e+02 - logprior: -1.1690e+00
Epoch 3/10
20/20 - 11s - loss: 639.3585 - loglik: -6.4081e+02 - logprior: 1.4549
Epoch 4/10
20/20 - 11s - loss: 634.9758 - loglik: -6.3694e+02 - logprior: 1.9666
Epoch 5/10
20/20 - 11s - loss: 637.1967 - loglik: -6.3940e+02 - logprior: 2.2001
Fitted a model with MAP estimate = -635.9640
Time for alignment: 184.1122
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 764.4562 - loglik: -7.5655e+02 - logprior: -7.9043e+00
Epoch 2/10
20/20 - 9s - loss: 696.9323 - loglik: -6.9717e+02 - logprior: 0.2338
Epoch 3/10
20/20 - 9s - loss: 663.5529 - loglik: -6.6361e+02 - logprior: 0.0537
Epoch 4/10
20/20 - 9s - loss: 654.6334 - loglik: -6.5489e+02 - logprior: 0.2594
Epoch 5/10
20/20 - 9s - loss: 651.3364 - loglik: -6.5170e+02 - logprior: 0.3624
Epoch 6/10
20/20 - 9s - loss: 650.4703 - loglik: -6.5091e+02 - logprior: 0.4445
Epoch 7/10
20/20 - 9s - loss: 651.9725 - loglik: -6.5244e+02 - logprior: 0.4697
Fitted a model with MAP estimate = -649.0014
expansions: [(0, 5), (11, 2), (12, 1), (13, 1), (46, 5), (48, 1), (51, 3), (74, 1), (80, 3), (82, 1), (115, 4), (117, 1), (128, 1), (153, 2), (170, 6), (171, 1), (198, 4), (201, 1), (208, 1)]
discards: [  1   2   3   4   5 217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 262 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 667.4705 - loglik: -6.5603e+02 - logprior: -1.1443e+01
Epoch 2/2
20/20 - 11s - loss: 643.9572 - loglik: -6.4336e+02 - logprior: -5.9871e-01
Fitted a model with MAP estimate = -643.0400
expansions: [(0, 3), (95, 1), (98, 6)]
discards: [  0 134 178 196 197]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 267 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 656.1897 - loglik: -6.4786e+02 - logprior: -8.3315e+00
Epoch 2/2
20/20 - 11s - loss: 640.8046 - loglik: -6.4112e+02 - logprior: 0.3178
Fitted a model with MAP estimate = -639.3022
expansions: [(0, 3), (13, 1)]
discards: [  1   2   3   4   5  52  56 203 239]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 262 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 654.4029 - loglik: -6.4370e+02 - logprior: -1.0704e+01
Epoch 2/10
20/20 - 11s - loss: 641.3513 - loglik: -6.4153e+02 - logprior: 0.1784
Epoch 3/10
20/20 - 11s - loss: 637.1710 - loglik: -6.3877e+02 - logprior: 1.5959
Epoch 4/10
20/20 - 11s - loss: 637.7797 - loglik: -6.3984e+02 - logprior: 2.0622
Fitted a model with MAP estimate = -636.5821
Time for alignment: 194.0600
Computed alignments with likelihoods: ['-636.6717', '-631.5977', '-635.3834', '-635.9640', '-636.5821']
Best model has likelihood: -631.5977  (prior= 2.9738 )
time for generating output: 0.2963
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.6578366445916115
Training of 5 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe88a89a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2001be12b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff0b08f10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fe83ee1f0>
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 136.6281 - loglik: -1.3340e+02 - logprior: -3.2283e+00
Epoch 2/10
19/19 - 1s - loss: 116.1228 - loglik: -1.1460e+02 - logprior: -1.5228e+00
Epoch 3/10
19/19 - 1s - loss: 106.4243 - loglik: -1.0496e+02 - logprior: -1.4631e+00
Epoch 4/10
19/19 - 1s - loss: 102.6118 - loglik: -1.0106e+02 - logprior: -1.5510e+00
Epoch 5/10
19/19 - 1s - loss: 101.4562 - loglik: -9.9965e+01 - logprior: -1.4908e+00
Epoch 6/10
19/19 - 1s - loss: 100.8992 - loglik: -9.9431e+01 - logprior: -1.4686e+00
Epoch 7/10
19/19 - 1s - loss: 100.8447 - loglik: -9.9386e+01 - logprior: -1.4589e+00
Epoch 8/10
19/19 - 1s - loss: 100.5755 - loglik: -9.9134e+01 - logprior: -1.4411e+00
Epoch 9/10
19/19 - 1s - loss: 100.6286 - loglik: -9.9191e+01 - logprior: -1.4376e+00
Fitted a model with MAP estimate = -99.1820
expansions: [(5, 1), (9, 1), (11, 1), (13, 1), (20, 1), (22, 1), (23, 1), (27, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 100.1583 - loglik: -9.6757e+01 - logprior: -3.4015e+00
Epoch 2/2
19/19 - 1s - loss: 94.3439 - loglik: -9.2915e+01 - logprior: -1.4293e+00
Fitted a model with MAP estimate = -92.5238
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.1405 - loglik: -9.1903e+01 - logprior: -3.2377e+00
Epoch 2/10
19/19 - 1s - loss: 92.4289 - loglik: -9.1050e+01 - logprior: -1.3793e+00
Epoch 3/10
19/19 - 1s - loss: 91.5322 - loglik: -9.0272e+01 - logprior: -1.2604e+00
Epoch 4/10
19/19 - 1s - loss: 91.7456 - loglik: -9.0518e+01 - logprior: -1.2278e+00
Fitted a model with MAP estimate = -91.1888
Time for alignment: 31.6077
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 136.6772 - loglik: -1.3345e+02 - logprior: -3.2249e+00
Epoch 2/10
19/19 - 1s - loss: 115.9532 - loglik: -1.1443e+02 - logprior: -1.5185e+00
Epoch 3/10
19/19 - 1s - loss: 105.9910 - loglik: -1.0452e+02 - logprior: -1.4686e+00
Epoch 4/10
19/19 - 1s - loss: 102.5082 - loglik: -1.0095e+02 - logprior: -1.5591e+00
Epoch 5/10
19/19 - 1s - loss: 101.3617 - loglik: -9.9873e+01 - logprior: -1.4886e+00
Epoch 6/10
19/19 - 1s - loss: 100.9028 - loglik: -9.9436e+01 - logprior: -1.4668e+00
Epoch 7/10
19/19 - 1s - loss: 100.7880 - loglik: -9.9331e+01 - logprior: -1.4571e+00
Epoch 8/10
19/19 - 1s - loss: 100.6138 - loglik: -9.9174e+01 - logprior: -1.4403e+00
Epoch 9/10
19/19 - 1s - loss: 100.5390 - loglik: -9.9102e+01 - logprior: -1.4367e+00
Epoch 10/10
19/19 - 1s - loss: 100.4216 - loglik: -9.8983e+01 - logprior: -1.4390e+00
Fitted a model with MAP estimate = -99.1033
expansions: [(5, 1), (9, 1), (11, 1), (13, 1), (20, 1), (22, 1), (23, 1), (27, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.0246 - loglik: -9.6624e+01 - logprior: -3.4004e+00
Epoch 2/2
19/19 - 1s - loss: 94.3690 - loglik: -9.2940e+01 - logprior: -1.4295e+00
Fitted a model with MAP estimate = -92.4686
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.0846 - loglik: -9.1851e+01 - logprior: -3.2335e+00
Epoch 2/10
19/19 - 1s - loss: 92.3896 - loglik: -9.1012e+01 - logprior: -1.3778e+00
Epoch 3/10
19/19 - 1s - loss: 91.6229 - loglik: -9.0363e+01 - logprior: -1.2597e+00
Epoch 4/10
19/19 - 1s - loss: 91.6141 - loglik: -9.0395e+01 - logprior: -1.2187e+00
Epoch 5/10
19/19 - 1s - loss: 91.0948 - loglik: -8.9892e+01 - logprior: -1.2033e+00
Epoch 6/10
19/19 - 1s - loss: 91.2202 - loglik: -9.0029e+01 - logprior: -1.1914e+00
Fitted a model with MAP estimate = -90.7980
Time for alignment: 35.1540
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 136.5641 - loglik: -1.3334e+02 - logprior: -3.2244e+00
Epoch 2/10
19/19 - 1s - loss: 115.6249 - loglik: -1.1410e+02 - logprior: -1.5228e+00
Epoch 3/10
19/19 - 1s - loss: 106.3475 - loglik: -1.0488e+02 - logprior: -1.4665e+00
Epoch 4/10
19/19 - 1s - loss: 103.1063 - loglik: -1.0158e+02 - logprior: -1.5232e+00
Epoch 5/10
19/19 - 1s - loss: 102.0684 - loglik: -1.0060e+02 - logprior: -1.4703e+00
Epoch 6/10
19/19 - 1s - loss: 101.5817 - loglik: -1.0013e+02 - logprior: -1.4530e+00
Epoch 7/10
19/19 - 1s - loss: 101.4246 - loglik: -9.9976e+01 - logprior: -1.4488e+00
Epoch 8/10
19/19 - 1s - loss: 101.0294 - loglik: -9.9600e+01 - logprior: -1.4293e+00
Epoch 9/10
19/19 - 1s - loss: 100.9716 - loglik: -9.9540e+01 - logprior: -1.4313e+00
Epoch 10/10
19/19 - 1s - loss: 100.9562 - loglik: -9.9532e+01 - logprior: -1.4241e+00
Fitted a model with MAP estimate = -99.5884
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (20, 1), (22, 1), (23, 1), (27, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.3174 - loglik: -9.6917e+01 - logprior: -3.4004e+00
Epoch 2/2
19/19 - 1s - loss: 94.4591 - loglik: -9.3025e+01 - logprior: -1.4344e+00
Fitted a model with MAP estimate = -92.4810
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.1816 - loglik: -9.1947e+01 - logprior: -3.2344e+00
Epoch 2/10
19/19 - 1s - loss: 92.3433 - loglik: -9.0964e+01 - logprior: -1.3793e+00
Epoch 3/10
19/19 - 1s - loss: 91.8894 - loglik: -9.0634e+01 - logprior: -1.2559e+00
Epoch 4/10
19/19 - 1s - loss: 91.4181 - loglik: -9.0200e+01 - logprior: -1.2183e+00
Epoch 5/10
19/19 - 1s - loss: 91.0814 - loglik: -8.9882e+01 - logprior: -1.1992e+00
Epoch 6/10
19/19 - 1s - loss: 91.1113 - loglik: -8.9920e+01 - logprior: -1.1912e+00
Fitted a model with MAP estimate = -90.8247
Time for alignment: 35.2224
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 136.6292 - loglik: -1.3340e+02 - logprior: -3.2246e+00
Epoch 2/10
19/19 - 1s - loss: 115.9034 - loglik: -1.1438e+02 - logprior: -1.5234e+00
Epoch 3/10
19/19 - 1s - loss: 107.0399 - loglik: -1.0558e+02 - logprior: -1.4631e+00
Epoch 4/10
19/19 - 1s - loss: 103.6842 - loglik: -1.0217e+02 - logprior: -1.5163e+00
Epoch 5/10
19/19 - 1s - loss: 102.5393 - loglik: -1.0107e+02 - logprior: -1.4653e+00
Epoch 6/10
19/19 - 1s - loss: 102.3215 - loglik: -1.0087e+02 - logprior: -1.4496e+00
Epoch 7/10
19/19 - 1s - loss: 101.8893 - loglik: -1.0045e+02 - logprior: -1.4370e+00
Epoch 8/10
19/19 - 1s - loss: 101.9133 - loglik: -1.0049e+02 - logprior: -1.4278e+00
Fitted a model with MAP estimate = -100.4883
expansions: [(9, 1), (11, 1), (12, 1), (13, 1), (14, 1), (22, 1), (23, 1), (27, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.4014 - loglik: -9.7010e+01 - logprior: -3.3912e+00
Epoch 2/2
19/19 - 1s - loss: 94.3489 - loglik: -9.2926e+01 - logprior: -1.4229e+00
Fitted a model with MAP estimate = -92.5237
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.1378 - loglik: -9.1902e+01 - logprior: -3.2354e+00
Epoch 2/10
19/19 - 1s - loss: 92.3269 - loglik: -9.0950e+01 - logprior: -1.3764e+00
Epoch 3/10
19/19 - 1s - loss: 92.0389 - loglik: -9.0786e+01 - logprior: -1.2531e+00
Epoch 4/10
19/19 - 1s - loss: 91.4048 - loglik: -9.0186e+01 - logprior: -1.2191e+00
Epoch 5/10
19/19 - 1s - loss: 91.2657 - loglik: -9.0067e+01 - logprior: -1.1984e+00
Epoch 6/10
19/19 - 1s - loss: 90.8791 - loglik: -8.9689e+01 - logprior: -1.1904e+00
Epoch 7/10
19/19 - 1s - loss: 90.7287 - loglik: -8.9548e+01 - logprior: -1.1812e+00
Epoch 8/10
19/19 - 1s - loss: 90.8267 - loglik: -8.9663e+01 - logprior: -1.1638e+00
Fitted a model with MAP estimate = -90.6162
Time for alignment: 35.2619
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 136.5838 - loglik: -1.3336e+02 - logprior: -3.2238e+00
Epoch 2/10
19/19 - 1s - loss: 115.6874 - loglik: -1.1418e+02 - logprior: -1.5106e+00
Epoch 3/10
19/19 - 1s - loss: 107.2240 - loglik: -1.0579e+02 - logprior: -1.4381e+00
Epoch 4/10
19/19 - 1s - loss: 103.8738 - loglik: -1.0238e+02 - logprior: -1.4923e+00
Epoch 5/10
19/19 - 1s - loss: 102.8748 - loglik: -1.0144e+02 - logprior: -1.4325e+00
Epoch 6/10
19/19 - 1s - loss: 102.1061 - loglik: -1.0069e+02 - logprior: -1.4184e+00
Epoch 7/10
19/19 - 1s - loss: 102.1308 - loglik: -1.0072e+02 - logprior: -1.4099e+00
Fitted a model with MAP estimate = -100.6335
expansions: [(5, 1), (9, 1), (11, 1), (13, 3), (22, 1), (23, 1), (27, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.6936 - loglik: -9.7271e+01 - logprior: -3.4229e+00
Epoch 2/2
19/19 - 1s - loss: 94.5315 - loglik: -9.3072e+01 - logprior: -1.4598e+00
Fitted a model with MAP estimate = -92.5697
expansions: []
discards: [17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.4399 - loglik: -9.3104e+01 - logprior: -3.3357e+00
Epoch 2/2
19/19 - 1s - loss: 93.8340 - loglik: -9.2440e+01 - logprior: -1.3938e+00
Fitted a model with MAP estimate = -92.1215
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.9627 - loglik: -9.1728e+01 - logprior: -3.2347e+00
Epoch 2/10
19/19 - 1s - loss: 92.2474 - loglik: -9.0869e+01 - logprior: -1.3782e+00
Epoch 3/10
19/19 - 1s - loss: 91.8443 - loglik: -9.0614e+01 - logprior: -1.2300e+00
Epoch 4/10
19/19 - 1s - loss: 91.4958 - loglik: -9.0292e+01 - logprior: -1.2040e+00
Epoch 5/10
19/19 - 1s - loss: 91.1295 - loglik: -8.9946e+01 - logprior: -1.1832e+00
Epoch 6/10
19/19 - 1s - loss: 90.9627 - loglik: -8.9790e+01 - logprior: -1.1723e+00
Epoch 7/10
19/19 - 1s - loss: 90.6123 - loglik: -8.9442e+01 - logprior: -1.1704e+00
Epoch 8/10
19/19 - 1s - loss: 90.9533 - loglik: -8.9808e+01 - logprior: -1.1450e+00
Fitted a model with MAP estimate = -90.5735
Time for alignment: 41.0947
Computed alignments with likelihoods: ['-91.1888', '-90.7980', '-90.8247', '-90.6162', '-90.5735']
Best model has likelihood: -90.5735  (prior= -1.1447 )
time for generating output: 0.0936
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9679358717434869
Training of 5 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2023ec4bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fac7c8310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201b59e9a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f91a04ca0>
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 658.2138 - loglik: -6.3971e+02 - logprior: -1.8504e+01
Epoch 2/10
15/15 - 6s - loss: 583.4109 - loglik: -5.8293e+02 - logprior: -4.8003e-01
Epoch 3/10
15/15 - 6s - loss: 518.6089 - loglik: -5.1860e+02 - logprior: -5.1669e-03
Epoch 4/10
15/15 - 6s - loss: 481.1171 - loglik: -4.8066e+02 - logprior: -4.6016e-01
Epoch 5/10
15/15 - 6s - loss: 476.6670 - loglik: -4.7586e+02 - logprior: -8.0515e-01
Epoch 6/10
15/15 - 6s - loss: 467.1988 - loglik: -4.6658e+02 - logprior: -6.2069e-01
Epoch 7/10
15/15 - 6s - loss: 466.5443 - loglik: -4.6620e+02 - logprior: -3.4454e-01
Epoch 8/10
15/15 - 6s - loss: 465.5209 - loglik: -4.6534e+02 - logprior: -1.8035e-01
Epoch 9/10
15/15 - 6s - loss: 468.7794 - loglik: -4.6867e+02 - logprior: -1.1234e-01
Fitted a model with MAP estimate = -466.2936
expansions: [(18, 1), (19, 1), (23, 2), (24, 1), (38, 2), (49, 1), (51, 2), (52, 1), (54, 2), (57, 1), (72, 3), (73, 1), (74, 5), (76, 1), (79, 1), (82, 1), (89, 1), (97, 1), (103, 1), (104, 2), (105, 4), (106, 2), (109, 2), (131, 2), (132, 2), (133, 1), (134, 2), (135, 2), (136, 1), (137, 1), (139, 1), (156, 10), (168, 8)]
discards: [  1   2   3 201 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 473.2320 - loglik: -4.5669e+02 - logprior: -1.6539e+01
Epoch 2/2
15/15 - 9s - loss: 438.2393 - loglik: -4.3853e+02 - logprior: 0.2942
Fitted a model with MAP estimate = -433.4506
expansions: [(27, 1), (205, 2), (206, 1)]
discards: [  1  23  84 130 138 170 178 229 230 231 232 233]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 281 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 454.2936 - loglik: -4.3892e+02 - logprior: -1.5378e+01
Epoch 2/2
15/15 - 9s - loss: 433.1008 - loglik: -4.3428e+02 - logprior: 1.1814
Fitted a model with MAP estimate = -430.7839
expansions: [(0, 4), (89, 1), (199, 1)]
discards: [163]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 459.6559 - loglik: -4.3556e+02 - logprior: -2.4100e+01
Epoch 2/10
15/15 - 9s - loss: 429.8431 - loglik: -4.2852e+02 - logprior: -1.3235e+00
Epoch 3/10
15/15 - 9s - loss: 428.9637 - loglik: -4.3286e+02 - logprior: 3.9000
Epoch 4/10
15/15 - 9s - loss: 423.2794 - loglik: -4.2890e+02 - logprior: 5.6166
Epoch 5/10
15/15 - 9s - loss: 425.8374 - loglik: -4.3204e+02 - logprior: 6.2042
Fitted a model with MAP estimate = -423.5666
Time for alignment: 167.5275
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 657.6564 - loglik: -6.3913e+02 - logprior: -1.8526e+01
Epoch 2/10
15/15 - 6s - loss: 582.2031 - loglik: -5.8171e+02 - logprior: -4.9284e-01
Epoch 3/10
15/15 - 6s - loss: 521.6335 - loglik: -5.2162e+02 - logprior: -1.2016e-02
Epoch 4/10
15/15 - 6s - loss: 484.5302 - loglik: -4.8401e+02 - logprior: -5.1891e-01
Epoch 5/10
15/15 - 6s - loss: 474.2788 - loglik: -4.7349e+02 - logprior: -7.8711e-01
Epoch 6/10
15/15 - 6s - loss: 470.3452 - loglik: -4.6957e+02 - logprior: -7.7569e-01
Epoch 7/10
15/15 - 6s - loss: 467.1128 - loglik: -4.6666e+02 - logprior: -4.5722e-01
Epoch 8/10
15/15 - 6s - loss: 468.4299 - loglik: -4.6805e+02 - logprior: -3.7933e-01
Fitted a model with MAP estimate = -466.7491
expansions: [(18, 1), (22, 2), (23, 1), (39, 1), (52, 2), (53, 1), (55, 1), (59, 1), (70, 1), (74, 2), (75, 7), (77, 2), (79, 1), (88, 1), (89, 1), (96, 1), (103, 1), (104, 2), (105, 4), (106, 2), (109, 2), (121, 1), (130, 2), (131, 2), (132, 1), (133, 2), (134, 2), (137, 1), (139, 1), (156, 10), (171, 1), (210, 1)]
discards: [  1   2   3   4 201 208 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 279 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 476.6794 - loglik: -4.5968e+02 - logprior: -1.6999e+01
Epoch 2/2
15/15 - 9s - loss: 439.8018 - loglik: -4.3957e+02 - logprior: -2.3577e-01
Fitted a model with MAP estimate = -436.4513
expansions: [(0, 4), (202, 2), (203, 2), (228, 1)]
discards: [ 81 127 135 167 175]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 461.5046 - loglik: -4.3737e+02 - logprior: -2.4134e+01
Epoch 2/2
15/15 - 9s - loss: 436.0232 - loglik: -4.3427e+02 - logprior: -1.7492e+00
Fitted a model with MAP estimate = -431.2765
expansions: [(20, 1), (89, 1)]
discards: [  0   1   2   3   5  21 165]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 278 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 454.9342 - loglik: -4.3752e+02 - logprior: -1.7411e+01
Epoch 2/10
15/15 - 8s - loss: 430.8812 - loglik: -4.3254e+02 - logprior: 1.6597
Epoch 3/10
15/15 - 8s - loss: 431.0032 - loglik: -4.3541e+02 - logprior: 4.4093
Fitted a model with MAP estimate = -428.1765
Time for alignment: 141.1938
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 659.7387 - loglik: -6.4123e+02 - logprior: -1.8505e+01
Epoch 2/10
15/15 - 6s - loss: 579.7631 - loglik: -5.7925e+02 - logprior: -5.1712e-01
Epoch 3/10
15/15 - 6s - loss: 517.4797 - loglik: -5.1741e+02 - logprior: -6.8067e-02
Epoch 4/10
15/15 - 6s - loss: 485.8676 - loglik: -4.8520e+02 - logprior: -6.6442e-01
Epoch 5/10
15/15 - 6s - loss: 472.9313 - loglik: -4.7184e+02 - logprior: -1.0939e+00
Epoch 6/10
15/15 - 6s - loss: 473.4986 - loglik: -4.7263e+02 - logprior: -8.6629e-01
Fitted a model with MAP estimate = -470.1115
expansions: [(18, 1), (21, 1), (24, 2), (28, 1), (49, 1), (51, 2), (52, 1), (54, 2), (57, 2), (71, 3), (72, 1), (73, 5), (75, 1), (78, 1), (86, 1), (88, 1), (95, 1), (102, 1), (103, 2), (104, 4), (106, 1), (109, 2), (131, 2), (132, 2), (133, 1), (134, 2), (135, 2), (136, 1), (137, 1), (159, 6), (163, 3), (170, 9), (172, 2), (210, 1)]
discards: [  1   2   3   4 201 208 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 477.5972 - loglik: -4.6074e+02 - logprior: -1.6857e+01
Epoch 2/2
15/15 - 9s - loss: 436.8210 - loglik: -4.3672e+02 - logprior: -9.9467e-02
Fitted a model with MAP estimate = -435.1357
expansions: [(16, 1), (21, 1), (204, 2), (205, 1)]
discards: [ 81 127 131 167 175 230 231 232 234]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 453.7139 - loglik: -4.3816e+02 - logprior: -1.5553e+01
Epoch 2/2
15/15 - 9s - loss: 434.1407 - loglik: -4.3527e+02 - logprior: 1.1264
Fitted a model with MAP estimate = -431.3428
expansions: [(0, 3), (88, 1)]
discards: [  1  21 163]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 459.4382 - loglik: -4.3568e+02 - logprior: -2.3753e+01
Epoch 2/10
15/15 - 9s - loss: 435.3098 - loglik: -4.3407e+02 - logprior: -1.2403e+00
Epoch 3/10
15/15 - 9s - loss: 430.7796 - loglik: -4.3452e+02 - logprior: 3.7452
Epoch 4/10
15/15 - 9s - loss: 427.1642 - loglik: -4.3251e+02 - logprior: 5.3476
Epoch 5/10
15/15 - 9s - loss: 424.6504 - loglik: -4.3054e+02 - logprior: 5.8894
Epoch 6/10
15/15 - 9s - loss: 424.3424 - loglik: -4.3061e+02 - logprior: 6.2639
Epoch 7/10
15/15 - 9s - loss: 425.3824 - loglik: -4.3190e+02 - logprior: 6.5191
Fitted a model with MAP estimate = -424.4092
Time for alignment: 164.7443
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 659.8323 - loglik: -6.4132e+02 - logprior: -1.8511e+01
Epoch 2/10
15/15 - 6s - loss: 578.7263 - loglik: -5.7823e+02 - logprior: -4.9177e-01
Epoch 3/10
15/15 - 6s - loss: 513.5178 - loglik: -5.1372e+02 - logprior: 0.2040
Epoch 4/10
15/15 - 6s - loss: 486.5810 - loglik: -4.8644e+02 - logprior: -1.4117e-01
Epoch 5/10
15/15 - 6s - loss: 473.2448 - loglik: -4.7274e+02 - logprior: -5.0909e-01
Epoch 6/10
15/15 - 6s - loss: 472.8908 - loglik: -4.7251e+02 - logprior: -3.7938e-01
Epoch 7/10
15/15 - 6s - loss: 470.5627 - loglik: -4.7030e+02 - logprior: -2.6129e-01
Epoch 8/10
15/15 - 6s - loss: 466.2427 - loglik: -4.6610e+02 - logprior: -1.4025e-01
Epoch 9/10
15/15 - 6s - loss: 468.7022 - loglik: -4.6861e+02 - logprior: -9.2395e-02
Fitted a model with MAP estimate = -467.4141
expansions: [(16, 2), (20, 1), (21, 2), (22, 1), (27, 1), (51, 2), (52, 1), (54, 2), (57, 1), (72, 5), (73, 3), (74, 2), (76, 2), (79, 1), (89, 1), (97, 1), (103, 1), (104, 1), (106, 3), (107, 2), (109, 1), (110, 1), (132, 2), (133, 2), (134, 4), (135, 2), (136, 1), (137, 1), (157, 1), (159, 4), (163, 3), (170, 9), (172, 2), (210, 1)]
discards: [  1   2   3   4 201 208 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 475.2316 - loglik: -4.5880e+02 - logprior: -1.6436e+01
Epoch 2/2
15/15 - 9s - loss: 438.3445 - loglik: -4.3871e+02 - logprior: 0.3638
Fitted a model with MAP estimate = -434.9298
expansions: [(0, 4), (14, 1), (92, 1), (204, 4)]
discards: [ 19  87  88 136 168 176 228 229 230 231]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 460.7433 - loglik: -4.3608e+02 - logprior: -2.4662e+01
Epoch 2/2
15/15 - 9s - loss: 437.4195 - loglik: -4.3550e+02 - logprior: -1.9214e+00
Fitted a model with MAP estimate = -430.7840
expansions: [(29, 1)]
discards: [  0   1   2   3   5  19 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 281 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 453.3973 - loglik: -4.3619e+02 - logprior: -1.7210e+01
Epoch 2/10
15/15 - 9s - loss: 432.7155 - loglik: -4.3461e+02 - logprior: 1.8907
Epoch 3/10
15/15 - 9s - loss: 429.6502 - loglik: -4.3443e+02 - logprior: 4.7812
Epoch 4/10
15/15 - 9s - loss: 425.2793 - loglik: -4.3100e+02 - logprior: 5.7257
Epoch 5/10
15/15 - 9s - loss: 429.1192 - loglik: -4.3544e+02 - logprior: 6.3208
Fitted a model with MAP estimate = -426.1809
Time for alignment: 165.6030
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 661.9064 - loglik: -6.4340e+02 - logprior: -1.8511e+01
Epoch 2/10
15/15 - 6s - loss: 583.0609 - loglik: -5.8261e+02 - logprior: -4.5584e-01
Epoch 3/10
15/15 - 6s - loss: 516.8756 - loglik: -5.1702e+02 - logprior: 0.1458
Epoch 4/10
15/15 - 6s - loss: 485.1417 - loglik: -4.8463e+02 - logprior: -5.1479e-01
Epoch 5/10
15/15 - 6s - loss: 471.5849 - loglik: -4.7071e+02 - logprior: -8.7163e-01
Epoch 6/10
15/15 - 6s - loss: 472.3761 - loglik: -4.7178e+02 - logprior: -6.0115e-01
Fitted a model with MAP estimate = -468.4382
expansions: [(18, 1), (19, 1), (23, 2), (24, 1), (38, 2), (49, 1), (51, 2), (52, 1), (54, 1), (58, 1), (73, 3), (74, 6), (75, 2), (77, 1), (79, 1), (87, 1), (89, 1), (95, 1), (103, 1), (104, 2), (105, 4), (106, 2), (108, 1), (109, 1), (131, 2), (132, 2), (133, 4), (134, 2), (137, 1), (157, 10), (169, 7), (210, 1)]
discards: [  1   2   3   4 201 206 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 475.2173 - loglik: -4.5855e+02 - logprior: -1.6669e+01
Epoch 2/2
15/15 - 9s - loss: 441.3392 - loglik: -4.4145e+02 - logprior: 0.1113
Fitted a model with MAP estimate = -434.9054
expansions: [(26, 1), (205, 2), (206, 2)]
discards: [ 83 130 138 170 178 229 230 231 232]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 454.1949 - loglik: -4.3872e+02 - logprior: -1.5477e+01
Epoch 2/2
15/15 - 9s - loss: 432.2414 - loglik: -4.3345e+02 - logprior: 1.2059
Fitted a model with MAP estimate = -429.9386
expansions: [(0, 4), (201, 1), (239, 1)]
discards: [  1 165]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 456.1852 - loglik: -4.3281e+02 - logprior: -2.3378e+01
Epoch 2/10
15/15 - 9s - loss: 435.0802 - loglik: -4.3424e+02 - logprior: -8.3892e-01
Epoch 3/10
15/15 - 9s - loss: 422.8803 - loglik: -4.2697e+02 - logprior: 4.0904
Epoch 4/10
15/15 - 9s - loss: 426.3813 - loglik: -4.3202e+02 - logprior: 5.6349
Fitted a model with MAP estimate = -424.3391
Time for alignment: 139.2438
Computed alignments with likelihoods: ['-423.5666', '-428.1765', '-424.4092', '-426.1809', '-424.3391']
Best model has likelihood: -423.5666  (prior= 6.4018 )
time for generating output: 0.3048
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.917087967644085
Training of 5 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff97eb970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202c891d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa315d910>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f200a2e6700>
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 231.8891 - loglik: -1.9025e+02 - logprior: -4.1638e+01
Epoch 2/10
10/10 - 1s - loss: 186.3645 - loglik: -1.7523e+02 - logprior: -1.1139e+01
Epoch 3/10
10/10 - 1s - loss: 165.3243 - loglik: -1.5987e+02 - logprior: -5.4533e+00
Epoch 4/10
10/10 - 1s - loss: 154.5415 - loglik: -1.5097e+02 - logprior: -3.5755e+00
Epoch 5/10
10/10 - 1s - loss: 149.5070 - loglik: -1.4685e+02 - logprior: -2.6611e+00
Epoch 6/10
10/10 - 1s - loss: 147.2746 - loglik: -1.4499e+02 - logprior: -2.2806e+00
Epoch 7/10
10/10 - 1s - loss: 145.9232 - loglik: -1.4402e+02 - logprior: -1.9061e+00
Epoch 8/10
10/10 - 1s - loss: 145.3979 - loglik: -1.4383e+02 - logprior: -1.5648e+00
Epoch 9/10
10/10 - 1s - loss: 144.6410 - loglik: -1.4325e+02 - logprior: -1.3872e+00
Epoch 10/10
10/10 - 1s - loss: 144.2045 - loglik: -1.4289e+02 - logprior: -1.3152e+00
Fitted a model with MAP estimate = -144.1077
expansions: [(0, 2), (16, 1), (17, 1), (18, 4), (19, 1), (20, 1), (37, 1), (38, 1), (42, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 196.6814 - loglik: -1.4198e+02 - logprior: -5.4700e+01
Epoch 2/2
10/10 - 1s - loss: 154.3058 - loglik: -1.3768e+02 - logprior: -1.6630e+01
Fitted a model with MAP estimate = -146.3724
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 184.4249 - loglik: -1.3721e+02 - logprior: -4.7217e+01
Epoch 2/2
10/10 - 1s - loss: 154.7715 - loglik: -1.3643e+02 - logprior: -1.8346e+01
Fitted a model with MAP estimate = -149.6471
expansions: []
discards: [ 0 26]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 178.6067 - loglik: -1.3617e+02 - logprior: -4.2440e+01
Epoch 2/10
10/10 - 1s - loss: 146.6759 - loglik: -1.3517e+02 - logprior: -1.1506e+01
Epoch 3/10
10/10 - 1s - loss: 138.8928 - loglik: -1.3441e+02 - logprior: -4.4809e+00
Epoch 4/10
10/10 - 1s - loss: 136.0230 - loglik: -1.3400e+02 - logprior: -2.0265e+00
Epoch 5/10
10/10 - 1s - loss: 134.7155 - loglik: -1.3376e+02 - logprior: -9.6011e-01
Epoch 6/10
10/10 - 1s - loss: 133.4780 - loglik: -1.3301e+02 - logprior: -4.7287e-01
Epoch 7/10
10/10 - 1s - loss: 133.1715 - loglik: -1.3319e+02 - logprior: 0.0184
Epoch 8/10
10/10 - 1s - loss: 132.9171 - loglik: -1.3334e+02 - logprior: 0.4259
Epoch 9/10
10/10 - 1s - loss: 132.5081 - loglik: -1.3313e+02 - logprior: 0.6223
Epoch 10/10
10/10 - 1s - loss: 132.2613 - loglik: -1.3301e+02 - logprior: 0.7488
Fitted a model with MAP estimate = -132.0347
Time for alignment: 29.4094
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 232.1077 - loglik: -1.9047e+02 - logprior: -4.1638e+01
Epoch 2/10
10/10 - 1s - loss: 186.0840 - loglik: -1.7495e+02 - logprior: -1.1135e+01
Epoch 3/10
10/10 - 1s - loss: 165.0564 - loglik: -1.5962e+02 - logprior: -5.4324e+00
Epoch 4/10
10/10 - 1s - loss: 154.3503 - loglik: -1.5079e+02 - logprior: -3.5610e+00
Epoch 5/10
10/10 - 1s - loss: 149.3255 - loglik: -1.4667e+02 - logprior: -2.6535e+00
Epoch 6/10
10/10 - 1s - loss: 146.8214 - loglik: -1.4455e+02 - logprior: -2.2690e+00
Epoch 7/10
10/10 - 1s - loss: 145.5989 - loglik: -1.4369e+02 - logprior: -1.9040e+00
Epoch 8/10
10/10 - 1s - loss: 145.3213 - loglik: -1.4377e+02 - logprior: -1.5518e+00
Epoch 9/10
10/10 - 1s - loss: 144.5986 - loglik: -1.4323e+02 - logprior: -1.3721e+00
Epoch 10/10
10/10 - 1s - loss: 144.3739 - loglik: -1.4308e+02 - logprior: -1.2984e+00
Fitted a model with MAP estimate = -144.0883
expansions: [(0, 2), (16, 1), (17, 1), (18, 4), (19, 1), (20, 1), (37, 1), (38, 1), (42, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 196.6936 - loglik: -1.4202e+02 - logprior: -5.4677e+01
Epoch 2/2
10/10 - 1s - loss: 154.0620 - loglik: -1.3744e+02 - logprior: -1.6617e+01
Fitted a model with MAP estimate = -146.3540
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 184.3641 - loglik: -1.3715e+02 - logprior: -4.7216e+01
Epoch 2/2
10/10 - 1s - loss: 154.6318 - loglik: -1.3628e+02 - logprior: -1.8354e+01
Fitted a model with MAP estimate = -149.3923
expansions: []
discards: [ 0 26]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 178.5540 - loglik: -1.3611e+02 - logprior: -4.2448e+01
Epoch 2/10
10/10 - 1s - loss: 146.0846 - loglik: -1.3458e+02 - logprior: -1.1506e+01
Epoch 3/10
10/10 - 1s - loss: 139.4580 - loglik: -1.3497e+02 - logprior: -4.4907e+00
Epoch 4/10
10/10 - 1s - loss: 136.0610 - loglik: -1.3403e+02 - logprior: -2.0292e+00
Epoch 5/10
10/10 - 1s - loss: 134.5765 - loglik: -1.3361e+02 - logprior: -9.6822e-01
Epoch 6/10
10/10 - 1s - loss: 133.8295 - loglik: -1.3335e+02 - logprior: -4.7943e-01
Epoch 7/10
10/10 - 1s - loss: 132.9130 - loglik: -1.3293e+02 - logprior: 0.0125
Epoch 8/10
10/10 - 1s - loss: 133.1449 - loglik: -1.3356e+02 - logprior: 0.4166
Fitted a model with MAP estimate = -132.5824
Time for alignment: 28.6082
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 232.0468 - loglik: -1.9041e+02 - logprior: -4.1638e+01
Epoch 2/10
10/10 - 1s - loss: 186.1962 - loglik: -1.7506e+02 - logprior: -1.1137e+01
Epoch 3/10
10/10 - 1s - loss: 164.8286 - loglik: -1.5941e+02 - logprior: -5.4221e+00
Epoch 4/10
10/10 - 1s - loss: 154.1070 - loglik: -1.5056e+02 - logprior: -3.5499e+00
Epoch 5/10
10/10 - 1s - loss: 149.1317 - loglik: -1.4646e+02 - logprior: -2.6751e+00
Epoch 6/10
10/10 - 1s - loss: 146.8063 - loglik: -1.4453e+02 - logprior: -2.2742e+00
Epoch 7/10
10/10 - 1s - loss: 145.7687 - loglik: -1.4387e+02 - logprior: -1.8967e+00
Epoch 8/10
10/10 - 1s - loss: 144.7058 - loglik: -1.4316e+02 - logprior: -1.5431e+00
Epoch 9/10
10/10 - 1s - loss: 144.7480 - loglik: -1.4337e+02 - logprior: -1.3752e+00
Fitted a model with MAP estimate = -144.3861
expansions: [(0, 2), (16, 1), (17, 1), (18, 4), (19, 1), (20, 1), (37, 1), (38, 1), (42, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 196.5110 - loglik: -1.4208e+02 - logprior: -5.4431e+01
Epoch 2/2
10/10 - 1s - loss: 154.2307 - loglik: -1.3772e+02 - logprior: -1.6507e+01
Fitted a model with MAP estimate = -146.3952
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 184.0946 - loglik: -1.3692e+02 - logprior: -4.7179e+01
Epoch 2/2
10/10 - 1s - loss: 155.1834 - loglik: -1.3684e+02 - logprior: -1.8347e+01
Fitted a model with MAP estimate = -149.8260
expansions: []
discards: [ 0 26]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 178.7620 - loglik: -1.3631e+02 - logprior: -4.2448e+01
Epoch 2/10
10/10 - 1s - loss: 146.7506 - loglik: -1.3524e+02 - logprior: -1.1512e+01
Epoch 3/10
10/10 - 1s - loss: 138.8661 - loglik: -1.3437e+02 - logprior: -4.4962e+00
Epoch 4/10
10/10 - 1s - loss: 136.4564 - loglik: -1.3442e+02 - logprior: -2.0338e+00
Epoch 5/10
10/10 - 1s - loss: 134.5330 - loglik: -1.3356e+02 - logprior: -9.7141e-01
Epoch 6/10
10/10 - 1s - loss: 133.7384 - loglik: -1.3326e+02 - logprior: -4.8014e-01
Epoch 7/10
10/10 - 1s - loss: 133.3875 - loglik: -1.3339e+02 - logprior: 0.0043
Epoch 8/10
10/10 - 1s - loss: 132.9375 - loglik: -1.3334e+02 - logprior: 0.4066
Epoch 9/10
10/10 - 1s - loss: 132.5494 - loglik: -1.3315e+02 - logprior: 0.6035
Epoch 10/10
10/10 - 1s - loss: 132.1832 - loglik: -1.3291e+02 - logprior: 0.7273
Fitted a model with MAP estimate = -132.0940
Time for alignment: 28.7314
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 232.0919 - loglik: -1.9045e+02 - logprior: -4.1638e+01
Epoch 2/10
10/10 - 1s - loss: 185.7181 - loglik: -1.7458e+02 - logprior: -1.1141e+01
Epoch 3/10
10/10 - 1s - loss: 165.1893 - loglik: -1.5974e+02 - logprior: -5.4471e+00
Epoch 4/10
10/10 - 1s - loss: 154.1042 - loglik: -1.5055e+02 - logprior: -3.5532e+00
Epoch 5/10
10/10 - 1s - loss: 149.0714 - loglik: -1.4641e+02 - logprior: -2.6620e+00
Epoch 6/10
10/10 - 1s - loss: 146.9598 - loglik: -1.4467e+02 - logprior: -2.2926e+00
Epoch 7/10
10/10 - 1s - loss: 145.7253 - loglik: -1.4382e+02 - logprior: -1.9029e+00
Epoch 8/10
10/10 - 1s - loss: 145.3320 - loglik: -1.4380e+02 - logprior: -1.5281e+00
Epoch 9/10
10/10 - 1s - loss: 144.7625 - loglik: -1.4342e+02 - logprior: -1.3388e+00
Epoch 10/10
10/10 - 1s - loss: 144.2888 - loglik: -1.4303e+02 - logprior: -1.2578e+00
Fitted a model with MAP estimate = -144.2525
expansions: [(0, 3), (16, 1), (17, 1), (18, 4), (19, 1), (20, 1), (37, 1), (38, 1), (42, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 196.7068 - loglik: -1.4217e+02 - logprior: -5.4540e+01
Epoch 2/2
10/10 - 1s - loss: 154.2898 - loglik: -1.3775e+02 - logprior: -1.6543e+01
Fitted a model with MAP estimate = -146.2339
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 184.1348 - loglik: -1.3704e+02 - logprior: -4.7093e+01
Epoch 2/2
10/10 - 1s - loss: 155.1817 - loglik: -1.3683e+02 - logprior: -1.8353e+01
Fitted a model with MAP estimate = -149.4821
expansions: []
discards: [ 0 26]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.9228 - loglik: -1.3621e+02 - logprior: -4.3711e+01
Epoch 2/10
10/10 - 1s - loss: 147.5694 - loglik: -1.3504e+02 - logprior: -1.2531e+01
Epoch 3/10
10/10 - 1s - loss: 139.3622 - loglik: -1.3467e+02 - logprior: -4.6940e+00
Epoch 4/10
10/10 - 1s - loss: 136.1642 - loglik: -1.3406e+02 - logprior: -2.1015e+00
Epoch 5/10
10/10 - 1s - loss: 134.5761 - loglik: -1.3355e+02 - logprior: -1.0264e+00
Epoch 6/10
10/10 - 1s - loss: 133.7947 - loglik: -1.3334e+02 - logprior: -4.5606e-01
Epoch 7/10
10/10 - 1s - loss: 133.2492 - loglik: -1.3331e+02 - logprior: 0.0571
Epoch 8/10
10/10 - 1s - loss: 132.6953 - loglik: -1.3311e+02 - logprior: 0.4175
Epoch 9/10
10/10 - 1s - loss: 132.4249 - loglik: -1.3302e+02 - logprior: 0.5996
Epoch 10/10
10/10 - 1s - loss: 132.2679 - loglik: -1.3301e+02 - logprior: 0.7387
Fitted a model with MAP estimate = -132.0214
Time for alignment: 28.6890
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 232.0805 - loglik: -1.9044e+02 - logprior: -4.1637e+01
Epoch 2/10
10/10 - 1s - loss: 186.3182 - loglik: -1.7519e+02 - logprior: -1.1132e+01
Epoch 3/10
10/10 - 1s - loss: 165.1221 - loglik: -1.5969e+02 - logprior: -5.4299e+00
Epoch 4/10
10/10 - 1s - loss: 154.1664 - loglik: -1.5061e+02 - logprior: -3.5611e+00
Epoch 5/10
10/10 - 1s - loss: 149.2140 - loglik: -1.4656e+02 - logprior: -2.6581e+00
Epoch 6/10
10/10 - 1s - loss: 146.8205 - loglik: -1.4456e+02 - logprior: -2.2638e+00
Epoch 7/10
10/10 - 1s - loss: 145.3894 - loglik: -1.4349e+02 - logprior: -1.8991e+00
Epoch 8/10
10/10 - 1s - loss: 145.1471 - loglik: -1.4361e+02 - logprior: -1.5391e+00
Epoch 9/10
10/10 - 1s - loss: 144.8666 - loglik: -1.4350e+02 - logprior: -1.3631e+00
Epoch 10/10
10/10 - 1s - loss: 144.1433 - loglik: -1.4285e+02 - logprior: -1.2928e+00
Fitted a model with MAP estimate = -144.0697
expansions: [(0, 2), (16, 1), (17, 1), (18, 4), (19, 1), (20, 1), (37, 1), (38, 1), (42, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 196.7654 - loglik: -1.4209e+02 - logprior: -5.4676e+01
Epoch 2/2
10/10 - 1s - loss: 154.1382 - loglik: -1.3753e+02 - logprior: -1.6609e+01
Fitted a model with MAP estimate = -146.3430
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 184.3008 - loglik: -1.3709e+02 - logprior: -4.7215e+01
Epoch 2/2
10/10 - 1s - loss: 154.8573 - loglik: -1.3651e+02 - logprior: -1.8347e+01
Fitted a model with MAP estimate = -149.7485
expansions: []
discards: [ 0 26]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 178.7516 - loglik: -1.3631e+02 - logprior: -4.2443e+01
Epoch 2/10
10/10 - 1s - loss: 146.3635 - loglik: -1.3486e+02 - logprior: -1.1506e+01
Epoch 3/10
10/10 - 1s - loss: 139.1954 - loglik: -1.3471e+02 - logprior: -4.4874e+00
Epoch 4/10
10/10 - 1s - loss: 136.1176 - loglik: -1.3409e+02 - logprior: -2.0271e+00
Epoch 5/10
10/10 - 1s - loss: 134.5446 - loglik: -1.3358e+02 - logprior: -9.6427e-01
Epoch 6/10
10/10 - 1s - loss: 133.5083 - loglik: -1.3304e+02 - logprior: -4.7087e-01
Epoch 7/10
10/10 - 1s - loss: 133.6694 - loglik: -1.3369e+02 - logprior: 0.0206
Fitted a model with MAP estimate = -132.8946
Time for alignment: 26.9709
Computed alignments with likelihoods: ['-132.0347', '-132.5824', '-132.0940', '-132.0214', '-132.8946']
Best model has likelihood: -132.0214  (prior= 0.8183 )
time for generating output: 0.0999
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.902232269002543
Training of 5 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff0c4c760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef6027a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdfd96f70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2023edf1f0>
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 252.2326 - loglik: -2.3905e+02 - logprior: -1.3182e+01
Epoch 2/10
11/11 - 1s - loss: 224.7067 - loglik: -2.2133e+02 - logprior: -3.3809e+00
Epoch 3/10
11/11 - 1s - loss: 201.4446 - loglik: -1.9951e+02 - logprior: -1.9390e+00
Epoch 4/10
11/11 - 1s - loss: 190.3913 - loglik: -1.8870e+02 - logprior: -1.6930e+00
Epoch 5/10
11/11 - 1s - loss: 187.3370 - loglik: -1.8578e+02 - logprior: -1.5525e+00
Epoch 6/10
11/11 - 1s - loss: 186.2500 - loglik: -1.8496e+02 - logprior: -1.2942e+00
Epoch 7/10
11/11 - 1s - loss: 186.4259 - loglik: -1.8523e+02 - logprior: -1.1964e+00
Fitted a model with MAP estimate = -185.1475
expansions: [(0, 5), (24, 1), (32, 1), (33, 1), (34, 1), (35, 1), (44, 1), (45, 2), (48, 1), (49, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 199.1323 - loglik: -1.8265e+02 - logprior: -1.6485e+01
Epoch 2/2
11/11 - 1s - loss: 182.5760 - loglik: -1.7783e+02 - logprior: -4.7448e+00
Fitted a model with MAP estimate = -179.1466
expansions: []
discards: [ 0 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 194.2969 - loglik: -1.7918e+02 - logprior: -1.5121e+01
Epoch 2/2
11/11 - 1s - loss: 183.0176 - loglik: -1.7696e+02 - logprior: -6.0543e+00
Fitted a model with MAP estimate = -180.8947
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 189.0729 - loglik: -1.7626e+02 - logprior: -1.2811e+01
Epoch 2/10
11/11 - 1s - loss: 178.7720 - loglik: -1.7549e+02 - logprior: -3.2858e+00
Epoch 3/10
11/11 - 1s - loss: 176.5991 - loglik: -1.7479e+02 - logprior: -1.8099e+00
Epoch 4/10
11/11 - 1s - loss: 176.3161 - loglik: -1.7488e+02 - logprior: -1.4330e+00
Epoch 5/10
11/11 - 1s - loss: 175.7359 - loglik: -1.7458e+02 - logprior: -1.1540e+00
Epoch 6/10
11/11 - 1s - loss: 176.2149 - loglik: -1.7523e+02 - logprior: -9.8002e-01
Fitted a model with MAP estimate = -175.2796
Time for alignment: 39.9718
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 251.9998 - loglik: -2.3882e+02 - logprior: -1.3180e+01
Epoch 2/10
11/11 - 1s - loss: 224.5497 - loglik: -2.2117e+02 - logprior: -3.3751e+00
Epoch 3/10
11/11 - 1s - loss: 202.2284 - loglik: -2.0026e+02 - logprior: -1.9675e+00
Epoch 4/10
11/11 - 1s - loss: 191.9229 - loglik: -1.9018e+02 - logprior: -1.7431e+00
Epoch 5/10
11/11 - 1s - loss: 188.2029 - loglik: -1.8661e+02 - logprior: -1.5949e+00
Epoch 6/10
11/11 - 1s - loss: 186.0078 - loglik: -1.8469e+02 - logprior: -1.3219e+00
Epoch 7/10
11/11 - 1s - loss: 185.8133 - loglik: -1.8458e+02 - logprior: -1.2368e+00
Epoch 8/10
11/11 - 1s - loss: 185.7616 - loglik: -1.8454e+02 - logprior: -1.2203e+00
Epoch 9/10
11/11 - 1s - loss: 185.4367 - loglik: -1.8426e+02 - logprior: -1.1728e+00
Epoch 10/10
11/11 - 1s - loss: 185.3318 - loglik: -1.8417e+02 - logprior: -1.1590e+00
Fitted a model with MAP estimate = -184.9128
expansions: [(0, 5), (16, 1), (32, 1), (33, 1), (34, 1), (35, 1), (44, 1), (45, 2), (50, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 200.5077 - loglik: -1.8390e+02 - logprior: -1.6612e+01
Epoch 2/2
11/11 - 1s - loss: 181.8775 - loglik: -1.7712e+02 - logprior: -4.7555e+00
Fitted a model with MAP estimate = -178.9919
expansions: []
discards: [ 0 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 194.3360 - loglik: -1.7926e+02 - logprior: -1.5074e+01
Epoch 2/2
11/11 - 1s - loss: 182.5440 - loglik: -1.7650e+02 - logprior: -6.0432e+00
Fitted a model with MAP estimate = -180.6412
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 188.7398 - loglik: -1.7592e+02 - logprior: -1.2824e+01
Epoch 2/10
11/11 - 1s - loss: 178.1339 - loglik: -1.7487e+02 - logprior: -3.2647e+00
Epoch 3/10
11/11 - 1s - loss: 177.1182 - loglik: -1.7534e+02 - logprior: -1.7822e+00
Epoch 4/10
11/11 - 1s - loss: 176.0807 - loglik: -1.7470e+02 - logprior: -1.3846e+00
Epoch 5/10
11/11 - 1s - loss: 175.1494 - loglik: -1.7404e+02 - logprior: -1.1088e+00
Epoch 6/10
11/11 - 1s - loss: 175.1248 - loglik: -1.7420e+02 - logprior: -9.2631e-01
Epoch 7/10
11/11 - 1s - loss: 174.9134 - loglik: -1.7402e+02 - logprior: -8.8889e-01
Epoch 8/10
11/11 - 1s - loss: 175.9367 - loglik: -1.7508e+02 - logprior: -8.5702e-01
Fitted a model with MAP estimate = -174.7140
Time for alignment: 46.0235
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 251.9866 - loglik: -2.3881e+02 - logprior: -1.3178e+01
Epoch 2/10
11/11 - 1s - loss: 224.0845 - loglik: -2.2071e+02 - logprior: -3.3758e+00
Epoch 3/10
11/11 - 1s - loss: 204.2565 - loglik: -2.0230e+02 - logprior: -1.9576e+00
Epoch 4/10
11/11 - 1s - loss: 191.0290 - loglik: -1.8930e+02 - logprior: -1.7275e+00
Epoch 5/10
11/11 - 1s - loss: 187.8026 - loglik: -1.8622e+02 - logprior: -1.5820e+00
Epoch 6/10
11/11 - 1s - loss: 186.8074 - loglik: -1.8549e+02 - logprior: -1.3191e+00
Epoch 7/10
11/11 - 1s - loss: 185.3412 - loglik: -1.8411e+02 - logprior: -1.2297e+00
Epoch 8/10
11/11 - 1s - loss: 185.3880 - loglik: -1.8416e+02 - logprior: -1.2238e+00
Fitted a model with MAP estimate = -185.1127
expansions: [(0, 5), (18, 1), (32, 1), (33, 1), (34, 1), (35, 1), (44, 1), (45, 2), (50, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 199.7062 - loglik: -1.8326e+02 - logprior: -1.6444e+01
Epoch 2/2
11/11 - 1s - loss: 182.2716 - loglik: -1.7757e+02 - logprior: -4.6967e+00
Fitted a model with MAP estimate = -178.9174
expansions: []
discards: [ 0 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 193.7108 - loglik: -1.7863e+02 - logprior: -1.5077e+01
Epoch 2/2
11/11 - 1s - loss: 182.6575 - loglik: -1.7664e+02 - logprior: -6.0202e+00
Fitted a model with MAP estimate = -180.5919
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 188.4446 - loglik: -1.7570e+02 - logprior: -1.2749e+01
Epoch 2/10
11/11 - 1s - loss: 179.0431 - loglik: -1.7580e+02 - logprior: -3.2406e+00
Epoch 3/10
11/11 - 1s - loss: 176.2389 - loglik: -1.7448e+02 - logprior: -1.7549e+00
Epoch 4/10
11/11 - 1s - loss: 176.3036 - loglik: -1.7493e+02 - logprior: -1.3694e+00
Fitted a model with MAP estimate = -175.5500
Time for alignment: 38.5311
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 251.5558 - loglik: -2.3838e+02 - logprior: -1.3180e+01
Epoch 2/10
11/11 - 1s - loss: 224.5993 - loglik: -2.2122e+02 - logprior: -3.3822e+00
Epoch 3/10
11/11 - 1s - loss: 202.5191 - loglik: -2.0055e+02 - logprior: -1.9705e+00
Epoch 4/10
11/11 - 1s - loss: 192.0227 - loglik: -1.9030e+02 - logprior: -1.7225e+00
Epoch 5/10
11/11 - 1s - loss: 187.7655 - loglik: -1.8616e+02 - logprior: -1.6048e+00
Epoch 6/10
11/11 - 1s - loss: 186.6944 - loglik: -1.8535e+02 - logprior: -1.3471e+00
Epoch 7/10
11/11 - 1s - loss: 185.6553 - loglik: -1.8442e+02 - logprior: -1.2321e+00
Epoch 8/10
11/11 - 1s - loss: 185.2131 - loglik: -1.8399e+02 - logprior: -1.2261e+00
Epoch 9/10
11/11 - 1s - loss: 185.4506 - loglik: -1.8427e+02 - logprior: -1.1783e+00
Fitted a model with MAP estimate = -184.9054
expansions: [(0, 5), (18, 1), (32, 1), (33, 1), (34, 1), (35, 1), (44, 1), (45, 2), (50, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 199.5774 - loglik: -1.8304e+02 - logprior: -1.6536e+01
Epoch 2/2
11/11 - 1s - loss: 182.3559 - loglik: -1.7763e+02 - logprior: -4.7290e+00
Fitted a model with MAP estimate = -178.9358
expansions: []
discards: [ 0 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 193.5782 - loglik: -1.7852e+02 - logprior: -1.5054e+01
Epoch 2/2
11/11 - 1s - loss: 184.0189 - loglik: -1.7801e+02 - logprior: -6.0102e+00
Fitted a model with MAP estimate = -180.6514
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 188.6587 - loglik: -1.7588e+02 - logprior: -1.2781e+01
Epoch 2/10
11/11 - 1s - loss: 179.0258 - loglik: -1.7577e+02 - logprior: -3.2556e+00
Epoch 3/10
11/11 - 1s - loss: 176.0722 - loglik: -1.7430e+02 - logprior: -1.7713e+00
Epoch 4/10
11/11 - 1s - loss: 176.0665 - loglik: -1.7469e+02 - logprior: -1.3811e+00
Epoch 5/10
11/11 - 1s - loss: 175.7703 - loglik: -1.7466e+02 - logprior: -1.1097e+00
Epoch 6/10
11/11 - 1s - loss: 175.0352 - loglik: -1.7410e+02 - logprior: -9.3114e-01
Epoch 7/10
11/11 - 1s - loss: 175.4075 - loglik: -1.7452e+02 - logprior: -8.8598e-01
Fitted a model with MAP estimate = -174.8499
Time for alignment: 42.3992
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 251.5670 - loglik: -2.3839e+02 - logprior: -1.3180e+01
Epoch 2/10
11/11 - 1s - loss: 225.2132 - loglik: -2.2184e+02 - logprior: -3.3757e+00
Epoch 3/10
11/11 - 1s - loss: 203.0150 - loglik: -2.0104e+02 - logprior: -1.9758e+00
Epoch 4/10
11/11 - 1s - loss: 191.6298 - loglik: -1.8986e+02 - logprior: -1.7738e+00
Epoch 5/10
11/11 - 1s - loss: 187.4467 - loglik: -1.8579e+02 - logprior: -1.6594e+00
Epoch 6/10
11/11 - 1s - loss: 185.8026 - loglik: -1.8436e+02 - logprior: -1.4437e+00
Epoch 7/10
11/11 - 1s - loss: 185.1034 - loglik: -1.8374e+02 - logprior: -1.3610e+00
Epoch 8/10
11/11 - 1s - loss: 184.6345 - loglik: -1.8329e+02 - logprior: -1.3448e+00
Epoch 9/10
11/11 - 1s - loss: 184.6598 - loglik: -1.8335e+02 - logprior: -1.3071e+00
Fitted a model with MAP estimate = -184.1267
expansions: [(0, 4), (18, 1), (32, 1), (33, 1), (34, 1), (35, 1), (44, 1), (45, 2), (48, 1), (49, 2), (50, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 201.4738 - loglik: -1.8487e+02 - logprior: -1.6604e+01
Epoch 2/2
11/11 - 1s - loss: 182.3203 - loglik: -1.7747e+02 - logprior: -4.8469e+00
Fitted a model with MAP estimate = -179.4294
expansions: []
discards: [55 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 188.6605 - loglik: -1.7649e+02 - logprior: -1.2175e+01
Epoch 2/2
11/11 - 1s - loss: 179.3039 - loglik: -1.7609e+02 - logprior: -3.2177e+00
Fitted a model with MAP estimate = -177.5705
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 187.9515 - loglik: -1.7620e+02 - logprior: -1.1749e+01
Epoch 2/10
11/11 - 1s - loss: 177.3530 - loglik: -1.7421e+02 - logprior: -3.1429e+00
Epoch 3/10
11/11 - 1s - loss: 177.4524 - loglik: -1.7559e+02 - logprior: -1.8590e+00
Fitted a model with MAP estimate = -176.3613
Time for alignment: 38.3943
Computed alignments with likelihoods: ['-175.2796', '-174.7140', '-175.5500', '-174.8499', '-176.3613']
Best model has likelihood: -174.7140  (prior= -0.8340 )
time for generating output: 0.1072
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.8676103247293921
Training of 5 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200a752df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff974afa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf3ced90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fa39e6670>
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 269.4128 - loglik: -2.2881e+02 - logprior: -4.0608e+01
Epoch 2/10
10/10 - 1s - loss: 215.5895 - loglik: -2.0491e+02 - logprior: -1.0676e+01
Epoch 3/10
10/10 - 1s - loss: 181.0041 - loglik: -1.7572e+02 - logprior: -5.2796e+00
Epoch 4/10
10/10 - 1s - loss: 157.3069 - loglik: -1.5343e+02 - logprior: -3.8746e+00
Epoch 5/10
10/10 - 1s - loss: 148.4290 - loglik: -1.4510e+02 - logprior: -3.3296e+00
Epoch 6/10
10/10 - 1s - loss: 144.8274 - loglik: -1.4208e+02 - logprior: -2.7519e+00
Epoch 7/10
10/10 - 1s - loss: 143.2278 - loglik: -1.4085e+02 - logprior: -2.3814e+00
Epoch 8/10
10/10 - 1s - loss: 142.0983 - loglik: -1.3985e+02 - logprior: -2.2445e+00
Epoch 9/10
10/10 - 1s - loss: 141.4862 - loglik: -1.3934e+02 - logprior: -2.1479e+00
Epoch 10/10
10/10 - 1s - loss: 141.4255 - loglik: -1.3935e+02 - logprior: -2.0746e+00
Fitted a model with MAP estimate = -141.1607
expansions: [(13, 1), (14, 1), (15, 1), (17, 1), (22, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1), (32, 1), (33, 1), (43, 2), (52, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 80 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 189.8227 - loglik: -1.4417e+02 - logprior: -4.5649e+01
Epoch 2/2
10/10 - 1s - loss: 153.1224 - loglik: -1.3455e+02 - logprior: -1.8568e+01
Fitted a model with MAP estimate = -146.6610
expansions: []
discards: [57 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.3999 - loglik: -1.3322e+02 - logprior: -4.3179e+01
Epoch 2/2
10/10 - 1s - loss: 142.8238 - loglik: -1.3017e+02 - logprior: -1.2653e+01
Fitted a model with MAP estimate = -135.6501
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 166.0855 - loglik: -1.2972e+02 - logprior: -3.6363e+01
Epoch 2/10
10/10 - 1s - loss: 137.8882 - loglik: -1.2856e+02 - logprior: -9.3331e+00
Epoch 3/10
10/10 - 1s - loss: 131.5106 - loglik: -1.2758e+02 - logprior: -3.9317e+00
Epoch 4/10
10/10 - 1s - loss: 129.0331 - loglik: -1.2706e+02 - logprior: -1.9727e+00
Epoch 5/10
10/10 - 1s - loss: 127.9251 - loglik: -1.2706e+02 - logprior: -8.6370e-01
Epoch 6/10
10/10 - 1s - loss: 127.1137 - loglik: -1.2711e+02 - logprior: -2.1167e-03
Epoch 7/10
10/10 - 1s - loss: 126.6891 - loglik: -1.2720e+02 - logprior: 0.5059
Epoch 8/10
10/10 - 1s - loss: 126.3241 - loglik: -1.2708e+02 - logprior: 0.7514
Epoch 9/10
10/10 - 1s - loss: 126.2354 - loglik: -1.2720e+02 - logprior: 0.9650
Epoch 10/10
10/10 - 1s - loss: 125.9023 - loglik: -1.2705e+02 - logprior: 1.1471
Fitted a model with MAP estimate = -125.8507
Time for alignment: 32.1862
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 269.6171 - loglik: -2.2901e+02 - logprior: -4.0608e+01
Epoch 2/10
10/10 - 1s - loss: 215.0619 - loglik: -2.0439e+02 - logprior: -1.0677e+01
Epoch 3/10
10/10 - 1s - loss: 180.6123 - loglik: -1.7534e+02 - logprior: -5.2680e+00
Epoch 4/10
10/10 - 1s - loss: 158.5771 - loglik: -1.5478e+02 - logprior: -3.7926e+00
Epoch 5/10
10/10 - 1s - loss: 150.0407 - loglik: -1.4704e+02 - logprior: -2.9984e+00
Epoch 6/10
10/10 - 1s - loss: 145.8968 - loglik: -1.4351e+02 - logprior: -2.3875e+00
Epoch 7/10
10/10 - 1s - loss: 144.2245 - loglik: -1.4218e+02 - logprior: -2.0438e+00
Epoch 8/10
10/10 - 1s - loss: 143.7528 - loglik: -1.4180e+02 - logprior: -1.9517e+00
Epoch 9/10
10/10 - 1s - loss: 142.6829 - loglik: -1.4079e+02 - logprior: -1.8960e+00
Epoch 10/10
10/10 - 1s - loss: 142.3044 - loglik: -1.4043e+02 - logprior: -1.8698e+00
Fitted a model with MAP estimate = -142.2197
expansions: [(12, 2), (13, 2), (14, 2), (17, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 2), (32, 1), (33, 1), (43, 2), (52, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 191.8650 - loglik: -1.4617e+02 - logprior: -4.5693e+01
Epoch 2/2
10/10 - 1s - loss: 154.2018 - loglik: -1.3529e+02 - logprior: -1.8909e+01
Fitted a model with MAP estimate = -147.5449
expansions: []
discards: [14 17 46 60 71]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 177.4933 - loglik: -1.3425e+02 - logprior: -4.3239e+01
Epoch 2/2
10/10 - 1s - loss: 142.7102 - loglik: -1.3003e+02 - logprior: -1.2679e+01
Fitted a model with MAP estimate = -135.7301
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 165.8403 - loglik: -1.2946e+02 - logprior: -3.6378e+01
Epoch 2/10
10/10 - 1s - loss: 137.8617 - loglik: -1.2852e+02 - logprior: -9.3397e+00
Epoch 3/10
10/10 - 1s - loss: 131.6998 - loglik: -1.2776e+02 - logprior: -3.9348e+00
Epoch 4/10
10/10 - 1s - loss: 129.3948 - loglik: -1.2742e+02 - logprior: -1.9714e+00
Epoch 5/10
10/10 - 1s - loss: 128.0443 - loglik: -1.2719e+02 - logprior: -8.5923e-01
Epoch 6/10
10/10 - 1s - loss: 127.2070 - loglik: -1.2722e+02 - logprior: 0.0153
Epoch 7/10
10/10 - 1s - loss: 126.7493 - loglik: -1.2726e+02 - logprior: 0.5129
Epoch 8/10
10/10 - 1s - loss: 126.4693 - loglik: -1.2723e+02 - logprior: 0.7637
Epoch 9/10
10/10 - 1s - loss: 126.2137 - loglik: -1.2719e+02 - logprior: 0.9731
Epoch 10/10
10/10 - 1s - loss: 126.0280 - loglik: -1.2718e+02 - logprior: 1.1514
Fitted a model with MAP estimate = -125.8738
Time for alignment: 32.6800
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 269.4167 - loglik: -2.2881e+02 - logprior: -4.0607e+01
Epoch 2/10
10/10 - 1s - loss: 215.4134 - loglik: -2.0474e+02 - logprior: -1.0677e+01
Epoch 3/10
10/10 - 1s - loss: 180.7887 - loglik: -1.7550e+02 - logprior: -5.2847e+00
Epoch 4/10
10/10 - 1s - loss: 157.6665 - loglik: -1.5373e+02 - logprior: -3.9403e+00
Epoch 5/10
10/10 - 1s - loss: 148.0823 - loglik: -1.4473e+02 - logprior: -3.3525e+00
Epoch 6/10
10/10 - 1s - loss: 144.2279 - loglik: -1.4146e+02 - logprior: -2.7648e+00
Epoch 7/10
10/10 - 1s - loss: 142.9494 - loglik: -1.4057e+02 - logprior: -2.3754e+00
Epoch 8/10
10/10 - 1s - loss: 142.2654 - loglik: -1.4004e+02 - logprior: -2.2270e+00
Epoch 9/10
10/10 - 1s - loss: 141.5075 - loglik: -1.3938e+02 - logprior: -2.1237e+00
Epoch 10/10
10/10 - 1s - loss: 141.4156 - loglik: -1.3938e+02 - logprior: -2.0336e+00
Fitted a model with MAP estimate = -141.1999
expansions: [(13, 1), (14, 1), (15, 1), (17, 1), (22, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 2), (32, 1), (33, 1), (43, 2), (52, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 80 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 189.7147 - loglik: -1.4412e+02 - logprior: -4.5594e+01
Epoch 2/2
10/10 - 1s - loss: 153.1451 - loglik: -1.3461e+02 - logprior: -1.8538e+01
Fitted a model with MAP estimate = -146.5991
expansions: []
discards: [44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 176.4286 - loglik: -1.3319e+02 - logprior: -4.3241e+01
Epoch 2/2
10/10 - 1s - loss: 142.5817 - loglik: -1.2987e+02 - logprior: -1.2714e+01
Fitted a model with MAP estimate = -135.4031
expansions: []
discards: [57]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 166.2187 - loglik: -1.2984e+02 - logprior: -3.6380e+01
Epoch 2/10
10/10 - 1s - loss: 137.8940 - loglik: -1.2852e+02 - logprior: -9.3724e+00
Epoch 3/10
10/10 - 1s - loss: 131.7092 - loglik: -1.2775e+02 - logprior: -3.9551e+00
Epoch 4/10
10/10 - 1s - loss: 129.1478 - loglik: -1.2716e+02 - logprior: -1.9848e+00
Epoch 5/10
10/10 - 1s - loss: 127.6729 - loglik: -1.2679e+02 - logprior: -8.8230e-01
Epoch 6/10
10/10 - 1s - loss: 127.2602 - loglik: -1.2726e+02 - logprior: -3.7085e-03
Epoch 7/10
10/10 - 1s - loss: 126.7463 - loglik: -1.2725e+02 - logprior: 0.5000
Epoch 8/10
10/10 - 1s - loss: 126.3296 - loglik: -1.2708e+02 - logprior: 0.7474
Epoch 9/10
10/10 - 1s - loss: 126.1477 - loglik: -1.2711e+02 - logprior: 0.9637
Epoch 10/10
10/10 - 1s - loss: 126.0039 - loglik: -1.2715e+02 - logprior: 1.1432
Fitted a model with MAP estimate = -125.8562
Time for alignment: 32.0600
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 269.5901 - loglik: -2.2898e+02 - logprior: -4.0608e+01
Epoch 2/10
10/10 - 1s - loss: 215.3177 - loglik: -2.0464e+02 - logprior: -1.0676e+01
Epoch 3/10
10/10 - 1s - loss: 180.8014 - loglik: -1.7552e+02 - logprior: -5.2765e+00
Epoch 4/10
10/10 - 1s - loss: 158.1080 - loglik: -1.5433e+02 - logprior: -3.7804e+00
Epoch 5/10
10/10 - 1s - loss: 148.6574 - loglik: -1.4555e+02 - logprior: -3.1050e+00
Epoch 6/10
10/10 - 1s - loss: 145.0636 - loglik: -1.4248e+02 - logprior: -2.5884e+00
Epoch 7/10
10/10 - 1s - loss: 143.7387 - loglik: -1.4149e+02 - logprior: -2.2452e+00
Epoch 8/10
10/10 - 1s - loss: 142.7014 - loglik: -1.4060e+02 - logprior: -2.1030e+00
Epoch 9/10
10/10 - 1s - loss: 142.4737 - loglik: -1.4047e+02 - logprior: -2.0078e+00
Epoch 10/10
10/10 - 1s - loss: 142.1701 - loglik: -1.4025e+02 - logprior: -1.9226e+00
Fitted a model with MAP estimate = -141.9239
expansions: [(13, 1), (14, 1), (15, 1), (17, 2), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1), (32, 1), (33, 1), (43, 2), (52, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 80 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 189.8015 - loglik: -1.4415e+02 - logprior: -4.5652e+01
Epoch 2/2
10/10 - 1s - loss: 153.2549 - loglik: -1.3467e+02 - logprior: -1.8583e+01
Fitted a model with MAP estimate = -146.7368
expansions: []
discards: [57 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 176.7859 - loglik: -1.3361e+02 - logprior: -4.3180e+01
Epoch 2/2
10/10 - 1s - loss: 142.7683 - loglik: -1.3013e+02 - logprior: -1.2637e+01
Fitted a model with MAP estimate = -135.7225
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 166.2684 - loglik: -1.2991e+02 - logprior: -3.6360e+01
Epoch 2/10
10/10 - 1s - loss: 137.8745 - loglik: -1.2854e+02 - logprior: -9.3333e+00
Epoch 3/10
10/10 - 1s - loss: 131.9745 - loglik: -1.2804e+02 - logprior: -3.9362e+00
Epoch 4/10
10/10 - 1s - loss: 129.1733 - loglik: -1.2720e+02 - logprior: -1.9729e+00
Epoch 5/10
10/10 - 1s - loss: 127.7690 - loglik: -1.2690e+02 - logprior: -8.6786e-01
Epoch 6/10
10/10 - 1s - loss: 127.2707 - loglik: -1.2728e+02 - logprior: 0.0087
Epoch 7/10
10/10 - 1s - loss: 126.7418 - loglik: -1.2726e+02 - logprior: 0.5197
Epoch 8/10
10/10 - 1s - loss: 126.2552 - loglik: -1.2702e+02 - logprior: 0.7685
Epoch 9/10
10/10 - 1s - loss: 126.2355 - loglik: -1.2722e+02 - logprior: 0.9803
Epoch 10/10
10/10 - 1s - loss: 125.9875 - loglik: -1.2715e+02 - logprior: 1.1605
Fitted a model with MAP estimate = -125.8561
Time for alignment: 31.4667
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 269.5244 - loglik: -2.2892e+02 - logprior: -4.0608e+01
Epoch 2/10
10/10 - 1s - loss: 215.5291 - loglik: -2.0485e+02 - logprior: -1.0675e+01
Epoch 3/10
10/10 - 1s - loss: 181.1124 - loglik: -1.7584e+02 - logprior: -5.2773e+00
Epoch 4/10
10/10 - 1s - loss: 158.5782 - loglik: -1.5476e+02 - logprior: -3.8169e+00
Epoch 5/10
10/10 - 1s - loss: 148.9246 - loglik: -1.4575e+02 - logprior: -3.1760e+00
Epoch 6/10
10/10 - 1s - loss: 144.5453 - loglik: -1.4176e+02 - logprior: -2.7871e+00
Epoch 7/10
10/10 - 1s - loss: 142.8263 - loglik: -1.4032e+02 - logprior: -2.5055e+00
Epoch 8/10
10/10 - 1s - loss: 141.7822 - loglik: -1.3940e+02 - logprior: -2.3805e+00
Epoch 9/10
10/10 - 1s - loss: 141.5263 - loglik: -1.3924e+02 - logprior: -2.2817e+00
Epoch 10/10
10/10 - 1s - loss: 140.9553 - loglik: -1.3879e+02 - logprior: -2.1679e+00
Fitted a model with MAP estimate = -140.9266
expansions: [(13, 1), (14, 1), (15, 1), (17, 1), (20, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1), (32, 1), (33, 1), (43, 2), (52, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 80 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 190.1034 - loglik: -1.4444e+02 - logprior: -4.5661e+01
Epoch 2/2
10/10 - 1s - loss: 152.9770 - loglik: -1.3439e+02 - logprior: -1.8584e+01
Fitted a model with MAP estimate = -146.6570
expansions: []
discards: [57 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.6909 - loglik: -1.3353e+02 - logprior: -4.3164e+01
Epoch 2/2
10/10 - 1s - loss: 142.4576 - loglik: -1.2983e+02 - logprior: -1.2629e+01
Fitted a model with MAP estimate = -135.6300
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 166.1358 - loglik: -1.2978e+02 - logprior: -3.6352e+01
Epoch 2/10
10/10 - 1s - loss: 137.8100 - loglik: -1.2848e+02 - logprior: -9.3281e+00
Epoch 3/10
10/10 - 1s - loss: 131.6970 - loglik: -1.2776e+02 - logprior: -3.9321e+00
Epoch 4/10
10/10 - 1s - loss: 128.9400 - loglik: -1.2696e+02 - logprior: -1.9759e+00
Epoch 5/10
10/10 - 1s - loss: 127.8521 - loglik: -1.2698e+02 - logprior: -8.6937e-01
Epoch 6/10
10/10 - 1s - loss: 127.0902 - loglik: -1.2710e+02 - logprior: 0.0060
Epoch 7/10
10/10 - 1s - loss: 126.5989 - loglik: -1.2711e+02 - logprior: 0.5119
Epoch 8/10
10/10 - 1s - loss: 126.4098 - loglik: -1.2717e+02 - logprior: 0.7637
Epoch 9/10
10/10 - 1s - loss: 125.9482 - loglik: -1.2693e+02 - logprior: 0.9772
Epoch 10/10
10/10 - 1s - loss: 126.1728 - loglik: -1.2733e+02 - logprior: 1.1589
Fitted a model with MAP estimate = -125.8359
Time for alignment: 30.4098
Computed alignments with likelihoods: ['-125.8507', '-125.8738', '-125.8562', '-125.8561', '-125.8359']
Best model has likelihood: -125.8359  (prior= 1.2374 )
time for generating output: 0.1159
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.8635879218472469
Training of 5 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdfdbeeb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91af2100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201b59d310>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2001e44670>
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 175.9697 - loglik: -1.7038e+02 - logprior: -5.5920e+00
Epoch 2/10
15/15 - 1s - loss: 151.7808 - loglik: -1.5010e+02 - logprior: -1.6820e+00
Epoch 3/10
15/15 - 1s - loss: 136.1949 - loglik: -1.3452e+02 - logprior: -1.6778e+00
Epoch 4/10
15/15 - 1s - loss: 129.3100 - loglik: -1.2763e+02 - logprior: -1.6755e+00
Epoch 5/10
15/15 - 1s - loss: 128.4428 - loglik: -1.2688e+02 - logprior: -1.5645e+00
Epoch 6/10
15/15 - 1s - loss: 127.8310 - loglik: -1.2625e+02 - logprior: -1.5811e+00
Epoch 7/10
15/15 - 1s - loss: 127.4162 - loglik: -1.2584e+02 - logprior: -1.5722e+00
Epoch 8/10
15/15 - 1s - loss: 127.1432 - loglik: -1.2558e+02 - logprior: -1.5586e+00
Epoch 9/10
15/15 - 1s - loss: 127.0583 - loglik: -1.2551e+02 - logprior: -1.5485e+00
Epoch 10/10
15/15 - 1s - loss: 126.8694 - loglik: -1.2532e+02 - logprior: -1.5447e+00
Fitted a model with MAP estimate = -126.8550
expansions: [(9, 2), (11, 2), (12, 2), (26, 1), (27, 3), (28, 2), (29, 2), (30, 1), (31, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 135.9810 - loglik: -1.2903e+02 - logprior: -6.9526e+00
Epoch 2/2
15/15 - 1s - loss: 127.0066 - loglik: -1.2367e+02 - logprior: -3.3337e+00
Fitted a model with MAP estimate = -125.3059
expansions: [(0, 2)]
discards: [ 0 12 15 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.7681 - loglik: -1.2266e+02 - logprior: -5.1089e+00
Epoch 2/2
15/15 - 1s - loss: 122.5906 - loglik: -1.2092e+02 - logprior: -1.6704e+00
Fitted a model with MAP estimate = -121.1897
expansions: []
discards: [ 0 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.6618 - loglik: -1.2310e+02 - logprior: -6.5583e+00
Epoch 2/10
15/15 - 1s - loss: 122.3346 - loglik: -1.2021e+02 - logprior: -2.1295e+00
Epoch 3/10
15/15 - 1s - loss: 121.0891 - loglik: -1.1968e+02 - logprior: -1.4125e+00
Epoch 4/10
15/15 - 1s - loss: 120.7154 - loglik: -1.1948e+02 - logprior: -1.2399e+00
Epoch 5/10
15/15 - 1s - loss: 120.4998 - loglik: -1.1933e+02 - logprior: -1.1736e+00
Epoch 6/10
15/15 - 1s - loss: 120.3275 - loglik: -1.1919e+02 - logprior: -1.1418e+00
Epoch 7/10
15/15 - 1s - loss: 120.3330 - loglik: -1.1921e+02 - logprior: -1.1183e+00
Fitted a model with MAP estimate = -120.1953
Time for alignment: 36.3022
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 175.9682 - loglik: -1.7037e+02 - logprior: -5.5943e+00
Epoch 2/10
15/15 - 1s - loss: 152.6809 - loglik: -1.5099e+02 - logprior: -1.6897e+00
Epoch 3/10
15/15 - 1s - loss: 136.1164 - loglik: -1.3439e+02 - logprior: -1.7311e+00
Epoch 4/10
15/15 - 1s - loss: 128.3104 - loglik: -1.2653e+02 - logprior: -1.7763e+00
Epoch 5/10
15/15 - 1s - loss: 126.9968 - loglik: -1.2533e+02 - logprior: -1.6716e+00
Epoch 6/10
15/15 - 1s - loss: 126.4417 - loglik: -1.2476e+02 - logprior: -1.6840e+00
Epoch 7/10
15/15 - 1s - loss: 126.1872 - loglik: -1.2453e+02 - logprior: -1.6606e+00
Epoch 8/10
15/15 - 1s - loss: 126.2619 - loglik: -1.2463e+02 - logprior: -1.6337e+00
Fitted a model with MAP estimate = -125.9903
expansions: [(9, 2), (11, 2), (12, 2), (19, 1), (25, 2), (26, 1), (27, 1), (28, 2), (29, 2), (31, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.5894 - loglik: -1.2962e+02 - logprior: -6.9664e+00
Epoch 2/2
15/15 - 1s - loss: 126.8937 - loglik: -1.2355e+02 - logprior: -3.3428e+00
Fitted a model with MAP estimate = -124.9763
expansions: []
discards: [12 15 31 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.0516 - loglik: -1.2198e+02 - logprior: -6.0699e+00
Epoch 2/2
15/15 - 1s - loss: 121.9820 - loglik: -1.2002e+02 - logprior: -1.9645e+00
Fitted a model with MAP estimate = -121.0744
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 125.6050 - loglik: -1.2016e+02 - logprior: -5.4401e+00
Epoch 2/10
15/15 - 1s - loss: 121.3843 - loglik: -1.1951e+02 - logprior: -1.8725e+00
Epoch 3/10
15/15 - 1s - loss: 120.9687 - loglik: -1.1956e+02 - logprior: -1.4045e+00
Epoch 4/10
15/15 - 1s - loss: 120.3848 - loglik: -1.1913e+02 - logprior: -1.2564e+00
Epoch 5/10
15/15 - 1s - loss: 120.3212 - loglik: -1.1913e+02 - logprior: -1.1942e+00
Epoch 6/10
15/15 - 1s - loss: 120.1884 - loglik: -1.1903e+02 - logprior: -1.1602e+00
Epoch 7/10
15/15 - 1s - loss: 120.1193 - loglik: -1.1899e+02 - logprior: -1.1256e+00
Epoch 8/10
15/15 - 1s - loss: 120.0001 - loglik: -1.1889e+02 - logprior: -1.1126e+00
Epoch 9/10
15/15 - 1s - loss: 119.8438 - loglik: -1.1875e+02 - logprior: -1.0973e+00
Epoch 10/10
15/15 - 1s - loss: 119.9060 - loglik: -1.1882e+02 - logprior: -1.0835e+00
Fitted a model with MAP estimate = -119.6949
Time for alignment: 37.1505
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 176.0018 - loglik: -1.7041e+02 - logprior: -5.5916e+00
Epoch 2/10
15/15 - 1s - loss: 152.3378 - loglik: -1.5066e+02 - logprior: -1.6810e+00
Epoch 3/10
15/15 - 1s - loss: 135.6783 - loglik: -1.3396e+02 - logprior: -1.7215e+00
Epoch 4/10
15/15 - 1s - loss: 128.8073 - loglik: -1.2705e+02 - logprior: -1.7616e+00
Epoch 5/10
15/15 - 1s - loss: 127.1044 - loglik: -1.2545e+02 - logprior: -1.6567e+00
Epoch 6/10
15/15 - 1s - loss: 126.6496 - loglik: -1.2498e+02 - logprior: -1.6684e+00
Epoch 7/10
15/15 - 1s - loss: 126.5413 - loglik: -1.2490e+02 - logprior: -1.6457e+00
Epoch 8/10
15/15 - 1s - loss: 126.2725 - loglik: -1.2466e+02 - logprior: -1.6171e+00
Epoch 9/10
15/15 - 1s - loss: 126.1524 - loglik: -1.2454e+02 - logprior: -1.6119e+00
Epoch 10/10
15/15 - 1s - loss: 126.1703 - loglik: -1.2457e+02 - logprior: -1.6009e+00
Fitted a model with MAP estimate = -126.0170
expansions: [(9, 2), (11, 2), (12, 2), (13, 2), (14, 1), (26, 1), (27, 1), (28, 2), (29, 2), (31, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.3330 - loglik: -1.2938e+02 - logprior: -6.9497e+00
Epoch 2/2
15/15 - 1s - loss: 127.2706 - loglik: -1.2390e+02 - logprior: -3.3707e+00
Fitted a model with MAP estimate = -125.4335
expansions: [(0, 2)]
discards: [ 0 15 18 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.9365 - loglik: -1.2282e+02 - logprior: -5.1182e+00
Epoch 2/2
15/15 - 1s - loss: 122.2510 - loglik: -1.2057e+02 - logprior: -1.6814e+00
Fitted a model with MAP estimate = -121.0362
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.4296 - loglik: -1.2289e+02 - logprior: -6.5414e+00
Epoch 2/10
15/15 - 1s - loss: 122.2488 - loglik: -1.2012e+02 - logprior: -2.1294e+00
Epoch 3/10
15/15 - 1s - loss: 120.9123 - loglik: -1.1950e+02 - logprior: -1.4150e+00
Epoch 4/10
15/15 - 1s - loss: 120.3483 - loglik: -1.1910e+02 - logprior: -1.2446e+00
Epoch 5/10
15/15 - 1s - loss: 120.5756 - loglik: -1.1941e+02 - logprior: -1.1702e+00
Fitted a model with MAP estimate = -120.2155
Time for alignment: 33.9967
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 175.8872 - loglik: -1.7030e+02 - logprior: -5.5912e+00
Epoch 2/10
15/15 - 1s - loss: 151.7348 - loglik: -1.5006e+02 - logprior: -1.6747e+00
Epoch 3/10
15/15 - 1s - loss: 134.6776 - loglik: -1.3297e+02 - logprior: -1.7105e+00
Epoch 4/10
15/15 - 1s - loss: 128.2832 - loglik: -1.2658e+02 - logprior: -1.7058e+00
Epoch 5/10
15/15 - 1s - loss: 127.3651 - loglik: -1.2576e+02 - logprior: -1.6018e+00
Epoch 6/10
15/15 - 1s - loss: 126.6765 - loglik: -1.2506e+02 - logprior: -1.6205e+00
Epoch 7/10
15/15 - 1s - loss: 126.7282 - loglik: -1.2513e+02 - logprior: -1.5982e+00
Fitted a model with MAP estimate = -126.4583
expansions: [(9, 2), (10, 3), (11, 2), (12, 1), (26, 1), (27, 1), (28, 2), (29, 2), (31, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 135.2793 - loglik: -1.2835e+02 - logprior: -6.9316e+00
Epoch 2/2
15/15 - 1s - loss: 126.5624 - loglik: -1.2329e+02 - logprior: -3.2692e+00
Fitted a model with MAP estimate = -124.5308
expansions: []
discards: [11 15]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.0436 - loglik: -1.2201e+02 - logprior: -6.0339e+00
Epoch 2/2
15/15 - 1s - loss: 121.8958 - loglik: -1.1993e+02 - logprior: -1.9683e+00
Fitted a model with MAP estimate = -121.0960
expansions: []
discards: [36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 125.9035 - loglik: -1.2047e+02 - logprior: -5.4359e+00
Epoch 2/10
15/15 - 1s - loss: 121.6491 - loglik: -1.1978e+02 - logprior: -1.8648e+00
Epoch 3/10
15/15 - 1s - loss: 120.8673 - loglik: -1.1946e+02 - logprior: -1.4090e+00
Epoch 4/10
15/15 - 1s - loss: 120.6129 - loglik: -1.1936e+02 - logprior: -1.2525e+00
Epoch 5/10
15/15 - 1s - loss: 120.5073 - loglik: -1.1932e+02 - logprior: -1.1869e+00
Epoch 6/10
15/15 - 1s - loss: 120.2705 - loglik: -1.1911e+02 - logprior: -1.1560e+00
Epoch 7/10
15/15 - 1s - loss: 120.3384 - loglik: -1.1921e+02 - logprior: -1.1291e+00
Fitted a model with MAP estimate = -120.1358
Time for alignment: 32.5674
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 175.8800 - loglik: -1.7029e+02 - logprior: -5.5921e+00
Epoch 2/10
15/15 - 1s - loss: 151.2016 - loglik: -1.4953e+02 - logprior: -1.6677e+00
Epoch 3/10
15/15 - 1s - loss: 134.8324 - loglik: -1.3316e+02 - logprior: -1.6695e+00
Epoch 4/10
15/15 - 1s - loss: 128.5762 - loglik: -1.2690e+02 - logprior: -1.6752e+00
Epoch 5/10
15/15 - 1s - loss: 127.5478 - loglik: -1.2598e+02 - logprior: -1.5640e+00
Epoch 6/10
15/15 - 1s - loss: 126.8497 - loglik: -1.2527e+02 - logprior: -1.5788e+00
Epoch 7/10
15/15 - 1s - loss: 126.7267 - loglik: -1.2517e+02 - logprior: -1.5549e+00
Epoch 8/10
15/15 - 1s - loss: 126.5965 - loglik: -1.2508e+02 - logprior: -1.5197e+00
Epoch 9/10
15/15 - 1s - loss: 126.5786 - loglik: -1.2507e+02 - logprior: -1.5134e+00
Epoch 10/10
15/15 - 1s - loss: 126.5033 - loglik: -1.2500e+02 - logprior: -1.5053e+00
Fitted a model with MAP estimate = -126.3803
expansions: [(9, 2), (10, 3), (11, 2), (12, 1), (27, 1), (29, 2), (30, 2), (31, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 134.9918 - loglik: -1.2808e+02 - logprior: -6.9083e+00
Epoch 2/2
15/15 - 1s - loss: 127.1415 - loglik: -1.2384e+02 - logprior: -3.2985e+00
Fitted a model with MAP estimate = -125.2115
expansions: [(0, 2)]
discards: [ 0 11 15]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.1467 - loglik: -1.2303e+02 - logprior: -5.1175e+00
Epoch 2/2
15/15 - 1s - loss: 122.5695 - loglik: -1.2088e+02 - logprior: -1.6851e+00
Fitted a model with MAP estimate = -121.2571
expansions: []
discards: [ 0 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.6976 - loglik: -1.2313e+02 - logprior: -6.5631e+00
Epoch 2/10
15/15 - 1s - loss: 122.4285 - loglik: -1.2030e+02 - logprior: -2.1320e+00
Epoch 3/10
15/15 - 1s - loss: 121.0576 - loglik: -1.1965e+02 - logprior: -1.4053e+00
Epoch 4/10
15/15 - 1s - loss: 120.6081 - loglik: -1.1937e+02 - logprior: -1.2411e+00
Epoch 5/10
15/15 - 1s - loss: 120.5184 - loglik: -1.1934e+02 - logprior: -1.1736e+00
Epoch 6/10
15/15 - 1s - loss: 120.3376 - loglik: -1.1920e+02 - logprior: -1.1345e+00
Epoch 7/10
15/15 - 1s - loss: 120.2707 - loglik: -1.1916e+02 - logprior: -1.1092e+00
Epoch 8/10
15/15 - 1s - loss: 120.2921 - loglik: -1.1919e+02 - logprior: -1.0974e+00
Fitted a model with MAP estimate = -120.0439
Time for alignment: 35.6694
Computed alignments with likelihoods: ['-120.1953', '-119.6949', '-120.2155', '-120.1358', '-120.0439']
Best model has likelihood: -119.6949  (prior= -1.0269 )
time for generating output: 0.8193
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9661290322580646
Training of 5 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200a327460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5d1f820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff94f8e20>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2001e44670>
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.7260 - loglik: -1.8440e+02 - logprior: -1.4329e+01
Epoch 2/10
10/10 - 1s - loss: 171.8774 - loglik: -1.6780e+02 - logprior: -4.0817e+00
Epoch 3/10
10/10 - 2s - loss: 154.9850 - loglik: -1.5258e+02 - logprior: -2.4099e+00
Epoch 4/10
10/10 - 2s - loss: 145.0598 - loglik: -1.4304e+02 - logprior: -2.0228e+00
Epoch 5/10
10/10 - 2s - loss: 141.1149 - loglik: -1.3916e+02 - logprior: -1.9554e+00
Epoch 6/10
10/10 - 2s - loss: 137.8941 - loglik: -1.3594e+02 - logprior: -1.9505e+00
Epoch 7/10
10/10 - 1s - loss: 136.1309 - loglik: -1.3422e+02 - logprior: -1.9082e+00
Epoch 8/10
10/10 - 2s - loss: 136.1357 - loglik: -1.3431e+02 - logprior: -1.8272e+00
Fitted a model with MAP estimate = -135.5252
expansions: [(4, 3), (5, 1), (9, 2), (25, 1), (27, 2), (29, 1), (33, 1), (38, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 148.3947 - loglik: -1.3237e+02 - logprior: -1.6026e+01
Epoch 2/2
10/10 - 2s - loss: 130.4636 - loglik: -1.2354e+02 - logprior: -6.9213e+00
Fitted a model with MAP estimate = -127.3923
expansions: [(0, 2)]
discards: [ 0 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 133.5989 - loglik: -1.2091e+02 - logprior: -1.2690e+01
Epoch 2/2
10/10 - 1s - loss: 123.3840 - loglik: -1.1989e+02 - logprior: -3.4953e+00
Fitted a model with MAP estimate = -121.9147
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 136.9117 - loglik: -1.2174e+02 - logprior: -1.5174e+01
Epoch 2/10
10/10 - 1s - loss: 125.5723 - loglik: -1.2059e+02 - logprior: -4.9872e+00
Epoch 3/10
10/10 - 2s - loss: 122.4454 - loglik: -1.1991e+02 - logprior: -2.5317e+00
Epoch 4/10
10/10 - 2s - loss: 121.1846 - loglik: -1.1961e+02 - logprior: -1.5718e+00
Epoch 5/10
10/10 - 2s - loss: 120.6117 - loglik: -1.1931e+02 - logprior: -1.2990e+00
Epoch 6/10
10/10 - 1s - loss: 120.2207 - loglik: -1.1916e+02 - logprior: -1.0608e+00
Epoch 7/10
10/10 - 1s - loss: 120.0286 - loglik: -1.1904e+02 - logprior: -9.8778e-01
Epoch 8/10
10/10 - 2s - loss: 119.7066 - loglik: -1.1878e+02 - logprior: -9.2868e-01
Epoch 9/10
10/10 - 2s - loss: 119.6026 - loglik: -1.1869e+02 - logprior: -9.1131e-01
Epoch 10/10
10/10 - 1s - loss: 119.0274 - loglik: -1.1814e+02 - logprior: -8.9065e-01
Fitted a model with MAP estimate = -119.2883
Time for alignment: 51.9602
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.8978 - loglik: -1.8457e+02 - logprior: -1.4330e+01
Epoch 2/10
10/10 - 1s - loss: 172.3234 - loglik: -1.6824e+02 - logprior: -4.0830e+00
Epoch 3/10
10/10 - 2s - loss: 156.1298 - loglik: -1.5371e+02 - logprior: -2.4191e+00
Epoch 4/10
10/10 - 1s - loss: 147.1279 - loglik: -1.4506e+02 - logprior: -2.0643e+00
Epoch 5/10
10/10 - 2s - loss: 141.4918 - loglik: -1.3942e+02 - logprior: -2.0753e+00
Epoch 6/10
10/10 - 2s - loss: 138.3978 - loglik: -1.3628e+02 - logprior: -2.1158e+00
Epoch 7/10
10/10 - 1s - loss: 136.9445 - loglik: -1.3485e+02 - logprior: -2.0938e+00
Epoch 8/10
10/10 - 1s - loss: 135.9697 - loglik: -1.3391e+02 - logprior: -2.0576e+00
Epoch 9/10
10/10 - 1s - loss: 135.1055 - loglik: -1.3306e+02 - logprior: -2.0443e+00
Epoch 10/10
10/10 - 2s - loss: 135.2974 - loglik: -1.3325e+02 - logprior: -2.0430e+00
Fitted a model with MAP estimate = -134.9816
expansions: [(4, 3), (5, 1), (9, 2), (24, 1), (26, 1), (27, 1), (28, 1), (33, 1), (36, 1), (37, 3), (38, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 150.1691 - loglik: -1.3411e+02 - logprior: -1.6057e+01
Epoch 2/2
10/10 - 2s - loss: 130.9090 - loglik: -1.2398e+02 - logprior: -6.9338e+00
Fitted a model with MAP estimate = -127.8354
expansions: [(0, 2)]
discards: [ 0 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 134.0485 - loglik: -1.2136e+02 - logprior: -1.2691e+01
Epoch 2/2
10/10 - 1s - loss: 123.8226 - loglik: -1.2033e+02 - logprior: -3.4899e+00
Fitted a model with MAP estimate = -121.9446
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 136.9707 - loglik: -1.2183e+02 - logprior: -1.5143e+01
Epoch 2/10
10/10 - 2s - loss: 125.5705 - loglik: -1.2062e+02 - logprior: -4.9500e+00
Epoch 3/10
10/10 - 1s - loss: 122.3281 - loglik: -1.1981e+02 - logprior: -2.5167e+00
Epoch 4/10
10/10 - 1s - loss: 120.8850 - loglik: -1.1931e+02 - logprior: -1.5726e+00
Epoch 5/10
10/10 - 1s - loss: 120.5466 - loglik: -1.1926e+02 - logprior: -1.2852e+00
Epoch 6/10
10/10 - 1s - loss: 120.5486 - loglik: -1.1949e+02 - logprior: -1.0603e+00
Fitted a model with MAP estimate = -120.0087
Time for alignment: 47.4397
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.7423 - loglik: -1.8441e+02 - logprior: -1.4330e+01
Epoch 2/10
10/10 - 2s - loss: 172.5123 - loglik: -1.6843e+02 - logprior: -4.0827e+00
Epoch 3/10
10/10 - 1s - loss: 154.8526 - loglik: -1.5244e+02 - logprior: -2.4142e+00
Epoch 4/10
10/10 - 1s - loss: 144.1409 - loglik: -1.4212e+02 - logprior: -2.0226e+00
Epoch 5/10
10/10 - 2s - loss: 140.0029 - loglik: -1.3803e+02 - logprior: -1.9760e+00
Epoch 6/10
10/10 - 2s - loss: 137.9534 - loglik: -1.3597e+02 - logprior: -1.9854e+00
Epoch 7/10
10/10 - 1s - loss: 136.8540 - loglik: -1.3493e+02 - logprior: -1.9282e+00
Epoch 8/10
10/10 - 1s - loss: 135.7646 - loglik: -1.3392e+02 - logprior: -1.8449e+00
Epoch 9/10
10/10 - 1s - loss: 135.2463 - loglik: -1.3342e+02 - logprior: -1.8276e+00
Epoch 10/10
10/10 - 2s - loss: 135.2329 - loglik: -1.3338e+02 - logprior: -1.8575e+00
Fitted a model with MAP estimate = -134.8985
expansions: [(4, 3), (5, 1), (9, 2), (25, 1), (26, 1), (27, 1), (28, 1), (33, 1), (38, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 149.1698 - loglik: -1.3314e+02 - logprior: -1.6031e+01
Epoch 2/2
10/10 - 1s - loss: 131.0811 - loglik: -1.2415e+02 - logprior: -6.9300e+00
Fitted a model with MAP estimate = -127.5279
expansions: [(0, 2)]
discards: [ 0 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 133.5985 - loglik: -1.2091e+02 - logprior: -1.2684e+01
Epoch 2/2
10/10 - 2s - loss: 123.4594 - loglik: -1.1997e+02 - logprior: -3.4885e+00
Fitted a model with MAP estimate = -121.8782
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 136.6324 - loglik: -1.2149e+02 - logprior: -1.5139e+01
Epoch 2/10
10/10 - 2s - loss: 125.9681 - loglik: -1.2103e+02 - logprior: -4.9333e+00
Epoch 3/10
10/10 - 1s - loss: 122.0462 - loglik: -1.1951e+02 - logprior: -2.5338e+00
Epoch 4/10
10/10 - 1s - loss: 121.6571 - loglik: -1.2009e+02 - logprior: -1.5700e+00
Epoch 5/10
10/10 - 1s - loss: 120.1457 - loglik: -1.1884e+02 - logprior: -1.3012e+00
Epoch 6/10
10/10 - 2s - loss: 120.2271 - loglik: -1.1916e+02 - logprior: -1.0639e+00
Fitted a model with MAP estimate = -119.9429
Time for alignment: 47.2850
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.5482 - loglik: -1.8422e+02 - logprior: -1.4329e+01
Epoch 2/10
10/10 - 2s - loss: 172.4966 - loglik: -1.6842e+02 - logprior: -4.0800e+00
Epoch 3/10
10/10 - 2s - loss: 154.5660 - loglik: -1.5215e+02 - logprior: -2.4157e+00
Epoch 4/10
10/10 - 2s - loss: 143.5003 - loglik: -1.4141e+02 - logprior: -2.0932e+00
Epoch 5/10
10/10 - 1s - loss: 138.9727 - loglik: -1.3684e+02 - logprior: -2.1296e+00
Epoch 6/10
10/10 - 1s - loss: 136.5160 - loglik: -1.3437e+02 - logprior: -2.1494e+00
Epoch 7/10
10/10 - 2s - loss: 135.8017 - loglik: -1.3373e+02 - logprior: -2.0687e+00
Epoch 8/10
10/10 - 1s - loss: 134.7279 - loglik: -1.3275e+02 - logprior: -1.9775e+00
Epoch 9/10
10/10 - 1s - loss: 134.4091 - loglik: -1.3247e+02 - logprior: -1.9375e+00
Epoch 10/10
10/10 - 1s - loss: 134.5823 - loglik: -1.3264e+02 - logprior: -1.9378e+00
Fitted a model with MAP estimate = -134.3284
expansions: [(4, 3), (5, 1), (9, 2), (25, 1), (27, 2), (28, 1), (29, 1), (32, 1), (37, 3), (38, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 148.5919 - loglik: -1.3252e+02 - logprior: -1.6070e+01
Epoch 2/2
10/10 - 2s - loss: 130.6222 - loglik: -1.2368e+02 - logprior: -6.9444e+00
Fitted a model with MAP estimate = -127.6608
expansions: [(0, 2)]
discards: [ 0 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 134.0403 - loglik: -1.2134e+02 - logprior: -1.2700e+01
Epoch 2/2
10/10 - 1s - loss: 123.0629 - loglik: -1.1956e+02 - logprior: -3.4984e+00
Fitted a model with MAP estimate = -121.9534
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 136.7299 - loglik: -1.2159e+02 - logprior: -1.5144e+01
Epoch 2/10
10/10 - 1s - loss: 125.7619 - loglik: -1.2081e+02 - logprior: -4.9532e+00
Epoch 3/10
10/10 - 1s - loss: 122.2329 - loglik: -1.1971e+02 - logprior: -2.5198e+00
Epoch 4/10
10/10 - 1s - loss: 121.6102 - loglik: -1.2005e+02 - logprior: -1.5614e+00
Epoch 5/10
10/10 - 1s - loss: 120.5140 - loglik: -1.1922e+02 - logprior: -1.2905e+00
Epoch 6/10
10/10 - 1s - loss: 119.9508 - loglik: -1.1890e+02 - logprior: -1.0547e+00
Epoch 7/10
10/10 - 2s - loss: 120.2541 - loglik: -1.1927e+02 - logprior: -9.8014e-01
Fitted a model with MAP estimate = -119.7581
Time for alignment: 49.0635
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.5845 - loglik: -1.8425e+02 - logprior: -1.4331e+01
Epoch 2/10
10/10 - 1s - loss: 172.3290 - loglik: -1.6824e+02 - logprior: -4.0851e+00
Epoch 3/10
10/10 - 2s - loss: 154.8233 - loglik: -1.5239e+02 - logprior: -2.4322e+00
Epoch 4/10
10/10 - 1s - loss: 143.9861 - loglik: -1.4186e+02 - logprior: -2.1239e+00
Epoch 5/10
10/10 - 2s - loss: 139.7317 - loglik: -1.3759e+02 - logprior: -2.1388e+00
Epoch 6/10
10/10 - 1s - loss: 137.0193 - loglik: -1.3487e+02 - logprior: -2.1524e+00
Epoch 7/10
10/10 - 2s - loss: 136.3201 - loglik: -1.3424e+02 - logprior: -2.0791e+00
Epoch 8/10
10/10 - 2s - loss: 134.9852 - loglik: -1.3300e+02 - logprior: -1.9883e+00
Epoch 9/10
10/10 - 1s - loss: 135.0979 - loglik: -1.3315e+02 - logprior: -1.9463e+00
Fitted a model with MAP estimate = -134.7971
expansions: [(4, 3), (5, 1), (9, 2), (25, 1), (27, 2), (29, 1), (33, 1), (36, 1), (37, 3), (38, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 148.3448 - loglik: -1.3229e+02 - logprior: -1.6058e+01
Epoch 2/2
10/10 - 1s - loss: 130.8988 - loglik: -1.2397e+02 - logprior: -6.9249e+00
Fitted a model with MAP estimate = -127.5797
expansions: [(0, 2)]
discards: [ 0 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 133.3581 - loglik: -1.2066e+02 - logprior: -1.2696e+01
Epoch 2/2
10/10 - 2s - loss: 124.1586 - loglik: -1.2066e+02 - logprior: -3.4970e+00
Fitted a model with MAP estimate = -121.9484
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 136.8005 - loglik: -1.2165e+02 - logprior: -1.5152e+01
Epoch 2/10
10/10 - 1s - loss: 125.6872 - loglik: -1.2073e+02 - logprior: -4.9606e+00
Epoch 3/10
10/10 - 1s - loss: 122.3069 - loglik: -1.1979e+02 - logprior: -2.5206e+00
Epoch 4/10
10/10 - 2s - loss: 121.1841 - loglik: -1.1961e+02 - logprior: -1.5711e+00
Epoch 5/10
10/10 - 2s - loss: 120.7756 - loglik: -1.1949e+02 - logprior: -1.2880e+00
Epoch 6/10
10/10 - 2s - loss: 120.3460 - loglik: -1.1929e+02 - logprior: -1.0610e+00
Epoch 7/10
10/10 - 1s - loss: 119.9052 - loglik: -1.1893e+02 - logprior: -9.7929e-01
Epoch 8/10
10/10 - 2s - loss: 119.7205 - loglik: -1.1879e+02 - logprior: -9.3067e-01
Epoch 9/10
10/10 - 1s - loss: 119.4389 - loglik: -1.1853e+02 - logprior: -9.1198e-01
Epoch 10/10
10/10 - 1s - loss: 119.2814 - loglik: -1.1839e+02 - logprior: -8.8656e-01
Fitted a model with MAP estimate = -119.2732
Time for alignment: 52.0633
Computed alignments with likelihoods: ['-119.2883', '-120.0087', '-119.9429', '-119.7581', '-119.2732']
Best model has likelihood: -119.2732  (prior= -0.8882 )
time for generating output: 0.2493
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.9959805373386926
Training of 5 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f804bc3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1efe63f9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200a27f400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2001e44670>
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 535.9849 - loglik: -5.2976e+02 - logprior: -6.2282e+00
Epoch 2/10
23/23 - 4s - loss: 465.3560 - loglik: -4.6453e+02 - logprior: -8.2780e-01
Epoch 3/10
23/23 - 4s - loss: 442.3364 - loglik: -4.4154e+02 - logprior: -8.0030e-01
Epoch 4/10
23/23 - 4s - loss: 436.5764 - loglik: -4.3591e+02 - logprior: -6.6234e-01
Epoch 5/10
23/23 - 4s - loss: 433.9589 - loglik: -4.3339e+02 - logprior: -5.7323e-01
Epoch 6/10
23/23 - 4s - loss: 433.4903 - loglik: -4.3294e+02 - logprior: -5.5028e-01
Epoch 7/10
23/23 - 4s - loss: 431.2539 - loglik: -4.3071e+02 - logprior: -5.4739e-01
Epoch 8/10
23/23 - 4s - loss: 432.3440 - loglik: -4.3181e+02 - logprior: -5.3008e-01
Fitted a model with MAP estimate = -431.4053
expansions: [(0, 8), (8, 3), (16, 1), (20, 1), (32, 1), (46, 1), (57, 2), (58, 1), (60, 1), (67, 1), (68, 1), (70, 1), (71, 1), (74, 3), (77, 1), (107, 1), (110, 1), (111, 2), (112, 1), (118, 1), (120, 1), (122, 1), (144, 1), (149, 1), (150, 2), (159, 6)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 445.2677 - loglik: -4.3708e+02 - logprior: -8.1856e+00
Epoch 2/2
23/23 - 6s - loss: 422.8448 - loglik: -4.2235e+02 - logprior: -4.9943e-01
Fitted a model with MAP estimate = -421.5067
expansions: [(0, 15)]
discards: [  1   2   3   4   5   6   7   8   9  18  73 141 198 199 200 201 202 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 201 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 436.5736 - loglik: -4.2803e+02 - logprior: -8.5457e+00
Epoch 2/2
23/23 - 6s - loss: 424.0519 - loglik: -4.2368e+02 - logprior: -3.7657e-01
Fitted a model with MAP estimate = -421.7205
expansions: [(0, 13), (201, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 19 20]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 436.6359 - loglik: -4.2817e+02 - logprior: -8.4635e+00
Epoch 2/10
23/23 - 6s - loss: 422.4324 - loglik: -4.2184e+02 - logprior: -5.9712e-01
Epoch 3/10
23/23 - 6s - loss: 420.1919 - loglik: -4.2111e+02 - logprior: 0.9191
Epoch 4/10
23/23 - 6s - loss: 416.1728 - loglik: -4.1720e+02 - logprior: 1.0280
Epoch 5/10
23/23 - 6s - loss: 415.4861 - loglik: -4.1665e+02 - logprior: 1.1687
Epoch 6/10
23/23 - 6s - loss: 414.1411 - loglik: -4.1543e+02 - logprior: 1.2917
Epoch 7/10
23/23 - 6s - loss: 412.4940 - loglik: -4.1390e+02 - logprior: 1.4062
Epoch 8/10
23/23 - 6s - loss: 414.5772 - loglik: -4.1608e+02 - logprior: 1.5065
Fitted a model with MAP estimate = -412.2319
Time for alignment: 140.8495
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 536.0736 - loglik: -5.2986e+02 - logprior: -6.2114e+00
Epoch 2/10
23/23 - 4s - loss: 464.7182 - loglik: -4.6403e+02 - logprior: -6.8978e-01
Epoch 3/10
23/23 - 4s - loss: 441.5364 - loglik: -4.4090e+02 - logprior: -6.3498e-01
Epoch 4/10
23/23 - 4s - loss: 435.9966 - loglik: -4.3549e+02 - logprior: -5.1131e-01
Epoch 5/10
23/23 - 4s - loss: 434.3249 - loglik: -4.3387e+02 - logprior: -4.5033e-01
Epoch 6/10
23/23 - 4s - loss: 432.3469 - loglik: -4.3190e+02 - logprior: -4.4263e-01
Epoch 7/10
23/23 - 4s - loss: 431.2919 - loglik: -4.3086e+02 - logprior: -4.3043e-01
Epoch 8/10
23/23 - 4s - loss: 431.4976 - loglik: -4.3108e+02 - logprior: -4.1481e-01
Fitted a model with MAP estimate = -431.1106
expansions: [(0, 8), (8, 5), (19, 1), (58, 2), (59, 2), (67, 1), (68, 1), (70, 1), (71, 1), (72, 2), (80, 1), (81, 2), (83, 1), (107, 1), (110, 1), (111, 2), (112, 1), (120, 1), (121, 1), (127, 1), (133, 1), (149, 3), (159, 6)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 442.9957 - loglik: -4.3479e+02 - logprior: -8.2081e+00
Epoch 2/2
23/23 - 6s - loss: 424.9796 - loglik: -4.2450e+02 - logprior: -4.8074e-01
Fitted a model with MAP estimate = -421.2443
expansions: [(0, 16)]
discards: [  1   2   3   4   5   6   7   8   9  18 106 142 199 200 201 202 203 204]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 436.3234 - loglik: -4.2775e+02 - logprior: -8.5747e+00
Epoch 2/2
23/23 - 6s - loss: 426.5077 - loglik: -4.2611e+02 - logprior: -3.9473e-01
Fitted a model with MAP estimate = -421.8769
expansions: [(0, 17), (92, 1), (203, 9)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  21  22
 100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 211 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 435.9377 - loglik: -4.2756e+02 - logprior: -8.3782e+00
Epoch 2/10
23/23 - 6s - loss: 424.4045 - loglik: -4.2376e+02 - logprior: -6.4520e-01
Epoch 3/10
23/23 - 6s - loss: 419.5665 - loglik: -4.2018e+02 - logprior: 0.6159
Epoch 4/10
23/23 - 6s - loss: 415.9982 - loglik: -4.1690e+02 - logprior: 0.9031
Epoch 5/10
23/23 - 6s - loss: 415.0801 - loglik: -4.1612e+02 - logprior: 1.0429
Epoch 6/10
23/23 - 6s - loss: 413.6072 - loglik: -4.1474e+02 - logprior: 1.1322
Epoch 7/10
23/23 - 6s - loss: 413.0481 - loglik: -4.1431e+02 - logprior: 1.2669
Epoch 8/10
23/23 - 6s - loss: 411.4492 - loglik: -4.1282e+02 - logprior: 1.3703
Epoch 9/10
23/23 - 6s - loss: 412.3533 - loglik: -4.1384e+02 - logprior: 1.4866
Fitted a model with MAP estimate = -411.2576
Time for alignment: 147.5832
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 536.8696 - loglik: -5.3063e+02 - logprior: -6.2423e+00
Epoch 2/10
23/23 - 4s - loss: 465.3496 - loglik: -4.6450e+02 - logprior: -8.5145e-01
Epoch 3/10
23/23 - 4s - loss: 442.7328 - loglik: -4.4191e+02 - logprior: -8.2447e-01
Epoch 4/10
23/23 - 4s - loss: 437.1880 - loglik: -4.3656e+02 - logprior: -6.2787e-01
Epoch 5/10
23/23 - 4s - loss: 434.4843 - loglik: -4.3393e+02 - logprior: -5.5287e-01
Epoch 6/10
23/23 - 4s - loss: 433.8635 - loglik: -4.3333e+02 - logprior: -5.2867e-01
Epoch 7/10
23/23 - 4s - loss: 433.5093 - loglik: -4.3299e+02 - logprior: -5.1452e-01
Epoch 8/10
23/23 - 4s - loss: 434.3651 - loglik: -4.3387e+02 - logprior: -4.9196e-01
Fitted a model with MAP estimate = -432.9549
expansions: [(0, 7), (10, 3), (17, 1), (33, 1), (44, 1), (51, 2), (58, 1), (67, 2), (68, 1), (71, 2), (72, 2), (75, 2), (78, 1), (83, 1), (107, 1), (110, 1), (111, 2), (112, 1), (114, 1), (120, 1), (127, 1), (148, 1), (149, 1), (150, 2), (159, 6)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 443.0667 - loglik: -4.3483e+02 - logprior: -8.2389e+00
Epoch 2/2
23/23 - 6s - loss: 425.5605 - loglik: -4.2507e+02 - logprior: -4.9156e-01
Fitted a model with MAP estimate = -421.5469
expansions: [(0, 17)]
discards: [  1   2   3   4   5   6   7   8   9  84  99 141 198 199 200 201 202 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 436.7070 - loglik: -4.2837e+02 - logprior: -8.3419e+00
Epoch 2/2
23/23 - 6s - loss: 425.0313 - loglik: -4.2473e+02 - logprior: -2.9685e-01
Fitted a model with MAP estimate = -421.5487
expansions: [(0, 18), (203, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 21 22 23 24 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 208 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 435.5149 - loglik: -4.2711e+02 - logprior: -8.4043e+00
Epoch 2/10
23/23 - 6s - loss: 422.7673 - loglik: -4.2205e+02 - logprior: -7.2042e-01
Epoch 3/10
23/23 - 6s - loss: 419.4189 - loglik: -4.2008e+02 - logprior: 0.6607
Epoch 4/10
23/23 - 6s - loss: 417.1543 - loglik: -4.1805e+02 - logprior: 0.8991
Epoch 5/10
23/23 - 6s - loss: 414.4656 - loglik: -4.1549e+02 - logprior: 1.0243
Epoch 6/10
23/23 - 6s - loss: 412.9089 - loglik: -4.1402e+02 - logprior: 1.1098
Epoch 7/10
23/23 - 6s - loss: 413.4045 - loglik: -4.1462e+02 - logprior: 1.2124
Fitted a model with MAP estimate = -412.2722
Time for alignment: 134.5254
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 536.4282 - loglik: -5.3021e+02 - logprior: -6.2199e+00
Epoch 2/10
23/23 - 4s - loss: 464.7627 - loglik: -4.6400e+02 - logprior: -7.6717e-01
Epoch 3/10
23/23 - 4s - loss: 444.2078 - loglik: -4.4355e+02 - logprior: -6.5293e-01
Epoch 4/10
23/23 - 4s - loss: 437.8519 - loglik: -4.3735e+02 - logprior: -5.0383e-01
Epoch 5/10
23/23 - 4s - loss: 437.4693 - loglik: -4.3701e+02 - logprior: -4.5985e-01
Epoch 6/10
23/23 - 4s - loss: 435.2456 - loglik: -4.3479e+02 - logprior: -4.5908e-01
Epoch 7/10
23/23 - 4s - loss: 433.2916 - loglik: -4.3285e+02 - logprior: -4.3700e-01
Epoch 8/10
23/23 - 4s - loss: 435.8713 - loglik: -4.3545e+02 - logprior: -4.2454e-01
Fitted a model with MAP estimate = -433.8728
expansions: [(0, 8), (10, 3), (16, 1), (20, 1), (32, 1), (46, 1), (57, 1), (58, 1), (60, 1), (66, 2), (67, 2), (69, 2), (107, 1), (109, 1), (112, 1), (114, 1), (115, 1), (120, 1), (127, 1), (130, 1), (149, 1), (150, 2), (159, 6)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 200 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 446.4290 - loglik: -4.3820e+02 - logprior: -8.2324e+00
Epoch 2/2
23/23 - 6s - loss: 426.3144 - loglik: -4.2587e+02 - logprior: -4.4166e-01
Fitted a model with MAP estimate = -423.3535
expansions: [(0, 14)]
discards: [  1   2   3   4   5   6   7   8   9  13  14  87 194 195 196 197 198 199]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 440.0569 - loglik: -4.3139e+02 - logprior: -8.6716e+00
Epoch 2/2
23/23 - 6s - loss: 424.3134 - loglik: -4.2399e+02 - logprior: -3.2775e-01
Fitted a model with MAP estimate = -423.1913
expansions: [(0, 14), (25, 1), (196, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 437.9234 - loglik: -4.2939e+02 - logprior: -8.5341e+00
Epoch 2/10
23/23 - 6s - loss: 423.8715 - loglik: -4.2329e+02 - logprior: -5.8412e-01
Epoch 3/10
23/23 - 6s - loss: 420.7970 - loglik: -4.2167e+02 - logprior: 0.8727
Epoch 4/10
23/23 - 6s - loss: 417.7471 - loglik: -4.1878e+02 - logprior: 1.0322
Epoch 5/10
23/23 - 6s - loss: 416.6854 - loglik: -4.1786e+02 - logprior: 1.1703
Epoch 6/10
23/23 - 6s - loss: 414.6732 - loglik: -4.1595e+02 - logprior: 1.2812
Epoch 7/10
23/23 - 6s - loss: 414.3010 - loglik: -4.1569e+02 - logprior: 1.3864
Epoch 8/10
23/23 - 6s - loss: 414.2798 - loglik: -4.1577e+02 - logprior: 1.4915
Epoch 9/10
23/23 - 6s - loss: 414.4087 - loglik: -4.1601e+02 - logprior: 1.6024
Fitted a model with MAP estimate = -413.1906
Time for alignment: 143.6750
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 535.4927 - loglik: -5.2927e+02 - logprior: -6.2184e+00
Epoch 2/10
23/23 - 4s - loss: 467.3819 - loglik: -4.6649e+02 - logprior: -8.9382e-01
Epoch 3/10
23/23 - 4s - loss: 442.8826 - loglik: -4.4195e+02 - logprior: -9.3270e-01
Epoch 4/10
23/23 - 4s - loss: 437.7484 - loglik: -4.3695e+02 - logprior: -8.0126e-01
Epoch 5/10
23/23 - 4s - loss: 433.0472 - loglik: -4.3232e+02 - logprior: -7.2598e-01
Epoch 6/10
23/23 - 4s - loss: 434.1730 - loglik: -4.3345e+02 - logprior: -7.2600e-01
Fitted a model with MAP estimate = -432.7983
expansions: [(0, 8), (9, 2), (10, 1), (16, 1), (17, 1), (19, 1), (46, 1), (57, 1), (58, 1), (67, 2), (68, 1), (70, 1), (71, 2), (77, 2), (78, 2), (106, 1), (108, 1), (110, 1), (111, 2), (112, 1), (114, 1), (120, 1), (127, 1), (131, 1), (132, 1), (148, 1), (149, 1), (159, 6)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 444.1822 - loglik: -4.3592e+02 - logprior: -8.2608e+00
Epoch 2/2
23/23 - 6s - loss: 425.9557 - loglik: -4.2537e+02 - logprior: -5.8861e-01
Fitted a model with MAP estimate = -422.3247
expansions: [(0, 12), (102, 1)]
discards: [  1   2   3   4   5   6   7   8  16  85  93 103 142 199 200 201 202 203
 204]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 199 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 8s - loss: 437.5037 - loglik: -4.2885e+02 - logprior: -8.6501e+00
Epoch 2/2
23/23 - 6s - loss: 424.4027 - loglik: -4.2405e+02 - logprior: -3.5388e-01
Fitted a model with MAP estimate = -421.9075
expansions: [(0, 12), (199, 9)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 208 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 437.0946 - loglik: -4.2848e+02 - logprior: -8.6106e+00
Epoch 2/10
23/23 - 6s - loss: 423.7943 - loglik: -4.2285e+02 - logprior: -9.4130e-01
Epoch 3/10
23/23 - 6s - loss: 420.1541 - loglik: -4.2083e+02 - logprior: 0.6801
Epoch 4/10
23/23 - 6s - loss: 416.8875 - loglik: -4.1785e+02 - logprior: 0.9652
Epoch 5/10
23/23 - 6s - loss: 416.2484 - loglik: -4.1737e+02 - logprior: 1.1253
Epoch 6/10
23/23 - 6s - loss: 414.1237 - loglik: -4.1535e+02 - logprior: 1.2220
Epoch 7/10
23/23 - 6s - loss: 413.2959 - loglik: -4.1466e+02 - logprior: 1.3593
Epoch 8/10
23/23 - 6s - loss: 412.5413 - loglik: -4.1401e+02 - logprior: 1.4689
Epoch 9/10
23/23 - 6s - loss: 413.3463 - loglik: -4.1494e+02 - logprior: 1.5973
Fitted a model with MAP estimate = -411.8005
Time for alignment: 137.9107
Computed alignments with likelihoods: ['-412.2319', '-411.2576', '-412.2722', '-413.1906', '-411.8005']
Best model has likelihood: -411.2576  (prior= 1.5667 )
time for generating output: 0.2275
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.7094563727729557
Training of 5 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef60dd250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f92d9c130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fac6ac4c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fac25eaf0>
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 251.1320 - loglik: -2.4999e+02 - logprior: -1.1384e+00
Epoch 2/10
29/29 - 3s - loss: 227.1751 - loglik: -2.2644e+02 - logprior: -7.3446e-01
Epoch 3/10
29/29 - 3s - loss: 222.0428 - loglik: -2.2121e+02 - logprior: -8.3062e-01
Epoch 4/10
29/29 - 3s - loss: 220.3371 - loglik: -2.1952e+02 - logprior: -8.1909e-01
Epoch 5/10
29/29 - 3s - loss: 220.0712 - loglik: -2.1927e+02 - logprior: -8.0229e-01
Epoch 6/10
29/29 - 3s - loss: 219.4947 - loglik: -2.1869e+02 - logprior: -8.0065e-01
Epoch 7/10
29/29 - 3s - loss: 218.6931 - loglik: -2.1790e+02 - logprior: -7.9430e-01
Epoch 8/10
29/29 - 3s - loss: 219.0598 - loglik: -2.1827e+02 - logprior: -7.8767e-01
Fitted a model with MAP estimate = -201.4550
expansions: [(1, 1), (2, 1), (13, 3), (14, 3), (40, 2), (41, 2), (42, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2), (51, 1), (53, 1), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 220.3014 - loglik: -2.1916e+02 - logprior: -1.1387e+00
Epoch 2/2
29/29 - 3s - loss: 216.8430 - loglik: -2.1609e+02 - logprior: -7.5512e-01
Fitted a model with MAP estimate = -197.8787
expansions: []
discards: [ 1 49 52 59 61 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 218.4777 - loglik: -2.1746e+02 - logprior: -1.0139e+00
Epoch 2/2
29/29 - 3s - loss: 216.5623 - loglik: -2.1585e+02 - logprior: -7.0820e-01
Fitted a model with MAP estimate = -198.1998
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.9706 - loglik: -1.9730e+02 - logprior: -6.7040e-01
Epoch 2/10
42/42 - 4s - loss: 197.2939 - loglik: -1.9680e+02 - logprior: -4.8939e-01
Epoch 3/10
42/42 - 4s - loss: 196.1064 - loglik: -1.9563e+02 - logprior: -4.7839e-01
Epoch 4/10
42/42 - 4s - loss: 196.1859 - loglik: -1.9571e+02 - logprior: -4.7428e-01
Fitted a model with MAP estimate = -195.3080
Time for alignment: 98.3971
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 251.4466 - loglik: -2.5031e+02 - logprior: -1.1371e+00
Epoch 2/10
29/29 - 3s - loss: 227.2096 - loglik: -2.2647e+02 - logprior: -7.3860e-01
Epoch 3/10
29/29 - 3s - loss: 221.7184 - loglik: -2.2088e+02 - logprior: -8.4006e-01
Epoch 4/10
29/29 - 3s - loss: 220.6663 - loglik: -2.1984e+02 - logprior: -8.2569e-01
Epoch 5/10
29/29 - 3s - loss: 220.1271 - loglik: -2.1932e+02 - logprior: -8.1042e-01
Epoch 6/10
29/29 - 3s - loss: 219.5439 - loglik: -2.1874e+02 - logprior: -8.0306e-01
Epoch 7/10
29/29 - 3s - loss: 219.1796 - loglik: -2.1838e+02 - logprior: -8.0068e-01
Epoch 8/10
29/29 - 3s - loss: 218.9569 - loglik: -2.1816e+02 - logprior: -7.9394e-01
Epoch 9/10
29/29 - 3s - loss: 218.7140 - loglik: -2.1793e+02 - logprior: -7.8782e-01
Epoch 10/10
29/29 - 3s - loss: 218.9121 - loglik: -2.1813e+02 - logprior: -7.8230e-01
Fitted a model with MAP estimate = -201.3254
expansions: [(1, 1), (2, 1), (12, 1), (13, 2), (14, 2), (40, 2), (41, 2), (42, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2), (51, 1), (53, 1), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 220.4425 - loglik: -2.1930e+02 - logprior: -1.1432e+00
Epoch 2/2
29/29 - 3s - loss: 216.6606 - loglik: -2.1591e+02 - logprior: -7.5566e-01
Fitted a model with MAP estimate = -197.8883
expansions: [(20, 1)]
discards: [ 1 48 51 58 60 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 218.3477 - loglik: -2.1734e+02 - logprior: -1.0109e+00
Epoch 2/2
29/29 - 3s - loss: 216.5193 - loglik: -2.1581e+02 - logprior: -7.1047e-01
Fitted a model with MAP estimate = -198.2546
expansions: [(18, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.7873 - loglik: -1.9712e+02 - logprior: -6.6834e-01
Epoch 2/10
42/42 - 4s - loss: 197.3478 - loglik: -1.9686e+02 - logprior: -4.8411e-01
Epoch 3/10
42/42 - 4s - loss: 196.6008 - loglik: -1.9613e+02 - logprior: -4.7457e-01
Epoch 4/10
42/42 - 4s - loss: 195.7532 - loglik: -1.9528e+02 - logprior: -4.7356e-01
Epoch 5/10
42/42 - 4s - loss: 195.0058 - loglik: -1.9454e+02 - logprior: -4.6970e-01
Epoch 6/10
42/42 - 4s - loss: 194.9606 - loglik: -1.9450e+02 - logprior: -4.6405e-01
Epoch 7/10
42/42 - 4s - loss: 194.8977 - loglik: -1.9444e+02 - logprior: -4.5830e-01
Epoch 8/10
42/42 - 4s - loss: 194.8364 - loglik: -1.9438e+02 - logprior: -4.5658e-01
Epoch 9/10
42/42 - 4s - loss: 194.6129 - loglik: -1.9416e+02 - logprior: -4.5015e-01
Epoch 10/10
42/42 - 4s - loss: 194.6661 - loglik: -1.9422e+02 - logprior: -4.4937e-01
Fitted a model with MAP estimate = -194.5385
Time for alignment: 127.1000
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 251.3698 - loglik: -2.5023e+02 - logprior: -1.1361e+00
Epoch 2/10
29/29 - 3s - loss: 227.0257 - loglik: -2.2629e+02 - logprior: -7.3741e-01
Epoch 3/10
29/29 - 3s - loss: 222.0573 - loglik: -2.2123e+02 - logprior: -8.2869e-01
Epoch 4/10
29/29 - 3s - loss: 221.0953 - loglik: -2.2028e+02 - logprior: -8.1302e-01
Epoch 5/10
29/29 - 3s - loss: 219.4719 - loglik: -2.1867e+02 - logprior: -8.0426e-01
Epoch 6/10
29/29 - 3s - loss: 219.7115 - loglik: -2.1892e+02 - logprior: -7.9578e-01
Fitted a model with MAP estimate = -201.5689
expansions: [(1, 1), (2, 1), (13, 3), (14, 3), (40, 2), (41, 1), (42, 1), (43, 1), (44, 2), (45, 2), (48, 1), (49, 2), (51, 1), (53, 1), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 220.1690 - loglik: -2.1903e+02 - logprior: -1.1352e+00
Epoch 2/2
29/29 - 3s - loss: 216.7220 - loglik: -2.1597e+02 - logprior: -7.5051e-01
Fitted a model with MAP estimate = -198.0016
expansions: []
discards: [ 1 49 58 60 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 218.5248 - loglik: -2.1751e+02 - logprior: -1.0173e+00
Epoch 2/2
29/29 - 3s - loss: 216.5647 - loglik: -2.1586e+02 - logprior: -7.0967e-01
Fitted a model with MAP estimate = -198.2168
expansions: []
discards: [16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 198.2758 - loglik: -1.9761e+02 - logprior: -6.7077e-01
Epoch 2/10
42/42 - 4s - loss: 197.0291 - loglik: -1.9654e+02 - logprior: -4.8518e-01
Epoch 3/10
42/42 - 4s - loss: 196.3311 - loglik: -1.9585e+02 - logprior: -4.7937e-01
Epoch 4/10
42/42 - 4s - loss: 195.9634 - loglik: -1.9549e+02 - logprior: -4.7135e-01
Epoch 5/10
42/42 - 4s - loss: 195.5817 - loglik: -1.9512e+02 - logprior: -4.6629e-01
Epoch 6/10
42/42 - 4s - loss: 195.2588 - loglik: -1.9479e+02 - logprior: -4.6391e-01
Epoch 7/10
42/42 - 4s - loss: 195.0921 - loglik: -1.9464e+02 - logprior: -4.5543e-01
Epoch 8/10
42/42 - 4s - loss: 194.7919 - loglik: -1.9434e+02 - logprior: -4.5525e-01
Epoch 9/10
42/42 - 4s - loss: 194.7660 - loglik: -1.9432e+02 - logprior: -4.4797e-01
Epoch 10/10
42/42 - 4s - loss: 194.6680 - loglik: -1.9422e+02 - logprior: -4.4566e-01
Fitted a model with MAP estimate = -194.6754
Time for alignment: 115.5941
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 251.4200 - loglik: -2.5029e+02 - logprior: -1.1344e+00
Epoch 2/10
29/29 - 3s - loss: 226.9186 - loglik: -2.2619e+02 - logprior: -7.2453e-01
Epoch 3/10
29/29 - 3s - loss: 222.0535 - loglik: -2.2122e+02 - logprior: -8.3376e-01
Epoch 4/10
29/29 - 3s - loss: 220.2608 - loglik: -2.1944e+02 - logprior: -8.2510e-01
Epoch 5/10
29/29 - 3s - loss: 220.1235 - loglik: -2.1931e+02 - logprior: -8.0958e-01
Epoch 6/10
29/29 - 3s - loss: 219.6575 - loglik: -2.1885e+02 - logprior: -8.0674e-01
Epoch 7/10
29/29 - 3s - loss: 218.8180 - loglik: -2.1802e+02 - logprior: -7.9947e-01
Epoch 8/10
29/29 - 3s - loss: 219.2524 - loglik: -2.1846e+02 - logprior: -7.9133e-01
Fitted a model with MAP estimate = -201.3892
expansions: [(1, 1), (2, 1), (12, 1), (13, 2), (14, 2), (40, 2), (41, 1), (42, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2), (51, 1), (53, 1), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 220.5935 - loglik: -2.1946e+02 - logprior: -1.1369e+00
Epoch 2/2
29/29 - 3s - loss: 216.2675 - loglik: -2.1552e+02 - logprior: -7.4695e-01
Fitted a model with MAP estimate = -197.8641
expansions: [(20, 1)]
discards: [ 1 48 57 59 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 218.3190 - loglik: -2.1731e+02 - logprior: -1.0135e+00
Epoch 2/2
29/29 - 3s - loss: 216.6561 - loglik: -2.1594e+02 - logprior: -7.1221e-01
Fitted a model with MAP estimate = -198.1553
expansions: [(18, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.9585 - loglik: -1.9729e+02 - logprior: -6.7340e-01
Epoch 2/10
42/42 - 4s - loss: 197.1470 - loglik: -1.9666e+02 - logprior: -4.8739e-01
Epoch 3/10
42/42 - 4s - loss: 196.0324 - loglik: -1.9555e+02 - logprior: -4.8567e-01
Epoch 4/10
42/42 - 4s - loss: 196.0679 - loglik: -1.9559e+02 - logprior: -4.7688e-01
Fitted a model with MAP estimate = -195.2612
Time for alignment: 97.5707
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 251.2865 - loglik: -2.5015e+02 - logprior: -1.1379e+00
Epoch 2/10
29/29 - 3s - loss: 226.8095 - loglik: -2.2606e+02 - logprior: -7.4607e-01
Epoch 3/10
29/29 - 3s - loss: 221.8976 - loglik: -2.2106e+02 - logprior: -8.3573e-01
Epoch 4/10
29/29 - 3s - loss: 220.9828 - loglik: -2.2016e+02 - logprior: -8.1824e-01
Epoch 5/10
29/29 - 3s - loss: 220.0268 - loglik: -2.1922e+02 - logprior: -8.0965e-01
Epoch 6/10
29/29 - 3s - loss: 219.3169 - loglik: -2.1851e+02 - logprior: -8.0347e-01
Epoch 7/10
29/29 - 3s - loss: 219.3471 - loglik: -2.1855e+02 - logprior: -7.9320e-01
Fitted a model with MAP estimate = -201.4671
expansions: [(1, 1), (2, 1), (13, 2), (14, 3), (40, 2), (41, 2), (42, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2), (51, 1), (53, 1), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 220.8037 - loglik: -2.1966e+02 - logprior: -1.1468e+00
Epoch 2/2
29/29 - 3s - loss: 216.3598 - loglik: -2.1560e+02 - logprior: -7.6066e-01
Fitted a model with MAP estimate = -198.0199
expansions: [(18, 1)]
discards: [ 1 48 51 58 61 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 218.4201 - loglik: -2.1740e+02 - logprior: -1.0167e+00
Epoch 2/2
29/29 - 3s - loss: 217.0937 - loglik: -2.1638e+02 - logprior: -7.1366e-01
Fitted a model with MAP estimate = -198.2343
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 198.1455 - loglik: -1.9747e+02 - logprior: -6.7416e-01
Epoch 2/10
42/42 - 4s - loss: 197.0030 - loglik: -1.9652e+02 - logprior: -4.8461e-01
Epoch 3/10
42/42 - 4s - loss: 196.6273 - loglik: -1.9615e+02 - logprior: -4.7624e-01
Epoch 4/10
42/42 - 4s - loss: 195.7014 - loglik: -1.9523e+02 - logprior: -4.7378e-01
Epoch 5/10
42/42 - 4s - loss: 195.8068 - loglik: -1.9534e+02 - logprior: -4.6741e-01
Fitted a model with MAP estimate = -195.0539
Time for alignment: 98.2289
Computed alignments with likelihoods: ['-195.3080', '-194.5385', '-194.6754', '-195.2612', '-195.0539']
Best model has likelihood: -194.5385  (prior= -0.4525 )
time for generating output: 0.1902
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.6080079090459714
Training of 5 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff0efac40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef6060c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef6143130>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2034e80f70>
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 384.0081 - loglik: -3.3881e+02 - logprior: -4.5196e+01
Epoch 2/10
10/10 - 1s - loss: 309.3623 - loglik: -2.9863e+02 - logprior: -1.0731e+01
Epoch 3/10
10/10 - 1s - loss: 260.5228 - loglik: -2.5554e+02 - logprior: -4.9837e+00
Epoch 4/10
10/10 - 1s - loss: 227.3070 - loglik: -2.2373e+02 - logprior: -3.5767e+00
Epoch 5/10
10/10 - 1s - loss: 215.4305 - loglik: -2.1269e+02 - logprior: -2.7415e+00
Epoch 6/10
10/10 - 1s - loss: 210.0307 - loglik: -2.0790e+02 - logprior: -2.1304e+00
Epoch 7/10
10/10 - 1s - loss: 206.9700 - loglik: -2.0526e+02 - logprior: -1.7134e+00
Epoch 8/10
10/10 - 1s - loss: 204.9184 - loglik: -2.0338e+02 - logprior: -1.5400e+00
Epoch 9/10
10/10 - 1s - loss: 204.0096 - loglik: -2.0254e+02 - logprior: -1.4674e+00
Epoch 10/10
10/10 - 1s - loss: 203.3901 - loglik: -2.0204e+02 - logprior: -1.3515e+00
Fitted a model with MAP estimate = -203.1004
expansions: [(7, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 2), (30, 1), (38, 1), (45, 1), (46, 2), (51, 2), (59, 1), (63, 1), (67, 1), (68, 2), (80, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 254.0328 - loglik: -2.0278e+02 - logprior: -5.1249e+01
Epoch 2/2
10/10 - 1s - loss: 210.5849 - loglik: -1.9053e+02 - logprior: -2.0055e+01
Fitted a model with MAP estimate = -203.5028
expansions: [(0, 3)]
discards: [ 0 12 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 228.9737 - loglik: -1.8861e+02 - logprior: -4.0360e+01
Epoch 2/2
10/10 - 1s - loss: 194.3572 - loglik: -1.8503e+02 - logprior: -9.3264e+00
Fitted a model with MAP estimate = -189.0142
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 236.9995 - loglik: -1.8765e+02 - logprior: -4.9346e+01
Epoch 2/10
10/10 - 1s - loss: 201.0883 - loglik: -1.8591e+02 - logprior: -1.5180e+01
Epoch 3/10
10/10 - 1s - loss: 189.0039 - loglik: -1.8464e+02 - logprior: -4.3607e+00
Epoch 4/10
10/10 - 1s - loss: 184.1626 - loglik: -1.8356e+02 - logprior: -6.0671e-01
Epoch 5/10
10/10 - 1s - loss: 181.9646 - loglik: -1.8284e+02 - logprior: 0.8777
Epoch 6/10
10/10 - 1s - loss: 180.4352 - loglik: -1.8237e+02 - logprior: 1.9301
Epoch 7/10
10/10 - 1s - loss: 179.3693 - loglik: -1.8199e+02 - logprior: 2.6170
Epoch 8/10
10/10 - 1s - loss: 178.4394 - loglik: -1.8141e+02 - logprior: 2.9752
Epoch 9/10
10/10 - 1s - loss: 177.6965 - loglik: -1.8096e+02 - logprior: 3.2651
Epoch 10/10
10/10 - 1s - loss: 177.4686 - loglik: -1.8102e+02 - logprior: 3.5473
Fitted a model with MAP estimate = -177.0339
Time for alignment: 44.2013
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 383.8470 - loglik: -3.3865e+02 - logprior: -4.5198e+01
Epoch 2/10
10/10 - 1s - loss: 309.4936 - loglik: -2.9876e+02 - logprior: -1.0734e+01
Epoch 3/10
10/10 - 1s - loss: 260.6992 - loglik: -2.5566e+02 - logprior: -5.0351e+00
Epoch 4/10
10/10 - 1s - loss: 228.1674 - loglik: -2.2454e+02 - logprior: -3.6268e+00
Epoch 5/10
10/10 - 1s - loss: 215.4737 - loglik: -2.1287e+02 - logprior: -2.6030e+00
Epoch 6/10
10/10 - 1s - loss: 209.6260 - loglik: -2.0788e+02 - logprior: -1.7474e+00
Epoch 7/10
10/10 - 1s - loss: 206.8554 - loglik: -2.0554e+02 - logprior: -1.3130e+00
Epoch 8/10
10/10 - 1s - loss: 205.1219 - loglik: -2.0398e+02 - logprior: -1.1377e+00
Epoch 9/10
10/10 - 1s - loss: 203.6728 - loglik: -2.0264e+02 - logprior: -1.0292e+00
Epoch 10/10
10/10 - 1s - loss: 202.5838 - loglik: -2.0162e+02 - logprior: -9.6685e-01
Fitted a model with MAP estimate = -202.3701
expansions: [(7, 1), (9, 2), (12, 1), (13, 2), (30, 1), (45, 2), (46, 2), (48, 2), (51, 2), (59, 1), (63, 1), (67, 1), (68, 2), (80, 1), (81, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 255.6700 - loglik: -2.0441e+02 - logprior: -5.1256e+01
Epoch 2/2
10/10 - 1s - loss: 211.7393 - loglik: -1.9149e+02 - logprior: -2.0249e+01
Fitted a model with MAP estimate = -204.0166
expansions: [(0, 3)]
discards: [ 0 51 52 58]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 230.1884 - loglik: -1.8977e+02 - logprior: -4.0420e+01
Epoch 2/2
10/10 - 1s - loss: 194.6063 - loglik: -1.8526e+02 - logprior: -9.3500e+00
Fitted a model with MAP estimate = -189.5305
expansions: [(11, 1)]
discards: [  0   2 100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 238.8246 - loglik: -1.8918e+02 - logprior: -4.9648e+01
Epoch 2/10
10/10 - 1s - loss: 203.6977 - loglik: -1.8738e+02 - logprior: -1.6321e+01
Epoch 3/10
10/10 - 1s - loss: 190.5880 - loglik: -1.8547e+02 - logprior: -5.1214e+00
Epoch 4/10
10/10 - 1s - loss: 185.5258 - loglik: -1.8479e+02 - logprior: -7.3327e-01
Epoch 5/10
10/10 - 1s - loss: 182.1936 - loglik: -1.8304e+02 - logprior: 0.8420
Epoch 6/10
10/10 - 1s - loss: 181.4138 - loglik: -1.8331e+02 - logprior: 1.8981
Epoch 7/10
10/10 - 1s - loss: 179.6836 - loglik: -1.8227e+02 - logprior: 2.5831
Epoch 8/10
10/10 - 1s - loss: 178.9815 - loglik: -1.8192e+02 - logprior: 2.9418
Epoch 9/10
10/10 - 1s - loss: 178.3546 - loglik: -1.8157e+02 - logprior: 3.2149
Epoch 10/10
10/10 - 1s - loss: 177.3942 - loglik: -1.8088e+02 - logprior: 3.4871
Fitted a model with MAP estimate = -177.3121
Time for alignment: 42.7523
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 384.2656 - loglik: -3.3907e+02 - logprior: -4.5198e+01
Epoch 2/10
10/10 - 1s - loss: 309.2589 - loglik: -2.9852e+02 - logprior: -1.0737e+01
Epoch 3/10
10/10 - 1s - loss: 261.0851 - loglik: -2.5601e+02 - logprior: -5.0711e+00
Epoch 4/10
10/10 - 1s - loss: 228.4960 - loglik: -2.2462e+02 - logprior: -3.8771e+00
Epoch 5/10
10/10 - 1s - loss: 214.0760 - loglik: -2.1077e+02 - logprior: -3.3027e+00
Epoch 6/10
10/10 - 1s - loss: 208.5223 - loglik: -2.0581e+02 - logprior: -2.7143e+00
Epoch 7/10
10/10 - 1s - loss: 206.2693 - loglik: -2.0400e+02 - logprior: -2.2676e+00
Epoch 8/10
10/10 - 1s - loss: 204.7863 - loglik: -2.0274e+02 - logprior: -2.0435e+00
Epoch 9/10
10/10 - 1s - loss: 204.0747 - loglik: -2.0213e+02 - logprior: -1.9413e+00
Epoch 10/10
10/10 - 1s - loss: 202.9707 - loglik: -2.0117e+02 - logprior: -1.7962e+00
Fitted a model with MAP estimate = -202.6569
expansions: [(7, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 2), (30, 1), (44, 1), (45, 1), (48, 2), (51, 1), (57, 1), (59, 1), (63, 1), (67, 1), (68, 1), (70, 1), (80, 1), (81, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 256.3605 - loglik: -2.0520e+02 - logprior: -5.1164e+01
Epoch 2/2
10/10 - 1s - loss: 211.4396 - loglik: -1.9144e+02 - logprior: -2.0004e+01
Fitted a model with MAP estimate = -203.4921
expansions: [(0, 3)]
discards: [ 0 12 58]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 228.6834 - loglik: -1.8843e+02 - logprior: -4.0256e+01
Epoch 2/2
10/10 - 1s - loss: 193.0974 - loglik: -1.8386e+02 - logprior: -9.2330e+00
Fitted a model with MAP estimate = -187.8919
expansions: []
discards: [  0   2 101]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 236.7688 - loglik: -1.8745e+02 - logprior: -4.9320e+01
Epoch 2/10
10/10 - 1s - loss: 200.8956 - loglik: -1.8558e+02 - logprior: -1.5312e+01
Epoch 3/10
10/10 - 1s - loss: 188.9372 - loglik: -1.8450e+02 - logprior: -4.4376e+00
Epoch 4/10
10/10 - 1s - loss: 183.9643 - loglik: -1.8338e+02 - logprior: -5.8647e-01
Epoch 5/10
10/10 - 1s - loss: 181.1401 - loglik: -1.8206e+02 - logprior: 0.9161
Epoch 6/10
10/10 - 1s - loss: 179.8681 - loglik: -1.8182e+02 - logprior: 1.9530
Epoch 7/10
10/10 - 1s - loss: 179.0652 - loglik: -1.8171e+02 - logprior: 2.6426
Epoch 8/10
10/10 - 1s - loss: 177.9528 - loglik: -1.8096e+02 - logprior: 3.0114
Epoch 9/10
10/10 - 1s - loss: 177.2786 - loglik: -1.8059e+02 - logprior: 3.3069
Epoch 10/10
10/10 - 1s - loss: 177.3350 - loglik: -1.8093e+02 - logprior: 3.5943
Fitted a model with MAP estimate = -176.7801
Time for alignment: 42.4438
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 384.2130 - loglik: -3.3901e+02 - logprior: -4.5198e+01
Epoch 2/10
10/10 - 1s - loss: 309.3157 - loglik: -2.9858e+02 - logprior: -1.0734e+01
Epoch 3/10
10/10 - 1s - loss: 261.4185 - loglik: -2.5642e+02 - logprior: -5.0001e+00
Epoch 4/10
10/10 - 1s - loss: 228.0915 - loglik: -2.2447e+02 - logprior: -3.6194e+00
Epoch 5/10
10/10 - 1s - loss: 215.5413 - loglik: -2.1275e+02 - logprior: -2.7867e+00
Epoch 6/10
10/10 - 1s - loss: 209.4980 - loglik: -2.0738e+02 - logprior: -2.1177e+00
Epoch 7/10
10/10 - 1s - loss: 206.5746 - loglik: -2.0489e+02 - logprior: -1.6867e+00
Epoch 8/10
10/10 - 1s - loss: 204.6602 - loglik: -2.0311e+02 - logprior: -1.5475e+00
Epoch 9/10
10/10 - 1s - loss: 203.5090 - loglik: -2.0205e+02 - logprior: -1.4545e+00
Epoch 10/10
10/10 - 1s - loss: 203.1856 - loglik: -2.0188e+02 - logprior: -1.3035e+00
Fitted a model with MAP estimate = -202.6650
expansions: [(7, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 2), (30, 1), (38, 1), (45, 1), (46, 2), (51, 2), (59, 1), (63, 1), (67, 1), (68, 2), (80, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 253.4008 - loglik: -2.0217e+02 - logprior: -5.1228e+01
Epoch 2/2
10/10 - 1s - loss: 210.4346 - loglik: -1.9038e+02 - logprior: -2.0053e+01
Fitted a model with MAP estimate = -203.0314
expansions: [(0, 3)]
discards: [ 0 12 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 228.6438 - loglik: -1.8829e+02 - logprior: -4.0355e+01
Epoch 2/2
10/10 - 1s - loss: 194.1292 - loglik: -1.8480e+02 - logprior: -9.3309e+00
Fitted a model with MAP estimate = -188.6477
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 236.4115 - loglik: -1.8701e+02 - logprior: -4.9397e+01
Epoch 2/10
10/10 - 1s - loss: 201.1698 - loglik: -1.8579e+02 - logprior: -1.5382e+01
Epoch 3/10
10/10 - 1s - loss: 188.8993 - loglik: -1.8443e+02 - logprior: -4.4740e+00
Epoch 4/10
10/10 - 1s - loss: 184.0359 - loglik: -1.8342e+02 - logprior: -6.1565e-01
Epoch 5/10
10/10 - 1s - loss: 181.9783 - loglik: -1.8286e+02 - logprior: 0.8775
Epoch 6/10
10/10 - 1s - loss: 180.4624 - loglik: -1.8235e+02 - logprior: 1.8856
Epoch 7/10
10/10 - 1s - loss: 179.7042 - loglik: -1.8227e+02 - logprior: 2.5688
Epoch 8/10
10/10 - 1s - loss: 178.6042 - loglik: -1.8153e+02 - logprior: 2.9268
Epoch 9/10
10/10 - 1s - loss: 178.4212 - loglik: -1.8159e+02 - logprior: 3.1674
Epoch 10/10
10/10 - 1s - loss: 177.9599 - loglik: -1.8135e+02 - logprior: 3.3894
Fitted a model with MAP estimate = -177.6234
Time for alignment: 41.5712
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 384.2444 - loglik: -3.3905e+02 - logprior: -4.5197e+01
Epoch 2/10
10/10 - 1s - loss: 309.1247 - loglik: -2.9839e+02 - logprior: -1.0734e+01
Epoch 3/10
10/10 - 1s - loss: 260.6453 - loglik: -2.5560e+02 - logprior: -5.0447e+00
Epoch 4/10
10/10 - 1s - loss: 227.3179 - loglik: -2.2355e+02 - logprior: -3.7676e+00
Epoch 5/10
10/10 - 1s - loss: 214.7750 - loglik: -2.1183e+02 - logprior: -2.9426e+00
Epoch 6/10
10/10 - 1s - loss: 208.6786 - loglik: -2.0655e+02 - logprior: -2.1284e+00
Epoch 7/10
10/10 - 1s - loss: 205.6616 - loglik: -2.0397e+02 - logprior: -1.6961e+00
Epoch 8/10
10/10 - 1s - loss: 203.9380 - loglik: -2.0244e+02 - logprior: -1.5019e+00
Epoch 9/10
10/10 - 1s - loss: 203.2840 - loglik: -2.0192e+02 - logprior: -1.3602e+00
Epoch 10/10
10/10 - 1s - loss: 202.3088 - loglik: -2.0106e+02 - logprior: -1.2517e+00
Fitted a model with MAP estimate = -201.9935
expansions: [(7, 1), (9, 2), (10, 2), (12, 1), (13, 2), (30, 1), (44, 2), (45, 1), (48, 2), (51, 1), (57, 1), (59, 1), (63, 1), (67, 1), (68, 2), (80, 1), (81, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 255.6794 - loglik: -2.0448e+02 - logprior: -5.1204e+01
Epoch 2/2
10/10 - 1s - loss: 210.4133 - loglik: -1.9025e+02 - logprior: -2.0158e+01
Fitted a model with MAP estimate = -203.5187
expansions: [(0, 3)]
discards: [ 0  9 53 59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 228.9384 - loglik: -1.8859e+02 - logprior: -4.0345e+01
Epoch 2/2
10/10 - 1s - loss: 194.2395 - loglik: -1.8496e+02 - logprior: -9.2804e+00
Fitted a model with MAP estimate = -188.5543
expansions: []
discards: [  0   2 101]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 237.5199 - loglik: -1.8812e+02 - logprior: -4.9398e+01
Epoch 2/10
10/10 - 1s - loss: 201.6460 - loglik: -1.8617e+02 - logprior: -1.5480e+01
Epoch 3/10
10/10 - 1s - loss: 189.2210 - loglik: -1.8467e+02 - logprior: -4.5522e+00
Epoch 4/10
10/10 - 1s - loss: 184.3378 - loglik: -1.8368e+02 - logprior: -6.5523e-01
Epoch 5/10
10/10 - 1s - loss: 181.8082 - loglik: -1.8265e+02 - logprior: 0.8423
Epoch 6/10
10/10 - 1s - loss: 180.5656 - loglik: -1.8246e+02 - logprior: 1.8919
Epoch 7/10
10/10 - 1s - loss: 179.1632 - loglik: -1.8175e+02 - logprior: 2.5828
Epoch 8/10
10/10 - 1s - loss: 178.4610 - loglik: -1.8140e+02 - logprior: 2.9434
Epoch 9/10
10/10 - 1s - loss: 177.4828 - loglik: -1.8073e+02 - logprior: 3.2430
Epoch 10/10
10/10 - 1s - loss: 177.5626 - loglik: -1.8109e+02 - logprior: 3.5235
Fitted a model with MAP estimate = -177.0367
Time for alignment: 42.0833
Computed alignments with likelihoods: ['-177.0339', '-177.3121', '-176.7801', '-177.6234', '-177.0367']
Best model has likelihood: -176.7801  (prior= 3.7293 )
time for generating output: 0.1572
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.914082075839412
Training of 5 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200a5cfdf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f917c6850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200a76dd60>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fdf577f70>
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 638.1201 - loglik: -6.1574e+02 - logprior: -2.2383e+01
Epoch 2/10
14/14 - 3s - loss: 557.0966 - loglik: -5.5523e+02 - logprior: -1.8679e+00
Epoch 3/10
14/14 - 3s - loss: 494.5138 - loglik: -4.9372e+02 - logprior: -7.9860e-01
Epoch 4/10
14/14 - 3s - loss: 473.5547 - loglik: -4.7283e+02 - logprior: -7.2474e-01
Epoch 5/10
14/14 - 3s - loss: 464.8189 - loglik: -4.6438e+02 - logprior: -4.3967e-01
Epoch 6/10
14/14 - 3s - loss: 461.8303 - loglik: -4.6165e+02 - logprior: -1.8052e-01
Epoch 7/10
14/14 - 3s - loss: 462.9057 - loglik: -4.6290e+02 - logprior: -1.8928e-03
Fitted a model with MAP estimate = -460.9384
expansions: [(15, 1), (17, 1), (31, 1), (32, 1), (33, 1), (42, 2), (43, 2), (44, 2), (52, 1), (53, 1), (54, 1), (55, 4), (77, 1), (78, 1), (82, 1), (84, 1), (109, 1), (115, 1), (116, 3), (119, 1), (120, 1), (121, 3), (155, 1), (158, 1), (164, 1), (167, 9)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 227 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 488.7749 - loglik: -4.6198e+02 - logprior: -2.6791e+01
Epoch 2/2
14/14 - 4s - loss: 454.2084 - loglik: -4.4629e+02 - logprior: -7.9208e+00
Fitted a model with MAP estimate = -448.0500
expansions: [(38, 1), (69, 2), (140, 1), (141, 1)]
discards: [ 0 72 73 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 472.0207 - loglik: -4.4610e+02 - logprior: -2.5923e+01
Epoch 2/2
14/14 - 4s - loss: 448.4732 - loglik: -4.4220e+02 - logprior: -6.2770e+00
Fitted a model with MAP estimate = -443.2332
expansions: [(0, 4), (77, 2)]
discards: [ 0 47 52]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 460.3825 - loglik: -4.4182e+02 - logprior: -1.8561e+01
Epoch 2/10
14/14 - 4s - loss: 438.7657 - loglik: -4.3887e+02 - logprior: 0.1007
Epoch 3/10
14/14 - 4s - loss: 434.3589 - loglik: -4.3754e+02 - logprior: 3.1844
Epoch 4/10
14/14 - 4s - loss: 433.6418 - loglik: -4.3802e+02 - logprior: 4.3791
Epoch 5/10
14/14 - 4s - loss: 431.8489 - loglik: -4.3676e+02 - logprior: 4.9062
Epoch 6/10
14/14 - 4s - loss: 431.5236 - loglik: -4.3679e+02 - logprior: 5.2635
Epoch 7/10
14/14 - 4s - loss: 429.7094 - loglik: -4.3531e+02 - logprior: 5.6015
Epoch 8/10
14/14 - 4s - loss: 431.1631 - loglik: -4.3706e+02 - logprior: 5.8982
Fitted a model with MAP estimate = -429.9403
Time for alignment: 99.9653
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 638.1500 - loglik: -6.1577e+02 - logprior: -2.2375e+01
Epoch 2/10
14/14 - 3s - loss: 555.1683 - loglik: -5.5331e+02 - logprior: -1.8542e+00
Epoch 3/10
14/14 - 3s - loss: 486.9372 - loglik: -4.8631e+02 - logprior: -6.2756e-01
Epoch 4/10
14/14 - 3s - loss: 467.0740 - loglik: -4.6686e+02 - logprior: -2.1080e-01
Epoch 5/10
14/14 - 3s - loss: 461.4274 - loglik: -4.6157e+02 - logprior: 0.1412
Epoch 6/10
14/14 - 3s - loss: 459.7283 - loglik: -4.6006e+02 - logprior: 0.3273
Epoch 7/10
14/14 - 3s - loss: 458.8455 - loglik: -4.5932e+02 - logprior: 0.4771
Epoch 8/10
14/14 - 3s - loss: 457.8230 - loglik: -4.5846e+02 - logprior: 0.6327
Epoch 9/10
14/14 - 3s - loss: 457.3113 - loglik: -4.5806e+02 - logprior: 0.7502
Epoch 10/10
14/14 - 3s - loss: 458.2903 - loglik: -4.5913e+02 - logprior: 0.8375
Fitted a model with MAP estimate = -457.1108
expansions: [(15, 1), (16, 2), (31, 1), (32, 2), (33, 2), (41, 2), (42, 2), (43, 2), (44, 1), (50, 1), (51, 1), (54, 4), (76, 1), (89, 6), (90, 2), (113, 1), (114, 3), (115, 1), (119, 1), (120, 3), (131, 2), (154, 1), (164, 1), (167, 9)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 486.9170 - loglik: -4.6031e+02 - logprior: -2.6611e+01
Epoch 2/2
14/14 - 4s - loss: 449.9643 - loglik: -4.4189e+02 - logprior: -8.0734e+00
Fitted a model with MAP estimate = -445.4537
expansions: [(0, 4), (71, 2), (233, 1)]
discards: [  0  18  49  54  74  75  76 112 113 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 465.7409 - loglik: -4.4639e+02 - logprior: -1.9350e+01
Epoch 2/2
14/14 - 4s - loss: 441.0608 - loglik: -4.4057e+02 - logprior: -4.9027e-01
Fitted a model with MAP estimate = -437.0705
expansions: [(79, 2), (112, 3), (114, 1), (144, 1)]
discards: [  1   2   3 230]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 458.3830 - loglik: -4.3989e+02 - logprior: -1.8498e+01
Epoch 2/10
14/14 - 4s - loss: 436.6886 - loglik: -4.3657e+02 - logprior: -1.1746e-01
Epoch 3/10
14/14 - 4s - loss: 431.9657 - loglik: -4.3464e+02 - logprior: 2.6715
Epoch 4/10
14/14 - 4s - loss: 428.9234 - loglik: -4.3295e+02 - logprior: 4.0306
Epoch 5/10
14/14 - 4s - loss: 427.7968 - loglik: -4.3267e+02 - logprior: 4.8752
Epoch 6/10
14/14 - 4s - loss: 426.5088 - loglik: -4.3174e+02 - logprior: 5.2262
Epoch 7/10
14/14 - 4s - loss: 427.6052 - loglik: -4.3327e+02 - logprior: 5.6689
Fitted a model with MAP estimate = -426.0873
Time for alignment: 106.7069
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 637.7949 - loglik: -6.1540e+02 - logprior: -2.2392e+01
Epoch 2/10
14/14 - 3s - loss: 554.9218 - loglik: -5.5301e+02 - logprior: -1.9130e+00
Epoch 3/10
14/14 - 3s - loss: 484.7507 - loglik: -4.8384e+02 - logprior: -9.0646e-01
Epoch 4/10
14/14 - 3s - loss: 468.4373 - loglik: -4.6783e+02 - logprior: -6.1117e-01
Epoch 5/10
14/14 - 3s - loss: 461.5077 - loglik: -4.6127e+02 - logprior: -2.3909e-01
Epoch 6/10
14/14 - 3s - loss: 460.0722 - loglik: -4.6001e+02 - logprior: -6.2604e-02
Epoch 7/10
14/14 - 3s - loss: 459.5407 - loglik: -4.5962e+02 - logprior: 0.0779
Epoch 8/10
14/14 - 3s - loss: 459.1510 - loglik: -4.5940e+02 - logprior: 0.2490
Epoch 9/10
14/14 - 3s - loss: 457.0405 - loglik: -4.5736e+02 - logprior: 0.3229
Epoch 10/10
14/14 - 3s - loss: 458.1617 - loglik: -4.5854e+02 - logprior: 0.3812
Fitted a model with MAP estimate = -457.6457
expansions: [(15, 1), (29, 1), (31, 1), (32, 2), (33, 2), (41, 2), (42, 2), (43, 1), (45, 1), (51, 1), (52, 1), (55, 4), (59, 2), (77, 1), (78, 1), (90, 6), (91, 2), (109, 1), (111, 1), (112, 2), (114, 1), (116, 1), (119, 1), (120, 1), (121, 3), (131, 2), (154, 1), (164, 1), (167, 9)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 486.8577 - loglik: -4.6030e+02 - logprior: -2.6555e+01
Epoch 2/2
14/14 - 5s - loss: 452.2444 - loglik: -4.4416e+02 - logprior: -8.0815e+00
Fitted a model with MAP estimate = -446.0101
expansions: [(0, 4), (17, 1), (117, 1), (146, 1), (236, 1)]
discards: [  0  48  71  72  78 114 115 173]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 463.6258 - loglik: -4.4444e+02 - logprior: -1.9182e+01
Epoch 2/2
14/14 - 5s - loss: 439.7743 - loglik: -4.3951e+02 - logprior: -2.6742e-01
Fitted a model with MAP estimate = -435.7325
expansions: [(73, 3), (114, 2)]
discards: [  1   2   3  52 236]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 459.3047 - loglik: -4.4072e+02 - logprior: -1.8588e+01
Epoch 2/10
14/14 - 5s - loss: 434.1593 - loglik: -4.3403e+02 - logprior: -1.2920e-01
Epoch 3/10
14/14 - 5s - loss: 433.5688 - loglik: -4.3629e+02 - logprior: 2.7245
Epoch 4/10
14/14 - 5s - loss: 428.4621 - loglik: -4.3251e+02 - logprior: 4.0434
Epoch 5/10
14/14 - 5s - loss: 428.6926 - loglik: -4.3355e+02 - logprior: 4.8560
Fitted a model with MAP estimate = -427.4377
Time for alignment: 98.8911
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 637.3369 - loglik: -6.1496e+02 - logprior: -2.2374e+01
Epoch 2/10
14/14 - 3s - loss: 558.3525 - loglik: -5.5649e+02 - logprior: -1.8620e+00
Epoch 3/10
14/14 - 3s - loss: 493.8682 - loglik: -4.9315e+02 - logprior: -7.1619e-01
Epoch 4/10
14/14 - 3s - loss: 470.9825 - loglik: -4.7044e+02 - logprior: -5.4008e-01
Epoch 5/10
14/14 - 3s - loss: 463.8463 - loglik: -4.6339e+02 - logprior: -4.5807e-01
Epoch 6/10
14/14 - 3s - loss: 460.6013 - loglik: -4.6035e+02 - logprior: -2.5029e-01
Epoch 7/10
14/14 - 3s - loss: 459.3138 - loglik: -4.5915e+02 - logprior: -1.6489e-01
Epoch 8/10
14/14 - 3s - loss: 457.4469 - loglik: -4.5746e+02 - logprior: 0.0128
Epoch 9/10
14/14 - 3s - loss: 456.7334 - loglik: -4.5684e+02 - logprior: 0.1047
Epoch 10/10
14/14 - 3s - loss: 457.3972 - loglik: -4.5758e+02 - logprior: 0.1782
Fitted a model with MAP estimate = -456.5343
expansions: [(15, 1), (17, 1), (31, 1), (32, 2), (33, 2), (43, 2), (44, 1), (46, 1), (52, 1), (53, 1), (54, 1), (55, 4), (76, 1), (77, 1), (81, 1), (88, 1), (108, 1), (112, 1), (113, 1), (114, 2), (115, 1), (119, 1), (120, 3), (121, 1), (125, 1), (154, 1), (157, 1), (167, 9)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 486.7607 - loglik: -4.6018e+02 - logprior: -2.6583e+01
Epoch 2/2
14/14 - 4s - loss: 453.2302 - loglik: -4.4541e+02 - logprior: -7.8156e+00
Fitted a model with MAP estimate = -447.6685
expansions: [(70, 1)]
discards: [  0  50 150]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 226 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 470.6660 - loglik: -4.4475e+02 - logprior: -2.5914e+01
Epoch 2/2
14/14 - 4s - loss: 446.4394 - loglik: -4.4038e+02 - logprior: -6.0545e+00
Fitted a model with MAP estimate = -441.7197
expansions: [(0, 4), (67, 2), (224, 1)]
discards: [ 0 71 72 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 463.6638 - loglik: -4.4518e+02 - logprior: -1.8483e+01
Epoch 2/10
14/14 - 4s - loss: 439.7021 - loglik: -4.3993e+02 - logprior: 0.2287
Epoch 3/10
14/14 - 4s - loss: 438.1522 - loglik: -4.4146e+02 - logprior: 3.3092
Epoch 4/10
14/14 - 4s - loss: 434.3708 - loglik: -4.3884e+02 - logprior: 4.4655
Epoch 5/10
14/14 - 4s - loss: 433.1026 - loglik: -4.3810e+02 - logprior: 4.9946
Epoch 6/10
14/14 - 4s - loss: 432.8126 - loglik: -4.3818e+02 - logprior: 5.3679
Epoch 7/10
14/14 - 4s - loss: 432.4100 - loglik: -4.3813e+02 - logprior: 5.7226
Epoch 8/10
14/14 - 4s - loss: 432.2808 - loglik: -4.3828e+02 - logprior: 5.9976
Epoch 9/10
14/14 - 4s - loss: 432.8657 - loglik: -4.3912e+02 - logprior: 6.2578
Fitted a model with MAP estimate = -431.2427
Time for alignment: 113.7121
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 636.5098 - loglik: -6.1413e+02 - logprior: -2.2378e+01
Epoch 2/10
14/14 - 3s - loss: 554.0942 - loglik: -5.5221e+02 - logprior: -1.8871e+00
Epoch 3/10
14/14 - 3s - loss: 487.3965 - loglik: -4.8673e+02 - logprior: -6.6746e-01
Epoch 4/10
14/14 - 3s - loss: 468.9778 - loglik: -4.6861e+02 - logprior: -3.7039e-01
Epoch 5/10
14/14 - 3s - loss: 462.9521 - loglik: -4.6278e+02 - logprior: -1.7647e-01
Epoch 6/10
14/14 - 3s - loss: 461.3707 - loglik: -4.6144e+02 - logprior: 0.0695
Epoch 7/10
14/14 - 3s - loss: 460.0034 - loglik: -4.6023e+02 - logprior: 0.2280
Epoch 8/10
14/14 - 3s - loss: 460.0548 - loglik: -4.6045e+02 - logprior: 0.3964
Fitted a model with MAP estimate = -459.0938
expansions: [(15, 1), (29, 1), (31, 1), (32, 2), (33, 1), (38, 2), (42, 2), (43, 1), (45, 1), (51, 1), (52, 1), (53, 1), (54, 5), (77, 1), (78, 1), (90, 6), (91, 2), (109, 1), (115, 1), (116, 4), (119, 1), (120, 3), (121, 1), (155, 1), (158, 1), (164, 1), (167, 9)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 487.7932 - loglik: -4.6127e+02 - logprior: -2.6527e+01
Epoch 2/2
14/14 - 4s - loss: 450.3494 - loglik: -4.4249e+02 - logprior: -7.8608e+00
Fitted a model with MAP estimate = -445.9215
expansions: [(116, 1), (148, 1), (234, 1)]
discards: [  0 113 114 158]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 470.2886 - loglik: -4.4446e+02 - logprior: -2.5824e+01
Epoch 2/2
14/14 - 4s - loss: 445.6548 - loglik: -4.3924e+02 - logprior: -6.4191e+00
Fitted a model with MAP estimate = -440.1725
expansions: [(0, 4), (16, 1), (68, 2), (112, 2)]
discards: [  0  49  72  73 233]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 461.0796 - loglik: -4.4246e+02 - logprior: -1.8619e+01
Epoch 2/10
14/14 - 5s - loss: 438.4302 - loglik: -4.3839e+02 - logprior: -4.1986e-02
Epoch 3/10
14/14 - 5s - loss: 432.4079 - loglik: -4.3539e+02 - logprior: 2.9825
Epoch 4/10
14/14 - 5s - loss: 432.3071 - loglik: -4.3646e+02 - logprior: 4.1557
Epoch 5/10
14/14 - 5s - loss: 430.3235 - loglik: -4.3501e+02 - logprior: 4.6913
Epoch 6/10
14/14 - 5s - loss: 428.8984 - loglik: -4.3395e+02 - logprior: 5.0544
Epoch 7/10
14/14 - 5s - loss: 429.0163 - loglik: -4.3439e+02 - logprior: 5.3763
Fitted a model with MAP estimate = -428.2742
Time for alignment: 101.7155
Computed alignments with likelihoods: ['-429.9403', '-426.0873', '-427.4377', '-431.2427', '-428.2742']
Best model has likelihood: -426.0873  (prior= 5.8352 )
time for generating output: 0.2829
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.8877374784110535
Training of 5 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2024075850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201ab29280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa4024a00>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fdf2ef940>
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 747.5998 - loglik: -7.0711e+02 - logprior: -4.0491e+01
Epoch 2/10
11/11 - 8s - loss: 659.0152 - loglik: -6.5637e+02 - logprior: -2.6410e+00
Epoch 3/10
11/11 - 8s - loss: 565.5325 - loglik: -5.6696e+02 - logprior: 1.4254
Epoch 4/10
11/11 - 8s - loss: 509.2229 - loglik: -5.1052e+02 - logprior: 1.2967
Epoch 5/10
11/11 - 8s - loss: 487.3759 - loglik: -4.8876e+02 - logprior: 1.3826
Epoch 6/10
11/11 - 8s - loss: 480.3630 - loglik: -4.8211e+02 - logprior: 1.7442
Epoch 7/10
11/11 - 8s - loss: 475.6205 - loglik: -4.7772e+02 - logprior: 2.1031
Epoch 8/10
11/11 - 9s - loss: 472.8577 - loglik: -4.7526e+02 - logprior: 2.4002
Epoch 9/10
11/11 - 8s - loss: 472.0396 - loglik: -4.7473e+02 - logprior: 2.6860
Epoch 10/10
11/11 - 8s - loss: 470.8235 - loglik: -4.7383e+02 - logprior: 3.0035
Fitted a model with MAP estimate = -471.0319
expansions: [(22, 5), (25, 1), (28, 1), (33, 1), (34, 2), (35, 1), (50, 2), (51, 1), (52, 1), (57, 1), (59, 1), (77, 1), (78, 1), (103, 3), (107, 1), (109, 5), (122, 2), (129, 1), (148, 1), (149, 2), (157, 1), (159, 1), (160, 6), (161, 1), (163, 3), (165, 1), (168, 1), (174, 1), (193, 1), (194, 4), (195, 2), (196, 2), (198, 2), (200, 3), (219, 1), (222, 1)]
discards: [  0 130 209 241]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 522.2625 - loglik: -4.7546e+02 - logprior: -4.6798e+01
Epoch 2/2
11/11 - 12s - loss: 460.0281 - loglik: -4.4750e+02 - logprior: -1.2526e+01
Fitted a model with MAP estimate = -450.2478
expansions: [(0, 3)]
discards: [  0  21  61 122 131 132 133 149 195]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 480.9690 - loglik: -4.4604e+02 - logprior: -3.4931e+01
Epoch 2/2
11/11 - 11s - loss: 440.5662 - loglik: -4.4057e+02 - logprior: 0.0081
Fitted a model with MAP estimate = -434.2478
expansions: [(239, 1)]
discards: [  1   2 121 196]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 15s - loss: 478.4375 - loglik: -4.4538e+02 - logprior: -3.3055e+01
Epoch 2/10
11/11 - 11s - loss: 435.8879 - loglik: -4.3679e+02 - logprior: 0.9064
Epoch 3/10
11/11 - 11s - loss: 431.7167 - loglik: -4.3968e+02 - logprior: 7.9664
Epoch 4/10
11/11 - 12s - loss: 428.1848 - loglik: -4.3909e+02 - logprior: 10.9007
Epoch 5/10
11/11 - 11s - loss: 424.6985 - loglik: -4.3743e+02 - logprior: 12.7309
Epoch 6/10
11/11 - 12s - loss: 422.2642 - loglik: -4.3593e+02 - logprior: 13.6615
Epoch 7/10
11/11 - 12s - loss: 420.3064 - loglik: -4.3475e+02 - logprior: 14.4386
Epoch 8/10
11/11 - 10s - loss: 418.9282 - loglik: -4.3407e+02 - logprior: 15.1463
Epoch 9/10
11/11 - 11s - loss: 418.2440 - loglik: -4.3395e+02 - logprior: 15.7108
Epoch 10/10
11/11 - 10s - loss: 418.3286 - loglik: -4.3449e+02 - logprior: 16.1634
Fitted a model with MAP estimate = -415.8245
Time for alignment: 267.6712
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 750.9316 - loglik: -7.1044e+02 - logprior: -4.0494e+01
Epoch 2/10
11/11 - 9s - loss: 658.1683 - loglik: -6.5549e+02 - logprior: -2.6751e+00
Epoch 3/10
11/11 - 9s - loss: 565.1728 - loglik: -5.6633e+02 - logprior: 1.1546
Epoch 4/10
11/11 - 8s - loss: 512.4725 - loglik: -5.1342e+02 - logprior: 0.9487
Epoch 5/10
11/11 - 8s - loss: 487.6701 - loglik: -4.8862e+02 - logprior: 0.9452
Epoch 6/10
11/11 - 7s - loss: 481.0310 - loglik: -4.8248e+02 - logprior: 1.4536
Epoch 7/10
11/11 - 8s - loss: 478.5197 - loglik: -4.8031e+02 - logprior: 1.7868
Epoch 8/10
11/11 - 7s - loss: 473.6991 - loglik: -4.7584e+02 - logprior: 2.1379
Epoch 9/10
11/11 - 8s - loss: 474.6980 - loglik: -4.7713e+02 - logprior: 2.4359
Fitted a model with MAP estimate = -472.8486
expansions: [(22, 6), (26, 1), (28, 1), (35, 1), (42, 1), (48, 1), (50, 2), (51, 1), (52, 1), (57, 1), (59, 1), (77, 1), (78, 1), (103, 3), (106, 1), (108, 1), (109, 1), (128, 1), (129, 1), (148, 1), (149, 2), (157, 1), (159, 1), (160, 6), (161, 1), (163, 3), (169, 1), (173, 1), (174, 1), (175, 1), (194, 4), (195, 2), (196, 2), (198, 1), (200, 3), (223, 1), (225, 1)]
discards: [  0 130 208 241]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 523.9417 - loglik: -4.7688e+02 - logprior: -4.7066e+01
Epoch 2/2
11/11 - 11s - loss: 466.8889 - loglik: -4.5429e+02 - logprior: -1.2597e+01
Fitted a model with MAP estimate = -454.8498
expansions: [(0, 3)]
discards: [  0  21  61 121 122 198 251 252 253]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 294 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 485.1905 - loglik: -4.5005e+02 - logprior: -3.5141e+01
Epoch 2/2
11/11 - 11s - loss: 446.9705 - loglik: -4.4653e+02 - logprior: -4.3712e-01
Fitted a model with MAP estimate = -440.4114
expansions: [(238, 1), (247, 1), (271, 1)]
discards: [  1   2 192]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 294 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 479.6421 - loglik: -4.4641e+02 - logprior: -3.3229e+01
Epoch 2/10
11/11 - 10s - loss: 443.3202 - loglik: -4.4424e+02 - logprior: 0.9226
Epoch 3/10
11/11 - 10s - loss: 433.8387 - loglik: -4.4159e+02 - logprior: 7.7545
Epoch 4/10
11/11 - 10s - loss: 431.1634 - loglik: -4.4195e+02 - logprior: 10.7855
Epoch 5/10
11/11 - 11s - loss: 429.4225 - loglik: -4.4194e+02 - logprior: 12.5155
Epoch 6/10
11/11 - 11s - loss: 428.5844 - loglik: -4.4205e+02 - logprior: 13.4660
Epoch 7/10
11/11 - 11s - loss: 425.4787 - loglik: -4.3973e+02 - logprior: 14.2546
Epoch 8/10
11/11 - 10s - loss: 423.1028 - loglik: -4.3816e+02 - logprior: 15.0574
Epoch 9/10
11/11 - 11s - loss: 423.0009 - loglik: -4.3867e+02 - logprior: 15.6667
Epoch 10/10
11/11 - 10s - loss: 422.1052 - loglik: -4.3823e+02 - logprior: 16.1222
Fitted a model with MAP estimate = -420.5646
Time for alignment: 254.3230
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 749.7903 - loglik: -7.0930e+02 - logprior: -4.0491e+01
Epoch 2/10
11/11 - 8s - loss: 655.9858 - loglik: -6.5331e+02 - logprior: -2.6782e+00
Epoch 3/10
11/11 - 8s - loss: 562.9764 - loglik: -5.6418e+02 - logprior: 1.2061
Epoch 4/10
11/11 - 8s - loss: 507.3967 - loglik: -5.0845e+02 - logprior: 1.0546
Epoch 5/10
11/11 - 8s - loss: 484.2787 - loglik: -4.8562e+02 - logprior: 1.3390
Epoch 6/10
11/11 - 8s - loss: 479.0671 - loglik: -4.8103e+02 - logprior: 1.9584
Epoch 7/10
11/11 - 9s - loss: 472.4893 - loglik: -4.7485e+02 - logprior: 2.3611
Epoch 8/10
11/11 - 8s - loss: 473.5997 - loglik: -4.7627e+02 - logprior: 2.6674
Fitted a model with MAP estimate = -471.7839
expansions: [(22, 5), (25, 1), (26, 1), (28, 1), (35, 1), (46, 1), (51, 4), (52, 1), (57, 1), (59, 1), (78, 1), (101, 1), (103, 1), (108, 4), (122, 3), (129, 1), (147, 1), (149, 1), (157, 1), (159, 1), (160, 5), (161, 2), (172, 1), (174, 1), (175, 3), (176, 1), (193, 3), (194, 2), (195, 3), (196, 2), (198, 1), (219, 1), (222, 1), (225, 1)]
discards: [  0 209 241]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 523.3881 - loglik: -4.7609e+02 - logprior: -4.7294e+01
Epoch 2/2
11/11 - 11s - loss: 460.9655 - loglik: -4.4820e+02 - logprior: -1.2767e+01
Fitted a model with MAP estimate = -452.1983
expansions: [(0, 3), (92, 1), (252, 1)]
discards: [  0  21  61 128 129 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 478.9509 - loglik: -4.4415e+02 - logprior: -3.4804e+01
Epoch 2/2
11/11 - 11s - loss: 441.3331 - loglik: -4.4134e+02 - logprior: 0.0056
Fitted a model with MAP estimate = -433.7677
expansions: []
discards: [  1   2 129 239]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 475.3416 - loglik: -4.4208e+02 - logprior: -3.3259e+01
Epoch 2/10
11/11 - 12s - loss: 439.2522 - loglik: -4.3996e+02 - logprior: 0.7088
Epoch 3/10
11/11 - 10s - loss: 432.3603 - loglik: -4.3998e+02 - logprior: 7.6198
Epoch 4/10
11/11 - 11s - loss: 427.3374 - loglik: -4.3810e+02 - logprior: 10.7609
Epoch 5/10
11/11 - 10s - loss: 425.4773 - loglik: -4.3789e+02 - logprior: 12.4108
Epoch 6/10
11/11 - 11s - loss: 426.4702 - loglik: -4.3988e+02 - logprior: 13.4076
Fitted a model with MAP estimate = -423.1860
Time for alignment: 204.3831
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 751.7240 - loglik: -7.1122e+02 - logprior: -4.0501e+01
Epoch 2/10
11/11 - 8s - loss: 655.8239 - loglik: -6.5316e+02 - logprior: -2.6618e+00
Epoch 3/10
11/11 - 8s - loss: 565.1866 - loglik: -5.6630e+02 - logprior: 1.1101
Epoch 4/10
11/11 - 9s - loss: 507.7566 - loglik: -5.0854e+02 - logprior: 0.7792
Epoch 5/10
11/11 - 9s - loss: 489.8623 - loglik: -4.9077e+02 - logprior: 0.9102
Epoch 6/10
11/11 - 8s - loss: 476.3714 - loglik: -4.7764e+02 - logprior: 1.2735
Epoch 7/10
11/11 - 8s - loss: 479.0404 - loglik: -4.8052e+02 - logprior: 1.4830
Fitted a model with MAP estimate = -474.9084
expansions: [(22, 5), (24, 1), (28, 1), (34, 1), (35, 1), (48, 1), (50, 2), (51, 1), (52, 1), (57, 1), (59, 1), (77, 1), (78, 1), (89, 1), (108, 4), (109, 1), (126, 1), (128, 1), (148, 1), (149, 2), (157, 1), (158, 2), (160, 5), (161, 1), (164, 3), (170, 1), (174, 1), (175, 1), (176, 1), (177, 1), (192, 1), (193, 2), (194, 2), (195, 2), (197, 2), (199, 2), (223, 1), (225, 1)]
discards: [  0 208]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 301 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 522.6892 - loglik: -4.7515e+02 - logprior: -4.7537e+01
Epoch 2/2
11/11 - 11s - loss: 462.0766 - loglik: -4.4921e+02 - logprior: -1.2867e+01
Fitted a model with MAP estimate = -451.5139
expansions: [(0, 3), (121, 1), (143, 1), (277, 1)]
discards: [  0  60 127 128 154 155 195 202 240 295]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 482.1645 - loglik: -4.4739e+02 - logprior: -3.4770e+01
Epoch 2/2
11/11 - 11s - loss: 443.7792 - loglik: -4.4369e+02 - logprior: -9.1716e-02
Fitted a model with MAP estimate = -435.4962
expansions: []
discards: [  1   2 129]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 294 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 477.4184 - loglik: -4.4430e+02 - logprior: -3.3114e+01
Epoch 2/10
11/11 - 11s - loss: 439.1859 - loglik: -4.3994e+02 - logprior: 0.7537
Epoch 3/10
11/11 - 10s - loss: 431.7946 - loglik: -4.3955e+02 - logprior: 7.7514
Epoch 4/10
11/11 - 11s - loss: 430.0407 - loglik: -4.4094e+02 - logprior: 10.8956
Epoch 5/10
11/11 - 10s - loss: 426.7701 - loglik: -4.3938e+02 - logprior: 12.6113
Epoch 6/10
11/11 - 11s - loss: 422.8588 - loglik: -4.3641e+02 - logprior: 13.5523
Epoch 7/10
11/11 - 10s - loss: 423.6059 - loglik: -4.3797e+02 - logprior: 14.3612
Fitted a model with MAP estimate = -422.3375
Time for alignment: 209.5291
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 750.0344 - loglik: -7.0954e+02 - logprior: -4.0498e+01
Epoch 2/10
11/11 - 8s - loss: 653.8886 - loglik: -6.5126e+02 - logprior: -2.6250e+00
Epoch 3/10
11/11 - 8s - loss: 573.8842 - loglik: -5.7530e+02 - logprior: 1.4131
Epoch 4/10
11/11 - 8s - loss: 508.9012 - loglik: -5.0977e+02 - logprior: 0.8662
Epoch 5/10
11/11 - 8s - loss: 492.5043 - loglik: -4.9346e+02 - logprior: 0.9599
Epoch 6/10
11/11 - 8s - loss: 478.9860 - loglik: -4.8043e+02 - logprior: 1.4429
Epoch 7/10
11/11 - 8s - loss: 476.4995 - loglik: -4.7835e+02 - logprior: 1.8478
Epoch 8/10
11/11 - 8s - loss: 471.4876 - loglik: -4.7363e+02 - logprior: 2.1436
Epoch 9/10
11/11 - 8s - loss: 472.2928 - loglik: -4.7459e+02 - logprior: 2.2991
Fitted a model with MAP estimate = -470.5498
expansions: [(22, 5), (25, 1), (28, 1), (33, 1), (34, 1), (35, 1), (41, 1), (50, 2), (51, 1), (52, 1), (54, 1), (56, 1), (58, 1), (76, 1), (77, 1), (102, 3), (108, 4), (130, 1), (148, 1), (150, 1), (151, 1), (157, 1), (158, 2), (160, 6), (162, 4), (168, 1), (174, 1), (177, 1), (192, 1), (193, 3), (194, 2), (195, 3), (196, 2), (198, 1), (219, 1), (222, 1)]
discards: [  0 209 241]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 522.5553 - loglik: -4.7530e+02 - logprior: -4.7251e+01
Epoch 2/2
11/11 - 11s - loss: 459.4858 - loglik: -4.4674e+02 - logprior: -1.2741e+01
Fitted a model with MAP estimate = -451.2303
expansions: [(0, 3), (255, 1), (284, 1)]
discards: [  0  21  61 121 122 130 131 240 244]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 480.9502 - loglik: -4.4609e+02 - logprior: -3.4855e+01
Epoch 2/2
11/11 - 12s - loss: 438.7967 - loglik: -4.3870e+02 - logprior: -9.1650e-02
Fitted a model with MAP estimate = -434.0785
expansions: []
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 474.1156 - loglik: -4.4099e+02 - logprior: -3.3128e+01
Epoch 2/10
11/11 - 12s - loss: 439.4000 - loglik: -4.4031e+02 - logprior: 0.9109
Epoch 3/10
11/11 - 11s - loss: 428.3559 - loglik: -4.3628e+02 - logprior: 7.9216
Epoch 4/10
11/11 - 10s - loss: 426.1758 - loglik: -4.3718e+02 - logprior: 11.0082
Epoch 5/10
11/11 - 11s - loss: 425.8597 - loglik: -4.3858e+02 - logprior: 12.7170
Epoch 6/10
11/11 - 11s - loss: 421.7540 - loglik: -4.3541e+02 - logprior: 13.6529
Epoch 7/10
11/11 - 11s - loss: 421.4936 - loglik: -4.3594e+02 - logprior: 14.4496
Epoch 8/10
11/11 - 10s - loss: 417.6371 - loglik: -4.3284e+02 - logprior: 15.2015
Epoch 9/10
11/11 - 11s - loss: 419.1779 - loglik: -4.3493e+02 - logprior: 15.7514
Fitted a model with MAP estimate = -416.8667
Time for alignment: 246.0993
Computed alignments with likelihoods: ['-415.8245', '-420.5646', '-423.1860', '-422.3375', '-416.8667']
Best model has likelihood: -415.8245  (prior= 16.4290 )
time for generating output: 0.5118
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.9285383368079685
Training of 5 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff0b04160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91bc7400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5c0dbe0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fa3a53310>
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 830.8395 - loglik: -8.2485e+02 - logprior: -5.9883e+00
Epoch 2/10
21/21 - 14s - loss: 714.4276 - loglik: -7.1381e+02 - logprior: -6.1796e-01
Epoch 3/10
21/21 - 15s - loss: 663.2548 - loglik: -6.6100e+02 - logprior: -2.2563e+00
Epoch 4/10
21/21 - 15s - loss: 651.6992 - loglik: -6.4954e+02 - logprior: -2.1617e+00
Epoch 5/10
21/21 - 15s - loss: 647.3414 - loglik: -6.4542e+02 - logprior: -1.9222e+00
Epoch 6/10
21/21 - 15s - loss: 647.4368 - loglik: -6.4543e+02 - logprior: -2.0063e+00
Fitted a model with MAP estimate = -644.6807
expansions: [(0, 4), (46, 1), (51, 3), (52, 1), (53, 1), (55, 1), (61, 1), (62, 1), (63, 1), (65, 2), (67, 1), (76, 2), (77, 4), (78, 1), (79, 1), (80, 1), (86, 1), (87, 1), (92, 1), (93, 1), (96, 1), (103, 1), (114, 2), (115, 1), (117, 1), (137, 1), (140, 1), (143, 1), (146, 1), (155, 1), (156, 1), (160, 2), (161, 2), (162, 1), (163, 1), (170, 1), (171, 2), (184, 1), (188, 1), (190, 1), (191, 1), (192, 2), (193, 2), (194, 1), (195, 1), (209, 1), (210, 1), (212, 1), (213, 1), (214, 1), (219, 1), (223, 1), (227, 1), (229, 1), (230, 3), (231, 1), (233, 1), (255, 1), (256, 1), (257, 1), (258, 2), (260, 1), (261, 1), (272, 2), (273, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 369 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 645.5325 - loglik: -6.3756e+02 - logprior: -7.9765e+00
Epoch 2/2
21/21 - 23s - loss: 616.6235 - loglik: -6.1734e+02 - logprior: 0.7200
Fitted a model with MAP estimate = -612.8222
expansions: [(95, 1), (144, 1)]
discards: [  0   1   5  56  57  96 218 219 251 301]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 361 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 24s - loss: 627.9736 - loglik: -6.2021e+02 - logprior: -7.7646e+00
Epoch 2/2
21/21 - 22s - loss: 615.1496 - loglik: -6.1419e+02 - logprior: -9.5982e-01
Fitted a model with MAP estimate = -612.2475
expansions: [(0, 3), (53, 2), (214, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 621.2650 - loglik: -6.1673e+02 - logprior: -4.5344e+00
Epoch 2/10
21/21 - 22s - loss: 607.0388 - loglik: -6.0924e+02 - logprior: 2.2039
Epoch 3/10
21/21 - 22s - loss: 608.3270 - loglik: -6.1112e+02 - logprior: 2.7883
Fitted a model with MAP estimate = -606.0581
Time for alignment: 321.1396
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 831.8848 - loglik: -8.2591e+02 - logprior: -5.9744e+00
Epoch 2/10
21/21 - 15s - loss: 713.7001 - loglik: -7.1318e+02 - logprior: -5.1859e-01
Epoch 3/10
21/21 - 15s - loss: 664.4160 - loglik: -6.6220e+02 - logprior: -2.2137e+00
Epoch 4/10
21/21 - 15s - loss: 650.3043 - loglik: -6.4813e+02 - logprior: -2.1764e+00
Epoch 5/10
21/21 - 15s - loss: 650.6035 - loglik: -6.4856e+02 - logprior: -2.0468e+00
Fitted a model with MAP estimate = -647.7140
expansions: [(13, 1), (14, 2), (15, 1), (34, 1), (50, 1), (51, 2), (52, 3), (62, 4), (63, 2), (64, 2), (73, 1), (74, 3), (75, 1), (76, 1), (77, 1), (78, 1), (80, 1), (83, 1), (84, 1), (85, 1), (88, 1), (94, 1), (101, 1), (110, 1), (111, 1), (112, 2), (135, 1), (137, 1), (141, 1), (144, 1), (153, 1), (154, 1), (158, 1), (159, 1), (160, 2), (161, 1), (162, 1), (170, 1), (184, 1), (191, 3), (192, 3), (193, 1), (194, 2), (195, 1), (207, 1), (208, 1), (209, 1), (213, 1), (214, 1), (218, 1), (223, 1), (227, 1), (228, 2), (229, 2), (231, 1), (233, 1), (255, 1), (256, 2), (258, 2), (261, 1), (272, 2), (273, 2)]
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 646.2068 - loglik: -6.4033e+02 - logprior: -5.8757e+00
Epoch 2/2
21/21 - 23s - loss: 617.8085 - loglik: -6.1837e+02 - logprior: 0.5614
Fitted a model with MAP estimate = -614.3595
expansions: [(0, 3), (121, 1), (149, 1)]
discards: [ 55  56  58 204 245 251 296]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 624.7465 - loglik: -6.1726e+02 - logprior: -7.4903e+00
Epoch 2/2
21/21 - 22s - loss: 611.5798 - loglik: -6.1292e+02 - logprior: 1.3426
Fitted a model with MAP estimate = -610.6109
expansions: [(58, 2)]
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 621.4689 - loglik: -6.1688e+02 - logprior: -4.5840e+00
Epoch 2/10
21/21 - 22s - loss: 610.7476 - loglik: -6.1287e+02 - logprior: 2.1206
Epoch 3/10
21/21 - 22s - loss: 610.5383 - loglik: -6.1350e+02 - logprior: 2.9641
Epoch 4/10
21/21 - 22s - loss: 607.1473 - loglik: -6.1034e+02 - logprior: 3.1937
Epoch 5/10
21/21 - 23s - loss: 605.4137 - loglik: -6.0891e+02 - logprior: 3.4939
Epoch 6/10
21/21 - 22s - loss: 602.2850 - loglik: -6.0598e+02 - logprior: 3.6958
Epoch 7/10
21/21 - 22s - loss: 603.9966 - loglik: -6.0791e+02 - logprior: 3.9152
Fitted a model with MAP estimate = -602.2734
Time for alignment: 392.5970
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 828.2867 - loglik: -8.2229e+02 - logprior: -5.9926e+00
Epoch 2/10
21/21 - 15s - loss: 716.0794 - loglik: -7.1557e+02 - logprior: -5.1139e-01
Epoch 3/10
21/21 - 15s - loss: 662.2330 - loglik: -6.6013e+02 - logprior: -2.0996e+00
Epoch 4/10
21/21 - 15s - loss: 651.7222 - loglik: -6.4973e+02 - logprior: -1.9924e+00
Epoch 5/10
21/21 - 15s - loss: 648.7306 - loglik: -6.4699e+02 - logprior: -1.7434e+00
Epoch 6/10
21/21 - 15s - loss: 645.8149 - loglik: -6.4413e+02 - logprior: -1.6876e+00
Epoch 7/10
21/21 - 15s - loss: 644.4619 - loglik: -6.4277e+02 - logprior: -1.6916e+00
Epoch 8/10
21/21 - 15s - loss: 642.7203 - loglik: -6.4106e+02 - logprior: -1.6559e+00
Epoch 9/10
21/21 - 15s - loss: 644.7107 - loglik: -6.4308e+02 - logprior: -1.6267e+00
Fitted a model with MAP estimate = -641.5607
expansions: [(0, 4), (50, 1), (51, 2), (52, 3), (62, 3), (63, 2), (64, 1), (74, 1), (75, 3), (76, 1), (77, 1), (78, 1), (79, 1), (81, 1), (85, 1), (86, 1), (90, 1), (91, 1), (92, 1), (95, 1), (110, 1), (111, 1), (113, 2), (114, 1), (116, 1), (136, 1), (139, 1), (142, 1), (146, 1), (154, 1), (159, 1), (160, 2), (161, 3), (162, 1), (171, 1), (185, 1), (189, 1), (191, 1), (192, 1), (193, 2), (194, 2), (195, 1), (196, 1), (210, 1), (211, 1), (213, 1), (214, 1), (215, 1), (220, 1), (223, 1), (228, 1), (229, 2), (230, 2), (232, 1), (236, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 370 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 641.7471 - loglik: -6.3352e+02 - logprior: -8.2278e+00
Epoch 2/2
21/21 - 22s - loss: 614.6396 - loglik: -6.1507e+02 - logprior: 0.4310
Fitted a model with MAP estimate = -610.8723
expansions: [(81, 1)]
discards: [  0   5  56  57  59 206 251 298 337]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 362 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 626.1628 - loglik: -6.1831e+02 - logprior: -7.8502e+00
Epoch 2/2
21/21 - 22s - loss: 611.6834 - loglik: -6.1112e+02 - logprior: -5.6662e-01
Fitted a model with MAP estimate = -609.3916
expansions: [(0, 3), (54, 2)]
discards: [  0 348]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 618.5620 - loglik: -6.1387e+02 - logprior: -4.6882e+00
Epoch 2/10
21/21 - 22s - loss: 609.3732 - loglik: -6.1149e+02 - logprior: 2.1174
Epoch 3/10
21/21 - 22s - loss: 606.7476 - loglik: -6.0954e+02 - logprior: 2.7960
Epoch 4/10
21/21 - 22s - loss: 604.7921 - loglik: -6.0799e+02 - logprior: 3.1950
Epoch 5/10
21/21 - 22s - loss: 604.5755 - loglik: -6.0791e+02 - logprior: 3.3301
Epoch 6/10
21/21 - 22s - loss: 603.4849 - loglik: -6.0699e+02 - logprior: 3.5047
Epoch 7/10
21/21 - 22s - loss: 600.9638 - loglik: -6.0468e+02 - logprior: 3.7145
Epoch 8/10
21/21 - 22s - loss: 602.0150 - loglik: -6.0599e+02 - logprior: 3.9770
Fitted a model with MAP estimate = -600.9632
Time for alignment: 476.1310
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 830.3903 - loglik: -8.2442e+02 - logprior: -5.9698e+00
Epoch 2/10
21/21 - 15s - loss: 715.8107 - loglik: -7.1535e+02 - logprior: -4.5623e-01
Epoch 3/10
21/21 - 15s - loss: 664.6866 - loglik: -6.6253e+02 - logprior: -2.1523e+00
Epoch 4/10
21/21 - 15s - loss: 653.0378 - loglik: -6.5084e+02 - logprior: -2.1994e+00
Epoch 5/10
21/21 - 15s - loss: 647.1639 - loglik: -6.4514e+02 - logprior: -2.0240e+00
Epoch 6/10
21/21 - 15s - loss: 647.7022 - loglik: -6.4567e+02 - logprior: -2.0292e+00
Fitted a model with MAP estimate = -645.8406
expansions: [(13, 1), (14, 2), (15, 1), (16, 1), (45, 1), (48, 1), (50, 2), (51, 1), (62, 4), (63, 1), (64, 1), (66, 1), (72, 2), (73, 3), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (84, 1), (85, 1), (88, 1), (89, 1), (90, 1), (93, 1), (108, 1), (109, 1), (111, 2), (112, 1), (128, 1), (134, 1), (137, 1), (153, 1), (154, 1), (157, 1), (158, 2), (159, 2), (160, 1), (161, 1), (180, 1), (188, 1), (190, 2), (191, 3), (192, 1), (193, 2), (194, 1), (195, 1), (212, 1), (220, 1), (221, 1), (222, 1), (223, 2), (229, 2), (230, 2), (232, 1), (234, 1), (256, 1), (257, 1), (259, 2), (260, 1), (262, 1), (271, 1), (272, 2), (273, 2)]
discards: [1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 642.8865 - loglik: -6.3699e+02 - logprior: -5.8999e+00
Epoch 2/2
21/21 - 23s - loss: 618.3910 - loglik: -6.1927e+02 - logprior: 0.8825
Fitted a model with MAP estimate = -614.0543
expansions: [(0, 3), (182, 1), (331, 1)]
discards: [ 54 202 248 287 295]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 625.9434 - loglik: -6.1833e+02 - logprior: -7.6109e+00
Epoch 2/2
21/21 - 22s - loss: 613.9030 - loglik: -6.1532e+02 - logprior: 1.4179
Fitted a model with MAP estimate = -610.1526
expansions: []
discards: [1 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 24s - loss: 620.7650 - loglik: -6.1638e+02 - logprior: -4.3812e+00
Epoch 2/10
21/21 - 22s - loss: 612.7421 - loglik: -6.1494e+02 - logprior: 2.2016
Epoch 3/10
21/21 - 22s - loss: 606.2194 - loglik: -6.0906e+02 - logprior: 2.8433
Epoch 4/10
21/21 - 22s - loss: 606.0745 - loglik: -6.0922e+02 - logprior: 3.1494
Epoch 5/10
21/21 - 22s - loss: 607.3319 - loglik: -6.1060e+02 - logprior: 3.2683
Fitted a model with MAP estimate = -604.1845
Time for alignment: 362.7377
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 832.4830 - loglik: -8.2651e+02 - logprior: -5.9727e+00
Epoch 2/10
21/21 - 15s - loss: 712.2627 - loglik: -7.1184e+02 - logprior: -4.2281e-01
Epoch 3/10
21/21 - 15s - loss: 663.5699 - loglik: -6.6145e+02 - logprior: -2.1159e+00
Epoch 4/10
21/21 - 15s - loss: 655.6056 - loglik: -6.5360e+02 - logprior: -2.0088e+00
Epoch 5/10
21/21 - 15s - loss: 648.9353 - loglik: -6.4716e+02 - logprior: -1.7794e+00
Epoch 6/10
21/21 - 15s - loss: 649.8112 - loglik: -6.4798e+02 - logprior: -1.8351e+00
Fitted a model with MAP estimate = -647.5451
expansions: [(13, 1), (14, 2), (15, 1), (52, 3), (53, 1), (56, 1), (63, 4), (64, 1), (65, 2), (66, 2), (73, 1), (74, 2), (75, 1), (76, 1), (78, 1), (79, 1), (85, 2), (86, 1), (91, 1), (92, 1), (95, 1), (112, 1), (113, 1), (114, 2), (130, 1), (136, 1), (139, 1), (146, 1), (155, 1), (156, 1), (157, 1), (159, 3), (160, 3), (161, 1), (184, 1), (191, 3), (192, 2), (193, 1), (194, 2), (195, 1), (209, 1), (210, 1), (212, 1), (213, 1), (214, 1), (219, 1), (228, 1), (229, 2), (230, 2), (232, 1), (234, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 1), (262, 1), (271, 1), (272, 2), (273, 2)]
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 644.1926 - loglik: -6.3832e+02 - logprior: -5.8734e+00
Epoch 2/2
21/21 - 22s - loss: 616.9030 - loglik: -6.1788e+02 - logprior: 0.9749
Fitted a model with MAP estimate = -613.3860
expansions: [(0, 3), (18, 1), (73, 1), (148, 1)]
discards: [ 54  55 248 296]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 624.1642 - loglik: -6.1643e+02 - logprior: -7.7387e+00
Epoch 2/2
21/21 - 23s - loss: 612.8641 - loglik: -6.1429e+02 - logprior: 1.4305
Fitted a model with MAP estimate = -609.5365
expansions: []
discards: [1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 617.5637 - loglik: -6.1306e+02 - logprior: -4.5014e+00
Epoch 2/10
21/21 - 22s - loss: 613.9413 - loglik: -6.1617e+02 - logprior: 2.2274
Epoch 3/10
21/21 - 22s - loss: 606.6780 - loglik: -6.0966e+02 - logprior: 2.9862
Epoch 4/10
21/21 - 22s - loss: 606.4540 - loglik: -6.0977e+02 - logprior: 3.3118
Epoch 5/10
21/21 - 22s - loss: 604.2841 - loglik: -6.0782e+02 - logprior: 3.5312
Epoch 6/10
21/21 - 22s - loss: 604.1642 - loglik: -6.0792e+02 - logprior: 3.7521
Epoch 7/10
21/21 - 22s - loss: 601.3913 - loglik: -6.0537e+02 - logprior: 3.9750
Epoch 8/10
21/21 - 22s - loss: 600.7534 - loglik: -6.0498e+02 - logprior: 4.2217
Epoch 9/10
21/21 - 22s - loss: 602.6066 - loglik: -6.0705e+02 - logprior: 4.4393
Fitted a model with MAP estimate = -600.4502
Time for alignment: 454.0249
Computed alignments with likelihoods: ['-606.0581', '-602.2734', '-600.9632', '-604.1845', '-600.4502']
Best model has likelihood: -600.4502  (prior= 4.5745 )
time for generating output: 0.3244
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.9604872251931076
Training of 5 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fac4d0ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ecb90a490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f9111b190>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1ff90d5700>
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 861.3658 - loglik: -8.5972e+02 - logprior: -1.6432e+00
Epoch 2/10
39/39 - 20s - loss: 752.8959 - loglik: -7.5126e+02 - logprior: -1.6390e+00
Epoch 3/10
39/39 - 20s - loss: 734.2422 - loglik: -7.3246e+02 - logprior: -1.7805e+00
Epoch 4/10
39/39 - 20s - loss: 730.0781 - loglik: -7.2829e+02 - logprior: -1.7830e+00
Epoch 5/10
39/39 - 20s - loss: 727.4189 - loglik: -7.2561e+02 - logprior: -1.8134e+00
Epoch 6/10
39/39 - 20s - loss: 726.8690 - loglik: -7.2506e+02 - logprior: -1.8063e+00
Epoch 7/10
39/39 - 20s - loss: 725.6796 - loglik: -7.2390e+02 - logprior: -1.7819e+00
Epoch 8/10
39/39 - 20s - loss: 725.5721 - loglik: -7.2379e+02 - logprior: -1.7870e+00
Epoch 9/10
39/39 - 20s - loss: 725.7774 - loglik: -7.2400e+02 - logprior: -1.7727e+00
Fitted a model with MAP estimate = -625.6111
expansions: [(14, 2), (17, 1), (28, 1), (35, 1), (38, 1), (41, 1), (49, 1), (50, 2), (51, 2), (68, 1), (70, 1), (72, 1), (73, 2), (74, 1), (75, 1), (77, 1), (82, 1), (86, 3), (91, 1), (106, 2), (111, 3), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (142, 2), (148, 1), (149, 1), (152, 2), (153, 3), (163, 2), (164, 2), (165, 5), (166, 3), (167, 2), (177, 3), (178, 1), (182, 1), (184, 3), (186, 2), (187, 1), (193, 1), (194, 1), (195, 1), (208, 2), (209, 1), (211, 1), (212, 4), (241, 1), (244, 3)]
discards: [  0  97  98  99 100 101 102 103 104 143 144 145 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 316 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 722.1253 - loglik: -7.1929e+02 - logprior: -2.8332e+00
Epoch 2/2
39/39 - 29s - loss: 705.8010 - loglik: -7.0421e+02 - logprior: -1.5908e+00
Fitted a model with MAP estimate = -605.7348
expansions: [(0, 2), (181, 1), (316, 2)]
discards: [  0  60 115 117 118 119 130 201 202 276 277 278 313 314 315]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 708.9313 - loglik: -7.0706e+02 - logprior: -1.8691e+00
Epoch 2/2
39/39 - 28s - loss: 703.7913 - loglik: -7.0296e+02 - logprior: -8.3625e-01
Fitted a model with MAP estimate = -605.0877
expansions: [(114, 1), (115, 2), (116, 2)]
discards: [  0  87 247 304 305]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 306 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 607.1209 - loglik: -6.0504e+02 - logprior: -2.0790e+00
Epoch 2/10
43/43 - 30s - loss: 603.3165 - loglik: -6.0277e+02 - logprior: -5.4788e-01
Epoch 3/10
43/43 - 31s - loss: 598.8088 - loglik: -5.9841e+02 - logprior: -3.9731e-01
Epoch 4/10
43/43 - 30s - loss: 596.2394 - loglik: -5.9593e+02 - logprior: -3.0451e-01
Epoch 5/10
43/43 - 30s - loss: 599.1484 - loglik: -5.9890e+02 - logprior: -2.5148e-01
Fitted a model with MAP estimate = -597.2969
Time for alignment: 608.1473
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 861.3873 - loglik: -8.5976e+02 - logprior: -1.6263e+00
Epoch 2/10
39/39 - 20s - loss: 754.5507 - loglik: -7.5289e+02 - logprior: -1.6570e+00
Epoch 3/10
39/39 - 20s - loss: 735.0157 - loglik: -7.3313e+02 - logprior: -1.8812e+00
Epoch 4/10
39/39 - 20s - loss: 730.7144 - loglik: -7.2888e+02 - logprior: -1.8328e+00
Epoch 5/10
39/39 - 20s - loss: 729.3085 - loglik: -7.2750e+02 - logprior: -1.8129e+00
Epoch 6/10
39/39 - 20s - loss: 728.7807 - loglik: -7.2697e+02 - logprior: -1.8136e+00
Epoch 7/10
39/39 - 20s - loss: 728.1347 - loglik: -7.2631e+02 - logprior: -1.8226e+00
Epoch 8/10
39/39 - 20s - loss: 727.5909 - loglik: -7.2577e+02 - logprior: -1.8191e+00
Epoch 9/10
39/39 - 20s - loss: 727.4616 - loglik: -7.2565e+02 - logprior: -1.8100e+00
Epoch 10/10
39/39 - 20s - loss: 727.2234 - loglik: -7.2541e+02 - logprior: -1.8103e+00
Fitted a model with MAP estimate = -626.9598
expansions: [(14, 1), (18, 2), (39, 2), (40, 1), (41, 1), (49, 1), (50, 1), (51, 1), (54, 1), (69, 1), (73, 4), (74, 2), (75, 1), (77, 1), (82, 1), (85, 2), (87, 2), (91, 1), (92, 1), (106, 2), (111, 2), (113, 1), (116, 1), (117, 3), (118, 1), (153, 4), (154, 3), (164, 1), (165, 1), (166, 4), (167, 3), (168, 2), (178, 4), (183, 4), (186, 1), (187, 2), (188, 1), (193, 1), (194, 1), (195, 1), (208, 2), (209, 1), (211, 1), (212, 4), (244, 3)]
discards: [  0  98  99 100 101 102 103 104 143 144 145 146 147 148 149 241]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 721.9147 - loglik: -7.1917e+02 - logprior: -2.7462e+00
Epoch 2/2
39/39 - 28s - loss: 705.9448 - loglik: -7.0448e+02 - logprior: -1.4640e+00
Fitted a model with MAP estimate = -606.3783
expansions: [(0, 2), (124, 4), (126, 1), (232, 3), (309, 3)]
discards: [  0  87  88  96  97 118 119 120 121 143 178 229 230 237 272 273 274 275]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 710.1826 - loglik: -7.0841e+02 - logprior: -1.7748e+00
Epoch 2/2
39/39 - 27s - loss: 704.1896 - loglik: -7.0344e+02 - logprior: -7.4644e-01
Fitted a model with MAP estimate = -605.6725
expansions: [(94, 1), (95, 1), (125, 3), (177, 1), (304, 3)]
discards: [  0 115 116 237 244 301 302 303]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 609.4502 - loglik: -6.0740e+02 - logprior: -2.0509e+00
Epoch 2/10
43/43 - 30s - loss: 600.4788 - loglik: -5.9986e+02 - logprior: -6.1479e-01
Epoch 3/10
43/43 - 30s - loss: 599.5880 - loglik: -5.9916e+02 - logprior: -4.2732e-01
Epoch 4/10
43/43 - 30s - loss: 602.1608 - loglik: -6.0153e+02 - logprior: -6.3451e-01
Fitted a model with MAP estimate = -598.2973
Time for alignment: 590.5282
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 860.7827 - loglik: -8.5915e+02 - logprior: -1.6307e+00
Epoch 2/10
39/39 - 20s - loss: 753.4250 - loglik: -7.5179e+02 - logprior: -1.6338e+00
Epoch 3/10
39/39 - 20s - loss: 734.1217 - loglik: -7.3236e+02 - logprior: -1.7648e+00
Epoch 4/10
39/39 - 20s - loss: 730.4731 - loglik: -7.2874e+02 - logprior: -1.7295e+00
Epoch 5/10
39/39 - 20s - loss: 729.6898 - loglik: -7.2795e+02 - logprior: -1.7431e+00
Epoch 6/10
39/39 - 20s - loss: 728.1187 - loglik: -7.2637e+02 - logprior: -1.7532e+00
Epoch 7/10
39/39 - 20s - loss: 727.8207 - loglik: -7.2608e+02 - logprior: -1.7420e+00
Epoch 8/10
39/39 - 20s - loss: 727.9149 - loglik: -7.2616e+02 - logprior: -1.7540e+00
Fitted a model with MAP estimate = -627.2073
expansions: [(14, 2), (17, 3), (38, 1), (39, 1), (40, 1), (48, 1), (50, 1), (51, 1), (68, 1), (70, 1), (72, 3), (73, 2), (74, 1), (76, 1), (81, 1), (85, 3), (90, 1), (91, 1), (106, 2), (111, 3), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (141, 1), (143, 2), (154, 6), (156, 1), (164, 1), (165, 1), (166, 5), (167, 3), (168, 2), (180, 2), (183, 1), (184, 2), (185, 2), (193, 3), (205, 1), (208, 1), (209, 1), (212, 4), (241, 1), (244, 3)]
discards: [  0  96  97  98  99 100 101 102 103 104 137 138 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 723.0512 - loglik: -7.2013e+02 - logprior: -2.9258e+00
Epoch 2/2
39/39 - 29s - loss: 706.9100 - loglik: -7.0532e+02 - logprior: -1.5933e+00
Fitted a model with MAP estimate = -606.3610
expansions: [(0, 2), (121, 4), (186, 2), (224, 2), (311, 2)]
discards: [  0  86 130 166 167 168 170 171 204 246 247 308 309 310]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 707.8966 - loglik: -7.0592e+02 - logprior: -1.9808e+00
Epoch 2/2
39/39 - 28s - loss: 700.9257 - loglik: -7.0013e+02 - logprior: -7.9482e-01
Fitted a model with MAP estimate = -602.9024
expansions: [(168, 1), (186, 1), (225, 1)]
discards: [  0  96 121 122 123 124 125 169 307 308]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 611.0074 - loglik: -6.0897e+02 - logprior: -2.0382e+00
Epoch 2/10
43/43 - 30s - loss: 601.0358 - loglik: -6.0039e+02 - logprior: -6.4905e-01
Epoch 3/10
43/43 - 30s - loss: 605.4998 - loglik: -6.0510e+02 - logprior: -4.0464e-01
Fitted a model with MAP estimate = -600.1301
Time for alignment: 523.8424
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 859.1514 - loglik: -8.5753e+02 - logprior: -1.6189e+00
Epoch 2/10
39/39 - 20s - loss: 753.8998 - loglik: -7.5238e+02 - logprior: -1.5243e+00
Epoch 3/10
39/39 - 20s - loss: 733.1594 - loglik: -7.3145e+02 - logprior: -1.7124e+00
Epoch 4/10
39/39 - 20s - loss: 728.9364 - loglik: -7.2722e+02 - logprior: -1.7121e+00
Epoch 5/10
39/39 - 20s - loss: 727.6895 - loglik: -7.2598e+02 - logprior: -1.7071e+00
Epoch 6/10
39/39 - 20s - loss: 726.8439 - loglik: -7.2515e+02 - logprior: -1.6989e+00
Epoch 7/10
39/39 - 20s - loss: 726.0571 - loglik: -7.2435e+02 - logprior: -1.7062e+00
Epoch 8/10
39/39 - 20s - loss: 725.5253 - loglik: -7.2382e+02 - logprior: -1.7023e+00
Epoch 9/10
39/39 - 20s - loss: 725.4139 - loglik: -7.2372e+02 - logprior: -1.6983e+00
Epoch 10/10
39/39 - 20s - loss: 724.7528 - loglik: -7.2305e+02 - logprior: -1.7009e+00
Fitted a model with MAP estimate = -624.7127
expansions: [(14, 1), (18, 2), (40, 1), (41, 1), (51, 1), (53, 1), (54, 1), (74, 1), (76, 3), (77, 2), (78, 2), (80, 1), (85, 1), (86, 1), (87, 2), (89, 1), (94, 3), (112, 1), (113, 1), (114, 1), (117, 1), (118, 3), (119, 1), (141, 7), (144, 2), (145, 1), (151, 2), (152, 3), (163, 1), (164, 1), (165, 4), (166, 3), (167, 2), (177, 3), (182, 2), (183, 4), (184, 2), (189, 1), (192, 2), (193, 1), (208, 1), (209, 1), (211, 5), (212, 1), (241, 1), (244, 3)]
discards: [  0 100 101 102 103 104 105 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 322 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 720.0851 - loglik: -7.1720e+02 - logprior: -2.8829e+00
Epoch 2/2
39/39 - 30s - loss: 703.9854 - loglik: -7.0232e+02 - logprior: -1.6647e+00
Fitted a model with MAP estimate = -604.7842
expansions: [(0, 2), (124, 3), (125, 8), (165, 2), (191, 1), (232, 1), (322, 2)]
discards: [  0  91 105 106 140 168 169 170 242 243 281 282 283 284 319 320 321]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 324 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 702.1370 - loglik: -7.0015e+02 - logprior: -1.9873e+00
Epoch 2/2
39/39 - 30s - loss: 694.6461 - loglik: -6.9387e+02 - logprior: -7.7584e-01
Fitted a model with MAP estimate = -597.4793
expansions: [(122, 2), (123, 2)]
discards: [  0 124 125 126 127 172 251 263 322 323]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 318 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 35s - loss: 603.5645 - loglik: -6.0142e+02 - logprior: -2.1426e+00
Epoch 2/10
43/43 - 32s - loss: 597.8596 - loglik: -5.9726e+02 - logprior: -6.0023e-01
Epoch 3/10
43/43 - 32s - loss: 595.1981 - loglik: -5.9478e+02 - logprior: -4.1540e-01
Epoch 4/10
43/43 - 32s - loss: 595.1282 - loglik: -5.9478e+02 - logprior: -3.4823e-01
Epoch 5/10
43/43 - 32s - loss: 591.9086 - loglik: -5.9162e+02 - logprior: -2.8623e-01
Epoch 6/10
43/43 - 32s - loss: 592.9988 - loglik: -5.9279e+02 - logprior: -2.0974e-01
Fitted a model with MAP estimate = -592.2259
Time for alignment: 678.4439
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 862.2437 - loglik: -8.6061e+02 - logprior: -1.6294e+00
Epoch 2/10
39/39 - 20s - loss: 755.3475 - loglik: -7.5374e+02 - logprior: -1.6091e+00
Epoch 3/10
39/39 - 20s - loss: 735.0553 - loglik: -7.3331e+02 - logprior: -1.7444e+00
Epoch 4/10
39/39 - 20s - loss: 731.1143 - loglik: -7.2941e+02 - logprior: -1.6995e+00
Epoch 5/10
39/39 - 20s - loss: 729.5304 - loglik: -7.2785e+02 - logprior: -1.6847e+00
Epoch 6/10
39/39 - 20s - loss: 729.0112 - loglik: -7.2734e+02 - logprior: -1.6740e+00
Epoch 7/10
39/39 - 20s - loss: 728.6264 - loglik: -7.2696e+02 - logprior: -1.6704e+00
Epoch 8/10
39/39 - 20s - loss: 727.5220 - loglik: -7.2584e+02 - logprior: -1.6819e+00
Epoch 9/10
39/39 - 20s - loss: 727.9110 - loglik: -7.2625e+02 - logprior: -1.6607e+00
Fitted a model with MAP estimate = -627.4638
expansions: [(14, 3), (16, 2), (37, 1), (49, 1), (50, 1), (51, 1), (54, 1), (71, 1), (73, 3), (74, 2), (75, 1), (77, 1), (82, 1), (85, 3), (87, 1), (92, 3), (105, 1), (106, 3), (111, 2), (113, 1), (116, 1), (117, 3), (118, 1), (143, 1), (144, 2), (148, 1), (152, 2), (153, 7), (164, 1), (165, 1), (166, 4), (167, 3), (168, 2), (178, 3), (183, 2), (184, 3), (186, 2), (187, 1), (193, 1), (194, 1), (195, 1), (208, 2), (209, 1), (211, 2), (212, 4), (241, 1), (244, 3)]
discards: [  0  98  99 100 101 102 103]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 717.8760 - loglik: -7.1504e+02 - logprior: -2.8328e+00
Epoch 2/2
39/39 - 30s - loss: 702.0865 - loglik: -7.0044e+02 - logprior: -1.6514e+00
Fitted a model with MAP estimate = -603.0902
expansions: [(0, 2), (123, 6), (237, 1), (326, 2)]
discards: [  0  85 103 104 105 125 145 176 178 179 188 254 285 286 287 288 323 324
 325]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 318 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 705.0195 - loglik: -7.0294e+02 - logprior: -2.0795e+00
Epoch 2/2
39/39 - 29s - loss: 697.6638 - loglik: -6.9675e+02 - logprior: -9.1841e-01
Fitted a model with MAP estimate = -600.4077
expansions: [(177, 2), (178, 1), (182, 1), (252, 1)]
discards: [  0 120 121 122 123 124 125 126 259 316 317]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 607.7458 - loglik: -6.0561e+02 - logprior: -2.1370e+00
Epoch 2/10
43/43 - 31s - loss: 600.5518 - loglik: -5.9994e+02 - logprior: -6.1643e-01
Epoch 3/10
43/43 - 30s - loss: 599.6205 - loglik: -5.9917e+02 - logprior: -4.5114e-01
Epoch 4/10
43/43 - 31s - loss: 598.0869 - loglik: -5.9772e+02 - logprior: -3.6612e-01
Epoch 5/10
43/43 - 31s - loss: 597.7520 - loglik: -5.9745e+02 - logprior: -3.0306e-01
Epoch 6/10
43/43 - 31s - loss: 596.7587 - loglik: -5.9653e+02 - logprior: -2.2726e-01
Epoch 7/10
43/43 - 31s - loss: 597.2226 - loglik: -5.9706e+02 - logprior: -1.6110e-01
Fitted a model with MAP estimate = -596.6580
Time for alignment: 684.5738
Computed alignments with likelihoods: ['-597.2969', '-598.2973', '-600.1301', '-592.2259', '-596.6580']
Best model has likelihood: -592.2259  (prior= -0.1765 )
time for generating output: 0.4026
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.5141969589623413
Training of 5 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff0fa5b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201b243eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91ea29a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fac2e4ee0>
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 277.1152 - loglik: -2.3900e+02 - logprior: -3.8117e+01
Epoch 2/10
10/10 - 1s - loss: 228.3670 - loglik: -2.1835e+02 - logprior: -1.0019e+01
Epoch 3/10
10/10 - 1s - loss: 200.3999 - loglik: -1.9540e+02 - logprior: -5.0017e+00
Epoch 4/10
10/10 - 1s - loss: 183.5123 - loglik: -1.8003e+02 - logprior: -3.4837e+00
Epoch 5/10
10/10 - 1s - loss: 176.0915 - loglik: -1.7332e+02 - logprior: -2.7705e+00
Epoch 6/10
10/10 - 1s - loss: 173.0170 - loglik: -1.7076e+02 - logprior: -2.2529e+00
Epoch 7/10
10/10 - 1s - loss: 171.1437 - loglik: -1.6920e+02 - logprior: -1.9410e+00
Epoch 8/10
10/10 - 1s - loss: 170.4080 - loglik: -1.6860e+02 - logprior: -1.8059e+00
Epoch 9/10
10/10 - 1s - loss: 169.8690 - loglik: -1.6810e+02 - logprior: -1.7720e+00
Epoch 10/10
10/10 - 1s - loss: 169.3084 - loglik: -1.6760e+02 - logprior: -1.7037e+00
Fitted a model with MAP estimate = -169.0881
expansions: [(7, 2), (13, 1), (15, 2), (20, 2), (21, 2), (22, 2), (25, 1), (29, 1), (46, 1), (47, 1), (53, 3), (54, 1), (55, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 213.7775 - loglik: -1.7111e+02 - logprior: -4.2665e+01
Epoch 2/2
10/10 - 1s - loss: 176.9429 - loglik: -1.5964e+02 - logprior: -1.7300e+01
Fitted a model with MAP estimate = -170.5152
expansions: [(0, 2)]
discards: [ 0 18 27]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 188.8722 - loglik: -1.5511e+02 - logprior: -3.3760e+01
Epoch 2/2
10/10 - 1s - loss: 161.0545 - loglik: -1.5257e+02 - logprior: -8.4828e+00
Fitted a model with MAP estimate = -157.3973
expansions: []
discards: [ 0 70 72]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 195.6954 - loglik: -1.5639e+02 - logprior: -3.9304e+01
Epoch 2/10
10/10 - 1s - loss: 165.4769 - loglik: -1.5486e+02 - logprior: -1.0612e+01
Epoch 3/10
10/10 - 1s - loss: 158.0359 - loglik: -1.5411e+02 - logprior: -3.9273e+00
Epoch 4/10
10/10 - 1s - loss: 155.0559 - loglik: -1.5325e+02 - logprior: -1.8017e+00
Epoch 5/10
10/10 - 1s - loss: 153.8501 - loglik: -1.5314e+02 - logprior: -7.0630e-01
Epoch 6/10
10/10 - 1s - loss: 153.1163 - loglik: -1.5326e+02 - logprior: 0.1420
Epoch 7/10
10/10 - 1s - loss: 152.6100 - loglik: -1.5323e+02 - logprior: 0.6236
Epoch 8/10
10/10 - 1s - loss: 152.5786 - loglik: -1.5345e+02 - logprior: 0.8753
Epoch 9/10
10/10 - 1s - loss: 151.7195 - loglik: -1.5279e+02 - logprior: 1.0731
Epoch 10/10
10/10 - 1s - loss: 151.8901 - loglik: -1.5315e+02 - logprior: 1.2626
Fitted a model with MAP estimate = -151.6697
Time for alignment: 44.8394
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 277.1866 - loglik: -2.3907e+02 - logprior: -3.8115e+01
Epoch 2/10
10/10 - 1s - loss: 228.2442 - loglik: -2.1822e+02 - logprior: -1.0022e+01
Epoch 3/10
10/10 - 1s - loss: 199.5973 - loglik: -1.9457e+02 - logprior: -5.0306e+00
Epoch 4/10
10/10 - 1s - loss: 183.1671 - loglik: -1.7966e+02 - logprior: -3.5058e+00
Epoch 5/10
10/10 - 1s - loss: 176.6731 - loglik: -1.7395e+02 - logprior: -2.7207e+00
Epoch 6/10
10/10 - 1s - loss: 173.2699 - loglik: -1.7110e+02 - logprior: -2.1732e+00
Epoch 7/10
10/10 - 1s - loss: 171.7743 - loglik: -1.6993e+02 - logprior: -1.8481e+00
Epoch 8/10
10/10 - 1s - loss: 170.7360 - loglik: -1.6903e+02 - logprior: -1.7083e+00
Epoch 9/10
10/10 - 1s - loss: 169.6272 - loglik: -1.6795e+02 - logprior: -1.6781e+00
Epoch 10/10
10/10 - 1s - loss: 169.3932 - loglik: -1.6773e+02 - logprior: -1.6589e+00
Fitted a model with MAP estimate = -169.0921
expansions: [(7, 2), (12, 1), (15, 2), (20, 2), (21, 2), (22, 2), (25, 1), (29, 1), (46, 1), (47, 1), (54, 3), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 213.9171 - loglik: -1.7122e+02 - logprior: -4.2702e+01
Epoch 2/2
10/10 - 1s - loss: 177.3139 - loglik: -1.5994e+02 - logprior: -1.7377e+01
Fitted a model with MAP estimate = -170.5501
expansions: [(0, 2)]
discards: [ 0 18 27 30 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 189.0242 - loglik: -1.5519e+02 - logprior: -3.3833e+01
Epoch 2/2
10/10 - 1s - loss: 161.6304 - loglik: -1.5301e+02 - logprior: -8.6224e+00
Fitted a model with MAP estimate = -157.5696
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 194.0755 - loglik: -1.5468e+02 - logprior: -3.9399e+01
Epoch 2/10
10/10 - 1s - loss: 164.8205 - loglik: -1.5414e+02 - logprior: -1.0676e+01
Epoch 3/10
10/10 - 1s - loss: 157.4017 - loglik: -1.5338e+02 - logprior: -4.0212e+00
Epoch 4/10
10/10 - 1s - loss: 154.3059 - loglik: -1.5239e+02 - logprior: -1.9131e+00
Epoch 5/10
10/10 - 1s - loss: 153.4647 - loglik: -1.5267e+02 - logprior: -7.9430e-01
Epoch 6/10
10/10 - 1s - loss: 152.7561 - loglik: -1.5282e+02 - logprior: 0.0629
Epoch 7/10
10/10 - 1s - loss: 152.1646 - loglik: -1.5272e+02 - logprior: 0.5526
Epoch 8/10
10/10 - 1s - loss: 152.0638 - loglik: -1.5286e+02 - logprior: 0.7993
Epoch 9/10
10/10 - 1s - loss: 151.6590 - loglik: -1.5267e+02 - logprior: 1.0121
Epoch 10/10
10/10 - 1s - loss: 151.5913 - loglik: -1.5279e+02 - logprior: 1.1993
Fitted a model with MAP estimate = -151.3804
Time for alignment: 44.7588
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 276.9832 - loglik: -2.3887e+02 - logprior: -3.8116e+01
Epoch 2/10
10/10 - 1s - loss: 228.2756 - loglik: -2.1825e+02 - logprior: -1.0021e+01
Epoch 3/10
10/10 - 1s - loss: 200.3219 - loglik: -1.9530e+02 - logprior: -5.0246e+00
Epoch 4/10
10/10 - 1s - loss: 185.0293 - loglik: -1.8158e+02 - logprior: -3.4500e+00
Epoch 5/10
10/10 - 1s - loss: 178.0375 - loglik: -1.7543e+02 - logprior: -2.6120e+00
Epoch 6/10
10/10 - 1s - loss: 174.0705 - loglik: -1.7190e+02 - logprior: -2.1744e+00
Epoch 7/10
10/10 - 1s - loss: 172.6093 - loglik: -1.7073e+02 - logprior: -1.8799e+00
Epoch 8/10
10/10 - 1s - loss: 170.9916 - loglik: -1.6924e+02 - logprior: -1.7466e+00
Epoch 9/10
10/10 - 1s - loss: 170.3166 - loglik: -1.6859e+02 - logprior: -1.7229e+00
Epoch 10/10
10/10 - 1s - loss: 169.6829 - loglik: -1.6798e+02 - logprior: -1.7044e+00
Fitted a model with MAP estimate = -169.5306
expansions: [(7, 2), (12, 1), (14, 2), (18, 1), (20, 4), (25, 1), (29, 1), (46, 1), (47, 1), (53, 3), (54, 1), (55, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 214.3182 - loglik: -1.7160e+02 - logprior: -4.2719e+01
Epoch 2/2
10/10 - 1s - loss: 176.8241 - loglik: -1.5948e+02 - logprior: -1.7344e+01
Fitted a model with MAP estimate = -170.6227
expansions: [(0, 2)]
discards: [ 0 16 27]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 188.9899 - loglik: -1.5510e+02 - logprior: -3.3886e+01
Epoch 2/2
10/10 - 1s - loss: 161.8498 - loglik: -1.5325e+02 - logprior: -8.5959e+00
Fitted a model with MAP estimate = -157.8433
expansions: []
discards: [ 0 71]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 194.4715 - loglik: -1.5513e+02 - logprior: -3.9343e+01
Epoch 2/10
10/10 - 1s - loss: 165.1178 - loglik: -1.5450e+02 - logprior: -1.0621e+01
Epoch 3/10
10/10 - 1s - loss: 157.3844 - loglik: -1.5339e+02 - logprior: -3.9976e+00
Epoch 4/10
10/10 - 1s - loss: 154.5482 - loglik: -1.5266e+02 - logprior: -1.8880e+00
Epoch 5/10
10/10 - 1s - loss: 153.5308 - loglik: -1.5276e+02 - logprior: -7.6945e-01
Epoch 6/10
10/10 - 1s - loss: 152.9937 - loglik: -1.5308e+02 - logprior: 0.0895
Epoch 7/10
10/10 - 1s - loss: 152.2824 - loglik: -1.5286e+02 - logprior: 0.5819
Epoch 8/10
10/10 - 1s - loss: 152.1115 - loglik: -1.5294e+02 - logprior: 0.8303
Epoch 9/10
10/10 - 1s - loss: 151.9277 - loglik: -1.5297e+02 - logprior: 1.0448
Epoch 10/10
10/10 - 1s - loss: 151.4616 - loglik: -1.5270e+02 - logprior: 1.2361
Fitted a model with MAP estimate = -151.4816
Time for alignment: 44.8174
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 276.9052 - loglik: -2.3879e+02 - logprior: -3.8115e+01
Epoch 2/10
10/10 - 1s - loss: 228.2701 - loglik: -2.1825e+02 - logprior: -1.0022e+01
Epoch 3/10
10/10 - 1s - loss: 200.3231 - loglik: -1.9528e+02 - logprior: -5.0419e+00
Epoch 4/10
10/10 - 1s - loss: 184.1219 - loglik: -1.8063e+02 - logprior: -3.4899e+00
Epoch 5/10
10/10 - 1s - loss: 176.8050 - loglik: -1.7413e+02 - logprior: -2.6772e+00
Epoch 6/10
10/10 - 1s - loss: 173.8395 - loglik: -1.7171e+02 - logprior: -2.1304e+00
Epoch 7/10
10/10 - 1s - loss: 172.5519 - loglik: -1.7071e+02 - logprior: -1.8464e+00
Epoch 8/10
10/10 - 1s - loss: 171.1920 - loglik: -1.6943e+02 - logprior: -1.7659e+00
Epoch 9/10
10/10 - 1s - loss: 170.5377 - loglik: -1.6877e+02 - logprior: -1.7636e+00
Epoch 10/10
10/10 - 1s - loss: 170.0083 - loglik: -1.6829e+02 - logprior: -1.7199e+00
Fitted a model with MAP estimate = -169.7966
expansions: [(7, 2), (12, 1), (14, 2), (20, 2), (21, 2), (22, 2), (25, 1), (29, 1), (46, 1), (47, 1), (53, 3), (54, 1), (55, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 214.3099 - loglik: -1.7162e+02 - logprior: -4.2685e+01
Epoch 2/2
10/10 - 1s - loss: 177.3981 - loglik: -1.6007e+02 - logprior: -1.7325e+01
Fitted a model with MAP estimate = -170.8792
expansions: [(0, 2)]
discards: [ 0 16 27 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 189.8755 - loglik: -1.5604e+02 - logprior: -3.3833e+01
Epoch 2/2
10/10 - 1s - loss: 161.7900 - loglik: -1.5320e+02 - logprior: -8.5934e+00
Fitted a model with MAP estimate = -157.8763
expansions: []
discards: [ 0 71]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 194.8215 - loglik: -1.5550e+02 - logprior: -3.9325e+01
Epoch 2/10
10/10 - 1s - loss: 165.2128 - loglik: -1.5463e+02 - logprior: -1.0588e+01
Epoch 3/10
10/10 - 1s - loss: 157.7644 - loglik: -1.5384e+02 - logprior: -3.9227e+00
Epoch 4/10
10/10 - 1s - loss: 154.9760 - loglik: -1.5318e+02 - logprior: -1.7960e+00
Epoch 5/10
10/10 - 1s - loss: 153.4387 - loglik: -1.5275e+02 - logprior: -6.8445e-01
Epoch 6/10
10/10 - 1s - loss: 152.8658 - loglik: -1.5304e+02 - logprior: 0.1774
Epoch 7/10
10/10 - 1s - loss: 152.6277 - loglik: -1.5332e+02 - logprior: 0.6886
Epoch 8/10
10/10 - 1s - loss: 152.0669 - loglik: -1.5301e+02 - logprior: 0.9402
Epoch 9/10
10/10 - 1s - loss: 152.0788 - loglik: -1.5322e+02 - logprior: 1.1402
Fitted a model with MAP estimate = -151.7445
Time for alignment: 43.0381
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 277.0979 - loglik: -2.3898e+02 - logprior: -3.8116e+01
Epoch 2/10
10/10 - 1s - loss: 228.2591 - loglik: -2.1824e+02 - logprior: -1.0022e+01
Epoch 3/10
10/10 - 1s - loss: 199.4240 - loglik: -1.9442e+02 - logprior: -5.0002e+00
Epoch 4/10
10/10 - 1s - loss: 182.9460 - loglik: -1.7960e+02 - logprior: -3.3428e+00
Epoch 5/10
10/10 - 1s - loss: 176.8896 - loglik: -1.7442e+02 - logprior: -2.4720e+00
Epoch 6/10
10/10 - 1s - loss: 174.1784 - loglik: -1.7228e+02 - logprior: -1.8943e+00
Epoch 7/10
10/10 - 1s - loss: 172.2599 - loglik: -1.7074e+02 - logprior: -1.5237e+00
Epoch 8/10
10/10 - 1s - loss: 171.1388 - loglik: -1.6975e+02 - logprior: -1.3856e+00
Epoch 9/10
10/10 - 1s - loss: 169.9252 - loglik: -1.6855e+02 - logprior: -1.3796e+00
Epoch 10/10
10/10 - 1s - loss: 169.7922 - loglik: -1.6844e+02 - logprior: -1.3506e+00
Fitted a model with MAP estimate = -169.4299
expansions: [(7, 2), (12, 1), (15, 2), (21, 5), (29, 1), (46, 1), (47, 1), (54, 2), (55, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 214.0673 - loglik: -1.7131e+02 - logprior: -4.2762e+01
Epoch 2/2
10/10 - 1s - loss: 178.0658 - loglik: -1.6069e+02 - logprior: -1.7371e+01
Fitted a model with MAP estimate = -171.6728
expansions: [(0, 2), (69, 1)]
discards: [ 0 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 190.1471 - loglik: -1.5622e+02 - logprior: -3.3924e+01
Epoch 2/2
10/10 - 1s - loss: 161.8707 - loglik: -1.5330e+02 - logprior: -8.5706e+00
Fitted a model with MAP estimate = -157.8076
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 194.7510 - loglik: -1.5519e+02 - logprior: -3.9558e+01
Epoch 2/10
10/10 - 1s - loss: 164.5450 - loglik: -1.5376e+02 - logprior: -1.0781e+01
Epoch 3/10
10/10 - 1s - loss: 157.3016 - loglik: -1.5326e+02 - logprior: -4.0404e+00
Epoch 4/10
10/10 - 1s - loss: 155.1119 - loglik: -1.5320e+02 - logprior: -1.9083e+00
Epoch 5/10
10/10 - 1s - loss: 153.2874 - loglik: -1.5249e+02 - logprior: -7.9501e-01
Epoch 6/10
10/10 - 1s - loss: 152.6787 - loglik: -1.5274e+02 - logprior: 0.0599
Epoch 7/10
10/10 - 1s - loss: 152.5462 - loglik: -1.5309e+02 - logprior: 0.5420
Epoch 8/10
10/10 - 1s - loss: 152.1346 - loglik: -1.5291e+02 - logprior: 0.7726
Epoch 9/10
10/10 - 1s - loss: 151.8719 - loglik: -1.5285e+02 - logprior: 0.9793
Epoch 10/10
10/10 - 1s - loss: 151.7312 - loglik: -1.5289e+02 - logprior: 1.1599
Fitted a model with MAP estimate = -151.6077
Time for alignment: 44.8839
Computed alignments with likelihoods: ['-151.6697', '-151.3804', '-151.4816', '-151.7445', '-151.6077']
Best model has likelihood: -151.3804  (prior= 1.2830 )
time for generating output: 0.1749
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.8007911839502685
Training of 5 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5a662e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdffa7520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5f17f40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2001fb7d30>
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 306.8033 - loglik: -2.9586e+02 - logprior: -1.0946e+01
Epoch 2/10
19/19 - 6s - loss: 245.0549 - loglik: -2.4281e+02 - logprior: -2.2477e+00
Epoch 3/10
19/19 - 4s - loss: 221.8899 - loglik: -2.1979e+02 - logprior: -2.0962e+00
Epoch 4/10
19/19 - 6s - loss: 219.4441 - loglik: -2.1759e+02 - logprior: -1.8568e+00
Epoch 5/10
19/19 - 4s - loss: 215.5648 - loglik: -2.1389e+02 - logprior: -1.6788e+00
Epoch 6/10
19/19 - 5s - loss: 216.8023 - loglik: -2.1513e+02 - logprior: -1.6717e+00
Fitted a model with MAP estimate = -216.4303
expansions: [(10, 3), (11, 2), (12, 2), (30, 1), (31, 1), (32, 1), (45, 1), (47, 1), (48, 4), (49, 2), (57, 1), (58, 1), (73, 1), (74, 1), (75, 2), (76, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 224.2225 - loglik: -2.1064e+02 - logprior: -1.3585e+01
Epoch 2/2
19/19 - 5s - loss: 207.1599 - loglik: -2.0283e+02 - logprior: -4.3312e+00
Fitted a model with MAP estimate = -202.7409
expansions: [(0, 2)]
discards: [ 0 13 14 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 211.2151 - loglik: -2.0184e+02 - logprior: -9.3786e+00
Epoch 2/2
19/19 - 6s - loss: 201.1735 - loglik: -1.9995e+02 - logprior: -1.2202e+00
Fitted a model with MAP estimate = -200.0719
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 213.1434 - loglik: -2.0191e+02 - logprior: -1.1232e+01
Epoch 2/10
19/19 - 4s - loss: 202.2670 - loglik: -2.0070e+02 - logprior: -1.5689e+00
Epoch 3/10
19/19 - 6s - loss: 200.4448 - loglik: -2.0016e+02 - logprior: -2.8250e-01
Epoch 4/10
19/19 - 4s - loss: 198.8292 - loglik: -1.9893e+02 - logprior: 0.0965
Epoch 5/10
19/19 - 5s - loss: 198.7277 - loglik: -1.9899e+02 - logprior: 0.2630
Epoch 6/10
19/19 - 4s - loss: 197.8138 - loglik: -1.9818e+02 - logprior: 0.3658
Epoch 7/10
19/19 - 5s - loss: 198.7203 - loglik: -1.9917e+02 - logprior: 0.4543
Fitted a model with MAP estimate = -197.8302
Time for alignment: 111.4332
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 305.9992 - loglik: -2.9505e+02 - logprior: -1.0946e+01
Epoch 2/10
19/19 - 5s - loss: 245.0442 - loglik: -2.4283e+02 - logprior: -2.2184e+00
Epoch 3/10
19/19 - 6s - loss: 222.6306 - loglik: -2.2058e+02 - logprior: -2.0512e+00
Epoch 4/10
19/19 - 5s - loss: 218.7406 - loglik: -2.1695e+02 - logprior: -1.7941e+00
Epoch 5/10
19/19 - 5s - loss: 217.5450 - loglik: -2.1594e+02 - logprior: -1.6062e+00
Epoch 6/10
19/19 - 6s - loss: 217.8472 - loglik: -2.1624e+02 - logprior: -1.6035e+00
Fitted a model with MAP estimate = -216.6625
expansions: [(10, 5), (11, 2), (30, 1), (31, 1), (32, 1), (45, 1), (47, 1), (48, 1), (49, 1), (50, 3), (56, 2), (73, 1), (74, 1), (75, 2), (76, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 223.9207 - loglik: -2.1031e+02 - logprior: -1.3612e+01
Epoch 2/2
19/19 - 5s - loss: 206.5335 - loglik: -2.0219e+02 - logprior: -4.3454e+00
Fitted a model with MAP estimate = -202.3337
expansions: [(0, 2)]
discards: [ 0 15 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 210.2606 - loglik: -2.0088e+02 - logprior: -9.3794e+00
Epoch 2/2
19/19 - 5s - loss: 199.8690 - loglik: -1.9866e+02 - logprior: -1.2085e+00
Fitted a model with MAP estimate = -199.2071
expansions: []
discards: [ 0 11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 213.6313 - loglik: -2.0236e+02 - logprior: -1.1274e+01
Epoch 2/10
19/19 - 6s - loss: 202.0559 - loglik: -2.0049e+02 - logprior: -1.5629e+00
Epoch 3/10
19/19 - 5s - loss: 200.9532 - loglik: -2.0067e+02 - logprior: -2.8374e-01
Epoch 4/10
19/19 - 5s - loss: 198.8488 - loglik: -1.9896e+02 - logprior: 0.1098
Epoch 5/10
19/19 - 6s - loss: 198.7179 - loglik: -1.9899e+02 - logprior: 0.2680
Epoch 6/10
19/19 - 5s - loss: 198.9359 - loglik: -1.9930e+02 - logprior: 0.3654
Fitted a model with MAP estimate = -198.3561
Time for alignment: 110.3393
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 306.8325 - loglik: -2.9589e+02 - logprior: -1.0939e+01
Epoch 2/10
19/19 - 5s - loss: 245.6337 - loglik: -2.4337e+02 - logprior: -2.2633e+00
Epoch 3/10
19/19 - 5s - loss: 223.0134 - loglik: -2.2083e+02 - logprior: -2.1816e+00
Epoch 4/10
19/19 - 4s - loss: 217.1452 - loglik: -2.1510e+02 - logprior: -2.0460e+00
Epoch 5/10
19/19 - 5s - loss: 216.0643 - loglik: -2.1424e+02 - logprior: -1.8280e+00
Epoch 6/10
19/19 - 5s - loss: 215.9076 - loglik: -2.1410e+02 - logprior: -1.8063e+00
Epoch 7/10
19/19 - 5s - loss: 215.0308 - loglik: -2.1325e+02 - logprior: -1.7810e+00
Epoch 8/10
19/19 - 4s - loss: 215.2529 - loglik: -2.1351e+02 - logprior: -1.7475e+00
Fitted a model with MAP estimate = -214.8203
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 1), (31, 1), (32, 1), (43, 1), (47, 1), (48, 4), (49, 2), (57, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 224.5112 - loglik: -2.1087e+02 - logprior: -1.3646e+01
Epoch 2/2
19/19 - 4s - loss: 205.7458 - loglik: -2.0132e+02 - logprior: -4.4274e+00
Fitted a model with MAP estimate = -202.8254
expansions: [(0, 2)]
discards: [ 0 13 60 61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 210.9120 - loglik: -2.0154e+02 - logprior: -9.3739e+00
Epoch 2/2
19/19 - 5s - loss: 201.0202 - loglik: -1.9981e+02 - logprior: -1.2102e+00
Fitted a model with MAP estimate = -199.6578
expansions: []
discards: [ 0 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 213.6890 - loglik: -2.0233e+02 - logprior: -1.1357e+01
Epoch 2/10
19/19 - 5s - loss: 201.5673 - loglik: -1.9999e+02 - logprior: -1.5793e+00
Epoch 3/10
19/19 - 5s - loss: 200.9893 - loglik: -2.0070e+02 - logprior: -2.9013e-01
Epoch 4/10
19/19 - 5s - loss: 199.3893 - loglik: -1.9950e+02 - logprior: 0.1108
Epoch 5/10
19/19 - 5s - loss: 197.5157 - loglik: -1.9779e+02 - logprior: 0.2699
Epoch 6/10
19/19 - 5s - loss: 198.7617 - loglik: -1.9913e+02 - logprior: 0.3707
Fitted a model with MAP estimate = -198.0544
Time for alignment: 115.9115
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 306.1570 - loglik: -2.9521e+02 - logprior: -1.0945e+01
Epoch 2/10
19/19 - 6s - loss: 245.4925 - loglik: -2.4324e+02 - logprior: -2.2557e+00
Epoch 3/10
19/19 - 6s - loss: 223.3247 - loglik: -2.2123e+02 - logprior: -2.0976e+00
Epoch 4/10
19/19 - 6s - loss: 218.7117 - loglik: -2.1687e+02 - logprior: -1.8397e+00
Epoch 5/10
19/19 - 5s - loss: 217.5019 - loglik: -2.1586e+02 - logprior: -1.6390e+00
Epoch 6/10
19/19 - 5s - loss: 215.6419 - loglik: -2.1401e+02 - logprior: -1.6364e+00
Epoch 7/10
19/19 - 5s - loss: 216.0141 - loglik: -2.1441e+02 - logprior: -1.6041e+00
Fitted a model with MAP estimate = -215.9356
expansions: [(10, 3), (11, 2), (12, 2), (30, 1), (31, 1), (32, 1), (47, 3), (48, 1), (49, 1), (50, 4), (54, 1), (56, 1), (73, 1), (74, 1), (75, 2), (76, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 224.2253 - loglik: -2.1064e+02 - logprior: -1.3581e+01
Epoch 2/2
19/19 - 6s - loss: 207.0995 - loglik: -2.0270e+02 - logprior: -4.4032e+00
Fitted a model with MAP estimate = -202.7729
expansions: [(0, 2)]
discards: [ 0 13 62 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 209.8244 - loglik: -2.0044e+02 - logprior: -9.3831e+00
Epoch 2/2
19/19 - 5s - loss: 201.0003 - loglik: -1.9978e+02 - logprior: -1.2219e+00
Fitted a model with MAP estimate = -199.4978
expansions: []
discards: [ 0 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 214.2015 - loglik: -2.0292e+02 - logprior: -1.1282e+01
Epoch 2/10
19/19 - 5s - loss: 201.5482 - loglik: -1.9997e+02 - logprior: -1.5791e+00
Epoch 3/10
19/19 - 5s - loss: 199.2454 - loglik: -1.9896e+02 - logprior: -2.9010e-01
Epoch 4/10
19/19 - 6s - loss: 200.2587 - loglik: -2.0037e+02 - logprior: 0.1096
Fitted a model with MAP estimate = -198.7710
Time for alignment: 102.3946
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 306.8774 - loglik: -2.9593e+02 - logprior: -1.0944e+01
Epoch 2/10
19/19 - 5s - loss: 245.0051 - loglik: -2.4281e+02 - logprior: -2.1918e+00
Epoch 3/10
19/19 - 5s - loss: 222.4054 - loglik: -2.2038e+02 - logprior: -2.0301e+00
Epoch 4/10
19/19 - 5s - loss: 218.5132 - loglik: -2.1670e+02 - logprior: -1.8115e+00
Epoch 5/10
19/19 - 5s - loss: 217.6753 - loglik: -2.1605e+02 - logprior: -1.6213e+00
Epoch 6/10
19/19 - 5s - loss: 217.4667 - loglik: -2.1585e+02 - logprior: -1.6169e+00
Epoch 7/10
19/19 - 6s - loss: 215.9613 - loglik: -2.1438e+02 - logprior: -1.5820e+00
Epoch 8/10
19/19 - 5s - loss: 216.6272 - loglik: -2.1507e+02 - logprior: -1.5570e+00
Fitted a model with MAP estimate = -216.1095
expansions: [(10, 5), (11, 1), (30, 1), (31, 1), (32, 1), (45, 1), (47, 1), (48, 2), (49, 1), (50, 3), (56, 2), (73, 1), (74, 1), (75, 2), (76, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 223.9746 - loglik: -2.1042e+02 - logprior: -1.3553e+01
Epoch 2/2
19/19 - 5s - loss: 205.8139 - loglik: -2.0141e+02 - logprior: -4.4003e+00
Fitted a model with MAP estimate = -202.3733
expansions: [(0, 2)]
discards: [ 0 61 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 210.6033 - loglik: -2.0123e+02 - logprior: -9.3757e+00
Epoch 2/2
19/19 - 5s - loss: 200.7883 - loglik: -1.9960e+02 - logprior: -1.1920e+00
Fitted a model with MAP estimate = -199.3502
expansions: []
discards: [ 0 11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 213.5233 - loglik: -2.0220e+02 - logprior: -1.1323e+01
Epoch 2/10
19/19 - 5s - loss: 202.8243 - loglik: -2.0127e+02 - logprior: -1.5516e+00
Epoch 3/10
19/19 - 6s - loss: 200.6887 - loglik: -2.0044e+02 - logprior: -2.4988e-01
Epoch 4/10
19/19 - 5s - loss: 199.2791 - loglik: -1.9942e+02 - logprior: 0.1428
Epoch 5/10
19/19 - 4s - loss: 197.9698 - loglik: -1.9828e+02 - logprior: 0.3065
Epoch 6/10
19/19 - 6s - loss: 199.6286 - loglik: -2.0004e+02 - logprior: 0.4151
Fitted a model with MAP estimate = -198.4128
Time for alignment: 115.5936
Computed alignments with likelihoods: ['-197.8302', '-198.3561', '-198.0544', '-198.7710', '-198.4128']
Best model has likelihood: -197.8302  (prior= 0.4879 )
time for generating output: 0.5254
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.21554637626736764
Training of 5 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2023cef6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91417100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdfd0ffd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2034a7c9d0>
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 1135.8225 - loglik: -1.1342e+03 - logprior: -1.6058e+00
Epoch 2/10
49/49 - 41s - loss: 968.6852 - loglik: -9.6791e+02 - logprior: -7.8014e-01
Epoch 3/10
49/49 - 41s - loss: 965.0320 - loglik: -9.6446e+02 - logprior: -5.7083e-01
Epoch 4/10
49/49 - 41s - loss: 958.6888 - loglik: -9.5811e+02 - logprior: -5.8099e-01
Epoch 5/10
49/49 - 41s - loss: 958.4458 - loglik: -9.5797e+02 - logprior: -4.7280e-01
Epoch 6/10
49/49 - 41s - loss: 958.0588 - loglik: -9.5732e+02 - logprior: -7.4025e-01
Epoch 7/10
49/49 - 41s - loss: 957.2834 - loglik: -9.5682e+02 - logprior: -4.6040e-01
Epoch 8/10
49/49 - 41s - loss: 957.4371 - loglik: -9.5698e+02 - logprior: -4.5458e-01
Fitted a model with MAP estimate = -955.2540
expansions: [(0, 5), (81, 1), (82, 5), (85, 3), (100, 2), (122, 1), (123, 1), (127, 1), (130, 1), (152, 2), (153, 1), (166, 1), (169, 1), (178, 1), (179, 1), (198, 1), (199, 1), (213, 1), (215, 4), (216, 3), (217, 2), (218, 2), (220, 1), (224, 2), (226, 1), (227, 1), (228, 1), (232, 1), (233, 1), (234, 1), (235, 2), (236, 2), (237, 5), (238, 1), (239, 1), (256, 1), (258, 2), (259, 2), (260, 2), (278, 2), (279, 5), (280, 2), (286, 2), (288, 3), (302, 1), (303, 2), (304, 1), (325, 1), (327, 1), (329, 1), (346, 4), (347, 1), (348, 1), (360, 17), (361, 2), (374, 2), (375, 3), (380, 1), (381, 1), (382, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
 396 397 398 399 400 401 402 403]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 500 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 63s - loss: 946.7733 - loglik: -9.4479e+02 - logprior: -1.9786e+00
Epoch 2/2
49/49 - 58s - loss: 929.5679 - loglik: -9.3169e+02 - logprior: 2.1231
Fitted a model with MAP estimate = -925.2423
expansions: [(0, 7), (346, 1), (347, 1), (441, 1), (443, 3)]
discards: [  1   2   3   4  74  75  76  77  78  79 154 233 306 368 417 419 420 470
 471 497 498 499]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 940.4710 - loglik: -9.3889e+02 - logprior: -1.5766e+00
Epoch 2/2
49/49 - 56s - loss: 924.1055 - loglik: -9.2740e+02 - logprior: 3.2972
Fitted a model with MAP estimate = -925.0611
expansions: [(0, 6), (25, 3), (77, 1), (331, 2), (412, 1), (490, 2), (491, 8)]
discards: [  1   2   3   4   5   6   7 322 323 324 436]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 503 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 62s - loss: 932.2737 - loglik: -9.3063e+02 - logprior: -1.6467e+00
Epoch 2/10
49/49 - 58s - loss: 924.7286 - loglik: -9.2823e+02 - logprior: 3.4979
Epoch 3/10
49/49 - 58s - loss: 919.2328 - loglik: -9.2339e+02 - logprior: 4.1525
Epoch 4/10
49/49 - 59s - loss: 920.6898 - loglik: -9.2532e+02 - logprior: 4.6319
Fitted a model with MAP estimate = -917.9805
Time for alignment: 967.4874
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 45s - loss: 1131.4106 - loglik: -1.1299e+03 - logprior: -1.5565e+00
Epoch 2/10
49/49 - 41s - loss: 971.8872 - loglik: -9.7159e+02 - logprior: -2.9221e-01
Epoch 3/10
49/49 - 41s - loss: 962.1312 - loglik: -9.6139e+02 - logprior: -7.3800e-01
Epoch 4/10
49/49 - 41s - loss: 959.0037 - loglik: -9.5844e+02 - logprior: -5.6149e-01
Epoch 5/10
49/49 - 41s - loss: 959.4789 - loglik: -9.5880e+02 - logprior: -6.7874e-01
Fitted a model with MAP estimate = -958.4623
expansions: [(0, 5), (131, 1), (135, 1), (138, 1), (160, 2), (161, 1), (170, 1), (177, 1), (187, 1), (207, 3), (208, 1), (213, 1), (219, 1), (221, 3), (222, 2), (223, 3), (224, 1), (225, 1), (226, 1), (227, 1), (230, 2), (232, 1), (233, 1), (239, 1), (240, 1), (241, 1), (242, 2), (243, 4), (244, 3), (245, 1), (261, 1), (262, 8), (263, 2), (280, 4), (281, 3), (282, 2), (286, 3), (287, 1), (289, 3), (291, 1), (302, 1), (303, 1), (304, 1), (305, 3), (306, 1), (307, 1), (320, 1), (342, 5), (343, 1), (358, 1), (359, 4), (360, 3), (361, 1), (363, 1), (394, 11)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  35 397 398 399 400 401 402 403]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 944.0659 - loglik: -9.4172e+02 - logprior: -2.3412e+00
Epoch 2/2
49/49 - 56s - loss: 934.2351 - loglik: -9.3663e+02 - logprior: 2.3917
Fitted a model with MAP estimate = -927.2639
expansions: [(0, 6), (298, 1), (330, 2), (487, 1), (488, 5)]
discards: [  1   2   3   4 149 224 321 322 410 435 489 490]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 494 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 938.1362 - loglik: -9.3643e+02 - logprior: -1.7100e+00
Epoch 2/2
49/49 - 57s - loss: 924.4431 - loglik: -9.2725e+02 - logprior: 2.8093
Fitted a model with MAP estimate = -925.1728
expansions: [(0, 6), (342, 1), (491, 1), (492, 1), (494, 7)]
discards: [  1   2   3   4   5   6 322]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 503 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 61s - loss: 932.5599 - loglik: -9.3081e+02 - logprior: -1.7481e+00
Epoch 2/10
49/49 - 58s - loss: 923.8701 - loglik: -9.2725e+02 - logprior: 3.3831
Epoch 3/10
49/49 - 58s - loss: 922.2929 - loglik: -9.2614e+02 - logprior: 3.8512
Epoch 4/10
49/49 - 58s - loss: 916.6902 - loglik: -9.2100e+02 - logprior: 4.3090
Epoch 5/10
49/49 - 58s - loss: 921.5132 - loglik: -9.2627e+02 - logprior: 4.7603
Fitted a model with MAP estimate = -917.2591
Time for alignment: 898.1243
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 1124.1465 - loglik: -1.1226e+03 - logprior: -1.5852e+00
Epoch 2/10
49/49 - 41s - loss: 971.5011 - loglik: -9.7104e+02 - logprior: -4.6321e-01
Epoch 3/10
49/49 - 41s - loss: 959.4198 - loglik: -9.5889e+02 - logprior: -5.2792e-01
Epoch 4/10
49/49 - 41s - loss: 954.9679 - loglik: -9.5437e+02 - logprior: -5.9560e-01
Epoch 5/10
49/49 - 41s - loss: 964.9800 - loglik: -9.6430e+02 - logprior: -6.7869e-01
Fitted a model with MAP estimate = -956.4311
expansions: [(0, 5), (36, 3), (132, 1), (140, 1), (162, 2), (173, 1), (183, 1), (190, 1), (210, 3), (211, 1), (217, 1), (223, 1), (225, 3), (226, 2), (227, 2), (228, 1), (229, 2), (230, 3), (234, 2), (235, 3), (244, 1), (246, 2), (247, 9), (261, 1), (262, 3), (263, 2), (264, 1), (265, 2), (282, 4), (283, 2), (284, 2), (287, 6), (289, 3), (290, 3), (300, 1), (301, 1), (302, 1), (304, 2), (305, 1), (306, 1), (311, 1), (313, 1), (315, 1), (316, 1), (317, 1), (318, 1), (321, 1), (359, 2), (360, 3), (361, 1), (363, 1), (365, 1), (366, 1), (394, 11)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18  19
  84 397 398 399 400 401 402 403]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 492 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 947.3112 - loglik: -9.4499e+02 - logprior: -2.3178e+00
Epoch 2/2
49/49 - 56s - loss: 928.8985 - loglik: -9.3087e+02 - logprior: 1.9679
Fitted a model with MAP estimate = -927.3230
expansions: [(0, 7), (235, 1), (253, 1), (274, 2), (342, 1), (488, 1), (489, 5)]
discards: [  1   2   3   4   5   6  23  24  25 153 207 229 239 242 435 436 490 491]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 492 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 937.1919 - loglik: -9.3545e+02 - logprior: -1.7443e+00
Epoch 2/2
49/49 - 56s - loss: 927.1094 - loglik: -9.3013e+02 - logprior: 3.0241
Fitted a model with MAP estimate = -925.1818
expansions: [(0, 6), (24, 3), (327, 1), (488, 1), (489, 1), (492, 7)]
discards: [  1   2   3   4   5   6   7 270]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 503 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 61s - loss: 931.3711 - loglik: -9.2968e+02 - logprior: -1.6890e+00
Epoch 2/10
49/49 - 58s - loss: 925.2905 - loglik: -9.2882e+02 - logprior: 3.5326
Epoch 3/10
49/49 - 58s - loss: 921.2124 - loglik: -9.2511e+02 - logprior: 3.8946
Epoch 4/10
49/49 - 58s - loss: 918.5012 - loglik: -9.2271e+02 - logprior: 4.2098
Epoch 5/10
49/49 - 58s - loss: 917.2036 - loglik: -9.2191e+02 - logprior: 4.7045
Epoch 6/10
49/49 - 58s - loss: 920.6015 - loglik: -9.2567e+02 - logprior: 5.0638
Fitted a model with MAP estimate = -916.4254
Time for alignment: 955.1606
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 1122.5800 - loglik: -1.1211e+03 - logprior: -1.4895e+00
Epoch 2/10
49/49 - 41s - loss: 971.1130 - loglik: -9.7119e+02 - logprior: 0.0739
Epoch 3/10
49/49 - 41s - loss: 961.3524 - loglik: -9.6128e+02 - logprior: -6.8601e-02
Epoch 4/10
49/49 - 41s - loss: 960.0347 - loglik: -9.5998e+02 - logprior: -5.6704e-02
Epoch 5/10
49/49 - 41s - loss: 959.5625 - loglik: -9.5963e+02 - logprior: 0.0693
Epoch 6/10
49/49 - 41s - loss: 957.8397 - loglik: -9.5784e+02 - logprior: 0.0015
Epoch 7/10
49/49 - 41s - loss: 961.0128 - loglik: -9.6093e+02 - logprior: -8.3195e-02
Fitted a model with MAP estimate = -957.1199
expansions: [(0, 5), (35, 3), (134, 1), (137, 2), (159, 2), (177, 1), (188, 1), (208, 3), (209, 1), (211, 1), (220, 1), (223, 2), (224, 2), (225, 3), (226, 2), (227, 1), (228, 1), (232, 2), (234, 1), (235, 1), (241, 1), (242, 1), (243, 1), (244, 2), (245, 10), (259, 1), (260, 7), (261, 2), (277, 4), (283, 1), (287, 4), (289, 3), (290, 2), (301, 1), (302, 1), (304, 4), (305, 1), (306, 1), (318, 1), (319, 1), (320, 1), (339, 1), (340, 1), (342, 1), (360, 2), (361, 3), (371, 1), (398, 9), (404, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 496 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 946.6284 - loglik: -9.4404e+02 - logprior: -2.5910e+00
Epoch 2/2
49/49 - 56s - loss: 928.1490 - loglik: -9.2974e+02 - logprior: 1.5888
Fitted a model with MAP estimate = -925.7488
expansions: [(0, 7), (301, 1), (484, 1), (485, 4)]
discards: [  1   2   3   4  22  23  24 152 228 337 433 490 491 492 493 494 495]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 492 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 935.2315 - loglik: -9.3336e+02 - logprior: -1.8730e+00
Epoch 2/2
49/49 - 56s - loss: 927.7020 - loglik: -9.3034e+02 - logprior: 2.6374
Fitted a model with MAP estimate = -923.4387
expansions: [(0, 6), (324, 2), (327, 1), (485, 2), (486, 1)]
discards: [  1   2   3   4   5   6   7 487 488 489 490 491]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 492 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 59s - loss: 935.0232 - loglik: -9.3355e+02 - logprior: -1.4724e+00
Epoch 2/10
49/49 - 56s - loss: 926.3516 - loglik: -9.3028e+02 - logprior: 3.9259
Epoch 3/10
49/49 - 56s - loss: 923.6271 - loglik: -9.2802e+02 - logprior: 4.3968
Epoch 4/10
49/49 - 56s - loss: 923.7104 - loglik: -9.2869e+02 - logprior: 4.9821
Fitted a model with MAP estimate = -921.4408
Time for alignment: 914.4848
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 45s - loss: 1127.7834 - loglik: -1.1263e+03 - logprior: -1.4682e+00
Epoch 2/10
49/49 - 41s - loss: 974.4058 - loglik: -9.7438e+02 - logprior: -2.8543e-02
Epoch 3/10
49/49 - 41s - loss: 964.3654 - loglik: -9.6435e+02 - logprior: -1.2456e-02
Epoch 4/10
49/49 - 41s - loss: 963.0473 - loglik: -9.6294e+02 - logprior: -1.0437e-01
Epoch 5/10
49/49 - 41s - loss: 966.1591 - loglik: -9.6594e+02 - logprior: -2.1508e-01
Fitted a model with MAP estimate = -961.9896
expansions: [(0, 5), (36, 3), (135, 1), (139, 1), (161, 2), (162, 1), (179, 1), (188, 1), (189, 1), (208, 3), (209, 1), (212, 1), (220, 1), (222, 3), (223, 2), (224, 2), (225, 1), (226, 2), (227, 3), (231, 2), (232, 2), (239, 2), (240, 1), (241, 1), (242, 2), (243, 8), (244, 2), (258, 1), (259, 8), (260, 2), (277, 4), (282, 2), (285, 4), (287, 3), (289, 1), (300, 1), (301, 1), (322, 1), (323, 1), (324, 1), (342, 3), (371, 1), (373, 1), (374, 1), (375, 1), (391, 4), (392, 1), (394, 12)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  19 399 400 401 402 403]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 489 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 946.1464 - loglik: -9.4421e+02 - logprior: -1.9340e+00
Epoch 2/2
49/49 - 56s - loss: 929.5964 - loglik: -9.3165e+02 - logprior: 2.0504
Fitted a model with MAP estimate = -927.4645
expansions: [(0, 7), (233, 1), (326, 2), (329, 1), (489, 6)]
discards: [  1   2   3   4  22  23  24 152 227 237 240 340 464 485]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 492 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 937.2033 - loglik: -9.3522e+02 - logprior: -1.9858e+00
Epoch 2/2
49/49 - 56s - loss: 925.1373 - loglik: -9.2759e+02 - logprior: 2.4574
Fitted a model with MAP estimate = -924.3311
expansions: [(0, 6), (25, 3), (331, 1), (410, 1), (483, 4)]
discards: [  1   2   3   4   5   6   7  74  75 321 486 487 488 489 490 491]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 59s - loss: 936.5355 - loglik: -9.3465e+02 - logprior: -1.8839e+00
Epoch 2/10
49/49 - 56s - loss: 924.6351 - loglik: -9.2806e+02 - logprior: 3.4277
Epoch 3/10
49/49 - 56s - loss: 924.6912 - loglik: -9.2859e+02 - logprior: 3.9006
Fitted a model with MAP estimate = -921.7058
Time for alignment: 771.5133
Computed alignments with likelihoods: ['-917.9805', '-917.2591', '-916.4254', '-921.4408', '-921.7058']
Best model has likelihood: -916.4254  (prior= 5.4174 )
time for generating output: 0.4202
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.8127500666844492
Training of 5 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fac23a7c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f92e9a790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff0ec1430>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2034a7c9d0>
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 490.5318 - loglik: -4.8862e+02 - logprior: -1.9150e+00
Epoch 2/10
39/39 - 8s - loss: 421.4088 - loglik: -4.1986e+02 - logprior: -1.5535e+00
Epoch 3/10
39/39 - 8s - loss: 411.5619 - loglik: -4.0997e+02 - logprior: -1.5881e+00
Epoch 4/10
39/39 - 8s - loss: 408.8668 - loglik: -4.0729e+02 - logprior: -1.5766e+00
Epoch 5/10
39/39 - 8s - loss: 407.7960 - loglik: -4.0623e+02 - logprior: -1.5638e+00
Epoch 6/10
39/39 - 8s - loss: 407.4146 - loglik: -4.0588e+02 - logprior: -1.5317e+00
Epoch 7/10
39/39 - 8s - loss: 407.1276 - loglik: -4.0559e+02 - logprior: -1.5382e+00
Epoch 8/10
39/39 - 8s - loss: 406.7206 - loglik: -4.0519e+02 - logprior: -1.5338e+00
Epoch 9/10
39/39 - 8s - loss: 406.5903 - loglik: -4.0505e+02 - logprior: -1.5408e+00
Epoch 10/10
39/39 - 8s - loss: 406.1827 - loglik: -4.0464e+02 - logprior: -1.5401e+00
Fitted a model with MAP estimate = -405.5219
expansions: [(8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 1), (31, 1), (39, 2), (40, 1), (43, 1), (56, 1), (57, 1), (58, 3), (69, 1), (70, 1), (79, 2), (92, 1), (96, 1), (100, 1), (102, 1), (103, 2), (104, 1), (105, 1), (106, 1), (109, 2), (113, 2), (117, 1), (122, 1), (126, 1), (129, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 176 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 400.4928 - loglik: -3.9769e+02 - logprior: -2.7987e+00
Epoch 2/2
39/39 - 10s - loss: 389.2812 - loglik: -3.8836e+02 - logprior: -9.2577e-01
Fitted a model with MAP estimate = -388.4322
expansions: []
discards: [ 13  99 100 141 146 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 392.5443 - loglik: -3.9062e+02 - logprior: -1.9216e+00
Epoch 2/2
39/39 - 10s - loss: 389.8828 - loglik: -3.8915e+02 - logprior: -7.2947e-01
Fitted a model with MAP estimate = -389.7881
expansions: []
discards: [ 74 126]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 391.5068 - loglik: -3.8991e+02 - logprior: -1.6010e+00
Epoch 2/10
41/41 - 10s - loss: 387.8773 - loglik: -3.8732e+02 - logprior: -5.6073e-01
Epoch 3/10
41/41 - 10s - loss: 386.9565 - loglik: -3.8646e+02 - logprior: -4.9347e-01
Epoch 4/10
41/41 - 10s - loss: 385.9967 - loglik: -3.8555e+02 - logprior: -4.4463e-01
Epoch 5/10
41/41 - 10s - loss: 385.2849 - loglik: -3.8489e+02 - logprior: -3.9555e-01
Epoch 6/10
41/41 - 10s - loss: 383.9337 - loglik: -3.8359e+02 - logprior: -3.3919e-01
Epoch 7/10
41/41 - 10s - loss: 385.1804 - loglik: -3.8489e+02 - logprior: -2.9088e-01
Fitted a model with MAP estimate = -383.8215
Time for alignment: 256.3366
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 489.8195 - loglik: -4.8791e+02 - logprior: -1.9133e+00
Epoch 2/10
39/39 - 8s - loss: 416.4709 - loglik: -4.1493e+02 - logprior: -1.5381e+00
Epoch 3/10
39/39 - 8s - loss: 407.7411 - loglik: -4.0617e+02 - logprior: -1.5665e+00
Epoch 4/10
39/39 - 8s - loss: 405.0397 - loglik: -4.0347e+02 - logprior: -1.5690e+00
Epoch 5/10
39/39 - 8s - loss: 404.1175 - loglik: -4.0256e+02 - logprior: -1.5614e+00
Epoch 6/10
39/39 - 8s - loss: 403.4288 - loglik: -4.0184e+02 - logprior: -1.5882e+00
Epoch 7/10
39/39 - 8s - loss: 403.2890 - loglik: -4.0170e+02 - logprior: -1.5891e+00
Epoch 8/10
39/39 - 8s - loss: 402.7261 - loglik: -4.0114e+02 - logprior: -1.5857e+00
Epoch 9/10
39/39 - 8s - loss: 402.4732 - loglik: -4.0089e+02 - logprior: -1.5791e+00
Epoch 10/10
39/39 - 8s - loss: 402.4998 - loglik: -4.0093e+02 - logprior: -1.5747e+00
Fitted a model with MAP estimate = -401.5046
expansions: [(8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (24, 1), (25, 1), (39, 3), (44, 1), (45, 1), (57, 2), (58, 2), (59, 2), (68, 1), (69, 2), (76, 1), (94, 2), (96, 1), (100, 2), (102, 1), (103, 2), (104, 2), (105, 1), (106, 2), (116, 1), (118, 1), (126, 1), (129, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 399.6185 - loglik: -3.9667e+02 - logprior: -2.9476e+00
Epoch 2/2
39/39 - 10s - loss: 387.5962 - loglik: -3.8653e+02 - logprior: -1.0655e+00
Fitted a model with MAP estimate = -385.7632
expansions: []
discards: [ 13  49  72  75  77  90 118 127 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 389.5564 - loglik: -3.8756e+02 - logprior: -2.0013e+00
Epoch 2/2
39/39 - 10s - loss: 386.4406 - loglik: -3.8572e+02 - logprior: -7.2518e-01
Fitted a model with MAP estimate = -386.3725
expansions: [(58, 1)]
discards: [125]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 387.7769 - loglik: -3.8613e+02 - logprior: -1.6503e+00
Epoch 2/10
41/41 - 10s - loss: 384.0838 - loglik: -3.8353e+02 - logprior: -5.5649e-01
Epoch 3/10
41/41 - 10s - loss: 384.0950 - loglik: -3.8360e+02 - logprior: -4.9710e-01
Fitted a model with MAP estimate = -382.0925
Time for alignment: 213.1924
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 490.8529 - loglik: -4.8895e+02 - logprior: -1.9017e+00
Epoch 2/10
39/39 - 8s - loss: 419.7747 - loglik: -4.1828e+02 - logprior: -1.4977e+00
Epoch 3/10
39/39 - 8s - loss: 410.0621 - loglik: -4.0850e+02 - logprior: -1.5587e+00
Epoch 4/10
39/39 - 8s - loss: 407.8297 - loglik: -4.0629e+02 - logprior: -1.5434e+00
Epoch 5/10
39/39 - 8s - loss: 406.0204 - loglik: -4.0451e+02 - logprior: -1.5069e+00
Epoch 6/10
39/39 - 8s - loss: 405.1868 - loglik: -4.0368e+02 - logprior: -1.5056e+00
Epoch 7/10
39/39 - 8s - loss: 404.7598 - loglik: -4.0325e+02 - logprior: -1.5063e+00
Epoch 8/10
39/39 - 8s - loss: 404.5115 - loglik: -4.0296e+02 - logprior: -1.5555e+00
Epoch 9/10
39/39 - 8s - loss: 404.1806 - loglik: -4.0262e+02 - logprior: -1.5594e+00
Epoch 10/10
39/39 - 8s - loss: 403.5578 - loglik: -4.0199e+02 - logprior: -1.5631e+00
Fitted a model with MAP estimate = -403.0350
expansions: [(7, 2), (8, 2), (9, 1), (10, 1), (11, 1), (22, 1), (23, 1), (24, 1), (34, 1), (39, 3), (40, 1), (43, 1), (55, 1), (57, 1), (58, 3), (69, 2), (70, 2), (75, 1), (78, 1), (96, 1), (100, 2), (102, 1), (103, 2), (104, 1), (105, 1), (106, 1), (107, 1), (109, 2), (117, 1), (123, 1), (126, 1), (129, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 399.6135 - loglik: -3.9674e+02 - logprior: -2.8752e+00
Epoch 2/2
39/39 - 10s - loss: 387.6247 - loglik: -3.8663e+02 - logprior: -9.9372e-01
Fitted a model with MAP estimate = -386.3941
expansions: [(61, 1)]
discards: [  7  50  77  90  92 127 133 145]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 389.3159 - loglik: -3.8736e+02 - logprior: -1.9559e+00
Epoch 2/2
39/39 - 10s - loss: 385.4415 - loglik: -3.8475e+02 - logprior: -6.9309e-01
Fitted a model with MAP estimate = -385.5776
expansions: []
discards: [8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 386.9537 - loglik: -3.8529e+02 - logprior: -1.6649e+00
Epoch 2/10
41/41 - 10s - loss: 382.9752 - loglik: -3.8242e+02 - logprior: -5.5865e-01
Epoch 3/10
41/41 - 10s - loss: 382.8476 - loglik: -3.8234e+02 - logprior: -5.0579e-01
Epoch 4/10
41/41 - 10s - loss: 381.0210 - loglik: -3.8059e+02 - logprior: -4.3286e-01
Epoch 5/10
41/41 - 10s - loss: 380.7835 - loglik: -3.8040e+02 - logprior: -3.8610e-01
Epoch 6/10
41/41 - 10s - loss: 379.4807 - loglik: -3.7916e+02 - logprior: -3.2445e-01
Epoch 7/10
41/41 - 10s - loss: 379.6081 - loglik: -3.7934e+02 - logprior: -2.6940e-01
Fitted a model with MAP estimate = -379.4566
Time for alignment: 255.9752
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 491.5096 - loglik: -4.8961e+02 - logprior: -1.8956e+00
Epoch 2/10
39/39 - 8s - loss: 421.9626 - loglik: -4.2048e+02 - logprior: -1.4862e+00
Epoch 3/10
39/39 - 8s - loss: 411.3577 - loglik: -4.0982e+02 - logprior: -1.5423e+00
Epoch 4/10
39/39 - 8s - loss: 408.8989 - loglik: -4.0734e+02 - logprior: -1.5593e+00
Epoch 5/10
39/39 - 8s - loss: 407.7637 - loglik: -4.0620e+02 - logprior: -1.5649e+00
Epoch 6/10
39/39 - 8s - loss: 407.1836 - loglik: -4.0563e+02 - logprior: -1.5547e+00
Epoch 7/10
39/39 - 8s - loss: 407.0703 - loglik: -4.0551e+02 - logprior: -1.5615e+00
Epoch 8/10
39/39 - 8s - loss: 406.7975 - loglik: -4.0524e+02 - logprior: -1.5553e+00
Epoch 9/10
39/39 - 8s - loss: 406.2752 - loglik: -4.0472e+02 - logprior: -1.5519e+00
Epoch 10/10
39/39 - 8s - loss: 406.1717 - loglik: -4.0462e+02 - logprior: -1.5538e+00
Fitted a model with MAP estimate = -405.0006
expansions: [(8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 1), (32, 1), (38, 1), (41, 1), (44, 1), (45, 1), (57, 2), (58, 1), (59, 2), (68, 1), (69, 1), (70, 2), (75, 1), (90, 1), (101, 1), (103, 2), (104, 1), (105, 3), (107, 2), (109, 1), (110, 1), (117, 1), (122, 1), (123, 1), (126, 1), (129, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 176 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 399.6963 - loglik: -3.9685e+02 - logprior: -2.8444e+00
Epoch 2/2
39/39 - 10s - loss: 386.1372 - loglik: -3.8515e+02 - logprior: -9.8910e-01
Fitted a model with MAP estimate = -384.5634
expansions: []
discards: [ 13  71  75  90 103 128 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 388.6767 - loglik: -3.8673e+02 - logprior: -1.9457e+00
Epoch 2/2
39/39 - 10s - loss: 384.5989 - loglik: -3.8390e+02 - logprior: -7.0153e-01
Fitted a model with MAP estimate = -384.3520
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 386.4885 - loglik: -3.8488e+02 - logprior: -1.6060e+00
Epoch 2/10
41/41 - 10s - loss: 382.8054 - loglik: -3.8232e+02 - logprior: -4.8633e-01
Epoch 3/10
41/41 - 10s - loss: 382.4088 - loglik: -3.8197e+02 - logprior: -4.4312e-01
Epoch 4/10
41/41 - 10s - loss: 381.2205 - loglik: -3.8083e+02 - logprior: -3.8825e-01
Epoch 5/10
41/41 - 10s - loss: 380.3938 - loglik: -3.8006e+02 - logprior: -3.3865e-01
Epoch 6/10
41/41 - 10s - loss: 379.3935 - loglik: -3.7910e+02 - logprior: -2.9380e-01
Epoch 7/10
41/41 - 10s - loss: 379.1401 - loglik: -3.7890e+02 - logprior: -2.4376e-01
Epoch 8/10
41/41 - 10s - loss: 380.1692 - loglik: -3.7997e+02 - logprior: -1.9736e-01
Fitted a model with MAP estimate = -379.3122
Time for alignment: 265.7196
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 491.8530 - loglik: -4.8992e+02 - logprior: -1.9368e+00
Epoch 2/10
39/39 - 8s - loss: 418.2988 - loglik: -4.1667e+02 - logprior: -1.6261e+00
Epoch 3/10
39/39 - 8s - loss: 407.2424 - loglik: -4.0563e+02 - logprior: -1.6096e+00
Epoch 4/10
39/39 - 8s - loss: 404.8593 - loglik: -4.0324e+02 - logprior: -1.6206e+00
Epoch 5/10
39/39 - 8s - loss: 403.5033 - loglik: -4.0188e+02 - logprior: -1.6261e+00
Epoch 6/10
39/39 - 8s - loss: 403.5423 - loglik: -4.0193e+02 - logprior: -1.6166e+00
Fitted a model with MAP estimate = -401.5536
expansions: [(8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 1), (31, 1), (39, 3), (44, 1), (45, 1), (55, 1), (57, 1), (58, 2), (59, 2), (71, 2), (89, 2), (92, 1), (96, 1), (100, 2), (102, 1), (103, 2), (104, 2), (105, 1), (106, 2), (109, 2), (117, 1), (122, 1), (123, 1), (126, 1), (129, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 397.6037 - loglik: -3.9473e+02 - logprior: -2.8720e+00
Epoch 2/2
39/39 - 11s - loss: 385.0005 - loglik: -3.8398e+02 - logprior: -1.0220e+00
Fitted a model with MAP estimate = -383.8816
expansions: []
discards: [ 13  49  74  77  91 111 126 132 137 145]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 386.8301 - loglik: -3.8490e+02 - logprior: -1.9309e+00
Epoch 2/2
39/39 - 10s - loss: 383.5959 - loglik: -3.8290e+02 - logprior: -6.9962e-01
Fitted a model with MAP estimate = -384.0680
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 385.2624 - loglik: -3.8366e+02 - logprior: -1.6038e+00
Epoch 2/10
41/41 - 10s - loss: 381.5832 - loglik: -3.8108e+02 - logprior: -5.0766e-01
Epoch 3/10
41/41 - 10s - loss: 382.0976 - loglik: -3.8166e+02 - logprior: -4.4026e-01
Fitted a model with MAP estimate = -380.7990
Time for alignment: 179.5333
Computed alignments with likelihoods: ['-383.8215', '-382.0925', '-379.4566', '-379.3122', '-380.7990']
Best model has likelihood: -379.3122  (prior= -0.1959 )
time for generating output: 0.2605
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.42547092547092547
Training of 5 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdfbd4e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f9272fee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf219b20>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f201adc81f0>
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 174.3583 - loglik: -1.1550e+02 - logprior: -5.8861e+01
Epoch 2/10
10/10 - 0s - loss: 111.1503 - loglik: -9.4690e+01 - logprior: -1.6460e+01
Epoch 3/10
10/10 - 0s - loss: 88.1744 - loglik: -8.0126e+01 - logprior: -8.0487e+00
Epoch 4/10
10/10 - 0s - loss: 75.5383 - loglik: -7.0428e+01 - logprior: -5.1102e+00
Epoch 5/10
10/10 - 0s - loss: 67.0870 - loglik: -6.3469e+01 - logprior: -3.6179e+00
Epoch 6/10
10/10 - 0s - loss: 63.4495 - loglik: -6.0583e+01 - logprior: -2.8669e+00
Epoch 7/10
10/10 - 0s - loss: 62.2469 - loglik: -5.9828e+01 - logprior: -2.4192e+00
Epoch 8/10
10/10 - 0s - loss: 61.6884 - loglik: -5.9615e+01 - logprior: -2.0731e+00
Epoch 9/10
10/10 - 0s - loss: 61.4077 - loglik: -5.9633e+01 - logprior: -1.7746e+00
Epoch 10/10
10/10 - 0s - loss: 61.0438 - loglik: -5.9471e+01 - logprior: -1.5730e+00
Fitted a model with MAP estimate = -60.9308
expansions: [(0, 4), (10, 2), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 138.2137 - loglik: -5.9231e+01 - logprior: -7.8983e+01
Epoch 2/2
10/10 - 0s - loss: 80.1721 - loglik: -5.4858e+01 - logprior: -2.5315e+01
Fitted a model with MAP estimate = -69.1419
expansions: [(0, 2)]
discards: [14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 118.1449 - loglik: -5.2364e+01 - logprior: -6.5781e+01
Epoch 2/2
10/10 - 0s - loss: 74.2921 - loglik: -5.1189e+01 - logprior: -2.3103e+01
Fitted a model with MAP estimate = -65.3187
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 38 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 110.7160 - loglik: -5.1605e+01 - logprior: -5.9111e+01
Epoch 2/10
10/10 - 0s - loss: 69.0331 - loglik: -5.2305e+01 - logprior: -1.6728e+01
Epoch 3/10
10/10 - 0s - loss: 60.6224 - loglik: -5.2908e+01 - logprior: -7.7148e+00
Epoch 4/10
10/10 - 0s - loss: 57.7030 - loglik: -5.3407e+01 - logprior: -4.2958e+00
Epoch 5/10
10/10 - 0s - loss: 56.2467 - loglik: -5.3637e+01 - logprior: -2.6092e+00
Epoch 6/10
10/10 - 0s - loss: 55.0410 - loglik: -5.3231e+01 - logprior: -1.8096e+00
Epoch 7/10
10/10 - 0s - loss: 54.2226 - loglik: -5.2877e+01 - logprior: -1.3458e+00
Epoch 8/10
10/10 - 0s - loss: 53.9230 - loglik: -5.2928e+01 - logprior: -9.9506e-01
Epoch 9/10
10/10 - 0s - loss: 53.5110 - loglik: -5.2837e+01 - logprior: -6.7421e-01
Epoch 10/10
10/10 - 0s - loss: 53.1587 - loglik: -5.2711e+01 - logprior: -4.4733e-01
Fitted a model with MAP estimate = -53.1410
Time for alignment: 26.5369
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 174.2866 - loglik: -1.1543e+02 - logprior: -5.8861e+01
Epoch 2/10
10/10 - 0s - loss: 111.2683 - loglik: -9.4807e+01 - logprior: -1.6461e+01
Epoch 3/10
10/10 - 0s - loss: 88.1760 - loglik: -8.0128e+01 - logprior: -8.0476e+00
Epoch 4/10
10/10 - 0s - loss: 75.5018 - loglik: -7.0393e+01 - logprior: -5.1084e+00
Epoch 5/10
10/10 - 0s - loss: 67.2008 - loglik: -6.3582e+01 - logprior: -3.6188e+00
Epoch 6/10
10/10 - 0s - loss: 63.3331 - loglik: -6.0464e+01 - logprior: -2.8694e+00
Epoch 7/10
10/10 - 0s - loss: 62.3534 - loglik: -5.9932e+01 - logprior: -2.4212e+00
Epoch 8/10
10/10 - 0s - loss: 61.6367 - loglik: -5.9564e+01 - logprior: -2.0727e+00
Epoch 9/10
10/10 - 0s - loss: 61.2693 - loglik: -5.9495e+01 - logprior: -1.7743e+00
Epoch 10/10
10/10 - 0s - loss: 61.1707 - loglik: -5.9598e+01 - logprior: -1.5730e+00
Fitted a model with MAP estimate = -60.9236
expansions: [(0, 4), (10, 2), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 138.1299 - loglik: -5.9148e+01 - logprior: -7.8982e+01
Epoch 2/2
10/10 - 0s - loss: 80.2409 - loglik: -5.4924e+01 - logprior: -2.5317e+01
Fitted a model with MAP estimate = -69.1363
expansions: [(0, 2)]
discards: [14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 118.0141 - loglik: -5.2232e+01 - logprior: -6.5782e+01
Epoch 2/2
10/10 - 0s - loss: 74.4119 - loglik: -5.1302e+01 - logprior: -2.3110e+01
Fitted a model with MAP estimate = -65.2948
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 38 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 110.8528 - loglik: -5.1736e+01 - logprior: -5.9117e+01
Epoch 2/10
10/10 - 0s - loss: 68.7401 - loglik: -5.2007e+01 - logprior: -1.6733e+01
Epoch 3/10
10/10 - 0s - loss: 60.7047 - loglik: -5.2991e+01 - logprior: -7.7136e+00
Epoch 4/10
10/10 - 0s - loss: 57.7943 - loglik: -5.3497e+01 - logprior: -4.2970e+00
Epoch 5/10
10/10 - 0s - loss: 56.2311 - loglik: -5.3621e+01 - logprior: -2.6100e+00
Epoch 6/10
10/10 - 0s - loss: 55.0792 - loglik: -5.3268e+01 - logprior: -1.8108e+00
Epoch 7/10
10/10 - 0s - loss: 54.1502 - loglik: -5.2804e+01 - logprior: -1.3459e+00
Epoch 8/10
10/10 - 0s - loss: 53.7283 - loglik: -5.2733e+01 - logprior: -9.9529e-01
Epoch 9/10
10/10 - 0s - loss: 53.5083 - loglik: -5.2832e+01 - logprior: -6.7671e-01
Epoch 10/10
10/10 - 0s - loss: 53.3160 - loglik: -5.2868e+01 - logprior: -4.4810e-01
Fitted a model with MAP estimate = -53.1345
Time for alignment: 24.4717
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 174.3849 - loglik: -1.1552e+02 - logprior: -5.8861e+01
Epoch 2/10
10/10 - 0s - loss: 111.1909 - loglik: -9.4731e+01 - logprior: -1.6460e+01
Epoch 3/10
10/10 - 0s - loss: 88.1465 - loglik: -8.0100e+01 - logprior: -8.0464e+00
Epoch 4/10
10/10 - 0s - loss: 75.4794 - loglik: -7.0370e+01 - logprior: -5.1092e+00
Epoch 5/10
10/10 - 0s - loss: 67.0737 - loglik: -6.3451e+01 - logprior: -3.6223e+00
Epoch 6/10
10/10 - 0s - loss: 63.3385 - loglik: -6.0469e+01 - logprior: -2.8698e+00
Epoch 7/10
10/10 - 0s - loss: 62.2946 - loglik: -5.9874e+01 - logprior: -2.4211e+00
Epoch 8/10
10/10 - 0s - loss: 61.6510 - loglik: -5.9581e+01 - logprior: -2.0703e+00
Epoch 9/10
10/10 - 0s - loss: 61.3408 - loglik: -5.9566e+01 - logprior: -1.7752e+00
Epoch 10/10
10/10 - 0s - loss: 61.1261 - loglik: -5.9554e+01 - logprior: -1.5724e+00
Fitted a model with MAP estimate = -60.9219
expansions: [(0, 4), (10, 2), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 138.1585 - loglik: -5.9174e+01 - logprior: -7.8985e+01
Epoch 2/2
10/10 - 0s - loss: 80.2395 - loglik: -5.4920e+01 - logprior: -2.5320e+01
Fitted a model with MAP estimate = -69.1357
expansions: [(0, 2)]
discards: [14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 118.0083 - loglik: -5.2228e+01 - logprior: -6.5780e+01
Epoch 2/2
10/10 - 0s - loss: 74.4323 - loglik: -5.1328e+01 - logprior: -2.3105e+01
Fitted a model with MAP estimate = -65.3031
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 38 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 110.7718 - loglik: -5.1656e+01 - logprior: -5.9116e+01
Epoch 2/10
10/10 - 0s - loss: 68.8796 - loglik: -5.2150e+01 - logprior: -1.6729e+01
Epoch 3/10
10/10 - 0s - loss: 60.7016 - loglik: -5.2988e+01 - logprior: -7.7137e+00
Epoch 4/10
10/10 - 0s - loss: 57.6797 - loglik: -5.3382e+01 - logprior: -4.2975e+00
Epoch 5/10
10/10 - 0s - loss: 56.2735 - loglik: -5.3662e+01 - logprior: -2.6117e+00
Epoch 6/10
10/10 - 0s - loss: 54.9669 - loglik: -5.3158e+01 - logprior: -1.8085e+00
Epoch 7/10
10/10 - 0s - loss: 54.3338 - loglik: -5.2993e+01 - logprior: -1.3405e+00
Epoch 8/10
10/10 - 0s - loss: 53.6460 - loglik: -5.2656e+01 - logprior: -9.9043e-01
Epoch 9/10
10/10 - 0s - loss: 53.5565 - loglik: -5.2883e+01 - logprior: -6.7322e-01
Epoch 10/10
10/10 - 0s - loss: 53.3426 - loglik: -5.2895e+01 - logprior: -4.4778e-01
Fitted a model with MAP estimate = -53.1327
Time for alignment: 25.3136
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 174.3860 - loglik: -1.1552e+02 - logprior: -5.8862e+01
Epoch 2/10
10/10 - 0s - loss: 111.1792 - loglik: -9.4719e+01 - logprior: -1.6460e+01
Epoch 3/10
10/10 - 0s - loss: 88.3176 - loglik: -8.0270e+01 - logprior: -8.0476e+00
Epoch 4/10
10/10 - 0s - loss: 75.6098 - loglik: -7.0501e+01 - logprior: -5.1090e+00
Epoch 5/10
10/10 - 0s - loss: 66.9627 - loglik: -6.3344e+01 - logprior: -3.6191e+00
Epoch 6/10
10/10 - 0s - loss: 63.4370 - loglik: -6.0566e+01 - logprior: -2.8713e+00
Epoch 7/10
10/10 - 0s - loss: 62.2453 - loglik: -5.9823e+01 - logprior: -2.4218e+00
Epoch 8/10
10/10 - 0s - loss: 61.6284 - loglik: -5.9555e+01 - logprior: -2.0737e+00
Epoch 9/10
10/10 - 0s - loss: 61.4032 - loglik: -5.9627e+01 - logprior: -1.7758e+00
Epoch 10/10
10/10 - 0s - loss: 60.9931 - loglik: -5.9420e+01 - logprior: -1.5735e+00
Fitted a model with MAP estimate = -60.9186
expansions: [(0, 4), (10, 2), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 138.3054 - loglik: -5.9319e+01 - logprior: -7.8986e+01
Epoch 2/2
10/10 - 0s - loss: 80.1636 - loglik: -5.4846e+01 - logprior: -2.5318e+01
Fitted a model with MAP estimate = -69.1416
expansions: [(0, 2)]
discards: [14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 118.1053 - loglik: -5.2326e+01 - logprior: -6.5780e+01
Epoch 2/2
10/10 - 0s - loss: 74.2639 - loglik: -5.1161e+01 - logprior: -2.3103e+01
Fitted a model with MAP estimate = -65.3099
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 38 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 110.7856 - loglik: -5.1673e+01 - logprior: -5.9112e+01
Epoch 2/10
10/10 - 0s - loss: 68.8961 - loglik: -5.2164e+01 - logprior: -1.6732e+01
Epoch 3/10
10/10 - 0s - loss: 60.6433 - loglik: -5.2930e+01 - logprior: -7.7129e+00
Epoch 4/10
10/10 - 0s - loss: 57.7901 - loglik: -5.3493e+01 - logprior: -4.2970e+00
Epoch 5/10
10/10 - 0s - loss: 56.2063 - loglik: -5.3594e+01 - logprior: -2.6126e+00
Epoch 6/10
10/10 - 0s - loss: 55.0369 - loglik: -5.3225e+01 - logprior: -1.8115e+00
Epoch 7/10
10/10 - 0s - loss: 54.2468 - loglik: -5.2902e+01 - logprior: -1.3447e+00
Epoch 8/10
10/10 - 0s - loss: 53.7789 - loglik: -5.2785e+01 - logprior: -9.9401e-01
Epoch 9/10
10/10 - 0s - loss: 53.4642 - loglik: -5.2786e+01 - logprior: -6.7785e-01
Epoch 10/10
10/10 - 0s - loss: 53.2683 - loglik: -5.2826e+01 - logprior: -4.4231e-01
Fitted a model with MAP estimate = -53.1354
Time for alignment: 24.7806
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 174.3722 - loglik: -1.1551e+02 - logprior: -5.8861e+01
Epoch 2/10
10/10 - 0s - loss: 111.1450 - loglik: -9.4684e+01 - logprior: -1.6461e+01
Epoch 3/10
10/10 - 0s - loss: 88.1801 - loglik: -8.0131e+01 - logprior: -8.0488e+00
Epoch 4/10
10/10 - 0s - loss: 75.6188 - loglik: -7.0508e+01 - logprior: -5.1107e+00
Epoch 5/10
10/10 - 0s - loss: 66.9877 - loglik: -6.3366e+01 - logprior: -3.6213e+00
Epoch 6/10
10/10 - 0s - loss: 63.3730 - loglik: -6.0504e+01 - logprior: -2.8692e+00
Epoch 7/10
10/10 - 0s - loss: 62.1029 - loglik: -5.9681e+01 - logprior: -2.4221e+00
Epoch 8/10
10/10 - 0s - loss: 61.8521 - loglik: -5.9780e+01 - logprior: -2.0723e+00
Epoch 9/10
10/10 - 0s - loss: 61.3391 - loglik: -5.9564e+01 - logprior: -1.7751e+00
Epoch 10/10
10/10 - 0s - loss: 60.9757 - loglik: -5.9402e+01 - logprior: -1.5733e+00
Fitted a model with MAP estimate = -60.9269
expansions: [(0, 4), (10, 2), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 138.0928 - loglik: -5.9103e+01 - logprior: -7.8990e+01
Epoch 2/2
10/10 - 0s - loss: 80.3206 - loglik: -5.4999e+01 - logprior: -2.5321e+01
Fitted a model with MAP estimate = -69.1321
expansions: [(0, 2)]
discards: [14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.9938 - loglik: -5.2209e+01 - logprior: -6.5784e+01
Epoch 2/2
10/10 - 0s - loss: 74.3092 - loglik: -5.1196e+01 - logprior: -2.3113e+01
Fitted a model with MAP estimate = -65.2657
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 38 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 110.7028 - loglik: -5.1588e+01 - logprior: -5.9114e+01
Epoch 2/10
10/10 - 0s - loss: 69.0052 - loglik: -5.2273e+01 - logprior: -1.6732e+01
Epoch 3/10
10/10 - 0s - loss: 60.5289 - loglik: -5.2820e+01 - logprior: -7.7085e+00
Epoch 4/10
10/10 - 0s - loss: 57.8325 - loglik: -5.3534e+01 - logprior: -4.2985e+00
Epoch 5/10
10/10 - 0s - loss: 56.2198 - loglik: -5.3609e+01 - logprior: -2.6112e+00
Epoch 6/10
10/10 - 0s - loss: 54.9861 - loglik: -5.3176e+01 - logprior: -1.8097e+00
Epoch 7/10
10/10 - 0s - loss: 54.1791 - loglik: -5.2838e+01 - logprior: -1.3411e+00
Epoch 8/10
10/10 - 0s - loss: 53.9074 - loglik: -5.2921e+01 - logprior: -9.8674e-01
Epoch 9/10
10/10 - 0s - loss: 53.4508 - loglik: -5.2783e+01 - logprior: -6.6811e-01
Epoch 10/10
10/10 - 0s - loss: 53.3601 - loglik: -5.2919e+01 - logprior: -4.4090e-01
Fitted a model with MAP estimate = -53.1319
Time for alignment: 25.3645
Computed alignments with likelihoods: ['-53.1410', '-53.1345', '-53.1327', '-53.1354', '-53.1319']
Best model has likelihood: -53.1319  (prior= -0.3615 )
time for generating output: 0.0800
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.961352657004831
Training of 5 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe88b39a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201b8b7d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20239853d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1ecb47e940>
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 375.5649 - loglik: -3.7278e+02 - logprior: -2.7839e+00
Epoch 2/10
20/20 - 3s - loss: 334.6404 - loglik: -3.3345e+02 - logprior: -1.1904e+00
Epoch 3/10
20/20 - 3s - loss: 313.2159 - loglik: -3.1173e+02 - logprior: -1.4895e+00
Epoch 4/10
20/20 - 3s - loss: 307.7542 - loglik: -3.0633e+02 - logprior: -1.4198e+00
Epoch 5/10
20/20 - 3s - loss: 305.6170 - loglik: -3.0416e+02 - logprior: -1.4524e+00
Epoch 6/10
20/20 - 4s - loss: 305.1943 - loglik: -3.0378e+02 - logprior: -1.4142e+00
Epoch 7/10
20/20 - 3s - loss: 304.5583 - loglik: -3.0317e+02 - logprior: -1.3871e+00
Epoch 8/10
20/20 - 3s - loss: 304.8516 - loglik: -3.0348e+02 - logprior: -1.3704e+00
Fitted a model with MAP estimate = -288.4673
expansions: [(5, 1), (8, 1), (9, 2), (10, 2), (12, 2), (20, 1), (23, 2), (37, 2), (39, 1), (40, 1), (47, 1), (51, 1), (56, 2), (57, 2), (58, 1), (59, 2), (60, 2), (63, 2), (75, 1), (76, 2), (81, 2), (83, 1), (85, 1), (93, 1), (94, 2), (97, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 303.4946 - loglik: -3.0084e+02 - logprior: -2.6594e+00
Epoch 2/2
40/40 - 5s - loss: 293.0827 - loglik: -2.9219e+02 - logprior: -8.9144e-01
Fitted a model with MAP estimate = -278.5785
expansions: []
discards: [ 11  13  31  47  75  80  83  89 105 111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 294.9329 - loglik: -2.9312e+02 - logprior: -1.8127e+00
Epoch 2/2
40/40 - 5s - loss: 292.5679 - loglik: -2.9193e+02 - logprior: -6.4140e-01
Fitted a model with MAP estimate = -278.7058
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 275.4350 - loglik: -2.7437e+02 - logprior: -1.0605e+00
Epoch 2/10
57/57 - 7s - loss: 273.6718 - loglik: -2.7296e+02 - logprior: -7.1662e-01
Epoch 3/10
57/57 - 7s - loss: 273.1198 - loglik: -2.7243e+02 - logprior: -6.8536e-01
Epoch 4/10
57/57 - 7s - loss: 272.2591 - loglik: -2.7159e+02 - logprior: -6.6533e-01
Epoch 5/10
57/57 - 6s - loss: 271.7555 - loglik: -2.7112e+02 - logprior: -6.3758e-01
Epoch 6/10
57/57 - 6s - loss: 271.4919 - loglik: -2.7088e+02 - logprior: -6.1276e-01
Epoch 7/10
57/57 - 7s - loss: 271.2998 - loglik: -2.7071e+02 - logprior: -5.8524e-01
Epoch 8/10
57/57 - 7s - loss: 271.8284 - loglik: -2.7127e+02 - logprior: -5.6178e-01
Fitted a model with MAP estimate = -271.2694
Time for alignment: 147.8382
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 375.6278 - loglik: -3.7284e+02 - logprior: -2.7874e+00
Epoch 2/10
20/20 - 3s - loss: 334.2119 - loglik: -3.3301e+02 - logprior: -1.2027e+00
Epoch 3/10
20/20 - 3s - loss: 314.2296 - loglik: -3.1281e+02 - logprior: -1.4171e+00
Epoch 4/10
20/20 - 3s - loss: 309.0936 - loglik: -3.0778e+02 - logprior: -1.3148e+00
Epoch 5/10
20/20 - 3s - loss: 307.4664 - loglik: -3.0616e+02 - logprior: -1.3107e+00
Epoch 6/10
20/20 - 3s - loss: 306.4810 - loglik: -3.0525e+02 - logprior: -1.2311e+00
Epoch 7/10
20/20 - 3s - loss: 306.1473 - loglik: -3.0495e+02 - logprior: -1.1972e+00
Epoch 8/10
20/20 - 3s - loss: 306.0059 - loglik: -3.0482e+02 - logprior: -1.1812e+00
Epoch 9/10
20/20 - 3s - loss: 305.3469 - loglik: -3.0417e+02 - logprior: -1.1780e+00
Epoch 10/10
20/20 - 3s - loss: 305.6248 - loglik: -3.0445e+02 - logprior: -1.1773e+00
Fitted a model with MAP estimate = -289.7268
expansions: [(5, 1), (8, 1), (9, 2), (10, 2), (12, 2), (23, 2), (26, 2), (37, 2), (39, 1), (40, 1), (42, 1), (46, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (75, 1), (76, 2), (77, 1), (81, 2), (83, 1), (85, 1), (94, 3), (103, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 302.6375 - loglik: -2.9995e+02 - logprior: -2.6857e+00
Epoch 2/2
40/40 - 5s - loss: 291.7905 - loglik: -2.9091e+02 - logprior: -8.8384e-01
Fitted a model with MAP estimate = -278.3513
expansions: [(140, 2)]
discards: [11 13 31 35 48 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 136 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 294.3326 - loglik: -2.9238e+02 - logprior: -1.9574e+00
Epoch 2/2
40/40 - 5s - loss: 291.0016 - loglik: -2.9020e+02 - logprior: -7.9874e-01
Fitted a model with MAP estimate = -278.4389
expansions: []
discards: [133]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 135 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 275.6532 - loglik: -2.7458e+02 - logprior: -1.0731e+00
Epoch 2/10
57/57 - 7s - loss: 273.1249 - loglik: -2.7242e+02 - logprior: -7.0176e-01
Epoch 3/10
57/57 - 6s - loss: 272.2440 - loglik: -2.7155e+02 - logprior: -6.9659e-01
Epoch 4/10
57/57 - 7s - loss: 271.5927 - loglik: -2.7092e+02 - logprior: -6.6858e-01
Epoch 5/10
57/57 - 6s - loss: 271.6572 - loglik: -2.7101e+02 - logprior: -6.4447e-01
Fitted a model with MAP estimate = -271.1721
Time for alignment: 134.6035
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 375.5484 - loglik: -3.7276e+02 - logprior: -2.7867e+00
Epoch 2/10
20/20 - 3s - loss: 334.4366 - loglik: -3.3324e+02 - logprior: -1.1966e+00
Epoch 3/10
20/20 - 3s - loss: 314.2019 - loglik: -3.1277e+02 - logprior: -1.4364e+00
Epoch 4/10
20/20 - 3s - loss: 309.3515 - loglik: -3.0799e+02 - logprior: -1.3585e+00
Epoch 5/10
20/20 - 3s - loss: 307.2247 - loglik: -3.0585e+02 - logprior: -1.3716e+00
Epoch 6/10
20/20 - 3s - loss: 306.1024 - loglik: -3.0474e+02 - logprior: -1.3591e+00
Epoch 7/10
20/20 - 3s - loss: 305.7111 - loglik: -3.0435e+02 - logprior: -1.3649e+00
Epoch 8/10
20/20 - 3s - loss: 305.7836 - loglik: -3.0442e+02 - logprior: -1.3619e+00
Fitted a model with MAP estimate = -289.1982
expansions: [(5, 1), (8, 1), (9, 2), (10, 2), (12, 2), (23, 2), (26, 2), (37, 2), (39, 1), (40, 1), (43, 2), (46, 1), (56, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 1), (76, 1), (77, 1), (78, 2), (81, 2), (83, 1), (85, 1), (93, 1), (94, 2), (97, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 303.9161 - loglik: -3.0127e+02 - logprior: -2.6477e+00
Epoch 2/2
40/40 - 5s - loss: 293.4286 - loglik: -2.9256e+02 - logprior: -8.7017e-01
Fitted a model with MAP estimate = -278.4880
expansions: []
discards: [ 11  13  31  35  58  80 105 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 294.9576 - loglik: -2.9314e+02 - logprior: -1.8201e+00
Epoch 2/2
40/40 - 5s - loss: 292.1310 - loglik: -2.9146e+02 - logprior: -6.7414e-01
Fitted a model with MAP estimate = -278.6750
expansions: []
discards: [44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 275.7496 - loglik: -2.7470e+02 - logprior: -1.0513e+00
Epoch 2/10
57/57 - 6s - loss: 273.3550 - loglik: -2.7266e+02 - logprior: -6.9798e-01
Epoch 3/10
57/57 - 7s - loss: 272.8607 - loglik: -2.7218e+02 - logprior: -6.8080e-01
Epoch 4/10
57/57 - 7s - loss: 271.7682 - loglik: -2.7111e+02 - logprior: -6.5734e-01
Epoch 5/10
57/57 - 7s - loss: 271.8027 - loglik: -2.7117e+02 - logprior: -6.3084e-01
Fitted a model with MAP estimate = -271.5114
Time for alignment: 129.0848
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 375.5945 - loglik: -3.7281e+02 - logprior: -2.7841e+00
Epoch 2/10
20/20 - 3s - loss: 334.2353 - loglik: -3.3305e+02 - logprior: -1.1867e+00
Epoch 3/10
20/20 - 3s - loss: 313.5630 - loglik: -3.1212e+02 - logprior: -1.4390e+00
Epoch 4/10
20/20 - 3s - loss: 308.7708 - loglik: -3.0744e+02 - logprior: -1.3347e+00
Epoch 5/10
20/20 - 3s - loss: 307.1324 - loglik: -3.0582e+02 - logprior: -1.3142e+00
Epoch 6/10
20/20 - 3s - loss: 305.6663 - loglik: -3.0441e+02 - logprior: -1.2594e+00
Epoch 7/10
20/20 - 3s - loss: 305.7184 - loglik: -3.0450e+02 - logprior: -1.2198e+00
Fitted a model with MAP estimate = -289.2634
expansions: [(5, 1), (8, 1), (9, 2), (10, 2), (12, 2), (20, 1), (23, 2), (30, 1), (39, 1), (40, 1), (43, 2), (46, 1), (56, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 1), (76, 1), (77, 2), (81, 2), (83, 1), (85, 1), (94, 3), (103, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 302.8784 - loglik: -3.0020e+02 - logprior: -2.6786e+00
Epoch 2/2
40/40 - 5s - loss: 292.5661 - loglik: -2.9163e+02 - logprior: -9.3382e-01
Fitted a model with MAP estimate = -278.2898
expansions: [(139, 2)]
discards: [ 11  13  31  56  78 102]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 135 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 294.2674 - loglik: -2.9232e+02 - logprior: -1.9478e+00
Epoch 2/2
40/40 - 5s - loss: 291.3835 - loglik: -2.9059e+02 - logprior: -7.9322e-01
Fitted a model with MAP estimate = -279.0624
expansions: []
discards: [133]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 275.2662 - loglik: -2.7421e+02 - logprior: -1.0573e+00
Epoch 2/10
57/57 - 6s - loss: 273.4991 - loglik: -2.7278e+02 - logprior: -7.1627e-01
Epoch 3/10
57/57 - 7s - loss: 272.4004 - loglik: -2.7171e+02 - logprior: -6.9208e-01
Epoch 4/10
57/57 - 6s - loss: 271.7812 - loglik: -2.7111e+02 - logprior: -6.6943e-01
Epoch 5/10
57/57 - 7s - loss: 271.3652 - loglik: -2.7071e+02 - logprior: -6.5608e-01
Epoch 6/10
57/57 - 6s - loss: 270.8116 - loglik: -2.7018e+02 - logprior: -6.3439e-01
Epoch 7/10
57/57 - 7s - loss: 271.4778 - loglik: -2.7087e+02 - logprior: -6.1148e-01
Fitted a model with MAP estimate = -270.7920
Time for alignment: 139.8081
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 375.5103 - loglik: -3.7272e+02 - logprior: -2.7863e+00
Epoch 2/10
20/20 - 3s - loss: 333.7485 - loglik: -3.3254e+02 - logprior: -1.2046e+00
Epoch 3/10
20/20 - 3s - loss: 314.5835 - loglik: -3.1314e+02 - logprior: -1.4452e+00
Epoch 4/10
20/20 - 3s - loss: 307.9185 - loglik: -3.0655e+02 - logprior: -1.3651e+00
Epoch 5/10
20/20 - 3s - loss: 306.7751 - loglik: -3.0543e+02 - logprior: -1.3451e+00
Epoch 6/10
20/20 - 3s - loss: 305.3740 - loglik: -3.0411e+02 - logprior: -1.2665e+00
Epoch 7/10
20/20 - 3s - loss: 304.6134 - loglik: -3.0338e+02 - logprior: -1.2297e+00
Epoch 8/10
20/20 - 3s - loss: 304.1798 - loglik: -3.0297e+02 - logprior: -1.2112e+00
Epoch 9/10
20/20 - 3s - loss: 304.3575 - loglik: -3.0316e+02 - logprior: -1.1932e+00
Fitted a model with MAP estimate = -288.2544
expansions: [(5, 1), (8, 1), (9, 2), (10, 2), (12, 2), (22, 1), (23, 2), (37, 2), (39, 1), (40, 1), (47, 1), (50, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 2), (64, 2), (76, 1), (77, 2), (78, 2), (81, 1), (83, 1), (85, 1), (94, 3), (103, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 302.5851 - loglik: -2.9987e+02 - logprior: -2.7106e+00
Epoch 2/2
40/40 - 5s - loss: 291.6036 - loglik: -2.9069e+02 - logprior: -9.1756e-01
Fitted a model with MAP estimate = -278.3021
expansions: [(142, 2)]
discards: [ 11  13  31  47  75  82  88 104 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 135 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 294.1951 - loglik: -2.9225e+02 - logprior: -1.9466e+00
Epoch 2/2
40/40 - 5s - loss: 291.3249 - loglik: -2.9054e+02 - logprior: -7.8073e-01
Fitted a model with MAP estimate = -278.4925
expansions: []
discards: [133]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 275.2270 - loglik: -2.7416e+02 - logprior: -1.0666e+00
Epoch 2/10
57/57 - 7s - loss: 273.8458 - loglik: -2.7314e+02 - logprior: -7.0906e-01
Epoch 3/10
57/57 - 7s - loss: 272.0730 - loglik: -2.7138e+02 - logprior: -6.9176e-01
Epoch 4/10
57/57 - 6s - loss: 271.9101 - loglik: -2.7123e+02 - logprior: -6.7829e-01
Epoch 5/10
57/57 - 7s - loss: 271.5206 - loglik: -2.7087e+02 - logprior: -6.4857e-01
Epoch 6/10
57/57 - 6s - loss: 270.6068 - loglik: -2.6998e+02 - logprior: -6.2439e-01
Epoch 7/10
57/57 - 6s - loss: 271.0142 - loglik: -2.7042e+02 - logprior: -5.9535e-01
Fitted a model with MAP estimate = -270.8254
Time for alignment: 143.8296
Computed alignments with likelihoods: ['-271.2694', '-271.1721', '-271.5114', '-270.7920', '-270.8254']
Best model has likelihood: -270.7920  (prior= -0.6299 )
time for generating output: 0.2905
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.6386577181208054
Training of 5 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f929e09d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fceadf850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fcea30dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2034cba790>
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 693.2899 - loglik: -6.8376e+02 - logprior: -9.5274e+00
Epoch 2/10
19/19 - 6s - loss: 632.2861 - loglik: -6.3201e+02 - logprior: -2.8080e-01
Epoch 3/10
19/19 - 6s - loss: 598.6051 - loglik: -5.9794e+02 - logprior: -6.6582e-01
Epoch 4/10
19/19 - 6s - loss: 591.1766 - loglik: -5.9078e+02 - logprior: -3.9487e-01
Epoch 5/10
19/19 - 6s - loss: 588.0836 - loglik: -5.8786e+02 - logprior: -2.2449e-01
Epoch 6/10
19/19 - 6s - loss: 587.6591 - loglik: -5.8747e+02 - logprior: -1.8844e-01
Epoch 7/10
19/19 - 6s - loss: 585.3035 - loglik: -5.8522e+02 - logprior: -8.5670e-02
Epoch 8/10
19/19 - 6s - loss: 585.6185 - loglik: -5.8560e+02 - logprior: -2.2379e-02
Fitted a model with MAP estimate = -584.9839
expansions: [(0, 2), (27, 2), (36, 2), (64, 4), (91, 2), (92, 1), (98, 5), (102, 1), (103, 1), (111, 1), (118, 1), (133, 1), (134, 1), (135, 1), (142, 1), (143, 3), (145, 2), (165, 7), (186, 3), (198, 5)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 600.3391 - loglik: -5.8748e+02 - logprior: -1.2857e+01
Epoch 2/2
19/19 - 8s - loss: 580.0993 - loglik: -5.7921e+02 - logprior: -8.9039e-01
Fitted a model with MAP estimate = -576.4807
expansions: [(71, 1), (225, 1)]
discards: [  0  29 111 112 169 170 198 199 200 205 206 238 239 240 241 242]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 594.9749 - loglik: -5.8235e+02 - logprior: -1.2630e+01
Epoch 2/2
19/19 - 7s - loss: 581.4641 - loglik: -5.7885e+02 - logprior: -2.6090e+00
Fitted a model with MAP estimate = -577.0004
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 588.0795 - loglik: -5.7982e+02 - logprior: -8.2583e+00
Epoch 2/10
19/19 - 7s - loss: 577.2948 - loglik: -5.7800e+02 - logprior: 0.7044
Epoch 3/10
19/19 - 7s - loss: 574.7609 - loglik: -5.7638e+02 - logprior: 1.6219
Epoch 4/10
19/19 - 7s - loss: 572.5680 - loglik: -5.7474e+02 - logprior: 2.1744
Epoch 5/10
19/19 - 8s - loss: 572.6741 - loglik: -5.7511e+02 - logprior: 2.4381
Fitted a model with MAP estimate = -572.0328
Time for alignment: 154.7714
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 693.4552 - loglik: -6.8390e+02 - logprior: -9.5583e+00
Epoch 2/10
19/19 - 6s - loss: 626.2932 - loglik: -6.2592e+02 - logprior: -3.6818e-01
Epoch 3/10
19/19 - 6s - loss: 594.8201 - loglik: -5.9423e+02 - logprior: -5.9075e-01
Epoch 4/10
19/19 - 6s - loss: 588.2410 - loglik: -5.8806e+02 - logprior: -1.8475e-01
Epoch 5/10
19/19 - 6s - loss: 588.1376 - loglik: -5.8820e+02 - logprior: 0.0575
Epoch 6/10
19/19 - 6s - loss: 585.2947 - loglik: -5.8542e+02 - logprior: 0.1207
Epoch 7/10
19/19 - 6s - loss: 583.7950 - loglik: -5.8398e+02 - logprior: 0.1872
Epoch 8/10
19/19 - 6s - loss: 584.0001 - loglik: -5.8424e+02 - logprior: 0.2438
Fitted a model with MAP estimate = -583.8114
expansions: [(0, 2), (27, 1), (39, 3), (63, 6), (88, 2), (103, 1), (116, 1), (118, 1), (133, 1), (134, 1), (148, 4), (164, 7), (183, 1), (184, 1), (185, 1), (198, 5)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 600.2484 - loglik: -5.8742e+02 - logprior: -1.2826e+01
Epoch 2/2
19/19 - 8s - loss: 580.8762 - loglik: -5.8001e+02 - logprior: -8.7008e-01
Fitted a model with MAP estimate = -577.4814
expansions: [(212, 1)]
discards: [  0 189 190 191 197 198 199 230 231 232 233 234]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 224 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 597.1141 - loglik: -5.8435e+02 - logprior: -1.2769e+01
Epoch 2/2
19/19 - 7s - loss: 582.3565 - loglik: -5.7949e+02 - logprior: -2.8622e+00
Fitted a model with MAP estimate = -578.2049
expansions: [(0, 2), (100, 2), (101, 2), (166, 2), (192, 1)]
discards: [  0 194]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 590.1395 - loglik: -5.8169e+02 - logprior: -8.4518e+00
Epoch 2/10
19/19 - 8s - loss: 578.3725 - loglik: -5.7887e+02 - logprior: 0.4974
Epoch 3/10
19/19 - 7s - loss: 572.8159 - loglik: -5.7424e+02 - logprior: 1.4235
Epoch 4/10
19/19 - 8s - loss: 573.9040 - loglik: -5.7587e+02 - logprior: 1.9621
Fitted a model with MAP estimate = -572.5157
Time for alignment: 143.2794
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 693.2817 - loglik: -6.8373e+02 - logprior: -9.5564e+00
Epoch 2/10
19/19 - 6s - loss: 631.6343 - loglik: -6.3134e+02 - logprior: -2.9831e-01
Epoch 3/10
19/19 - 6s - loss: 599.7444 - loglik: -5.9911e+02 - logprior: -6.3600e-01
Epoch 4/10
19/19 - 6s - loss: 591.3853 - loglik: -5.9106e+02 - logprior: -3.2770e-01
Epoch 5/10
19/19 - 6s - loss: 588.5861 - loglik: -5.8846e+02 - logprior: -1.2796e-01
Epoch 6/10
19/19 - 6s - loss: 588.6121 - loglik: -5.8866e+02 - logprior: 0.0512
Fitted a model with MAP estimate = -587.3967
expansions: [(27, 1), (39, 2), (40, 1), (63, 5), (90, 2), (102, 1), (103, 1), (111, 1), (118, 1), (133, 3), (134, 1), (143, 2), (146, 1), (163, 1), (164, 1), (165, 6), (186, 3), (198, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 597.9012 - loglik: -5.8864e+02 - logprior: -9.2605e+00
Epoch 2/2
19/19 - 8s - loss: 581.8950 - loglik: -5.8113e+02 - logprior: -7.6411e-01
Fitted a model with MAP estimate = -578.4575
expansions: [(218, 1)]
discards: [163 190 191 192 198 199 231 232 233 234 235]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 226 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 592.5845 - loglik: -5.8381e+02 - logprior: -8.7780e+00
Epoch 2/2
19/19 - 7s - loss: 580.4299 - loglik: -5.8043e+02 - logprior: 0.0024
Fitted a model with MAP estimate = -578.3743
expansions: [(38, 2), (189, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 590.1323 - loglik: -5.8179e+02 - logprior: -8.3442e+00
Epoch 2/10
19/19 - 8s - loss: 579.0201 - loglik: -5.7951e+02 - logprior: 0.4900
Epoch 3/10
19/19 - 8s - loss: 576.1043 - loglik: -5.7760e+02 - logprior: 1.4984
Epoch 4/10
19/19 - 8s - loss: 575.4641 - loglik: -5.7746e+02 - logprior: 1.9935
Epoch 5/10
19/19 - 8s - loss: 574.6312 - loglik: -5.7689e+02 - logprior: 2.2582
Epoch 6/10
19/19 - 8s - loss: 573.5246 - loglik: -5.7600e+02 - logprior: 2.4750
Epoch 7/10
19/19 - 8s - loss: 573.5725 - loglik: -5.7622e+02 - logprior: 2.6524
Fitted a model with MAP estimate = -573.0176
Time for alignment: 155.5034
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 693.0207 - loglik: -6.8346e+02 - logprior: -9.5649e+00
Epoch 2/10
19/19 - 6s - loss: 625.8929 - loglik: -6.2552e+02 - logprior: -3.7363e-01
Epoch 3/10
19/19 - 6s - loss: 593.6460 - loglik: -5.9307e+02 - logprior: -5.7322e-01
Epoch 4/10
19/19 - 6s - loss: 589.9932 - loglik: -5.8981e+02 - logprior: -1.8095e-01
Epoch 5/10
19/19 - 6s - loss: 586.6807 - loglik: -5.8677e+02 - logprior: 0.0907
Epoch 6/10
19/19 - 6s - loss: 585.3682 - loglik: -5.8551e+02 - logprior: 0.1371
Epoch 7/10
19/19 - 6s - loss: 584.9839 - loglik: -5.8516e+02 - logprior: 0.1760
Epoch 8/10
19/19 - 6s - loss: 583.3038 - loglik: -5.8357e+02 - logprior: 0.2680
Epoch 9/10
19/19 - 6s - loss: 585.7020 - loglik: -5.8597e+02 - logprior: 0.2640
Fitted a model with MAP estimate = -583.4825
expansions: [(25, 1), (26, 1), (38, 5), (62, 3), (89, 4), (90, 3), (103, 1), (111, 1), (118, 1), (133, 1), (134, 1), (148, 4), (164, 7), (183, 1), (184, 1), (185, 1), (198, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 596.5714 - loglik: -5.8759e+02 - logprior: -8.9837e+00
Epoch 2/2
19/19 - 8s - loss: 579.8247 - loglik: -5.7934e+02 - logprior: -4.8383e-01
Fitted a model with MAP estimate = -576.8693
expansions: [(31, 3), (72, 2), (171, 2), (216, 1)]
discards: [ 42 104 201 202 234 235 236 237 238]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 590.4772 - loglik: -5.8191e+02 - logprior: -8.5666e+00
Epoch 2/2
19/19 - 8s - loss: 578.4611 - loglik: -5.7861e+02 - logprior: 0.1518
Fitted a model with MAP estimate = -575.3603
expansions: []
discards: [ 31  32 173 195 196]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 588.1205 - loglik: -5.7978e+02 - logprior: -8.3386e+00
Epoch 2/10
19/19 - 8s - loss: 578.1221 - loglik: -5.7862e+02 - logprior: 0.5005
Epoch 3/10
19/19 - 8s - loss: 574.3930 - loglik: -5.7594e+02 - logprior: 1.5463
Epoch 4/10
19/19 - 8s - loss: 573.2590 - loglik: -5.7533e+02 - logprior: 2.0677
Epoch 5/10
19/19 - 8s - loss: 572.0538 - loglik: -5.7440e+02 - logprior: 2.3449
Epoch 6/10
19/19 - 8s - loss: 572.9061 - loglik: -5.7547e+02 - logprior: 2.5681
Fitted a model with MAP estimate = -571.6298
Time for alignment: 168.8523
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 692.3913 - loglik: -6.8283e+02 - logprior: -9.5589e+00
Epoch 2/10
19/19 - 6s - loss: 629.8482 - loglik: -6.2950e+02 - logprior: -3.4757e-01
Epoch 3/10
19/19 - 6s - loss: 597.0895 - loglik: -5.9650e+02 - logprior: -5.9091e-01
Epoch 4/10
19/19 - 6s - loss: 589.0327 - loglik: -5.8881e+02 - logprior: -2.2342e-01
Epoch 5/10
19/19 - 6s - loss: 587.8734 - loglik: -5.8795e+02 - logprior: 0.0797
Epoch 6/10
19/19 - 6s - loss: 585.6958 - loglik: -5.8592e+02 - logprior: 0.2241
Epoch 7/10
19/19 - 6s - loss: 585.9777 - loglik: -5.8629e+02 - logprior: 0.3100
Fitted a model with MAP estimate = -585.1445
expansions: [(0, 2), (25, 1), (26, 1), (47, 14), (64, 3), (89, 2), (102, 1), (103, 1), (111, 1), (118, 1), (133, 1), (140, 1), (146, 5), (166, 7), (183, 1), (184, 2), (185, 1), (198, 5)]
discards: [168 169]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 246 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 602.6833 - loglik: -5.9010e+02 - logprior: -1.2579e+01
Epoch 2/2
19/19 - 8s - loss: 581.0005 - loglik: -5.8019e+02 - logprior: -8.1302e-01
Fitted a model with MAP estimate = -577.7317
expansions: [(112, 1)]
discards: [  0   1  55  56 176 207 208 209 210 211 212 241 242 243 244 245]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 594.0096 - loglik: -5.8466e+02 - logprior: -9.3505e+00
Epoch 2/2
19/19 - 8s - loss: 580.9017 - loglik: -5.8094e+02 - logprior: 0.0422
Fitted a model with MAP estimate = -578.2120
expansions: [(197, 8)]
discards: [ 45 200 201 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 590.5529 - loglik: -5.8232e+02 - logprior: -8.2279e+00
Epoch 2/10
19/19 - 8s - loss: 577.9514 - loglik: -5.7828e+02 - logprior: 0.3268
Epoch 3/10
19/19 - 8s - loss: 574.6230 - loglik: -5.7625e+02 - logprior: 1.6319
Epoch 4/10
19/19 - 8s - loss: 573.5272 - loglik: -5.7567e+02 - logprior: 2.1403
Epoch 5/10
19/19 - 8s - loss: 572.9926 - loglik: -5.7539e+02 - logprior: 2.3948
Epoch 6/10
19/19 - 8s - loss: 571.9127 - loglik: -5.7451e+02 - logprior: 2.6022
Epoch 7/10
19/19 - 8s - loss: 571.8857 - loglik: -5.7470e+02 - logprior: 2.8110
Epoch 8/10
19/19 - 8s - loss: 572.1859 - loglik: -5.7518e+02 - logprior: 2.9953
Fitted a model with MAP estimate = -570.8913
Time for alignment: 170.3918
Computed alignments with likelihoods: ['-572.0328', '-572.5157', '-573.0176', '-571.6298', '-570.8913']
Best model has likelihood: -570.8913  (prior= 3.0918 )
time for generating output: 0.3077
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.7607515657620042
Training of 5 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202c103b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe0072490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fc64ee040>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f92a631f0>
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 157.1564 - loglik: -1.5394e+02 - logprior: -3.2153e+00
Epoch 2/10
19/19 - 1s - loss: 124.1411 - loglik: -1.2260e+02 - logprior: -1.5375e+00
Epoch 3/10
19/19 - 1s - loss: 108.4884 - loglik: -1.0681e+02 - logprior: -1.6775e+00
Epoch 4/10
19/19 - 1s - loss: 105.5263 - loglik: -1.0389e+02 - logprior: -1.6379e+00
Epoch 5/10
19/19 - 1s - loss: 104.1117 - loglik: -1.0252e+02 - logprior: -1.5939e+00
Epoch 6/10
19/19 - 1s - loss: 103.9012 - loglik: -1.0233e+02 - logprior: -1.5749e+00
Epoch 7/10
19/19 - 1s - loss: 103.3408 - loglik: -1.0178e+02 - logprior: -1.5593e+00
Epoch 8/10
19/19 - 1s - loss: 103.3026 - loglik: -1.0175e+02 - logprior: -1.5492e+00
Epoch 9/10
19/19 - 1s - loss: 103.0642 - loglik: -1.0151e+02 - logprior: -1.5567e+00
Epoch 10/10
19/19 - 1s - loss: 102.8820 - loglik: -1.0134e+02 - logprior: -1.5447e+00
Fitted a model with MAP estimate = -99.2830
expansions: [(5, 1), (7, 1), (10, 1), (17, 2), (20, 1), (21, 1), (24, 2), (28, 1), (29, 2), (30, 1), (31, 2), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.4227 - loglik: -1.0409e+02 - logprior: -3.3361e+00
Epoch 2/2
19/19 - 1s - loss: 97.4809 - loglik: -9.6022e+01 - logprior: -1.4590e+00
Fitted a model with MAP estimate = -92.1102
expansions: []
discards: [20 31 39 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.4575 - loglik: -9.6274e+01 - logprior: -3.1838e+00
Epoch 2/2
19/19 - 1s - loss: 95.9892 - loglik: -9.4695e+01 - logprior: -1.2939e+00
Fitted a model with MAP estimate = -91.6058
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 93.6603 - loglik: -9.1068e+01 - logprior: -2.5923e+00
Epoch 2/10
21/21 - 1s - loss: 90.7677 - loglik: -8.9481e+01 - logprior: -1.2866e+00
Epoch 3/10
21/21 - 1s - loss: 90.4667 - loglik: -8.9270e+01 - logprior: -1.1967e+00
Epoch 4/10
21/21 - 1s - loss: 89.4393 - loglik: -8.8286e+01 - logprior: -1.1533e+00
Epoch 5/10
21/21 - 1s - loss: 89.3675 - loglik: -8.8233e+01 - logprior: -1.1344e+00
Epoch 6/10
21/21 - 1s - loss: 88.6715 - loglik: -8.7528e+01 - logprior: -1.1440e+00
Epoch 7/10
21/21 - 1s - loss: 88.3764 - loglik: -8.7248e+01 - logprior: -1.1285e+00
Epoch 8/10
21/21 - 1s - loss: 88.4424 - loglik: -8.7314e+01 - logprior: -1.1281e+00
Fitted a model with MAP estimate = -88.0789
Time for alignment: 47.8252
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 157.3092 - loglik: -1.5410e+02 - logprior: -3.2135e+00
Epoch 2/10
19/19 - 1s - loss: 124.3036 - loglik: -1.2277e+02 - logprior: -1.5385e+00
Epoch 3/10
19/19 - 1s - loss: 108.9584 - loglik: -1.0730e+02 - logprior: -1.6582e+00
Epoch 4/10
19/19 - 1s - loss: 105.7284 - loglik: -1.0410e+02 - logprior: -1.6313e+00
Epoch 5/10
19/19 - 1s - loss: 104.4564 - loglik: -1.0287e+02 - logprior: -1.5875e+00
Epoch 6/10
19/19 - 1s - loss: 103.6939 - loglik: -1.0212e+02 - logprior: -1.5758e+00
Epoch 7/10
19/19 - 1s - loss: 103.5843 - loglik: -1.0203e+02 - logprior: -1.5575e+00
Epoch 8/10
19/19 - 1s - loss: 103.1412 - loglik: -1.0159e+02 - logprior: -1.5499e+00
Epoch 9/10
19/19 - 1s - loss: 103.2205 - loglik: -1.0167e+02 - logprior: -1.5457e+00
Fitted a model with MAP estimate = -99.3435
expansions: [(5, 1), (7, 1), (10, 1), (17, 2), (20, 1), (21, 1), (24, 2), (28, 1), (29, 2), (30, 2), (31, 2), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.7918 - loglik: -1.0444e+02 - logprior: -3.3519e+00
Epoch 2/2
19/19 - 1s - loss: 97.4456 - loglik: -9.5971e+01 - logprior: -1.4749e+00
Fitted a model with MAP estimate = -92.0897
expansions: []
discards: [20 31 39 43 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.5541 - loglik: -9.6366e+01 - logprior: -3.1878e+00
Epoch 2/2
19/19 - 1s - loss: 96.0099 - loglik: -9.4717e+01 - logprior: -1.2934e+00
Fitted a model with MAP estimate = -91.6037
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 93.7206 - loglik: -9.1122e+01 - logprior: -2.5990e+00
Epoch 2/10
21/21 - 1s - loss: 90.7123 - loglik: -8.9426e+01 - logprior: -1.2868e+00
Epoch 3/10
21/21 - 1s - loss: 90.2053 - loglik: -8.9010e+01 - logprior: -1.1948e+00
Epoch 4/10
21/21 - 1s - loss: 89.8944 - loglik: -8.8737e+01 - logprior: -1.1570e+00
Epoch 5/10
21/21 - 1s - loss: 89.2004 - loglik: -8.8065e+01 - logprior: -1.1350e+00
Epoch 6/10
21/21 - 1s - loss: 88.6826 - loglik: -8.7544e+01 - logprior: -1.1389e+00
Epoch 7/10
21/21 - 1s - loss: 88.6468 - loglik: -8.7516e+01 - logprior: -1.1305e+00
Epoch 8/10
21/21 - 1s - loss: 88.1154 - loglik: -8.6990e+01 - logprior: -1.1250e+00
Epoch 9/10
21/21 - 1s - loss: 88.0790 - loglik: -8.6960e+01 - logprior: -1.1190e+00
Epoch 10/10
21/21 - 1s - loss: 87.9669 - loglik: -8.6851e+01 - logprior: -1.1161e+00
Fitted a model with MAP estimate = -87.8673
Time for alignment: 48.6569
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 157.3475 - loglik: -1.5413e+02 - logprior: -3.2143e+00
Epoch 2/10
19/19 - 1s - loss: 124.3333 - loglik: -1.2280e+02 - logprior: -1.5309e+00
Epoch 3/10
19/19 - 1s - loss: 108.4299 - loglik: -1.0676e+02 - logprior: -1.6697e+00
Epoch 4/10
19/19 - 1s - loss: 105.4371 - loglik: -1.0380e+02 - logprior: -1.6333e+00
Epoch 5/10
19/19 - 1s - loss: 104.1673 - loglik: -1.0258e+02 - logprior: -1.5915e+00
Epoch 6/10
19/19 - 1s - loss: 104.1109 - loglik: -1.0255e+02 - logprior: -1.5639e+00
Epoch 7/10
19/19 - 1s - loss: 103.3493 - loglik: -1.0179e+02 - logprior: -1.5559e+00
Epoch 8/10
19/19 - 1s - loss: 103.2139 - loglik: -1.0167e+02 - logprior: -1.5484e+00
Epoch 9/10
19/19 - 1s - loss: 102.9602 - loglik: -1.0140e+02 - logprior: -1.5569e+00
Epoch 10/10
19/19 - 1s - loss: 102.7503 - loglik: -1.0120e+02 - logprior: -1.5523e+00
Fitted a model with MAP estimate = -99.2627
expansions: [(5, 1), (7, 1), (10, 1), (17, 2), (20, 1), (21, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.4067 - loglik: -1.0310e+02 - logprior: -3.3046e+00
Epoch 2/2
19/19 - 1s - loss: 97.1874 - loglik: -9.5789e+01 - logprior: -1.3988e+00
Fitted a model with MAP estimate = -91.9536
expansions: []
discards: [20 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.4325 - loglik: -9.6250e+01 - logprior: -3.1825e+00
Epoch 2/2
19/19 - 1s - loss: 96.1109 - loglik: -9.4821e+01 - logprior: -1.2897e+00
Fitted a model with MAP estimate = -91.7088
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 93.5136 - loglik: -9.0918e+01 - logprior: -2.5952e+00
Epoch 2/10
21/21 - 1s - loss: 91.1197 - loglik: -8.9840e+01 - logprior: -1.2798e+00
Epoch 3/10
21/21 - 1s - loss: 90.2158 - loglik: -8.9019e+01 - logprior: -1.1963e+00
Epoch 4/10
21/21 - 1s - loss: 89.5272 - loglik: -8.8372e+01 - logprior: -1.1555e+00
Epoch 5/10
21/21 - 1s - loss: 89.3474 - loglik: -8.8204e+01 - logprior: -1.1429e+00
Epoch 6/10
21/21 - 1s - loss: 88.3065 - loglik: -8.7167e+01 - logprior: -1.1391e+00
Epoch 7/10
21/21 - 1s - loss: 88.7445 - loglik: -8.7623e+01 - logprior: -1.1213e+00
Fitted a model with MAP estimate = -88.2754
Time for alignment: 46.4149
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 157.3020 - loglik: -1.5409e+02 - logprior: -3.2156e+00
Epoch 2/10
19/19 - 1s - loss: 124.2145 - loglik: -1.2270e+02 - logprior: -1.5185e+00
Epoch 3/10
19/19 - 1s - loss: 109.0625 - loglik: -1.0739e+02 - logprior: -1.6677e+00
Epoch 4/10
19/19 - 1s - loss: 105.7510 - loglik: -1.0412e+02 - logprior: -1.6307e+00
Epoch 5/10
19/19 - 1s - loss: 104.4033 - loglik: -1.0282e+02 - logprior: -1.5856e+00
Epoch 6/10
19/19 - 1s - loss: 103.6295 - loglik: -1.0206e+02 - logprior: -1.5656e+00
Epoch 7/10
19/19 - 1s - loss: 103.6172 - loglik: -1.0206e+02 - logprior: -1.5555e+00
Epoch 8/10
19/19 - 1s - loss: 103.1691 - loglik: -1.0161e+02 - logprior: -1.5561e+00
Epoch 9/10
19/19 - 1s - loss: 103.2902 - loglik: -1.0174e+02 - logprior: -1.5505e+00
Fitted a model with MAP estimate = -99.3127
expansions: [(5, 1), (8, 1), (10, 1), (17, 2), (20, 1), (21, 1), (24, 2), (28, 1), (29, 2), (30, 1), (31, 1), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.7891 - loglik: -1.0347e+02 - logprior: -3.3204e+00
Epoch 2/2
19/19 - 1s - loss: 97.0760 - loglik: -9.5649e+01 - logprior: -1.4269e+00
Fitted a model with MAP estimate = -91.9523
expansions: []
discards: [20 31 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.3308 - loglik: -9.6147e+01 - logprior: -3.1840e+00
Epoch 2/2
19/19 - 1s - loss: 96.0939 - loglik: -9.4808e+01 - logprior: -1.2858e+00
Fitted a model with MAP estimate = -91.6630
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 93.6068 - loglik: -9.1013e+01 - logprior: -2.5941e+00
Epoch 2/10
21/21 - 1s - loss: 90.9161 - loglik: -8.9636e+01 - logprior: -1.2798e+00
Epoch 3/10
21/21 - 1s - loss: 90.2902 - loglik: -8.9095e+01 - logprior: -1.1952e+00
Epoch 4/10
21/21 - 1s - loss: 89.8254 - loglik: -8.8666e+01 - logprior: -1.1592e+00
Epoch 5/10
21/21 - 1s - loss: 89.0032 - loglik: -8.7867e+01 - logprior: -1.1366e+00
Epoch 6/10
21/21 - 1s - loss: 88.8410 - loglik: -8.7700e+01 - logprior: -1.1415e+00
Epoch 7/10
21/21 - 1s - loss: 88.5845 - loglik: -8.7460e+01 - logprior: -1.1240e+00
Epoch 8/10
21/21 - 1s - loss: 88.2636 - loglik: -8.7137e+01 - logprior: -1.1268e+00
Epoch 9/10
21/21 - 1s - loss: 88.1278 - loglik: -8.7006e+01 - logprior: -1.1214e+00
Epoch 10/10
21/21 - 1s - loss: 87.9995 - loglik: -8.6888e+01 - logprior: -1.1112e+00
Fitted a model with MAP estimate = -87.8732
Time for alignment: 47.3041
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 157.1790 - loglik: -1.5396e+02 - logprior: -3.2152e+00
Epoch 2/10
19/19 - 1s - loss: 124.6389 - loglik: -1.2311e+02 - logprior: -1.5283e+00
Epoch 3/10
19/19 - 1s - loss: 108.8903 - loglik: -1.0723e+02 - logprior: -1.6641e+00
Epoch 4/10
19/19 - 1s - loss: 105.5226 - loglik: -1.0390e+02 - logprior: -1.6264e+00
Epoch 5/10
19/19 - 1s - loss: 104.3982 - loglik: -1.0281e+02 - logprior: -1.5892e+00
Epoch 6/10
19/19 - 1s - loss: 103.7738 - loglik: -1.0221e+02 - logprior: -1.5683e+00
Epoch 7/10
19/19 - 1s - loss: 103.4775 - loglik: -1.0192e+02 - logprior: -1.5572e+00
Epoch 8/10
19/19 - 1s - loss: 103.2233 - loglik: -1.0168e+02 - logprior: -1.5472e+00
Epoch 9/10
19/19 - 1s - loss: 103.1638 - loglik: -1.0161e+02 - logprior: -1.5542e+00
Epoch 10/10
19/19 - 1s - loss: 102.5402 - loglik: -1.0099e+02 - logprior: -1.5539e+00
Fitted a model with MAP estimate = -99.2858
expansions: [(5, 1), (7, 1), (10, 1), (17, 2), (20, 1), (21, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.4434 - loglik: -1.0314e+02 - logprior: -3.3053e+00
Epoch 2/2
19/19 - 1s - loss: 97.2622 - loglik: -9.5865e+01 - logprior: -1.3977e+00
Fitted a model with MAP estimate = -92.0527
expansions: []
discards: [20 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.4276 - loglik: -9.6244e+01 - logprior: -3.1832e+00
Epoch 2/2
19/19 - 1s - loss: 96.0099 - loglik: -9.4716e+01 - logprior: -1.2935e+00
Fitted a model with MAP estimate = -91.5822
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 93.6612 - loglik: -9.1063e+01 - logprior: -2.5987e+00
Epoch 2/10
21/21 - 1s - loss: 90.9029 - loglik: -8.9631e+01 - logprior: -1.2723e+00
Epoch 3/10
21/21 - 1s - loss: 89.8858 - loglik: -8.8689e+01 - logprior: -1.1963e+00
Epoch 4/10
21/21 - 1s - loss: 90.0182 - loglik: -8.8873e+01 - logprior: -1.1453e+00
Fitted a model with MAP estimate = -89.2556
Time for alignment: 40.5050
Computed alignments with likelihoods: ['-88.0789', '-87.8673', '-88.2754', '-87.8732', '-89.2556']
Best model has likelihood: -87.8673  (prior= -1.0906 )
time for generating output: 0.1074
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.9240180296200902
Training of 5 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fbd891550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2023e4bd90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2023e4bd00>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fbd165040>
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 6s - loss: 425.4949 - loglik: -4.2084e+02 - logprior: -4.6541e+00
Epoch 2/10
27/27 - 4s - loss: 339.3148 - loglik: -3.3730e+02 - logprior: -2.0143e+00
Epoch 3/10
27/27 - 4s - loss: 321.0695 - loglik: -3.1884e+02 - logprior: -2.2249e+00
Epoch 4/10
27/27 - 4s - loss: 317.2195 - loglik: -3.1496e+02 - logprior: -2.2557e+00
Epoch 5/10
27/27 - 4s - loss: 315.9107 - loglik: -3.1370e+02 - logprior: -2.2106e+00
Epoch 6/10
27/27 - 4s - loss: 315.3295 - loglik: -3.1315e+02 - logprior: -2.1789e+00
Epoch 7/10
27/27 - 4s - loss: 315.4135 - loglik: -3.1328e+02 - logprior: -2.1317e+00
Fitted a model with MAP estimate = -314.9411
expansions: [(11, 3), (19, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (40, 1), (43, 1), (44, 1), (46, 1), (48, 1), (49, 1), (50, 2), (66, 1), (67, 1), (68, 1), (71, 1), (73, 1), (76, 1), (77, 1), (81, 1), (85, 1), (98, 1), (99, 1), (101, 1), (106, 1), (114, 1), (115, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 159 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 308.1598 - loglik: -3.0180e+02 - logprior: -6.3559e+00
Epoch 2/2
27/27 - 4s - loss: 291.8008 - loglik: -2.8985e+02 - logprior: -1.9471e+00
Fitted a model with MAP estimate = -289.1983
expansions: [(0, 2)]
discards: [ 0 11 29 48 49 50 69 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 295.9186 - loglik: -2.9195e+02 - logprior: -3.9717e+00
Epoch 2/2
27/27 - 4s - loss: 290.6544 - loglik: -2.8991e+02 - logprior: -7.4701e-01
Fitted a model with MAP estimate = -289.8362
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 298.0644 - loglik: -2.9229e+02 - logprior: -5.7733e+00
Epoch 2/10
27/27 - 4s - loss: 291.7380 - loglik: -2.9095e+02 - logprior: -7.9102e-01
Epoch 3/10
27/27 - 4s - loss: 290.3652 - loglik: -2.8996e+02 - logprior: -4.0972e-01
Epoch 4/10
27/27 - 4s - loss: 290.3513 - loglik: -2.9003e+02 - logprior: -3.2142e-01
Epoch 5/10
27/27 - 4s - loss: 288.9925 - loglik: -2.8875e+02 - logprior: -2.3757e-01
Epoch 6/10
27/27 - 4s - loss: 288.9154 - loglik: -2.8875e+02 - logprior: -1.6379e-01
Epoch 7/10
27/27 - 4s - loss: 288.2581 - loglik: -2.8818e+02 - logprior: -8.1391e-02
Epoch 8/10
27/27 - 4s - loss: 288.2105 - loglik: -2.8821e+02 - logprior: -1.4507e-03
Epoch 9/10
27/27 - 4s - loss: 288.1730 - loglik: -2.8823e+02 - logprior: 0.0573
Epoch 10/10
27/27 - 4s - loss: 287.8571 - loglik: -2.8799e+02 - logprior: 0.1357
Fitted a model with MAP estimate = -287.5668
Time for alignment: 117.7919
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 426.0728 - loglik: -4.2141e+02 - logprior: -4.6636e+00
Epoch 2/10
27/27 - 4s - loss: 337.2202 - loglik: -3.3509e+02 - logprior: -2.1274e+00
Epoch 3/10
27/27 - 4s - loss: 318.3145 - loglik: -3.1597e+02 - logprior: -2.3490e+00
Epoch 4/10
27/27 - 4s - loss: 313.5272 - loglik: -3.1122e+02 - logprior: -2.3078e+00
Epoch 5/10
27/27 - 4s - loss: 312.0750 - loglik: -3.0980e+02 - logprior: -2.2717e+00
Epoch 6/10
27/27 - 4s - loss: 310.8349 - loglik: -3.0859e+02 - logprior: -2.2461e+00
Epoch 7/10
27/27 - 4s - loss: 310.8770 - loglik: -3.0864e+02 - logprior: -2.2338e+00
Fitted a model with MAP estimate = -310.6354
expansions: [(4, 1), (5, 1), (19, 1), (22, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (50, 1), (66, 1), (67, 1), (68, 1), (71, 1), (73, 1), (78, 1), (79, 1), (81, 1), (82, 2), (100, 1), (102, 1), (105, 2), (106, 1), (114, 1), (115, 1), (120, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 307.8579 - loglik: -3.0149e+02 - logprior: -6.3677e+00
Epoch 2/2
27/27 - 4s - loss: 292.1198 - loglik: -2.9006e+02 - logprior: -2.0607e+00
Fitted a model with MAP estimate = -289.3055
expansions: [(0, 3), (41, 1)]
discards: [  0  28  46  47  48  65  89 110 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 295.9716 - loglik: -2.9201e+02 - logprior: -3.9571e+00
Epoch 2/2
27/27 - 4s - loss: 290.1537 - loglik: -2.8938e+02 - logprior: -7.7632e-01
Fitted a model with MAP estimate = -289.5643
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 297.8654 - loglik: -2.9189e+02 - logprior: -5.9754e+00
Epoch 2/10
27/27 - 4s - loss: 292.4528 - loglik: -2.9153e+02 - logprior: -9.2558e-01
Epoch 3/10
27/27 - 4s - loss: 289.6741 - loglik: -2.8945e+02 - logprior: -2.2172e-01
Epoch 4/10
27/27 - 4s - loss: 289.3301 - loglik: -2.8919e+02 - logprior: -1.4220e-01
Epoch 5/10
27/27 - 4s - loss: 288.9173 - loglik: -2.8884e+02 - logprior: -7.9046e-02
Epoch 6/10
27/27 - 4s - loss: 288.2350 - loglik: -2.8825e+02 - logprior: 0.0146
Epoch 7/10
27/27 - 4s - loss: 288.6156 - loglik: -2.8870e+02 - logprior: 0.0797
Fitted a model with MAP estimate = -287.7383
Time for alignment: 106.1199
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 426.2214 - loglik: -4.2155e+02 - logprior: -4.6696e+00
Epoch 2/10
27/27 - 4s - loss: 341.1992 - loglik: -3.3902e+02 - logprior: -2.1774e+00
Epoch 3/10
27/27 - 4s - loss: 321.1470 - loglik: -3.1875e+02 - logprior: -2.3947e+00
Epoch 4/10
27/27 - 4s - loss: 318.0898 - loglik: -3.1582e+02 - logprior: -2.2693e+00
Epoch 5/10
27/27 - 4s - loss: 316.8047 - loglik: -3.1461e+02 - logprior: -2.1936e+00
Epoch 6/10
27/27 - 4s - loss: 316.1906 - loglik: -3.1402e+02 - logprior: -2.1725e+00
Epoch 7/10
27/27 - 4s - loss: 315.9966 - loglik: -3.1383e+02 - logprior: -2.1698e+00
Epoch 8/10
27/27 - 4s - loss: 315.9438 - loglik: -3.1379e+02 - logprior: -2.1551e+00
Epoch 9/10
27/27 - 4s - loss: 315.3025 - loglik: -3.1316e+02 - logprior: -2.1406e+00
Epoch 10/10
27/27 - 4s - loss: 315.3538 - loglik: -3.1321e+02 - logprior: -2.1401e+00
Fitted a model with MAP estimate = -315.2311
expansions: [(4, 1), (10, 2), (14, 1), (18, 1), (24, 2), (26, 1), (36, 2), (37, 2), (38, 2), (40, 1), (42, 1), (43, 1), (44, 1), (45, 1), (47, 2), (50, 1), (53, 1), (67, 2), (70, 1), (71, 1), (76, 1), (77, 1), (81, 1), (85, 1), (100, 1), (102, 1), (105, 2), (106, 1), (111, 1), (113, 1), (114, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 311.4092 - loglik: -3.0493e+02 - logprior: -6.4832e+00
Epoch 2/2
27/27 - 4s - loss: 293.6198 - loglik: -2.9134e+02 - logprior: -2.2773e+00
Fitted a model with MAP estimate = -290.9640
expansions: [(0, 2), (42, 1)]
discards: [  0  11  28  48  49  50  66  90 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 297.8263 - loglik: -2.9379e+02 - logprior: -4.0389e+00
Epoch 2/2
27/27 - 4s - loss: 292.8852 - loglik: -2.9209e+02 - logprior: -7.9515e-01
Fitted a model with MAP estimate = -291.7716
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 300.0179 - loglik: -2.9407e+02 - logprior: -5.9527e+00
Epoch 2/10
27/27 - 4s - loss: 293.9678 - loglik: -2.9298e+02 - logprior: -9.8628e-01
Epoch 3/10
27/27 - 4s - loss: 292.1724 - loglik: -2.9165e+02 - logprior: -5.1767e-01
Epoch 4/10
27/27 - 4s - loss: 291.9260 - loglik: -2.9151e+02 - logprior: -4.1573e-01
Epoch 5/10
27/27 - 4s - loss: 290.7889 - loglik: -2.9046e+02 - logprior: -3.3028e-01
Epoch 6/10
27/27 - 4s - loss: 290.8130 - loglik: -2.9056e+02 - logprior: -2.5201e-01
Fitted a model with MAP estimate = -290.2489
Time for alignment: 112.0425
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 425.1692 - loglik: -4.2052e+02 - logprior: -4.6479e+00
Epoch 2/10
27/27 - 4s - loss: 338.5701 - loglik: -3.3657e+02 - logprior: -2.0021e+00
Epoch 3/10
27/27 - 4s - loss: 321.4539 - loglik: -3.1925e+02 - logprior: -2.1991e+00
Epoch 4/10
27/27 - 4s - loss: 315.7448 - loglik: -3.1350e+02 - logprior: -2.2479e+00
Epoch 5/10
27/27 - 4s - loss: 315.4048 - loglik: -3.1317e+02 - logprior: -2.2326e+00
Epoch 6/10
27/27 - 4s - loss: 314.8187 - loglik: -3.1261e+02 - logprior: -2.2071e+00
Epoch 7/10
27/27 - 4s - loss: 314.1314 - loglik: -3.1195e+02 - logprior: -2.1842e+00
Epoch 8/10
27/27 - 4s - loss: 313.8940 - loglik: -3.1173e+02 - logprior: -2.1678e+00
Epoch 9/10
27/27 - 4s - loss: 314.0128 - loglik: -3.1185e+02 - logprior: -2.1642e+00
Fitted a model with MAP estimate = -313.7435
expansions: [(4, 1), (5, 1), (19, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 4), (39, 1), (42, 1), (43, 1), (45, 1), (47, 2), (49, 1), (66, 1), (67, 1), (70, 1), (71, 1), (72, 1), (77, 1), (78, 1), (80, 1), (98, 1), (99, 1), (101, 1), (106, 1), (114, 1), (115, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 308.9652 - loglik: -3.0254e+02 - logprior: -6.4228e+00
Epoch 2/2
27/27 - 4s - loss: 293.3143 - loglik: -2.9106e+02 - logprior: -2.2516e+00
Fitted a model with MAP estimate = -291.0187
expansions: [(0, 3), (41, 1)]
discards: [ 0 28 45 46 63]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 296.3850 - loglik: -2.9239e+02 - logprior: -3.9962e+00
Epoch 2/2
27/27 - 4s - loss: 290.8930 - loglik: -2.9009e+02 - logprior: -8.0208e-01
Fitted a model with MAP estimate = -289.9881
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 298.5672 - loglik: -2.9243e+02 - logprior: -6.1420e+00
Epoch 2/10
27/27 - 4s - loss: 292.1923 - loglik: -2.9099e+02 - logprior: -1.1987e+00
Epoch 3/10
27/27 - 4s - loss: 290.1401 - loglik: -2.8989e+02 - logprior: -2.5265e-01
Epoch 4/10
27/27 - 4s - loss: 290.4592 - loglik: -2.9029e+02 - logprior: -1.6943e-01
Fitted a model with MAP estimate = -289.2282
Time for alignment: 101.2754
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 426.0681 - loglik: -4.2140e+02 - logprior: -4.6633e+00
Epoch 2/10
27/27 - 4s - loss: 337.8079 - loglik: -3.3574e+02 - logprior: -2.0675e+00
Epoch 3/10
27/27 - 4s - loss: 321.1240 - loglik: -3.1896e+02 - logprior: -2.1591e+00
Epoch 4/10
27/27 - 4s - loss: 316.8373 - loglik: -3.1476e+02 - logprior: -2.0792e+00
Epoch 5/10
27/27 - 4s - loss: 316.2558 - loglik: -3.1422e+02 - logprior: -2.0321e+00
Epoch 6/10
27/27 - 4s - loss: 314.7175 - loglik: -3.1273e+02 - logprior: -1.9872e+00
Epoch 7/10
27/27 - 4s - loss: 315.0533 - loglik: -3.1308e+02 - logprior: -1.9773e+00
Fitted a model with MAP estimate = -314.6214
expansions: [(11, 3), (19, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (49, 1), (67, 1), (68, 1), (70, 2), (71, 1), (76, 1), (77, 1), (81, 1), (93, 1), (96, 5), (102, 1), (105, 2), (106, 1), (114, 1), (115, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 164 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 309.2227 - loglik: -3.0281e+02 - logprior: -6.4085e+00
Epoch 2/2
27/27 - 4s - loss: 291.9060 - loglik: -2.8980e+02 - logprior: -2.1109e+00
Fitted a model with MAP estimate = -288.8436
expansions: [(0, 2), (42, 1)]
discards: [  0  11  29  48  49  50  65  94 127 128 129 130 141]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 295.5415 - loglik: -2.9159e+02 - logprior: -3.9542e+00
Epoch 2/2
27/27 - 4s - loss: 290.8176 - loglik: -2.9010e+02 - logprior: -7.1984e-01
Fitted a model with MAP estimate = -289.6300
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 297.9635 - loglik: -2.9191e+02 - logprior: -6.0560e+00
Epoch 2/10
27/27 - 4s - loss: 292.1952 - loglik: -2.9112e+02 - logprior: -1.0787e+00
Epoch 3/10
27/27 - 4s - loss: 289.6454 - loglik: -2.8921e+02 - logprior: -4.3395e-01
Epoch 4/10
27/27 - 4s - loss: 289.9317 - loglik: -2.8959e+02 - logprior: -3.3718e-01
Fitted a model with MAP estimate = -288.7921
Time for alignment: 92.3355
Computed alignments with likelihoods: ['-287.5668', '-287.7383', '-290.2489', '-289.2282', '-288.7921']
Best model has likelihood: -287.5668  (prior= 0.1958 )
time for generating output: 0.2857
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.4887043189368771
Training of 5 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5d4e880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5cd0b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f911f0fd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fdfc2e670>
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 19s - loss: 779.0420 - loglik: -7.7692e+02 - logprior: -2.1226e+00
Epoch 2/10
34/34 - 14s - loss: 643.1688 - loglik: -6.4135e+02 - logprior: -1.8192e+00
Epoch 3/10
34/34 - 14s - loss: 622.7684 - loglik: -6.2071e+02 - logprior: -2.0537e+00
Epoch 4/10
34/34 - 14s - loss: 621.9408 - loglik: -6.1989e+02 - logprior: -2.0533e+00
Epoch 5/10
34/34 - 14s - loss: 619.5906 - loglik: -6.1753e+02 - logprior: -2.0568e+00
Epoch 6/10
34/34 - 14s - loss: 618.8624 - loglik: -6.1680e+02 - logprior: -2.0595e+00
Epoch 7/10
34/34 - 14s - loss: 616.0117 - loglik: -6.1393e+02 - logprior: -2.0796e+00
Epoch 8/10
34/34 - 14s - loss: 616.3208 - loglik: -6.1425e+02 - logprior: -2.0712e+00
Fitted a model with MAP estimate = -615.8126
expansions: [(9, 1), (12, 2), (13, 1), (14, 1), (19, 4), (28, 2), (29, 2), (41, 1), (48, 1), (49, 3), (50, 2), (51, 3), (53, 1), (54, 1), (65, 3), (66, 1), (67, 1), (94, 1), (95, 1), (98, 1), (99, 1), (100, 2), (102, 1), (104, 1), (113, 1), (130, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 1), (173, 1), (174, 1), (176, 1), (179, 1), (185, 1), (187, 1), (188, 2), (191, 1), (198, 1), (201, 1), (203, 1), (204, 1), (206, 1), (208, 1), (225, 1), (226, 1), (227, 1), (228, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 600.5858 - loglik: -5.9728e+02 - logprior: -3.3092e+00
Epoch 2/2
34/34 - 20s - loss: 578.8154 - loglik: -5.7747e+02 - logprior: -1.3468e+00
Fitted a model with MAP estimate = -573.3450
expansions: [(18, 1)]
discards: [ 24  89 187 188 243 244]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 299 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 580.8935 - loglik: -5.7888e+02 - logprior: -2.0178e+00
Epoch 2/2
34/34 - 20s - loss: 575.9788 - loglik: -5.7575e+02 - logprior: -2.3018e-01
Fitted a model with MAP estimate = -574.1352
expansions: [(25, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 578.0934 - loglik: -5.7622e+02 - logprior: -1.8712e+00
Epoch 2/10
34/34 - 20s - loss: 573.0471 - loglik: -5.7295e+02 - logprior: -1.0055e-01
Epoch 3/10
34/34 - 20s - loss: 573.6566 - loglik: -5.7375e+02 - logprior: 0.0911
Fitted a model with MAP estimate = -571.5129
Time for alignment: 363.8840
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 780.1947 - loglik: -7.7808e+02 - logprior: -2.1171e+00
Epoch 2/10
34/34 - 14s - loss: 645.6271 - loglik: -6.4386e+02 - logprior: -1.7642e+00
Epoch 3/10
34/34 - 14s - loss: 626.6063 - loglik: -6.2463e+02 - logprior: -1.9731e+00
Epoch 4/10
34/34 - 14s - loss: 620.7386 - loglik: -6.1878e+02 - logprior: -1.9556e+00
Epoch 5/10
34/34 - 14s - loss: 617.3748 - loglik: -6.1537e+02 - logprior: -2.0030e+00
Epoch 6/10
34/34 - 14s - loss: 616.7259 - loglik: -6.1469e+02 - logprior: -2.0395e+00
Epoch 7/10
34/34 - 14s - loss: 615.9387 - loglik: -6.1392e+02 - logprior: -2.0235e+00
Epoch 8/10
34/34 - 14s - loss: 614.8433 - loglik: -6.1282e+02 - logprior: -2.0207e+00
Epoch 9/10
34/34 - 14s - loss: 614.6571 - loglik: -6.1264e+02 - logprior: -2.0162e+00
Epoch 10/10
34/34 - 14s - loss: 615.4453 - loglik: -6.1342e+02 - logprior: -2.0219e+00
Fitted a model with MAP estimate = -614.1192
expansions: [(9, 1), (12, 1), (19, 5), (28, 2), (29, 2), (48, 4), (49, 2), (50, 4), (65, 3), (66, 1), (67, 1), (94, 1), (95, 3), (100, 1), (101, 3), (102, 1), (103, 1), (105, 1), (113, 1), (130, 1), (131, 1), (140, 1), (143, 1), (144, 3), (145, 1), (166, 1), (169, 1), (174, 1), (175, 1), (177, 1), (179, 1), (185, 1), (188, 1), (189, 1), (192, 1), (194, 1), (195, 1), (198, 1), (201, 1), (203, 1), (204, 1), (205, 1), (208, 1), (211, 1), (225, 1), (227, 1), (228, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 599.9238 - loglik: -5.9653e+02 - logprior: -3.3938e+00
Epoch 2/2
34/34 - 20s - loss: 579.8849 - loglik: -5.7830e+02 - logprior: -1.5800e+00
Fitted a model with MAP estimate = -575.2005
expansions: [(61, 1), (62, 1), (188, 1)]
discards: [ 21  22  63  85 122 123 132]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 299 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 579.9763 - loglik: -5.7795e+02 - logprior: -2.0295e+00
Epoch 2/2
34/34 - 19s - loss: 574.0837 - loglik: -5.7380e+02 - logprior: -2.7917e-01
Fitted a model with MAP estimate = -572.5860
expansions: [(227, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 576.7130 - loglik: -5.7488e+02 - logprior: -1.8312e+00
Epoch 2/10
34/34 - 20s - loss: 573.3002 - loglik: -5.7322e+02 - logprior: -7.8403e-02
Epoch 3/10
34/34 - 19s - loss: 571.4163 - loglik: -5.7153e+02 - logprior: 0.1141
Epoch 4/10
34/34 - 20s - loss: 569.6616 - loglik: -5.6985e+02 - logprior: 0.1920
Epoch 5/10
34/34 - 20s - loss: 569.0118 - loglik: -5.6927e+02 - logprior: 0.2620
Epoch 6/10
34/34 - 20s - loss: 566.7497 - loglik: -5.6716e+02 - logprior: 0.4135
Epoch 7/10
34/34 - 20s - loss: 567.4876 - loglik: -5.6795e+02 - logprior: 0.4660
Fitted a model with MAP estimate = -566.3277
Time for alignment: 468.8811
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 780.9432 - loglik: -7.7884e+02 - logprior: -2.1065e+00
Epoch 2/10
34/34 - 14s - loss: 643.6414 - loglik: -6.4192e+02 - logprior: -1.7239e+00
Epoch 3/10
34/34 - 14s - loss: 623.9429 - loglik: -6.2208e+02 - logprior: -1.8625e+00
Epoch 4/10
34/34 - 14s - loss: 619.2302 - loglik: -6.1742e+02 - logprior: -1.8135e+00
Epoch 5/10
34/34 - 14s - loss: 616.6116 - loglik: -6.1481e+02 - logprior: -1.7997e+00
Epoch 6/10
34/34 - 14s - loss: 616.0384 - loglik: -6.1424e+02 - logprior: -1.8021e+00
Epoch 7/10
34/34 - 14s - loss: 614.8154 - loglik: -6.1301e+02 - logprior: -1.8061e+00
Epoch 8/10
34/34 - 14s - loss: 613.9906 - loglik: -6.1217e+02 - logprior: -1.8186e+00
Epoch 9/10
34/34 - 14s - loss: 612.7993 - loglik: -6.1097e+02 - logprior: -1.8278e+00
Epoch 10/10
34/34 - 14s - loss: 613.3215 - loglik: -6.1150e+02 - logprior: -1.8236e+00
Fitted a model with MAP estimate = -612.3150
expansions: [(9, 2), (12, 1), (14, 1), (19, 4), (29, 2), (31, 1), (48, 1), (49, 1), (50, 1), (51, 2), (52, 4), (64, 3), (65, 1), (70, 1), (93, 1), (94, 1), (99, 1), (100, 2), (101, 1), (102, 1), (103, 1), (121, 1), (131, 1), (134, 1), (139, 1), (142, 1), (143, 3), (144, 1), (147, 1), (164, 3), (173, 1), (174, 1), (176, 1), (178, 1), (186, 1), (187, 1), (191, 1), (195, 1), (201, 1), (203, 1), (204, 1), (205, 1), (208, 1), (211, 1), (225, 1), (227, 1), (228, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 601.5847 - loglik: -5.9829e+02 - logprior: -3.2984e+00
Epoch 2/2
34/34 - 19s - loss: 580.9742 - loglik: -5.7957e+02 - logprior: -1.4079e+00
Fitted a model with MAP estimate = -576.8279
expansions: []
discards: [  8  64  67  68  69  70  83 207]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 290 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 586.6702 - loglik: -5.8469e+02 - logprior: -1.9809e+00
Epoch 2/2
34/34 - 19s - loss: 581.5801 - loglik: -5.8139e+02 - logprior: -1.8712e-01
Fitted a model with MAP estimate = -580.7080
expansions: [(65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 583.1206 - loglik: -5.8140e+02 - logprior: -1.7245e+00
Epoch 2/10
34/34 - 19s - loss: 578.3787 - loglik: -5.7832e+02 - logprior: -5.8162e-02
Epoch 3/10
34/34 - 19s - loss: 577.3697 - loglik: -5.7757e+02 - logprior: 0.1980
Epoch 4/10
34/34 - 19s - loss: 573.8080 - loglik: -5.7402e+02 - logprior: 0.2110
Epoch 5/10
34/34 - 19s - loss: 571.2447 - loglik: -5.7162e+02 - logprior: 0.3707
Epoch 6/10
34/34 - 19s - loss: 571.9828 - loglik: -5.7249e+02 - logprior: 0.5068
Fitted a model with MAP estimate = -570.5501
Time for alignment: 441.0106
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 780.9839 - loglik: -7.7886e+02 - logprior: -2.1227e+00
Epoch 2/10
34/34 - 14s - loss: 646.6816 - loglik: -6.4475e+02 - logprior: -1.9320e+00
Epoch 3/10
34/34 - 14s - loss: 623.7967 - loglik: -6.2161e+02 - logprior: -2.1872e+00
Epoch 4/10
34/34 - 14s - loss: 617.2551 - loglik: -6.1511e+02 - logprior: -2.1405e+00
Epoch 5/10
34/34 - 14s - loss: 616.0737 - loglik: -6.1398e+02 - logprior: -2.0956e+00
Epoch 6/10
34/34 - 14s - loss: 616.4015 - loglik: -6.1431e+02 - logprior: -2.0939e+00
Fitted a model with MAP estimate = -614.6965
expansions: [(9, 1), (11, 2), (13, 1), (14, 2), (18, 3), (19, 2), (21, 1), (22, 2), (28, 3), (41, 1), (48, 1), (49, 2), (50, 1), (51, 1), (52, 3), (54, 1), (65, 3), (66, 1), (67, 1), (89, 1), (90, 1), (94, 3), (99, 1), (100, 3), (101, 1), (102, 1), (104, 1), (112, 1), (131, 1), (137, 1), (139, 1), (142, 1), (145, 1), (148, 1), (165, 3), (172, 1), (173, 1), (174, 1), (176, 1), (179, 1), (185, 1), (188, 1), (191, 1), (196, 1), (197, 1), (198, 1), (199, 1), (202, 1), (204, 1), (205, 1), (206, 1), (208, 1), (211, 1), (226, 1), (228, 1), (229, 1), (230, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 311 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 596.7407 - loglik: -5.9350e+02 - logprior: -3.2357e+00
Epoch 2/2
34/34 - 21s - loss: 572.9124 - loglik: -5.7152e+02 - logprior: -1.3934e+00
Fitted a model with MAP estimate = -568.0355
expansions: [(76, 1)]
discards: [ 17  26  34  43  91 128 129 138 216]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 579.1396 - loglik: -5.7714e+02 - logprior: -1.9996e+00
Epoch 2/2
34/34 - 20s - loss: 572.7046 - loglik: -5.7249e+02 - logprior: -2.1023e-01
Fitted a model with MAP estimate = -571.1013
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 576.0416 - loglik: -5.7420e+02 - logprior: -1.8393e+00
Epoch 2/10
34/34 - 20s - loss: 570.4949 - loglik: -5.7043e+02 - logprior: -6.2805e-02
Epoch 3/10
34/34 - 20s - loss: 569.8989 - loglik: -5.6999e+02 - logprior: 0.0888
Epoch 4/10
34/34 - 20s - loss: 570.2814 - loglik: -5.7051e+02 - logprior: 0.2264
Fitted a model with MAP estimate = -567.4067
Time for alignment: 360.9636
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 781.6216 - loglik: -7.7950e+02 - logprior: -2.1230e+00
Epoch 2/10
34/34 - 14s - loss: 643.4913 - loglik: -6.4158e+02 - logprior: -1.9077e+00
Epoch 3/10
34/34 - 14s - loss: 622.6143 - loglik: -6.2045e+02 - logprior: -2.1676e+00
Epoch 4/10
34/34 - 14s - loss: 618.8431 - loglik: -6.1673e+02 - logprior: -2.1094e+00
Epoch 5/10
34/34 - 14s - loss: 617.1382 - loglik: -6.1503e+02 - logprior: -2.1077e+00
Epoch 6/10
34/34 - 14s - loss: 614.8654 - loglik: -6.1277e+02 - logprior: -2.0995e+00
Epoch 7/10
34/34 - 14s - loss: 615.1701 - loglik: -6.1308e+02 - logprior: -2.0889e+00
Fitted a model with MAP estimate = -614.1541
expansions: [(9, 2), (12, 1), (14, 1), (19, 2), (23, 3), (29, 1), (30, 2), (34, 1), (47, 4), (48, 2), (49, 3), (54, 1), (65, 1), (66, 3), (67, 1), (94, 1), (95, 1), (98, 1), (99, 1), (100, 2), (102, 1), (104, 1), (110, 1), (130, 1), (131, 1), (134, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 1), (173, 1), (174, 1), (176, 1), (177, 1), (186, 1), (187, 1), (190, 1), (193, 1), (194, 1), (196, 1), (198, 1), (201, 1), (203, 1), (204, 1), (205, 1), (208, 1), (211, 1), (225, 1), (227, 1), (228, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 598.5630 - loglik: -5.9513e+02 - logprior: -3.4341e+00
Epoch 2/2
34/34 - 20s - loss: 576.3455 - loglik: -5.7485e+02 - logprior: -1.4988e+00
Fitted a model with MAP estimate = -572.3296
expansions: [(23, 1), (62, 1), (63, 1)]
discards: [ 8 29 38 64 89 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 580.4872 - loglik: -5.7838e+02 - logprior: -2.1064e+00
Epoch 2/2
34/34 - 20s - loss: 575.0415 - loglik: -5.7470e+02 - logprior: -3.4308e-01
Fitted a model with MAP estimate = -573.2776
expansions: [(184, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 301 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 577.7232 - loglik: -5.7584e+02 - logprior: -1.8836e+00
Epoch 2/10
34/34 - 20s - loss: 574.4043 - loglik: -5.7428e+02 - logprior: -1.2322e-01
Epoch 3/10
34/34 - 20s - loss: 572.0281 - loglik: -5.7207e+02 - logprior: 0.0406
Epoch 4/10
34/34 - 20s - loss: 572.7256 - loglik: -5.7292e+02 - logprior: 0.1947
Fitted a model with MAP estimate = -570.2793
Time for alignment: 367.9987
Computed alignments with likelihoods: ['-571.5129', '-566.3277', '-570.5501', '-567.4067', '-570.2793']
Best model has likelihood: -566.3277  (prior= 0.5209 )
time for generating output: 0.3572
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.7088888888888889
Training of 5 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0700c2b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2023cff4c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f9309eee0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f911f49d0>
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 276.2305 - loglik: -1.8808e+02 - logprior: -8.8148e+01
Epoch 2/10
10/10 - 1s - loss: 189.7402 - loglik: -1.6618e+02 - logprior: -2.3562e+01
Epoch 3/10
10/10 - 1s - loss: 160.0818 - loglik: -1.4915e+02 - logprior: -1.0931e+01
Epoch 4/10
10/10 - 1s - loss: 143.4211 - loglik: -1.3700e+02 - logprior: -6.4183e+00
Epoch 5/10
10/10 - 1s - loss: 133.3760 - loglik: -1.2920e+02 - logprior: -4.1810e+00
Epoch 6/10
10/10 - 1s - loss: 128.6876 - loglik: -1.2587e+02 - logprior: -2.8216e+00
Epoch 7/10
10/10 - 1s - loss: 126.8742 - loglik: -1.2491e+02 - logprior: -1.9689e+00
Epoch 8/10
10/10 - 1s - loss: 125.8966 - loglik: -1.2446e+02 - logprior: -1.4357e+00
Epoch 9/10
10/10 - 1s - loss: 125.3276 - loglik: -1.2427e+02 - logprior: -1.0618e+00
Epoch 10/10
10/10 - 1s - loss: 124.9304 - loglik: -1.2410e+02 - logprior: -8.2812e-01
Fitted a model with MAP estimate = -124.7534
expansions: [(7, 1), (8, 3), (9, 2), (16, 1), (27, 2), (29, 2), (41, 2), (43, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 227.7801 - loglik: -1.2887e+02 - logprior: -9.8908e+01
Epoch 2/2
10/10 - 1s - loss: 160.5955 - loglik: -1.1981e+02 - logprior: -4.0787e+01
Fitted a model with MAP estimate = -149.1288
expansions: [(0, 2), (51, 1)]
discards: [ 0 33 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.4038 - loglik: -1.1639e+02 - logprior: -7.9017e+01
Epoch 2/2
10/10 - 1s - loss: 133.6915 - loglik: -1.1303e+02 - logprior: -2.0661e+01
Fitted a model with MAP estimate = -124.4097
expansions: [(10, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 211.1311 - loglik: -1.1579e+02 - logprior: -9.5343e+01
Epoch 2/10
10/10 - 1s - loss: 145.5804 - loglik: -1.1425e+02 - logprior: -3.1326e+01
Epoch 3/10
10/10 - 1s - loss: 124.8033 - loglik: -1.1353e+02 - logprior: -1.1276e+01
Epoch 4/10
10/10 - 1s - loss: 116.7914 - loglik: -1.1286e+02 - logprior: -3.9333e+00
Epoch 5/10
10/10 - 1s - loss: 113.2829 - loglik: -1.1238e+02 - logprior: -8.9898e-01
Epoch 6/10
10/10 - 1s - loss: 111.4861 - loglik: -1.1215e+02 - logprior: 0.6623
Epoch 7/10
10/10 - 1s - loss: 110.4454 - loglik: -1.1208e+02 - logprior: 1.6396
Epoch 8/10
10/10 - 1s - loss: 109.7319 - loglik: -1.1212e+02 - logprior: 2.3869
Epoch 9/10
10/10 - 1s - loss: 109.1915 - loglik: -1.1216e+02 - logprior: 2.9667
Epoch 10/10
10/10 - 1s - loss: 108.7496 - loglik: -1.1216e+02 - logprior: 3.4095
Fitted a model with MAP estimate = -108.5299
Time for alignment: 30.8241
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 276.2305 - loglik: -1.8808e+02 - logprior: -8.8148e+01
Epoch 2/10
10/10 - 1s - loss: 189.7402 - loglik: -1.6618e+02 - logprior: -2.3562e+01
Epoch 3/10
10/10 - 1s - loss: 160.0816 - loglik: -1.4915e+02 - logprior: -1.0931e+01
Epoch 4/10
10/10 - 1s - loss: 143.4204 - loglik: -1.3700e+02 - logprior: -6.4183e+00
Epoch 5/10
10/10 - 1s - loss: 133.3753 - loglik: -1.2919e+02 - logprior: -4.1810e+00
Epoch 6/10
10/10 - 1s - loss: 128.6874 - loglik: -1.2587e+02 - logprior: -2.8216e+00
Epoch 7/10
10/10 - 1s - loss: 126.8741 - loglik: -1.2491e+02 - logprior: -1.9689e+00
Epoch 8/10
10/10 - 1s - loss: 125.8966 - loglik: -1.2446e+02 - logprior: -1.4357e+00
Epoch 9/10
10/10 - 1s - loss: 125.3275 - loglik: -1.2427e+02 - logprior: -1.0618e+00
Epoch 10/10
10/10 - 1s - loss: 124.9303 - loglik: -1.2410e+02 - logprior: -8.2810e-01
Fitted a model with MAP estimate = -124.7535
expansions: [(7, 1), (8, 3), (9, 2), (16, 1), (27, 2), (29, 2), (41, 2), (43, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.7800 - loglik: -1.2887e+02 - logprior: -9.8908e+01
Epoch 2/2
10/10 - 1s - loss: 160.5954 - loglik: -1.1981e+02 - logprior: -4.0787e+01
Fitted a model with MAP estimate = -149.1285
expansions: [(0, 2), (51, 1)]
discards: [ 0 33 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.4034 - loglik: -1.1639e+02 - logprior: -7.9017e+01
Epoch 2/2
10/10 - 1s - loss: 133.6912 - loglik: -1.1303e+02 - logprior: -2.0661e+01
Fitted a model with MAP estimate = -124.4094
expansions: [(10, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 211.1311 - loglik: -1.1579e+02 - logprior: -9.5343e+01
Epoch 2/10
10/10 - 1s - loss: 145.5789 - loglik: -1.1425e+02 - logprior: -3.1326e+01
Epoch 3/10
10/10 - 1s - loss: 124.8021 - loglik: -1.1353e+02 - logprior: -1.1276e+01
Epoch 4/10
10/10 - 1s - loss: 116.7903 - loglik: -1.1286e+02 - logprior: -3.9333e+00
Epoch 5/10
10/10 - 1s - loss: 113.2825 - loglik: -1.1238e+02 - logprior: -8.9884e-01
Epoch 6/10
10/10 - 1s - loss: 111.4859 - loglik: -1.1215e+02 - logprior: 0.6625
Epoch 7/10
10/10 - 1s - loss: 110.4454 - loglik: -1.1208e+02 - logprior: 1.6394
Epoch 8/10
10/10 - 1s - loss: 109.7320 - loglik: -1.1212e+02 - logprior: 2.3869
Epoch 9/10
10/10 - 1s - loss: 109.1917 - loglik: -1.1216e+02 - logprior: 2.9665
Epoch 10/10
10/10 - 1s - loss: 108.7499 - loglik: -1.1216e+02 - logprior: 3.4092
Fitted a model with MAP estimate = -108.5305
Time for alignment: 30.8644
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 276.2304 - loglik: -1.8808e+02 - logprior: -8.8148e+01
Epoch 2/10
10/10 - 1s - loss: 189.7399 - loglik: -1.6618e+02 - logprior: -2.3562e+01
Epoch 3/10
10/10 - 1s - loss: 160.0806 - loglik: -1.4915e+02 - logprior: -1.0931e+01
Epoch 4/10
10/10 - 1s - loss: 143.4148 - loglik: -1.3700e+02 - logprior: -6.4186e+00
Epoch 5/10
10/10 - 1s - loss: 133.3728 - loglik: -1.2919e+02 - logprior: -4.1812e+00
Epoch 6/10
10/10 - 1s - loss: 128.6868 - loglik: -1.2587e+02 - logprior: -2.8217e+00
Epoch 7/10
10/10 - 1s - loss: 126.8739 - loglik: -1.2491e+02 - logprior: -1.9688e+00
Epoch 8/10
10/10 - 1s - loss: 125.8964 - loglik: -1.2446e+02 - logprior: -1.4357e+00
Epoch 9/10
10/10 - 1s - loss: 125.3276 - loglik: -1.2427e+02 - logprior: -1.0618e+00
Epoch 10/10
10/10 - 1s - loss: 124.9303 - loglik: -1.2410e+02 - logprior: -8.2802e-01
Fitted a model with MAP estimate = -124.7535
expansions: [(7, 1), (8, 3), (9, 2), (16, 1), (27, 2), (29, 2), (41, 2), (43, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 227.7797 - loglik: -1.2887e+02 - logprior: -9.8908e+01
Epoch 2/2
10/10 - 1s - loss: 160.5952 - loglik: -1.1981e+02 - logprior: -4.0788e+01
Fitted a model with MAP estimate = -149.1282
expansions: [(0, 2), (51, 1)]
discards: [ 0 33 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.4019 - loglik: -1.1638e+02 - logprior: -7.9017e+01
Epoch 2/2
10/10 - 1s - loss: 133.6901 - loglik: -1.1303e+02 - logprior: -2.0661e+01
Fitted a model with MAP estimate = -124.4088
expansions: [(10, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 211.1303 - loglik: -1.1579e+02 - logprior: -9.5343e+01
Epoch 2/10
10/10 - 1s - loss: 145.5780 - loglik: -1.1425e+02 - logprior: -3.1326e+01
Epoch 3/10
10/10 - 1s - loss: 124.8015 - loglik: -1.1353e+02 - logprior: -1.1276e+01
Epoch 4/10
10/10 - 1s - loss: 116.7898 - loglik: -1.1286e+02 - logprior: -3.9332e+00
Epoch 5/10
10/10 - 1s - loss: 113.2824 - loglik: -1.1238e+02 - logprior: -8.9878e-01
Epoch 6/10
10/10 - 1s - loss: 111.4858 - loglik: -1.1215e+02 - logprior: 0.6626
Epoch 7/10
10/10 - 1s - loss: 110.4453 - loglik: -1.1208e+02 - logprior: 1.6395
Epoch 8/10
10/10 - 1s - loss: 109.7321 - loglik: -1.1212e+02 - logprior: 2.3869
Epoch 9/10
10/10 - 1s - loss: 109.1919 - loglik: -1.1216e+02 - logprior: 2.9666
Epoch 10/10
10/10 - 1s - loss: 108.7502 - loglik: -1.1216e+02 - logprior: 3.4093
Fitted a model with MAP estimate = -108.5307
Time for alignment: 29.1349
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 276.2304 - loglik: -1.8808e+02 - logprior: -8.8148e+01
Epoch 2/10
10/10 - 1s - loss: 189.7399 - loglik: -1.6618e+02 - logprior: -2.3562e+01
Epoch 3/10
10/10 - 1s - loss: 160.0815 - loglik: -1.4915e+02 - logprior: -1.0931e+01
Epoch 4/10
10/10 - 1s - loss: 143.4201 - loglik: -1.3700e+02 - logprior: -6.4183e+00
Epoch 5/10
10/10 - 1s - loss: 133.3756 - loglik: -1.2919e+02 - logprior: -4.1809e+00
Epoch 6/10
10/10 - 1s - loss: 128.6875 - loglik: -1.2587e+02 - logprior: -2.8216e+00
Epoch 7/10
10/10 - 1s - loss: 126.8741 - loglik: -1.2491e+02 - logprior: -1.9689e+00
Epoch 8/10
10/10 - 1s - loss: 125.8966 - loglik: -1.2446e+02 - logprior: -1.4358e+00
Epoch 9/10
10/10 - 1s - loss: 125.3275 - loglik: -1.2427e+02 - logprior: -1.0618e+00
Epoch 10/10
10/10 - 1s - loss: 124.9303 - loglik: -1.2410e+02 - logprior: -8.2809e-01
Fitted a model with MAP estimate = -124.7534
expansions: [(7, 1), (8, 3), (9, 2), (16, 1), (27, 2), (29, 2), (41, 2), (43, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.7800 - loglik: -1.2887e+02 - logprior: -9.8908e+01
Epoch 2/2
10/10 - 1s - loss: 160.5954 - loglik: -1.1981e+02 - logprior: -4.0787e+01
Fitted a model with MAP estimate = -149.1286
expansions: [(0, 2), (51, 1)]
discards: [ 0 33 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.4037 - loglik: -1.1639e+02 - logprior: -7.9017e+01
Epoch 2/2
10/10 - 1s - loss: 133.6914 - loglik: -1.1303e+02 - logprior: -2.0661e+01
Fitted a model with MAP estimate = -124.4097
expansions: [(10, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 211.1306 - loglik: -1.1579e+02 - logprior: -9.5343e+01
Epoch 2/10
10/10 - 1s - loss: 145.5799 - loglik: -1.1425e+02 - logprior: -3.1326e+01
Epoch 3/10
10/10 - 1s - loss: 124.8029 - loglik: -1.1353e+02 - logprior: -1.1276e+01
Epoch 4/10
10/10 - 1s - loss: 116.7909 - loglik: -1.1286e+02 - logprior: -3.9332e+00
Epoch 5/10
10/10 - 1s - loss: 113.2827 - loglik: -1.1238e+02 - logprior: -8.9892e-01
Epoch 6/10
10/10 - 1s - loss: 111.4861 - loglik: -1.1215e+02 - logprior: 0.6625
Epoch 7/10
10/10 - 1s - loss: 110.4452 - loglik: -1.1208e+02 - logprior: 1.6397
Epoch 8/10
10/10 - 1s - loss: 109.7316 - loglik: -1.1212e+02 - logprior: 2.3871
Epoch 9/10
10/10 - 1s - loss: 109.1912 - loglik: -1.1216e+02 - logprior: 2.9670
Epoch 10/10
10/10 - 1s - loss: 108.7491 - loglik: -1.1216e+02 - logprior: 3.4099
Fitted a model with MAP estimate = -108.5296
Time for alignment: 29.6402
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 276.2306 - loglik: -1.8808e+02 - logprior: -8.8148e+01
Epoch 2/10
10/10 - 1s - loss: 189.7405 - loglik: -1.6618e+02 - logprior: -2.3562e+01
Epoch 3/10
10/10 - 1s - loss: 160.0817 - loglik: -1.4915e+02 - logprior: -1.0931e+01
Epoch 4/10
10/10 - 1s - loss: 143.4198 - loglik: -1.3700e+02 - logprior: -6.4183e+00
Epoch 5/10
10/10 - 1s - loss: 133.3751 - loglik: -1.2919e+02 - logprior: -4.1810e+00
Epoch 6/10
10/10 - 1s - loss: 128.6874 - loglik: -1.2587e+02 - logprior: -2.8216e+00
Epoch 7/10
10/10 - 1s - loss: 126.8742 - loglik: -1.2491e+02 - logprior: -1.9689e+00
Epoch 8/10
10/10 - 1s - loss: 125.8966 - loglik: -1.2446e+02 - logprior: -1.4357e+00
Epoch 9/10
10/10 - 1s - loss: 125.3276 - loglik: -1.2427e+02 - logprior: -1.0618e+00
Epoch 10/10
10/10 - 1s - loss: 124.9304 - loglik: -1.2410e+02 - logprior: -8.2810e-01
Fitted a model with MAP estimate = -124.7535
expansions: [(7, 1), (8, 3), (9, 2), (16, 1), (27, 2), (29, 2), (41, 2), (43, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 227.7805 - loglik: -1.2887e+02 - logprior: -9.8908e+01
Epoch 2/2
10/10 - 1s - loss: 160.5956 - loglik: -1.1981e+02 - logprior: -4.0787e+01
Fitted a model with MAP estimate = -149.1288
expansions: [(0, 2), (51, 1)]
discards: [ 0 33 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.4042 - loglik: -1.1639e+02 - logprior: -7.9017e+01
Epoch 2/2
10/10 - 1s - loss: 133.6917 - loglik: -1.1303e+02 - logprior: -2.0661e+01
Fitted a model with MAP estimate = -124.4098
expansions: [(10, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 211.1311 - loglik: -1.1579e+02 - logprior: -9.5343e+01
Epoch 2/10
10/10 - 1s - loss: 145.5805 - loglik: -1.1425e+02 - logprior: -3.1326e+01
Epoch 3/10
10/10 - 1s - loss: 124.8033 - loglik: -1.1353e+02 - logprior: -1.1276e+01
Epoch 4/10
10/10 - 1s - loss: 116.7915 - loglik: -1.1286e+02 - logprior: -3.9333e+00
Epoch 5/10
10/10 - 1s - loss: 113.2830 - loglik: -1.1238e+02 - logprior: -8.9894e-01
Epoch 6/10
10/10 - 1s - loss: 111.4862 - loglik: -1.1215e+02 - logprior: 0.6623
Epoch 7/10
10/10 - 1s - loss: 110.4454 - loglik: -1.1208e+02 - logprior: 1.6395
Epoch 8/10
10/10 - 1s - loss: 109.7319 - loglik: -1.1212e+02 - logprior: 2.3869
Epoch 9/10
10/10 - 1s - loss: 109.1915 - loglik: -1.1216e+02 - logprior: 2.9666
Epoch 10/10
10/10 - 1s - loss: 108.7496 - loglik: -1.1216e+02 - logprior: 3.4095
Fitted a model with MAP estimate = -108.5302
Time for alignment: 28.6957
Computed alignments with likelihoods: ['-108.5299', '-108.5305', '-108.5307', '-108.5296', '-108.5302']
Best model has likelihood: -108.5296  (prior= 3.6114 )
time for generating output: 0.0965
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.8429143169001273
Training of 5 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fb4db00d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2023fa0430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf1bf5e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1efe9373a0>
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 321.5834 - loglik: -3.1391e+02 - logprior: -7.6704e+00
Epoch 2/10
13/13 - 1s - loss: 291.5602 - loglik: -2.8973e+02 - logprior: -1.8352e+00
Epoch 3/10
13/13 - 1s - loss: 265.2784 - loglik: -2.6369e+02 - logprior: -1.5892e+00
Epoch 4/10
13/13 - 1s - loss: 255.4849 - loglik: -2.5369e+02 - logprior: -1.7951e+00
Epoch 5/10
13/13 - 1s - loss: 251.5763 - loglik: -2.4983e+02 - logprior: -1.7482e+00
Epoch 6/10
13/13 - 1s - loss: 250.3950 - loglik: -2.4870e+02 - logprior: -1.6932e+00
Epoch 7/10
13/13 - 1s - loss: 249.7440 - loglik: -2.4804e+02 - logprior: -1.7009e+00
Epoch 8/10
13/13 - 1s - loss: 249.9245 - loglik: -2.4822e+02 - logprior: -1.7061e+00
Fitted a model with MAP estimate = -249.3665
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (29, 1), (30, 2), (31, 3), (32, 1), (40, 1), (41, 1), (51, 1), (52, 2), (63, 3), (64, 1), (65, 1), (67, 1), (68, 3), (69, 1), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 260.1180 - loglik: -2.5099e+02 - logprior: -9.1260e+00
Epoch 2/2
13/13 - 2s - loss: 249.2191 - loglik: -2.4514e+02 - logprior: -4.0755e+00
Fitted a model with MAP estimate = -247.3081
expansions: [(0, 2)]
discards: [ 0 67 81 95 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 251.0838 - loglik: -2.4416e+02 - logprior: -6.9205e+00
Epoch 2/2
13/13 - 2s - loss: 245.2832 - loglik: -2.4336e+02 - logprior: -1.9260e+00
Fitted a model with MAP estimate = -243.9872
expansions: []
discards: [ 0 15 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 253.7021 - loglik: -2.4504e+02 - logprior: -8.6579e+00
Epoch 2/10
13/13 - 2s - loss: 246.3613 - loglik: -2.4376e+02 - logprior: -2.6044e+00
Epoch 3/10
13/13 - 1s - loss: 244.1855 - loglik: -2.4288e+02 - logprior: -1.3024e+00
Epoch 4/10
13/13 - 1s - loss: 243.4991 - loglik: -2.4252e+02 - logprior: -9.8008e-01
Epoch 5/10
13/13 - 1s - loss: 243.6620 - loglik: -2.4276e+02 - logprior: -9.0456e-01
Fitted a model with MAP estimate = -243.0971
Time for alignment: 46.9038
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 321.1264 - loglik: -3.1346e+02 - logprior: -7.6710e+00
Epoch 2/10
13/13 - 1s - loss: 291.7227 - loglik: -2.8989e+02 - logprior: -1.8308e+00
Epoch 3/10
13/13 - 1s - loss: 266.7740 - loglik: -2.6522e+02 - logprior: -1.5571e+00
Epoch 4/10
13/13 - 1s - loss: 255.7514 - loglik: -2.5395e+02 - logprior: -1.7992e+00
Epoch 5/10
13/13 - 1s - loss: 251.6944 - loglik: -2.4994e+02 - logprior: -1.7547e+00
Epoch 6/10
13/13 - 1s - loss: 250.6935 - loglik: -2.4901e+02 - logprior: -1.6807e+00
Epoch 7/10
13/13 - 1s - loss: 250.2315 - loglik: -2.4854e+02 - logprior: -1.6940e+00
Epoch 8/10
13/13 - 1s - loss: 250.0256 - loglik: -2.4833e+02 - logprior: -1.6960e+00
Epoch 9/10
13/13 - 1s - loss: 249.7524 - loglik: -2.4809e+02 - logprior: -1.6626e+00
Epoch 10/10
13/13 - 1s - loss: 249.6307 - loglik: -2.4798e+02 - logprior: -1.6529e+00
Fitted a model with MAP estimate = -249.4977
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (29, 1), (30, 2), (31, 3), (32, 1), (41, 1), (52, 1), (53, 2), (64, 4), (65, 2), (67, 1), (68, 3), (69, 1), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 260.6069 - loglik: -2.5147e+02 - logprior: -9.1359e+00
Epoch 2/2
13/13 - 2s - loss: 248.9687 - loglik: -2.4487e+02 - logprior: -4.0987e+00
Fitted a model with MAP estimate = -247.2956
expansions: [(0, 2)]
discards: [ 0 14 67 85 95 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 250.9110 - loglik: -2.4404e+02 - logprior: -6.8680e+00
Epoch 2/2
13/13 - 2s - loss: 244.8634 - loglik: -2.4300e+02 - logprior: -1.8673e+00
Fitted a model with MAP estimate = -243.9232
expansions: []
discards: [ 0 37 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 253.5115 - loglik: -2.4484e+02 - logprior: -8.6669e+00
Epoch 2/10
13/13 - 1s - loss: 246.2635 - loglik: -2.4366e+02 - logprior: -2.6045e+00
Epoch 3/10
13/13 - 1s - loss: 244.7120 - loglik: -2.4343e+02 - logprior: -1.2867e+00
Epoch 4/10
13/13 - 1s - loss: 243.6825 - loglik: -2.4270e+02 - logprior: -9.8257e-01
Epoch 5/10
13/13 - 1s - loss: 243.2149 - loglik: -2.4232e+02 - logprior: -8.9162e-01
Epoch 6/10
13/13 - 1s - loss: 243.2368 - loglik: -2.4239e+02 - logprior: -8.4773e-01
Fitted a model with MAP estimate = -242.9587
Time for alignment: 50.3855
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 321.5671 - loglik: -3.1390e+02 - logprior: -7.6708e+00
Epoch 2/10
13/13 - 1s - loss: 291.1730 - loglik: -2.8934e+02 - logprior: -1.8309e+00
Epoch 3/10
13/13 - 1s - loss: 267.5956 - loglik: -2.6599e+02 - logprior: -1.6034e+00
Epoch 4/10
13/13 - 1s - loss: 256.1520 - loglik: -2.5429e+02 - logprior: -1.8610e+00
Epoch 5/10
13/13 - 1s - loss: 252.5029 - loglik: -2.5066e+02 - logprior: -1.8429e+00
Epoch 6/10
13/13 - 1s - loss: 251.0241 - loglik: -2.4927e+02 - logprior: -1.7549e+00
Epoch 7/10
13/13 - 1s - loss: 250.5067 - loglik: -2.4875e+02 - logprior: -1.7591e+00
Epoch 8/10
13/13 - 1s - loss: 250.1304 - loglik: -2.4838e+02 - logprior: -1.7527e+00
Epoch 9/10
13/13 - 1s - loss: 249.8844 - loglik: -2.4816e+02 - logprior: -1.7230e+00
Epoch 10/10
13/13 - 1s - loss: 249.8904 - loglik: -2.4818e+02 - logprior: -1.7118e+00
Fitted a model with MAP estimate = -249.6911
expansions: [(9, 1), (10, 1), (13, 1), (18, 1), (19, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1), (40, 1), (45, 1), (51, 2), (63, 3), (64, 1), (65, 1), (67, 1), (68, 3), (69, 1), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 259.7903 - loglik: -2.5068e+02 - logprior: -9.1063e+00
Epoch 2/2
13/13 - 2s - loss: 249.2110 - loglik: -2.4518e+02 - logprior: -4.0343e+00
Fitted a model with MAP estimate = -247.1692
expansions: [(0, 2)]
discards: [ 0 65 79 93 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 250.9875 - loglik: -2.4412e+02 - logprior: -6.8626e+00
Epoch 2/2
13/13 - 1s - loss: 244.8962 - loglik: -2.4302e+02 - logprior: -1.8795e+00
Fitted a model with MAP estimate = -243.9019
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 253.2449 - loglik: -2.4459e+02 - logprior: -8.6598e+00
Epoch 2/10
13/13 - 1s - loss: 246.1962 - loglik: -2.4361e+02 - logprior: -2.5876e+00
Epoch 3/10
13/13 - 2s - loss: 244.3930 - loglik: -2.4312e+02 - logprior: -1.2694e+00
Epoch 4/10
13/13 - 1s - loss: 243.8972 - loglik: -2.4293e+02 - logprior: -9.6917e-01
Epoch 5/10
13/13 - 1s - loss: 242.8831 - loglik: -2.4201e+02 - logprior: -8.7163e-01
Epoch 6/10
13/13 - 2s - loss: 243.1978 - loglik: -2.4237e+02 - logprior: -8.3049e-01
Fitted a model with MAP estimate = -242.8847
Time for alignment: 50.8078
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 321.5463 - loglik: -3.1387e+02 - logprior: -7.6721e+00
Epoch 2/10
13/13 - 1s - loss: 291.2206 - loglik: -2.8939e+02 - logprior: -1.8308e+00
Epoch 3/10
13/13 - 1s - loss: 268.0500 - loglik: -2.6645e+02 - logprior: -1.5959e+00
Epoch 4/10
13/13 - 1s - loss: 256.0831 - loglik: -2.5423e+02 - logprior: -1.8568e+00
Epoch 5/10
13/13 - 1s - loss: 251.9800 - loglik: -2.5013e+02 - logprior: -1.8471e+00
Epoch 6/10
13/13 - 1s - loss: 251.0322 - loglik: -2.4927e+02 - logprior: -1.7601e+00
Epoch 7/10
13/13 - 1s - loss: 250.1048 - loglik: -2.4835e+02 - logprior: -1.7576e+00
Epoch 8/10
13/13 - 1s - loss: 250.5243 - loglik: -2.4877e+02 - logprior: -1.7544e+00
Fitted a model with MAP estimate = -249.9595
expansions: [(9, 1), (10, 1), (13, 2), (18, 1), (19, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1), (40, 1), (45, 1), (51, 2), (63, 2), (64, 1), (65, 1), (67, 1), (68, 3), (69, 1), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 260.7169 - loglik: -2.5162e+02 - logprior: -9.0982e+00
Epoch 2/2
13/13 - 2s - loss: 249.3668 - loglik: -2.4531e+02 - logprior: -4.0533e+00
Fitted a model with MAP estimate = -247.2822
expansions: [(0, 2)]
discards: [ 0 14 66 93 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 251.3020 - loglik: -2.4442e+02 - logprior: -6.8808e+00
Epoch 2/2
13/13 - 2s - loss: 244.5918 - loglik: -2.4271e+02 - logprior: -1.8840e+00
Fitted a model with MAP estimate = -243.9743
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 253.3489 - loglik: -2.4469e+02 - logprior: -8.6612e+00
Epoch 2/10
13/13 - 2s - loss: 246.2552 - loglik: -2.4365e+02 - logprior: -2.6005e+00
Epoch 3/10
13/13 - 1s - loss: 244.3271 - loglik: -2.4304e+02 - logprior: -1.2844e+00
Epoch 4/10
13/13 - 1s - loss: 243.7995 - loglik: -2.4283e+02 - logprior: -9.6601e-01
Epoch 5/10
13/13 - 1s - loss: 243.4645 - loglik: -2.4259e+02 - logprior: -8.7713e-01
Epoch 6/10
13/13 - 1s - loss: 243.1146 - loglik: -2.4229e+02 - logprior: -8.2324e-01
Epoch 7/10
13/13 - 1s - loss: 243.2162 - loglik: -2.4241e+02 - logprior: -8.0774e-01
Fitted a model with MAP estimate = -242.8343
Time for alignment: 48.5246
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 321.4232 - loglik: -3.1375e+02 - logprior: -7.6724e+00
Epoch 2/10
13/13 - 1s - loss: 291.0076 - loglik: -2.8917e+02 - logprior: -1.8365e+00
Epoch 3/10
13/13 - 1s - loss: 263.9225 - loglik: -2.6235e+02 - logprior: -1.5772e+00
Epoch 4/10
13/13 - 1s - loss: 253.7014 - loglik: -2.5190e+02 - logprior: -1.8000e+00
Epoch 5/10
13/13 - 1s - loss: 252.0874 - loglik: -2.5036e+02 - logprior: -1.7308e+00
Epoch 6/10
13/13 - 1s - loss: 250.4910 - loglik: -2.4882e+02 - logprior: -1.6731e+00
Epoch 7/10
13/13 - 1s - loss: 249.3535 - loglik: -2.4766e+02 - logprior: -1.6923e+00
Epoch 8/10
13/13 - 1s - loss: 249.3434 - loglik: -2.4763e+02 - logprior: -1.7113e+00
Epoch 9/10
13/13 - 1s - loss: 249.2345 - loglik: -2.4755e+02 - logprior: -1.6845e+00
Epoch 10/10
13/13 - 1s - loss: 249.0114 - loglik: -2.4734e+02 - logprior: -1.6751e+00
Fitted a model with MAP estimate = -248.9050
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (29, 1), (30, 1), (31, 3), (32, 1), (40, 1), (41, 1), (51, 1), (52, 2), (63, 3), (64, 1), (68, 1), (69, 5), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 259.6711 - loglik: -2.5056e+02 - logprior: -9.1136e+00
Epoch 2/2
13/13 - 2s - loss: 248.8100 - loglik: -2.4470e+02 - logprior: -4.1113e+00
Fitted a model with MAP estimate = -247.2456
expansions: [(0, 2)]
discards: [ 0 14 66 80 96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 251.2254 - loglik: -2.4436e+02 - logprior: -6.8624e+00
Epoch 2/2
13/13 - 2s - loss: 244.5494 - loglik: -2.4268e+02 - logprior: -1.8709e+00
Fitted a model with MAP estimate = -243.8650
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 253.3845 - loglik: -2.4472e+02 - logprior: -8.6665e+00
Epoch 2/10
13/13 - 1s - loss: 246.0286 - loglik: -2.4342e+02 - logprior: -2.6087e+00
Epoch 3/10
13/13 - 1s - loss: 244.3379 - loglik: -2.4307e+02 - logprior: -1.2665e+00
Epoch 4/10
13/13 - 1s - loss: 243.5983 - loglik: -2.4264e+02 - logprior: -9.6328e-01
Epoch 5/10
13/13 - 1s - loss: 243.2067 - loglik: -2.4234e+02 - logprior: -8.6355e-01
Epoch 6/10
13/13 - 2s - loss: 243.2684 - loglik: -2.4244e+02 - logprior: -8.3055e-01
Fitted a model with MAP estimate = -242.8376
Time for alignment: 50.7267
Computed alignments with likelihoods: ['-243.0971', '-242.9587', '-242.8847', '-242.8343', '-242.8376']
Best model has likelihood: -242.8343  (prior= -0.8274 )
time for generating output: 0.1804
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.9405216284987278
Training of 5 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa36d59d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff91a2430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f93068fa0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f200a59c820>
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 444.1493 - loglik: -3.5874e+02 - logprior: -8.5406e+01
Epoch 2/10
10/10 - 1s - loss: 340.5244 - loglik: -3.2094e+02 - logprior: -1.9583e+01
Epoch 3/10
10/10 - 1s - loss: 287.4653 - loglik: -2.7981e+02 - logprior: -7.6516e+00
Epoch 4/10
10/10 - 1s - loss: 254.4738 - loglik: -2.5051e+02 - logprior: -3.9648e+00
Epoch 5/10
10/10 - 1s - loss: 238.1335 - loglik: -2.3629e+02 - logprior: -1.8446e+00
Epoch 6/10
10/10 - 1s - loss: 231.0253 - loglik: -2.3046e+02 - logprior: -5.6397e-01
Epoch 7/10
10/10 - 1s - loss: 227.6534 - loglik: -2.2803e+02 - logprior: 0.3744
Epoch 8/10
10/10 - 1s - loss: 225.6324 - loglik: -2.2661e+02 - logprior: 0.9788
Epoch 9/10
10/10 - 1s - loss: 224.2593 - loglik: -2.2567e+02 - logprior: 1.4156
Epoch 10/10
10/10 - 1s - loss: 223.2004 - loglik: -2.2498e+02 - logprior: 1.7768
Fitted a model with MAP estimate = -222.7065
expansions: [(13, 3), (17, 1), (29, 1), (40, 2), (41, 2), (47, 3), (61, 3), (62, 1), (79, 2), (80, 2), (81, 2), (88, 5), (89, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 326.7149 - loglik: -2.3068e+02 - logprior: -9.6036e+01
Epoch 2/2
10/10 - 1s - loss: 245.2011 - loglik: -2.0823e+02 - logprior: -3.6974e+01
Fitted a model with MAP estimate = -230.3774
expansions: [(0, 2), (16, 1)]
discards: [  0  44  94  97 111 112 115]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 279.1905 - loglik: -2.0422e+02 - logprior: -7.4969e+01
Epoch 2/2
10/10 - 1s - loss: 213.3538 - loglik: -1.9768e+02 - logprior: -1.5677e+01
Fitted a model with MAP estimate = -203.4558
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 289.8434 - loglik: -2.0011e+02 - logprior: -8.9729e+01
Epoch 2/10
10/10 - 1s - loss: 220.5197 - loglik: -1.9802e+02 - logprior: -2.2499e+01
Epoch 3/10
10/10 - 1s - loss: 201.4387 - loglik: -1.9729e+02 - logprior: -4.1471e+00
Epoch 4/10
10/10 - 1s - loss: 194.2821 - loglik: -1.9658e+02 - logprior: 2.2946
Epoch 5/10
10/10 - 1s - loss: 190.1958 - loglik: -1.9550e+02 - logprior: 5.3018
Epoch 6/10
10/10 - 1s - loss: 187.5586 - loglik: -1.9444e+02 - logprior: 6.8782
Epoch 7/10
10/10 - 1s - loss: 185.5751 - loglik: -1.9351e+02 - logprior: 7.9363
Epoch 8/10
10/10 - 1s - loss: 184.1223 - loglik: -1.9293e+02 - logprior: 8.8078
Epoch 9/10
10/10 - 1s - loss: 183.0106 - loglik: -1.9260e+02 - logprior: 9.5872
Epoch 10/10
10/10 - 1s - loss: 182.0828 - loglik: -1.9233e+02 - logprior: 10.2469
Fitted a model with MAP estimate = -181.6075
Time for alignment: 44.5106
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 444.1491 - loglik: -3.5874e+02 - logprior: -8.5406e+01
Epoch 2/10
10/10 - 1s - loss: 340.5242 - loglik: -3.2094e+02 - logprior: -1.9583e+01
Epoch 3/10
10/10 - 1s - loss: 287.4675 - loglik: -2.7982e+02 - logprior: -7.6517e+00
Epoch 4/10
10/10 - 1s - loss: 254.4751 - loglik: -2.5051e+02 - logprior: -3.9652e+00
Epoch 5/10
10/10 - 1s - loss: 238.1161 - loglik: -2.3627e+02 - logprior: -1.8458e+00
Epoch 6/10
10/10 - 1s - loss: 231.0132 - loglik: -2.3044e+02 - logprior: -5.6891e-01
Epoch 7/10
10/10 - 1s - loss: 227.6421 - loglik: -2.2801e+02 - logprior: 0.3704
Epoch 8/10
10/10 - 1s - loss: 225.6294 - loglik: -2.2661e+02 - logprior: 0.9808
Epoch 9/10
10/10 - 1s - loss: 224.2598 - loglik: -2.2567e+02 - logprior: 1.4145
Epoch 10/10
10/10 - 1s - loss: 223.2012 - loglik: -2.2498e+02 - logprior: 1.7763
Fitted a model with MAP estimate = -222.7071
expansions: [(13, 3), (17, 1), (29, 1), (40, 2), (41, 2), (47, 3), (61, 3), (62, 1), (79, 2), (80, 2), (81, 2), (88, 5), (89, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 326.7202 - loglik: -2.3068e+02 - logprior: -9.6036e+01
Epoch 2/2
10/10 - 1s - loss: 245.2037 - loglik: -2.0823e+02 - logprior: -3.6974e+01
Fitted a model with MAP estimate = -230.3798
expansions: [(0, 2), (16, 1)]
discards: [  0  44  94  97 111 112 115]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 279.1788 - loglik: -2.0421e+02 - logprior: -7.4968e+01
Epoch 2/2
10/10 - 1s - loss: 213.3490 - loglik: -1.9767e+02 - logprior: -1.5677e+01
Fitted a model with MAP estimate = -203.4543
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 289.8402 - loglik: -2.0011e+02 - logprior: -8.9729e+01
Epoch 2/10
10/10 - 1s - loss: 220.5196 - loglik: -1.9802e+02 - logprior: -2.2499e+01
Epoch 3/10
10/10 - 1s - loss: 201.4387 - loglik: -1.9729e+02 - logprior: -4.1470e+00
Epoch 4/10
10/10 - 1s - loss: 194.2812 - loglik: -1.9658e+02 - logprior: 2.2947
Epoch 5/10
10/10 - 1s - loss: 190.1941 - loglik: -1.9549e+02 - logprior: 5.3006
Epoch 6/10
10/10 - 1s - loss: 187.5582 - loglik: -1.9444e+02 - logprior: 6.8775
Epoch 7/10
10/10 - 1s - loss: 185.5755 - loglik: -1.9351e+02 - logprior: 7.9351
Epoch 8/10
10/10 - 1s - loss: 184.1233 - loglik: -1.9293e+02 - logprior: 8.8066
Epoch 9/10
10/10 - 1s - loss: 183.0120 - loglik: -1.9260e+02 - logprior: 9.5860
Epoch 10/10
10/10 - 1s - loss: 182.0841 - loglik: -1.9233e+02 - logprior: 10.2450
Fitted a model with MAP estimate = -181.6082
Time for alignment: 44.5654
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 444.1492 - loglik: -3.5874e+02 - logprior: -8.5406e+01
Epoch 2/10
10/10 - 1s - loss: 340.5239 - loglik: -3.2094e+02 - logprior: -1.9583e+01
Epoch 3/10
10/10 - 1s - loss: 287.4651 - loglik: -2.7981e+02 - logprior: -7.6516e+00
Epoch 4/10
10/10 - 1s - loss: 254.4729 - loglik: -2.5051e+02 - logprior: -3.9648e+00
Epoch 5/10
10/10 - 1s - loss: 238.1248 - loglik: -2.3628e+02 - logprior: -1.8450e+00
Epoch 6/10
10/10 - 1s - loss: 231.0191 - loglik: -2.3045e+02 - logprior: -5.6631e-01
Epoch 7/10
10/10 - 1s - loss: 227.6475 - loglik: -2.2802e+02 - logprior: 0.3725
Epoch 8/10
10/10 - 1s - loss: 225.6304 - loglik: -2.2661e+02 - logprior: 0.9799
Epoch 9/10
10/10 - 1s - loss: 224.2594 - loglik: -2.2567e+02 - logprior: 1.4153
Epoch 10/10
10/10 - 1s - loss: 223.2008 - loglik: -2.2498e+02 - logprior: 1.7765
Fitted a model with MAP estimate = -222.7070
expansions: [(13, 3), (17, 1), (29, 1), (40, 2), (41, 2), (47, 3), (61, 3), (62, 1), (79, 2), (80, 2), (81, 2), (88, 5), (89, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 326.7178 - loglik: -2.3068e+02 - logprior: -9.6036e+01
Epoch 2/2
10/10 - 1s - loss: 245.2025 - loglik: -2.0823e+02 - logprior: -3.6974e+01
Fitted a model with MAP estimate = -230.3783
expansions: [(0, 2), (16, 1)]
discards: [  0  44  94  97 111 112 115]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 279.1857 - loglik: -2.0422e+02 - logprior: -7.4969e+01
Epoch 2/2
10/10 - 1s - loss: 213.3519 - loglik: -1.9767e+02 - logprior: -1.5677e+01
Fitted a model with MAP estimate = -203.4552
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 289.8420 - loglik: -2.0011e+02 - logprior: -8.9729e+01
Epoch 2/10
10/10 - 1s - loss: 220.5201 - loglik: -1.9802e+02 - logprior: -2.2499e+01
Epoch 3/10
10/10 - 1s - loss: 201.4389 - loglik: -1.9729e+02 - logprior: -4.1469e+00
Epoch 4/10
10/10 - 1s - loss: 194.2809 - loglik: -1.9658e+02 - logprior: 2.2951
Epoch 5/10
10/10 - 1s - loss: 190.1931 - loglik: -1.9549e+02 - logprior: 5.3016
Epoch 6/10
10/10 - 1s - loss: 187.5563 - loglik: -1.9443e+02 - logprior: 6.8781
Epoch 7/10
10/10 - 1s - loss: 185.5726 - loglik: -1.9351e+02 - logprior: 7.9362
Epoch 8/10
10/10 - 1s - loss: 184.1209 - loglik: -1.9293e+02 - logprior: 8.8082
Epoch 9/10
10/10 - 1s - loss: 183.0097 - loglik: -1.9260e+02 - logprior: 9.5878
Epoch 10/10
10/10 - 1s - loss: 182.0816 - loglik: -1.9233e+02 - logprior: 10.2476
Fitted a model with MAP estimate = -181.6063
Time for alignment: 44.0343
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 444.1491 - loglik: -3.5874e+02 - logprior: -8.5406e+01
Epoch 2/10
10/10 - 1s - loss: 340.5240 - loglik: -3.2094e+02 - logprior: -1.9583e+01
Epoch 3/10
10/10 - 1s - loss: 287.4666 - loglik: -2.7981e+02 - logprior: -7.6517e+00
Epoch 4/10
10/10 - 1s - loss: 254.4727 - loglik: -2.5051e+02 - logprior: -3.9652e+00
Epoch 5/10
10/10 - 1s - loss: 238.1281 - loglik: -2.3628e+02 - logprior: -1.8450e+00
Epoch 6/10
10/10 - 1s - loss: 231.0212 - loglik: -2.3046e+02 - logprior: -5.6562e-01
Epoch 7/10
10/10 - 1s - loss: 227.6497 - loglik: -2.2802e+02 - logprior: 0.3729
Epoch 8/10
10/10 - 1s - loss: 225.6313 - loglik: -2.2661e+02 - logprior: 0.9791
Epoch 9/10
10/10 - 1s - loss: 224.2590 - loglik: -2.2567e+02 - logprior: 1.4152
Epoch 10/10
10/10 - 1s - loss: 223.2001 - loglik: -2.2498e+02 - logprior: 1.7763
Fitted a model with MAP estimate = -222.7063
expansions: [(13, 3), (17, 1), (29, 1), (40, 2), (41, 2), (47, 3), (61, 3), (62, 1), (79, 2), (80, 2), (81, 2), (88, 5), (89, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 326.7172 - loglik: -2.3068e+02 - logprior: -9.6036e+01
Epoch 2/2
10/10 - 1s - loss: 245.2017 - loglik: -2.0823e+02 - logprior: -3.6974e+01
Fitted a model with MAP estimate = -230.3768
expansions: [(0, 2), (16, 1)]
discards: [  0  44  94  97 111 112 115]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 279.1891 - loglik: -2.0422e+02 - logprior: -7.4969e+01
Epoch 2/2
10/10 - 1s - loss: 213.3524 - loglik: -1.9768e+02 - logprior: -1.5677e+01
Fitted a model with MAP estimate = -203.4549
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 289.8439 - loglik: -2.0012e+02 - logprior: -8.9728e+01
Epoch 2/10
10/10 - 1s - loss: 220.5199 - loglik: -1.9802e+02 - logprior: -2.2499e+01
Epoch 3/10
10/10 - 1s - loss: 201.4388 - loglik: -1.9729e+02 - logprior: -4.1471e+00
Epoch 4/10
10/10 - 1s - loss: 194.2818 - loglik: -1.9658e+02 - logprior: 2.2948
Epoch 5/10
10/10 - 1s - loss: 190.1951 - loglik: -1.9550e+02 - logprior: 5.3016
Epoch 6/10
10/10 - 1s - loss: 187.5584 - loglik: -1.9444e+02 - logprior: 6.8781
Epoch 7/10
10/10 - 1s - loss: 185.5751 - loglik: -1.9351e+02 - logprior: 7.9357
Epoch 8/10
10/10 - 1s - loss: 184.1231 - loglik: -1.9293e+02 - logprior: 8.8074
Epoch 9/10
10/10 - 1s - loss: 183.0117 - loglik: -1.9260e+02 - logprior: 9.5868
Epoch 10/10
10/10 - 1s - loss: 182.0838 - loglik: -1.9233e+02 - logprior: 10.2458
Fitted a model with MAP estimate = -181.6087
Time for alignment: 44.3064
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 444.1491 - loglik: -3.5874e+02 - logprior: -8.5406e+01
Epoch 2/10
10/10 - 1s - loss: 340.5243 - loglik: -3.2094e+02 - logprior: -1.9583e+01
Epoch 3/10
10/10 - 1s - loss: 287.4664 - loglik: -2.7981e+02 - logprior: -7.6516e+00
Epoch 4/10
10/10 - 1s - loss: 254.4751 - loglik: -2.5051e+02 - logprior: -3.9649e+00
Epoch 5/10
10/10 - 1s - loss: 238.1277 - loglik: -2.3628e+02 - logprior: -1.8449e+00
Epoch 6/10
10/10 - 1s - loss: 231.0213 - loglik: -2.3046e+02 - logprior: -5.6579e-01
Epoch 7/10
10/10 - 1s - loss: 227.6494 - loglik: -2.2802e+02 - logprior: 0.3729
Epoch 8/10
10/10 - 1s - loss: 225.6313 - loglik: -2.2661e+02 - logprior: 0.9796
Epoch 9/10
10/10 - 1s - loss: 224.2598 - loglik: -2.2568e+02 - logprior: 1.4153
Epoch 10/10
10/10 - 1s - loss: 223.2010 - loglik: -2.2498e+02 - logprior: 1.7764
Fitted a model with MAP estimate = -222.7073
expansions: [(13, 3), (17, 1), (29, 1), (40, 2), (41, 2), (47, 3), (61, 3), (62, 1), (79, 2), (80, 2), (81, 2), (88, 5), (89, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 326.7170 - loglik: -2.3068e+02 - logprior: -9.6036e+01
Epoch 2/2
10/10 - 1s - loss: 245.2032 - loglik: -2.0823e+02 - logprior: -3.6974e+01
Fitted a model with MAP estimate = -230.3797
expansions: [(0, 2), (16, 1)]
discards: [  0  44  94  97 111 112 115]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 279.1848 - loglik: -2.0422e+02 - logprior: -7.4969e+01
Epoch 2/2
10/10 - 1s - loss: 213.3514 - loglik: -1.9767e+02 - logprior: -1.5677e+01
Fitted a model with MAP estimate = -203.4556
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 289.8424 - loglik: -2.0011e+02 - logprior: -8.9729e+01
Epoch 2/10
10/10 - 1s - loss: 220.5201 - loglik: -1.9802e+02 - logprior: -2.2499e+01
Epoch 3/10
10/10 - 1s - loss: 201.4388 - loglik: -1.9729e+02 - logprior: -4.1471e+00
Epoch 4/10
10/10 - 1s - loss: 194.2817 - loglik: -1.9658e+02 - logprior: 2.2950
Epoch 5/10
10/10 - 1s - loss: 190.1945 - loglik: -1.9550e+02 - logprior: 5.3014
Epoch 6/10
10/10 - 1s - loss: 187.5577 - loglik: -1.9444e+02 - logprior: 6.8780
Epoch 7/10
10/10 - 1s - loss: 185.5744 - loglik: -1.9351e+02 - logprior: 7.9360
Epoch 8/10
10/10 - 1s - loss: 184.1228 - loglik: -1.9293e+02 - logprior: 8.8075
Epoch 9/10
10/10 - 1s - loss: 183.0117 - loglik: -1.9260e+02 - logprior: 9.5869
Epoch 10/10
10/10 - 1s - loss: 182.0837 - loglik: -1.9233e+02 - logprior: 10.2467
Fitted a model with MAP estimate = -181.6089
Time for alignment: 44.4688
Computed alignments with likelihoods: ['-181.6075', '-181.6082', '-181.6063', '-181.6087', '-181.6089']
Best model has likelihood: -181.6063  (prior= 10.5555 )
time for generating output: 0.1372
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.9038805970149254
Training of 5 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe8877f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201ab58dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5cac6d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fac1f7310>
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 168.0260 - loglik: -1.3636e+02 - logprior: -3.1661e+01
Epoch 2/10
10/10 - 0s - loss: 126.2980 - loglik: -1.1737e+02 - logprior: -8.9284e+00
Epoch 3/10
10/10 - 0s - loss: 102.1947 - loglik: -9.7424e+01 - logprior: -4.7708e+00
Epoch 4/10
10/10 - 0s - loss: 86.5937 - loglik: -8.2941e+01 - logprior: -3.6524e+00
Epoch 5/10
10/10 - 0s - loss: 79.3902 - loglik: -7.5913e+01 - logprior: -3.4767e+00
Epoch 6/10
10/10 - 0s - loss: 76.8929 - loglik: -7.3439e+01 - logprior: -3.4543e+00
Epoch 7/10
10/10 - 0s - loss: 75.7866 - loglik: -7.2860e+01 - logprior: -2.9270e+00
Epoch 8/10
10/10 - 0s - loss: 75.3529 - loglik: -7.2805e+01 - logprior: -2.5482e+00
Epoch 9/10
10/10 - 0s - loss: 75.0542 - loglik: -7.2584e+01 - logprior: -2.4699e+00
Epoch 10/10
10/10 - 0s - loss: 74.9453 - loglik: -7.2530e+01 - logprior: -2.4152e+00
Fitted a model with MAP estimate = -74.8267
expansions: [(0, 2), (3, 2), (16, 1), (17, 1), (21, 1), (22, 1), (23, 2), (24, 1), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 112.0279 - loglik: -6.9928e+01 - logprior: -4.2100e+01
Epoch 2/2
10/10 - 0s - loss: 76.7888 - loglik: -6.3323e+01 - logprior: -1.3466e+01
Fitted a model with MAP estimate = -70.2670
expansions: []
discards: [ 0 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 99.4300 - loglik: -6.3412e+01 - logprior: -3.6018e+01
Epoch 2/2
10/10 - 0s - loss: 77.5584 - loglik: -6.3149e+01 - logprior: -1.4410e+01
Fitted a model with MAP estimate = -73.2715
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 93.9286 - loglik: -6.2013e+01 - logprior: -3.1915e+01
Epoch 2/10
10/10 - 0s - loss: 71.0348 - loglik: -6.1881e+01 - logprior: -9.1543e+00
Epoch 3/10
10/10 - 0s - loss: 65.9201 - loglik: -6.1361e+01 - logprior: -4.5589e+00
Epoch 4/10
10/10 - 0s - loss: 64.1264 - loglik: -6.1061e+01 - logprior: -3.0649e+00
Epoch 5/10
10/10 - 0s - loss: 63.2350 - loglik: -6.1246e+01 - logprior: -1.9893e+00
Epoch 6/10
10/10 - 0s - loss: 62.8139 - loglik: -6.1453e+01 - logprior: -1.3607e+00
Epoch 7/10
10/10 - 0s - loss: 62.5274 - loglik: -6.1412e+01 - logprior: -1.1154e+00
Epoch 8/10
10/10 - 0s - loss: 62.2165 - loglik: -6.1315e+01 - logprior: -9.0138e-01
Epoch 9/10
10/10 - 0s - loss: 62.3389 - loglik: -6.1594e+01 - logprior: -7.4484e-01
Fitted a model with MAP estimate = -62.1220
Time for alignment: 26.8977
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.9679 - loglik: -1.3631e+02 - logprior: -3.1662e+01
Epoch 2/10
10/10 - 0s - loss: 126.3688 - loglik: -1.1744e+02 - logprior: -8.9322e+00
Epoch 3/10
10/10 - 0s - loss: 102.1841 - loglik: -9.7387e+01 - logprior: -4.7970e+00
Epoch 4/10
10/10 - 0s - loss: 86.3369 - loglik: -8.2641e+01 - logprior: -3.6962e+00
Epoch 5/10
10/10 - 0s - loss: 79.4316 - loglik: -7.5951e+01 - logprior: -3.4810e+00
Epoch 6/10
10/10 - 0s - loss: 77.2356 - loglik: -7.3812e+01 - logprior: -3.4233e+00
Epoch 7/10
10/10 - 0s - loss: 76.1684 - loglik: -7.3276e+01 - logprior: -2.8920e+00
Epoch 8/10
10/10 - 0s - loss: 75.4764 - loglik: -7.2943e+01 - logprior: -2.5329e+00
Epoch 9/10
10/10 - 0s - loss: 75.0698 - loglik: -7.2593e+01 - logprior: -2.4764e+00
Epoch 10/10
10/10 - 0s - loss: 74.8587 - loglik: -7.2430e+01 - logprior: -2.4291e+00
Fitted a model with MAP estimate = -74.8402
expansions: [(0, 2), (3, 2), (16, 1), (17, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 113.1120 - loglik: -7.1024e+01 - logprior: -4.2088e+01
Epoch 2/2
10/10 - 0s - loss: 77.2665 - loglik: -6.3661e+01 - logprior: -1.3605e+01
Fitted a model with MAP estimate = -70.5688
expansions: []
discards: [ 0 31 34]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 99.6338 - loglik: -6.3584e+01 - logprior: -3.6050e+01
Epoch 2/2
10/10 - 0s - loss: 77.5948 - loglik: -6.3179e+01 - logprior: -1.4416e+01
Fitted a model with MAP estimate = -73.3042
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 94.0148 - loglik: -6.2079e+01 - logprior: -3.1936e+01
Epoch 2/10
10/10 - 0s - loss: 71.0283 - loglik: -6.1864e+01 - logprior: -9.1648e+00
Epoch 3/10
10/10 - 0s - loss: 65.9453 - loglik: -6.1380e+01 - logprior: -4.5655e+00
Epoch 4/10
10/10 - 0s - loss: 64.0240 - loglik: -6.0953e+01 - logprior: -3.0712e+00
Epoch 5/10
10/10 - 0s - loss: 63.1699 - loglik: -6.1177e+01 - logprior: -1.9930e+00
Epoch 6/10
10/10 - 0s - loss: 62.9213 - loglik: -6.1552e+01 - logprior: -1.3689e+00
Epoch 7/10
10/10 - 0s - loss: 62.4931 - loglik: -6.1372e+01 - logprior: -1.1215e+00
Epoch 8/10
10/10 - 0s - loss: 62.3963 - loglik: -6.1490e+01 - logprior: -9.0627e-01
Epoch 9/10
10/10 - 0s - loss: 62.1740 - loglik: -6.1413e+01 - logprior: -7.6054e-01
Epoch 10/10
10/10 - 0s - loss: 61.9864 - loglik: -6.1308e+01 - logprior: -6.7852e-01
Fitted a model with MAP estimate = -61.9799
Time for alignment: 26.1402
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.9973 - loglik: -1.3634e+02 - logprior: -3.1662e+01
Epoch 2/10
10/10 - 0s - loss: 126.5317 - loglik: -1.1760e+02 - logprior: -8.9301e+00
Epoch 3/10
10/10 - 0s - loss: 101.8610 - loglik: -9.7089e+01 - logprior: -4.7719e+00
Epoch 4/10
10/10 - 0s - loss: 86.1221 - loglik: -8.2469e+01 - logprior: -3.6526e+00
Epoch 5/10
10/10 - 0s - loss: 78.9960 - loglik: -7.5513e+01 - logprior: -3.4830e+00
Epoch 6/10
10/10 - 0s - loss: 76.8065 - loglik: -7.3325e+01 - logprior: -3.4816e+00
Epoch 7/10
10/10 - 0s - loss: 75.7865 - loglik: -7.2829e+01 - logprior: -2.9570e+00
Epoch 8/10
10/10 - 0s - loss: 75.2993 - loglik: -7.2742e+01 - logprior: -2.5569e+00
Epoch 9/10
10/10 - 0s - loss: 75.0148 - loglik: -7.2556e+01 - logprior: -2.4593e+00
Epoch 10/10
10/10 - 0s - loss: 74.9144 - loglik: -7.2505e+01 - logprior: -2.4098e+00
Fitted a model with MAP estimate = -74.8193
expansions: [(0, 2), (3, 2), (16, 1), (17, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 113.0772 - loglik: -7.0992e+01 - logprior: -4.2085e+01
Epoch 2/2
10/10 - 0s - loss: 77.0265 - loglik: -6.3426e+01 - logprior: -1.3601e+01
Fitted a model with MAP estimate = -70.5245
expansions: []
discards: [ 0 31 34]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 99.5811 - loglik: -6.3533e+01 - logprior: -3.6048e+01
Epoch 2/2
10/10 - 0s - loss: 77.5605 - loglik: -6.3142e+01 - logprior: -1.4419e+01
Fitted a model with MAP estimate = -73.2974
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 93.9383 - loglik: -6.2005e+01 - logprior: -3.1933e+01
Epoch 2/10
10/10 - 0s - loss: 71.1112 - loglik: -6.1943e+01 - logprior: -9.1683e+00
Epoch 3/10
10/10 - 0s - loss: 65.9951 - loglik: -6.1438e+01 - logprior: -4.5569e+00
Epoch 4/10
10/10 - 0s - loss: 63.9416 - loglik: -6.0868e+01 - logprior: -3.0738e+00
Epoch 5/10
10/10 - 0s - loss: 63.2772 - loglik: -6.1290e+01 - logprior: -1.9874e+00
Epoch 6/10
10/10 - 0s - loss: 62.8961 - loglik: -6.1529e+01 - logprior: -1.3674e+00
Epoch 7/10
10/10 - 0s - loss: 62.4223 - loglik: -6.1304e+01 - logprior: -1.1181e+00
Epoch 8/10
10/10 - 0s - loss: 62.3802 - loglik: -6.1476e+01 - logprior: -9.0428e-01
Epoch 9/10
10/10 - 0s - loss: 62.2413 - loglik: -6.1485e+01 - logprior: -7.5589e-01
Epoch 10/10
10/10 - 0s - loss: 62.0737 - loglik: -6.1395e+01 - logprior: -6.7879e-01
Fitted a model with MAP estimate = -62.0096
Time for alignment: 27.5385
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 168.0542 - loglik: -1.3639e+02 - logprior: -3.1662e+01
Epoch 2/10
10/10 - 0s - loss: 126.3822 - loglik: -1.1745e+02 - logprior: -8.9297e+00
Epoch 3/10
10/10 - 0s - loss: 102.0010 - loglik: -9.7224e+01 - logprior: -4.7773e+00
Epoch 4/10
10/10 - 0s - loss: 86.3659 - loglik: -8.2716e+01 - logprior: -3.6497e+00
Epoch 5/10
10/10 - 0s - loss: 79.3404 - loglik: -7.5871e+01 - logprior: -3.4694e+00
Epoch 6/10
10/10 - 0s - loss: 76.7299 - loglik: -7.3262e+01 - logprior: -3.4675e+00
Epoch 7/10
10/10 - 0s - loss: 75.7799 - loglik: -7.2829e+01 - logprior: -2.9505e+00
Epoch 8/10
10/10 - 0s - loss: 75.3435 - loglik: -7.2791e+01 - logprior: -2.5527e+00
Epoch 9/10
10/10 - 0s - loss: 75.0137 - loglik: -7.2547e+01 - logprior: -2.4666e+00
Epoch 10/10
10/10 - 0s - loss: 74.9516 - loglik: -7.2541e+01 - logprior: -2.4108e+00
Fitted a model with MAP estimate = -74.8272
expansions: [(0, 2), (3, 2), (16, 1), (17, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 113.0293 - loglik: -7.0945e+01 - logprior: -4.2084e+01
Epoch 2/2
10/10 - 0s - loss: 77.1562 - loglik: -6.3554e+01 - logprior: -1.3602e+01
Fitted a model with MAP estimate = -70.5282
expansions: []
discards: [ 0 31 34]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 99.5278 - loglik: -6.3479e+01 - logprior: -3.6049e+01
Epoch 2/2
10/10 - 0s - loss: 77.6146 - loglik: -6.3195e+01 - logprior: -1.4420e+01
Fitted a model with MAP estimate = -73.3011
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 93.9622 - loglik: -6.2024e+01 - logprior: -3.1938e+01
Epoch 2/10
10/10 - 0s - loss: 71.0699 - loglik: -6.1904e+01 - logprior: -9.1661e+00
Epoch 3/10
10/10 - 0s - loss: 66.0170 - loglik: -6.1455e+01 - logprior: -4.5618e+00
Epoch 4/10
10/10 - 0s - loss: 64.0148 - loglik: -6.0945e+01 - logprior: -3.0695e+00
Epoch 5/10
10/10 - 0s - loss: 63.1898 - loglik: -6.1194e+01 - logprior: -1.9961e+00
Epoch 6/10
10/10 - 0s - loss: 62.8928 - loglik: -6.1529e+01 - logprior: -1.3639e+00
Epoch 7/10
10/10 - 0s - loss: 62.4739 - loglik: -6.1354e+01 - logprior: -1.1199e+00
Epoch 8/10
10/10 - 0s - loss: 62.3622 - loglik: -6.1456e+01 - logprior: -9.0601e-01
Epoch 9/10
10/10 - 0s - loss: 62.2213 - loglik: -6.1470e+01 - logprior: -7.5147e-01
Epoch 10/10
10/10 - 0s - loss: 62.1715 - loglik: -6.1489e+01 - logprior: -6.8275e-01
Fitted a model with MAP estimate = -62.0301
Time for alignment: 25.6834
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 168.0759 - loglik: -1.3641e+02 - logprior: -3.1662e+01
Epoch 2/10
10/10 - 0s - loss: 126.2048 - loglik: -1.1728e+02 - logprior: -8.9285e+00
Epoch 3/10
10/10 - 0s - loss: 102.0789 - loglik: -9.7306e+01 - logprior: -4.7726e+00
Epoch 4/10
10/10 - 0s - loss: 86.3763 - loglik: -8.2715e+01 - logprior: -3.6612e+00
Epoch 5/10
10/10 - 0s - loss: 79.4990 - loglik: -7.6016e+01 - logprior: -3.4831e+00
Epoch 6/10
10/10 - 0s - loss: 76.8609 - loglik: -7.3407e+01 - logprior: -3.4538e+00
Epoch 7/10
10/10 - 0s - loss: 75.8282 - loglik: -7.2900e+01 - logprior: -2.9286e+00
Epoch 8/10
10/10 - 0s - loss: 75.3291 - loglik: -7.2782e+01 - logprior: -2.5468e+00
Epoch 9/10
10/10 - 0s - loss: 75.0518 - loglik: -7.2578e+01 - logprior: -2.4736e+00
Epoch 10/10
10/10 - 0s - loss: 74.8987 - loglik: -7.2482e+01 - logprior: -2.4166e+00
Fitted a model with MAP estimate = -74.8238
expansions: [(0, 2), (3, 2), (16, 1), (17, 1), (21, 1), (22, 1), (23, 2), (24, 1), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 112.0922 - loglik: -6.9994e+01 - logprior: -4.2098e+01
Epoch 2/2
10/10 - 0s - loss: 76.8252 - loglik: -6.3353e+01 - logprior: -1.3472e+01
Fitted a model with MAP estimate = -70.2834
expansions: []
discards: [ 0 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 99.5204 - loglik: -6.3500e+01 - logprior: -3.6021e+01
Epoch 2/2
10/10 - 0s - loss: 77.4551 - loglik: -6.3049e+01 - logprior: -1.4406e+01
Fitted a model with MAP estimate = -73.2711
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 94.0827 - loglik: -6.2166e+01 - logprior: -3.1917e+01
Epoch 2/10
10/10 - 0s - loss: 70.9023 - loglik: -6.1738e+01 - logprior: -9.1642e+00
Epoch 3/10
10/10 - 0s - loss: 65.9472 - loglik: -6.1388e+01 - logprior: -4.5593e+00
Epoch 4/10
10/10 - 0s - loss: 64.0882 - loglik: -6.1017e+01 - logprior: -3.0717e+00
Epoch 5/10
10/10 - 0s - loss: 63.1894 - loglik: -6.1198e+01 - logprior: -1.9914e+00
Epoch 6/10
10/10 - 0s - loss: 62.8911 - loglik: -6.1523e+01 - logprior: -1.3683e+00
Epoch 7/10
10/10 - 0s - loss: 62.5235 - loglik: -6.1405e+01 - logprior: -1.1180e+00
Epoch 8/10
10/10 - 0s - loss: 62.3296 - loglik: -6.1424e+01 - logprior: -9.0580e-01
Epoch 9/10
10/10 - 0s - loss: 62.2065 - loglik: -6.1453e+01 - logprior: -7.5355e-01
Epoch 10/10
10/10 - 0s - loss: 62.1480 - loglik: -6.1469e+01 - logprior: -6.7907e-01
Fitted a model with MAP estimate = -62.0189
Time for alignment: 27.4609
Computed alignments with likelihoods: ['-62.1220', '-61.9799', '-62.0096', '-62.0301', '-62.0189']
Best model has likelihood: -61.9799  (prior= -0.6538 )
time for generating output: 0.0927
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 5 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201ac7f700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdfd40df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2023bcbdc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2034c83820>
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 452.3378 - loglik: -4.4701e+02 - logprior: -5.3318e+00
Epoch 2/10
15/15 - 3s - loss: 377.5584 - loglik: -3.7634e+02 - logprior: -1.2211e+00
Epoch 3/10
15/15 - 3s - loss: 322.3899 - loglik: -3.2090e+02 - logprior: -1.4850e+00
Epoch 4/10
15/15 - 3s - loss: 305.8672 - loglik: -3.0418e+02 - logprior: -1.6877e+00
Epoch 5/10
15/15 - 3s - loss: 302.0030 - loglik: -3.0045e+02 - logprior: -1.5574e+00
Epoch 6/10
15/15 - 3s - loss: 300.4814 - loglik: -2.9892e+02 - logprior: -1.5586e+00
Epoch 7/10
15/15 - 3s - loss: 298.4545 - loglik: -2.9695e+02 - logprior: -1.5091e+00
Epoch 8/10
15/15 - 3s - loss: 298.4782 - loglik: -2.9700e+02 - logprior: -1.4799e+00
Fitted a model with MAP estimate = -298.1235
expansions: [(0, 2), (6, 1), (16, 1), (21, 1), (24, 1), (26, 2), (32, 1), (56, 1), (58, 1), (65, 3), (68, 1), (90, 2), (104, 2), (110, 1), (113, 3), (115, 1), (117, 1), (118, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 301.9053 - loglik: -2.9521e+02 - logprior: -6.6956e+00
Epoch 2/2
15/15 - 4s - loss: 287.7109 - loglik: -2.8595e+02 - logprior: -1.7626e+00
Fitted a model with MAP estimate = -286.1721
expansions: []
discards: [  0  77  82 121]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 294.5546 - loglik: -2.8791e+02 - logprior: -6.6435e+00
Epoch 2/2
15/15 - 4s - loss: 288.6733 - loglik: -2.8599e+02 - logprior: -2.6821e+00
Fitted a model with MAP estimate = -286.7061
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 290.5783 - loglik: -2.8556e+02 - logprior: -5.0221e+00
Epoch 2/10
15/15 - 4s - loss: 287.4722 - loglik: -2.8610e+02 - logprior: -1.3763e+00
Epoch 3/10
15/15 - 4s - loss: 284.4760 - loglik: -2.8355e+02 - logprior: -9.2639e-01
Epoch 4/10
15/15 - 4s - loss: 283.8803 - loglik: -2.8307e+02 - logprior: -8.1162e-01
Epoch 5/10
15/15 - 4s - loss: 284.0513 - loglik: -2.8330e+02 - logprior: -7.4645e-01
Fitted a model with MAP estimate = -283.3079
Time for alignment: 99.2626
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 451.8584 - loglik: -4.4653e+02 - logprior: -5.3242e+00
Epoch 2/10
15/15 - 3s - loss: 378.3582 - loglik: -3.7713e+02 - logprior: -1.2328e+00
Epoch 3/10
15/15 - 3s - loss: 323.4916 - loglik: -3.2204e+02 - logprior: -1.4475e+00
Epoch 4/10
15/15 - 3s - loss: 308.7634 - loglik: -3.0714e+02 - logprior: -1.6186e+00
Epoch 5/10
15/15 - 3s - loss: 304.3739 - loglik: -3.0274e+02 - logprior: -1.6334e+00
Epoch 6/10
15/15 - 3s - loss: 300.9475 - loglik: -2.9942e+02 - logprior: -1.5315e+00
Epoch 7/10
15/15 - 3s - loss: 299.5499 - loglik: -2.9801e+02 - logprior: -1.5403e+00
Epoch 8/10
15/15 - 3s - loss: 299.0018 - loglik: -2.9750e+02 - logprior: -1.5027e+00
Epoch 9/10
15/15 - 3s - loss: 298.7590 - loglik: -2.9727e+02 - logprior: -1.4878e+00
Epoch 10/10
15/15 - 3s - loss: 297.6592 - loglik: -2.9618e+02 - logprior: -1.4809e+00
Fitted a model with MAP estimate = -297.6951
expansions: [(0, 2), (10, 1), (16, 1), (21, 1), (24, 1), (26, 2), (32, 1), (56, 1), (58, 1), (65, 3), (68, 1), (90, 3), (102, 1), (104, 2), (113, 3), (115, 1), (117, 1), (118, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 155 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 303.2086 - loglik: -2.9644e+02 - logprior: -6.7637e+00
Epoch 2/2
15/15 - 4s - loss: 288.3371 - loglik: -2.8653e+02 - logprior: -1.8083e+00
Fitted a model with MAP estimate = -285.9641
expansions: []
discards: [  0  77  82 106 123]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 294.0317 - loglik: -2.8739e+02 - logprior: -6.6452e+00
Epoch 2/2
15/15 - 4s - loss: 287.9946 - loglik: -2.8532e+02 - logprior: -2.6762e+00
Fitted a model with MAP estimate = -286.5219
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 290.8537 - loglik: -2.8582e+02 - logprior: -5.0364e+00
Epoch 2/10
15/15 - 4s - loss: 286.0020 - loglik: -2.8464e+02 - logprior: -1.3603e+00
Epoch 3/10
15/15 - 4s - loss: 284.6973 - loglik: -2.8378e+02 - logprior: -9.2098e-01
Epoch 4/10
15/15 - 4s - loss: 284.0153 - loglik: -2.8322e+02 - logprior: -7.9690e-01
Epoch 5/10
15/15 - 4s - loss: 283.2819 - loglik: -2.8255e+02 - logprior: -7.3281e-01
Epoch 6/10
15/15 - 4s - loss: 282.5387 - loglik: -2.8183e+02 - logprior: -7.0419e-01
Epoch 7/10
15/15 - 4s - loss: 282.9419 - loglik: -2.8226e+02 - logprior: -6.8515e-01
Fitted a model with MAP estimate = -281.5825
Time for alignment: 114.8665
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 451.7328 - loglik: -4.4640e+02 - logprior: -5.3310e+00
Epoch 2/10
15/15 - 3s - loss: 377.1690 - loglik: -3.7595e+02 - logprior: -1.2219e+00
Epoch 3/10
15/15 - 3s - loss: 323.3244 - loglik: -3.2190e+02 - logprior: -1.4223e+00
Epoch 4/10
15/15 - 3s - loss: 306.0453 - loglik: -3.0439e+02 - logprior: -1.6600e+00
Epoch 5/10
15/15 - 3s - loss: 301.8990 - loglik: -3.0032e+02 - logprior: -1.5809e+00
Epoch 6/10
15/15 - 3s - loss: 300.1773 - loglik: -2.9865e+02 - logprior: -1.5315e+00
Epoch 7/10
15/15 - 3s - loss: 299.3134 - loglik: -2.9781e+02 - logprior: -1.5074e+00
Epoch 8/10
15/15 - 3s - loss: 297.9775 - loglik: -2.9648e+02 - logprior: -1.4949e+00
Epoch 9/10
15/15 - 3s - loss: 298.4589 - loglik: -2.9698e+02 - logprior: -1.4818e+00
Fitted a model with MAP estimate = -297.7286
expansions: [(0, 2), (10, 1), (16, 1), (21, 1), (24, 1), (26, 2), (32, 1), (56, 1), (58, 1), (65, 3), (68, 1), (90, 2), (104, 2), (110, 1), (113, 1), (114, 1), (115, 3), (117, 1), (118, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 155 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 302.0821 - loglik: -2.9539e+02 - logprior: -6.6938e+00
Epoch 2/2
15/15 - 4s - loss: 287.9217 - loglik: -2.8613e+02 - logprior: -1.7920e+00
Fitted a model with MAP estimate = -285.7265
expansions: []
discards: [  0  77  82 122]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 293.7105 - loglik: -2.8702e+02 - logprior: -6.6875e+00
Epoch 2/2
15/15 - 4s - loss: 287.7201 - loglik: -2.8499e+02 - logprior: -2.7318e+00
Fitted a model with MAP estimate = -286.1524
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 289.8021 - loglik: -2.8474e+02 - logprior: -5.0634e+00
Epoch 2/10
15/15 - 4s - loss: 285.8351 - loglik: -2.8445e+02 - logprior: -1.3886e+00
Epoch 3/10
15/15 - 4s - loss: 284.6718 - loglik: -2.8372e+02 - logprior: -9.5648e-01
Epoch 4/10
15/15 - 4s - loss: 283.4759 - loglik: -2.8264e+02 - logprior: -8.3323e-01
Epoch 5/10
15/15 - 4s - loss: 283.4825 - loglik: -2.8271e+02 - logprior: -7.6836e-01
Fitted a model with MAP estimate = -282.6674
Time for alignment: 103.4935
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 451.5678 - loglik: -4.4624e+02 - logprior: -5.3263e+00
Epoch 2/10
15/15 - 3s - loss: 378.6624 - loglik: -3.7744e+02 - logprior: -1.2248e+00
Epoch 3/10
15/15 - 3s - loss: 329.4195 - loglik: -3.2797e+02 - logprior: -1.4539e+00
Epoch 4/10
15/15 - 3s - loss: 310.4033 - loglik: -3.0869e+02 - logprior: -1.7102e+00
Epoch 5/10
15/15 - 3s - loss: 304.3023 - loglik: -3.0270e+02 - logprior: -1.5980e+00
Epoch 6/10
15/15 - 3s - loss: 302.9587 - loglik: -3.0137e+02 - logprior: -1.5873e+00
Epoch 7/10
15/15 - 3s - loss: 301.0336 - loglik: -2.9947e+02 - logprior: -1.5629e+00
Epoch 8/10
15/15 - 3s - loss: 300.8380 - loglik: -2.9927e+02 - logprior: -1.5682e+00
Epoch 9/10
15/15 - 3s - loss: 299.7344 - loglik: -2.9818e+02 - logprior: -1.5528e+00
Epoch 10/10
15/15 - 3s - loss: 300.2188 - loglik: -2.9866e+02 - logprior: -1.5620e+00
Fitted a model with MAP estimate = -298.8422
expansions: [(0, 2), (10, 1), (16, 1), (21, 1), (24, 1), (25, 1), (32, 1), (38, 1), (56, 1), (58, 1), (65, 1), (66, 1), (68, 1), (90, 3), (102, 1), (104, 2), (113, 3), (115, 2), (117, 1), (118, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 155 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 303.5631 - loglik: -2.9681e+02 - logprior: -6.7568e+00
Epoch 2/2
15/15 - 4s - loss: 288.3710 - loglik: -2.8653e+02 - logprior: -1.8459e+00
Fitted a model with MAP estimate = -285.6627
expansions: []
discards: [  0  81 105 122]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 293.4141 - loglik: -2.8671e+02 - logprior: -6.7016e+00
Epoch 2/2
15/15 - 4s - loss: 288.0001 - loglik: -2.8527e+02 - logprior: -2.7280e+00
Fitted a model with MAP estimate = -286.0062
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 290.2430 - loglik: -2.8519e+02 - logprior: -5.0518e+00
Epoch 2/10
15/15 - 4s - loss: 285.0276 - loglik: -2.8363e+02 - logprior: -1.4024e+00
Epoch 3/10
15/15 - 4s - loss: 284.8100 - loglik: -2.8386e+02 - logprior: -9.5383e-01
Epoch 4/10
15/15 - 4s - loss: 283.2031 - loglik: -2.8237e+02 - logprior: -8.3451e-01
Epoch 5/10
15/15 - 4s - loss: 282.8193 - loglik: -2.8204e+02 - logprior: -7.7808e-01
Epoch 6/10
15/15 - 4s - loss: 282.7041 - loglik: -2.8195e+02 - logprior: -7.5256e-01
Epoch 7/10
15/15 - 4s - loss: 281.3635 - loglik: -2.8063e+02 - logprior: -7.3011e-01
Epoch 8/10
15/15 - 4s - loss: 280.7056 - loglik: -2.8000e+02 - logprior: -7.0129e-01
Epoch 9/10
15/15 - 4s - loss: 280.5576 - loglik: -2.7987e+02 - logprior: -6.8934e-01
Epoch 10/10
15/15 - 4s - loss: 279.7727 - loglik: -2.7909e+02 - logprior: -6.8052e-01
Fitted a model with MAP estimate = -279.6406
Time for alignment: 126.2675
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 451.4784 - loglik: -4.4615e+02 - logprior: -5.3316e+00
Epoch 2/10
15/15 - 3s - loss: 377.2793 - loglik: -3.7604e+02 - logprior: -1.2430e+00
Epoch 3/10
15/15 - 3s - loss: 320.6254 - loglik: -3.1910e+02 - logprior: -1.5283e+00
Epoch 4/10
15/15 - 3s - loss: 306.4968 - loglik: -3.0467e+02 - logprior: -1.8241e+00
Epoch 5/10
15/15 - 3s - loss: 301.9285 - loglik: -3.0022e+02 - logprior: -1.7113e+00
Epoch 6/10
15/15 - 3s - loss: 300.8246 - loglik: -2.9916e+02 - logprior: -1.6682e+00
Epoch 7/10
15/15 - 3s - loss: 299.3438 - loglik: -2.9770e+02 - logprior: -1.6453e+00
Epoch 8/10
15/15 - 3s - loss: 299.8351 - loglik: -2.9821e+02 - logprior: -1.6201e+00
Fitted a model with MAP estimate = -298.9799
expansions: [(7, 3), (10, 1), (16, 1), (21, 1), (24, 1), (26, 2), (32, 1), (56, 1), (58, 1), (65, 3), (68, 1), (90, 2), (104, 1), (110, 1), (113, 3), (115, 1), (117, 1), (118, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 303.0397 - loglik: -2.9644e+02 - logprior: -6.5996e+00
Epoch 2/2
15/15 - 4s - loss: 289.9845 - loglik: -2.8701e+02 - logprior: -2.9761e+00
Fitted a model with MAP estimate = -287.5354
expansions: [(0, 2)]
discards: [ 0  7 77 82]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 291.7486 - loglik: -2.8684e+02 - logprior: -4.9085e+00
Epoch 2/2
15/15 - 4s - loss: 285.0978 - loglik: -2.8369e+02 - logprior: -1.4097e+00
Fitted a model with MAP estimate = -284.6768
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 293.5113 - loglik: -2.8708e+02 - logprior: -6.4297e+00
Epoch 2/10
15/15 - 4s - loss: 287.4179 - loglik: -2.8539e+02 - logprior: -2.0303e+00
Epoch 3/10
15/15 - 4s - loss: 284.6152 - loglik: -2.8366e+02 - logprior: -9.5656e-01
Epoch 4/10
15/15 - 4s - loss: 284.8321 - loglik: -2.8406e+02 - logprior: -7.7161e-01
Fitted a model with MAP estimate = -283.6942
Time for alignment: 95.2052
Computed alignments with likelihoods: ['-283.3079', '-281.5825', '-282.6674', '-279.6406', '-283.6942']
Best model has likelihood: -279.6406  (prior= -0.6695 )
time for generating output: 0.1917
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.9105351170568562
Training of 5 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa314e850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe84da220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa3485cd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2034c83820>
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 310.1815 - loglik: -3.0735e+02 - logprior: -2.8316e+00
Epoch 2/10
34/34 - 4s - loss: 216.5476 - loglik: -2.1456e+02 - logprior: -1.9854e+00
Epoch 3/10
34/34 - 5s - loss: 207.8356 - loglik: -2.0596e+02 - logprior: -1.8714e+00
Epoch 4/10
34/34 - 4s - loss: 205.0557 - loglik: -2.0325e+02 - logprior: -1.8102e+00
Epoch 5/10
34/34 - 5s - loss: 204.7209 - loglik: -2.0293e+02 - logprior: -1.7915e+00
Epoch 6/10
34/34 - 5s - loss: 204.9469 - loglik: -2.0316e+02 - logprior: -1.7872e+00
Fitted a model with MAP estimate = -204.1015
expansions: [(0, 2), (12, 1), (15, 1), (16, 4), (26, 2), (27, 2), (28, 1), (41, 1), (45, 1), (48, 1), (54, 1), (55, 1), (56, 1), (57, 1), (65, 1), (66, 1), (76, 1), (79, 1), (82, 1), (84, 2), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 191.3930 - loglik: -1.8838e+02 - logprior: -3.0161e+00
Epoch 2/2
34/34 - 5s - loss: 182.5548 - loglik: -1.8136e+02 - logprior: -1.1909e+00
Fitted a model with MAP estimate = -180.8044
expansions: []
discards: [  0  35 140]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 187.7050 - loglik: -1.8411e+02 - logprior: -3.5957e+00
Epoch 2/2
34/34 - 5s - loss: 183.5481 - loglik: -1.8182e+02 - logprior: -1.7269e+00
Fitted a model with MAP estimate = -181.7710
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 186.4903 - loglik: -1.8382e+02 - logprior: -2.6664e+00
Epoch 2/10
34/34 - 5s - loss: 182.7446 - loglik: -1.8181e+02 - logprior: -9.3185e-01
Epoch 3/10
34/34 - 5s - loss: 181.4681 - loglik: -1.8061e+02 - logprior: -8.5399e-01
Epoch 4/10
34/34 - 4s - loss: 181.2838 - loglik: -1.8052e+02 - logprior: -7.6095e-01
Epoch 5/10
34/34 - 5s - loss: 180.3285 - loglik: -1.7963e+02 - logprior: -7.0038e-01
Epoch 6/10
34/34 - 5s - loss: 180.6104 - loglik: -1.7996e+02 - logprior: -6.4854e-01
Fitted a model with MAP estimate = -179.8050
Time for alignment: 115.1378
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 308.9022 - loglik: -3.0607e+02 - logprior: -2.8371e+00
Epoch 2/10
34/34 - 5s - loss: 213.4637 - loglik: -2.1146e+02 - logprior: -2.0073e+00
Epoch 3/10
34/34 - 5s - loss: 204.7900 - loglik: -2.0287e+02 - logprior: -1.9179e+00
Epoch 4/10
34/34 - 5s - loss: 203.8923 - loglik: -2.0204e+02 - logprior: -1.8566e+00
Epoch 5/10
34/34 - 5s - loss: 202.7747 - loglik: -2.0093e+02 - logprior: -1.8411e+00
Epoch 6/10
34/34 - 4s - loss: 202.6830 - loglik: -2.0087e+02 - logprior: -1.8178e+00
Epoch 7/10
34/34 - 5s - loss: 203.0654 - loglik: -2.0124e+02 - logprior: -1.8205e+00
Fitted a model with MAP estimate = -202.1684
expansions: [(0, 2), (12, 1), (16, 4), (17, 1), (25, 2), (26, 2), (41, 1), (45, 1), (48, 1), (49, 1), (53, 1), (55, 1), (56, 1), (60, 1), (66, 1), (76, 1), (79, 1), (82, 2), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 190.8775 - loglik: -1.8781e+02 - logprior: -3.0675e+00
Epoch 2/2
34/34 - 5s - loss: 181.7211 - loglik: -1.8047e+02 - logprior: -1.2505e+00
Fitted a model with MAP estimate = -180.4539
expansions: [(18, 1)]
discards: [  0  34 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 187.0121 - loglik: -1.8330e+02 - logprior: -3.7071e+00
Epoch 2/2
34/34 - 5s - loss: 181.8615 - loglik: -1.8009e+02 - logprior: -1.7688e+00
Fitted a model with MAP estimate = -180.7896
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 185.0835 - loglik: -1.8231e+02 - logprior: -2.7758e+00
Epoch 2/10
34/34 - 5s - loss: 181.5561 - loglik: -1.8059e+02 - logprior: -9.6423e-01
Epoch 3/10
34/34 - 5s - loss: 180.7749 - loglik: -1.7992e+02 - logprior: -8.5045e-01
Epoch 4/10
34/34 - 5s - loss: 179.4185 - loglik: -1.7864e+02 - logprior: -7.8173e-01
Epoch 5/10
34/34 - 5s - loss: 179.9470 - loglik: -1.7923e+02 - logprior: -7.1293e-01
Fitted a model with MAP estimate = -179.0567
Time for alignment: 116.5274
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 309.5345 - loglik: -3.0669e+02 - logprior: -2.8484e+00
Epoch 2/10
34/34 - 5s - loss: 213.9067 - loglik: -2.1193e+02 - logprior: -1.9753e+00
Epoch 3/10
34/34 - 5s - loss: 206.3076 - loglik: -2.0439e+02 - logprior: -1.9134e+00
Epoch 4/10
34/34 - 5s - loss: 203.1745 - loglik: -2.0127e+02 - logprior: -1.9053e+00
Epoch 5/10
34/34 - 4s - loss: 202.8838 - loglik: -2.0101e+02 - logprior: -1.8754e+00
Epoch 6/10
34/34 - 5s - loss: 203.3483 - loglik: -2.0148e+02 - logprior: -1.8637e+00
Fitted a model with MAP estimate = -202.6253
expansions: [(0, 2), (12, 1), (16, 4), (17, 1), (25, 1), (26, 1), (42, 1), (45, 1), (46, 1), (48, 1), (54, 1), (55, 1), (56, 1), (57, 1), (61, 1), (66, 1), (76, 1), (79, 1), (82, 1), (84, 2), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 191.4464 - loglik: -1.8839e+02 - logprior: -3.0520e+00
Epoch 2/2
34/34 - 5s - loss: 183.2248 - loglik: -1.8198e+02 - logprior: -1.2483e+00
Fitted a model with MAP estimate = -181.0250
expansions: [(18, 1)]
discards: [  0 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 186.8440 - loglik: -1.8314e+02 - logprior: -3.7039e+00
Epoch 2/2
34/34 - 5s - loss: 182.0471 - loglik: -1.8028e+02 - logprior: -1.7643e+00
Fitted a model with MAP estimate = -180.9191
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 185.7876 - loglik: -1.8301e+02 - logprior: -2.7815e+00
Epoch 2/10
34/34 - 5s - loss: 181.2857 - loglik: -1.8031e+02 - logprior: -9.7149e-01
Epoch 3/10
34/34 - 5s - loss: 181.3596 - loglik: -1.8050e+02 - logprior: -8.5565e-01
Fitted a model with MAP estimate = -180.2742
Time for alignment: 101.2535
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 309.8415 - loglik: -3.0699e+02 - logprior: -2.8564e+00
Epoch 2/10
34/34 - 5s - loss: 214.8513 - loglik: -2.1291e+02 - logprior: -1.9434e+00
Epoch 3/10
34/34 - 4s - loss: 207.2853 - loglik: -2.0540e+02 - logprior: -1.8899e+00
Epoch 4/10
34/34 - 5s - loss: 205.9604 - loglik: -2.0414e+02 - logprior: -1.8218e+00
Epoch 5/10
34/34 - 5s - loss: 203.9439 - loglik: -2.0214e+02 - logprior: -1.8021e+00
Epoch 6/10
34/34 - 5s - loss: 204.8304 - loglik: -2.0303e+02 - logprior: -1.7997e+00
Fitted a model with MAP estimate = -204.1861
expansions: [(0, 2), (16, 3), (17, 1), (18, 1), (26, 1), (27, 2), (40, 1), (41, 1), (45, 1), (48, 1), (49, 1), (53, 1), (55, 1), (56, 1), (60, 1), (66, 1), (76, 1), (79, 1), (82, 1), (84, 2), (107, 1), (108, 1), (110, 1), (111, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 193.0678 - loglik: -1.9004e+02 - logprior: -3.0275e+00
Epoch 2/2
34/34 - 5s - loss: 183.4100 - loglik: -1.8214e+02 - logprior: -1.2740e+00
Fitted a model with MAP estimate = -181.7672
expansions: [(17, 1)]
discards: [19]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 184.9331 - loglik: -1.8227e+02 - logprior: -2.6676e+00
Epoch 2/2
34/34 - 5s - loss: 180.8025 - loglik: -1.7967e+02 - logprior: -1.1320e+00
Fitted a model with MAP estimate = -180.4853
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 184.2900 - loglik: -1.8171e+02 - logprior: -2.5823e+00
Epoch 2/10
34/34 - 5s - loss: 180.6195 - loglik: -1.7959e+02 - logprior: -1.0344e+00
Epoch 3/10
34/34 - 5s - loss: 180.3016 - loglik: -1.7937e+02 - logprior: -9.3216e-01
Epoch 4/10
34/34 - 5s - loss: 179.2341 - loglik: -1.7837e+02 - logprior: -8.6665e-01
Epoch 5/10
34/34 - 5s - loss: 178.7016 - loglik: -1.7790e+02 - logprior: -7.9663e-01
Epoch 6/10
34/34 - 5s - loss: 177.9196 - loglik: -1.7718e+02 - logprior: -7.3724e-01
Epoch 7/10
34/34 - 5s - loss: 178.8408 - loglik: -1.7816e+02 - logprior: -6.7648e-01
Fitted a model with MAP estimate = -177.7562
Time for alignment: 119.8924
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 309.5251 - loglik: -3.0669e+02 - logprior: -2.8372e+00
Epoch 2/10
34/34 - 5s - loss: 214.8793 - loglik: -2.1286e+02 - logprior: -2.0214e+00
Epoch 3/10
34/34 - 5s - loss: 206.7330 - loglik: -2.0479e+02 - logprior: -1.9477e+00
Epoch 4/10
34/34 - 5s - loss: 204.5951 - loglik: -2.0269e+02 - logprior: -1.9076e+00
Epoch 5/10
34/34 - 5s - loss: 204.0718 - loglik: -2.0217e+02 - logprior: -1.9011e+00
Epoch 6/10
34/34 - 5s - loss: 203.8405 - loglik: -2.0195e+02 - logprior: -1.8907e+00
Epoch 7/10
34/34 - 5s - loss: 202.9563 - loglik: -2.0106e+02 - logprior: -1.8914e+00
Epoch 8/10
34/34 - 5s - loss: 203.1013 - loglik: -2.0121e+02 - logprior: -1.8864e+00
Fitted a model with MAP estimate = -202.6913
expansions: [(0, 2), (13, 1), (15, 1), (16, 4), (17, 1), (25, 1), (26, 1), (40, 1), (41, 1), (45, 1), (48, 1), (49, 1), (53, 1), (55, 1), (56, 1), (60, 1), (66, 1), (76, 1), (79, 1), (82, 2), (84, 1), (107, 1), (108, 1), (110, 1), (111, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 191.1287 - loglik: -1.8807e+02 - logprior: -3.0607e+00
Epoch 2/2
34/34 - 5s - loss: 181.9512 - loglik: -1.8074e+02 - logprior: -1.2076e+00
Fitted a model with MAP estimate = -180.1709
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 186.3118 - loglik: -1.8259e+02 - logprior: -3.7196e+00
Epoch 2/2
34/34 - 5s - loss: 182.5329 - loglik: -1.8077e+02 - logprior: -1.7596e+00
Fitted a model with MAP estimate = -180.5315
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 184.5884 - loglik: -1.8182e+02 - logprior: -2.7731e+00
Epoch 2/10
34/34 - 5s - loss: 181.6347 - loglik: -1.8068e+02 - logprior: -9.5803e-01
Epoch 3/10
34/34 - 5s - loss: 180.6145 - loglik: -1.7977e+02 - logprior: -8.4900e-01
Epoch 4/10
34/34 - 5s - loss: 179.4680 - loglik: -1.7870e+02 - logprior: -7.7046e-01
Epoch 5/10
34/34 - 5s - loss: 179.3319 - loglik: -1.7864e+02 - logprior: -6.9619e-01
Epoch 6/10
34/34 - 5s - loss: 179.8353 - loglik: -1.7919e+02 - logprior: -6.4433e-01
Fitted a model with MAP estimate = -178.6375
Time for alignment: 126.9320
Computed alignments with likelihoods: ['-179.8050', '-179.0567', '-180.2742', '-177.7562', '-178.6375']
Best model has likelihood: -177.7562  (prior= -0.6544 )
time for generating output: 0.2958
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.6329770003239391
Training of 5 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202c3d9fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ecb727490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f202c87e730>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f076c34c0>
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.9618 - loglik: -4.0131e+02 - logprior: -2.0651e+01
Epoch 2/10
10/10 - 2s - loss: 375.4353 - loglik: -3.7081e+02 - logprior: -4.6273e+00
Epoch 3/10
10/10 - 2s - loss: 330.5059 - loglik: -3.2827e+02 - logprior: -2.2406e+00
Epoch 4/10
10/10 - 2s - loss: 302.4750 - loglik: -3.0078e+02 - logprior: -1.6929e+00
Epoch 5/10
10/10 - 2s - loss: 288.7398 - loglik: -2.8727e+02 - logprior: -1.4743e+00
Epoch 6/10
10/10 - 2s - loss: 283.4207 - loglik: -2.8199e+02 - logprior: -1.4305e+00
Epoch 7/10
10/10 - 2s - loss: 278.9540 - loglik: -2.7744e+02 - logprior: -1.5146e+00
Epoch 8/10
10/10 - 2s - loss: 277.3953 - loglik: -2.7575e+02 - logprior: -1.6494e+00
Epoch 9/10
10/10 - 2s - loss: 274.6406 - loglik: -2.7294e+02 - logprior: -1.7052e+00
Epoch 10/10
10/10 - 2s - loss: 274.8600 - loglik: -2.7322e+02 - logprior: -1.6442e+00
Fitted a model with MAP estimate = -274.1888
expansions: [(4, 1), (5, 3), (9, 1), (21, 1), (22, 1), (23, 1), (24, 1), (29, 2), (56, 1), (73, 1), (80, 3), (81, 1), (82, 1), (83, 1), (85, 2), (86, 3), (98, 1), (101, 2), (103, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 299.5912 - loglik: -2.7589e+02 - logprior: -2.3703e+01
Epoch 2/2
10/10 - 2s - loss: 274.0883 - loglik: -2.6465e+02 - logprior: -9.4365e+00
Fitted a model with MAP estimate = -269.9049
expansions: [(0, 5)]
discards: [  0   6  37 103 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 281.8936 - loglik: -2.6319e+02 - logprior: -1.8703e+01
Epoch 2/2
10/10 - 2s - loss: 264.9720 - loglik: -2.6064e+02 - logprior: -4.3315e+00
Fitted a model with MAP estimate = -262.2764
expansions: []
discards: [1 2 3 4 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 280.1480 - loglik: -2.6179e+02 - logprior: -1.8362e+01
Epoch 2/10
10/10 - 2s - loss: 264.8130 - loglik: -2.6058e+02 - logprior: -4.2319e+00
Epoch 3/10
10/10 - 2s - loss: 261.8172 - loglik: -2.6035e+02 - logprior: -1.4689e+00
Epoch 4/10
10/10 - 2s - loss: 260.4829 - loglik: -2.5993e+02 - logprior: -5.5117e-01
Epoch 5/10
10/10 - 2s - loss: 258.7165 - loglik: -2.5881e+02 - logprior: 0.0922
Epoch 6/10
10/10 - 2s - loss: 258.7908 - loglik: -2.5934e+02 - logprior: 0.5536
Fitted a model with MAP estimate = -258.1247
Time for alignment: 55.5920
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 421.9374 - loglik: -4.0129e+02 - logprior: -2.0652e+01
Epoch 2/10
10/10 - 2s - loss: 375.3312 - loglik: -3.7070e+02 - logprior: -4.6285e+00
Epoch 3/10
10/10 - 2s - loss: 331.3651 - loglik: -3.2912e+02 - logprior: -2.2450e+00
Epoch 4/10
10/10 - 2s - loss: 299.1792 - loglik: -2.9743e+02 - logprior: -1.7513e+00
Epoch 5/10
10/10 - 2s - loss: 285.8284 - loglik: -2.8429e+02 - logprior: -1.5400e+00
Epoch 6/10
10/10 - 2s - loss: 279.9242 - loglik: -2.7835e+02 - logprior: -1.5755e+00
Epoch 7/10
10/10 - 2s - loss: 276.9362 - loglik: -2.7530e+02 - logprior: -1.6355e+00
Epoch 8/10
10/10 - 2s - loss: 275.5406 - loglik: -2.7389e+02 - logprior: -1.6463e+00
Epoch 9/10
10/10 - 2s - loss: 274.3524 - loglik: -2.7272e+02 - logprior: -1.6312e+00
Epoch 10/10
10/10 - 2s - loss: 274.4450 - loglik: -2.7286e+02 - logprior: -1.5800e+00
Fitted a model with MAP estimate = -273.7480
expansions: [(4, 1), (5, 3), (9, 1), (21, 1), (22, 1), (23, 1), (24, 1), (29, 2), (53, 1), (63, 1), (80, 2), (81, 1), (82, 1), (83, 1), (85, 2), (86, 3), (98, 1), (101, 2), (103, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 300.1153 - loglik: -2.7639e+02 - logprior: -2.3724e+01
Epoch 2/2
10/10 - 2s - loss: 274.4135 - loglik: -2.6490e+02 - logprior: -9.5106e+00
Fitted a model with MAP estimate = -270.6967
expansions: [(0, 6), (93, 1)]
discards: [  0   5   6  37 102 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 282.7166 - loglik: -2.6405e+02 - logprior: -1.8666e+01
Epoch 2/2
10/10 - 2s - loss: 265.5836 - loglik: -2.6131e+02 - logprior: -4.2773e+00
Fitted a model with MAP estimate = -262.7566
expansions: [(27, 1)]
discards: [1 2 3 4 5]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 279.9708 - loglik: -2.6163e+02 - logprior: -1.8339e+01
Epoch 2/10
10/10 - 2s - loss: 265.1635 - loglik: -2.6093e+02 - logprior: -4.2346e+00
Epoch 3/10
10/10 - 2s - loss: 260.8947 - loglik: -2.5941e+02 - logprior: -1.4879e+00
Epoch 4/10
10/10 - 2s - loss: 260.4164 - loglik: -2.5986e+02 - logprior: -5.5887e-01
Epoch 5/10
10/10 - 2s - loss: 258.8253 - loglik: -2.5891e+02 - logprior: 0.0866
Epoch 6/10
10/10 - 2s - loss: 258.3335 - loglik: -2.5889e+02 - logprior: 0.5613
Epoch 7/10
10/10 - 2s - loss: 257.9943 - loglik: -2.5874e+02 - logprior: 0.7455
Epoch 8/10
10/10 - 2s - loss: 257.0959 - loglik: -2.5796e+02 - logprior: 0.8667
Epoch 9/10
10/10 - 2s - loss: 256.4463 - loglik: -2.5742e+02 - logprior: 0.9713
Epoch 10/10
10/10 - 2s - loss: 255.8588 - loglik: -2.5687e+02 - logprior: 1.0086
Fitted a model with MAP estimate = -255.8403
Time for alignment: 63.7755
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 422.0157 - loglik: -4.0137e+02 - logprior: -2.0651e+01
Epoch 2/10
10/10 - 2s - loss: 375.6889 - loglik: -3.7106e+02 - logprior: -4.6260e+00
Epoch 3/10
10/10 - 2s - loss: 330.4077 - loglik: -3.2818e+02 - logprior: -2.2288e+00
Epoch 4/10
10/10 - 2s - loss: 300.0848 - loglik: -2.9839e+02 - logprior: -1.6962e+00
Epoch 5/10
10/10 - 2s - loss: 286.6193 - loglik: -2.8513e+02 - logprior: -1.4882e+00
Epoch 6/10
10/10 - 2s - loss: 280.9531 - loglik: -2.7950e+02 - logprior: -1.4574e+00
Epoch 7/10
10/10 - 2s - loss: 278.5287 - loglik: -2.7708e+02 - logprior: -1.4468e+00
Epoch 8/10
10/10 - 2s - loss: 277.0263 - loglik: -2.7562e+02 - logprior: -1.4108e+00
Epoch 9/10
10/10 - 2s - loss: 277.7534 - loglik: -2.7639e+02 - logprior: -1.3665e+00
Fitted a model with MAP estimate = -276.0433
expansions: [(0, 3), (4, 1), (5, 3), (9, 1), (21, 1), (22, 3), (29, 2), (56, 1), (63, 1), (80, 2), (81, 1), (82, 1), (83, 1), (85, 2), (86, 3), (98, 1), (101, 2), (103, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 299.9705 - loglik: -2.7401e+02 - logprior: -2.5963e+01
Epoch 2/2
10/10 - 2s - loss: 271.0647 - loglik: -2.6371e+02 - logprior: -7.3510e+00
Fitted a model with MAP estimate = -266.2178
expansions: [(28, 1), (97, 1)]
discards: [  1   2   3   4   9  10  41 106 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 282.6646 - loglik: -2.6384e+02 - logprior: -1.8823e+01
Epoch 2/2
10/10 - 2s - loss: 265.3625 - loglik: -2.6097e+02 - logprior: -4.3878e+00
Fitted a model with MAP estimate = -262.9287
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 279.0962 - loglik: -2.6072e+02 - logprior: -1.8372e+01
Epoch 2/10
10/10 - 2s - loss: 264.8820 - loglik: -2.6067e+02 - logprior: -4.2075e+00
Epoch 3/10
10/10 - 2s - loss: 261.6127 - loglik: -2.6013e+02 - logprior: -1.4858e+00
Epoch 4/10
10/10 - 2s - loss: 259.4667 - loglik: -2.5890e+02 - logprior: -5.6469e-01
Epoch 5/10
10/10 - 2s - loss: 258.6693 - loglik: -2.5880e+02 - logprior: 0.1315
Epoch 6/10
10/10 - 2s - loss: 258.5378 - loglik: -2.5911e+02 - logprior: 0.5702
Epoch 7/10
10/10 - 2s - loss: 257.2344 - loglik: -2.5798e+02 - logprior: 0.7465
Epoch 8/10
10/10 - 2s - loss: 257.6950 - loglik: -2.5857e+02 - logprior: 0.8747
Fitted a model with MAP estimate = -256.7303
Time for alignment: 58.6277
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.8904 - loglik: -4.0124e+02 - logprior: -2.0652e+01
Epoch 2/10
10/10 - 2s - loss: 375.5671 - loglik: -3.7094e+02 - logprior: -4.6263e+00
Epoch 3/10
10/10 - 2s - loss: 329.1608 - loglik: -3.2691e+02 - logprior: -2.2468e+00
Epoch 4/10
10/10 - 2s - loss: 297.3963 - loglik: -2.9561e+02 - logprior: -1.7883e+00
Epoch 5/10
10/10 - 2s - loss: 284.8421 - loglik: -2.8319e+02 - logprior: -1.6529e+00
Epoch 6/10
10/10 - 2s - loss: 280.5109 - loglik: -2.7888e+02 - logprior: -1.6290e+00
Epoch 7/10
10/10 - 2s - loss: 278.1118 - loglik: -2.7650e+02 - logprior: -1.6110e+00
Epoch 8/10
10/10 - 2s - loss: 276.5121 - loglik: -2.7491e+02 - logprior: -1.6016e+00
Epoch 9/10
10/10 - 2s - loss: 276.0684 - loglik: -2.7449e+02 - logprior: -1.5814e+00
Epoch 10/10
10/10 - 2s - loss: 275.5979 - loglik: -2.7405e+02 - logprior: -1.5492e+00
Fitted a model with MAP estimate = -274.8494
expansions: [(4, 1), (5, 3), (9, 1), (21, 1), (22, 1), (23, 1), (24, 1), (37, 1), (53, 1), (63, 1), (80, 2), (81, 2), (82, 1), (83, 1), (85, 2), (86, 3), (98, 1), (101, 2), (103, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 299.5455 - loglik: -2.7585e+02 - logprior: -2.3694e+01
Epoch 2/2
10/10 - 2s - loss: 274.6126 - loglik: -2.6519e+02 - logprior: -9.4255e+00
Fitted a model with MAP estimate = -270.0135
expansions: [(0, 6)]
discards: [  0   5 102 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 282.2004 - loglik: -2.6350e+02 - logprior: -1.8705e+01
Epoch 2/2
10/10 - 2s - loss: 264.6320 - loglik: -2.6030e+02 - logprior: -4.3318e+00
Fitted a model with MAP estimate = -262.1226
expansions: []
discards: [1 2 3 4 5]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 279.6743 - loglik: -2.6126e+02 - logprior: -1.8414e+01
Epoch 2/10
10/10 - 2s - loss: 264.5589 - loglik: -2.6028e+02 - logprior: -4.2796e+00
Epoch 3/10
10/10 - 2s - loss: 261.8423 - loglik: -2.6030e+02 - logprior: -1.5420e+00
Epoch 4/10
10/10 - 2s - loss: 259.2733 - loglik: -2.5867e+02 - logprior: -6.0203e-01
Epoch 5/10
10/10 - 2s - loss: 259.4210 - loglik: -2.5945e+02 - logprior: 0.0270
Fitted a model with MAP estimate = -258.4246
Time for alignment: 52.7681
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.9642 - loglik: -4.0131e+02 - logprior: -2.0654e+01
Epoch 2/10
10/10 - 2s - loss: 375.2908 - loglik: -3.7066e+02 - logprior: -4.6314e+00
Epoch 3/10
10/10 - 2s - loss: 329.5158 - loglik: -3.2728e+02 - logprior: -2.2367e+00
Epoch 4/10
10/10 - 2s - loss: 299.4837 - loglik: -2.9772e+02 - logprior: -1.7661e+00
Epoch 5/10
10/10 - 2s - loss: 287.0831 - loglik: -2.8555e+02 - logprior: -1.5326e+00
Epoch 6/10
10/10 - 2s - loss: 281.2136 - loglik: -2.7969e+02 - logprior: -1.5205e+00
Epoch 7/10
10/10 - 2s - loss: 277.8342 - loglik: -2.7626e+02 - logprior: -1.5738e+00
Epoch 8/10
10/10 - 2s - loss: 276.5110 - loglik: -2.7484e+02 - logprior: -1.6684e+00
Epoch 9/10
10/10 - 2s - loss: 275.4961 - loglik: -2.7380e+02 - logprior: -1.6987e+00
Epoch 10/10
10/10 - 2s - loss: 274.3101 - loglik: -2.7267e+02 - logprior: -1.6427e+00
Fitted a model with MAP estimate = -274.3427
expansions: [(4, 1), (5, 3), (9, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (73, 1), (80, 3), (81, 1), (82, 1), (83, 1), (85, 2), (86, 3), (98, 1), (101, 4), (103, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 291.2078 - loglik: -2.7219e+02 - logprior: -1.9014e+01
Epoch 2/2
10/10 - 2s - loss: 267.0507 - loglik: -2.6245e+02 - logprior: -4.6050e+00
Fitted a model with MAP estimate = -262.4860
expansions: []
discards: [  0   6   7 103 107 127 128]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 288.6472 - loglik: -2.6517e+02 - logprior: -2.3478e+01
Epoch 2/2
10/10 - 2s - loss: 271.0487 - loglik: -2.6177e+02 - logprior: -9.2787e+00
Fitted a model with MAP estimate = -268.8568
expansions: [(0, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 280.4829 - loglik: -2.6188e+02 - logprior: -1.8606e+01
Epoch 2/10
10/10 - 2s - loss: 264.9246 - loglik: -2.6072e+02 - logprior: -4.2057e+00
Epoch 3/10
10/10 - 2s - loss: 261.0800 - loglik: -2.5971e+02 - logprior: -1.3678e+00
Epoch 4/10
10/10 - 2s - loss: 260.0428 - loglik: -2.5969e+02 - logprior: -3.5462e-01
Epoch 5/10
10/10 - 2s - loss: 259.0721 - loglik: -2.5914e+02 - logprior: 0.0719
Epoch 6/10
10/10 - 2s - loss: 258.2458 - loglik: -2.5861e+02 - logprior: 0.3674
Epoch 7/10
10/10 - 2s - loss: 257.5099 - loglik: -2.5810e+02 - logprior: 0.5876
Epoch 8/10
10/10 - 2s - loss: 257.5815 - loglik: -2.5830e+02 - logprior: 0.7199
Fitted a model with MAP estimate = -256.8854
Time for alignment: 58.3951
Computed alignments with likelihoods: ['-258.1247', '-255.8403', '-256.7303', '-258.4246', '-256.8854']
Best model has likelihood: -255.8403  (prior= 1.0369 )
time for generating output: 0.1589
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.8267457180500659
Training of 5 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2001c69820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf5a7460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa3b11f70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fbd1f81f0>
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 270.8199 - loglik: -2.3353e+02 - logprior: -3.7294e+01
Epoch 2/10
10/10 - 1s - loss: 233.2749 - loglik: -2.2371e+02 - logprior: -9.5611e+00
Epoch 3/10
10/10 - 1s - loss: 216.2913 - loglik: -2.1189e+02 - logprior: -4.4013e+00
Epoch 4/10
10/10 - 1s - loss: 205.2014 - loglik: -2.0249e+02 - logprior: -2.7081e+00
Epoch 5/10
10/10 - 1s - loss: 198.8946 - loglik: -1.9672e+02 - logprior: -2.1710e+00
Epoch 6/10
10/10 - 1s - loss: 195.7055 - loglik: -1.9420e+02 - logprior: -1.5099e+00
Epoch 7/10
10/10 - 1s - loss: 194.3544 - loglik: -1.9351e+02 - logprior: -8.4063e-01
Epoch 8/10
10/10 - 1s - loss: 193.1550 - loglik: -1.9250e+02 - logprior: -6.5084e-01
Epoch 9/10
10/10 - 1s - loss: 192.7836 - loglik: -1.9227e+02 - logprior: -5.1070e-01
Epoch 10/10
10/10 - 1s - loss: 192.4294 - loglik: -1.9210e+02 - logprior: -3.2498e-01
Fitted a model with MAP estimate = -192.2501
expansions: [(0, 3), (6, 1), (22, 1), (31, 3), (34, 1), (45, 2), (47, 1), (51, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.3190 - loglik: -1.9265e+02 - logprior: -4.8669e+01
Epoch 2/2
10/10 - 1s - loss: 202.3804 - loglik: -1.8819e+02 - logprior: -1.4193e+01
Fitted a model with MAP estimate = -195.2904
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 230.7582 - loglik: -1.8869e+02 - logprior: -4.2069e+01
Epoch 2/2
10/10 - 1s - loss: 204.0511 - loglik: -1.8800e+02 - logprior: -1.6050e+01
Fitted a model with MAP estimate = -199.5217
expansions: [(0, 3), (31, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.9667 - loglik: -1.8764e+02 - logprior: -3.7328e+01
Epoch 2/10
10/10 - 1s - loss: 195.6303 - loglik: -1.8629e+02 - logprior: -9.3396e+00
Epoch 3/10
10/10 - 1s - loss: 189.1403 - loglik: -1.8599e+02 - logprior: -3.1542e+00
Epoch 4/10
10/10 - 1s - loss: 186.9718 - loglik: -1.8607e+02 - logprior: -8.9915e-01
Epoch 5/10
10/10 - 1s - loss: 185.3168 - loglik: -1.8548e+02 - logprior: 0.1631
Epoch 6/10
10/10 - 1s - loss: 184.9968 - loglik: -1.8566e+02 - logprior: 0.6672
Epoch 7/10
10/10 - 1s - loss: 184.3977 - loglik: -1.8527e+02 - logprior: 0.8740
Epoch 8/10
10/10 - 1s - loss: 184.1538 - loglik: -1.8523e+02 - logprior: 1.0732
Epoch 9/10
10/10 - 1s - loss: 183.9565 - loglik: -1.8529e+02 - logprior: 1.3337
Epoch 10/10
10/10 - 1s - loss: 183.7615 - loglik: -1.8527e+02 - logprior: 1.5132
Fitted a model with MAP estimate = -183.6007
Time for alignment: 38.8907
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.8665 - loglik: -2.3357e+02 - logprior: -3.7293e+01
Epoch 2/10
10/10 - 1s - loss: 233.6741 - loglik: -2.2411e+02 - logprior: -9.5598e+00
Epoch 3/10
10/10 - 1s - loss: 216.1554 - loglik: -2.1176e+02 - logprior: -4.3938e+00
Epoch 4/10
10/10 - 1s - loss: 205.0751 - loglik: -2.0241e+02 - logprior: -2.6616e+00
Epoch 5/10
10/10 - 1s - loss: 199.4612 - loglik: -1.9738e+02 - logprior: -2.0835e+00
Epoch 6/10
10/10 - 1s - loss: 196.2907 - loglik: -1.9471e+02 - logprior: -1.5813e+00
Epoch 7/10
10/10 - 1s - loss: 194.3527 - loglik: -1.9342e+02 - logprior: -9.3242e-01
Epoch 8/10
10/10 - 1s - loss: 193.5727 - loglik: -1.9293e+02 - logprior: -6.4012e-01
Epoch 9/10
10/10 - 1s - loss: 192.8509 - loglik: -1.9240e+02 - logprior: -4.5121e-01
Epoch 10/10
10/10 - 1s - loss: 192.5406 - loglik: -1.9228e+02 - logprior: -2.6281e-01
Fitted a model with MAP estimate = -192.3911
expansions: [(0, 3), (6, 1), (21, 2), (45, 2), (49, 1), (51, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 241.8344 - loglik: -1.9292e+02 - logprior: -4.8918e+01
Epoch 2/2
10/10 - 1s - loss: 203.3861 - loglik: -1.8915e+02 - logprior: -1.4232e+01
Fitted a model with MAP estimate = -196.3858
expansions: [(36, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.6130 - loglik: -1.8837e+02 - logprior: -3.4248e+01
Epoch 2/2
10/10 - 1s - loss: 195.6707 - loglik: -1.8735e+02 - logprior: -8.3174e+00
Fitted a model with MAP estimate = -191.6966
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 219.3669 - loglik: -1.8704e+02 - logprior: -3.2324e+01
Epoch 2/10
10/10 - 1s - loss: 194.4478 - loglik: -1.8666e+02 - logprior: -7.7908e+00
Epoch 3/10
10/10 - 1s - loss: 189.3247 - loglik: -1.8648e+02 - logprior: -2.8411e+00
Epoch 4/10
10/10 - 1s - loss: 187.3324 - loglik: -1.8646e+02 - logprior: -8.7006e-01
Epoch 5/10
10/10 - 1s - loss: 185.8674 - loglik: -1.8602e+02 - logprior: 0.1567
Epoch 6/10
10/10 - 1s - loss: 185.3700 - loglik: -1.8602e+02 - logprior: 0.6471
Epoch 7/10
10/10 - 1s - loss: 184.8823 - loglik: -1.8575e+02 - logprior: 0.8647
Epoch 8/10
10/10 - 1s - loss: 184.7105 - loglik: -1.8572e+02 - logprior: 1.0062
Epoch 9/10
10/10 - 1s - loss: 184.4730 - loglik: -1.8573e+02 - logprior: 1.2521
Epoch 10/10
10/10 - 1s - loss: 184.1560 - loglik: -1.8562e+02 - logprior: 1.4689
Fitted a model with MAP estimate = -184.0725
Time for alignment: 36.8419
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 270.8078 - loglik: -2.3351e+02 - logprior: -3.7293e+01
Epoch 2/10
10/10 - 1s - loss: 233.4519 - loglik: -2.2389e+02 - logprior: -9.5594e+00
Epoch 3/10
10/10 - 1s - loss: 216.0535 - loglik: -2.1167e+02 - logprior: -4.3827e+00
Epoch 4/10
10/10 - 1s - loss: 206.5146 - loglik: -2.0392e+02 - logprior: -2.5989e+00
Epoch 5/10
10/10 - 1s - loss: 201.8615 - loglik: -2.0009e+02 - logprior: -1.7707e+00
Epoch 6/10
10/10 - 1s - loss: 198.8351 - loglik: -1.9749e+02 - logprior: -1.3478e+00
Epoch 7/10
10/10 - 1s - loss: 197.0222 - loglik: -1.9604e+02 - logprior: -9.8570e-01
Epoch 8/10
10/10 - 1s - loss: 195.6277 - loglik: -1.9491e+02 - logprior: -7.1489e-01
Epoch 9/10
10/10 - 1s - loss: 194.6953 - loglik: -1.9415e+02 - logprior: -5.4690e-01
Epoch 10/10
10/10 - 1s - loss: 193.8639 - loglik: -1.9352e+02 - logprior: -3.4060e-01
Fitted a model with MAP estimate = -193.5796
expansions: [(0, 3), (6, 1), (26, 1), (30, 2), (45, 1), (48, 3), (53, 1), (65, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 80 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 242.2346 - loglik: -1.9342e+02 - logprior: -4.8815e+01
Epoch 2/2
10/10 - 1s - loss: 203.9181 - loglik: -1.8967e+02 - logprior: -1.4249e+01
Fitted a model with MAP estimate = -196.7212
expansions: [(36, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.6593 - loglik: -1.8834e+02 - logprior: -3.4315e+01
Epoch 2/2
10/10 - 1s - loss: 195.4982 - loglik: -1.8712e+02 - logprior: -8.3810e+00
Fitted a model with MAP estimate = -191.6432
expansions: [(35, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 219.5380 - loglik: -1.8718e+02 - logprior: -3.2356e+01
Epoch 2/10
10/10 - 1s - loss: 194.2692 - loglik: -1.8646e+02 - logprior: -7.8047e+00
Epoch 3/10
10/10 - 1s - loss: 189.2491 - loglik: -1.8645e+02 - logprior: -2.8039e+00
Epoch 4/10
10/10 - 1s - loss: 187.3297 - loglik: -1.8651e+02 - logprior: -8.1770e-01
Epoch 5/10
10/10 - 1s - loss: 186.1126 - loglik: -1.8631e+02 - logprior: 0.1987
Epoch 6/10
10/10 - 1s - loss: 185.5713 - loglik: -1.8627e+02 - logprior: 0.7014
Epoch 7/10
10/10 - 1s - loss: 185.1752 - loglik: -1.8608e+02 - logprior: 0.9080
Epoch 8/10
10/10 - 1s - loss: 184.7242 - loglik: -1.8578e+02 - logprior: 1.0606
Epoch 9/10
10/10 - 1s - loss: 184.3988 - loglik: -1.8570e+02 - logprior: 1.3036
Epoch 10/10
10/10 - 1s - loss: 184.4056 - loglik: -1.8593e+02 - logprior: 1.5216
Fitted a model with MAP estimate = -184.1674
Time for alignment: 37.9119
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.8437 - loglik: -2.3355e+02 - logprior: -3.7294e+01
Epoch 2/10
10/10 - 1s - loss: 233.7567 - loglik: -2.2420e+02 - logprior: -9.5572e+00
Epoch 3/10
10/10 - 1s - loss: 216.3896 - loglik: -2.1202e+02 - logprior: -4.3649e+00
Epoch 4/10
10/10 - 1s - loss: 205.2019 - loglik: -2.0255e+02 - logprior: -2.6500e+00
Epoch 5/10
10/10 - 1s - loss: 199.2194 - loglik: -1.9713e+02 - logprior: -2.0847e+00
Epoch 6/10
10/10 - 1s - loss: 196.4268 - loglik: -1.9483e+02 - logprior: -1.5937e+00
Epoch 7/10
10/10 - 1s - loss: 194.4803 - loglik: -1.9352e+02 - logprior: -9.5879e-01
Epoch 8/10
10/10 - 1s - loss: 193.4231 - loglik: -1.9274e+02 - logprior: -6.8751e-01
Epoch 9/10
10/10 - 1s - loss: 192.5752 - loglik: -1.9206e+02 - logprior: -5.1756e-01
Epoch 10/10
10/10 - 1s - loss: 192.2763 - loglik: -1.9194e+02 - logprior: -3.3900e-01
Fitted a model with MAP estimate = -192.0971
expansions: [(0, 3), (6, 1), (20, 1), (21, 1), (45, 2), (49, 1), (54, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.8236 - loglik: -1.9294e+02 - logprior: -4.8885e+01
Epoch 2/2
10/10 - 1s - loss: 203.2345 - loglik: -1.8902e+02 - logprior: -1.4212e+01
Fitted a model with MAP estimate = -196.3130
expansions: [(36, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 80 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 232.0294 - loglik: -1.8972e+02 - logprior: -4.2311e+01
Epoch 2/2
10/10 - 1s - loss: 204.7876 - loglik: -1.8872e+02 - logprior: -1.6071e+01
Fitted a model with MAP estimate = -200.2210
expansions: [(0, 3)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 225.4853 - loglik: -1.8825e+02 - logprior: -3.7234e+01
Epoch 2/10
10/10 - 1s - loss: 196.2402 - loglik: -1.8697e+02 - logprior: -9.2654e+00
Epoch 3/10
10/10 - 1s - loss: 189.6454 - loglik: -1.8645e+02 - logprior: -3.1912e+00
Epoch 4/10
10/10 - 1s - loss: 187.2397 - loglik: -1.8628e+02 - logprior: -9.5703e-01
Epoch 5/10
10/10 - 1s - loss: 186.2440 - loglik: -1.8636e+02 - logprior: 0.1158
Epoch 6/10
10/10 - 1s - loss: 185.3941 - loglik: -1.8599e+02 - logprior: 0.5979
Epoch 7/10
10/10 - 1s - loss: 185.0048 - loglik: -1.8582e+02 - logprior: 0.8141
Epoch 8/10
10/10 - 1s - loss: 184.5070 - loglik: -1.8552e+02 - logprior: 1.0168
Epoch 9/10
10/10 - 1s - loss: 184.5280 - loglik: -1.8581e+02 - logprior: 1.2787
Fitted a model with MAP estimate = -184.3001
Time for alignment: 35.8488
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.0876 - loglik: -2.3379e+02 - logprior: -3.7293e+01
Epoch 2/10
10/10 - 1s - loss: 233.3675 - loglik: -2.2381e+02 - logprior: -9.5556e+00
Epoch 3/10
10/10 - 1s - loss: 215.7613 - loglik: -2.1141e+02 - logprior: -4.3535e+00
Epoch 4/10
10/10 - 1s - loss: 205.2789 - loglik: -2.0267e+02 - logprior: -2.6060e+00
Epoch 5/10
10/10 - 1s - loss: 200.0372 - loglik: -1.9806e+02 - logprior: -1.9772e+00
Epoch 6/10
10/10 - 1s - loss: 197.1409 - loglik: -1.9569e+02 - logprior: -1.4524e+00
Epoch 7/10
10/10 - 1s - loss: 195.2047 - loglik: -1.9437e+02 - logprior: -8.3208e-01
Epoch 8/10
10/10 - 1s - loss: 194.1444 - loglik: -1.9360e+02 - logprior: -5.4779e-01
Epoch 9/10
10/10 - 1s - loss: 193.7321 - loglik: -1.9338e+02 - logprior: -3.5305e-01
Epoch 10/10
10/10 - 1s - loss: 193.1913 - loglik: -1.9303e+02 - logprior: -1.6088e-01
Fitted a model with MAP estimate = -193.0142
expansions: [(0, 3), (6, 1), (26, 4), (31, 2), (45, 2), (49, 1), (54, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 241.7490 - loglik: -1.9316e+02 - logprior: -4.8591e+01
Epoch 2/2
10/10 - 1s - loss: 202.2644 - loglik: -1.8807e+02 - logprior: -1.4192e+01
Fitted a model with MAP estimate = -195.2098
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 230.0757 - loglik: -1.8812e+02 - logprior: -4.1956e+01
Epoch 2/2
10/10 - 1s - loss: 203.6578 - loglik: -1.8767e+02 - logprior: -1.5986e+01
Fitted a model with MAP estimate = -199.0178
expansions: [(0, 3)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.3720 - loglik: -1.8709e+02 - logprior: -3.7281e+01
Epoch 2/10
10/10 - 1s - loss: 195.3983 - loglik: -1.8614e+02 - logprior: -9.2551e+00
Epoch 3/10
10/10 - 1s - loss: 188.9701 - loglik: -1.8589e+02 - logprior: -3.0801e+00
Epoch 4/10
10/10 - 1s - loss: 186.8086 - loglik: -1.8594e+02 - logprior: -8.6380e-01
Epoch 5/10
10/10 - 1s - loss: 185.1675 - loglik: -1.8536e+02 - logprior: 0.1897
Epoch 6/10
10/10 - 1s - loss: 184.6398 - loglik: -1.8532e+02 - logprior: 0.6801
Epoch 7/10
10/10 - 1s - loss: 184.1253 - loglik: -1.8503e+02 - logprior: 0.9066
Epoch 8/10
10/10 - 1s - loss: 183.5672 - loglik: -1.8468e+02 - logprior: 1.1169
Epoch 9/10
10/10 - 1s - loss: 183.5977 - loglik: -1.8499e+02 - logprior: 1.3873
Fitted a model with MAP estimate = -183.3103
Time for alignment: 35.7026
Computed alignments with likelihoods: ['-183.6007', '-184.0725', '-184.1674', '-184.3001', '-183.3103']
Best model has likelihood: -183.3103  (prior= 1.5157 )
time for generating output: 0.1686
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.7441253263707572
Training of 5 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f92e3a5b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f808d2af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f9221e220>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f06dc79d0>
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 960.0240 - loglik: -9.5876e+02 - logprior: -1.2656e+00
Epoch 2/10
43/43 - 27s - loss: 844.2658 - loglik: -8.4270e+02 - logprior: -1.5677e+00
Epoch 3/10
43/43 - 27s - loss: 832.9879 - loglik: -8.3142e+02 - logprior: -1.5655e+00
Epoch 4/10
43/43 - 27s - loss: 831.8581 - loglik: -8.3034e+02 - logprior: -1.5185e+00
Epoch 5/10
43/43 - 27s - loss: 828.3062 - loglik: -8.2677e+02 - logprior: -1.5333e+00
Epoch 6/10
43/43 - 27s - loss: 828.4797 - loglik: -8.2695e+02 - logprior: -1.5313e+00
Fitted a model with MAP estimate = -820.4740
expansions: [(0, 2), (16, 1), (20, 2), (21, 2), (22, 2), (23, 1), (24, 2), (30, 1), (33, 1), (39, 1), (41, 1), (42, 1), (44, 1), (45, 2), (46, 1), (56, 1), (58, 1), (61, 1), (64, 1), (79, 2), (80, 1), (81, 1), (90, 2), (91, 2), (92, 1), (95, 1), (101, 2), (102, 3), (121, 1), (129, 1), (130, 1), (131, 1), (144, 1), (145, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 3), (156, 2), (181, 2), (182, 4), (183, 2), (198, 2), (201, 2), (207, 1), (208, 1), (209, 1), (211, 1), (218, 1), (220, 3), (223, 1), (226, 1), (227, 2), (237, 1), (239, 4), (240, 1), (242, 1), (251, 1), (262, 2), (263, 1), (265, 1), (270, 3), (271, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 376 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 819.8832 - loglik: -8.1793e+02 - logprior: -1.9576e+00
Epoch 2/2
43/43 - 43s - loss: 801.2612 - loglik: -8.0031e+02 - logprior: -9.5164e-01
Fitted a model with MAP estimate = -798.7526
expansions: []
discards: [  1  26  27  30  64 105 121 122 140 141 203 210 242 243 244 263 267 293
 305 321 322 366]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 42s - loss: 802.7675 - loglik: -8.0162e+02 - logprior: -1.1519e+00
Epoch 2/2
43/43 - 39s - loss: 799.2753 - loglik: -7.9884e+02 - logprior: -4.3515e-01
Fitted a model with MAP estimate = -796.6122
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 354 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 58s - loss: 781.2896 - loglik: -7.8055e+02 - logprior: -7.3558e-01
Epoch 2/10
61/61 - 54s - loss: 776.4014 - loglik: -7.7591e+02 - logprior: -4.9508e-01
Epoch 3/10
61/61 - 55s - loss: 776.2973 - loglik: -7.7584e+02 - logprior: -4.5356e-01
Epoch 4/10
61/61 - 54s - loss: 775.0245 - loglik: -7.7462e+02 - logprior: -4.0915e-01
Epoch 5/10
61/61 - 55s - loss: 773.4299 - loglik: -7.7306e+02 - logprior: -3.7418e-01
Epoch 6/10
61/61 - 55s - loss: 772.4821 - loglik: -7.7215e+02 - logprior: -3.3508e-01
Epoch 7/10
61/61 - 54s - loss: 774.2672 - loglik: -7.7400e+02 - logprior: -2.6507e-01
Fitted a model with MAP estimate = -772.5615
Time for alignment: 1017.9873
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 958.2969 - loglik: -9.5703e+02 - logprior: -1.2674e+00
Epoch 2/10
43/43 - 27s - loss: 843.4486 - loglik: -8.4184e+02 - logprior: -1.6051e+00
Epoch 3/10
43/43 - 27s - loss: 833.0817 - loglik: -8.3149e+02 - logprior: -1.5920e+00
Epoch 4/10
43/43 - 27s - loss: 830.3569 - loglik: -8.2878e+02 - logprior: -1.5816e+00
Epoch 5/10
43/43 - 27s - loss: 830.1764 - loglik: -8.2861e+02 - logprior: -1.5632e+00
Epoch 6/10
43/43 - 27s - loss: 827.4927 - loglik: -8.2592e+02 - logprior: -1.5733e+00
Epoch 7/10
43/43 - 27s - loss: 826.9154 - loglik: -8.2535e+02 - logprior: -1.5622e+00
Epoch 8/10
43/43 - 27s - loss: 827.5842 - loglik: -8.2601e+02 - logprior: -1.5727e+00
Fitted a model with MAP estimate = -820.6803
expansions: [(0, 2), (16, 1), (20, 2), (21, 1), (22, 2), (23, 1), (24, 2), (30, 1), (33, 1), (37, 1), (38, 1), (42, 1), (44, 1), (45, 2), (46, 1), (55, 1), (58, 1), (61, 1), (64, 1), (79, 1), (80, 1), (81, 1), (85, 2), (89, 1), (90, 1), (95, 1), (97, 1), (98, 1), (101, 1), (120, 1), (126, 1), (129, 2), (131, 1), (144, 1), (145, 1), (148, 1), (153, 1), (154, 2), (155, 3), (157, 2), (181, 2), (182, 4), (183, 1), (201, 2), (205, 1), (206, 2), (207, 1), (208, 2), (209, 2), (219, 1), (224, 5), (226, 2), (227, 2), (237, 1), (239, 3), (240, 1), (242, 1), (251, 1), (262, 2), (263, 1), (265, 1), (270, 4), (271, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 374 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 816.2383 - loglik: -8.1429e+02 - logprior: -1.9469e+00
Epoch 2/2
43/43 - 43s - loss: 798.2361 - loglik: -7.9735e+02 - logprior: -8.8203e-01
Fitted a model with MAP estimate = -797.9402
expansions: []
discards: [  0   1  28  29  63 112 166 206 235 237 259 267 268 274 275 295 301 303
 319 360 364]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 42s - loss: 802.1954 - loglik: -8.0100e+02 - logprior: -1.1951e+00
Epoch 2/2
43/43 - 39s - loss: 798.3869 - loglik: -7.9811e+02 - logprior: -2.7978e-01
Fitted a model with MAP estimate = -797.5808
expansions: [(0, 2)]
discards: [281 301]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 781.0769 - loglik: -7.8028e+02 - logprior: -7.9649e-01
Epoch 2/10
61/61 - 54s - loss: 777.8754 - loglik: -7.7744e+02 - logprior: -4.3734e-01
Epoch 3/10
61/61 - 54s - loss: 775.9995 - loglik: -7.7558e+02 - logprior: -4.2407e-01
Epoch 4/10
61/61 - 54s - loss: 774.6232 - loglik: -7.7426e+02 - logprior: -3.6825e-01
Epoch 5/10
61/61 - 55s - loss: 774.5681 - loglik: -7.7424e+02 - logprior: -3.2767e-01
Epoch 6/10
61/61 - 54s - loss: 773.4029 - loglik: -7.7315e+02 - logprior: -2.5495e-01
Epoch 7/10
61/61 - 54s - loss: 773.6546 - loglik: -7.7343e+02 - logprior: -2.2333e-01
Fitted a model with MAP estimate = -772.6667
Time for alignment: 1067.5293
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 961.6414 - loglik: -9.6038e+02 - logprior: -1.2583e+00
Epoch 2/10
43/43 - 27s - loss: 845.4312 - loglik: -8.4378e+02 - logprior: -1.6470e+00
Epoch 3/10
43/43 - 27s - loss: 832.6187 - loglik: -8.3098e+02 - logprior: -1.6390e+00
Epoch 4/10
43/43 - 27s - loss: 830.2548 - loglik: -8.2866e+02 - logprior: -1.5948e+00
Epoch 5/10
43/43 - 27s - loss: 829.8766 - loglik: -8.2822e+02 - logprior: -1.6561e+00
Epoch 6/10
43/43 - 27s - loss: 826.9498 - loglik: -8.2530e+02 - logprior: -1.6534e+00
Epoch 7/10
43/43 - 27s - loss: 826.7512 - loglik: -8.2512e+02 - logprior: -1.6280e+00
Epoch 8/10
43/43 - 27s - loss: 827.1706 - loglik: -8.2557e+02 - logprior: -1.6013e+00
Fitted a model with MAP estimate = -820.1262
expansions: [(0, 2), (16, 1), (20, 2), (21, 2), (22, 2), (23, 1), (24, 2), (30, 1), (33, 1), (39, 1), (41, 1), (42, 1), (44, 1), (45, 2), (46, 1), (55, 1), (58, 1), (61, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (96, 1), (98, 1), (102, 1), (103, 2), (120, 1), (121, 1), (129, 1), (130, 1), (131, 1), (143, 1), (145, 1), (148, 1), (153, 1), (154, 1), (156, 2), (157, 2), (168, 1), (181, 1), (182, 2), (183, 1), (184, 2), (188, 1), (198, 1), (201, 2), (206, 2), (207, 1), (209, 1), (212, 1), (219, 1), (221, 3), (224, 1), (227, 1), (228, 2), (238, 1), (240, 3), (241, 1), (243, 1), (246, 1), (251, 1), (258, 1), (262, 1), (263, 1), (265, 1), (270, 4), (271, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 46s - loss: 818.1708 - loglik: -8.1623e+02 - logprior: -1.9372e+00
Epoch 2/2
43/43 - 43s - loss: 799.3036 - loglik: -7.9842e+02 - logprior: -8.8404e-01
Fitted a model with MAP estimate = -798.3418
expansions: []
discards: [  0   1  28  29  30  64 113 138 205 236 239 260 267 268 289 300 316 357
 361]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 352 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 44s - loss: 802.5826 - loglik: -8.0141e+02 - logprior: -1.1701e+00
Epoch 2/2
43/43 - 39s - loss: 798.9286 - loglik: -7.9865e+02 - logprior: -2.7950e-01
Fitted a model with MAP estimate = -797.4838
expansions: [(0, 2)]
discards: [300]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 781.3532 - loglik: -7.8057e+02 - logprior: -7.8401e-01
Epoch 2/10
61/61 - 54s - loss: 777.4174 - loglik: -7.7698e+02 - logprior: -4.3243e-01
Epoch 3/10
61/61 - 54s - loss: 775.2651 - loglik: -7.7488e+02 - logprior: -3.8532e-01
Epoch 4/10
61/61 - 55s - loss: 774.8774 - loglik: -7.7452e+02 - logprior: -3.5913e-01
Epoch 5/10
61/61 - 55s - loss: 774.2574 - loglik: -7.7395e+02 - logprior: -3.1062e-01
Epoch 6/10
61/61 - 54s - loss: 772.9443 - loglik: -7.7268e+02 - logprior: -2.6820e-01
Epoch 7/10
61/61 - 54s - loss: 774.7906 - loglik: -7.7457e+02 - logprior: -2.1849e-01
Fitted a model with MAP estimate = -772.6021
Time for alignment: 1065.2022
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 960.6135 - loglik: -9.5937e+02 - logprior: -1.2463e+00
Epoch 2/10
43/43 - 27s - loss: 847.7430 - loglik: -8.4617e+02 - logprior: -1.5734e+00
Epoch 3/10
43/43 - 27s - loss: 835.2808 - loglik: -8.3369e+02 - logprior: -1.5859e+00
Epoch 4/10
43/43 - 27s - loss: 832.9666 - loglik: -8.3140e+02 - logprior: -1.5631e+00
Epoch 5/10
43/43 - 27s - loss: 831.6276 - loglik: -8.3004e+02 - logprior: -1.5895e+00
Epoch 6/10
43/43 - 27s - loss: 829.5164 - loglik: -8.2794e+02 - logprior: -1.5733e+00
Epoch 7/10
43/43 - 27s - loss: 829.0706 - loglik: -8.2752e+02 - logprior: -1.5522e+00
Epoch 8/10
43/43 - 27s - loss: 829.5352 - loglik: -8.2800e+02 - logprior: -1.5304e+00
Fitted a model with MAP estimate = -821.2801
expansions: [(0, 2), (16, 1), (20, 2), (21, 2), (22, 2), (23, 1), (24, 2), (30, 1), (33, 1), (37, 1), (38, 1), (42, 1), (44, 2), (45, 1), (46, 1), (56, 1), (58, 1), (61, 1), (77, 1), (80, 2), (81, 1), (82, 2), (90, 2), (91, 2), (92, 1), (95, 1), (97, 1), (98, 1), (102, 2), (123, 1), (129, 2), (130, 1), (131, 1), (133, 2), (143, 1), (144, 1), (147, 1), (152, 1), (153, 2), (154, 3), (181, 2), (182, 4), (183, 2), (198, 2), (201, 2), (205, 3), (207, 1), (211, 1), (218, 1), (220, 3), (223, 1), (226, 1), (227, 2), (237, 1), (239, 3), (240, 1), (246, 1), (251, 1), (262, 2), (263, 1), (265, 1), (270, 3), (271, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 376 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 822.1285 - loglik: -8.2013e+02 - logprior: -1.9948e+00
Epoch 2/2
43/43 - 43s - loss: 799.3906 - loglik: -7.9841e+02 - logprior: -9.7873e-01
Fitted a model with MAP estimate = -797.5058
expansions: []
discards: [  0   1  28  29  30  66 108 109 122 126 141 171 178 205 242 243 245 263
 267 274 295 306 322 366]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 352 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 803.8225 - loglik: -8.0258e+02 - logprior: -1.2431e+00
Epoch 2/2
43/43 - 39s - loss: 799.0556 - loglik: -7.9876e+02 - logprior: -2.9152e-01
Fitted a model with MAP estimate = -797.1375
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 354 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 781.4497 - loglik: -7.8064e+02 - logprior: -8.1370e-01
Epoch 2/10
61/61 - 54s - loss: 777.3094 - loglik: -7.7687e+02 - logprior: -4.3552e-01
Epoch 3/10
61/61 - 55s - loss: 774.2966 - loglik: -7.7389e+02 - logprior: -4.0568e-01
Epoch 4/10
61/61 - 55s - loss: 774.0936 - loglik: -7.7373e+02 - logprior: -3.6449e-01
Epoch 5/10
61/61 - 54s - loss: 775.1376 - loglik: -7.7481e+02 - logprior: -3.2452e-01
Fitted a model with MAP estimate = -773.3517
Time for alignment: 960.8255
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 960.3229 - loglik: -9.5908e+02 - logprior: -1.2412e+00
Epoch 2/10
43/43 - 27s - loss: 847.2260 - loglik: -8.4568e+02 - logprior: -1.5421e+00
Epoch 3/10
43/43 - 27s - loss: 836.2673 - loglik: -8.3474e+02 - logprior: -1.5234e+00
Epoch 4/10
43/43 - 27s - loss: 833.0242 - loglik: -8.3154e+02 - logprior: -1.4824e+00
Epoch 5/10
43/43 - 27s - loss: 832.0789 - loglik: -8.3058e+02 - logprior: -1.4962e+00
Epoch 6/10
43/43 - 27s - loss: 831.1523 - loglik: -8.2960e+02 - logprior: -1.5557e+00
Epoch 7/10
43/43 - 27s - loss: 831.3635 - loglik: -8.2985e+02 - logprior: -1.5159e+00
Fitted a model with MAP estimate = -822.9144
expansions: [(0, 2), (16, 1), (20, 2), (21, 2), (22, 2), (23, 1), (24, 2), (30, 1), (33, 1), (37, 1), (38, 1), (42, 1), (44, 2), (45, 2), (46, 1), (55, 1), (58, 1), (61, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (91, 2), (92, 1), (96, 1), (97, 1), (98, 1), (102, 2), (123, 1), (130, 2), (132, 1), (134, 2), (144, 1), (145, 1), (148, 1), (153, 1), (154, 2), (155, 3), (156, 2), (183, 5), (184, 2), (202, 2), (204, 1), (209, 1), (210, 1), (212, 1), (219, 1), (221, 2), (222, 1), (223, 1), (227, 1), (228, 2), (239, 1), (240, 6), (246, 1), (251, 1), (262, 2), (263, 1), (265, 1), (270, 3), (271, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 375 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 820.2833 - loglik: -8.1836e+02 - logprior: -1.9226e+00
Epoch 2/2
43/43 - 43s - loss: 800.1185 - loglik: -7.9919e+02 - logprior: -9.2698e-01
Fitted a model with MAP estimate = -798.2397
expansions: []
discards: [  0   1  26  27  30  63  64 111 121 123 141 171 178 211 243 244 246 299
 304 321 322 365]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 804.7100 - loglik: -8.0340e+02 - logprior: -1.3146e+00
Epoch 2/2
43/43 - 39s - loss: 797.4764 - loglik: -7.9714e+02 - logprior: -3.3793e-01
Fitted a model with MAP estimate = -797.5723
expansions: [(0, 2)]
discards: [280 301]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 59s - loss: 781.0351 - loglik: -7.8024e+02 - logprior: -7.9595e-01
Epoch 2/10
61/61 - 54s - loss: 777.2540 - loglik: -7.7683e+02 - logprior: -4.2889e-01
Epoch 3/10
61/61 - 54s - loss: 775.9458 - loglik: -7.7557e+02 - logprior: -3.7425e-01
Epoch 4/10
61/61 - 54s - loss: 772.9788 - loglik: -7.7263e+02 - logprior: -3.4971e-01
Epoch 5/10
61/61 - 54s - loss: 774.9717 - loglik: -7.7468e+02 - logprior: -2.9116e-01
Fitted a model with MAP estimate = -773.1443
Time for alignment: 933.1473
Computed alignments with likelihoods: ['-772.5615', '-772.6667', '-772.6021', '-773.3517', '-773.1443']
Best model has likelihood: -772.5615  (prior= -0.1981 )
time for generating output: 0.4105
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.8135941722476795
Training of 5 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f8105aa00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f92eb86d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff9254670>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1ecb55c430>
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 252.2776 - loglik: -1.9564e+02 - logprior: -5.6634e+01
Epoch 2/10
10/10 - 1s - loss: 193.3407 - loglik: -1.7792e+02 - logprior: -1.5419e+01
Epoch 3/10
10/10 - 1s - loss: 167.3086 - loglik: -1.5985e+02 - logprior: -7.4602e+00
Epoch 4/10
10/10 - 1s - loss: 155.6987 - loglik: -1.5124e+02 - logprior: -4.4615e+00
Epoch 5/10
10/10 - 1s - loss: 152.3137 - loglik: -1.4932e+02 - logprior: -2.9955e+00
Epoch 6/10
10/10 - 1s - loss: 149.9868 - loglik: -1.4785e+02 - logprior: -2.1380e+00
Epoch 7/10
10/10 - 1s - loss: 148.5775 - loglik: -1.4698e+02 - logprior: -1.5997e+00
Epoch 8/10
10/10 - 1s - loss: 148.0372 - loglik: -1.4671e+02 - logprior: -1.3246e+00
Epoch 9/10
10/10 - 1s - loss: 147.3241 - loglik: -1.4615e+02 - logprior: -1.1728e+00
Epoch 10/10
10/10 - 1s - loss: 146.9176 - loglik: -1.4586e+02 - logprior: -1.0589e+00
Fitted a model with MAP estimate = -146.6919
expansions: [(12, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.6040 - loglik: -1.4724e+02 - logprior: -6.4366e+01
Epoch 2/2
10/10 - 1s - loss: 171.8055 - loglik: -1.4452e+02 - logprior: -2.7288e+01
Fitted a model with MAP estimate = -165.8460
expansions: [(0, 2), (12, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.7360 - loglik: -1.4289e+02 - logprior: -5.2844e+01
Epoch 2/2
10/10 - 1s - loss: 156.3722 - loglik: -1.4183e+02 - logprior: -1.4547e+01
Fitted a model with MAP estimate = -150.3419
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 205.2009 - loglik: -1.4281e+02 - logprior: -6.2388e+01
Epoch 2/10
10/10 - 1s - loss: 163.1173 - loglik: -1.4144e+02 - logprior: -2.1675e+01
Epoch 3/10
10/10 - 1s - loss: 151.1502 - loglik: -1.4255e+02 - logprior: -8.6035e+00
Epoch 4/10
10/10 - 1s - loss: 144.8054 - loglik: -1.4085e+02 - logprior: -3.9582e+00
Epoch 5/10
10/10 - 1s - loss: 143.6996 - loglik: -1.4145e+02 - logprior: -2.2483e+00
Epoch 6/10
10/10 - 1s - loss: 142.6295 - loglik: -1.4122e+02 - logprior: -1.4072e+00
Epoch 7/10
10/10 - 1s - loss: 141.9151 - loglik: -1.4112e+02 - logprior: -7.9817e-01
Epoch 8/10
10/10 - 1s - loss: 141.4104 - loglik: -1.4103e+02 - logprior: -3.7724e-01
Epoch 9/10
10/10 - 1s - loss: 141.4894 - loglik: -1.4134e+02 - logprior: -1.4674e-01
Fitted a model with MAP estimate = -140.8320
Time for alignment: 42.3797
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 252.7336 - loglik: -1.9610e+02 - logprior: -5.6634e+01
Epoch 2/10
10/10 - 1s - loss: 192.8991 - loglik: -1.7748e+02 - logprior: -1.5419e+01
Epoch 3/10
10/10 - 1s - loss: 167.0011 - loglik: -1.5955e+02 - logprior: -7.4467e+00
Epoch 4/10
10/10 - 1s - loss: 156.2701 - loglik: -1.5180e+02 - logprior: -4.4700e+00
Epoch 5/10
10/10 - 1s - loss: 152.1356 - loglik: -1.4911e+02 - logprior: -3.0306e+00
Epoch 6/10
10/10 - 1s - loss: 149.8115 - loglik: -1.4764e+02 - logprior: -2.1700e+00
Epoch 7/10
10/10 - 1s - loss: 149.3089 - loglik: -1.4770e+02 - logprior: -1.6128e+00
Epoch 8/10
10/10 - 1s - loss: 148.2363 - loglik: -1.4693e+02 - logprior: -1.3029e+00
Epoch 9/10
10/10 - 1s - loss: 148.2572 - loglik: -1.4709e+02 - logprior: -1.1676e+00
Fitted a model with MAP estimate = -147.7251
expansions: [(11, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.2448 - loglik: -1.4690e+02 - logprior: -6.4344e+01
Epoch 2/2
10/10 - 1s - loss: 172.7249 - loglik: -1.4545e+02 - logprior: -2.7271e+01
Fitted a model with MAP estimate = -165.9551
expansions: [(0, 3), (12, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.7187 - loglik: -1.4234e+02 - logprior: -5.3382e+01
Epoch 2/2
10/10 - 1s - loss: 156.0832 - loglik: -1.4129e+02 - logprior: -1.4790e+01
Fitted a model with MAP estimate = -149.2580
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 205.0697 - loglik: -1.4191e+02 - logprior: -6.3161e+01
Epoch 2/10
10/10 - 1s - loss: 166.8166 - loglik: -1.4140e+02 - logprior: -2.5418e+01
Epoch 3/10
10/10 - 1s - loss: 154.1147 - loglik: -1.4127e+02 - logprior: -1.2844e+01
Epoch 4/10
10/10 - 1s - loss: 145.2263 - loglik: -1.4052e+02 - logprior: -4.7109e+00
Epoch 5/10
10/10 - 1s - loss: 142.4748 - loglik: -1.4027e+02 - logprior: -2.2020e+00
Epoch 6/10
10/10 - 1s - loss: 142.1715 - loglik: -1.4090e+02 - logprior: -1.2674e+00
Epoch 7/10
10/10 - 1s - loss: 140.4391 - loglik: -1.3976e+02 - logprior: -6.7701e-01
Epoch 8/10
10/10 - 1s - loss: 140.4527 - loglik: -1.4018e+02 - logprior: -2.6905e-01
Fitted a model with MAP estimate = -140.0133
Time for alignment: 38.8769
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 253.0322 - loglik: -1.9640e+02 - logprior: -5.6633e+01
Epoch 2/10
10/10 - 1s - loss: 192.9271 - loglik: -1.7750e+02 - logprior: -1.5422e+01
Epoch 3/10
10/10 - 1s - loss: 167.8048 - loglik: -1.6033e+02 - logprior: -7.4789e+00
Epoch 4/10
10/10 - 1s - loss: 155.7611 - loglik: -1.5129e+02 - logprior: -4.4717e+00
Epoch 5/10
10/10 - 1s - loss: 151.6138 - loglik: -1.4862e+02 - logprior: -2.9971e+00
Epoch 6/10
10/10 - 1s - loss: 149.6720 - loglik: -1.4751e+02 - logprior: -2.1658e+00
Epoch 7/10
10/10 - 1s - loss: 148.2221 - loglik: -1.4658e+02 - logprior: -1.6408e+00
Epoch 8/10
10/10 - 1s - loss: 147.5433 - loglik: -1.4621e+02 - logprior: -1.3331e+00
Epoch 9/10
10/10 - 1s - loss: 146.9713 - loglik: -1.4578e+02 - logprior: -1.1882e+00
Epoch 10/10
10/10 - 1s - loss: 147.6202 - loglik: -1.4656e+02 - logprior: -1.0565e+00
Fitted a model with MAP estimate = -146.6419
expansions: [(12, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.7754 - loglik: -1.4729e+02 - logprior: -6.4482e+01
Epoch 2/2
10/10 - 1s - loss: 171.1665 - loglik: -1.4382e+02 - logprior: -2.7352e+01
Fitted a model with MAP estimate = -165.3996
expansions: [(0, 3), (12, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.7504 - loglik: -1.4258e+02 - logprior: -5.3173e+01
Epoch 2/2
10/10 - 1s - loss: 157.1945 - loglik: -1.4245e+02 - logprior: -1.4747e+01
Fitted a model with MAP estimate = -150.4979
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 206.7039 - loglik: -1.4359e+02 - logprior: -6.3119e+01
Epoch 2/10
10/10 - 1s - loss: 167.5239 - loglik: -1.4251e+02 - logprior: -2.5011e+01
Epoch 3/10
10/10 - 1s - loss: 154.4245 - loglik: -1.4252e+02 - logprior: -1.1902e+01
Epoch 4/10
10/10 - 1s - loss: 146.2723 - loglik: -1.4175e+02 - logprior: -4.5209e+00
Epoch 5/10
10/10 - 1s - loss: 144.1504 - loglik: -1.4186e+02 - logprior: -2.2932e+00
Epoch 6/10
10/10 - 1s - loss: 142.8366 - loglik: -1.4145e+02 - logprior: -1.3872e+00
Epoch 7/10
10/10 - 1s - loss: 142.3501 - loglik: -1.4157e+02 - logprior: -7.8247e-01
Epoch 8/10
10/10 - 1s - loss: 142.2619 - loglik: -1.4189e+02 - logprior: -3.7073e-01
Epoch 9/10
10/10 - 1s - loss: 141.0535 - loglik: -1.4091e+02 - logprior: -1.4595e-01
Epoch 10/10
10/10 - 1s - loss: 141.3002 - loglik: -1.4132e+02 - logprior: 0.0223
Fitted a model with MAP estimate = -141.0329
Time for alignment: 44.5291
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 252.1706 - loglik: -1.9554e+02 - logprior: -5.6632e+01
Epoch 2/10
10/10 - 1s - loss: 193.6604 - loglik: -1.7824e+02 - logprior: -1.5420e+01
Epoch 3/10
10/10 - 1s - loss: 166.8004 - loglik: -1.5933e+02 - logprior: -7.4685e+00
Epoch 4/10
10/10 - 1s - loss: 155.7614 - loglik: -1.5130e+02 - logprior: -4.4564e+00
Epoch 5/10
10/10 - 1s - loss: 151.4436 - loglik: -1.4845e+02 - logprior: -2.9941e+00
Epoch 6/10
10/10 - 1s - loss: 149.7592 - loglik: -1.4764e+02 - logprior: -2.1190e+00
Epoch 7/10
10/10 - 1s - loss: 148.7121 - loglik: -1.4713e+02 - logprior: -1.5838e+00
Epoch 8/10
10/10 - 1s - loss: 148.0813 - loglik: -1.4679e+02 - logprior: -1.2941e+00
Epoch 9/10
10/10 - 1s - loss: 146.7550 - loglik: -1.4560e+02 - logprior: -1.1515e+00
Epoch 10/10
10/10 - 1s - loss: 146.5579 - loglik: -1.4552e+02 - logprior: -1.0341e+00
Fitted a model with MAP estimate = -146.5955
expansions: [(12, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.3824 - loglik: -1.4702e+02 - logprior: -6.4360e+01
Epoch 2/2
10/10 - 1s - loss: 172.0319 - loglik: -1.4476e+02 - logprior: -2.7273e+01
Fitted a model with MAP estimate = -165.7809
expansions: [(0, 3), (12, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 196.3625 - loglik: -1.4326e+02 - logprior: -5.3107e+01
Epoch 2/2
10/10 - 1s - loss: 156.9513 - loglik: -1.4219e+02 - logprior: -1.4758e+01
Fitted a model with MAP estimate = -150.5696
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 206.7143 - loglik: -1.4347e+02 - logprior: -6.3247e+01
Epoch 2/10
10/10 - 1s - loss: 168.6205 - loglik: -1.4297e+02 - logprior: -2.5646e+01
Epoch 3/10
10/10 - 1s - loss: 156.4255 - loglik: -1.4300e+02 - logprior: -1.3423e+01
Epoch 4/10
10/10 - 1s - loss: 146.3957 - loglik: -1.4143e+02 - logprior: -4.9641e+00
Epoch 5/10
10/10 - 1s - loss: 144.6294 - loglik: -1.4231e+02 - logprior: -2.3235e+00
Epoch 6/10
10/10 - 1s - loss: 142.9955 - loglik: -1.4161e+02 - logprior: -1.3854e+00
Epoch 7/10
10/10 - 1s - loss: 142.7593 - loglik: -1.4197e+02 - logprior: -7.8620e-01
Epoch 8/10
10/10 - 1s - loss: 142.1529 - loglik: -1.4176e+02 - logprior: -3.9197e-01
Epoch 9/10
10/10 - 1s - loss: 141.5368 - loglik: -1.4136e+02 - logprior: -1.7784e-01
Epoch 10/10
10/10 - 1s - loss: 141.0467 - loglik: -1.4106e+02 - logprior: 0.0084
Fitted a model with MAP estimate = -141.1107
Time for alignment: 42.7092
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 252.7926 - loglik: -1.9616e+02 - logprior: -5.6633e+01
Epoch 2/10
10/10 - 1s - loss: 192.9057 - loglik: -1.7750e+02 - logprior: -1.5409e+01
Epoch 3/10
10/10 - 1s - loss: 167.3351 - loglik: -1.5988e+02 - logprior: -7.4584e+00
Epoch 4/10
10/10 - 1s - loss: 155.7855 - loglik: -1.5132e+02 - logprior: -4.4702e+00
Epoch 5/10
10/10 - 1s - loss: 151.3978 - loglik: -1.4838e+02 - logprior: -3.0167e+00
Epoch 6/10
10/10 - 1s - loss: 149.7371 - loglik: -1.4756e+02 - logprior: -2.1756e+00
Epoch 7/10
10/10 - 1s - loss: 148.4327 - loglik: -1.4679e+02 - logprior: -1.6439e+00
Epoch 8/10
10/10 - 1s - loss: 148.2459 - loglik: -1.4691e+02 - logprior: -1.3347e+00
Epoch 9/10
10/10 - 1s - loss: 147.1611 - loglik: -1.4597e+02 - logprior: -1.1864e+00
Epoch 10/10
10/10 - 1s - loss: 147.3423 - loglik: -1.4626e+02 - logprior: -1.0817e+00
Fitted a model with MAP estimate = -146.6119
expansions: [(12, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.5192 - loglik: -1.4713e+02 - logprior: -6.4392e+01
Epoch 2/2
10/10 - 1s - loss: 171.4965 - loglik: -1.4421e+02 - logprior: -2.7289e+01
Fitted a model with MAP estimate = -165.8103
expansions: [(0, 2), (12, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 195.8693 - loglik: -1.4301e+02 - logprior: -5.2859e+01
Epoch 2/2
10/10 - 1s - loss: 156.1286 - loglik: -1.4159e+02 - logprior: -1.4542e+01
Fitted a model with MAP estimate = -150.3009
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 205.0414 - loglik: -1.4270e+02 - logprior: -6.2343e+01
Epoch 2/10
10/10 - 1s - loss: 163.1862 - loglik: -1.4165e+02 - logprior: -2.1536e+01
Epoch 3/10
10/10 - 1s - loss: 150.6527 - loglik: -1.4212e+02 - logprior: -8.5305e+00
Epoch 4/10
10/10 - 1s - loss: 145.2923 - loglik: -1.4135e+02 - logprior: -3.9398e+00
Epoch 5/10
10/10 - 1s - loss: 143.5978 - loglik: -1.4136e+02 - logprior: -2.2401e+00
Epoch 6/10
10/10 - 1s - loss: 142.2537 - loglik: -1.4085e+02 - logprior: -1.4049e+00
Epoch 7/10
10/10 - 1s - loss: 142.0247 - loglik: -1.4123e+02 - logprior: -7.9342e-01
Epoch 8/10
10/10 - 1s - loss: 141.5897 - loglik: -1.4124e+02 - logprior: -3.5337e-01
Epoch 9/10
10/10 - 1s - loss: 140.7439 - loglik: -1.4066e+02 - logprior: -8.4583e-02
Epoch 10/10
10/10 - 1s - loss: 141.1668 - loglik: -1.4126e+02 - logprior: 0.0915
Fitted a model with MAP estimate = -140.5121
Time for alignment: 42.1974
Computed alignments with likelihoods: ['-140.8320', '-140.0133', '-141.0329', '-141.1107', '-140.5121']
Best model has likelihood: -140.0133  (prior= -0.1317 )
time for generating output: 0.1048
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.8904299583911235
Training of 5 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdfe0cd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91f7e3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5adee80>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f92de9430>
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 540.2373 - loglik: -4.7358e+02 - logprior: -6.6659e+01
Epoch 2/10
10/10 - 2s - loss: 445.4433 - loglik: -4.3235e+02 - logprior: -1.3093e+01
Epoch 3/10
10/10 - 2s - loss: 388.5014 - loglik: -3.8445e+02 - logprior: -4.0500e+00
Epoch 4/10
10/10 - 2s - loss: 357.1645 - loglik: -3.5587e+02 - logprior: -1.2896e+00
Epoch 5/10
10/10 - 2s - loss: 344.4854 - loglik: -3.4519e+02 - logprior: 0.7072
Epoch 6/10
10/10 - 2s - loss: 339.1407 - loglik: -3.4078e+02 - logprior: 1.6346
Epoch 7/10
10/10 - 2s - loss: 335.4956 - loglik: -3.3768e+02 - logprior: 2.1843
Epoch 8/10
10/10 - 2s - loss: 333.6324 - loglik: -3.3613e+02 - logprior: 2.4962
Epoch 9/10
10/10 - 2s - loss: 330.5299 - loglik: -3.3309e+02 - logprior: 2.5628
Epoch 10/10
10/10 - 2s - loss: 329.1209 - loglik: -3.3183e+02 - logprior: 2.7068
Fitted a model with MAP estimate = -328.1706
expansions: [(14, 2), (18, 3), (21, 1), (25, 1), (29, 2), (37, 2), (38, 2), (39, 1), (53, 1), (56, 1), (58, 1), (79, 2), (80, 6), (81, 1), (90, 3), (101, 2), (110, 1), (111, 3), (120, 1), (123, 1), (127, 1), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 411.8970 - loglik: -3.3707e+02 - logprior: -7.4830e+01
Epoch 2/2
10/10 - 3s - loss: 342.4927 - loglik: -3.1566e+02 - logprior: -2.6834e+01
Fitted a model with MAP estimate = -329.9574
expansions: [(0, 2)]
discards: [ 0 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 367.3973 - loglik: -3.0936e+02 - logprior: -5.8041e+01
Epoch 2/2
10/10 - 3s - loss: 315.6630 - loglik: -3.0578e+02 - logprior: -9.8837e+00
Fitted a model with MAP estimate = -306.9037
expansions: [(62, 1), (158, 1)]
discards: [  0  48 165]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 380.7968 - loglik: -3.0887e+02 - logprior: -7.1923e+01
Epoch 2/10
10/10 - 3s - loss: 325.7979 - loglik: -3.0588e+02 - logprior: -1.9922e+01
Epoch 3/10
10/10 - 3s - loss: 307.0989 - loglik: -3.0479e+02 - logprior: -2.3087e+00
Epoch 4/10
10/10 - 3s - loss: 298.4587 - loglik: -3.0318e+02 - logprior: 4.7257
Epoch 5/10
10/10 - 3s - loss: 295.4688 - loglik: -3.0292e+02 - logprior: 7.4556
Epoch 6/10
10/10 - 3s - loss: 293.0860 - loglik: -3.0204e+02 - logprior: 8.9501
Epoch 7/10
10/10 - 3s - loss: 291.1031 - loglik: -3.0108e+02 - logprior: 9.9810
Epoch 8/10
10/10 - 3s - loss: 289.9547 - loglik: -3.0065e+02 - logprior: 10.6941
Epoch 9/10
10/10 - 3s - loss: 288.6398 - loglik: -2.9986e+02 - logprior: 11.2232
Epoch 10/10
10/10 - 3s - loss: 287.2536 - loglik: -2.9894e+02 - logprior: 11.6893
Fitted a model with MAP estimate = -287.0110
Time for alignment: 74.5204
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 540.2574 - loglik: -4.7360e+02 - logprior: -6.6658e+01
Epoch 2/10
10/10 - 2s - loss: 445.9428 - loglik: -4.3285e+02 - logprior: -1.3094e+01
Epoch 3/10
10/10 - 2s - loss: 389.7170 - loglik: -3.8567e+02 - logprior: -4.0449e+00
Epoch 4/10
10/10 - 2s - loss: 358.2120 - loglik: -3.5681e+02 - logprior: -1.3978e+00
Epoch 5/10
10/10 - 2s - loss: 343.9246 - loglik: -3.4430e+02 - logprior: 0.3799
Epoch 6/10
10/10 - 2s - loss: 338.2878 - loglik: -3.3960e+02 - logprior: 1.3171
Epoch 7/10
10/10 - 2s - loss: 335.6794 - loglik: -3.3765e+02 - logprior: 1.9685
Epoch 8/10
10/10 - 2s - loss: 333.3488 - loglik: -3.3568e+02 - logprior: 2.3315
Epoch 9/10
10/10 - 2s - loss: 332.2496 - loglik: -3.3487e+02 - logprior: 2.6165
Epoch 10/10
10/10 - 2s - loss: 331.7473 - loglik: -3.3452e+02 - logprior: 2.7732
Fitted a model with MAP estimate = -330.9661
expansions: [(14, 2), (18, 3), (21, 1), (25, 1), (27, 2), (37, 2), (39, 2), (40, 1), (49, 1), (53, 1), (56, 1), (58, 1), (79, 7), (81, 1), (90, 3), (98, 1), (110, 2), (111, 2), (113, 2), (123, 1), (127, 2), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 176 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 410.9601 - loglik: -3.3623e+02 - logprior: -7.4733e+01
Epoch 2/2
10/10 - 3s - loss: 342.1841 - loglik: -3.1537e+02 - logprior: -2.6810e+01
Fitted a model with MAP estimate = -329.2984
expansions: [(0, 2), (100, 1)]
discards: [  0  49 139 164]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 367.1705 - loglik: -3.0919e+02 - logprior: -5.7977e+01
Epoch 2/2
10/10 - 3s - loss: 314.1120 - loglik: -3.0438e+02 - logprior: -9.7334e+00
Fitted a model with MAP estimate = -305.5759
expansions: [(35, 1), (133, 1)]
discards: [  0  62 143 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 381.2108 - loglik: -3.0944e+02 - logprior: -7.1774e+01
Epoch 2/10
10/10 - 3s - loss: 326.2491 - loglik: -3.0625e+02 - logprior: -1.9999e+01
Epoch 3/10
10/10 - 3s - loss: 307.1134 - loglik: -3.0478e+02 - logprior: -2.3293e+00
Epoch 4/10
10/10 - 3s - loss: 298.7979 - loglik: -3.0354e+02 - logprior: 4.7405
Epoch 5/10
10/10 - 3s - loss: 295.3459 - loglik: -3.0281e+02 - logprior: 7.4614
Epoch 6/10
10/10 - 3s - loss: 293.0776 - loglik: -3.0195e+02 - logprior: 8.8773
Epoch 7/10
10/10 - 3s - loss: 291.7292 - loglik: -3.0158e+02 - logprior: 9.8552
Epoch 8/10
10/10 - 3s - loss: 289.6474 - loglik: -3.0023e+02 - logprior: 10.5782
Epoch 9/10
10/10 - 3s - loss: 288.6522 - loglik: -2.9977e+02 - logprior: 11.1195
Epoch 10/10
10/10 - 3s - loss: 287.7317 - loglik: -2.9932e+02 - logprior: 11.5914
Fitted a model with MAP estimate = -287.3056
Time for alignment: 74.5688
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 540.1926 - loglik: -4.7353e+02 - logprior: -6.6659e+01
Epoch 2/10
10/10 - 2s - loss: 445.8424 - loglik: -4.3274e+02 - logprior: -1.3099e+01
Epoch 3/10
10/10 - 2s - loss: 389.7273 - loglik: -3.8563e+02 - logprior: -4.0936e+00
Epoch 4/10
10/10 - 2s - loss: 357.0371 - loglik: -3.5577e+02 - logprior: -1.2658e+00
Epoch 5/10
10/10 - 2s - loss: 343.0588 - loglik: -3.4361e+02 - logprior: 0.5526
Epoch 6/10
10/10 - 2s - loss: 336.9680 - loglik: -3.3835e+02 - logprior: 1.3814
Epoch 7/10
10/10 - 2s - loss: 333.9422 - loglik: -3.3592e+02 - logprior: 1.9801
Epoch 8/10
10/10 - 2s - loss: 331.7704 - loglik: -3.3416e+02 - logprior: 2.3910
Epoch 9/10
10/10 - 2s - loss: 329.7986 - loglik: -3.3240e+02 - logprior: 2.5973
Epoch 10/10
10/10 - 2s - loss: 329.1122 - loglik: -3.3184e+02 - logprior: 2.7268
Fitted a model with MAP estimate = -328.3171
expansions: [(14, 2), (18, 3), (25, 1), (29, 1), (37, 2), (38, 2), (40, 1), (49, 1), (53, 1), (56, 1), (62, 1), (79, 6), (80, 1), (82, 1), (90, 3), (101, 2), (110, 2), (111, 2), (113, 3), (120, 1), (123, 1), (127, 1), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 176 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 410.9930 - loglik: -3.3602e+02 - logprior: -7.4975e+01
Epoch 2/2
10/10 - 3s - loss: 342.1330 - loglik: -3.1504e+02 - logprior: -2.7091e+01
Fitted a model with MAP estimate = -330.2223
expansions: [(0, 2), (25, 1), (99, 2)]
discards: [  0  46 138 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 367.6439 - loglik: -3.0963e+02 - logprior: -5.8013e+01
Epoch 2/2
10/10 - 3s - loss: 314.4527 - loglik: -3.0462e+02 - logprior: -9.8349e+00
Fitted a model with MAP estimate = -305.7273
expansions: [(34, 2), (160, 1)]
discards: [  0  61 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 379.9042 - loglik: -3.0775e+02 - logprior: -7.2150e+01
Epoch 2/10
10/10 - 3s - loss: 325.1673 - loglik: -3.0430e+02 - logprior: -2.0870e+01
Epoch 3/10
10/10 - 3s - loss: 305.9360 - loglik: -3.0287e+02 - logprior: -3.0632e+00
Epoch 4/10
10/10 - 3s - loss: 297.3124 - loglik: -3.0180e+02 - logprior: 4.4889
Epoch 5/10
10/10 - 3s - loss: 293.5928 - loglik: -3.0091e+02 - logprior: 7.3165
Epoch 6/10
10/10 - 3s - loss: 291.4998 - loglik: -3.0032e+02 - logprior: 8.8152
Epoch 7/10
10/10 - 3s - loss: 289.3477 - loglik: -2.9917e+02 - logprior: 9.8192
Epoch 8/10
10/10 - 3s - loss: 288.5352 - loglik: -2.9904e+02 - logprior: 10.5071
Epoch 9/10
10/10 - 3s - loss: 286.6559 - loglik: -2.9770e+02 - logprior: 11.0408
Epoch 10/10
10/10 - 3s - loss: 285.8366 - loglik: -2.9731e+02 - logprior: 11.4771
Fitted a model with MAP estimate = -285.3503
Time for alignment: 76.1485
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 540.2223 - loglik: -4.7356e+02 - logprior: -6.6661e+01
Epoch 2/10
10/10 - 2s - loss: 445.8472 - loglik: -4.3275e+02 - logprior: -1.3097e+01
Epoch 3/10
10/10 - 2s - loss: 387.9444 - loglik: -3.8389e+02 - logprior: -4.0500e+00
Epoch 4/10
10/10 - 2s - loss: 355.0126 - loglik: -3.5382e+02 - logprior: -1.1964e+00
Epoch 5/10
10/10 - 2s - loss: 342.4808 - loglik: -3.4317e+02 - logprior: 0.6862
Epoch 6/10
10/10 - 2s - loss: 336.3155 - loglik: -3.3785e+02 - logprior: 1.5356
Epoch 7/10
10/10 - 2s - loss: 332.9733 - loglik: -3.3499e+02 - logprior: 2.0155
Epoch 8/10
10/10 - 2s - loss: 330.9248 - loglik: -3.3328e+02 - logprior: 2.3571
Epoch 9/10
10/10 - 2s - loss: 329.3070 - loglik: -3.3189e+02 - logprior: 2.5829
Epoch 10/10
10/10 - 2s - loss: 328.2210 - loglik: -3.3094e+02 - logprior: 2.7154
Fitted a model with MAP estimate = -327.6253
expansions: [(14, 2), (18, 3), (21, 1), (25, 1), (29, 2), (37, 2), (38, 2), (40, 1), (49, 1), (56, 1), (57, 1), (62, 1), (80, 7), (101, 2), (110, 2), (111, 2), (120, 1), (123, 1), (127, 1), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 412.7695 - loglik: -3.3779e+02 - logprior: -7.4976e+01
Epoch 2/2
10/10 - 3s - loss: 342.7587 - loglik: -3.1588e+02 - logprior: -2.6878e+01
Fitted a model with MAP estimate = -330.8461
expansions: [(0, 2)]
discards: [ 0 35 48]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 368.5289 - loglik: -3.1073e+02 - logprior: -5.7795e+01
Epoch 2/2
10/10 - 3s - loss: 316.3704 - loglik: -3.0676e+02 - logprior: -9.6150e+00
Fitted a model with MAP estimate = -308.0495
expansions: [(114, 3), (153, 1)]
discards: [  0  61 160]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 383.4297 - loglik: -3.1131e+02 - logprior: -7.2118e+01
Epoch 2/10
10/10 - 3s - loss: 328.5523 - loglik: -3.0724e+02 - logprior: -2.1313e+01
Epoch 3/10
10/10 - 3s - loss: 309.3408 - loglik: -3.0580e+02 - logprior: -3.5419e+00
Epoch 4/10
10/10 - 3s - loss: 299.9893 - loglik: -3.0452e+02 - logprior: 4.5316
Epoch 5/10
10/10 - 3s - loss: 295.8883 - loglik: -3.0324e+02 - logprior: 7.3560
Epoch 6/10
10/10 - 3s - loss: 293.2469 - loglik: -3.0203e+02 - logprior: 8.7838
Epoch 7/10
10/10 - 3s - loss: 291.4155 - loglik: -3.0114e+02 - logprior: 9.7277
Epoch 8/10
10/10 - 3s - loss: 290.2725 - loglik: -3.0074e+02 - logprior: 10.4660
Epoch 9/10
10/10 - 3s - loss: 288.7913 - loglik: -2.9984e+02 - logprior: 11.0523
Epoch 10/10
10/10 - 3s - loss: 288.4297 - loglik: -2.9996e+02 - logprior: 11.5343
Fitted a model with MAP estimate = -287.8767
Time for alignment: 71.6798
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 540.2369 - loglik: -4.7358e+02 - logprior: -6.6659e+01
Epoch 2/10
10/10 - 2s - loss: 445.6269 - loglik: -4.3253e+02 - logprior: -1.3097e+01
Epoch 3/10
10/10 - 2s - loss: 390.2167 - loglik: -3.8620e+02 - logprior: -4.0186e+00
Epoch 4/10
10/10 - 2s - loss: 359.4957 - loglik: -3.5830e+02 - logprior: -1.1948e+00
Epoch 5/10
10/10 - 2s - loss: 344.3587 - loglik: -3.4489e+02 - logprior: 0.5284
Epoch 6/10
10/10 - 2s - loss: 338.1652 - loglik: -3.3969e+02 - logprior: 1.5243
Epoch 7/10
10/10 - 2s - loss: 334.7626 - loglik: -3.3690e+02 - logprior: 2.1344
Epoch 8/10
10/10 - 2s - loss: 332.5273 - loglik: -3.3493e+02 - logprior: 2.4033
Epoch 9/10
10/10 - 2s - loss: 330.4642 - loglik: -3.3308e+02 - logprior: 2.6179
Epoch 10/10
10/10 - 2s - loss: 330.2174 - loglik: -3.3300e+02 - logprior: 2.7851
Fitted a model with MAP estimate = -329.3963
expansions: [(14, 2), (18, 3), (21, 1), (25, 1), (29, 2), (37, 2), (39, 2), (40, 1), (49, 1), (56, 1), (79, 2), (80, 6), (98, 1), (110, 3), (113, 3), (123, 1), (127, 2), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 412.0043 - loglik: -3.3688e+02 - logprior: -7.5123e+01
Epoch 2/2
10/10 - 3s - loss: 345.7297 - loglik: -3.1873e+02 - logprior: -2.6997e+01
Fitted a model with MAP estimate = -333.7682
expansions: [(0, 2), (33, 2), (74, 1)]
discards: [  0  50 141 142 159]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 372.7291 - loglik: -3.1454e+02 - logprior: -5.8191e+01
Epoch 2/2
10/10 - 3s - loss: 318.3740 - loglik: -3.0840e+02 - logprior: -9.9720e+00
Fitted a model with MAP estimate = -310.1589
expansions: [(130, 1)]
discards: [ 0 38 48]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 384.3937 - loglik: -3.1276e+02 - logprior: -7.1638e+01
Epoch 2/10
10/10 - 3s - loss: 328.5600 - loglik: -3.1000e+02 - logprior: -1.8558e+01
Epoch 3/10
10/10 - 3s - loss: 309.0484 - loglik: -3.0749e+02 - logprior: -1.5541e+00
Epoch 4/10
10/10 - 3s - loss: 300.9365 - loglik: -3.0562e+02 - logprior: 4.6831
Epoch 5/10
10/10 - 3s - loss: 297.3368 - loglik: -3.0462e+02 - logprior: 7.2817
Epoch 6/10
10/10 - 3s - loss: 295.1470 - loglik: -3.0384e+02 - logprior: 8.6906
Epoch 7/10
10/10 - 3s - loss: 293.2612 - loglik: -3.0289e+02 - logprior: 9.6286
Epoch 8/10
10/10 - 3s - loss: 291.8162 - loglik: -3.0219e+02 - logprior: 10.3715
Epoch 9/10
10/10 - 3s - loss: 290.6574 - loglik: -3.0159e+02 - logprior: 10.9339
Epoch 10/10
10/10 - 3s - loss: 289.7097 - loglik: -3.0111e+02 - logprior: 11.4042
Fitted a model with MAP estimate = -289.4700
Time for alignment: 72.0165
Computed alignments with likelihoods: ['-287.0110', '-287.3056', '-285.3503', '-287.8767', '-289.4700']
Best model has likelihood: -285.3503  (prior= 11.7135 )
time for generating output: 0.2050
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.7737135771853689
Training of 5 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e87025940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f070176d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91bb0910>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f8001ea60>
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 295.2075 - loglik: -2.8529e+02 - logprior: -9.9163e+00
Epoch 2/10
12/12 - 1s - loss: 257.9932 - loglik: -2.5561e+02 - logprior: -2.3847e+00
Epoch 3/10
12/12 - 1s - loss: 229.1696 - loglik: -2.2752e+02 - logprior: -1.6543e+00
Epoch 4/10
12/12 - 1s - loss: 219.0212 - loglik: -2.1734e+02 - logprior: -1.6853e+00
Epoch 5/10
12/12 - 1s - loss: 213.8032 - loglik: -2.1210e+02 - logprior: -1.7076e+00
Epoch 6/10
12/12 - 1s - loss: 211.3942 - loglik: -2.0974e+02 - logprior: -1.6521e+00
Epoch 7/10
12/12 - 1s - loss: 210.5488 - loglik: -2.0892e+02 - logprior: -1.6241e+00
Epoch 8/10
12/12 - 1s - loss: 209.0284 - loglik: -2.0737e+02 - logprior: -1.6589e+00
Epoch 9/10
12/12 - 1s - loss: 208.8091 - loglik: -2.0715e+02 - logprior: -1.6624e+00
Epoch 10/10
12/12 - 1s - loss: 207.3032 - loglik: -2.0566e+02 - logprior: -1.6417e+00
Fitted a model with MAP estimate = -207.7056
expansions: [(8, 1), (10, 5), (12, 1), (35, 1), (46, 1), (49, 1), (50, 3), (58, 2), (59, 6), (60, 1), (63, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 221.4491 - loglik: -2.0999e+02 - logprior: -1.1462e+01
Epoch 2/2
12/12 - 1s - loss: 201.7080 - loglik: -1.9680e+02 - logprior: -4.9057e+00
Fitted a model with MAP estimate = -197.3494
expansions: [(0, 2)]
discards: [ 0 73 75]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 202.9953 - loglik: -1.9408e+02 - logprior: -8.9114e+00
Epoch 2/2
12/12 - 1s - loss: 192.2137 - loglik: -1.8997e+02 - logprior: -2.2468e+00
Fitted a model with MAP estimate = -190.9977
expansions: [(13, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 205.4295 - loglik: -1.9437e+02 - logprior: -1.1062e+01
Epoch 2/10
12/12 - 1s - loss: 194.8384 - loglik: -1.9136e+02 - logprior: -3.4788e+00
Epoch 3/10
12/12 - 1s - loss: 191.5453 - loglik: -1.9003e+02 - logprior: -1.5177e+00
Epoch 4/10
12/12 - 1s - loss: 187.8551 - loglik: -1.8687e+02 - logprior: -9.8159e-01
Epoch 5/10
12/12 - 1s - loss: 186.9878 - loglik: -1.8606e+02 - logprior: -9.2615e-01
Epoch 6/10
12/12 - 1s - loss: 185.1136 - loglik: -1.8428e+02 - logprior: -8.3323e-01
Epoch 7/10
12/12 - 1s - loss: 184.7046 - loglik: -1.8386e+02 - logprior: -8.3972e-01
Epoch 8/10
12/12 - 1s - loss: 183.8949 - loglik: -1.8307e+02 - logprior: -8.2217e-01
Epoch 9/10
12/12 - 1s - loss: 184.2925 - loglik: -1.8351e+02 - logprior: -7.8600e-01
Fitted a model with MAP estimate = -183.6661
Time for alignment: 51.9322
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 295.4077 - loglik: -2.8549e+02 - logprior: -9.9130e+00
Epoch 2/10
12/12 - 1s - loss: 258.8496 - loglik: -2.5647e+02 - logprior: -2.3793e+00
Epoch 3/10
12/12 - 1s - loss: 230.1364 - loglik: -2.2847e+02 - logprior: -1.6681e+00
Epoch 4/10
12/12 - 1s - loss: 218.9944 - loglik: -2.1725e+02 - logprior: -1.7494e+00
Epoch 5/10
12/12 - 1s - loss: 214.1800 - loglik: -2.1238e+02 - logprior: -1.8007e+00
Epoch 6/10
12/12 - 1s - loss: 211.4002 - loglik: -2.0966e+02 - logprior: -1.7354e+00
Epoch 7/10
12/12 - 1s - loss: 210.8919 - loglik: -2.0920e+02 - logprior: -1.6966e+00
Epoch 8/10
12/12 - 1s - loss: 209.3692 - loglik: -2.0766e+02 - logprior: -1.7139e+00
Epoch 9/10
12/12 - 1s - loss: 209.0805 - loglik: -2.0736e+02 - logprior: -1.7230e+00
Epoch 10/10
12/12 - 1s - loss: 209.1241 - loglik: -2.0741e+02 - logprior: -1.7103e+00
Fitted a model with MAP estimate = -208.5716
expansions: [(8, 1), (10, 5), (12, 1), (22, 1), (35, 1), (37, 3), (49, 2), (50, 2), (51, 1), (58, 2), (59, 7), (60, 1), (63, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 222.6256 - loglik: -2.1120e+02 - logprior: -1.1425e+01
Epoch 2/2
12/12 - 1s - loss: 201.8334 - loglik: -1.9697e+02 - logprior: -4.8683e+00
Fitted a model with MAP estimate = -197.6215
expansions: [(0, 3)]
discards: [ 0 12 45 63 78 80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 204.9524 - loglik: -1.9608e+02 - logprior: -8.8768e+00
Epoch 2/2
12/12 - 1s - loss: 193.9600 - loglik: -1.9176e+02 - logprior: -2.1957e+00
Fitted a model with MAP estimate = -192.2993
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 205.7758 - loglik: -1.9466e+02 - logprior: -1.1119e+01
Epoch 2/10
12/12 - 1s - loss: 197.8552 - loglik: -1.9412e+02 - logprior: -3.7308e+00
Epoch 3/10
12/12 - 1s - loss: 193.2018 - loglik: -1.9158e+02 - logprior: -1.6208e+00
Epoch 4/10
12/12 - 1s - loss: 189.7825 - loglik: -1.8883e+02 - logprior: -9.5082e-01
Epoch 5/10
12/12 - 1s - loss: 188.7439 - loglik: -1.8790e+02 - logprior: -8.4868e-01
Epoch 6/10
12/12 - 1s - loss: 186.0650 - loglik: -1.8529e+02 - logprior: -7.7070e-01
Epoch 7/10
12/12 - 1s - loss: 186.8255 - loglik: -1.8608e+02 - logprior: -7.5013e-01
Fitted a model with MAP estimate = -185.9444
Time for alignment: 47.0622
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 295.6449 - loglik: -2.8573e+02 - logprior: -9.9137e+00
Epoch 2/10
12/12 - 1s - loss: 257.1066 - loglik: -2.5472e+02 - logprior: -2.3832e+00
Epoch 3/10
12/12 - 1s - loss: 227.9601 - loglik: -2.2631e+02 - logprior: -1.6542e+00
Epoch 4/10
12/12 - 1s - loss: 218.4613 - loglik: -2.1676e+02 - logprior: -1.6997e+00
Epoch 5/10
12/12 - 1s - loss: 213.7518 - loglik: -2.1200e+02 - logprior: -1.7473e+00
Epoch 6/10
12/12 - 1s - loss: 211.6288 - loglik: -2.0992e+02 - logprior: -1.7083e+00
Epoch 7/10
12/12 - 1s - loss: 210.2565 - loglik: -2.0859e+02 - logprior: -1.6666e+00
Epoch 8/10
12/12 - 1s - loss: 209.7904 - loglik: -2.0810e+02 - logprior: -1.6869e+00
Epoch 9/10
12/12 - 1s - loss: 209.2413 - loglik: -2.0755e+02 - logprior: -1.6884e+00
Epoch 10/10
12/12 - 1s - loss: 208.6923 - loglik: -2.0701e+02 - logprior: -1.6795e+00
Fitted a model with MAP estimate = -208.6382
expansions: [(8, 1), (10, 5), (12, 1), (22, 1), (35, 1), (37, 3), (41, 1), (49, 1), (50, 3), (58, 2), (59, 6), (60, 1), (63, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 223.2018 - loglik: -2.1178e+02 - logprior: -1.1427e+01
Epoch 2/2
12/12 - 1s - loss: 202.9562 - loglik: -1.9816e+02 - logprior: -4.7977e+00
Fitted a model with MAP estimate = -198.5113
expansions: [(0, 3)]
discards: [ 0 12 45 78 80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 206.2055 - loglik: -1.9729e+02 - logprior: -8.9113e+00
Epoch 2/2
12/12 - 1s - loss: 195.3569 - loglik: -1.9310e+02 - logprior: -2.2519e+00
Fitted a model with MAP estimate = -193.6860
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 207.9555 - loglik: -1.9680e+02 - logprior: -1.1152e+01
Epoch 2/10
12/12 - 1s - loss: 197.8651 - loglik: -1.9420e+02 - logprior: -3.6660e+00
Epoch 3/10
12/12 - 1s - loss: 194.6999 - loglik: -1.9313e+02 - logprior: -1.5662e+00
Epoch 4/10
12/12 - 1s - loss: 191.3257 - loglik: -1.9040e+02 - logprior: -9.2801e-01
Epoch 5/10
12/12 - 1s - loss: 188.9060 - loglik: -1.8804e+02 - logprior: -8.6863e-01
Epoch 6/10
12/12 - 1s - loss: 188.1776 - loglik: -1.8736e+02 - logprior: -8.1674e-01
Epoch 7/10
12/12 - 1s - loss: 187.9445 - loglik: -1.8715e+02 - logprior: -7.9462e-01
Epoch 8/10
12/12 - 1s - loss: 186.5366 - loglik: -1.8573e+02 - logprior: -8.0528e-01
Epoch 9/10
12/12 - 1s - loss: 186.4471 - loglik: -1.8569e+02 - logprior: -7.5769e-01
Epoch 10/10
12/12 - 1s - loss: 185.8390 - loglik: -1.8511e+02 - logprior: -7.3201e-01
Fitted a model with MAP estimate = -186.2142
Time for alignment: 52.3059
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 295.6358 - loglik: -2.8572e+02 - logprior: -9.9148e+00
Epoch 2/10
12/12 - 1s - loss: 257.9338 - loglik: -2.5555e+02 - logprior: -2.3855e+00
Epoch 3/10
12/12 - 1s - loss: 229.1840 - loglik: -2.2753e+02 - logprior: -1.6498e+00
Epoch 4/10
12/12 - 1s - loss: 218.0083 - loglik: -2.1634e+02 - logprior: -1.6731e+00
Epoch 5/10
12/12 - 1s - loss: 213.6175 - loglik: -2.1192e+02 - logprior: -1.6956e+00
Epoch 6/10
12/12 - 1s - loss: 212.1752 - loglik: -2.1054e+02 - logprior: -1.6315e+00
Epoch 7/10
12/12 - 1s - loss: 209.8704 - loglik: -2.0827e+02 - logprior: -1.6013e+00
Epoch 8/10
12/12 - 1s - loss: 209.3973 - loglik: -2.0778e+02 - logprior: -1.6169e+00
Epoch 9/10
12/12 - 1s - loss: 208.8141 - loglik: -2.0719e+02 - logprior: -1.6231e+00
Epoch 10/10
12/12 - 1s - loss: 208.3834 - loglik: -2.0677e+02 - logprior: -1.6086e+00
Fitted a model with MAP estimate = -208.3769
expansions: [(8, 1), (10, 5), (12, 1), (22, 1), (35, 1), (37, 3), (49, 2), (50, 3), (58, 2), (59, 6), (60, 1), (63, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 222.6902 - loglik: -2.1127e+02 - logprior: -1.1425e+01
Epoch 2/2
12/12 - 1s - loss: 201.6953 - loglik: -1.9683e+02 - logprior: -4.8678e+00
Fitted a model with MAP estimate = -197.3519
expansions: [(0, 3)]
discards: [ 0 12 45 77 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 204.5267 - loglik: -1.9559e+02 - logprior: -8.9357e+00
Epoch 2/2
12/12 - 1s - loss: 194.8281 - loglik: -1.9257e+02 - logprior: -2.2596e+00
Fitted a model with MAP estimate = -192.4698
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 205.9218 - loglik: -1.9477e+02 - logprior: -1.1156e+01
Epoch 2/10
12/12 - 1s - loss: 197.7259 - loglik: -1.9401e+02 - logprior: -3.7167e+00
Epoch 3/10
12/12 - 1s - loss: 192.9568 - loglik: -1.9133e+02 - logprior: -1.6241e+00
Epoch 4/10
12/12 - 1s - loss: 190.3501 - loglik: -1.8936e+02 - logprior: -9.8654e-01
Epoch 5/10
12/12 - 1s - loss: 187.9314 - loglik: -1.8703e+02 - logprior: -9.0276e-01
Epoch 6/10
12/12 - 1s - loss: 187.0093 - loglik: -1.8619e+02 - logprior: -8.1868e-01
Epoch 7/10
12/12 - 1s - loss: 186.5358 - loglik: -1.8572e+02 - logprior: -8.1329e-01
Epoch 8/10
12/12 - 1s - loss: 185.4491 - loglik: -1.8466e+02 - logprior: -7.8713e-01
Epoch 9/10
12/12 - 1s - loss: 185.6523 - loglik: -1.8488e+02 - logprior: -7.6910e-01
Fitted a model with MAP estimate = -185.2771
Time for alignment: 49.7665
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 295.1398 - loglik: -2.8522e+02 - logprior: -9.9192e+00
Epoch 2/10
12/12 - 1s - loss: 258.4277 - loglik: -2.5604e+02 - logprior: -2.3853e+00
Epoch 3/10
12/12 - 1s - loss: 229.5343 - loglik: -2.2789e+02 - logprior: -1.6431e+00
Epoch 4/10
12/12 - 1s - loss: 218.5240 - loglik: -2.1681e+02 - logprior: -1.7091e+00
Epoch 5/10
12/12 - 1s - loss: 214.0016 - loglik: -2.1223e+02 - logprior: -1.7689e+00
Epoch 6/10
12/12 - 1s - loss: 211.8022 - loglik: -2.1009e+02 - logprior: -1.7125e+00
Epoch 7/10
12/12 - 1s - loss: 210.3523 - loglik: -2.0867e+02 - logprior: -1.6804e+00
Epoch 8/10
12/12 - 1s - loss: 209.5795 - loglik: -2.0787e+02 - logprior: -1.7100e+00
Epoch 9/10
12/12 - 1s - loss: 209.3702 - loglik: -2.0763e+02 - logprior: -1.7360e+00
Epoch 10/10
12/12 - 1s - loss: 208.5297 - loglik: -2.0679e+02 - logprior: -1.7353e+00
Fitted a model with MAP estimate = -208.4424
expansions: [(8, 1), (10, 5), (12, 1), (35, 1), (37, 3), (49, 2), (50, 2), (51, 1), (58, 2), (59, 7), (60, 1), (63, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 222.5386 - loglik: -2.1104e+02 - logprior: -1.1500e+01
Epoch 2/2
12/12 - 1s - loss: 199.7553 - loglik: -1.9476e+02 - logprior: -4.9956e+00
Fitted a model with MAP estimate = -196.0074
expansions: [(0, 2)]
discards: [ 0 46 62 77 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 201.5890 - loglik: -1.9268e+02 - logprior: -8.9097e+00
Epoch 2/2
12/12 - 1s - loss: 191.6816 - loglik: -1.8942e+02 - logprior: -2.2666e+00
Fitted a model with MAP estimate = -189.7581
expansions: [(13, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 203.9458 - loglik: -1.9287e+02 - logprior: -1.1073e+01
Epoch 2/10
12/12 - 1s - loss: 193.7462 - loglik: -1.9022e+02 - logprior: -3.5219e+00
Epoch 3/10
12/12 - 1s - loss: 189.6085 - loglik: -1.8805e+02 - logprior: -1.5595e+00
Epoch 4/10
12/12 - 1s - loss: 187.6014 - loglik: -1.8662e+02 - logprior: -9.7807e-01
Epoch 5/10
12/12 - 1s - loss: 185.5346 - loglik: -1.8461e+02 - logprior: -9.2124e-01
Epoch 6/10
12/12 - 1s - loss: 184.2040 - loglik: -1.8336e+02 - logprior: -8.4574e-01
Epoch 7/10
12/12 - 1s - loss: 183.8882 - loglik: -1.8305e+02 - logprior: -8.3723e-01
Epoch 8/10
12/12 - 1s - loss: 182.8532 - loglik: -1.8203e+02 - logprior: -8.2761e-01
Epoch 9/10
12/12 - 1s - loss: 182.9515 - loglik: -1.8215e+02 - logprior: -8.0148e-01
Fitted a model with MAP estimate = -182.6155
Time for alignment: 51.2966
Computed alignments with likelihoods: ['-183.6661', '-185.9444', '-186.2142', '-185.2771', '-182.6155']
Best model has likelihood: -182.6155  (prior= -0.7833 )
time for generating output: 0.1809
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.8951312495092865
Training of 5 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91fc52b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f80e84430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf179490>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f92743550>
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 331.0046 - loglik: -2.9082e+02 - logprior: -4.0186e+01
Epoch 2/10
10/10 - 1s - loss: 282.7518 - loglik: -2.7276e+02 - logprior: -9.9938e+00
Epoch 3/10
10/10 - 1s - loss: 256.8375 - loglik: -2.5264e+02 - logprior: -4.1981e+00
Epoch 4/10
10/10 - 1s - loss: 241.1984 - loglik: -2.3889e+02 - logprior: -2.3039e+00
Epoch 5/10
10/10 - 1s - loss: 234.3267 - loglik: -2.3290e+02 - logprior: -1.4283e+00
Epoch 6/10
10/10 - 1s - loss: 230.3929 - loglik: -2.2936e+02 - logprior: -1.0291e+00
Epoch 7/10
10/10 - 1s - loss: 228.6389 - loglik: -2.2784e+02 - logprior: -7.9407e-01
Epoch 8/10
10/10 - 1s - loss: 227.4280 - loglik: -2.2684e+02 - logprior: -5.8408e-01
Epoch 9/10
10/10 - 1s - loss: 226.9099 - loglik: -2.2651e+02 - logprior: -3.9782e-01
Epoch 10/10
10/10 - 1s - loss: 226.1886 - loglik: -2.2590e+02 - logprior: -2.9291e-01
Fitted a model with MAP estimate = -225.6890
expansions: [(0, 3), (10, 1), (14, 1), (37, 1), (44, 10), (49, 2), (59, 1), (60, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 278.3841 - loglik: -2.2642e+02 - logprior: -5.1961e+01
Epoch 2/2
10/10 - 1s - loss: 234.0765 - loglik: -2.1886e+02 - logprior: -1.5213e+01
Fitted a model with MAP estimate = -226.3278
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 255.1441 - loglik: -2.1778e+02 - logprior: -3.7366e+01
Epoch 2/2
10/10 - 1s - loss: 225.6726 - loglik: -2.1659e+02 - logprior: -9.0860e+00
Fitted a model with MAP estimate = -221.2483
expansions: [(0, 3), (6, 1)]
discards: [51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 267.4202 - loglik: -2.1756e+02 - logprior: -4.9857e+01
Epoch 2/10
10/10 - 1s - loss: 229.6003 - loglik: -2.1567e+02 - logprior: -1.3927e+01
Epoch 3/10
10/10 - 1s - loss: 220.6842 - loglik: -2.1557e+02 - logprior: -5.1145e+00
Epoch 4/10
10/10 - 1s - loss: 217.0696 - loglik: -2.1540e+02 - logprior: -1.6740e+00
Epoch 5/10
10/10 - 1s - loss: 215.2678 - loglik: -2.1525e+02 - logprior: -1.9879e-02
Epoch 6/10
10/10 - 1s - loss: 214.8799 - loglik: -2.1566e+02 - logprior: 0.7833
Epoch 7/10
10/10 - 1s - loss: 213.9254 - loglik: -2.1514e+02 - logprior: 1.2172
Epoch 8/10
10/10 - 1s - loss: 213.4632 - loglik: -2.1493e+02 - logprior: 1.4666
Epoch 9/10
10/10 - 1s - loss: 213.3445 - loglik: -2.1502e+02 - logprior: 1.6757
Epoch 10/10
10/10 - 1s - loss: 212.9528 - loglik: -2.1482e+02 - logprior: 1.8692
Fitted a model with MAP estimate = -212.9534
Time for alignment: 42.6319
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 331.1678 - loglik: -2.9098e+02 - logprior: -4.0187e+01
Epoch 2/10
10/10 - 1s - loss: 282.7689 - loglik: -2.7277e+02 - logprior: -9.9943e+00
Epoch 3/10
10/10 - 1s - loss: 257.5222 - loglik: -2.5331e+02 - logprior: -4.2086e+00
Epoch 4/10
10/10 - 1s - loss: 242.7295 - loglik: -2.4037e+02 - logprior: -2.3570e+00
Epoch 5/10
10/10 - 1s - loss: 234.6432 - loglik: -2.3310e+02 - logprior: -1.5433e+00
Epoch 6/10
10/10 - 1s - loss: 230.6194 - loglik: -2.2937e+02 - logprior: -1.2503e+00
Epoch 7/10
10/10 - 1s - loss: 228.7454 - loglik: -2.2767e+02 - logprior: -1.0764e+00
Epoch 8/10
10/10 - 1s - loss: 226.6229 - loglik: -2.2574e+02 - logprior: -8.8578e-01
Epoch 9/10
10/10 - 1s - loss: 226.0758 - loglik: -2.2538e+02 - logprior: -6.9623e-01
Epoch 10/10
10/10 - 1s - loss: 225.4348 - loglik: -2.2485e+02 - logprior: -5.8418e-01
Fitted a model with MAP estimate = -225.1229
expansions: [(0, 3), (10, 1), (18, 2), (30, 2), (36, 1), (44, 9), (52, 1), (55, 2), (59, 1), (60, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 279.3630 - loglik: -2.2774e+02 - logprior: -5.1622e+01
Epoch 2/2
10/10 - 1s - loss: 234.6680 - loglik: -2.1923e+02 - logprior: -1.5440e+01
Fitted a model with MAP estimate = -226.3783
expansions: []
discards: [ 0  1  2 22 37 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 255.6244 - loglik: -2.1814e+02 - logprior: -3.7483e+01
Epoch 2/2
10/10 - 1s - loss: 225.4385 - loglik: -2.1636e+02 - logprior: -9.0798e+00
Fitted a model with MAP estimate = -221.0219
expansions: [(0, 3), (6, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 265.5698 - loglik: -2.1591e+02 - logprior: -4.9655e+01
Epoch 2/10
10/10 - 1s - loss: 228.8729 - loglik: -2.1512e+02 - logprior: -1.3754e+01
Epoch 3/10
10/10 - 1s - loss: 219.8443 - loglik: -2.1488e+02 - logprior: -4.9629e+00
Epoch 4/10
10/10 - 1s - loss: 216.5180 - loglik: -2.1499e+02 - logprior: -1.5237e+00
Epoch 5/10
10/10 - 1s - loss: 214.6309 - loglik: -2.1475e+02 - logprior: 0.1208
Epoch 6/10
10/10 - 1s - loss: 213.9292 - loglik: -2.1486e+02 - logprior: 0.9283
Epoch 7/10
10/10 - 1s - loss: 213.0053 - loglik: -2.1436e+02 - logprior: 1.3543
Epoch 8/10
10/10 - 1s - loss: 212.5625 - loglik: -2.1418e+02 - logprior: 1.6179
Epoch 9/10
10/10 - 1s - loss: 212.7431 - loglik: -2.1458e+02 - logprior: 1.8320
Fitted a model with MAP estimate = -212.4765
Time for alignment: 40.0707
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 331.1331 - loglik: -2.9095e+02 - logprior: -4.0185e+01
Epoch 2/10
10/10 - 1s - loss: 282.8626 - loglik: -2.7287e+02 - logprior: -9.9965e+00
Epoch 3/10
10/10 - 1s - loss: 256.7281 - loglik: -2.5254e+02 - logprior: -4.1851e+00
Epoch 4/10
10/10 - 1s - loss: 242.6294 - loglik: -2.4032e+02 - logprior: -2.3070e+00
Epoch 5/10
10/10 - 1s - loss: 234.6248 - loglik: -2.3317e+02 - logprior: -1.4513e+00
Epoch 6/10
10/10 - 1s - loss: 230.7819 - loglik: -2.2962e+02 - logprior: -1.1601e+00
Epoch 7/10
10/10 - 1s - loss: 228.5437 - loglik: -2.2755e+02 - logprior: -9.9245e-01
Epoch 8/10
10/10 - 1s - loss: 227.0814 - loglik: -2.2629e+02 - logprior: -7.9640e-01
Epoch 9/10
10/10 - 1s - loss: 226.6165 - loglik: -2.2602e+02 - logprior: -6.0046e-01
Epoch 10/10
10/10 - 1s - loss: 225.4993 - loglik: -2.2499e+02 - logprior: -5.0450e-01
Fitted a model with MAP estimate = -225.3190
expansions: [(0, 3), (10, 1), (18, 2), (40, 2), (43, 1), (44, 8), (52, 1), (55, 2), (59, 1), (60, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 278.7808 - loglik: -2.2717e+02 - logprior: -5.1613e+01
Epoch 2/2
10/10 - 1s - loss: 234.9557 - loglik: -2.1969e+02 - logprior: -1.5266e+01
Fitted a model with MAP estimate = -226.8205
expansions: []
discards: [ 0  1  2 22 46 53 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 256.2118 - loglik: -2.1890e+02 - logprior: -3.7316e+01
Epoch 2/2
10/10 - 1s - loss: 226.5051 - loglik: -2.1741e+02 - logprior: -9.0994e+00
Fitted a model with MAP estimate = -222.0322
expansions: [(0, 3), (6, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 266.3827 - loglik: -2.1671e+02 - logprior: -4.9668e+01
Epoch 2/10
10/10 - 1s - loss: 230.2045 - loglik: -2.1645e+02 - logprior: -1.3751e+01
Epoch 3/10
10/10 - 1s - loss: 221.1003 - loglik: -2.1612e+02 - logprior: -4.9822e+00
Epoch 4/10
10/10 - 1s - loss: 217.7156 - loglik: -2.1615e+02 - logprior: -1.5689e+00
Epoch 5/10
10/10 - 1s - loss: 215.7940 - loglik: -2.1586e+02 - logprior: 0.0623
Epoch 6/10
10/10 - 1s - loss: 214.9734 - loglik: -2.1586e+02 - logprior: 0.8838
Epoch 7/10
10/10 - 1s - loss: 214.3350 - loglik: -2.1565e+02 - logprior: 1.3174
Epoch 8/10
10/10 - 1s - loss: 214.0985 - loglik: -2.1568e+02 - logprior: 1.5828
Epoch 9/10
10/10 - 1s - loss: 213.7693 - loglik: -2.1555e+02 - logprior: 1.7769
Epoch 10/10
10/10 - 1s - loss: 213.4122 - loglik: -2.1539e+02 - logprior: 1.9743
Fitted a model with MAP estimate = -213.2349
Time for alignment: 42.8588
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 331.0436 - loglik: -2.9086e+02 - logprior: -4.0185e+01
Epoch 2/10
10/10 - 1s - loss: 283.3404 - loglik: -2.7335e+02 - logprior: -9.9913e+00
Epoch 3/10
10/10 - 1s - loss: 256.4527 - loglik: -2.5223e+02 - logprior: -4.2189e+00
Epoch 4/10
10/10 - 1s - loss: 241.7805 - loglik: -2.3942e+02 - logprior: -2.3633e+00
Epoch 5/10
10/10 - 1s - loss: 233.6834 - loglik: -2.3215e+02 - logprior: -1.5301e+00
Epoch 6/10
10/10 - 1s - loss: 229.4913 - loglik: -2.2826e+02 - logprior: -1.2310e+00
Epoch 7/10
10/10 - 1s - loss: 227.3869 - loglik: -2.2638e+02 - logprior: -1.0093e+00
Epoch 8/10
10/10 - 1s - loss: 226.5620 - loglik: -2.2578e+02 - logprior: -7.8615e-01
Epoch 9/10
10/10 - 1s - loss: 225.3753 - loglik: -2.2475e+02 - logprior: -6.2087e-01
Epoch 10/10
10/10 - 1s - loss: 225.3105 - loglik: -2.2479e+02 - logprior: -5.1866e-01
Fitted a model with MAP estimate = -224.9411
expansions: [(0, 3), (10, 1), (14, 1), (40, 2), (44, 10), (52, 1), (55, 2), (59, 1), (60, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 278.5537 - loglik: -2.2664e+02 - logprior: -5.1914e+01
Epoch 2/2
10/10 - 1s - loss: 234.7650 - loglik: -2.1948e+02 - logprior: -1.5284e+01
Fitted a model with MAP estimate = -226.8253
expansions: []
discards: [ 0  1  2 45 51 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 256.3311 - loglik: -2.1903e+02 - logprior: -3.7305e+01
Epoch 2/2
10/10 - 1s - loss: 226.1180 - loglik: -2.1698e+02 - logprior: -9.1387e+00
Fitted a model with MAP estimate = -221.8956
expansions: [(0, 3), (7, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 266.7016 - loglik: -2.1676e+02 - logprior: -4.9943e+01
Epoch 2/10
10/10 - 1s - loss: 229.3700 - loglik: -2.1543e+02 - logprior: -1.3944e+01
Epoch 3/10
10/10 - 1s - loss: 220.4082 - loglik: -2.1526e+02 - logprior: -5.1475e+00
Epoch 4/10
10/10 - 1s - loss: 216.8169 - loglik: -2.1508e+02 - logprior: -1.7389e+00
Epoch 5/10
10/10 - 1s - loss: 214.9834 - loglik: -2.1486e+02 - logprior: -1.2221e-01
Epoch 6/10
10/10 - 1s - loss: 213.7970 - loglik: -2.1450e+02 - logprior: 0.7017
Epoch 7/10
10/10 - 1s - loss: 213.4701 - loglik: -2.1461e+02 - logprior: 1.1354
Epoch 8/10
10/10 - 1s - loss: 212.9372 - loglik: -2.1434e+02 - logprior: 1.4012
Epoch 9/10
10/10 - 1s - loss: 212.6077 - loglik: -2.1422e+02 - logprior: 1.6150
Epoch 10/10
10/10 - 1s - loss: 212.6983 - loglik: -2.1451e+02 - logprior: 1.8163
Fitted a model with MAP estimate = -212.3294
Time for alignment: 40.3172
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 331.0755 - loglik: -2.9089e+02 - logprior: -4.0187e+01
Epoch 2/10
10/10 - 1s - loss: 282.5923 - loglik: -2.7260e+02 - logprior: -9.9930e+00
Epoch 3/10
10/10 - 1s - loss: 256.6954 - loglik: -2.5248e+02 - logprior: -4.2165e+00
Epoch 4/10
10/10 - 1s - loss: 241.6565 - loglik: -2.3929e+02 - logprior: -2.3616e+00
Epoch 5/10
10/10 - 1s - loss: 233.6318 - loglik: -2.3212e+02 - logprior: -1.5109e+00
Epoch 6/10
10/10 - 1s - loss: 230.2763 - loglik: -2.2905e+02 - logprior: -1.2271e+00
Epoch 7/10
10/10 - 1s - loss: 227.8614 - loglik: -2.2684e+02 - logprior: -1.0217e+00
Epoch 8/10
10/10 - 1s - loss: 226.9365 - loglik: -2.2613e+02 - logprior: -8.0643e-01
Epoch 9/10
10/10 - 1s - loss: 225.7272 - loglik: -2.2511e+02 - logprior: -6.1262e-01
Epoch 10/10
10/10 - 1s - loss: 225.6153 - loglik: -2.2507e+02 - logprior: -5.4887e-01
Fitted a model with MAP estimate = -225.0708
expansions: [(0, 3), (10, 1), (18, 2), (40, 1), (43, 1), (44, 8), (52, 1), (55, 2), (59, 1), (60, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 278.7401 - loglik: -2.2707e+02 - logprior: -5.1673e+01
Epoch 2/2
10/10 - 1s - loss: 235.3219 - loglik: -2.2001e+02 - logprior: -1.5310e+01
Fitted a model with MAP estimate = -226.8688
expansions: []
discards: [ 0  1  2 22 46 52 72]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 256.6699 - loglik: -2.1936e+02 - logprior: -3.7310e+01
Epoch 2/2
10/10 - 1s - loss: 226.7440 - loglik: -2.1758e+02 - logprior: -9.1611e+00
Fitted a model with MAP estimate = -222.3576
expansions: [(0, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 267.1557 - loglik: -2.1744e+02 - logprior: -4.9717e+01
Epoch 2/10
10/10 - 1s - loss: 230.4986 - loglik: -2.1676e+02 - logprior: -1.3743e+01
Epoch 3/10
10/10 - 1s - loss: 220.9506 - loglik: -2.1600e+02 - logprior: -4.9464e+00
Epoch 4/10
10/10 - 1s - loss: 217.7853 - loglik: -2.1622e+02 - logprior: -1.5610e+00
Epoch 5/10
10/10 - 1s - loss: 215.9389 - loglik: -2.1594e+02 - logprior: -2.4784e-03
Epoch 6/10
10/10 - 1s - loss: 215.4690 - loglik: -2.1626e+02 - logprior: 0.7880
Epoch 7/10
10/10 - 1s - loss: 214.7186 - loglik: -2.1590e+02 - logprior: 1.1802
Epoch 8/10
10/10 - 1s - loss: 214.4568 - loglik: -2.1592e+02 - logprior: 1.4666
Epoch 9/10
10/10 - 1s - loss: 214.0653 - loglik: -2.1578e+02 - logprior: 1.7179
Epoch 10/10
10/10 - 1s - loss: 213.4856 - loglik: -2.1543e+02 - logprior: 1.9397
Fitted a model with MAP estimate = -213.6105
Time for alignment: 40.8240
Computed alignments with likelihoods: ['-212.9534', '-212.4765', '-213.2349', '-212.3294', '-213.6105']
Best model has likelihood: -212.3294  (prior= 1.9254 )
time for generating output: 0.1701
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7255902334903819
Training of 5 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff0ede700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201b5ea880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2024082340>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fe83ee430>
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 557.8922 - loglik: -4.8024e+02 - logprior: -7.7649e+01
Epoch 2/10
10/10 - 2s - loss: 439.5562 - loglik: -4.2473e+02 - logprior: -1.4828e+01
Epoch 3/10
10/10 - 2s - loss: 360.4572 - loglik: -3.5577e+02 - logprior: -4.6877e+00
Epoch 4/10
10/10 - 2s - loss: 303.7192 - loglik: -3.0068e+02 - logprior: -3.0350e+00
Epoch 5/10
10/10 - 2s - loss: 282.3513 - loglik: -2.8074e+02 - logprior: -1.6066e+00
Epoch 6/10
10/10 - 2s - loss: 274.6119 - loglik: -2.7443e+02 - logprior: -1.8164e-01
Epoch 7/10
10/10 - 2s - loss: 271.8932 - loglik: -2.7263e+02 - logprior: 0.7411
Epoch 8/10
10/10 - 2s - loss: 269.6657 - loglik: -2.7112e+02 - logprior: 1.4574
Epoch 9/10
10/10 - 2s - loss: 269.2476 - loglik: -2.7112e+02 - logprior: 1.8758
Epoch 10/10
10/10 - 2s - loss: 268.3698 - loglik: -2.7065e+02 - logprior: 2.2826
Fitted a model with MAP estimate = -268.2442
expansions: [(17, 5), (27, 1), (40, 5), (41, 2), (46, 2), (53, 3), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (89, 1), (90, 1), (91, 1), (96, 1), (116, 2), (117, 2), (124, 1), (127, 2), (128, 2), (129, 1), (132, 1), (133, 2), (134, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 362.0092 - loglik: -2.7441e+02 - logprior: -8.7602e+01
Epoch 2/2
10/10 - 2s - loss: 275.9935 - loglik: -2.4436e+02 - logprior: -3.1636e+01
Fitted a model with MAP estimate = -260.0424
expansions: [(0, 2), (14, 1), (15, 2)]
discards: [  0  46  47  51  58  68 146 162 171]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 306.3949 - loglik: -2.3778e+02 - logprior: -6.8616e+01
Epoch 2/2
10/10 - 2s - loss: 237.6390 - loglik: -2.2655e+02 - logprior: -1.1088e+01
Fitted a model with MAP estimate = -226.9554
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 314.9305 - loglik: -2.3034e+02 - logprior: -8.4592e+01
Epoch 2/10
10/10 - 2s - loss: 251.5807 - loglik: -2.2675e+02 - logprior: -2.4835e+01
Epoch 3/10
10/10 - 2s - loss: 227.2530 - loglik: -2.2357e+02 - logprior: -3.6833e+00
Epoch 4/10
10/10 - 2s - loss: 216.9010 - loglik: -2.2312e+02 - logprior: 6.2203
Epoch 5/10
10/10 - 2s - loss: 212.2584 - loglik: -2.2213e+02 - logprior: 9.8697
Epoch 6/10
10/10 - 2s - loss: 210.4533 - loglik: -2.2218e+02 - logprior: 11.7298
Epoch 7/10
10/10 - 2s - loss: 209.9669 - loglik: -2.2290e+02 - logprior: 12.9289
Epoch 8/10
10/10 - 2s - loss: 207.9836 - loglik: -2.2186e+02 - logprior: 13.8766
Epoch 9/10
10/10 - 2s - loss: 208.1029 - loglik: -2.2278e+02 - logprior: 14.6723
Fitted a model with MAP estimate = -207.2813
Time for alignment: 69.1456
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 557.4933 - loglik: -4.7984e+02 - logprior: -7.7649e+01
Epoch 2/10
10/10 - 2s - loss: 439.8228 - loglik: -4.2499e+02 - logprior: -1.4829e+01
Epoch 3/10
10/10 - 2s - loss: 359.2823 - loglik: -3.5457e+02 - logprior: -4.7083e+00
Epoch 4/10
10/10 - 2s - loss: 303.4196 - loglik: -3.0030e+02 - logprior: -3.1237e+00
Epoch 5/10
10/10 - 2s - loss: 281.8940 - loglik: -2.7998e+02 - logprior: -1.9133e+00
Epoch 6/10
10/10 - 2s - loss: 274.1596 - loglik: -2.7360e+02 - logprior: -5.6334e-01
Epoch 7/10
10/10 - 2s - loss: 270.9007 - loglik: -2.7114e+02 - logprior: 0.2376
Epoch 8/10
10/10 - 2s - loss: 269.8710 - loglik: -2.7084e+02 - logprior: 0.9700
Epoch 9/10
10/10 - 2s - loss: 268.1859 - loglik: -2.6964e+02 - logprior: 1.4566
Epoch 10/10
10/10 - 2s - loss: 267.9944 - loglik: -2.6989e+02 - logprior: 1.8945
Fitted a model with MAP estimate = -267.5580
expansions: [(17, 5), (27, 1), (40, 5), (41, 2), (51, 2), (53, 1), (58, 1), (76, 1), (77, 1), (78, 2), (79, 1), (80, 1), (89, 1), (90, 1), (91, 1), (96, 1), (116, 2), (117, 2), (124, 1), (127, 2), (128, 2), (129, 1), (132, 1), (133, 2), (134, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 360.5216 - loglik: -2.7287e+02 - logprior: -8.7654e+01
Epoch 2/2
10/10 - 2s - loss: 274.7681 - loglik: -2.4331e+02 - logprior: -3.1459e+01
Fitted a model with MAP estimate = -259.0416
expansions: [(0, 2), (14, 1), (15, 2)]
discards: [  0  46  47  51  63 145 162 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 305.0364 - loglik: -2.3634e+02 - logprior: -6.8697e+01
Epoch 2/2
10/10 - 2s - loss: 238.2469 - loglik: -2.2720e+02 - logprior: -1.1048e+01
Fitted a model with MAP estimate = -226.7668
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 314.9547 - loglik: -2.3038e+02 - logprior: -8.4574e+01
Epoch 2/10
10/10 - 2s - loss: 251.1208 - loglik: -2.2636e+02 - logprior: -2.4761e+01
Epoch 3/10
10/10 - 2s - loss: 227.5356 - loglik: -2.2394e+02 - logprior: -3.5913e+00
Epoch 4/10
10/10 - 2s - loss: 217.4202 - loglik: -2.2366e+02 - logprior: 6.2399
Epoch 5/10
10/10 - 2s - loss: 211.6429 - loglik: -2.2151e+02 - logprior: 9.8653
Epoch 6/10
10/10 - 2s - loss: 211.7085 - loglik: -2.2342e+02 - logprior: 11.7161
Fitted a model with MAP estimate = -209.7319
Time for alignment: 61.3270
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 557.5941 - loglik: -4.7995e+02 - logprior: -7.7649e+01
Epoch 2/10
10/10 - 2s - loss: 440.0762 - loglik: -4.2524e+02 - logprior: -1.4832e+01
Epoch 3/10
10/10 - 2s - loss: 358.5628 - loglik: -3.5385e+02 - logprior: -4.7142e+00
Epoch 4/10
10/10 - 2s - loss: 301.8870 - loglik: -2.9864e+02 - logprior: -3.2424e+00
Epoch 5/10
10/10 - 2s - loss: 280.4427 - loglik: -2.7834e+02 - logprior: -2.1064e+00
Epoch 6/10
10/10 - 2s - loss: 273.3227 - loglik: -2.7268e+02 - logprior: -6.4161e-01
Epoch 7/10
10/10 - 2s - loss: 270.0680 - loglik: -2.7040e+02 - logprior: 0.3279
Epoch 8/10
10/10 - 2s - loss: 268.6536 - loglik: -2.6963e+02 - logprior: 0.9807
Epoch 9/10
10/10 - 2s - loss: 268.0293 - loglik: -2.6945e+02 - logprior: 1.4213
Epoch 10/10
10/10 - 2s - loss: 267.3683 - loglik: -2.6922e+02 - logprior: 1.8472
Fitted a model with MAP estimate = -267.0041
expansions: [(17, 5), (27, 1), (40, 4), (41, 2), (44, 1), (53, 1), (58, 1), (77, 1), (78, 2), (79, 1), (80, 1), (89, 1), (90, 1), (91, 1), (96, 1), (116, 2), (117, 2), (124, 1), (127, 2), (128, 2), (129, 1), (132, 1), (133, 2), (134, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 360.7781 - loglik: -2.7296e+02 - logprior: -8.7818e+01
Epoch 2/2
10/10 - 2s - loss: 274.9099 - loglik: -2.4351e+02 - logprior: -3.1399e+01
Fitted a model with MAP estimate = -259.8786
expansions: [(0, 2), (14, 1), (15, 2), (90, 1)]
discards: [  0  46  50 142 159 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 306.3157 - loglik: -2.3783e+02 - logprior: -6.8483e+01
Epoch 2/2
10/10 - 2s - loss: 237.7879 - loglik: -2.2674e+02 - logprior: -1.1043e+01
Fitted a model with MAP estimate = -227.2694
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 315.8260 - loglik: -2.3125e+02 - logprior: -8.4578e+01
Epoch 2/10
10/10 - 2s - loss: 251.3291 - loglik: -2.2651e+02 - logprior: -2.4821e+01
Epoch 3/10
10/10 - 2s - loss: 227.8124 - loglik: -2.2413e+02 - logprior: -3.6831e+00
Epoch 4/10
10/10 - 2s - loss: 217.0977 - loglik: -2.2332e+02 - logprior: 6.2252
Epoch 5/10
10/10 - 2s - loss: 212.8708 - loglik: -2.2270e+02 - logprior: 9.8320
Epoch 6/10
10/10 - 2s - loss: 210.8162 - loglik: -2.2251e+02 - logprior: 11.6903
Epoch 7/10
10/10 - 2s - loss: 210.2203 - loglik: -2.2312e+02 - logprior: 12.8984
Epoch 8/10
10/10 - 2s - loss: 207.4695 - loglik: -2.2131e+02 - logprior: 13.8398
Epoch 9/10
10/10 - 2s - loss: 208.2010 - loglik: -2.2284e+02 - logprior: 14.6355
Fitted a model with MAP estimate = -207.3748
Time for alignment: 66.3434
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 557.5119 - loglik: -4.7986e+02 - logprior: -7.7647e+01
Epoch 2/10
10/10 - 2s - loss: 439.4208 - loglik: -4.2459e+02 - logprior: -1.4829e+01
Epoch 3/10
10/10 - 2s - loss: 359.2704 - loglik: -3.5457e+02 - logprior: -4.6981e+00
Epoch 4/10
10/10 - 2s - loss: 303.5408 - loglik: -3.0048e+02 - logprior: -3.0587e+00
Epoch 5/10
10/10 - 2s - loss: 282.6437 - loglik: -2.8092e+02 - logprior: -1.7286e+00
Epoch 6/10
10/10 - 2s - loss: 275.0541 - loglik: -2.7478e+02 - logprior: -2.7092e-01
Epoch 7/10
10/10 - 2s - loss: 271.9150 - loglik: -2.7256e+02 - logprior: 0.6422
Epoch 8/10
10/10 - 2s - loss: 270.5481 - loglik: -2.7184e+02 - logprior: 1.2925
Epoch 9/10
10/10 - 2s - loss: 269.5538 - loglik: -2.7131e+02 - logprior: 1.7605
Epoch 10/10
10/10 - 2s - loss: 268.8242 - loglik: -2.7100e+02 - logprior: 2.1781
Fitted a model with MAP estimate = -268.7137
expansions: [(17, 5), (27, 1), (40, 4), (41, 2), (53, 3), (58, 1), (77, 1), (78, 2), (79, 1), (80, 1), (89, 1), (90, 1), (91, 1), (96, 1), (116, 2), (117, 2), (124, 1), (127, 2), (128, 2), (129, 1), (132, 1), (133, 2), (134, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 360.8704 - loglik: -2.7304e+02 - logprior: -8.7826e+01
Epoch 2/2
10/10 - 2s - loss: 275.2474 - loglik: -2.4364e+02 - logprior: -3.1612e+01
Fitted a model with MAP estimate = -259.7496
expansions: [(0, 2), (14, 1), (15, 2), (91, 1)]
discards: [  0  46  50  65 143 160 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 305.4885 - loglik: -2.3677e+02 - logprior: -6.8715e+01
Epoch 2/2
10/10 - 2s - loss: 238.5845 - loglik: -2.2751e+02 - logprior: -1.1076e+01
Fitted a model with MAP estimate = -226.8970
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 314.9949 - loglik: -2.3042e+02 - logprior: -8.4578e+01
Epoch 2/10
10/10 - 2s - loss: 251.6986 - loglik: -2.2692e+02 - logprior: -2.4781e+01
Epoch 3/10
10/10 - 2s - loss: 227.3462 - loglik: -2.2372e+02 - logprior: -3.6241e+00
Epoch 4/10
10/10 - 2s - loss: 216.6690 - loglik: -2.2290e+02 - logprior: 6.2331
Epoch 5/10
10/10 - 2s - loss: 213.0464 - loglik: -2.2290e+02 - logprior: 9.8514
Epoch 6/10
10/10 - 2s - loss: 210.4057 - loglik: -2.2211e+02 - logprior: 11.7049
Epoch 7/10
10/10 - 2s - loss: 209.7179 - loglik: -2.2263e+02 - logprior: 12.9107
Epoch 8/10
10/10 - 2s - loss: 207.0312 - loglik: -2.2089e+02 - logprior: 13.8561
Epoch 9/10
10/10 - 2s - loss: 208.0265 - loglik: -2.2268e+02 - logprior: 14.6487
Fitted a model with MAP estimate = -207.0708
Time for alignment: 65.7475
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 557.6066 - loglik: -4.7996e+02 - logprior: -7.7648e+01
Epoch 2/10
10/10 - 2s - loss: 439.2320 - loglik: -4.2440e+02 - logprior: -1.4829e+01
Epoch 3/10
10/10 - 2s - loss: 359.9105 - loglik: -3.5520e+02 - logprior: -4.7139e+00
Epoch 4/10
10/10 - 2s - loss: 303.5793 - loglik: -3.0044e+02 - logprior: -3.1370e+00
Epoch 5/10
10/10 - 2s - loss: 282.7770 - loglik: -2.8092e+02 - logprior: -1.8559e+00
Epoch 6/10
10/10 - 2s - loss: 274.1208 - loglik: -2.7362e+02 - logprior: -4.9773e-01
Epoch 7/10
10/10 - 2s - loss: 270.7977 - loglik: -2.7105e+02 - logprior: 0.2535
Epoch 8/10
10/10 - 2s - loss: 269.7673 - loglik: -2.7074e+02 - logprior: 0.9706
Epoch 9/10
10/10 - 2s - loss: 268.5845 - loglik: -2.7005e+02 - logprior: 1.4696
Epoch 10/10
10/10 - 2s - loss: 267.5408 - loglik: -2.6942e+02 - logprior: 1.8750
Fitted a model with MAP estimate = -267.5678
expansions: [(16, 3), (17, 4), (27, 1), (40, 5), (41, 2), (51, 2), (53, 1), (58, 1), (77, 1), (78, 2), (79, 1), (80, 1), (89, 1), (90, 1), (91, 1), (96, 1), (116, 2), (117, 2), (124, 1), (127, 2), (128, 2), (129, 1), (132, 1), (133, 2), (134, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 359.3822 - loglik: -2.7188e+02 - logprior: -8.7501e+01
Epoch 2/2
10/10 - 2s - loss: 274.4682 - loglik: -2.4326e+02 - logprior: -3.1205e+01
Fitted a model with MAP estimate = -258.1789
expansions: [(0, 2), (94, 1)]
discards: [  0  48  49  53  65 146 163 171]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 305.9235 - loglik: -2.3722e+02 - logprior: -6.8701e+01
Epoch 2/2
10/10 - 2s - loss: 239.0641 - loglik: -2.2791e+02 - logprior: -1.1155e+01
Fitted a model with MAP estimate = -228.4989
expansions: [(21, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 316.3434 - loglik: -2.3173e+02 - logprior: -8.4614e+01
Epoch 2/10
10/10 - 2s - loss: 252.7797 - loglik: -2.2761e+02 - logprior: -2.5173e+01
Epoch 3/10
10/10 - 2s - loss: 227.4989 - loglik: -2.2339e+02 - logprior: -4.1091e+00
Epoch 4/10
10/10 - 2s - loss: 217.3909 - loglik: -2.2357e+02 - logprior: 6.1812
Epoch 5/10
10/10 - 2s - loss: 212.9950 - loglik: -2.2289e+02 - logprior: 9.8924
Epoch 6/10
10/10 - 2s - loss: 209.8839 - loglik: -2.2162e+02 - logprior: 11.7408
Epoch 7/10
10/10 - 2s - loss: 209.5781 - loglik: -2.2254e+02 - logprior: 12.9662
Epoch 8/10
10/10 - 2s - loss: 208.8883 - loglik: -2.2279e+02 - logprior: 13.9022
Epoch 9/10
10/10 - 2s - loss: 207.5530 - loglik: -2.2227e+02 - logprior: 14.7185
Epoch 10/10
10/10 - 2s - loss: 206.6600 - loglik: -2.2206e+02 - logprior: 15.3964
Fitted a model with MAP estimate = -206.5644
Time for alignment: 67.8666
Computed alignments with likelihoods: ['-207.2813', '-209.7319', '-207.3748', '-207.0708', '-206.5644']
Best model has likelihood: -206.5644  (prior= 15.7582 )
time for generating output: 0.1948
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9176008968609866
Training of 5 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201b3382b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fcec553d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201af03400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2001fb7f70>
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 24s - loss: 831.6108 - loglik: -8.2966e+02 - logprior: -1.9485e+00
Epoch 2/10
33/33 - 21s - loss: 730.3044 - loglik: -7.2970e+02 - logprior: -6.0184e-01
Epoch 3/10
33/33 - 21s - loss: 715.9750 - loglik: -7.1557e+02 - logprior: -4.0826e-01
Epoch 4/10
33/33 - 20s - loss: 713.5051 - loglik: -7.1319e+02 - logprior: -3.1682e-01
Epoch 5/10
33/33 - 21s - loss: 709.5051 - loglik: -7.0919e+02 - logprior: -3.1174e-01
Epoch 6/10
33/33 - 20s - loss: 713.5172 - loglik: -7.1319e+02 - logprior: -3.2494e-01
Fitted a model with MAP estimate = -710.9726
expansions: [(0, 6), (5, 1), (8, 1), (9, 1), (44, 1), (67, 1), (74, 1), (75, 4), (79, 2), (109, 2), (110, 4), (111, 1), (112, 1), (133, 2), (166, 5), (174, 3), (175, 1), (205, 1), (207, 1), (221, 5), (230, 4)]
discards: [ 64 224 225 226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 272 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 710.6419 - loglik: -7.0781e+02 - logprior: -2.8304e+00
Epoch 2/2
33/33 - 26s - loss: 701.3795 - loglik: -7.0096e+02 - logprior: -4.1809e-01
Fitted a model with MAP estimate = -699.3463
expansions: [(88, 1), (263, 1), (265, 1), (272, 5)]
discards: [  1   2   6   7   8   9  17 129 130 131 132 158 194 195 196 197 198 207
 267 268 270 271]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 705.1593 - loglik: -7.0313e+02 - logprior: -2.0247e+00
Epoch 2/2
33/33 - 24s - loss: 703.9932 - loglik: -7.0397e+02 - logprior: -2.2781e-02
Fitted a model with MAP estimate = -700.7956
expansions: [(0, 6), (11, 1), (258, 5)]
discards: [248 251 252 253 254 255 256 257]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 262 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 706.3036 - loglik: -7.0330e+02 - logprior: -2.9986e+00
Epoch 2/10
33/33 - 24s - loss: 701.4250 - loglik: -7.0140e+02 - logprior: -2.6480e-02
Epoch 3/10
33/33 - 25s - loss: 698.5813 - loglik: -6.9871e+02 - logprior: 0.1237
Epoch 4/10
33/33 - 25s - loss: 699.1790 - loglik: -6.9938e+02 - logprior: 0.1986
Fitted a model with MAP estimate = -698.5513
Time for alignment: 434.5693
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 25s - loss: 825.3665 - loglik: -8.2343e+02 - logprior: -1.9343e+00
Epoch 2/10
33/33 - 20s - loss: 730.2410 - loglik: -7.2966e+02 - logprior: -5.7945e-01
Epoch 3/10
33/33 - 21s - loss: 716.3748 - loglik: -7.1585e+02 - logprior: -5.2364e-01
Epoch 4/10
33/33 - 21s - loss: 710.8514 - loglik: -7.1033e+02 - logprior: -5.1838e-01
Epoch 5/10
33/33 - 21s - loss: 710.1977 - loglik: -7.0971e+02 - logprior: -4.8694e-01
Epoch 6/10
33/33 - 20s - loss: 712.2032 - loglik: -7.1171e+02 - logprior: -4.9145e-01
Fitted a model with MAP estimate = -708.7087
expansions: [(0, 6), (3, 1), (4, 1), (66, 1), (68, 1), (74, 1), (75, 2), (80, 2), (95, 1), (109, 2), (110, 4), (111, 1), (112, 1), (138, 1), (154, 2), (166, 4), (174, 1), (175, 1), (205, 1), (211, 1), (222, 4), (223, 2), (230, 4)]
discards: [224 225 226 227 228 229]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 269 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 709.8245 - loglik: -7.0702e+02 - logprior: -2.8053e+00
Epoch 2/2
33/33 - 26s - loss: 700.5004 - loglik: -7.0017e+02 - logprior: -3.3337e-01
Fitted a model with MAP estimate = -699.0491
expansions: [(0, 5), (256, 1), (269, 5)]
discards: [  1   6   7   8   9 128 129 130 131 180 194 195 196 261 265 266 267]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 263 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 707.2928 - loglik: -7.0416e+02 - logprior: -3.1298e+00
Epoch 2/2
33/33 - 25s - loss: 700.8490 - loglik: -7.0067e+02 - logprior: -1.8094e-01
Fitted a model with MAP estimate = -700.3949
expansions: [(263, 6)]
discards: [  1   5   6   7   8 254 255 256 257 258 259 260 261 262]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 705.6086 - loglik: -7.0372e+02 - logprior: -1.8898e+00
Epoch 2/10
33/33 - 24s - loss: 701.4571 - loglik: -7.0160e+02 - logprior: 0.1381
Epoch 3/10
33/33 - 24s - loss: 702.2626 - loglik: -7.0260e+02 - logprior: 0.3368
Fitted a model with MAP estimate = -699.7445
Time for alignment: 407.5865
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 25s - loss: 831.3215 - loglik: -8.2939e+02 - logprior: -1.9357e+00
Epoch 2/10
33/33 - 21s - loss: 728.7919 - loglik: -7.2819e+02 - logprior: -6.0263e-01
Epoch 3/10
33/33 - 20s - loss: 717.0578 - loglik: -7.1659e+02 - logprior: -4.7053e-01
Epoch 4/10
33/33 - 21s - loss: 712.9440 - loglik: -7.1259e+02 - logprior: -3.5424e-01
Epoch 5/10
33/33 - 21s - loss: 713.9091 - loglik: -7.1356e+02 - logprior: -3.4501e-01
Fitted a model with MAP estimate = -711.9798
expansions: [(0, 6), (5, 1), (8, 1), (67, 1), (74, 1), (75, 4), (96, 1), (110, 2), (111, 4), (112, 4), (137, 2), (165, 5), (173, 1), (179, 2), (204, 1), (214, 1), (221, 4), (230, 4)]
discards: [225 226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 271 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 711.5529 - loglik: -7.0874e+02 - logprior: -2.8158e+00
Epoch 2/2
33/33 - 26s - loss: 700.1053 - loglik: -6.9968e+02 - logprior: -4.2490e-01
Fitted a model with MAP estimate = -699.7471
expansions: [(87, 1), (257, 1), (271, 5)]
discards: [  1   2   6   7   8   9  16 128 129 130 131 193 194 195 196 212 265 266
 267 268 269 270]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 256 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 706.6396 - loglik: -7.0464e+02 - logprior: -1.9998e+00
Epoch 2/2
33/33 - 24s - loss: 703.5068 - loglik: -7.0355e+02 - logprior: 0.0456
Fitted a model with MAP estimate = -700.7158
expansions: [(0, 6), (256, 5)]
discards: [248 250 251 252 253 254 255]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 29s - loss: 706.7720 - loglik: -7.0380e+02 - logprior: -2.9719e+00
Epoch 2/10
33/33 - 24s - loss: 703.4920 - loglik: -7.0348e+02 - logprior: -1.0061e-02
Epoch 3/10
33/33 - 24s - loss: 697.2026 - loglik: -6.9737e+02 - logprior: 0.1671
Epoch 4/10
33/33 - 25s - loss: 702.3123 - loglik: -7.0255e+02 - logprior: 0.2384
Fitted a model with MAP estimate = -699.3177
Time for alignment: 411.0673
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 23s - loss: 829.0127 - loglik: -8.2709e+02 - logprior: -1.9220e+00
Epoch 2/10
33/33 - 21s - loss: 729.1804 - loglik: -7.2863e+02 - logprior: -5.4904e-01
Epoch 3/10
33/33 - 21s - loss: 712.3149 - loglik: -7.1182e+02 - logprior: -4.9168e-01
Epoch 4/10
33/33 - 21s - loss: 711.1408 - loglik: -7.1070e+02 - logprior: -4.4190e-01
Epoch 5/10
33/33 - 21s - loss: 713.0743 - loglik: -7.1263e+02 - logprior: -4.4225e-01
Fitted a model with MAP estimate = -709.9945
expansions: [(0, 8), (3, 1), (44, 1), (66, 1), (68, 1), (72, 1), (73, 1), (74, 2), (79, 1), (92, 1), (109, 2), (110, 1), (111, 2), (112, 4), (137, 1), (153, 2), (165, 5), (173, 4), (174, 1), (204, 1), (214, 1), (220, 1), (221, 4), (230, 4)]
discards: [  7 224 225 226 227 228 229]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 274 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 710.6218 - loglik: -7.0781e+02 - logprior: -2.8080e+00
Epoch 2/2
33/33 - 26s - loss: 698.0034 - loglik: -6.9765e+02 - logprior: -3.5102e-01
Fitted a model with MAP estimate = -698.7947
expansions: [(274, 6)]
discards: [  1   3   4   6   7 127 130 131 132 181 194 195 196 197 198 209 210 267
 268 270 271 272 273]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 708.2477 - loglik: -7.0623e+02 - logprior: -2.0222e+00
Epoch 2/2
33/33 - 24s - loss: 698.9752 - loglik: -6.9892e+02 - logprior: -5.9032e-02
Fitted a model with MAP estimate = -699.8710
expansions: [(0, 5), (1, 1), (71, 1), (257, 5)]
discards: [ 69 250 251 252 253 254 255 256]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 261 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 707.1285 - loglik: -7.0410e+02 - logprior: -3.0334e+00
Epoch 2/10
33/33 - 25s - loss: 700.3092 - loglik: -7.0031e+02 - logprior: 0.0039
Epoch 3/10
33/33 - 25s - loss: 700.5193 - loglik: -7.0064e+02 - logprior: 0.1235
Fitted a model with MAP estimate = -699.0860
Time for alignment: 385.8717
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 23s - loss: 828.4799 - loglik: -8.2655e+02 - logprior: -1.9268e+00
Epoch 2/10
33/33 - 21s - loss: 730.5821 - loglik: -7.3006e+02 - logprior: -5.2312e-01
Epoch 3/10
33/33 - 21s - loss: 714.9866 - loglik: -7.1455e+02 - logprior: -4.3830e-01
Epoch 4/10
33/33 - 20s - loss: 711.4851 - loglik: -7.1108e+02 - logprior: -4.0899e-01
Epoch 5/10
33/33 - 21s - loss: 708.5011 - loglik: -7.0809e+02 - logprior: -4.0995e-01
Epoch 6/10
33/33 - 21s - loss: 711.8351 - loglik: -7.1145e+02 - logprior: -3.8291e-01
Fitted a model with MAP estimate = -709.3028
expansions: [(0, 8), (2, 1), (44, 1), (65, 2), (72, 1), (73, 1), (74, 2), (79, 2), (109, 2), (110, 1), (111, 2), (112, 4), (132, 2), (152, 2), (164, 4), (172, 1), (173, 1), (203, 1), (220, 1), (221, 4)]
discards: [225 226 227 228 229]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 268 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 709.0273 - loglik: -7.0623e+02 - logprior: -2.7994e+00
Epoch 2/2
33/33 - 26s - loss: 701.4615 - loglik: -7.0109e+02 - logprior: -3.7171e-01
Fitted a model with MAP estimate = -699.3479
expansions: [(262, 2), (268, 5)]
discards: [  1   3   4   5   6   7  17 132 133 134 182 196 197 198 263 264 265 267]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 705.7250 - loglik: -7.0368e+02 - logprior: -2.0467e+00
Epoch 2/2
33/33 - 24s - loss: 702.8539 - loglik: -7.0280e+02 - logprior: -5.3871e-02
Fitted a model with MAP estimate = -700.6662
expansions: [(0, 5), (257, 5)]
discards: [124 250 251 252 253 254 255]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 707.6160 - loglik: -7.0463e+02 - logprior: -2.9837e+00
Epoch 2/10
33/33 - 24s - loss: 700.7707 - loglik: -7.0078e+02 - logprior: 0.0134
Epoch 3/10
33/33 - 25s - loss: 700.0276 - loglik: -7.0021e+02 - logprior: 0.1796
Epoch 4/10
33/33 - 24s - loss: 697.7598 - loglik: -6.9801e+02 - logprior: 0.2529
Epoch 5/10
33/33 - 25s - loss: 701.5976 - loglik: -7.0193e+02 - logprior: 0.3331
Fitted a model with MAP estimate = -698.7648
Time for alignment: 454.7599
Computed alignments with likelihoods: ['-698.5513', '-699.0491', '-699.3177', '-698.7947', '-698.7648']
Best model has likelihood: -698.5513  (prior= 0.2478 )
time for generating output: 0.3108
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.7619508151423044
Training of 5 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91ec2f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf8fd670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91924520>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2001fb7f70>
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 414.0727 - loglik: -3.5222e+02 - logprior: -6.1857e+01
Epoch 2/10
10/10 - 1s - loss: 326.7769 - loglik: -3.1251e+02 - logprior: -1.4264e+01
Epoch 3/10
10/10 - 1s - loss: 282.2907 - loglik: -2.7598e+02 - logprior: -6.3126e+00
Epoch 4/10
10/10 - 1s - loss: 255.8262 - loglik: -2.5179e+02 - logprior: -4.0325e+00
Epoch 5/10
10/10 - 1s - loss: 244.0521 - loglik: -2.4106e+02 - logprior: -2.9884e+00
Epoch 6/10
10/10 - 1s - loss: 238.8083 - loglik: -2.3666e+02 - logprior: -2.1444e+00
Epoch 7/10
10/10 - 1s - loss: 235.4503 - loglik: -2.3398e+02 - logprior: -1.4673e+00
Epoch 8/10
10/10 - 1s - loss: 233.6773 - loglik: -2.3257e+02 - logprior: -1.1105e+00
Epoch 9/10
10/10 - 1s - loss: 232.9858 - loglik: -2.3209e+02 - logprior: -8.9469e-01
Epoch 10/10
10/10 - 1s - loss: 232.2576 - loglik: -2.3155e+02 - logprior: -7.0764e-01
Fitted a model with MAP estimate = -231.9651
expansions: [(9, 1), (12, 3), (14, 4), (38, 3), (40, 1), (48, 1), (56, 1), (60, 1), (63, 1), (64, 1), (71, 1), (75, 1), (78, 1), (80, 1), (85, 1), (87, 3), (88, 1), (92, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 309.5414 - loglik: -2.4000e+02 - logprior: -6.9540e+01
Epoch 2/2
10/10 - 1s - loss: 243.1947 - loglik: -2.1694e+02 - logprior: -2.6258e+01
Fitted a model with MAP estimate = -230.8958
expansions: [(0, 4)]
discards: [  0  13  14 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 269.3847 - loglik: -2.1516e+02 - logprior: -5.4225e+01
Epoch 2/2
10/10 - 2s - loss: 220.8620 - loglik: -2.0947e+02 - logprior: -1.1394e+01
Fitted a model with MAP estimate = -212.5804
expansions: []
discards: [ 1  2  3 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 265.8542 - loglik: -2.1267e+02 - logprior: -5.3181e+01
Epoch 2/10
10/10 - 1s - loss: 220.8492 - loglik: -2.0999e+02 - logprior: -1.0864e+01
Epoch 3/10
10/10 - 1s - loss: 209.8238 - loglik: -2.0775e+02 - logprior: -2.0736e+00
Epoch 4/10
10/10 - 1s - loss: 205.3416 - loglik: -2.0685e+02 - logprior: 1.5061
Epoch 5/10
10/10 - 1s - loss: 201.9956 - loglik: -2.0538e+02 - logprior: 3.3809
Epoch 6/10
10/10 - 1s - loss: 198.8624 - loglik: -2.0325e+02 - logprior: 4.3875
Epoch 7/10
10/10 - 1s - loss: 198.4704 - loglik: -2.0348e+02 - logprior: 5.0085
Epoch 8/10
10/10 - 1s - loss: 197.0234 - loglik: -2.0263e+02 - logprior: 5.6068
Epoch 9/10
10/10 - 1s - loss: 197.3648 - loglik: -2.0357e+02 - logprior: 6.2032
Fitted a model with MAP estimate = -196.5213
Time for alignment: 48.4688
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 414.1075 - loglik: -3.5225e+02 - logprior: -6.1854e+01
Epoch 2/10
10/10 - 1s - loss: 327.2993 - loglik: -3.1304e+02 - logprior: -1.4262e+01
Epoch 3/10
10/10 - 1s - loss: 281.9897 - loglik: -2.7555e+02 - logprior: -6.4347e+00
Epoch 4/10
10/10 - 1s - loss: 256.4068 - loglik: -2.5198e+02 - logprior: -4.4236e+00
Epoch 5/10
10/10 - 1s - loss: 243.4994 - loglik: -2.4014e+02 - logprior: -3.3609e+00
Epoch 6/10
10/10 - 1s - loss: 239.6282 - loglik: -2.3721e+02 - logprior: -2.4163e+00
Epoch 7/10
10/10 - 1s - loss: 235.3064 - loglik: -2.3354e+02 - logprior: -1.7632e+00
Epoch 8/10
10/10 - 1s - loss: 234.6881 - loglik: -2.3316e+02 - logprior: -1.5260e+00
Epoch 9/10
10/10 - 1s - loss: 232.6112 - loglik: -2.3132e+02 - logprior: -1.2955e+00
Epoch 10/10
10/10 - 1s - loss: 232.9634 - loglik: -2.3188e+02 - logprior: -1.0785e+00
Fitted a model with MAP estimate = -232.2058
expansions: [(9, 1), (12, 3), (14, 3), (23, 2), (37, 1), (38, 1), (40, 1), (46, 1), (54, 1), (55, 1), (63, 1), (64, 1), (71, 1), (78, 1), (79, 1), (80, 1), (85, 1), (87, 3), (88, 1), (92, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 311.1717 - loglik: -2.4152e+02 - logprior: -6.9648e+01
Epoch 2/2
10/10 - 1s - loss: 244.8477 - loglik: -2.1839e+02 - logprior: -2.6453e+01
Fitted a model with MAP estimate = -232.1306
expansions: [(0, 3)]
discards: [  0  13  14  29 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 270.5725 - loglik: -2.1625e+02 - logprior: -5.4325e+01
Epoch 2/2
10/10 - 1s - loss: 221.5485 - loglik: -2.1002e+02 - logprior: -1.1532e+01
Fitted a model with MAP estimate = -213.3566
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 279.3126 - loglik: -2.1405e+02 - logprior: -6.5267e+01
Epoch 2/10
10/10 - 1s - loss: 227.0580 - loglik: -2.1041e+02 - logprior: -1.6645e+01
Epoch 3/10
10/10 - 1s - loss: 211.4511 - loglik: -2.0814e+02 - logprior: -3.3119e+00
Epoch 4/10
10/10 - 1s - loss: 205.8752 - loglik: -2.0704e+02 - logprior: 1.1689
Epoch 5/10
10/10 - 1s - loss: 201.1459 - loglik: -2.0427e+02 - logprior: 3.1275
Epoch 6/10
10/10 - 1s - loss: 199.6213 - loglik: -2.0378e+02 - logprior: 4.1600
Epoch 7/10
10/10 - 1s - loss: 198.0084 - loglik: -2.0297e+02 - logprior: 4.9660
Epoch 8/10
10/10 - 1s - loss: 197.6703 - loglik: -2.0335e+02 - logprior: 5.6793
Epoch 9/10
10/10 - 1s - loss: 196.8815 - loglik: -2.0311e+02 - logprior: 6.2275
Epoch 10/10
10/10 - 1s - loss: 197.2660 - loglik: -2.0390e+02 - logprior: 6.6326
Fitted a model with MAP estimate = -196.2028
Time for alignment: 48.8391
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 413.9408 - loglik: -3.5208e+02 - logprior: -6.1859e+01
Epoch 2/10
10/10 - 1s - loss: 327.6641 - loglik: -3.1340e+02 - logprior: -1.4269e+01
Epoch 3/10
10/10 - 1s - loss: 282.2206 - loglik: -2.7580e+02 - logprior: -6.4179e+00
Epoch 4/10
10/10 - 1s - loss: 255.2295 - loglik: -2.5090e+02 - logprior: -4.3324e+00
Epoch 5/10
10/10 - 1s - loss: 242.5750 - loglik: -2.3935e+02 - logprior: -3.2267e+00
Epoch 6/10
10/10 - 1s - loss: 238.5362 - loglik: -2.3631e+02 - logprior: -2.2265e+00
Epoch 7/10
10/10 - 1s - loss: 235.8871 - loglik: -2.3437e+02 - logprior: -1.5210e+00
Epoch 8/10
10/10 - 1s - loss: 234.1205 - loglik: -2.3289e+02 - logprior: -1.2336e+00
Epoch 9/10
10/10 - 1s - loss: 233.3625 - loglik: -2.3233e+02 - logprior: -1.0342e+00
Epoch 10/10
10/10 - 1s - loss: 232.7824 - loglik: -2.3198e+02 - logprior: -8.0593e-01
Fitted a model with MAP estimate = -232.3281
expansions: [(9, 1), (12, 3), (14, 4), (29, 1), (37, 1), (38, 1), (40, 1), (48, 1), (56, 1), (60, 1), (63, 1), (64, 1), (71, 1), (78, 1), (79, 1), (80, 1), (85, 1), (87, 4), (92, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 309.8111 - loglik: -2.4029e+02 - logprior: -6.9516e+01
Epoch 2/2
10/10 - 1s - loss: 245.1268 - loglik: -2.1888e+02 - logprior: -2.6250e+01
Fitted a model with MAP estimate = -231.8789
expansions: [(0, 3)]
discards: [  0  13  14 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 270.7379 - loglik: -2.1655e+02 - logprior: -5.4192e+01
Epoch 2/2
10/10 - 1s - loss: 220.6968 - loglik: -2.0928e+02 - logprior: -1.1418e+01
Fitted a model with MAP estimate = -213.0645
expansions: []
discards: [ 0  2 17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 279.7838 - loglik: -2.1455e+02 - logprior: -6.5231e+01
Epoch 2/10
10/10 - 1s - loss: 227.9540 - loglik: -2.1131e+02 - logprior: -1.6648e+01
Epoch 3/10
10/10 - 1s - loss: 211.9174 - loglik: -2.0863e+02 - logprior: -3.2903e+00
Epoch 4/10
10/10 - 1s - loss: 205.2704 - loglik: -2.0651e+02 - logprior: 1.2408
Epoch 5/10
10/10 - 1s - loss: 202.4935 - loglik: -2.0571e+02 - logprior: 3.2157
Epoch 6/10
10/10 - 1s - loss: 199.4275 - loglik: -2.0366e+02 - logprior: 4.2325
Epoch 7/10
10/10 - 1s - loss: 198.7134 - loglik: -2.0374e+02 - logprior: 5.0243
Epoch 8/10
10/10 - 1s - loss: 197.8501 - loglik: -2.0359e+02 - logprior: 5.7446
Epoch 9/10
10/10 - 1s - loss: 196.5581 - loglik: -2.0285e+02 - logprior: 6.2941
Epoch 10/10
10/10 - 1s - loss: 196.5900 - loglik: -2.0329e+02 - logprior: 6.6965
Fitted a model with MAP estimate = -196.3105
Time for alignment: 50.0957
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 413.8941 - loglik: -3.5204e+02 - logprior: -6.1857e+01
Epoch 2/10
10/10 - 1s - loss: 327.8639 - loglik: -3.1360e+02 - logprior: -1.4268e+01
Epoch 3/10
10/10 - 1s - loss: 281.9294 - loglik: -2.7549e+02 - logprior: -6.4401e+00
Epoch 4/10
10/10 - 1s - loss: 255.0347 - loglik: -2.5062e+02 - logprior: -4.4132e+00
Epoch 5/10
10/10 - 1s - loss: 242.4913 - loglik: -2.3915e+02 - logprior: -3.3388e+00
Epoch 6/10
10/10 - 1s - loss: 238.4215 - loglik: -2.3603e+02 - logprior: -2.3923e+00
Epoch 7/10
10/10 - 1s - loss: 235.5380 - loglik: -2.3386e+02 - logprior: -1.6830e+00
Epoch 8/10
10/10 - 1s - loss: 233.6207 - loglik: -2.3222e+02 - logprior: -1.3986e+00
Epoch 9/10
10/10 - 1s - loss: 233.6591 - loglik: -2.3248e+02 - logprior: -1.1771e+00
Fitted a model with MAP estimate = -232.7129
expansions: [(9, 1), (12, 3), (14, 4), (29, 2), (37, 1), (38, 1), (40, 1), (48, 1), (56, 1), (60, 1), (63, 1), (64, 1), (71, 1), (78, 1), (79, 1), (80, 1), (85, 1), (87, 3), (88, 1), (92, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 310.7527 - loglik: -2.4115e+02 - logprior: -6.9604e+01
Epoch 2/2
10/10 - 2s - loss: 243.8922 - loglik: -2.1756e+02 - logprior: -2.6331e+01
Fitted a model with MAP estimate = -231.8234
expansions: [(0, 4)]
discards: [  0  13  14  36 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 271.1103 - loglik: -2.1683e+02 - logprior: -5.4283e+01
Epoch 2/2
10/10 - 1s - loss: 221.7085 - loglik: -2.1027e+02 - logprior: -1.1440e+01
Fitted a model with MAP estimate = -213.6861
expansions: []
discards: [ 1  2  3 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 266.8237 - loglik: -2.1359e+02 - logprior: -5.3238e+01
Epoch 2/10
10/10 - 1s - loss: 221.4823 - loglik: -2.1057e+02 - logprior: -1.0915e+01
Epoch 3/10
10/10 - 1s - loss: 211.8947 - loglik: -2.0981e+02 - logprior: -2.0859e+00
Epoch 4/10
10/10 - 1s - loss: 206.1206 - loglik: -2.0764e+02 - logprior: 1.5184
Epoch 5/10
10/10 - 1s - loss: 202.8242 - loglik: -2.0620e+02 - logprior: 3.3798
Epoch 6/10
10/10 - 1s - loss: 200.0043 - loglik: -2.0438e+02 - logprior: 4.3728
Epoch 7/10
10/10 - 1s - loss: 199.1458 - loglik: -2.0413e+02 - logprior: 4.9812
Epoch 8/10
10/10 - 1s - loss: 198.4083 - loglik: -2.0397e+02 - logprior: 5.5602
Epoch 9/10
10/10 - 1s - loss: 197.8166 - loglik: -2.0397e+02 - logprior: 6.1513
Epoch 10/10
10/10 - 1s - loss: 197.7913 - loglik: -2.0440e+02 - logprior: 6.6054
Fitted a model with MAP estimate = -196.9422
Time for alignment: 47.8370
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 414.1776 - loglik: -3.5232e+02 - logprior: -6.1859e+01
Epoch 2/10
10/10 - 1s - loss: 326.7480 - loglik: -3.1248e+02 - logprior: -1.4264e+01
Epoch 3/10
10/10 - 1s - loss: 281.3807 - loglik: -2.7501e+02 - logprior: -6.3684e+00
Epoch 4/10
10/10 - 1s - loss: 255.1941 - loglik: -2.5095e+02 - logprior: -4.2411e+00
Epoch 5/10
10/10 - 1s - loss: 242.8330 - loglik: -2.3963e+02 - logprior: -3.1987e+00
Epoch 6/10
10/10 - 1s - loss: 237.8055 - loglik: -2.3548e+02 - logprior: -2.3232e+00
Epoch 7/10
10/10 - 1s - loss: 234.7701 - loglik: -2.3312e+02 - logprior: -1.6534e+00
Epoch 8/10
10/10 - 1s - loss: 232.6555 - loglik: -2.3127e+02 - logprior: -1.3855e+00
Epoch 9/10
10/10 - 1s - loss: 232.1391 - loglik: -2.3091e+02 - logprior: -1.2301e+00
Epoch 10/10
10/10 - 1s - loss: 231.5710 - loglik: -2.3057e+02 - logprior: -9.9649e-01
Fitted a model with MAP estimate = -230.9480
expansions: [(9, 1), (12, 3), (14, 4), (33, 1), (37, 1), (38, 1), (40, 1), (48, 1), (56, 1), (60, 1), (63, 1), (64, 1), (71, 1), (75, 1), (78, 1), (80, 1), (85, 1), (87, 3), (88, 1), (92, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 308.8737 - loglik: -2.3935e+02 - logprior: -6.9525e+01
Epoch 2/2
10/10 - 1s - loss: 242.8662 - loglik: -2.1663e+02 - logprior: -2.6236e+01
Fitted a model with MAP estimate = -231.0116
expansions: [(0, 4)]
discards: [  0  13  14 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 269.7236 - loglik: -2.1551e+02 - logprior: -5.4217e+01
Epoch 2/2
10/10 - 1s - loss: 220.9674 - loglik: -2.0957e+02 - logprior: -1.1397e+01
Fitted a model with MAP estimate = -212.7274
expansions: []
discards: [ 1  2  3 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 265.8452 - loglik: -2.1263e+02 - logprior: -5.3216e+01
Epoch 2/10
10/10 - 1s - loss: 220.9170 - loglik: -2.1003e+02 - logprior: -1.0886e+01
Epoch 3/10
10/10 - 1s - loss: 210.0814 - loglik: -2.0798e+02 - logprior: -2.1058e+00
Epoch 4/10
10/10 - 1s - loss: 205.2286 - loglik: -2.0670e+02 - logprior: 1.4721
Epoch 5/10
10/10 - 1s - loss: 201.1901 - loglik: -2.0455e+02 - logprior: 3.3625
Epoch 6/10
10/10 - 1s - loss: 199.6115 - loglik: -2.0397e+02 - logprior: 4.3632
Epoch 7/10
10/10 - 1s - loss: 198.2644 - loglik: -2.0325e+02 - logprior: 4.9814
Epoch 8/10
10/10 - 1s - loss: 197.0418 - loglik: -2.0263e+02 - logprior: 5.5913
Epoch 9/10
10/10 - 1s - loss: 196.8600 - loglik: -2.0305e+02 - logprior: 6.1853
Epoch 10/10
10/10 - 1s - loss: 196.4055 - loglik: -2.0305e+02 - logprior: 6.6477
Fitted a model with MAP estimate = -196.0618
Time for alignment: 47.4752
Computed alignments with likelihoods: ['-196.5213', '-196.2028', '-196.3105', '-196.9422', '-196.0618']
Best model has likelihood: -196.0618  (prior= 6.8448 )
time for generating output: 0.1481
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.9200323101777059
Training of 5 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e86fbbd60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fac47cf10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff954dd30>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fdf2efca0>
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 196.7021 - loglik: -1.9453e+02 - logprior: -2.1768e+00
Epoch 2/10
22/22 - 1s - loss: 165.8673 - loglik: -1.6459e+02 - logprior: -1.2780e+00
Epoch 3/10
22/22 - 1s - loss: 158.2794 - loglik: -1.5697e+02 - logprior: -1.3121e+00
Epoch 4/10
22/22 - 1s - loss: 156.3811 - loglik: -1.5513e+02 - logprior: -1.2486e+00
Epoch 5/10
22/22 - 1s - loss: 155.7060 - loglik: -1.5445e+02 - logprior: -1.2600e+00
Epoch 6/10
22/22 - 1s - loss: 155.2658 - loglik: -1.5402e+02 - logprior: -1.2458e+00
Epoch 7/10
22/22 - 1s - loss: 154.9505 - loglik: -1.5372e+02 - logprior: -1.2353e+00
Epoch 8/10
22/22 - 1s - loss: 155.2492 - loglik: -1.5402e+02 - logprior: -1.2294e+00
Fitted a model with MAP estimate = -153.3263
expansions: [(8, 1), (9, 2), (12, 1), (13, 2), (20, 2), (21, 2), (22, 1), (25, 1), (42, 1), (46, 1), (48, 4), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 157.5007 - loglik: -1.5465e+02 - logprior: -2.8522e+00
Epoch 2/2
22/22 - 1s - loss: 148.1749 - loglik: -1.4673e+02 - logprior: -1.4401e+00
Fitted a model with MAP estimate = -146.2293
expansions: [(0, 2)]
discards: [ 0  9 17 26 29 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 148.2168 - loglik: -1.4609e+02 - logprior: -2.1234e+00
Epoch 2/2
22/22 - 1s - loss: 145.3774 - loglik: -1.4435e+02 - logprior: -1.0257e+00
Fitted a model with MAP estimate = -145.2194
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.4446 - loglik: -1.4430e+02 - logprior: -1.1400e+00
Epoch 2/10
32/32 - 2s - loss: 143.5493 - loglik: -1.4274e+02 - logprior: -8.1109e-01
Epoch 3/10
32/32 - 2s - loss: 142.7042 - loglik: -1.4191e+02 - logprior: -7.9042e-01
Epoch 4/10
32/32 - 2s - loss: 142.4860 - loglik: -1.4170e+02 - logprior: -7.8394e-01
Epoch 5/10
32/32 - 2s - loss: 142.6125 - loglik: -1.4184e+02 - logprior: -7.7607e-01
Fitted a model with MAP estimate = -142.3146
Time for alignment: 58.0371
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 196.7717 - loglik: -1.9460e+02 - logprior: -2.1731e+00
Epoch 2/10
22/22 - 1s - loss: 165.8018 - loglik: -1.6453e+02 - logprior: -1.2698e+00
Epoch 3/10
22/22 - 1s - loss: 157.9417 - loglik: -1.5661e+02 - logprior: -1.3329e+00
Epoch 4/10
22/22 - 1s - loss: 155.8109 - loglik: -1.5451e+02 - logprior: -1.2978e+00
Epoch 5/10
22/22 - 1s - loss: 154.9656 - loglik: -1.5366e+02 - logprior: -1.3026e+00
Epoch 6/10
22/22 - 1s - loss: 154.7657 - loglik: -1.5348e+02 - logprior: -1.2808e+00
Epoch 7/10
22/22 - 1s - loss: 154.7451 - loglik: -1.5347e+02 - logprior: -1.2736e+00
Epoch 8/10
22/22 - 1s - loss: 154.3588 - loglik: -1.5309e+02 - logprior: -1.2676e+00
Epoch 9/10
22/22 - 1s - loss: 154.1305 - loglik: -1.5287e+02 - logprior: -1.2618e+00
Epoch 10/10
22/22 - 1s - loss: 154.3133 - loglik: -1.5305e+02 - logprior: -1.2611e+00
Fitted a model with MAP estimate = -153.0627
expansions: [(8, 1), (9, 2), (12, 1), (13, 2), (20, 2), (21, 2), (23, 1), (25, 1), (42, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 156.7655 - loglik: -1.5391e+02 - logprior: -2.8586e+00
Epoch 2/2
22/22 - 1s - loss: 148.3857 - loglik: -1.4694e+02 - logprior: -1.4480e+00
Fitted a model with MAP estimate = -146.2069
expansions: [(0, 2)]
discards: [ 0  9 16 26 28 61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 147.8605 - loglik: -1.4574e+02 - logprior: -2.1229e+00
Epoch 2/2
22/22 - 1s - loss: 145.4301 - loglik: -1.4441e+02 - logprior: -1.0234e+00
Fitted a model with MAP estimate = -145.2685
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.5418 - loglik: -1.4439e+02 - logprior: -1.1471e+00
Epoch 2/10
32/32 - 2s - loss: 143.1542 - loglik: -1.4234e+02 - logprior: -8.1290e-01
Epoch 3/10
32/32 - 2s - loss: 142.8121 - loglik: -1.4202e+02 - logprior: -7.9448e-01
Epoch 4/10
32/32 - 2s - loss: 142.6641 - loglik: -1.4188e+02 - logprior: -7.8620e-01
Epoch 5/10
32/32 - 2s - loss: 142.3737 - loglik: -1.4159e+02 - logprior: -7.7940e-01
Epoch 6/10
32/32 - 2s - loss: 142.1109 - loglik: -1.4134e+02 - logprior: -7.7120e-01
Epoch 7/10
32/32 - 2s - loss: 142.4578 - loglik: -1.4169e+02 - logprior: -7.7032e-01
Fitted a model with MAP estimate = -142.0842
Time for alignment: 64.9841
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 197.0243 - loglik: -1.9485e+02 - logprior: -2.1774e+00
Epoch 2/10
22/22 - 1s - loss: 166.1754 - loglik: -1.6486e+02 - logprior: -1.3118e+00
Epoch 3/10
22/22 - 1s - loss: 157.4560 - loglik: -1.5609e+02 - logprior: -1.3676e+00
Epoch 4/10
22/22 - 1s - loss: 155.2176 - loglik: -1.5392e+02 - logprior: -1.2998e+00
Epoch 5/10
22/22 - 1s - loss: 154.9088 - loglik: -1.5361e+02 - logprior: -1.3018e+00
Epoch 6/10
22/22 - 1s - loss: 154.5470 - loglik: -1.5327e+02 - logprior: -1.2764e+00
Epoch 7/10
22/22 - 1s - loss: 154.1796 - loglik: -1.5291e+02 - logprior: -1.2687e+00
Epoch 8/10
22/22 - 1s - loss: 154.2040 - loglik: -1.5295e+02 - logprior: -1.2583e+00
Fitted a model with MAP estimate = -152.5387
expansions: [(9, 2), (12, 1), (14, 2), (15, 2), (20, 2), (21, 2), (23, 1), (25, 1), (47, 1), (48, 2), (49, 1), (50, 1), (51, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 157.8198 - loglik: -1.5496e+02 - logprior: -2.8647e+00
Epoch 2/2
22/22 - 1s - loss: 148.3764 - loglik: -1.4691e+02 - logprior: -1.4690e+00
Fitted a model with MAP estimate = -146.1966
expansions: [(0, 2)]
discards: [ 0  8 17 19 27 29 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.9247 - loglik: -1.4580e+02 - logprior: -2.1251e+00
Epoch 2/2
22/22 - 1s - loss: 144.8677 - loglik: -1.4384e+02 - logprior: -1.0238e+00
Fitted a model with MAP estimate = -145.2436
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.4819 - loglik: -1.4434e+02 - logprior: -1.1398e+00
Epoch 2/10
32/32 - 2s - loss: 143.3659 - loglik: -1.4256e+02 - logprior: -8.1084e-01
Epoch 3/10
32/32 - 2s - loss: 142.7962 - loglik: -1.4201e+02 - logprior: -7.8781e-01
Epoch 4/10
32/32 - 2s - loss: 142.5499 - loglik: -1.4176e+02 - logprior: -7.8644e-01
Epoch 5/10
32/32 - 2s - loss: 142.4955 - loglik: -1.4171e+02 - logprior: -7.8207e-01
Epoch 6/10
32/32 - 2s - loss: 142.3852 - loglik: -1.4161e+02 - logprior: -7.7210e-01
Epoch 7/10
32/32 - 2s - loss: 142.1075 - loglik: -1.4134e+02 - logprior: -7.6807e-01
Epoch 8/10
32/32 - 2s - loss: 142.1969 - loglik: -1.4143e+02 - logprior: -7.6276e-01
Fitted a model with MAP estimate = -142.0744
Time for alignment: 62.0678
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 196.8644 - loglik: -1.9469e+02 - logprior: -2.1779e+00
Epoch 2/10
22/22 - 1s - loss: 166.0153 - loglik: -1.6472e+02 - logprior: -1.2957e+00
Epoch 3/10
22/22 - 1s - loss: 158.4393 - loglik: -1.5710e+02 - logprior: -1.3389e+00
Epoch 4/10
22/22 - 1s - loss: 156.6671 - loglik: -1.5538e+02 - logprior: -1.2849e+00
Epoch 5/10
22/22 - 1s - loss: 155.6623 - loglik: -1.5437e+02 - logprior: -1.2945e+00
Epoch 6/10
22/22 - 1s - loss: 155.5362 - loglik: -1.5426e+02 - logprior: -1.2720e+00
Epoch 7/10
22/22 - 1s - loss: 155.1645 - loglik: -1.5390e+02 - logprior: -1.2613e+00
Epoch 8/10
22/22 - 1s - loss: 154.9744 - loglik: -1.5372e+02 - logprior: -1.2563e+00
Epoch 9/10
22/22 - 1s - loss: 155.3353 - loglik: -1.5409e+02 - logprior: -1.2491e+00
Fitted a model with MAP estimate = -153.4643
expansions: [(8, 1), (9, 2), (12, 1), (13, 2), (20, 2), (21, 1), (22, 1), (25, 1), (46, 1), (48, 2), (49, 1), (50, 1), (51, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 156.9568 - loglik: -1.5410e+02 - logprior: -2.8543e+00
Epoch 2/2
22/22 - 1s - loss: 148.4727 - loglik: -1.4705e+02 - logprior: -1.4277e+00
Fitted a model with MAP estimate = -146.2335
expansions: [(0, 2)]
discards: [ 0  9 17 25 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 148.0337 - loglik: -1.4591e+02 - logprior: -2.1208e+00
Epoch 2/2
22/22 - 1s - loss: 145.3963 - loglik: -1.4438e+02 - logprior: -1.0174e+00
Fitted a model with MAP estimate = -145.1982
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.4353 - loglik: -1.4429e+02 - logprior: -1.1455e+00
Epoch 2/10
32/32 - 2s - loss: 143.7123 - loglik: -1.4290e+02 - logprior: -8.0897e-01
Epoch 3/10
32/32 - 2s - loss: 142.6825 - loglik: -1.4189e+02 - logprior: -7.9558e-01
Epoch 4/10
32/32 - 2s - loss: 142.6309 - loglik: -1.4185e+02 - logprior: -7.8010e-01
Epoch 5/10
32/32 - 2s - loss: 142.3877 - loglik: -1.4160e+02 - logprior: -7.8272e-01
Epoch 6/10
32/32 - 2s - loss: 142.1471 - loglik: -1.4137e+02 - logprior: -7.7657e-01
Epoch 7/10
32/32 - 2s - loss: 142.4776 - loglik: -1.4171e+02 - logprior: -7.6640e-01
Fitted a model with MAP estimate = -142.1163
Time for alignment: 63.7266
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 196.6367 - loglik: -1.9446e+02 - logprior: -2.1774e+00
Epoch 2/10
22/22 - 1s - loss: 165.8041 - loglik: -1.6453e+02 - logprior: -1.2728e+00
Epoch 3/10
22/22 - 1s - loss: 159.5421 - loglik: -1.5827e+02 - logprior: -1.2727e+00
Epoch 4/10
22/22 - 1s - loss: 156.7858 - loglik: -1.5557e+02 - logprior: -1.2178e+00
Epoch 5/10
22/22 - 1s - loss: 156.4103 - loglik: -1.5517e+02 - logprior: -1.2400e+00
Epoch 6/10
22/22 - 1s - loss: 155.9408 - loglik: -1.5472e+02 - logprior: -1.2197e+00
Epoch 7/10
22/22 - 1s - loss: 155.6532 - loglik: -1.5444e+02 - logprior: -1.2122e+00
Epoch 8/10
22/22 - 1s - loss: 155.7155 - loglik: -1.5451e+02 - logprior: -1.2078e+00
Fitted a model with MAP estimate = -153.8273
expansions: [(8, 1), (9, 2), (12, 1), (13, 2), (20, 2), (21, 1), (22, 1), (25, 1), (46, 1), (48, 5), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 156.9446 - loglik: -1.5410e+02 - logprior: -2.8427e+00
Epoch 2/2
22/22 - 1s - loss: 148.4793 - loglik: -1.4707e+02 - logprior: -1.4116e+00
Fitted a model with MAP estimate = -146.1772
expansions: [(0, 2)]
discards: [ 0  9 17 25 63]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.8359 - loglik: -1.4571e+02 - logprior: -2.1234e+00
Epoch 2/2
22/22 - 1s - loss: 145.4205 - loglik: -1.4440e+02 - logprior: -1.0215e+00
Fitted a model with MAP estimate = -145.2536
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.4384 - loglik: -1.4429e+02 - logprior: -1.1463e+00
Epoch 2/10
32/32 - 2s - loss: 143.3980 - loglik: -1.4258e+02 - logprior: -8.1866e-01
Epoch 3/10
32/32 - 2s - loss: 142.8292 - loglik: -1.4203e+02 - logprior: -7.9529e-01
Epoch 4/10
32/32 - 2s - loss: 142.6112 - loglik: -1.4183e+02 - logprior: -7.7956e-01
Epoch 5/10
32/32 - 2s - loss: 142.5601 - loglik: -1.4177e+02 - logprior: -7.8770e-01
Epoch 6/10
32/32 - 2s - loss: 142.2552 - loglik: -1.4148e+02 - logprior: -7.7797e-01
Epoch 7/10
32/32 - 2s - loss: 142.2514 - loglik: -1.4148e+02 - logprior: -7.7143e-01
Epoch 8/10
32/32 - 2s - loss: 142.2713 - loglik: -1.4151e+02 - logprior: -7.6099e-01
Fitted a model with MAP estimate = -142.1143
Time for alignment: 61.8807
Computed alignments with likelihoods: ['-142.3146', '-142.0842', '-142.0744', '-142.1163', '-142.1143']
Best model has likelihood: -142.0744  (prior= -0.7667 )
time for generating output: 0.1340
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.8367318337888024
Training of 5 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f806d5250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fac5ecc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fc64e3910>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f078cee50>
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 484.3035 - loglik: -4.7093e+02 - logprior: -1.3372e+01
Epoch 2/10
17/17 - 4s - loss: 351.0988 - loglik: -3.4950e+02 - logprior: -1.6017e+00
Epoch 3/10
17/17 - 4s - loss: 293.8947 - loglik: -2.9223e+02 - logprior: -1.6664e+00
Epoch 4/10
17/17 - 4s - loss: 277.0950 - loglik: -2.7547e+02 - logprior: -1.6295e+00
Epoch 5/10
17/17 - 4s - loss: 274.9210 - loglik: -2.7349e+02 - logprior: -1.4332e+00
Epoch 6/10
17/17 - 4s - loss: 271.3287 - loglik: -2.6999e+02 - logprior: -1.3407e+00
Epoch 7/10
17/17 - 4s - loss: 268.9552 - loglik: -2.6768e+02 - logprior: -1.2736e+00
Epoch 8/10
17/17 - 4s - loss: 271.1418 - loglik: -2.6991e+02 - logprior: -1.2307e+00
Fitted a model with MAP estimate = -268.9662
expansions: [(0, 14), (39, 1), (40, 3), (45, 1), (46, 1), (47, 18), (50, 2), (92, 1), (93, 1), (137, 1), (138, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 306.0599 - loglik: -2.9246e+02 - logprior: -1.3601e+01
Epoch 2/2
17/17 - 4s - loss: 265.6990 - loglik: -2.6433e+02 - logprior: -1.3645e+00
Fitted a model with MAP estimate = -257.5559
expansions: [(0, 30), (23, 1), (39, 2), (40, 3), (41, 2), (43, 2), (44, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 199 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 271.4291 - loglik: -2.5891e+02 - logprior: -1.2516e+01
Epoch 2/2
17/17 - 5s - loss: 233.5040 - loglik: -2.3258e+02 - logprior: -9.2622e-01
Fitted a model with MAP estimate = -224.3062
expansions: [(0, 21), (37, 1), (54, 1), (59, 1), (61, 1), (65, 1), (66, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 258.5533 - loglik: -2.4676e+02 - logprior: -1.1794e+01
Epoch 2/10
17/17 - 5s - loss: 230.0371 - loglik: -2.2999e+02 - logprior: -4.4100e-02
Epoch 3/10
17/17 - 4s - loss: 223.7855 - loglik: -2.2521e+02 - logprior: 1.4284
Epoch 4/10
17/17 - 5s - loss: 221.1578 - loglik: -2.2319e+02 - logprior: 2.0283
Epoch 5/10
17/17 - 5s - loss: 216.8341 - loglik: -2.1908e+02 - logprior: 2.2485
Epoch 6/10
17/17 - 5s - loss: 217.2434 - loglik: -2.1962e+02 - logprior: 2.3811
Fitted a model with MAP estimate = -215.3257
Time for alignment: 103.1942
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 485.0517 - loglik: -4.7166e+02 - logprior: -1.3390e+01
Epoch 2/10
17/17 - 4s - loss: 353.0893 - loglik: -3.5140e+02 - logprior: -1.6908e+00
Epoch 3/10
17/17 - 4s - loss: 291.8738 - loglik: -2.9006e+02 - logprior: -1.8134e+00
Epoch 4/10
17/17 - 4s - loss: 281.1245 - loglik: -2.7954e+02 - logprior: -1.5863e+00
Epoch 5/10
17/17 - 4s - loss: 274.3390 - loglik: -2.7306e+02 - logprior: -1.2765e+00
Epoch 6/10
17/17 - 4s - loss: 274.5800 - loglik: -2.7342e+02 - logprior: -1.1626e+00
Fitted a model with MAP estimate = -272.1471
expansions: [(0, 16), (38, 1), (39, 4), (45, 1), (47, 17), (50, 1), (57, 1), (94, 1), (118, 1), (137, 1), (138, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 174 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 304.7958 - loglik: -2.9127e+02 - logprior: -1.3521e+01
Epoch 2/2
17/17 - 4s - loss: 263.1470 - loglik: -2.6176e+02 - logprior: -1.3822e+00
Fitted a model with MAP estimate = -254.2671
expansions: [(0, 30), (26, 1), (27, 1), (42, 3), (43, 1), (44, 1), (45, 2), (46, 2), (47, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 201 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 267.7172 - loglik: -2.5533e+02 - logprior: -1.2386e+01
Epoch 2/2
17/17 - 5s - loss: 232.5482 - loglik: -2.3181e+02 - logprior: -7.3871e-01
Fitted a model with MAP estimate = -223.0100
expansions: [(0, 20), (59, 1), (60, 1), (74, 1), (75, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 258.0250 - loglik: -2.4620e+02 - logprior: -1.1826e+01
Epoch 2/10
17/17 - 5s - loss: 229.8694 - loglik: -2.2974e+02 - logprior: -1.3424e-01
Epoch 3/10
17/17 - 4s - loss: 226.3156 - loglik: -2.2766e+02 - logprior: 1.3433
Epoch 4/10
17/17 - 5s - loss: 221.1126 - loglik: -2.2306e+02 - logprior: 1.9480
Epoch 5/10
17/17 - 5s - loss: 220.4386 - loglik: -2.2259e+02 - logprior: 2.1520
Epoch 6/10
17/17 - 4s - loss: 218.7350 - loglik: -2.2098e+02 - logprior: 2.2441
Epoch 7/10
17/17 - 5s - loss: 216.0947 - loglik: -2.1846e+02 - logprior: 2.3696
Epoch 8/10
17/17 - 5s - loss: 215.1885 - loglik: -2.1766e+02 - logprior: 2.4712
Epoch 9/10
17/17 - 5s - loss: 215.7015 - loglik: -2.1831e+02 - logprior: 2.6070
Fitted a model with MAP estimate = -213.8084
Time for alignment: 108.7592
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 484.4804 - loglik: -4.7110e+02 - logprior: -1.3385e+01
Epoch 2/10
17/17 - 4s - loss: 348.0223 - loglik: -3.4638e+02 - logprior: -1.6417e+00
Epoch 3/10
17/17 - 4s - loss: 293.8077 - loglik: -2.9209e+02 - logprior: -1.7152e+00
Epoch 4/10
17/17 - 4s - loss: 279.2039 - loglik: -2.7779e+02 - logprior: -1.4146e+00
Epoch 5/10
17/17 - 4s - loss: 275.4154 - loglik: -2.7425e+02 - logprior: -1.1611e+00
Epoch 6/10
17/17 - 3s - loss: 272.7288 - loglik: -2.7165e+02 - logprior: -1.0827e+00
Epoch 7/10
17/17 - 4s - loss: 270.9852 - loglik: -2.6995e+02 - logprior: -1.0396e+00
Epoch 8/10
17/17 - 4s - loss: 269.8135 - loglik: -2.6882e+02 - logprior: -9.9072e-01
Epoch 9/10
17/17 - 4s - loss: 268.7776 - loglik: -2.6781e+02 - logprior: -9.6941e-01
Epoch 10/10
17/17 - 4s - loss: 268.0475 - loglik: -2.6704e+02 - logprior: -1.0099e+00
Fitted a model with MAP estimate = -267.2943
expansions: [(0, 10), (30, 1), (31, 2), (32, 3), (38, 1), (39, 1), (40, 3), (45, 1), (49, 1), (80, 1), (93, 1), (137, 1), (138, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 166 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 298.3683 - loglik: -2.8466e+02 - logprior: -1.3705e+01
Epoch 2/2
17/17 - 4s - loss: 269.9694 - loglik: -2.6856e+02 - logprior: -1.4098e+00
Fitted a model with MAP estimate = -264.5507
expansions: [(0, 25), (36, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 166 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 296.2038 - loglik: -2.8355e+02 - logprior: -1.2656e+01
Epoch 2/2
17/17 - 4s - loss: 267.8496 - loglik: -2.6689e+02 - logprior: -9.5476e-01
Fitted a model with MAP estimate = -262.7928
expansions: [(0, 24)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 165 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 294.5604 - loglik: -2.8219e+02 - logprior: -1.2367e+01
Epoch 2/10
17/17 - 4s - loss: 268.8030 - loglik: -2.6811e+02 - logprior: -6.9240e-01
Epoch 3/10
17/17 - 4s - loss: 261.5795 - loglik: -2.6223e+02 - logprior: 0.6486
Epoch 4/10
17/17 - 4s - loss: 259.8453 - loglik: -2.6111e+02 - logprior: 1.2626
Epoch 5/10
17/17 - 4s - loss: 257.7395 - loglik: -2.5926e+02 - logprior: 1.5238
Epoch 6/10
17/17 - 4s - loss: 253.1487 - loglik: -2.5478e+02 - logprior: 1.6296
Epoch 7/10
17/17 - 4s - loss: 254.2869 - loglik: -2.5600e+02 - logprior: 1.7091
Fitted a model with MAP estimate = -251.9179
Time for alignment: 104.4519
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 484.9821 - loglik: -4.7159e+02 - logprior: -1.3393e+01
Epoch 2/10
17/17 - 4s - loss: 355.6642 - loglik: -3.5393e+02 - logprior: -1.7361e+00
Epoch 3/10
17/17 - 4s - loss: 296.0436 - loglik: -2.9427e+02 - logprior: -1.7739e+00
Epoch 4/10
17/17 - 4s - loss: 281.9983 - loglik: -2.8045e+02 - logprior: -1.5433e+00
Epoch 5/10
17/17 - 4s - loss: 275.2534 - loglik: -2.7394e+02 - logprior: -1.3112e+00
Epoch 6/10
17/17 - 4s - loss: 275.0791 - loglik: -2.7388e+02 - logprior: -1.1999e+00
Epoch 7/10
17/17 - 4s - loss: 274.6932 - loglik: -2.7357e+02 - logprior: -1.1225e+00
Epoch 8/10
17/17 - 4s - loss: 271.7296 - loglik: -2.7061e+02 - logprior: -1.1232e+00
Epoch 9/10
17/17 - 4s - loss: 271.6061 - loglik: -2.7048e+02 - logprior: -1.1218e+00
Epoch 10/10
17/17 - 4s - loss: 269.9345 - loglik: -2.6879e+02 - logprior: -1.1429e+00
Fitted a model with MAP estimate = -269.8197
expansions: [(0, 12), (30, 1), (31, 2), (32, 3), (38, 1), (39, 4), (45, 1), (47, 5), (48, 13), (51, 2), (74, 1), (94, 1), (106, 1), (138, 1), (142, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 298.3668 - loglik: -2.8474e+02 - logprior: -1.3629e+01
Epoch 2/2
17/17 - 4s - loss: 254.6655 - loglik: -2.5332e+02 - logprior: -1.3449e+00
Fitted a model with MAP estimate = -247.3913
expansions: [(0, 25), (32, 1), (33, 1), (51, 5), (53, 1), (54, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 270.8450 - loglik: -2.5848e+02 - logprior: -1.2368e+01
Epoch 2/2
17/17 - 4s - loss: 237.9392 - loglik: -2.3728e+02 - logprior: -6.6342e-01
Fitted a model with MAP estimate = -230.9904
expansions: [(0, 23), (56, 3), (65, 1), (67, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 197 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 260.7609 - loglik: -2.4892e+02 - logprior: -1.1837e+01
Epoch 2/10
17/17 - 5s - loss: 231.4609 - loglik: -2.3126e+02 - logprior: -1.9644e-01
Epoch 3/10
17/17 - 5s - loss: 223.4123 - loglik: -2.2468e+02 - logprior: 1.2647
Epoch 4/10
17/17 - 5s - loss: 219.4190 - loglik: -2.2129e+02 - logprior: 1.8733
Epoch 5/10
17/17 - 5s - loss: 217.7627 - loglik: -2.1987e+02 - logprior: 2.1055
Epoch 6/10
17/17 - 5s - loss: 216.4533 - loglik: -2.1868e+02 - logprior: 2.2276
Epoch 7/10
17/17 - 4s - loss: 212.6623 - loglik: -2.1501e+02 - logprior: 2.3427
Epoch 8/10
17/17 - 5s - loss: 212.7495 - loglik: -2.1524e+02 - logprior: 2.4861
Fitted a model with MAP estimate = -212.0070
Time for alignment: 117.4655
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 480.9550 - loglik: -4.6789e+02 - logprior: -1.3062e+01
Epoch 2/10
17/17 - 4s - loss: 338.2256 - loglik: -3.3706e+02 - logprior: -1.1639e+00
Epoch 3/10
17/17 - 3s - loss: 274.7676 - loglik: -2.7357e+02 - logprior: -1.2016e+00
Epoch 4/10
17/17 - 4s - loss: 267.6219 - loglik: -2.6679e+02 - logprior: -8.3057e-01
Epoch 5/10
17/17 - 4s - loss: 261.4261 - loglik: -2.6092e+02 - logprior: -5.0641e-01
Epoch 6/10
17/17 - 4s - loss: 258.8692 - loglik: -2.5842e+02 - logprior: -4.5174e-01
Epoch 7/10
17/17 - 4s - loss: 256.2553 - loglik: -2.5579e+02 - logprior: -4.6941e-01
Epoch 8/10
17/17 - 4s - loss: 258.2145 - loglik: -2.5776e+02 - logprior: -4.5402e-01
Fitted a model with MAP estimate = -256.1972
expansions: [(0, 37), (8, 1), (9, 2), (10, 1), (19, 1), (25, 1), (26, 1), (28, 1), (32, 1), (49, 1), (80, 1), (101, 1), (137, 1), (138, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 212 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 263.3783 - loglik: -2.5089e+02 - logprior: -1.2492e+01
Epoch 2/2
17/17 - 5s - loss: 221.3932 - loglik: -2.2106e+02 - logprior: -3.3224e-01
Fitted a model with MAP estimate = -212.3279
expansions: [(0, 17)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 257.3724 - loglik: -2.4545e+02 - logprior: -1.1925e+01
Epoch 2/2
17/17 - 5s - loss: 231.4552 - loglik: -2.3120e+02 - logprior: -2.5149e-01
Fitted a model with MAP estimate = -228.1197
expansions: [(0, 30)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 252.3650 - loglik: -2.4096e+02 - logprior: -1.1404e+01
Epoch 2/10
17/17 - 5s - loss: 223.6551 - loglik: -2.2372e+02 - logprior: 0.0645
Epoch 3/10
17/17 - 5s - loss: 215.5841 - loglik: -2.1702e+02 - logprior: 1.4387
Epoch 4/10
17/17 - 5s - loss: 210.9416 - loglik: -2.1291e+02 - logprior: 1.9722
Epoch 5/10
17/17 - 5s - loss: 208.8250 - loglik: -2.1107e+02 - logprior: 2.2451
Epoch 6/10
17/17 - 5s - loss: 207.2476 - loglik: -2.0966e+02 - logprior: 2.4107
Epoch 7/10
17/17 - 5s - loss: 205.3010 - loglik: -2.0783e+02 - logprior: 2.5304
Epoch 8/10
17/17 - 5s - loss: 203.4442 - loglik: -2.0609e+02 - logprior: 2.6449
Epoch 9/10
17/17 - 5s - loss: 203.2258 - loglik: -2.0604e+02 - logprior: 2.8119
Epoch 10/10
17/17 - 5s - loss: 202.4997 - loglik: -2.0550e+02 - logprior: 3.0011
Fitted a model with MAP estimate = -202.0641
Time for alignment: 123.4101
Computed alignments with likelihoods: ['-215.3257', '-213.8084', '-251.9179', '-212.0070', '-202.0641']
Best model has likelihood: -202.0641  (prior= 3.1048 )
time for generating output: 0.3056
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.2458604066233494
Training of 5 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf478cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5bd0160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f92ac94c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f7f9f3d30>
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 247.1230 - loglik: -2.2688e+02 - logprior: -2.0246e+01
Epoch 2/10
10/10 - 1s - loss: 218.4120 - loglik: -2.1298e+02 - logprior: -5.4344e+00
Epoch 3/10
10/10 - 1s - loss: 201.8491 - loglik: -1.9893e+02 - logprior: -2.9142e+00
Epoch 4/10
10/10 - 1s - loss: 191.9826 - loglik: -1.8967e+02 - logprior: -2.3172e+00
Epoch 5/10
10/10 - 1s - loss: 187.6578 - loglik: -1.8553e+02 - logprior: -2.1313e+00
Epoch 6/10
10/10 - 1s - loss: 185.6468 - loglik: -1.8382e+02 - logprior: -1.8239e+00
Epoch 7/10
10/10 - 1s - loss: 184.8531 - loglik: -1.8330e+02 - logprior: -1.5490e+00
Epoch 8/10
10/10 - 1s - loss: 183.7790 - loglik: -1.8230e+02 - logprior: -1.4832e+00
Epoch 9/10
10/10 - 1s - loss: 183.9035 - loglik: -1.8247e+02 - logprior: -1.4353e+00
Fitted a model with MAP estimate = -183.6076
expansions: [(0, 2), (7, 2), (8, 1), (9, 1), (33, 1), (41, 1), (42, 2), (43, 1), (44, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 210.0498 - loglik: -1.8363e+02 - logprior: -2.6424e+01
Epoch 2/2
10/10 - 1s - loss: 188.0260 - loglik: -1.7999e+02 - logprior: -8.0320e+00
Fitted a model with MAP estimate = -183.5896
expansions: []
discards: [ 0 55]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.0977 - loglik: -1.7998e+02 - logprior: -2.3121e+01
Epoch 2/2
10/10 - 1s - loss: 187.9003 - loglik: -1.7879e+02 - logprior: -9.1115e+00
Fitted a model with MAP estimate = -184.9504
expansions: [(0, 2)]
discards: [ 0 11 51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.6867 - loglik: -1.7944e+02 - logprior: -2.0245e+01
Epoch 2/10
10/10 - 1s - loss: 183.6529 - loglik: -1.7830e+02 - logprior: -5.3502e+00
Epoch 3/10
10/10 - 1s - loss: 179.8387 - loglik: -1.7750e+02 - logprior: -2.3349e+00
Epoch 4/10
10/10 - 1s - loss: 178.6859 - loglik: -1.7738e+02 - logprior: -1.3053e+00
Epoch 5/10
10/10 - 1s - loss: 177.6633 - loglik: -1.7676e+02 - logprior: -9.0039e-01
Epoch 6/10
10/10 - 1s - loss: 177.6688 - loglik: -1.7702e+02 - logprior: -6.4840e-01
Fitted a model with MAP estimate = -177.2671
Time for alignment: 35.3006
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.9066 - loglik: -2.2666e+02 - logprior: -2.0244e+01
Epoch 2/10
10/10 - 1s - loss: 218.8941 - loglik: -2.1347e+02 - logprior: -5.4284e+00
Epoch 3/10
10/10 - 1s - loss: 201.2742 - loglik: -1.9837e+02 - logprior: -2.9079e+00
Epoch 4/10
10/10 - 1s - loss: 192.3890 - loglik: -1.9012e+02 - logprior: -2.2656e+00
Epoch 5/10
10/10 - 1s - loss: 187.9001 - loglik: -1.8585e+02 - logprior: -2.0498e+00
Epoch 6/10
10/10 - 1s - loss: 185.9822 - loglik: -1.8422e+02 - logprior: -1.7668e+00
Epoch 7/10
10/10 - 1s - loss: 184.9305 - loglik: -1.8343e+02 - logprior: -1.4960e+00
Epoch 8/10
10/10 - 1s - loss: 184.6434 - loglik: -1.8321e+02 - logprior: -1.4323e+00
Epoch 9/10
10/10 - 1s - loss: 184.0421 - loglik: -1.8263e+02 - logprior: -1.4123e+00
Epoch 10/10
10/10 - 1s - loss: 183.5252 - loglik: -1.8216e+02 - logprior: -1.3692e+00
Fitted a model with MAP estimate = -183.3788
expansions: [(0, 2), (7, 2), (8, 2), (40, 1), (41, 1), (43, 1), (44, 2), (45, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.3113 - loglik: -1.8372e+02 - logprior: -2.6594e+01
Epoch 2/2
10/10 - 1s - loss: 187.4262 - loglik: -1.7932e+02 - logprior: -8.1100e+00
Fitted a model with MAP estimate = -183.2213
expansions: []
discards: [ 0 12 54]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.1628 - loglik: -1.7996e+02 - logprior: -2.3207e+01
Epoch 2/2
10/10 - 1s - loss: 188.1571 - loglik: -1.7901e+02 - logprior: -9.1487e+00
Fitted a model with MAP estimate = -185.1984
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.9409 - loglik: -1.7861e+02 - logprior: -2.0332e+01
Epoch 2/10
10/10 - 1s - loss: 183.0975 - loglik: -1.7767e+02 - logprior: -5.4247e+00
Epoch 3/10
10/10 - 1s - loss: 179.8528 - loglik: -1.7749e+02 - logprior: -2.3636e+00
Epoch 4/10
10/10 - 1s - loss: 178.5686 - loglik: -1.7718e+02 - logprior: -1.3864e+00
Epoch 5/10
10/10 - 1s - loss: 177.8023 - loglik: -1.7677e+02 - logprior: -1.0344e+00
Epoch 6/10
10/10 - 1s - loss: 177.2545 - loglik: -1.7654e+02 - logprior: -7.1692e-01
Epoch 7/10
10/10 - 1s - loss: 176.9825 - loglik: -1.7653e+02 - logprior: -4.5715e-01
Epoch 8/10
10/10 - 1s - loss: 177.0490 - loglik: -1.7666e+02 - logprior: -3.9231e-01
Fitted a model with MAP estimate = -176.6421
Time for alignment: 35.5635
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 247.0793 - loglik: -2.2683e+02 - logprior: -2.0245e+01
Epoch 2/10
10/10 - 1s - loss: 218.1655 - loglik: -2.1274e+02 - logprior: -5.4253e+00
Epoch 3/10
10/10 - 1s - loss: 200.2758 - loglik: -1.9738e+02 - logprior: -2.8943e+00
Epoch 4/10
10/10 - 1s - loss: 191.2627 - loglik: -1.8903e+02 - logprior: -2.2345e+00
Epoch 5/10
10/10 - 1s - loss: 187.9143 - loglik: -1.8594e+02 - logprior: -1.9721e+00
Epoch 6/10
10/10 - 1s - loss: 185.9303 - loglik: -1.8433e+02 - logprior: -1.5984e+00
Epoch 7/10
10/10 - 1s - loss: 185.5343 - loglik: -1.8420e+02 - logprior: -1.3306e+00
Epoch 8/10
10/10 - 1s - loss: 184.7932 - loglik: -1.8351e+02 - logprior: -1.2784e+00
Epoch 9/10
10/10 - 1s - loss: 184.4253 - loglik: -1.8320e+02 - logprior: -1.2225e+00
Epoch 10/10
10/10 - 1s - loss: 184.2630 - loglik: -1.8308e+02 - logprior: -1.1833e+00
Fitted a model with MAP estimate = -184.0933
expansions: [(0, 2), (7, 2), (8, 2), (41, 3), (42, 2), (43, 2), (44, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 210.4311 - loglik: -1.8390e+02 - logprior: -2.6531e+01
Epoch 2/2
10/10 - 1s - loss: 187.5667 - loglik: -1.7946e+02 - logprior: -8.1087e+00
Fitted a model with MAP estimate = -183.2496
expansions: []
discards: [ 0 12 49 54 55]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.7247 - loglik: -1.8048e+02 - logprior: -2.3244e+01
Epoch 2/2
10/10 - 1s - loss: 188.4793 - loglik: -1.7936e+02 - logprior: -9.1204e+00
Fitted a model with MAP estimate = -185.4629
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 198.7262 - loglik: -1.7843e+02 - logprior: -2.0296e+01
Epoch 2/10
10/10 - 1s - loss: 183.3582 - loglik: -1.7799e+02 - logprior: -5.3714e+00
Epoch 3/10
10/10 - 1s - loss: 180.0665 - loglik: -1.7774e+02 - logprior: -2.3268e+00
Epoch 4/10
10/10 - 1s - loss: 178.3892 - loglik: -1.7698e+02 - logprior: -1.4062e+00
Epoch 5/10
10/10 - 1s - loss: 177.8963 - loglik: -1.7683e+02 - logprior: -1.0667e+00
Epoch 6/10
10/10 - 1s - loss: 177.3027 - loglik: -1.7657e+02 - logprior: -7.3579e-01
Epoch 7/10
10/10 - 1s - loss: 176.8995 - loglik: -1.7644e+02 - logprior: -4.5960e-01
Epoch 8/10
10/10 - 1s - loss: 176.8995 - loglik: -1.7651e+02 - logprior: -3.8709e-01
Epoch 9/10
10/10 - 1s - loss: 176.5167 - loglik: -1.7618e+02 - logprior: -3.3237e-01
Epoch 10/10
10/10 - 1s - loss: 176.1560 - loglik: -1.7588e+02 - logprior: -2.7222e-01
Fitted a model with MAP estimate = -176.2898
Time for alignment: 38.1836
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 247.2325 - loglik: -2.2699e+02 - logprior: -2.0244e+01
Epoch 2/10
10/10 - 1s - loss: 218.8953 - loglik: -2.1347e+02 - logprior: -5.4262e+00
Epoch 3/10
10/10 - 1s - loss: 200.8474 - loglik: -1.9795e+02 - logprior: -2.8987e+00
Epoch 4/10
10/10 - 1s - loss: 191.9009 - loglik: -1.8965e+02 - logprior: -2.2528e+00
Epoch 5/10
10/10 - 1s - loss: 188.1459 - loglik: -1.8610e+02 - logprior: -2.0460e+00
Epoch 6/10
10/10 - 1s - loss: 186.2026 - loglik: -1.8450e+02 - logprior: -1.7013e+00
Epoch 7/10
10/10 - 1s - loss: 185.4584 - loglik: -1.8401e+02 - logprior: -1.4485e+00
Epoch 8/10
10/10 - 1s - loss: 184.6411 - loglik: -1.8324e+02 - logprior: -1.4036e+00
Epoch 9/10
10/10 - 1s - loss: 184.5372 - loglik: -1.8318e+02 - logprior: -1.3583e+00
Epoch 10/10
10/10 - 1s - loss: 184.2425 - loglik: -1.8293e+02 - logprior: -1.3151e+00
Fitted a model with MAP estimate = -183.9729
expansions: [(0, 2), (7, 2), (8, 2), (40, 1), (41, 1), (42, 3), (44, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 210.5268 - loglik: -1.8392e+02 - logprior: -2.6604e+01
Epoch 2/2
10/10 - 1s - loss: 187.9177 - loglik: -1.7977e+02 - logprior: -8.1436e+00
Fitted a model with MAP estimate = -183.3738
expansions: []
discards: [ 0 12 50 55]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 203.5181 - loglik: -1.8024e+02 - logprior: -2.3279e+01
Epoch 2/2
10/10 - 1s - loss: 188.3891 - loglik: -1.7923e+02 - logprior: -9.1615e+00
Fitted a model with MAP estimate = -185.6770
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.2994 - loglik: -1.7895e+02 - logprior: -2.0354e+01
Epoch 2/10
10/10 - 1s - loss: 183.6837 - loglik: -1.7825e+02 - logprior: -5.4288e+00
Epoch 3/10
10/10 - 1s - loss: 180.2509 - loglik: -1.7786e+02 - logprior: -2.3861e+00
Epoch 4/10
10/10 - 1s - loss: 178.9227 - loglik: -1.7752e+02 - logprior: -1.3977e+00
Epoch 5/10
10/10 - 1s - loss: 178.2914 - loglik: -1.7725e+02 - logprior: -1.0416e+00
Epoch 6/10
10/10 - 1s - loss: 177.9231 - loglik: -1.7720e+02 - logprior: -7.1855e-01
Epoch 7/10
10/10 - 1s - loss: 177.5435 - loglik: -1.7706e+02 - logprior: -4.8706e-01
Epoch 8/10
10/10 - 1s - loss: 177.1656 - loglik: -1.7675e+02 - logprior: -4.1132e-01
Epoch 9/10
10/10 - 1s - loss: 177.2224 - loglik: -1.7685e+02 - logprior: -3.7243e-01
Fitted a model with MAP estimate = -176.9362
Time for alignment: 35.2857
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 247.1842 - loglik: -2.2694e+02 - logprior: -2.0243e+01
Epoch 2/10
10/10 - 1s - loss: 218.5763 - loglik: -2.1315e+02 - logprior: -5.4272e+00
Epoch 3/10
10/10 - 1s - loss: 202.6824 - loglik: -1.9978e+02 - logprior: -2.9010e+00
Epoch 4/10
10/10 - 1s - loss: 193.9077 - loglik: -1.9165e+02 - logprior: -2.2610e+00
Epoch 5/10
10/10 - 1s - loss: 188.9016 - loglik: -1.8685e+02 - logprior: -2.0516e+00
Epoch 6/10
10/10 - 1s - loss: 186.7955 - loglik: -1.8504e+02 - logprior: -1.7603e+00
Epoch 7/10
10/10 - 1s - loss: 185.7374 - loglik: -1.8423e+02 - logprior: -1.5049e+00
Epoch 8/10
10/10 - 1s - loss: 185.1608 - loglik: -1.8373e+02 - logprior: -1.4290e+00
Epoch 9/10
10/10 - 1s - loss: 184.6291 - loglik: -1.8327e+02 - logprior: -1.3618e+00
Epoch 10/10
10/10 - 1s - loss: 184.3693 - loglik: -1.8304e+02 - logprior: -1.3283e+00
Fitted a model with MAP estimate = -184.0559
expansions: [(0, 2), (7, 2), (8, 2), (22, 1), (41, 1), (42, 2), (43, 1), (44, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 210.6704 - loglik: -1.8405e+02 - logprior: -2.6619e+01
Epoch 2/2
10/10 - 1s - loss: 188.0385 - loglik: -1.7983e+02 - logprior: -8.2051e+00
Fitted a model with MAP estimate = -183.3248
expansions: []
discards: [ 0 12 55]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.2359 - loglik: -1.8001e+02 - logprior: -2.3225e+01
Epoch 2/2
10/10 - 1s - loss: 188.0713 - loglik: -1.7892e+02 - logprior: -9.1534e+00
Fitted a model with MAP estimate = -185.1147
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.6871 - loglik: -1.7836e+02 - logprior: -2.0325e+01
Epoch 2/10
10/10 - 1s - loss: 183.0929 - loglik: -1.7768e+02 - logprior: -5.4144e+00
Epoch 3/10
10/10 - 1s - loss: 179.6822 - loglik: -1.7733e+02 - logprior: -2.3496e+00
Epoch 4/10
10/10 - 1s - loss: 178.2387 - loglik: -1.7686e+02 - logprior: -1.3783e+00
Epoch 5/10
10/10 - 1s - loss: 177.7206 - loglik: -1.7675e+02 - logprior: -9.7141e-01
Epoch 6/10
10/10 - 1s - loss: 176.9610 - loglik: -1.7633e+02 - logprior: -6.3538e-01
Epoch 7/10
10/10 - 1s - loss: 176.6209 - loglik: -1.7623e+02 - logprior: -3.9148e-01
Epoch 8/10
10/10 - 1s - loss: 176.6128 - loglik: -1.7631e+02 - logprior: -3.0584e-01
Epoch 9/10
10/10 - 1s - loss: 176.5103 - loglik: -1.7628e+02 - logprior: -2.3426e-01
Epoch 10/10
10/10 - 1s - loss: 176.4224 - loglik: -1.7626e+02 - logprior: -1.6617e-01
Fitted a model with MAP estimate = -176.1340
Time for alignment: 35.6185
Computed alignments with likelihoods: ['-177.2671', '-176.6421', '-176.2898', '-176.9362', '-176.1340']
Best model has likelihood: -176.1340  (prior= -0.1466 )
time for generating output: 0.1410
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.6736318407960199
Training of 5 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91861880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f7fe202e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f929f2190>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f915e4af0>
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 204.4867 - loglik: -1.9304e+02 - logprior: -1.1451e+01
Epoch 2/10
11/11 - 1s - loss: 164.8912 - loglik: -1.6172e+02 - logprior: -3.1689e+00
Epoch 3/10
11/11 - 1s - loss: 131.3954 - loglik: -1.2908e+02 - logprior: -2.3113e+00
Epoch 4/10
11/11 - 1s - loss: 111.3933 - loglik: -1.0938e+02 - logprior: -2.0116e+00
Epoch 5/10
11/11 - 1s - loss: 106.5076 - loglik: -1.0467e+02 - logprior: -1.8342e+00
Epoch 6/10
11/11 - 1s - loss: 104.8541 - loglik: -1.0302e+02 - logprior: -1.8315e+00
Epoch 7/10
11/11 - 1s - loss: 104.2721 - loglik: -1.0253e+02 - logprior: -1.7409e+00
Epoch 8/10
11/11 - 1s - loss: 103.5787 - loglik: -1.0187e+02 - logprior: -1.7129e+00
Epoch 9/10
11/11 - 1s - loss: 103.6067 - loglik: -1.0190e+02 - logprior: -1.7083e+00
Fitted a model with MAP estimate = -103.3395
expansions: [(0, 3), (15, 1), (27, 1), (28, 1), (30, 2), (31, 2), (32, 1), (33, 2), (34, 1), (35, 1), (37, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 114.9613 - loglik: -1.0135e+02 - logprior: -1.3616e+01
Epoch 2/2
11/11 - 1s - loss: 97.9331 - loglik: -9.3778e+01 - logprior: -4.1546e+00
Fitted a model with MAP estimate = -95.1929
expansions: []
discards: [ 0 36 39 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 107.6606 - loglik: -9.4555e+01 - logprior: -1.3105e+01
Epoch 2/2
11/11 - 1s - loss: 98.3163 - loglik: -9.2817e+01 - logprior: -5.4988e+00
Fitted a model with MAP estimate = -96.0202
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 103.7830 - loglik: -9.3014e+01 - logprior: -1.0769e+01
Epoch 2/10
11/11 - 1s - loss: 95.0839 - loglik: -9.2143e+01 - logprior: -2.9410e+00
Epoch 3/10
11/11 - 1s - loss: 93.6260 - loglik: -9.1857e+01 - logprior: -1.7686e+00
Epoch 4/10
11/11 - 1s - loss: 92.4244 - loglik: -9.0861e+01 - logprior: -1.5637e+00
Epoch 5/10
11/11 - 1s - loss: 92.2087 - loglik: -9.0704e+01 - logprior: -1.5048e+00
Epoch 6/10
11/11 - 1s - loss: 91.6565 - loglik: -9.0308e+01 - logprior: -1.3490e+00
Epoch 7/10
11/11 - 1s - loss: 91.6859 - loglik: -9.0443e+01 - logprior: -1.2427e+00
Fitted a model with MAP estimate = -91.4882
Time for alignment: 32.1992
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 204.7667 - loglik: -1.9332e+02 - logprior: -1.1451e+01
Epoch 2/10
11/11 - 1s - loss: 164.1922 - loglik: -1.6103e+02 - logprior: -3.1650e+00
Epoch 3/10
11/11 - 1s - loss: 129.3909 - loglik: -1.2709e+02 - logprior: -2.3031e+00
Epoch 4/10
11/11 - 1s - loss: 111.6575 - loglik: -1.0970e+02 - logprior: -1.9566e+00
Epoch 5/10
11/11 - 1s - loss: 106.8138 - loglik: -1.0503e+02 - logprior: -1.7836e+00
Epoch 6/10
11/11 - 1s - loss: 105.1105 - loglik: -1.0331e+02 - logprior: -1.8013e+00
Epoch 7/10
11/11 - 1s - loss: 104.4262 - loglik: -1.0271e+02 - logprior: -1.7135e+00
Epoch 8/10
11/11 - 1s - loss: 103.8802 - loglik: -1.0218e+02 - logprior: -1.6973e+00
Epoch 9/10
11/11 - 1s - loss: 103.4893 - loglik: -1.0178e+02 - logprior: -1.7060e+00
Epoch 10/10
11/11 - 1s - loss: 103.1105 - loglik: -1.0141e+02 - logprior: -1.6959e+00
Fitted a model with MAP estimate = -103.2419
expansions: [(0, 3), (15, 1), (27, 1), (28, 1), (30, 2), (31, 2), (32, 1), (33, 2), (34, 1), (35, 1), (37, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 115.2715 - loglik: -1.0162e+02 - logprior: -1.3654e+01
Epoch 2/2
11/11 - 1s - loss: 97.7151 - loglik: -9.3543e+01 - logprior: -4.1716e+00
Fitted a model with MAP estimate = -95.1879
expansions: []
discards: [ 0 36 39 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 107.7306 - loglik: -9.4623e+01 - logprior: -1.3108e+01
Epoch 2/2
11/11 - 1s - loss: 98.2407 - loglik: -9.2746e+01 - logprior: -5.4946e+00
Fitted a model with MAP estimate = -96.0440
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 103.6835 - loglik: -9.2904e+01 - logprior: -1.0779e+01
Epoch 2/10
11/11 - 1s - loss: 95.0592 - loglik: -9.2113e+01 - logprior: -2.9458e+00
Epoch 3/10
11/11 - 1s - loss: 93.5595 - loglik: -9.1797e+01 - logprior: -1.7620e+00
Epoch 4/10
11/11 - 1s - loss: 92.6364 - loglik: -9.1072e+01 - logprior: -1.5639e+00
Epoch 5/10
11/11 - 1s - loss: 91.9346 - loglik: -9.0438e+01 - logprior: -1.4968e+00
Epoch 6/10
11/11 - 1s - loss: 91.8372 - loglik: -9.0492e+01 - logprior: -1.3451e+00
Epoch 7/10
11/11 - 1s - loss: 92.1765 - loglik: -9.0939e+01 - logprior: -1.2379e+00
Fitted a model with MAP estimate = -91.4788
Time for alignment: 32.9816
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 204.6675 - loglik: -1.9322e+02 - logprior: -1.1451e+01
Epoch 2/10
11/11 - 1s - loss: 164.6467 - loglik: -1.6148e+02 - logprior: -3.1681e+00
Epoch 3/10
11/11 - 1s - loss: 132.7376 - loglik: -1.3043e+02 - logprior: -2.3041e+00
Epoch 4/10
11/11 - 1s - loss: 112.9410 - loglik: -1.1095e+02 - logprior: -1.9921e+00
Epoch 5/10
11/11 - 1s - loss: 106.4974 - loglik: -1.0469e+02 - logprior: -1.8066e+00
Epoch 6/10
11/11 - 1s - loss: 104.3790 - loglik: -1.0257e+02 - logprior: -1.8080e+00
Epoch 7/10
11/11 - 1s - loss: 103.9308 - loglik: -1.0222e+02 - logprior: -1.7100e+00
Epoch 8/10
11/11 - 1s - loss: 103.0505 - loglik: -1.0138e+02 - logprior: -1.6690e+00
Epoch 9/10
11/11 - 1s - loss: 102.7926 - loglik: -1.0113e+02 - logprior: -1.6591e+00
Epoch 10/10
11/11 - 1s - loss: 103.1364 - loglik: -1.0148e+02 - logprior: -1.6593e+00
Fitted a model with MAP estimate = -102.7572
expansions: [(0, 3), (1, 1), (14, 1), (28, 1), (29, 3), (30, 1), (31, 1), (32, 1), (35, 1), (37, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 113.2457 - loglik: -9.9622e+01 - logprior: -1.3624e+01
Epoch 2/2
11/11 - 1s - loss: 97.4003 - loglik: -9.3316e+01 - logprior: -4.0842e+00
Fitted a model with MAP estimate = -94.9751
expansions: []
discards: [ 0 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 107.5318 - loglik: -9.4433e+01 - logprior: -1.3099e+01
Epoch 2/2
11/11 - 1s - loss: 98.4151 - loglik: -9.2934e+01 - logprior: -5.4812e+00
Fitted a model with MAP estimate = -96.0223
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 103.5604 - loglik: -9.2788e+01 - logprior: -1.0772e+01
Epoch 2/10
11/11 - 1s - loss: 95.2978 - loglik: -9.2358e+01 - logprior: -2.9396e+00
Epoch 3/10
11/11 - 1s - loss: 93.3352 - loglik: -9.1573e+01 - logprior: -1.7619e+00
Epoch 4/10
11/11 - 1s - loss: 92.7021 - loglik: -9.1152e+01 - logprior: -1.5499e+00
Epoch 5/10
11/11 - 1s - loss: 92.1496 - loglik: -9.0659e+01 - logprior: -1.4907e+00
Epoch 6/10
11/11 - 1s - loss: 91.9459 - loglik: -9.0608e+01 - logprior: -1.3376e+00
Epoch 7/10
11/11 - 1s - loss: 91.6866 - loglik: -9.0455e+01 - logprior: -1.2312e+00
Epoch 8/10
11/11 - 1s - loss: 91.3006 - loglik: -9.0084e+01 - logprior: -1.2165e+00
Epoch 9/10
11/11 - 1s - loss: 91.3293 - loglik: -9.0114e+01 - logprior: -1.2157e+00
Fitted a model with MAP estimate = -91.1149
Time for alignment: 33.1475
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 204.7751 - loglik: -1.9332e+02 - logprior: -1.1450e+01
Epoch 2/10
11/11 - 1s - loss: 164.7658 - loglik: -1.6160e+02 - logprior: -3.1667e+00
Epoch 3/10
11/11 - 1s - loss: 130.3135 - loglik: -1.2801e+02 - logprior: -2.2990e+00
Epoch 4/10
11/11 - 1s - loss: 111.0221 - loglik: -1.0903e+02 - logprior: -1.9889e+00
Epoch 5/10
11/11 - 1s - loss: 106.6634 - loglik: -1.0483e+02 - logprior: -1.8286e+00
Epoch 6/10
11/11 - 1s - loss: 104.9809 - loglik: -1.0315e+02 - logprior: -1.8322e+00
Epoch 7/10
11/11 - 1s - loss: 104.0809 - loglik: -1.0234e+02 - logprior: -1.7449e+00
Epoch 8/10
11/11 - 1s - loss: 103.8798 - loglik: -1.0216e+02 - logprior: -1.7209e+00
Epoch 9/10
11/11 - 1s - loss: 103.4719 - loglik: -1.0177e+02 - logprior: -1.7043e+00
Epoch 10/10
11/11 - 1s - loss: 103.1556 - loglik: -1.0146e+02 - logprior: -1.6943e+00
Fitted a model with MAP estimate = -103.1888
expansions: [(0, 3), (15, 1), (27, 1), (28, 1), (30, 2), (31, 2), (32, 1), (33, 2), (34, 1), (35, 1), (37, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 115.1763 - loglik: -1.0154e+02 - logprior: -1.3635e+01
Epoch 2/2
11/11 - 1s - loss: 97.8076 - loglik: -9.3641e+01 - logprior: -4.1671e+00
Fitted a model with MAP estimate = -95.2041
expansions: []
discards: [ 0 36 39 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 108.0758 - loglik: -9.4964e+01 - logprior: -1.3111e+01
Epoch 2/2
11/11 - 1s - loss: 97.7765 - loglik: -9.2268e+01 - logprior: -5.5085e+00
Fitted a model with MAP estimate = -96.0616
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 103.6593 - loglik: -9.2866e+01 - logprior: -1.0793e+01
Epoch 2/10
11/11 - 1s - loss: 95.1949 - loglik: -9.2242e+01 - logprior: -2.9524e+00
Epoch 3/10
11/11 - 1s - loss: 93.4368 - loglik: -9.1673e+01 - logprior: -1.7641e+00
Epoch 4/10
11/11 - 1s - loss: 92.6500 - loglik: -9.1087e+01 - logprior: -1.5629e+00
Epoch 5/10
11/11 - 1s - loss: 92.1304 - loglik: -9.0632e+01 - logprior: -1.4980e+00
Epoch 6/10
11/11 - 1s - loss: 91.8622 - loglik: -9.0512e+01 - logprior: -1.3500e+00
Epoch 7/10
11/11 - 1s - loss: 91.6214 - loglik: -9.0380e+01 - logprior: -1.2418e+00
Epoch 8/10
11/11 - 1s - loss: 91.3346 - loglik: -9.0106e+01 - logprior: -1.2285e+00
Epoch 9/10
11/11 - 1s - loss: 91.5923 - loglik: -9.0365e+01 - logprior: -1.2276e+00
Fitted a model with MAP estimate = -91.1286
Time for alignment: 32.2545
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 204.5611 - loglik: -1.9311e+02 - logprior: -1.1452e+01
Epoch 2/10
11/11 - 1s - loss: 164.9206 - loglik: -1.6175e+02 - logprior: -3.1658e+00
Epoch 3/10
11/11 - 1s - loss: 129.8822 - loglik: -1.2760e+02 - logprior: -2.2856e+00
Epoch 4/10
11/11 - 1s - loss: 110.5622 - loglik: -1.0858e+02 - logprior: -1.9841e+00
Epoch 5/10
11/11 - 1s - loss: 106.4036 - loglik: -1.0458e+02 - logprior: -1.8221e+00
Epoch 6/10
11/11 - 1s - loss: 105.2641 - loglik: -1.0344e+02 - logprior: -1.8251e+00
Epoch 7/10
11/11 - 1s - loss: 104.0644 - loglik: -1.0232e+02 - logprior: -1.7395e+00
Epoch 8/10
11/11 - 1s - loss: 103.7553 - loglik: -1.0204e+02 - logprior: -1.7178e+00
Epoch 9/10
11/11 - 1s - loss: 103.5275 - loglik: -1.0182e+02 - logprior: -1.7035e+00
Epoch 10/10
11/11 - 1s - loss: 103.1837 - loglik: -1.0149e+02 - logprior: -1.6982e+00
Fitted a model with MAP estimate = -103.1874
expansions: [(0, 3), (15, 1), (27, 1), (28, 1), (30, 2), (31, 2), (32, 1), (33, 2), (34, 1), (35, 1), (37, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 115.4552 - loglik: -1.0181e+02 - logprior: -1.3646e+01
Epoch 2/2
11/11 - 1s - loss: 97.6161 - loglik: -9.3451e+01 - logprior: -4.1655e+00
Fitted a model with MAP estimate = -95.1910
expansions: []
discards: [ 0 36 39 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 107.6484 - loglik: -9.4548e+01 - logprior: -1.3100e+01
Epoch 2/2
11/11 - 1s - loss: 98.3267 - loglik: -9.2815e+01 - logprior: -5.5120e+00
Fitted a model with MAP estimate = -96.0432
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 103.8821 - loglik: -9.3091e+01 - logprior: -1.0791e+01
Epoch 2/10
11/11 - 1s - loss: 94.9711 - loglik: -9.2026e+01 - logprior: -2.9452e+00
Epoch 3/10
11/11 - 1s - loss: 93.0875 - loglik: -9.1324e+01 - logprior: -1.7637e+00
Epoch 4/10
11/11 - 1s - loss: 93.0359 - loglik: -9.1475e+01 - logprior: -1.5613e+00
Epoch 5/10
11/11 - 1s - loss: 92.0999 - loglik: -9.0601e+01 - logprior: -1.4987e+00
Epoch 6/10
11/11 - 1s - loss: 91.9489 - loglik: -9.0604e+01 - logprior: -1.3444e+00
Epoch 7/10
11/11 - 1s - loss: 91.5954 - loglik: -9.0361e+01 - logprior: -1.2343e+00
Epoch 8/10
11/11 - 1s - loss: 91.4330 - loglik: -9.0205e+01 - logprior: -1.2278e+00
Epoch 9/10
11/11 - 1s - loss: 91.0886 - loglik: -8.9864e+01 - logprior: -1.2250e+00
Epoch 10/10
11/11 - 1s - loss: 91.2329 - loglik: -9.0036e+01 - logprior: -1.1967e+00
Fitted a model with MAP estimate = -90.9608
Time for alignment: 33.3879
Computed alignments with likelihoods: ['-91.4882', '-91.4788', '-91.1149', '-91.1286', '-90.9608']
Best model has likelihood: -90.9608  (prior= -1.1738 )
time for generating output: 0.1008
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 5 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fcf08e490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ee4c38610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f07337d90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f7f9f43a0>
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 200.0738 - loglik: -1.9938e+02 - logprior: -6.8978e-01
Epoch 2/10
42/42 - 3s - loss: 103.9914 - loglik: -1.0332e+02 - logprior: -6.6691e-01
Epoch 3/10
42/42 - 3s - loss: 98.9432 - loglik: -9.8300e+01 - logprior: -6.4339e-01
Epoch 4/10
42/42 - 3s - loss: 95.1433 - loglik: -9.4520e+01 - logprior: -6.2356e-01
Epoch 5/10
42/42 - 3s - loss: 90.5642 - loglik: -8.9973e+01 - logprior: -5.9166e-01
Epoch 6/10
42/42 - 3s - loss: 85.0047 - loglik: -8.4428e+01 - logprior: -5.7699e-01
Epoch 7/10
42/42 - 3s - loss: 81.0579 - loglik: -8.0489e+01 - logprior: -5.6919e-01
Epoch 8/10
42/42 - 3s - loss: 79.3056 - loglik: -7.8743e+01 - logprior: -5.6257e-01
Epoch 9/10
42/42 - 3s - loss: 77.8189 - loglik: -7.7256e+01 - logprior: -5.6319e-01
Epoch 10/10
42/42 - 3s - loss: 77.5576 - loglik: -7.6994e+01 - logprior: -5.6363e-01
Fitted a model with MAP estimate = -78.7149
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (12, 1), (23, 1), (26, 1), (36, 1), (38, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (57, 1), (66, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 54.5526 - loglik: -5.3653e+01 - logprior: -8.9934e-01
Epoch 2/2
42/42 - 4s - loss: 40.9877 - loglik: -4.0355e+01 - logprior: -6.3266e-01
Fitted a model with MAP estimate = -38.5790
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 40.4092 - loglik: -3.9353e+01 - logprior: -1.0567e+00
Epoch 2/2
42/42 - 4s - loss: 37.6000 - loglik: -3.6841e+01 - logprior: -7.5870e-01
Fitted a model with MAP estimate = -37.4129
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 37.1338 - loglik: -3.6535e+01 - logprior: -5.9871e-01
Epoch 2/10
59/59 - 5s - loss: 36.3344 - loglik: -3.5829e+01 - logprior: -5.0542e-01
Epoch 3/10
59/59 - 5s - loss: 36.5172 - loglik: -3.6021e+01 - logprior: -4.9653e-01
Fitted a model with MAP estimate = -36.2278
Time for alignment: 148.0586
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 199.6998 - loglik: -1.9901e+02 - logprior: -6.8926e-01
Epoch 2/10
42/42 - 3s - loss: 104.7905 - loglik: -1.0413e+02 - logprior: -6.6525e-01
Epoch 3/10
42/42 - 3s - loss: 98.8702 - loglik: -9.8227e+01 - logprior: -6.4292e-01
Epoch 4/10
42/42 - 3s - loss: 95.6927 - loglik: -9.5072e+01 - logprior: -6.2099e-01
Epoch 5/10
42/42 - 3s - loss: 91.0452 - loglik: -9.0456e+01 - logprior: -5.8904e-01
Epoch 6/10
42/42 - 3s - loss: 85.7171 - loglik: -8.5142e+01 - logprior: -5.7534e-01
Epoch 7/10
42/42 - 3s - loss: 81.8129 - loglik: -8.1247e+01 - logprior: -5.6610e-01
Epoch 8/10
42/42 - 3s - loss: 79.5489 - loglik: -7.8985e+01 - logprior: -5.6373e-01
Epoch 9/10
42/42 - 3s - loss: 78.5167 - loglik: -7.7954e+01 - logprior: -5.6306e-01
Epoch 10/10
42/42 - 3s - loss: 78.1022 - loglik: -7.7540e+01 - logprior: -5.6195e-01
Fitted a model with MAP estimate = -79.6281
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (12, 1), (23, 1), (26, 1), (36, 1), (38, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (57, 1), (66, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 54.7905 - loglik: -5.3935e+01 - logprior: -8.5541e-01
Epoch 2/2
42/42 - 4s - loss: 40.9661 - loglik: -4.0336e+01 - logprior: -6.3015e-01
Fitted a model with MAP estimate = -38.4703
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 40.7330 - loglik: -3.9665e+01 - logprior: -1.0683e+00
Epoch 2/2
42/42 - 4s - loss: 37.5145 - loglik: -3.6755e+01 - logprior: -7.5919e-01
Fitted a model with MAP estimate = -37.3382
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 9s - loss: 36.8142 - loglik: -3.6218e+01 - logprior: -5.9603e-01
Epoch 2/10
59/59 - 5s - loss: 36.4926 - loglik: -3.5996e+01 - logprior: -4.9666e-01
Epoch 3/10
59/59 - 5s - loss: 36.3270 - loglik: -3.5842e+01 - logprior: -4.8463e-01
Epoch 4/10
59/59 - 5s - loss: 36.4813 - loglik: -3.5997e+01 - logprior: -4.8435e-01
Fitted a model with MAP estimate = -35.8471
Time for alignment: 152.2637
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 199.5429 - loglik: -1.9885e+02 - logprior: -6.9201e-01
Epoch 2/10
42/42 - 3s - loss: 103.5538 - loglik: -1.0289e+02 - logprior: -6.6713e-01
Epoch 3/10
42/42 - 3s - loss: 98.8682 - loglik: -9.8225e+01 - logprior: -6.4300e-01
Epoch 4/10
42/42 - 3s - loss: 94.9722 - loglik: -9.4348e+01 - logprior: -6.2371e-01
Epoch 5/10
42/42 - 3s - loss: 90.1580 - loglik: -8.9565e+01 - logprior: -5.9309e-01
Epoch 6/10
42/42 - 3s - loss: 85.0112 - loglik: -8.4434e+01 - logprior: -5.7725e-01
Epoch 7/10
42/42 - 3s - loss: 82.0297 - loglik: -8.1465e+01 - logprior: -5.6426e-01
Epoch 8/10
42/42 - 3s - loss: 78.8591 - loglik: -7.8294e+01 - logprior: -5.6468e-01
Epoch 9/10
42/42 - 3s - loss: 78.0916 - loglik: -7.7526e+01 - logprior: -5.6531e-01
Epoch 10/10
42/42 - 3s - loss: 77.6112 - loglik: -7.7046e+01 - logprior: -5.6567e-01
Fitted a model with MAP estimate = -78.8289
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (12, 1), (23, 1), (26, 1), (36, 1), (38, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (57, 1), (66, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 54.7230 - loglik: -5.3824e+01 - logprior: -8.9935e-01
Epoch 2/2
42/42 - 4s - loss: 41.0008 - loglik: -4.0366e+01 - logprior: -6.3428e-01
Fitted a model with MAP estimate = -38.7025
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 40.2331 - loglik: -3.9176e+01 - logprior: -1.0571e+00
Epoch 2/2
42/42 - 4s - loss: 37.7199 - loglik: -3.6959e+01 - logprior: -7.6047e-01
Fitted a model with MAP estimate = -37.4129
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 37.2520 - loglik: -3.6653e+01 - logprior: -5.9939e-01
Epoch 2/10
59/59 - 5s - loss: 36.2733 - loglik: -3.5768e+01 - logprior: -5.0516e-01
Epoch 3/10
59/59 - 5s - loss: 36.3654 - loglik: -3.5867e+01 - logprior: -4.9872e-01
Fitted a model with MAP estimate = -36.1773
Time for alignment: 144.9518
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 199.6717 - loglik: -1.9898e+02 - logprior: -6.9002e-01
Epoch 2/10
42/42 - 3s - loss: 104.0257 - loglik: -1.0336e+02 - logprior: -6.6669e-01
Epoch 3/10
42/42 - 3s - loss: 98.3622 - loglik: -9.7718e+01 - logprior: -6.4413e-01
Epoch 4/10
42/42 - 3s - loss: 95.2861 - loglik: -9.4664e+01 - logprior: -6.2248e-01
Epoch 5/10
42/42 - 3s - loss: 90.0150 - loglik: -8.9421e+01 - logprior: -5.9424e-01
Epoch 6/10
42/42 - 3s - loss: 85.4888 - loglik: -8.4914e+01 - logprior: -5.7517e-01
Epoch 7/10
42/42 - 3s - loss: 81.2636 - loglik: -8.0696e+01 - logprior: -5.6780e-01
Epoch 8/10
42/42 - 3s - loss: 79.0688 - loglik: -7.8505e+01 - logprior: -5.6341e-01
Epoch 9/10
42/42 - 3s - loss: 78.2974 - loglik: -7.7734e+01 - logprior: -5.6315e-01
Epoch 10/10
42/42 - 3s - loss: 77.4123 - loglik: -7.6847e+01 - logprior: -5.6556e-01
Fitted a model with MAP estimate = -78.8578
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (12, 1), (23, 1), (26, 1), (36, 1), (38, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (57, 1), (66, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 54.9746 - loglik: -5.4129e+01 - logprior: -8.4598e-01
Epoch 2/2
42/42 - 4s - loss: 40.7419 - loglik: -4.0113e+01 - logprior: -6.2874e-01
Fitted a model with MAP estimate = -38.6648
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 37.1041 - loglik: -3.6633e+01 - logprior: -4.7080e-01
Epoch 2/10
59/59 - 5s - loss: 36.1982 - loglik: -3.5790e+01 - logprior: -4.0850e-01
Epoch 3/10
59/59 - 5s - loss: 36.0403 - loglik: -3.5643e+01 - logprior: -3.9764e-01
Epoch 4/10
59/59 - 5s - loss: 35.6088 - loglik: -3.5216e+01 - logprior: -3.9263e-01
Epoch 5/10
59/59 - 5s - loss: 35.6366 - loglik: -3.5249e+01 - logprior: -3.8805e-01
Fitted a model with MAP estimate = -35.3405
Time for alignment: 122.4406
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 199.8944 - loglik: -1.9920e+02 - logprior: -6.9202e-01
Epoch 2/10
42/42 - 3s - loss: 103.5930 - loglik: -1.0293e+02 - logprior: -6.6745e-01
Epoch 3/10
42/42 - 3s - loss: 98.2693 - loglik: -9.7625e+01 - logprior: -6.4458e-01
Epoch 4/10
42/42 - 3s - loss: 95.1693 - loglik: -9.4546e+01 - logprior: -6.2322e-01
Epoch 5/10
42/42 - 3s - loss: 90.0840 - loglik: -8.9491e+01 - logprior: -5.9268e-01
Epoch 6/10
42/42 - 3s - loss: 85.3851 - loglik: -8.4809e+01 - logprior: -5.7594e-01
Epoch 7/10
42/42 - 3s - loss: 81.3018 - loglik: -8.0734e+01 - logprior: -5.6753e-01
Epoch 8/10
42/42 - 3s - loss: 78.9452 - loglik: -7.8381e+01 - logprior: -5.6437e-01
Epoch 9/10
42/42 - 3s - loss: 78.3528 - loglik: -7.7792e+01 - logprior: -5.6088e-01
Epoch 10/10
42/42 - 3s - loss: 77.8416 - loglik: -7.7277e+01 - logprior: -5.6448e-01
Fitted a model with MAP estimate = -78.9272
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (12, 1), (23, 1), (26, 1), (36, 1), (38, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (57, 1), (66, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 54.6667 - loglik: -5.3820e+01 - logprior: -8.4703e-01
Epoch 2/2
42/42 - 4s - loss: 41.3752 - loglik: -4.0748e+01 - logprior: -6.2719e-01
Fitted a model with MAP estimate = -38.8174
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 40.8429 - loglik: -3.9781e+01 - logprior: -1.0616e+00
Epoch 2/2
42/42 - 4s - loss: 37.6922 - loglik: -3.6941e+01 - logprior: -7.5123e-01
Fitted a model with MAP estimate = -37.4983
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 37.4876 - loglik: -3.6888e+01 - logprior: -5.9916e-01
Epoch 2/10
59/59 - 5s - loss: 36.1968 - loglik: -3.5687e+01 - logprior: -5.0965e-01
Epoch 3/10
59/59 - 5s - loss: 36.1432 - loglik: -3.5642e+01 - logprior: -5.0107e-01
Epoch 4/10
59/59 - 5s - loss: 36.1016 - loglik: -3.5606e+01 - logprior: -4.9549e-01
Epoch 5/10
59/59 - 5s - loss: 35.5807 - loglik: -3.5094e+01 - logprior: -4.8678e-01
Epoch 6/10
59/59 - 5s - loss: 35.8833 - loglik: -3.5403e+01 - logprior: -4.8040e-01
Fitted a model with MAP estimate = -35.1923
Time for alignment: 157.3929
Computed alignments with likelihoods: ['-36.2278', '-35.8471', '-36.1773', '-35.3405', '-35.1923']
Best model has likelihood: -35.1923  (prior= -0.4800 )
time for generating output: 0.1460
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.2050067658998647
Training of 5 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91beee20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200a87dc10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa320b040>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fc619d8b0>
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 478.4681 - loglik: -4.5521e+02 - logprior: -2.3263e+01
Epoch 2/10
14/14 - 3s - loss: 419.1352 - loglik: -4.1588e+02 - logprior: -3.2511e+00
Epoch 3/10
14/14 - 3s - loss: 376.2442 - loglik: -3.7490e+02 - logprior: -1.3481e+00
Epoch 4/10
14/14 - 3s - loss: 361.5266 - loglik: -3.6059e+02 - logprior: -9.3209e-01
Epoch 5/10
14/14 - 3s - loss: 357.1370 - loglik: -3.5632e+02 - logprior: -8.1453e-01
Epoch 6/10
14/14 - 3s - loss: 356.3504 - loglik: -3.5580e+02 - logprior: -5.4679e-01
Epoch 7/10
14/14 - 3s - loss: 355.4407 - loglik: -3.5500e+02 - logprior: -4.4264e-01
Epoch 8/10
14/14 - 3s - loss: 353.9559 - loglik: -3.5358e+02 - logprior: -3.7932e-01
Epoch 9/10
14/14 - 3s - loss: 353.8162 - loglik: -3.5344e+02 - logprior: -3.7218e-01
Epoch 10/10
14/14 - 3s - loss: 352.9538 - loglik: -3.5265e+02 - logprior: -3.0292e-01
Fitted a model with MAP estimate = -353.0693
expansions: [(12, 1), (13, 1), (16, 5), (17, 1), (36, 3), (37, 2), (42, 1), (67, 1), (68, 1), (81, 6), (84, 1), (101, 1), (110, 5)]
discards: [ 0  1 44 45 46 47]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 389.6666 - loglik: -3.6167e+02 - logprior: -2.8000e+01
Epoch 2/2
14/14 - 3s - loss: 361.0721 - loglik: -3.5125e+02 - logprior: -9.8219e+00
Fitted a model with MAP estimate = -356.7838
expansions: [(0, 27), (56, 3)]
discards: [ 0 10 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 174 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 371.6592 - loglik: -3.5052e+02 - logprior: -2.1144e+01
Epoch 2/2
14/14 - 4s - loss: 349.8556 - loglik: -3.4732e+02 - logprior: -2.5357e+00
Fitted a model with MAP estimate = -344.8196
expansions: [(156, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 370.2664 - loglik: -3.4516e+02 - logprior: -2.5102e+01
Epoch 2/10
14/14 - 3s - loss: 347.0131 - loglik: -3.4374e+02 - logprior: -3.2707e+00
Epoch 3/10
14/14 - 3s - loss: 342.4200 - loglik: -3.4303e+02 - logprior: 0.6072
Epoch 4/10
14/14 - 3s - loss: 340.4453 - loglik: -3.4229e+02 - logprior: 1.8484
Epoch 5/10
14/14 - 3s - loss: 339.1621 - loglik: -3.4158e+02 - logprior: 2.4173
Epoch 6/10
14/14 - 3s - loss: 338.8586 - loglik: -3.4165e+02 - logprior: 2.7962
Epoch 7/10
14/14 - 3s - loss: 337.6367 - loglik: -3.4071e+02 - logprior: 3.0727
Epoch 8/10
14/14 - 3s - loss: 336.9658 - loglik: -3.4025e+02 - logprior: 3.2844
Epoch 9/10
14/14 - 3s - loss: 337.9719 - loglik: -3.4147e+02 - logprior: 3.4934
Fitted a model with MAP estimate = -336.6501
Time for alignment: 93.9192
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 479.3009 - loglik: -4.5604e+02 - logprior: -2.3266e+01
Epoch 2/10
14/14 - 3s - loss: 417.9565 - loglik: -4.1470e+02 - logprior: -3.2598e+00
Epoch 3/10
14/14 - 3s - loss: 373.4009 - loglik: -3.7203e+02 - logprior: -1.3752e+00
Epoch 4/10
14/14 - 3s - loss: 361.4112 - loglik: -3.6051e+02 - logprior: -9.0067e-01
Epoch 5/10
14/14 - 3s - loss: 357.1913 - loglik: -3.5641e+02 - logprior: -7.7848e-01
Epoch 6/10
14/14 - 3s - loss: 355.9457 - loglik: -3.5545e+02 - logprior: -4.9386e-01
Epoch 7/10
14/14 - 3s - loss: 355.6763 - loglik: -3.5525e+02 - logprior: -4.2774e-01
Epoch 8/10
14/14 - 3s - loss: 352.6302 - loglik: -3.5235e+02 - logprior: -2.7975e-01
Epoch 9/10
14/14 - 3s - loss: 353.1193 - loglik: -3.5286e+02 - logprior: -2.6052e-01
Fitted a model with MAP estimate = -353.5065
expansions: [(12, 1), (16, 4), (17, 1), (36, 5), (67, 1), (68, 1), (81, 5), (82, 1), (85, 1), (100, 1), (102, 1), (110, 5)]
discards: [ 0  1 44 45 46 47]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 390.4962 - loglik: -3.6243e+02 - logprior: -2.8067e+01
Epoch 2/2
14/14 - 3s - loss: 361.8357 - loglik: -3.5194e+02 - logprior: -9.8911e+00
Fitted a model with MAP estimate = -357.9975
expansions: [(0, 28), (12, 1), (17, 1), (53, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 373.0565 - loglik: -3.5189e+02 - logprior: -2.1164e+01
Epoch 2/2
14/14 - 4s - loss: 347.8465 - loglik: -3.4547e+02 - logprior: -2.3740e+00
Fitted a model with MAP estimate = -344.4351
expansions: [(159, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 369.2535 - loglik: -3.4416e+02 - logprior: -2.5097e+01
Epoch 2/10
14/14 - 3s - loss: 347.8047 - loglik: -3.4443e+02 - logprior: -3.3724e+00
Epoch 3/10
14/14 - 3s - loss: 341.0222 - loglik: -3.4153e+02 - logprior: 0.5060
Epoch 4/10
14/14 - 3s - loss: 341.0576 - loglik: -3.4282e+02 - logprior: 1.7644
Fitted a model with MAP estimate = -339.0852
Time for alignment: 73.4295
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 478.4425 - loglik: -4.5517e+02 - logprior: -2.3268e+01
Epoch 2/10
14/14 - 3s - loss: 420.2672 - loglik: -4.1702e+02 - logprior: -3.2499e+00
Epoch 3/10
14/14 - 3s - loss: 379.3943 - loglik: -3.7816e+02 - logprior: -1.2355e+00
Epoch 4/10
14/14 - 3s - loss: 360.9378 - loglik: -3.6000e+02 - logprior: -9.4131e-01
Epoch 5/10
14/14 - 3s - loss: 358.6153 - loglik: -3.5763e+02 - logprior: -9.8525e-01
Epoch 6/10
14/14 - 3s - loss: 356.5458 - loglik: -3.5588e+02 - logprior: -6.6715e-01
Epoch 7/10
14/14 - 3s - loss: 355.1667 - loglik: -3.5463e+02 - logprior: -5.3561e-01
Epoch 8/10
14/14 - 3s - loss: 354.8331 - loglik: -3.5438e+02 - logprior: -4.5112e-01
Epoch 9/10
14/14 - 3s - loss: 355.0362 - loglik: -3.5467e+02 - logprior: -3.6797e-01
Fitted a model with MAP estimate = -354.4123
expansions: [(12, 1), (13, 1), (16, 5), (17, 1), (34, 1), (35, 3), (36, 1), (58, 2), (78, 1), (80, 5), (81, 1), (100, 1), (101, 1), (110, 5)]
discards: [ 0  1 44 45 46 47 48]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 392.8237 - loglik: -3.6479e+02 - logprior: -2.8038e+01
Epoch 2/2
14/14 - 3s - loss: 360.1558 - loglik: -3.5033e+02 - logprior: -9.8251e+00
Fitted a model with MAP estimate = -357.8532
expansions: [(0, 29), (55, 4), (128, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 374.0945 - loglik: -3.5287e+02 - logprior: -2.1229e+01
Epoch 2/2
14/14 - 4s - loss: 346.6451 - loglik: -3.4441e+02 - logprior: -2.2340e+00
Fitted a model with MAP estimate = -344.3792
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 38 39 40 89]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 373.3079 - loglik: -3.4835e+02 - logprior: -2.4959e+01
Epoch 2/10
14/14 - 3s - loss: 349.8850 - loglik: -3.4664e+02 - logprior: -3.2481e+00
Epoch 3/10
14/14 - 3s - loss: 345.0588 - loglik: -3.4556e+02 - logprior: 0.5004
Epoch 4/10
14/14 - 3s - loss: 343.1635 - loglik: -3.4487e+02 - logprior: 1.7081
Epoch 5/10
14/14 - 3s - loss: 342.0387 - loglik: -3.4426e+02 - logprior: 2.2176
Epoch 6/10
14/14 - 3s - loss: 341.3076 - loglik: -3.4377e+02 - logprior: 2.4628
Epoch 7/10
14/14 - 3s - loss: 338.8203 - loglik: -3.4149e+02 - logprior: 2.6711
Epoch 8/10
14/14 - 3s - loss: 341.2717 - loglik: -3.4419e+02 - logprior: 2.9167
Fitted a model with MAP estimate = -339.7329
Time for alignment: 86.1886
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 481.2698 - loglik: -4.5801e+02 - logprior: -2.3264e+01
Epoch 2/10
14/14 - 3s - loss: 417.8860 - loglik: -4.1463e+02 - logprior: -3.2576e+00
Epoch 3/10
14/14 - 3s - loss: 378.4021 - loglik: -3.7677e+02 - logprior: -1.6277e+00
Epoch 4/10
14/14 - 3s - loss: 362.8979 - loglik: -3.6135e+02 - logprior: -1.5457e+00
Epoch 5/10
14/14 - 3s - loss: 357.0797 - loglik: -3.5560e+02 - logprior: -1.4759e+00
Epoch 6/10
14/14 - 3s - loss: 357.9745 - loglik: -3.5679e+02 - logprior: -1.1850e+00
Fitted a model with MAP estimate = -355.0632
expansions: [(12, 1), (16, 3), (19, 1), (33, 1), (35, 1), (36, 3), (37, 2), (42, 1), (58, 1), (68, 1), (78, 1), (80, 1), (81, 3), (82, 1), (85, 1), (100, 1), (102, 1), (109, 2), (110, 5)]
discards: [ 0 44 45 46 47 48]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 149 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 389.2331 - loglik: -3.6114e+02 - logprior: -2.8097e+01
Epoch 2/2
14/14 - 3s - loss: 359.6682 - loglik: -3.4980e+02 - logprior: -9.8670e+00
Fitted a model with MAP estimate = -356.1082
expansions: [(0, 26), (56, 3)]
discards: [ 0 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 176 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 370.4135 - loglik: -3.4919e+02 - logprior: -2.1227e+01
Epoch 2/2
14/14 - 4s - loss: 349.9340 - loglik: -3.4727e+02 - logprior: -2.6597e+00
Fitted a model with MAP estimate = -345.0610
expansions: [(80, 1), (81, 1), (84, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 371.6014 - loglik: -3.4664e+02 - logprior: -2.4963e+01
Epoch 2/10
14/14 - 3s - loss: 343.8272 - loglik: -3.4065e+02 - logprior: -3.1777e+00
Epoch 3/10
14/14 - 3s - loss: 340.8672 - loglik: -3.4145e+02 - logprior: 0.5865
Epoch 4/10
14/14 - 3s - loss: 339.3729 - loglik: -3.4113e+02 - logprior: 1.7598
Epoch 5/10
14/14 - 3s - loss: 337.3472 - loglik: -3.3962e+02 - logprior: 2.2743
Epoch 6/10
14/14 - 3s - loss: 338.2452 - loglik: -3.4080e+02 - logprior: 2.5538
Fitted a model with MAP estimate = -336.7198
Time for alignment: 72.3815
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 477.9637 - loglik: -4.5469e+02 - logprior: -2.3270e+01
Epoch 2/10
14/14 - 3s - loss: 420.4111 - loglik: -4.1714e+02 - logprior: -3.2733e+00
Epoch 3/10
14/14 - 3s - loss: 376.0935 - loglik: -3.7467e+02 - logprior: -1.4244e+00
Epoch 4/10
14/14 - 3s - loss: 362.1099 - loglik: -3.6093e+02 - logprior: -1.1805e+00
Epoch 5/10
14/14 - 3s - loss: 358.2007 - loglik: -3.5697e+02 - logprior: -1.2267e+00
Epoch 6/10
14/14 - 3s - loss: 355.4421 - loglik: -3.5445e+02 - logprior: -9.9053e-01
Epoch 7/10
14/14 - 3s - loss: 353.7525 - loglik: -3.5291e+02 - logprior: -8.3759e-01
Epoch 8/10
14/14 - 3s - loss: 353.6313 - loglik: -3.5290e+02 - logprior: -7.3501e-01
Epoch 9/10
14/14 - 3s - loss: 354.6382 - loglik: -3.5397e+02 - logprior: -6.6896e-01
Fitted a model with MAP estimate = -353.5348
expansions: [(12, 1), (16, 4), (32, 1), (33, 1), (34, 1), (35, 3), (36, 1), (67, 1), (68, 1), (78, 1), (80, 5), (81, 1), (84, 1), (102, 1), (109, 2), (110, 4)]
discards: [ 0  1 44 45 46 47 48]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 391.6217 - loglik: -3.6359e+02 - logprior: -2.8030e+01
Epoch 2/2
14/14 - 3s - loss: 362.0911 - loglik: -3.5233e+02 - logprior: -9.7648e+00
Fitted a model with MAP estimate = -358.2287
expansions: [(0, 29), (54, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 373.2639 - loglik: -3.5202e+02 - logprior: -2.1241e+01
Epoch 2/2
14/14 - 4s - loss: 348.0595 - loglik: -3.4582e+02 - logprior: -2.2368e+00
Fitted a model with MAP estimate = -345.1460
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  88  89 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 373.9452 - loglik: -3.4891e+02 - logprior: -2.5036e+01
Epoch 2/10
14/14 - 3s - loss: 351.4815 - loglik: -3.4825e+02 - logprior: -3.2266e+00
Epoch 3/10
14/14 - 3s - loss: 346.3881 - loglik: -3.4700e+02 - logprior: 0.6147
Epoch 4/10
14/14 - 3s - loss: 342.6355 - loglik: -3.4450e+02 - logprior: 1.8617
Epoch 5/10
14/14 - 3s - loss: 343.1173 - loglik: -3.4555e+02 - logprior: 2.4356
Fitted a model with MAP estimate = -342.3155
Time for alignment: 77.5095
Computed alignments with likelihoods: ['-336.6501', '-339.0852', '-339.7329', '-336.7198', '-342.3155']
Best model has likelihood: -336.6501  (prior= 3.5665 )
time for generating output: 0.1889
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.9709962168978562
Training of 5 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200a6ed4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fb4dc0c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f92b80e50>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f078ce790>
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 872.1687 - loglik: -8.6944e+02 - logprior: -2.7307e+00
Epoch 2/10
29/29 - 19s - loss: 673.9942 - loglik: -6.7221e+02 - logprior: -1.7882e+00
Epoch 3/10
29/29 - 19s - loss: 640.1816 - loglik: -6.3808e+02 - logprior: -2.1014e+00
Epoch 4/10
29/29 - 19s - loss: 638.6885 - loglik: -6.3671e+02 - logprior: -1.9810e+00
Epoch 5/10
29/29 - 19s - loss: 633.8580 - loglik: -6.3192e+02 - logprior: -1.9381e+00
Epoch 6/10
29/29 - 19s - loss: 632.4724 - loglik: -6.3051e+02 - logprior: -1.9609e+00
Epoch 7/10
29/29 - 18s - loss: 633.6060 - loglik: -6.3166e+02 - logprior: -1.9422e+00
Fitted a model with MAP estimate = -632.3237
expansions: [(16, 1), (18, 2), (23, 1), (26, 1), (27, 1), (28, 1), (29, 1), (37, 1), (48, 1), (50, 1), (51, 1), (52, 1), (66, 2), (80, 1), (88, 1), (89, 1), (97, 2), (121, 2), (122, 1), (123, 1), (124, 1), (138, 1), (141, 1), (146, 1), (151, 1), (155, 1), (157, 1), (163, 1), (165, 1), (183, 1), (184, 1), (185, 1), (192, 1), (201, 1), (216, 1), (217, 1), (219, 2), (220, 1), (230, 1), (232, 1), (233, 1), (249, 2), (251, 1), (258, 1), (260, 2), (261, 1), (262, 1), (263, 1), (265, 1), (267, 3), (269, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 617.0593 - loglik: -6.1224e+02 - logprior: -4.8198e+00
Epoch 2/2
29/29 - 25s - loss: 596.6599 - loglik: -5.9485e+02 - logprior: -1.8060e+00
Fitted a model with MAP estimate = -592.6401
expansions: [(0, 2), (203, 1)]
discards: [  0  78 258 294 324]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 597.2548 - loglik: -5.9475e+02 - logprior: -2.5044e+00
Epoch 2/2
29/29 - 25s - loss: 591.5755 - loglik: -5.9124e+02 - logprior: -3.3533e-01
Fitted a model with MAP estimate = -587.6831
expansions: [(137, 1), (203, 1), (323, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 595.1701 - loglik: -5.9092e+02 - logprior: -4.2492e+00
Epoch 2/10
29/29 - 25s - loss: 588.4565 - loglik: -5.8802e+02 - logprior: -4.3569e-01
Epoch 3/10
29/29 - 25s - loss: 587.2843 - loglik: -5.8765e+02 - logprior: 0.3610
Epoch 4/10
29/29 - 25s - loss: 585.3516 - loglik: -5.8570e+02 - logprior: 0.3439
Epoch 5/10
29/29 - 25s - loss: 582.0739 - loglik: -5.8301e+02 - logprior: 0.9362
Epoch 6/10
29/29 - 25s - loss: 584.8021 - loglik: -5.8546e+02 - logprior: 0.6626
Fitted a model with MAP estimate = -583.0518
Time for alignment: 496.6759
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 871.9458 - loglik: -8.6922e+02 - logprior: -2.7235e+00
Epoch 2/10
29/29 - 19s - loss: 674.2803 - loglik: -6.7258e+02 - logprior: -1.6964e+00
Epoch 3/10
29/29 - 19s - loss: 643.5897 - loglik: -6.4166e+02 - logprior: -1.9301e+00
Epoch 4/10
29/29 - 19s - loss: 637.2787 - loglik: -6.3541e+02 - logprior: -1.8695e+00
Epoch 5/10
29/29 - 19s - loss: 638.7415 - loglik: -6.3692e+02 - logprior: -1.8177e+00
Fitted a model with MAP estimate = -635.8276
expansions: [(16, 1), (18, 2), (23, 1), (26, 1), (27, 1), (28, 1), (29, 1), (39, 1), (49, 2), (50, 1), (53, 1), (65, 4), (66, 1), (87, 1), (88, 1), (89, 1), (120, 3), (122, 1), (123, 1), (127, 1), (141, 1), (151, 1), (154, 1), (157, 1), (164, 1), (165, 2), (170, 1), (183, 1), (184, 1), (185, 1), (192, 1), (201, 1), (216, 1), (218, 1), (219, 2), (220, 1), (230, 1), (232, 1), (233, 2), (249, 1), (251, 2), (261, 4), (262, 1), (263, 2), (264, 1), (266, 3), (269, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 342 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 620.7690 - loglik: -6.1597e+02 - logprior: -4.7942e+00
Epoch 2/2
29/29 - 26s - loss: 595.1990 - loglik: -5.9329e+02 - logprior: -1.9041e+00
Fitted a model with MAP estimate = -593.0581
expansions: [(0, 2)]
discards: [  0  58  78  79  80 196 260 279 327 332]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 601.7596 - loglik: -5.9901e+02 - logprior: -2.7542e+00
Epoch 2/2
29/29 - 25s - loss: 595.0827 - loglik: -5.9496e+02 - logprior: -1.2545e-01
Fitted a model with MAP estimate = -593.2408
expansions: [(133, 9), (134, 1), (321, 2)]
discards: [  0 294]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 344 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 601.4974 - loglik: -5.9738e+02 - logprior: -4.1165e+00
Epoch 2/10
29/29 - 26s - loss: 592.0176 - loglik: -5.9114e+02 - logprior: -8.7554e-01
Epoch 3/10
29/29 - 26s - loss: 589.3859 - loglik: -5.8972e+02 - logprior: 0.3341
Epoch 4/10
29/29 - 26s - loss: 586.7636 - loglik: -5.8742e+02 - logprior: 0.6535
Epoch 5/10
29/29 - 26s - loss: 586.4833 - loglik: -5.8733e+02 - logprior: 0.8456
Epoch 6/10
29/29 - 26s - loss: 585.7348 - loglik: -5.8676e+02 - logprior: 1.0283
Epoch 7/10
29/29 - 26s - loss: 585.4252 - loglik: -5.8659e+02 - logprior: 1.1635
Epoch 8/10
29/29 - 26s - loss: 584.3735 - loglik: -5.8566e+02 - logprior: 1.2910
Epoch 9/10
29/29 - 26s - loss: 582.9604 - loglik: -5.8437e+02 - logprior: 1.4077
Epoch 10/10
29/29 - 26s - loss: 582.9210 - loglik: -5.8446e+02 - logprior: 1.5392
Fitted a model with MAP estimate = -582.4943
Time for alignment: 567.3591
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 23s - loss: 870.3695 - loglik: -8.6764e+02 - logprior: -2.7277e+00
Epoch 2/10
29/29 - 19s - loss: 674.6759 - loglik: -6.7298e+02 - logprior: -1.7008e+00
Epoch 3/10
29/29 - 19s - loss: 644.1392 - loglik: -6.4221e+02 - logprior: -1.9266e+00
Epoch 4/10
29/29 - 19s - loss: 638.4096 - loglik: -6.3656e+02 - logprior: -1.8450e+00
Epoch 5/10
29/29 - 19s - loss: 634.6406 - loglik: -6.3283e+02 - logprior: -1.8150e+00
Epoch 6/10
29/29 - 19s - loss: 635.9334 - loglik: -6.3411e+02 - logprior: -1.8220e+00
Fitted a model with MAP estimate = -633.8552
expansions: [(16, 1), (17, 1), (18, 1), (23, 2), (27, 1), (28, 1), (29, 1), (36, 1), (48, 1), (50, 2), (52, 1), (67, 1), (77, 1), (88, 2), (89, 2), (113, 1), (116, 10), (120, 2), (121, 1), (122, 1), (123, 1), (141, 1), (151, 1), (154, 1), (156, 1), (157, 1), (164, 1), (170, 3), (182, 2), (183, 1), (185, 1), (191, 1), (216, 1), (218, 1), (219, 2), (220, 1), (230, 1), (232, 1), (233, 2), (249, 1), (251, 2), (261, 4), (262, 1), (263, 2), (264, 1), (266, 3), (269, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 351 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 615.0710 - loglik: -6.1010e+02 - logprior: -4.9759e+00
Epoch 2/2
29/29 - 26s - loss: 587.7983 - loglik: -5.8558e+02 - logprior: -2.2142e+00
Fitted a model with MAP estimate = -583.8437
expansions: [(0, 2)]
discards: [  0  26  59 103 104 131 142 143 144 145 146 226 269 288 336 341]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 596.4299 - loglik: -5.9364e+02 - logprior: -2.7943e+00
Epoch 2/2
29/29 - 25s - loss: 589.5961 - loglik: -5.8940e+02 - logprior: -1.9679e-01
Fitted a model with MAP estimate = -587.4853
expansions: []
discards: [  0 297]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 596.5665 - loglik: -5.9232e+02 - logprior: -4.2475e+00
Epoch 2/10
29/29 - 25s - loss: 591.5599 - loglik: -5.9103e+02 - logprior: -5.3182e-01
Epoch 3/10
29/29 - 25s - loss: 587.5760 - loglik: -5.8820e+02 - logprior: 0.6199
Epoch 4/10
29/29 - 25s - loss: 586.9394 - loglik: -5.8742e+02 - logprior: 0.4759
Epoch 5/10
29/29 - 25s - loss: 586.7893 - loglik: -5.8768e+02 - logprior: 0.8867
Epoch 6/10
29/29 - 25s - loss: 584.0263 - loglik: -5.8466e+02 - logprior: 0.6373
Epoch 7/10
29/29 - 25s - loss: 585.7676 - loglik: -5.8694e+02 - logprior: 1.1703
Fitted a model with MAP estimate = -583.8930
Time for alignment: 506.0412
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 21s - loss: 868.6838 - loglik: -8.6595e+02 - logprior: -2.7296e+00
Epoch 2/10
29/29 - 19s - loss: 674.2935 - loglik: -6.7251e+02 - logprior: -1.7833e+00
Epoch 3/10
29/29 - 19s - loss: 642.3465 - loglik: -6.4032e+02 - logprior: -2.0313e+00
Epoch 4/10
29/29 - 19s - loss: 638.3132 - loglik: -6.3639e+02 - logprior: -1.9270e+00
Epoch 5/10
29/29 - 19s - loss: 635.2119 - loglik: -6.3332e+02 - logprior: -1.8963e+00
Epoch 6/10
29/29 - 19s - loss: 634.6598 - loglik: -6.3271e+02 - logprior: -1.9528e+00
Epoch 7/10
29/29 - 19s - loss: 633.5311 - loglik: -6.3159e+02 - logprior: -1.9413e+00
Epoch 8/10
29/29 - 19s - loss: 632.5562 - loglik: -6.3059e+02 - logprior: -1.9679e+00
Epoch 9/10
29/29 - 19s - loss: 631.3843 - loglik: -6.2937e+02 - logprior: -2.0142e+00
Epoch 10/10
29/29 - 19s - loss: 631.5134 - loglik: -6.2949e+02 - logprior: -2.0231e+00
Fitted a model with MAP estimate = -631.0532
expansions: [(16, 1), (22, 1), (24, 2), (26, 1), (28, 1), (29, 1), (30, 1), (37, 1), (39, 1), (49, 2), (50, 1), (53, 1), (67, 1), (86, 2), (88, 1), (89, 1), (96, 1), (120, 2), (122, 2), (123, 2), (141, 1), (154, 1), (156, 1), (157, 1), (158, 1), (163, 1), (164, 1), (171, 2), (183, 1), (184, 1), (185, 1), (192, 1), (201, 1), (216, 1), (218, 1), (219, 2), (220, 1), (230, 1), (232, 1), (233, 1), (249, 2), (251, 1), (258, 1), (260, 2), (261, 1), (262, 1), (263, 1), (265, 1), (267, 3), (269, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 615.9037 - loglik: -6.1109e+02 - logprior: -4.8177e+00
Epoch 2/2
29/29 - 25s - loss: 593.8221 - loglik: -5.9176e+02 - logprior: -2.0617e+00
Fitted a model with MAP estimate = -589.2566
expansions: [(0, 2)]
discards: [  0  30  59 146 260 296 326]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 594.9374 - loglik: -5.9228e+02 - logprior: -2.6593e+00
Epoch 2/2
29/29 - 25s - loss: 590.3926 - loglik: -5.9010e+02 - logprior: -2.8776e-01
Fitted a model with MAP estimate = -587.5253
expansions: [(111, 2), (322, 2)]
discards: [  0 325]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 597.3304 - loglik: -5.9286e+02 - logprior: -4.4733e+00
Epoch 2/10
29/29 - 25s - loss: 588.5406 - loglik: -5.8764e+02 - logprior: -8.9996e-01
Epoch 3/10
29/29 - 25s - loss: 586.6847 - loglik: -5.8718e+02 - logprior: 0.4969
Epoch 4/10
29/29 - 25s - loss: 585.1777 - loglik: -5.8596e+02 - logprior: 0.7842
Epoch 5/10
29/29 - 25s - loss: 584.5809 - loglik: -5.8556e+02 - logprior: 0.9818
Epoch 6/10
29/29 - 25s - loss: 583.2684 - loglik: -5.8382e+02 - logprior: 0.5488
Epoch 7/10
29/29 - 25s - loss: 582.9297 - loglik: -5.8355e+02 - logprior: 0.6221
Epoch 8/10
29/29 - 25s - loss: 581.6498 - loglik: -5.8293e+02 - logprior: 1.2785
Epoch 9/10
29/29 - 25s - loss: 582.8082 - loglik: -5.8382e+02 - logprior: 1.0146
Fitted a model with MAP estimate = -581.3019
Time for alignment: 627.5256
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 21s - loss: 870.2720 - loglik: -8.6754e+02 - logprior: -2.7295e+00
Epoch 2/10
29/29 - 19s - loss: 671.2106 - loglik: -6.6959e+02 - logprior: -1.6179e+00
Epoch 3/10
29/29 - 19s - loss: 643.9660 - loglik: -6.4210e+02 - logprior: -1.8662e+00
Epoch 4/10
29/29 - 19s - loss: 638.9765 - loglik: -6.3718e+02 - logprior: -1.7927e+00
Epoch 5/10
29/29 - 19s - loss: 635.5114 - loglik: -6.3375e+02 - logprior: -1.7656e+00
Epoch 6/10
29/29 - 19s - loss: 634.9792 - loglik: -6.3321e+02 - logprior: -1.7670e+00
Epoch 7/10
29/29 - 19s - loss: 633.9874 - loglik: -6.3220e+02 - logprior: -1.7830e+00
Epoch 8/10
29/29 - 19s - loss: 633.9324 - loglik: -6.3214e+02 - logprior: -1.7949e+00
Epoch 9/10
29/29 - 19s - loss: 633.3733 - loglik: -6.3156e+02 - logprior: -1.8088e+00
Epoch 10/10
29/29 - 19s - loss: 632.3039 - loglik: -6.3049e+02 - logprior: -1.8182e+00
Fitted a model with MAP estimate = -632.0231
expansions: [(16, 1), (18, 2), (23, 1), (26, 1), (27, 1), (28, 1), (29, 1), (37, 1), (48, 1), (49, 1), (50, 1), (51, 1), (65, 4), (89, 2), (90, 3), (121, 2), (122, 1), (123, 2), (124, 2), (152, 1), (156, 1), (162, 1), (164, 1), (165, 1), (166, 2), (183, 1), (184, 1), (185, 1), (192, 1), (201, 1), (216, 1), (218, 1), (219, 2), (220, 1), (230, 1), (234, 1), (248, 1), (249, 2), (251, 1), (261, 4), (262, 1), (263, 1), (265, 1), (267, 3), (269, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 342 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 615.1324 - loglik: -6.1038e+02 - logprior: -4.7559e+00
Epoch 2/2
29/29 - 25s - loss: 593.7850 - loglik: -5.9164e+02 - logprior: -2.1474e+00
Fitted a model with MAP estimate = -589.9835
expansions: [(0, 2)]
discards: [  0  77  78  79  80 106 109 150 199 261 297 327]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 332 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 27s - loss: 599.7468 - loglik: -5.9690e+02 - logprior: -2.8479e+00
Epoch 2/2
29/29 - 24s - loss: 593.0735 - loglik: -5.9274e+02 - logprior: -3.3645e-01
Fitted a model with MAP estimate = -591.3699
expansions: [(78, 4), (318, 2)]
discards: [  0 321]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 600.6049 - loglik: -5.9641e+02 - logprior: -4.1915e+00
Epoch 2/10
29/29 - 25s - loss: 590.5670 - loglik: -5.9007e+02 - logprior: -4.9420e-01
Epoch 3/10
29/29 - 25s - loss: 588.9069 - loglik: -5.8963e+02 - logprior: 0.7215
Epoch 4/10
29/29 - 25s - loss: 586.9684 - loglik: -5.8784e+02 - logprior: 0.8683
Epoch 5/10
29/29 - 25s - loss: 588.2523 - loglik: -5.8916e+02 - logprior: 0.9040
Fitted a model with MAP estimate = -586.8536
Time for alignment: 525.0435
Computed alignments with likelihoods: ['-583.0518', '-582.4943', '-583.8437', '-581.3019', '-586.8536']
Best model has likelihood: -581.3019  (prior= 0.9059 )
time for generating output: 0.3454
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.8995658465991317
Training of 5 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1efe754910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2023ccd220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f06ed5940>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fbd7b3ee0>
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 619.8027 - loglik: -1.3137e+02 - logprior: -4.8843e+02
Epoch 2/10
10/10 - 0s - loss: 245.6510 - loglik: -1.1131e+02 - logprior: -1.3435e+02
Epoch 3/10
10/10 - 0s - loss: 152.6914 - loglik: -9.1422e+01 - logprior: -6.1269e+01
Epoch 4/10
10/10 - 0s - loss: 111.5339 - loglik: -7.8168e+01 - logprior: -3.3366e+01
Epoch 5/10
10/10 - 0s - loss: 92.0293 - loglik: -7.3782e+01 - logprior: -1.8247e+01
Epoch 6/10
10/10 - 0s - loss: 80.8598 - loglik: -7.1878e+01 - logprior: -8.9820e+00
Epoch 7/10
10/10 - 0s - loss: 74.3369 - loglik: -7.1256e+01 - logprior: -3.0807e+00
Epoch 8/10
10/10 - 0s - loss: 70.4278 - loglik: -7.1187e+01 - logprior: 0.7588
Epoch 9/10
10/10 - 0s - loss: 67.7774 - loglik: -7.1051e+01 - logprior: 3.2740
Epoch 10/10
10/10 - 0s - loss: 65.7769 - loglik: -7.0922e+01 - logprior: 5.1451
Fitted a model with MAP estimate = -64.8537
expansions: [(0, 5), (17, 3), (26, 3), (29, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 725.2106 - loglik: -7.9126e+01 - logprior: -6.4609e+02
Epoch 2/2
10/10 - 0s - loss: 261.6566 - loglik: -6.0279e+01 - logprior: -2.0138e+02
Fitted a model with MAP estimate = -176.5936
expansions: []
discards: [ 0 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 609.2760 - loglik: -6.1142e+01 - logprior: -5.4813e+02
Epoch 2/2
10/10 - 0s - loss: 271.1738 - loglik: -5.7282e+01 - logprior: -2.1389e+02
Fitted a model with MAP estimate = -213.7884
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 551.0931 - loglik: -5.6191e+01 - logprior: -4.9490e+02
Epoch 2/10
10/10 - 0s - loss: 188.2706 - loglik: -5.3150e+01 - logprior: -1.3512e+02
Epoch 3/10
10/10 - 0s - loss: 103.4711 - loglik: -5.1913e+01 - logprior: -5.1558e+01
Epoch 4/10
10/10 - 0s - loss: 71.8100 - loglik: -5.1952e+01 - logprior: -1.9858e+01
Epoch 5/10
10/10 - 0s - loss: 55.3589 - loglik: -5.2212e+01 - logprior: -3.1471e+00
Epoch 6/10
10/10 - 0s - loss: 45.8569 - loglik: -5.2375e+01 - logprior: 6.5182
Epoch 7/10
10/10 - 0s - loss: 39.9849 - loglik: -5.2570e+01 - logprior: 12.5849
Epoch 8/10
10/10 - 0s - loss: 35.9913 - loglik: -5.2720e+01 - logprior: 16.7288
Epoch 9/10
10/10 - 0s - loss: 33.0135 - loglik: -5.2832e+01 - logprior: 19.8181
Epoch 10/10
10/10 - 0s - loss: 30.6286 - loglik: -5.2933e+01 - logprior: 22.3040
Fitted a model with MAP estimate = -29.4704
Time for alignment: 26.3257
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 619.8027 - loglik: -1.3137e+02 - logprior: -4.8843e+02
Epoch 2/10
10/10 - 0s - loss: 245.6505 - loglik: -1.1130e+02 - logprior: -1.3435e+02
Epoch 3/10
10/10 - 0s - loss: 152.6906 - loglik: -9.1421e+01 - logprior: -6.1269e+01
Epoch 4/10
10/10 - 0s - loss: 111.5337 - loglik: -7.8167e+01 - logprior: -3.3366e+01
Epoch 5/10
10/10 - 0s - loss: 92.0289 - loglik: -7.3782e+01 - logprior: -1.8247e+01
Epoch 6/10
10/10 - 0s - loss: 80.8593 - loglik: -7.1877e+01 - logprior: -8.9823e+00
Epoch 7/10
10/10 - 0s - loss: 74.3367 - loglik: -7.1256e+01 - logprior: -3.0807e+00
Epoch 8/10
10/10 - 0s - loss: 70.4277 - loglik: -7.1187e+01 - logprior: 0.7589
Epoch 9/10
10/10 - 0s - loss: 67.7774 - loglik: -7.1051e+01 - logprior: 3.2740
Epoch 10/10
10/10 - 0s - loss: 65.7771 - loglik: -7.0922e+01 - logprior: 5.1451
Fitted a model with MAP estimate = -64.8541
expansions: [(0, 5), (17, 3), (26, 3), (29, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 725.2115 - loglik: -7.9127e+01 - logprior: -6.4608e+02
Epoch 2/2
10/10 - 0s - loss: 261.6565 - loglik: -6.0279e+01 - logprior: -2.0138e+02
Fitted a model with MAP estimate = -176.5931
expansions: []
discards: [ 0 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 609.2768 - loglik: -6.1143e+01 - logprior: -5.4813e+02
Epoch 2/2
10/10 - 0s - loss: 271.1747 - loglik: -5.7283e+01 - logprior: -2.1389e+02
Fitted a model with MAP estimate = -213.7893
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 551.0942 - loglik: -5.6192e+01 - logprior: -4.9490e+02
Epoch 2/10
10/10 - 0s - loss: 188.2696 - loglik: -5.3149e+01 - logprior: -1.3512e+02
Epoch 3/10
10/10 - 0s - loss: 103.4708 - loglik: -5.1913e+01 - logprior: -5.1558e+01
Epoch 4/10
10/10 - 0s - loss: 71.8107 - loglik: -5.1951e+01 - logprior: -1.9860e+01
Epoch 5/10
10/10 - 0s - loss: 55.3598 - loglik: -5.2212e+01 - logprior: -3.1473e+00
Epoch 6/10
10/10 - 0s - loss: 45.8584 - loglik: -5.2374e+01 - logprior: 6.5156
Epoch 7/10
10/10 - 0s - loss: 39.9870 - loglik: -5.2570e+01 - logprior: 12.5830
Epoch 8/10
10/10 - 0s - loss: 35.9932 - loglik: -5.2719e+01 - logprior: 16.7262
Epoch 9/10
10/10 - 0s - loss: 33.0164 - loglik: -5.2832e+01 - logprior: 19.8154
Epoch 10/10
10/10 - 0s - loss: 30.6321 - loglik: -5.2933e+01 - logprior: 22.3009
Fitted a model with MAP estimate = -29.4738
Time for alignment: 25.0738
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 619.8027 - loglik: -1.3137e+02 - logprior: -4.8843e+02
Epoch 2/10
10/10 - 0s - loss: 245.6511 - loglik: -1.1131e+02 - logprior: -1.3435e+02
Epoch 3/10
10/10 - 0s - loss: 152.6917 - loglik: -9.1422e+01 - logprior: -6.1269e+01
Epoch 4/10
10/10 - 0s - loss: 111.5340 - loglik: -7.8168e+01 - logprior: -3.3366e+01
Epoch 5/10
10/10 - 0s - loss: 92.0297 - loglik: -7.3783e+01 - logprior: -1.8247e+01
Epoch 6/10
10/10 - 0s - loss: 80.8601 - loglik: -7.1878e+01 - logprior: -8.9820e+00
Epoch 7/10
10/10 - 0s - loss: 74.3370 - loglik: -7.1256e+01 - logprior: -3.0808e+00
Epoch 8/10
10/10 - 0s - loss: 70.4280 - loglik: -7.1187e+01 - logprior: 0.7589
Epoch 9/10
10/10 - 0s - loss: 67.7777 - loglik: -7.1052e+01 - logprior: 3.2740
Epoch 10/10
10/10 - 0s - loss: 65.7773 - loglik: -7.0922e+01 - logprior: 5.1451
Fitted a model with MAP estimate = -64.8544
expansions: [(0, 5), (17, 3), (26, 3), (29, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 725.2103 - loglik: -7.9125e+01 - logprior: -6.4608e+02
Epoch 2/2
10/10 - 0s - loss: 261.6563 - loglik: -6.0279e+01 - logprior: -2.0138e+02
Fitted a model with MAP estimate = -176.5925
expansions: []
discards: [ 0 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 609.2773 - loglik: -6.1144e+01 - logprior: -5.4813e+02
Epoch 2/2
10/10 - 0s - loss: 271.1756 - loglik: -5.7284e+01 - logprior: -2.1389e+02
Fitted a model with MAP estimate = -213.7907
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 551.0953 - loglik: -5.6193e+01 - logprior: -4.9490e+02
Epoch 2/10
10/10 - 0s - loss: 188.2715 - loglik: -5.3151e+01 - logprior: -1.3512e+02
Epoch 3/10
10/10 - 0s - loss: 103.4706 - loglik: -5.1914e+01 - logprior: -5.1557e+01
Epoch 4/10
10/10 - 0s - loss: 71.8101 - loglik: -5.1950e+01 - logprior: -1.9860e+01
Epoch 5/10
10/10 - 0s - loss: 55.3591 - loglik: -5.2213e+01 - logprior: -3.1465e+00
Epoch 6/10
10/10 - 0s - loss: 45.8578 - loglik: -5.2374e+01 - logprior: 6.5163
Epoch 7/10
10/10 - 0s - loss: 39.9857 - loglik: -5.2570e+01 - logprior: 12.5839
Epoch 8/10
10/10 - 0s - loss: 35.9925 - loglik: -5.2720e+01 - logprior: 16.7271
Epoch 9/10
10/10 - 0s - loss: 33.0154 - loglik: -5.2832e+01 - logprior: 19.8165
Epoch 10/10
10/10 - 0s - loss: 30.6312 - loglik: -5.2933e+01 - logprior: 22.3021
Fitted a model with MAP estimate = -29.4729
Time for alignment: 24.5320
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 619.8027 - loglik: -1.3137e+02 - logprior: -4.8843e+02
Epoch 2/10
10/10 - 0s - loss: 245.6508 - loglik: -1.1131e+02 - logprior: -1.3435e+02
Epoch 3/10
10/10 - 0s - loss: 152.6916 - loglik: -9.1422e+01 - logprior: -6.1269e+01
Epoch 4/10
10/10 - 0s - loss: 111.5341 - loglik: -7.8168e+01 - logprior: -3.3366e+01
Epoch 5/10
10/10 - 0s - loss: 92.0290 - loglik: -7.3782e+01 - logprior: -1.8247e+01
Epoch 6/10
10/10 - 0s - loss: 80.8596 - loglik: -7.1877e+01 - logprior: -8.9821e+00
Epoch 7/10
10/10 - 0s - loss: 74.3368 - loglik: -7.1256e+01 - logprior: -3.0805e+00
Epoch 8/10
10/10 - 0s - loss: 70.4278 - loglik: -7.1187e+01 - logprior: 0.7589
Epoch 9/10
10/10 - 0s - loss: 67.7775 - loglik: -7.1051e+01 - logprior: 3.2740
Epoch 10/10
10/10 - 0s - loss: 65.7770 - loglik: -7.0922e+01 - logprior: 5.1451
Fitted a model with MAP estimate = -64.8539
expansions: [(0, 5), (17, 3), (26, 3), (29, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 725.2108 - loglik: -7.9126e+01 - logprior: -6.4608e+02
Epoch 2/2
10/10 - 0s - loss: 261.6566 - loglik: -6.0279e+01 - logprior: -2.0138e+02
Fitted a model with MAP estimate = -176.5934
expansions: []
discards: [ 0 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 609.2767 - loglik: -6.1143e+01 - logprior: -5.4813e+02
Epoch 2/2
10/10 - 0s - loss: 271.1741 - loglik: -5.7282e+01 - logprior: -2.1389e+02
Fitted a model with MAP estimate = -213.7887
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 551.0927 - loglik: -5.6191e+01 - logprior: -4.9490e+02
Epoch 2/10
10/10 - 0s - loss: 188.2696 - loglik: -5.3149e+01 - logprior: -1.3512e+02
Epoch 3/10
10/10 - 0s - loss: 103.4716 - loglik: -5.1914e+01 - logprior: -5.1558e+01
Epoch 4/10
10/10 - 0s - loss: 71.8108 - loglik: -5.1953e+01 - logprior: -1.9858e+01
Epoch 5/10
10/10 - 0s - loss: 55.3603 - loglik: -5.2213e+01 - logprior: -3.1477e+00
Epoch 6/10
10/10 - 0s - loss: 45.8584 - loglik: -5.2375e+01 - logprior: 6.5162
Epoch 7/10
10/10 - 0s - loss: 39.9867 - loglik: -5.2571e+01 - logprior: 12.5840
Epoch 8/10
10/10 - 0s - loss: 35.9929 - loglik: -5.2720e+01 - logprior: 16.7270
Epoch 9/10
10/10 - 0s - loss: 33.0156 - loglik: -5.2832e+01 - logprior: 19.8163
Epoch 10/10
10/10 - 0s - loss: 30.6309 - loglik: -5.2933e+01 - logprior: 22.3018
Fitted a model with MAP estimate = -29.4734
Time for alignment: 24.9631
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 619.8028 - loglik: -1.3137e+02 - logprior: -4.8843e+02
Epoch 2/10
10/10 - 0s - loss: 245.6513 - loglik: -1.1131e+02 - logprior: -1.3435e+02
Epoch 3/10
10/10 - 0s - loss: 152.6920 - loglik: -9.1423e+01 - logprior: -6.1269e+01
Epoch 4/10
10/10 - 0s - loss: 111.5342 - loglik: -7.8168e+01 - logprior: -3.3366e+01
Epoch 5/10
10/10 - 0s - loss: 92.0304 - loglik: -7.3783e+01 - logprior: -1.8247e+01
Epoch 6/10
10/10 - 0s - loss: 80.8603 - loglik: -7.1878e+01 - logprior: -8.9821e+00
Epoch 7/10
10/10 - 0s - loss: 74.3373 - loglik: -7.1256e+01 - logprior: -3.0816e+00
Epoch 8/10
10/10 - 0s - loss: 70.4278 - loglik: -7.1187e+01 - logprior: 0.7587
Epoch 9/10
10/10 - 0s - loss: 67.7775 - loglik: -7.1052e+01 - logprior: 3.2742
Epoch 10/10
10/10 - 0s - loss: 65.7770 - loglik: -7.0922e+01 - logprior: 5.1449
Fitted a model with MAP estimate = -64.8538
expansions: [(0, 5), (17, 3), (26, 3), (29, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 725.2121 - loglik: -7.9127e+01 - logprior: -6.4609e+02
Epoch 2/2
10/10 - 0s - loss: 261.6567 - loglik: -6.0279e+01 - logprior: -2.0138e+02
Fitted a model with MAP estimate = -176.5932
expansions: []
discards: [ 0 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 609.2772 - loglik: -6.1144e+01 - logprior: -5.4813e+02
Epoch 2/2
10/10 - 0s - loss: 271.1745 - loglik: -5.7282e+01 - logprior: -2.1389e+02
Fitted a model with MAP estimate = -213.7892
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 551.0946 - loglik: -5.6193e+01 - logprior: -4.9490e+02
Epoch 2/10
10/10 - 0s - loss: 188.2708 - loglik: -5.3151e+01 - logprior: -1.3512e+02
Epoch 3/10
10/10 - 0s - loss: 103.4705 - loglik: -5.1913e+01 - logprior: -5.1558e+01
Epoch 4/10
10/10 - 0s - loss: 71.8098 - loglik: -5.1952e+01 - logprior: -1.9858e+01
Epoch 5/10
10/10 - 0s - loss: 55.3589 - loglik: -5.2212e+01 - logprior: -3.1472e+00
Epoch 6/10
10/10 - 0s - loss: 45.8573 - loglik: -5.2375e+01 - logprior: 6.5181
Epoch 7/10
10/10 - 0s - loss: 39.9856 - loglik: -5.2570e+01 - logprior: 12.5840
Epoch 8/10
10/10 - 0s - loss: 35.9925 - loglik: -5.2721e+01 - logprior: 16.7281
Epoch 9/10
10/10 - 0s - loss: 33.0153 - loglik: -5.2832e+01 - logprior: 19.8169
Epoch 10/10
10/10 - 0s - loss: 30.6306 - loglik: -5.2933e+01 - logprior: 22.3026
Fitted a model with MAP estimate = -29.4719
Time for alignment: 25.2043
Computed alignments with likelihoods: ['-29.4704', '-29.4738', '-29.4729', '-29.4734', '-29.4719']
Best model has likelihood: -29.4704  (prior= 23.5137 )
time for generating output: 0.0851
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.5787037037037037
Training of 5 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1efe68fb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f07433dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f07466fd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fdf2efca0>
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 479.3940 - loglik: -4.7520e+02 - logprior: -4.1937e+00
Epoch 2/10
16/16 - 4s - loss: 445.8483 - loglik: -4.4481e+02 - logprior: -1.0416e+00
Epoch 3/10
16/16 - 4s - loss: 421.2604 - loglik: -4.1989e+02 - logprior: -1.3709e+00
Epoch 4/10
16/16 - 4s - loss: 414.0365 - loglik: -4.1266e+02 - logprior: -1.3807e+00
Epoch 5/10
16/16 - 4s - loss: 410.8755 - loglik: -4.0953e+02 - logprior: -1.3468e+00
Epoch 6/10
16/16 - 4s - loss: 410.4630 - loglik: -4.0909e+02 - logprior: -1.3739e+00
Epoch 7/10
16/16 - 4s - loss: 408.4359 - loglik: -4.0707e+02 - logprior: -1.3684e+00
Epoch 8/10
16/16 - 4s - loss: 407.4891 - loglik: -4.0612e+02 - logprior: -1.3708e+00
Epoch 9/10
16/16 - 4s - loss: 407.7968 - loglik: -4.0643e+02 - logprior: -1.3682e+00
Fitted a model with MAP estimate = -406.9078
expansions: [(13, 1), (14, 1), (28, 4), (29, 1), (42, 2), (44, 1), (49, 1), (50, 1), (53, 1), (55, 1), (56, 1), (70, 1), (74, 2), (75, 1), (94, 3), (96, 1), (97, 1), (99, 1), (105, 1), (116, 1), (117, 1), (122, 2), (128, 1), (129, 1), (130, 1), (139, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 175 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 409.8453 - loglik: -4.0599e+02 - logprior: -3.8552e+00
Epoch 2/2
33/33 - 6s - loss: 401.2325 - loglik: -4.0012e+02 - logprior: -1.1139e+00
Fitted a model with MAP estimate = -398.0668
expansions: [(175, 2)]
discards: [  0  31  32 112 171 172 173 174]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 407.4812 - loglik: -4.0385e+02 - logprior: -3.6287e+00
Epoch 2/2
33/33 - 6s - loss: 402.0137 - loglik: -4.0119e+02 - logprior: -8.2678e-01
Fitted a model with MAP estimate = -399.6451
expansions: [(0, 1), (30, 2), (169, 2)]
discards: [167 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 403.2679 - loglik: -4.0081e+02 - logprior: -2.4583e+00
Epoch 2/10
33/33 - 6s - loss: 398.5075 - loglik: -3.9796e+02 - logprior: -5.5041e-01
Epoch 3/10
33/33 - 6s - loss: 396.7268 - loglik: -3.9633e+02 - logprior: -3.9792e-01
Epoch 4/10
33/33 - 6s - loss: 396.4043 - loglik: -3.9606e+02 - logprior: -3.4249e-01
Epoch 5/10
33/33 - 6s - loss: 394.4392 - loglik: -3.9415e+02 - logprior: -2.9377e-01
Epoch 6/10
33/33 - 6s - loss: 395.2886 - loglik: -3.9505e+02 - logprior: -2.3990e-01
Fitted a model with MAP estimate = -394.0765
Time for alignment: 141.1673
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 479.1443 - loglik: -4.7494e+02 - logprior: -4.2036e+00
Epoch 2/10
16/16 - 4s - loss: 446.3309 - loglik: -4.4527e+02 - logprior: -1.0576e+00
Epoch 3/10
16/16 - 4s - loss: 421.1534 - loglik: -4.1976e+02 - logprior: -1.3934e+00
Epoch 4/10
16/16 - 4s - loss: 412.6564 - loglik: -4.1125e+02 - logprior: -1.4029e+00
Epoch 5/10
16/16 - 4s - loss: 409.6678 - loglik: -4.0829e+02 - logprior: -1.3804e+00
Epoch 6/10
16/16 - 4s - loss: 408.4720 - loglik: -4.0706e+02 - logprior: -1.4119e+00
Epoch 7/10
16/16 - 4s - loss: 407.0260 - loglik: -4.0561e+02 - logprior: -1.4127e+00
Epoch 8/10
16/16 - 4s - loss: 406.3849 - loglik: -4.0496e+02 - logprior: -1.4219e+00
Epoch 9/10
16/16 - 4s - loss: 406.2345 - loglik: -4.0481e+02 - logprior: -1.4222e+00
Epoch 10/10
16/16 - 4s - loss: 406.0353 - loglik: -4.0461e+02 - logprior: -1.4299e+00
Fitted a model with MAP estimate = -405.5751
expansions: [(13, 1), (14, 1), (16, 1), (17, 1), (22, 1), (28, 2), (29, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (49, 2), (50, 1), (53, 1), (55, 1), (56, 1), (70, 1), (73, 1), (74, 2), (93, 3), (95, 1), (97, 1), (98, 1), (116, 1), (117, 2), (119, 1), (122, 1), (128, 1), (129, 1), (139, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 410.4868 - loglik: -4.0661e+02 - logprior: -3.8791e+00
Epoch 2/2
33/33 - 6s - loss: 398.5126 - loglik: -3.9735e+02 - logprior: -1.1620e+00
Fitted a model with MAP estimate = -397.5139
expansions: [(178, 2)]
discards: [  0  31  32  61  62  96 115 146 174 175 176 177]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 408.1472 - loglik: -4.0450e+02 - logprior: -3.6432e+00
Epoch 2/2
33/33 - 6s - loss: 402.2901 - loglik: -4.0141e+02 - logprior: -8.7970e-01
Fitted a model with MAP estimate = -399.9209
expansions: [(0, 1), (168, 2)]
discards: [166 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 403.6250 - loglik: -4.0117e+02 - logprior: -2.4532e+00
Epoch 2/10
33/33 - 6s - loss: 399.3073 - loglik: -3.9880e+02 - logprior: -5.1065e-01
Epoch 3/10
33/33 - 6s - loss: 397.8328 - loglik: -3.9748e+02 - logprior: -3.5758e-01
Epoch 4/10
33/33 - 6s - loss: 396.6244 - loglik: -3.9632e+02 - logprior: -3.0738e-01
Epoch 5/10
33/33 - 6s - loss: 396.1456 - loglik: -3.9589e+02 - logprior: -2.5182e-01
Epoch 6/10
33/33 - 6s - loss: 395.8300 - loglik: -3.9563e+02 - logprior: -1.9834e-01
Epoch 7/10
33/33 - 6s - loss: 394.5420 - loglik: -3.9438e+02 - logprior: -1.5772e-01
Epoch 8/10
33/33 - 6s - loss: 395.8759 - loglik: -3.9578e+02 - logprior: -9.8249e-02
Fitted a model with MAP estimate = -394.7190
Time for alignment: 157.3438
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 479.1424 - loglik: -4.7495e+02 - logprior: -4.1955e+00
Epoch 2/10
16/16 - 4s - loss: 445.6448 - loglik: -4.4459e+02 - logprior: -1.0519e+00
Epoch 3/10
16/16 - 4s - loss: 421.5491 - loglik: -4.2012e+02 - logprior: -1.4265e+00
Epoch 4/10
16/16 - 4s - loss: 411.8134 - loglik: -4.1031e+02 - logprior: -1.4989e+00
Epoch 5/10
16/16 - 4s - loss: 410.2752 - loglik: -4.0876e+02 - logprior: -1.5181e+00
Epoch 6/10
16/16 - 4s - loss: 408.0392 - loglik: -4.0650e+02 - logprior: -1.5411e+00
Epoch 7/10
16/16 - 4s - loss: 407.0841 - loglik: -4.0555e+02 - logprior: -1.5294e+00
Epoch 8/10
16/16 - 4s - loss: 406.5028 - loglik: -4.0497e+02 - logprior: -1.5377e+00
Epoch 9/10
16/16 - 4s - loss: 406.0138 - loglik: -4.0449e+02 - logprior: -1.5274e+00
Epoch 10/10
16/16 - 4s - loss: 404.4598 - loglik: -4.0292e+02 - logprior: -1.5440e+00
Fitted a model with MAP estimate = -405.0212
expansions: [(13, 1), (14, 1), (16, 2), (17, 1), (22, 1), (27, 2), (29, 1), (40, 1), (42, 1), (44, 1), (49, 1), (50, 1), (52, 1), (55, 1), (56, 1), (65, 1), (70, 1), (73, 1), (74, 2), (93, 2), (96, 1), (97, 1), (99, 1), (105, 1), (116, 1), (117, 1), (119, 1), (122, 1), (128, 1), (129, 1), (130, 1), (131, 1), (139, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 408.6210 - loglik: -4.0472e+02 - logprior: -3.9008e+00
Epoch 2/2
33/33 - 6s - loss: 399.0967 - loglik: -3.9790e+02 - logprior: -1.1946e+00
Fitted a model with MAP estimate = -396.5132
expansions: [(113, 1), (178, 2)]
discards: [  0  18  33  34  72  95 114 115 174 175 176 177]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 407.1346 - loglik: -4.0349e+02 - logprior: -3.6417e+00
Epoch 2/2
33/33 - 6s - loss: 401.4214 - loglik: -4.0049e+02 - logprior: -9.3311e-01
Fitted a model with MAP estimate = -398.9335
expansions: [(0, 1), (169, 2)]
discards: [167 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 403.1956 - loglik: -4.0071e+02 - logprior: -2.4875e+00
Epoch 2/10
33/33 - 6s - loss: 398.5247 - loglik: -3.9797e+02 - logprior: -5.5305e-01
Epoch 3/10
33/33 - 6s - loss: 396.9483 - loglik: -3.9653e+02 - logprior: -4.1508e-01
Epoch 4/10
33/33 - 6s - loss: 395.6031 - loglik: -3.9526e+02 - logprior: -3.4605e-01
Epoch 5/10
33/33 - 6s - loss: 394.4915 - loglik: -3.9419e+02 - logprior: -2.9865e-01
Epoch 6/10
33/33 - 6s - loss: 394.6030 - loglik: -3.9436e+02 - logprior: -2.4744e-01
Fitted a model with MAP estimate = -394.0244
Time for alignment: 146.2821
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 479.2216 - loglik: -4.7502e+02 - logprior: -4.2002e+00
Epoch 2/10
16/16 - 4s - loss: 447.4204 - loglik: -4.4637e+02 - logprior: -1.0473e+00
Epoch 3/10
16/16 - 4s - loss: 422.2029 - loglik: -4.2081e+02 - logprior: -1.3967e+00
Epoch 4/10
16/16 - 4s - loss: 412.1858 - loglik: -4.1071e+02 - logprior: -1.4734e+00
Epoch 5/10
16/16 - 4s - loss: 409.2984 - loglik: -4.0782e+02 - logprior: -1.4781e+00
Epoch 6/10
16/16 - 4s - loss: 408.8384 - loglik: -4.0730e+02 - logprior: -1.5350e+00
Epoch 7/10
16/16 - 4s - loss: 406.3946 - loglik: -4.0487e+02 - logprior: -1.5211e+00
Epoch 8/10
16/16 - 4s - loss: 406.1621 - loglik: -4.0463e+02 - logprior: -1.5287e+00
Epoch 9/10
16/16 - 4s - loss: 406.4592 - loglik: -4.0494e+02 - logprior: -1.5231e+00
Fitted a model with MAP estimate = -405.4745
expansions: [(13, 1), (14, 1), (17, 1), (19, 1), (22, 1), (28, 2), (29, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (49, 1), (51, 1), (52, 1), (54, 2), (55, 1), (70, 2), (71, 1), (72, 1), (73, 2), (93, 2), (95, 1), (96, 1), (98, 1), (116, 1), (117, 1), (118, 1), (122, 1), (128, 1), (129, 1), (130, 1), (131, 1), (139, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 408.7244 - loglik: -4.0487e+02 - logprior: -3.8553e+00
Epoch 2/2
33/33 - 7s - loss: 398.0015 - loglik: -3.9687e+02 - logprior: -1.1286e+00
Fitted a model with MAP estimate = -396.2150
expansions: [(180, 2)]
discards: [  0  31  32  33  69  73  97 176 177 178 179]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 406.7934 - loglik: -4.0318e+02 - logprior: -3.6175e+00
Epoch 2/2
33/33 - 6s - loss: 400.9402 - loglik: -4.0009e+02 - logprior: -8.4594e-01
Fitted a model with MAP estimate = -398.7357
expansions: [(0, 1), (30, 2), (171, 2)]
discards: [169 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 174 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 11s - loss: 402.5744 - loglik: -4.0011e+02 - logprior: -2.4643e+00
Epoch 2/10
33/33 - 6s - loss: 396.7411 - loglik: -3.9622e+02 - logprior: -5.2236e-01
Epoch 3/10
33/33 - 6s - loss: 395.8096 - loglik: -3.9544e+02 - logprior: -3.6495e-01
Epoch 4/10
33/33 - 6s - loss: 395.8682 - loglik: -3.9554e+02 - logprior: -3.2489e-01
Fitted a model with MAP estimate = -394.0047
Time for alignment: 130.3163
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 479.7283 - loglik: -4.7553e+02 - logprior: -4.1975e+00
Epoch 2/10
16/16 - 4s - loss: 445.8116 - loglik: -4.4475e+02 - logprior: -1.0646e+00
Epoch 3/10
16/16 - 4s - loss: 421.2149 - loglik: -4.1979e+02 - logprior: -1.4209e+00
Epoch 4/10
16/16 - 4s - loss: 413.3459 - loglik: -4.1194e+02 - logprior: -1.4090e+00
Epoch 5/10
16/16 - 4s - loss: 409.7877 - loglik: -4.0837e+02 - logprior: -1.4146e+00
Epoch 6/10
16/16 - 4s - loss: 409.1572 - loglik: -4.0771e+02 - logprior: -1.4460e+00
Epoch 7/10
16/16 - 4s - loss: 408.6765 - loglik: -4.0725e+02 - logprior: -1.4315e+00
Epoch 8/10
16/16 - 4s - loss: 406.2777 - loglik: -4.0485e+02 - logprior: -1.4258e+00
Epoch 9/10
16/16 - 4s - loss: 407.3244 - loglik: -4.0590e+02 - logprior: -1.4235e+00
Fitted a model with MAP estimate = -406.6826
expansions: [(13, 1), (14, 1), (16, 1), (17, 1), (22, 1), (28, 2), (29, 1), (42, 1), (44, 1), (48, 2), (49, 1), (50, 1), (53, 1), (55, 1), (56, 1), (71, 1), (72, 1), (74, 2), (75, 1), (80, 1), (94, 3), (96, 1), (98, 1), (99, 1), (102, 1), (116, 1), (117, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 409.1222 - loglik: -4.0530e+02 - logprior: -3.8265e+00
Epoch 2/2
33/33 - 6s - loss: 400.1538 - loglik: -3.9909e+02 - logprior: -1.0679e+00
Fitted a model with MAP estimate = -397.6282
expansions: [(178, 2)]
discards: [  0  31  32  33  57  72 117 118 174 175 176 177]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 408.2283 - loglik: -4.0459e+02 - logprior: -3.6385e+00
Epoch 2/2
33/33 - 6s - loss: 401.8094 - loglik: -4.0096e+02 - logprior: -8.4945e-01
Fitted a model with MAP estimate = -400.0794
expansions: [(0, 1), (30, 2), (111, 2), (168, 2)]
discards: [166 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 403.1645 - loglik: -4.0070e+02 - logprior: -2.4664e+00
Epoch 2/10
33/33 - 6s - loss: 398.8761 - loglik: -3.9832e+02 - logprior: -5.5288e-01
Epoch 3/10
33/33 - 6s - loss: 397.1256 - loglik: -3.9672e+02 - logprior: -4.0278e-01
Epoch 4/10
33/33 - 6s - loss: 396.5673 - loglik: -3.9623e+02 - logprior: -3.3808e-01
Epoch 5/10
33/33 - 6s - loss: 394.3638 - loglik: -3.9406e+02 - logprior: -3.0789e-01
Epoch 6/10
33/33 - 6s - loss: 395.1701 - loglik: -3.9491e+02 - logprior: -2.6212e-01
Fitted a model with MAP estimate = -394.3135
Time for alignment: 141.9420
Computed alignments with likelihoods: ['-394.0765', '-394.7190', '-394.0244', '-394.0047', '-394.3135']
Best model has likelihood: -394.0047  (prior= -0.3292 )
time for generating output: 0.2063
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.8095768374164811
Training of 5 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f20239f7550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200a23c310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f06ebd760>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1ee5114b80>
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 294.8987 - loglik: -2.8000e+02 - logprior: -1.4898e+01
Epoch 2/10
10/10 - 1s - loss: 259.0508 - loglik: -2.5527e+02 - logprior: -3.7766e+00
Epoch 3/10
10/10 - 1s - loss: 229.2368 - loglik: -2.2731e+02 - logprior: -1.9251e+00
Epoch 4/10
10/10 - 1s - loss: 211.9777 - loglik: -2.1052e+02 - logprior: -1.4567e+00
Epoch 5/10
10/10 - 1s - loss: 206.6026 - loglik: -2.0525e+02 - logprior: -1.3524e+00
Epoch 6/10
10/10 - 1s - loss: 205.2635 - loglik: -2.0395e+02 - logprior: -1.3098e+00
Epoch 7/10
10/10 - 1s - loss: 203.8584 - loglik: -2.0263e+02 - logprior: -1.2283e+00
Epoch 8/10
10/10 - 1s - loss: 202.6545 - loglik: -2.0148e+02 - logprior: -1.1745e+00
Epoch 9/10
10/10 - 1s - loss: 202.9885 - loglik: -2.0184e+02 - logprior: -1.1472e+00
Fitted a model with MAP estimate = -202.5203
expansions: [(42, 1), (46, 1), (48, 3), (49, 1), (53, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.6501 - loglik: -2.1242e+02 - logprior: -1.7226e+01
Epoch 2/2
10/10 - 1s - loss: 209.0249 - loglik: -2.0161e+02 - logprior: -7.4117e+00
Fitted a model with MAP estimate = -207.1110
expansions: []
discards: [ 0 50 51 59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 226.6359 - loglik: -2.0968e+02 - logprior: -1.6954e+01
Epoch 2/2
10/10 - 1s - loss: 208.3344 - loglik: -2.0221e+02 - logprior: -6.1261e+00
Fitted a model with MAP estimate = -206.5688
expansions: [(0, 22), (39, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 217.8333 - loglik: -2.0345e+02 - logprior: -1.4379e+01
Epoch 2/10
10/10 - 1s - loss: 197.9015 - loglik: -1.9350e+02 - logprior: -4.4025e+00
Epoch 3/10
10/10 - 1s - loss: 191.3980 - loglik: -1.8895e+02 - logprior: -2.4520e+00
Epoch 4/10
10/10 - 1s - loss: 187.6076 - loglik: -1.8586e+02 - logprior: -1.7492e+00
Epoch 5/10
10/10 - 1s - loss: 188.6951 - loglik: -1.8721e+02 - logprior: -1.4845e+00
Fitted a model with MAP estimate = -186.7955
Time for alignment: 40.8347
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 294.4442 - loglik: -2.7955e+02 - logprior: -1.4895e+01
Epoch 2/10
10/10 - 1s - loss: 258.7202 - loglik: -2.5494e+02 - logprior: -3.7773e+00
Epoch 3/10
10/10 - 1s - loss: 230.7765 - loglik: -2.2884e+02 - logprior: -1.9398e+00
Epoch 4/10
10/10 - 1s - loss: 212.2094 - loglik: -2.1075e+02 - logprior: -1.4583e+00
Epoch 5/10
10/10 - 1s - loss: 207.3591 - loglik: -2.0601e+02 - logprior: -1.3485e+00
Epoch 6/10
10/10 - 1s - loss: 203.3651 - loglik: -2.0211e+02 - logprior: -1.2588e+00
Epoch 7/10
10/10 - 1s - loss: 203.7213 - loglik: -2.0253e+02 - logprior: -1.1919e+00
Fitted a model with MAP estimate = -202.7997
expansions: [(41, 1), (46, 1), (48, 3), (49, 1), (53, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.1561 - loglik: -2.1098e+02 - logprior: -1.7174e+01
Epoch 2/2
10/10 - 1s - loss: 210.1817 - loglik: -2.0292e+02 - logprior: -7.2620e+00
Fitted a model with MAP estimate = -206.9051
expansions: [(0, 21)]
discards: [ 0 50 51 59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 218.0605 - loglik: -2.0371e+02 - logprior: -1.4354e+01
Epoch 2/2
10/10 - 1s - loss: 199.3800 - loglik: -1.9505e+02 - logprior: -4.3265e+00
Fitted a model with MAP estimate = -193.7448
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 219.2430 - loglik: -2.0431e+02 - logprior: -1.4932e+01
Epoch 2/10
10/10 - 1s - loss: 203.7698 - loglik: -1.9959e+02 - logprior: -4.1779e+00
Epoch 3/10
10/10 - 1s - loss: 200.4859 - loglik: -1.9857e+02 - logprior: -1.9206e+00
Epoch 4/10
10/10 - 1s - loss: 201.0129 - loglik: -1.9991e+02 - logprior: -1.0982e+00
Fitted a model with MAP estimate = -199.2685
Time for alignment: 36.3426
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 293.7636 - loglik: -2.7887e+02 - logprior: -1.4898e+01
Epoch 2/10
10/10 - 1s - loss: 260.6621 - loglik: -2.5688e+02 - logprior: -3.7773e+00
Epoch 3/10
10/10 - 1s - loss: 232.1069 - loglik: -2.3015e+02 - logprior: -1.9588e+00
Epoch 4/10
10/10 - 1s - loss: 214.1435 - loglik: -2.1266e+02 - logprior: -1.4803e+00
Epoch 5/10
10/10 - 1s - loss: 208.4236 - loglik: -2.0706e+02 - logprior: -1.3669e+00
Epoch 6/10
10/10 - 1s - loss: 204.0043 - loglik: -2.0270e+02 - logprior: -1.3011e+00
Epoch 7/10
10/10 - 1s - loss: 203.9380 - loglik: -2.0275e+02 - logprior: -1.1877e+00
Epoch 8/10
10/10 - 1s - loss: 202.3994 - loglik: -2.0125e+02 - logprior: -1.1529e+00
Epoch 9/10
10/10 - 1s - loss: 202.6837 - loglik: -2.0157e+02 - logprior: -1.1161e+00
Fitted a model with MAP estimate = -201.8088
expansions: [(35, 1), (46, 1), (48, 3), (49, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 73 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.2202 - loglik: -2.1203e+02 - logprior: -1.7186e+01
Epoch 2/2
10/10 - 1s - loss: 210.9750 - loglik: -2.0364e+02 - logprior: -7.3398e+00
Fitted a model with MAP estimate = -207.9340
expansions: []
discards: [ 0 50 51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.3482 - loglik: -2.0844e+02 - logprior: -1.6907e+01
Epoch 2/2
10/10 - 1s - loss: 209.9383 - loglik: -2.0385e+02 - logprior: -6.0835e+00
Fitted a model with MAP estimate = -206.6960
expansions: [(0, 22)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 216.9888 - loglik: -2.0264e+02 - logprior: -1.4349e+01
Epoch 2/10
10/10 - 1s - loss: 199.2170 - loglik: -1.9484e+02 - logprior: -4.3737e+00
Epoch 3/10
10/10 - 1s - loss: 191.7790 - loglik: -1.8936e+02 - logprior: -2.4206e+00
Epoch 4/10
10/10 - 1s - loss: 190.9383 - loglik: -1.8920e+02 - logprior: -1.7370e+00
Epoch 5/10
10/10 - 1s - loss: 187.7777 - loglik: -1.8630e+02 - logprior: -1.4768e+00
Epoch 6/10
10/10 - 1s - loss: 188.5081 - loglik: -1.8714e+02 - logprior: -1.3631e+00
Fitted a model with MAP estimate = -187.7243
Time for alignment: 40.7328
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 293.7537 - loglik: -2.7886e+02 - logprior: -1.4897e+01
Epoch 2/10
10/10 - 1s - loss: 259.3896 - loglik: -2.5563e+02 - logprior: -3.7628e+00
Epoch 3/10
10/10 - 1s - loss: 229.9991 - loglik: -2.2808e+02 - logprior: -1.9211e+00
Epoch 4/10
10/10 - 1s - loss: 213.4995 - loglik: -2.1204e+02 - logprior: -1.4551e+00
Epoch 5/10
10/10 - 1s - loss: 207.3425 - loglik: -2.0603e+02 - logprior: -1.3163e+00
Epoch 6/10
10/10 - 1s - loss: 205.5526 - loglik: -2.0431e+02 - logprior: -1.2411e+00
Epoch 7/10
10/10 - 1s - loss: 202.1534 - loglik: -2.0096e+02 - logprior: -1.1910e+00
Epoch 8/10
10/10 - 1s - loss: 203.9256 - loglik: -2.0276e+02 - logprior: -1.1639e+00
Fitted a model with MAP estimate = -202.8526
expansions: [(42, 1), (43, 1), (45, 1), (46, 1), (47, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.8477 - loglik: -2.0969e+02 - logprior: -1.7155e+01
Epoch 2/2
10/10 - 1s - loss: 211.1562 - loglik: -2.0395e+02 - logprior: -7.2094e+00
Fitted a model with MAP estimate = -206.3325
expansions: [(0, 21)]
discards: [ 0 50]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 217.5051 - loglik: -2.0314e+02 - logprior: -1.4366e+01
Epoch 2/2
10/10 - 1s - loss: 197.1442 - loglik: -1.9282e+02 - logprior: -4.3256e+00
Fitted a model with MAP estimate = -192.0581
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 73 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 217.8451 - loglik: -2.0297e+02 - logprior: -1.4870e+01
Epoch 2/10
10/10 - 1s - loss: 201.2857 - loglik: -1.9718e+02 - logprior: -4.1073e+00
Epoch 3/10
10/10 - 1s - loss: 200.8669 - loglik: -1.9894e+02 - logprior: -1.9220e+00
Epoch 4/10
10/10 - 1s - loss: 197.5666 - loglik: -1.9647e+02 - logprior: -1.0975e+00
Epoch 5/10
10/10 - 1s - loss: 197.1295 - loglik: -1.9641e+02 - logprior: -7.1491e-01
Epoch 6/10
10/10 - 1s - loss: 197.1740 - loglik: -1.9667e+02 - logprior: -5.0590e-01
Fitted a model with MAP estimate = -196.6097
Time for alignment: 40.7553
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 293.7178 - loglik: -2.7882e+02 - logprior: -1.4897e+01
Epoch 2/10
10/10 - 1s - loss: 260.4422 - loglik: -2.5667e+02 - logprior: -3.7759e+00
Epoch 3/10
10/10 - 1s - loss: 232.3672 - loglik: -2.3043e+02 - logprior: -1.9421e+00
Epoch 4/10
10/10 - 1s - loss: 213.3226 - loglik: -2.1182e+02 - logprior: -1.4985e+00
Epoch 5/10
10/10 - 1s - loss: 207.8386 - loglik: -2.0642e+02 - logprior: -1.4213e+00
Epoch 6/10
10/10 - 1s - loss: 205.8664 - loglik: -2.0447e+02 - logprior: -1.3929e+00
Epoch 7/10
10/10 - 1s - loss: 203.2900 - loglik: -2.0200e+02 - logprior: -1.2917e+00
Epoch 8/10
10/10 - 1s - loss: 202.4115 - loglik: -2.0115e+02 - logprior: -1.2595e+00
Epoch 9/10
10/10 - 1s - loss: 203.6186 - loglik: -2.0240e+02 - logprior: -1.2199e+00
Fitted a model with MAP estimate = -202.2579
expansions: [(41, 1), (42, 1), (46, 1), (48, 1), (49, 1), (53, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.0523 - loglik: -2.1185e+02 - logprior: -1.7206e+01
Epoch 2/2
10/10 - 1s - loss: 208.8638 - loglik: -2.0159e+02 - logprior: -7.2750e+00
Fitted a model with MAP estimate = -206.8687
expansions: []
discards: [ 0 58]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 223.5841 - loglik: -2.0679e+02 - logprior: -1.6796e+01
Epoch 2/2
10/10 - 1s - loss: 208.2494 - loglik: -2.0239e+02 - logprior: -5.8569e+00
Fitted a model with MAP estimate = -204.7566
expansions: [(0, 21)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 217.8532 - loglik: -2.0361e+02 - logprior: -1.4245e+01
Epoch 2/10
10/10 - 1s - loss: 195.3036 - loglik: -1.9107e+02 - logprior: -4.2348e+00
Epoch 3/10
10/10 - 1s - loss: 192.1499 - loglik: -1.8981e+02 - logprior: -2.3424e+00
Epoch 4/10
10/10 - 1s - loss: 186.9834 - loglik: -1.8532e+02 - logprior: -1.6648e+00
Epoch 5/10
10/10 - 1s - loss: 186.9669 - loglik: -1.8556e+02 - logprior: -1.4023e+00
Epoch 6/10
10/10 - 1s - loss: 187.6505 - loglik: -1.8639e+02 - logprior: -1.2591e+00
Fitted a model with MAP estimate = -186.3364
Time for alignment: 40.0896
Computed alignments with likelihoods: ['-186.7955', '-193.7448', '-187.7243', '-192.0581', '-186.3364']
Best model has likelihood: -186.3364  (prior= -1.1788 )
time for generating output: 0.1625
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.3530631479736098
Training of 5 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e8645d670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fbd73af70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f077b6370>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1ee5114b80>
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 617.8076 - loglik: -5.6504e+02 - logprior: -5.2766e+01
Epoch 2/10
10/10 - 2s - loss: 533.5610 - loglik: -5.2483e+02 - logprior: -8.7338e+00
Epoch 3/10
10/10 - 2s - loss: 472.7330 - loglik: -4.7136e+02 - logprior: -1.3745e+00
Epoch 4/10
10/10 - 2s - loss: 434.3569 - loglik: -4.3531e+02 - logprior: 0.9559
Epoch 5/10
10/10 - 2s - loss: 421.0919 - loglik: -4.2315e+02 - logprior: 2.0531
Epoch 6/10
10/10 - 2s - loss: 417.1244 - loglik: -4.1993e+02 - logprior: 2.8032
Epoch 7/10
10/10 - 2s - loss: 411.1883 - loglik: -4.1437e+02 - logprior: 3.1772
Epoch 8/10
10/10 - 2s - loss: 409.1269 - loglik: -4.1249e+02 - logprior: 3.3622
Epoch 9/10
10/10 - 2s - loss: 407.9614 - loglik: -4.1155e+02 - logprior: 3.5885
Epoch 10/10
10/10 - 2s - loss: 406.8124 - loglik: -4.1059e+02 - logprior: 3.7760
Fitted a model with MAP estimate = -406.1059
expansions: [(10, 1), (12, 1), (32, 1), (44, 1), (45, 5), (77, 1), (83, 4), (105, 1), (107, 1), (114, 2), (120, 1), (147, 5), (151, 1), (152, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 485.3912 - loglik: -4.2526e+02 - logprior: -6.0130e+01
Epoch 2/2
10/10 - 2s - loss: 424.3543 - loglik: -4.0366e+02 - logprior: -2.0690e+01
Fitted a model with MAP estimate = -413.1798
expansions: [(0, 16), (8, 1)]
discards: [  0 176 177 178]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 209 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 453.8635 - loglik: -4.0763e+02 - logprior: -4.6229e+01
Epoch 2/2
10/10 - 3s - loss: 402.6093 - loglik: -3.9606e+02 - logprior: -6.5457e+00
Fitted a model with MAP estimate = -394.2550
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 24 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 453.1238 - loglik: -4.0064e+02 - logprior: -5.2481e+01
Epoch 2/10
10/10 - 2s - loss: 404.3583 - loglik: -3.9622e+02 - logprior: -8.1365e+00
Epoch 3/10
10/10 - 2s - loss: 391.2248 - loglik: -3.9294e+02 - logprior: 1.7162
Epoch 4/10
10/10 - 2s - loss: 386.3459 - loglik: -3.9193e+02 - logprior: 5.5798
Epoch 5/10
10/10 - 2s - loss: 382.4971 - loglik: -3.9008e+02 - logprior: 7.5784
Epoch 6/10
10/10 - 2s - loss: 381.3183 - loglik: -3.9003e+02 - logprior: 8.7157
Epoch 7/10
10/10 - 2s - loss: 378.4973 - loglik: -3.8799e+02 - logprior: 9.4968
Epoch 8/10
10/10 - 2s - loss: 377.9424 - loglik: -3.8803e+02 - logprior: 10.0868
Epoch 9/10
10/10 - 2s - loss: 373.2232 - loglik: -3.8376e+02 - logprior: 10.5413
Epoch 10/10
10/10 - 2s - loss: 376.1184 - loglik: -3.8700e+02 - logprior: 10.8777
Fitted a model with MAP estimate = -373.6245
Time for alignment: 73.5622
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 618.8600 - loglik: -5.6610e+02 - logprior: -5.2764e+01
Epoch 2/10
10/10 - 2s - loss: 533.7047 - loglik: -5.2499e+02 - logprior: -8.7143e+00
Epoch 3/10
10/10 - 2s - loss: 472.7232 - loglik: -4.7108e+02 - logprior: -1.6452e+00
Epoch 4/10
10/10 - 2s - loss: 438.0441 - loglik: -4.3835e+02 - logprior: 0.3109
Epoch 5/10
10/10 - 2s - loss: 422.5234 - loglik: -4.2413e+02 - logprior: 1.6074
Epoch 6/10
10/10 - 2s - loss: 416.4664 - loglik: -4.1902e+02 - logprior: 2.5543
Epoch 7/10
10/10 - 2s - loss: 413.1000 - loglik: -4.1603e+02 - logprior: 2.9348
Epoch 8/10
10/10 - 2s - loss: 411.5843 - loglik: -4.1471e+02 - logprior: 3.1260
Epoch 9/10
10/10 - 2s - loss: 407.7962 - loglik: -4.1110e+02 - logprior: 3.3062
Epoch 10/10
10/10 - 2s - loss: 407.6665 - loglik: -4.1113e+02 - logprior: 3.4627
Fitted a model with MAP estimate = -407.3355
expansions: [(10, 1), (12, 1), (18, 4), (21, 1), (35, 1), (45, 4), (77, 1), (83, 4), (84, 1), (99, 2), (105, 2), (106, 2), (114, 1), (119, 2), (145, 4), (152, 5), (154, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 484.4321 - loglik: -4.2456e+02 - logprior: -5.9876e+01
Epoch 2/2
10/10 - 2s - loss: 422.8364 - loglik: -4.0203e+02 - logprior: -2.0804e+01
Fitted a model with MAP estimate = -409.4528
expansions: [(0, 17), (143, 1)]
discards: [  0  20  21  22  52  53 124 127]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 213 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 450.8458 - loglik: -4.0476e+02 - logprior: -4.6084e+01
Epoch 2/2
10/10 - 3s - loss: 403.2491 - loglik: -3.9674e+02 - logprior: -6.5057e+00
Fitted a model with MAP estimate = -393.7019
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 452.1372 - loglik: -3.9939e+02 - logprior: -5.2746e+01
Epoch 2/10
10/10 - 2s - loss: 402.3914 - loglik: -3.9424e+02 - logprior: -8.1495e+00
Epoch 3/10
10/10 - 2s - loss: 389.6555 - loglik: -3.9154e+02 - logprior: 1.8831
Epoch 4/10
10/10 - 2s - loss: 385.0433 - loglik: -3.9082e+02 - logprior: 5.7722
Epoch 5/10
10/10 - 2s - loss: 381.5109 - loglik: -3.8931e+02 - logprior: 7.7998
Epoch 6/10
10/10 - 2s - loss: 379.0986 - loglik: -3.8814e+02 - logprior: 9.0401
Epoch 7/10
10/10 - 2s - loss: 377.8106 - loglik: -3.8767e+02 - logprior: 9.8558
Epoch 8/10
10/10 - 2s - loss: 376.6018 - loglik: -3.8709e+02 - logprior: 10.4867
Epoch 9/10
10/10 - 2s - loss: 374.3746 - loglik: -3.8530e+02 - logprior: 10.9267
Epoch 10/10
10/10 - 2s - loss: 373.0208 - loglik: -3.8428e+02 - logprior: 11.2598
Fitted a model with MAP estimate = -372.4224
Time for alignment: 73.9599
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 618.9404 - loglik: -5.6618e+02 - logprior: -5.2763e+01
Epoch 2/10
10/10 - 2s - loss: 532.7947 - loglik: -5.2407e+02 - logprior: -8.7228e+00
Epoch 3/10
10/10 - 2s - loss: 472.1100 - loglik: -4.7064e+02 - logprior: -1.4666e+00
Epoch 4/10
10/10 - 2s - loss: 440.2901 - loglik: -4.4118e+02 - logprior: 0.8925
Epoch 5/10
10/10 - 2s - loss: 425.2080 - loglik: -4.2739e+02 - logprior: 2.1836
Epoch 6/10
10/10 - 2s - loss: 419.6823 - loglik: -4.2260e+02 - logprior: 2.9183
Epoch 7/10
10/10 - 2s - loss: 416.9676 - loglik: -4.2030e+02 - logprior: 3.3317
Epoch 8/10
10/10 - 2s - loss: 414.0743 - loglik: -4.1760e+02 - logprior: 3.5215
Epoch 9/10
10/10 - 2s - loss: 412.5165 - loglik: -4.1626e+02 - logprior: 3.7420
Epoch 10/10
10/10 - 2s - loss: 411.2382 - loglik: -4.1517e+02 - logprior: 3.9359
Fitted a model with MAP estimate = -410.6605
expansions: [(18, 4), (39, 1), (44, 1), (45, 4), (99, 1), (112, 2), (115, 1), (116, 2), (145, 3), (146, 1), (151, 1), (152, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 490.1312 - loglik: -4.2974e+02 - logprior: -6.0391e+01
Epoch 2/2
10/10 - 2s - loss: 433.0457 - loglik: -4.1202e+02 - logprior: -2.1030e+01
Fitted a model with MAP estimate = -421.3468
expansions: [(0, 20), (93, 4)]
discards: [ 0 18 50 51]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 212 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 460.6955 - loglik: -4.1405e+02 - logprior: -4.6642e+01
Epoch 2/2
10/10 - 3s - loss: 410.1036 - loglik: -4.0310e+02 - logprior: -7.0061e+00
Fitted a model with MAP estimate = -400.7154
expansions: [(57, 1), (109, 1), (110, 1), (153, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24 146 191]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 189 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 474.6864 - loglik: -4.1561e+02 - logprior: -5.9081e+01
Epoch 2/10
10/10 - 2s - loss: 426.6162 - loglik: -4.0641e+02 - logprior: -2.0206e+01
Epoch 3/10
10/10 - 2s - loss: 415.2786 - loglik: -4.0328e+02 - logprior: -1.2001e+01
Epoch 4/10
10/10 - 2s - loss: 411.4730 - loglik: -4.0317e+02 - logprior: -8.3048e+00
Epoch 5/10
10/10 - 2s - loss: 405.7276 - loglik: -4.0057e+02 - logprior: -5.1541e+00
Epoch 6/10
10/10 - 2s - loss: 397.3741 - loglik: -4.0004e+02 - logprior: 2.6644
Epoch 7/10
10/10 - 2s - loss: 390.2839 - loglik: -3.9889e+02 - logprior: 8.6024
Epoch 8/10
10/10 - 2s - loss: 388.7078 - loglik: -3.9845e+02 - logprior: 9.7441
Epoch 9/10
10/10 - 2s - loss: 388.0035 - loglik: -3.9832e+02 - logprior: 10.3197
Epoch 10/10
10/10 - 2s - loss: 385.2662 - loglik: -3.9598e+02 - logprior: 10.7171
Fitted a model with MAP estimate = -384.9562
Time for alignment: 72.9320
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 618.9056 - loglik: -5.6615e+02 - logprior: -5.2760e+01
Epoch 2/10
10/10 - 2s - loss: 531.2333 - loglik: -5.2250e+02 - logprior: -8.7332e+00
Epoch 3/10
10/10 - 2s - loss: 470.2782 - loglik: -4.6874e+02 - logprior: -1.5336e+00
Epoch 4/10
10/10 - 2s - loss: 432.5648 - loglik: -4.3295e+02 - logprior: 0.3891
Epoch 5/10
10/10 - 2s - loss: 420.9574 - loglik: -4.2260e+02 - logprior: 1.6474
Epoch 6/10
10/10 - 2s - loss: 414.2113 - loglik: -4.1677e+02 - logprior: 2.5636
Epoch 7/10
10/10 - 2s - loss: 412.6819 - loglik: -4.1567e+02 - logprior: 2.9835
Epoch 8/10
10/10 - 2s - loss: 407.9406 - loglik: -4.1111e+02 - logprior: 3.1649
Epoch 9/10
10/10 - 2s - loss: 410.6048 - loglik: -4.1395e+02 - logprior: 3.3406
Fitted a model with MAP estimate = -407.5938
expansions: [(12, 1), (18, 3), (21, 1), (44, 1), (45, 5), (77, 2), (83, 4), (104, 1), (106, 1), (107, 1), (116, 1), (117, 2), (119, 2), (151, 2), (154, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 483.2451 - loglik: -4.2303e+02 - logprior: -6.0216e+01
Epoch 2/2
10/10 - 2s - loss: 424.7058 - loglik: -4.0401e+02 - logprior: -2.0691e+01
Fitted a model with MAP estimate = -413.2545
expansions: [(0, 20)]
discards: [ 0 18 19 87]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 211 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 453.2110 - loglik: -4.0697e+02 - logprior: -4.6240e+01
Epoch 2/2
10/10 - 3s - loss: 404.1244 - loglik: -3.9789e+02 - logprior: -6.2361e+00
Fitted a model with MAP estimate = -396.7913
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18 192]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 191 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 452.7151 - loglik: -3.9955e+02 - logprior: -5.3164e+01
Epoch 2/10
10/10 - 2s - loss: 404.6874 - loglik: -3.9598e+02 - logprior: -8.7096e+00
Epoch 3/10
10/10 - 2s - loss: 392.1907 - loglik: -3.9361e+02 - logprior: 1.4204
Epoch 4/10
10/10 - 2s - loss: 389.1725 - loglik: -3.9453e+02 - logprior: 5.3593
Epoch 5/10
10/10 - 2s - loss: 384.7645 - loglik: -3.9226e+02 - logprior: 7.4977
Epoch 6/10
10/10 - 2s - loss: 381.2581 - loglik: -3.9001e+02 - logprior: 8.7544
Epoch 7/10
10/10 - 2s - loss: 382.7519 - loglik: -3.9239e+02 - logprior: 9.6396
Fitted a model with MAP estimate = -380.6802
Time for alignment: 65.0646
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 619.0834 - loglik: -5.6632e+02 - logprior: -5.2760e+01
Epoch 2/10
10/10 - 2s - loss: 532.5938 - loglik: -5.2388e+02 - logprior: -8.7171e+00
Epoch 3/10
10/10 - 2s - loss: 472.9476 - loglik: -4.7134e+02 - logprior: -1.6088e+00
Epoch 4/10
10/10 - 2s - loss: 436.6537 - loglik: -4.3692e+02 - logprior: 0.2682
Epoch 5/10
10/10 - 2s - loss: 421.7357 - loglik: -4.2309e+02 - logprior: 1.3546
Epoch 6/10
10/10 - 2s - loss: 416.2767 - loglik: -4.1824e+02 - logprior: 1.9677
Epoch 7/10
10/10 - 2s - loss: 411.6467 - loglik: -4.1404e+02 - logprior: 2.3949
Epoch 8/10
10/10 - 2s - loss: 408.5054 - loglik: -4.1107e+02 - logprior: 2.5601
Epoch 9/10
10/10 - 2s - loss: 407.1697 - loglik: -4.0983e+02 - logprior: 2.6631
Epoch 10/10
10/10 - 2s - loss: 406.8408 - loglik: -4.0968e+02 - logprior: 2.8412
Fitted a model with MAP estimate = -405.3572
expansions: [(12, 1), (32, 1), (44, 1), (45, 3), (77, 3), (83, 3), (96, 1), (99, 1), (103, 1), (104, 1), (107, 1), (115, 1), (116, 1), (119, 1), (120, 1), (147, 5), (151, 1), (152, 1), (154, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 484.9833 - loglik: -4.2463e+02 - logprior: -6.0355e+01
Epoch 2/2
10/10 - 2s - loss: 425.1564 - loglik: -4.0419e+02 - logprior: -2.0966e+01
Fitted a model with MAP estimate = -413.5226
expansions: [(0, 20)]
discards: [ 0 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 213 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 451.7726 - loglik: -4.0531e+02 - logprior: -4.6463e+01
Epoch 2/2
10/10 - 3s - loss: 403.1617 - loglik: -3.9647e+02 - logprior: -6.6937e+00
Fitted a model with MAP estimate = -394.3148
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 467.4494 - loglik: -4.0829e+02 - logprior: -5.9163e+01
Epoch 2/10
10/10 - 2s - loss: 421.4624 - loglik: -4.0136e+02 - logprior: -2.0098e+01
Epoch 3/10
10/10 - 2s - loss: 411.9216 - loglik: -4.0025e+02 - logprior: -1.1667e+01
Epoch 4/10
10/10 - 2s - loss: 407.3385 - loglik: -3.9965e+02 - logprior: -7.6838e+00
Epoch 5/10
10/10 - 2s - loss: 400.1739 - loglik: -3.9790e+02 - logprior: -2.2768e+00
Epoch 6/10
10/10 - 2s - loss: 391.4424 - loglik: -3.9805e+02 - logprior: 6.6103
Epoch 7/10
10/10 - 2s - loss: 386.3449 - loglik: -3.9580e+02 - logprior: 9.4525
Epoch 8/10
10/10 - 2s - loss: 385.0113 - loglik: -3.9527e+02 - logprior: 10.2573
Epoch 9/10
10/10 - 2s - loss: 385.0805 - loglik: -3.9584e+02 - logprior: 10.7591
Fitted a model with MAP estimate = -383.6462
Time for alignment: 68.8170
Computed alignments with likelihoods: ['-373.6245', '-372.4224', '-384.9562', '-380.6802', '-383.6462']
Best model has likelihood: -372.4224  (prior= 11.4717 )
time for generating output: 0.2388
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.7689601250977326
Training of 5 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff918a130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f914d55b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f9286e7f0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f80b73550>
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 239.4624 - loglik: -2.3637e+02 - logprior: -3.0973e+00
Epoch 2/10
19/19 - 2s - loss: 213.7740 - loglik: -2.1269e+02 - logprior: -1.0882e+00
Epoch 3/10
19/19 - 2s - loss: 201.2746 - loglik: -2.0007e+02 - logprior: -1.2088e+00
Epoch 4/10
19/19 - 2s - loss: 199.0136 - loglik: -1.9787e+02 - logprior: -1.1454e+00
Epoch 5/10
19/19 - 2s - loss: 198.0291 - loglik: -1.9691e+02 - logprior: -1.1150e+00
Epoch 6/10
19/19 - 2s - loss: 197.6270 - loglik: -1.9650e+02 - logprior: -1.1246e+00
Epoch 7/10
19/19 - 2s - loss: 197.2177 - loglik: -1.9607e+02 - logprior: -1.1471e+00
Epoch 8/10
19/19 - 2s - loss: 197.4456 - loglik: -1.9632e+02 - logprior: -1.1296e+00
Fitted a model with MAP estimate = -188.9990
expansions: [(0, 2), (3, 1), (4, 1), (6, 2), (16, 5), (17, 1), (18, 2), (23, 1), (34, 4), (46, 2), (47, 2), (48, 1), (52, 1), (54, 3), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 197.5608 - loglik: -1.9346e+02 - logprior: -4.0970e+00
Epoch 2/2
19/19 - 2s - loss: 188.5013 - loglik: -1.8711e+02 - logprior: -1.3872e+00
Fitted a model with MAP estimate = -180.2699
expansions: [(30, 1)]
discards: [ 0  1  2 24 25 26 49 66 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 193.9629 - loglik: -1.9021e+02 - logprior: -3.7502e+00
Epoch 2/2
19/19 - 2s - loss: 189.2725 - loglik: -1.8756e+02 - logprior: -1.7116e+00
Fitted a model with MAP estimate = -180.8924
expansions: [(0, 4), (21, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.5962 - loglik: -1.7970e+02 - logprior: -1.8999e+00
Epoch 2/10
23/23 - 2s - loss: 178.1603 - loglik: -1.7719e+02 - logprior: -9.6750e-01
Epoch 3/10
23/23 - 2s - loss: 178.0649 - loglik: -1.7712e+02 - logprior: -9.4744e-01
Epoch 4/10
23/23 - 2s - loss: 177.8024 - loglik: -1.7690e+02 - logprior: -9.0361e-01
Epoch 5/10
23/23 - 2s - loss: 177.3066 - loglik: -1.7641e+02 - logprior: -8.9432e-01
Epoch 6/10
23/23 - 2s - loss: 177.2910 - loglik: -1.7641e+02 - logprior: -8.8316e-01
Epoch 7/10
23/23 - 2s - loss: 176.5745 - loglik: -1.7570e+02 - logprior: -8.7233e-01
Epoch 8/10
23/23 - 2s - loss: 177.3664 - loglik: -1.7652e+02 - logprior: -8.4623e-01
Fitted a model with MAP estimate = -176.8510
Time for alignment: 64.6691
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 239.4606 - loglik: -2.3636e+02 - logprior: -3.1001e+00
Epoch 2/10
19/19 - 2s - loss: 213.9829 - loglik: -2.1289e+02 - logprior: -1.0925e+00
Epoch 3/10
19/19 - 2s - loss: 200.0381 - loglik: -1.9878e+02 - logprior: -1.2555e+00
Epoch 4/10
19/19 - 2s - loss: 197.5975 - loglik: -1.9638e+02 - logprior: -1.2140e+00
Epoch 5/10
19/19 - 2s - loss: 196.5461 - loglik: -1.9537e+02 - logprior: -1.1738e+00
Epoch 6/10
19/19 - 2s - loss: 196.0332 - loglik: -1.9486e+02 - logprior: -1.1710e+00
Epoch 7/10
19/19 - 2s - loss: 195.7488 - loglik: -1.9456e+02 - logprior: -1.1896e+00
Epoch 8/10
19/19 - 2s - loss: 195.8012 - loglik: -1.9462e+02 - logprior: -1.1826e+00
Fitted a model with MAP estimate = -187.4882
expansions: [(0, 2), (3, 1), (4, 1), (6, 2), (16, 6), (17, 1), (18, 2), (25, 1), (34, 2), (35, 2), (40, 1), (46, 2), (47, 2), (48, 1), (52, 1), (55, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 198.0216 - loglik: -1.9390e+02 - logprior: -4.1219e+00
Epoch 2/2
19/19 - 2s - loss: 188.7984 - loglik: -1.8736e+02 - logprior: -1.4339e+00
Fitted a model with MAP estimate = -180.2518
expansions: [(31, 1)]
discards: [ 0  1  2 26 27 50 53 68 71]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 193.7974 - loglik: -1.9005e+02 - logprior: -3.7487e+00
Epoch 2/2
19/19 - 2s - loss: 189.0566 - loglik: -1.8738e+02 - logprior: -1.6801e+00
Fitted a model with MAP estimate = -180.7179
expansions: [(0, 4), (75, 1)]
discards: [ 0 19 20 21 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 182.2521 - loglik: -1.8036e+02 - logprior: -1.8897e+00
Epoch 2/10
23/23 - 2s - loss: 179.6861 - loglik: -1.7874e+02 - logprior: -9.4975e-01
Epoch 3/10
23/23 - 2s - loss: 179.0383 - loglik: -1.7811e+02 - logprior: -9.2526e-01
Epoch 4/10
23/23 - 2s - loss: 178.7090 - loglik: -1.7782e+02 - logprior: -8.8742e-01
Epoch 5/10
23/23 - 2s - loss: 178.7611 - loglik: -1.7788e+02 - logprior: -8.7657e-01
Fitted a model with MAP estimate = -178.4241
Time for alignment: 56.7043
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 239.2188 - loglik: -2.3612e+02 - logprior: -3.0966e+00
Epoch 2/10
19/19 - 2s - loss: 214.0089 - loglik: -2.1293e+02 - logprior: -1.0835e+00
Epoch 3/10
19/19 - 2s - loss: 200.2703 - loglik: -1.9909e+02 - logprior: -1.1844e+00
Epoch 4/10
19/19 - 2s - loss: 197.8341 - loglik: -1.9677e+02 - logprior: -1.0630e+00
Epoch 5/10
19/19 - 2s - loss: 196.5822 - loglik: -1.9559e+02 - logprior: -9.8912e-01
Epoch 6/10
19/19 - 2s - loss: 196.0789 - loglik: -1.9511e+02 - logprior: -9.6540e-01
Epoch 7/10
19/19 - 2s - loss: 195.5225 - loglik: -1.9455e+02 - logprior: -9.7147e-01
Epoch 8/10
19/19 - 2s - loss: 195.2404 - loglik: -1.9428e+02 - logprior: -9.5917e-01
Epoch 9/10
19/19 - 2s - loss: 195.4483 - loglik: -1.9451e+02 - logprior: -9.4116e-01
Fitted a model with MAP estimate = -187.3886
expansions: [(0, 4), (17, 5), (18, 2), (32, 1), (34, 2), (35, 2), (40, 1), (46, 2), (47, 2), (48, 1), (52, 1), (54, 3), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 198.3267 - loglik: -1.9414e+02 - logprior: -4.1854e+00
Epoch 2/2
19/19 - 2s - loss: 189.0592 - loglik: -1.8778e+02 - logprior: -1.2777e+00
Fitted a model with MAP estimate = -180.6093
expansions: [(0, 3), (20, 1), (27, 1)]
discards: [ 2  3 21 22 23 24 25 46 48 64 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 193.2151 - loglik: -1.8928e+02 - logprior: -3.9378e+00
Epoch 2/2
19/19 - 2s - loss: 188.4736 - loglik: -1.8713e+02 - logprior: -1.3423e+00
Fitted a model with MAP estimate = -180.6054
expansions: [(23, 4)]
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 181.5639 - loglik: -1.7980e+02 - logprior: -1.7607e+00
Epoch 2/10
23/23 - 2s - loss: 179.1711 - loglik: -1.7840e+02 - logprior: -7.7035e-01
Epoch 3/10
23/23 - 2s - loss: 178.6324 - loglik: -1.7790e+02 - logprior: -7.2950e-01
Epoch 4/10
23/23 - 2s - loss: 178.3903 - loglik: -1.7771e+02 - logprior: -6.7912e-01
Epoch 5/10
23/23 - 2s - loss: 178.1724 - loglik: -1.7752e+02 - logprior: -6.5651e-01
Epoch 6/10
23/23 - 2s - loss: 177.7413 - loglik: -1.7710e+02 - logprior: -6.3703e-01
Epoch 7/10
23/23 - 2s - loss: 178.0866 - loglik: -1.7746e+02 - logprior: -6.2276e-01
Fitted a model with MAP estimate = -177.6987
Time for alignment: 63.4663
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 239.4716 - loglik: -2.3637e+02 - logprior: -3.0989e+00
Epoch 2/10
19/19 - 2s - loss: 214.1062 - loglik: -2.1300e+02 - logprior: -1.1106e+00
Epoch 3/10
19/19 - 2s - loss: 199.5637 - loglik: -1.9830e+02 - logprior: -1.2678e+00
Epoch 4/10
19/19 - 2s - loss: 196.6819 - loglik: -1.9549e+02 - logprior: -1.1870e+00
Epoch 5/10
19/19 - 2s - loss: 195.7266 - loglik: -1.9454e+02 - logprior: -1.1893e+00
Epoch 6/10
19/19 - 2s - loss: 195.2407 - loglik: -1.9405e+02 - logprior: -1.1860e+00
Epoch 7/10
19/19 - 2s - loss: 194.7738 - loglik: -1.9357e+02 - logprior: -1.1989e+00
Epoch 8/10
19/19 - 2s - loss: 194.9298 - loglik: -1.9374e+02 - logprior: -1.1900e+00
Fitted a model with MAP estimate = -186.6782
expansions: [(0, 2), (3, 1), (4, 1), (6, 2), (16, 1), (17, 5), (18, 2), (25, 1), (32, 1), (33, 2), (34, 2), (46, 2), (47, 2), (48, 1), (52, 2), (55, 2), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 197.7688 - loglik: -1.9363e+02 - logprior: -4.1425e+00
Epoch 2/2
19/19 - 2s - loss: 188.3444 - loglik: -1.8687e+02 - logprior: -1.4774e+00
Fitted a model with MAP estimate = -179.8831
expansions: []
discards: [ 0  1  2 12 25 26 27 28 29 49 51 67 70 78]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 80 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 194.7328 - loglik: -1.9093e+02 - logprior: -3.8027e+00
Epoch 2/2
19/19 - 2s - loss: 189.9564 - loglik: -1.8825e+02 - logprior: -1.7044e+00
Fitted a model with MAP estimate = -181.1800
expansions: [(0, 4), (23, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.7448 - loglik: -1.7983e+02 - logprior: -1.9171e+00
Epoch 2/10
23/23 - 2s - loss: 179.0749 - loglik: -1.7807e+02 - logprior: -1.0094e+00
Epoch 3/10
23/23 - 2s - loss: 179.1954 - loglik: -1.7823e+02 - logprior: -9.6214e-01
Fitted a model with MAP estimate = -178.5765
Time for alignment: 51.8320
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 239.4091 - loglik: -2.3631e+02 - logprior: -3.0962e+00
Epoch 2/10
19/19 - 2s - loss: 213.6109 - loglik: -2.1252e+02 - logprior: -1.0952e+00
Epoch 3/10
19/19 - 2s - loss: 200.7030 - loglik: -1.9948e+02 - logprior: -1.2199e+00
Epoch 4/10
19/19 - 2s - loss: 198.2208 - loglik: -1.9709e+02 - logprior: -1.1292e+00
Epoch 5/10
19/19 - 2s - loss: 197.7341 - loglik: -1.9660e+02 - logprior: -1.1292e+00
Epoch 6/10
19/19 - 2s - loss: 197.1772 - loglik: -1.9605e+02 - logprior: -1.1262e+00
Epoch 7/10
19/19 - 2s - loss: 196.8248 - loglik: -1.9568e+02 - logprior: -1.1398e+00
Epoch 8/10
19/19 - 2s - loss: 196.4774 - loglik: -1.9534e+02 - logprior: -1.1380e+00
Epoch 9/10
19/19 - 2s - loss: 196.5919 - loglik: -1.9547e+02 - logprior: -1.1247e+00
Fitted a model with MAP estimate = -188.5234
expansions: [(0, 2), (3, 1), (4, 1), (6, 2), (16, 1), (17, 5), (18, 2), (32, 1), (34, 4), (46, 2), (47, 2), (48, 1), (52, 1), (54, 3), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 196.8544 - loglik: -1.9272e+02 - logprior: -4.1310e+00
Epoch 2/2
19/19 - 2s - loss: 187.5761 - loglik: -1.8616e+02 - logprior: -1.4132e+00
Fitted a model with MAP estimate = -179.3982
expansions: []
discards: [ 0  1  2 12 25 26 27 28 29 49 65 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 194.0818 - loglik: -1.9029e+02 - logprior: -3.7925e+00
Epoch 2/2
19/19 - 2s - loss: 189.5820 - loglik: -1.8788e+02 - logprior: -1.7049e+00
Fitted a model with MAP estimate = -181.0322
expansions: [(0, 4), (23, 2)]
discards: [0 7]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.7293 - loglik: -1.7980e+02 - logprior: -1.9297e+00
Epoch 2/10
23/23 - 2s - loss: 179.4390 - loglik: -1.7843e+02 - logprior: -1.0124e+00
Epoch 3/10
23/23 - 2s - loss: 178.6200 - loglik: -1.7764e+02 - logprior: -9.8337e-01
Epoch 4/10
23/23 - 2s - loss: 178.4514 - loglik: -1.7752e+02 - logprior: -9.3505e-01
Epoch 5/10
23/23 - 2s - loss: 178.1758 - loglik: -1.7724e+02 - logprior: -9.3375e-01
Epoch 6/10
23/23 - 2s - loss: 178.3771 - loglik: -1.7746e+02 - logprior: -9.1228e-01
Fitted a model with MAP estimate = -177.9667
Time for alignment: 59.4318
Computed alignments with likelihoods: ['-176.8510', '-178.4241', '-177.6987', '-178.5765', '-177.9667']
Best model has likelihood: -176.8510  (prior= -0.8546 )
time for generating output: 0.1322
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.8172043010752689
Training of 5 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa3315670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f809c2280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fbd65fc40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f91a04160>
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 806.2208 - loglik: -8.0046e+02 - logprior: -5.7587e+00
Epoch 2/10
22/22 - 12s - loss: 703.8462 - loglik: -7.0344e+02 - logprior: -4.0618e-01
Epoch 3/10
22/22 - 12s - loss: 657.0911 - loglik: -6.5539e+02 - logprior: -1.7053e+00
Epoch 4/10
22/22 - 12s - loss: 646.5303 - loglik: -6.4481e+02 - logprior: -1.7230e+00
Epoch 5/10
22/22 - 12s - loss: 645.0779 - loglik: -6.4338e+02 - logprior: -1.7003e+00
Epoch 6/10
22/22 - 12s - loss: 645.4278 - loglik: -6.4377e+02 - logprior: -1.6614e+00
Fitted a model with MAP estimate = -642.4964
expansions: [(12, 1), (13, 1), (14, 1), (33, 1), (36, 1), (48, 1), (49, 2), (51, 1), (52, 1), (53, 1), (58, 1), (67, 1), (71, 1), (72, 2), (73, 1), (78, 1), (79, 1), (80, 2), (98, 2), (100, 1), (106, 2), (107, 1), (108, 3), (119, 2), (133, 1), (134, 1), (141, 1), (147, 1), (151, 1), (154, 6), (156, 1), (179, 5), (182, 2), (183, 2), (184, 1), (192, 3), (204, 1), (205, 1), (208, 1), (211, 1), (214, 1), (225, 1), (235, 1), (236, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 317 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 648.3180 - loglik: -6.3981e+02 - logprior: -8.5059e+00
Epoch 2/2
22/22 - 17s - loss: 626.4378 - loglik: -6.2407e+02 - logprior: -2.3718e+00
Fitted a model with MAP estimate = -622.3956
expansions: [(0, 3)]
discards: [  0  35  36  54 195 224 225 231 233]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 311 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 631.1176 - loglik: -6.2606e+02 - logprior: -5.0540e+00
Epoch 2/2
22/22 - 17s - loss: 618.1787 - loglik: -6.1906e+02 - logprior: 0.8820
Fitted a model with MAP estimate = -618.3162
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 632.7068 - loglik: -6.2510e+02 - logprior: -7.6086e+00
Epoch 2/10
22/22 - 16s - loss: 627.9788 - loglik: -6.2631e+02 - logprior: -1.6699e+00
Epoch 3/10
22/22 - 16s - loss: 617.6695 - loglik: -6.1752e+02 - logprior: -1.4975e-01
Epoch 4/10
22/22 - 16s - loss: 618.3503 - loglik: -6.2063e+02 - logprior: 2.2826
Fitted a model with MAP estimate = -616.0729
Time for alignment: 265.9541
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 805.8893 - loglik: -8.0009e+02 - logprior: -5.7944e+00
Epoch 2/10
22/22 - 12s - loss: 701.3167 - loglik: -7.0087e+02 - logprior: -4.4458e-01
Epoch 3/10
22/22 - 12s - loss: 653.9629 - loglik: -6.5222e+02 - logprior: -1.7399e+00
Epoch 4/10
22/22 - 12s - loss: 644.4645 - loglik: -6.4281e+02 - logprior: -1.6584e+00
Epoch 5/10
22/22 - 12s - loss: 644.4455 - loglik: -6.4286e+02 - logprior: -1.5822e+00
Epoch 6/10
22/22 - 12s - loss: 642.3266 - loglik: -6.4082e+02 - logprior: -1.5088e+00
Epoch 7/10
22/22 - 12s - loss: 639.8785 - loglik: -6.3838e+02 - logprior: -1.4968e+00
Epoch 8/10
22/22 - 12s - loss: 641.9200 - loglik: -6.4039e+02 - logprior: -1.5316e+00
Fitted a model with MAP estimate = -639.6016
expansions: [(14, 1), (15, 1), (32, 2), (33, 1), (36, 1), (48, 1), (49, 2), (51, 1), (52, 1), (53, 1), (58, 1), (67, 1), (70, 2), (71, 1), (73, 1), (78, 1), (79, 1), (80, 2), (98, 2), (104, 1), (106, 3), (107, 1), (108, 1), (120, 2), (131, 1), (135, 1), (136, 1), (142, 1), (147, 1), (149, 1), (155, 3), (183, 4), (184, 6), (192, 3), (204, 1), (205, 1), (206, 1), (211, 1), (225, 1), (236, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 648.6083 - loglik: -6.4017e+02 - logprior: -8.4413e+00
Epoch 2/2
22/22 - 16s - loss: 626.0602 - loglik: -6.2378e+02 - logprior: -2.2819e+00
Fitted a model with MAP estimate = -622.9086
expansions: [(0, 3)]
discards: [  0  34  36  37  55 225 226]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 19s - loss: 631.1724 - loglik: -6.2624e+02 - logprior: -4.9275e+00
Epoch 2/2
22/22 - 16s - loss: 620.7356 - loglik: -6.2173e+02 - logprior: 0.9933
Fitted a model with MAP estimate = -619.1193
expansions: [(36, 2), (277, 2)]
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 309 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 632.8994 - loglik: -6.2540e+02 - logprior: -7.5043e+00
Epoch 2/10
22/22 - 16s - loss: 623.8313 - loglik: -6.2223e+02 - logprior: -1.6037e+00
Epoch 3/10
22/22 - 16s - loss: 621.6995 - loglik: -6.2137e+02 - logprior: -3.2972e-01
Epoch 4/10
22/22 - 16s - loss: 617.6627 - loglik: -6.1983e+02 - logprior: 2.1646
Epoch 5/10
22/22 - 16s - loss: 613.7260 - loglik: -6.1620e+02 - logprior: 2.4775
Epoch 6/10
22/22 - 16s - loss: 617.3899 - loglik: -6.1997e+02 - logprior: 2.5756
Fitted a model with MAP estimate = -614.1703
Time for alignment: 322.7002
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 805.2326 - loglik: -7.9948e+02 - logprior: -5.7564e+00
Epoch 2/10
22/22 - 12s - loss: 703.1024 - loglik: -7.0266e+02 - logprior: -4.3761e-01
Epoch 3/10
22/22 - 12s - loss: 653.1140 - loglik: -6.5137e+02 - logprior: -1.7472e+00
Epoch 4/10
22/22 - 12s - loss: 644.9188 - loglik: -6.4320e+02 - logprior: -1.7191e+00
Epoch 5/10
22/22 - 12s - loss: 642.4775 - loglik: -6.4084e+02 - logprior: -1.6367e+00
Epoch 6/10
22/22 - 12s - loss: 642.6220 - loglik: -6.4098e+02 - logprior: -1.6446e+00
Fitted a model with MAP estimate = -641.2922
expansions: [(14, 1), (15, 1), (32, 2), (33, 1), (36, 1), (48, 1), (49, 2), (51, 1), (52, 1), (53, 1), (61, 1), (67, 1), (71, 1), (72, 2), (73, 1), (78, 1), (79, 1), (80, 1), (82, 1), (84, 1), (98, 1), (104, 1), (106, 2), (107, 1), (108, 1), (109, 1), (120, 2), (131, 1), (135, 1), (142, 1), (143, 1), (147, 1), (154, 6), (177, 4), (183, 3), (191, 1), (192, 1), (204, 1), (205, 1), (206, 1), (207, 1), (225, 1), (236, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 19s - loss: 649.5137 - loglik: -6.4089e+02 - logprior: -8.6189e+00
Epoch 2/2
22/22 - 16s - loss: 625.5950 - loglik: -6.2317e+02 - logprior: -2.4289e+00
Fitted a model with MAP estimate = -622.7077
expansions: [(0, 3), (224, 1), (279, 2)]
discards: [ 0 34 36 37 55]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 311 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 631.5192 - loglik: -6.2649e+02 - logprior: -5.0336e+00
Epoch 2/2
22/22 - 16s - loss: 618.3853 - loglik: -6.1923e+02 - logprior: 0.8445
Fitted a model with MAP estimate = -617.9993
expansions: []
discards: [  0   1   2 279]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 307 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 635.0947 - loglik: -6.2742e+02 - logprior: -7.6717e+00
Epoch 2/10
22/22 - 16s - loss: 623.5647 - loglik: -6.2191e+02 - logprior: -1.6594e+00
Epoch 3/10
22/22 - 16s - loss: 620.5842 - loglik: -6.2042e+02 - logprior: -1.6691e-01
Epoch 4/10
22/22 - 16s - loss: 616.2626 - loglik: -6.1849e+02 - logprior: 2.2234
Epoch 5/10
22/22 - 16s - loss: 618.2709 - loglik: -6.2062e+02 - logprior: 2.3536
Fitted a model with MAP estimate = -615.1081
Time for alignment: 280.3884
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 805.3708 - loglik: -7.9962e+02 - logprior: -5.7469e+00
Epoch 2/10
22/22 - 12s - loss: 699.7815 - loglik: -6.9933e+02 - logprior: -4.5180e-01
Epoch 3/10
22/22 - 12s - loss: 654.5228 - loglik: -6.5281e+02 - logprior: -1.7125e+00
Epoch 4/10
22/22 - 12s - loss: 645.4722 - loglik: -6.4386e+02 - logprior: -1.6150e+00
Epoch 5/10
22/22 - 12s - loss: 645.7644 - loglik: -6.4417e+02 - logprior: -1.5994e+00
Fitted a model with MAP estimate = -642.6834
expansions: [(14, 1), (15, 1), (32, 2), (36, 1), (48, 2), (51, 1), (52, 1), (53, 1), (58, 1), (63, 1), (71, 1), (72, 2), (73, 1), (78, 1), (79, 1), (80, 2), (95, 1), (98, 1), (100, 1), (106, 2), (107, 1), (108, 1), (109, 1), (120, 2), (131, 1), (135, 1), (142, 1), (143, 2), (155, 6), (156, 1), (180, 4), (183, 3), (184, 2), (192, 1), (193, 1), (204, 3), (205, 1), (206, 1), (211, 1), (225, 1), (235, 1), (236, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 650.2360 - loglik: -6.4148e+02 - logprior: -8.7557e+00
Epoch 2/2
22/22 - 17s - loss: 625.5585 - loglik: -6.2300e+02 - logprior: -2.5614e+00
Fitted a model with MAP estimate = -622.4011
expansions: [(0, 3)]
discards: [  0  34  36  98 175 193 224 231 257]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 631.3115 - loglik: -6.2623e+02 - logprior: -5.0841e+00
Epoch 2/2
22/22 - 16s - loss: 620.6811 - loglik: -6.2153e+02 - logprior: 0.8457
Fitted a model with MAP estimate = -619.1629
expansions: [(36, 2)]
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 307 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 633.1894 - loglik: -6.2548e+02 - logprior: -7.7057e+00
Epoch 2/10
22/22 - 16s - loss: 625.3231 - loglik: -6.2359e+02 - logprior: -1.7303e+00
Epoch 3/10
22/22 - 16s - loss: 620.6595 - loglik: -6.2059e+02 - logprior: -7.1932e-02
Epoch 4/10
22/22 - 16s - loss: 616.5082 - loglik: -6.1871e+02 - logprior: 2.1968
Epoch 5/10
22/22 - 16s - loss: 619.4026 - loglik: -6.2176e+02 - logprior: 2.3573
Fitted a model with MAP estimate = -615.5192
Time for alignment: 267.8355
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 807.3329 - loglik: -8.0156e+02 - logprior: -5.7747e+00
Epoch 2/10
22/22 - 12s - loss: 700.9210 - loglik: -7.0052e+02 - logprior: -4.0369e-01
Epoch 3/10
22/22 - 12s - loss: 651.4278 - loglik: -6.4977e+02 - logprior: -1.6610e+00
Epoch 4/10
22/22 - 12s - loss: 646.1753 - loglik: -6.4452e+02 - logprior: -1.6543e+00
Epoch 5/10
22/22 - 12s - loss: 643.4601 - loglik: -6.4190e+02 - logprior: -1.5590e+00
Epoch 6/10
22/22 - 12s - loss: 641.9014 - loglik: -6.4037e+02 - logprior: -1.5295e+00
Epoch 7/10
22/22 - 12s - loss: 641.4734 - loglik: -6.3996e+02 - logprior: -1.5133e+00
Epoch 8/10
22/22 - 12s - loss: 642.2160 - loglik: -6.4075e+02 - logprior: -1.4704e+00
Fitted a model with MAP estimate = -640.6314
expansions: [(14, 1), (15, 1), (32, 2), (33, 1), (36, 1), (48, 3), (50, 1), (51, 1), (52, 1), (67, 1), (70, 1), (71, 1), (72, 1), (73, 1), (78, 1), (79, 1), (80, 2), (98, 2), (107, 2), (108, 3), (109, 1), (120, 2), (131, 1), (135, 1), (142, 1), (143, 1), (147, 1), (148, 1), (154, 6), (155, 1), (179, 5), (182, 2), (183, 2), (192, 2), (193, 1), (204, 1), (205, 1), (211, 1), (225, 1), (235, 1), (236, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 647.7681 - loglik: -6.3925e+02 - logprior: -8.5149e+00
Epoch 2/2
22/22 - 17s - loss: 624.0107 - loglik: -6.2159e+02 - logprior: -2.4180e+00
Fitted a model with MAP estimate = -622.8007
expansions: [(0, 3), (263, 1)]
discards: [  0  34  36  37 194 195 224 226]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 19s - loss: 631.5436 - loglik: -6.2655e+02 - logprior: -4.9937e+00
Epoch 2/2
22/22 - 16s - loss: 618.2849 - loglik: -6.1924e+02 - logprior: 0.9540
Fitted a model with MAP estimate = -618.3395
expansions: [(36, 2)]
discards: [  0   1   2  98 192 226]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 306 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 634.4183 - loglik: -6.2690e+02 - logprior: -7.5181e+00
Epoch 2/10
22/22 - 16s - loss: 623.6845 - loglik: -6.2212e+02 - logprior: -1.5596e+00
Epoch 3/10
22/22 - 16s - loss: 619.7183 - loglik: -6.1948e+02 - logprior: -2.4235e-01
Epoch 4/10
22/22 - 16s - loss: 619.7667 - loglik: -6.2206e+02 - logprior: 2.2891
Fitted a model with MAP estimate = -616.3311
Time for alignment: 287.9639
Computed alignments with likelihoods: ['-616.0729', '-614.1703', '-615.1081', '-615.5192', '-616.3311']
Best model has likelihood: -614.1703  (prior= 2.7211 )
time for generating output: 0.3331
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.9202221864434068
Training of 5 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f7fc2de50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e6d1e7e20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe0062790>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f80a8fe50>
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.9104 - loglik: -8.3571e+01 - logprior: -9.2339e+01
Epoch 2/10
10/10 - 0s - loss: 97.6018 - loglik: -7.1178e+01 - logprior: -2.6424e+01
Epoch 3/10
10/10 - 1s - loss: 76.6610 - loglik: -6.3640e+01 - logprior: -1.3021e+01
Epoch 4/10
10/10 - 1s - loss: 67.6651 - loglik: -5.9843e+01 - logprior: -7.8221e+00
Epoch 5/10
10/10 - 1s - loss: 62.0151 - loglik: -5.6901e+01 - logprior: -5.1144e+00
Epoch 6/10
10/10 - 1s - loss: 59.3206 - loglik: -5.5627e+01 - logprior: -3.6939e+00
Epoch 7/10
10/10 - 1s - loss: 57.9313 - loglik: -5.5035e+01 - logprior: -2.8966e+00
Epoch 8/10
10/10 - 1s - loss: 57.0216 - loglik: -5.4640e+01 - logprior: -2.3821e+00
Epoch 9/10
10/10 - 1s - loss: 56.3305 - loglik: -5.4337e+01 - logprior: -1.9931e+00
Epoch 10/10
10/10 - 1s - loss: 55.9209 - loglik: -5.4249e+01 - logprior: -1.6720e+00
Fitted a model with MAP estimate = -55.7598
expansions: [(0, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 176.3680 - loglik: -5.3309e+01 - logprior: -1.2306e+02
Epoch 2/2
10/10 - 1s - loss: 90.1271 - loglik: -5.0190e+01 - logprior: -3.9937e+01
Fitted a model with MAP estimate = -73.5448
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 135.2160 - loglik: -4.8375e+01 - logprior: -8.6841e+01
Epoch 2/10
10/10 - 1s - loss: 72.0699 - loglik: -4.7115e+01 - logprior: -2.4955e+01
Epoch 3/10
10/10 - 1s - loss: 58.6977 - loglik: -4.6446e+01 - logprior: -1.2251e+01
Epoch 4/10
10/10 - 1s - loss: 53.5543 - loglik: -4.6535e+01 - logprior: -7.0190e+00
Epoch 5/10
10/10 - 1s - loss: 50.4798 - loglik: -4.6327e+01 - logprior: -4.1527e+00
Epoch 6/10
10/10 - 1s - loss: 48.3188 - loglik: -4.5706e+01 - logprior: -2.6127e+00
Epoch 7/10
10/10 - 1s - loss: 47.0000 - loglik: -4.5100e+01 - logprior: -1.9003e+00
Epoch 8/10
10/10 - 1s - loss: 46.2762 - loglik: -4.4694e+01 - logprior: -1.5821e+00
Epoch 9/10
10/10 - 1s - loss: 45.8189 - loglik: -4.4623e+01 - logprior: -1.1962e+00
Epoch 10/10
10/10 - 1s - loss: 45.4672 - loglik: -4.4745e+01 - logprior: -7.2260e-01
Fitted a model with MAP estimate = -45.3112
Time for alignment: 23.9850
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.9104 - loglik: -8.3571e+01 - logprior: -9.2339e+01
Epoch 2/10
10/10 - 1s - loss: 97.6019 - loglik: -7.1178e+01 - logprior: -2.6423e+01
Epoch 3/10
10/10 - 1s - loss: 76.6610 - loglik: -6.3640e+01 - logprior: -1.3021e+01
Epoch 4/10
10/10 - 1s - loss: 67.6654 - loglik: -5.9843e+01 - logprior: -7.8221e+00
Epoch 5/10
10/10 - 1s - loss: 62.0149 - loglik: -5.6900e+01 - logprior: -5.1144e+00
Epoch 6/10
10/10 - 1s - loss: 59.3204 - loglik: -5.5627e+01 - logprior: -3.6939e+00
Epoch 7/10
10/10 - 1s - loss: 57.9306 - loglik: -5.5034e+01 - logprior: -2.8966e+00
Epoch 8/10
10/10 - 1s - loss: 57.0216 - loglik: -5.4639e+01 - logprior: -2.3821e+00
Epoch 9/10
10/10 - 1s - loss: 56.3302 - loglik: -5.4337e+01 - logprior: -1.9931e+00
Epoch 10/10
10/10 - 1s - loss: 55.9203 - loglik: -5.4248e+01 - logprior: -1.6721e+00
Fitted a model with MAP estimate = -55.7599
expansions: [(0, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.3678 - loglik: -5.3309e+01 - logprior: -1.2306e+02
Epoch 2/2
10/10 - 1s - loss: 90.1266 - loglik: -5.0190e+01 - logprior: -3.9937e+01
Fitted a model with MAP estimate = -73.5449
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 135.2154 - loglik: -4.8375e+01 - logprior: -8.6841e+01
Epoch 2/10
10/10 - 1s - loss: 72.0691 - loglik: -4.7114e+01 - logprior: -2.4955e+01
Epoch 3/10
10/10 - 1s - loss: 58.6974 - loglik: -4.6446e+01 - logprior: -1.2251e+01
Epoch 4/10
10/10 - 1s - loss: 53.5534 - loglik: -4.6535e+01 - logprior: -7.0188e+00
Epoch 5/10
10/10 - 1s - loss: 50.4773 - loglik: -4.6325e+01 - logprior: -4.1526e+00
Epoch 6/10
10/10 - 1s - loss: 48.3169 - loglik: -4.5704e+01 - logprior: -2.6131e+00
Epoch 7/10
10/10 - 1s - loss: 46.9991 - loglik: -4.5098e+01 - logprior: -1.9010e+00
Epoch 8/10
10/10 - 1s - loss: 46.2760 - loglik: -4.4694e+01 - logprior: -1.5823e+00
Epoch 9/10
10/10 - 1s - loss: 45.8188 - loglik: -4.4623e+01 - logprior: -1.1958e+00
Epoch 10/10
10/10 - 1s - loss: 45.4670 - loglik: -4.4745e+01 - logprior: -7.2208e-01
Fitted a model with MAP estimate = -45.3113
Time for alignment: 22.7596
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.9103 - loglik: -8.3571e+01 - logprior: -9.2339e+01
Epoch 2/10
10/10 - 1s - loss: 97.6015 - loglik: -7.1178e+01 - logprior: -2.6424e+01
Epoch 3/10
10/10 - 1s - loss: 76.6603 - loglik: -6.3639e+01 - logprior: -1.3021e+01
Epoch 4/10
10/10 - 1s - loss: 67.6639 - loglik: -5.9842e+01 - logprior: -7.8222e+00
Epoch 5/10
10/10 - 1s - loss: 62.0136 - loglik: -5.6899e+01 - logprior: -5.1145e+00
Epoch 6/10
10/10 - 1s - loss: 59.3198 - loglik: -5.5626e+01 - logprior: -3.6938e+00
Epoch 7/10
10/10 - 1s - loss: 57.9304 - loglik: -5.5034e+01 - logprior: -2.8966e+00
Epoch 8/10
10/10 - 1s - loss: 57.0219 - loglik: -5.4640e+01 - logprior: -2.3821e+00
Epoch 9/10
10/10 - 1s - loss: 56.3307 - loglik: -5.4338e+01 - logprior: -1.9932e+00
Epoch 10/10
10/10 - 1s - loss: 55.9204 - loglik: -5.4248e+01 - logprior: -1.6720e+00
Fitted a model with MAP estimate = -55.7605
expansions: [(0, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 176.3679 - loglik: -5.3309e+01 - logprior: -1.2306e+02
Epoch 2/2
10/10 - 1s - loss: 90.1269 - loglik: -5.0190e+01 - logprior: -3.9937e+01
Fitted a model with MAP estimate = -73.5447
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 135.2151 - loglik: -4.8375e+01 - logprior: -8.6841e+01
Epoch 2/10
10/10 - 1s - loss: 72.0683 - loglik: -4.7113e+01 - logprior: -2.4955e+01
Epoch 3/10
10/10 - 1s - loss: 58.6971 - loglik: -4.6446e+01 - logprior: -1.2251e+01
Epoch 4/10
10/10 - 1s - loss: 53.5529 - loglik: -4.6534e+01 - logprior: -7.0187e+00
Epoch 5/10
10/10 - 1s - loss: 50.4762 - loglik: -4.6324e+01 - logprior: -4.1525e+00
Epoch 6/10
10/10 - 1s - loss: 48.3159 - loglik: -4.5703e+01 - logprior: -2.6133e+00
Epoch 7/10
10/10 - 1s - loss: 46.9986 - loglik: -4.5097e+01 - logprior: -1.9015e+00
Epoch 8/10
10/10 - 1s - loss: 46.2759 - loglik: -4.4693e+01 - logprior: -1.5826e+00
Epoch 9/10
10/10 - 1s - loss: 45.8186 - loglik: -4.4623e+01 - logprior: -1.1956e+00
Epoch 10/10
10/10 - 1s - loss: 45.4669 - loglik: -4.4745e+01 - logprior: -7.2190e-01
Fitted a model with MAP estimate = -45.3113
Time for alignment: 23.1848
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.9104 - loglik: -8.3571e+01 - logprior: -9.2339e+01
Epoch 2/10
10/10 - 1s - loss: 97.6018 - loglik: -7.1178e+01 - logprior: -2.6424e+01
Epoch 3/10
10/10 - 1s - loss: 76.6602 - loglik: -6.3639e+01 - logprior: -1.3021e+01
Epoch 4/10
10/10 - 1s - loss: 67.6627 - loglik: -5.9840e+01 - logprior: -7.8226e+00
Epoch 5/10
10/10 - 1s - loss: 62.0147 - loglik: -5.6900e+01 - logprior: -5.1145e+00
Epoch 6/10
10/10 - 1s - loss: 59.3215 - loglik: -5.5627e+01 - logprior: -3.6941e+00
Epoch 7/10
10/10 - 1s - loss: 57.9316 - loglik: -5.5035e+01 - logprior: -2.8967e+00
Epoch 8/10
10/10 - 1s - loss: 57.0231 - loglik: -5.4641e+01 - logprior: -2.3822e+00
Epoch 9/10
10/10 - 1s - loss: 56.3315 - loglik: -5.4338e+01 - logprior: -1.9933e+00
Epoch 10/10
10/10 - 1s - loss: 55.9209 - loglik: -5.4249e+01 - logprior: -1.6720e+00
Fitted a model with MAP estimate = -55.7602
expansions: [(0, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 176.3680 - loglik: -5.3308e+01 - logprior: -1.2306e+02
Epoch 2/2
10/10 - 1s - loss: 90.1267 - loglik: -5.0189e+01 - logprior: -3.9937e+01
Fitted a model with MAP estimate = -73.5444
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 135.2171 - loglik: -4.8377e+01 - logprior: -8.6841e+01
Epoch 2/10
10/10 - 1s - loss: 72.0731 - loglik: -4.7118e+01 - logprior: -2.4955e+01
Epoch 3/10
10/10 - 1s - loss: 58.6983 - loglik: -4.6447e+01 - logprior: -1.2251e+01
Epoch 4/10
10/10 - 1s - loss: 53.5528 - loglik: -4.6534e+01 - logprior: -7.0188e+00
Epoch 5/10
10/10 - 1s - loss: 50.4761 - loglik: -4.6324e+01 - logprior: -4.1525e+00
Epoch 6/10
10/10 - 1s - loss: 48.3152 - loglik: -4.5702e+01 - logprior: -2.6133e+00
Epoch 7/10
10/10 - 1s - loss: 46.9972 - loglik: -4.5096e+01 - logprior: -1.9016e+00
Epoch 8/10
10/10 - 1s - loss: 46.2745 - loglik: -4.4692e+01 - logprior: -1.5829e+00
Epoch 9/10
10/10 - 1s - loss: 45.8174 - loglik: -4.4622e+01 - logprior: -1.1958e+00
Epoch 10/10
10/10 - 1s - loss: 45.4663 - loglik: -4.4744e+01 - logprior: -7.2202e-01
Fitted a model with MAP estimate = -45.3107
Time for alignment: 22.6780
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.9104 - loglik: -8.3571e+01 - logprior: -9.2339e+01
Epoch 2/10
10/10 - 1s - loss: 97.6017 - loglik: -7.1178e+01 - logprior: -2.6424e+01
Epoch 3/10
10/10 - 1s - loss: 76.6600 - loglik: -6.3639e+01 - logprior: -1.3021e+01
Epoch 4/10
10/10 - 1s - loss: 67.6621 - loglik: -5.9840e+01 - logprior: -7.8225e+00
Epoch 5/10
10/10 - 1s - loss: 62.0139 - loglik: -5.6899e+01 - logprior: -5.1145e+00
Epoch 6/10
10/10 - 1s - loss: 59.3208 - loglik: -5.5627e+01 - logprior: -3.6941e+00
Epoch 7/10
10/10 - 1s - loss: 57.9313 - loglik: -5.5035e+01 - logprior: -2.8966e+00
Epoch 8/10
10/10 - 1s - loss: 57.0238 - loglik: -5.4642e+01 - logprior: -2.3821e+00
Epoch 9/10
10/10 - 1s - loss: 56.3317 - loglik: -5.4338e+01 - logprior: -1.9935e+00
Epoch 10/10
10/10 - 1s - loss: 55.9206 - loglik: -5.4249e+01 - logprior: -1.6721e+00
Fitted a model with MAP estimate = -55.7615
expansions: [(0, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 176.3686 - loglik: -5.3309e+01 - logprior: -1.2306e+02
Epoch 2/2
10/10 - 1s - loss: 90.1271 - loglik: -5.0191e+01 - logprior: -3.9937e+01
Fitted a model with MAP estimate = -73.5455
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 135.2160 - loglik: -4.8375e+01 - logprior: -8.6841e+01
Epoch 2/10
10/10 - 1s - loss: 72.0698 - loglik: -4.7114e+01 - logprior: -2.4956e+01
Epoch 3/10
10/10 - 1s - loss: 58.6979 - loglik: -4.6446e+01 - logprior: -1.2251e+01
Epoch 4/10
10/10 - 1s - loss: 53.5561 - loglik: -4.6537e+01 - logprior: -7.0194e+00
Epoch 5/10
10/10 - 1s - loss: 50.4841 - loglik: -4.6331e+01 - logprior: -4.1531e+00
Epoch 6/10
10/10 - 1s - loss: 48.3222 - loglik: -4.5710e+01 - logprior: -2.6122e+00
Epoch 7/10
10/10 - 1s - loss: 47.0018 - loglik: -4.5103e+01 - logprior: -1.8992e+00
Epoch 8/10
10/10 - 0s - loss: 46.2771 - loglik: -4.4696e+01 - logprior: -1.5815e+00
Epoch 9/10
10/10 - 1s - loss: 45.8196 - loglik: -4.4623e+01 - logprior: -1.1970e+00
Epoch 10/10
10/10 - 1s - loss: 45.4678 - loglik: -4.4744e+01 - logprior: -7.2342e-01
Fitted a model with MAP estimate = -45.3118
Time for alignment: 21.2266
Computed alignments with likelihoods: ['-45.3112', '-45.3113', '-45.3113', '-45.3107', '-45.3118']
Best model has likelihood: -45.3107  (prior= -0.4994 )
time for generating output: 0.0964
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9418604651162791
Training of 5 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf470ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f8047acd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91789550>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f80a8fe50>
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 365.1821 - loglik: -3.5286e+02 - logprior: -1.2322e+01
Epoch 2/10
11/11 - 2s - loss: 316.1109 - loglik: -3.1315e+02 - logprior: -2.9605e+00
Epoch 3/10
11/11 - 2s - loss: 273.3432 - loglik: -2.7135e+02 - logprior: -1.9955e+00
Epoch 4/10
11/11 - 2s - loss: 256.0503 - loglik: -2.5386e+02 - logprior: -2.1857e+00
Epoch 5/10
11/11 - 2s - loss: 249.8032 - loglik: -2.4744e+02 - logprior: -2.3635e+00
Epoch 6/10
11/11 - 2s - loss: 241.8278 - loglik: -2.3942e+02 - logprior: -2.4096e+00
Epoch 7/10
11/11 - 2s - loss: 241.8602 - loglik: -2.3960e+02 - logprior: -2.2634e+00
Fitted a model with MAP estimate = -240.5226
expansions: [(9, 1), (10, 2), (11, 1), (13, 1), (18, 1), (34, 1), (35, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (56, 1), (66, 1), (67, 1), (68, 2), (69, 1), (70, 2), (83, 1), (84, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 250.9110 - loglik: -2.3669e+02 - logprior: -1.4224e+01
Epoch 2/2
11/11 - 2s - loss: 226.9133 - loglik: -2.2097e+02 - logprior: -5.9459e+00
Fitted a model with MAP estimate = -222.9501
expansions: [(0, 10)]
discards: [ 0 83 88]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 230.9536 - loglik: -2.1921e+02 - logprior: -1.1740e+01
Epoch 2/2
11/11 - 2s - loss: 217.1534 - loglik: -2.1371e+02 - logprior: -3.4469e+00
Fitted a model with MAP estimate = -215.9085
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 226.0206 - loglik: -2.1491e+02 - logprior: -1.1113e+01
Epoch 2/10
11/11 - 2s - loss: 217.2496 - loglik: -2.1448e+02 - logprior: -2.7671e+00
Epoch 3/10
11/11 - 2s - loss: 214.6205 - loglik: -2.1322e+02 - logprior: -1.4043e+00
Epoch 4/10
11/11 - 2s - loss: 213.5085 - loglik: -2.1259e+02 - logprior: -9.2114e-01
Epoch 5/10
11/11 - 2s - loss: 212.2936 - loglik: -2.1165e+02 - logprior: -6.3970e-01
Epoch 6/10
11/11 - 2s - loss: 212.4303 - loglik: -2.1190e+02 - logprior: -5.3091e-01
Fitted a model with MAP estimate = -211.6213
Time for alignment: 60.4497
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 365.3315 - loglik: -3.5301e+02 - logprior: -1.2324e+01
Epoch 2/10
11/11 - 2s - loss: 315.8291 - loglik: -3.1287e+02 - logprior: -2.9626e+00
Epoch 3/10
11/11 - 2s - loss: 272.3113 - loglik: -2.7030e+02 - logprior: -2.0097e+00
Epoch 4/10
11/11 - 2s - loss: 251.2578 - loglik: -2.4897e+02 - logprior: -2.2916e+00
Epoch 5/10
11/11 - 2s - loss: 242.9283 - loglik: -2.4058e+02 - logprior: -2.3466e+00
Epoch 6/10
11/11 - 2s - loss: 240.7648 - loglik: -2.3847e+02 - logprior: -2.2920e+00
Epoch 7/10
11/11 - 2s - loss: 240.2870 - loglik: -2.3810e+02 - logprior: -2.1862e+00
Epoch 8/10
11/11 - 2s - loss: 237.3868 - loglik: -2.3525e+02 - logprior: -2.1380e+00
Epoch 9/10
11/11 - 2s - loss: 239.0820 - loglik: -2.3692e+02 - logprior: -2.1594e+00
Fitted a model with MAP estimate = -237.5870
expansions: [(9, 1), (10, 2), (11, 1), (13, 1), (18, 1), (34, 1), (35, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (60, 1), (62, 2), (65, 2), (66, 2), (67, 2), (69, 1), (83, 1), (84, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 249.8204 - loglik: -2.3560e+02 - logprior: -1.4224e+01
Epoch 2/2
11/11 - 2s - loss: 225.0534 - loglik: -2.1903e+02 - logprior: -6.0207e+00
Fitted a model with MAP estimate = -221.8925
expansions: [(0, 9)]
discards: [ 0 75 76 83 86]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 230.9261 - loglik: -2.1929e+02 - logprior: -1.1636e+01
Epoch 2/2
11/11 - 2s - loss: 217.5552 - loglik: -2.1432e+02 - logprior: -3.2349e+00
Fitted a model with MAP estimate = -215.7735
expansions: []
discards: [1 2 3 4 5 6 7 8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 226.0882 - loglik: -2.1508e+02 - logprior: -1.1012e+01
Epoch 2/10
11/11 - 2s - loss: 215.5130 - loglik: -2.1281e+02 - logprior: -2.7030e+00
Epoch 3/10
11/11 - 2s - loss: 214.4892 - loglik: -2.1313e+02 - logprior: -1.3606e+00
Epoch 4/10
11/11 - 2s - loss: 214.3495 - loglik: -2.1345e+02 - logprior: -9.0138e-01
Epoch 5/10
11/11 - 2s - loss: 211.9083 - loglik: -2.1128e+02 - logprior: -6.3078e-01
Epoch 6/10
11/11 - 2s - loss: 211.5919 - loglik: -2.1106e+02 - logprior: -5.3614e-01
Epoch 7/10
11/11 - 2s - loss: 210.6192 - loglik: -2.1010e+02 - logprior: -5.1487e-01
Epoch 8/10
11/11 - 2s - loss: 210.4788 - loglik: -2.0999e+02 - logprior: -4.9180e-01
Epoch 9/10
11/11 - 2s - loss: 210.4346 - loglik: -2.0991e+02 - logprior: -5.2115e-01
Epoch 10/10
11/11 - 2s - loss: 209.5756 - loglik: -2.0898e+02 - logprior: -5.9074e-01
Fitted a model with MAP estimate = -208.7926
Time for alignment: 71.4171
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 365.6026 - loglik: -3.5328e+02 - logprior: -1.2324e+01
Epoch 2/10
11/11 - 2s - loss: 314.8175 - loglik: -3.1185e+02 - logprior: -2.9721e+00
Epoch 3/10
11/11 - 2s - loss: 271.2331 - loglik: -2.6920e+02 - logprior: -2.0323e+00
Epoch 4/10
11/11 - 2s - loss: 251.4224 - loglik: -2.4912e+02 - logprior: -2.3015e+00
Epoch 5/10
11/11 - 2s - loss: 243.6118 - loglik: -2.4127e+02 - logprior: -2.3409e+00
Epoch 6/10
11/11 - 2s - loss: 241.2242 - loglik: -2.3898e+02 - logprior: -2.2404e+00
Epoch 7/10
11/11 - 2s - loss: 239.5809 - loglik: -2.3740e+02 - logprior: -2.1833e+00
Epoch 8/10
11/11 - 2s - loss: 238.3240 - loglik: -2.3615e+02 - logprior: -2.1705e+00
Epoch 9/10
11/11 - 2s - loss: 235.5000 - loglik: -2.3330e+02 - logprior: -2.2005e+00
Epoch 10/10
11/11 - 2s - loss: 236.6981 - loglik: -2.3448e+02 - logprior: -2.2221e+00
Fitted a model with MAP estimate = -235.9238
expansions: [(9, 2), (10, 2), (11, 1), (13, 1), (18, 1), (34, 1), (35, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (60, 1), (62, 3), (66, 1), (67, 1), (69, 1), (83, 1), (84, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 247.3624 - loglik: -2.3316e+02 - logprior: -1.4200e+01
Epoch 2/2
11/11 - 2s - loss: 225.1887 - loglik: -2.1924e+02 - logprior: -5.9492e+00
Fitted a model with MAP estimate = -220.8297
expansions: [(0, 5)]
discards: [ 0  9 76]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 227.4585 - loglik: -2.1611e+02 - logprior: -1.1346e+01
Epoch 2/2
11/11 - 2s - loss: 217.3683 - loglik: -2.1443e+02 - logprior: -2.9401e+00
Fitted a model with MAP estimate = -215.0206
expansions: []
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 224.3210 - loglik: -2.1323e+02 - logprior: -1.1091e+01
Epoch 2/10
11/11 - 2s - loss: 216.3960 - loglik: -2.1347e+02 - logprior: -2.9303e+00
Epoch 3/10
11/11 - 2s - loss: 214.0331 - loglik: -2.1234e+02 - logprior: -1.6941e+00
Epoch 4/10
11/11 - 2s - loss: 213.2378 - loglik: -2.1197e+02 - logprior: -1.2685e+00
Epoch 5/10
11/11 - 2s - loss: 211.7473 - loglik: -2.1076e+02 - logprior: -9.8476e-01
Epoch 6/10
11/11 - 2s - loss: 210.6106 - loglik: -2.0968e+02 - logprior: -9.2704e-01
Epoch 7/10
11/11 - 2s - loss: 210.2377 - loglik: -2.0940e+02 - logprior: -8.4021e-01
Epoch 8/10
11/11 - 2s - loss: 210.6713 - loglik: -2.0989e+02 - logprior: -7.8513e-01
Fitted a model with MAP estimate = -209.5987
Time for alignment: 69.5951
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 365.5750 - loglik: -3.5325e+02 - logprior: -1.2324e+01
Epoch 2/10
11/11 - 2s - loss: 315.5618 - loglik: -3.1260e+02 - logprior: -2.9646e+00
Epoch 3/10
11/11 - 2s - loss: 272.2947 - loglik: -2.7030e+02 - logprior: -1.9971e+00
Epoch 4/10
11/11 - 2s - loss: 251.6974 - loglik: -2.4943e+02 - logprior: -2.2661e+00
Epoch 5/10
11/11 - 2s - loss: 245.0820 - loglik: -2.4271e+02 - logprior: -2.3752e+00
Epoch 6/10
11/11 - 2s - loss: 240.0593 - loglik: -2.3778e+02 - logprior: -2.2834e+00
Epoch 7/10
11/11 - 2s - loss: 239.5299 - loglik: -2.3730e+02 - logprior: -2.2344e+00
Epoch 8/10
11/11 - 2s - loss: 237.8447 - loglik: -2.3564e+02 - logprior: -2.2003e+00
Epoch 9/10
11/11 - 2s - loss: 236.5822 - loglik: -2.3438e+02 - logprior: -2.2013e+00
Epoch 10/10
11/11 - 2s - loss: 236.7455 - loglik: -2.3454e+02 - logprior: -2.2007e+00
Fitted a model with MAP estimate = -236.0698
expansions: [(9, 2), (10, 2), (11, 1), (13, 1), (18, 1), (34, 1), (35, 1), (37, 2), (38, 1), (39, 1), (40, 1), (41, 1), (60, 1), (61, 2), (62, 2), (66, 1), (67, 1), (69, 1), (83, 1), (84, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 248.7112 - loglik: -2.3452e+02 - logprior: -1.4189e+01
Epoch 2/2
11/11 - 2s - loss: 224.7050 - loglik: -2.1870e+02 - logprior: -6.0057e+00
Fitted a model with MAP estimate = -220.3176
expansions: [(0, 4)]
discards: [ 0  9 45 76 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 229.8610 - loglik: -2.1861e+02 - logprior: -1.1247e+01
Epoch 2/2
11/11 - 2s - loss: 215.5176 - loglik: -2.1266e+02 - logprior: -2.8573e+00
Fitted a model with MAP estimate = -214.8633
expansions: []
discards: [1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 224.6911 - loglik: -2.1360e+02 - logprior: -1.1095e+01
Epoch 2/10
11/11 - 2s - loss: 216.7323 - loglik: -2.1382e+02 - logprior: -2.9148e+00
Epoch 3/10
11/11 - 2s - loss: 212.6093 - loglik: -2.1091e+02 - logprior: -1.7015e+00
Epoch 4/10
11/11 - 2s - loss: 212.6017 - loglik: -2.1134e+02 - logprior: -1.2602e+00
Epoch 5/10
11/11 - 2s - loss: 212.0594 - loglik: -2.1108e+02 - logprior: -9.7583e-01
Epoch 6/10
11/11 - 2s - loss: 210.8091 - loglik: -2.0989e+02 - logprior: -9.1951e-01
Epoch 7/10
11/11 - 2s - loss: 210.0159 - loglik: -2.0917e+02 - logprior: -8.4395e-01
Epoch 8/10
11/11 - 2s - loss: 210.0567 - loglik: -2.0929e+02 - logprior: -7.6521e-01
Fitted a model with MAP estimate = -209.2470
Time for alignment: 66.9773
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 365.6014 - loglik: -3.5328e+02 - logprior: -1.2324e+01
Epoch 2/10
11/11 - 2s - loss: 315.4628 - loglik: -3.1252e+02 - logprior: -2.9433e+00
Epoch 3/10
11/11 - 2s - loss: 273.6040 - loglik: -2.7169e+02 - logprior: -1.9184e+00
Epoch 4/10
11/11 - 2s - loss: 251.5494 - loglik: -2.4953e+02 - logprior: -2.0154e+00
Epoch 5/10
11/11 - 2s - loss: 246.9932 - loglik: -2.4474e+02 - logprior: -2.2496e+00
Epoch 6/10
11/11 - 2s - loss: 242.6160 - loglik: -2.4020e+02 - logprior: -2.4160e+00
Epoch 7/10
11/11 - 2s - loss: 240.1021 - loglik: -2.3785e+02 - logprior: -2.2542e+00
Epoch 8/10
11/11 - 2s - loss: 240.4375 - loglik: -2.3824e+02 - logprior: -2.2008e+00
Fitted a model with MAP estimate = -239.7147
expansions: [(9, 1), (10, 2), (11, 1), (13, 1), (18, 1), (34, 1), (35, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (60, 1), (62, 3), (66, 1), (67, 1), (69, 1), (83, 1), (84, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 250.9707 - loglik: -2.3670e+02 - logprior: -1.4270e+01
Epoch 2/2
11/11 - 2s - loss: 229.3416 - loglik: -2.2362e+02 - logprior: -5.7180e+00
Fitted a model with MAP estimate = -223.6685
expansions: [(0, 29)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 227.4651 - loglik: -2.2006e+02 - logprior: -7.4093e+00
Epoch 2/2
22/22 - 3s - loss: 217.3268 - loglik: -2.1573e+02 - logprior: -1.5970e+00
Fitted a model with MAP estimate = -215.3908
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27 103]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 233.1104 - loglik: -2.1918e+02 - logprior: -1.3931e+01
Epoch 2/10
11/11 - 2s - loss: 221.0229 - loglik: -2.1690e+02 - logprior: -4.1195e+00
Epoch 3/10
11/11 - 2s - loss: 216.4782 - loglik: -2.1511e+02 - logprior: -1.3710e+00
Epoch 4/10
11/11 - 2s - loss: 215.9469 - loglik: -2.1543e+02 - logprior: -5.1937e-01
Epoch 5/10
11/11 - 2s - loss: 215.0259 - loglik: -2.1480e+02 - logprior: -2.2337e-01
Epoch 6/10
11/11 - 2s - loss: 213.2729 - loglik: -2.1313e+02 - logprior: -1.3924e-01
Epoch 7/10
11/11 - 2s - loss: 214.7650 - loglik: -2.1463e+02 - logprior: -1.3475e-01
Fitted a model with MAP estimate = -213.4879
Time for alignment: 63.5447
Computed alignments with likelihoods: ['-211.6213', '-208.7926', '-209.5987', '-209.2470', '-213.4879']
Best model has likelihood: -208.7926  (prior= -0.5850 )
time for generating output: 0.2216
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.681782945736434
Training of 5 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fcea3c250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f80506e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fc64becd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fe86d65e0>
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 766.3500 - loglik: -7.6469e+02 - logprior: -1.6599e+00
Epoch 2/10
39/39 - 16s - loss: 645.1802 - loglik: -6.4349e+02 - logprior: -1.6855e+00
Epoch 3/10
39/39 - 16s - loss: 632.6654 - loglik: -6.3092e+02 - logprior: -1.7475e+00
Epoch 4/10
39/39 - 16s - loss: 630.0726 - loglik: -6.2840e+02 - logprior: -1.6739e+00
Epoch 5/10
39/39 - 16s - loss: 628.8057 - loglik: -6.2714e+02 - logprior: -1.6666e+00
Epoch 6/10
39/39 - 16s - loss: 628.4930 - loglik: -6.2684e+02 - logprior: -1.6522e+00
Epoch 7/10
39/39 - 16s - loss: 627.9884 - loglik: -6.2635e+02 - logprior: -1.6392e+00
Epoch 8/10
39/39 - 16s - loss: 627.8533 - loglik: -6.2622e+02 - logprior: -1.6359e+00
Epoch 9/10
39/39 - 16s - loss: 627.5776 - loglik: -6.2595e+02 - logprior: -1.6262e+00
Epoch 10/10
39/39 - 16s - loss: 627.3149 - loglik: -6.2570e+02 - logprior: -1.6179e+00
Fitted a model with MAP estimate = -583.6588
expansions: [(11, 2), (13, 2), (14, 2), (15, 1), (16, 1), (19, 1), (21, 1), (22, 1), (40, 1), (43, 1), (44, 3), (45, 1), (57, 6), (59, 3), (60, 1), (63, 1), (69, 1), (123, 1), (124, 2), (125, 5), (127, 1), (136, 2), (137, 3), (138, 5), (139, 1), (140, 1), (144, 1), (145, 1), (151, 1), (161, 2), (162, 1), (165, 3), (168, 3), (169, 5), (170, 2), (179, 1), (180, 2), (191, 1), (193, 1), (203, 1), (206, 1), (209, 2), (210, 3), (211, 1), (213, 1)]
discards: [  0 155 156 157]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 618.0560 - loglik: -6.1529e+02 - logprior: -2.7654e+00
Epoch 2/2
39/39 - 25s - loss: 601.2169 - loglik: -6.0012e+02 - logprior: -1.0919e+00
Fitted a model with MAP estimate = -557.4998
expansions: [(0, 2), (154, 1), (209, 2)]
discards: [  0  12  13  14  18  81 173 177 182 210 211 212 246 282 284]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 293 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 603.0013 - loglik: -6.0140e+02 - logprior: -1.6055e+00
Epoch 2/2
39/39 - 23s - loss: 598.8241 - loglik: -5.9825e+02 - logprior: -5.7265e-01
Fitted a model with MAP estimate = -556.4812
expansions: []
discards: [  0 153 154 155 201 202 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 286 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 28s - loss: 559.6130 - loglik: -5.5798e+02 - logprior: -1.6370e+00
Epoch 2/10
45/45 - 25s - loss: 554.3651 - loglik: -5.5384e+02 - logprior: -5.2263e-01
Epoch 3/10
45/45 - 26s - loss: 553.5479 - loglik: -5.5303e+02 - logprior: -5.1340e-01
Epoch 4/10
45/45 - 26s - loss: 552.2460 - loglik: -5.5181e+02 - logprior: -4.3686e-01
Epoch 5/10
45/45 - 25s - loss: 551.2250 - loglik: -5.5080e+02 - logprior: -4.2237e-01
Epoch 6/10
45/45 - 25s - loss: 550.7761 - loglik: -5.5042e+02 - logprior: -3.5591e-01
Epoch 7/10
45/45 - 25s - loss: 551.1127 - loglik: -5.5086e+02 - logprior: -2.5656e-01
Fitted a model with MAP estimate = -550.4246
Time for alignment: 579.1736
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 766.0164 - loglik: -7.6436e+02 - logprior: -1.6536e+00
Epoch 2/10
39/39 - 16s - loss: 645.7777 - loglik: -6.4422e+02 - logprior: -1.5574e+00
Epoch 3/10
39/39 - 16s - loss: 633.0651 - loglik: -6.3144e+02 - logprior: -1.6259e+00
Epoch 4/10
39/39 - 16s - loss: 631.1441 - loglik: -6.2958e+02 - logprior: -1.5656e+00
Epoch 5/10
39/39 - 16s - loss: 629.9697 - loglik: -6.2843e+02 - logprior: -1.5360e+00
Epoch 6/10
39/39 - 16s - loss: 629.7025 - loglik: -6.2818e+02 - logprior: -1.5272e+00
Epoch 7/10
39/39 - 16s - loss: 629.2248 - loglik: -6.2771e+02 - logprior: -1.5114e+00
Epoch 8/10
39/39 - 16s - loss: 628.9365 - loglik: -6.2743e+02 - logprior: -1.5080e+00
Epoch 9/10
39/39 - 16s - loss: 628.8065 - loglik: -6.2731e+02 - logprior: -1.4919e+00
Epoch 10/10
39/39 - 16s - loss: 628.5989 - loglik: -6.2711e+02 - logprior: -1.4887e+00
Fitted a model with MAP estimate = -584.2914
expansions: [(13, 2), (15, 1), (17, 1), (18, 1), (19, 1), (21, 2), (37, 1), (43, 1), (44, 2), (45, 2), (46, 2), (57, 5), (59, 3), (64, 1), (77, 1), (123, 1), (125, 3), (126, 2), (127, 2), (138, 1), (140, 2), (141, 5), (142, 1), (143, 2), (144, 1), (161, 5), (162, 1), (165, 1), (166, 1), (169, 5), (170, 3), (171, 2), (172, 1), (181, 2), (190, 1), (191, 1), (193, 1), (206, 1), (209, 2), (210, 3), (211, 1), (213, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 617.2206 - loglik: -6.1452e+02 - logprior: -2.7051e+00
Epoch 2/2
39/39 - 24s - loss: 600.6771 - loglik: -5.9965e+02 - logprior: -1.0250e+00
Fitted a model with MAP estimate = -557.5742
expansions: [(0, 2), (73, 1)]
discards: [  0  12  26  54  81 156 157 174 179 185 206 207 208 209 210 211 213 214
 215 245 283]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 283 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 606.6263 - loglik: -6.0514e+02 - logprior: -1.4834e+00
Epoch 2/2
39/39 - 22s - loss: 601.8386 - loglik: -6.0128e+02 - logprior: -5.6076e-01
Fitted a model with MAP estimate = -558.6793
expansions: [(201, 1), (203, 1), (204, 4)]
discards: [ 0 57]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 30s - loss: 559.3351 - loglik: -5.5771e+02 - logprior: -1.6226e+00
Epoch 2/10
45/45 - 26s - loss: 554.7951 - loglik: -5.5433e+02 - logprior: -4.6737e-01
Epoch 3/10
45/45 - 26s - loss: 552.6607 - loglik: -5.5223e+02 - logprior: -4.3453e-01
Epoch 4/10
45/45 - 25s - loss: 552.2233 - loglik: -5.5185e+02 - logprior: -3.7708e-01
Epoch 5/10
45/45 - 26s - loss: 551.4886 - loglik: -5.5098e+02 - logprior: -5.0616e-01
Epoch 6/10
45/45 - 25s - loss: 551.8737 - loglik: -5.5162e+02 - logprior: -2.5249e-01
Fitted a model with MAP estimate = -550.3475
Time for alignment: 548.4956
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 767.4005 - loglik: -7.6575e+02 - logprior: -1.6478e+00
Epoch 2/10
39/39 - 16s - loss: 649.4669 - loglik: -6.4789e+02 - logprior: -1.5791e+00
Epoch 3/10
39/39 - 16s - loss: 635.7705 - loglik: -6.3412e+02 - logprior: -1.6462e+00
Epoch 4/10
39/39 - 16s - loss: 633.8116 - loglik: -6.3224e+02 - logprior: -1.5698e+00
Epoch 5/10
39/39 - 16s - loss: 632.7133 - loglik: -6.3118e+02 - logprior: -1.5370e+00
Epoch 6/10
39/39 - 16s - loss: 632.3010 - loglik: -6.3077e+02 - logprior: -1.5336e+00
Epoch 7/10
39/39 - 16s - loss: 631.8148 - loglik: -6.3030e+02 - logprior: -1.5141e+00
Epoch 8/10
39/39 - 16s - loss: 631.5046 - loglik: -6.3000e+02 - logprior: -1.5059e+00
Epoch 9/10
39/39 - 16s - loss: 631.4583 - loglik: -6.2996e+02 - logprior: -1.4946e+00
Epoch 10/10
39/39 - 16s - loss: 631.1945 - loglik: -6.2971e+02 - logprior: -1.4821e+00
Fitted a model with MAP estimate = -587.3089
expansions: [(11, 2), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (19, 1), (22, 1), (43, 1), (44, 2), (45, 4), (57, 6), (59, 3), (60, 1), (63, 1), (90, 1), (118, 2), (121, 1), (125, 5), (139, 3), (140, 5), (141, 1), (142, 1), (143, 1), (145, 4), (149, 1), (165, 1), (166, 2), (167, 1), (170, 8), (179, 1), (180, 2), (191, 1), (193, 1), (206, 1), (209, 2), (210, 3), (211, 1), (213, 1)]
discards: [  0 155 156 157 158 159 162]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 296 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 621.2886 - loglik: -6.1861e+02 - logprior: -2.6793e+00
Epoch 2/2
39/39 - 24s - loss: 604.1353 - loglik: -6.0313e+02 - logprior: -1.0090e+00
Fitted a model with MAP estimate = -560.7579
expansions: [(0, 2), (209, 5), (224, 1)]
discards: [  0  12  13  14  17  56  82 147 159 177 182 194 240 278]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 604.2988 - loglik: -6.0277e+02 - logprior: -1.5336e+00
Epoch 2/2
39/39 - 23s - loss: 599.6385 - loglik: -5.9901e+02 - logprior: -6.3169e-01
Fitted a model with MAP estimate = -557.3005
expansions: [(218, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 290 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 557.2779 - loglik: -5.5559e+02 - logprior: -1.6895e+00
Epoch 2/10
45/45 - 26s - loss: 552.1989 - loglik: -5.5169e+02 - logprior: -5.0947e-01
Epoch 3/10
45/45 - 26s - loss: 552.7433 - loglik: -5.5219e+02 - logprior: -5.4995e-01
Fitted a model with MAP estimate = -550.8354
Time for alignment: 475.0371
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 765.3812 - loglik: -7.6374e+02 - logprior: -1.6364e+00
Epoch 2/10
39/39 - 16s - loss: 643.1664 - loglik: -6.4172e+02 - logprior: -1.4494e+00
Epoch 3/10
39/39 - 16s - loss: 631.1044 - loglik: -6.2963e+02 - logprior: -1.4769e+00
Epoch 4/10
39/39 - 16s - loss: 629.1418 - loglik: -6.2771e+02 - logprior: -1.4359e+00
Epoch 5/10
39/39 - 16s - loss: 628.2339 - loglik: -6.2679e+02 - logprior: -1.4396e+00
Epoch 6/10
39/39 - 16s - loss: 627.5367 - loglik: -6.2610e+02 - logprior: -1.4335e+00
Epoch 7/10
39/39 - 16s - loss: 626.9825 - loglik: -6.2555e+02 - logprior: -1.4309e+00
Epoch 8/10
39/39 - 16s - loss: 626.9288 - loglik: -6.2550e+02 - logprior: -1.4307e+00
Epoch 9/10
39/39 - 16s - loss: 626.5276 - loglik: -6.2510e+02 - logprior: -1.4255e+00
Epoch 10/10
39/39 - 16s - loss: 626.3751 - loglik: -6.2496e+02 - logprior: -1.4143e+00
Fitted a model with MAP estimate = -583.4022
expansions: [(11, 3), (13, 1), (14, 2), (15, 1), (16, 1), (19, 1), (23, 1), (37, 1), (43, 1), (45, 3), (46, 1), (58, 6), (60, 3), (61, 1), (64, 1), (76, 2), (123, 1), (125, 3), (126, 2), (127, 3), (128, 1), (139, 2), (141, 1), (148, 1), (162, 1), (163, 1), (164, 5), (165, 1), (166, 1), (167, 3), (168, 1), (169, 5), (170, 3), (171, 2), (180, 1), (181, 2), (192, 1), (194, 1), (195, 1), (207, 1), (209, 2), (210, 3), (211, 1), (213, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 617.6937 - loglik: -6.1502e+02 - logprior: -2.6739e+00
Epoch 2/2
39/39 - 25s - loss: 600.0447 - loglik: -5.9897e+02 - logprior: -1.0783e+00
Fitted a model with MAP estimate = -557.4683
expansions: [(0, 2), (210, 1)]
discards: [  0  11  12  13  17 102 159 160 161 178 215 216 247 284]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 292 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 602.9880 - loglik: -6.0141e+02 - logprior: -1.5755e+00
Epoch 2/2
39/39 - 23s - loss: 599.1715 - loglik: -5.9863e+02 - logprior: -5.4386e-01
Fitted a model with MAP estimate = -556.8870
expansions: [(205, 2)]
discards: [  0  78 200 201]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 290 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 28s - loss: 558.4131 - loglik: -5.5680e+02 - logprior: -1.6166e+00
Epoch 2/10
45/45 - 26s - loss: 554.1937 - loglik: -5.5369e+02 - logprior: -5.0857e-01
Epoch 3/10
45/45 - 26s - loss: 551.7776 - loglik: -5.5128e+02 - logprior: -4.9433e-01
Epoch 4/10
45/45 - 26s - loss: 551.1999 - loglik: -5.5075e+02 - logprior: -4.5135e-01
Epoch 5/10
45/45 - 26s - loss: 550.4095 - loglik: -5.5001e+02 - logprior: -4.0425e-01
Epoch 6/10
45/45 - 26s - loss: 549.6498 - loglik: -5.4935e+02 - logprior: -2.9986e-01
Epoch 7/10
45/45 - 27s - loss: 550.5552 - loglik: -5.5027e+02 - logprior: -2.8305e-01
Fitted a model with MAP estimate = -549.1863
Time for alignment: 582.3126
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 765.8411 - loglik: -7.6419e+02 - logprior: -1.6499e+00
Epoch 2/10
39/39 - 16s - loss: 644.6192 - loglik: -6.4306e+02 - logprior: -1.5547e+00
Epoch 3/10
39/39 - 16s - loss: 630.6251 - loglik: -6.2901e+02 - logprior: -1.6109e+00
Epoch 4/10
39/39 - 16s - loss: 628.5911 - loglik: -6.2705e+02 - logprior: -1.5442e+00
Epoch 5/10
39/39 - 16s - loss: 627.5741 - loglik: -6.2604e+02 - logprior: -1.5299e+00
Epoch 6/10
39/39 - 16s - loss: 627.0119 - loglik: -6.2550e+02 - logprior: -1.5151e+00
Epoch 7/10
39/39 - 16s - loss: 626.4809 - loglik: -6.2498e+02 - logprior: -1.5039e+00
Epoch 8/10
39/39 - 16s - loss: 626.3804 - loglik: -6.2489e+02 - logprior: -1.4938e+00
Epoch 9/10
39/39 - 16s - loss: 625.9520 - loglik: -6.2446e+02 - logprior: -1.4879e+00
Epoch 10/10
39/39 - 16s - loss: 626.1589 - loglik: -6.2468e+02 - logprior: -1.4819e+00
Fitted a model with MAP estimate = -582.8554
expansions: [(11, 2), (13, 2), (14, 2), (15, 1), (16, 1), (19, 1), (23, 1), (37, 1), (43, 1), (44, 2), (45, 2), (46, 2), (57, 6), (59, 2), (64, 1), (77, 1), (98, 1), (123, 1), (124, 3), (125, 2), (137, 1), (140, 1), (146, 1), (148, 1), (160, 1), (161, 1), (162, 1), (163, 4), (164, 1), (165, 1), (167, 2), (168, 1), (169, 3), (170, 7), (179, 1), (180, 2), (191, 1), (193, 1), (203, 1), (206, 1), (209, 2), (210, 3), (211, 1), (213, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 617.4412 - loglik: -6.1475e+02 - logprior: -2.6880e+00
Epoch 2/2
39/39 - 24s - loss: 601.3210 - loglik: -6.0026e+02 - logprior: -1.0600e+00
Fitted a model with MAP estimate = -557.3530
expansions: [(0, 2), (208, 2)]
discards: [  0  12  13  14  17  56 158 203 204 205 210 211 241 277 279]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 604.9885 - loglik: -6.0342e+02 - logprior: -1.5644e+00
Epoch 2/2
39/39 - 23s - loss: 600.2633 - loglik: -5.9970e+02 - logprior: -5.6483e-01
Fitted a model with MAP estimate = -557.5459
expansions: [(203, 1), (215, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 28s - loss: 558.3741 - loglik: -5.5670e+02 - logprior: -1.6724e+00
Epoch 2/10
45/45 - 26s - loss: 553.3704 - loglik: -5.5287e+02 - logprior: -5.0029e-01
Epoch 3/10
45/45 - 26s - loss: 553.1191 - loglik: -5.5266e+02 - logprior: -4.5950e-01
Epoch 4/10
45/45 - 27s - loss: 551.2338 - loglik: -5.5079e+02 - logprior: -4.4869e-01
Epoch 5/10
45/45 - 25s - loss: 548.9570 - loglik: -5.4853e+02 - logprior: -4.2333e-01
Epoch 6/10
45/45 - 26s - loss: 550.6072 - loglik: -5.5028e+02 - logprior: -3.2929e-01
Fitted a model with MAP estimate = -549.8424
Time for alignment: 549.0218
Computed alignments with likelihoods: ['-550.4246', '-550.3475', '-550.8354', '-549.1863', '-549.8424']
Best model has likelihood: -549.1863  (prior= -0.1542 )
time for generating output: 0.2699
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.902228134353176
Training of 5 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5d6e430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200a8b8490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ec2edc970>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f20239a3040>
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 451.3350 - loglik: -4.5032e+02 - logprior: -1.0101e+00
Epoch 2/10
30/30 - 6s - loss: 380.1794 - loglik: -3.7912e+02 - logprior: -1.0624e+00
Epoch 3/10
30/30 - 6s - loss: 368.4506 - loglik: -3.6737e+02 - logprior: -1.0777e+00
Epoch 4/10
30/30 - 6s - loss: 366.4272 - loglik: -3.6535e+02 - logprior: -1.0741e+00
Epoch 5/10
30/30 - 7s - loss: 365.5744 - loglik: -3.6451e+02 - logprior: -1.0621e+00
Epoch 6/10
30/30 - 6s - loss: 365.1107 - loglik: -3.6406e+02 - logprior: -1.0536e+00
Epoch 7/10
30/30 - 6s - loss: 364.5485 - loglik: -3.6350e+02 - logprior: -1.0486e+00
Epoch 8/10
30/30 - 7s - loss: 364.5951 - loglik: -3.6355e+02 - logprior: -1.0435e+00
Fitted a model with MAP estimate = -352.1513
expansions: [(14, 1), (15, 1), (16, 1), (18, 2), (19, 2), (21, 2), (28, 2), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (52, 1), (53, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (89, 1), (92, 1), (95, 1), (96, 1), (97, 2), (99, 1), (113, 3), (114, 2), (116, 2), (126, 3), (127, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 356.8424 - loglik: -3.5568e+02 - logprior: -1.1669e+00
Epoch 2/2
61/61 - 11s - loss: 347.8704 - loglik: -3.4701e+02 - logprior: -8.6435e-01
Fitted a model with MAP estimate = -337.2959
expansions: []
discards: [ 22  23  27  36  51  54  60  96  98 106 131 151 155 158]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 349.6520 - loglik: -3.4865e+02 - logprior: -9.9783e-01
Epoch 2/2
61/61 - 10s - loss: 348.2059 - loglik: -3.4748e+02 - logprior: -7.2584e-01
Fitted a model with MAP estimate = -337.2390
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 18s - loss: 335.8468 - loglik: -3.3518e+02 - logprior: -6.6182e-01
Epoch 2/10
87/87 - 14s - loss: 335.0577 - loglik: -3.3450e+02 - logprior: -5.5363e-01
Epoch 3/10
87/87 - 14s - loss: 334.1625 - loglik: -3.3363e+02 - logprior: -5.3417e-01
Epoch 4/10
87/87 - 14s - loss: 334.6039 - loglik: -3.3408e+02 - logprior: -5.2420e-01
Fitted a model with MAP estimate = -333.4379
Time for alignment: 281.2514
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 10s - loss: 451.2652 - loglik: -4.5025e+02 - logprior: -1.0145e+00
Epoch 2/10
30/30 - 6s - loss: 380.0668 - loglik: -3.7900e+02 - logprior: -1.0677e+00
Epoch 3/10
30/30 - 7s - loss: 368.3608 - loglik: -3.6727e+02 - logprior: -1.0889e+00
Epoch 4/10
30/30 - 7s - loss: 367.0287 - loglik: -3.6594e+02 - logprior: -1.0888e+00
Epoch 5/10
30/30 - 6s - loss: 365.6099 - loglik: -3.6454e+02 - logprior: -1.0726e+00
Epoch 6/10
30/30 - 6s - loss: 365.1926 - loglik: -3.6413e+02 - logprior: -1.0638e+00
Epoch 7/10
30/30 - 7s - loss: 364.9091 - loglik: -3.6385e+02 - logprior: -1.0578e+00
Epoch 8/10
30/30 - 6s - loss: 364.2342 - loglik: -3.6318e+02 - logprior: -1.0568e+00
Epoch 9/10
30/30 - 7s - loss: 364.5941 - loglik: -3.6354e+02 - logprior: -1.0497e+00
Fitted a model with MAP estimate = -352.2875
expansions: [(14, 1), (15, 1), (16, 1), (18, 2), (19, 1), (21, 2), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (42, 1), (43, 2), (48, 1), (52, 1), (54, 1), (56, 1), (72, 1), (73, 2), (75, 1), (78, 2), (89, 1), (90, 1), (95, 1), (96, 1), (97, 2), (99, 1), (113, 3), (114, 2), (116, 2), (125, 2), (126, 1), (127, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 356.2538 - loglik: -3.5511e+02 - logprior: -1.1472e+00
Epoch 2/2
61/61 - 10s - loss: 348.8500 - loglik: -3.4801e+02 - logprior: -8.4042e-01
Fitted a model with MAP estimate = -337.2429
expansions: []
discards: [ 21  26  49  52  58  95 103 128 148 152 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 15s - loss: 349.4176 - loglik: -3.4842e+02 - logprior: -1.0024e+00
Epoch 2/2
61/61 - 10s - loss: 348.6955 - loglik: -3.4797e+02 - logprior: -7.2808e-01
Fitted a model with MAP estimate = -337.2772
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 335.4762 - loglik: -3.3482e+02 - logprior: -6.6080e-01
Epoch 2/10
87/87 - 14s - loss: 335.4678 - loglik: -3.3491e+02 - logprior: -5.5676e-01
Epoch 3/10
87/87 - 14s - loss: 334.4535 - loglik: -3.3392e+02 - logprior: -5.3606e-01
Epoch 4/10
87/87 - 14s - loss: 333.9427 - loglik: -3.3342e+02 - logprior: -5.2385e-01
Epoch 5/10
87/87 - 14s - loss: 333.7577 - loglik: -3.3325e+02 - logprior: -5.1241e-01
Epoch 6/10
87/87 - 14s - loss: 333.7012 - loglik: -3.3320e+02 - logprior: -5.0105e-01
Epoch 7/10
87/87 - 14s - loss: 332.3862 - loglik: -3.3190e+02 - logprior: -4.8640e-01
Epoch 8/10
87/87 - 14s - loss: 332.9059 - loglik: -3.3243e+02 - logprior: -4.7371e-01
Fitted a model with MAP estimate = -332.6738
Time for alignment: 340.2290
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 451.5460 - loglik: -4.5052e+02 - logprior: -1.0235e+00
Epoch 2/10
30/30 - 7s - loss: 381.4875 - loglik: -3.8041e+02 - logprior: -1.0761e+00
Epoch 3/10
30/30 - 6s - loss: 368.3818 - loglik: -3.6729e+02 - logprior: -1.0937e+00
Epoch 4/10
30/30 - 6s - loss: 366.0734 - loglik: -3.6498e+02 - logprior: -1.0899e+00
Epoch 5/10
30/30 - 7s - loss: 365.7088 - loglik: -3.6463e+02 - logprior: -1.0755e+00
Epoch 6/10
30/30 - 7s - loss: 365.1480 - loglik: -3.6408e+02 - logprior: -1.0695e+00
Epoch 7/10
30/30 - 7s - loss: 364.2563 - loglik: -3.6319e+02 - logprior: -1.0643e+00
Epoch 8/10
30/30 - 6s - loss: 364.2637 - loglik: -3.6320e+02 - logprior: -1.0609e+00
Fitted a model with MAP estimate = -351.7795
expansions: [(14, 1), (15, 1), (16, 1), (18, 2), (19, 2), (21, 2), (28, 2), (36, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (53, 1), (56, 1), (72, 1), (73, 2), (75, 1), (78, 2), (89, 1), (90, 1), (95, 1), (96, 1), (97, 1), (99, 1), (113, 3), (114, 2), (116, 2), (125, 2), (126, 1), (127, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 15s - loss: 356.0694 - loglik: -3.5494e+02 - logprior: -1.1328e+00
Epoch 2/2
61/61 - 11s - loss: 348.6550 - loglik: -3.4783e+02 - logprior: -8.2550e-01
Fitted a model with MAP estimate = -336.9569
expansions: []
discards: [ 22  23  27  36  51  54  96 104 148 152 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 349.7837 - loglik: -3.4878e+02 - logprior: -1.0019e+00
Epoch 2/2
61/61 - 10s - loss: 348.1877 - loglik: -3.4746e+02 - logprior: -7.2552e-01
Fitted a model with MAP estimate = -337.1152
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 335.9667 - loglik: -3.3531e+02 - logprior: -6.6017e-01
Epoch 2/10
87/87 - 14s - loss: 334.8028 - loglik: -3.3425e+02 - logprior: -5.5693e-01
Epoch 3/10
87/87 - 14s - loss: 334.3113 - loglik: -3.3378e+02 - logprior: -5.3574e-01
Epoch 4/10
87/87 - 14s - loss: 333.7276 - loglik: -3.3321e+02 - logprior: -5.2093e-01
Epoch 5/10
87/87 - 14s - loss: 333.7789 - loglik: -3.3327e+02 - logprior: -5.1146e-01
Fitted a model with MAP estimate = -333.0359
Time for alignment: 290.9953
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 451.9863 - loglik: -4.5098e+02 - logprior: -1.0108e+00
Epoch 2/10
30/30 - 6s - loss: 380.6361 - loglik: -3.7957e+02 - logprior: -1.0676e+00
Epoch 3/10
30/30 - 7s - loss: 368.4022 - loglik: -3.6730e+02 - logprior: -1.0997e+00
Epoch 4/10
30/30 - 6s - loss: 366.8939 - loglik: -3.6579e+02 - logprior: -1.1005e+00
Epoch 5/10
30/30 - 7s - loss: 365.5581 - loglik: -3.6448e+02 - logprior: -1.0826e+00
Epoch 6/10
30/30 - 6s - loss: 365.3949 - loglik: -3.6433e+02 - logprior: -1.0677e+00
Epoch 7/10
30/30 - 6s - loss: 364.4600 - loglik: -3.6340e+02 - logprior: -1.0646e+00
Epoch 8/10
30/30 - 7s - loss: 364.4092 - loglik: -3.6335e+02 - logprior: -1.0608e+00
Epoch 9/10
30/30 - 6s - loss: 364.3434 - loglik: -3.6329e+02 - logprior: -1.0564e+00
Epoch 10/10
30/30 - 7s - loss: 364.1962 - loglik: -3.6314e+02 - logprior: -1.0548e+00
Fitted a model with MAP estimate = -352.1106
expansions: [(14, 1), (15, 1), (16, 1), (18, 2), (19, 2), (21, 2), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (53, 1), (56, 1), (72, 1), (73, 2), (75, 1), (78, 2), (89, 1), (92, 1), (95, 1), (96, 1), (97, 2), (99, 1), (113, 3), (114, 2), (116, 2), (125, 2), (126, 1), (127, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 356.5910 - loglik: -3.5545e+02 - logprior: -1.1429e+00
Epoch 2/2
61/61 - 11s - loss: 348.2650 - loglik: -3.4741e+02 - logprior: -8.5377e-01
Fitted a model with MAP estimate = -337.0994
expansions: []
discards: [ 22  23  27  53  95 103 128 148 151 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 349.6956 - loglik: -3.4868e+02 - logprior: -1.0131e+00
Epoch 2/2
61/61 - 10s - loss: 347.7534 - loglik: -3.4703e+02 - logprior: -7.2190e-01
Fitted a model with MAP estimate = -337.1296
expansions: []
discards: [47]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 18s - loss: 336.1650 - loglik: -3.3550e+02 - logprior: -6.6239e-01
Epoch 2/10
87/87 - 14s - loss: 334.9425 - loglik: -3.3439e+02 - logprior: -5.5333e-01
Epoch 3/10
87/87 - 14s - loss: 333.9611 - loglik: -3.3342e+02 - logprior: -5.3779e-01
Epoch 4/10
87/87 - 14s - loss: 334.2249 - loglik: -3.3370e+02 - logprior: -5.2167e-01
Fitted a model with MAP estimate = -333.4917
Time for alignment: 292.2702
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 451.6342 - loglik: -4.5062e+02 - logprior: -1.0145e+00
Epoch 2/10
30/30 - 6s - loss: 379.9265 - loglik: -3.7886e+02 - logprior: -1.0712e+00
Epoch 3/10
30/30 - 7s - loss: 368.4821 - loglik: -3.6739e+02 - logprior: -1.0912e+00
Epoch 4/10
30/30 - 6s - loss: 366.4346 - loglik: -3.6535e+02 - logprior: -1.0866e+00
Epoch 5/10
30/30 - 6s - loss: 365.1322 - loglik: -3.6406e+02 - logprior: -1.0733e+00
Epoch 6/10
30/30 - 6s - loss: 365.1266 - loglik: -3.6407e+02 - logprior: -1.0615e+00
Epoch 7/10
30/30 - 7s - loss: 364.5246 - loglik: -3.6347e+02 - logprior: -1.0587e+00
Epoch 8/10
30/30 - 6s - loss: 364.3242 - loglik: -3.6327e+02 - logprior: -1.0561e+00
Epoch 9/10
30/30 - 7s - loss: 363.5096 - loglik: -3.6246e+02 - logprior: -1.0542e+00
Epoch 10/10
30/30 - 7s - loss: 364.3733 - loglik: -3.6332e+02 - logprior: -1.0495e+00
Fitted a model with MAP estimate = -352.1869
expansions: [(14, 1), (15, 1), (16, 1), (18, 2), (19, 2), (21, 2), (28, 1), (36, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (52, 1), (53, 1), (54, 1), (56, 1), (72, 1), (73, 2), (75, 1), (78, 2), (89, 1), (92, 1), (95, 1), (96, 1), (97, 1), (99, 1), (113, 3), (114, 2), (116, 2), (125, 2), (126, 1), (127, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 356.5253 - loglik: -3.5540e+02 - logprior: -1.1302e+00
Epoch 2/2
61/61 - 11s - loss: 348.4245 - loglik: -3.4760e+02 - logprior: -8.2555e-01
Fitted a model with MAP estimate = -337.1001
expansions: []
discards: [ 22  23  27  50  53  95 103 148 151 154]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 349.8150 - loglik: -3.4882e+02 - logprior: -9.9279e-01
Epoch 2/2
61/61 - 10s - loss: 348.0098 - loglik: -3.4729e+02 - logprior: -7.1843e-01
Fitted a model with MAP estimate = -337.0862
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 336.1221 - loglik: -3.3547e+02 - logprior: -6.5393e-01
Epoch 2/10
87/87 - 14s - loss: 334.9563 - loglik: -3.3441e+02 - logprior: -5.4709e-01
Epoch 3/10
87/87 - 14s - loss: 333.6453 - loglik: -3.3311e+02 - logprior: -5.3107e-01
Epoch 4/10
87/87 - 14s - loss: 334.3018 - loglik: -3.3379e+02 - logprior: -5.1369e-01
Fitted a model with MAP estimate = -333.3687
Time for alignment: 288.2584
Computed alignments with likelihoods: ['-333.4379', '-332.6738', '-333.0359', '-333.4917', '-333.3687']
Best model has likelihood: -332.6738  (prior= -0.4751 )
time for generating output: 0.2531
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.6803002704642049
Training of 5 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ee4f31f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f808dba30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f808dbd90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1ee4e78430>
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 1252.6859 - loglik: -1.2513e+03 - logprior: -1.3935e+00
Epoch 2/10
40/40 - 39s - loss: 1106.2734 - loglik: -1.1048e+03 - logprior: -1.4440e+00
Epoch 3/10
40/40 - 39s - loss: 1092.1072 - loglik: -1.0906e+03 - logprior: -1.5328e+00
Epoch 4/10
40/40 - 39s - loss: 1089.0768 - loglik: -1.0876e+03 - logprior: -1.4347e+00
Epoch 5/10
40/40 - 39s - loss: 1088.5776 - loglik: -1.0872e+03 - logprior: -1.4105e+00
Epoch 6/10
40/40 - 39s - loss: 1087.4904 - loglik: -1.0861e+03 - logprior: -1.4007e+00
Epoch 7/10
40/40 - 39s - loss: 1086.7211 - loglik: -1.0854e+03 - logprior: -1.3298e+00
Epoch 8/10
40/40 - 39s - loss: 1086.8533 - loglik: -1.0854e+03 - logprior: -1.4814e+00
Fitted a model with MAP estimate = -806.6094
expansions: [(69, 1), (71, 1), (108, 1), (109, 2), (117, 4), (118, 6), (119, 11), (126, 1), (130, 1), (131, 2), (132, 3), (134, 2), (135, 4), (136, 1), (137, 1), (138, 3), (139, 2), (140, 2), (141, 2), (142, 1), (148, 1), (152, 2), (153, 2), (154, 2), (155, 3), (156, 1), (157, 1), (158, 1), (159, 2), (160, 2), (166, 5), (167, 1), (168, 2), (169, 1), (177, 3), (180, 5), (181, 1), (184, 3), (185, 1), (191, 1), (198, 1), (219, 5), (220, 4), (221, 3), (222, 2), (223, 1), (227, 1), (228, 1), (238, 1), (239, 2), (242, 1), (243, 2), (245, 2), (246, 2), (249, 1), (254, 1), (255, 1), (257, 1), (278, 2), (280, 5), (281, 2), (283, 1), (284, 1), (296, 1), (302, 1), (314, 1), (318, 3), (319, 2), (320, 4), (330, 2)]
discards: [ 0 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 476 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 75s - loss: 1056.2214 - loglik: -1.0545e+03 - logprior: -1.6856e+00
Epoch 2/2
80/80 - 72s - loss: 1041.1272 - loglik: -1.0406e+03 - logprior: -5.2378e-01
Fitted a model with MAP estimate = -775.2448
expansions: [(0, 2)]
discards: [  0 110 127 160 161 187 202 203 221 222 235 236 237 238 259 260 312 313
 319 348 355 358 359 405 406 407 456 457 474 475]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 448 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 68s - loss: 1045.5144 - loglik: -1.0443e+03 - logprior: -1.1702e+00
Epoch 2/2
40/40 - 64s - loss: 1040.7228 - loglik: -1.0410e+03 - logprior: 0.2617
Fitted a model with MAP estimate = -776.7016
expansions: [(159, 1), (429, 1), (430, 2), (448, 2)]
discards: [  0 163 164]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 96s - loss: 769.3785 - loglik: -7.6879e+02 - logprior: -5.8975e-01
Epoch 2/10
113/113 - 93s - loss: 769.9257 - loglik: -7.6976e+02 - logprior: -1.6078e-01
Fitted a model with MAP estimate = -765.7482
Time for alignment: 1211.8092
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 1254.7610 - loglik: -1.2534e+03 - logprior: -1.3386e+00
Epoch 2/10
40/40 - 39s - loss: 1108.2507 - loglik: -1.1072e+03 - logprior: -1.0893e+00
Epoch 3/10
40/40 - 39s - loss: 1092.9324 - loglik: -1.0916e+03 - logprior: -1.3123e+00
Epoch 4/10
40/40 - 39s - loss: 1090.8088 - loglik: -1.0896e+03 - logprior: -1.2001e+00
Epoch 5/10
40/40 - 39s - loss: 1089.9050 - loglik: -1.0889e+03 - logprior: -1.0198e+00
Epoch 6/10
40/40 - 39s - loss: 1088.8043 - loglik: -1.0878e+03 - logprior: -9.7704e-01
Epoch 7/10
40/40 - 39s - loss: 1089.1946 - loglik: -1.0881e+03 - logprior: -1.1020e+00
Fitted a model with MAP estimate = -808.6281
expansions: [(69, 1), (71, 1), (116, 1), (117, 4), (118, 6), (119, 12), (121, 1), (125, 1), (129, 1), (132, 4), (134, 3), (135, 4), (136, 1), (137, 2), (138, 3), (139, 1), (149, 1), (152, 2), (153, 2), (154, 1), (155, 3), (156, 2), (157, 2), (158, 1), (159, 1), (160, 1), (166, 6), (168, 2), (169, 1), (177, 2), (180, 1), (181, 3), (183, 1), (185, 4), (186, 1), (192, 1), (220, 6), (221, 1), (222, 3), (223, 2), (224, 3), (230, 1), (243, 2), (244, 1), (246, 2), (247, 2), (248, 1), (249, 2), (252, 1), (254, 2), (283, 2), (284, 8), (285, 2), (286, 1), (287, 1), (299, 1), (302, 1), (304, 1), (316, 2), (317, 1), (319, 3), (320, 5), (330, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 474 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 75s - loss: 1057.7151 - loglik: -1.0561e+03 - logprior: -1.6606e+00
Epoch 2/2
80/80 - 72s - loss: 1041.1520 - loglik: -1.0407e+03 - logprior: -4.6288e-01
Fitted a model with MAP estimate = -775.6501
expansions: [(0, 2), (450, 2)]
discards: [  0 126 131 161 176 200 201 204 232 252 263 307 308 319 343 349 350 354
 364 403 404 405 406 452 453 470 471 472 473]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 449 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 70s - loss: 1044.7759 - loglik: -1.0442e+03 - logprior: -6.0084e-01
Epoch 2/2
80/80 - 66s - loss: 1040.6317 - loglik: -1.0408e+03 - logprior: 0.1221
Fitted a model with MAP estimate = -777.3093
expansions: [(429, 2), (430, 1), (449, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 453 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 96s - loss: 769.6585 - loglik: -7.6918e+02 - logprior: -4.8230e-01
Epoch 2/10
113/113 - 93s - loss: 768.5210 - loglik: -7.6838e+02 - logprior: -1.3939e-01
Epoch 3/10
113/113 - 93s - loss: 768.8436 - loglik: -7.6884e+02 - logprior: -1.8126e-04
Fitted a model with MAP estimate = -765.3122
Time for alignment: 1273.2348
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 1250.6700 - loglik: -1.2493e+03 - logprior: -1.3825e+00
Epoch 2/10
40/40 - 39s - loss: 1105.9890 - loglik: -1.1046e+03 - logprior: -1.4019e+00
Epoch 3/10
40/40 - 39s - loss: 1091.9375 - loglik: -1.0906e+03 - logprior: -1.3452e+00
Epoch 4/10
40/40 - 39s - loss: 1089.5326 - loglik: -1.0881e+03 - logprior: -1.4361e+00
Epoch 5/10
40/40 - 39s - loss: 1088.9187 - loglik: -1.0877e+03 - logprior: -1.2600e+00
Epoch 6/10
40/40 - 39s - loss: 1088.1462 - loglik: -1.0868e+03 - logprior: -1.2991e+00
Epoch 7/10
40/40 - 39s - loss: 1087.5344 - loglik: -1.0863e+03 - logprior: -1.2365e+00
Epoch 8/10
40/40 - 39s - loss: 1087.1567 - loglik: -1.0859e+03 - logprior: -1.2341e+00
Epoch 9/10
40/40 - 39s - loss: 1085.5812 - loglik: -1.0844e+03 - logprior: -1.1607e+00
Epoch 10/10
40/40 - 39s - loss: 1086.6843 - loglik: -1.0854e+03 - logprior: -1.2753e+00
Fitted a model with MAP estimate = -806.6678
expansions: [(25, 1), (68, 1), (70, 1), (110, 1), (111, 1), (112, 3), (113, 3), (114, 5), (115, 10), (116, 3), (117, 1), (126, 1), (129, 1), (130, 1), (132, 3), (133, 4), (134, 1), (135, 1), (136, 4), (137, 2), (140, 1), (150, 2), (151, 3), (152, 2), (153, 1), (154, 2), (155, 1), (157, 1), (158, 1), (159, 1), (166, 5), (167, 1), (178, 2), (181, 6), (182, 1), (183, 1), (184, 4), (185, 1), (191, 1), (198, 1), (205, 1), (218, 1), (219, 2), (220, 5), (221, 3), (223, 1), (228, 1), (229, 1), (230, 1), (240, 2), (241, 2), (242, 2), (243, 3), (245, 1), (246, 2), (248, 1), (253, 1), (254, 1), (278, 2), (280, 4), (281, 3), (283, 1), (284, 1), (299, 1), (315, 2), (316, 1), (318, 2), (319, 2), (320, 4), (330, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 472 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 74s - loss: 1057.7601 - loglik: -1.0559e+03 - logprior: -1.9069e+00
Epoch 2/2
80/80 - 71s - loss: 1040.3832 - loglik: -1.0398e+03 - logprior: -5.7398e-01
Fitted a model with MAP estimate = -775.2826
expansions: [(0, 2)]
discards: [  0 131 132 182 183 200 201 210 252 253 254 263 307 315 343 344 345 348
 350 357 399 400 401 451 452 469 470 471]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 446 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 67s - loss: 1046.5358 - loglik: -1.0455e+03 - logprior: -1.0808e+00
Epoch 2/2
40/40 - 64s - loss: 1041.4495 - loglik: -1.0417e+03 - logprior: 0.2024
Fitted a model with MAP estimate = -776.9757
expansions: [(331, 2), (427, 1), (428, 2), (446, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 97s - loss: 772.0125 - loglik: -7.7144e+02 - logprior: -5.6797e-01
Epoch 2/10
113/113 - 93s - loss: 767.6241 - loglik: -7.6739e+02 - logprior: -2.3409e-01
Epoch 3/10
113/113 - 93s - loss: 766.5580 - loglik: -7.6643e+02 - logprior: -1.3118e-01
Epoch 4/10
113/113 - 92s - loss: 767.2404 - loglik: -7.6720e+02 - logprior: -4.0676e-02
Fitted a model with MAP estimate = -764.4381
Time for alignment: 1468.7353
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 1252.3323 - loglik: -1.2510e+03 - logprior: -1.3636e+00
Epoch 2/10
40/40 - 39s - loss: 1107.5017 - loglik: -1.1062e+03 - logprior: -1.3264e+00
Epoch 3/10
40/40 - 39s - loss: 1092.8171 - loglik: -1.0914e+03 - logprior: -1.4339e+00
Epoch 4/10
40/40 - 39s - loss: 1089.0879 - loglik: -1.0877e+03 - logprior: -1.3882e+00
Epoch 5/10
40/40 - 39s - loss: 1088.5526 - loglik: -1.0872e+03 - logprior: -1.3178e+00
Epoch 6/10
40/40 - 39s - loss: 1087.8138 - loglik: -1.0864e+03 - logprior: -1.4617e+00
Epoch 7/10
40/40 - 39s - loss: 1086.8486 - loglik: -1.0855e+03 - logprior: -1.3464e+00
Epoch 8/10
40/40 - 39s - loss: 1086.2174 - loglik: -1.0848e+03 - logprior: -1.3832e+00
Epoch 9/10
40/40 - 39s - loss: 1086.6926 - loglik: -1.0854e+03 - logprior: -1.2897e+00
Fitted a model with MAP estimate = -807.0233
expansions: [(62, 1), (71, 2), (114, 2), (115, 2), (116, 4), (117, 5), (118, 7), (119, 5), (121, 2), (125, 1), (129, 1), (130, 1), (132, 2), (134, 2), (135, 4), (136, 1), (137, 1), (138, 3), (139, 2), (140, 2), (141, 2), (142, 1), (152, 2), (153, 2), (155, 4), (156, 3), (157, 1), (158, 1), (159, 1), (166, 5), (167, 1), (168, 2), (169, 1), (177, 3), (180, 5), (181, 1), (184, 3), (185, 1), (196, 1), (198, 1), (219, 1), (220, 2), (221, 2), (222, 4), (223, 2), (225, 1), (230, 3), (241, 1), (242, 2), (243, 2), (244, 1), (245, 2), (247, 3), (248, 2), (250, 1), (255, 1), (281, 2), (282, 7), (283, 2), (284, 1), (285, 1), (300, 1), (316, 2), (317, 2), (319, 3), (320, 2), (321, 4), (322, 1), (323, 1), (330, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 481 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 76s - loss: 1060.9226 - loglik: -1.0591e+03 - logprior: -1.8314e+00
Epoch 2/2
80/80 - 73s - loss: 1039.9795 - loglik: -1.0396e+03 - logprior: -3.9763e-01
Fitted a model with MAP estimate = -774.7148
expansions: [(0, 2)]
discards: [  0  71 119 120 140 141 148 163 164 165 166 190 204 238 239 259 260 314
 315 316 332 350 351 356 362 363 407 408 409 461 462 463 479 480]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 449 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 69s - loss: 1045.2094 - loglik: -1.0447e+03 - logprior: -4.8675e-01
Epoch 2/2
80/80 - 66s - loss: 1041.5662 - loglik: -1.0419e+03 - logprior: 0.3646
Fitted a model with MAP estimate = -777.5416
expansions: [(159, 2), (449, 2)]
discards: [  0 430]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 95s - loss: 773.2785 - loglik: -7.7279e+02 - logprior: -4.8459e-01
Epoch 2/10
113/113 - 92s - loss: 764.5814 - loglik: -7.6446e+02 - logprior: -1.2287e-01
Epoch 3/10
113/113 - 93s - loss: 769.3386 - loglik: -7.6930e+02 - logprior: -3.5486e-02
Fitted a model with MAP estimate = -765.7696
Time for alignment: 1350.2855
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 1254.6449 - loglik: -1.2533e+03 - logprior: -1.3835e+00
Epoch 2/10
40/40 - 39s - loss: 1110.2601 - loglik: -1.1089e+03 - logprior: -1.3666e+00
Epoch 3/10
40/40 - 39s - loss: 1093.9910 - loglik: -1.0926e+03 - logprior: -1.4203e+00
Epoch 4/10
40/40 - 39s - loss: 1090.9620 - loglik: -1.0898e+03 - logprior: -1.2110e+00
Epoch 5/10
40/40 - 39s - loss: 1089.6479 - loglik: -1.0884e+03 - logprior: -1.2650e+00
Epoch 6/10
40/40 - 39s - loss: 1089.5262 - loglik: -1.0881e+03 - logprior: -1.3830e+00
Epoch 7/10
40/40 - 39s - loss: 1088.4340 - loglik: -1.0872e+03 - logprior: -1.2820e+00
Epoch 8/10
40/40 - 39s - loss: 1087.2292 - loglik: -1.0859e+03 - logprior: -1.3389e+00
Epoch 9/10
40/40 - 39s - loss: 1087.8807 - loglik: -1.0866e+03 - logprior: -1.2903e+00
Fitted a model with MAP estimate = -807.4542
expansions: [(69, 1), (70, 2), (115, 3), (116, 4), (117, 5), (118, 7), (119, 4), (121, 2), (130, 1), (131, 1), (133, 3), (135, 3), (136, 4), (137, 1), (138, 1), (139, 5), (140, 2), (141, 2), (142, 1), (148, 1), (151, 2), (152, 3), (153, 2), (154, 4), (155, 1), (156, 1), (157, 1), (158, 2), (159, 2), (165, 5), (166, 1), (167, 2), (168, 1), (176, 3), (179, 6), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (190, 1), (205, 1), (218, 1), (219, 2), (220, 2), (221, 4), (222, 2), (223, 3), (228, 3), (229, 2), (239, 2), (240, 1), (242, 2), (243, 2), (244, 3), (245, 2), (247, 1), (252, 1), (253, 1), (277, 2), (278, 6), (279, 3), (281, 1), (282, 1), (314, 2), (315, 1), (317, 3), (318, 2), (319, 3), (320, 1), (330, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 486 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 78s - loss: 1059.9265 - loglik: -1.0580e+03 - logprior: -1.8846e+00
Epoch 2/2
80/80 - 75s - loss: 1040.8390 - loglik: -1.0404e+03 - logprior: -4.8670e-01
Fitted a model with MAP estimate = -775.6518
expansions: [(0, 2)]
discards: [  0  70 127 140 146 163 170 203 213 214 224 225 241 242 262 263 264 316
 321 322 329 339 354 359 360 367 368 412 413 414 415 462 470 484 485]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 453 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 69s - loss: 1044.3015 - loglik: -1.0438e+03 - logprior: -5.3548e-01
Epoch 2/2
80/80 - 66s - loss: 1040.0563 - loglik: -1.0403e+03 - logprior: 0.2284
Fitted a model with MAP estimate = -776.4273
expansions: [(453, 2)]
discards: [  0 157 158 435 436]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 450 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 96s - loss: 770.5533 - loglik: -7.7013e+02 - logprior: -4.2147e-01
Epoch 2/10
113/113 - 92s - loss: 769.0515 - loglik: -7.6897e+02 - logprior: -8.4229e-02
Epoch 3/10
113/113 - 92s - loss: 769.6036 - loglik: -7.6958e+02 - logprior: -2.7682e-02
Fitted a model with MAP estimate = -765.5293
Time for alignment: 1361.3808
Computed alignments with likelihoods: ['-765.7482', '-765.3122', '-764.4381', '-765.7696', '-765.5293']
Best model has likelihood: -764.4381  (prior= 0.1815 )
time for generating output: 0.4410
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.742915895537734
Training of 5 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf5748e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ecb177c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe85dd070>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1e870b0280>
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 471.5540 - loglik: -4.6870e+02 - logprior: -2.8572e+00
Epoch 2/10
19/19 - 4s - loss: 300.4722 - loglik: -2.9902e+02 - logprior: -1.4535e+00
Epoch 3/10
19/19 - 4s - loss: 224.3657 - loglik: -2.2240e+02 - logprior: -1.9684e+00
Epoch 4/10
19/19 - 4s - loss: 213.2575 - loglik: -2.1114e+02 - logprior: -2.1138e+00
Epoch 5/10
19/19 - 4s - loss: 209.4594 - loglik: -2.0745e+02 - logprior: -2.0078e+00
Epoch 6/10
19/19 - 4s - loss: 208.5688 - loglik: -2.0659e+02 - logprior: -1.9761e+00
Epoch 7/10
19/19 - 4s - loss: 206.4552 - loglik: -2.0453e+02 - logprior: -1.9230e+00
Epoch 8/10
19/19 - 4s - loss: 206.3130 - loglik: -2.0440e+02 - logprior: -1.9103e+00
Epoch 9/10
19/19 - 4s - loss: 206.1841 - loglik: -2.0428e+02 - logprior: -1.9062e+00
Epoch 10/10
19/19 - 4s - loss: 202.6246 - loglik: -2.0072e+02 - logprior: -1.9011e+00
Fitted a model with MAP estimate = -193.5716
expansions: [(0, 2), (7, 1), (8, 1), (9, 1), (13, 1), (17, 1), (22, 1), (23, 1), (32, 1), (34, 1), (53, 1), (54, 1), (55, 1), (59, 1), (60, 2), (61, 1), (71, 1), (75, 1), (76, 1), (81, 1), (91, 1), (93, 1), (97, 1), (99, 1), (104, 1), (110, 1), (113, 1), (121, 1), (122, 1), (124, 1), (125, 1), (127, 1), (128, 1), (129, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 185.7330 - loglik: -1.8211e+02 - logprior: -3.6264e+00
Epoch 2/2
19/19 - 5s - loss: 150.2218 - loglik: -1.4939e+02 - logprior: -8.3297e-01
Fitted a model with MAP estimate = -147.9039
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 155.6501 - loglik: -1.5197e+02 - logprior: -3.6802e+00
Epoch 2/2
19/19 - 5s - loss: 148.2791 - loglik: -1.4721e+02 - logprior: -1.0688e+00
Fitted a model with MAP estimate = -147.5614
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 149.0773 - loglik: -1.4724e+02 - logprior: -1.8382e+00
Epoch 2/10
22/22 - 6s - loss: 145.0977 - loglik: -1.4418e+02 - logprior: -9.2147e-01
Epoch 3/10
22/22 - 6s - loss: 145.2210 - loglik: -1.4435e+02 - logprior: -8.6679e-01
Fitted a model with MAP estimate = -142.1885
Time for alignment: 128.5710
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 471.8877 - loglik: -4.6904e+02 - logprior: -2.8492e+00
Epoch 2/10
19/19 - 4s - loss: 302.6671 - loglik: -3.0122e+02 - logprior: -1.4491e+00
Epoch 3/10
19/19 - 4s - loss: 225.6597 - loglik: -2.2370e+02 - logprior: -1.9558e+00
Epoch 4/10
19/19 - 4s - loss: 213.9649 - loglik: -2.1186e+02 - logprior: -2.1040e+00
Epoch 5/10
19/19 - 4s - loss: 210.8696 - loglik: -2.0887e+02 - logprior: -1.9981e+00
Epoch 6/10
19/19 - 4s - loss: 208.3765 - loglik: -2.0643e+02 - logprior: -1.9444e+00
Epoch 7/10
19/19 - 4s - loss: 206.8368 - loglik: -2.0495e+02 - logprior: -1.8916e+00
Epoch 8/10
19/19 - 4s - loss: 206.7293 - loglik: -2.0485e+02 - logprior: -1.8810e+00
Epoch 9/10
19/19 - 4s - loss: 206.0387 - loglik: -2.0417e+02 - logprior: -1.8652e+00
Epoch 10/10
19/19 - 4s - loss: 204.6314 - loglik: -2.0277e+02 - logprior: -1.8634e+00
Fitted a model with MAP estimate = -193.8051
expansions: [(0, 2), (7, 1), (8, 1), (9, 1), (13, 1), (17, 1), (23, 1), (24, 1), (32, 1), (34, 1), (53, 1), (54, 1), (55, 1), (59, 1), (60, 2), (61, 1), (71, 1), (75, 1), (76, 1), (81, 1), (91, 1), (93, 1), (99, 2), (104, 1), (110, 1), (113, 1), (121, 1), (122, 1), (124, 1), (125, 1), (127, 1), (128, 1), (129, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 184.6161 - loglik: -1.8098e+02 - logprior: -3.6391e+00
Epoch 2/2
19/19 - 5s - loss: 150.4054 - loglik: -1.4957e+02 - logprior: -8.3485e-01
Fitted a model with MAP estimate = -147.8772
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 155.4158 - loglik: -1.5176e+02 - logprior: -3.6599e+00
Epoch 2/2
19/19 - 5s - loss: 147.9898 - loglik: -1.4688e+02 - logprior: -1.1122e+00
Fitted a model with MAP estimate = -147.3169
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 149.1333 - loglik: -1.4729e+02 - logprior: -1.8448e+00
Epoch 2/10
22/22 - 6s - loss: 146.5669 - loglik: -1.4566e+02 - logprior: -9.0329e-01
Epoch 3/10
22/22 - 6s - loss: 142.9720 - loglik: -1.4211e+02 - logprior: -8.6079e-01
Epoch 4/10
22/22 - 6s - loss: 141.7353 - loglik: -1.4092e+02 - logprior: -8.1497e-01
Epoch 5/10
22/22 - 6s - loss: 140.5867 - loglik: -1.3978e+02 - logprior: -8.0807e-01
Epoch 6/10
22/22 - 6s - loss: 139.5423 - loglik: -1.3874e+02 - logprior: -8.0593e-01
Epoch 7/10
22/22 - 6s - loss: 136.5624 - loglik: -1.3577e+02 - logprior: -7.9583e-01
Epoch 8/10
22/22 - 6s - loss: 135.0261 - loglik: -1.3421e+02 - logprior: -8.1455e-01
Epoch 9/10
22/22 - 6s - loss: 131.4019 - loglik: -1.3059e+02 - logprior: -8.1350e-01
Epoch 10/10
22/22 - 6s - loss: 132.0965 - loglik: -1.3129e+02 - logprior: -8.0567e-01
Fitted a model with MAP estimate = -130.7252
Time for alignment: 168.9102
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 471.4519 - loglik: -4.6859e+02 - logprior: -2.8631e+00
Epoch 2/10
19/19 - 4s - loss: 298.7319 - loglik: -2.9726e+02 - logprior: -1.4687e+00
Epoch 3/10
19/19 - 4s - loss: 222.0809 - loglik: -2.2009e+02 - logprior: -1.9931e+00
Epoch 4/10
19/19 - 4s - loss: 211.0234 - loglik: -2.0887e+02 - logprior: -2.1567e+00
Epoch 5/10
19/19 - 4s - loss: 209.1470 - loglik: -2.0711e+02 - logprior: -2.0350e+00
Epoch 6/10
19/19 - 4s - loss: 207.1553 - loglik: -2.0517e+02 - logprior: -1.9815e+00
Epoch 7/10
19/19 - 4s - loss: 205.4822 - loglik: -2.0354e+02 - logprior: -1.9402e+00
Epoch 8/10
19/19 - 4s - loss: 205.6660 - loglik: -2.0375e+02 - logprior: -1.9180e+00
Fitted a model with MAP estimate = -194.4502
expansions: [(0, 2), (7, 1), (8, 1), (9, 1), (13, 1), (17, 1), (23, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (59, 1), (60, 2), (61, 1), (71, 1), (75, 1), (76, 1), (81, 1), (91, 1), (93, 1), (96, 1), (99, 1), (104, 1), (110, 1), (113, 1), (121, 1), (122, 1), (124, 1), (125, 1), (127, 1), (128, 1), (129, 1), (130, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 188.3458 - loglik: -1.8477e+02 - logprior: -3.5737e+00
Epoch 2/2
19/19 - 5s - loss: 151.2334 - loglik: -1.5038e+02 - logprior: -8.5040e-01
Fitted a model with MAP estimate = -148.0043
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 155.1008 - loglik: -1.5140e+02 - logprior: -3.7013e+00
Epoch 2/2
19/19 - 5s - loss: 148.7593 - loglik: -1.4728e+02 - logprior: -1.4833e+00
Fitted a model with MAP estimate = -147.3382
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 149.1279 - loglik: -1.4707e+02 - logprior: -2.0554e+00
Epoch 2/10
22/22 - 6s - loss: 145.8725 - loglik: -1.4500e+02 - logprior: -8.7666e-01
Epoch 3/10
22/22 - 6s - loss: 142.8345 - loglik: -1.4197e+02 - logprior: -8.6638e-01
Epoch 4/10
22/22 - 6s - loss: 143.2827 - loglik: -1.4244e+02 - logprior: -8.4279e-01
Fitted a model with MAP estimate = -141.4062
Time for alignment: 126.6456
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 471.5032 - loglik: -4.6865e+02 - logprior: -2.8490e+00
Epoch 2/10
19/19 - 4s - loss: 303.3150 - loglik: -3.0187e+02 - logprior: -1.4499e+00
Epoch 3/10
19/19 - 4s - loss: 225.7458 - loglik: -2.2379e+02 - logprior: -1.9594e+00
Epoch 4/10
19/19 - 4s - loss: 215.2793 - loglik: -2.1318e+02 - logprior: -2.0986e+00
Epoch 5/10
19/19 - 4s - loss: 211.1160 - loglik: -2.0911e+02 - logprior: -2.0065e+00
Epoch 6/10
19/19 - 4s - loss: 209.0133 - loglik: -2.0706e+02 - logprior: -1.9575e+00
Epoch 7/10
19/19 - 4s - loss: 207.5148 - loglik: -2.0559e+02 - logprior: -1.9205e+00
Epoch 8/10
19/19 - 4s - loss: 206.5948 - loglik: -2.0470e+02 - logprior: -1.8960e+00
Epoch 9/10
19/19 - 4s - loss: 205.1118 - loglik: -2.0322e+02 - logprior: -1.8956e+00
Epoch 10/10
19/19 - 4s - loss: 205.2446 - loglik: -2.0335e+02 - logprior: -1.8934e+00
Fitted a model with MAP estimate = -194.1836
expansions: [(0, 2), (7, 1), (8, 1), (9, 1), (13, 1), (17, 1), (22, 1), (23, 1), (32, 1), (34, 1), (53, 1), (54, 1), (55, 1), (59, 1), (60, 2), (61, 1), (70, 1), (75, 1), (76, 1), (81, 1), (91, 1), (93, 1), (99, 1), (100, 1), (104, 1), (110, 1), (113, 1), (121, 1), (122, 1), (124, 1), (125, 1), (127, 1), (128, 1), (129, 1), (130, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 186.2658 - loglik: -1.8246e+02 - logprior: -3.8077e+00
Epoch 2/2
19/19 - 5s - loss: 150.5624 - loglik: -1.4945e+02 - logprior: -1.1148e+00
Fitted a model with MAP estimate = -147.9750
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 154.6291 - loglik: -1.5073e+02 - logprior: -3.8981e+00
Epoch 2/2
19/19 - 5s - loss: 148.8911 - loglik: -1.4739e+02 - logprior: -1.5000e+00
Fitted a model with MAP estimate = -146.8576
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 149.2793 - loglik: -1.4720e+02 - logprior: -2.0767e+00
Epoch 2/10
22/22 - 6s - loss: 145.1038 - loglik: -1.4422e+02 - logprior: -8.8451e-01
Epoch 3/10
22/22 - 6s - loss: 143.3149 - loglik: -1.4247e+02 - logprior: -8.4590e-01
Epoch 4/10
22/22 - 6s - loss: 142.2859 - loglik: -1.4145e+02 - logprior: -8.3344e-01
Epoch 5/10
22/22 - 6s - loss: 140.7986 - loglik: -1.3999e+02 - logprior: -8.0953e-01
Epoch 6/10
22/22 - 6s - loss: 140.0276 - loglik: -1.3922e+02 - logprior: -8.0405e-01
Epoch 7/10
22/22 - 6s - loss: 137.4296 - loglik: -1.3662e+02 - logprior: -8.1218e-01
Epoch 8/10
22/22 - 6s - loss: 133.4132 - loglik: -1.3261e+02 - logprior: -8.0263e-01
Epoch 9/10
22/22 - 6s - loss: 133.3325 - loglik: -1.3252e+02 - logprior: -8.1393e-01
Epoch 10/10
22/22 - 6s - loss: 131.0582 - loglik: -1.3026e+02 - logprior: -8.0182e-01
Fitted a model with MAP estimate = -130.9559
Time for alignment: 168.4045
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 471.7398 - loglik: -4.6887e+02 - logprior: -2.8664e+00
Epoch 2/10
19/19 - 4s - loss: 303.0914 - loglik: -3.0163e+02 - logprior: -1.4632e+00
Epoch 3/10
19/19 - 4s - loss: 225.9873 - loglik: -2.2403e+02 - logprior: -1.9617e+00
Epoch 4/10
19/19 - 4s - loss: 215.0761 - loglik: -2.1295e+02 - logprior: -2.1247e+00
Epoch 5/10
19/19 - 4s - loss: 211.6759 - loglik: -2.0966e+02 - logprior: -2.0135e+00
Epoch 6/10
19/19 - 4s - loss: 208.9553 - loglik: -2.0699e+02 - logprior: -1.9689e+00
Epoch 7/10
19/19 - 4s - loss: 208.7118 - loglik: -2.0679e+02 - logprior: -1.9175e+00
Epoch 8/10
19/19 - 4s - loss: 205.7533 - loglik: -2.0385e+02 - logprior: -1.8997e+00
Epoch 9/10
19/19 - 4s - loss: 207.8396 - loglik: -2.0595e+02 - logprior: -1.8858e+00
Fitted a model with MAP estimate = -195.0973
expansions: [(0, 2), (7, 1), (8, 1), (9, 1), (13, 1), (17, 1), (23, 1), (24, 1), (32, 1), (34, 1), (53, 1), (54, 1), (55, 1), (59, 1), (60, 2), (61, 1), (70, 1), (75, 1), (76, 1), (81, 1), (91, 1), (93, 1), (97, 1), (99, 1), (104, 1), (110, 1), (113, 1), (121, 1), (122, 1), (124, 1), (125, 1), (127, 1), (128, 1), (129, 1), (130, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 185.7264 - loglik: -1.8212e+02 - logprior: -3.6048e+00
Epoch 2/2
19/19 - 5s - loss: 150.1085 - loglik: -1.4927e+02 - logprior: -8.4216e-01
Fitted a model with MAP estimate = -147.9909
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 154.9124 - loglik: -1.5121e+02 - logprior: -3.7014e+00
Epoch 2/2
19/19 - 5s - loss: 148.5378 - loglik: -1.4712e+02 - logprior: -1.4182e+00
Fitted a model with MAP estimate = -147.5283
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 149.0288 - loglik: -1.4697e+02 - logprior: -2.0627e+00
Epoch 2/10
22/22 - 6s - loss: 145.3890 - loglik: -1.4448e+02 - logprior: -9.0784e-01
Epoch 3/10
22/22 - 6s - loss: 144.4830 - loglik: -1.4358e+02 - logprior: -8.9975e-01
Epoch 4/10
22/22 - 6s - loss: 141.5896 - loglik: -1.4072e+02 - logprior: -8.7014e-01
Epoch 5/10
22/22 - 6s - loss: 140.6400 - loglik: -1.3980e+02 - logprior: -8.4246e-01
Epoch 6/10
22/22 - 6s - loss: 140.0419 - loglik: -1.3922e+02 - logprior: -8.2579e-01
Epoch 7/10
22/22 - 6s - loss: 137.6763 - loglik: -1.3686e+02 - logprior: -8.1209e-01
Epoch 8/10
22/22 - 6s - loss: 135.5270 - loglik: -1.3470e+02 - logprior: -8.2541e-01
Epoch 9/10
22/22 - 6s - loss: 131.3674 - loglik: -1.3056e+02 - logprior: -8.1111e-01
Epoch 10/10
22/22 - 6s - loss: 131.4079 - loglik: -1.3061e+02 - logprior: -8.0281e-01
Fitted a model with MAP estimate = -130.9488
Time for alignment: 165.1926
Computed alignments with likelihoods: ['-142.1885', '-130.7252', '-141.4062', '-130.9559', '-130.9488']
Best model has likelihood: -130.7252  (prior= -0.7916 )
time for generating output: 0.1927
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 5 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201a90fc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f80b5ea60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201b426ee0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1e6d42d9d0>
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 609.6434 - loglik: -5.7763e+02 - logprior: -3.2015e+01
Epoch 2/10
12/12 - 3s - loss: 533.5430 - loglik: -5.3032e+02 - logprior: -3.2213e+00
Epoch 3/10
12/12 - 3s - loss: 469.8195 - loglik: -4.6916e+02 - logprior: -6.5913e-01
Epoch 4/10
12/12 - 3s - loss: 437.0490 - loglik: -4.3638e+02 - logprior: -6.6649e-01
Epoch 5/10
12/12 - 3s - loss: 428.2646 - loglik: -4.2811e+02 - logprior: -1.5850e-01
Epoch 6/10
12/12 - 3s - loss: 422.7002 - loglik: -4.2290e+02 - logprior: 0.2006
Epoch 7/10
12/12 - 3s - loss: 425.1160 - loglik: -4.2542e+02 - logprior: 0.2999
Fitted a model with MAP estimate = -422.1641
expansions: [(11, 3), (13, 1), (18, 1), (19, 1), (28, 1), (29, 3), (30, 1), (42, 1), (46, 1), (58, 2), (59, 1), (60, 1), (61, 1), (73, 1), (74, 2), (75, 1), (76, 1), (85, 1), (91, 1), (102, 1), (103, 2), (118, 1), (120, 1), (127, 2), (129, 5), (130, 1), (137, 1), (139, 1), (146, 2), (147, 2), (153, 1), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 463.6236 - loglik: -4.2635e+02 - logprior: -3.7270e+01
Epoch 2/2
12/12 - 4s - loss: 421.0529 - loglik: -4.0981e+02 - logprior: -1.1240e+01
Fitted a model with MAP estimate = -414.9212
expansions: [(0, 6)]
discards: [  0 156 161 162 163 187 196 221]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 437.9664 - loglik: -4.1047e+02 - logprior: -2.7495e+01
Epoch 2/2
12/12 - 4s - loss: 408.1709 - loglik: -4.0682e+02 - logprior: -1.3501e+00
Fitted a model with MAP estimate = -402.8760
expansions: [(160, 5)]
discards: [  1   2   3   4   5  39  75 135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 434.4694 - loglik: -4.0801e+02 - logprior: -2.6460e+01
Epoch 2/10
12/12 - 4s - loss: 404.2266 - loglik: -4.0344e+02 - logprior: -7.8799e-01
Epoch 3/10
12/12 - 4s - loss: 399.4029 - loglik: -4.0342e+02 - logprior: 4.0187
Epoch 4/10
12/12 - 4s - loss: 397.2914 - loglik: -4.0337e+02 - logprior: 6.0807
Epoch 5/10
12/12 - 4s - loss: 394.7536 - loglik: -4.0197e+02 - logprior: 7.2137
Epoch 6/10
12/12 - 4s - loss: 393.5966 - loglik: -4.0138e+02 - logprior: 7.7810
Epoch 7/10
12/12 - 4s - loss: 393.3468 - loglik: -4.0155e+02 - logprior: 8.2081
Epoch 8/10
12/12 - 4s - loss: 392.7241 - loglik: -4.0129e+02 - logprior: 8.5687
Epoch 9/10
12/12 - 4s - loss: 391.6417 - loglik: -4.0055e+02 - logprior: 8.9125
Epoch 10/10
12/12 - 4s - loss: 390.2942 - loglik: -3.9952e+02 - logprior: 9.2299
Fitted a model with MAP estimate = -390.8977
Time for alignment: 97.1143
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 609.9612 - loglik: -5.7795e+02 - logprior: -3.2014e+01
Epoch 2/10
12/12 - 3s - loss: 534.9626 - loglik: -5.3175e+02 - logprior: -3.2090e+00
Epoch 3/10
12/12 - 3s - loss: 472.9731 - loglik: -4.7248e+02 - logprior: -4.9735e-01
Epoch 4/10
12/12 - 3s - loss: 441.5262 - loglik: -4.4146e+02 - logprior: -6.4325e-02
Epoch 5/10
12/12 - 3s - loss: 430.3091 - loglik: -4.3058e+02 - logprior: 0.2758
Epoch 6/10
12/12 - 3s - loss: 426.1470 - loglik: -4.2677e+02 - logprior: 0.6223
Epoch 7/10
12/12 - 3s - loss: 423.2135 - loglik: -4.2396e+02 - logprior: 0.7425
Epoch 8/10
12/12 - 3s - loss: 422.9502 - loglik: -4.2384e+02 - logprior: 0.8905
Epoch 9/10
12/12 - 3s - loss: 421.2278 - loglik: -4.2227e+02 - logprior: 1.0413
Epoch 10/10
12/12 - 3s - loss: 421.5345 - loglik: -4.2268e+02 - logprior: 1.1465
Fitted a model with MAP estimate = -420.8493
expansions: [(11, 3), (13, 1), (18, 1), (19, 1), (28, 5), (29, 2), (46, 1), (56, 1), (57, 2), (58, 1), (59, 1), (60, 1), (72, 1), (73, 2), (85, 1), (101, 2), (102, 1), (103, 1), (118, 1), (119, 1), (126, 2), (128, 5), (137, 1), (141, 1), (147, 1), (152, 1), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 463.6857 - loglik: -4.2639e+02 - logprior: -3.7299e+01
Epoch 2/2
12/12 - 4s - loss: 424.1266 - loglik: -4.1268e+02 - logprior: -1.1442e+01
Fitted a model with MAP estimate = -416.0119
expansions: [(0, 6), (96, 1), (184, 1)]
discards: [  0  35  39  72  93 155 160 161 162 217]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 439.4287 - loglik: -4.1191e+02 - logprior: -2.7514e+01
Epoch 2/2
12/12 - 4s - loss: 406.7928 - loglik: -4.0555e+02 - logprior: -1.2471e+00
Fitted a model with MAP estimate = -403.0179
expansions: [(109, 1), (156, 4), (184, 1)]
discards: [  1   2   3   4   5 131]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 434.5586 - loglik: -4.0832e+02 - logprior: -2.6239e+01
Epoch 2/10
12/12 - 4s - loss: 405.3732 - loglik: -4.0483e+02 - logprior: -5.4699e-01
Epoch 3/10
12/12 - 4s - loss: 398.2849 - loglik: -4.0260e+02 - logprior: 4.3149
Epoch 4/10
12/12 - 4s - loss: 395.8954 - loglik: -4.0234e+02 - logprior: 6.4397
Epoch 5/10
12/12 - 4s - loss: 394.2354 - loglik: -4.0180e+02 - logprior: 7.5633
Epoch 6/10
12/12 - 4s - loss: 392.9363 - loglik: -4.0111e+02 - logprior: 8.1723
Epoch 7/10
12/12 - 4s - loss: 393.8441 - loglik: -4.0246e+02 - logprior: 8.6112
Fitted a model with MAP estimate = -391.9782
Time for alignment: 89.8423
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 608.7242 - loglik: -5.7671e+02 - logprior: -3.2017e+01
Epoch 2/10
12/12 - 3s - loss: 533.3831 - loglik: -5.3015e+02 - logprior: -3.2359e+00
Epoch 3/10
12/12 - 3s - loss: 473.7097 - loglik: -4.7318e+02 - logprior: -5.3352e-01
Epoch 4/10
12/12 - 3s - loss: 441.1308 - loglik: -4.4101e+02 - logprior: -1.1904e-01
Epoch 5/10
12/12 - 3s - loss: 430.5798 - loglik: -4.3110e+02 - logprior: 0.5186
Epoch 6/10
12/12 - 3s - loss: 427.6583 - loglik: -4.2849e+02 - logprior: 0.8330
Epoch 7/10
12/12 - 3s - loss: 427.4468 - loglik: -4.2838e+02 - logprior: 0.9379
Epoch 8/10
12/12 - 3s - loss: 423.6512 - loglik: -4.2472e+02 - logprior: 1.0665
Epoch 9/10
12/12 - 3s - loss: 423.5580 - loglik: -4.2477e+02 - logprior: 1.2076
Epoch 10/10
12/12 - 3s - loss: 422.2029 - loglik: -4.2349e+02 - logprior: 1.2916
Fitted a model with MAP estimate = -421.9553
expansions: [(11, 3), (13, 1), (18, 1), (19, 1), (28, 2), (29, 3), (30, 1), (42, 1), (43, 1), (46, 1), (57, 1), (58, 1), (59, 1), (60, 1), (72, 1), (73, 2), (76, 1), (85, 1), (102, 1), (103, 2), (118, 1), (127, 2), (129, 5), (130, 1), (146, 1), (147, 2), (149, 2), (154, 2), (158, 1), (159, 1), (172, 2), (173, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 465.2746 - loglik: -4.2797e+02 - logprior: -3.7300e+01
Epoch 2/2
12/12 - 4s - loss: 422.7152 - loglik: -4.1131e+02 - logprior: -1.1405e+01
Fitted a model with MAP estimate = -415.9037
expansions: [(0, 5), (109, 1)]
discards: [  0  34  35 154 159 160 161 187 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 438.9635 - loglik: -4.1156e+02 - logprior: -2.7406e+01
Epoch 2/2
12/12 - 4s - loss: 407.2276 - loglik: -4.0612e+02 - logprior: -1.1081e+00
Fitted a model with MAP estimate = -402.8785
expansions: [(156, 4), (184, 2)]
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 433.2442 - loglik: -4.0698e+02 - logprior: -2.6262e+01
Epoch 2/10
12/12 - 4s - loss: 403.2488 - loglik: -4.0275e+02 - logprior: -4.9725e-01
Epoch 3/10
12/12 - 4s - loss: 397.7131 - loglik: -4.0195e+02 - logprior: 4.2391
Epoch 4/10
12/12 - 4s - loss: 396.7087 - loglik: -4.0308e+02 - logprior: 6.3715
Epoch 5/10
12/12 - 4s - loss: 392.6363 - loglik: -4.0011e+02 - logprior: 7.4736
Epoch 6/10
12/12 - 4s - loss: 392.5922 - loglik: -4.0071e+02 - logprior: 8.1190
Epoch 7/10
12/12 - 4s - loss: 392.1281 - loglik: -4.0066e+02 - logprior: 8.5269
Epoch 8/10
12/12 - 4s - loss: 390.5818 - loglik: -3.9951e+02 - logprior: 8.9235
Epoch 9/10
12/12 - 4s - loss: 391.5559 - loglik: -4.0082e+02 - logprior: 9.2621
Fitted a model with MAP estimate = -390.2049
Time for alignment: 97.8003
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 609.7002 - loglik: -5.7770e+02 - logprior: -3.2003e+01
Epoch 2/10
12/12 - 3s - loss: 532.7962 - loglik: -5.2959e+02 - logprior: -3.2081e+00
Epoch 3/10
12/12 - 3s - loss: 469.6130 - loglik: -4.6916e+02 - logprior: -4.5222e-01
Epoch 4/10
12/12 - 3s - loss: 438.3318 - loglik: -4.3847e+02 - logprior: 0.1358
Epoch 5/10
12/12 - 3s - loss: 433.4112 - loglik: -4.3417e+02 - logprior: 0.7619
Epoch 6/10
12/12 - 3s - loss: 427.5404 - loglik: -4.2867e+02 - logprior: 1.1264
Epoch 7/10
12/12 - 3s - loss: 426.0138 - loglik: -4.2740e+02 - logprior: 1.3874
Epoch 8/10
12/12 - 3s - loss: 425.9138 - loglik: -4.2755e+02 - logprior: 1.6363
Epoch 9/10
12/12 - 3s - loss: 424.9402 - loglik: -4.2672e+02 - logprior: 1.7796
Epoch 10/10
12/12 - 3s - loss: 423.8813 - loglik: -4.2577e+02 - logprior: 1.8854
Fitted a model with MAP estimate = -424.2765
expansions: [(12, 3), (19, 1), (20, 1), (29, 2), (30, 1), (42, 1), (46, 1), (58, 2), (59, 1), (60, 1), (61, 1), (73, 1), (74, 2), (77, 1), (86, 2), (102, 3), (103, 2), (127, 2), (129, 7), (147, 2), (149, 2), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 467.7858 - loglik: -4.3034e+02 - logprior: -3.7445e+01
Epoch 2/2
12/12 - 4s - loss: 426.8613 - loglik: -4.1513e+02 - logprior: -1.1734e+01
Fitted a model with MAP estimate = -419.0995
expansions: [(0, 6), (10, 3), (174, 2)]
discards: [  0  11  68 123 157 158 159 160 185 215]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 440.6134 - loglik: -4.1318e+02 - logprior: -2.7430e+01
Epoch 2/2
12/12 - 4s - loss: 408.7522 - loglik: -4.0750e+02 - logprior: -1.2514e+00
Fitted a model with MAP estimate = -404.1688
expansions: []
discards: [1 2 3 4 5]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 434.3156 - loglik: -4.0795e+02 - logprior: -2.6367e+01
Epoch 2/10
12/12 - 3s - loss: 407.0991 - loglik: -4.0649e+02 - logprior: -6.0463e-01
Epoch 3/10
12/12 - 3s - loss: 401.1549 - loglik: -4.0536e+02 - logprior: 4.2089
Epoch 4/10
12/12 - 3s - loss: 397.5590 - loglik: -4.0384e+02 - logprior: 6.2830
Epoch 5/10
12/12 - 3s - loss: 398.2928 - loglik: -4.0572e+02 - logprior: 7.4320
Fitted a model with MAP estimate = -396.4564
Time for alignment: 82.6904
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 609.0569 - loglik: -5.7704e+02 - logprior: -3.2015e+01
Epoch 2/10
12/12 - 3s - loss: 533.6190 - loglik: -5.3039e+02 - logprior: -3.2274e+00
Epoch 3/10
12/12 - 3s - loss: 471.7593 - loglik: -4.7112e+02 - logprior: -6.4335e-01
Epoch 4/10
12/12 - 3s - loss: 437.5754 - loglik: -4.3720e+02 - logprior: -3.7239e-01
Epoch 5/10
12/12 - 3s - loss: 428.6719 - loglik: -4.2872e+02 - logprior: 0.0515
Epoch 6/10
12/12 - 3s - loss: 424.4202 - loglik: -4.2475e+02 - logprior: 0.3277
Epoch 7/10
12/12 - 3s - loss: 423.8366 - loglik: -4.2430e+02 - logprior: 0.4631
Epoch 8/10
12/12 - 3s - loss: 421.1800 - loglik: -4.2179e+02 - logprior: 0.6109
Epoch 9/10
12/12 - 3s - loss: 420.5877 - loglik: -4.2131e+02 - logprior: 0.7199
Epoch 10/10
12/12 - 3s - loss: 421.2795 - loglik: -4.2214e+02 - logprior: 0.8626
Fitted a model with MAP estimate = -420.1971
expansions: [(11, 3), (13, 1), (18, 1), (19, 1), (28, 2), (29, 3), (30, 1), (42, 1), (46, 2), (58, 2), (59, 1), (60, 1), (61, 1), (73, 1), (74, 2), (75, 1), (76, 1), (85, 1), (93, 1), (101, 1), (102, 1), (103, 1), (118, 1), (127, 2), (129, 5), (130, 1), (137, 1), (139, 1), (146, 3), (151, 1), (154, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 465.1100 - loglik: -4.2790e+02 - logprior: -3.7206e+01
Epoch 2/2
12/12 - 4s - loss: 421.3246 - loglik: -4.0998e+02 - logprior: -1.1344e+01
Fitted a model with MAP estimate = -415.1316
expansions: [(0, 6), (116, 1), (188, 1)]
discards: [  0  35  36  58  73  95 157 162 163 164 189 190 221]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 439.8011 - loglik: -4.1241e+02 - logprior: -2.7390e+01
Epoch 2/2
12/12 - 4s - loss: 408.0952 - loglik: -4.0689e+02 - logprior: -1.2092e+00
Fitted a model with MAP estimate = -403.2713
expansions: [(157, 5), (185, 2)]
discards: [  1   2   3   4   5 132]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 433.8936 - loglik: -4.0765e+02 - logprior: -2.6248e+01
Epoch 2/10
12/12 - 4s - loss: 404.7371 - loglik: -4.0412e+02 - logprior: -6.1380e-01
Epoch 3/10
12/12 - 4s - loss: 398.5492 - loglik: -4.0273e+02 - logprior: 4.1802
Epoch 4/10
12/12 - 4s - loss: 395.9219 - loglik: -4.0227e+02 - logprior: 6.3499
Epoch 5/10
12/12 - 4s - loss: 393.8447 - loglik: -4.0133e+02 - logprior: 7.4815
Epoch 6/10
12/12 - 4s - loss: 392.2090 - loglik: -4.0033e+02 - logprior: 8.1219
Epoch 7/10
12/12 - 4s - loss: 392.0917 - loglik: -4.0062e+02 - logprior: 8.5248
Epoch 8/10
12/12 - 4s - loss: 392.3587 - loglik: -4.0131e+02 - logprior: 8.9494
Fitted a model with MAP estimate = -391.0091
Time for alignment: 96.0292
Computed alignments with likelihoods: ['-390.8977', '-391.9782', '-390.2049', '-396.4564', '-391.0091']
Best model has likelihood: -390.2049  (prior= 9.4505 )
time for generating output: 0.2192
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.934348239771646
Training of 5 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2001ccc460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fced121f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f92d4a490>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1e6d42d9d0>
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 10s - loss: 615.1069 - loglik: -6.0787e+02 - logprior: -7.2355e+00
Epoch 2/10
21/21 - 6s - loss: 495.8286 - loglik: -4.9430e+02 - logprior: -1.5318e+00
Epoch 3/10
21/21 - 6s - loss: 453.7596 - loglik: -4.5209e+02 - logprior: -1.6689e+00
Epoch 4/10
21/21 - 6s - loss: 444.1585 - loglik: -4.4262e+02 - logprior: -1.5345e+00
Epoch 5/10
21/21 - 6s - loss: 441.1079 - loglik: -4.3954e+02 - logprior: -1.5649e+00
Epoch 6/10
21/21 - 7s - loss: 438.4335 - loglik: -4.3677e+02 - logprior: -1.6608e+00
Epoch 7/10
21/21 - 7s - loss: 438.0899 - loglik: -4.3644e+02 - logprior: -1.6536e+00
Epoch 8/10
21/21 - 7s - loss: 437.4754 - loglik: -4.3583e+02 - logprior: -1.6427e+00
Epoch 9/10
21/21 - 6s - loss: 437.0446 - loglik: -4.3540e+02 - logprior: -1.6468e+00
Epoch 10/10
21/21 - 6s - loss: 435.8006 - loglik: -4.3415e+02 - logprior: -1.6529e+00
Fitted a model with MAP estimate = -436.0459
expansions: [(4, 1), (16, 1), (21, 3), (22, 2), (23, 1), (37, 1), (38, 1), (39, 3), (40, 2), (42, 1), (49, 1), (59, 1), (62, 3), (65, 1), (75, 4), (76, 2), (78, 1), (81, 1), (100, 1), (116, 1), (118, 2), (119, 1), (120, 1), (121, 1), (147, 1), (156, 1), (159, 1), (161, 1), (165, 3), (170, 3), (171, 1), (177, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 11s - loss: 431.6671 - loglik: -4.2470e+02 - logprior: -6.9713e+00
Epoch 2/2
21/21 - 9s - loss: 407.6062 - loglik: -4.0712e+02 - logprior: -4.8724e-01
Fitted a model with MAP estimate = -405.1956
expansions: [(97, 1), (207, 1)]
discards: [ 24  53  81 151 215 216]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 415.7240 - loglik: -4.0924e+02 - logprior: -6.4868e+00
Epoch 2/2
21/21 - 9s - loss: 403.3244 - loglik: -4.0345e+02 - logprior: 0.1249
Fitted a model with MAP estimate = -403.0751
expansions: [(203, 1)]
discards: [83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 413.3321 - loglik: -4.0712e+02 - logprior: -6.2171e+00
Epoch 2/10
21/21 - 8s - loss: 403.6273 - loglik: -4.0411e+02 - logprior: 0.4796
Epoch 3/10
21/21 - 9s - loss: 400.7993 - loglik: -4.0211e+02 - logprior: 1.3063
Epoch 4/10
21/21 - 9s - loss: 401.6137 - loglik: -4.0328e+02 - logprior: 1.6694
Fitted a model with MAP estimate = -399.7761
Time for alignment: 175.1214
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 10s - loss: 615.4931 - loglik: -6.0823e+02 - logprior: -7.2654e+00
Epoch 2/10
21/21 - 6s - loss: 495.2611 - loglik: -4.9368e+02 - logprior: -1.5825e+00
Epoch 3/10
21/21 - 6s - loss: 443.9494 - loglik: -4.4205e+02 - logprior: -1.9010e+00
Epoch 4/10
21/21 - 6s - loss: 436.4197 - loglik: -4.3448e+02 - logprior: -1.9354e+00
Epoch 5/10
21/21 - 6s - loss: 431.8983 - loglik: -4.3009e+02 - logprior: -1.8127e+00
Epoch 6/10
21/21 - 6s - loss: 430.8764 - loglik: -4.2902e+02 - logprior: -1.8570e+00
Epoch 7/10
21/21 - 7s - loss: 429.3513 - loglik: -4.2749e+02 - logprior: -1.8621e+00
Epoch 8/10
21/21 - 7s - loss: 429.3280 - loglik: -4.2749e+02 - logprior: -1.8347e+00
Epoch 9/10
21/21 - 6s - loss: 426.8470 - loglik: -4.2501e+02 - logprior: -1.8419e+00
Epoch 10/10
21/21 - 7s - loss: 428.1065 - loglik: -4.2625e+02 - logprior: -1.8540e+00
Fitted a model with MAP estimate = -427.1362
expansions: [(4, 1), (16, 1), (19, 1), (20, 1), (22, 3), (23, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 2), (50, 1), (60, 1), (63, 3), (66, 1), (70, 1), (74, 1), (75, 1), (76, 2), (77, 1), (79, 2), (82, 1), (97, 1), (101, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (147, 1), (148, 1), (151, 1), (155, 1), (161, 2), (165, 1), (170, 3), (171, 1), (172, 1), (181, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 420.5465 - loglik: -4.1378e+02 - logprior: -6.7668e+00
Epoch 2/2
21/21 - 9s - loss: 398.8867 - loglik: -3.9864e+02 - logprior: -2.4862e-01
Fitted a model with MAP estimate = -394.5621
expansions: [(232, 1)]
discards: [ 27  53  81 202 215 216]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 404.5837 - loglik: -3.9830e+02 - logprior: -6.2797e+00
Epoch 2/2
21/21 - 9s - loss: 393.4291 - loglik: -3.9385e+02 - logprior: 0.4250
Fitted a model with MAP estimate = -393.1311
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 402.6589 - loglik: -3.9672e+02 - logprior: -5.9425e+00
Epoch 2/10
21/21 - 9s - loss: 394.2770 - loglik: -3.9501e+02 - logprior: 0.7358
Epoch 3/10
21/21 - 8s - loss: 391.3742 - loglik: -3.9295e+02 - logprior: 1.5715
Epoch 4/10
21/21 - 8s - loss: 391.9259 - loglik: -3.9387e+02 - logprior: 1.9453
Fitted a model with MAP estimate = -390.4116
Time for alignment: 175.1214
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 10s - loss: 615.7457 - loglik: -6.0853e+02 - logprior: -7.2113e+00
Epoch 2/10
21/21 - 6s - loss: 486.2494 - loglik: -4.8473e+02 - logprior: -1.5152e+00
Epoch 3/10
21/21 - 7s - loss: 442.1752 - loglik: -4.4023e+02 - logprior: -1.9493e+00
Epoch 4/10
21/21 - 6s - loss: 431.4413 - loglik: -4.2959e+02 - logprior: -1.8514e+00
Epoch 5/10
21/21 - 7s - loss: 429.4505 - loglik: -4.2768e+02 - logprior: -1.7736e+00
Epoch 6/10
21/21 - 6s - loss: 428.3643 - loglik: -4.2661e+02 - logprior: -1.7532e+00
Epoch 7/10
21/21 - 6s - loss: 426.1020 - loglik: -4.2433e+02 - logprior: -1.7693e+00
Epoch 8/10
21/21 - 6s - loss: 425.8835 - loglik: -4.2412e+02 - logprior: -1.7618e+00
Epoch 9/10
21/21 - 6s - loss: 427.0548 - loglik: -4.2530e+02 - logprior: -1.7591e+00
Fitted a model with MAP estimate = -425.3103
expansions: [(4, 1), (16, 1), (19, 1), (20, 2), (21, 1), (22, 3), (37, 1), (38, 1), (39, 2), (40, 2), (42, 1), (60, 1), (63, 3), (66, 1), (74, 2), (75, 4), (76, 1), (79, 1), (82, 1), (97, 1), (101, 1), (119, 1), (120, 1), (121, 1), (122, 1), (124, 1), (147, 1), (148, 1), (151, 1), (159, 1), (161, 1), (165, 3), (170, 3), (171, 1), (172, 1), (176, 1), (177, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 418.0373 - loglik: -4.1129e+02 - logprior: -6.7476e+00
Epoch 2/2
21/21 - 9s - loss: 394.0801 - loglik: -3.9388e+02 - logprior: -2.0225e-01
Fitted a model with MAP estimate = -391.4413
expansions: [(208, 1)]
discards: [ 28  53  81 100 215 216]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 402.8047 - loglik: -3.9661e+02 - logprior: -6.1952e+00
Epoch 2/2
21/21 - 8s - loss: 392.0829 - loglik: -3.9247e+02 - logprior: 0.3846
Fitted a model with MAP estimate = -390.6113
expansions: [(203, 1)]
discards: [83 97 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 401.4245 - loglik: -3.9551e+02 - logprior: -5.9148e+00
Epoch 2/10
21/21 - 8s - loss: 392.2052 - loglik: -3.9294e+02 - logprior: 0.7389
Epoch 3/10
21/21 - 9s - loss: 391.7315 - loglik: -3.9332e+02 - logprior: 1.5891
Epoch 4/10
21/21 - 9s - loss: 390.4022 - loglik: -3.9235e+02 - logprior: 1.9523
Epoch 5/10
21/21 - 9s - loss: 388.9364 - loglik: -3.9106e+02 - logprior: 2.1219
Epoch 6/10
21/21 - 8s - loss: 386.6777 - loglik: -3.8893e+02 - logprior: 2.2493
Epoch 7/10
21/21 - 9s - loss: 387.5566 - loglik: -3.8994e+02 - logprior: 2.3850
Fitted a model with MAP estimate = -386.3333
Time for alignment: 192.9223
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 615.7733 - loglik: -6.0852e+02 - logprior: -7.2571e+00
Epoch 2/10
21/21 - 6s - loss: 495.5441 - loglik: -4.9407e+02 - logprior: -1.4789e+00
Epoch 3/10
21/21 - 7s - loss: 455.2384 - loglik: -4.5350e+02 - logprior: -1.7335e+00
Epoch 4/10
21/21 - 6s - loss: 441.8658 - loglik: -4.4023e+02 - logprior: -1.6345e+00
Epoch 5/10
21/21 - 6s - loss: 437.2817 - loglik: -4.3565e+02 - logprior: -1.6316e+00
Epoch 6/10
21/21 - 6s - loss: 436.2181 - loglik: -4.3459e+02 - logprior: -1.6271e+00
Epoch 7/10
21/21 - 6s - loss: 434.8637 - loglik: -4.3325e+02 - logprior: -1.6103e+00
Epoch 8/10
21/21 - 6s - loss: 434.2253 - loglik: -4.3262e+02 - logprior: -1.6072e+00
Epoch 9/10
21/21 - 6s - loss: 434.1116 - loglik: -4.3251e+02 - logprior: -1.6038e+00
Epoch 10/10
21/21 - 6s - loss: 433.8637 - loglik: -4.3223e+02 - logprior: -1.6378e+00
Fitted a model with MAP estimate = -432.5592
expansions: [(4, 1), (16, 1), (21, 3), (22, 2), (23, 1), (36, 1), (37, 1), (38, 1), (39, 2), (40, 2), (50, 1), (60, 1), (61, 1), (74, 1), (76, 4), (77, 2), (78, 1), (82, 3), (100, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 2), (147, 1), (160, 1), (162, 3), (165, 1), (170, 3), (171, 1), (172, 1), (176, 1), (177, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 425.9515 - loglik: -4.1909e+02 - logprior: -6.8601e+00
Epoch 2/2
21/21 - 8s - loss: 404.2897 - loglik: -4.0399e+02 - logprior: -3.0300e-01
Fitted a model with MAP estimate = -400.9978
expansions: []
discards: [ 24  53 109 158 201 213 214]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 11s - loss: 412.4600 - loglik: -4.0612e+02 - logprior: -6.3360e+00
Epoch 2/2
21/21 - 8s - loss: 402.0640 - loglik: -4.0238e+02 - logprior: 0.3138
Fitted a model with MAP estimate = -400.9902
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 410.1679 - loglik: -4.0410e+02 - logprior: -6.0673e+00
Epoch 2/10
21/21 - 8s - loss: 402.4475 - loglik: -4.0306e+02 - logprior: 0.6111
Epoch 3/10
21/21 - 9s - loss: 400.0356 - loglik: -4.0148e+02 - logprior: 1.4431
Epoch 4/10
21/21 - 8s - loss: 399.4183 - loglik: -4.0122e+02 - logprior: 1.8054
Epoch 5/10
21/21 - 9s - loss: 398.8557 - loglik: -4.0083e+02 - logprior: 1.9722
Epoch 6/10
21/21 - 8s - loss: 396.0689 - loglik: -3.9816e+02 - logprior: 2.0873
Epoch 7/10
21/21 - 9s - loss: 397.0048 - loglik: -3.9921e+02 - logprior: 2.2040
Fitted a model with MAP estimate = -395.4984
Time for alignment: 195.7951
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 614.7703 - loglik: -6.0753e+02 - logprior: -7.2405e+00
Epoch 2/10
21/21 - 6s - loss: 494.9831 - loglik: -4.9357e+02 - logprior: -1.4153e+00
Epoch 3/10
21/21 - 7s - loss: 455.4296 - loglik: -4.5393e+02 - logprior: -1.5024e+00
Epoch 4/10
21/21 - 7s - loss: 441.7639 - loglik: -4.4040e+02 - logprior: -1.3608e+00
Epoch 5/10
21/21 - 6s - loss: 440.6029 - loglik: -4.3929e+02 - logprior: -1.3095e+00
Epoch 6/10
21/21 - 7s - loss: 438.0080 - loglik: -4.3664e+02 - logprior: -1.3674e+00
Epoch 7/10
21/21 - 6s - loss: 437.3830 - loglik: -4.3601e+02 - logprior: -1.3732e+00
Epoch 8/10
21/21 - 6s - loss: 436.4305 - loglik: -4.3505e+02 - logprior: -1.3791e+00
Epoch 9/10
21/21 - 7s - loss: 436.5317 - loglik: -4.3515e+02 - logprior: -1.3812e+00
Fitted a model with MAP estimate = -435.7079
expansions: [(4, 1), (17, 1), (21, 3), (22, 1), (23, 3), (36, 1), (37, 1), (38, 1), (39, 2), (40, 2), (50, 1), (60, 1), (63, 3), (66, 1), (75, 1), (76, 3), (77, 2), (78, 1), (82, 3), (100, 1), (119, 1), (120, 1), (121, 4), (147, 1), (160, 1), (162, 3), (165, 1), (170, 1), (171, 2), (177, 1), (178, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 428.7646 - loglik: -4.2181e+02 - logprior: -6.9497e+00
Epoch 2/2
21/21 - 9s - loss: 403.6918 - loglik: -4.0323e+02 - logprior: -4.6210e-01
Fitted a model with MAP estimate = -401.7258
expansions: [(101, 1)]
discards: [ 24  29  54  83 112 157 204 217]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 412.4043 - loglik: -4.0611e+02 - logprior: -6.2966e+00
Epoch 2/2
21/21 - 9s - loss: 403.0125 - loglik: -4.0338e+02 - logprior: 0.3647
Fitted a model with MAP estimate = -401.0879
expansions: []
discards: [83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 411.0629 - loglik: -4.0501e+02 - logprior: -6.0492e+00
Epoch 2/10
21/21 - 8s - loss: 400.3454 - loglik: -4.0094e+02 - logprior: 0.5900
Epoch 3/10
21/21 - 8s - loss: 401.1711 - loglik: -4.0257e+02 - logprior: 1.3985
Fitted a model with MAP estimate = -399.5380
Time for alignment: 157.9475
Computed alignments with likelihoods: ['-399.7761', '-390.4116', '-386.3333', '-395.4984', '-399.5380']
Best model has likelihood: -386.3333  (prior= 2.4593 )
time for generating output: 0.3606
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.5720203351534551
Training of 5 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ecb63a040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ecb2d20a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf1c2af0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fac7b5820>
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 285.2531 - loglik: -2.8225e+02 - logprior: -3.0003e+00
Epoch 2/10
19/19 - 2s - loss: 256.6866 - loglik: -2.5586e+02 - logprior: -8.2889e-01
Epoch 3/10
19/19 - 2s - loss: 245.7844 - loglik: -2.4494e+02 - logprior: -8.3941e-01
Epoch 4/10
19/19 - 2s - loss: 243.9154 - loglik: -2.4317e+02 - logprior: -7.4260e-01
Epoch 5/10
19/19 - 2s - loss: 242.8936 - loglik: -2.4217e+02 - logprior: -7.2336e-01
Epoch 6/10
19/19 - 2s - loss: 242.7496 - loglik: -2.4203e+02 - logprior: -7.1879e-01
Epoch 7/10
19/19 - 2s - loss: 242.5851 - loglik: -2.4188e+02 - logprior: -7.0039e-01
Epoch 8/10
19/19 - 2s - loss: 241.7634 - loglik: -2.4107e+02 - logprior: -6.9256e-01
Epoch 9/10
19/19 - 2s - loss: 241.6432 - loglik: -2.4096e+02 - logprior: -6.8688e-01
Epoch 10/10
19/19 - 2s - loss: 241.9275 - loglik: -2.4124e+02 - logprior: -6.8750e-01
Fitted a model with MAP estimate = -240.4942
expansions: [(0, 11), (14, 3), (16, 1), (20, 1), (56, 1), (57, 3), (58, 3), (61, 3), (62, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 245.5962 - loglik: -2.4179e+02 - logprior: -3.8089e+00
Epoch 2/2
19/19 - 3s - loss: 239.3591 - loglik: -2.3822e+02 - logprior: -1.1426e+00
Fitted a model with MAP estimate = -236.8060
expansions: [(0, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 76 77 78 85 86]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 243.8281 - loglik: -2.4006e+02 - logprior: -3.7665e+00
Epoch 2/2
19/19 - 3s - loss: 239.1531 - loglik: -2.3806e+02 - logprior: -1.0973e+00
Fitted a model with MAP estimate = -237.4788
expansions: [(0, 7)]
discards: [0 1 2 3 4 5 6 7]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 241.6390 - loglik: -2.3853e+02 - logprior: -3.1048e+00
Epoch 2/10
19/19 - 3s - loss: 238.1366 - loglik: -2.3720e+02 - logprior: -9.3844e-01
Epoch 3/10
19/19 - 3s - loss: 236.7286 - loglik: -2.3600e+02 - logprior: -7.2883e-01
Epoch 4/10
19/19 - 3s - loss: 236.5797 - loglik: -2.3592e+02 - logprior: -6.5632e-01
Epoch 5/10
19/19 - 3s - loss: 236.5162 - loglik: -2.3590e+02 - logprior: -6.1575e-01
Epoch 6/10
19/19 - 3s - loss: 236.4445 - loglik: -2.3586e+02 - logprior: -5.8429e-01
Epoch 7/10
19/19 - 3s - loss: 235.6904 - loglik: -2.3513e+02 - logprior: -5.6361e-01
Epoch 8/10
19/19 - 3s - loss: 235.9972 - loglik: -2.3546e+02 - logprior: -5.3458e-01
Fitted a model with MAP estimate = -235.7425
Time for alignment: 84.4865
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 284.7984 - loglik: -2.8179e+02 - logprior: -3.0036e+00
Epoch 2/10
19/19 - 2s - loss: 256.2329 - loglik: -2.5542e+02 - logprior: -8.1744e-01
Epoch 3/10
19/19 - 2s - loss: 246.1160 - loglik: -2.4528e+02 - logprior: -8.3154e-01
Epoch 4/10
19/19 - 2s - loss: 243.7774 - loglik: -2.4303e+02 - logprior: -7.4812e-01
Epoch 5/10
19/19 - 2s - loss: 242.6207 - loglik: -2.4189e+02 - logprior: -7.3456e-01
Epoch 6/10
19/19 - 2s - loss: 242.6210 - loglik: -2.4191e+02 - logprior: -7.1249e-01
Fitted a model with MAP estimate = -241.2530
expansions: [(0, 11), (20, 3), (55, 1), (57, 2), (58, 2), (59, 2), (61, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 246.4550 - loglik: -2.4277e+02 - logprior: -3.6865e+00
Epoch 2/2
19/19 - 3s - loss: 240.3026 - loglik: -2.3914e+02 - logprior: -1.1615e+00
Fitted a model with MAP estimate = -238.0954
expansions: [(0, 7), (32, 1), (33, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 74 75 78 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 244.5628 - loglik: -2.4074e+02 - logprior: -3.8206e+00
Epoch 2/2
19/19 - 3s - loss: 239.8634 - loglik: -2.3876e+02 - logprior: -1.1048e+00
Fitted a model with MAP estimate = -237.8536
expansions: [(0, 9)]
discards: [0 1 2 3 4 5 6]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 241.7874 - loglik: -2.3866e+02 - logprior: -3.1288e+00
Epoch 2/10
19/19 - 3s - loss: 238.2689 - loglik: -2.3727e+02 - logprior: -9.9895e-01
Epoch 3/10
19/19 - 3s - loss: 237.4112 - loglik: -2.3663e+02 - logprior: -7.8379e-01
Epoch 4/10
19/19 - 3s - loss: 236.3583 - loglik: -2.3563e+02 - logprior: -7.2550e-01
Epoch 5/10
19/19 - 3s - loss: 236.2933 - loglik: -2.3560e+02 - logprior: -6.9687e-01
Epoch 6/10
19/19 - 3s - loss: 236.7280 - loglik: -2.3605e+02 - logprior: -6.7468e-01
Fitted a model with MAP estimate = -235.9779
Time for alignment: 71.2298
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 284.8068 - loglik: -2.8180e+02 - logprior: -3.0019e+00
Epoch 2/10
19/19 - 2s - loss: 255.7127 - loglik: -2.5488e+02 - logprior: -8.3627e-01
Epoch 3/10
19/19 - 2s - loss: 245.3360 - loglik: -2.4450e+02 - logprior: -8.3780e-01
Epoch 4/10
19/19 - 2s - loss: 243.9148 - loglik: -2.4319e+02 - logprior: -7.2964e-01
Epoch 5/10
19/19 - 2s - loss: 242.8857 - loglik: -2.4218e+02 - logprior: -7.1038e-01
Epoch 6/10
19/19 - 2s - loss: 241.7769 - loglik: -2.4108e+02 - logprior: -6.9692e-01
Epoch 7/10
19/19 - 2s - loss: 241.9049 - loglik: -2.4122e+02 - logprior: -6.8678e-01
Fitted a model with MAP estimate = -240.5862
expansions: [(0, 11), (11, 2), (12, 3), (56, 1), (57, 3), (58, 2), (61, 3), (62, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 245.6327 - loglik: -2.4189e+02 - logprior: -3.7449e+00
Epoch 2/2
19/19 - 3s - loss: 239.3057 - loglik: -2.3816e+02 - logprior: -1.1462e+00
Fitted a model with MAP estimate = -237.0273
expansions: [(0, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 76 77 84 85]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 243.6323 - loglik: -2.3993e+02 - logprior: -3.6997e+00
Epoch 2/2
19/19 - 3s - loss: 239.4323 - loglik: -2.3834e+02 - logprior: -1.0953e+00
Fitted a model with MAP estimate = -237.4886
expansions: [(0, 8)]
discards: [0 1 2 3 4 5 6 7]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 241.4386 - loglik: -2.3834e+02 - logprior: -3.0981e+00
Epoch 2/10
19/19 - 3s - loss: 238.0569 - loglik: -2.3708e+02 - logprior: -9.7541e-01
Epoch 3/10
19/19 - 3s - loss: 237.1210 - loglik: -2.3637e+02 - logprior: -7.5521e-01
Epoch 4/10
19/19 - 3s - loss: 236.3811 - loglik: -2.3569e+02 - logprior: -6.8727e-01
Epoch 5/10
19/19 - 3s - loss: 236.1721 - loglik: -2.3551e+02 - logprior: -6.6054e-01
Epoch 6/10
19/19 - 3s - loss: 235.9951 - loglik: -2.3536e+02 - logprior: -6.3295e-01
Epoch 7/10
19/19 - 3s - loss: 236.0210 - loglik: -2.3540e+02 - logprior: -6.1629e-01
Fitted a model with MAP estimate = -235.6703
Time for alignment: 74.8666
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 284.8781 - loglik: -2.8187e+02 - logprior: -3.0079e+00
Epoch 2/10
19/19 - 2s - loss: 255.1945 - loglik: -2.5436e+02 - logprior: -8.3037e-01
Epoch 3/10
19/19 - 2s - loss: 245.9711 - loglik: -2.4513e+02 - logprior: -8.4060e-01
Epoch 4/10
19/19 - 2s - loss: 243.5379 - loglik: -2.4280e+02 - logprior: -7.3754e-01
Epoch 5/10
19/19 - 2s - loss: 242.9873 - loglik: -2.4226e+02 - logprior: -7.2941e-01
Epoch 6/10
19/19 - 2s - loss: 241.8160 - loglik: -2.4109e+02 - logprior: -7.3007e-01
Epoch 7/10
19/19 - 2s - loss: 242.0054 - loglik: -2.4128e+02 - logprior: -7.2235e-01
Fitted a model with MAP estimate = -240.4881
expansions: [(0, 10), (11, 3), (12, 2), (20, 1), (56, 1), (57, 3), (58, 2), (61, 3), (62, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 245.7217 - loglik: -2.4195e+02 - logprior: -3.7727e+00
Epoch 2/2
19/19 - 3s - loss: 239.4069 - loglik: -2.3820e+02 - logprior: -1.2083e+00
Fitted a model with MAP estimate = -237.1072
expansions: [(0, 7), (25, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 22 76 77 84 85]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 243.4474 - loglik: -2.3986e+02 - logprior: -3.5885e+00
Epoch 2/2
19/19 - 3s - loss: 239.4042 - loglik: -2.3828e+02 - logprior: -1.1257e+00
Fitted a model with MAP estimate = -237.5873
expansions: [(0, 7)]
discards: [ 0  1  2  3  4  5  6  7 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 241.7643 - loglik: -2.3859e+02 - logprior: -3.1777e+00
Epoch 2/10
19/19 - 3s - loss: 238.1430 - loglik: -2.3722e+02 - logprior: -9.2420e-01
Epoch 3/10
19/19 - 3s - loss: 237.4118 - loglik: -2.3667e+02 - logprior: -7.3904e-01
Epoch 4/10
19/19 - 3s - loss: 236.7538 - loglik: -2.3605e+02 - logprior: -7.0120e-01
Epoch 5/10
19/19 - 3s - loss: 236.3383 - loglik: -2.3567e+02 - logprior: -6.6899e-01
Epoch 6/10
19/19 - 3s - loss: 236.3918 - loglik: -2.3575e+02 - logprior: -6.3874e-01
Fitted a model with MAP estimate = -236.0955
Time for alignment: 70.5879
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 284.9421 - loglik: -2.8194e+02 - logprior: -3.0025e+00
Epoch 2/10
19/19 - 2s - loss: 255.8862 - loglik: -2.5506e+02 - logprior: -8.2812e-01
Epoch 3/10
19/19 - 2s - loss: 245.9522 - loglik: -2.4512e+02 - logprior: -8.3214e-01
Epoch 4/10
19/19 - 2s - loss: 243.2717 - loglik: -2.4253e+02 - logprior: -7.3765e-01
Epoch 5/10
19/19 - 2s - loss: 243.1396 - loglik: -2.4243e+02 - logprior: -7.1355e-01
Epoch 6/10
19/19 - 2s - loss: 242.5455 - loglik: -2.4185e+02 - logprior: -6.9213e-01
Epoch 7/10
19/19 - 2s - loss: 241.8388 - loglik: -2.4115e+02 - logprior: -6.8891e-01
Epoch 8/10
19/19 - 2s - loss: 242.1808 - loglik: -2.4150e+02 - logprior: -6.7870e-01
Fitted a model with MAP estimate = -240.8688
expansions: [(0, 11), (11, 2), (16, 3), (56, 1), (57, 3), (58, 2), (61, 2), (62, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 245.6814 - loglik: -2.4199e+02 - logprior: -3.6895e+00
Epoch 2/2
19/19 - 3s - loss: 238.9431 - loglik: -2.3784e+02 - logprior: -1.1069e+00
Fitted a model with MAP estimate = -236.6767
expansions: [(0, 8)]
discards: [ 1  2  3  4  5  6  7  8 75 77 84]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 243.6888 - loglik: -2.4002e+02 - logprior: -3.6676e+00
Epoch 2/2
19/19 - 3s - loss: 238.9651 - loglik: -2.3779e+02 - logprior: -1.1788e+00
Fitted a model with MAP estimate = -236.8493
expansions: [(0, 7)]
discards: [0 1 2 3 4 5 6 7 8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 241.3051 - loglik: -2.3819e+02 - logprior: -3.1198e+00
Epoch 2/10
19/19 - 3s - loss: 237.2983 - loglik: -2.3639e+02 - logprior: -9.0448e-01
Epoch 3/10
19/19 - 3s - loss: 236.2015 - loglik: -2.3541e+02 - logprior: -7.9076e-01
Epoch 4/10
19/19 - 3s - loss: 235.8052 - loglik: -2.3512e+02 - logprior: -6.8342e-01
Epoch 5/10
19/19 - 3s - loss: 235.9826 - loglik: -2.3534e+02 - logprior: -6.3972e-01
Fitted a model with MAP estimate = -235.3540
Time for alignment: 71.2416
Computed alignments with likelihoods: ['-235.7425', '-235.9779', '-235.6703', '-236.0955', '-235.3540']
Best model has likelihood: -235.3540  (prior= -0.6198 )
time for generating output: 0.2026
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.6313157894736842
Training of 5 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2034cbfc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201b1f6ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201acfb400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f201b4cb700>
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 747.2794 - loglik: -7.3537e+02 - logprior: -1.1906e+01
Epoch 2/10
17/17 - 7s - loss: 565.9315 - loglik: -5.6470e+02 - logprior: -1.2313e+00
Epoch 3/10
17/17 - 7s - loss: 469.1912 - loglik: -4.6692e+02 - logprior: -2.2738e+00
Epoch 4/10
17/17 - 6s - loss: 449.2087 - loglik: -4.4660e+02 - logprior: -2.6121e+00
Epoch 5/10
17/17 - 7s - loss: 443.2356 - loglik: -4.4093e+02 - logprior: -2.3047e+00
Epoch 6/10
17/17 - 7s - loss: 441.4149 - loglik: -4.3933e+02 - logprior: -2.0816e+00
Epoch 7/10
17/17 - 6s - loss: 438.7511 - loglik: -4.3673e+02 - logprior: -2.0172e+00
Epoch 8/10
17/17 - 7s - loss: 439.6726 - loglik: -4.3768e+02 - logprior: -1.9891e+00
Fitted a model with MAP estimate = -438.5210
expansions: [(9, 1), (10, 2), (11, 1), (13, 1), (22, 1), (23, 1), (32, 1), (42, 1), (45, 1), (59, 3), (60, 1), (64, 2), (87, 2), (96, 2), (98, 1), (116, 2), (117, 3), (131, 2), (133, 1), (142, 1), (143, 1), (150, 1), (157, 1), (161, 1), (163, 3), (176, 1), (179, 1), (180, 1), (189, 1), (193, 1), (196, 1), (197, 2), (198, 1), (199, 1), (200, 1), (204, 1), (206, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 434.8060 - loglik: -4.1925e+02 - logprior: -1.5553e+01
Epoch 2/2
17/17 - 9s - loss: 396.5317 - loglik: -3.9201e+02 - logprior: -4.5191e+00
Fitted a model with MAP estimate = -392.3131
expansions: [(0, 12)]
discards: [  0 102 136 156 239]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 275 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 401.4100 - loglik: -3.9058e+02 - logprior: -1.0831e+01
Epoch 2/2
17/17 - 9s - loss: 385.5289 - loglik: -3.8589e+02 - logprior: 0.3603
Fitted a model with MAP estimate = -382.0400
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10 11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 397.7507 - loglik: -3.8781e+02 - logprior: -9.9455e+00
Epoch 2/10
17/17 - 9s - loss: 383.2203 - loglik: -3.8449e+02 - logprior: 1.2659
Epoch 3/10
17/17 - 9s - loss: 381.5329 - loglik: -3.8451e+02 - logprior: 2.9758
Epoch 4/10
17/17 - 9s - loss: 382.0752 - loglik: -3.8583e+02 - logprior: 3.7559
Fitted a model with MAP estimate = -379.4430
Time for alignment: 162.9608
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 747.7834 - loglik: -7.3588e+02 - logprior: -1.1902e+01
Epoch 2/10
17/17 - 7s - loss: 565.8247 - loglik: -5.6463e+02 - logprior: -1.1974e+00
Epoch 3/10
17/17 - 7s - loss: 469.0456 - loglik: -4.6704e+02 - logprior: -2.0046e+00
Epoch 4/10
17/17 - 6s - loss: 449.4620 - loglik: -4.4715e+02 - logprior: -2.3154e+00
Epoch 5/10
17/17 - 7s - loss: 445.7731 - loglik: -4.4382e+02 - logprior: -1.9542e+00
Epoch 6/10
17/17 - 7s - loss: 442.1635 - loglik: -4.4041e+02 - logprior: -1.7562e+00
Epoch 7/10
17/17 - 6s - loss: 442.4917 - loglik: -4.4079e+02 - logprior: -1.6977e+00
Fitted a model with MAP estimate = -440.8245
expansions: [(9, 1), (10, 3), (11, 1), (13, 1), (22, 1), (23, 1), (31, 1), (32, 1), (45, 1), (59, 3), (60, 1), (64, 2), (97, 3), (98, 1), (116, 2), (117, 3), (131, 2), (132, 3), (142, 1), (143, 1), (161, 1), (163, 3), (164, 1), (178, 1), (180, 1), (184, 1), (193, 1), (197, 2), (198, 1), (199, 1), (200, 1), (204, 1), (205, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 435.7161 - loglik: -4.2019e+02 - logprior: -1.5529e+01
Epoch 2/2
17/17 - 9s - loss: 396.4958 - loglik: -3.9213e+02 - logprior: -4.3686e+00
Fitted a model with MAP estimate = -391.7406
expansions: [(0, 11)]
discards: [  0  11 137 160]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 274 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 402.2657 - loglik: -3.9137e+02 - logprior: -1.0899e+01
Epoch 2/2
17/17 - 9s - loss: 385.8465 - loglik: -3.8618e+02 - logprior: 0.3321
Fitted a model with MAP estimate = -383.4657
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 397.4441 - loglik: -3.8751e+02 - logprior: -9.9374e+00
Epoch 2/10
17/17 - 9s - loss: 385.4266 - loglik: -3.8668e+02 - logprior: 1.2561
Epoch 3/10
17/17 - 8s - loss: 382.8179 - loglik: -3.8578e+02 - logprior: 2.9579
Epoch 4/10
17/17 - 9s - loss: 381.1418 - loglik: -3.8491e+02 - logprior: 3.7700
Epoch 5/10
17/17 - 8s - loss: 379.5954 - loglik: -3.8378e+02 - logprior: 4.1842
Epoch 6/10
17/17 - 9s - loss: 378.7914 - loglik: -3.8329e+02 - logprior: 4.4937
Epoch 7/10
17/17 - 9s - loss: 379.7629 - loglik: -3.8452e+02 - logprior: 4.7543
Fitted a model with MAP estimate = -378.4899
Time for alignment: 180.8667
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 747.9993 - loglik: -7.3609e+02 - logprior: -1.1912e+01
Epoch 2/10
17/17 - 7s - loss: 561.3286 - loglik: -5.6010e+02 - logprior: -1.2305e+00
Epoch 3/10
17/17 - 7s - loss: 466.3842 - loglik: -4.6396e+02 - logprior: -2.4202e+00
Epoch 4/10
17/17 - 7s - loss: 448.0407 - loglik: -4.4538e+02 - logprior: -2.6588e+00
Epoch 5/10
17/17 - 7s - loss: 442.4639 - loglik: -4.4018e+02 - logprior: -2.2867e+00
Epoch 6/10
17/17 - 6s - loss: 438.6295 - loglik: -4.3651e+02 - logprior: -2.1225e+00
Epoch 7/10
17/17 - 6s - loss: 439.0988 - loglik: -4.3701e+02 - logprior: -2.0921e+00
Fitted a model with MAP estimate = -438.4625
expansions: [(9, 1), (10, 2), (11, 1), (13, 1), (22, 1), (23, 1), (31, 1), (32, 1), (45, 1), (59, 3), (60, 1), (64, 1), (72, 1), (97, 2), (98, 1), (99, 1), (116, 1), (117, 1), (118, 1), (132, 3), (133, 2), (148, 2), (150, 1), (162, 1), (164, 3), (165, 1), (180, 1), (181, 1), (182, 1), (192, 1), (193, 1), (197, 2), (198, 1), (199, 1), (200, 1), (204, 1), (205, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 433.0573 - loglik: -4.1752e+02 - logprior: -1.5541e+01
Epoch 2/2
17/17 - 9s - loss: 395.6982 - loglik: -3.9146e+02 - logprior: -4.2368e+00
Fitted a model with MAP estimate = -390.6706
expansions: [(0, 12)]
discards: [  0 155 158]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 275 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 401.3522 - loglik: -3.9036e+02 - logprior: -1.0992e+01
Epoch 2/2
17/17 - 9s - loss: 385.3816 - loglik: -3.8565e+02 - logprior: 0.2720
Fitted a model with MAP estimate = -382.2859
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10 11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 397.6615 - loglik: -3.8771e+02 - logprior: -9.9556e+00
Epoch 2/10
17/17 - 9s - loss: 384.1595 - loglik: -3.8542e+02 - logprior: 1.2627
Epoch 3/10
17/17 - 8s - loss: 381.2172 - loglik: -3.8417e+02 - logprior: 2.9553
Epoch 4/10
17/17 - 9s - loss: 379.6909 - loglik: -3.8344e+02 - logprior: 3.7477
Epoch 5/10
17/17 - 8s - loss: 380.3071 - loglik: -3.8449e+02 - logprior: 4.1807
Fitted a model with MAP estimate = -378.9009
Time for alignment: 165.9681
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 746.5097 - loglik: -7.3463e+02 - logprior: -1.1882e+01
Epoch 2/10
17/17 - 7s - loss: 560.5228 - loglik: -5.5938e+02 - logprior: -1.1456e+00
Epoch 3/10
17/17 - 7s - loss: 464.6751 - loglik: -4.6259e+02 - logprior: -2.0881e+00
Epoch 4/10
17/17 - 7s - loss: 448.3311 - loglik: -4.4597e+02 - logprior: -2.3612e+00
Epoch 5/10
17/17 - 6s - loss: 444.3105 - loglik: -4.4233e+02 - logprior: -1.9839e+00
Epoch 6/10
17/17 - 7s - loss: 440.9540 - loglik: -4.3920e+02 - logprior: -1.7510e+00
Epoch 7/10
17/17 - 7s - loss: 442.2714 - loglik: -4.4058e+02 - logprior: -1.6890e+00
Fitted a model with MAP estimate = -440.5467
expansions: [(9, 1), (10, 2), (11, 1), (13, 1), (22, 1), (23, 1), (32, 1), (33, 2), (45, 1), (59, 3), (60, 1), (63, 2), (87, 2), (96, 2), (97, 1), (98, 1), (115, 2), (116, 3), (130, 2), (132, 1), (151, 2), (153, 1), (161, 1), (162, 4), (178, 1), (180, 1), (181, 1), (188, 1), (192, 1), (196, 2), (197, 1), (200, 1), (204, 1), (205, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 434.5903 - loglik: -4.1903e+02 - logprior: -1.5556e+01
Epoch 2/2
17/17 - 9s - loss: 395.9514 - loglik: -3.9166e+02 - logprior: -4.2909e+00
Fitted a model with MAP estimate = -391.6092
expansions: [(0, 12)]
discards: [  0  40 103 137 157]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 275 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 403.2232 - loglik: -3.9236e+02 - logprior: -1.0860e+01
Epoch 2/2
17/17 - 9s - loss: 385.4426 - loglik: -3.8573e+02 - logprior: 0.2840
Fitted a model with MAP estimate = -383.0306
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10 11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 398.4386 - loglik: -3.8845e+02 - logprior: -9.9907e+00
Epoch 2/10
17/17 - 9s - loss: 386.5056 - loglik: -3.8772e+02 - logprior: 1.2155
Epoch 3/10
17/17 - 9s - loss: 382.1227 - loglik: -3.8507e+02 - logprior: 2.9445
Epoch 4/10
17/17 - 9s - loss: 382.0451 - loglik: -3.8578e+02 - logprior: 3.7362
Epoch 5/10
17/17 - 9s - loss: 379.4975 - loglik: -3.8364e+02 - logprior: 4.1452
Epoch 6/10
17/17 - 9s - loss: 379.1762 - loglik: -3.8362e+02 - logprior: 4.4420
Epoch 7/10
17/17 - 9s - loss: 381.3221 - loglik: -3.8603e+02 - logprior: 4.7114
Fitted a model with MAP estimate = -378.8586
Time for alignment: 181.3167
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 747.4907 - loglik: -7.3558e+02 - logprior: -1.1912e+01
Epoch 2/10
17/17 - 7s - loss: 565.8527 - loglik: -5.6470e+02 - logprior: -1.1563e+00
Epoch 3/10
17/17 - 7s - loss: 473.7272 - loglik: -4.7177e+02 - logprior: -1.9607e+00
Epoch 4/10
17/17 - 6s - loss: 449.9121 - loglik: -4.4762e+02 - logprior: -2.2877e+00
Epoch 5/10
17/17 - 7s - loss: 446.6512 - loglik: -4.4462e+02 - logprior: -2.0336e+00
Epoch 6/10
17/17 - 7s - loss: 444.7523 - loglik: -4.4293e+02 - logprior: -1.8234e+00
Epoch 7/10
17/17 - 7s - loss: 439.7512 - loglik: -4.3796e+02 - logprior: -1.7940e+00
Epoch 8/10
17/17 - 7s - loss: 442.6625 - loglik: -4.4091e+02 - logprior: -1.7484e+00
Fitted a model with MAP estimate = -441.0922
expansions: [(9, 4), (12, 1), (22, 1), (23, 1), (31, 1), (32, 1), (45, 1), (47, 1), (58, 3), (59, 1), (63, 1), (91, 1), (96, 2), (98, 1), (115, 2), (116, 2), (117, 1), (131, 3), (132, 2), (133, 1), (142, 1), (143, 1), (161, 1), (163, 3), (176, 1), (179, 1), (180, 1), (184, 1), (193, 1), (196, 2), (197, 1), (198, 1), (199, 2), (200, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 439.2068 - loglik: -4.2338e+02 - logprior: -1.5825e+01
Epoch 2/2
17/17 - 9s - loss: 399.1141 - loglik: -3.9448e+02 - logprior: -4.6329e+00
Fitted a model with MAP estimate = -393.9038
expansions: [(0, 11)]
discards: [  0 137 156 159 247]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 274 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 403.5107 - loglik: -3.9267e+02 - logprior: -1.0839e+01
Epoch 2/2
17/17 - 9s - loss: 386.6538 - loglik: -3.8704e+02 - logprior: 0.3876
Fitted a model with MAP estimate = -383.9092
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 398.2878 - loglik: -3.8842e+02 - logprior: -9.8648e+00
Epoch 2/10
17/17 - 9s - loss: 385.1850 - loglik: -3.8654e+02 - logprior: 1.3525
Epoch 3/10
17/17 - 8s - loss: 382.6762 - loglik: -3.8565e+02 - logprior: 2.9732
Epoch 4/10
17/17 - 8s - loss: 381.7928 - loglik: -3.8559e+02 - logprior: 3.7952
Epoch 5/10
17/17 - 9s - loss: 378.9525 - loglik: -3.8317e+02 - logprior: 4.2178
Epoch 6/10
17/17 - 9s - loss: 380.3165 - loglik: -3.8484e+02 - logprior: 4.5194
Fitted a model with MAP estimate = -379.1096
Time for alignment: 178.4275
Computed alignments with likelihoods: ['-379.4430', '-378.4899', '-378.9009', '-378.8586', '-379.1096']
Best model has likelihood: -378.4899  (prior= 4.8730 )
time for generating output: 0.2897
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.948295672156262
Training of 5 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff0a532e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1edc91afd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f06a32c40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2023ec7dc0>
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 182.4166 - loglik: -1.6242e+02 - logprior: -1.9994e+01
Epoch 2/10
10/10 - 1s - loss: 153.0405 - loglik: -1.4738e+02 - logprior: -5.6637e+00
Epoch 3/10
10/10 - 1s - loss: 137.6192 - loglik: -1.3441e+02 - logprior: -3.2046e+00
Epoch 4/10
10/10 - 1s - loss: 126.6391 - loglik: -1.2411e+02 - logprior: -2.5332e+00
Epoch 5/10
10/10 - 1s - loss: 119.9337 - loglik: -1.1792e+02 - logprior: -2.0159e+00
Epoch 6/10
10/10 - 1s - loss: 117.0664 - loglik: -1.1536e+02 - logprior: -1.7060e+00
Epoch 7/10
10/10 - 1s - loss: 116.4391 - loglik: -1.1480e+02 - logprior: -1.6390e+00
Epoch 8/10
10/10 - 1s - loss: 114.8046 - loglik: -1.1313e+02 - logprior: -1.6753e+00
Epoch 9/10
10/10 - 1s - loss: 113.9435 - loglik: -1.1220e+02 - logprior: -1.7479e+00
Epoch 10/10
10/10 - 1s - loss: 113.4896 - loglik: -1.1176e+02 - logprior: -1.7304e+00
Fitted a model with MAP estimate = -113.4976
expansions: [(0, 2), (1, 2), (10, 1), (11, 2), (19, 1), (22, 1), (35, 4), (36, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 136.8594 - loglik: -1.1091e+02 - logprior: -2.5948e+01
Epoch 2/2
10/10 - 1s - loss: 114.1337 - loglik: -1.0586e+02 - logprior: -8.2762e+00
Fitted a model with MAP estimate = -109.6056
expansions: [(0, 2)]
discards: [ 0  1 16 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 125.7751 - loglik: -1.0491e+02 - logprior: -2.0869e+01
Epoch 2/2
10/10 - 1s - loss: 109.5173 - loglik: -1.0357e+02 - logprior: -5.9451e+00
Fitted a model with MAP estimate = -106.7963
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 123.4993 - loglik: -1.0479e+02 - logprior: -1.8714e+01
Epoch 2/10
10/10 - 1s - loss: 110.0190 - loglik: -1.0466e+02 - logprior: -5.3634e+00
Epoch 3/10
10/10 - 1s - loss: 106.9064 - loglik: -1.0383e+02 - logprior: -3.0721e+00
Epoch 4/10
10/10 - 1s - loss: 105.5802 - loglik: -1.0356e+02 - logprior: -2.0237e+00
Epoch 5/10
10/10 - 1s - loss: 105.1152 - loglik: -1.0371e+02 - logprior: -1.4101e+00
Epoch 6/10
10/10 - 1s - loss: 104.8421 - loglik: -1.0365e+02 - logprior: -1.1915e+00
Epoch 7/10
10/10 - 1s - loss: 104.4102 - loglik: -1.0342e+02 - logprior: -9.9087e-01
Epoch 8/10
10/10 - 1s - loss: 104.4731 - loglik: -1.0356e+02 - logprior: -9.1433e-01
Fitted a model with MAP estimate = -104.2370
Time for alignment: 29.9573
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 182.5357 - loglik: -1.6254e+02 - logprior: -1.9994e+01
Epoch 2/10
10/10 - 1s - loss: 152.9846 - loglik: -1.4732e+02 - logprior: -5.6620e+00
Epoch 3/10
10/10 - 1s - loss: 137.1262 - loglik: -1.3392e+02 - logprior: -3.2022e+00
Epoch 4/10
10/10 - 1s - loss: 125.6298 - loglik: -1.2309e+02 - logprior: -2.5353e+00
Epoch 5/10
10/10 - 1s - loss: 119.4984 - loglik: -1.1749e+02 - logprior: -2.0046e+00
Epoch 6/10
10/10 - 1s - loss: 117.5241 - loglik: -1.1584e+02 - logprior: -1.6815e+00
Epoch 7/10
10/10 - 1s - loss: 116.2172 - loglik: -1.1460e+02 - logprior: -1.6160e+00
Epoch 8/10
10/10 - 1s - loss: 115.4889 - loglik: -1.1387e+02 - logprior: -1.6220e+00
Epoch 9/10
10/10 - 1s - loss: 114.0909 - loglik: -1.1235e+02 - logprior: -1.7360e+00
Epoch 10/10
10/10 - 1s - loss: 113.6012 - loglik: -1.1182e+02 - logprior: -1.7798e+00
Fitted a model with MAP estimate = -113.5556
expansions: [(0, 2), (1, 2), (10, 1), (17, 1), (19, 1), (22, 1), (35, 4), (36, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 136.2462 - loglik: -1.1032e+02 - logprior: -2.5927e+01
Epoch 2/2
10/10 - 1s - loss: 113.4486 - loglik: -1.0530e+02 - logprior: -8.1512e+00
Fitted a model with MAP estimate = -109.3158
expansions: [(0, 2)]
discards: [ 0  1 48]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 125.6844 - loglik: -1.0486e+02 - logprior: -2.0829e+01
Epoch 2/2
10/10 - 1s - loss: 109.3885 - loglik: -1.0346e+02 - logprior: -5.9291e+00
Fitted a model with MAP estimate = -106.7473
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 123.7421 - loglik: -1.0503e+02 - logprior: -1.8709e+01
Epoch 2/10
10/10 - 1s - loss: 109.6030 - loglik: -1.0424e+02 - logprior: -5.3605e+00
Epoch 3/10
10/10 - 1s - loss: 106.6256 - loglik: -1.0356e+02 - logprior: -3.0678e+00
Epoch 4/10
10/10 - 1s - loss: 105.8979 - loglik: -1.0388e+02 - logprior: -2.0227e+00
Epoch 5/10
10/10 - 1s - loss: 105.0634 - loglik: -1.0366e+02 - logprior: -1.4082e+00
Epoch 6/10
10/10 - 1s - loss: 104.6519 - loglik: -1.0346e+02 - logprior: -1.1884e+00
Epoch 7/10
10/10 - 1s - loss: 104.5161 - loglik: -1.0353e+02 - logprior: -9.8806e-01
Epoch 8/10
10/10 - 1s - loss: 104.3553 - loglik: -1.0345e+02 - logprior: -9.0952e-01
Epoch 9/10
10/10 - 1s - loss: 104.3236 - loglik: -1.0346e+02 - logprior: -8.5948e-01
Epoch 10/10
10/10 - 1s - loss: 103.8576 - loglik: -1.0303e+02 - logprior: -8.2397e-01
Fitted a model with MAP estimate = -103.9741
Time for alignment: 30.4983
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 182.4723 - loglik: -1.6248e+02 - logprior: -1.9993e+01
Epoch 2/10
10/10 - 1s - loss: 152.8387 - loglik: -1.4717e+02 - logprior: -5.6645e+00
Epoch 3/10
10/10 - 1s - loss: 137.4049 - loglik: -1.3419e+02 - logprior: -3.2186e+00
Epoch 4/10
10/10 - 1s - loss: 126.0092 - loglik: -1.2345e+02 - logprior: -2.5619e+00
Epoch 5/10
10/10 - 1s - loss: 120.0200 - loglik: -1.1801e+02 - logprior: -2.0060e+00
Epoch 6/10
10/10 - 1s - loss: 117.2558 - loglik: -1.1560e+02 - logprior: -1.6607e+00
Epoch 7/10
10/10 - 1s - loss: 115.9522 - loglik: -1.1433e+02 - logprior: -1.6229e+00
Epoch 8/10
10/10 - 1s - loss: 114.4658 - loglik: -1.1279e+02 - logprior: -1.6790e+00
Epoch 9/10
10/10 - 1s - loss: 113.6414 - loglik: -1.1187e+02 - logprior: -1.7664e+00
Epoch 10/10
10/10 - 1s - loss: 113.0629 - loglik: -1.1132e+02 - logprior: -1.7455e+00
Fitted a model with MAP estimate = -113.0471
expansions: [(0, 2), (1, 2), (10, 1), (11, 2), (17, 1), (22, 1), (35, 4), (36, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 136.5904 - loglik: -1.1063e+02 - logprior: -2.5957e+01
Epoch 2/2
10/10 - 1s - loss: 113.6239 - loglik: -1.0535e+02 - logprior: -8.2705e+00
Fitted a model with MAP estimate = -109.5547
expansions: [(0, 2)]
discards: [ 0  1 16 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 125.7194 - loglik: -1.0486e+02 - logprior: -2.0864e+01
Epoch 2/2
10/10 - 1s - loss: 109.2693 - loglik: -1.0333e+02 - logprior: -5.9391e+00
Fitted a model with MAP estimate = -106.7867
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 123.4604 - loglik: -1.0475e+02 - logprior: -1.8708e+01
Epoch 2/10
10/10 - 1s - loss: 109.9638 - loglik: -1.0461e+02 - logprior: -5.3492e+00
Epoch 3/10
10/10 - 1s - loss: 106.8301 - loglik: -1.0377e+02 - logprior: -3.0560e+00
Epoch 4/10
10/10 - 1s - loss: 105.7371 - loglik: -1.0372e+02 - logprior: -2.0158e+00
Epoch 5/10
10/10 - 1s - loss: 105.0514 - loglik: -1.0365e+02 - logprior: -1.3987e+00
Epoch 6/10
10/10 - 1s - loss: 104.7491 - loglik: -1.0357e+02 - logprior: -1.1802e+00
Epoch 7/10
10/10 - 1s - loss: 104.5358 - loglik: -1.0356e+02 - logprior: -9.7754e-01
Epoch 8/10
10/10 - 1s - loss: 104.3418 - loglik: -1.0344e+02 - logprior: -9.0002e-01
Epoch 9/10
10/10 - 1s - loss: 104.2142 - loglik: -1.0337e+02 - logprior: -8.4864e-01
Epoch 10/10
10/10 - 1s - loss: 104.0984 - loglik: -1.0329e+02 - logprior: -8.1304e-01
Fitted a model with MAP estimate = -104.0160
Time for alignment: 29.9057
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 182.5000 - loglik: -1.6251e+02 - logprior: -1.9993e+01
Epoch 2/10
10/10 - 1s - loss: 152.7606 - loglik: -1.4710e+02 - logprior: -5.6643e+00
Epoch 3/10
10/10 - 1s - loss: 137.3288 - loglik: -1.3412e+02 - logprior: -3.2132e+00
Epoch 4/10
10/10 - 1s - loss: 125.4937 - loglik: -1.2296e+02 - logprior: -2.5380e+00
Epoch 5/10
10/10 - 1s - loss: 119.9882 - loglik: -1.1804e+02 - logprior: -1.9441e+00
Epoch 6/10
10/10 - 1s - loss: 117.8205 - loglik: -1.1625e+02 - logprior: -1.5692e+00
Epoch 7/10
10/10 - 1s - loss: 116.6089 - loglik: -1.1509e+02 - logprior: -1.5179e+00
Epoch 8/10
10/10 - 1s - loss: 114.8629 - loglik: -1.1326e+02 - logprior: -1.5991e+00
Epoch 9/10
10/10 - 1s - loss: 113.6214 - loglik: -1.1189e+02 - logprior: -1.7343e+00
Epoch 10/10
10/10 - 1s - loss: 113.3384 - loglik: -1.1158e+02 - logprior: -1.7566e+00
Fitted a model with MAP estimate = -113.0947
expansions: [(0, 2), (1, 2), (10, 1), (11, 2), (17, 1), (22, 1), (35, 4), (36, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 136.5630 - loglik: -1.1061e+02 - logprior: -2.5957e+01
Epoch 2/2
10/10 - 1s - loss: 113.8341 - loglik: -1.0556e+02 - logprior: -8.2702e+00
Fitted a model with MAP estimate = -109.6243
expansions: [(0, 2)]
discards: [ 0  1 16 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 125.6559 - loglik: -1.0479e+02 - logprior: -2.0865e+01
Epoch 2/2
10/10 - 1s - loss: 109.4202 - loglik: -1.0349e+02 - logprior: -5.9347e+00
Fitted a model with MAP estimate = -106.7752
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 123.6436 - loglik: -1.0494e+02 - logprior: -1.8702e+01
Epoch 2/10
10/10 - 1s - loss: 109.9091 - loglik: -1.0456e+02 - logprior: -5.3502e+00
Epoch 3/10
10/10 - 1s - loss: 106.5164 - loglik: -1.0346e+02 - logprior: -3.0553e+00
Epoch 4/10
10/10 - 1s - loss: 105.7880 - loglik: -1.0377e+02 - logprior: -2.0168e+00
Epoch 5/10
10/10 - 1s - loss: 105.0777 - loglik: -1.0368e+02 - logprior: -1.4001e+00
Epoch 6/10
10/10 - 1s - loss: 104.8809 - loglik: -1.0370e+02 - logprior: -1.1794e+00
Epoch 7/10
10/10 - 1s - loss: 104.5624 - loglik: -1.0358e+02 - logprior: -9.8078e-01
Epoch 8/10
10/10 - 1s - loss: 104.1273 - loglik: -1.0322e+02 - logprior: -9.0361e-01
Epoch 9/10
10/10 - 1s - loss: 104.3902 - loglik: -1.0354e+02 - logprior: -8.5314e-01
Fitted a model with MAP estimate = -104.1118
Time for alignment: 29.2214
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 182.5372 - loglik: -1.6254e+02 - logprior: -1.9994e+01
Epoch 2/10
10/10 - 1s - loss: 152.5970 - loglik: -1.4693e+02 - logprior: -5.6630e+00
Epoch 3/10
10/10 - 1s - loss: 137.1978 - loglik: -1.3399e+02 - logprior: -3.2103e+00
Epoch 4/10
10/10 - 1s - loss: 125.6204 - loglik: -1.2307e+02 - logprior: -2.5501e+00
Epoch 5/10
10/10 - 1s - loss: 119.5696 - loglik: -1.1758e+02 - logprior: -1.9894e+00
Epoch 6/10
10/10 - 1s - loss: 117.2657 - loglik: -1.1561e+02 - logprior: -1.6527e+00
Epoch 7/10
10/10 - 1s - loss: 115.9009 - loglik: -1.1428e+02 - logprior: -1.6163e+00
Epoch 8/10
10/10 - 1s - loss: 114.7313 - loglik: -1.1310e+02 - logprior: -1.6341e+00
Epoch 9/10
10/10 - 1s - loss: 113.6298 - loglik: -1.1189e+02 - logprior: -1.7409e+00
Epoch 10/10
10/10 - 1s - loss: 113.1282 - loglik: -1.1137e+02 - logprior: -1.7588e+00
Fitted a model with MAP estimate = -113.0517
expansions: [(0, 2), (1, 2), (10, 1), (11, 2), (17, 1), (22, 1), (35, 4), (36, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 136.6124 - loglik: -1.1066e+02 - logprior: -2.5957e+01
Epoch 2/2
10/10 - 1s - loss: 113.8584 - loglik: -1.0558e+02 - logprior: -8.2750e+00
Fitted a model with MAP estimate = -109.5966
expansions: [(0, 2)]
discards: [ 0  1 16 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 125.9525 - loglik: -1.0509e+02 - logprior: -2.0867e+01
Epoch 2/2
10/10 - 1s - loss: 109.2556 - loglik: -1.0332e+02 - logprior: -5.9344e+00
Fitted a model with MAP estimate = -106.8127
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 123.8112 - loglik: -1.0511e+02 - logprior: -1.8705e+01
Epoch 2/10
10/10 - 1s - loss: 109.7236 - loglik: -1.0437e+02 - logprior: -5.3514e+00
Epoch 3/10
10/10 - 1s - loss: 106.7860 - loglik: -1.0373e+02 - logprior: -3.0566e+00
Epoch 4/10
10/10 - 1s - loss: 105.6887 - loglik: -1.0367e+02 - logprior: -2.0175e+00
Epoch 5/10
10/10 - 1s - loss: 105.0358 - loglik: -1.0363e+02 - logprior: -1.4023e+00
Epoch 6/10
10/10 - 1s - loss: 104.9037 - loglik: -1.0372e+02 - logprior: -1.1838e+00
Epoch 7/10
10/10 - 1s - loss: 104.3192 - loglik: -1.0334e+02 - logprior: -9.8266e-01
Epoch 8/10
10/10 - 1s - loss: 104.3719 - loglik: -1.0347e+02 - logprior: -9.0146e-01
Fitted a model with MAP estimate = -104.2233
Time for alignment: 28.5042
Computed alignments with likelihoods: ['-104.2370', '-103.9741', '-104.0160', '-104.1118', '-104.2233']
Best model has likelihood: -103.9741  (prior= -0.8170 )
time for generating output: 0.0939
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.975049504950495
Training of 5 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f80b7dc10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5b16310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f9216f190>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1ff93ceaf0>
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 482.7534 - loglik: -4.8101e+02 - logprior: -1.7469e+00
Epoch 2/10
39/39 - 8s - loss: 394.6267 - loglik: -3.9348e+02 - logprior: -1.1476e+00
Epoch 3/10
39/39 - 8s - loss: 384.4911 - loglik: -3.8331e+02 - logprior: -1.1819e+00
Epoch 4/10
39/39 - 8s - loss: 381.5283 - loglik: -3.8036e+02 - logprior: -1.1660e+00
Epoch 5/10
39/39 - 7s - loss: 379.8649 - loglik: -3.7875e+02 - logprior: -1.1173e+00
Epoch 6/10
39/39 - 7s - loss: 379.0133 - loglik: -3.7790e+02 - logprior: -1.1153e+00
Epoch 7/10
39/39 - 7s - loss: 378.4248 - loglik: -3.7731e+02 - logprior: -1.1195e+00
Epoch 8/10
39/39 - 7s - loss: 377.7138 - loglik: -3.7659e+02 - logprior: -1.1193e+00
Epoch 9/10
39/39 - 8s - loss: 377.3661 - loglik: -3.7624e+02 - logprior: -1.1256e+00
Epoch 10/10
39/39 - 8s - loss: 376.8998 - loglik: -3.7577e+02 - logprior: -1.1319e+00
Fitted a model with MAP estimate = -310.4179
expansions: [(0, 14), (11, 1), (14, 1), (19, 1), (20, 1), (28, 1), (30, 2), (31, 1), (32, 1), (35, 2), (36, 1), (37, 1), (42, 2), (43, 3), (71, 2), (85, 1), (87, 1), (88, 1), (90, 1), (97, 1), (100, 3), (101, 1), (106, 1), (125, 7), (126, 1), (134, 10)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 359.4371 - loglik: -3.5690e+02 - logprior: -2.5414e+00
Epoch 2/2
39/39 - 11s - loss: 340.6170 - loglik: -3.3913e+02 - logprior: -1.4826e+00
Fitted a model with MAP estimate = -282.5804
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  48  57
  68 185 186 187 188 189 190 191 192 193 194]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 166 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 356.5207 - loglik: -3.5469e+02 - logprior: -1.8306e+00
Epoch 2/2
39/39 - 9s - loss: 352.2357 - loglik: -3.5181e+02 - logprior: -4.2590e-01
Fitted a model with MAP estimate = -292.2297
expansions: [(0, 16), (1, 1), (166, 10)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 193 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 17s - loss: 284.9458 - loglik: -2.8346e+02 - logprior: -1.4854e+00
Epoch 2/10
52/52 - 12s - loss: 279.3230 - loglik: -2.7818e+02 - logprior: -1.1461e+00
Epoch 3/10
52/52 - 13s - loss: 274.3348 - loglik: -2.7322e+02 - logprior: -1.1109e+00
Epoch 4/10
52/52 - 13s - loss: 275.4677 - loglik: -2.7444e+02 - logprior: -1.0252e+00
Fitted a model with MAP estimate = -273.8934
Time for alignment: 243.0360
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 483.4958 - loglik: -4.8173e+02 - logprior: -1.7674e+00
Epoch 2/10
39/39 - 8s - loss: 392.6819 - loglik: -3.9146e+02 - logprior: -1.2250e+00
Epoch 3/10
39/39 - 7s - loss: 380.5790 - loglik: -3.7932e+02 - logprior: -1.2569e+00
Epoch 4/10
39/39 - 8s - loss: 378.0990 - loglik: -3.7685e+02 - logprior: -1.2532e+00
Epoch 5/10
39/39 - 8s - loss: 376.8905 - loglik: -3.7565e+02 - logprior: -1.2359e+00
Epoch 6/10
39/39 - 8s - loss: 375.5833 - loglik: -3.7435e+02 - logprior: -1.2380e+00
Epoch 7/10
39/39 - 8s - loss: 375.4229 - loglik: -3.7418e+02 - logprior: -1.2406e+00
Epoch 8/10
39/39 - 8s - loss: 375.0274 - loglik: -3.7378e+02 - logprior: -1.2426e+00
Epoch 9/10
39/39 - 8s - loss: 374.2523 - loglik: -3.7300e+02 - logprior: -1.2496e+00
Epoch 10/10
39/39 - 7s - loss: 374.0616 - loglik: -3.7280e+02 - logprior: -1.2573e+00
Fitted a model with MAP estimate = -308.7975
expansions: [(0, 18), (11, 1), (14, 1), (19, 1), (20, 1), (28, 1), (30, 2), (31, 2), (32, 1), (33, 1), (38, 2), (42, 2), (43, 1), (44, 1), (59, 1), (71, 1), (72, 1), (87, 1), (89, 1), (90, 1), (97, 1), (100, 2), (101, 1), (106, 1), (123, 6), (125, 4), (126, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 358.5928 - loglik: -3.5603e+02 - logprior: -2.5675e+00
Epoch 2/2
39/39 - 11s - loss: 340.0992 - loglik: -3.3881e+02 - logprior: -1.2914e+00
Fitted a model with MAP estimate = -282.3635
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 17 18 55 72]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 350.1715 - loglik: -3.4825e+02 - logprior: -1.9251e+00
Epoch 2/2
39/39 - 9s - loss: 344.8438 - loglik: -3.4402e+02 - logprior: -8.2466e-01
Fitted a model with MAP estimate = -286.9171
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 283.8195 - loglik: -2.8268e+02 - logprior: -1.1387e+00
Epoch 2/10
52/52 - 13s - loss: 281.8614 - loglik: -2.8110e+02 - logprior: -7.5807e-01
Epoch 3/10
52/52 - 10s - loss: 276.7500 - loglik: -2.7604e+02 - logprior: -7.1444e-01
Epoch 4/10
52/52 - 11s - loss: 277.0888 - loglik: -2.7642e+02 - logprior: -6.6952e-01
Fitted a model with MAP estimate = -276.0684
Time for alignment: 237.1433
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 482.9701 - loglik: -4.8122e+02 - logprior: -1.7548e+00
Epoch 2/10
39/39 - 7s - loss: 391.6972 - loglik: -3.9054e+02 - logprior: -1.1602e+00
Epoch 3/10
39/39 - 7s - loss: 380.1694 - loglik: -3.7895e+02 - logprior: -1.2223e+00
Epoch 4/10
39/39 - 8s - loss: 376.1725 - loglik: -3.7503e+02 - logprior: -1.1442e+00
Epoch 5/10
39/39 - 8s - loss: 374.2063 - loglik: -3.7309e+02 - logprior: -1.1141e+00
Epoch 6/10
39/39 - 8s - loss: 372.4861 - loglik: -3.7138e+02 - logprior: -1.1047e+00
Epoch 7/10
39/39 - 8s - loss: 372.3088 - loglik: -3.7120e+02 - logprior: -1.1111e+00
Epoch 8/10
39/39 - 8s - loss: 371.2718 - loglik: -3.7016e+02 - logprior: -1.1124e+00
Epoch 9/10
39/39 - 8s - loss: 370.9867 - loglik: -3.6988e+02 - logprior: -1.1091e+00
Epoch 10/10
39/39 - 8s - loss: 371.1361 - loglik: -3.7003e+02 - logprior: -1.1105e+00
Fitted a model with MAP estimate = -306.9092
expansions: [(0, 15), (11, 1), (19, 1), (20, 1), (28, 1), (30, 1), (31, 3), (33, 1), (38, 1), (42, 2), (43, 1), (44, 1), (68, 4), (71, 2), (73, 1), (86, 1), (87, 1), (88, 1), (89, 1), (99, 1), (100, 2), (101, 1), (104, 2), (123, 6), (125, 3), (126, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 190 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 357.6250 - loglik: -3.5499e+02 - logprior: -2.6348e+00
Epoch 2/2
39/39 - 11s - loss: 337.3880 - loglik: -3.3605e+02 - logprior: -1.3354e+00
Fitted a model with MAP estimate = -280.4480
expansions: [(172, 3), (177, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  98
  99 100 141 148]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 350.2488 - loglik: -3.4839e+02 - logprior: -1.8615e+00
Epoch 2/2
39/39 - 9s - loss: 343.4534 - loglik: -3.4284e+02 - logprior: -6.0923e-01
Fitted a model with MAP estimate = -286.7228
expansions: [(0, 17)]
discards: [152]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 188 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 282.1801 - loglik: -2.8077e+02 - logprior: -1.4130e+00
Epoch 2/10
52/52 - 13s - loss: 273.0689 - loglik: -2.7214e+02 - logprior: -9.2466e-01
Epoch 3/10
52/52 - 13s - loss: 273.5222 - loglik: -2.7269e+02 - logprior: -8.2943e-01
Fitted a model with MAP estimate = -270.5467
Time for alignment: 227.0931
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 481.8625 - loglik: -4.8010e+02 - logprior: -1.7587e+00
Epoch 2/10
39/39 - 7s - loss: 391.7697 - loglik: -3.9067e+02 - logprior: -1.1002e+00
Epoch 3/10
39/39 - 7s - loss: 380.1916 - loglik: -3.7910e+02 - logprior: -1.0896e+00
Epoch 4/10
39/39 - 8s - loss: 378.0418 - loglik: -3.7698e+02 - logprior: -1.0597e+00
Epoch 5/10
39/39 - 8s - loss: 376.5083 - loglik: -3.7546e+02 - logprior: -1.0504e+00
Epoch 6/10
39/39 - 8s - loss: 376.0807 - loglik: -3.7504e+02 - logprior: -1.0422e+00
Epoch 7/10
39/39 - 8s - loss: 375.4370 - loglik: -3.7440e+02 - logprior: -1.0407e+00
Epoch 8/10
39/39 - 8s - loss: 374.9716 - loglik: -3.7393e+02 - logprior: -1.0381e+00
Epoch 9/10
39/39 - 8s - loss: 374.6166 - loglik: -3.7357e+02 - logprior: -1.0448e+00
Epoch 10/10
39/39 - 8s - loss: 374.1116 - loglik: -3.7307e+02 - logprior: -1.0435e+00
Fitted a model with MAP estimate = -309.0471
expansions: [(0, 16), (11, 1), (14, 1), (19, 1), (20, 1), (28, 1), (29, 1), (31, 2), (32, 1), (33, 1), (38, 1), (43, 1), (44, 4), (59, 1), (71, 2), (85, 1), (87, 1), (88, 1), (90, 1), (97, 1), (100, 3), (101, 1), (105, 1), (123, 5), (125, 3), (126, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 360.2087 - loglik: -3.5762e+02 - logprior: -2.5913e+00
Epoch 2/2
39/39 - 10s - loss: 341.9804 - loglik: -3.4072e+02 - logprior: -1.2618e+00
Fitted a model with MAP estimate = -283.3870
expansions: [(176, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  73 104 105
 141 170 171 172]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 167 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 352.3942 - loglik: -3.5054e+02 - logprior: -1.8497e+00
Epoch 2/2
39/39 - 9s - loss: 347.0013 - loglik: -3.4641e+02 - logprior: -5.9019e-01
Fitted a model with MAP estimate = -289.0302
expansions: [(0, 18), (88, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 16s - loss: 285.3373 - loglik: -2.8380e+02 - logprior: -1.5350e+00
Epoch 2/10
52/52 - 11s - loss: 281.4575 - loglik: -2.8031e+02 - logprior: -1.1458e+00
Epoch 3/10
52/52 - 11s - loss: 277.2772 - loglik: -2.7615e+02 - logprior: -1.1233e+00
Epoch 4/10
52/52 - 13s - loss: 277.4702 - loglik: -2.7640e+02 - logprior: -1.0744e+00
Fitted a model with MAP estimate = -276.0401
Time for alignment: 235.6019
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 482.0859 - loglik: -4.8034e+02 - logprior: -1.7487e+00
Epoch 2/10
39/39 - 7s - loss: 390.2923 - loglik: -3.8919e+02 - logprior: -1.1061e+00
Epoch 3/10
39/39 - 7s - loss: 380.0008 - loglik: -3.7892e+02 - logprior: -1.0820e+00
Epoch 4/10
39/39 - 7s - loss: 377.3171 - loglik: -3.7624e+02 - logprior: -1.0769e+00
Epoch 5/10
39/39 - 8s - loss: 376.0169 - loglik: -3.7494e+02 - logprior: -1.0761e+00
Epoch 6/10
39/39 - 7s - loss: 375.0681 - loglik: -3.7399e+02 - logprior: -1.0750e+00
Epoch 7/10
39/39 - 8s - loss: 374.9146 - loglik: -3.7384e+02 - logprior: -1.0785e+00
Epoch 8/10
39/39 - 8s - loss: 374.3765 - loglik: -3.7331e+02 - logprior: -1.0690e+00
Epoch 9/10
39/39 - 7s - loss: 374.0098 - loglik: -3.7295e+02 - logprior: -1.0589e+00
Epoch 10/10
39/39 - 7s - loss: 373.8095 - loglik: -3.7274e+02 - logprior: -1.0698e+00
Fitted a model with MAP estimate = -309.2677
expansions: [(0, 15), (11, 1), (14, 1), (19, 1), (20, 1), (28, 1), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (43, 1), (44, 2), (69, 1), (72, 1), (87, 1), (88, 1), (89, 1), (90, 1), (97, 1), (100, 2), (101, 1), (106, 1), (123, 8), (125, 2), (134, 8)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 359.4861 - loglik: -3.5672e+02 - logprior: -2.7670e+00
Epoch 2/2
39/39 - 11s - loss: 341.2046 - loglik: -3.3967e+02 - logprior: -1.5395e+00
Fitted a model with MAP estimate = -283.4722
expansions: [(176, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  50  55 186 187
 188 189 190 191 192 193]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 349.7081 - loglik: -3.4770e+02 - logprior: -2.0128e+00
Epoch 2/2
39/39 - 9s - loss: 344.4308 - loglik: -3.4371e+02 - logprior: -7.2261e-01
Fitted a model with MAP estimate = -287.3505
expansions: [(0, 15), (171, 7)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 193 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 283.3116 - loglik: -2.8162e+02 - logprior: -1.6954e+00
Epoch 2/10
52/52 - 12s - loss: 277.5908 - loglik: -2.7628e+02 - logprior: -1.3067e+00
Epoch 3/10
52/52 - 13s - loss: 274.4313 - loglik: -2.7316e+02 - logprior: -1.2701e+00
Epoch 4/10
52/52 - 13s - loss: 274.9439 - loglik: -2.7372e+02 - logprior: -1.2260e+00
Fitted a model with MAP estimate = -273.5892
Time for alignment: 239.0916
Computed alignments with likelihoods: ['-273.8934', '-276.0684', '-270.5467', '-276.0401', '-273.5892']
Best model has likelihood: -270.5467  (prior= -0.9076 )
time for generating output: 0.5538
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.2255452900276217
Training of 5 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5a03610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf23c670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff0e9d7c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f2001c0fd30>
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 22s - loss: 827.9225 - loglik: -8.2642e+02 - logprior: -1.4991e+00
Epoch 2/10
37/37 - 20s - loss: 735.8123 - loglik: -7.3539e+02 - logprior: -4.2684e-01
Epoch 3/10
37/37 - 20s - loss: 722.1738 - loglik: -7.2178e+02 - logprior: -3.9415e-01
Epoch 4/10
37/37 - 20s - loss: 719.3279 - loglik: -7.1893e+02 - logprior: -4.0024e-01
Epoch 5/10
37/37 - 20s - loss: 718.2890 - loglik: -7.1789e+02 - logprior: -4.0051e-01
Epoch 6/10
37/37 - 20s - loss: 713.4658 - loglik: -7.1310e+02 - logprior: -3.6685e-01
Epoch 7/10
37/37 - 20s - loss: 714.5888 - loglik: -7.1425e+02 - logprior: -3.3982e-01
Fitted a model with MAP estimate = -714.5226
expansions: [(0, 3), (1, 1), (23, 1), (33, 4), (34, 2), (67, 1), (72, 5), (91, 1), (93, 10), (102, 1), (156, 6), (172, 1), (192, 1), (240, 1), (254, 2)]
discards: [152]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 293 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 720.9180 - loglik: -7.1858e+02 - logprior: -2.3428e+00
Epoch 2/2
37/37 - 24s - loss: 709.0486 - loglik: -7.0833e+02 - logprior: -7.1367e-01
Fitted a model with MAP estimate = -706.5397
expansions: [(0, 2), (40, 1), (43, 1), (111, 6), (113, 2), (246, 2), (293, 3)]
discards: [  0 179 180 184 187 188 189 291 292]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 301 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 713.9396 - loglik: -7.1203e+02 - logprior: -1.9054e+00
Epoch 2/2
37/37 - 26s - loss: 705.8262 - loglik: -7.0544e+02 - logprior: -3.8142e-01
Fitted a model with MAP estimate = -704.5124
expansions: [(0, 2), (190, 1), (193, 5), (194, 2), (251, 2), (301, 3)]
discards: [  1   2   3   4 116 117 118 119 298 299 300]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 711.3549 - loglik: -7.0919e+02 - logprior: -2.1650e+00
Epoch 2/10
37/37 - 26s - loss: 704.6160 - loglik: -7.0447e+02 - logprior: -1.4781e-01
Epoch 3/10
37/37 - 26s - loss: 701.8906 - loglik: -7.0196e+02 - logprior: 0.0656
Epoch 4/10
37/37 - 26s - loss: 698.9912 - loglik: -6.9917e+02 - logprior: 0.1805
Epoch 5/10
37/37 - 26s - loss: 701.0358 - loglik: -7.0134e+02 - logprior: 0.2998
Fitted a model with MAP estimate = -698.4430
Time for alignment: 505.2306
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 25s - loss: 828.4289 - loglik: -8.2692e+02 - logprior: -1.5124e+00
Epoch 2/10
37/37 - 20s - loss: 741.3714 - loglik: -7.4078e+02 - logprior: -5.9291e-01
Epoch 3/10
37/37 - 20s - loss: 727.4081 - loglik: -7.2685e+02 - logprior: -5.5723e-01
Epoch 4/10
37/37 - 20s - loss: 724.0783 - loglik: -7.2356e+02 - logprior: -5.1368e-01
Epoch 5/10
37/37 - 20s - loss: 723.6718 - loglik: -7.2317e+02 - logprior: -5.0468e-01
Epoch 6/10
37/37 - 20s - loss: 719.3057 - loglik: -7.1882e+02 - logprior: -4.9015e-01
Epoch 7/10
37/37 - 20s - loss: 720.5324 - loglik: -7.2006e+02 - logprior: -4.7244e-01
Fitted a model with MAP estimate = -719.2970
expansions: [(0, 3), (10, 1), (23, 1), (29, 1), (31, 4), (32, 2), (65, 1), (69, 1), (70, 5), (89, 1), (90, 3), (91, 9), (92, 5), (124, 7), (152, 5), (153, 5), (189, 2), (206, 2), (240, 1), (254, 3)]
discards: [  0 154 155 156 157 158 159 160 161 162]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 306 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 723.1752 - loglik: -7.2116e+02 - logprior: -2.0113e+00
Epoch 2/2
37/37 - 26s - loss: 708.7405 - loglik: -7.0801e+02 - logprior: -7.2779e-01
Fitted a model with MAP estimate = -705.2147
expansions: [(0, 3), (38, 1), (42, 1), (110, 4), (162, 1), (253, 1), (306, 3)]
discards: [ 85 114 115 179 180 181 182 183 184 185 186 187 197 198 205 206 233 303
 304 305]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 717.0454 - loglik: -7.1490e+02 - logprior: -2.1449e+00
Epoch 2/2
37/37 - 25s - loss: 709.6947 - loglik: -7.0932e+02 - logprior: -3.7502e-01
Fitted a model with MAP estimate = -706.6551
expansions: [(0, 2), (185, 4), (186, 2), (300, 3)]
discards: [  1   2   3   4   5 117 118 120 121 297 298 299]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 299 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 30s - loss: 713.0803 - loglik: -7.1077e+02 - logprior: -2.3068e+00
Epoch 2/10
37/37 - 25s - loss: 707.6992 - loglik: -7.0755e+02 - logprior: -1.4727e-01
Epoch 3/10
37/37 - 25s - loss: 703.6603 - loglik: -7.0345e+02 - logprior: -2.1294e-01
Epoch 4/10
37/37 - 25s - loss: 702.5334 - loglik: -7.0245e+02 - logprior: -7.9175e-02
Epoch 5/10
37/37 - 25s - loss: 701.0331 - loglik: -7.0116e+02 - logprior: 0.1306
Epoch 6/10
37/37 - 25s - loss: 699.2503 - loglik: -6.9952e+02 - logprior: 0.2675
Epoch 7/10
37/37 - 25s - loss: 699.9282 - loglik: -7.0031e+02 - logprior: 0.3786
Fitted a model with MAP estimate = -699.3071
Time for alignment: 557.6637
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 827.9355 - loglik: -8.2641e+02 - logprior: -1.5260e+00
Epoch 2/10
37/37 - 20s - loss: 734.5236 - loglik: -7.3397e+02 - logprior: -5.4892e-01
Epoch 3/10
37/37 - 20s - loss: 721.2698 - loglik: -7.2077e+02 - logprior: -5.0360e-01
Epoch 4/10
37/37 - 20s - loss: 716.9257 - loglik: -7.1641e+02 - logprior: -5.1865e-01
Epoch 5/10
37/37 - 20s - loss: 716.4377 - loglik: -7.1592e+02 - logprior: -5.2134e-01
Epoch 6/10
37/37 - 20s - loss: 714.8977 - loglik: -7.1441e+02 - logprior: -4.9000e-01
Epoch 7/10
37/37 - 20s - loss: 713.0385 - loglik: -7.1259e+02 - logprior: -4.4428e-01
Epoch 8/10
37/37 - 20s - loss: 713.9415 - loglik: -7.1351e+02 - logprior: -4.2710e-01
Fitted a model with MAP estimate = -713.0393
expansions: [(0, 3), (1, 1), (23, 1), (30, 2), (31, 3), (32, 2), (66, 1), (70, 1), (71, 5), (88, 1), (89, 2), (91, 4), (92, 6), (93, 3), (94, 4), (151, 9), (204, 1), (210, 2), (240, 1), (245, 1), (254, 2)]
discards: [152 153 154 155 156 157 158 159 160 161 250 251 252 253]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 295 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 721.7099 - loglik: -7.1946e+02 - logprior: -2.2529e+00
Epoch 2/2
37/37 - 25s - loss: 709.1437 - loglik: -7.0874e+02 - logprior: -3.9979e-01
Fitted a model with MAP estimate = -706.0972
expansions: [(0, 2), (40, 1), (41, 1), (135, 1), (294, 1), (295, 3)]
discards: [  0  87 110 111 112 115 249]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 297 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 713.1617 - loglik: -7.1125e+02 - logprior: -1.9085e+00
Epoch 2/2
37/37 - 25s - loss: 707.0392 - loglik: -7.0672e+02 - logprior: -3.1536e-01
Fitted a model with MAP estimate = -704.0711
expansions: [(0, 2), (248, 2), (297, 3)]
discards: [  1   2   3   4 110 111 112 113 295 296]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 294 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 30s - loss: 712.3598 - loglik: -7.1031e+02 - logprior: -2.0511e+00
Epoch 2/10
37/37 - 25s - loss: 707.9053 - loglik: -7.0789e+02 - logprior: -1.8252e-02
Epoch 3/10
37/37 - 25s - loss: 703.1891 - loglik: -7.0340e+02 - logprior: 0.2073
Epoch 4/10
37/37 - 25s - loss: 700.2866 - loglik: -7.0060e+02 - logprior: 0.3180
Epoch 5/10
37/37 - 25s - loss: 701.9482 - loglik: -7.0238e+02 - logprior: 0.4349
Fitted a model with MAP estimate = -700.1374
Time for alignment: 514.9748
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 829.4998 - loglik: -8.2799e+02 - logprior: -1.5116e+00
Epoch 2/10
37/37 - 20s - loss: 742.4534 - loglik: -7.4191e+02 - logprior: -5.4203e-01
Epoch 3/10
37/37 - 20s - loss: 727.8383 - loglik: -7.2734e+02 - logprior: -4.9940e-01
Epoch 4/10
37/37 - 20s - loss: 725.6618 - loglik: -7.2517e+02 - logprior: -4.9660e-01
Epoch 5/10
37/37 - 20s - loss: 721.6664 - loglik: -7.2117e+02 - logprior: -5.0054e-01
Epoch 6/10
37/37 - 20s - loss: 721.2323 - loglik: -7.2075e+02 - logprior: -4.7827e-01
Epoch 7/10
37/37 - 20s - loss: 721.8637 - loglik: -7.2139e+02 - logprior: -4.7184e-01
Fitted a model with MAP estimate = -720.0227
expansions: [(0, 3), (30, 4), (31, 2), (32, 1), (33, 1), (63, 1), (65, 1), (70, 6), (89, 1), (91, 3), (92, 6), (127, 4), (153, 9), (194, 1), (197, 7), (239, 1), (240, 1), (254, 2)]
discards: [154 155 156 157 158 159 160 161 162]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 299 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 723.6437 - loglik: -7.2118e+02 - logprior: -2.4616e+00
Epoch 2/2
37/37 - 25s - loss: 708.7307 - loglik: -7.0812e+02 - logprior: -6.0932e-01
Fitted a model with MAP estimate = -704.9830
expansions: [(0, 2), (36, 1), (159, 1), (239, 2), (299, 3)]
discards: [  1   2   3  84  85 116 178 297 298]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 299 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 713.5081 - loglik: -7.1077e+02 - logprior: -2.7363e+00
Epoch 2/2
37/37 - 25s - loss: 706.4448 - loglik: -7.0604e+02 - logprior: -4.0134e-01
Fitted a model with MAP estimate = -704.0951
expansions: [(0, 3), (3, 1), (176, 1), (235, 4), (299, 2)]
discards: [186 187 188 296 297 298]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 713.2146 - loglik: -7.1036e+02 - logprior: -2.8586e+00
Epoch 2/10
37/37 - 26s - loss: 707.1002 - loglik: -7.0677e+02 - logprior: -3.3070e-01
Epoch 3/10
37/37 - 26s - loss: 703.5048 - loglik: -7.0344e+02 - logprior: -6.9191e-02
Epoch 4/10
37/37 - 26s - loss: 701.1304 - loglik: -7.0119e+02 - logprior: 0.0587
Epoch 5/10
37/37 - 26s - loss: 702.1268 - loglik: -7.0230e+02 - logprior: 0.1731
Fitted a model with MAP estimate = -700.0498
Time for alignment: 503.3524
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 829.2025 - loglik: -8.2766e+02 - logprior: -1.5414e+00
Epoch 2/10
37/37 - 20s - loss: 740.6770 - loglik: -7.4007e+02 - logprior: -6.0538e-01
Epoch 3/10
37/37 - 20s - loss: 728.9395 - loglik: -7.2840e+02 - logprior: -5.3861e-01
Epoch 4/10
37/37 - 20s - loss: 725.1550 - loglik: -7.2460e+02 - logprior: -5.5142e-01
Epoch 5/10
37/37 - 20s - loss: 721.7970 - loglik: -7.2124e+02 - logprior: -5.6167e-01
Epoch 6/10
37/37 - 20s - loss: 722.3166 - loglik: -7.2178e+02 - logprior: -5.3505e-01
Fitted a model with MAP estimate = -721.1273
expansions: [(0, 3), (23, 1), (25, 1), (32, 6), (65, 1), (70, 5), (90, 1), (92, 5), (93, 1), (99, 1), (104, 1), (109, 1), (110, 1), (125, 1), (127, 1), (128, 4), (150, 3), (151, 1), (153, 5), (189, 1), (211, 2), (239, 1), (240, 1), (254, 2)]
discards: [155 156 157]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 301 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 723.4803 - loglik: -7.2112e+02 - logprior: -2.3575e+00
Epoch 2/2
37/37 - 25s - loss: 711.8911 - loglik: -7.1106e+02 - logprior: -8.3030e-01
Fitted a model with MAP estimate = -708.3629
expansions: [(0, 3), (37, 1), (38, 1), (39, 1), (252, 1), (253, 2), (301, 3)]
discards: [  1   2   3  84 114 159 189 299 300]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 716.6394 - loglik: -7.1393e+02 - logprior: -2.7106e+00
Epoch 2/2
37/37 - 26s - loss: 709.2394 - loglik: -7.0890e+02 - logprior: -3.3474e-01
Fitted a model with MAP estimate = -706.4557
expansions: [(0, 3), (189, 1), (253, 1), (304, 2)]
discards: [ 86 116 301 302 303]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 306 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 714.2553 - loglik: -7.1149e+02 - logprior: -2.7608e+00
Epoch 2/10
37/37 - 26s - loss: 709.2548 - loglik: -7.0903e+02 - logprior: -2.2949e-01
Epoch 3/10
37/37 - 26s - loss: 704.8022 - loglik: -7.0479e+02 - logprior: -1.0771e-02
Epoch 4/10
37/37 - 26s - loss: 704.1940 - loglik: -7.0431e+02 - logprior: 0.1128
Epoch 5/10
37/37 - 26s - loss: 701.5846 - loglik: -7.0181e+02 - logprior: 0.2235
Epoch 6/10
37/37 - 26s - loss: 700.3091 - loglik: -7.0064e+02 - logprior: 0.3271
Epoch 7/10
37/37 - 26s - loss: 702.9430 - loglik: -7.0336e+02 - logprior: 0.4127
Fitted a model with MAP estimate = -700.7707
Time for alignment: 540.7205
Computed alignments with likelihoods: ['-698.4430', '-699.3071', '-700.1374', '-700.0498', '-700.7707']
Best model has likelihood: -698.4430  (prior= 0.3133 )
time for generating output: 0.2834
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.8801670146137787
Training of 5 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fbd76ed30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1eed7f3760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f913ba070>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1e4a43cca0>
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 389.4366 - loglik: -2.7652e+02 - logprior: -1.1291e+02
Epoch 2/10
10/10 - 1s - loss: 293.7086 - loglik: -2.6690e+02 - logprior: -2.6809e+01
Epoch 3/10
10/10 - 1s - loss: 268.8610 - loglik: -2.5958e+02 - logprior: -9.2764e+00
Epoch 4/10
10/10 - 1s - loss: 251.7329 - loglik: -2.4907e+02 - logprior: -2.6618e+00
Epoch 5/10
10/10 - 1s - loss: 239.3154 - loglik: -2.3962e+02 - logprior: 0.3026
Epoch 6/10
10/10 - 1s - loss: 232.5068 - loglik: -2.3428e+02 - logprior: 1.7710
Epoch 7/10
10/10 - 1s - loss: 228.7603 - loglik: -2.3136e+02 - logprior: 2.5963
Epoch 8/10
10/10 - 1s - loss: 226.4574 - loglik: -2.2959e+02 - logprior: 3.1346
Epoch 9/10
10/10 - 1s - loss: 224.7677 - loglik: -2.2833e+02 - logprior: 3.5670
Epoch 10/10
10/10 - 1s - loss: 223.7221 - loglik: -2.2799e+02 - logprior: 4.2703
Fitted a model with MAP estimate = -223.3024
expansions: [(0, 7), (37, 5), (48, 3), (64, 4), (76, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 378.3917 - loglik: -2.3205e+02 - logprior: -1.4634e+02
Epoch 2/2
10/10 - 1s - loss: 267.8021 - loglik: -2.2658e+02 - logprior: -4.1219e+01
Fitted a model with MAP estimate = -247.8063
expansions: []
discards: [ 0  1 95 96 97 98 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 353.7990 - loglik: -2.2616e+02 - logprior: -1.2764e+02
Epoch 2/2
10/10 - 1s - loss: 272.0466 - loglik: -2.2485e+02 - logprior: -4.7194e+01
Fitted a model with MAP estimate = -259.2572
expansions: [(0, 6), (44, 4)]
discards: [ 0  1  2 80 81]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 338.2079 - loglik: -2.2487e+02 - logprior: -1.1334e+02
Epoch 2/10
10/10 - 1s - loss: 249.6471 - loglik: -2.2341e+02 - logprior: -2.6240e+01
Epoch 3/10
10/10 - 1s - loss: 229.5066 - loglik: -2.2356e+02 - logprior: -5.9454e+00
Epoch 4/10
10/10 - 1s - loss: 221.9984 - loglik: -2.2373e+02 - logprior: 1.7331
Epoch 5/10
10/10 - 1s - loss: 218.0137 - loglik: -2.2374e+02 - logprior: 5.7267
Epoch 6/10
10/10 - 1s - loss: 215.6127 - loglik: -2.2361e+02 - logprior: 7.9940
Epoch 7/10
10/10 - 1s - loss: 214.0465 - loglik: -2.2341e+02 - logprior: 9.3672
Epoch 8/10
10/10 - 1s - loss: 212.9374 - loglik: -2.2322e+02 - logprior: 10.2826
Epoch 9/10
10/10 - 1s - loss: 212.0889 - loglik: -2.2314e+02 - logprior: 11.0489
Epoch 10/10
10/10 - 1s - loss: 211.3766 - loglik: -2.2314e+02 - logprior: 11.7670
Fitted a model with MAP estimate = -211.0178
Time for alignment: 37.9925
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 389.4370 - loglik: -2.7652e+02 - logprior: -1.1291e+02
Epoch 2/10
10/10 - 1s - loss: 293.7082 - loglik: -2.6690e+02 - logprior: -2.6809e+01
Epoch 3/10
10/10 - 1s - loss: 268.8587 - loglik: -2.5958e+02 - logprior: -9.2765e+00
Epoch 4/10
10/10 - 1s - loss: 251.7315 - loglik: -2.4907e+02 - logprior: -2.6619e+00
Epoch 5/10
10/10 - 1s - loss: 239.3242 - loglik: -2.3963e+02 - logprior: 0.3031
Epoch 6/10
10/10 - 1s - loss: 232.5258 - loglik: -2.3430e+02 - logprior: 1.7733
Epoch 7/10
10/10 - 1s - loss: 228.7962 - loglik: -2.3140e+02 - logprior: 2.6073
Epoch 8/10
10/10 - 1s - loss: 226.4809 - loglik: -2.2963e+02 - logprior: 3.1486
Epoch 9/10
10/10 - 1s - loss: 224.7843 - loglik: -2.2835e+02 - logprior: 3.5699
Epoch 10/10
10/10 - 1s - loss: 223.7363 - loglik: -2.2801e+02 - logprior: 4.2737
Fitted a model with MAP estimate = -223.3155
expansions: [(0, 7), (37, 5), (48, 3), (64, 4), (76, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 378.3584 - loglik: -2.3202e+02 - logprior: -1.4634e+02
Epoch 2/2
10/10 - 1s - loss: 267.8000 - loglik: -2.2658e+02 - logprior: -4.1217e+01
Fitted a model with MAP estimate = -247.8072
expansions: [(66, 3)]
discards: [ 0  1 95 96 97 98 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 353.3466 - loglik: -2.2612e+02 - logprior: -1.2723e+02
Epoch 2/2
10/10 - 1s - loss: 271.1782 - loglik: -2.2438e+02 - logprior: -4.6794e+01
Fitted a model with MAP estimate = -258.3444
expansions: [(0, 5), (44, 4)]
discards: [ 0  1 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 337.0230 - loglik: -2.2393e+02 - logprior: -1.1309e+02
Epoch 2/10
10/10 - 1s - loss: 248.4304 - loglik: -2.2260e+02 - logprior: -2.5832e+01
Epoch 3/10
10/10 - 1s - loss: 228.1533 - loglik: -2.2279e+02 - logprior: -5.3603e+00
Epoch 4/10
10/10 - 1s - loss: 220.6539 - loglik: -2.2304e+02 - logprior: 2.3908
Epoch 5/10
10/10 - 1s - loss: 216.6507 - loglik: -2.2306e+02 - logprior: 6.4049
Epoch 6/10
10/10 - 1s - loss: 214.2167 - loglik: -2.2290e+02 - logprior: 8.6789
Epoch 7/10
10/10 - 1s - loss: 212.6280 - loglik: -2.2268e+02 - logprior: 10.0488
Epoch 8/10
10/10 - 1s - loss: 211.5168 - loglik: -2.2250e+02 - logprior: 10.9810
Epoch 9/10
10/10 - 1s - loss: 210.6627 - loglik: -2.2244e+02 - logprior: 11.7732
Epoch 10/10
10/10 - 1s - loss: 209.9396 - loglik: -2.2247e+02 - logprior: 12.5262
Fitted a model with MAP estimate = -209.5733
Time for alignment: 38.0440
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 389.4372 - loglik: -2.7652e+02 - logprior: -1.1291e+02
Epoch 2/10
10/10 - 1s - loss: 293.7070 - loglik: -2.6690e+02 - logprior: -2.6809e+01
Epoch 3/10
10/10 - 1s - loss: 268.8549 - loglik: -2.5958e+02 - logprior: -9.2767e+00
Epoch 4/10
10/10 - 1s - loss: 251.7151 - loglik: -2.4905e+02 - logprior: -2.6625e+00
Epoch 5/10
10/10 - 1s - loss: 239.2966 - loglik: -2.3960e+02 - logprior: 0.3015
Epoch 6/10
10/10 - 1s - loss: 232.5110 - loglik: -2.3428e+02 - logprior: 1.7724
Epoch 7/10
10/10 - 1s - loss: 228.7839 - loglik: -2.3139e+02 - logprior: 2.6043
Epoch 8/10
10/10 - 1s - loss: 226.4713 - loglik: -2.2962e+02 - logprior: 3.1439
Epoch 9/10
10/10 - 1s - loss: 224.7779 - loglik: -2.2835e+02 - logprior: 3.5705
Epoch 10/10
10/10 - 1s - loss: 223.7323 - loglik: -2.2801e+02 - logprior: 4.2737
Fitted a model with MAP estimate = -223.3115
expansions: [(0, 7), (37, 5), (48, 3), (64, 4), (76, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 378.3638 - loglik: -2.3202e+02 - logprior: -1.4634e+02
Epoch 2/2
10/10 - 1s - loss: 267.8000 - loglik: -2.2658e+02 - logprior: -4.1217e+01
Fitted a model with MAP estimate = -247.8072
expansions: [(66, 3)]
discards: [ 0  1 95 96 97 98 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 353.3482 - loglik: -2.2612e+02 - logprior: -1.2723e+02
Epoch 2/2
10/10 - 1s - loss: 271.1720 - loglik: -2.2438e+02 - logprior: -4.6794e+01
Fitted a model with MAP estimate = -258.3254
expansions: [(0, 5), (44, 4)]
discards: [ 0  1 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 337.0083 - loglik: -2.2392e+02 - logprior: -1.1309e+02
Epoch 2/10
10/10 - 1s - loss: 248.4296 - loglik: -2.2260e+02 - logprior: -2.5833e+01
Epoch 3/10
10/10 - 1s - loss: 228.1435 - loglik: -2.2277e+02 - logprior: -5.3763e+00
Epoch 4/10
10/10 - 1s - loss: 220.6196 - loglik: -2.2299e+02 - logprior: 2.3677
Epoch 5/10
10/10 - 1s - loss: 216.6151 - loglik: -2.2300e+02 - logprior: 6.3876
Epoch 6/10
10/10 - 1s - loss: 214.1804 - loglik: -2.2284e+02 - logprior: 8.6596
Epoch 7/10
10/10 - 1s - loss: 212.5778 - loglik: -2.2261e+02 - logprior: 10.0321
Epoch 8/10
10/10 - 1s - loss: 211.4500 - loglik: -2.2242e+02 - logprior: 10.9715
Epoch 9/10
10/10 - 1s - loss: 210.5886 - loglik: -2.2235e+02 - logprior: 11.7663
Epoch 10/10
10/10 - 1s - loss: 209.8631 - loglik: -2.2238e+02 - logprior: 12.5180
Fitted a model with MAP estimate = -209.4969
Time for alignment: 37.7136
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 389.4369 - loglik: -2.7652e+02 - logprior: -1.1291e+02
Epoch 2/10
10/10 - 1s - loss: 293.7087 - loglik: -2.6690e+02 - logprior: -2.6809e+01
Epoch 3/10
10/10 - 1s - loss: 268.8593 - loglik: -2.5958e+02 - logprior: -9.2764e+00
Epoch 4/10
10/10 - 1s - loss: 251.7250 - loglik: -2.4906e+02 - logprior: -2.6621e+00
Epoch 5/10
10/10 - 1s - loss: 239.3051 - loglik: -2.3961e+02 - logprior: 0.3021
Epoch 6/10
10/10 - 1s - loss: 232.5113 - loglik: -2.3428e+02 - logprior: 1.7715
Epoch 7/10
10/10 - 1s - loss: 228.7780 - loglik: -2.3138e+02 - logprior: 2.6013
Epoch 8/10
10/10 - 1s - loss: 226.4681 - loglik: -2.2961e+02 - logprior: 3.1412
Epoch 9/10
10/10 - 1s - loss: 224.7751 - loglik: -2.2834e+02 - logprior: 3.5688
Epoch 10/10
10/10 - 1s - loss: 223.7289 - loglik: -2.2800e+02 - logprior: 4.2722
Fitted a model with MAP estimate = -223.3083
expansions: [(0, 7), (37, 5), (48, 3), (64, 4), (76, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 378.3743 - loglik: -2.3203e+02 - logprior: -1.4634e+02
Epoch 2/2
10/10 - 1s - loss: 267.8010 - loglik: -2.2658e+02 - logprior: -4.1217e+01
Fitted a model with MAP estimate = -247.8089
expansions: [(66, 3)]
discards: [ 0  1 95 96 97 98 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 353.3586 - loglik: -2.2613e+02 - logprior: -1.2723e+02
Epoch 2/2
10/10 - 1s - loss: 271.1622 - loglik: -2.2437e+02 - logprior: -4.6797e+01
Fitted a model with MAP estimate = -258.2943
expansions: [(0, 6), (44, 4)]
discards: [ 0  1 83 84]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 337.3452 - loglik: -2.2422e+02 - logprior: -1.1313e+02
Epoch 2/10
10/10 - 1s - loss: 248.6537 - loglik: -2.2279e+02 - logprior: -2.5861e+01
Epoch 3/10
10/10 - 1s - loss: 228.3276 - loglik: -2.2291e+02 - logprior: -5.4136e+00
Epoch 4/10
10/10 - 1s - loss: 220.7869 - loglik: -2.2310e+02 - logprior: 2.3117
Epoch 5/10
10/10 - 1s - loss: 216.7834 - loglik: -2.2307e+02 - logprior: 6.2888
Epoch 6/10
10/10 - 1s - loss: 214.3659 - loglik: -2.2295e+02 - logprior: 8.5826
Epoch 7/10
10/10 - 1s - loss: 212.7832 - loglik: -2.2275e+02 - logprior: 9.9627
Epoch 8/10
10/10 - 1s - loss: 211.6763 - loglik: -2.2258e+02 - logprior: 10.9001
Epoch 9/10
10/10 - 1s - loss: 210.8255 - loglik: -2.2251e+02 - logprior: 11.6869
Epoch 10/10
10/10 - 1s - loss: 210.1049 - loglik: -2.2253e+02 - logprior: 12.4265
Fitted a model with MAP estimate = -209.7388
Time for alignment: 37.4634
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 389.4358 - loglik: -2.7652e+02 - logprior: -1.1291e+02
Epoch 2/10
10/10 - 1s - loss: 293.7106 - loglik: -2.6690e+02 - logprior: -2.6808e+01
Epoch 3/10
10/10 - 1s - loss: 268.8701 - loglik: -2.5959e+02 - logprior: -9.2760e+00
Epoch 4/10
10/10 - 1s - loss: 251.7469 - loglik: -2.4909e+02 - logprior: -2.6608e+00
Epoch 5/10
10/10 - 1s - loss: 239.3173 - loglik: -2.3962e+02 - logprior: 0.3033
Epoch 6/10
10/10 - 1s - loss: 232.5049 - loglik: -2.3428e+02 - logprior: 1.7707
Epoch 7/10
10/10 - 1s - loss: 228.7560 - loglik: -2.3135e+02 - logprior: 2.5934
Epoch 8/10
10/10 - 1s - loss: 226.4553 - loglik: -2.2959e+02 - logprior: 3.1323
Epoch 9/10
10/10 - 1s - loss: 224.7658 - loglik: -2.2833e+02 - logprior: 3.5650
Epoch 10/10
10/10 - 1s - loss: 223.7193 - loglik: -2.2799e+02 - logprior: 4.2689
Fitted a model with MAP estimate = -223.2998
expansions: [(0, 7), (37, 5), (48, 3), (64, 4), (76, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 378.4074 - loglik: -2.3206e+02 - logprior: -1.4635e+02
Epoch 2/2
10/10 - 1s - loss: 267.8048 - loglik: -2.2658e+02 - logprior: -4.1222e+01
Fitted a model with MAP estimate = -247.8044
expansions: []
discards: [ 0  1 95 96 97 98 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 353.7933 - loglik: -2.2615e+02 - logprior: -1.2764e+02
Epoch 2/2
10/10 - 1s - loss: 272.0071 - loglik: -2.2481e+02 - logprior: -4.7198e+01
Fitted a model with MAP estimate = -259.1969
expansions: [(0, 6), (44, 4)]
discards: [ 0  1 80 81]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 338.3110 - loglik: -2.2483e+02 - logprior: -1.1348e+02
Epoch 2/10
10/10 - 1s - loss: 249.6942 - loglik: -2.2338e+02 - logprior: -2.6309e+01
Epoch 3/10
10/10 - 1s - loss: 229.3980 - loglik: -2.2350e+02 - logprior: -5.8993e+00
Epoch 4/10
10/10 - 1s - loss: 221.8744 - loglik: -2.2368e+02 - logprior: 1.8039
Epoch 5/10
10/10 - 1s - loss: 217.8881 - loglik: -2.2368e+02 - logprior: 5.7899
Epoch 6/10
10/10 - 1s - loss: 215.4761 - loglik: -2.2355e+02 - logprior: 8.0694
Epoch 7/10
10/10 - 1s - loss: 213.8972 - loglik: -2.2335e+02 - logprior: 9.4500
Epoch 8/10
10/10 - 1s - loss: 212.7776 - loglik: -2.2315e+02 - logprior: 10.3702
Epoch 9/10
10/10 - 1s - loss: 211.9213 - loglik: -2.2306e+02 - logprior: 11.1385
Epoch 10/10
10/10 - 1s - loss: 211.2033 - loglik: -2.2307e+02 - logprior: 11.8670
Fitted a model with MAP estimate = -210.8409
Time for alignment: 37.7728
Computed alignments with likelihoods: ['-211.0178', '-209.5733', '-209.4969', '-209.7388', '-210.8409']
Best model has likelihood: -209.4969  (prior= 12.9087 )
time for generating output: 0.1406
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.7406434668417596
Training of 5 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f06f13fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f200a111a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f806c4640>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fdfc938b0>
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 1010.8000 - loglik: -1.0032e+03 - logprior: -7.6452e+00
Epoch 2/10
19/19 - 17s - loss: 883.7258 - loglik: -8.8485e+02 - logprior: 1.1277
Epoch 3/10
19/19 - 17s - loss: 825.8837 - loglik: -8.2612e+02 - logprior: 0.2353
Epoch 4/10
19/19 - 17s - loss: 805.7989 - loglik: -8.0560e+02 - logprior: -1.9577e-01
Epoch 5/10
19/19 - 17s - loss: 801.1942 - loglik: -8.0099e+02 - logprior: -2.0693e-01
Epoch 6/10
19/19 - 17s - loss: 796.2748 - loglik: -7.9600e+02 - logprior: -2.7535e-01
Epoch 7/10
19/19 - 17s - loss: 794.6574 - loglik: -7.9437e+02 - logprior: -2.9183e-01
Epoch 8/10
19/19 - 17s - loss: 792.7753 - loglik: -7.9254e+02 - logprior: -2.4022e-01
Epoch 9/10
19/19 - 17s - loss: 794.9051 - loglik: -7.9468e+02 - logprior: -2.2245e-01
Fitted a model with MAP estimate = -791.9780
expansions: [(14, 1), (63, 2), (64, 1), (119, 1), (124, 4), (125, 8), (127, 3), (128, 1), (144, 1), (146, 1), (147, 1), (165, 2), (167, 1), (168, 1), (169, 2), (171, 1), (180, 1), (181, 1), (182, 1), (191, 4), (207, 1), (209, 1), (213, 1), (222, 1), (223, 1), (229, 1), (238, 2), (240, 1), (242, 2), (258, 1), (266, 4), (268, 1), (274, 3), (277, 4), (278, 1), (289, 5), (301, 1), (303, 1), (312, 2), (313, 1), (314, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 393 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 805.3467 - loglik: -7.9408e+02 - logprior: -1.1270e+01
Epoch 2/2
19/19 - 23s - loss: 768.6736 - loglik: -7.6618e+02 - logprior: -2.4901e+00
Fitted a model with MAP estimate = -762.7398
expansions: [(0, 2), (226, 1)]
discards: [  0 131 145 146 196]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 391 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 770.1364 - loglik: -7.6336e+02 - logprior: -6.7726e+00
Epoch 2/2
19/19 - 23s - loss: 756.0065 - loglik: -7.5829e+02 - logprior: 2.2812
Fitted a model with MAP estimate = -751.7161
expansions: [(144, 3)]
discards: [  0 327 352]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 391 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 773.4896 - loglik: -7.6390e+02 - logprior: -9.5855e+00
Epoch 2/10
19/19 - 24s - loss: 756.7951 - loglik: -7.5729e+02 - logprior: 0.4959
Epoch 3/10
19/19 - 23s - loss: 754.8173 - loglik: -7.5875e+02 - logprior: 3.9297
Epoch 4/10
19/19 - 24s - loss: 747.0096 - loglik: -7.5159e+02 - logprior: 4.5782
Epoch 5/10
19/19 - 23s - loss: 743.3705 - loglik: -7.4809e+02 - logprior: 4.7179
Epoch 6/10
19/19 - 24s - loss: 742.0488 - loglik: -7.4690e+02 - logprior: 4.8469
Epoch 7/10
19/19 - 23s - loss: 739.6779 - loglik: -7.4456e+02 - logprior: 4.8859
Epoch 8/10
19/19 - 24s - loss: 744.2678 - loglik: -7.4961e+02 - logprior: 5.3382
Fitted a model with MAP estimate = -740.0427
Time for alignment: 516.1171
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 1010.5074 - loglik: -1.0029e+03 - logprior: -7.6367e+00
Epoch 2/10
19/19 - 17s - loss: 879.3593 - loglik: -8.8033e+02 - logprior: 0.9672
Epoch 3/10
19/19 - 17s - loss: 817.0486 - loglik: -8.1696e+02 - logprior: -8.6649e-02
Epoch 4/10
19/19 - 17s - loss: 797.3240 - loglik: -7.9703e+02 - logprior: -2.9708e-01
Epoch 5/10
19/19 - 17s - loss: 791.5782 - loglik: -7.9134e+02 - logprior: -2.4124e-01
Epoch 6/10
19/19 - 17s - loss: 787.0612 - loglik: -7.8672e+02 - logprior: -3.3871e-01
Epoch 7/10
19/19 - 17s - loss: 784.7209 - loglik: -7.8428e+02 - logprior: -4.4178e-01
Epoch 8/10
19/19 - 17s - loss: 786.5822 - loglik: -7.8606e+02 - logprior: -5.2508e-01
Fitted a model with MAP estimate = -783.9304
expansions: [(33, 1), (40, 1), (41, 3), (66, 1), (95, 1), (96, 1), (97, 2), (98, 1), (113, 1), (119, 1), (121, 8), (122, 2), (142, 1), (143, 1), (145, 1), (152, 2), (162, 2), (164, 1), (165, 1), (166, 2), (175, 1), (177, 1), (178, 1), (190, 3), (198, 1), (205, 2), (207, 1), (210, 2), (217, 2), (221, 1), (222, 2), (237, 1), (238, 2), (239, 1), (241, 1), (265, 8), (272, 1), (273, 1), (276, 1), (277, 1), (278, 1), (279, 1), (293, 1), (301, 1), (302, 1), (303, 1), (312, 2), (313, 1), (314, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 397 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 797.8243 - loglik: -7.8708e+02 - logprior: -1.0743e+01
Epoch 2/2
19/19 - 24s - loss: 761.1429 - loglik: -7.5889e+02 - logprior: -2.2509e+00
Fitted a model with MAP estimate = -755.6552
expansions: [(0, 2), (140, 1), (377, 1)]
discards: [  0  42 105 144 145 178 197 253 323]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 392 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 767.7835 - loglik: -7.6138e+02 - logprior: -6.4004e+00
Epoch 2/2
19/19 - 24s - loss: 749.5679 - loglik: -7.5208e+02 - logprior: 2.5155
Fitted a model with MAP estimate = -746.4573
expansions: []
discards: [  0 144]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 28s - loss: 768.7480 - loglik: -7.5924e+02 - logprior: -9.5061e+00
Epoch 2/10
19/19 - 24s - loss: 753.8015 - loglik: -7.5440e+02 - logprior: 0.6003
Epoch 3/10
19/19 - 23s - loss: 745.3221 - loglik: -7.4909e+02 - logprior: 3.7646
Epoch 4/10
19/19 - 23s - loss: 740.4032 - loglik: -7.4485e+02 - logprior: 4.4513
Epoch 5/10
19/19 - 23s - loss: 736.9140 - loglik: -7.4161e+02 - logprior: 4.6928
Epoch 6/10
19/19 - 23s - loss: 740.3449 - loglik: -7.4520e+02 - logprior: 4.8553
Fitted a model with MAP estimate = -734.3662
Time for alignment: 454.0084
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 1010.0892 - loglik: -1.0025e+03 - logprior: -7.6332e+00
Epoch 2/10
19/19 - 17s - loss: 886.3711 - loglik: -8.8745e+02 - logprior: 1.0749
Epoch 3/10
19/19 - 17s - loss: 825.9744 - loglik: -8.2628e+02 - logprior: 0.3013
Epoch 4/10
19/19 - 17s - loss: 799.3110 - loglik: -7.9941e+02 - logprior: 0.0966
Epoch 5/10
19/19 - 17s - loss: 799.5305 - loglik: -7.9956e+02 - logprior: 0.0275
Fitted a model with MAP estimate = -793.4379
expansions: [(14, 1), (40, 1), (41, 3), (55, 1), (94, 1), (95, 1), (96, 3), (120, 3), (121, 8), (122, 2), (142, 1), (144, 1), (149, 1), (166, 2), (167, 2), (170, 1), (177, 2), (181, 2), (182, 1), (191, 2), (192, 1), (204, 1), (206, 1), (209, 2), (212, 2), (223, 2), (238, 3), (267, 7), (279, 4), (280, 2), (290, 4), (300, 1), (301, 1), (302, 2), (304, 1), (312, 2), (313, 1), (314, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 396 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 806.1175 - loglik: -7.9482e+02 - logprior: -1.1300e+01
Epoch 2/2
19/19 - 24s - loss: 772.0993 - loglik: -7.6930e+02 - logprior: -2.8033e+00
Fitted a model with MAP estimate = -764.1399
expansions: [(0, 2), (286, 1)]
discards: [  0  42  43 104 138 139 192 208 318 337 353 354 367]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 386 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 776.9906 - loglik: -7.7043e+02 - logprior: -6.5558e+00
Epoch 2/2
19/19 - 23s - loss: 756.8671 - loglik: -7.5893e+02 - logprior: 2.0581
Fitted a model with MAP estimate = -756.0368
expansions: [(136, 1)]
discards: [  0 208]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 777.5135 - loglik: -7.6772e+02 - logprior: -9.7923e+00
Epoch 2/10
19/19 - 23s - loss: 761.9331 - loglik: -7.6217e+02 - logprior: 0.2361
Epoch 3/10
19/19 - 23s - loss: 754.2538 - loglik: -7.5770e+02 - logprior: 3.4508
Epoch 4/10
19/19 - 23s - loss: 750.4896 - loglik: -7.5449e+02 - logprior: 4.0029
Epoch 5/10
19/19 - 23s - loss: 744.8478 - loglik: -7.4899e+02 - logprior: 4.1450
Epoch 6/10
19/19 - 23s - loss: 748.0097 - loglik: -7.5234e+02 - logprior: 4.3299
Fitted a model with MAP estimate = -744.7927
Time for alignment: 394.5422
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 1008.7775 - loglik: -1.0012e+03 - logprior: -7.6166e+00
Epoch 2/10
19/19 - 17s - loss: 879.0242 - loglik: -8.8007e+02 - logprior: 1.0464
Epoch 3/10
19/19 - 17s - loss: 819.1631 - loglik: -8.1933e+02 - logprior: 0.1664
Epoch 4/10
19/19 - 17s - loss: 799.4417 - loglik: -7.9944e+02 - logprior: -3.8201e-03
Epoch 5/10
19/19 - 17s - loss: 793.3620 - loglik: -7.9344e+02 - logprior: 0.0826
Epoch 6/10
19/19 - 17s - loss: 791.6696 - loglik: -7.9170e+02 - logprior: 0.0312
Epoch 7/10
19/19 - 17s - loss: 786.3176 - loglik: -7.8636e+02 - logprior: 0.0419
Epoch 8/10
19/19 - 17s - loss: 788.9016 - loglik: -7.8900e+02 - logprior: 0.0979
Fitted a model with MAP estimate = -787.0432
expansions: [(14, 2), (40, 2), (95, 1), (96, 1), (97, 3), (121, 1), (123, 9), (124, 2), (143, 1), (145, 1), (146, 1), (149, 1), (164, 1), (165, 1), (166, 2), (170, 1), (176, 1), (181, 1), (191, 3), (199, 1), (207, 1), (212, 2), (213, 2), (221, 1), (222, 2), (237, 3), (266, 6), (293, 9), (301, 1), (302, 1), (312, 2), (313, 1), (314, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 798.6600 - loglik: -7.8779e+02 - logprior: -1.0868e+01
Epoch 2/2
19/19 - 23s - loss: 763.9190 - loglik: -7.6171e+02 - logprior: -2.2111e+00
Fitted a model with MAP estimate = -759.8408
expansions: [(0, 2), (245, 1), (257, 2), (281, 1), (282, 1)]
discards: [  0  13  42 102 103 144 145 248 249]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 771.6590 - loglik: -7.6537e+02 - logprior: -6.2863e+00
Epoch 2/2
19/19 - 23s - loss: 754.0428 - loglik: -7.5644e+02 - logprior: 2.3939
Fitted a model with MAP estimate = -751.5394
expansions: [(140, 2)]
discards: [  0 252]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 772.9529 - loglik: -7.6345e+02 - logprior: -9.5010e+00
Epoch 2/10
19/19 - 23s - loss: 756.5828 - loglik: -7.5704e+02 - logprior: 0.4571
Epoch 3/10
19/19 - 23s - loss: 748.5206 - loglik: -7.5237e+02 - logprior: 3.8446
Epoch 4/10
19/19 - 23s - loss: 746.7408 - loglik: -7.5113e+02 - logprior: 4.3936
Epoch 5/10
19/19 - 23s - loss: 741.7428 - loglik: -7.4636e+02 - logprior: 4.6195
Epoch 6/10
19/19 - 23s - loss: 744.3833 - loglik: -7.4921e+02 - logprior: 4.8246
Fitted a model with MAP estimate = -740.9224
Time for alignment: 443.1851
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 1010.0853 - loglik: -1.0024e+03 - logprior: -7.6378e+00
Epoch 2/10
19/19 - 17s - loss: 889.4444 - loglik: -8.9068e+02 - logprior: 1.2357
Epoch 3/10
19/19 - 17s - loss: 820.1351 - loglik: -8.2054e+02 - logprior: 0.4063
Epoch 4/10
19/19 - 17s - loss: 804.0807 - loglik: -8.0400e+02 - logprior: -7.9858e-02
Epoch 5/10
19/19 - 17s - loss: 798.7361 - loglik: -7.9856e+02 - logprior: -1.7675e-01
Epoch 6/10
19/19 - 17s - loss: 795.3314 - loglik: -7.9515e+02 - logprior: -1.8049e-01
Epoch 7/10
19/19 - 17s - loss: 792.0676 - loglik: -7.9184e+02 - logprior: -2.2685e-01
Epoch 8/10
19/19 - 17s - loss: 794.7970 - loglik: -7.9459e+02 - logprior: -2.0205e-01
Fitted a model with MAP estimate = -791.7406
expansions: [(14, 1), (40, 1), (41, 3), (68, 2), (97, 5), (119, 1), (120, 1), (121, 3), (123, 6), (124, 1), (143, 1), (145, 1), (146, 1), (164, 2), (165, 2), (166, 1), (168, 1), (173, 1), (179, 1), (180, 1), (181, 1), (191, 2), (192, 1), (204, 1), (206, 2), (208, 1), (211, 2), (212, 1), (221, 1), (222, 1), (223, 1), (228, 1), (238, 3), (264, 1), (266, 11), (302, 1), (303, 1), (304, 3), (312, 2), (313, 1), (314, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 394 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 810.9415 - loglik: -7.9931e+02 - logprior: -1.1630e+01
Epoch 2/2
19/19 - 24s - loss: 776.7100 - loglik: -7.7326e+02 - logprior: -3.4542e+00
Fitted a model with MAP estimate = -771.0385
expansions: [(0, 2), (290, 1)]
discards: [  0  42  43  73 136 137 138 139 147 193 254 255 321 322 323 324 325 326
 327 371 372]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 376 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 784.4288 - loglik: -7.7792e+02 - logprior: -6.5114e+00
Epoch 2/2
19/19 - 22s - loss: 770.7770 - loglik: -7.7294e+02 - logprior: 2.1678
Fitted a model with MAP estimate = -765.1687
expansions: [(124, 1)]
discards: [  0 101 102]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 374 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 787.6891 - loglik: -7.7794e+02 - logprior: -9.7443e+00
Epoch 2/10
19/19 - 22s - loss: 769.0228 - loglik: -7.6932e+02 - logprior: 0.2996
Epoch 3/10
19/19 - 22s - loss: 765.3022 - loglik: -7.6903e+02 - logprior: 3.7309
Epoch 4/10
19/19 - 22s - loss: 763.0482 - loglik: -7.6732e+02 - logprior: 4.2672
Epoch 5/10
19/19 - 22s - loss: 754.5158 - loglik: -7.5896e+02 - logprior: 4.4448
Epoch 6/10
19/19 - 22s - loss: 759.0584 - loglik: -7.6366e+02 - logprior: 4.6027
Fitted a model with MAP estimate = -755.6146
Time for alignment: 435.9085
Computed alignments with likelihoods: ['-740.0427', '-734.3662', '-744.7927', '-740.9224', '-755.6146']
Best model has likelihood: -734.3662  (prior= 5.1957 )
time for generating output: 0.4279
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.7671274038461539
Training of 5 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5a03a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f927b24f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e4ac2b130>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fa3ec6b80>
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.2301 - loglik: -3.2004e+02 - logprior: -5.6193e+01
Epoch 2/10
10/10 - 1s - loss: 300.3954 - loglik: -2.8715e+02 - logprior: -1.3250e+01
Epoch 3/10
10/10 - 1s - loss: 251.3780 - loglik: -2.4576e+02 - logprior: -5.6183e+00
Epoch 4/10
10/10 - 1s - loss: 220.2782 - loglik: -2.1693e+02 - logprior: -3.3465e+00
Epoch 5/10
10/10 - 1s - loss: 207.1370 - loglik: -2.0494e+02 - logprior: -2.1993e+00
Epoch 6/10
10/10 - 1s - loss: 202.3136 - loglik: -2.0079e+02 - logprior: -1.5236e+00
Epoch 7/10
10/10 - 1s - loss: 200.0504 - loglik: -1.9902e+02 - logprior: -1.0293e+00
Epoch 8/10
10/10 - 1s - loss: 199.0390 - loglik: -1.9840e+02 - logprior: -6.4148e-01
Epoch 9/10
10/10 - 1s - loss: 197.8878 - loglik: -1.9751e+02 - logprior: -3.7723e-01
Epoch 10/10
10/10 - 1s - loss: 197.9865 - loglik: -1.9781e+02 - logprior: -1.8056e-01
Fitted a model with MAP estimate = -197.5367
expansions: [(13, 3), (18, 5), (31, 1), (34, 2), (57, 5), (58, 1), (59, 1), (64, 1), (77, 3), (78, 1), (81, 1), (90, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 257.3325 - loglik: -1.9424e+02 - logprior: -6.3090e+01
Epoch 2/2
10/10 - 1s - loss: 204.4061 - loglik: -1.8006e+02 - logprior: -2.4342e+01
Fitted a model with MAP estimate = -194.5135
expansions: [(0, 2)]
discards: [ 0 43 95 96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 225.1960 - loglik: -1.7538e+02 - logprior: -4.9813e+01
Epoch 2/2
10/10 - 1s - loss: 182.8252 - loglik: -1.7180e+02 - logprior: -1.1020e+01
Fitted a model with MAP estimate = -176.4660
expansions: [(114, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 235.5661 - loglik: -1.7453e+02 - logprior: -6.1034e+01
Epoch 2/10
10/10 - 1s - loss: 191.1065 - loglik: -1.7257e+02 - logprior: -1.8534e+01
Epoch 3/10
10/10 - 1s - loss: 175.9836 - loglik: -1.7120e+02 - logprior: -4.7852e+00
Epoch 4/10
10/10 - 1s - loss: 170.3508 - loglik: -1.7061e+02 - logprior: 0.2622
Epoch 5/10
10/10 - 1s - loss: 167.9860 - loglik: -1.7021e+02 - logprior: 2.2201
Epoch 6/10
10/10 - 1s - loss: 167.1989 - loglik: -1.7050e+02 - logprior: 3.2966
Epoch 7/10
10/10 - 1s - loss: 165.9071 - loglik: -1.7007e+02 - logprior: 4.1618
Epoch 8/10
10/10 - 1s - loss: 165.5408 - loglik: -1.7033e+02 - logprior: 4.7918
Epoch 9/10
10/10 - 1s - loss: 165.1987 - loglik: -1.7041e+02 - logprior: 5.2131
Epoch 10/10
10/10 - 1s - loss: 164.7245 - loglik: -1.7027e+02 - logprior: 5.5411
Fitted a model with MAP estimate = -164.5267
Time for alignment: 43.6923
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.2285 - loglik: -3.2003e+02 - logprior: -5.6193e+01
Epoch 2/10
10/10 - 1s - loss: 300.7750 - loglik: -2.8752e+02 - logprior: -1.3258e+01
Epoch 3/10
10/10 - 1s - loss: 254.1469 - loglik: -2.4845e+02 - logprior: -5.6933e+00
Epoch 4/10
10/10 - 1s - loss: 222.8493 - loglik: -2.1929e+02 - logprior: -3.5642e+00
Epoch 5/10
10/10 - 1s - loss: 208.3486 - loglik: -2.0582e+02 - logprior: -2.5308e+00
Epoch 6/10
10/10 - 1s - loss: 203.0922 - loglik: -2.0126e+02 - logprior: -1.8352e+00
Epoch 7/10
10/10 - 1s - loss: 200.2176 - loglik: -1.9883e+02 - logprior: -1.3839e+00
Epoch 8/10
10/10 - 1s - loss: 199.2207 - loglik: -1.9818e+02 - logprior: -1.0387e+00
Epoch 9/10
10/10 - 1s - loss: 198.0674 - loglik: -1.9713e+02 - logprior: -9.3596e-01
Epoch 10/10
10/10 - 1s - loss: 196.5287 - loglik: -1.9569e+02 - logprior: -8.4239e-01
Fitted a model with MAP estimate = -196.4925
expansions: [(11, 1), (13, 2), (18, 6), (31, 1), (34, 1), (35, 1), (56, 1), (57, 4), (58, 1), (59, 1), (64, 1), (77, 3), (78, 1), (81, 1), (86, 1), (89, 1), (90, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 257.5933 - loglik: -1.9433e+02 - logprior: -6.3262e+01
Epoch 2/2
10/10 - 1s - loss: 203.7823 - loglik: -1.7924e+02 - logprior: -2.4541e+01
Fitted a model with MAP estimate = -193.9278
expansions: [(0, 2)]
discards: [ 0 23 70 96 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.4792 - loglik: -1.7670e+02 - logprior: -4.9776e+01
Epoch 2/2
10/10 - 1s - loss: 184.5107 - loglik: -1.7357e+02 - logprior: -1.0942e+01
Fitted a model with MAP estimate = -177.6773
expansions: []
discards: [ 0 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 237.4244 - loglik: -1.7699e+02 - logprior: -6.0433e+01
Epoch 2/10
10/10 - 1s - loss: 192.2896 - loglik: -1.7519e+02 - logprior: -1.7100e+01
Epoch 3/10
10/10 - 1s - loss: 178.5161 - loglik: -1.7431e+02 - logprior: -4.2103e+00
Epoch 4/10
10/10 - 1s - loss: 173.4877 - loglik: -1.7373e+02 - logprior: 0.2415
Epoch 5/10
10/10 - 1s - loss: 171.4109 - loglik: -1.7355e+02 - logprior: 2.1352
Epoch 6/10
10/10 - 1s - loss: 170.0124 - loglik: -1.7324e+02 - logprior: 3.2263
Epoch 7/10
10/10 - 1s - loss: 169.6062 - loglik: -1.7369e+02 - logprior: 4.0832
Epoch 8/10
10/10 - 1s - loss: 168.8224 - loglik: -1.7354e+02 - logprior: 4.7146
Epoch 9/10
10/10 - 1s - loss: 168.6048 - loglik: -1.7374e+02 - logprior: 5.1323
Epoch 10/10
10/10 - 1s - loss: 167.7233 - loglik: -1.7318e+02 - logprior: 5.4562
Fitted a model with MAP estimate = -167.8588
Time for alignment: 42.7303
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.1396 - loglik: -3.1994e+02 - logprior: -5.6195e+01
Epoch 2/10
10/10 - 1s - loss: 300.4882 - loglik: -2.8723e+02 - logprior: -1.3254e+01
Epoch 3/10
10/10 - 1s - loss: 250.4196 - loglik: -2.4473e+02 - logprior: -5.6942e+00
Epoch 4/10
10/10 - 1s - loss: 218.6307 - loglik: -2.1502e+02 - logprior: -3.6147e+00
Epoch 5/10
10/10 - 1s - loss: 206.1466 - loglik: -2.0352e+02 - logprior: -2.6288e+00
Epoch 6/10
10/10 - 1s - loss: 201.7592 - loglik: -1.9979e+02 - logprior: -1.9694e+00
Epoch 7/10
10/10 - 1s - loss: 199.9268 - loglik: -1.9854e+02 - logprior: -1.3878e+00
Epoch 8/10
10/10 - 1s - loss: 198.6478 - loglik: -1.9765e+02 - logprior: -1.0000e+00
Epoch 9/10
10/10 - 1s - loss: 197.1953 - loglik: -1.9627e+02 - logprior: -9.2601e-01
Epoch 10/10
10/10 - 1s - loss: 196.9426 - loglik: -1.9614e+02 - logprior: -7.9881e-01
Fitted a model with MAP estimate = -196.4015
expansions: [(13, 3), (18, 5), (31, 1), (34, 1), (35, 1), (56, 3), (57, 1), (58, 1), (59, 1), (61, 1), (75, 1), (77, 1), (81, 1), (86, 1), (89, 1), (90, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 258.3797 - loglik: -1.9515e+02 - logprior: -6.3233e+01
Epoch 2/2
10/10 - 1s - loss: 205.1476 - loglik: -1.8076e+02 - logprior: -2.4387e+01
Fitted a model with MAP estimate = -195.5127
expansions: [(0, 2)]
discards: [ 0 21]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.2698 - loglik: -1.7635e+02 - logprior: -4.9924e+01
Epoch 2/2
10/10 - 1s - loss: 183.4556 - loglik: -1.7229e+02 - logprior: -1.1161e+01
Fitted a model with MAP estimate = -177.0797
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 235.4377 - loglik: -1.7496e+02 - logprior: -6.0482e+01
Epoch 2/10
10/10 - 1s - loss: 190.0465 - loglik: -1.7318e+02 - logprior: -1.6869e+01
Epoch 3/10
10/10 - 1s - loss: 176.4580 - loglik: -1.7226e+02 - logprior: -4.1956e+00
Epoch 4/10
10/10 - 1s - loss: 171.8100 - loglik: -1.7193e+02 - logprior: 0.1219
Epoch 5/10
10/10 - 1s - loss: 169.5928 - loglik: -1.7158e+02 - logprior: 1.9828
Epoch 6/10
10/10 - 1s - loss: 168.5435 - loglik: -1.7162e+02 - logprior: 3.0755
Epoch 7/10
10/10 - 1s - loss: 167.2921 - loglik: -1.7123e+02 - logprior: 3.9365
Epoch 8/10
10/10 - 1s - loss: 166.9960 - loglik: -1.7157e+02 - logprior: 4.5698
Epoch 9/10
10/10 - 1s - loss: 167.0109 - loglik: -1.7200e+02 - logprior: 4.9933
Fitted a model with MAP estimate = -166.3770
Time for alignment: 40.8848
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.8138 - loglik: -3.1962e+02 - logprior: -5.6193e+01
Epoch 2/10
10/10 - 1s - loss: 300.7160 - loglik: -2.8746e+02 - logprior: -1.3252e+01
Epoch 3/10
10/10 - 1s - loss: 252.6539 - loglik: -2.4696e+02 - logprior: -5.6980e+00
Epoch 4/10
10/10 - 1s - loss: 218.9830 - loglik: -2.1534e+02 - logprior: -3.6425e+00
Epoch 5/10
10/10 - 1s - loss: 206.3201 - loglik: -2.0372e+02 - logprior: -2.5999e+00
Epoch 6/10
10/10 - 1s - loss: 201.4522 - loglik: -1.9953e+02 - logprior: -1.9197e+00
Epoch 7/10
10/10 - 1s - loss: 198.9801 - loglik: -1.9748e+02 - logprior: -1.4966e+00
Epoch 8/10
10/10 - 1s - loss: 197.5092 - loglik: -1.9635e+02 - logprior: -1.1610e+00
Epoch 9/10
10/10 - 1s - loss: 196.7895 - loglik: -1.9586e+02 - logprior: -9.2754e-01
Epoch 10/10
10/10 - 1s - loss: 196.1980 - loglik: -1.9539e+02 - logprior: -8.1155e-01
Fitted a model with MAP estimate = -195.7295
expansions: [(11, 1), (13, 2), (18, 6), (31, 1), (34, 1), (35, 1), (56, 3), (57, 2), (58, 1), (59, 1), (61, 1), (77, 3), (78, 1), (81, 1), (86, 1), (89, 1), (90, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 257.5552 - loglik: -1.9430e+02 - logprior: -6.3259e+01
Epoch 2/2
10/10 - 1s - loss: 203.8051 - loglik: -1.7922e+02 - logprior: -2.4587e+01
Fitted a model with MAP estimate = -194.6519
expansions: [(0, 2)]
discards: [ 0 23 71 96 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.3722 - loglik: -1.7658e+02 - logprior: -4.9796e+01
Epoch 2/2
10/10 - 1s - loss: 183.5615 - loglik: -1.7261e+02 - logprior: -1.0955e+01
Fitted a model with MAP estimate = -177.0497
expansions: []
discards: [ 0 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 236.2553 - loglik: -1.7582e+02 - logprior: -6.0438e+01
Epoch 2/10
10/10 - 1s - loss: 191.7125 - loglik: -1.7462e+02 - logprior: -1.7095e+01
Epoch 3/10
10/10 - 1s - loss: 177.7263 - loglik: -1.7350e+02 - logprior: -4.2216e+00
Epoch 4/10
10/10 - 1s - loss: 172.6965 - loglik: -1.7293e+02 - logprior: 0.2343
Epoch 5/10
10/10 - 1s - loss: 170.7123 - loglik: -1.7284e+02 - logprior: 2.1229
Epoch 6/10
10/10 - 1s - loss: 169.2758 - loglik: -1.7250e+02 - logprior: 3.2205
Epoch 7/10
10/10 - 1s - loss: 168.7188 - loglik: -1.7280e+02 - logprior: 4.0788
Epoch 8/10
10/10 - 1s - loss: 168.0203 - loglik: -1.7273e+02 - logprior: 4.7090
Epoch 9/10
10/10 - 1s - loss: 167.8088 - loglik: -1.7294e+02 - logprior: 5.1282
Epoch 10/10
10/10 - 1s - loss: 167.3881 - loglik: -1.7283e+02 - logprior: 5.4451
Fitted a model with MAP estimate = -167.1185
Time for alignment: 42.6589
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.0649 - loglik: -3.1987e+02 - logprior: -5.6194e+01
Epoch 2/10
10/10 - 1s - loss: 300.3351 - loglik: -2.8708e+02 - logprior: -1.3256e+01
Epoch 3/10
10/10 - 1s - loss: 251.6005 - loglik: -2.4591e+02 - logprior: -5.6866e+00
Epoch 4/10
10/10 - 1s - loss: 218.7038 - loglik: -2.1517e+02 - logprior: -3.5319e+00
Epoch 5/10
10/10 - 1s - loss: 206.8062 - loglik: -2.0446e+02 - logprior: -2.3432e+00
Epoch 6/10
10/10 - 1s - loss: 201.4742 - loglik: -1.9973e+02 - logprior: -1.7416e+00
Epoch 7/10
10/10 - 1s - loss: 199.8676 - loglik: -1.9858e+02 - logprior: -1.2920e+00
Epoch 8/10
10/10 - 1s - loss: 198.6449 - loglik: -1.9776e+02 - logprior: -8.8124e-01
Epoch 9/10
10/10 - 1s - loss: 197.7060 - loglik: -1.9705e+02 - logprior: -6.5183e-01
Epoch 10/10
10/10 - 1s - loss: 197.3011 - loglik: -1.9675e+02 - logprior: -5.4724e-01
Fitted a model with MAP estimate = -196.7087
expansions: [(13, 3), (18, 6), (31, 1), (34, 1), (35, 1), (56, 1), (57, 4), (58, 1), (59, 1), (64, 1), (77, 3), (78, 1), (81, 1), (86, 1), (89, 1), (90, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 257.7660 - loglik: -1.9456e+02 - logprior: -6.3209e+01
Epoch 2/2
10/10 - 1s - loss: 203.6430 - loglik: -1.7915e+02 - logprior: -2.4498e+01
Fitted a model with MAP estimate = -194.0209
expansions: [(0, 2)]
discards: [ 0 23 70 97 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.4321 - loglik: -1.7666e+02 - logprior: -4.9771e+01
Epoch 2/2
10/10 - 1s - loss: 183.8861 - loglik: -1.7295e+02 - logprior: -1.0941e+01
Fitted a model with MAP estimate = -177.3648
expansions: []
discards: [ 0 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 237.1638 - loglik: -1.7672e+02 - logprior: -6.0439e+01
Epoch 2/10
10/10 - 1s - loss: 191.9639 - loglik: -1.7486e+02 - logprior: -1.7102e+01
Epoch 3/10
10/10 - 1s - loss: 178.0252 - loglik: -1.7382e+02 - logprior: -4.2035e+00
Epoch 4/10
10/10 - 1s - loss: 173.0276 - loglik: -1.7327e+02 - logprior: 0.2443
Epoch 5/10
10/10 - 1s - loss: 170.6482 - loglik: -1.7276e+02 - logprior: 2.1167
Epoch 6/10
10/10 - 1s - loss: 169.0358 - loglik: -1.7225e+02 - logprior: 3.2174
Epoch 7/10
10/10 - 1s - loss: 168.6491 - loglik: -1.7272e+02 - logprior: 4.0689
Epoch 8/10
10/10 - 1s - loss: 167.9098 - loglik: -1.7262e+02 - logprior: 4.7071
Epoch 9/10
10/10 - 1s - loss: 167.6438 - loglik: -1.7276e+02 - logprior: 5.1211
Epoch 10/10
10/10 - 1s - loss: 166.9726 - loglik: -1.7242e+02 - logprior: 5.4454
Fitted a model with MAP estimate = -166.9091
Time for alignment: 42.2001
Computed alignments with likelihoods: ['-164.5267', '-167.8588', '-166.3770', '-167.1185', '-166.9091']
Best model has likelihood: -164.5267  (prior= 5.6952 )
time for generating output: 0.1458
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9286324264974991
Training of 5 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fcea699a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5d798e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201b2d1340>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fceee7310>
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 398.8006 - loglik: -3.9071e+02 - logprior: -8.0888e+00
Epoch 2/10
13/13 - 2s - loss: 361.0767 - loglik: -3.5924e+02 - logprior: -1.8330e+00
Epoch 3/10
13/13 - 2s - loss: 330.9988 - loglik: -3.2950e+02 - logprior: -1.4961e+00
Epoch 4/10
13/13 - 2s - loss: 318.3087 - loglik: -3.1662e+02 - logprior: -1.6897e+00
Epoch 5/10
13/13 - 2s - loss: 312.1772 - loglik: -3.1047e+02 - logprior: -1.7049e+00
Epoch 6/10
13/13 - 2s - loss: 309.0615 - loglik: -3.0732e+02 - logprior: -1.7455e+00
Epoch 7/10
13/13 - 2s - loss: 307.9755 - loglik: -3.0617e+02 - logprior: -1.8033e+00
Epoch 8/10
13/13 - 2s - loss: 306.2796 - loglik: -3.0445e+02 - logprior: -1.8249e+00
Epoch 9/10
13/13 - 2s - loss: 305.9168 - loglik: -3.0409e+02 - logprior: -1.8265e+00
Epoch 10/10
13/13 - 2s - loss: 305.2144 - loglik: -3.0339e+02 - logprior: -1.8199e+00
Fitted a model with MAP estimate = -305.0873
expansions: [(16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (24, 2), (25, 1), (27, 1), (29, 1), (30, 1), (32, 1), (52, 1), (53, 1), (55, 1), (57, 1), (84, 1), (85, 1), (86, 1), (87, 4), (92, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 318.6460 - loglik: -3.0910e+02 - logprior: -9.5416e+00
Epoch 2/2
13/13 - 3s - loss: 301.9305 - loglik: -2.9784e+02 - logprior: -4.0919e+00
Fitted a model with MAP estimate = -298.3430
expansions: [(0, 2)]
discards: [  0  28 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 302.0318 - loglik: -2.9492e+02 - logprior: -7.1130e+00
Epoch 2/2
13/13 - 2s - loss: 293.8025 - loglik: -2.9204e+02 - logprior: -1.7661e+00
Fitted a model with MAP estimate = -292.3956
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 303.9421 - loglik: -2.9510e+02 - logprior: -8.8459e+00
Epoch 2/10
13/13 - 2s - loss: 294.7573 - loglik: -2.9240e+02 - logprior: -2.3563e+00
Epoch 3/10
13/13 - 2s - loss: 292.1362 - loglik: -2.9119e+02 - logprior: -9.4220e-01
Epoch 4/10
13/13 - 2s - loss: 290.7443 - loglik: -2.9018e+02 - logprior: -5.6080e-01
Epoch 5/10
13/13 - 2s - loss: 289.9548 - loglik: -2.8956e+02 - logprior: -3.9377e-01
Epoch 6/10
13/13 - 2s - loss: 290.2123 - loglik: -2.8986e+02 - logprior: -3.5143e-01
Fitted a model with MAP estimate = -289.2383
Time for alignment: 71.7455
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 399.0429 - loglik: -3.9097e+02 - logprior: -8.0779e+00
Epoch 2/10
13/13 - 2s - loss: 361.0132 - loglik: -3.5918e+02 - logprior: -1.8288e+00
Epoch 3/10
13/13 - 2s - loss: 330.8340 - loglik: -3.2935e+02 - logprior: -1.4805e+00
Epoch 4/10
13/13 - 2s - loss: 318.7983 - loglik: -3.1721e+02 - logprior: -1.5913e+00
Epoch 5/10
13/13 - 2s - loss: 312.4446 - loglik: -3.1094e+02 - logprior: -1.5093e+00
Epoch 6/10
13/13 - 2s - loss: 310.5811 - loglik: -3.0911e+02 - logprior: -1.4756e+00
Epoch 7/10
13/13 - 2s - loss: 308.6796 - loglik: -3.0719e+02 - logprior: -1.4883e+00
Epoch 8/10
13/13 - 2s - loss: 308.2066 - loglik: -3.0674e+02 - logprior: -1.4624e+00
Epoch 9/10
13/13 - 2s - loss: 307.6853 - loglik: -3.0626e+02 - logprior: -1.4296e+00
Epoch 10/10
13/13 - 2s - loss: 307.0280 - loglik: -3.0560e+02 - logprior: -1.4232e+00
Fitted a model with MAP estimate = -306.8542
expansions: [(16, 1), (19, 1), (20, 3), (21, 3), (27, 1), (28, 1), (30, 2), (32, 1), (52, 3), (59, 3), (64, 2), (89, 4), (91, 1), (92, 1), (94, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 320.4369 - loglik: -3.1087e+02 - logprior: -9.5642e+00
Epoch 2/2
13/13 - 3s - loss: 303.7896 - loglik: -2.9962e+02 - logprior: -4.1723e+00
Fitted a model with MAP estimate = -300.1333
expansions: [(0, 2), (129, 2)]
discards: [  0  26  39  75  76  82 111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 304.8586 - loglik: -2.9771e+02 - logprior: -7.1519e+00
Epoch 2/2
13/13 - 2s - loss: 295.0534 - loglik: -2.9327e+02 - logprior: -1.7818e+00
Fitted a model with MAP estimate = -293.5317
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 305.7788 - loglik: -2.9687e+02 - logprior: -8.9090e+00
Epoch 2/10
13/13 - 2s - loss: 295.7477 - loglik: -2.9331e+02 - logprior: -2.4352e+00
Epoch 3/10
13/13 - 2s - loss: 293.1988 - loglik: -2.9221e+02 - logprior: -9.8919e-01
Epoch 4/10
13/13 - 2s - loss: 292.2420 - loglik: -2.9165e+02 - logprior: -5.9093e-01
Epoch 5/10
13/13 - 2s - loss: 291.5410 - loglik: -2.9111e+02 - logprior: -4.2788e-01
Epoch 6/10
13/13 - 2s - loss: 290.0539 - loglik: -2.8968e+02 - logprior: -3.7004e-01
Epoch 7/10
13/13 - 2s - loss: 290.4297 - loglik: -2.9009e+02 - logprior: -3.3987e-01
Fitted a model with MAP estimate = -289.8970
Time for alignment: 73.4500
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 399.3206 - loglik: -3.9124e+02 - logprior: -8.0841e+00
Epoch 2/10
13/13 - 2s - loss: 360.1609 - loglik: -3.5833e+02 - logprior: -1.8356e+00
Epoch 3/10
13/13 - 2s - loss: 331.9785 - loglik: -3.3048e+02 - logprior: -1.4998e+00
Epoch 4/10
13/13 - 2s - loss: 318.5025 - loglik: -3.1686e+02 - logprior: -1.6448e+00
Epoch 5/10
13/13 - 2s - loss: 312.8170 - loglik: -3.1114e+02 - logprior: -1.6817e+00
Epoch 6/10
13/13 - 2s - loss: 309.3154 - loglik: -3.0759e+02 - logprior: -1.7267e+00
Epoch 7/10
13/13 - 2s - loss: 308.0278 - loglik: -3.0624e+02 - logprior: -1.7886e+00
Epoch 8/10
13/13 - 2s - loss: 306.1718 - loglik: -3.0437e+02 - logprior: -1.7978e+00
Epoch 9/10
13/13 - 2s - loss: 306.1063 - loglik: -3.0434e+02 - logprior: -1.7672e+00
Epoch 10/10
13/13 - 2s - loss: 305.4796 - loglik: -3.0374e+02 - logprior: -1.7385e+00
Fitted a model with MAP estimate = -305.1731
expansions: [(16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (24, 2), (27, 2), (28, 1), (30, 2), (32, 1), (52, 1), (53, 1), (55, 1), (59, 2), (64, 2), (82, 1), (83, 1), (84, 1), (91, 1), (92, 7), (94, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 320.1770 - loglik: -3.1064e+02 - logprior: -9.5403e+00
Epoch 2/2
13/13 - 3s - loss: 300.0356 - loglik: -2.9581e+02 - logprior: -4.2249e+00
Fitted a model with MAP estimate = -297.0780
expansions: [(0, 2)]
discards: [  0  28  35  81 118 119 120]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 301.2287 - loglik: -2.9410e+02 - logprior: -7.1310e+00
Epoch 2/2
13/13 - 2s - loss: 293.4469 - loglik: -2.9168e+02 - logprior: -1.7654e+00
Fitted a model with MAP estimate = -291.3461
expansions: []
discards: [  0  74 116]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 304.0810 - loglik: -2.9523e+02 - logprior: -8.8469e+00
Epoch 2/10
13/13 - 2s - loss: 295.2207 - loglik: -2.9283e+02 - logprior: -2.3909e+00
Epoch 3/10
13/13 - 2s - loss: 292.1758 - loglik: -2.9118e+02 - logprior: -9.9297e-01
Epoch 4/10
13/13 - 2s - loss: 290.8309 - loglik: -2.9025e+02 - logprior: -5.7930e-01
Epoch 5/10
13/13 - 2s - loss: 290.0660 - loglik: -2.8962e+02 - logprior: -4.4986e-01
Epoch 6/10
13/13 - 2s - loss: 289.4583 - loglik: -2.8908e+02 - logprior: -3.8060e-01
Epoch 7/10
13/13 - 2s - loss: 288.7519 - loglik: -2.8842e+02 - logprior: -3.3660e-01
Epoch 8/10
13/13 - 2s - loss: 288.7091 - loglik: -2.8839e+02 - logprior: -3.2312e-01
Epoch 9/10
13/13 - 2s - loss: 288.9432 - loglik: -2.8866e+02 - logprior: -2.8489e-01
Fitted a model with MAP estimate = -288.3970
Time for alignment: 79.9235
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 399.1176 - loglik: -3.9103e+02 - logprior: -8.0882e+00
Epoch 2/10
13/13 - 2s - loss: 361.3011 - loglik: -3.5946e+02 - logprior: -1.8361e+00
Epoch 3/10
13/13 - 2s - loss: 331.1338 - loglik: -3.2958e+02 - logprior: -1.5537e+00
Epoch 4/10
13/13 - 2s - loss: 318.8774 - loglik: -3.1714e+02 - logprior: -1.7407e+00
Epoch 5/10
13/13 - 2s - loss: 313.0537 - loglik: -3.1134e+02 - logprior: -1.7168e+00
Epoch 6/10
13/13 - 2s - loss: 309.9385 - loglik: -3.0826e+02 - logprior: -1.6784e+00
Epoch 7/10
13/13 - 2s - loss: 308.1985 - loglik: -3.0652e+02 - logprior: -1.6755e+00
Epoch 8/10
13/13 - 2s - loss: 307.3983 - loglik: -3.0574e+02 - logprior: -1.6579e+00
Epoch 9/10
13/13 - 2s - loss: 306.8966 - loglik: -3.0526e+02 - logprior: -1.6381e+00
Epoch 10/10
13/13 - 2s - loss: 306.7392 - loglik: -3.0512e+02 - logprior: -1.6159e+00
Fitted a model with MAP estimate = -306.3320
expansions: [(16, 1), (19, 1), (20, 3), (22, 1), (23, 1), (27, 1), (30, 2), (32, 2), (52, 1), (53, 1), (55, 1), (59, 2), (64, 2), (82, 1), (83, 2), (84, 1), (91, 1), (92, 7), (94, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 319.6307 - loglik: -3.1010e+02 - logprior: -9.5343e+00
Epoch 2/2
13/13 - 3s - loss: 300.9931 - loglik: -2.9680e+02 - logprior: -4.1938e+00
Fitted a model with MAP estimate = -297.8425
expansions: [(0, 2)]
discards: [  0  80 100 118 119 120]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 302.3503 - loglik: -2.9521e+02 - logprior: -7.1404e+00
Epoch 2/2
13/13 - 3s - loss: 294.4820 - loglik: -2.9270e+02 - logprior: -1.7812e+00
Fitted a model with MAP estimate = -292.6183
expansions: []
discards: [  0  75 117]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 305.5143 - loglik: -2.9664e+02 - logprior: -8.8736e+00
Epoch 2/10
13/13 - 3s - loss: 296.5238 - loglik: -2.9412e+02 - logprior: -2.4011e+00
Epoch 3/10
13/13 - 2s - loss: 292.8755 - loglik: -2.9187e+02 - logprior: -1.0074e+00
Epoch 4/10
13/13 - 2s - loss: 292.1799 - loglik: -2.9158e+02 - logprior: -5.9806e-01
Epoch 5/10
13/13 - 2s - loss: 291.0199 - loglik: -2.9058e+02 - logprior: -4.4293e-01
Epoch 6/10
13/13 - 2s - loss: 290.4367 - loglik: -2.9005e+02 - logprior: -3.8325e-01
Epoch 7/10
13/13 - 2s - loss: 290.6555 - loglik: -2.9031e+02 - logprior: -3.4663e-01
Fitted a model with MAP estimate = -290.0348
Time for alignment: 74.4054
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 399.5511 - loglik: -3.9147e+02 - logprior: -8.0802e+00
Epoch 2/10
13/13 - 2s - loss: 360.9128 - loglik: -3.5908e+02 - logprior: -1.8302e+00
Epoch 3/10
13/13 - 2s - loss: 332.4199 - loglik: -3.3093e+02 - logprior: -1.4932e+00
Epoch 4/10
13/13 - 2s - loss: 319.0793 - loglik: -3.1747e+02 - logprior: -1.6119e+00
Epoch 5/10
13/13 - 2s - loss: 314.8341 - loglik: -3.1331e+02 - logprior: -1.5260e+00
Epoch 6/10
13/13 - 2s - loss: 312.3218 - loglik: -3.1081e+02 - logprior: -1.5071e+00
Epoch 7/10
13/13 - 2s - loss: 310.6133 - loglik: -3.0907e+02 - logprior: -1.5397e+00
Epoch 8/10
13/13 - 2s - loss: 309.5043 - loglik: -3.0794e+02 - logprior: -1.5638e+00
Epoch 9/10
13/13 - 2s - loss: 308.5391 - loglik: -3.0695e+02 - logprior: -1.5925e+00
Epoch 10/10
13/13 - 2s - loss: 308.1520 - loglik: -3.0658e+02 - logprior: -1.5741e+00
Fitted a model with MAP estimate = -307.9100
expansions: [(18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (24, 2), (25, 1), (27, 2), (28, 2), (30, 2), (51, 1), (52, 1), (58, 2), (83, 3), (84, 2), (85, 1), (92, 1), (93, 1), (94, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 320.0186 - loglik: -3.1045e+02 - logprior: -9.5715e+00
Epoch 2/2
13/13 - 3s - loss: 304.6767 - loglik: -3.0052e+02 - logprior: -4.1548e+00
Fitted a model with MAP estimate = -301.4310
expansions: [(0, 2), (42, 1)]
discards: [ 0 28 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 304.8131 - loglik: -2.9763e+02 - logprior: -7.1809e+00
Epoch 2/2
13/13 - 2s - loss: 296.5853 - loglik: -2.9474e+02 - logprior: -1.8426e+00
Fitted a model with MAP estimate = -295.1098
expansions: []
discards: [ 0 36 37 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 307.5172 - loglik: -2.9866e+02 - logprior: -8.8609e+00
Epoch 2/10
13/13 - 2s - loss: 297.4482 - loglik: -2.9507e+02 - logprior: -2.3762e+00
Epoch 3/10
13/13 - 2s - loss: 295.0280 - loglik: -2.9408e+02 - logprior: -9.4558e-01
Epoch 4/10
13/13 - 2s - loss: 293.3747 - loglik: -2.9281e+02 - logprior: -5.6109e-01
Epoch 5/10
13/13 - 2s - loss: 293.1277 - loglik: -2.9272e+02 - logprior: -4.0848e-01
Epoch 6/10
13/13 - 2s - loss: 292.2011 - loglik: -2.9184e+02 - logprior: -3.6171e-01
Epoch 7/10
13/13 - 2s - loss: 292.2473 - loglik: -2.9192e+02 - logprior: -3.2845e-01
Fitted a model with MAP estimate = -291.7327
Time for alignment: 71.1249
Computed alignments with likelihoods: ['-289.2383', '-289.8970', '-288.3970', '-290.0348', '-291.7327']
Best model has likelihood: -288.3970  (prior= -0.2936 )
time for generating output: 0.1775
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.8282153983088563
Training of 5 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe85dde20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e4a6a0fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e490a3b80>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1ed3f020d0>
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 226.9671 - loglik: -2.1840e+02 - logprior: -8.5665e+00
Epoch 2/10
13/13 - 1s - loss: 181.0403 - loglik: -1.7888e+02 - logprior: -2.1630e+00
Epoch 3/10
13/13 - 1s - loss: 148.2172 - loglik: -1.4655e+02 - logprior: -1.6670e+00
Epoch 4/10
13/13 - 1s - loss: 138.8480 - loglik: -1.3735e+02 - logprior: -1.4952e+00
Epoch 5/10
13/13 - 1s - loss: 136.3969 - loglik: -1.3504e+02 - logprior: -1.3533e+00
Epoch 6/10
13/13 - 1s - loss: 135.0595 - loglik: -1.3373e+02 - logprior: -1.3278e+00
Epoch 7/10
13/13 - 1s - loss: 134.4288 - loglik: -1.3313e+02 - logprior: -1.3001e+00
Epoch 8/10
13/13 - 1s - loss: 133.9380 - loglik: -1.3266e+02 - logprior: -1.2818e+00
Epoch 9/10
13/13 - 1s - loss: 133.9036 - loglik: -1.3264e+02 - logprior: -1.2638e+00
Epoch 10/10
13/13 - 1s - loss: 133.4888 - loglik: -1.3225e+02 - logprior: -1.2403e+00
Fitted a model with MAP estimate = -133.4854
expansions: [(0, 5), (13, 1), (36, 4), (37, 2), (38, 1), (44, 6), (45, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 139.2693 - loglik: -1.2916e+02 - logprior: -1.0110e+01
Epoch 2/2
13/13 - 1s - loss: 122.5173 - loglik: -1.1937e+02 - logprior: -3.1467e+00
Fitted a model with MAP estimate = -119.5427
expansions: [(0, 2)]
discards: [48 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 129.8418 - loglik: -1.1979e+02 - logprior: -1.0051e+01
Epoch 2/2
13/13 - 1s - loss: 120.7963 - loglik: -1.1760e+02 - logprior: -3.2003e+00
Fitted a model with MAP estimate = -118.9379
expansions: [(0, 2), (61, 2)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 126.6771 - loglik: -1.1762e+02 - logprior: -9.0571e+00
Epoch 2/10
13/13 - 1s - loss: 117.6332 - loglik: -1.1523e+02 - logprior: -2.4025e+00
Epoch 3/10
13/13 - 2s - loss: 116.0904 - loglik: -1.1440e+02 - logprior: -1.6934e+00
Epoch 4/10
13/13 - 1s - loss: 116.0443 - loglik: -1.1459e+02 - logprior: -1.4499e+00
Epoch 5/10
13/13 - 1s - loss: 115.5394 - loglik: -1.1426e+02 - logprior: -1.2779e+00
Epoch 6/10
13/13 - 1s - loss: 114.9886 - loglik: -1.1372e+02 - logprior: -1.2637e+00
Epoch 7/10
13/13 - 1s - loss: 115.1743 - loglik: -1.1395e+02 - logprior: -1.2261e+00
Fitted a model with MAP estimate = -114.9144
Time for alignment: 53.2093
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 227.0384 - loglik: -2.1847e+02 - logprior: -8.5672e+00
Epoch 2/10
13/13 - 1s - loss: 181.6151 - loglik: -1.7944e+02 - logprior: -2.1764e+00
Epoch 3/10
13/13 - 1s - loss: 148.1810 - loglik: -1.4648e+02 - logprior: -1.7055e+00
Epoch 4/10
13/13 - 1s - loss: 138.6645 - loglik: -1.3710e+02 - logprior: -1.5605e+00
Epoch 5/10
13/13 - 1s - loss: 135.7089 - loglik: -1.3426e+02 - logprior: -1.4441e+00
Epoch 6/10
13/13 - 2s - loss: 134.4900 - loglik: -1.3309e+02 - logprior: -1.4046e+00
Epoch 7/10
13/13 - 1s - loss: 133.7966 - loglik: -1.3243e+02 - logprior: -1.3654e+00
Epoch 8/10
13/13 - 1s - loss: 133.0587 - loglik: -1.3171e+02 - logprior: -1.3492e+00
Epoch 9/10
13/13 - 1s - loss: 133.1348 - loglik: -1.3182e+02 - logprior: -1.3139e+00
Fitted a model with MAP estimate = -132.9098
expansions: [(0, 5), (13, 1), (34, 1), (36, 2), (37, 3), (38, 1), (44, 7), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 138.6415 - loglik: -1.2851e+02 - logprior: -1.0130e+01
Epoch 2/2
13/13 - 1s - loss: 121.1341 - loglik: -1.1796e+02 - logprior: -3.1718e+00
Fitted a model with MAP estimate = -117.9294
expansions: [(0, 2), (60, 2)]
discards: [47 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 127.8934 - loglik: -1.1785e+02 - logprior: -1.0043e+01
Epoch 2/2
13/13 - 2s - loss: 118.2820 - loglik: -1.1507e+02 - logprior: -3.2088e+00
Fitted a model with MAP estimate = -116.1944
expansions: [(0, 2), (62, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 124.0697 - loglik: -1.1499e+02 - logprior: -9.0783e+00
Epoch 2/10
13/13 - 1s - loss: 115.9936 - loglik: -1.1361e+02 - logprior: -2.3831e+00
Epoch 3/10
13/13 - 1s - loss: 114.7238 - loglik: -1.1307e+02 - logprior: -1.6567e+00
Epoch 4/10
13/13 - 1s - loss: 113.7458 - loglik: -1.1233e+02 - logprior: -1.4155e+00
Epoch 5/10
13/13 - 1s - loss: 113.8060 - loglik: -1.1255e+02 - logprior: -1.2599e+00
Fitted a model with MAP estimate = -113.3882
Time for alignment: 46.8098
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 226.8127 - loglik: -2.1825e+02 - logprior: -8.5663e+00
Epoch 2/10
13/13 - 1s - loss: 180.8179 - loglik: -1.7864e+02 - logprior: -2.1780e+00
Epoch 3/10
13/13 - 1s - loss: 148.0783 - loglik: -1.4640e+02 - logprior: -1.6811e+00
Epoch 4/10
13/13 - 1s - loss: 139.6069 - loglik: -1.3807e+02 - logprior: -1.5335e+00
Epoch 5/10
13/13 - 1s - loss: 136.9719 - loglik: -1.3556e+02 - logprior: -1.4160e+00
Epoch 6/10
13/13 - 1s - loss: 135.1183 - loglik: -1.3374e+02 - logprior: -1.3788e+00
Epoch 7/10
13/13 - 1s - loss: 134.4202 - loglik: -1.3309e+02 - logprior: -1.3304e+00
Epoch 8/10
13/13 - 1s - loss: 134.0052 - loglik: -1.3269e+02 - logprior: -1.3166e+00
Epoch 9/10
13/13 - 1s - loss: 133.4135 - loglik: -1.3211e+02 - logprior: -1.3041e+00
Epoch 10/10
13/13 - 2s - loss: 133.1927 - loglik: -1.3190e+02 - logprior: -1.2937e+00
Fitted a model with MAP estimate = -133.0211
expansions: [(0, 5), (13, 1), (35, 1), (36, 2), (37, 3), (38, 1), (44, 7), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 138.1526 - loglik: -1.2803e+02 - logprior: -1.0125e+01
Epoch 2/2
13/13 - 1s - loss: 121.3020 - loglik: -1.1812e+02 - logprior: -3.1824e+00
Fitted a model with MAP estimate = -117.7561
expansions: [(0, 2), (60, 2)]
discards: [47 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 127.7722 - loglik: -1.1773e+02 - logprior: -1.0042e+01
Epoch 2/2
13/13 - 1s - loss: 117.8474 - loglik: -1.1463e+02 - logprior: -3.2140e+00
Fitted a model with MAP estimate = -116.0147
expansions: [(0, 2)]
discards: [ 0  1 61 62 63]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 126.6107 - loglik: -1.1761e+02 - logprior: -9.0018e+00
Epoch 2/10
13/13 - 1s - loss: 118.5785 - loglik: -1.1628e+02 - logprior: -2.3026e+00
Epoch 3/10
13/13 - 1s - loss: 116.9556 - loglik: -1.1538e+02 - logprior: -1.5717e+00
Epoch 4/10
13/13 - 1s - loss: 116.6796 - loglik: -1.1534e+02 - logprior: -1.3398e+00
Epoch 5/10
13/13 - 1s - loss: 116.3325 - loglik: -1.1516e+02 - logprior: -1.1740e+00
Epoch 6/10
13/13 - 2s - loss: 116.1070 - loglik: -1.1496e+02 - logprior: -1.1495e+00
Epoch 7/10
13/13 - 2s - loss: 115.7063 - loglik: -1.1460e+02 - logprior: -1.1086e+00
Epoch 8/10
13/13 - 1s - loss: 115.7237 - loglik: -1.1465e+02 - logprior: -1.0706e+00
Fitted a model with MAP estimate = -115.4742
Time for alignment: 51.0197
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 227.1159 - loglik: -2.1855e+02 - logprior: -8.5658e+00
Epoch 2/10
13/13 - 1s - loss: 181.4445 - loglik: -1.7927e+02 - logprior: -2.1696e+00
Epoch 3/10
13/13 - 1s - loss: 147.7788 - loglik: -1.4611e+02 - logprior: -1.6713e+00
Epoch 4/10
13/13 - 1s - loss: 139.5267 - loglik: -1.3804e+02 - logprior: -1.4916e+00
Epoch 5/10
13/13 - 2s - loss: 136.3309 - loglik: -1.3495e+02 - logprior: -1.3783e+00
Epoch 6/10
13/13 - 1s - loss: 135.1831 - loglik: -1.3383e+02 - logprior: -1.3536e+00
Epoch 7/10
13/13 - 1s - loss: 134.4708 - loglik: -1.3317e+02 - logprior: -1.2992e+00
Epoch 8/10
13/13 - 1s - loss: 133.9439 - loglik: -1.3266e+02 - logprior: -1.2839e+00
Epoch 9/10
13/13 - 1s - loss: 133.5447 - loglik: -1.3230e+02 - logprior: -1.2467e+00
Epoch 10/10
13/13 - 1s - loss: 133.5789 - loglik: -1.3234e+02 - logprior: -1.2434e+00
Fitted a model with MAP estimate = -133.3708
expansions: [(0, 5), (13, 1), (36, 3), (37, 2), (38, 1), (44, 7), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 138.3794 - loglik: -1.2829e+02 - logprior: -1.0086e+01
Epoch 2/2
13/13 - 1s - loss: 121.4865 - loglik: -1.1832e+02 - logprior: -3.1714e+00
Fitted a model with MAP estimate = -118.2854
expansions: [(0, 2), (59, 2)]
discards: [60 61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 127.4785 - loglik: -1.1744e+02 - logprior: -1.0043e+01
Epoch 2/2
13/13 - 2s - loss: 118.6220 - loglik: -1.1543e+02 - logprior: -3.1914e+00
Fitted a model with MAP estimate = -116.2952
expansions: [(0, 2)]
discards: [ 0  1 63]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 125.4002 - loglik: -1.1634e+02 - logprior: -9.0563e+00
Epoch 2/10
13/13 - 1s - loss: 116.7993 - loglik: -1.1443e+02 - logprior: -2.3671e+00
Epoch 3/10
13/13 - 1s - loss: 115.8931 - loglik: -1.1424e+02 - logprior: -1.6519e+00
Epoch 4/10
13/13 - 2s - loss: 115.2883 - loglik: -1.1387e+02 - logprior: -1.4142e+00
Epoch 5/10
13/13 - 1s - loss: 114.9205 - loglik: -1.1366e+02 - logprior: -1.2573e+00
Epoch 6/10
13/13 - 2s - loss: 114.4331 - loglik: -1.1319e+02 - logprior: -1.2403e+00
Epoch 7/10
13/13 - 1s - loss: 114.6176 - loglik: -1.1341e+02 - logprior: -1.2060e+00
Fitted a model with MAP estimate = -114.2694
Time for alignment: 49.8679
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 226.5191 - loglik: -2.1795e+02 - logprior: -8.5675e+00
Epoch 2/10
13/13 - 1s - loss: 181.4180 - loglik: -1.7924e+02 - logprior: -2.1734e+00
Epoch 3/10
13/13 - 1s - loss: 147.9722 - loglik: -1.4630e+02 - logprior: -1.6758e+00
Epoch 4/10
13/13 - 1s - loss: 139.6109 - loglik: -1.3810e+02 - logprior: -1.5089e+00
Epoch 5/10
13/13 - 1s - loss: 137.0940 - loglik: -1.3570e+02 - logprior: -1.3892e+00
Epoch 6/10
13/13 - 1s - loss: 135.1812 - loglik: -1.3382e+02 - logprior: -1.3630e+00
Epoch 7/10
13/13 - 1s - loss: 134.5495 - loglik: -1.3322e+02 - logprior: -1.3250e+00
Epoch 8/10
13/13 - 1s - loss: 134.0763 - loglik: -1.3278e+02 - logprior: -1.2962e+00
Epoch 9/10
13/13 - 1s - loss: 133.6389 - loglik: -1.3235e+02 - logprior: -1.2888e+00
Epoch 10/10
13/13 - 1s - loss: 133.3270 - loglik: -1.3205e+02 - logprior: -1.2741e+00
Fitted a model with MAP estimate = -133.2485
expansions: [(0, 5), (13, 1), (35, 1), (36, 2), (37, 3), (38, 1), (44, 7), (55, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 139.2429 - loglik: -1.2909e+02 - logprior: -1.0156e+01
Epoch 2/2
13/13 - 1s - loss: 121.4005 - loglik: -1.1820e+02 - logprior: -3.1961e+00
Fitted a model with MAP estimate = -118.3152
expansions: [(0, 2)]
discards: [47 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 127.6292 - loglik: -1.1758e+02 - logprior: -1.0051e+01
Epoch 2/2
13/13 - 1s - loss: 119.4807 - loglik: -1.1630e+02 - logprior: -3.1805e+00
Fitted a model with MAP estimate = -117.4886
expansions: [(0, 2), (61, 3)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 124.9986 - loglik: -1.1595e+02 - logprior: -9.0449e+00
Epoch 2/10
13/13 - 2s - loss: 116.2951 - loglik: -1.1393e+02 - logprior: -2.3647e+00
Epoch 3/10
13/13 - 1s - loss: 114.4783 - loglik: -1.1282e+02 - logprior: -1.6559e+00
Epoch 4/10
13/13 - 1s - loss: 114.2330 - loglik: -1.1281e+02 - logprior: -1.4233e+00
Epoch 5/10
13/13 - 1s - loss: 113.3014 - loglik: -1.1204e+02 - logprior: -1.2660e+00
Epoch 6/10
13/13 - 1s - loss: 113.7084 - loglik: -1.1247e+02 - logprior: -1.2425e+00
Fitted a model with MAP estimate = -113.1663
Time for alignment: 47.9679
Computed alignments with likelihoods: ['-114.9144', '-113.3882', '-115.4742', '-114.2694', '-113.1663']
Best model has likelihood: -113.1663  (prior= -1.2173 )
time for generating output: 0.1852
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.4856981558148288
Training of 5 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f80ba5700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff98b1580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2001c66ac0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fac2be550>
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 61.9931 - loglik: -6.1234e+01 - logprior: -7.5894e-01
Epoch 2/10
41/41 - 1s - loss: 46.9591 - loglik: -4.6160e+01 - logprior: -7.9938e-01
Epoch 3/10
41/41 - 1s - loss: 45.3814 - loglik: -4.4590e+01 - logprior: -7.9165e-01
Epoch 4/10
41/41 - 1s - loss: 45.1395 - loglik: -4.4354e+01 - logprior: -7.8585e-01
Epoch 5/10
41/41 - 1s - loss: 44.7926 - loglik: -4.4010e+01 - logprior: -7.8245e-01
Epoch 6/10
41/41 - 1s - loss: 44.6660 - loglik: -4.3882e+01 - logprior: -7.8391e-01
Epoch 7/10
41/41 - 1s - loss: 44.6010 - loglik: -4.3818e+01 - logprior: -7.8350e-01
Epoch 8/10
41/41 - 1s - loss: 44.3249 - loglik: -4.3543e+01 - logprior: -7.8146e-01
Epoch 9/10
41/41 - 1s - loss: 44.3087 - loglik: -4.3526e+01 - logprior: -7.8261e-01
Epoch 10/10
41/41 - 1s - loss: 44.1469 - loglik: -4.3366e+01 - logprior: -7.8083e-01
Fitted a model with MAP estimate = -44.0761
expansions: [(1, 1), (2, 1), (9, 2), (11, 1), (13, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 45.6307 - loglik: -4.4629e+01 - logprior: -1.0018e+00
Epoch 2/2
41/41 - 1s - loss: 43.6444 - loglik: -4.2876e+01 - logprior: -7.6836e-01
Fitted a model with MAP estimate = -42.7895
expansions: []
discards: [12 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 7s - loss: 44.2275 - loglik: -4.3248e+01 - logprior: -9.7991e-01
Epoch 2/2
41/41 - 1s - loss: 43.5271 - loglik: -4.2782e+01 - logprior: -7.4470e-01
Fitted a model with MAP estimate = -42.7878
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.8494 - loglik: -4.2240e+01 - logprior: -6.0910e-01
Epoch 2/10
58/58 - 2s - loss: 42.4566 - loglik: -4.1950e+01 - logprior: -5.0629e-01
Epoch 3/10
58/58 - 2s - loss: 42.3514 - loglik: -4.1849e+01 - logprior: -5.0247e-01
Epoch 4/10
58/58 - 2s - loss: 41.8743 - loglik: -4.1373e+01 - logprior: -5.0091e-01
Epoch 5/10
58/58 - 2s - loss: 41.8361 - loglik: -4.1338e+01 - logprior: -4.9792e-01
Epoch 6/10
58/58 - 2s - loss: 41.8257 - loglik: -4.1328e+01 - logprior: -4.9740e-01
Epoch 7/10
58/58 - 2s - loss: 41.5057 - loglik: -4.1008e+01 - logprior: -4.9770e-01
Epoch 8/10
58/58 - 2s - loss: 41.5574 - loglik: -4.1062e+01 - logprior: -4.9530e-01
Fitted a model with MAP estimate = -41.4665
Time for alignment: 67.6838
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 61.9286 - loglik: -6.1173e+01 - logprior: -7.5551e-01
Epoch 2/10
41/41 - 1s - loss: 46.9724 - loglik: -4.6174e+01 - logprior: -7.9817e-01
Epoch 3/10
41/41 - 1s - loss: 45.3529 - loglik: -4.4561e+01 - logprior: -7.9143e-01
Epoch 4/10
41/41 - 1s - loss: 45.2006 - loglik: -4.4416e+01 - logprior: -7.8483e-01
Epoch 5/10
41/41 - 1s - loss: 44.8134 - loglik: -4.4029e+01 - logprior: -7.8446e-01
Epoch 6/10
41/41 - 1s - loss: 44.7784 - loglik: -4.3997e+01 - logprior: -7.8173e-01
Epoch 7/10
41/41 - 1s - loss: 44.4995 - loglik: -4.3718e+01 - logprior: -7.8153e-01
Epoch 8/10
41/41 - 1s - loss: 44.4364 - loglik: -4.3656e+01 - logprior: -7.8062e-01
Epoch 9/10
41/41 - 1s - loss: 44.3836 - loglik: -4.3602e+01 - logprior: -7.8142e-01
Epoch 10/10
41/41 - 1s - loss: 44.2397 - loglik: -4.3460e+01 - logprior: -7.7957e-01
Fitted a model with MAP estimate = -44.1014
expansions: [(1, 1), (2, 1), (9, 2), (11, 1), (13, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 45.6190 - loglik: -4.4617e+01 - logprior: -1.0019e+00
Epoch 2/2
41/41 - 1s - loss: 43.6772 - loglik: -4.2910e+01 - logprior: -7.6709e-01
Fitted a model with MAP estimate = -42.7644
expansions: []
discards: [12 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.2933 - loglik: -4.3314e+01 - logprior: -9.7897e-01
Epoch 2/2
41/41 - 1s - loss: 43.4989 - loglik: -4.2755e+01 - logprior: -7.4381e-01
Fitted a model with MAP estimate = -42.7577
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.8495 - loglik: -4.2239e+01 - logprior: -6.1028e-01
Epoch 2/10
58/58 - 2s - loss: 42.4145 - loglik: -4.1908e+01 - logprior: -5.0693e-01
Epoch 3/10
58/58 - 2s - loss: 42.3630 - loglik: -4.1860e+01 - logprior: -5.0271e-01
Epoch 4/10
58/58 - 2s - loss: 41.8526 - loglik: -4.1352e+01 - logprior: -5.0067e-01
Epoch 5/10
58/58 - 2s - loss: 41.8681 - loglik: -4.1369e+01 - logprior: -4.9874e-01
Fitted a model with MAP estimate = -41.6560
Time for alignment: 61.6864
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 62.7399 - loglik: -6.2028e+01 - logprior: -7.1175e-01
Epoch 2/10
41/41 - 1s - loss: 48.5031 - loglik: -4.7786e+01 - logprior: -7.1672e-01
Epoch 3/10
41/41 - 1s - loss: 46.6884 - loglik: -4.6007e+01 - logprior: -6.8179e-01
Epoch 4/10
41/41 - 1s - loss: 46.3625 - loglik: -4.5686e+01 - logprior: -6.7666e-01
Epoch 5/10
41/41 - 1s - loss: 45.9140 - loglik: -4.5239e+01 - logprior: -6.7509e-01
Epoch 6/10
41/41 - 1s - loss: 45.9951 - loglik: -4.5323e+01 - logprior: -6.7232e-01
Fitted a model with MAP estimate = -45.6418
expansions: [(2, 1), (3, 3), (9, 1), (11, 1), (13, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 46.5567 - loglik: -4.5527e+01 - logprior: -1.0299e+00
Epoch 2/2
41/41 - 1s - loss: 43.4369 - loglik: -4.2669e+01 - logprior: -7.6805e-01
Fitted a model with MAP estimate = -42.8093
expansions: []
discards: [4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.2992 - loglik: -4.3323e+01 - logprior: -9.7653e-01
Epoch 2/2
41/41 - 1s - loss: 43.5541 - loglik: -4.2809e+01 - logprior: -7.4482e-01
Fitted a model with MAP estimate = -42.8686
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.8417 - loglik: -4.2232e+01 - logprior: -6.1010e-01
Epoch 2/10
58/58 - 2s - loss: 42.4296 - loglik: -4.1923e+01 - logprior: -5.0621e-01
Epoch 3/10
58/58 - 2s - loss: 42.3811 - loglik: -4.1879e+01 - logprior: -5.0234e-01
Epoch 4/10
58/58 - 2s - loss: 41.8959 - loglik: -4.1397e+01 - logprior: -4.9912e-01
Epoch 5/10
58/58 - 2s - loss: 41.9816 - loglik: -4.1483e+01 - logprior: -4.9850e-01
Fitted a model with MAP estimate = -41.7042
Time for alignment: 57.2940
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 61.6736 - loglik: -6.0906e+01 - logprior: -7.6770e-01
Epoch 2/10
41/41 - 1s - loss: 46.8562 - loglik: -4.6063e+01 - logprior: -7.9364e-01
Epoch 3/10
41/41 - 1s - loss: 45.4408 - loglik: -4.4652e+01 - logprior: -7.8916e-01
Epoch 4/10
41/41 - 1s - loss: 45.1517 - loglik: -4.4364e+01 - logprior: -7.8745e-01
Epoch 5/10
41/41 - 1s - loss: 44.8421 - loglik: -4.4058e+01 - logprior: -7.8388e-01
Epoch 6/10
41/41 - 1s - loss: 44.7254 - loglik: -4.3944e+01 - logprior: -7.8139e-01
Epoch 7/10
41/41 - 1s - loss: 44.5124 - loglik: -4.3730e+01 - logprior: -7.8215e-01
Epoch 8/10
41/41 - 1s - loss: 44.3993 - loglik: -4.3618e+01 - logprior: -7.8150e-01
Epoch 9/10
41/41 - 1s - loss: 44.3314 - loglik: -4.3552e+01 - logprior: -7.7912e-01
Epoch 10/10
41/41 - 1s - loss: 44.3053 - loglik: -4.3523e+01 - logprior: -7.8228e-01
Fitted a model with MAP estimate = -44.0968
expansions: [(1, 1), (2, 1), (9, 2), (11, 1), (13, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 45.5921 - loglik: -4.4592e+01 - logprior: -9.9998e-01
Epoch 2/2
41/41 - 1s - loss: 43.6874 - loglik: -4.2920e+01 - logprior: -7.6755e-01
Fitted a model with MAP estimate = -42.7884
expansions: []
discards: [12 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.2709 - loglik: -4.3292e+01 - logprior: -9.7868e-01
Epoch 2/2
41/41 - 1s - loss: 43.5744 - loglik: -4.2830e+01 - logprior: -7.4429e-01
Fitted a model with MAP estimate = -42.7924
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.9002 - loglik: -4.2293e+01 - logprior: -6.0769e-01
Epoch 2/10
58/58 - 2s - loss: 42.4297 - loglik: -4.1924e+01 - logprior: -5.0554e-01
Epoch 3/10
58/58 - 2s - loss: 42.3442 - loglik: -4.1842e+01 - logprior: -5.0206e-01
Epoch 4/10
58/58 - 2s - loss: 41.9273 - loglik: -4.1428e+01 - logprior: -4.9934e-01
Epoch 5/10
58/58 - 2s - loss: 41.8303 - loglik: -4.1332e+01 - logprior: -4.9832e-01
Epoch 6/10
58/58 - 2s - loss: 41.8115 - loglik: -4.1314e+01 - logprior: -4.9772e-01
Epoch 7/10
58/58 - 2s - loss: 41.5859 - loglik: -4.1092e+01 - logprior: -4.9411e-01
Epoch 8/10
58/58 - 2s - loss: 41.5742 - loglik: -4.1077e+01 - logprior: -4.9744e-01
Epoch 9/10
58/58 - 2s - loss: 41.4504 - loglik: -4.0956e+01 - logprior: -4.9451e-01
Epoch 10/10
58/58 - 2s - loss: 41.4472 - loglik: -4.0953e+01 - logprior: -4.9417e-01
Fitted a model with MAP estimate = -41.4045
Time for alignment: 69.9809
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 61.8059 - loglik: -6.1041e+01 - logprior: -7.6522e-01
Epoch 2/10
41/41 - 1s - loss: 47.0689 - loglik: -4.6271e+01 - logprior: -7.9748e-01
Epoch 3/10
41/41 - 1s - loss: 45.3204 - loglik: -4.4529e+01 - logprior: -7.9114e-01
Epoch 4/10
41/41 - 1s - loss: 45.2365 - loglik: -4.4451e+01 - logprior: -7.8594e-01
Epoch 5/10
41/41 - 1s - loss: 44.8825 - loglik: -4.4099e+01 - logprior: -7.8312e-01
Epoch 6/10
41/41 - 1s - loss: 44.7060 - loglik: -4.3924e+01 - logprior: -7.8229e-01
Epoch 7/10
41/41 - 1s - loss: 44.5659 - loglik: -4.3784e+01 - logprior: -7.8222e-01
Epoch 8/10
41/41 - 1s - loss: 44.3588 - loglik: -4.3577e+01 - logprior: -7.8137e-01
Epoch 9/10
41/41 - 1s - loss: 44.3343 - loglik: -4.3554e+01 - logprior: -7.8059e-01
Epoch 10/10
41/41 - 1s - loss: 44.2080 - loglik: -4.3427e+01 - logprior: -7.8149e-01
Fitted a model with MAP estimate = -44.1315
expansions: [(1, 1), (2, 1), (9, 2), (11, 1), (13, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 45.6423 - loglik: -4.4640e+01 - logprior: -1.0023e+00
Epoch 2/2
41/41 - 1s - loss: 43.6296 - loglik: -4.2862e+01 - logprior: -7.6733e-01
Fitted a model with MAP estimate = -42.7675
expansions: []
discards: [12 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.3115 - loglik: -4.3330e+01 - logprior: -9.8125e-01
Epoch 2/2
41/41 - 1s - loss: 43.5672 - loglik: -4.2824e+01 - logprior: -7.4338e-01
Fitted a model with MAP estimate = -42.7321
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.8960 - loglik: -4.2288e+01 - logprior: -6.0773e-01
Epoch 2/10
58/58 - 2s - loss: 42.4191 - loglik: -4.1912e+01 - logprior: -5.0695e-01
Epoch 3/10
58/58 - 2s - loss: 42.3240 - loglik: -4.1822e+01 - logprior: -5.0240e-01
Epoch 4/10
58/58 - 2s - loss: 41.8252 - loglik: -4.1324e+01 - logprior: -5.0117e-01
Epoch 5/10
58/58 - 2s - loss: 41.8858 - loglik: -4.1388e+01 - logprior: -4.9825e-01
Fitted a model with MAP estimate = -41.6551
Time for alignment: 61.6149
Computed alignments with likelihoods: ['-41.4665', '-41.6560', '-41.7042', '-41.4045', '-41.6551']
Best model has likelihood: -41.4045  (prior= -0.4944 )
time for generating output: 0.0792
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.9714591509097396
Training of 5 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f805d9130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e75ceaf40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e5bb02f40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f06a2e670>
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 283.4718 - loglik: -1.5937e+02 - logprior: -1.2410e+02
Epoch 2/10
10/10 - 0s - loss: 172.3377 - loglik: -1.3879e+02 - logprior: -3.3548e+01
Epoch 3/10
10/10 - 1s - loss: 139.2737 - loglik: -1.2388e+02 - logprior: -1.5391e+01
Epoch 4/10
10/10 - 1s - loss: 120.6407 - loglik: -1.1192e+02 - logprior: -8.7178e+00
Epoch 5/10
10/10 - 1s - loss: 109.7781 - loglik: -1.0455e+02 - logprior: -5.2254e+00
Epoch 6/10
10/10 - 1s - loss: 105.3491 - loglik: -1.0211e+02 - logprior: -3.2420e+00
Epoch 7/10
10/10 - 1s - loss: 103.5842 - loglik: -1.0167e+02 - logprior: -1.9093e+00
Epoch 8/10
10/10 - 1s - loss: 102.5190 - loglik: -1.0151e+02 - logprior: -1.0098e+00
Epoch 9/10
10/10 - 1s - loss: 101.7526 - loglik: -1.0125e+02 - logprior: -5.0284e-01
Epoch 10/10
10/10 - 1s - loss: 101.1648 - loglik: -1.0109e+02 - logprior: -7.6760e-02
Fitted a model with MAP estimate = -100.9231
expansions: [(12, 1), (13, 1), (14, 6), (21, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 238.8133 - loglik: -1.0030e+02 - logprior: -1.3851e+02
Epoch 2/2
10/10 - 1s - loss: 151.3767 - loglik: -9.4506e+01 - logprior: -5.6870e+01
Fitted a model with MAP estimate = -136.5416
expansions: [(0, 2)]
discards: [ 0 17 28 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 203.2305 - loglik: -9.1720e+01 - logprior: -1.1151e+02
Epoch 2/2
10/10 - 1s - loss: 120.0947 - loglik: -8.9902e+01 - logprior: -3.0192e+01
Fitted a model with MAP estimate = -107.9020
expansions: [(38, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.6178 - loglik: -9.1978e+01 - logprior: -1.3564e+02
Epoch 2/10
10/10 - 1s - loss: 138.6953 - loglik: -9.1221e+01 - logprior: -4.7474e+01
Epoch 3/10
10/10 - 1s - loss: 109.2961 - loglik: -9.1064e+01 - logprior: -1.8232e+01
Epoch 4/10
10/10 - 1s - loss: 97.5493 - loglik: -9.0936e+01 - logprior: -6.6130e+00
Epoch 5/10
10/10 - 1s - loss: 92.8566 - loglik: -9.0804e+01 - logprior: -2.0531e+00
Epoch 6/10
10/10 - 1s - loss: 90.4419 - loglik: -9.0763e+01 - logprior: 0.3213
Epoch 7/10
10/10 - 1s - loss: 89.0459 - loglik: -9.0761e+01 - logprior: 1.7147
Epoch 8/10
10/10 - 1s - loss: 88.1345 - loglik: -9.0771e+01 - logprior: 2.6365
Epoch 9/10
10/10 - 1s - loss: 87.4588 - loglik: -9.0823e+01 - logprior: 3.3646
Epoch 10/10
10/10 - 1s - loss: 86.9174 - loglik: -9.0882e+01 - logprior: 3.9650
Fitted a model with MAP estimate = -86.6550
Time for alignment: 29.1310
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 283.4718 - loglik: -1.5937e+02 - logprior: -1.2410e+02
Epoch 2/10
10/10 - 1s - loss: 172.3376 - loglik: -1.3879e+02 - logprior: -3.3548e+01
Epoch 3/10
10/10 - 0s - loss: 139.2737 - loglik: -1.2388e+02 - logprior: -1.5391e+01
Epoch 4/10
10/10 - 1s - loss: 120.6409 - loglik: -1.1192e+02 - logprior: -8.7178e+00
Epoch 5/10
10/10 - 0s - loss: 109.7791 - loglik: -1.0455e+02 - logprior: -5.2252e+00
Epoch 6/10
10/10 - 1s - loss: 105.3497 - loglik: -1.0211e+02 - logprior: -3.2419e+00
Epoch 7/10
10/10 - 1s - loss: 103.5845 - loglik: -1.0168e+02 - logprior: -1.9094e+00
Epoch 8/10
10/10 - 1s - loss: 102.5193 - loglik: -1.0151e+02 - logprior: -1.0097e+00
Epoch 9/10
10/10 - 1s - loss: 101.7529 - loglik: -1.0125e+02 - logprior: -5.0280e-01
Epoch 10/10
10/10 - 0s - loss: 101.1651 - loglik: -1.0109e+02 - logprior: -7.6766e-02
Fitted a model with MAP estimate = -100.9234
expansions: [(12, 1), (13, 1), (14, 6), (21, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 238.8128 - loglik: -1.0030e+02 - logprior: -1.3851e+02
Epoch 2/2
10/10 - 1s - loss: 151.3767 - loglik: -9.4506e+01 - logprior: -5.6870e+01
Fitted a model with MAP estimate = -136.5415
expansions: [(0, 2)]
discards: [ 0 17 28 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.2304 - loglik: -9.1720e+01 - logprior: -1.1151e+02
Epoch 2/2
10/10 - 1s - loss: 120.0949 - loglik: -8.9902e+01 - logprior: -3.0193e+01
Fitted a model with MAP estimate = -107.9024
expansions: [(38, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 227.6176 - loglik: -9.1978e+01 - logprior: -1.3564e+02
Epoch 2/10
10/10 - 1s - loss: 138.6954 - loglik: -9.1221e+01 - logprior: -4.7474e+01
Epoch 3/10
10/10 - 1s - loss: 109.2962 - loglik: -9.1064e+01 - logprior: -1.8232e+01
Epoch 4/10
10/10 - 1s - loss: 97.5494 - loglik: -9.0936e+01 - logprior: -6.6130e+00
Epoch 5/10
10/10 - 1s - loss: 92.8567 - loglik: -9.0804e+01 - logprior: -2.0531e+00
Epoch 6/10
10/10 - 1s - loss: 90.4420 - loglik: -9.0763e+01 - logprior: 0.3214
Epoch 7/10
10/10 - 1s - loss: 89.0458 - loglik: -9.0761e+01 - logprior: 1.7147
Epoch 8/10
10/10 - 1s - loss: 88.1342 - loglik: -9.0771e+01 - logprior: 2.6366
Epoch 9/10
10/10 - 1s - loss: 87.4587 - loglik: -9.0823e+01 - logprior: 3.3648
Epoch 10/10
10/10 - 0s - loss: 86.9172 - loglik: -9.0882e+01 - logprior: 3.9652
Fitted a model with MAP estimate = -86.6548
Time for alignment: 27.7047
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 283.4719 - loglik: -1.5937e+02 - logprior: -1.2410e+02
Epoch 2/10
10/10 - 1s - loss: 172.3377 - loglik: -1.3879e+02 - logprior: -3.3548e+01
Epoch 3/10
10/10 - 0s - loss: 139.2739 - loglik: -1.2388e+02 - logprior: -1.5391e+01
Epoch 4/10
10/10 - 1s - loss: 120.6411 - loglik: -1.1192e+02 - logprior: -8.7178e+00
Epoch 5/10
10/10 - 1s - loss: 109.7785 - loglik: -1.0455e+02 - logprior: -5.2253e+00
Epoch 6/10
10/10 - 1s - loss: 105.3494 - loglik: -1.0211e+02 - logprior: -3.2419e+00
Epoch 7/10
10/10 - 1s - loss: 103.5843 - loglik: -1.0167e+02 - logprior: -1.9094e+00
Epoch 8/10
10/10 - 1s - loss: 102.5190 - loglik: -1.0151e+02 - logprior: -1.0098e+00
Epoch 9/10
10/10 - 1s - loss: 101.7525 - loglik: -1.0125e+02 - logprior: -5.0286e-01
Epoch 10/10
10/10 - 1s - loss: 101.1647 - loglik: -1.0109e+02 - logprior: -7.6756e-02
Fitted a model with MAP estimate = -100.9230
expansions: [(12, 1), (13, 1), (14, 6), (21, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 238.8125 - loglik: -1.0030e+02 - logprior: -1.3851e+02
Epoch 2/2
10/10 - 0s - loss: 151.3765 - loglik: -9.4506e+01 - logprior: -5.6870e+01
Fitted a model with MAP estimate = -136.5417
expansions: [(0, 2)]
discards: [ 0 17 28 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.2304 - loglik: -9.1720e+01 - logprior: -1.1151e+02
Epoch 2/2
10/10 - 1s - loss: 120.0946 - loglik: -8.9902e+01 - logprior: -3.0192e+01
Fitted a model with MAP estimate = -107.9021
expansions: [(38, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 227.6174 - loglik: -9.1977e+01 - logprior: -1.3564e+02
Epoch 2/10
10/10 - 1s - loss: 138.6956 - loglik: -9.1221e+01 - logprior: -4.7474e+01
Epoch 3/10
10/10 - 1s - loss: 109.2964 - loglik: -9.1064e+01 - logprior: -1.8232e+01
Epoch 4/10
10/10 - 0s - loss: 97.5494 - loglik: -9.0937e+01 - logprior: -6.6129e+00
Epoch 5/10
10/10 - 1s - loss: 92.8566 - loglik: -9.0804e+01 - logprior: -2.0530e+00
Epoch 6/10
10/10 - 1s - loss: 90.4419 - loglik: -9.0763e+01 - logprior: 0.3214
Epoch 7/10
10/10 - 1s - loss: 89.0457 - loglik: -9.0760e+01 - logprior: 1.7148
Epoch 8/10
10/10 - 0s - loss: 88.1343 - loglik: -9.0771e+01 - logprior: 2.6367
Epoch 9/10
10/10 - 1s - loss: 87.4587 - loglik: -9.0824e+01 - logprior: 3.3648
Epoch 10/10
10/10 - 1s - loss: 86.9173 - loglik: -9.0883e+01 - logprior: 3.9652
Fitted a model with MAP estimate = -86.6549
Time for alignment: 27.7833
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 283.4719 - loglik: -1.5937e+02 - logprior: -1.2410e+02
Epoch 2/10
10/10 - 0s - loss: 172.3377 - loglik: -1.3879e+02 - logprior: -3.3548e+01
Epoch 3/10
10/10 - 0s - loss: 139.2737 - loglik: -1.2388e+02 - logprior: -1.5391e+01
Epoch 4/10
10/10 - 1s - loss: 120.6407 - loglik: -1.1192e+02 - logprior: -8.7179e+00
Epoch 5/10
10/10 - 1s - loss: 109.7792 - loglik: -1.0455e+02 - logprior: -5.2251e+00
Epoch 6/10
10/10 - 1s - loss: 105.3498 - loglik: -1.0211e+02 - logprior: -3.2418e+00
Epoch 7/10
10/10 - 1s - loss: 103.5845 - loglik: -1.0168e+02 - logprior: -1.9095e+00
Epoch 8/10
10/10 - 1s - loss: 102.5192 - loglik: -1.0151e+02 - logprior: -1.0098e+00
Epoch 9/10
10/10 - 1s - loss: 101.7528 - loglik: -1.0125e+02 - logprior: -5.0280e-01
Epoch 10/10
10/10 - 0s - loss: 101.1650 - loglik: -1.0109e+02 - logprior: -7.6761e-02
Fitted a model with MAP estimate = -100.9233
expansions: [(12, 1), (13, 1), (14, 6), (21, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 238.8133 - loglik: -1.0030e+02 - logprior: -1.3851e+02
Epoch 2/2
10/10 - 1s - loss: 151.3767 - loglik: -9.4506e+01 - logprior: -5.6870e+01
Fitted a model with MAP estimate = -136.5416
expansions: [(0, 2)]
discards: [ 0 17 28 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.2306 - loglik: -9.1720e+01 - logprior: -1.1151e+02
Epoch 2/2
10/10 - 1s - loss: 120.0947 - loglik: -8.9902e+01 - logprior: -3.0192e+01
Fitted a model with MAP estimate = -107.9021
expansions: [(38, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 227.6178 - loglik: -9.1978e+01 - logprior: -1.3564e+02
Epoch 2/10
10/10 - 1s - loss: 138.6954 - loglik: -9.1221e+01 - logprior: -4.7474e+01
Epoch 3/10
10/10 - 1s - loss: 109.2963 - loglik: -9.1064e+01 - logprior: -1.8232e+01
Epoch 4/10
10/10 - 1s - loss: 97.5493 - loglik: -9.0936e+01 - logprior: -6.6129e+00
Epoch 5/10
10/10 - 1s - loss: 92.8566 - loglik: -9.0803e+01 - logprior: -2.0531e+00
Epoch 6/10
10/10 - 1s - loss: 90.4418 - loglik: -9.0763e+01 - logprior: 0.3214
Epoch 7/10
10/10 - 1s - loss: 89.0459 - loglik: -9.0761e+01 - logprior: 1.7147
Epoch 8/10
10/10 - 1s - loss: 88.1343 - loglik: -9.0771e+01 - logprior: 2.6366
Epoch 9/10
10/10 - 1s - loss: 87.4586 - loglik: -9.0823e+01 - logprior: 3.3647
Epoch 10/10
10/10 - 1s - loss: 86.9173 - loglik: -9.0882e+01 - logprior: 3.9652
Fitted a model with MAP estimate = -86.6543
Time for alignment: 27.9397
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 283.4719 - loglik: -1.5937e+02 - logprior: -1.2410e+02
Epoch 2/10
10/10 - 1s - loss: 172.3378 - loglik: -1.3879e+02 - logprior: -3.3548e+01
Epoch 3/10
10/10 - 1s - loss: 139.2740 - loglik: -1.2388e+02 - logprior: -1.5391e+01
Epoch 4/10
10/10 - 1s - loss: 120.6409 - loglik: -1.1192e+02 - logprior: -8.7178e+00
Epoch 5/10
10/10 - 1s - loss: 109.7790 - loglik: -1.0455e+02 - logprior: -5.2252e+00
Epoch 6/10
10/10 - 1s - loss: 105.3497 - loglik: -1.0211e+02 - logprior: -3.2419e+00
Epoch 7/10
10/10 - 1s - loss: 103.5846 - loglik: -1.0168e+02 - logprior: -1.9094e+00
Epoch 8/10
10/10 - 1s - loss: 102.5197 - loglik: -1.0151e+02 - logprior: -1.0096e+00
Epoch 9/10
10/10 - 1s - loss: 101.7534 - loglik: -1.0125e+02 - logprior: -5.0259e-01
Epoch 10/10
10/10 - 0s - loss: 101.1652 - loglik: -1.0109e+02 - logprior: -7.6732e-02
Fitted a model with MAP estimate = -100.9234
expansions: [(12, 1), (13, 1), (14, 6), (21, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 238.8133 - loglik: -1.0030e+02 - logprior: -1.3851e+02
Epoch 2/2
10/10 - 1s - loss: 151.3768 - loglik: -9.4506e+01 - logprior: -5.6870e+01
Fitted a model with MAP estimate = -136.5416
expansions: [(0, 2)]
discards: [ 0 17 28 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 203.2307 - loglik: -9.1720e+01 - logprior: -1.1151e+02
Epoch 2/2
10/10 - 1s - loss: 120.0948 - loglik: -8.9902e+01 - logprior: -3.0193e+01
Fitted a model with MAP estimate = -107.9023
expansions: [(38, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 227.6178 - loglik: -9.1978e+01 - logprior: -1.3564e+02
Epoch 2/10
10/10 - 1s - loss: 138.6954 - loglik: -9.1221e+01 - logprior: -4.7474e+01
Epoch 3/10
10/10 - 1s - loss: 109.2962 - loglik: -9.1064e+01 - logprior: -1.8232e+01
Epoch 4/10
10/10 - 1s - loss: 97.5493 - loglik: -9.0936e+01 - logprior: -6.6130e+00
Epoch 5/10
10/10 - 1s - loss: 92.8565 - loglik: -9.0803e+01 - logprior: -2.0531e+00
Epoch 6/10
10/10 - 1s - loss: 90.4420 - loglik: -9.0763e+01 - logprior: 0.3214
Epoch 7/10
10/10 - 1s - loss: 89.0459 - loglik: -9.0761e+01 - logprior: 1.7147
Epoch 8/10
10/10 - 1s - loss: 88.1344 - loglik: -9.0771e+01 - logprior: 2.6366
Epoch 9/10
10/10 - 1s - loss: 87.4587 - loglik: -9.0823e+01 - logprior: 3.3647
Epoch 10/10
10/10 - 1s - loss: 86.9174 - loglik: -9.0883e+01 - logprior: 3.9651
Fitted a model with MAP estimate = -86.6547
Time for alignment: 28.0959
Computed alignments with likelihoods: ['-86.6550', '-86.6548', '-86.6549', '-86.6543', '-86.6547']
Best model has likelihood: -86.6543  (prior= 4.2617 )
time for generating output: 0.1108
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.8972684085510689
Training of 5 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f922eeaf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f80d41820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e6d5d5a00>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f075188b0>
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 196.9331 - loglik: -1.9384e+02 - logprior: -3.0921e+00
Epoch 2/10
19/19 - 1s - loss: 157.8630 - loglik: -1.5677e+02 - logprior: -1.0898e+00
Epoch 3/10
19/19 - 1s - loss: 144.9081 - loglik: -1.4383e+02 - logprior: -1.0830e+00
Epoch 4/10
19/19 - 1s - loss: 142.1608 - loglik: -1.4111e+02 - logprior: -1.0527e+00
Epoch 5/10
19/19 - 1s - loss: 141.2926 - loglik: -1.4028e+02 - logprior: -1.0169e+00
Epoch 6/10
19/19 - 1s - loss: 140.7847 - loglik: -1.3979e+02 - logprior: -9.9801e-01
Epoch 7/10
19/19 - 1s - loss: 140.4419 - loglik: -1.3946e+02 - logprior: -9.7854e-01
Epoch 8/10
19/19 - 1s - loss: 140.1999 - loglik: -1.3923e+02 - logprior: -9.6905e-01
Epoch 9/10
19/19 - 1s - loss: 140.0789 - loglik: -1.3912e+02 - logprior: -9.6269e-01
Epoch 10/10
19/19 - 1s - loss: 140.0244 - loglik: -1.3907e+02 - logprior: -9.5693e-01
Fitted a model with MAP estimate = -140.4385
expansions: [(0, 5), (13, 2), (14, 1), (34, 1), (35, 1), (40, 2), (44, 1), (45, 1), (46, 2), (47, 1), (49, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.6492 - loglik: -1.3745e+02 - logprior: -4.2038e+00
Epoch 2/2
19/19 - 1s - loss: 130.4112 - loglik: -1.2893e+02 - logprior: -1.4818e+00
Fitted a model with MAP estimate = -130.1903
expansions: []
discards: [ 0 50 60 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.7574 - loglik: -1.3074e+02 - logprior: -4.0216e+00
Epoch 2/2
19/19 - 1s - loss: 130.1337 - loglik: -1.2850e+02 - logprior: -1.6305e+00
Fitted a model with MAP estimate = -130.2749
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 7s - loss: 131.2440 - loglik: -1.2880e+02 - logprior: -2.4426e+00
Epoch 2/10
21/21 - 1s - loss: 128.9087 - loglik: -1.2785e+02 - logprior: -1.0617e+00
Epoch 3/10
21/21 - 1s - loss: 128.4204 - loglik: -1.2736e+02 - logprior: -1.0563e+00
Epoch 4/10
21/21 - 1s - loss: 128.2328 - loglik: -1.2724e+02 - logprior: -9.9038e-01
Epoch 5/10
21/21 - 1s - loss: 127.7997 - loglik: -1.2683e+02 - logprior: -9.7028e-01
Epoch 6/10
21/21 - 1s - loss: 127.9080 - loglik: -1.2696e+02 - logprior: -9.4708e-01
Fitted a model with MAP estimate = -127.6557
Time for alignment: 47.6745
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 196.8386 - loglik: -1.9375e+02 - logprior: -3.0900e+00
Epoch 2/10
19/19 - 1s - loss: 157.9130 - loglik: -1.5682e+02 - logprior: -1.0947e+00
Epoch 3/10
19/19 - 1s - loss: 144.8562 - loglik: -1.4376e+02 - logprior: -1.0994e+00
Epoch 4/10
19/19 - 1s - loss: 142.5812 - loglik: -1.4153e+02 - logprior: -1.0491e+00
Epoch 5/10
19/19 - 1s - loss: 141.5525 - loglik: -1.4056e+02 - logprior: -9.9203e-01
Epoch 6/10
19/19 - 1s - loss: 141.3125 - loglik: -1.4035e+02 - logprior: -9.6231e-01
Epoch 7/10
19/19 - 1s - loss: 140.9341 - loglik: -1.3999e+02 - logprior: -9.4352e-01
Epoch 8/10
19/19 - 1s - loss: 140.6161 - loglik: -1.3968e+02 - logprior: -9.3438e-01
Epoch 9/10
19/19 - 1s - loss: 140.4431 - loglik: -1.3952e+02 - logprior: -9.2569e-01
Epoch 10/10
19/19 - 1s - loss: 140.1912 - loglik: -1.3927e+02 - logprior: -9.2048e-01
Fitted a model with MAP estimate = -140.6229
expansions: [(0, 4), (13, 2), (14, 1), (34, 1), (35, 1), (40, 2), (44, 2), (45, 1), (46, 1), (49, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.3057 - loglik: -1.3710e+02 - logprior: -4.2093e+00
Epoch 2/2
19/19 - 1s - loss: 130.7764 - loglik: -1.2933e+02 - logprior: -1.4461e+00
Fitted a model with MAP estimate = -130.8881
expansions: []
discards: [49 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 131.9569 - loglik: -1.2911e+02 - logprior: -2.8470e+00
Epoch 2/2
19/19 - 1s - loss: 129.0011 - loglik: -1.2776e+02 - logprior: -1.2437e+00
Fitted a model with MAP estimate = -129.7231
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 131.2666 - loglik: -1.2885e+02 - logprior: -2.4147e+00
Epoch 2/10
21/21 - 1s - loss: 128.8210 - loglik: -1.2774e+02 - logprior: -1.0838e+00
Epoch 3/10
21/21 - 1s - loss: 128.8284 - loglik: -1.2776e+02 - logprior: -1.0669e+00
Fitted a model with MAP estimate = -128.3604
Time for alignment: 45.2409
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 196.8756 - loglik: -1.9379e+02 - logprior: -3.0896e+00
Epoch 2/10
19/19 - 1s - loss: 157.7307 - loglik: -1.5663e+02 - logprior: -1.0973e+00
Epoch 3/10
19/19 - 1s - loss: 144.2639 - loglik: -1.4315e+02 - logprior: -1.1134e+00
Epoch 4/10
19/19 - 1s - loss: 141.7269 - loglik: -1.4066e+02 - logprior: -1.0705e+00
Epoch 5/10
19/19 - 1s - loss: 141.1319 - loglik: -1.4012e+02 - logprior: -1.0102e+00
Epoch 6/10
19/19 - 1s - loss: 140.4464 - loglik: -1.3947e+02 - logprior: -9.7435e-01
Epoch 7/10
19/19 - 1s - loss: 140.3916 - loglik: -1.3944e+02 - logprior: -9.4989e-01
Epoch 8/10
19/19 - 1s - loss: 140.1910 - loglik: -1.3925e+02 - logprior: -9.3648e-01
Epoch 9/10
19/19 - 1s - loss: 140.0238 - loglik: -1.3909e+02 - logprior: -9.2938e-01
Epoch 10/10
19/19 - 1s - loss: 139.8578 - loglik: -1.3893e+02 - logprior: -9.2486e-01
Fitted a model with MAP estimate = -140.1303
expansions: [(0, 4), (13, 2), (14, 1), (16, 1), (34, 1), (40, 2), (44, 2), (45, 1), (46, 1), (49, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 140.6883 - loglik: -1.3649e+02 - logprior: -4.2032e+00
Epoch 2/2
19/19 - 1s - loss: 130.6678 - loglik: -1.2922e+02 - logprior: -1.4471e+00
Fitted a model with MAP estimate = -130.7860
expansions: []
discards: [49 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 131.8848 - loglik: -1.2903e+02 - logprior: -2.8526e+00
Epoch 2/2
19/19 - 1s - loss: 128.9502 - loglik: -1.2770e+02 - logprior: -1.2501e+00
Fitted a model with MAP estimate = -129.6081
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 131.1226 - loglik: -1.2871e+02 - logprior: -2.4173e+00
Epoch 2/10
21/21 - 1s - loss: 129.0364 - loglik: -1.2794e+02 - logprior: -1.0955e+00
Epoch 3/10
21/21 - 1s - loss: 128.6646 - loglik: -1.2759e+02 - logprior: -1.0759e+00
Epoch 4/10
21/21 - 1s - loss: 128.2923 - loglik: -1.2728e+02 - logprior: -1.0144e+00
Epoch 5/10
21/21 - 1s - loss: 128.1176 - loglik: -1.2712e+02 - logprior: -9.9285e-01
Epoch 6/10
21/21 - 1s - loss: 128.0865 - loglik: -1.2712e+02 - logprior: -9.7040e-01
Epoch 7/10
21/21 - 1s - loss: 127.9307 - loglik: -1.2698e+02 - logprior: -9.5182e-01
Epoch 8/10
21/21 - 1s - loss: 127.8346 - loglik: -1.2690e+02 - logprior: -9.3699e-01
Epoch 9/10
21/21 - 1s - loss: 127.7895 - loglik: -1.2687e+02 - logprior: -9.1964e-01
Epoch 10/10
21/21 - 1s - loss: 127.5711 - loglik: -1.2666e+02 - logprior: -9.0621e-01
Fitted a model with MAP estimate = -127.5625
Time for alignment: 53.2201
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 196.9603 - loglik: -1.9387e+02 - logprior: -3.0916e+00
Epoch 2/10
19/19 - 1s - loss: 160.4385 - loglik: -1.5933e+02 - logprior: -1.1089e+00
Epoch 3/10
19/19 - 1s - loss: 144.9786 - loglik: -1.4384e+02 - logprior: -1.1363e+00
Epoch 4/10
19/19 - 1s - loss: 141.7166 - loglik: -1.4063e+02 - logprior: -1.0882e+00
Epoch 5/10
19/19 - 1s - loss: 140.5771 - loglik: -1.3956e+02 - logprior: -1.0196e+00
Epoch 6/10
19/19 - 1s - loss: 140.3206 - loglik: -1.3934e+02 - logprior: -9.7949e-01
Epoch 7/10
19/19 - 1s - loss: 139.9586 - loglik: -1.3900e+02 - logprior: -9.5565e-01
Epoch 8/10
19/19 - 1s - loss: 139.5157 - loglik: -1.3857e+02 - logprior: -9.4115e-01
Epoch 9/10
19/19 - 1s - loss: 139.6473 - loglik: -1.3871e+02 - logprior: -9.3246e-01
Fitted a model with MAP estimate = -139.7944
expansions: [(0, 4), (13, 2), (14, 1), (25, 1), (34, 1), (40, 2), (44, 2), (45, 1), (46, 1), (49, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 140.5621 - loglik: -1.3639e+02 - logprior: -4.1677e+00
Epoch 2/2
19/19 - 1s - loss: 130.7068 - loglik: -1.2927e+02 - logprior: -1.4356e+00
Fitted a model with MAP estimate = -130.7458
expansions: []
discards: [49 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 131.8147 - loglik: -1.2892e+02 - logprior: -2.8909e+00
Epoch 2/2
19/19 - 1s - loss: 128.9330 - loglik: -1.2771e+02 - logprior: -1.2255e+00
Fitted a model with MAP estimate = -129.5311
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 131.0875 - loglik: -1.2867e+02 - logprior: -2.4170e+00
Epoch 2/10
21/21 - 1s - loss: 129.2449 - loglik: -1.2816e+02 - logprior: -1.0833e+00
Epoch 3/10
21/21 - 1s - loss: 128.5264 - loglik: -1.2745e+02 - logprior: -1.0747e+00
Epoch 4/10
21/21 - 1s - loss: 128.1467 - loglik: -1.2714e+02 - logprior: -1.0098e+00
Epoch 5/10
21/21 - 1s - loss: 128.3734 - loglik: -1.2739e+02 - logprior: -9.8623e-01
Fitted a model with MAP estimate = -127.9838
Time for alignment: 44.9339
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 196.7662 - loglik: -1.9368e+02 - logprior: -3.0912e+00
Epoch 2/10
19/19 - 1s - loss: 157.4894 - loglik: -1.5639e+02 - logprior: -1.0964e+00
Epoch 3/10
19/19 - 1s - loss: 144.7868 - loglik: -1.4368e+02 - logprior: -1.1067e+00
Epoch 4/10
19/19 - 1s - loss: 142.0065 - loglik: -1.4093e+02 - logprior: -1.0725e+00
Epoch 5/10
19/19 - 1s - loss: 140.9179 - loglik: -1.3989e+02 - logprior: -1.0233e+00
Epoch 6/10
19/19 - 1s - loss: 140.4133 - loglik: -1.3942e+02 - logprior: -9.9688e-01
Epoch 7/10
19/19 - 1s - loss: 140.2395 - loglik: -1.3926e+02 - logprior: -9.7838e-01
Epoch 8/10
19/19 - 1s - loss: 139.8516 - loglik: -1.3889e+02 - logprior: -9.6639e-01
Epoch 9/10
19/19 - 1s - loss: 139.7014 - loglik: -1.3874e+02 - logprior: -9.6094e-01
Epoch 10/10
19/19 - 1s - loss: 139.5990 - loglik: -1.3864e+02 - logprior: -9.5720e-01
Fitted a model with MAP estimate = -139.9025
expansions: [(0, 4), (13, 2), (14, 1), (34, 1), (35, 1), (40, 2), (42, 2), (44, 1), (45, 1), (46, 1), (49, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 142.3477 - loglik: -1.3812e+02 - logprior: -4.2275e+00
Epoch 2/2
19/19 - 1s - loss: 131.0846 - loglik: -1.2961e+02 - logprior: -1.4760e+00
Fitted a model with MAP estimate = -130.9188
expansions: []
discards: [49 53 65]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 132.0938 - loglik: -1.2925e+02 - logprior: -2.8442e+00
Epoch 2/2
19/19 - 1s - loss: 128.9569 - loglik: -1.2772e+02 - logprior: -1.2361e+00
Fitted a model with MAP estimate = -129.6891
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 131.1739 - loglik: -1.2876e+02 - logprior: -2.4157e+00
Epoch 2/10
21/21 - 1s - loss: 128.9616 - loglik: -1.2787e+02 - logprior: -1.0875e+00
Epoch 3/10
21/21 - 1s - loss: 128.7578 - loglik: -1.2769e+02 - logprior: -1.0696e+00
Epoch 4/10
21/21 - 1s - loss: 128.2477 - loglik: -1.2724e+02 - logprior: -1.0037e+00
Epoch 5/10
21/21 - 1s - loss: 128.2492 - loglik: -1.2726e+02 - logprior: -9.8461e-01
Fitted a model with MAP estimate = -128.0110
Time for alignment: 46.0548
Computed alignments with likelihoods: ['-127.6557', '-128.3604', '-127.5625', '-127.9838', '-128.0110']
Best model has likelihood: -127.5625  (prior= -0.8740 )
time for generating output: 0.1266
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.9444791016843419
Training of 5 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f919133a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e4aa76490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e4a644a90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1fdf5e4310>
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 394.4347 - loglik: -3.8633e+02 - logprior: -8.1047e+00
Epoch 2/10
13/13 - 2s - loss: 346.1198 - loglik: -3.4427e+02 - logprior: -1.8474e+00
Epoch 3/10
13/13 - 2s - loss: 301.4960 - loglik: -2.9974e+02 - logprior: -1.7569e+00
Epoch 4/10
13/13 - 2s - loss: 286.2574 - loglik: -2.8424e+02 - logprior: -2.0162e+00
Epoch 5/10
13/13 - 2s - loss: 280.4615 - loglik: -2.7838e+02 - logprior: -2.0864e+00
Epoch 6/10
13/13 - 2s - loss: 278.8906 - loglik: -2.7691e+02 - logprior: -1.9819e+00
Epoch 7/10
13/13 - 2s - loss: 277.5729 - loglik: -2.7561e+02 - logprior: -1.9623e+00
Epoch 8/10
13/13 - 2s - loss: 277.5124 - loglik: -2.7554e+02 - logprior: -1.9737e+00
Epoch 9/10
13/13 - 2s - loss: 276.1579 - loglik: -2.7419e+02 - logprior: -1.9700e+00
Epoch 10/10
13/13 - 2s - loss: 276.5017 - loglik: -2.7453e+02 - logprior: -1.9694e+00
Fitted a model with MAP estimate = -276.2467
expansions: [(7, 3), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 2), (60, 2), (76, 1), (78, 1), (96, 1), (97, 3), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 285.1486 - loglik: -2.7563e+02 - logprior: -9.5202e+00
Epoch 2/2
13/13 - 2s - loss: 267.4366 - loglik: -2.6351e+02 - logprior: -3.9263e+00
Fitted a model with MAP estimate = -264.7317
expansions: [(0, 3)]
discards: [ 0  8 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 269.4619 - loglik: -2.6209e+02 - logprior: -7.3765e+00
Epoch 2/2
13/13 - 2s - loss: 260.9443 - loglik: -2.5916e+02 - logprior: -1.7836e+00
Fitted a model with MAP estimate = -259.3523
expansions: []
discards: [ 0  2 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 271.8562 - loglik: -2.6256e+02 - logprior: -9.2916e+00
Epoch 2/10
13/13 - 2s - loss: 263.1693 - loglik: -2.6022e+02 - logprior: -2.9510e+00
Epoch 3/10
13/13 - 2s - loss: 259.5463 - loglik: -2.5854e+02 - logprior: -1.0044e+00
Epoch 4/10
13/13 - 2s - loss: 258.3813 - loglik: -2.5802e+02 - logprior: -3.5692e-01
Epoch 5/10
13/13 - 2s - loss: 257.5465 - loglik: -2.5734e+02 - logprior: -2.0810e-01
Epoch 6/10
13/13 - 2s - loss: 256.4190 - loglik: -2.5623e+02 - logprior: -1.8440e-01
Epoch 7/10
13/13 - 2s - loss: 256.7220 - loglik: -2.5654e+02 - logprior: -1.8545e-01
Fitted a model with MAP estimate = -256.0277
Time for alignment: 72.2855
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 394.4089 - loglik: -3.8631e+02 - logprior: -8.1032e+00
Epoch 2/10
13/13 - 2s - loss: 346.0358 - loglik: -3.4420e+02 - logprior: -1.8368e+00
Epoch 3/10
13/13 - 2s - loss: 301.9837 - loglik: -3.0026e+02 - logprior: -1.7228e+00
Epoch 4/10
13/13 - 2s - loss: 286.2907 - loglik: -2.8433e+02 - logprior: -1.9603e+00
Epoch 5/10
13/13 - 2s - loss: 280.8196 - loglik: -2.7873e+02 - logprior: -2.0881e+00
Epoch 6/10
13/13 - 2s - loss: 279.1241 - loglik: -2.7714e+02 - logprior: -1.9827e+00
Epoch 7/10
13/13 - 2s - loss: 277.6602 - loglik: -2.7570e+02 - logprior: -1.9646e+00
Epoch 8/10
13/13 - 2s - loss: 277.1148 - loglik: -2.7514e+02 - logprior: -1.9766e+00
Epoch 9/10
13/13 - 2s - loss: 276.8456 - loglik: -2.7488e+02 - logprior: -1.9606e+00
Epoch 10/10
13/13 - 2s - loss: 276.4159 - loglik: -2.7447e+02 - logprior: -1.9470e+00
Fitted a model with MAP estimate = -276.3671
expansions: [(7, 3), (8, 1), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 2), (60, 2), (76, 1), (78, 1), (96, 1), (97, 3), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 284.8382 - loglik: -2.7535e+02 - logprior: -9.4910e+00
Epoch 2/2
13/13 - 2s - loss: 267.3930 - loglik: -2.6353e+02 - logprior: -3.8625e+00
Fitted a model with MAP estimate = -264.3129
expansions: [(0, 3)]
discards: [ 0 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 268.7221 - loglik: -2.6134e+02 - logprior: -7.3773e+00
Epoch 2/2
13/13 - 2s - loss: 260.1564 - loglik: -2.5838e+02 - logprior: -1.7733e+00
Fitted a model with MAP estimate = -258.6539
expansions: []
discards: [ 0  2 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 270.9468 - loglik: -2.6171e+02 - logprior: -9.2361e+00
Epoch 2/10
13/13 - 2s - loss: 262.7932 - loglik: -2.6002e+02 - logprior: -2.7777e+00
Epoch 3/10
13/13 - 2s - loss: 258.7386 - loglik: -2.5780e+02 - logprior: -9.3512e-01
Epoch 4/10
13/13 - 2s - loss: 258.3063 - loglik: -2.5797e+02 - logprior: -3.3640e-01
Epoch 5/10
13/13 - 2s - loss: 258.1033 - loglik: -2.5789e+02 - logprior: -2.1746e-01
Epoch 6/10
13/13 - 2s - loss: 256.0516 - loglik: -2.5586e+02 - logprior: -1.9007e-01
Epoch 7/10
13/13 - 2s - loss: 256.7807 - loglik: -2.5659e+02 - logprior: -1.8609e-01
Fitted a model with MAP estimate = -255.9777
Time for alignment: 72.0384
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 393.6159 - loglik: -3.8550e+02 - logprior: -8.1156e+00
Epoch 2/10
13/13 - 2s - loss: 346.8065 - loglik: -3.4496e+02 - logprior: -1.8452e+00
Epoch 3/10
13/13 - 2s - loss: 301.9272 - loglik: -3.0021e+02 - logprior: -1.7141e+00
Epoch 4/10
13/13 - 2s - loss: 285.0664 - loglik: -2.8311e+02 - logprior: -1.9546e+00
Epoch 5/10
13/13 - 2s - loss: 281.1082 - loglik: -2.7904e+02 - logprior: -2.0637e+00
Epoch 6/10
13/13 - 2s - loss: 277.8183 - loglik: -2.7589e+02 - logprior: -1.9269e+00
Epoch 7/10
13/13 - 2s - loss: 277.6960 - loglik: -2.7578e+02 - logprior: -1.9135e+00
Epoch 8/10
13/13 - 2s - loss: 276.0607 - loglik: -2.7413e+02 - logprior: -1.9311e+00
Epoch 9/10
13/13 - 2s - loss: 276.0460 - loglik: -2.7414e+02 - logprior: -1.9100e+00
Epoch 10/10
13/13 - 2s - loss: 276.1933 - loglik: -2.7429e+02 - logprior: -1.8992e+00
Fitted a model with MAP estimate = -275.8402
expansions: [(7, 3), (8, 1), (11, 1), (15, 1), (16, 2), (23, 1), (24, 1), (25, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (60, 2), (76, 1), (78, 1), (96, 1), (97, 3), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 285.3329 - loglik: -2.7582e+02 - logprior: -9.5088e+00
Epoch 2/2
13/13 - 2s - loss: 266.9160 - loglik: -2.6300e+02 - logprior: -3.9170e+00
Fitted a model with MAP estimate = -264.2530
expansions: [(0, 3)]
discards: [ 0 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 268.6703 - loglik: -2.6125e+02 - logprior: -7.4241e+00
Epoch 2/2
13/13 - 2s - loss: 259.7023 - loglik: -2.5790e+02 - logprior: -1.8064e+00
Fitted a model with MAP estimate = -258.5433
expansions: []
discards: [ 0  2 24 78]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 270.5427 - loglik: -2.6125e+02 - logprior: -9.2929e+00
Epoch 2/10
13/13 - 2s - loss: 263.4359 - loglik: -2.6053e+02 - logprior: -2.9028e+00
Epoch 3/10
13/13 - 2s - loss: 259.0491 - loglik: -2.5806e+02 - logprior: -9.8591e-01
Epoch 4/10
13/13 - 2s - loss: 258.4104 - loglik: -2.5805e+02 - logprior: -3.6286e-01
Epoch 5/10
13/13 - 2s - loss: 257.6078 - loglik: -2.5738e+02 - logprior: -2.2611e-01
Epoch 6/10
13/13 - 2s - loss: 256.8628 - loglik: -2.5668e+02 - logprior: -1.8098e-01
Epoch 7/10
13/13 - 2s - loss: 256.2552 - loglik: -2.5606e+02 - logprior: -1.9582e-01
Epoch 8/10
13/13 - 2s - loss: 255.5990 - loglik: -2.5542e+02 - logprior: -1.7575e-01
Epoch 9/10
13/13 - 2s - loss: 256.4059 - loglik: -2.5626e+02 - logprior: -1.4754e-01
Fitted a model with MAP estimate = -255.5771
Time for alignment: 76.3884
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 393.9442 - loglik: -3.8584e+02 - logprior: -8.1023e+00
Epoch 2/10
13/13 - 2s - loss: 346.4422 - loglik: -3.4461e+02 - logprior: -1.8359e+00
Epoch 3/10
13/13 - 2s - loss: 303.0631 - loglik: -3.0136e+02 - logprior: -1.7059e+00
Epoch 4/10
13/13 - 2s - loss: 289.6418 - loglik: -2.8772e+02 - logprior: -1.9169e+00
Epoch 5/10
13/13 - 2s - loss: 280.9341 - loglik: -2.7889e+02 - logprior: -2.0445e+00
Epoch 6/10
13/13 - 2s - loss: 279.3673 - loglik: -2.7736e+02 - logprior: -2.0089e+00
Epoch 7/10
13/13 - 2s - loss: 277.0117 - loglik: -2.7501e+02 - logprior: -1.9976e+00
Epoch 8/10
13/13 - 2s - loss: 277.7155 - loglik: -2.7570e+02 - logprior: -2.0155e+00
Fitted a model with MAP estimate = -276.8708
expansions: [(7, 3), (8, 2), (11, 1), (14, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (60, 2), (76, 1), (78, 1), (96, 1), (97, 3), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 285.8531 - loglik: -2.7636e+02 - logprior: -9.4967e+00
Epoch 2/2
13/13 - 2s - loss: 267.5148 - loglik: -2.6359e+02 - logprior: -3.9288e+00
Fitted a model with MAP estimate = -264.6213
expansions: [(0, 3)]
discards: [ 0  8 67 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 269.3612 - loglik: -2.6197e+02 - logprior: -7.3942e+00
Epoch 2/2
13/13 - 2s - loss: 260.9904 - loglik: -2.5923e+02 - logprior: -1.7587e+00
Fitted a model with MAP estimate = -259.1467
expansions: []
discards: [ 0  2 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 271.5822 - loglik: -2.6233e+02 - logprior: -9.2529e+00
Epoch 2/10
13/13 - 2s - loss: 262.4765 - loglik: -2.5970e+02 - logprior: -2.7786e+00
Epoch 3/10
13/13 - 2s - loss: 259.1589 - loglik: -2.5823e+02 - logprior: -9.2891e-01
Epoch 4/10
13/13 - 2s - loss: 258.4851 - loglik: -2.5815e+02 - logprior: -3.3682e-01
Epoch 5/10
13/13 - 2s - loss: 257.3937 - loglik: -2.5718e+02 - logprior: -2.1034e-01
Epoch 6/10
13/13 - 2s - loss: 256.7259 - loglik: -2.5655e+02 - logprior: -1.7084e-01
Epoch 7/10
13/13 - 2s - loss: 256.9119 - loglik: -2.5673e+02 - logprior: -1.7976e-01
Fitted a model with MAP estimate = -256.1836
Time for alignment: 67.5300
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 393.7348 - loglik: -3.8564e+02 - logprior: -8.0991e+00
Epoch 2/10
13/13 - 2s - loss: 345.8757 - loglik: -3.4404e+02 - logprior: -1.8382e+00
Epoch 3/10
13/13 - 2s - loss: 302.0009 - loglik: -3.0027e+02 - logprior: -1.7328e+00
Epoch 4/10
13/13 - 2s - loss: 286.4209 - loglik: -2.8444e+02 - logprior: -1.9803e+00
Epoch 5/10
13/13 - 2s - loss: 281.7332 - loglik: -2.7968e+02 - logprior: -2.0547e+00
Epoch 6/10
13/13 - 2s - loss: 278.7716 - loglik: -2.7679e+02 - logprior: -1.9824e+00
Epoch 7/10
13/13 - 2s - loss: 278.2089 - loglik: -2.7625e+02 - logprior: -1.9610e+00
Epoch 8/10
13/13 - 2s - loss: 276.2194 - loglik: -2.7423e+02 - logprior: -1.9870e+00
Epoch 9/10
13/13 - 2s - loss: 276.5708 - loglik: -2.7460e+02 - logprior: -1.9742e+00
Fitted a model with MAP estimate = -276.5713
expansions: [(7, 3), (8, 1), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 2), (60, 2), (76, 1), (78, 1), (96, 1), (97, 2), (98, 3), (99, 2), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 284.5783 - loglik: -2.7505e+02 - logprior: -9.5312e+00
Epoch 2/2
13/13 - 2s - loss: 268.0764 - loglik: -2.6416e+02 - logprior: -3.9188e+00
Fitted a model with MAP estimate = -264.6369
expansions: [(0, 3)]
discards: [ 0 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 268.6379 - loglik: -2.6123e+02 - logprior: -7.4122e+00
Epoch 2/2
13/13 - 2s - loss: 260.3171 - loglik: -2.5851e+02 - logprior: -1.8058e+00
Fitted a model with MAP estimate = -258.6825
expansions: []
discards: [ 0  2 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 271.3693 - loglik: -2.6209e+02 - logprior: -9.2783e+00
Epoch 2/10
13/13 - 2s - loss: 262.6331 - loglik: -2.5979e+02 - logprior: -2.8395e+00
Epoch 3/10
13/13 - 2s - loss: 259.1606 - loglik: -2.5823e+02 - logprior: -9.3462e-01
Epoch 4/10
13/13 - 2s - loss: 258.0779 - loglik: -2.5776e+02 - logprior: -3.1500e-01
Epoch 5/10
13/13 - 2s - loss: 257.1208 - loglik: -2.5690e+02 - logprior: -2.2317e-01
Epoch 6/10
13/13 - 2s - loss: 257.1010 - loglik: -2.5690e+02 - logprior: -1.9998e-01
Epoch 7/10
13/13 - 2s - loss: 256.5208 - loglik: -2.5632e+02 - logprior: -2.0010e-01
Epoch 8/10
13/13 - 2s - loss: 256.4409 - loglik: -2.5626e+02 - logprior: -1.7750e-01
Epoch 9/10
13/13 - 2s - loss: 255.9399 - loglik: -2.5578e+02 - logprior: -1.5842e-01
Epoch 10/10
13/13 - 2s - loss: 254.6139 - loglik: -2.5448e+02 - logprior: -1.3259e-01
Fitted a model with MAP estimate = -255.4592
Time for alignment: 76.2117
Computed alignments with likelihoods: ['-256.0277', '-255.9777', '-255.5771', '-256.1836', '-255.4592']
Best model has likelihood: -255.4592  (prior= -0.1190 )
time for generating output: 0.1670
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.933149678604224
Training of 5 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201a972b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ef5a022e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f92069400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f06a08ca0>
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1108.2931 - loglik: -1.1048e+03 - logprior: -3.4986e+00
Epoch 2/10
25/25 - 28s - loss: 837.6547 - loglik: -8.3644e+02 - logprior: -1.2160e+00
Epoch 3/10
25/25 - 29s - loss: 772.4154 - loglik: -7.7010e+02 - logprior: -2.3106e+00
Epoch 4/10
25/25 - 29s - loss: 765.7352 - loglik: -7.6349e+02 - logprior: -2.2406e+00
Epoch 5/10
25/25 - 29s - loss: 761.7527 - loglik: -7.5962e+02 - logprior: -2.1290e+00
Epoch 6/10
25/25 - 29s - loss: 760.9875 - loglik: -7.5874e+02 - logprior: -2.2527e+00
Epoch 7/10
25/25 - 29s - loss: 760.1847 - loglik: -7.5792e+02 - logprior: -2.2626e+00
Epoch 8/10
25/25 - 29s - loss: 757.5803 - loglik: -7.5521e+02 - logprior: -2.3747e+00
Epoch 9/10
25/25 - 29s - loss: 759.9944 - loglik: -7.5775e+02 - logprior: -2.2487e+00
Fitted a model with MAP estimate = -757.4043
expansions: [(0, 2), (42, 5), (91, 1), (97, 1), (100, 1), (102, 1), (103, 1), (117, 1), (118, 1), (124, 1), (126, 1), (127, 1), (128, 1), (129, 1), (132, 1), (135, 1), (138, 1), (152, 2), (158, 1), (162, 3), (163, 2), (164, 2), (179, 1), (180, 1), (181, 1), (182, 1), (183, 2), (184, 1), (191, 1), (192, 1), (201, 1), (205, 1), (208, 1), (212, 1), (214, 1), (215, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (232, 1), (234, 1), (255, 5), (256, 1), (259, 2), (281, 2), (282, 1), (283, 1), (297, 1), (299, 1), (300, 4), (302, 1), (316, 1), (317, 1), (318, 1), (326, 1), (328, 2), (337, 1), (354, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 741.2562 - loglik: -7.3604e+02 - logprior: -5.2120e+00
Epoch 2/2
25/25 - 41s - loss: 707.1774 - loglik: -7.0772e+02 - logprior: 0.5391
Fitted a model with MAP estimate = -704.4929
expansions: [(254, 1)]
discards: [  3  45  46  47 189 192 219 314 315]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 42s - loss: 713.1948 - loglik: -7.1075e+02 - logprior: -2.4423e+00
Epoch 2/2
25/25 - 39s - loss: 705.7487 - loglik: -7.0720e+02 - logprior: 1.4484
Fitted a model with MAP estimate = -703.2543
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 712.4896 - loglik: -7.1020e+02 - logprior: -2.2857e+00
Epoch 2/10
25/25 - 39s - loss: 703.7098 - loglik: -7.0536e+02 - logprior: 1.6480
Epoch 3/10
25/25 - 39s - loss: 701.8697 - loglik: -7.0399e+02 - logprior: 2.1207
Epoch 4/10
25/25 - 39s - loss: 702.4994 - loglik: -7.0487e+02 - logprior: 2.3708
Fitted a model with MAP estimate = -700.3853
Time for alignment: 734.4491
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1112.7528 - loglik: -1.1093e+03 - logprior: -3.4203e+00
Epoch 2/10
25/25 - 29s - loss: 842.3669 - loglik: -8.4118e+02 - logprior: -1.1901e+00
Epoch 3/10
25/25 - 28s - loss: 774.6928 - loglik: -7.7230e+02 - logprior: -2.3905e+00
Epoch 4/10
25/25 - 28s - loss: 763.9194 - loglik: -7.6167e+02 - logprior: -2.2531e+00
Epoch 5/10
25/25 - 29s - loss: 758.4568 - loglik: -7.5636e+02 - logprior: -2.1016e+00
Epoch 6/10
25/25 - 28s - loss: 754.8036 - loglik: -7.5268e+02 - logprior: -2.1194e+00
Epoch 7/10
25/25 - 29s - loss: 753.2893 - loglik: -7.5120e+02 - logprior: -2.0943e+00
Epoch 8/10
25/25 - 28s - loss: 755.3769 - loglik: -7.5328e+02 - logprior: -2.1018e+00
Fitted a model with MAP estimate = -753.3942
expansions: [(0, 2), (42, 5), (61, 1), (86, 2), (92, 1), (99, 1), (100, 1), (101, 1), (102, 1), (124, 1), (126, 2), (127, 1), (129, 1), (133, 1), (135, 1), (152, 1), (153, 1), (159, 1), (163, 4), (164, 2), (165, 2), (180, 1), (181, 1), (182, 4), (183, 1), (184, 1), (191, 1), (195, 1), (202, 1), (205, 1), (209, 1), (213, 1), (215, 1), (216, 1), (220, 1), (221, 1), (223, 1), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (231, 1), (234, 1), (235, 1), (255, 3), (256, 1), (259, 3), (266, 1), (281, 1), (283, 1), (298, 1), (300, 1), (301, 2), (302, 2), (304, 1), (305, 1), (314, 1), (316, 2), (317, 1), (330, 1), (336, 1), (338, 1), (351, 1), (356, 1), (358, 1), (364, 1), (366, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 461 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 738.3069 - loglik: -7.3315e+02 - logprior: -5.1577e+00
Epoch 2/2
25/25 - 41s - loss: 706.2316 - loglik: -7.0664e+02 - logprior: 0.4097
Fitted a model with MAP estimate = -703.7106
expansions: []
discards: [  1  45  46  47 190 191 193 219 375]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 713.7617 - loglik: -7.1119e+02 - logprior: -2.5749e+00
Epoch 2/2
25/25 - 39s - loss: 705.5211 - loglik: -7.0696e+02 - logprior: 1.4416
Fitted a model with MAP estimate = -703.2339
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 713.2057 - loglik: -7.1086e+02 - logprior: -2.3424e+00
Epoch 2/10
25/25 - 40s - loss: 703.6627 - loglik: -7.0525e+02 - logprior: 1.5885
Epoch 3/10
25/25 - 40s - loss: 702.1836 - loglik: -7.0432e+02 - logprior: 2.1333
Epoch 4/10
25/25 - 40s - loss: 703.3865 - loglik: -7.0576e+02 - logprior: 2.3759
Fitted a model with MAP estimate = -700.5421
Time for alignment: 706.9939
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1112.0593 - loglik: -1.1087e+03 - logprior: -3.3833e+00
Epoch 2/10
25/25 - 29s - loss: 847.6731 - loglik: -8.4637e+02 - logprior: -1.3036e+00
Epoch 3/10
25/25 - 28s - loss: 778.6823 - loglik: -7.7640e+02 - logprior: -2.2808e+00
Epoch 4/10
25/25 - 29s - loss: 769.5259 - loglik: -7.6732e+02 - logprior: -2.2105e+00
Epoch 5/10
25/25 - 28s - loss: 765.5406 - loglik: -7.6348e+02 - logprior: -2.0589e+00
Epoch 6/10
25/25 - 29s - loss: 764.7848 - loglik: -7.6279e+02 - logprior: -1.9939e+00
Epoch 7/10
25/25 - 29s - loss: 763.1104 - loglik: -7.6111e+02 - logprior: -2.0021e+00
Epoch 8/10
25/25 - 29s - loss: 762.9073 - loglik: -7.6089e+02 - logprior: -2.0207e+00
Epoch 9/10
25/25 - 28s - loss: 761.9598 - loglik: -7.5990e+02 - logprior: -2.0620e+00
Epoch 10/10
25/25 - 28s - loss: 760.2430 - loglik: -7.5806e+02 - logprior: -2.1801e+00
Fitted a model with MAP estimate = -759.3625
expansions: [(0, 2), (43, 1), (68, 1), (97, 1), (100, 1), (101, 2), (102, 1), (103, 1), (117, 1), (123, 1), (125, 1), (126, 1), (127, 1), (129, 1), (132, 1), (134, 1), (151, 1), (152, 1), (155, 1), (157, 1), (161, 4), (162, 2), (163, 2), (178, 1), (179, 1), (180, 4), (181, 1), (182, 1), (189, 1), (200, 1), (204, 1), (206, 1), (207, 1), (211, 1), (213, 1), (214, 1), (218, 1), (219, 1), (221, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (231, 1), (234, 1), (253, 1), (254, 2), (255, 1), (258, 2), (260, 1), (280, 2), (282, 1), (297, 1), (299, 1), (300, 2), (301, 2), (303, 1), (304, 1), (318, 3), (326, 1), (337, 1), (338, 1), (341, 1), (354, 3), (358, 1), (360, 1), (366, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 746.3597 - loglik: -7.4079e+02 - logprior: -5.5685e+00
Epoch 2/2
25/25 - 40s - loss: 707.0536 - loglik: -7.0744e+02 - logprior: 0.3843
Fitted a model with MAP estimate = -705.8408
expansions: [(435, 1)]
discards: [  1   2 185 186 188 214 313 370 436]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 715.2751 - loglik: -7.1242e+02 - logprior: -2.8537e+00
Epoch 2/2
25/25 - 39s - loss: 709.6802 - loglik: -7.1097e+02 - logprior: 1.2906
Fitted a model with MAP estimate = -706.3704
expansions: [(0, 2)]
discards: [311 428]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 42s - loss: 717.0112 - loglik: -7.1163e+02 - logprior: -5.3795e+00
Epoch 2/10
25/25 - 39s - loss: 712.4863 - loglik: -7.1371e+02 - logprior: 1.2271
Epoch 3/10
25/25 - 39s - loss: 704.3143 - loglik: -7.0643e+02 - logprior: 2.1164
Epoch 4/10
25/25 - 39s - loss: 704.3897 - loglik: -7.0679e+02 - logprior: 2.3983
Fitted a model with MAP estimate = -703.0980
Time for alignment: 761.9283
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1108.8295 - loglik: -1.1053e+03 - logprior: -3.4810e+00
Epoch 2/10
25/25 - 29s - loss: 844.8011 - loglik: -8.4355e+02 - logprior: -1.2521e+00
Epoch 3/10
25/25 - 29s - loss: 774.6541 - loglik: -7.7236e+02 - logprior: -2.2943e+00
Epoch 4/10
25/25 - 29s - loss: 765.4297 - loglik: -7.6342e+02 - logprior: -2.0105e+00
Epoch 5/10
25/25 - 28s - loss: 762.3240 - loglik: -7.6050e+02 - logprior: -1.8212e+00
Epoch 6/10
25/25 - 28s - loss: 762.0795 - loglik: -7.6031e+02 - logprior: -1.7720e+00
Epoch 7/10
25/25 - 29s - loss: 764.0147 - loglik: -7.6229e+02 - logprior: -1.7269e+00
Fitted a model with MAP estimate = -761.0551
expansions: [(0, 2), (42, 5), (61, 1), (86, 2), (92, 1), (101, 3), (102, 1), (117, 1), (123, 1), (126, 1), (127, 1), (129, 1), (132, 1), (135, 1), (139, 1), (152, 1), (159, 1), (163, 4), (164, 2), (165, 2), (181, 1), (183, 4), (184, 2), (185, 1), (190, 1), (191, 1), (192, 1), (202, 1), (205, 1), (209, 1), (210, 1), (214, 1), (215, 1), (220, 1), (222, 3), (223, 1), (224, 1), (226, 1), (227, 1), (228, 1), (231, 1), (234, 1), (235, 1), (255, 4), (256, 1), (259, 1), (260, 1), (282, 2), (283, 1), (284, 1), (298, 1), (300, 1), (301, 4), (314, 1), (317, 2), (318, 2), (330, 1), (337, 1), (338, 2), (339, 1), (353, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 463 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 46s - loss: 746.1803 - loglik: -7.4043e+02 - logprior: -5.7462e+00
Epoch 2/2
25/25 - 41s - loss: 713.5140 - loglik: -7.1286e+02 - logprior: -6.5217e-01
Fitted a model with MAP estimate = -708.4918
expansions: []
discards: [  1   2  45  46  47  94 190 191 219 320 321 424]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 717.6172 - loglik: -7.1466e+02 - logprior: -2.9594e+00
Epoch 2/2
25/25 - 40s - loss: 711.0715 - loglik: -7.1217e+02 - logprior: 1.1007
Fitted a model with MAP estimate = -707.3138
expansions: [(0, 2)]
discards: [184]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 717.8563 - loglik: -7.1325e+02 - logprior: -4.6068e+00
Epoch 2/10
25/25 - 40s - loss: 708.7531 - loglik: -7.1024e+02 - logprior: 1.4849
Epoch 3/10
25/25 - 40s - loss: 708.3153 - loglik: -7.1044e+02 - logprior: 2.1242
Epoch 4/10
25/25 - 40s - loss: 701.5352 - loglik: -7.0395e+02 - logprior: 2.4199
Epoch 5/10
25/25 - 40s - loss: 707.3228 - loglik: -7.0994e+02 - logprior: 2.6172
Fitted a model with MAP estimate = -702.9582
Time for alignment: 719.2997
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1111.2256 - loglik: -1.1077e+03 - logprior: -3.5008e+00
Epoch 2/10
25/25 - 28s - loss: 846.5042 - loglik: -8.4507e+02 - logprior: -1.4307e+00
Epoch 3/10
25/25 - 28s - loss: 777.4271 - loglik: -7.7500e+02 - logprior: -2.4264e+00
Epoch 4/10
25/25 - 29s - loss: 765.7713 - loglik: -7.6354e+02 - logprior: -2.2303e+00
Epoch 5/10
25/25 - 28s - loss: 767.1866 - loglik: -7.6512e+02 - logprior: -2.0622e+00
Fitted a model with MAP estimate = -763.4634
expansions: [(0, 2), (43, 1), (69, 1), (86, 2), (96, 1), (101, 3), (102, 1), (117, 1), (123, 1), (125, 1), (126, 1), (127, 1), (132, 1), (135, 1), (138, 1), (151, 1), (152, 1), (155, 1), (157, 1), (162, 1), (163, 2), (164, 2), (179, 1), (180, 1), (181, 2), (182, 1), (183, 1), (184, 1), (190, 2), (201, 1), (205, 1), (207, 1), (208, 1), (209, 1), (213, 1), (214, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (232, 1), (234, 1), (255, 4), (257, 1), (259, 3), (260, 2), (280, 2), (281, 1), (282, 1), (283, 1), (298, 1), (299, 2), (300, 2), (302, 1), (316, 2), (317, 2), (337, 1), (338, 2), (339, 1), (350, 1), (353, 1), (355, 1), (360, 1), (364, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 459 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 742.8505 - loglik: -7.3697e+02 - logprior: -5.8779e+00
Epoch 2/2
25/25 - 41s - loss: 714.0291 - loglik: -7.1366e+02 - logprior: -3.7341e-01
Fitted a model with MAP estimate = -708.3408
expansions: [(437, 1)]
discards: [  1  90 313 314 323 370 419 422 439]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 42s - loss: 718.4265 - loglik: -7.1509e+02 - logprior: -3.3351e+00
Epoch 2/2
25/25 - 39s - loss: 712.6086 - loglik: -7.1337e+02 - logprior: 0.7588
Fitted a model with MAP estimate = -708.4554
expansions: [(310, 2), (413, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 715.5232 - loglik: -7.1222e+02 - logprior: -3.2982e+00
Epoch 2/10
25/25 - 40s - loss: 707.6057 - loglik: -7.0866e+02 - logprior: 1.0539
Epoch 3/10
25/25 - 40s - loss: 703.6853 - loglik: -7.0549e+02 - logprior: 1.8003
Epoch 4/10
25/25 - 40s - loss: 706.2048 - loglik: -7.0834e+02 - logprior: 2.1352
Fitted a model with MAP estimate = -702.3709
Time for alignment: 623.1661
Computed alignments with likelihoods: ['-700.3853', '-700.5421', '-703.0980', '-702.9582', '-702.3709']
Best model has likelihood: -700.3853  (prior= 2.5345 )
time for generating output: 0.4420
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.8855421686746988
Training of 5 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fbd7de1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e48764e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e5c4b5100>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f80e79ee0>
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 334.1634 - loglik: -3.2972e+02 - logprior: -4.4429e+00
Epoch 2/10
16/16 - 3s - loss: 267.7480 - loglik: -2.6633e+02 - logprior: -1.4188e+00
Epoch 3/10
16/16 - 3s - loss: 223.6340 - loglik: -2.2177e+02 - logprior: -1.8590e+00
Epoch 4/10
16/16 - 4s - loss: 212.7574 - loglik: -2.1089e+02 - logprior: -1.8706e+00
Epoch 5/10
16/16 - 4s - loss: 210.2029 - loglik: -2.0831e+02 - logprior: -1.8941e+00
Epoch 6/10
16/16 - 3s - loss: 207.4611 - loglik: -2.0553e+02 - logprior: -1.9276e+00
Epoch 7/10
16/16 - 4s - loss: 205.4726 - loglik: -2.0355e+02 - logprior: -1.9239e+00
Epoch 8/10
16/16 - 3s - loss: 204.7784 - loglik: -2.0289e+02 - logprior: -1.8852e+00
Epoch 9/10
16/16 - 3s - loss: 204.6631 - loglik: -2.0279e+02 - logprior: -1.8745e+00
Epoch 10/10
16/16 - 3s - loss: 203.3039 - loglik: -2.0142e+02 - logprior: -1.8879e+00
Fitted a model with MAP estimate = -203.1939
expansions: [(11, 1), (14, 1), (15, 1), (17, 1), (18, 2), (19, 1), (39, 9), (66, 1), (70, 1), (71, 1), (72, 1), (76, 1), (82, 1), (97, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 133 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 205.9618 - loglik: -2.0175e+02 - logprior: -4.2121e+00
Epoch 2/2
33/33 - 4s - loss: 193.5927 - loglik: -1.9122e+02 - logprior: -2.3703e+00
Fitted a model with MAP estimate = -189.8598
expansions: [(0, 1), (19, 1), (33, 2), (47, 1), (48, 1), (49, 2)]
discards: [ 0 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 192.5581 - loglik: -1.8956e+02 - logprior: -3.0026e+00
Epoch 2/2
33/33 - 5s - loss: 187.3735 - loglik: -1.8597e+02 - logprior: -1.4016e+00
Fitted a model with MAP estimate = -185.8667
expansions: [(0, 1), (49, 1)]
discards: [33 34]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 8s - loss: 190.2389 - loglik: -1.8703e+02 - logprior: -3.2086e+00
Epoch 2/10
33/33 - 5s - loss: 186.4821 - loglik: -1.8506e+02 - logprior: -1.4233e+00
Epoch 3/10
33/33 - 5s - loss: 184.8979 - loglik: -1.8362e+02 - logprior: -1.2816e+00
Epoch 4/10
33/33 - 5s - loss: 184.1151 - loglik: -1.8291e+02 - logprior: -1.2072e+00
Epoch 5/10
33/33 - 4s - loss: 182.7156 - loglik: -1.8157e+02 - logprior: -1.1415e+00
Epoch 6/10
33/33 - 5s - loss: 182.2014 - loglik: -1.8112e+02 - logprior: -1.0771e+00
Epoch 7/10
33/33 - 5s - loss: 181.5893 - loglik: -1.8056e+02 - logprior: -1.0277e+00
Epoch 8/10
33/33 - 5s - loss: 180.4751 - loglik: -1.7951e+02 - logprior: -9.6389e-01
Epoch 9/10
33/33 - 5s - loss: 180.5087 - loglik: -1.7961e+02 - logprior: -8.9569e-01
Fitted a model with MAP estimate = -180.2629
Time for alignment: 131.6258
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 333.8653 - loglik: -3.2941e+02 - logprior: -4.4579e+00
Epoch 2/10
16/16 - 3s - loss: 272.5244 - loglik: -2.7104e+02 - logprior: -1.4823e+00
Epoch 3/10
16/16 - 4s - loss: 230.6786 - loglik: -2.2865e+02 - logprior: -2.0294e+00
Epoch 4/10
16/16 - 3s - loss: 215.6737 - loglik: -2.1340e+02 - logprior: -2.2720e+00
Epoch 5/10
16/16 - 3s - loss: 210.6713 - loglik: -2.0852e+02 - logprior: -2.1531e+00
Epoch 6/10
16/16 - 3s - loss: 211.3019 - loglik: -2.0913e+02 - logprior: -2.1748e+00
Fitted a model with MAP estimate = -208.9806
expansions: [(11, 1), (14, 1), (16, 1), (17, 2), (18, 1), (19, 1), (27, 1), (28, 2), (29, 1), (37, 4), (39, 4), (46, 1), (66, 1), (68, 1), (69, 1), (70, 1), (72, 1), (73, 1), (74, 1), (75, 1), (77, 1), (83, 1), (97, 1), (98, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 203.2890 - loglik: -1.9919e+02 - logprior: -4.1018e+00
Epoch 2/2
33/33 - 5s - loss: 189.5330 - loglik: -1.8735e+02 - logprior: -2.1805e+00
Fitted a model with MAP estimate = -185.5202
expansions: [(0, 1)]
discards: [ 0 24 34 47 48 49 52 53 54]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 193.1498 - loglik: -1.9028e+02 - logprior: -2.8692e+00
Epoch 2/2
33/33 - 5s - loss: 188.6224 - loglik: -1.8742e+02 - logprior: -1.1983e+00
Fitted a model with MAP estimate = -187.5483
expansions: [(0, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 135 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 191.2349 - loglik: -1.8821e+02 - logprior: -3.0297e+00
Epoch 2/10
33/33 - 5s - loss: 187.5105 - loglik: -1.8626e+02 - logprior: -1.2498e+00
Epoch 3/10
33/33 - 5s - loss: 185.1923 - loglik: -1.8407e+02 - logprior: -1.1194e+00
Epoch 4/10
33/33 - 5s - loss: 186.2085 - loglik: -1.8517e+02 - logprior: -1.0415e+00
Fitted a model with MAP estimate = -184.3838
Time for alignment: 95.9514
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 333.4150 - loglik: -3.2896e+02 - logprior: -4.4542e+00
Epoch 2/10
16/16 - 3s - loss: 274.4658 - loglik: -2.7301e+02 - logprior: -1.4578e+00
Epoch 3/10
16/16 - 3s - loss: 230.2955 - loglik: -2.2837e+02 - logprior: -1.9240e+00
Epoch 4/10
16/16 - 3s - loss: 214.6121 - loglik: -2.1248e+02 - logprior: -2.1356e+00
Epoch 5/10
16/16 - 4s - loss: 211.3983 - loglik: -2.0935e+02 - logprior: -2.0486e+00
Epoch 6/10
16/16 - 3s - loss: 209.4141 - loglik: -2.0733e+02 - logprior: -2.0854e+00
Epoch 7/10
16/16 - 3s - loss: 207.7787 - loglik: -2.0569e+02 - logprior: -2.0898e+00
Epoch 8/10
16/16 - 3s - loss: 208.7442 - loglik: -2.0668e+02 - logprior: -2.0608e+00
Fitted a model with MAP estimate = -207.7261
expansions: [(11, 1), (14, 1), (15, 1), (17, 1), (18, 2), (19, 1), (26, 1), (27, 1), (65, 1), (67, 1), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (75, 1), (83, 1), (84, 1), (97, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 7s - loss: 205.8744 - loglik: -2.0169e+02 - logprior: -4.1831e+00
Epoch 2/2
33/33 - 5s - loss: 196.0407 - loglik: -1.9387e+02 - logprior: -2.1704e+00
Fitted a model with MAP estimate = -192.2510
expansions: [(0, 1), (19, 1)]
discards: [ 0 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 197.1104 - loglik: -1.9419e+02 - logprior: -2.9181e+00
Epoch 2/2
33/33 - 4s - loss: 190.5777 - loglik: -1.8935e+02 - logprior: -1.2296e+00
Fitted a model with MAP estimate = -190.3490
expansions: [(0, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 193.5899 - loglik: -1.9054e+02 - logprior: -3.0542e+00
Epoch 2/10
33/33 - 5s - loss: 190.6045 - loglik: -1.8934e+02 - logprior: -1.2687e+00
Epoch 3/10
33/33 - 5s - loss: 189.5880 - loglik: -1.8845e+02 - logprior: -1.1343e+00
Epoch 4/10
33/33 - 5s - loss: 187.8886 - loglik: -1.8683e+02 - logprior: -1.0578e+00
Epoch 5/10
33/33 - 5s - loss: 187.2157 - loglik: -1.8621e+02 - logprior: -1.0021e+00
Epoch 6/10
33/33 - 5s - loss: 186.0733 - loglik: -1.8513e+02 - logprior: -9.4333e-01
Epoch 7/10
33/33 - 5s - loss: 185.3918 - loglik: -1.8450e+02 - logprior: -8.8783e-01
Epoch 8/10
33/33 - 5s - loss: 184.8736 - loglik: -1.8405e+02 - logprior: -8.2427e-01
Epoch 9/10
33/33 - 5s - loss: 185.3056 - loglik: -1.8454e+02 - logprior: -7.6753e-01
Fitted a model with MAP estimate = -184.4163
Time for alignment: 123.1500
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 333.8965 - loglik: -3.2944e+02 - logprior: -4.4610e+00
Epoch 2/10
16/16 - 3s - loss: 274.1926 - loglik: -2.7271e+02 - logprior: -1.4869e+00
Epoch 3/10
16/16 - 3s - loss: 230.2211 - loglik: -2.2816e+02 - logprior: -2.0626e+00
Epoch 4/10
16/16 - 3s - loss: 217.8458 - loglik: -2.1571e+02 - logprior: -2.1322e+00
Epoch 5/10
16/16 - 4s - loss: 212.3747 - loglik: -2.1031e+02 - logprior: -2.0614e+00
Epoch 6/10
16/16 - 3s - loss: 211.2710 - loglik: -2.0918e+02 - logprior: -2.0908e+00
Epoch 7/10
16/16 - 3s - loss: 210.3823 - loglik: -2.0830e+02 - logprior: -2.0848e+00
Epoch 8/10
16/16 - 4s - loss: 210.4291 - loglik: -2.0834e+02 - logprior: -2.0889e+00
Fitted a model with MAP estimate = -209.0047
expansions: [(11, 1), (14, 1), (15, 1), (17, 2), (18, 1), (19, 1), (28, 3), (65, 1), (67, 1), (68, 1), (69, 2), (70, 1), (73, 1), (74, 1), (76, 1), (82, 1), (97, 1)]
discards: [ 0  2 43 44 45 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 7s - loss: 208.4814 - loglik: -2.0428e+02 - logprior: -4.2011e+00
Epoch 2/2
33/33 - 5s - loss: 199.4232 - loglik: -1.9726e+02 - logprior: -2.1667e+00
Fitted a model with MAP estimate = -195.2445
expansions: [(0, 1)]
discards: [ 0 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 198.9247 - loglik: -1.9599e+02 - logprior: -2.9340e+00
Epoch 2/2
33/33 - 5s - loss: 194.1859 - loglik: -1.9292e+02 - logprior: -1.2692e+00
Fitted a model with MAP estimate = -193.5339
expansions: [(0, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 196.7105 - loglik: -1.9360e+02 - logprior: -3.1152e+00
Epoch 2/10
33/33 - 5s - loss: 194.5883 - loglik: -1.9328e+02 - logprior: -1.3128e+00
Epoch 3/10
33/33 - 5s - loss: 191.8317 - loglik: -1.9065e+02 - logprior: -1.1857e+00
Epoch 4/10
33/33 - 5s - loss: 190.8832 - loglik: -1.8978e+02 - logprior: -1.1048e+00
Epoch 5/10
33/33 - 4s - loss: 189.3827 - loglik: -1.8833e+02 - logprior: -1.0517e+00
Epoch 6/10
33/33 - 5s - loss: 190.0462 - loglik: -1.8905e+02 - logprior: -9.9723e-01
Fitted a model with MAP estimate = -188.7195
Time for alignment: 108.4138
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 333.3219 - loglik: -3.2888e+02 - logprior: -4.4463e+00
Epoch 2/10
16/16 - 3s - loss: 269.5246 - loglik: -2.6808e+02 - logprior: -1.4431e+00
Epoch 3/10
16/16 - 3s - loss: 226.2238 - loglik: -2.2428e+02 - logprior: -1.9424e+00
Epoch 4/10
16/16 - 3s - loss: 213.4543 - loglik: -2.1150e+02 - logprior: -1.9543e+00
Epoch 5/10
16/16 - 3s - loss: 208.7403 - loglik: -2.0684e+02 - logprior: -1.8953e+00
Epoch 6/10
16/16 - 3s - loss: 206.2804 - loglik: -2.0439e+02 - logprior: -1.8953e+00
Epoch 7/10
16/16 - 4s - loss: 207.2575 - loglik: -2.0537e+02 - logprior: -1.8830e+00
Fitted a model with MAP estimate = -205.3086
expansions: [(11, 1), (14, 1), (15, 1), (17, 1), (18, 2), (27, 3), (39, 9), (63, 1), (65, 1), (73, 1), (74, 1), (76, 1), (82, 1), (97, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 135 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 206.5221 - loglik: -2.0231e+02 - logprior: -4.2161e+00
Epoch 2/2
33/33 - 4s - loss: 193.5154 - loglik: -1.9121e+02 - logprior: -2.3011e+00
Fitted a model with MAP estimate = -190.7049
expansions: [(0, 1), (19, 1), (49, 3), (50, 1)]
discards: [ 0 24]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 194.6856 - loglik: -1.9162e+02 - logprior: -3.0691e+00
Epoch 2/2
33/33 - 5s - loss: 189.1832 - loglik: -1.8768e+02 - logprior: -1.5014e+00
Fitted a model with MAP estimate = -188.0086
expansions: [(0, 1), (49, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 191.4530 - loglik: -1.8818e+02 - logprior: -3.2681e+00
Epoch 2/10
33/33 - 5s - loss: 185.6072 - loglik: -1.8413e+02 - logprior: -1.4790e+00
Epoch 3/10
33/33 - 5s - loss: 184.1210 - loglik: -1.8278e+02 - logprior: -1.3372e+00
Epoch 4/10
33/33 - 5s - loss: 183.4919 - loglik: -1.8225e+02 - logprior: -1.2411e+00
Epoch 5/10
33/33 - 5s - loss: 182.2869 - loglik: -1.8111e+02 - logprior: -1.1816e+00
Epoch 6/10
33/33 - 5s - loss: 181.6437 - loglik: -1.8052e+02 - logprior: -1.1262e+00
Epoch 7/10
33/33 - 5s - loss: 181.1987 - loglik: -1.8012e+02 - logprior: -1.0746e+00
Epoch 8/10
33/33 - 5s - loss: 179.6889 - loglik: -1.7867e+02 - logprior: -1.0147e+00
Epoch 9/10
33/33 - 5s - loss: 181.0685 - loglik: -1.8012e+02 - logprior: -9.4404e-01
Fitted a model with MAP estimate = -179.6797
Time for alignment: 118.1915
Computed alignments with likelihoods: ['-180.2629', '-184.3838', '-184.4163', '-188.7195', '-179.6797']
Best model has likelihood: -179.6797  (prior= -0.8874 )
time for generating output: 0.2604
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.5644335551817451
Training of 5 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe8833f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f929c0700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e483568e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f20019f3790>
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 310.4547 - loglik: -3.0733e+02 - logprior: -3.1207e+00
Epoch 2/10
19/19 - 2s - loss: 279.3562 - loglik: -2.7820e+02 - logprior: -1.1512e+00
Epoch 3/10
19/19 - 2s - loss: 264.3874 - loglik: -2.6287e+02 - logprior: -1.5139e+00
Epoch 4/10
19/19 - 2s - loss: 260.8790 - loglik: -2.5945e+02 - logprior: -1.4259e+00
Epoch 5/10
19/19 - 2s - loss: 258.8559 - loglik: -2.5747e+02 - logprior: -1.3867e+00
Epoch 6/10
19/19 - 2s - loss: 258.7352 - loglik: -2.5737e+02 - logprior: -1.3686e+00
Epoch 7/10
19/19 - 2s - loss: 258.1658 - loglik: -2.5683e+02 - logprior: -1.3375e+00
Epoch 8/10
19/19 - 2s - loss: 257.9321 - loglik: -2.5661e+02 - logprior: -1.3175e+00
Epoch 9/10
19/19 - 2s - loss: 257.7431 - loglik: -2.5643e+02 - logprior: -1.3116e+00
Epoch 10/10
19/19 - 2s - loss: 257.3036 - loglik: -2.5600e+02 - logprior: -1.3005e+00
Fitted a model with MAP estimate = -243.6775
expansions: [(6, 1), (7, 1), (8, 2), (11, 1), (23, 4), (37, 2), (38, 5), (42, 9), (58, 2), (60, 1), (63, 2), (66, 2), (67, 1), (69, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 263.4362 - loglik: -2.5948e+02 - logprior: -3.9543e+00
Epoch 2/2
19/19 - 3s - loss: 252.7332 - loglik: -2.5064e+02 - logprior: -2.0979e+00
Fitted a model with MAP estimate = -237.3795
expansions: [(0, 2)]
discards: [ 0  9 28 29 30 82 91]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 253.6030 - loglik: -2.5066e+02 - logprior: -2.9474e+00
Epoch 2/2
19/19 - 2s - loss: 249.3504 - loglik: -2.4819e+02 - logprior: -1.1587e+00
Fitted a model with MAP estimate = -235.0772
expansions: []
discards: [ 0 45 46 47 48 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 238.6068 - loglik: -2.3604e+02 - logprior: -2.5643e+00
Epoch 2/10
23/23 - 3s - loss: 234.3464 - loglik: -2.3333e+02 - logprior: -1.0149e+00
Epoch 3/10
23/23 - 3s - loss: 233.1012 - loglik: -2.3214e+02 - logprior: -9.6277e-01
Epoch 4/10
23/23 - 3s - loss: 233.2559 - loglik: -2.3233e+02 - logprior: -9.2181e-01
Fitted a model with MAP estimate = -232.4494
Time for alignment: 76.2411
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 310.8806 - loglik: -3.0776e+02 - logprior: -3.1194e+00
Epoch 2/10
19/19 - 2s - loss: 279.9884 - loglik: -2.7884e+02 - logprior: -1.1441e+00
Epoch 3/10
19/19 - 2s - loss: 264.4501 - loglik: -2.6293e+02 - logprior: -1.5191e+00
Epoch 4/10
19/19 - 2s - loss: 261.0042 - loglik: -2.5956e+02 - logprior: -1.4424e+00
Epoch 5/10
19/19 - 2s - loss: 259.1356 - loglik: -2.5774e+02 - logprior: -1.3993e+00
Epoch 6/10
19/19 - 2s - loss: 258.1867 - loglik: -2.5680e+02 - logprior: -1.3841e+00
Epoch 7/10
19/19 - 2s - loss: 258.1216 - loglik: -2.5676e+02 - logprior: -1.3578e+00
Epoch 8/10
19/19 - 2s - loss: 257.1154 - loglik: -2.5578e+02 - logprior: -1.3376e+00
Epoch 9/10
19/19 - 2s - loss: 257.3313 - loglik: -2.5600e+02 - logprior: -1.3300e+00
Fitted a model with MAP estimate = -243.4908
expansions: [(6, 1), (7, 1), (8, 2), (11, 1), (22, 2), (33, 2), (38, 5), (42, 9), (58, 2), (60, 1), (63, 2), (66, 3), (67, 1), (69, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 263.6891 - loglik: -2.5971e+02 - logprior: -3.9813e+00
Epoch 2/2
19/19 - 2s - loss: 252.9551 - loglik: -2.5082e+02 - logprior: -2.1325e+00
Fitted a model with MAP estimate = -237.6369
expansions: [(0, 2)]
discards: [ 0  9 27 40 47 48 61 80 89 94]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 255.0120 - loglik: -2.5208e+02 - logprior: -2.9367e+00
Epoch 2/2
19/19 - 2s - loss: 250.8685 - loglik: -2.4970e+02 - logprior: -1.1731e+00
Fitted a model with MAP estimate = -236.1434
expansions: [(57, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 238.1742 - loglik: -2.3557e+02 - logprior: -2.6008e+00
Epoch 2/10
23/23 - 3s - loss: 233.9198 - loglik: -2.3287e+02 - logprior: -1.0463e+00
Epoch 3/10
23/23 - 3s - loss: 233.4272 - loglik: -2.3244e+02 - logprior: -9.8837e-01
Epoch 4/10
23/23 - 3s - loss: 232.3119 - loglik: -2.3136e+02 - logprior: -9.4911e-01
Epoch 5/10
23/23 - 3s - loss: 231.6509 - loglik: -2.3072e+02 - logprior: -9.2736e-01
Epoch 6/10
23/23 - 3s - loss: 232.0744 - loglik: -2.3116e+02 - logprior: -9.1496e-01
Fitted a model with MAP estimate = -231.5772
Time for alignment: 76.1109
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 310.5677 - loglik: -3.0745e+02 - logprior: -3.1222e+00
Epoch 2/10
19/19 - 2s - loss: 279.4150 - loglik: -2.7826e+02 - logprior: -1.1586e+00
Epoch 3/10
19/19 - 2s - loss: 263.9021 - loglik: -2.6238e+02 - logprior: -1.5199e+00
Epoch 4/10
19/19 - 2s - loss: 259.9207 - loglik: -2.5850e+02 - logprior: -1.4195e+00
Epoch 5/10
19/19 - 2s - loss: 259.0052 - loglik: -2.5763e+02 - logprior: -1.3712e+00
Epoch 6/10
19/19 - 2s - loss: 258.0071 - loglik: -2.5666e+02 - logprior: -1.3500e+00
Epoch 7/10
19/19 - 2s - loss: 257.1994 - loglik: -2.5588e+02 - logprior: -1.3238e+00
Epoch 8/10
19/19 - 2s - loss: 257.4569 - loglik: -2.5615e+02 - logprior: -1.3103e+00
Fitted a model with MAP estimate = -242.8638
expansions: [(6, 1), (7, 1), (8, 2), (11, 1), (22, 2), (39, 8), (58, 2), (60, 1), (63, 2), (66, 3), (67, 1), (69, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 262.8070 - loglik: -2.5886e+02 - logprior: -3.9460e+00
Epoch 2/2
19/19 - 2s - loss: 253.8881 - loglik: -2.5181e+02 - logprior: -2.0804e+00
Fitted a model with MAP estimate = -237.9108
expansions: [(0, 2)]
discards: [ 0  9 27 72 81 86]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 254.6654 - loglik: -2.5175e+02 - logprior: -2.9141e+00
Epoch 2/2
19/19 - 2s - loss: 251.1642 - loglik: -2.5003e+02 - logprior: -1.1327e+00
Fitted a model with MAP estimate = -236.3120
expansions: [(49, 1)]
discards: [ 0 52]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 239.3471 - loglik: -2.3679e+02 - logprior: -2.5523e+00
Epoch 2/10
23/23 - 3s - loss: 235.8167 - loglik: -2.3481e+02 - logprior: -1.0060e+00
Epoch 3/10
23/23 - 3s - loss: 234.5043 - loglik: -2.3355e+02 - logprior: -9.5002e-01
Epoch 4/10
23/23 - 3s - loss: 233.5805 - loglik: -2.3267e+02 - logprior: -9.0993e-01
Epoch 5/10
23/23 - 3s - loss: 232.9102 - loglik: -2.3201e+02 - logprior: -8.9746e-01
Epoch 6/10
23/23 - 3s - loss: 233.2614 - loglik: -2.3239e+02 - logprior: -8.7455e-01
Fitted a model with MAP estimate = -232.9381
Time for alignment: 71.9994
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 310.6985 - loglik: -3.0758e+02 - logprior: -3.1213e+00
Epoch 2/10
19/19 - 2s - loss: 280.2731 - loglik: -2.7912e+02 - logprior: -1.1517e+00
Epoch 3/10
19/19 - 2s - loss: 264.6098 - loglik: -2.6309e+02 - logprior: -1.5192e+00
Epoch 4/10
19/19 - 2s - loss: 260.3811 - loglik: -2.5894e+02 - logprior: -1.4370e+00
Epoch 5/10
19/19 - 2s - loss: 259.0606 - loglik: -2.5767e+02 - logprior: -1.3917e+00
Epoch 6/10
19/19 - 2s - loss: 258.1246 - loglik: -2.5675e+02 - logprior: -1.3706e+00
Epoch 7/10
19/19 - 2s - loss: 257.7737 - loglik: -2.5643e+02 - logprior: -1.3469e+00
Epoch 8/10
19/19 - 2s - loss: 257.4193 - loglik: -2.5609e+02 - logprior: -1.3284e+00
Epoch 9/10
19/19 - 2s - loss: 257.1928 - loglik: -2.5587e+02 - logprior: -1.3191e+00
Epoch 10/10
19/19 - 2s - loss: 257.5034 - loglik: -2.5619e+02 - logprior: -1.3086e+00
Fitted a model with MAP estimate = -243.3436
expansions: [(6, 1), (7, 1), (8, 2), (11, 1), (22, 2), (33, 2), (38, 8), (58, 2), (60, 1), (63, 2), (66, 2), (67, 1), (69, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 263.2343 - loglik: -2.5928e+02 - logprior: -3.9512e+00
Epoch 2/2
19/19 - 2s - loss: 253.5623 - loglik: -2.5147e+02 - logprior: -2.0942e+00
Fitted a model with MAP estimate = -237.7547
expansions: [(0, 2)]
discards: [ 0  9 27 40 74 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 254.4088 - loglik: -2.5150e+02 - logprior: -2.9076e+00
Epoch 2/2
19/19 - 2s - loss: 250.8989 - loglik: -2.4977e+02 - logprior: -1.1275e+00
Fitted a model with MAP estimate = -236.3355
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 238.6440 - loglik: -2.3612e+02 - logprior: -2.5281e+00
Epoch 2/10
23/23 - 3s - loss: 235.1435 - loglik: -2.3416e+02 - logprior: -9.8067e-01
Epoch 3/10
23/23 - 3s - loss: 233.6407 - loglik: -2.3270e+02 - logprior: -9.3680e-01
Epoch 4/10
23/23 - 3s - loss: 233.7415 - loglik: -2.3285e+02 - logprior: -8.9361e-01
Fitted a model with MAP estimate = -232.9225
Time for alignment: 70.2621
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 310.7395 - loglik: -3.0762e+02 - logprior: -3.1176e+00
Epoch 2/10
19/19 - 2s - loss: 278.4679 - loglik: -2.7732e+02 - logprior: -1.1494e+00
Epoch 3/10
19/19 - 2s - loss: 263.7020 - loglik: -2.6220e+02 - logprior: -1.5053e+00
Epoch 4/10
19/19 - 2s - loss: 259.9813 - loglik: -2.5856e+02 - logprior: -1.4201e+00
Epoch 5/10
19/19 - 2s - loss: 257.9815 - loglik: -2.5660e+02 - logprior: -1.3860e+00
Epoch 6/10
19/19 - 2s - loss: 258.4656 - loglik: -2.5710e+02 - logprior: -1.3624e+00
Fitted a model with MAP estimate = -243.1012
expansions: [(6, 1), (7, 1), (8, 2), (11, 1), (22, 2), (37, 2), (38, 5), (42, 8), (58, 2), (60, 1), (63, 2), (66, 3), (67, 1), (69, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 263.0019 - loglik: -2.5903e+02 - logprior: -3.9677e+00
Epoch 2/2
19/19 - 2s - loss: 253.0659 - loglik: -2.5100e+02 - logprior: -2.0688e+00
Fitted a model with MAP estimate = -237.5906
expansions: [(0, 2), (59, 1)]
discards: [ 0  9 27 46 47 79 88 93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 254.1573 - loglik: -2.5123e+02 - logprior: -2.9311e+00
Epoch 2/2
19/19 - 2s - loss: 250.5642 - loglik: -2.4940e+02 - logprior: -1.1605e+00
Fitted a model with MAP estimate = -235.9814
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 237.9016 - loglik: -2.3532e+02 - logprior: -2.5839e+00
Epoch 2/10
23/23 - 3s - loss: 234.1002 - loglik: -2.3307e+02 - logprior: -1.0301e+00
Epoch 3/10
23/23 - 3s - loss: 232.8800 - loglik: -2.3189e+02 - logprior: -9.8646e-01
Epoch 4/10
23/23 - 3s - loss: 232.0885 - loglik: -2.3114e+02 - logprior: -9.5284e-01
Epoch 5/10
23/23 - 3s - loss: 231.9975 - loglik: -2.3106e+02 - logprior: -9.3292e-01
Epoch 6/10
23/23 - 3s - loss: 231.7614 - loglik: -2.3085e+02 - logprior: -9.1224e-01
Epoch 7/10
23/23 - 3s - loss: 231.2930 - loglik: -2.3039e+02 - logprior: -8.9816e-01
Epoch 8/10
23/23 - 3s - loss: 231.4690 - loglik: -2.3059e+02 - logprior: -8.8343e-01
Fitted a model with MAP estimate = -231.1506
Time for alignment: 74.5234
Computed alignments with likelihoods: ['-232.4494', '-231.5772', '-232.9381', '-232.9225', '-231.1506']
Best model has likelihood: -231.1506  (prior= -0.8525 )
time for generating output: 0.1736
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.8177874186550976
Training of 5 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ea0d65940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1efe50a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ed3c368e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1e5bc19dc0>
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 433.2181 - loglik: -3.8776e+02 - logprior: -4.5459e+01
Epoch 2/10
10/10 - 2s - loss: 372.9917 - loglik: -3.6272e+02 - logprior: -1.0270e+01
Epoch 3/10
10/10 - 2s - loss: 338.3310 - loglik: -3.3414e+02 - logprior: -4.1945e+00
Epoch 4/10
10/10 - 2s - loss: 313.9912 - loglik: -3.1170e+02 - logprior: -2.2883e+00
Epoch 5/10
10/10 - 2s - loss: 303.8178 - loglik: -3.0250e+02 - logprior: -1.3131e+00
Epoch 6/10
10/10 - 2s - loss: 300.2679 - loglik: -2.9961e+02 - logprior: -6.5298e-01
Epoch 7/10
10/10 - 2s - loss: 298.3666 - loglik: -2.9811e+02 - logprior: -2.6063e-01
Epoch 8/10
10/10 - 2s - loss: 297.2625 - loglik: -2.9726e+02 - logprior: 0.0023
Epoch 9/10
10/10 - 2s - loss: 296.2311 - loglik: -2.9637e+02 - logprior: 0.1409
Epoch 10/10
10/10 - 2s - loss: 296.4528 - loglik: -2.9668e+02 - logprior: 0.2266
Fitted a model with MAP estimate = -295.6919
expansions: [(7, 2), (9, 1), (24, 1), (27, 1), (40, 2), (41, 3), (48, 2), (59, 1), (62, 2), (71, 1), (73, 1), (82, 1), (90, 2), (91, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 350.5236 - loglik: -2.9958e+02 - logprior: -5.0948e+01
Epoch 2/2
10/10 - 2s - loss: 307.9946 - loglik: -2.8864e+02 - logprior: -1.9358e+01
Fitted a model with MAP estimate = -300.9532
expansions: [(9, 3)]
discards: [  0  47  57  74 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 336.5641 - loglik: -2.8684e+02 - logprior: -4.9726e+01
Epoch 2/2
10/10 - 2s - loss: 300.2974 - loglik: -2.8326e+02 - logprior: -1.7034e+01
Fitted a model with MAP estimate = -293.2073
expansions: [(0, 4)]
discards: [ 0 48 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 322.3000 - loglik: -2.8335e+02 - logprior: -3.8953e+01
Epoch 2/10
10/10 - 2s - loss: 288.0909 - loglik: -2.8049e+02 - logprior: -7.5982e+00
Epoch 3/10
10/10 - 2s - loss: 281.4788 - loglik: -2.8032e+02 - logprior: -1.1541e+00
Epoch 4/10
10/10 - 2s - loss: 278.2647 - loglik: -2.7974e+02 - logprior: 1.4739
Epoch 5/10
10/10 - 2s - loss: 277.0723 - loglik: -2.7998e+02 - logprior: 2.9030
Epoch 6/10
10/10 - 2s - loss: 275.5019 - loglik: -2.7921e+02 - logprior: 3.7105
Epoch 7/10
10/10 - 2s - loss: 274.8581 - loglik: -2.7902e+02 - logprior: 4.1645
Epoch 8/10
10/10 - 2s - loss: 274.6231 - loglik: -2.7911e+02 - logprior: 4.4885
Epoch 9/10
10/10 - 2s - loss: 274.5824 - loglik: -2.7934e+02 - logprior: 4.7538
Epoch 10/10
10/10 - 2s - loss: 273.5072 - loglik: -2.7850e+02 - logprior: 4.9960
Fitted a model with MAP estimate = -273.7095
Time for alignment: 65.0169
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.7459 - loglik: -3.8729e+02 - logprior: -4.5459e+01
Epoch 2/10
10/10 - 2s - loss: 373.3564 - loglik: -3.6308e+02 - logprior: -1.0272e+01
Epoch 3/10
10/10 - 2s - loss: 336.5718 - loglik: -3.3237e+02 - logprior: -4.2008e+00
Epoch 4/10
10/10 - 2s - loss: 312.7326 - loglik: -3.1038e+02 - logprior: -2.3496e+00
Epoch 5/10
10/10 - 2s - loss: 303.0135 - loglik: -3.0165e+02 - logprior: -1.3585e+00
Epoch 6/10
10/10 - 2s - loss: 298.9553 - loglik: -2.9826e+02 - logprior: -6.9250e-01
Epoch 7/10
10/10 - 2s - loss: 297.7067 - loglik: -2.9743e+02 - logprior: -2.7838e-01
Epoch 8/10
10/10 - 2s - loss: 296.4736 - loglik: -2.9639e+02 - logprior: -8.0277e-02
Epoch 9/10
10/10 - 2s - loss: 295.3037 - loglik: -2.9538e+02 - logprior: 0.0775
Epoch 10/10
10/10 - 2s - loss: 295.5447 - loglik: -2.9575e+02 - logprior: 0.2040
Fitted a model with MAP estimate = -294.9873
expansions: [(7, 2), (9, 1), (17, 1), (27, 1), (40, 1), (42, 1), (48, 2), (59, 1), (62, 2), (65, 1), (73, 1), (85, 1), (90, 2), (91, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 349.1879 - loglik: -2.9821e+02 - logprior: -5.0982e+01
Epoch 2/2
10/10 - 2s - loss: 307.7376 - loglik: -2.8840e+02 - logprior: -1.9338e+01
Fitted a model with MAP estimate = -300.9331
expansions: [(9, 3)]
discards: [  0  54  71 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 336.6553 - loglik: -2.8693e+02 - logprior: -4.9729e+01
Epoch 2/2
10/10 - 2s - loss: 300.6552 - loglik: -2.8377e+02 - logprior: -1.6880e+01
Fitted a model with MAP estimate = -293.5021
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 321.8189 - loglik: -2.8296e+02 - logprior: -3.8862e+01
Epoch 2/10
10/10 - 2s - loss: 287.4440 - loglik: -2.7995e+02 - logprior: -7.4895e+00
Epoch 3/10
10/10 - 2s - loss: 281.4340 - loglik: -2.8038e+02 - logprior: -1.0576e+00
Epoch 4/10
10/10 - 2s - loss: 278.5115 - loglik: -2.8009e+02 - logprior: 1.5826
Epoch 5/10
10/10 - 2s - loss: 276.6732 - loglik: -2.7970e+02 - logprior: 3.0263
Epoch 6/10
10/10 - 2s - loss: 275.9320 - loglik: -2.7975e+02 - logprior: 3.8153
Epoch 7/10
10/10 - 2s - loss: 274.9369 - loglik: -2.7919e+02 - logprior: 4.2528
Epoch 8/10
10/10 - 2s - loss: 274.8368 - loglik: -2.7940e+02 - logprior: 4.5668
Epoch 9/10
10/10 - 2s - loss: 274.3349 - loglik: -2.7916e+02 - logprior: 4.8273
Epoch 10/10
10/10 - 2s - loss: 274.0076 - loglik: -2.7907e+02 - logprior: 5.0597
Fitted a model with MAP estimate = -273.8238
Time for alignment: 64.7091
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 433.5291 - loglik: -3.8807e+02 - logprior: -4.5459e+01
Epoch 2/10
10/10 - 2s - loss: 372.5189 - loglik: -3.6225e+02 - logprior: -1.0268e+01
Epoch 3/10
10/10 - 2s - loss: 337.6109 - loglik: -3.3344e+02 - logprior: -4.1675e+00
Epoch 4/10
10/10 - 2s - loss: 312.9152 - loglik: -3.1069e+02 - logprior: -2.2240e+00
Epoch 5/10
10/10 - 2s - loss: 303.5321 - loglik: -3.0222e+02 - logprior: -1.3141e+00
Epoch 6/10
10/10 - 2s - loss: 300.2986 - loglik: -2.9955e+02 - logprior: -7.4371e-01
Epoch 7/10
10/10 - 2s - loss: 298.1478 - loglik: -2.9781e+02 - logprior: -3.3403e-01
Epoch 8/10
10/10 - 2s - loss: 297.2092 - loglik: -2.9713e+02 - logprior: -7.6790e-02
Epoch 9/10
10/10 - 2s - loss: 296.7069 - loglik: -2.9676e+02 - logprior: 0.0483
Epoch 10/10
10/10 - 2s - loss: 295.9487 - loglik: -2.9606e+02 - logprior: 0.1121
Fitted a model with MAP estimate = -295.6533
expansions: [(7, 2), (9, 1), (24, 1), (27, 1), (40, 1), (42, 1), (48, 2), (59, 1), (62, 2), (65, 1), (73, 1), (74, 1), (90, 2), (91, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 349.8160 - loglik: -2.9883e+02 - logprior: -5.0985e+01
Epoch 2/2
10/10 - 2s - loss: 307.7751 - loglik: -2.8843e+02 - logprior: -1.9346e+01
Fitted a model with MAP estimate = -300.8614
expansions: [(9, 3)]
discards: [ 54  71 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 333.3284 - loglik: -2.8468e+02 - logprior: -4.8652e+01
Epoch 2/2
10/10 - 2s - loss: 295.6312 - loglik: -2.8233e+02 - logprior: -1.3303e+01
Fitted a model with MAP estimate = -286.9056
expansions: [(36, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 322.9164 - loglik: -2.8146e+02 - logprior: -4.1457e+01
Epoch 2/10
10/10 - 2s - loss: 288.6774 - loglik: -2.8059e+02 - logprior: -8.0915e+00
Epoch 3/10
10/10 - 2s - loss: 280.9654 - loglik: -2.7971e+02 - logprior: -1.2539e+00
Epoch 4/10
10/10 - 2s - loss: 278.4886 - loglik: -2.7982e+02 - logprior: 1.3277
Epoch 5/10
10/10 - 2s - loss: 276.2066 - loglik: -2.7877e+02 - logprior: 2.5637
Epoch 6/10
10/10 - 2s - loss: 274.9205 - loglik: -2.7831e+02 - logprior: 3.3929
Epoch 7/10
10/10 - 2s - loss: 273.8499 - loglik: -2.7791e+02 - logprior: 4.0565
Epoch 8/10
10/10 - 2s - loss: 273.7054 - loglik: -2.7821e+02 - logprior: 4.5039
Epoch 9/10
10/10 - 2s - loss: 273.1378 - loglik: -2.7793e+02 - logprior: 4.7954
Epoch 10/10
10/10 - 2s - loss: 273.2553 - loglik: -2.7832e+02 - logprior: 5.0601
Fitted a model with MAP estimate = -272.8028
Time for alignment: 63.9123
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.6696 - loglik: -3.8721e+02 - logprior: -4.5459e+01
Epoch 2/10
10/10 - 2s - loss: 373.7882 - loglik: -3.6352e+02 - logprior: -1.0269e+01
Epoch 3/10
10/10 - 2s - loss: 337.1630 - loglik: -3.3299e+02 - logprior: -4.1729e+00
Epoch 4/10
10/10 - 2s - loss: 312.5680 - loglik: -3.1027e+02 - logprior: -2.3029e+00
Epoch 5/10
10/10 - 2s - loss: 303.2639 - loglik: -3.0191e+02 - logprior: -1.3537e+00
Epoch 6/10
10/10 - 2s - loss: 299.6870 - loglik: -2.9894e+02 - logprior: -7.4503e-01
Epoch 7/10
10/10 - 2s - loss: 298.0326 - loglik: -2.9773e+02 - logprior: -3.0090e-01
Epoch 8/10
10/10 - 2s - loss: 297.0198 - loglik: -2.9696e+02 - logprior: -5.7606e-02
Epoch 9/10
10/10 - 2s - loss: 296.1280 - loglik: -2.9619e+02 - logprior: 0.0648
Epoch 10/10
10/10 - 2s - loss: 295.4986 - loglik: -2.9566e+02 - logprior: 0.1644
Fitted a model with MAP estimate = -295.2789
expansions: [(7, 2), (8, 2), (9, 1), (24, 1), (28, 1), (29, 3), (40, 1), (42, 1), (48, 2), (59, 1), (62, 2), (65, 1), (73, 1), (85, 1), (90, 2), (91, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 348.7682 - loglik: -2.9795e+02 - logprior: -5.0815e+01
Epoch 2/2
10/10 - 2s - loss: 305.1379 - loglik: -2.8588e+02 - logprior: -1.9254e+01
Fitted a model with MAP estimate = -297.7634
expansions: [(10, 1)]
discards: [ 59  76 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 331.6244 - loglik: -2.8323e+02 - logprior: -4.8392e+01
Epoch 2/2
10/10 - 2s - loss: 293.7982 - loglik: -2.8086e+02 - logprior: -1.2941e+01
Fitted a model with MAP estimate = -285.5906
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 320.1496 - loglik: -2.8028e+02 - logprior: -3.9874e+01
Epoch 2/10
10/10 - 2s - loss: 287.8297 - loglik: -2.8010e+02 - logprior: -7.7347e+00
Epoch 3/10
10/10 - 2s - loss: 281.1319 - loglik: -2.7996e+02 - logprior: -1.1766e+00
Epoch 4/10
10/10 - 2s - loss: 277.6993 - loglik: -2.7907e+02 - logprior: 1.3710
Epoch 5/10
10/10 - 2s - loss: 275.9613 - loglik: -2.7856e+02 - logprior: 2.6011
Epoch 6/10
10/10 - 2s - loss: 275.0991 - loglik: -2.7850e+02 - logprior: 3.4012
Epoch 7/10
10/10 - 2s - loss: 273.4727 - loglik: -2.7751e+02 - logprior: 4.0345
Epoch 8/10
10/10 - 2s - loss: 273.9438 - loglik: -2.7842e+02 - logprior: 4.4725
Fitted a model with MAP estimate = -273.2356
Time for alignment: 60.1876
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 433.3218 - loglik: -3.8787e+02 - logprior: -4.5457e+01
Epoch 2/10
10/10 - 2s - loss: 372.8919 - loglik: -3.6262e+02 - logprior: -1.0269e+01
Epoch 3/10
10/10 - 2s - loss: 336.9619 - loglik: -3.3280e+02 - logprior: -4.1595e+00
Epoch 4/10
10/10 - 2s - loss: 312.8533 - loglik: -3.1064e+02 - logprior: -2.2164e+00
Epoch 5/10
10/10 - 2s - loss: 303.2910 - loglik: -3.0197e+02 - logprior: -1.3166e+00
Epoch 6/10
10/10 - 2s - loss: 300.0240 - loglik: -2.9930e+02 - logprior: -7.1952e-01
Epoch 7/10
10/10 - 2s - loss: 297.7840 - loglik: -2.9743e+02 - logprior: -3.5358e-01
Epoch 8/10
10/10 - 2s - loss: 296.8126 - loglik: -2.9667e+02 - logprior: -1.4612e-01
Epoch 9/10
10/10 - 2s - loss: 295.9069 - loglik: -2.9589e+02 - logprior: -1.9165e-02
Epoch 10/10
10/10 - 2s - loss: 295.9814 - loglik: -2.9606e+02 - logprior: 0.0748
Fitted a model with MAP estimate = -295.3002
expansions: [(7, 2), (8, 2), (9, 1), (24, 1), (27, 1), (40, 1), (42, 1), (48, 2), (59, 1), (62, 2), (65, 1), (73, 1), (74, 1), (90, 2), (91, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 349.2488 - loglik: -2.9836e+02 - logprior: -5.0890e+01
Epoch 2/2
10/10 - 2s - loss: 305.2813 - loglik: -2.8596e+02 - logprior: -1.9317e+01
Fitted a model with MAP estimate = -298.3236
expansions: [(10, 1)]
discards: [ 56  73 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 332.5877 - loglik: -2.8420e+02 - logprior: -4.8391e+01
Epoch 2/2
10/10 - 2s - loss: 294.0760 - loglik: -2.8126e+02 - logprior: -1.2812e+01
Fitted a model with MAP estimate = -286.4244
expansions: [(36, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 322.3781 - loglik: -2.8102e+02 - logprior: -4.1354e+01
Epoch 2/10
10/10 - 2s - loss: 288.8447 - loglik: -2.8077e+02 - logprior: -8.0723e+00
Epoch 3/10
10/10 - 2s - loss: 281.6802 - loglik: -2.8043e+02 - logprior: -1.2542e+00
Epoch 4/10
10/10 - 2s - loss: 277.9383 - loglik: -2.7928e+02 - logprior: 1.3395
Epoch 5/10
10/10 - 2s - loss: 275.8171 - loglik: -2.7838e+02 - logprior: 2.5644
Epoch 6/10
10/10 - 2s - loss: 275.2331 - loglik: -2.7859e+02 - logprior: 3.3528
Epoch 7/10
10/10 - 2s - loss: 274.2296 - loglik: -2.7823e+02 - logprior: 4.0025
Epoch 8/10
10/10 - 2s - loss: 273.3442 - loglik: -2.7780e+02 - logprior: 4.4542
Epoch 9/10
10/10 - 2s - loss: 273.1532 - loglik: -2.7791e+02 - logprior: 4.7552
Epoch 10/10
10/10 - 2s - loss: 273.3821 - loglik: -2.7840e+02 - logprior: 5.0181
Fitted a model with MAP estimate = -272.7769
Time for alignment: 63.2485
Computed alignments with likelihoods: ['-273.7095', '-273.8238', '-272.8028', '-273.2356', '-272.7769']
Best model has likelihood: -272.7769  (prior= 5.1667 )
time for generating output: 0.1465
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.9288476411446249
Training of 5 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1eed3fad90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e4a4fbca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ea0d65940>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f9148c1f0>
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 456.1148 - loglik: -1.8944e+02 - logprior: -2.6667e+02
Epoch 2/10
10/10 - 1s - loss: 234.5588 - loglik: -1.6512e+02 - logprior: -6.9442e+01
Epoch 3/10
10/10 - 1s - loss: 173.0073 - loglik: -1.4291e+02 - logprior: -3.0096e+01
Epoch 4/10
10/10 - 1s - loss: 144.0434 - loglik: -1.2806e+02 - logprior: -1.5981e+01
Epoch 5/10
10/10 - 1s - loss: 128.8938 - loglik: -1.2040e+02 - logprior: -8.4906e+00
Epoch 6/10
10/10 - 1s - loss: 121.4804 - loglik: -1.1862e+02 - logprior: -2.8578e+00
Epoch 7/10
10/10 - 1s - loss: 117.1617 - loglik: -1.1776e+02 - logprior: 0.6011
Epoch 8/10
10/10 - 1s - loss: 114.3152 - loglik: -1.1699e+02 - logprior: 2.6699
Epoch 9/10
10/10 - 1s - loss: 112.5425 - loglik: -1.1684e+02 - logprior: 4.2936
Epoch 10/10
10/10 - 1s - loss: 111.3094 - loglik: -1.1677e+02 - logprior: 5.4648
Fitted a model with MAP estimate = -110.7504
expansions: [(7, 2), (16, 4), (21, 2), (23, 1), (24, 1), (32, 2), (34, 2), (43, 1), (44, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 418.7802 - loglik: -1.2389e+02 - logprior: -2.9489e+02
Epoch 2/2
10/10 - 1s - loss: 221.3064 - loglik: -1.0456e+02 - logprior: -1.1674e+02
Fitted a model with MAP estimate = -190.6243
expansions: [(0, 5)]
discards: [ 0 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 338.2716 - loglik: -1.0105e+02 - logprior: -2.3722e+02
Epoch 2/2
10/10 - 1s - loss: 153.8128 - loglik: -9.6626e+01 - logprior: -5.7186e+01
Fitted a model with MAP estimate = -125.9740
expansions: []
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 327.0518 - loglik: -9.7261e+01 - logprior: -2.2979e+02
Epoch 2/10
10/10 - 1s - loss: 151.2310 - loglik: -9.6089e+01 - logprior: -5.5142e+01
Epoch 3/10
10/10 - 1s - loss: 114.1349 - loglik: -9.5992e+01 - logprior: -1.8143e+01
Epoch 4/10
10/10 - 1s - loss: 98.3396 - loglik: -9.6177e+01 - logprior: -2.1624e+00
Epoch 5/10
10/10 - 1s - loss: 89.5162 - loglik: -9.6373e+01 - logprior: 6.8566
Epoch 6/10
10/10 - 1s - loss: 84.2210 - loglik: -9.6450e+01 - logprior: 12.2292
Epoch 7/10
10/10 - 1s - loss: 80.8138 - loglik: -9.6488e+01 - logprior: 15.6740
Epoch 8/10
10/10 - 1s - loss: 78.4000 - loglik: -9.6496e+01 - logprior: 18.0957
Epoch 9/10
10/10 - 1s - loss: 76.5264 - loglik: -9.6481e+01 - logprior: 19.9550
Epoch 10/10
10/10 - 1s - loss: 74.9629 - loglik: -9.6459e+01 - logprior: 21.4957
Fitted a model with MAP estimate = -74.1830
Time for alignment: 27.7899
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 456.1143 - loglik: -1.8944e+02 - logprior: -2.6667e+02
Epoch 2/10
10/10 - 1s - loss: 234.5577 - loglik: -1.6512e+02 - logprior: -6.9442e+01
Epoch 3/10
10/10 - 1s - loss: 173.0082 - loglik: -1.4291e+02 - logprior: -3.0096e+01
Epoch 4/10
10/10 - 1s - loss: 144.0619 - loglik: -1.2808e+02 - logprior: -1.5978e+01
Epoch 5/10
10/10 - 1s - loss: 128.9094 - loglik: -1.2042e+02 - logprior: -8.4853e+00
Epoch 6/10
10/10 - 1s - loss: 121.4922 - loglik: -1.1863e+02 - logprior: -2.8633e+00
Epoch 7/10
10/10 - 1s - loss: 117.1745 - loglik: -1.1778e+02 - logprior: 0.6027
Epoch 8/10
10/10 - 1s - loss: 114.3230 - loglik: -1.1699e+02 - logprior: 2.6695
Epoch 9/10
10/10 - 1s - loss: 112.5447 - loglik: -1.1684e+02 - logprior: 4.2933
Epoch 10/10
10/10 - 1s - loss: 111.3111 - loglik: -1.1677e+02 - logprior: 5.4637
Fitted a model with MAP estimate = -110.7516
expansions: [(7, 2), (16, 4), (21, 2), (23, 1), (24, 1), (32, 2), (34, 2), (43, 1), (44, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 418.7751 - loglik: -1.2389e+02 - logprior: -2.9489e+02
Epoch 2/2
10/10 - 1s - loss: 221.3057 - loglik: -1.0456e+02 - logprior: -1.1674e+02
Fitted a model with MAP estimate = -190.6241
expansions: [(0, 5)]
discards: [ 0 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 338.2697 - loglik: -1.0105e+02 - logprior: -2.3722e+02
Epoch 2/2
10/10 - 1s - loss: 153.8125 - loglik: -9.6626e+01 - logprior: -5.7187e+01
Fitted a model with MAP estimate = -125.9733
expansions: []
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 327.0528 - loglik: -9.7262e+01 - logprior: -2.2979e+02
Epoch 2/10
10/10 - 1s - loss: 151.2308 - loglik: -9.6089e+01 - logprior: -5.5142e+01
Epoch 3/10
10/10 - 1s - loss: 114.1349 - loglik: -9.5991e+01 - logprior: -1.8144e+01
Epoch 4/10
10/10 - 1s - loss: 98.3400 - loglik: -9.6177e+01 - logprior: -2.1625e+00
Epoch 5/10
10/10 - 1s - loss: 89.5167 - loglik: -9.6372e+01 - logprior: 6.8554
Epoch 6/10
10/10 - 1s - loss: 84.2215 - loglik: -9.6450e+01 - logprior: 12.2283
Epoch 7/10
10/10 - 1s - loss: 80.8145 - loglik: -9.6487e+01 - logprior: 15.6725
Epoch 8/10
10/10 - 1s - loss: 78.4008 - loglik: -9.6495e+01 - logprior: 18.0943
Epoch 9/10
10/10 - 1s - loss: 76.5273 - loglik: -9.6480e+01 - logprior: 19.9531
Epoch 10/10
10/10 - 1s - loss: 74.9637 - loglik: -9.6458e+01 - logprior: 21.4940
Fitted a model with MAP estimate = -74.1838
Time for alignment: 32.2991
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 456.1149 - loglik: -1.8944e+02 - logprior: -2.6667e+02
Epoch 2/10
10/10 - 1s - loss: 234.5584 - loglik: -1.6512e+02 - logprior: -6.9442e+01
Epoch 3/10
10/10 - 1s - loss: 173.0074 - loglik: -1.4291e+02 - logprior: -3.0096e+01
Epoch 4/10
10/10 - 1s - loss: 144.0470 - loglik: -1.2807e+02 - logprior: -1.5980e+01
Epoch 5/10
10/10 - 1s - loss: 128.8968 - loglik: -1.2041e+02 - logprior: -8.4899e+00
Epoch 6/10
10/10 - 1s - loss: 121.4836 - loglik: -1.1863e+02 - logprior: -2.8584e+00
Epoch 7/10
10/10 - 1s - loss: 117.1658 - loglik: -1.1777e+02 - logprior: 0.6015
Epoch 8/10
10/10 - 1s - loss: 114.3176 - loglik: -1.1699e+02 - logprior: 2.6698
Epoch 9/10
10/10 - 1s - loss: 112.5432 - loglik: -1.1684e+02 - logprior: 4.2935
Epoch 10/10
10/10 - 1s - loss: 111.3100 - loglik: -1.1677e+02 - logprior: 5.4647
Fitted a model with MAP estimate = -110.7509
expansions: [(7, 2), (16, 4), (21, 2), (23, 1), (24, 1), (32, 2), (34, 2), (43, 1), (44, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 418.7704 - loglik: -1.2388e+02 - logprior: -2.9489e+02
Epoch 2/2
10/10 - 1s - loss: 221.3065 - loglik: -1.0456e+02 - logprior: -1.1674e+02
Fitted a model with MAP estimate = -190.6242
expansions: [(0, 5)]
discards: [ 0 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 338.2706 - loglik: -1.0105e+02 - logprior: -2.3722e+02
Epoch 2/2
10/10 - 1s - loss: 153.8123 - loglik: -9.6626e+01 - logprior: -5.7186e+01
Fitted a model with MAP estimate = -125.9733
expansions: []
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 327.0523 - loglik: -9.7261e+01 - logprior: -2.2979e+02
Epoch 2/10
10/10 - 1s - loss: 151.2309 - loglik: -9.6089e+01 - logprior: -5.5142e+01
Epoch 3/10
10/10 - 1s - loss: 114.1351 - loglik: -9.5992e+01 - logprior: -1.8144e+01
Epoch 4/10
10/10 - 1s - loss: 98.3403 - loglik: -9.6177e+01 - logprior: -2.1629e+00
Epoch 5/10
10/10 - 1s - loss: 89.5172 - loglik: -9.6373e+01 - logprior: 6.8558
Epoch 6/10
10/10 - 1s - loss: 84.2221 - loglik: -9.6450e+01 - logprior: 12.2280
Epoch 7/10
10/10 - 1s - loss: 80.8151 - loglik: -9.6487e+01 - logprior: 15.6724
Epoch 8/10
10/10 - 1s - loss: 78.4015 - loglik: -9.6495e+01 - logprior: 18.0938
Epoch 9/10
10/10 - 1s - loss: 76.5284 - loglik: -9.6481e+01 - logprior: 19.9527
Epoch 10/10
10/10 - 1s - loss: 74.9653 - loglik: -9.6458e+01 - logprior: 21.4930
Fitted a model with MAP estimate = -74.1857
Time for alignment: 31.2827
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 456.1147 - loglik: -1.8944e+02 - logprior: -2.6667e+02
Epoch 2/10
10/10 - 1s - loss: 234.5579 - loglik: -1.6512e+02 - logprior: -6.9442e+01
Epoch 3/10
10/10 - 1s - loss: 173.0074 - loglik: -1.4291e+02 - logprior: -3.0096e+01
Epoch 4/10
10/10 - 1s - loss: 144.0555 - loglik: -1.2808e+02 - logprior: -1.5979e+01
Epoch 5/10
10/10 - 1s - loss: 128.9043 - loglik: -1.2042e+02 - logprior: -8.4867e+00
Epoch 6/10
10/10 - 1s - loss: 121.4897 - loglik: -1.1863e+02 - logprior: -2.8612e+00
Epoch 7/10
10/10 - 1s - loss: 117.1729 - loglik: -1.1778e+02 - logprior: 0.6024
Epoch 8/10
10/10 - 1s - loss: 114.3219 - loglik: -1.1699e+02 - logprior: 2.6696
Epoch 9/10
10/10 - 1s - loss: 112.5443 - loglik: -1.1684e+02 - logprior: 4.2934
Epoch 10/10
10/10 - 1s - loss: 111.3110 - loglik: -1.1678e+02 - logprior: 5.4642
Fitted a model with MAP estimate = -110.7514
expansions: [(7, 2), (16, 4), (21, 2), (23, 1), (24, 1), (32, 2), (34, 2), (43, 1), (44, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 418.7748 - loglik: -1.2389e+02 - logprior: -2.9489e+02
Epoch 2/2
10/10 - 1s - loss: 221.3058 - loglik: -1.0456e+02 - logprior: -1.1674e+02
Fitted a model with MAP estimate = -190.6241
expansions: [(0, 5)]
discards: [ 0 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 338.2701 - loglik: -1.0105e+02 - logprior: -2.3722e+02
Epoch 2/2
10/10 - 1s - loss: 153.8125 - loglik: -9.6626e+01 - logprior: -5.7187e+01
Fitted a model with MAP estimate = -125.9733
expansions: []
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 327.0529 - loglik: -9.7262e+01 - logprior: -2.2979e+02
Epoch 2/10
10/10 - 1s - loss: 151.2307 - loglik: -9.6089e+01 - logprior: -5.5142e+01
Epoch 3/10
10/10 - 1s - loss: 114.1346 - loglik: -9.5991e+01 - logprior: -1.8143e+01
Epoch 4/10
10/10 - 1s - loss: 98.3395 - loglik: -9.6177e+01 - logprior: -2.1622e+00
Epoch 5/10
10/10 - 1s - loss: 89.5160 - loglik: -9.6372e+01 - logprior: 6.8560
Epoch 6/10
10/10 - 1s - loss: 84.2207 - loglik: -9.6450e+01 - logprior: 12.2291
Epoch 7/10
10/10 - 1s - loss: 80.8133 - loglik: -9.6487e+01 - logprior: 15.6734
Epoch 8/10
10/10 - 1s - loss: 78.3995 - loglik: -9.6495e+01 - logprior: 18.0954
Epoch 9/10
10/10 - 1s - loss: 76.5259 - loglik: -9.6480e+01 - logprior: 19.9543
Epoch 10/10
10/10 - 1s - loss: 74.9623 - loglik: -9.6457e+01 - logprior: 21.4951
Fitted a model with MAP estimate = -74.1825
Time for alignment: 31.0375
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 456.1149 - loglik: -1.8944e+02 - logprior: -2.6667e+02
Epoch 2/10
10/10 - 1s - loss: 234.5583 - loglik: -1.6512e+02 - logprior: -6.9442e+01
Epoch 3/10
10/10 - 1s - loss: 173.0079 - loglik: -1.4291e+02 - logprior: -3.0096e+01
Epoch 4/10
10/10 - 1s - loss: 144.0532 - loglik: -1.2807e+02 - logprior: -1.5980e+01
Epoch 5/10
10/10 - 1s - loss: 128.9014 - loglik: -1.2041e+02 - logprior: -8.4884e+00
Epoch 6/10
10/10 - 1s - loss: 121.4852 - loglik: -1.1862e+02 - logprior: -2.8607e+00
Epoch 7/10
10/10 - 1s - loss: 117.1662 - loglik: -1.1777e+02 - logprior: 0.6017
Epoch 8/10
10/10 - 1s - loss: 114.3179 - loglik: -1.1699e+02 - logprior: 2.6697
Epoch 9/10
10/10 - 1s - loss: 112.5433 - loglik: -1.1684e+02 - logprior: 4.2935
Epoch 10/10
10/10 - 1s - loss: 111.3100 - loglik: -1.1677e+02 - logprior: 5.4642
Fitted a model with MAP estimate = -110.7510
expansions: [(7, 2), (16, 4), (21, 2), (23, 1), (24, 1), (32, 2), (34, 2), (43, 1), (44, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 418.7695 - loglik: -1.2388e+02 - logprior: -2.9489e+02
Epoch 2/2
10/10 - 1s - loss: 221.3065 - loglik: -1.0456e+02 - logprior: -1.1674e+02
Fitted a model with MAP estimate = -190.6245
expansions: [(0, 5)]
discards: [ 0 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 338.2698 - loglik: -1.0105e+02 - logprior: -2.3722e+02
Epoch 2/2
10/10 - 1s - loss: 153.8125 - loglik: -9.6626e+01 - logprior: -5.7187e+01
Fitted a model with MAP estimate = -125.9733
expansions: []
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 327.0520 - loglik: -9.7261e+01 - logprior: -2.2979e+02
Epoch 2/10
10/10 - 1s - loss: 151.2309 - loglik: -9.6089e+01 - logprior: -5.5141e+01
Epoch 3/10
10/10 - 1s - loss: 114.1350 - loglik: -9.5992e+01 - logprior: -1.8143e+01
Epoch 4/10
10/10 - 1s - loss: 98.3400 - loglik: -9.6177e+01 - logprior: -2.1627e+00
Epoch 5/10
10/10 - 1s - loss: 89.5166 - loglik: -9.6373e+01 - logprior: 6.8560
Epoch 6/10
10/10 - 1s - loss: 84.2215 - loglik: -9.6450e+01 - logprior: 12.2285
Epoch 7/10
10/10 - 1s - loss: 80.8142 - loglik: -9.6487e+01 - logprior: 15.6730
Epoch 8/10
10/10 - 1s - loss: 78.4006 - loglik: -9.6495e+01 - logprior: 18.0946
Epoch 9/10
10/10 - 1s - loss: 76.5273 - loglik: -9.6481e+01 - logprior: 19.9535
Epoch 10/10
10/10 - 1s - loss: 74.9642 - loglik: -9.6458e+01 - logprior: 21.4940
Fitted a model with MAP estimate = -74.1844
Time for alignment: 31.2589
Computed alignments with likelihoods: ['-74.1830', '-74.1838', '-74.1857', '-74.1825', '-74.1844']
Best model has likelihood: -74.1825  (prior= 22.2648 )
time for generating output: 0.1217
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.7809667673716012
Training of 5 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fb4c3b160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201a9d65e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f076bd220>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1e4a6e8b80>
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 713.4385 - loglik: -7.0940e+02 - logprior: -4.0350e+00
Epoch 2/10
26/26 - 9s - loss: 611.5549 - loglik: -6.1064e+02 - logprior: -9.1933e-01
Epoch 3/10
26/26 - 9s - loss: 582.3854 - loglik: -5.8116e+02 - logprior: -1.2300e+00
Epoch 4/10
26/26 - 9s - loss: 578.4966 - loglik: -5.7727e+02 - logprior: -1.2303e+00
Epoch 5/10
26/26 - 9s - loss: 576.4435 - loglik: -5.7524e+02 - logprior: -1.2068e+00
Epoch 6/10
26/26 - 9s - loss: 574.7963 - loglik: -5.7361e+02 - logprior: -1.1909e+00
Epoch 7/10
26/26 - 9s - loss: 574.7714 - loglik: -5.7360e+02 - logprior: -1.1716e+00
Epoch 8/10
26/26 - 9s - loss: 572.2762 - loglik: -5.7111e+02 - logprior: -1.1618e+00
Epoch 9/10
26/26 - 9s - loss: 571.7014 - loglik: -5.7056e+02 - logprior: -1.1424e+00
Epoch 10/10
26/26 - 9s - loss: 571.3832 - loglik: -5.7024e+02 - logprior: -1.1404e+00
Fitted a model with MAP estimate = -570.9716
expansions: [(10, 2), (32, 2), (37, 1), (38, 1), (39, 1), (72, 1), (75, 2), (76, 2), (81, 1), (105, 1), (114, 2), (163, 1), (173, 1), (174, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 219 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 586.0311 - loglik: -5.7930e+02 - logprior: -6.7274e+00
Epoch 2/2
26/26 - 10s - loss: 566.6971 - loglik: -5.6401e+02 - logprior: -2.6919e+00
Fitted a model with MAP estimate = -563.5592
expansions: [(0, 3), (11, 1)]
discards: [ 0 34 85 86]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 219 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 570.4071 - loglik: -5.6620e+02 - logprior: -4.2120e+00
Epoch 2/2
26/26 - 10s - loss: 563.5888 - loglik: -5.6297e+02 - logprior: -6.2379e-01
Fitted a model with MAP estimate = -561.5922
expansions: [(87, 2)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 219 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 571.1287 - loglik: -5.6505e+02 - logprior: -6.0814e+00
Epoch 2/10
26/26 - 10s - loss: 562.5551 - loglik: -5.6146e+02 - logprior: -1.0986e+00
Epoch 3/10
26/26 - 10s - loss: 560.6793 - loglik: -5.6077e+02 - logprior: 0.0919
Epoch 4/10
26/26 - 10s - loss: 557.7987 - loglik: -5.5796e+02 - logprior: 0.1646
Epoch 5/10
26/26 - 10s - loss: 557.3342 - loglik: -5.5759e+02 - logprior: 0.2545
Epoch 6/10
26/26 - 10s - loss: 556.1772 - loglik: -5.5655e+02 - logprior: 0.3730
Epoch 7/10
26/26 - 10s - loss: 555.0234 - loglik: -5.5552e+02 - logprior: 0.4978
Epoch 8/10
26/26 - 10s - loss: 554.7569 - loglik: -5.5534e+02 - logprior: 0.5863
Epoch 9/10
26/26 - 10s - loss: 553.2374 - loglik: -5.5393e+02 - logprior: 0.6966
Epoch 10/10
26/26 - 10s - loss: 554.8751 - loglik: -5.5567e+02 - logprior: 0.7968
Fitted a model with MAP estimate = -553.3600
Time for alignment: 278.5690
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 714.2807 - loglik: -7.1026e+02 - logprior: -4.0159e+00
Epoch 2/10
26/26 - 9s - loss: 611.9511 - loglik: -6.1108e+02 - logprior: -8.7047e-01
Epoch 3/10
26/26 - 9s - loss: 589.1807 - loglik: -5.8809e+02 - logprior: -1.0950e+00
Epoch 4/10
26/26 - 9s - loss: 581.4430 - loglik: -5.8038e+02 - logprior: -1.0610e+00
Epoch 5/10
26/26 - 9s - loss: 580.1261 - loglik: -5.7909e+02 - logprior: -1.0377e+00
Epoch 6/10
26/26 - 9s - loss: 577.2136 - loglik: -5.7618e+02 - logprior: -1.0316e+00
Epoch 7/10
26/26 - 9s - loss: 577.0314 - loglik: -5.7600e+02 - logprior: -1.0315e+00
Epoch 8/10
26/26 - 9s - loss: 577.3610 - loglik: -5.7632e+02 - logprior: -1.0361e+00
Fitted a model with MAP estimate = -575.5265
expansions: [(10, 3), (26, 4), (37, 1), (39, 1), (40, 1), (51, 2), (54, 1), (70, 1), (71, 1), (80, 2), (106, 1), (107, 4), (114, 2), (139, 1), (161, 3), (163, 1), (173, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 14s - loss: 579.8964 - loglik: -5.7324e+02 - logprior: -6.6568e+00
Epoch 2/2
26/26 - 11s - loss: 557.4181 - loglik: -5.5424e+02 - logprior: -3.1815e+00
Fitted a model with MAP estimate = -553.0917
expansions: [(0, 5), (61, 1), (71, 1)]
discards: [  0   9  10  11  29  30  31  94 124 125 126 127 188 189 190 203 204]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 222 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 567.2512 - loglik: -5.6272e+02 - logprior: -4.5291e+00
Epoch 2/2
26/26 - 10s - loss: 558.3691 - loglik: -5.5728e+02 - logprior: -1.0931e+00
Fitted a model with MAP estimate = -556.3042
expansions: [(181, 2)]
discards: [0 1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 220 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 567.1941 - loglik: -5.6082e+02 - logprior: -6.3737e+00
Epoch 2/10
26/26 - 10s - loss: 558.3614 - loglik: -5.5641e+02 - logprior: -1.9508e+00
Epoch 3/10
26/26 - 10s - loss: 554.0665 - loglik: -5.5389e+02 - logprior: -1.7896e-01
Epoch 4/10
26/26 - 10s - loss: 554.7816 - loglik: -5.5456e+02 - logprior: -2.2608e-01
Fitted a model with MAP estimate = -552.9410
Time for alignment: 203.2603
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 712.0141 - loglik: -7.0797e+02 - logprior: -4.0457e+00
Epoch 2/10
26/26 - 9s - loss: 606.4671 - loglik: -6.0536e+02 - logprior: -1.1076e+00
Epoch 3/10
26/26 - 9s - loss: 576.4846 - loglik: -5.7507e+02 - logprior: -1.4194e+00
Epoch 4/10
26/26 - 9s - loss: 574.2570 - loglik: -5.7292e+02 - logprior: -1.3370e+00
Epoch 5/10
26/26 - 9s - loss: 571.2540 - loglik: -5.6994e+02 - logprior: -1.3102e+00
Epoch 6/10
26/26 - 9s - loss: 569.3400 - loglik: -5.6804e+02 - logprior: -1.2970e+00
Epoch 7/10
26/26 - 9s - loss: 569.6443 - loglik: -5.6835e+02 - logprior: -1.2989e+00
Fitted a model with MAP estimate = -568.2558
expansions: [(11, 2), (12, 1), (32, 1), (33, 2), (39, 1), (52, 6), (53, 2), (77, 3), (82, 2), (84, 1), (105, 1), (107, 7), (108, 3), (109, 1), (114, 2), (142, 1), (161, 2), (163, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 14s - loss: 571.0697 - loglik: -5.6443e+02 - logprior: -6.6415e+00
Epoch 2/2
26/26 - 11s - loss: 551.2631 - loglik: -5.4861e+02 - logprior: -2.6560e+00
Fitted a model with MAP estimate = -546.5176
expansions: [(0, 3)]
discards: [  0  65  92 101 102 130 131 132 133 196]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 555.9113 - loglik: -5.5173e+02 - logprior: -4.1785e+00
Epoch 2/2
26/26 - 10s - loss: 545.8140 - loglik: -5.4519e+02 - logprior: -6.2450e-01
Fitted a model with MAP estimate = -543.6492
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 553.8759 - loglik: -5.4782e+02 - logprior: -6.0565e+00
Epoch 2/10
26/26 - 10s - loss: 549.2700 - loglik: -5.4705e+02 - logprior: -2.2205e+00
Epoch 3/10
26/26 - 10s - loss: 544.8414 - loglik: -5.4420e+02 - logprior: -6.4373e-01
Epoch 4/10
26/26 - 10s - loss: 543.0770 - loglik: -5.4344e+02 - logprior: 0.3635
Epoch 5/10
26/26 - 10s - loss: 541.1720 - loglik: -5.4160e+02 - logprior: 0.4280
Epoch 6/10
26/26 - 10s - loss: 540.7390 - loglik: -5.4128e+02 - logprior: 0.5384
Epoch 7/10
26/26 - 10s - loss: 540.1271 - loglik: -5.4079e+02 - logprior: 0.6591
Epoch 8/10
26/26 - 10s - loss: 538.0485 - loglik: -5.3881e+02 - logprior: 0.7599
Epoch 9/10
26/26 - 10s - loss: 538.7047 - loglik: -5.3958e+02 - logprior: 0.8704
Fitted a model with MAP estimate = -537.9607
Time for alignment: 249.2671
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 714.5644 - loglik: -7.1055e+02 - logprior: -4.0099e+00
Epoch 2/10
26/26 - 9s - loss: 612.5470 - loglik: -6.1156e+02 - logprior: -9.8571e-01
Epoch 3/10
26/26 - 9s - loss: 587.6127 - loglik: -5.8641e+02 - logprior: -1.2062e+00
Epoch 4/10
26/26 - 9s - loss: 582.0298 - loglik: -5.8086e+02 - logprior: -1.1731e+00
Epoch 5/10
26/26 - 9s - loss: 579.5142 - loglik: -5.7834e+02 - logprior: -1.1714e+00
Epoch 6/10
26/26 - 9s - loss: 578.9338 - loglik: -5.7777e+02 - logprior: -1.1619e+00
Epoch 7/10
26/26 - 9s - loss: 577.3853 - loglik: -5.7622e+02 - logprior: -1.1671e+00
Epoch 8/10
26/26 - 9s - loss: 575.3780 - loglik: -5.7423e+02 - logprior: -1.1441e+00
Epoch 9/10
26/26 - 9s - loss: 576.1771 - loglik: -5.7505e+02 - logprior: -1.1313e+00
Fitted a model with MAP estimate = -575.8502
expansions: [(10, 3), (25, 4), (38, 1), (39, 1), (56, 3), (57, 2), (70, 1), (71, 1), (72, 1), (104, 3), (107, 1), (109, 1), (110, 2), (114, 1), (142, 1), (161, 1), (163, 1), (173, 1), (174, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 578.7089 - loglik: -5.7214e+02 - logprior: -6.5645e+00
Epoch 2/2
26/26 - 10s - loss: 557.6609 - loglik: -5.5470e+02 - logprior: -2.9610e+00
Fitted a model with MAP estimate = -554.2838
expansions: [(0, 5), (58, 8), (91, 1)]
discards: [  0   9  10  11  28  29  30  69 122 131]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 560.2500 - loglik: -5.5577e+02 - logprior: -4.4781e+00
Epoch 2/2
26/26 - 11s - loss: 551.9454 - loglik: -5.5093e+02 - logprior: -1.0104e+00
Fitted a model with MAP estimate = -548.1068
expansions: [(13, 2)]
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 554.6173 - loglik: -5.5039e+02 - logprior: -4.2301e+00
Epoch 2/10
26/26 - 11s - loss: 547.5333 - loglik: -5.4688e+02 - logprior: -6.5724e-01
Epoch 3/10
26/26 - 11s - loss: 547.6810 - loglik: -5.4717e+02 - logprior: -5.0848e-01
Fitted a model with MAP estimate = -545.1327
Time for alignment: 204.5657
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 11s - loss: 713.1748 - loglik: -7.0916e+02 - logprior: -4.0179e+00
Epoch 2/10
26/26 - 9s - loss: 612.2015 - loglik: -6.1120e+02 - logprior: -1.0025e+00
Epoch 3/10
26/26 - 9s - loss: 582.2997 - loglik: -5.8104e+02 - logprior: -1.2639e+00
Epoch 4/10
26/26 - 9s - loss: 577.0447 - loglik: -5.7582e+02 - logprior: -1.2199e+00
Epoch 5/10
26/26 - 9s - loss: 574.4314 - loglik: -5.7326e+02 - logprior: -1.1674e+00
Epoch 6/10
26/26 - 9s - loss: 573.7414 - loglik: -5.7260e+02 - logprior: -1.1422e+00
Epoch 7/10
26/26 - 9s - loss: 571.2921 - loglik: -5.7015e+02 - logprior: -1.1406e+00
Epoch 8/10
26/26 - 9s - loss: 572.7772 - loglik: -5.7166e+02 - logprior: -1.1170e+00
Fitted a model with MAP estimate = -571.0319
expansions: [(10, 3), (32, 1), (38, 1), (39, 1), (49, 6), (50, 1), (72, 1), (75, 1), (82, 1), (109, 1), (114, 2), (160, 1), (173, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 224 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 578.8052 - loglik: -5.7229e+02 - logprior: -6.5194e+00
Epoch 2/2
26/26 - 10s - loss: 562.9794 - loglik: -5.6051e+02 - logprior: -2.4715e+00
Fitted a model with MAP estimate = -559.5044
expansions: [(0, 3)]
discards: [  0 131]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 225 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 564.6212 - loglik: -5.6042e+02 - logprior: -4.2002e+00
Epoch 2/2
26/26 - 10s - loss: 560.6416 - loglik: -5.6000e+02 - logprior: -6.3795e-01
Fitted a model with MAP estimate = -557.7083
expansions: [(153, 1)]
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 223 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 568.4894 - loglik: -5.6235e+02 - logprior: -6.1436e+00
Epoch 2/10
26/26 - 10s - loss: 559.2877 - loglik: -5.5682e+02 - logprior: -2.4711e+00
Epoch 3/10
26/26 - 10s - loss: 556.3362 - loglik: -5.5519e+02 - logprior: -1.1500e+00
Epoch 4/10
26/26 - 10s - loss: 554.4471 - loglik: -5.5438e+02 - logprior: -6.5248e-02
Epoch 5/10
26/26 - 10s - loss: 551.7096 - loglik: -5.5170e+02 - logprior: -9.4451e-03
Epoch 6/10
26/26 - 10s - loss: 552.4401 - loglik: -5.5252e+02 - logprior: 0.0803
Fitted a model with MAP estimate = -550.4079
Time for alignment: 220.4023
Computed alignments with likelihoods: ['-553.3600', '-552.9410', '-537.9607', '-545.1327', '-550.4079']
Best model has likelihood: -537.9607  (prior= 0.9240 )
time for generating output: 0.3068
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.694516129032258
Training of 5 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ecb71ddc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e5c60d3a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa381df10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1e48616670>
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 423.6631 - loglik: -3.4696e+02 - logprior: -7.6704e+01
Epoch 2/10
10/10 - 1s - loss: 339.9633 - loglik: -3.2234e+02 - logprior: -1.7622e+01
Epoch 3/10
10/10 - 1s - loss: 298.0844 - loglik: -2.9166e+02 - logprior: -6.4234e+00
Epoch 4/10
10/10 - 1s - loss: 274.7649 - loglik: -2.7245e+02 - logprior: -2.3126e+00
Epoch 5/10
10/10 - 1s - loss: 266.2484 - loglik: -2.6611e+02 - logprior: -1.3578e-01
Epoch 6/10
10/10 - 1s - loss: 262.3986 - loglik: -2.6345e+02 - logprior: 1.0563
Epoch 7/10
10/10 - 1s - loss: 260.3090 - loglik: -2.6205e+02 - logprior: 1.7410
Epoch 8/10
10/10 - 1s - loss: 258.8303 - loglik: -2.6103e+02 - logprior: 2.2009
Epoch 9/10
10/10 - 1s - loss: 257.8896 - loglik: -2.6051e+02 - logprior: 2.6171
Epoch 10/10
10/10 - 1s - loss: 257.6379 - loglik: -2.6053e+02 - logprior: 2.8910
Fitted a model with MAP estimate = -257.1335
expansions: [(4, 2), (12, 4), (19, 1), (31, 1), (37, 5), (38, 1), (47, 3), (59, 4), (84, 4), (86, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 348.0846 - loglik: -2.6275e+02 - logprior: -8.5338e+01
Epoch 2/2
10/10 - 1s - loss: 280.9692 - loglik: -2.4882e+02 - logprior: -3.2148e+01
Fitted a model with MAP estimate = -269.1279
expansions: [(0, 3), (5, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 312.4194 - loglik: -2.4541e+02 - logprior: -6.7009e+01
Epoch 2/2
10/10 - 1s - loss: 254.4376 - loglik: -2.4078e+02 - logprior: -1.3659e+01
Fitted a model with MAP estimate = -245.3681
expansions: [(16, 1)]
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 307.3008 - loglik: -2.4213e+02 - logprior: -6.5169e+01
Epoch 2/10
10/10 - 1s - loss: 252.2304 - loglik: -2.3945e+02 - logprior: -1.2782e+01
Epoch 3/10
10/10 - 1s - loss: 240.3079 - loglik: -2.3847e+02 - logprior: -1.8343e+00
Epoch 4/10
10/10 - 1s - loss: 234.9903 - loglik: -2.3781e+02 - logprior: 2.8155
Epoch 5/10
10/10 - 1s - loss: 232.3259 - loglik: -2.3772e+02 - logprior: 5.3974
Epoch 6/10
10/10 - 1s - loss: 230.4414 - loglik: -2.3737e+02 - logprior: 6.9268
Epoch 7/10
10/10 - 1s - loss: 229.4242 - loglik: -2.3735e+02 - logprior: 7.9284
Epoch 8/10
10/10 - 1s - loss: 228.8001 - loglik: -2.3745e+02 - logprior: 8.6480
Epoch 9/10
10/10 - 2s - loss: 227.8645 - loglik: -2.3705e+02 - logprior: 9.1864
Epoch 10/10
10/10 - 1s - loss: 227.4841 - loglik: -2.3712e+02 - logprior: 9.6321
Fitted a model with MAP estimate = -227.0124
Time for alignment: 50.1404
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 423.6415 - loglik: -3.4694e+02 - logprior: -7.6703e+01
Epoch 2/10
10/10 - 1s - loss: 340.4770 - loglik: -3.2287e+02 - logprior: -1.7611e+01
Epoch 3/10
10/10 - 1s - loss: 296.9956 - loglik: -2.9062e+02 - logprior: -6.3776e+00
Epoch 4/10
10/10 - 1s - loss: 272.9932 - loglik: -2.7076e+02 - logprior: -2.2306e+00
Epoch 5/10
10/10 - 1s - loss: 264.2280 - loglik: -2.6418e+02 - logprior: -4.8816e-02
Epoch 6/10
10/10 - 1s - loss: 260.3482 - loglik: -2.6159e+02 - logprior: 1.2467
Epoch 7/10
10/10 - 1s - loss: 258.8467 - loglik: -2.6088e+02 - logprior: 2.0373
Epoch 8/10
10/10 - 1s - loss: 257.6427 - loglik: -2.6022e+02 - logprior: 2.5725
Epoch 9/10
10/10 - 1s - loss: 257.3300 - loglik: -2.6029e+02 - logprior: 2.9556
Epoch 10/10
10/10 - 1s - loss: 256.5228 - loglik: -2.5977e+02 - logprior: 3.2463
Fitted a model with MAP estimate = -256.3119
expansions: [(4, 2), (7, 1), (12, 5), (33, 1), (36, 6), (47, 3), (59, 4), (84, 4), (86, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 347.3341 - loglik: -2.6221e+02 - logprior: -8.5128e+01
Epoch 2/2
10/10 - 1s - loss: 279.2141 - loglik: -2.4729e+02 - logprior: -3.1924e+01
Fitted a model with MAP estimate = -267.3728
expansions: [(0, 3), (13, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 310.6968 - loglik: -2.4376e+02 - logprior: -6.6938e+01
Epoch 2/2
10/10 - 2s - loss: 252.1237 - loglik: -2.3854e+02 - logprior: -1.3588e+01
Fitted a model with MAP estimate = -242.9782
expansions: []
discards: [ 1  2 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 305.9930 - loglik: -2.4066e+02 - logprior: -6.5334e+01
Epoch 2/10
10/10 - 1s - loss: 251.5174 - loglik: -2.3847e+02 - logprior: -1.3049e+01
Epoch 3/10
10/10 - 1s - loss: 239.7403 - loglik: -2.3762e+02 - logprior: -2.1239e+00
Epoch 4/10
10/10 - 1s - loss: 234.6077 - loglik: -2.3723e+02 - logprior: 2.6255
Epoch 5/10
10/10 - 1s - loss: 231.7165 - loglik: -2.3697e+02 - logprior: 5.2534
Epoch 6/10
10/10 - 1s - loss: 230.3142 - loglik: -2.3712e+02 - logprior: 6.8027
Epoch 7/10
10/10 - 1s - loss: 228.9405 - loglik: -2.3675e+02 - logprior: 7.8136
Epoch 8/10
10/10 - 1s - loss: 228.1559 - loglik: -2.3665e+02 - logprior: 8.4911
Epoch 9/10
10/10 - 1s - loss: 227.1985 - loglik: -2.3617e+02 - logprior: 8.9750
Epoch 10/10
10/10 - 1s - loss: 226.5751 - loglik: -2.3592e+02 - logprior: 9.3490
Fitted a model with MAP estimate = -226.1363
Time for alignment: 49.9686
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 423.5331 - loglik: -3.4683e+02 - logprior: -7.6702e+01
Epoch 2/10
10/10 - 1s - loss: 340.7872 - loglik: -3.2317e+02 - logprior: -1.7614e+01
Epoch 3/10
10/10 - 1s - loss: 298.0664 - loglik: -2.9169e+02 - logprior: -6.3755e+00
Epoch 4/10
10/10 - 1s - loss: 273.9263 - loglik: -2.7168e+02 - logprior: -2.2476e+00
Epoch 5/10
10/10 - 1s - loss: 264.6348 - loglik: -2.6461e+02 - logprior: -1.9959e-02
Epoch 6/10
10/10 - 1s - loss: 260.9191 - loglik: -2.6211e+02 - logprior: 1.1920
Epoch 7/10
10/10 - 1s - loss: 259.2695 - loglik: -2.6124e+02 - logprior: 1.9714
Epoch 8/10
10/10 - 1s - loss: 257.9350 - loglik: -2.6044e+02 - logprior: 2.5060
Epoch 9/10
10/10 - 1s - loss: 257.6798 - loglik: -2.6059e+02 - logprior: 2.9129
Epoch 10/10
10/10 - 1s - loss: 256.5645 - loglik: -2.5979e+02 - logprior: 3.2219
Fitted a model with MAP estimate = -256.4869
expansions: [(4, 2), (7, 1), (12, 5), (31, 1), (36, 6), (47, 2), (59, 5), (84, 4), (86, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 347.6468 - loglik: -2.6237e+02 - logprior: -8.5274e+01
Epoch 2/2
10/10 - 1s - loss: 280.2067 - loglik: -2.4811e+02 - logprior: -3.2092e+01
Fitted a model with MAP estimate = -268.1568
expansions: [(0, 3), (13, 1), (61, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 311.0243 - loglik: -2.4413e+02 - logprior: -6.6898e+01
Epoch 2/2
10/10 - 2s - loss: 252.9021 - loglik: -2.3943e+02 - logprior: -1.3474e+01
Fitted a model with MAP estimate = -243.4473
expansions: []
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 305.5280 - loglik: -2.4039e+02 - logprior: -6.5140e+01
Epoch 2/10
10/10 - 1s - loss: 251.1374 - loglik: -2.3835e+02 - logprior: -1.2791e+01
Epoch 3/10
10/10 - 1s - loss: 239.3319 - loglik: -2.3747e+02 - logprior: -1.8609e+00
Epoch 4/10
10/10 - 1s - loss: 234.1457 - loglik: -2.3694e+02 - logprior: 2.7935
Epoch 5/10
10/10 - 1s - loss: 230.9804 - loglik: -2.3633e+02 - logprior: 5.3460
Epoch 6/10
10/10 - 1s - loss: 229.4816 - loglik: -2.3636e+02 - logprior: 6.8795
Epoch 7/10
10/10 - 1s - loss: 228.1781 - loglik: -2.3605e+02 - logprior: 7.8744
Epoch 8/10
10/10 - 1s - loss: 227.4149 - loglik: -2.3602e+02 - logprior: 8.6072
Epoch 9/10
10/10 - 1s - loss: 226.6295 - loglik: -2.3577e+02 - logprior: 9.1382
Epoch 10/10
10/10 - 2s - loss: 226.0728 - loglik: -2.3563e+02 - logprior: 9.5536
Fitted a model with MAP estimate = -225.6180
Time for alignment: 49.1799
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 423.8943 - loglik: -3.4719e+02 - logprior: -7.6704e+01
Epoch 2/10
10/10 - 1s - loss: 339.9403 - loglik: -3.2232e+02 - logprior: -1.7616e+01
Epoch 3/10
10/10 - 1s - loss: 298.4823 - loglik: -2.9212e+02 - logprior: -6.3588e+00
Epoch 4/10
10/10 - 1s - loss: 277.5645 - loglik: -2.7534e+02 - logprior: -2.2253e+00
Epoch 5/10
10/10 - 1s - loss: 269.6001 - loglik: -2.6963e+02 - logprior: 0.0251
Epoch 6/10
10/10 - 1s - loss: 262.3117 - loglik: -2.6355e+02 - logprior: 1.2343
Epoch 7/10
10/10 - 1s - loss: 259.1273 - loglik: -2.6109e+02 - logprior: 1.9637
Epoch 8/10
10/10 - 1s - loss: 257.8268 - loglik: -2.6032e+02 - logprior: 2.4896
Epoch 9/10
10/10 - 1s - loss: 256.7260 - loglik: -2.5959e+02 - logprior: 2.8618
Epoch 10/10
10/10 - 1s - loss: 255.9700 - loglik: -2.5914e+02 - logprior: 3.1710
Fitted a model with MAP estimate = -255.8494
expansions: [(4, 2), (7, 1), (12, 5), (32, 1), (36, 7), (47, 2), (59, 1), (60, 4), (84, 4), (86, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 346.3112 - loglik: -2.6110e+02 - logprior: -8.5211e+01
Epoch 2/2
10/10 - 1s - loss: 279.0720 - loglik: -2.4703e+02 - logprior: -3.2042e+01
Fitted a model with MAP estimate = -267.3060
expansions: [(0, 3), (13, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 310.4450 - loglik: -2.4349e+02 - logprior: -6.6956e+01
Epoch 2/2
10/10 - 2s - loss: 252.6557 - loglik: -2.3907e+02 - logprior: -1.3589e+01
Fitted a model with MAP estimate = -243.4899
expansions: []
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 305.8666 - loglik: -2.4064e+02 - logprior: -6.5232e+01
Epoch 2/10
10/10 - 1s - loss: 251.7707 - loglik: -2.3891e+02 - logprior: -1.2865e+01
Epoch 3/10
10/10 - 1s - loss: 239.6969 - loglik: -2.3776e+02 - logprior: -1.9320e+00
Epoch 4/10
10/10 - 1s - loss: 234.8617 - loglik: -2.3759e+02 - logprior: 2.7284
Epoch 5/10
10/10 - 1s - loss: 231.8535 - loglik: -2.3716e+02 - logprior: 5.3065
Epoch 6/10
10/10 - 1s - loss: 230.2102 - loglik: -2.3704e+02 - logprior: 6.8310
Epoch 7/10
10/10 - 1s - loss: 228.7700 - loglik: -2.3660e+02 - logprior: 7.8331
Epoch 8/10
10/10 - 1s - loss: 227.5759 - loglik: -2.3613e+02 - logprior: 8.5513
Epoch 9/10
10/10 - 1s - loss: 227.5706 - loglik: -2.3664e+02 - logprior: 9.0708
Epoch 10/10
10/10 - 1s - loss: 226.3158 - loglik: -2.3580e+02 - logprior: 9.4793
Fitted a model with MAP estimate = -226.1601
Time for alignment: 48.9562
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 423.7255 - loglik: -3.4702e+02 - logprior: -7.6703e+01
Epoch 2/10
10/10 - 1s - loss: 339.9741 - loglik: -3.2236e+02 - logprior: -1.7615e+01
Epoch 3/10
10/10 - 1s - loss: 296.3907 - loglik: -2.9001e+02 - logprior: -6.3817e+00
Epoch 4/10
10/10 - 1s - loss: 272.7891 - loglik: -2.7056e+02 - logprior: -2.2308e+00
Epoch 5/10
10/10 - 1s - loss: 264.3699 - loglik: -2.6434e+02 - logprior: -2.9982e-02
Epoch 6/10
10/10 - 1s - loss: 260.9124 - loglik: -2.6219e+02 - logprior: 1.2809
Epoch 7/10
10/10 - 1s - loss: 258.9865 - loglik: -2.6105e+02 - logprior: 2.0634
Epoch 8/10
10/10 - 1s - loss: 257.9517 - loglik: -2.6053e+02 - logprior: 2.5746
Epoch 9/10
10/10 - 1s - loss: 257.3453 - loglik: -2.6030e+02 - logprior: 2.9597
Epoch 10/10
10/10 - 1s - loss: 256.5585 - loglik: -2.5983e+02 - logprior: 3.2756
Fitted a model with MAP estimate = -256.4743
expansions: [(4, 2), (7, 1), (12, 5), (33, 1), (36, 5), (44, 2), (47, 3), (59, 4), (84, 4), (86, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 347.2921 - loglik: -2.6209e+02 - logprior: -8.5203e+01
Epoch 2/2
10/10 - 1s - loss: 280.5668 - loglik: -2.4850e+02 - logprior: -3.2069e+01
Fitted a model with MAP estimate = -268.4777
expansions: [(0, 3), (13, 1)]
discards: [ 0 41 57 58]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 313.2457 - loglik: -2.4619e+02 - logprior: -6.7055e+01
Epoch 2/2
10/10 - 1s - loss: 253.9415 - loglik: -2.4021e+02 - logprior: -1.3727e+01
Fitted a model with MAP estimate = -244.7794
expansions: [(48, 4)]
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 306.9478 - loglik: -2.4172e+02 - logprior: -6.5231e+01
Epoch 2/10
10/10 - 2s - loss: 251.0195 - loglik: -2.3815e+02 - logprior: -1.2866e+01
Epoch 3/10
10/10 - 1s - loss: 239.5850 - loglik: -2.3763e+02 - logprior: -1.9529e+00
Epoch 4/10
10/10 - 2s - loss: 234.2982 - loglik: -2.3704e+02 - logprior: 2.7385
Epoch 5/10
10/10 - 1s - loss: 231.4858 - loglik: -2.3685e+02 - logprior: 5.3619
Epoch 6/10
10/10 - 1s - loss: 229.5664 - loglik: -2.3650e+02 - logprior: 6.9308
Epoch 7/10
10/10 - 1s - loss: 228.9013 - loglik: -2.3686e+02 - logprior: 7.9574
Epoch 8/10
10/10 - 2s - loss: 227.5255 - loglik: -2.3620e+02 - logprior: 8.6779
Epoch 9/10
10/10 - 2s - loss: 226.7910 - loglik: -2.3600e+02 - logprior: 9.2094
Epoch 10/10
10/10 - 1s - loss: 226.3462 - loglik: -2.3594e+02 - logprior: 9.5888
Fitted a model with MAP estimate = -225.7663
Time for alignment: 47.2291
Computed alignments with likelihoods: ['-227.0124', '-226.1363', '-225.6180', '-226.1601', '-225.7663']
Best model has likelihood: -225.6180  (prior= 9.7370 )
time for generating output: 0.1569
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.7410071942446043
Training of 5 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e5bc8dac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf44ca00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fdf27b640>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1f91c7a430>
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 99.3850 - loglik: -9.4989e+01 - logprior: -4.3965e+00
Epoch 2/10
17/17 - 1s - loss: 79.8683 - loglik: -7.8434e+01 - logprior: -1.4346e+00
Epoch 3/10
17/17 - 1s - loss: 70.1725 - loglik: -6.8610e+01 - logprior: -1.5629e+00
Epoch 4/10
17/17 - 1s - loss: 65.1638 - loglik: -6.3543e+01 - logprior: -1.6205e+00
Epoch 5/10
17/17 - 1s - loss: 62.6231 - loglik: -6.1083e+01 - logprior: -1.5399e+00
Epoch 6/10
17/17 - 1s - loss: 62.0214 - loglik: -6.0483e+01 - logprior: -1.5381e+00
Epoch 7/10
17/17 - 1s - loss: 61.7140 - loglik: -6.0183e+01 - logprior: -1.5306e+00
Epoch 8/10
17/17 - 1s - loss: 61.6278 - loglik: -6.0103e+01 - logprior: -1.5251e+00
Epoch 9/10
17/17 - 1s - loss: 61.5485 - loglik: -6.0033e+01 - logprior: -1.5154e+00
Epoch 10/10
17/17 - 1s - loss: 61.4863 - loglik: -5.9978e+01 - logprior: -1.5086e+00
Fitted a model with MAP estimate = -61.4580
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 2), (14, 1), (15, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.6506 - loglik: -6.5097e+01 - logprior: -5.5540e+00
Epoch 2/2
17/17 - 1s - loss: 62.0402 - loglik: -5.9440e+01 - logprior: -2.6000e+00
Fitted a model with MAP estimate = -59.3740
expansions: []
discards: [12 16 19]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.0913 - loglik: -5.7669e+01 - logprior: -4.4227e+00
Epoch 2/2
17/17 - 1s - loss: 58.3847 - loglik: -5.6768e+01 - logprior: -1.6171e+00
Fitted a model with MAP estimate = -57.8885
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 61.4325 - loglik: -5.7149e+01 - logprior: -4.2839e+00
Epoch 2/10
17/17 - 1s - loss: 58.2428 - loglik: -5.6653e+01 - logprior: -1.5899e+00
Epoch 3/10
17/17 - 1s - loss: 57.6067 - loglik: -5.6248e+01 - logprior: -1.3582e+00
Epoch 4/10
17/17 - 1s - loss: 57.6388 - loglik: -5.6343e+01 - logprior: -1.2957e+00
Fitted a model with MAP estimate = -57.4181
Time for alignment: 34.5243
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 99.4489 - loglik: -9.5050e+01 - logprior: -4.3984e+00
Epoch 2/10
17/17 - 1s - loss: 79.6401 - loglik: -7.8192e+01 - logprior: -1.4480e+00
Epoch 3/10
17/17 - 1s - loss: 70.0910 - loglik: -6.8518e+01 - logprior: -1.5726e+00
Epoch 4/10
17/17 - 1s - loss: 65.0685 - loglik: -6.3448e+01 - logprior: -1.6206e+00
Epoch 5/10
17/17 - 1s - loss: 62.5555 - loglik: -6.1013e+01 - logprior: -1.5427e+00
Epoch 6/10
17/17 - 1s - loss: 62.0811 - loglik: -6.0539e+01 - logprior: -1.5417e+00
Epoch 7/10
17/17 - 1s - loss: 61.7678 - loglik: -6.0237e+01 - logprior: -1.5311e+00
Epoch 8/10
17/17 - 1s - loss: 61.6033 - loglik: -6.0079e+01 - logprior: -1.5246e+00
Epoch 9/10
17/17 - 1s - loss: 61.5967 - loglik: -6.0082e+01 - logprior: -1.5151e+00
Epoch 10/10
17/17 - 1s - loss: 61.5128 - loglik: -6.0006e+01 - logprior: -1.5069e+00
Fitted a model with MAP estimate = -61.4598
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 2), (14, 1), (15, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.6462 - loglik: -6.5095e+01 - logprior: -5.5511e+00
Epoch 2/2
17/17 - 1s - loss: 62.0125 - loglik: -5.9409e+01 - logprior: -2.6035e+00
Fitted a model with MAP estimate = -59.4002
expansions: []
discards: [12 16 19]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.0928 - loglik: -5.7669e+01 - logprior: -4.4236e+00
Epoch 2/2
17/17 - 1s - loss: 58.3727 - loglik: -5.6757e+01 - logprior: -1.6159e+00
Fitted a model with MAP estimate = -57.8949
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.4268 - loglik: -5.7142e+01 - logprior: -4.2852e+00
Epoch 2/10
17/17 - 1s - loss: 58.3172 - loglik: -5.6723e+01 - logprior: -1.5943e+00
Epoch 3/10
17/17 - 1s - loss: 57.5787 - loglik: -5.6224e+01 - logprior: -1.3546e+00
Epoch 4/10
17/17 - 1s - loss: 57.5433 - loglik: -5.6235e+01 - logprior: -1.3082e+00
Epoch 5/10
17/17 - 1s - loss: 57.3681 - loglik: -5.6101e+01 - logprior: -1.2668e+00
Epoch 6/10
17/17 - 1s - loss: 57.4764 - loglik: -5.6233e+01 - logprior: -1.2429e+00
Fitted a model with MAP estimate = -57.2630
Time for alignment: 32.7749
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 99.3837 - loglik: -9.4986e+01 - logprior: -4.3977e+00
Epoch 2/10
17/17 - 1s - loss: 79.6572 - loglik: -7.8216e+01 - logprior: -1.4415e+00
Epoch 3/10
17/17 - 1s - loss: 70.1794 - loglik: -6.8619e+01 - logprior: -1.5608e+00
Epoch 4/10
17/17 - 1s - loss: 64.8379 - loglik: -6.3208e+01 - logprior: -1.6301e+00
Epoch 5/10
17/17 - 1s - loss: 62.3805 - loglik: -6.0838e+01 - logprior: -1.5424e+00
Epoch 6/10
17/17 - 1s - loss: 62.0556 - loglik: -6.0518e+01 - logprior: -1.5373e+00
Epoch 7/10
17/17 - 1s - loss: 61.7913 - loglik: -6.0259e+01 - logprior: -1.5321e+00
Epoch 8/10
17/17 - 1s - loss: 61.5606 - loglik: -6.0037e+01 - logprior: -1.5235e+00
Epoch 9/10
17/17 - 1s - loss: 61.5401 - loglik: -6.0027e+01 - logprior: -1.5132e+00
Epoch 10/10
17/17 - 1s - loss: 61.4937 - loglik: -5.9984e+01 - logprior: -1.5095e+00
Fitted a model with MAP estimate = -61.4611
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 2), (14, 1), (15, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.7843 - loglik: -6.5238e+01 - logprior: -5.5462e+00
Epoch 2/2
17/17 - 1s - loss: 61.7519 - loglik: -5.9153e+01 - logprior: -2.5986e+00
Fitted a model with MAP estimate = -59.3350
expansions: []
discards: [12 16 19]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 62.0478 - loglik: -5.7628e+01 - logprior: -4.4201e+00
Epoch 2/2
17/17 - 1s - loss: 58.4193 - loglik: -5.6810e+01 - logprior: -1.6091e+00
Fitted a model with MAP estimate = -57.8864
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.4216 - loglik: -5.7143e+01 - logprior: -4.2789e+00
Epoch 2/10
17/17 - 1s - loss: 58.2161 - loglik: -5.6627e+01 - logprior: -1.5893e+00
Epoch 3/10
17/17 - 1s - loss: 57.6494 - loglik: -5.6282e+01 - logprior: -1.3672e+00
Epoch 4/10
17/17 - 1s - loss: 57.5599 - loglik: -5.6269e+01 - logprior: -1.2909e+00
Epoch 5/10
17/17 - 1s - loss: 57.4746 - loglik: -5.6210e+01 - logprior: -1.2648e+00
Epoch 6/10
17/17 - 1s - loss: 57.4207 - loglik: -5.6188e+01 - logprior: -1.2330e+00
Epoch 7/10
17/17 - 1s - loss: 57.1971 - loglik: -5.5967e+01 - logprior: -1.2298e+00
Epoch 8/10
17/17 - 1s - loss: 57.2492 - loglik: -5.6036e+01 - logprior: -1.2132e+00
Fitted a model with MAP estimate = -57.1705
Time for alignment: 33.5031
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 99.3726 - loglik: -9.4976e+01 - logprior: -4.3970e+00
Epoch 2/10
17/17 - 1s - loss: 79.7892 - loglik: -7.8349e+01 - logprior: -1.4398e+00
Epoch 3/10
17/17 - 1s - loss: 69.9951 - loglik: -6.8434e+01 - logprior: -1.5609e+00
Epoch 4/10
17/17 - 1s - loss: 65.1920 - loglik: -6.3571e+01 - logprior: -1.6211e+00
Epoch 5/10
17/17 - 1s - loss: 62.4166 - loglik: -6.0875e+01 - logprior: -1.5412e+00
Epoch 6/10
17/17 - 1s - loss: 62.1138 - loglik: -6.0576e+01 - logprior: -1.5383e+00
Epoch 7/10
17/17 - 1s - loss: 61.6838 - loglik: -6.0154e+01 - logprior: -1.5298e+00
Epoch 8/10
17/17 - 1s - loss: 61.6452 - loglik: -6.0121e+01 - logprior: -1.5242e+00
Epoch 9/10
17/17 - 1s - loss: 61.5186 - loglik: -6.0001e+01 - logprior: -1.5172e+00
Epoch 10/10
17/17 - 1s - loss: 61.4161 - loglik: -5.9909e+01 - logprior: -1.5073e+00
Fitted a model with MAP estimate = -61.4537
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 2), (14, 1), (15, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.7155 - loglik: -6.5151e+01 - logprior: -5.5642e+00
Epoch 2/2
17/17 - 1s - loss: 61.9701 - loglik: -5.9378e+01 - logprior: -2.5919e+00
Fitted a model with MAP estimate = -59.3652
expansions: []
discards: [12 16 19]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 62.1513 - loglik: -5.7733e+01 - logprior: -4.4179e+00
Epoch 2/2
17/17 - 1s - loss: 58.3808 - loglik: -5.6759e+01 - logprior: -1.6221e+00
Fitted a model with MAP estimate = -57.8967
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.4862 - loglik: -5.7199e+01 - logprior: -4.2872e+00
Epoch 2/10
17/17 - 1s - loss: 58.2101 - loglik: -5.6616e+01 - logprior: -1.5946e+00
Epoch 3/10
17/17 - 1s - loss: 57.6037 - loglik: -5.6236e+01 - logprior: -1.3675e+00
Epoch 4/10
17/17 - 1s - loss: 57.4673 - loglik: -5.6168e+01 - logprior: -1.2990e+00
Epoch 5/10
17/17 - 1s - loss: 57.5848 - loglik: -5.6307e+01 - logprior: -1.2782e+00
Fitted a model with MAP estimate = -57.3255
Time for alignment: 30.7184
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 99.3578 - loglik: -9.4961e+01 - logprior: -4.3969e+00
Epoch 2/10
17/17 - 1s - loss: 79.8279 - loglik: -7.8385e+01 - logprior: -1.4426e+00
Epoch 3/10
17/17 - 1s - loss: 70.0300 - loglik: -6.8466e+01 - logprior: -1.5642e+00
Epoch 4/10
17/17 - 1s - loss: 65.1064 - loglik: -6.3485e+01 - logprior: -1.6216e+00
Epoch 5/10
17/17 - 1s - loss: 62.6106 - loglik: -6.1070e+01 - logprior: -1.5401e+00
Epoch 6/10
17/17 - 1s - loss: 61.9944 - loglik: -6.0455e+01 - logprior: -1.5389e+00
Epoch 7/10
17/17 - 1s - loss: 61.7183 - loglik: -6.0187e+01 - logprior: -1.5313e+00
Epoch 8/10
17/17 - 1s - loss: 61.5995 - loglik: -6.0076e+01 - logprior: -1.5240e+00
Epoch 9/10
17/17 - 1s - loss: 61.5618 - loglik: -6.0045e+01 - logprior: -1.5169e+00
Epoch 10/10
17/17 - 1s - loss: 61.4084 - loglik: -5.9902e+01 - logprior: -1.5066e+00
Fitted a model with MAP estimate = -61.4629
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (14, 1), (15, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 70.2696 - loglik: -6.4726e+01 - logprior: -5.5436e+00
Epoch 2/2
17/17 - 1s - loss: 61.7493 - loglik: -5.9196e+01 - logprior: -2.5529e+00
Fitted a model with MAP estimate = -59.2255
expansions: []
discards: [12 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 62.0566 - loglik: -5.7638e+01 - logprior: -4.4187e+00
Epoch 2/2
17/17 - 1s - loss: 58.2318 - loglik: -5.6616e+01 - logprior: -1.6160e+00
Fitted a model with MAP estimate = -57.8923
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.4464 - loglik: -5.7160e+01 - logprior: -4.2861e+00
Epoch 2/10
17/17 - 1s - loss: 58.2840 - loglik: -5.6695e+01 - logprior: -1.5889e+00
Epoch 3/10
17/17 - 1s - loss: 57.7217 - loglik: -5.6358e+01 - logprior: -1.3636e+00
Epoch 4/10
17/17 - 1s - loss: 57.5825 - loglik: -5.6283e+01 - logprior: -1.2999e+00
Epoch 5/10
17/17 - 1s - loss: 57.3782 - loglik: -5.6109e+01 - logprior: -1.2689e+00
Epoch 6/10
17/17 - 1s - loss: 57.1616 - loglik: -5.5915e+01 - logprior: -1.2463e+00
Epoch 7/10
17/17 - 1s - loss: 57.3610 - loglik: -5.6134e+01 - logprior: -1.2267e+00
Fitted a model with MAP estimate = -57.2026
Time for alignment: 30.3242
Computed alignments with likelihoods: ['-57.4181', '-57.2630', '-57.1705', '-57.3255', '-57.2026']
Best model has likelihood: -57.1705  (prior= -1.1665 )
time for generating output: 0.1091
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.7746043998456195
Training of 5 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e86a441f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f914a9c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff0b12520>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1e86f850d0>
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 196.0681 - loglik: -1.8799e+02 - logprior: -8.0765e+00
Epoch 2/10
13/13 - 1s - loss: 167.6828 - loglik: -1.6545e+02 - logprior: -2.2329e+00
Epoch 3/10
13/13 - 1s - loss: 149.1013 - loglik: -1.4708e+02 - logprior: -2.0172e+00
Epoch 4/10
13/13 - 1s - loss: 140.7448 - loglik: -1.3854e+02 - logprior: -2.2029e+00
Epoch 5/10
13/13 - 1s - loss: 137.3395 - loglik: -1.3523e+02 - logprior: -2.1101e+00
Epoch 6/10
13/13 - 1s - loss: 135.3591 - loglik: -1.3332e+02 - logprior: -2.0353e+00
Epoch 7/10
13/13 - 1s - loss: 133.9239 - loglik: -1.3185e+02 - logprior: -2.0757e+00
Epoch 8/10
13/13 - 1s - loss: 133.1652 - loglik: -1.3107e+02 - logprior: -2.0956e+00
Epoch 9/10
13/13 - 1s - loss: 132.8608 - loglik: -1.3077e+02 - logprior: -2.0889e+00
Epoch 10/10
13/13 - 1s - loss: 132.6707 - loglik: -1.3058e+02 - logprior: -2.0882e+00
Fitted a model with MAP estimate = -132.5109
expansions: [(12, 1), (13, 1), (16, 2), (17, 2), (20, 1), (21, 2), (22, 2), (25, 1), (28, 1), (35, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 145.5103 - loglik: -1.3584e+02 - logprior: -9.6679e+00
Epoch 2/2
13/13 - 1s - loss: 130.0384 - loglik: -1.2556e+02 - logprior: -4.4797e+00
Fitted a model with MAP estimate = -127.5885
expansions: [(0, 2)]
discards: [ 0 18 27 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 131.4297 - loglik: -1.2410e+02 - logprior: -7.3254e+00
Epoch 2/2
13/13 - 1s - loss: 123.4433 - loglik: -1.2124e+02 - logprior: -2.2018e+00
Fitted a model with MAP estimate = -122.3946
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 133.6674 - loglik: -1.2431e+02 - logprior: -9.3616e+00
Epoch 2/10
13/13 - 1s - loss: 125.4967 - loglik: -1.2210e+02 - logprior: -3.3994e+00
Epoch 3/10
13/13 - 1s - loss: 122.4438 - loglik: -1.2075e+02 - logprior: -1.6947e+00
Epoch 4/10
13/13 - 1s - loss: 121.7249 - loglik: -1.2044e+02 - logprior: -1.2828e+00
Epoch 5/10
13/13 - 1s - loss: 120.5493 - loglik: -1.1936e+02 - logprior: -1.1907e+00
Epoch 6/10
13/13 - 1s - loss: 120.0331 - loglik: -1.1882e+02 - logprior: -1.2121e+00
Epoch 7/10
13/13 - 1s - loss: 120.0534 - loglik: -1.1885e+02 - logprior: -1.2074e+00
Fitted a model with MAP estimate = -119.6788
Time for alignment: 39.5898
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 195.9571 - loglik: -1.8788e+02 - logprior: -8.0764e+00
Epoch 2/10
13/13 - 1s - loss: 167.9528 - loglik: -1.6572e+02 - logprior: -2.2351e+00
Epoch 3/10
13/13 - 1s - loss: 149.5602 - loglik: -1.4753e+02 - logprior: -2.0299e+00
Epoch 4/10
13/13 - 1s - loss: 141.5451 - loglik: -1.3935e+02 - logprior: -2.1909e+00
Epoch 5/10
13/13 - 1s - loss: 137.7035 - loglik: -1.3561e+02 - logprior: -2.0929e+00
Epoch 6/10
13/13 - 1s - loss: 136.4667 - loglik: -1.3443e+02 - logprior: -2.0326e+00
Epoch 7/10
13/13 - 1s - loss: 134.9929 - loglik: -1.3292e+02 - logprior: -2.0687e+00
Epoch 8/10
13/13 - 1s - loss: 134.9704 - loglik: -1.3290e+02 - logprior: -2.0745e+00
Epoch 9/10
13/13 - 1s - loss: 134.0724 - loglik: -1.3202e+02 - logprior: -2.0490e+00
Epoch 10/10
13/13 - 1s - loss: 134.1367 - loglik: -1.3209e+02 - logprior: -2.0491e+00
Fitted a model with MAP estimate = -133.7911
expansions: [(10, 1), (13, 1), (16, 2), (17, 2), (20, 1), (21, 2), (23, 2), (24, 1), (28, 1), (31, 1), (40, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 146.3659 - loglik: -1.3670e+02 - logprior: -9.6696e+00
Epoch 2/2
13/13 - 1s - loss: 130.9693 - loglik: -1.2651e+02 - logprior: -4.4566e+00
Fitted a model with MAP estimate = -128.4713
expansions: [(0, 2)]
discards: [ 0 13 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 132.7444 - loglik: -1.2537e+02 - logprior: -7.3733e+00
Epoch 2/2
13/13 - 1s - loss: 125.1368 - loglik: -1.2285e+02 - logprior: -2.2863e+00
Fitted a model with MAP estimate = -123.6770
expansions: []
discards: [ 0 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 134.5619 - loglik: -1.2517e+02 - logprior: -9.3895e+00
Epoch 2/10
13/13 - 1s - loss: 126.2453 - loglik: -1.2294e+02 - logprior: -3.3044e+00
Epoch 3/10
13/13 - 1s - loss: 122.8739 - loglik: -1.2118e+02 - logprior: -1.6893e+00
Epoch 4/10
13/13 - 1s - loss: 121.0876 - loglik: -1.1979e+02 - logprior: -1.2980e+00
Epoch 5/10
13/13 - 1s - loss: 120.9860 - loglik: -1.1978e+02 - logprior: -1.2079e+00
Epoch 6/10
13/13 - 1s - loss: 120.1579 - loglik: -1.1895e+02 - logprior: -1.2058e+00
Epoch 7/10
13/13 - 1s - loss: 120.0084 - loglik: -1.1879e+02 - logprior: -1.2144e+00
Epoch 8/10
13/13 - 1s - loss: 119.5575 - loglik: -1.1837e+02 - logprior: -1.1844e+00
Epoch 9/10
13/13 - 1s - loss: 119.4156 - loglik: -1.1825e+02 - logprior: -1.1657e+00
Epoch 10/10
13/13 - 1s - loss: 119.3916 - loglik: -1.1825e+02 - logprior: -1.1369e+00
Fitted a model with MAP estimate = -119.2062
Time for alignment: 38.3878
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 196.0188 - loglik: -1.8794e+02 - logprior: -8.0768e+00
Epoch 2/10
13/13 - 1s - loss: 167.6387 - loglik: -1.6540e+02 - logprior: -2.2358e+00
Epoch 3/10
13/13 - 1s - loss: 149.4139 - loglik: -1.4738e+02 - logprior: -2.0385e+00
Epoch 4/10
13/13 - 1s - loss: 141.3161 - loglik: -1.3914e+02 - logprior: -2.1735e+00
Epoch 5/10
13/13 - 1s - loss: 137.6030 - loglik: -1.3553e+02 - logprior: -2.0700e+00
Epoch 6/10
13/13 - 1s - loss: 135.6814 - loglik: -1.3367e+02 - logprior: -2.0104e+00
Epoch 7/10
13/13 - 1s - loss: 135.1614 - loglik: -1.3311e+02 - logprior: -2.0471e+00
Epoch 8/10
13/13 - 1s - loss: 134.0922 - loglik: -1.3204e+02 - logprior: -2.0505e+00
Epoch 9/10
13/13 - 1s - loss: 133.5659 - loglik: -1.3150e+02 - logprior: -2.0620e+00
Epoch 10/10
13/13 - 1s - loss: 133.2668 - loglik: -1.3120e+02 - logprior: -2.0669e+00
Fitted a model with MAP estimate = -133.1299
expansions: [(12, 1), (13, 1), (16, 2), (17, 2), (21, 1), (23, 1), (24, 1), (25, 1), (28, 1), (31, 1), (40, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 145.3832 - loglik: -1.3573e+02 - logprior: -9.6567e+00
Epoch 2/2
13/13 - 1s - loss: 130.8484 - loglik: -1.2648e+02 - logprior: -4.3705e+00
Fitted a model with MAP estimate = -128.2139
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 131.9160 - loglik: -1.2459e+02 - logprior: -7.3257e+00
Epoch 2/2
13/13 - 1s - loss: 124.4191 - loglik: -1.2219e+02 - logprior: -2.2308e+00
Fitted a model with MAP estimate = -123.4915
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 134.7432 - loglik: -1.2536e+02 - logprior: -9.3854e+00
Epoch 2/10
13/13 - 1s - loss: 126.0306 - loglik: -1.2271e+02 - logprior: -3.3178e+00
Epoch 3/10
13/13 - 1s - loss: 123.9811 - loglik: -1.2227e+02 - logprior: -1.7073e+00
Epoch 4/10
13/13 - 1s - loss: 122.4016 - loglik: -1.2108e+02 - logprior: -1.3238e+00
Epoch 5/10
13/13 - 1s - loss: 121.4818 - loglik: -1.2024e+02 - logprior: -1.2391e+00
Epoch 6/10
13/13 - 1s - loss: 120.8997 - loglik: -1.1964e+02 - logprior: -1.2579e+00
Epoch 7/10
13/13 - 1s - loss: 120.7570 - loglik: -1.1951e+02 - logprior: -1.2463e+00
Epoch 8/10
13/13 - 1s - loss: 120.7928 - loglik: -1.1958e+02 - logprior: -1.2169e+00
Fitted a model with MAP estimate = -120.3824
Time for alignment: 36.3227
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 196.1005 - loglik: -1.8802e+02 - logprior: -8.0772e+00
Epoch 2/10
13/13 - 1s - loss: 166.9369 - loglik: -1.6470e+02 - logprior: -2.2327e+00
Epoch 3/10
13/13 - 1s - loss: 147.8492 - loglik: -1.4583e+02 - logprior: -2.0220e+00
Epoch 4/10
13/13 - 1s - loss: 140.6610 - loglik: -1.3854e+02 - logprior: -2.1244e+00
Epoch 5/10
13/13 - 1s - loss: 137.3437 - loglik: -1.3534e+02 - logprior: -2.0021e+00
Epoch 6/10
13/13 - 1s - loss: 134.7935 - loglik: -1.3281e+02 - logprior: -1.9852e+00
Epoch 7/10
13/13 - 1s - loss: 133.8692 - loglik: -1.3184e+02 - logprior: -2.0269e+00
Epoch 8/10
13/13 - 1s - loss: 133.5459 - loglik: -1.3151e+02 - logprior: -2.0358e+00
Epoch 9/10
13/13 - 1s - loss: 132.6871 - loglik: -1.3066e+02 - logprior: -2.0224e+00
Epoch 10/10
13/13 - 1s - loss: 132.6204 - loglik: -1.3060e+02 - logprior: -2.0182e+00
Fitted a model with MAP estimate = -132.4225
expansions: [(12, 1), (13, 1), (16, 2), (17, 3), (20, 2), (21, 1), (22, 2), (23, 1), (25, 1), (27, 1), (40, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 73 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 146.2155 - loglik: -1.3654e+02 - logprior: -9.6803e+00
Epoch 2/2
13/13 - 1s - loss: 130.1856 - loglik: -1.2567e+02 - logprior: -4.5123e+00
Fitted a model with MAP estimate = -127.8603
expansions: [(0, 2)]
discards: [ 0 18 27 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 132.2527 - loglik: -1.2491e+02 - logprior: -7.3387e+00
Epoch 2/2
13/13 - 1s - loss: 124.2728 - loglik: -1.2202e+02 - logprior: -2.2553e+00
Fitted a model with MAP estimate = -123.4420
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 134.6736 - loglik: -1.2527e+02 - logprior: -9.4023e+00
Epoch 2/10
13/13 - 1s - loss: 126.3183 - loglik: -1.2288e+02 - logprior: -3.4371e+00
Epoch 3/10
13/13 - 1s - loss: 123.6312 - loglik: -1.2189e+02 - logprior: -1.7427e+00
Epoch 4/10
13/13 - 1s - loss: 122.2063 - loglik: -1.2087e+02 - logprior: -1.3322e+00
Epoch 5/10
13/13 - 1s - loss: 121.9281 - loglik: -1.2067e+02 - logprior: -1.2580e+00
Epoch 6/10
13/13 - 1s - loss: 121.2560 - loglik: -1.1998e+02 - logprior: -1.2749e+00
Epoch 7/10
13/13 - 1s - loss: 120.5725 - loglik: -1.1932e+02 - logprior: -1.2546e+00
Epoch 8/10
13/13 - 1s - loss: 120.4920 - loglik: -1.1927e+02 - logprior: -1.2198e+00
Epoch 9/10
13/13 - 1s - loss: 120.5338 - loglik: -1.1932e+02 - logprior: -1.2088e+00
Fitted a model with MAP estimate = -120.2221
Time for alignment: 36.6178
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 195.8351 - loglik: -1.8776e+02 - logprior: -8.0776e+00
Epoch 2/10
13/13 - 1s - loss: 167.5018 - loglik: -1.6528e+02 - logprior: -2.2180e+00
Epoch 3/10
13/13 - 1s - loss: 149.4667 - loglik: -1.4755e+02 - logprior: -1.9164e+00
Epoch 4/10
13/13 - 1s - loss: 141.3452 - loglik: -1.3931e+02 - logprior: -2.0304e+00
Epoch 5/10
13/13 - 1s - loss: 138.7969 - loglik: -1.3685e+02 - logprior: -1.9451e+00
Epoch 6/10
13/13 - 1s - loss: 136.9021 - loglik: -1.3500e+02 - logprior: -1.9021e+00
Epoch 7/10
13/13 - 1s - loss: 135.2928 - loglik: -1.3333e+02 - logprior: -1.9671e+00
Epoch 8/10
13/13 - 1s - loss: 134.6010 - loglik: -1.3260e+02 - logprior: -1.9966e+00
Epoch 9/10
13/13 - 1s - loss: 133.8288 - loglik: -1.3184e+02 - logprior: -1.9855e+00
Epoch 10/10
13/13 - 1s - loss: 134.0404 - loglik: -1.3205e+02 - logprior: -1.9858e+00
Fitted a model with MAP estimate = -133.6459
expansions: [(12, 1), (13, 1), (16, 2), (17, 2), (20, 1), (21, 2), (22, 2), (25, 3), (27, 1), (40, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 73 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 146.6068 - loglik: -1.3692e+02 - logprior: -9.6857e+00
Epoch 2/2
13/13 - 1s - loss: 130.7996 - loglik: -1.2628e+02 - logprior: -4.5206e+00
Fitted a model with MAP estimate = -128.1384
expansions: [(0, 2)]
discards: [ 0 30 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 131.9010 - loglik: -1.2449e+02 - logprior: -7.4086e+00
Epoch 2/2
13/13 - 1s - loss: 124.7695 - loglik: -1.2249e+02 - logprior: -2.2835e+00
Fitted a model with MAP estimate = -123.4108
expansions: []
discards: [ 0 29]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 134.6197 - loglik: -1.2522e+02 - logprior: -9.4035e+00
Epoch 2/10
13/13 - 1s - loss: 126.4359 - loglik: -1.2303e+02 - logprior: -3.4081e+00
Epoch 3/10
13/13 - 1s - loss: 123.7240 - loglik: -1.2199e+02 - logprior: -1.7307e+00
Epoch 4/10
13/13 - 1s - loss: 122.1969 - loglik: -1.2086e+02 - logprior: -1.3397e+00
Epoch 5/10
13/13 - 1s - loss: 121.8475 - loglik: -1.2059e+02 - logprior: -1.2567e+00
Epoch 6/10
13/13 - 1s - loss: 120.7601 - loglik: -1.1949e+02 - logprior: -1.2725e+00
Epoch 7/10
13/13 - 1s - loss: 121.3593 - loglik: -1.2011e+02 - logprior: -1.2512e+00
Fitted a model with MAP estimate = -120.6847
Time for alignment: 34.6711
Computed alignments with likelihoods: ['-119.6788', '-119.2062', '-120.3824', '-120.2221', '-120.6847']
Best model has likelihood: -119.2062  (prior= -1.1277 )
time for generating output: 0.1215
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.9730878186968839
Training of 5 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1efe1e7fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fb4d18be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ff09439a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1e53643d30>
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 243.3174 - loglik: -2.3080e+02 - logprior: -1.2517e+01
Epoch 2/10
11/11 - 1s - loss: 212.9872 - loglik: -2.0974e+02 - logprior: -3.2422e+00
Epoch 3/10
11/11 - 1s - loss: 190.3994 - loglik: -1.8826e+02 - logprior: -2.1384e+00
Epoch 4/10
11/11 - 1s - loss: 176.2207 - loglik: -1.7435e+02 - logprior: -1.8717e+00
Epoch 5/10
11/11 - 1s - loss: 169.2318 - loglik: -1.6756e+02 - logprior: -1.6750e+00
Epoch 6/10
11/11 - 1s - loss: 164.5307 - loglik: -1.6281e+02 - logprior: -1.7222e+00
Epoch 7/10
11/11 - 1s - loss: 162.2354 - loglik: -1.6056e+02 - logprior: -1.6711e+00
Epoch 8/10
11/11 - 1s - loss: 161.2919 - loglik: -1.5966e+02 - logprior: -1.6292e+00
Epoch 9/10
11/11 - 1s - loss: 160.6613 - loglik: -1.5905e+02 - logprior: -1.6156e+00
Epoch 10/10
11/11 - 1s - loss: 160.5297 - loglik: -1.5893e+02 - logprior: -1.5992e+00
Fitted a model with MAP estimate = -160.2505
expansions: [(0, 3), (1, 1), (5, 1), (6, 1), (19, 1), (21, 1), (33, 2), (34, 1), (42, 1), (48, 1), (50, 1), (51, 1), (61, 1), (62, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 173.4182 - loglik: -1.5779e+02 - logprior: -1.5626e+01
Epoch 2/2
11/11 - 1s - loss: 152.5804 - loglik: -1.4818e+02 - logprior: -4.4037e+00
Fitted a model with MAP estimate = -148.8768
expansions: []
discards: [ 0 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 162.8026 - loglik: -1.4869e+02 - logprior: -1.4115e+01
Epoch 2/2
11/11 - 1s - loss: 152.2261 - loglik: -1.4676e+02 - logprior: -5.4622e+00
Fitted a model with MAP estimate = -149.4704
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 158.7760 - loglik: -1.4675e+02 - logprior: -1.2026e+01
Epoch 2/10
11/11 - 1s - loss: 149.0394 - loglik: -1.4591e+02 - logprior: -3.1305e+00
Epoch 3/10
11/11 - 1s - loss: 146.3498 - loglik: -1.4471e+02 - logprior: -1.6352e+00
Epoch 4/10
11/11 - 1s - loss: 145.6453 - loglik: -1.4467e+02 - logprior: -9.7413e-01
Epoch 5/10
11/11 - 1s - loss: 145.1800 - loglik: -1.4441e+02 - logprior: -7.7411e-01
Epoch 6/10
11/11 - 1s - loss: 144.4870 - loglik: -1.4387e+02 - logprior: -6.1551e-01
Epoch 7/10
11/11 - 1s - loss: 144.2386 - loglik: -1.4368e+02 - logprior: -5.5468e-01
Epoch 8/10
11/11 - 1s - loss: 143.4850 - loglik: -1.4297e+02 - logprior: -5.1621e-01
Epoch 9/10
11/11 - 1s - loss: 143.7996 - loglik: -1.4330e+02 - logprior: -4.9805e-01
Fitted a model with MAP estimate = -143.4843
Time for alignment: 33.8037
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 243.3214 - loglik: -2.3081e+02 - logprior: -1.2516e+01
Epoch 2/10
11/11 - 1s - loss: 213.0933 - loglik: -2.0986e+02 - logprior: -3.2356e+00
Epoch 3/10
11/11 - 1s - loss: 188.5031 - loglik: -1.8639e+02 - logprior: -2.1089e+00
Epoch 4/10
11/11 - 1s - loss: 174.2156 - loglik: -1.7247e+02 - logprior: -1.7434e+00
Epoch 5/10
11/11 - 1s - loss: 168.2437 - loglik: -1.6674e+02 - logprior: -1.5081e+00
Epoch 6/10
11/11 - 1s - loss: 164.1303 - loglik: -1.6264e+02 - logprior: -1.4871e+00
Epoch 7/10
11/11 - 1s - loss: 162.6644 - loglik: -1.6131e+02 - logprior: -1.3512e+00
Epoch 8/10
11/11 - 1s - loss: 161.9385 - loglik: -1.6069e+02 - logprior: -1.2497e+00
Epoch 9/10
11/11 - 1s - loss: 161.1427 - loglik: -1.5991e+02 - logprior: -1.2282e+00
Epoch 10/10
11/11 - 1s - loss: 161.1828 - loglik: -1.5996e+02 - logprior: -1.2269e+00
Fitted a model with MAP estimate = -160.7347
expansions: [(0, 6), (19, 1), (21, 1), (32, 1), (33, 2), (45, 1), (48, 1), (50, 1), (58, 1), (61, 1), (62, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 173.4955 - loglik: -1.5791e+02 - logprior: -1.5586e+01
Epoch 2/2
11/11 - 1s - loss: 152.3912 - loglik: -1.4798e+02 - logprior: -4.4138e+00
Fitted a model with MAP estimate = -149.0263
expansions: []
discards: [ 0 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 163.4174 - loglik: -1.4930e+02 - logprior: -1.4116e+01
Epoch 2/2
11/11 - 1s - loss: 152.7843 - loglik: -1.4730e+02 - logprior: -5.4837e+00
Fitted a model with MAP estimate = -149.7859
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.8952 - loglik: -1.4669e+02 - logprior: -1.2203e+01
Epoch 2/10
11/11 - 1s - loss: 149.0049 - loglik: -1.4584e+02 - logprior: -3.1613e+00
Epoch 3/10
11/11 - 1s - loss: 146.6033 - loglik: -1.4493e+02 - logprior: -1.6738e+00
Epoch 4/10
11/11 - 1s - loss: 145.7999 - loglik: -1.4483e+02 - logprior: -9.7111e-01
Epoch 5/10
11/11 - 1s - loss: 144.8497 - loglik: -1.4406e+02 - logprior: -7.8714e-01
Epoch 6/10
11/11 - 1s - loss: 144.5592 - loglik: -1.4394e+02 - logprior: -6.2371e-01
Epoch 7/10
11/11 - 1s - loss: 144.0839 - loglik: -1.4352e+02 - logprior: -5.6019e-01
Epoch 8/10
11/11 - 1s - loss: 143.5864 - loglik: -1.4306e+02 - logprior: -5.2428e-01
Epoch 9/10
11/11 - 1s - loss: 143.8084 - loglik: -1.4332e+02 - logprior: -4.9083e-01
Fitted a model with MAP estimate = -143.4730
Time for alignment: 38.3452
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 243.3338 - loglik: -2.3082e+02 - logprior: -1.2518e+01
Epoch 2/10
11/11 - 1s - loss: 212.7554 - loglik: -2.0951e+02 - logprior: -3.2420e+00
Epoch 3/10
11/11 - 1s - loss: 189.7969 - loglik: -1.8769e+02 - logprior: -2.1056e+00
Epoch 4/10
11/11 - 1s - loss: 172.9293 - loglik: -1.7120e+02 - logprior: -1.7248e+00
Epoch 5/10
11/11 - 1s - loss: 166.2854 - loglik: -1.6480e+02 - logprior: -1.4833e+00
Epoch 6/10
11/11 - 1s - loss: 163.0127 - loglik: -1.6158e+02 - logprior: -1.4351e+00
Epoch 7/10
11/11 - 1s - loss: 161.9599 - loglik: -1.6065e+02 - logprior: -1.3065e+00
Epoch 8/10
11/11 - 1s - loss: 161.2173 - loglik: -1.5994e+02 - logprior: -1.2738e+00
Epoch 9/10
11/11 - 1s - loss: 160.4648 - loglik: -1.5920e+02 - logprior: -1.2671e+00
Epoch 10/10
11/11 - 1s - loss: 160.3541 - loglik: -1.5909e+02 - logprior: -1.2620e+00
Fitted a model with MAP estimate = -160.1764
expansions: [(0, 6), (19, 1), (27, 1), (32, 1), (33, 2), (46, 1), (48, 1), (50, 1), (51, 1), (61, 1), (62, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 173.0514 - loglik: -1.5747e+02 - logprior: -1.5581e+01
Epoch 2/2
11/11 - 1s - loss: 152.7085 - loglik: -1.4829e+02 - logprior: -4.4215e+00
Fitted a model with MAP estimate = -149.0633
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 163.3352 - loglik: -1.4919e+02 - logprior: -1.4146e+01
Epoch 2/2
11/11 - 1s - loss: 152.4565 - loglik: -1.4695e+02 - logprior: -5.5115e+00
Fitted a model with MAP estimate = -149.8333
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.9916 - loglik: -1.4677e+02 - logprior: -1.2226e+01
Epoch 2/10
11/11 - 1s - loss: 148.8453 - loglik: -1.4565e+02 - logprior: -3.1945e+00
Epoch 3/10
11/11 - 1s - loss: 146.9295 - loglik: -1.4522e+02 - logprior: -1.7062e+00
Epoch 4/10
11/11 - 1s - loss: 145.4471 - loglik: -1.4444e+02 - logprior: -1.0066e+00
Epoch 5/10
11/11 - 1s - loss: 145.6281 - loglik: -1.4482e+02 - logprior: -8.0787e-01
Fitted a model with MAP estimate = -144.7613
Time for alignment: 33.8128
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 243.2329 - loglik: -2.3072e+02 - logprior: -1.2517e+01
Epoch 2/10
11/11 - 1s - loss: 213.4612 - loglik: -2.1022e+02 - logprior: -3.2399e+00
Epoch 3/10
11/11 - 1s - loss: 190.5137 - loglik: -1.8842e+02 - logprior: -2.0958e+00
Epoch 4/10
11/11 - 1s - loss: 175.4227 - loglik: -1.7372e+02 - logprior: -1.6993e+00
Epoch 5/10
11/11 - 1s - loss: 167.3093 - loglik: -1.6588e+02 - logprior: -1.4336e+00
Epoch 6/10
11/11 - 1s - loss: 164.1025 - loglik: -1.6271e+02 - logprior: -1.3966e+00
Epoch 7/10
11/11 - 1s - loss: 162.8651 - loglik: -1.6157e+02 - logprior: -1.2936e+00
Epoch 8/10
11/11 - 1s - loss: 161.8067 - loglik: -1.6055e+02 - logprior: -1.2599e+00
Epoch 9/10
11/11 - 1s - loss: 161.5252 - loglik: -1.6027e+02 - logprior: -1.2571e+00
Epoch 10/10
11/11 - 1s - loss: 160.9239 - loglik: -1.5967e+02 - logprior: -1.2511e+00
Fitted a model with MAP estimate = -161.0367
expansions: [(0, 6), (21, 1), (23, 2), (32, 1), (33, 1), (45, 1), (48, 1), (50, 1), (53, 1), (61, 1), (62, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 173.7486 - loglik: -1.5819e+02 - logprior: -1.5554e+01
Epoch 2/2
11/11 - 1s - loss: 152.5097 - loglik: -1.4811e+02 - logprior: -4.4028e+00
Fitted a model with MAP estimate = -148.8261
expansions: []
discards: [ 0 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 163.1963 - loglik: -1.4909e+02 - logprior: -1.4103e+01
Epoch 2/2
11/11 - 1s - loss: 152.2093 - loglik: -1.4674e+02 - logprior: -5.4685e+00
Fitted a model with MAP estimate = -149.7224
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.7692 - loglik: -1.4658e+02 - logprior: -1.2187e+01
Epoch 2/10
11/11 - 1s - loss: 148.9941 - loglik: -1.4584e+02 - logprior: -3.1567e+00
Epoch 3/10
11/11 - 1s - loss: 146.6694 - loglik: -1.4501e+02 - logprior: -1.6640e+00
Epoch 4/10
11/11 - 1s - loss: 145.6224 - loglik: -1.4466e+02 - logprior: -9.5987e-01
Epoch 5/10
11/11 - 1s - loss: 145.2807 - loglik: -1.4451e+02 - logprior: -7.7346e-01
Epoch 6/10
11/11 - 1s - loss: 144.7199 - loglik: -1.4413e+02 - logprior: -5.9297e-01
Epoch 7/10
11/11 - 1s - loss: 144.3938 - loglik: -1.4384e+02 - logprior: -5.4911e-01
Epoch 8/10
11/11 - 1s - loss: 143.9286 - loglik: -1.4343e+02 - logprior: -4.9766e-01
Epoch 9/10
11/11 - 1s - loss: 143.5362 - loglik: -1.4305e+02 - logprior: -4.8571e-01
Epoch 10/10
11/11 - 1s - loss: 143.5173 - loglik: -1.4306e+02 - logprior: -4.5767e-01
Fitted a model with MAP estimate = -143.3862
Time for alignment: 37.5441
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 243.1331 - loglik: -2.3062e+02 - logprior: -1.2518e+01
Epoch 2/10
11/11 - 1s - loss: 213.6209 - loglik: -2.1038e+02 - logprior: -3.2419e+00
Epoch 3/10
11/11 - 1s - loss: 189.9926 - loglik: -1.8789e+02 - logprior: -2.1005e+00
Epoch 4/10
11/11 - 1s - loss: 172.9044 - loglik: -1.7117e+02 - logprior: -1.7302e+00
Epoch 5/10
11/11 - 1s - loss: 166.2649 - loglik: -1.6476e+02 - logprior: -1.5002e+00
Epoch 6/10
11/11 - 1s - loss: 163.3886 - loglik: -1.6193e+02 - logprior: -1.4542e+00
Epoch 7/10
11/11 - 1s - loss: 162.3079 - loglik: -1.6098e+02 - logprior: -1.3272e+00
Epoch 8/10
11/11 - 1s - loss: 161.3882 - loglik: -1.6010e+02 - logprior: -1.2886e+00
Epoch 9/10
11/11 - 1s - loss: 160.9799 - loglik: -1.5972e+02 - logprior: -1.2635e+00
Epoch 10/10
11/11 - 1s - loss: 160.6722 - loglik: -1.5941e+02 - logprior: -1.2581e+00
Fitted a model with MAP estimate = -160.6137
expansions: [(0, 6), (19, 1), (21, 1), (32, 1), (33, 2), (45, 1), (48, 1), (50, 1), (51, 1), (61, 1), (62, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 173.3224 - loglik: -1.5773e+02 - logprior: -1.5591e+01
Epoch 2/2
11/11 - 1s - loss: 152.5503 - loglik: -1.4813e+02 - logprior: -4.4200e+00
Fitted a model with MAP estimate = -149.0073
expansions: []
discards: [ 0 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 163.3993 - loglik: -1.4929e+02 - logprior: -1.4113e+01
Epoch 2/2
11/11 - 1s - loss: 152.3566 - loglik: -1.4688e+02 - logprior: -5.4766e+00
Fitted a model with MAP estimate = -149.7695
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.8363 - loglik: -1.4665e+02 - logprior: -1.2186e+01
Epoch 2/10
11/11 - 1s - loss: 148.8384 - loglik: -1.4568e+02 - logprior: -3.1626e+00
Epoch 3/10
11/11 - 1s - loss: 146.6928 - loglik: -1.4503e+02 - logprior: -1.6608e+00
Epoch 4/10
11/11 - 1s - loss: 145.7708 - loglik: -1.4480e+02 - logprior: -9.7100e-01
Epoch 5/10
11/11 - 1s - loss: 145.0646 - loglik: -1.4428e+02 - logprior: -7.8171e-01
Epoch 6/10
11/11 - 1s - loss: 144.4306 - loglik: -1.4383e+02 - logprior: -6.0396e-01
Epoch 7/10
11/11 - 1s - loss: 144.3493 - loglik: -1.4380e+02 - logprior: -5.5174e-01
Epoch 8/10
11/11 - 1s - loss: 143.6052 - loglik: -1.4309e+02 - logprior: -5.1714e-01
Epoch 9/10
11/11 - 1s - loss: 143.7445 - loglik: -1.4327e+02 - logprior: -4.7692e-01
Fitted a model with MAP estimate = -143.4887
Time for alignment: 36.6242
Computed alignments with likelihoods: ['-143.4843', '-143.4730', '-144.7613', '-143.3862', '-143.4887']
Best model has likelihood: -143.3862  (prior= -0.4410 )
time for generating output: 0.1287
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 5 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f929bec10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f91417910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1edc821be0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f201aeb5160>
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 537.0670 - loglik: -5.3180e+02 - logprior: -5.2693e+00
Epoch 2/10
24/24 - 6s - loss: 382.7594 - loglik: -3.8054e+02 - logprior: -2.2150e+00
Epoch 3/10
24/24 - 6s - loss: 352.7559 - loglik: -3.5010e+02 - logprior: -2.6589e+00
Epoch 4/10
24/24 - 6s - loss: 348.8465 - loglik: -3.4628e+02 - logprior: -2.5689e+00
Epoch 5/10
24/24 - 6s - loss: 346.5077 - loglik: -3.4397e+02 - logprior: -2.5361e+00
Epoch 6/10
24/24 - 6s - loss: 346.5057 - loglik: -3.4400e+02 - logprior: -2.5058e+00
Epoch 7/10
24/24 - 6s - loss: 346.3127 - loglik: -3.4379e+02 - logprior: -2.5178e+00
Epoch 8/10
24/24 - 6s - loss: 345.4240 - loglik: -3.4291e+02 - logprior: -2.5127e+00
Epoch 9/10
24/24 - 6s - loss: 344.6801 - loglik: -3.4216e+02 - logprior: -2.5223e+00
Epoch 10/10
24/24 - 6s - loss: 345.3750 - loglik: -3.4286e+02 - logprior: -2.5188e+00
Fitted a model with MAP estimate = -344.5238
expansions: [(13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 1), (36, 1), (37, 1), (38, 1), (39, 1), (41, 1), (42, 1), (49, 1), (61, 1), (63, 1), (64, 1), (66, 1), (84, 1), (88, 1), (89, 1), (90, 1), (91, 1), (111, 1), (114, 1), (117, 1), (118, 1), (119, 2), (120, 2), (122, 1), (149, 1), (152, 1), (153, 1), (154, 3), (155, 1), (171, 2), (172, 1), (173, 1), (174, 1), (175, 1), (185, 1), (186, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 338.4229 - loglik: -3.3118e+02 - logprior: -7.2455e+00
Epoch 2/2
24/24 - 8s - loss: 319.6562 - loglik: -3.1730e+02 - logprior: -2.3560e+00
Fitted a model with MAP estimate = -317.3517
expansions: [(0, 3), (26, 1), (188, 1), (189, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 318.9103 - loglik: -3.1451e+02 - logprior: -4.3999e+00
Epoch 2/2
24/24 - 8s - loss: 312.0646 - loglik: -3.1209e+02 - logprior: 0.0302
Fitted a model with MAP estimate = -310.3700
expansions: [(152, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 322.2670 - loglik: -3.1587e+02 - logprior: -6.3974e+00
Epoch 2/10
24/24 - 8s - loss: 313.8739 - loglik: -3.1302e+02 - logprior: -8.5601e-01
Epoch 3/10
24/24 - 8s - loss: 311.7789 - loglik: -3.1299e+02 - logprior: 1.2076
Epoch 4/10
24/24 - 8s - loss: 310.4127 - loglik: -3.1167e+02 - logprior: 1.2565
Epoch 5/10
24/24 - 8s - loss: 310.8802 - loglik: -3.1233e+02 - logprior: 1.4449
Fitted a model with MAP estimate = -309.5759
Time for alignment: 171.1267
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 538.1127 - loglik: -5.3284e+02 - logprior: -5.2689e+00
Epoch 2/10
24/24 - 6s - loss: 384.2188 - loglik: -3.8199e+02 - logprior: -2.2336e+00
Epoch 3/10
24/24 - 6s - loss: 355.4175 - loglik: -3.5279e+02 - logprior: -2.6281e+00
Epoch 4/10
24/24 - 6s - loss: 348.8171 - loglik: -3.4633e+02 - logprior: -2.4874e+00
Epoch 5/10
24/24 - 6s - loss: 348.9753 - loglik: -3.4654e+02 - logprior: -2.4328e+00
Fitted a model with MAP estimate = -348.0731
expansions: [(13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 2), (19, 1), (21, 2), (35, 1), (36, 1), (37, 1), (41, 2), (42, 1), (49, 1), (50, 1), (60, 1), (64, 1), (66, 1), (84, 1), (88, 1), (89, 1), (90, 1), (91, 1), (102, 1), (111, 1), (113, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (122, 1), (137, 1), (152, 2), (153, 1), (154, 3), (155, 1), (171, 2), (172, 1), (173, 3), (185, 1), (186, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 338.8112 - loglik: -3.3161e+02 - logprior: -7.2030e+00
Epoch 2/2
24/24 - 8s - loss: 318.8174 - loglik: -3.1634e+02 - logprior: -2.4744e+00
Fitted a model with MAP estimate = -316.1738
expansions: [(0, 2), (193, 1)]
discards: [ 0 29 54]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 318.4501 - loglik: -3.1401e+02 - logprior: -4.4448e+00
Epoch 2/2
24/24 - 8s - loss: 313.2013 - loglik: -3.1318e+02 - logprior: -2.3117e-02
Fitted a model with MAP estimate = -311.3329
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 323.2747 - loglik: -3.1674e+02 - logprior: -6.5357e+00
Epoch 2/10
24/24 - 8s - loss: 317.0025 - loglik: -3.1530e+02 - logprior: -1.7000e+00
Epoch 3/10
24/24 - 8s - loss: 312.5952 - loglik: -3.1325e+02 - logprior: 0.6540
Epoch 4/10
24/24 - 8s - loss: 312.2621 - loglik: -3.1328e+02 - logprior: 1.0169
Epoch 5/10
24/24 - 8s - loss: 311.3593 - loglik: -3.1247e+02 - logprior: 1.1070
Epoch 6/10
24/24 - 8s - loss: 311.2851 - loglik: -3.1256e+02 - logprior: 1.2713
Epoch 7/10
24/24 - 8s - loss: 310.2144 - loglik: -3.1162e+02 - logprior: 1.4082
Epoch 8/10
24/24 - 8s - loss: 310.1294 - loglik: -3.1169e+02 - logprior: 1.5598
Epoch 9/10
24/24 - 8s - loss: 309.6299 - loglik: -3.1132e+02 - logprior: 1.6909
Epoch 10/10
24/24 - 8s - loss: 308.5602 - loglik: -3.1036e+02 - logprior: 1.8019
Fitted a model with MAP estimate = -308.1309
Time for alignment: 180.4455
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 538.2171 - loglik: -5.3296e+02 - logprior: -5.2527e+00
Epoch 2/10
24/24 - 6s - loss: 381.5149 - loglik: -3.7938e+02 - logprior: -2.1366e+00
Epoch 3/10
24/24 - 6s - loss: 353.2216 - loglik: -3.5072e+02 - logprior: -2.5002e+00
Epoch 4/10
24/24 - 6s - loss: 350.4260 - loglik: -3.4802e+02 - logprior: -2.4103e+00
Epoch 5/10
24/24 - 6s - loss: 348.8241 - loglik: -3.4650e+02 - logprior: -2.3269e+00
Epoch 6/10
24/24 - 6s - loss: 348.2718 - loglik: -3.4597e+02 - logprior: -2.3016e+00
Epoch 7/10
24/24 - 6s - loss: 347.8283 - loglik: -3.4553e+02 - logprior: -2.3026e+00
Epoch 8/10
24/24 - 6s - loss: 346.9176 - loglik: -3.4464e+02 - logprior: -2.2800e+00
Epoch 9/10
24/24 - 6s - loss: 347.3435 - loglik: -3.4505e+02 - logprior: -2.2910e+00
Fitted a model with MAP estimate = -346.5083
expansions: [(13, 1), (14, 1), (15, 2), (16, 2), (17, 2), (18, 2), (35, 1), (36, 1), (37, 1), (41, 2), (42, 1), (49, 1), (50, 1), (63, 1), (64, 1), (66, 1), (84, 1), (88, 1), (89, 1), (90, 1), (91, 1), (111, 1), (112, 1), (114, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (122, 1), (150, 1), (152, 2), (153, 4), (154, 1), (155, 1), (171, 2), (172, 3), (173, 1), (185, 1), (186, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 339.3623 - loglik: -3.3225e+02 - logprior: -7.1096e+00
Epoch 2/2
24/24 - 8s - loss: 317.8854 - loglik: -3.1565e+02 - logprior: -2.2393e+00
Fitted a model with MAP estimate = -316.7047
expansions: [(0, 3)]
discards: [ 0 24 52]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 319.9036 - loglik: -3.1548e+02 - logprior: -4.4227e+00
Epoch 2/2
24/24 - 8s - loss: 312.8314 - loglik: -3.1283e+02 - logprior: 0.0022
Fitted a model with MAP estimate = -311.4086
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 322.3860 - loglik: -3.1591e+02 - logprior: -6.4788e+00
Epoch 2/10
24/24 - 8s - loss: 316.9324 - loglik: -3.1611e+02 - logprior: -8.2647e-01
Epoch 3/10
24/24 - 8s - loss: 312.8232 - loglik: -3.1397e+02 - logprior: 1.1473
Epoch 4/10
24/24 - 8s - loss: 311.9102 - loglik: -3.1311e+02 - logprior: 1.2011
Epoch 5/10
24/24 - 8s - loss: 311.5577 - loglik: -3.1293e+02 - logprior: 1.3717
Epoch 6/10
24/24 - 8s - loss: 312.0578 - loglik: -3.1361e+02 - logprior: 1.5497
Fitted a model with MAP estimate = -310.7821
Time for alignment: 174.6748
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 538.8190 - loglik: -5.3357e+02 - logprior: -5.2476e+00
Epoch 2/10
24/24 - 6s - loss: 385.9043 - loglik: -3.8372e+02 - logprior: -2.1891e+00
Epoch 3/10
24/24 - 6s - loss: 356.3329 - loglik: -3.5376e+02 - logprior: -2.5751e+00
Epoch 4/10
24/24 - 6s - loss: 349.6625 - loglik: -3.4723e+02 - logprior: -2.4337e+00
Epoch 5/10
24/24 - 6s - loss: 349.5787 - loglik: -3.4715e+02 - logprior: -2.4277e+00
Epoch 6/10
24/24 - 6s - loss: 348.1117 - loglik: -3.4572e+02 - logprior: -2.3958e+00
Epoch 7/10
24/24 - 6s - loss: 348.0424 - loglik: -3.4565e+02 - logprior: -2.3956e+00
Epoch 8/10
24/24 - 6s - loss: 347.3971 - loglik: -3.4500e+02 - logprior: -2.4005e+00
Epoch 9/10
24/24 - 6s - loss: 347.2018 - loglik: -3.4480e+02 - logprior: -2.4012e+00
Epoch 10/10
24/24 - 6s - loss: 346.2223 - loglik: -3.4382e+02 - logprior: -2.4069e+00
Fitted a model with MAP estimate = -346.3923
expansions: [(13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (36, 1), (37, 1), (38, 1), (39, 1), (41, 1), (42, 1), (49, 1), (61, 1), (63, 1), (64, 1), (66, 1), (77, 1), (83, 1), (87, 1), (88, 1), (90, 1), (111, 1), (112, 1), (114, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (122, 1), (123, 2), (152, 2), (153, 4), (154, 1), (155, 1), (171, 2), (172, 3), (173, 1), (174, 1), (186, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 337.0551 - loglik: -3.2999e+02 - logprior: -7.0619e+00
Epoch 2/2
24/24 - 8s - loss: 318.8123 - loglik: -3.1661e+02 - logprior: -2.2017e+00
Fitted a model with MAP estimate = -316.2525
expansions: [(0, 3)]
discards: [  0 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 318.9840 - loglik: -3.1461e+02 - logprior: -4.3711e+00
Epoch 2/2
24/24 - 8s - loss: 313.1512 - loglik: -3.1321e+02 - logprior: 0.0626
Fitted a model with MAP estimate = -310.9952
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 322.9111 - loglik: -3.1649e+02 - logprior: -6.4258e+00
Epoch 2/10
24/24 - 8s - loss: 315.4297 - loglik: -3.1452e+02 - logprior: -9.1344e-01
Epoch 3/10
24/24 - 8s - loss: 311.8519 - loglik: -3.1288e+02 - logprior: 1.0330
Epoch 4/10
24/24 - 8s - loss: 312.1419 - loglik: -3.1320e+02 - logprior: 1.0537
Fitted a model with MAP estimate = -310.9631
Time for alignment: 163.2732
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 537.2968 - loglik: -5.3204e+02 - logprior: -5.2526e+00
Epoch 2/10
24/24 - 6s - loss: 386.3146 - loglik: -3.8405e+02 - logprior: -2.2646e+00
Epoch 3/10
24/24 - 6s - loss: 354.2432 - loglik: -3.5158e+02 - logprior: -2.6621e+00
Epoch 4/10
24/24 - 6s - loss: 348.9737 - loglik: -3.4643e+02 - logprior: -2.5424e+00
Epoch 5/10
24/24 - 6s - loss: 349.0967 - loglik: -3.4660e+02 - logprior: -2.4936e+00
Fitted a model with MAP estimate = -347.7014
expansions: [(11, 1), (12, 1), (15, 2), (16, 3), (17, 1), (18, 1), (19, 1), (35, 1), (37, 1), (39, 1), (41, 1), (42, 1), (49, 1), (50, 1), (63, 1), (64, 1), (66, 1), (84, 1), (88, 1), (89, 1), (90, 1), (91, 1), (93, 1), (110, 1), (113, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (122, 1), (137, 1), (152, 2), (153, 4), (154, 1), (155, 1), (171, 2), (172, 1), (173, 1), (174, 1), (175, 1), (185, 1), (186, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 336.8178 - loglik: -3.2976e+02 - logprior: -7.0616e+00
Epoch 2/2
24/24 - 8s - loss: 319.2245 - loglik: -3.1710e+02 - logprior: -2.1291e+00
Fitted a model with MAP estimate = -316.4350
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 319.1603 - loglik: -3.1472e+02 - logprior: -4.4381e+00
Epoch 2/2
24/24 - 8s - loss: 312.1890 - loglik: -3.1217e+02 - logprior: -1.6731e-02
Fitted a model with MAP estimate = -311.4223
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 10s - loss: 322.9366 - loglik: -3.1642e+02 - logprior: -6.5164e+00
Epoch 2/10
24/24 - 8s - loss: 315.7003 - loglik: -3.1471e+02 - logprior: -9.9039e-01
Epoch 3/10
24/24 - 8s - loss: 312.4578 - loglik: -3.1339e+02 - logprior: 0.9369
Epoch 4/10
24/24 - 8s - loss: 312.9236 - loglik: -3.1390e+02 - logprior: 0.9805
Fitted a model with MAP estimate = -311.3532
Time for alignment: 131.4156
Computed alignments with likelihoods: ['-309.5759', '-308.1309', '-310.7821', '-310.9631', '-311.3532']
Best model has likelihood: -308.1309  (prior= 1.8447 )
time for generating output: 0.2439
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.9792835172788131
Training of 5 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e5382b1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1eed787850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fe8506f40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1ee5120f70>
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 309.9677 - loglik: -2.8294e+02 - logprior: -2.7030e+01
Epoch 2/10
10/10 - 1s - loss: 257.7832 - loglik: -2.5095e+02 - logprior: -6.8294e+00
Epoch 3/10
10/10 - 1s - loss: 216.0202 - loglik: -2.1245e+02 - logprior: -3.5716e+00
Epoch 4/10
10/10 - 1s - loss: 191.4962 - loglik: -1.8882e+02 - logprior: -2.6774e+00
Epoch 5/10
10/10 - 1s - loss: 182.9325 - loglik: -1.8072e+02 - logprior: -2.2145e+00
Epoch 6/10
10/10 - 1s - loss: 179.7487 - loglik: -1.7785e+02 - logprior: -1.8995e+00
Epoch 7/10
10/10 - 1s - loss: 178.7527 - loglik: -1.7702e+02 - logprior: -1.7309e+00
Epoch 8/10
10/10 - 1s - loss: 177.7578 - loglik: -1.7613e+02 - logprior: -1.6245e+00
Epoch 9/10
10/10 - 1s - loss: 176.8629 - loglik: -1.7530e+02 - logprior: -1.5670e+00
Epoch 10/10
10/10 - 1s - loss: 176.3069 - loglik: -1.7480e+02 - logprior: -1.5091e+00
Fitted a model with MAP estimate = -176.1926
expansions: [(1, 1), (8, 1), (11, 1), (12, 1), (33, 1), (36, 4), (37, 1), (48, 3), (49, 2), (69, 2), (70, 1), (72, 2), (73, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 198.8670 - loglik: -1.7406e+02 - logprior: -2.4809e+01
Epoch 2/2
10/10 - 1s - loss: 168.4754 - loglik: -1.6205e+02 - logprior: -6.4208e+00
Fitted a model with MAP estimate = -162.0779
expansions: []
discards: [91]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.9692 - loglik: -1.5876e+02 - logprior: -2.4208e+01
Epoch 2/2
10/10 - 1s - loss: 161.9225 - loglik: -1.5570e+02 - logprior: -6.2254e+00
Fitted a model with MAP estimate = -158.2214
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 179.7152 - loglik: -1.5563e+02 - logprior: -2.4086e+01
Epoch 2/10
10/10 - 1s - loss: 160.0672 - loglik: -1.5402e+02 - logprior: -6.0501e+00
Epoch 3/10
10/10 - 1s - loss: 156.4395 - loglik: -1.5397e+02 - logprior: -2.4698e+00
Epoch 4/10
10/10 - 1s - loss: 154.0353 - loglik: -1.5292e+02 - logprior: -1.1127e+00
Epoch 5/10
10/10 - 1s - loss: 152.4635 - loglik: -1.5197e+02 - logprior: -4.9484e-01
Epoch 6/10
10/10 - 1s - loss: 151.4070 - loglik: -1.5124e+02 - logprior: -1.6732e-01
Epoch 7/10
10/10 - 1s - loss: 151.7475 - loglik: -1.5183e+02 - logprior: 0.0826
Fitted a model with MAP estimate = -150.8595
Time for alignment: 40.2711
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 309.9286 - loglik: -2.8290e+02 - logprior: -2.7027e+01
Epoch 2/10
10/10 - 1s - loss: 258.2023 - loglik: -2.5138e+02 - logprior: -6.8264e+00
Epoch 3/10
10/10 - 1s - loss: 217.3024 - loglik: -2.1372e+02 - logprior: -3.5785e+00
Epoch 4/10
10/10 - 1s - loss: 191.8828 - loglik: -1.8913e+02 - logprior: -2.7501e+00
Epoch 5/10
10/10 - 1s - loss: 183.2971 - loglik: -1.8105e+02 - logprior: -2.2494e+00
Epoch 6/10
10/10 - 1s - loss: 179.6284 - loglik: -1.7767e+02 - logprior: -1.9550e+00
Epoch 7/10
10/10 - 1s - loss: 177.3144 - loglik: -1.7545e+02 - logprior: -1.8669e+00
Epoch 8/10
10/10 - 1s - loss: 176.4530 - loglik: -1.7457e+02 - logprior: -1.8788e+00
Epoch 9/10
10/10 - 1s - loss: 176.0996 - loglik: -1.7422e+02 - logprior: -1.8842e+00
Epoch 10/10
10/10 - 1s - loss: 175.1859 - loglik: -1.7335e+02 - logprior: -1.8383e+00
Fitted a model with MAP estimate = -174.7223
expansions: [(7, 1), (8, 1), (11, 1), (12, 1), (33, 1), (36, 4), (37, 1), (48, 3), (49, 1), (50, 1), (69, 2), (70, 1), (72, 2), (73, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 198.5047 - loglik: -1.7365e+02 - logprior: -2.4851e+01
Epoch 2/2
10/10 - 1s - loss: 167.5400 - loglik: -1.6112e+02 - logprior: -6.4167e+00
Fitted a model with MAP estimate = -161.4353
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 182.5070 - loglik: -1.5827e+02 - logprior: -2.4232e+01
Epoch 2/10
10/10 - 1s - loss: 161.3763 - loglik: -1.5515e+02 - logprior: -6.2266e+00
Epoch 3/10
10/10 - 1s - loss: 156.4516 - loglik: -1.5380e+02 - logprior: -2.6557e+00
Epoch 4/10
10/10 - 1s - loss: 154.5817 - loglik: -1.5325e+02 - logprior: -1.3295e+00
Epoch 5/10
10/10 - 1s - loss: 153.0151 - loglik: -1.5230e+02 - logprior: -7.1798e-01
Epoch 6/10
10/10 - 1s - loss: 151.9204 - loglik: -1.5151e+02 - logprior: -4.1131e-01
Epoch 7/10
10/10 - 1s - loss: 151.2078 - loglik: -1.5106e+02 - logprior: -1.5153e-01
Epoch 8/10
10/10 - 1s - loss: 150.9700 - loglik: -1.5102e+02 - logprior: 0.0536
Epoch 9/10
10/10 - 1s - loss: 150.3536 - loglik: -1.5054e+02 - logprior: 0.1870
Epoch 10/10
10/10 - 1s - loss: 150.3360 - loglik: -1.5060e+02 - logprior: 0.2680
Fitted a model with MAP estimate = -150.1491
Time for alignment: 31.4710
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 309.9884 - loglik: -2.8296e+02 - logprior: -2.7029e+01
Epoch 2/10
10/10 - 1s - loss: 258.4569 - loglik: -2.5163e+02 - logprior: -6.8266e+00
Epoch 3/10
10/10 - 1s - loss: 216.8933 - loglik: -2.1330e+02 - logprior: -3.5947e+00
Epoch 4/10
10/10 - 1s - loss: 191.9887 - loglik: -1.8923e+02 - logprior: -2.7540e+00
Epoch 5/10
10/10 - 1s - loss: 183.3061 - loglik: -1.8103e+02 - logprior: -2.2792e+00
Epoch 6/10
10/10 - 1s - loss: 179.6313 - loglik: -1.7767e+02 - logprior: -1.9644e+00
Epoch 7/10
10/10 - 1s - loss: 177.7840 - loglik: -1.7592e+02 - logprior: -1.8614e+00
Epoch 8/10
10/10 - 1s - loss: 176.3902 - loglik: -1.7450e+02 - logprior: -1.8853e+00
Epoch 9/10
10/10 - 1s - loss: 175.2301 - loglik: -1.7331e+02 - logprior: -1.9173e+00
Epoch 10/10
10/10 - 1s - loss: 175.1411 - loglik: -1.7323e+02 - logprior: -1.9135e+00
Fitted a model with MAP estimate = -174.0003
expansions: [(7, 1), (8, 1), (11, 1), (12, 1), (33, 1), (36, 4), (37, 1), (48, 3), (49, 1), (50, 1), (69, 2), (70, 2), (71, 2), (72, 2), (73, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 198.3914 - loglik: -1.7348e+02 - logprior: -2.4909e+01
Epoch 2/2
10/10 - 1s - loss: 166.9114 - loglik: -1.6041e+02 - logprior: -6.4988e+00
Fitted a model with MAP estimate = -161.4064
expansions: []
discards: [90 93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.7679 - loglik: -1.5855e+02 - logprior: -2.4213e+01
Epoch 2/2
10/10 - 1s - loss: 161.1541 - loglik: -1.5496e+02 - logprior: -6.1977e+00
Fitted a model with MAP estimate = -157.7663
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.5116 - loglik: -1.5543e+02 - logprior: -2.4077e+01
Epoch 2/10
10/10 - 1s - loss: 160.1173 - loglik: -1.5410e+02 - logprior: -6.0162e+00
Epoch 3/10
10/10 - 1s - loss: 155.7526 - loglik: -1.5332e+02 - logprior: -2.4354e+00
Epoch 4/10
10/10 - 1s - loss: 154.1630 - loglik: -1.5307e+02 - logprior: -1.0951e+00
Epoch 5/10
10/10 - 1s - loss: 152.5254 - loglik: -1.5206e+02 - logprior: -4.6897e-01
Epoch 6/10
10/10 - 1s - loss: 151.7481 - loglik: -1.5161e+02 - logprior: -1.3408e-01
Epoch 7/10
10/10 - 1s - loss: 151.2656 - loglik: -1.5140e+02 - logprior: 0.1343
Epoch 8/10
10/10 - 1s - loss: 150.7758 - loglik: -1.5112e+02 - logprior: 0.3403
Epoch 9/10
10/10 - 1s - loss: 150.4381 - loglik: -1.5092e+02 - logprior: 0.4812
Epoch 10/10
10/10 - 1s - loss: 149.9055 - loglik: -1.5047e+02 - logprior: 0.5651
Fitted a model with MAP estimate = -149.9229
Time for alignment: 40.0223
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 310.3730 - loglik: -2.8334e+02 - logprior: -2.7029e+01
Epoch 2/10
10/10 - 1s - loss: 257.6100 - loglik: -2.5078e+02 - logprior: -6.8285e+00
Epoch 3/10
10/10 - 1s - loss: 216.2652 - loglik: -2.1269e+02 - logprior: -3.5734e+00
Epoch 4/10
10/10 - 1s - loss: 191.5661 - loglik: -1.8886e+02 - logprior: -2.7066e+00
Epoch 5/10
10/10 - 1s - loss: 184.1622 - loglik: -1.8195e+02 - logprior: -2.2148e+00
Epoch 6/10
10/10 - 1s - loss: 180.9210 - loglik: -1.7900e+02 - logprior: -1.9185e+00
Epoch 7/10
10/10 - 1s - loss: 178.6273 - loglik: -1.7682e+02 - logprior: -1.8103e+00
Epoch 8/10
10/10 - 1s - loss: 178.0090 - loglik: -1.7619e+02 - logprior: -1.8232e+00
Epoch 9/10
10/10 - 1s - loss: 177.1520 - loglik: -1.7531e+02 - logprior: -1.8467e+00
Epoch 10/10
10/10 - 1s - loss: 176.0176 - loglik: -1.7418e+02 - logprior: -1.8378e+00
Fitted a model with MAP estimate = -175.8033
expansions: [(7, 1), (8, 1), (11, 1), (12, 1), (34, 1), (35, 3), (36, 2), (46, 1), (47, 3), (49, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 198.7513 - loglik: -1.7392e+02 - logprior: -2.4828e+01
Epoch 2/2
10/10 - 1s - loss: 167.7508 - loglik: -1.6142e+02 - logprior: -6.3340e+00
Fitted a model with MAP estimate = -162.0673
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 183.0512 - loglik: -1.5885e+02 - logprior: -2.4203e+01
Epoch 2/10
10/10 - 1s - loss: 162.1148 - loglik: -1.5589e+02 - logprior: -6.2233e+00
Epoch 3/10
10/10 - 1s - loss: 157.7525 - loglik: -1.5511e+02 - logprior: -2.6438e+00
Epoch 4/10
10/10 - 1s - loss: 155.1860 - loglik: -1.5389e+02 - logprior: -1.2956e+00
Epoch 5/10
10/10 - 1s - loss: 153.9720 - loglik: -1.5329e+02 - logprior: -6.7776e-01
Epoch 6/10
10/10 - 1s - loss: 152.8607 - loglik: -1.5252e+02 - logprior: -3.3914e-01
Epoch 7/10
10/10 - 1s - loss: 152.0717 - loglik: -1.5199e+02 - logprior: -8.1522e-02
Epoch 8/10
10/10 - 1s - loss: 151.7570 - loglik: -1.5189e+02 - logprior: 0.1347
Epoch 9/10
10/10 - 1s - loss: 151.3617 - loglik: -1.5162e+02 - logprior: 0.2628
Epoch 10/10
10/10 - 1s - loss: 151.0263 - loglik: -1.5137e+02 - logprior: 0.3411
Fitted a model with MAP estimate = -150.9036
Time for alignment: 33.0272
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 310.2862 - loglik: -2.8326e+02 - logprior: -2.7030e+01
Epoch 2/10
10/10 - 1s - loss: 257.5349 - loglik: -2.5070e+02 - logprior: -6.8323e+00
Epoch 3/10
10/10 - 1s - loss: 215.3744 - loglik: -2.1181e+02 - logprior: -3.5675e+00
Epoch 4/10
10/10 - 1s - loss: 189.6621 - loglik: -1.8702e+02 - logprior: -2.6420e+00
Epoch 5/10
10/10 - 1s - loss: 181.9885 - loglik: -1.7982e+02 - logprior: -2.1693e+00
Epoch 6/10
10/10 - 1s - loss: 178.8380 - loglik: -1.7696e+02 - logprior: -1.8803e+00
Epoch 7/10
10/10 - 1s - loss: 177.1556 - loglik: -1.7546e+02 - logprior: -1.6964e+00
Epoch 8/10
10/10 - 1s - loss: 176.1355 - loglik: -1.7454e+02 - logprior: -1.5952e+00
Epoch 9/10
10/10 - 1s - loss: 176.3215 - loglik: -1.7478e+02 - logprior: -1.5450e+00
Fitted a model with MAP estimate = -175.4384
expansions: [(1, 1), (12, 1), (13, 1), (15, 1), (33, 1), (36, 4), (37, 1), (48, 3), (49, 2), (69, 2), (70, 1), (72, 2), (73, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 197.6873 - loglik: -1.7288e+02 - logprior: -2.4811e+01
Epoch 2/2
10/10 - 1s - loss: 167.7486 - loglik: -1.6129e+02 - logprior: -6.4598e+00
Fitted a model with MAP estimate = -162.0085
expansions: []
discards: [91]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 182.8957 - loglik: -1.5862e+02 - logprior: -2.4276e+01
Epoch 2/2
10/10 - 1s - loss: 162.5818 - loglik: -1.5632e+02 - logprior: -6.2636e+00
Fitted a model with MAP estimate = -158.5611
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.9972 - loglik: -1.5585e+02 - logprior: -2.4143e+01
Epoch 2/10
10/10 - 1s - loss: 160.7141 - loglik: -1.5462e+02 - logprior: -6.0912e+00
Epoch 3/10
10/10 - 1s - loss: 156.2343 - loglik: -1.5373e+02 - logprior: -2.5065e+00
Epoch 4/10
10/10 - 1s - loss: 154.2985 - loglik: -1.5316e+02 - logprior: -1.1403e+00
Epoch 5/10
10/10 - 1s - loss: 153.4350 - loglik: -1.5292e+02 - logprior: -5.1680e-01
Epoch 6/10
10/10 - 1s - loss: 151.9751 - loglik: -1.5180e+02 - logprior: -1.7618e-01
Epoch 7/10
10/10 - 1s - loss: 151.7743 - loglik: -1.5185e+02 - logprior: 0.0806
Epoch 8/10
10/10 - 1s - loss: 151.0398 - loglik: -1.5133e+02 - logprior: 0.2882
Epoch 9/10
10/10 - 1s - loss: 150.5079 - loglik: -1.5093e+02 - logprior: 0.4252
Epoch 10/10
10/10 - 1s - loss: 150.0468 - loglik: -1.5055e+02 - logprior: 0.5036
Fitted a model with MAP estimate = -149.9836
Time for alignment: 37.5877
Computed alignments with likelihoods: ['-150.8595', '-150.1491', '-149.9229', '-150.9036', '-149.9836']
Best model has likelihood: -149.9229  (prior= 0.6038 )
time for generating output: 0.1355
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.7957805907172996
Training of 5 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ec2ffe2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fa4028220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1fb49b5c70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1edc4478b0>
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 152.1453 - loglik: -1.4706e+02 - logprior: -5.0863e+00
Epoch 2/10
16/16 - 1s - loss: 129.2719 - loglik: -1.2768e+02 - logprior: -1.5883e+00
Epoch 3/10
16/16 - 1s - loss: 117.3735 - loglik: -1.1578e+02 - logprior: -1.5916e+00
Epoch 4/10
16/16 - 1s - loss: 114.0230 - loglik: -1.1252e+02 - logprior: -1.4984e+00
Epoch 5/10
16/16 - 1s - loss: 112.3956 - loglik: -1.1091e+02 - logprior: -1.4862e+00
Epoch 6/10
16/16 - 1s - loss: 111.8162 - loglik: -1.1031e+02 - logprior: -1.5022e+00
Epoch 7/10
16/16 - 1s - loss: 111.3776 - loglik: -1.0991e+02 - logprior: -1.4644e+00
Epoch 8/10
16/16 - 1s - loss: 110.9308 - loglik: -1.0948e+02 - logprior: -1.4469e+00
Epoch 9/10
16/16 - 1s - loss: 111.2571 - loglik: -1.0981e+02 - logprior: -1.4445e+00
Fitted a model with MAP estimate = -110.9051
expansions: [(3, 1), (6, 2), (15, 2), (16, 3), (22, 3), (23, 2), (25, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 115.3320 - loglik: -1.0896e+02 - logprior: -6.3714e+00
Epoch 2/2
16/16 - 1s - loss: 106.2879 - loglik: -1.0317e+02 - logprior: -3.1193e+00
Fitted a model with MAP estimate = -105.1333
expansions: [(0, 1)]
discards: [ 0 21 33 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 107.6116 - loglik: -1.0297e+02 - logprior: -4.6380e+00
Epoch 2/2
16/16 - 1s - loss: 103.6952 - loglik: -1.0205e+02 - logprior: -1.6500e+00
Fitted a model with MAP estimate = -103.0188
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 109.4980 - loglik: -1.0322e+02 - logprior: -6.2810e+00
Epoch 2/10
16/16 - 1s - loss: 105.0695 - loglik: -1.0231e+02 - logprior: -2.7614e+00
Epoch 3/10
16/16 - 1s - loss: 103.2094 - loglik: -1.0168e+02 - logprior: -1.5340e+00
Epoch 4/10
16/16 - 1s - loss: 102.8560 - loglik: -1.0161e+02 - logprior: -1.2508e+00
Epoch 5/10
16/16 - 1s - loss: 102.3056 - loglik: -1.0109e+02 - logprior: -1.2110e+00
Epoch 6/10
16/16 - 1s - loss: 102.0588 - loglik: -1.0087e+02 - logprior: -1.1882e+00
Epoch 7/10
16/16 - 1s - loss: 101.4528 - loglik: -1.0028e+02 - logprior: -1.1733e+00
Epoch 8/10
16/16 - 1s - loss: 101.3497 - loglik: -1.0020e+02 - logprior: -1.1544e+00
Epoch 9/10
16/16 - 1s - loss: 101.3640 - loglik: -1.0023e+02 - logprior: -1.1379e+00
Fitted a model with MAP estimate = -101.1683
Time for alignment: 50.1537
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 152.3581 - loglik: -1.4727e+02 - logprior: -5.0836e+00
Epoch 2/10
16/16 - 1s - loss: 128.4267 - loglik: -1.2684e+02 - logprior: -1.5833e+00
Epoch 3/10
16/16 - 1s - loss: 117.5956 - loglik: -1.1595e+02 - logprior: -1.6489e+00
Epoch 4/10
16/16 - 1s - loss: 113.2510 - loglik: -1.1165e+02 - logprior: -1.5992e+00
Epoch 5/10
16/16 - 1s - loss: 111.2504 - loglik: -1.0965e+02 - logprior: -1.5992e+00
Epoch 6/10
16/16 - 1s - loss: 110.4791 - loglik: -1.0887e+02 - logprior: -1.6042e+00
Epoch 7/10
16/16 - 1s - loss: 109.6160 - loglik: -1.0806e+02 - logprior: -1.5540e+00
Epoch 8/10
16/16 - 1s - loss: 109.6111 - loglik: -1.0807e+02 - logprior: -1.5399e+00
Epoch 9/10
16/16 - 1s - loss: 109.3689 - loglik: -1.0784e+02 - logprior: -1.5267e+00
Epoch 10/10
16/16 - 1s - loss: 109.2251 - loglik: -1.0771e+02 - logprior: -1.5163e+00
Fitted a model with MAP estimate = -109.1408
expansions: [(3, 1), (6, 1), (13, 1), (16, 1), (17, 2), (23, 6), (25, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 114.5616 - loglik: -1.0821e+02 - logprior: -6.3475e+00
Epoch 2/2
16/16 - 1s - loss: 106.3283 - loglik: -1.0328e+02 - logprior: -3.0446e+00
Fitted a model with MAP estimate = -105.0632
expansions: [(0, 1)]
discards: [ 0 31 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 107.5656 - loglik: -1.0295e+02 - logprior: -4.6164e+00
Epoch 2/2
16/16 - 1s - loss: 103.6023 - loglik: -1.0197e+02 - logprior: -1.6279e+00
Fitted a model with MAP estimate = -103.0422
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 109.7190 - loglik: -1.0346e+02 - logprior: -6.2589e+00
Epoch 2/10
16/16 - 1s - loss: 105.1416 - loglik: -1.0238e+02 - logprior: -2.7588e+00
Epoch 3/10
16/16 - 1s - loss: 103.2968 - loglik: -1.0177e+02 - logprior: -1.5240e+00
Epoch 4/10
16/16 - 1s - loss: 102.4531 - loglik: -1.0123e+02 - logprior: -1.2276e+00
Epoch 5/10
16/16 - 1s - loss: 102.6155 - loglik: -1.0142e+02 - logprior: -1.1952e+00
Fitted a model with MAP estimate = -102.1411
Time for alignment: 46.3122
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 152.4893 - loglik: -1.4740e+02 - logprior: -5.0851e+00
Epoch 2/10
16/16 - 1s - loss: 128.7593 - loglik: -1.2716e+02 - logprior: -1.5953e+00
Epoch 3/10
16/16 - 1s - loss: 116.6252 - loglik: -1.1493e+02 - logprior: -1.6948e+00
Epoch 4/10
16/16 - 1s - loss: 112.2653 - loglik: -1.1060e+02 - logprior: -1.6650e+00
Epoch 5/10
16/16 - 1s - loss: 110.8075 - loglik: -1.0915e+02 - logprior: -1.6607e+00
Epoch 6/10
16/16 - 1s - loss: 109.9211 - loglik: -1.0828e+02 - logprior: -1.6406e+00
Epoch 7/10
16/16 - 1s - loss: 109.4261 - loglik: -1.0784e+02 - logprior: -1.5840e+00
Epoch 8/10
16/16 - 1s - loss: 109.2454 - loglik: -1.0767e+02 - logprior: -1.5735e+00
Epoch 9/10
16/16 - 1s - loss: 109.4977 - loglik: -1.0794e+02 - logprior: -1.5583e+00
Fitted a model with MAP estimate = -109.0483
expansions: [(3, 1), (6, 1), (13, 1), (16, 1), (17, 2), (19, 1), (22, 3), (23, 2), (25, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 114.4944 - loglik: -1.0814e+02 - logprior: -6.3527e+00
Epoch 2/2
16/16 - 1s - loss: 106.6510 - loglik: -1.0359e+02 - logprior: -3.0578e+00
Fitted a model with MAP estimate = -104.9699
expansions: [(0, 1)]
discards: [ 0 32 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 107.2833 - loglik: -1.0265e+02 - logprior: -4.6325e+00
Epoch 2/2
16/16 - 1s - loss: 103.4725 - loglik: -1.0183e+02 - logprior: -1.6458e+00
Fitted a model with MAP estimate = -103.0043
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 109.7029 - loglik: -1.0343e+02 - logprior: -6.2680e+00
Epoch 2/10
16/16 - 1s - loss: 104.9882 - loglik: -1.0222e+02 - logprior: -2.7652e+00
Epoch 3/10
16/16 - 1s - loss: 103.3466 - loglik: -1.0182e+02 - logprior: -1.5232e+00
Epoch 4/10
16/16 - 1s - loss: 102.6929 - loglik: -1.0145e+02 - logprior: -1.2464e+00
Epoch 5/10
16/16 - 1s - loss: 102.2712 - loglik: -1.0107e+02 - logprior: -1.2051e+00
Epoch 6/10
16/16 - 1s - loss: 101.9347 - loglik: -1.0075e+02 - logprior: -1.1798e+00
Epoch 7/10
16/16 - 1s - loss: 101.5898 - loglik: -1.0043e+02 - logprior: -1.1646e+00
Epoch 8/10
16/16 - 1s - loss: 101.4402 - loglik: -1.0030e+02 - logprior: -1.1378e+00
Epoch 9/10
16/16 - 1s - loss: 101.0540 - loglik: -9.9923e+01 - logprior: -1.1306e+00
Epoch 10/10
16/16 - 1s - loss: 101.1481 - loglik: -1.0004e+02 - logprior: -1.1124e+00
Fitted a model with MAP estimate = -101.0696
Time for alignment: 50.2516
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 152.4031 - loglik: -1.4732e+02 - logprior: -5.0847e+00
Epoch 2/10
16/16 - 1s - loss: 129.6127 - loglik: -1.2802e+02 - logprior: -1.5966e+00
Epoch 3/10
16/16 - 1s - loss: 118.7247 - loglik: -1.1708e+02 - logprior: -1.6428e+00
Epoch 4/10
16/16 - 1s - loss: 113.9415 - loglik: -1.1236e+02 - logprior: -1.5814e+00
Epoch 5/10
16/16 - 1s - loss: 111.9291 - loglik: -1.1037e+02 - logprior: -1.5600e+00
Epoch 6/10
16/16 - 1s - loss: 110.8021 - loglik: -1.0922e+02 - logprior: -1.5799e+00
Epoch 7/10
16/16 - 1s - loss: 110.8670 - loglik: -1.0933e+02 - logprior: -1.5420e+00
Fitted a model with MAP estimate = -110.4585
expansions: [(3, 1), (6, 2), (16, 1), (17, 2), (19, 1), (22, 4), (23, 1), (25, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 115.2536 - loglik: -1.0890e+02 - logprior: -6.3558e+00
Epoch 2/2
16/16 - 1s - loss: 105.8335 - loglik: -1.0278e+02 - logprior: -3.0510e+00
Fitted a model with MAP estimate = -105.1579
expansions: [(0, 1)]
discards: [ 0 31 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 107.7648 - loglik: -1.0314e+02 - logprior: -4.6267e+00
Epoch 2/2
16/16 - 1s - loss: 103.5638 - loglik: -1.0192e+02 - logprior: -1.6459e+00
Fitted a model with MAP estimate = -103.1204
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 109.7800 - loglik: -1.0350e+02 - logprior: -6.2796e+00
Epoch 2/10
16/16 - 1s - loss: 104.9052 - loglik: -1.0211e+02 - logprior: -2.7979e+00
Epoch 3/10
16/16 - 1s - loss: 103.6108 - loglik: -1.0205e+02 - logprior: -1.5594e+00
Epoch 4/10
16/16 - 1s - loss: 102.6412 - loglik: -1.0139e+02 - logprior: -1.2465e+00
Epoch 5/10
16/16 - 1s - loss: 102.5713 - loglik: -1.0137e+02 - logprior: -1.2042e+00
Epoch 6/10
16/16 - 1s - loss: 102.0657 - loglik: -1.0089e+02 - logprior: -1.1767e+00
Epoch 7/10
16/16 - 1s - loss: 102.0231 - loglik: -1.0086e+02 - logprior: -1.1637e+00
Epoch 8/10
16/16 - 1s - loss: 101.3353 - loglik: -1.0018e+02 - logprior: -1.1509e+00
Epoch 9/10
16/16 - 1s - loss: 101.6716 - loglik: -1.0054e+02 - logprior: -1.1312e+00
Fitted a model with MAP estimate = -101.2757
Time for alignment: 46.1043
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 152.2857 - loglik: -1.4720e+02 - logprior: -5.0824e+00
Epoch 2/10
16/16 - 1s - loss: 130.5786 - loglik: -1.2897e+02 - logprior: -1.6123e+00
Epoch 3/10
16/16 - 1s - loss: 119.4038 - loglik: -1.1768e+02 - logprior: -1.7285e+00
Epoch 4/10
16/16 - 1s - loss: 114.2858 - loglik: -1.1261e+02 - logprior: -1.6798e+00
Epoch 5/10
16/16 - 1s - loss: 112.4299 - loglik: -1.1076e+02 - logprior: -1.6722e+00
Epoch 6/10
16/16 - 1s - loss: 111.6611 - loglik: -1.0996e+02 - logprior: -1.6992e+00
Epoch 7/10
16/16 - 1s - loss: 110.7173 - loglik: -1.0905e+02 - logprior: -1.6631e+00
Epoch 8/10
16/16 - 1s - loss: 110.3578 - loglik: -1.0870e+02 - logprior: -1.6603e+00
Epoch 9/10
16/16 - 1s - loss: 110.1470 - loglik: -1.0849e+02 - logprior: -1.6530e+00
Epoch 10/10
16/16 - 1s - loss: 109.9027 - loglik: -1.0826e+02 - logprior: -1.6467e+00
Fitted a model with MAP estimate = -109.8074
expansions: [(3, 1), (6, 1), (13, 1), (15, 1), (17, 2), (19, 1), (22, 1), (25, 1), (30, 2), (33, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 114.7206 - loglik: -1.0838e+02 - logprior: -6.3376e+00
Epoch 2/2
16/16 - 1s - loss: 106.6829 - loglik: -1.0367e+02 - logprior: -3.0167e+00
Fitted a model with MAP estimate = -105.3630
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 107.8801 - loglik: -1.0324e+02 - logprior: -4.6389e+00
Epoch 2/2
16/16 - 1s - loss: 103.7233 - loglik: -1.0207e+02 - logprior: -1.6520e+00
Fitted a model with MAP estimate = -103.3876
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 110.2863 - loglik: -1.0399e+02 - logprior: -6.2957e+00
Epoch 2/10
16/16 - 1s - loss: 105.1220 - loglik: -1.0229e+02 - logprior: -2.8315e+00
Epoch 3/10
16/16 - 1s - loss: 103.8896 - loglik: -1.0230e+02 - logprior: -1.5855e+00
Epoch 4/10
16/16 - 1s - loss: 102.9302 - loglik: -1.0167e+02 - logprior: -1.2577e+00
Epoch 5/10
16/16 - 1s - loss: 102.8416 - loglik: -1.0162e+02 - logprior: -1.2199e+00
Epoch 6/10
16/16 - 1s - loss: 102.2825 - loglik: -1.0110e+02 - logprior: -1.1824e+00
Epoch 7/10
16/16 - 1s - loss: 102.6727 - loglik: -1.0151e+02 - logprior: -1.1621e+00
Fitted a model with MAP estimate = -102.1835
Time for alignment: 46.0118
Computed alignments with likelihoods: ['-101.1683', '-102.1411', '-101.0696', '-101.2757', '-102.1835']
Best model has likelihood: -101.0696  (prior= -1.1117 )
time for generating output: 0.1082
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.9240710823909531
Training of 5 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ee4dc4f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f2023f23760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f06da9d90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1edc4478b0>
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 590.6523 - loglik: -5.8907e+02 - logprior: -1.5853e+00
Epoch 2/10
39/39 - 9s - loss: 540.7968 - loglik: -5.4024e+02 - logprior: -5.5740e-01
Epoch 3/10
39/39 - 9s - loss: 531.8311 - loglik: -5.3129e+02 - logprior: -5.4466e-01
Epoch 4/10
39/39 - 9s - loss: 528.2120 - loglik: -5.2772e+02 - logprior: -4.9043e-01
Epoch 5/10
39/39 - 9s - loss: 526.8904 - loglik: -5.2640e+02 - logprior: -4.8847e-01
Epoch 6/10
39/39 - 9s - loss: 526.5381 - loglik: -5.2607e+02 - logprior: -4.6654e-01
Epoch 7/10
39/39 - 9s - loss: 526.1382 - loglik: -5.2569e+02 - logprior: -4.5213e-01
Epoch 8/10
39/39 - 9s - loss: 525.9647 - loglik: -5.2553e+02 - logprior: -4.3930e-01
Epoch 9/10
39/39 - 9s - loss: 525.5991 - loglik: -5.2517e+02 - logprior: -4.3289e-01
Epoch 10/10
39/39 - 9s - loss: 525.7914 - loglik: -5.2536e+02 - logprior: -4.3217e-01
Fitted a model with MAP estimate = -482.1462
expansions: [(0, 3), (10, 1), (20, 1), (24, 6), (27, 3), (53, 1), (55, 1), (77, 12), (112, 1), (114, 1), (125, 4), (126, 10), (127, 2), (137, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 206 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 527.6669 - loglik: -5.2510e+02 - logprior: -2.5636e+00
Epoch 2/2
39/39 - 12s - loss: 518.2869 - loglik: -5.1757e+02 - logprior: -7.1303e-01
Fitted a model with MAP estimate = -475.4735
expansions: [(31, 4), (141, 1), (158, 2), (186, 2)]
discards: [  0  94  95  96  97  98  99 100 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 526.7496 - loglik: -5.2435e+02 - logprior: -2.4020e+00
Epoch 2/2
39/39 - 11s - loss: 520.4898 - loglik: -5.2021e+02 - logprior: -2.7588e-01
Fitted a model with MAP estimate = -476.8457
expansions: [(0, 2), (169, 2), (171, 2)]
discards: [  0  32  33  34  35 156]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 19s - loss: 472.9730 - loglik: -4.7199e+02 - logprior: -9.8439e-01
Epoch 2/10
51/51 - 14s - loss: 467.3036 - loglik: -4.6663e+02 - logprior: -6.7009e-01
Epoch 3/10
51/51 - 14s - loss: 466.0066 - loglik: -4.6536e+02 - logprior: -6.4723e-01
Epoch 4/10
51/51 - 14s - loss: 464.8519 - loglik: -4.6424e+02 - logprior: -6.1321e-01
Epoch 5/10
51/51 - 14s - loss: 464.7588 - loglik: -4.6418e+02 - logprior: -5.7423e-01
Epoch 6/10
51/51 - 14s - loss: 464.0573 - loglik: -4.6352e+02 - logprior: -5.3363e-01
Epoch 7/10
51/51 - 14s - loss: 463.3395 - loglik: -4.6284e+02 - logprior: -4.9979e-01
Epoch 8/10
51/51 - 14s - loss: 463.1278 - loglik: -4.6266e+02 - logprior: -4.6816e-01
Epoch 9/10
51/51 - 15s - loss: 463.7393 - loglik: -4.6331e+02 - logprior: -4.3353e-01
Fitted a model with MAP estimate = -463.3009
Time for alignment: 350.0388
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 589.0349 - loglik: -5.8746e+02 - logprior: -1.5747e+00
Epoch 2/10
39/39 - 9s - loss: 537.9483 - loglik: -5.3738e+02 - logprior: -5.6792e-01
Epoch 3/10
39/39 - 9s - loss: 531.4771 - loglik: -5.3090e+02 - logprior: -5.7262e-01
Epoch 4/10
39/39 - 9s - loss: 529.0135 - loglik: -5.2845e+02 - logprior: -5.6677e-01
Epoch 5/10
39/39 - 9s - loss: 528.0878 - loglik: -5.2755e+02 - logprior: -5.4191e-01
Epoch 6/10
39/39 - 9s - loss: 527.7154 - loglik: -5.2718e+02 - logprior: -5.3947e-01
Epoch 7/10
39/39 - 9s - loss: 527.3383 - loglik: -5.2681e+02 - logprior: -5.3259e-01
Epoch 8/10
39/39 - 9s - loss: 527.0192 - loglik: -5.2648e+02 - logprior: -5.4079e-01
Epoch 9/10
39/39 - 9s - loss: 526.9462 - loglik: -5.2641e+02 - logprior: -5.3182e-01
Epoch 10/10
39/39 - 9s - loss: 526.7811 - loglik: -5.2625e+02 - logprior: -5.2769e-01
Fitted a model with MAP estimate = -483.8750
expansions: [(0, 3), (10, 1), (20, 1), (24, 6), (27, 3), (54, 1), (55, 2), (71, 2), (77, 4), (78, 1), (83, 2), (84, 2), (104, 1), (133, 2), (138, 8)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 527.8217 - loglik: -5.2525e+02 - logprior: -2.5764e+00
Epoch 2/2
39/39 - 12s - loss: 518.8165 - loglik: -5.1810e+02 - logprior: -7.1260e-01
Fitted a model with MAP estimate = -475.7234
expansions: [(87, 1), (156, 1)]
discards: [  0  26  71 108 163 164 165 166 167 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 527.0911 - loglik: -5.2465e+02 - logprior: -2.4432e+00
Epoch 2/2
39/39 - 11s - loss: 521.6545 - loglik: -5.2134e+02 - logprior: -3.1284e-01
Fitted a model with MAP estimate = -478.1635
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 19s - loss: 472.9706 - loglik: -4.7200e+02 - logprior: -9.7184e-01
Epoch 2/10
51/51 - 14s - loss: 469.7672 - loglik: -4.6917e+02 - logprior: -5.9602e-01
Epoch 3/10
51/51 - 14s - loss: 468.4806 - loglik: -4.6789e+02 - logprior: -5.9004e-01
Epoch 4/10
51/51 - 14s - loss: 466.5695 - loglik: -4.6602e+02 - logprior: -5.5178e-01
Epoch 5/10
51/51 - 14s - loss: 465.5824 - loglik: -4.6506e+02 - logprior: -5.1800e-01
Epoch 6/10
51/51 - 14s - loss: 466.3170 - loglik: -4.6584e+02 - logprior: -4.7582e-01
Fitted a model with MAP estimate = -465.5106
Time for alignment: 295.7117
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 590.6760 - loglik: -5.8909e+02 - logprior: -1.5846e+00
Epoch 2/10
39/39 - 9s - loss: 539.3934 - loglik: -5.3883e+02 - logprior: -5.6586e-01
Epoch 3/10
39/39 - 9s - loss: 531.6022 - loglik: -5.3103e+02 - logprior: -5.7108e-01
Epoch 4/10
39/39 - 9s - loss: 528.8122 - loglik: -5.2821e+02 - logprior: -5.9844e-01
Epoch 5/10
39/39 - 9s - loss: 527.7280 - loglik: -5.2715e+02 - logprior: -5.8267e-01
Epoch 6/10
39/39 - 9s - loss: 526.9727 - loglik: -5.2641e+02 - logprior: -5.6448e-01
Epoch 7/10
39/39 - 9s - loss: 526.9103 - loglik: -5.2635e+02 - logprior: -5.5929e-01
Epoch 8/10
39/39 - 9s - loss: 526.6284 - loglik: -5.2607e+02 - logprior: -5.5870e-01
Epoch 9/10
39/39 - 9s - loss: 526.3268 - loglik: -5.2577e+02 - logprior: -5.6123e-01
Epoch 10/10
39/39 - 9s - loss: 526.3618 - loglik: -5.2580e+02 - logprior: -5.6126e-01
Fitted a model with MAP estimate = -483.2433
expansions: [(0, 3), (10, 1), (20, 2), (24, 6), (27, 3), (56, 1), (57, 2), (69, 1), (79, 3), (85, 1), (88, 1), (92, 3), (110, 1), (124, 1), (125, 7), (126, 3), (127, 7), (138, 5)]
discards: [131 132 133 134 135 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 200 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 529.5724 - loglik: -5.2708e+02 - logprior: -2.4874e+00
Epoch 2/2
39/39 - 12s - loss: 519.4748 - loglik: -5.1881e+02 - logprior: -6.6580e-01
Fitted a model with MAP estimate = -475.6853
expansions: [(161, 2)]
discards: [  0  25  72 100 116 169 170 171 172 173 174 175 176 177 178 179]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 528.0943 - loglik: -5.2567e+02 - logprior: -2.4215e+00
Epoch 2/2
39/39 - 11s - loss: 522.1124 - loglik: -5.2184e+02 - logprior: -2.7537e-01
Fitted a model with MAP estimate = -477.4779
expansions: [(0, 2), (161, 3)]
discards: [  0  25 157]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 188 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 17s - loss: 473.2834 - loglik: -4.7230e+02 - logprior: -9.8423e-01
Epoch 2/10
51/51 - 14s - loss: 470.0480 - loglik: -4.6942e+02 - logprior: -6.2677e-01
Epoch 3/10
51/51 - 14s - loss: 468.5006 - loglik: -4.6790e+02 - logprior: -6.0456e-01
Epoch 4/10
51/51 - 14s - loss: 466.6770 - loglik: -4.6609e+02 - logprior: -5.8259e-01
Epoch 5/10
51/51 - 14s - loss: 466.2681 - loglik: -4.6572e+02 - logprior: -5.5025e-01
Epoch 6/10
51/51 - 14s - loss: 466.0609 - loglik: -4.6555e+02 - logprior: -5.1361e-01
Epoch 7/10
51/51 - 14s - loss: 465.5236 - loglik: -4.6505e+02 - logprior: -4.7798e-01
Epoch 8/10
51/51 - 14s - loss: 464.9857 - loglik: -4.6454e+02 - logprior: -4.4726e-01
Epoch 9/10
51/51 - 14s - loss: 465.2377 - loglik: -4.6483e+02 - logprior: -4.0964e-01
Fitted a model with MAP estimate = -465.1264
Time for alignment: 340.3113
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 590.8081 - loglik: -5.8922e+02 - logprior: -1.5836e+00
Epoch 2/10
39/39 - 9s - loss: 538.5322 - loglik: -5.3800e+02 - logprior: -5.2753e-01
Epoch 3/10
39/39 - 9s - loss: 530.3754 - loglik: -5.2981e+02 - logprior: -5.6659e-01
Epoch 4/10
39/39 - 9s - loss: 527.8928 - loglik: -5.2733e+02 - logprior: -5.6160e-01
Epoch 5/10
39/39 - 9s - loss: 526.5538 - loglik: -5.2600e+02 - logprior: -5.4939e-01
Epoch 6/10
39/39 - 9s - loss: 525.9807 - loglik: -5.2545e+02 - logprior: -5.2800e-01
Epoch 7/10
39/39 - 9s - loss: 525.6065 - loglik: -5.2508e+02 - logprior: -5.2437e-01
Epoch 8/10
39/39 - 9s - loss: 525.5698 - loglik: -5.2505e+02 - logprior: -5.1504e-01
Epoch 9/10
39/39 - 9s - loss: 525.2392 - loglik: -5.2473e+02 - logprior: -5.1117e-01
Epoch 10/10
39/39 - 9s - loss: 525.2350 - loglik: -5.2473e+02 - logprior: -5.0620e-01
Fitted a model with MAP estimate = -482.5548
expansions: [(0, 3), (14, 2), (24, 7), (27, 3), (54, 1), (55, 2), (75, 2), (76, 1), (110, 1), (111, 1), (123, 1), (124, 2), (125, 4), (126, 5), (127, 4), (138, 8)]
discards: [130 131 132 133 134 135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 529.0872 - loglik: -5.2657e+02 - logprior: -2.5187e+00
Epoch 2/2
39/39 - 12s - loss: 518.7638 - loglik: -5.1806e+02 - logprior: -7.0663e-01
Fitted a model with MAP estimate = -475.7026
expansions: [(94, 1), (95, 1), (153, 1), (154, 2), (161, 3), (162, 2), (176, 1)]
discards: [  0  17  26  72 146 147 148 149 150 151 168 169]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 525.2867 - loglik: -5.2284e+02 - logprior: -2.4509e+00
Epoch 2/2
39/39 - 12s - loss: 519.0434 - loglik: -5.1865e+02 - logprior: -3.9181e-01
Fitted a model with MAP estimate = -475.6674
expansions: [(0, 2), (24, 2), (146, 2), (157, 1), (159, 2)]
discards: [  0 144 162 163 164 165 166 167 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 19s - loss: 473.5152 - loglik: -4.7250e+02 - logprior: -1.0125e+00
Epoch 2/10
51/51 - 14s - loss: 469.1967 - loglik: -4.6854e+02 - logprior: -6.6136e-01
Epoch 3/10
51/51 - 14s - loss: 467.4115 - loglik: -4.6675e+02 - logprior: -6.6016e-01
Epoch 4/10
51/51 - 14s - loss: 465.4273 - loglik: -4.6482e+02 - logprior: -6.0962e-01
Epoch 5/10
51/51 - 15s - loss: 464.8066 - loglik: -4.6424e+02 - logprior: -5.7068e-01
Epoch 6/10
51/51 - 15s - loss: 465.6127 - loglik: -4.6507e+02 - logprior: -5.4188e-01
Fitted a model with MAP estimate = -464.5632
Time for alignment: 303.7211
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 590.3960 - loglik: -5.8881e+02 - logprior: -1.5817e+00
Epoch 2/10
39/39 - 9s - loss: 544.2962 - loglik: -5.4371e+02 - logprior: -5.8462e-01
Epoch 3/10
39/39 - 9s - loss: 535.8612 - loglik: -5.3516e+02 - logprior: -7.0221e-01
Epoch 4/10
39/39 - 9s - loss: 532.6821 - loglik: -5.3198e+02 - logprior: -6.9972e-01
Epoch 5/10
39/39 - 9s - loss: 531.5873 - loglik: -5.3090e+02 - logprior: -6.8900e-01
Epoch 6/10
39/39 - 9s - loss: 531.1558 - loglik: -5.3048e+02 - logprior: -6.7606e-01
Epoch 7/10
39/39 - 9s - loss: 530.7551 - loglik: -5.3009e+02 - logprior: -6.6942e-01
Epoch 8/10
39/39 - 9s - loss: 530.3638 - loglik: -5.2972e+02 - logprior: -6.4854e-01
Epoch 9/10
39/39 - 9s - loss: 530.2283 - loglik: -5.2958e+02 - logprior: -6.4996e-01
Epoch 10/10
39/39 - 9s - loss: 530.1940 - loglik: -5.2954e+02 - logprior: -6.5285e-01
Fitted a model with MAP estimate = -487.1676
expansions: [(0, 3), (10, 1), (24, 7), (55, 1), (56, 2), (57, 1), (72, 2), (74, 11), (79, 2), (88, 3), (111, 5), (124, 3), (125, 2), (126, 7), (138, 5)]
discards: [ 94  95  96  97  98  99 101 127 128 129 130 131 132 133 134 135 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 534.3466 - loglik: -5.3178e+02 - logprior: -2.5661e+00
Epoch 2/2
39/39 - 11s - loss: 522.3672 - loglik: -5.2166e+02 - logprior: -7.0589e-01
Fitted a model with MAP estimate = -477.9371
expansions: [(38, 1), (39, 1), (137, 1), (138, 2), (140, 1), (141, 1)]
discards: [  0  25  69  95  96  97  98  99 108 118 164 165 166 167 168 169 170 171]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 530.3656 - loglik: -5.2799e+02 - logprior: -2.3761e+00
Epoch 2/2
39/39 - 11s - loss: 523.9539 - loglik: -5.2369e+02 - logprior: -2.6868e-01
Fitted a model with MAP estimate = -478.7922
expansions: [(0, 2), (24, 2), (29, 2), (161, 5)]
discards: [  0 132 152]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 190 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 18s - loss: 472.9719 - loglik: -4.7202e+02 - logprior: -9.5416e-01
Epoch 2/10
51/51 - 14s - loss: 469.7835 - loglik: -4.6918e+02 - logprior: -6.0346e-01
Epoch 3/10
51/51 - 14s - loss: 467.6830 - loglik: -4.6708e+02 - logprior: -5.9961e-01
Epoch 4/10
51/51 - 14s - loss: 466.4967 - loglik: -4.6594e+02 - logprior: -5.5926e-01
Epoch 5/10
51/51 - 14s - loss: 465.3443 - loglik: -4.6482e+02 - logprior: -5.2735e-01
Epoch 6/10
51/51 - 14s - loss: 466.1708 - loglik: -4.6568e+02 - logprior: -4.8736e-01
Fitted a model with MAP estimate = -465.4000
Time for alignment: 295.1748
Computed alignments with likelihoods: ['-463.3009', '-465.5106', '-465.1264', '-464.5632', '-465.4000']
Best model has likelihood: -463.3009  (prior= -0.4369 )
time for generating output: 0.2543
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.682529335071708
Training of 5 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f1f0b318fa0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f1f0b318af0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318d30>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318f40>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318ca0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318be0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f1f0b318880>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b3189a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b20>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b50>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318970>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c70>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318b80>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318df0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318820>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a30>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318c40>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1f0b318a90>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1dffacf2b0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1dffacf340>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1dffacf370> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f1df01640d0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1e87018430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f201ab929d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1ed3d2f820>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f1f1612d790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f1f0aae23a0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1dffacf2e0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1edc8728b0>
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 495.2494 - loglik: -4.8931e+02 - logprior: -5.9435e+00
Epoch 2/10
14/14 - 3s - loss: 446.8055 - loglik: -4.4558e+02 - logprior: -1.2205e+00
Epoch 3/10
14/14 - 3s - loss: 411.4961 - loglik: -4.1022e+02 - logprior: -1.2785e+00
Epoch 4/10
14/14 - 3s - loss: 397.9397 - loglik: -3.9663e+02 - logprior: -1.3068e+00
Epoch 5/10
14/14 - 3s - loss: 391.4011 - loglik: -3.9014e+02 - logprior: -1.2597e+00
Epoch 6/10
14/14 - 3s - loss: 389.6924 - loglik: -3.8846e+02 - logprior: -1.2309e+00
Epoch 7/10
14/14 - 3s - loss: 387.5713 - loglik: -3.8637e+02 - logprior: -1.1998e+00
Epoch 8/10
14/14 - 3s - loss: 388.0720 - loglik: -3.8690e+02 - logprior: -1.1739e+00
Fitted a model with MAP estimate = -387.0334
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 2), (25, 1), (26, 1), (33, 1), (49, 1), (52, 2), (54, 2), (59, 1), (64, 2), (67, 2), (90, 3), (96, 1), (102, 5), (115, 1), (117, 1), (124, 1), (129, 2), (131, 1), (133, 1), (134, 1), (135, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 383.6959 - loglik: -3.7941e+02 - logprior: -4.2852e+00
Epoch 2/2
29/29 - 5s - loss: 370.6370 - loglik: -3.6971e+02 - logprior: -9.2464e-01
Fitted a model with MAP estimate = -368.7144
expansions: [(160, 2)]
discards: [ 30  82  86 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 373.8627 - loglik: -3.7082e+02 - logprior: -3.0465e+00
Epoch 2/2
29/29 - 5s - loss: 368.3934 - loglik: -3.6787e+02 - logprior: -5.1934e-01
Fitted a model with MAP estimate = -367.0541
expansions: [(91, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 370.9723 - loglik: -3.6804e+02 - logprior: -2.9363e+00
Epoch 2/10
29/29 - 6s - loss: 366.6002 - loglik: -3.6614e+02 - logprior: -4.5632e-01
Epoch 3/10
29/29 - 5s - loss: 365.8629 - loglik: -3.6559e+02 - logprior: -2.7403e-01
Epoch 4/10
29/29 - 6s - loss: 364.0875 - loglik: -3.6387e+02 - logprior: -2.1560e-01
Epoch 5/10
29/29 - 5s - loss: 363.8660 - loglik: -3.6373e+02 - logprior: -1.3501e-01
Epoch 6/10
29/29 - 5s - loss: 362.8734 - loglik: -3.6283e+02 - logprior: -4.8454e-02
Epoch 7/10
29/29 - 5s - loss: 362.5986 - loglik: -3.6263e+02 - logprior: 0.0344
Epoch 8/10
29/29 - 5s - loss: 361.9717 - loglik: -3.6209e+02 - logprior: 0.1206
Epoch 9/10
29/29 - 5s - loss: 361.9990 - loglik: -3.6221e+02 - logprior: 0.2085
Fitted a model with MAP estimate = -361.7439
Time for alignment: 134.9741
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 10s - loss: 495.7191 - loglik: -4.8978e+02 - logprior: -5.9418e+00
Epoch 2/10
14/14 - 3s - loss: 447.2076 - loglik: -4.4601e+02 - logprior: -1.1980e+00
Epoch 3/10
14/14 - 4s - loss: 410.4868 - loglik: -4.0923e+02 - logprior: -1.2602e+00
Epoch 4/10
14/14 - 3s - loss: 396.1703 - loglik: -3.9487e+02 - logprior: -1.2969e+00
Epoch 5/10
14/14 - 3s - loss: 391.5641 - loglik: -3.9033e+02 - logprior: -1.2331e+00
Epoch 6/10
14/14 - 4s - loss: 390.1733 - loglik: -3.8898e+02 - logprior: -1.1929e+00
Epoch 7/10
14/14 - 3s - loss: 387.4182 - loglik: -3.8627e+02 - logprior: -1.1503e+00
Epoch 8/10
14/14 - 3s - loss: 387.9400 - loglik: -3.8680e+02 - logprior: -1.1362e+00
Fitted a model with MAP estimate = -387.2930
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 2), (25, 1), (26, 1), (33, 1), (49, 1), (52, 2), (54, 2), (63, 1), (64, 2), (67, 2), (90, 3), (102, 1), (103, 4), (115, 1), (117, 1), (118, 1), (128, 2), (129, 1), (132, 2), (133, 2), (134, 1), (135, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 382.9191 - loglik: -3.7858e+02 - logprior: -4.3428e+00
Epoch 2/2
29/29 - 6s - loss: 369.2070 - loglik: -3.6819e+02 - logprior: -1.0139e+00
Fitted a model with MAP estimate = -366.2732
expansions: [(161, 1)]
discards: [ 30  82 113 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 372.5167 - loglik: -3.6937e+02 - logprior: -3.1512e+00
Epoch 2/2
29/29 - 6s - loss: 367.0387 - loglik: -3.6650e+02 - logprior: -5.3899e-01
Fitted a model with MAP estimate = -365.3052
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 370.5476 - loglik: -3.6750e+02 - logprior: -3.0434e+00
Epoch 2/10
29/29 - 5s - loss: 366.0966 - loglik: -3.6572e+02 - logprior: -3.7703e-01
Epoch 3/10
29/29 - 5s - loss: 365.2181 - loglik: -3.6501e+02 - logprior: -2.1014e-01
Epoch 4/10
29/29 - 6s - loss: 363.0370 - loglik: -3.6288e+02 - logprior: -1.5409e-01
Epoch 5/10
29/29 - 6s - loss: 362.9225 - loglik: -3.6285e+02 - logprior: -7.5748e-02
Epoch 6/10
29/29 - 5s - loss: 361.9492 - loglik: -3.6197e+02 - logprior: 0.0158
Epoch 7/10
29/29 - 6s - loss: 361.6380 - loglik: -3.6174e+02 - logprior: 0.1039
Epoch 8/10
29/29 - 6s - loss: 360.8471 - loglik: -3.6103e+02 - logprior: 0.1848
Epoch 9/10
29/29 - 5s - loss: 362.0925 - loglik: -3.6237e+02 - logprior: 0.2792
Fitted a model with MAP estimate = -360.8825
Time for alignment: 142.3541
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 495.3316 - loglik: -4.8939e+02 - logprior: -5.9390e+00
Epoch 2/10
14/14 - 3s - loss: 446.8088 - loglik: -4.4560e+02 - logprior: -1.2095e+00
Epoch 3/10
14/14 - 3s - loss: 409.0952 - loglik: -4.0783e+02 - logprior: -1.2690e+00
Epoch 4/10
14/14 - 3s - loss: 394.7369 - loglik: -3.9345e+02 - logprior: -1.2891e+00
Epoch 5/10
14/14 - 3s - loss: 389.9115 - loglik: -3.8870e+02 - logprior: -1.2096e+00
Epoch 6/10
14/14 - 3s - loss: 386.8743 - loglik: -3.8570e+02 - logprior: -1.1722e+00
Epoch 7/10
14/14 - 3s - loss: 385.3613 - loglik: -3.8422e+02 - logprior: -1.1367e+00
Epoch 8/10
14/14 - 3s - loss: 385.5135 - loglik: -3.8439e+02 - logprior: -1.1254e+00
Fitted a model with MAP estimate = -384.8610
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 2), (25, 1), (26, 1), (33, 1), (52, 3), (54, 1), (63, 1), (64, 2), (67, 2), (77, 1), (90, 3), (97, 1), (103, 3), (117, 1), (124, 1), (129, 2), (131, 1), (133, 1), (134, 1), (135, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 383.5316 - loglik: -3.7921e+02 - logprior: -4.3220e+00
Epoch 2/2
29/29 - 5s - loss: 369.0798 - loglik: -3.6810e+02 - logprior: -9.8184e-01
Fitted a model with MAP estimate = -367.4878
expansions: [(65, 1), (128, 2), (129, 1), (157, 2)]
discards: [ 30  81 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 371.2615 - loglik: -3.6826e+02 - logprior: -2.9972e+00
Epoch 2/2
29/29 - 5s - loss: 364.6642 - loglik: -3.6398e+02 - logprior: -6.8035e-01
Fitted a model with MAP estimate = -363.0255
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 367.8629 - loglik: -3.6486e+02 - logprior: -3.0026e+00
Epoch 2/10
29/29 - 5s - loss: 364.3577 - loglik: -3.6384e+02 - logprior: -5.1406e-01
Epoch 3/10
29/29 - 5s - loss: 361.9185 - loglik: -3.6157e+02 - logprior: -3.4809e-01
Epoch 4/10
29/29 - 5s - loss: 360.8166 - loglik: -3.6053e+02 - logprior: -2.8261e-01
Epoch 5/10
29/29 - 5s - loss: 360.0367 - loglik: -3.5983e+02 - logprior: -2.0264e-01
Epoch 6/10
29/29 - 6s - loss: 359.8235 - loglik: -3.5970e+02 - logprior: -1.1928e-01
Epoch 7/10
29/29 - 5s - loss: 359.9510 - loglik: -3.5991e+02 - logprior: -3.6295e-02
Fitted a model with MAP estimate = -359.0757
Time for alignment: 127.0799
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 495.2490 - loglik: -4.8929e+02 - logprior: -5.9604e+00
Epoch 2/10
14/14 - 3s - loss: 446.0285 - loglik: -4.4479e+02 - logprior: -1.2348e+00
Epoch 3/10
14/14 - 3s - loss: 409.7689 - loglik: -4.0851e+02 - logprior: -1.2579e+00
Epoch 4/10
14/14 - 3s - loss: 396.2277 - loglik: -3.9497e+02 - logprior: -1.2573e+00
Epoch 5/10
14/14 - 3s - loss: 392.2189 - loglik: -3.9106e+02 - logprior: -1.1601e+00
Epoch 6/10
14/14 - 3s - loss: 391.5854 - loglik: -3.9044e+02 - logprior: -1.1470e+00
Epoch 7/10
14/14 - 3s - loss: 388.2512 - loglik: -3.8713e+02 - logprior: -1.1182e+00
Epoch 8/10
14/14 - 3s - loss: 387.6222 - loglik: -3.8650e+02 - logprior: -1.1206e+00
Epoch 9/10
14/14 - 3s - loss: 386.5225 - loglik: -3.8538e+02 - logprior: -1.1409e+00
Epoch 10/10
14/14 - 3s - loss: 386.0107 - loglik: -3.8486e+02 - logprior: -1.1505e+00
Fitted a model with MAP estimate = -386.1473
expansions: [(0, 2), (9, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (33, 1), (52, 2), (54, 1), (63, 1), (64, 2), (67, 2), (90, 3), (96, 1), (102, 3), (115, 1), (118, 2), (130, 1), (132, 3), (133, 2), (134, 1), (135, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 386.5424 - loglik: -3.8223e+02 - logprior: -4.3160e+00
Epoch 2/2
29/29 - 6s - loss: 371.2536 - loglik: -3.7023e+02 - logprior: -1.0200e+00
Fitted a model with MAP estimate = -369.9046
expansions: [(62, 1), (63, 1), (128, 1)]
discards: [ 79 110 162 165]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 375.1818 - loglik: -3.7210e+02 - logprior: -3.0819e+00
Epoch 2/2
29/29 - 5s - loss: 370.2129 - loglik: -3.6963e+02 - logprior: -5.8572e-01
Fitted a model with MAP estimate = -368.0097
expansions: [(92, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 372.5154 - loglik: -3.6955e+02 - logprior: -2.9634e+00
Epoch 2/10
29/29 - 5s - loss: 367.7475 - loglik: -3.6732e+02 - logprior: -4.2468e-01
Epoch 3/10
29/29 - 5s - loss: 365.9382 - loglik: -3.6567e+02 - logprior: -2.6619e-01
Epoch 4/10
29/29 - 5s - loss: 365.2777 - loglik: -3.6508e+02 - logprior: -1.9272e-01
Epoch 5/10
29/29 - 5s - loss: 363.3234 - loglik: -3.6321e+02 - logprior: -1.1318e-01
Epoch 6/10
29/29 - 5s - loss: 364.8846 - loglik: -3.6485e+02 - logprior: -3.1944e-02
Fitted a model with MAP estimate = -363.4407
Time for alignment: 126.6646
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 495.0657 - loglik: -4.8912e+02 - logprior: -5.9414e+00
Epoch 2/10
14/14 - 3s - loss: 446.6704 - loglik: -4.4547e+02 - logprior: -1.2034e+00
Epoch 3/10
14/14 - 3s - loss: 410.1248 - loglik: -4.0887e+02 - logprior: -1.2524e+00
Epoch 4/10
14/14 - 3s - loss: 395.3692 - loglik: -3.9410e+02 - logprior: -1.2664e+00
Epoch 5/10
14/14 - 3s - loss: 391.0070 - loglik: -3.8976e+02 - logprior: -1.2456e+00
Epoch 6/10
14/14 - 3s - loss: 389.0137 - loglik: -3.8779e+02 - logprior: -1.2234e+00
Epoch 7/10
14/14 - 3s - loss: 387.0065 - loglik: -3.8580e+02 - logprior: -1.2073e+00
Epoch 8/10
14/14 - 3s - loss: 386.4987 - loglik: -3.8531e+02 - logprior: -1.1898e+00
Epoch 9/10
14/14 - 3s - loss: 385.5625 - loglik: -3.8438e+02 - logprior: -1.1821e+00
Epoch 10/10
14/14 - 3s - loss: 386.1101 - loglik: -3.8494e+02 - logprior: -1.1711e+00
Fitted a model with MAP estimate = -385.6285
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (52, 2), (63, 1), (64, 2), (67, 2), (77, 1), (90, 3), (97, 1), (102, 3), (103, 1), (115, 1), (117, 1), (118, 1), (130, 1), (132, 3), (133, 2), (134, 1), (135, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 383.7698 - loglik: -3.7943e+02 - logprior: -4.3437e+00
Epoch 2/2
29/29 - 6s - loss: 370.2060 - loglik: -3.6925e+02 - logprior: -9.5290e-01
Fitted a model with MAP estimate = -368.0235
expansions: [(73, 1), (75, 1)]
discards: [ 41  80 112 165 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 373.4412 - loglik: -3.7039e+02 - logprior: -3.0499e+00
Epoch 2/2
29/29 - 5s - loss: 368.5443 - loglik: -3.6809e+02 - logprior: -4.5105e-01
Fitted a model with MAP estimate = -366.8072
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 371.9984 - loglik: -3.6910e+02 - logprior: -2.9024e+00
Epoch 2/10
29/29 - 5s - loss: 367.1404 - loglik: -3.6685e+02 - logprior: -2.8865e-01
Epoch 3/10
29/29 - 5s - loss: 366.8154 - loglik: -3.6669e+02 - logprior: -1.2568e-01
Epoch 4/10
29/29 - 5s - loss: 364.5916 - loglik: -3.6452e+02 - logprior: -7.6471e-02
Epoch 5/10
29/29 - 5s - loss: 364.5533 - loglik: -3.6458e+02 - logprior: 0.0227
Epoch 6/10
29/29 - 5s - loss: 363.8538 - loglik: -3.6396e+02 - logprior: 0.1076
Epoch 7/10
29/29 - 5s - loss: 363.3744 - loglik: -3.6357e+02 - logprior: 0.1940
Epoch 8/10
29/29 - 5s - loss: 362.5067 - loglik: -3.6279e+02 - logprior: 0.2831
Epoch 9/10
29/29 - 5s - loss: 363.6860 - loglik: -3.6405e+02 - logprior: 0.3617
Fitted a model with MAP estimate = -362.5351
Time for alignment: 144.9703
Computed alignments with likelihoods: ['-361.7439', '-360.8825', '-359.0757', '-363.4407', '-362.5351']
Best model has likelihood: -359.0757  (prior= -0.0023 )
time for generating output: 0.2158
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.8311807063664419
