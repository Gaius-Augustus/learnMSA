Training of 5 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5ac88340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5ac885e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5ac885b0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 538.7536 - loglik: -5.3390e+02 - logprior: -4.8489e+00
Epoch 2/10
25/25 - 6s - loss: 432.5674 - loglik: -4.3094e+02 - logprior: -1.6268e+00
Epoch 3/10
25/25 - 6s - loss: 411.2986 - loglik: -4.0950e+02 - logprior: -1.7967e+00
Epoch 4/10
25/25 - 6s - loss: 406.2187 - loglik: -4.0452e+02 - logprior: -1.6973e+00
Epoch 5/10
25/25 - 6s - loss: 405.5948 - loglik: -4.0390e+02 - logprior: -1.6958e+00
Epoch 6/10
25/25 - 6s - loss: 406.3676 - loglik: -4.0465e+02 - logprior: -1.7134e+00
Fitted a model with MAP estimate = -404.8436
expansions: [(9, 2), (10, 1), (11, 3), (12, 1), (31, 1), (32, 1), (33, 2), (34, 2), (47, 3), (58, 1), (60, 1), (62, 2), (76, 1), (81, 2), (82, 2), (83, 2), (90, 2), (92, 1), (96, 1), (98, 1), (102, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (149, 1), (159, 1), (167, 1), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 402.0911 - loglik: -3.9528e+02 - logprior: -6.8137e+00
Epoch 2/2
25/25 - 8s - loss: 389.9392 - loglik: -3.8743e+02 - logprior: -2.5060e+00
Fitted a model with MAP estimate = -385.7210
expansions: [(0, 3)]
discards: [  0  14  44  61  81 102 144 178]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 393.9286 - loglik: -3.8953e+02 - logprior: -4.3954e+00
Epoch 2/2
25/25 - 7s - loss: 385.6171 - loglik: -3.8508e+02 - logprior: -5.4105e-01
Fitted a model with MAP estimate = -384.6977
expansions: []
discards: [  0   2 104]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 395.7316 - loglik: -3.8930e+02 - logprior: -6.4302e+00
Epoch 2/10
25/25 - 7s - loss: 387.8394 - loglik: -3.8659e+02 - logprior: -1.2449e+00
Epoch 3/10
25/25 - 7s - loss: 384.6351 - loglik: -3.8487e+02 - logprior: 0.2349
Epoch 4/10
25/25 - 7s - loss: 383.0881 - loglik: -3.8340e+02 - logprior: 0.3091
Epoch 5/10
25/25 - 7s - loss: 382.8570 - loglik: -3.8326e+02 - logprior: 0.3993
Epoch 6/10
25/25 - 7s - loss: 382.4680 - loglik: -3.8296e+02 - logprior: 0.4925
Epoch 7/10
25/25 - 7s - loss: 380.0724 - loglik: -3.8064e+02 - logprior: 0.5710
Epoch 8/10
25/25 - 7s - loss: 382.1959 - loglik: -3.8288e+02 - logprior: 0.6817
Fitted a model with MAP estimate = -381.1422
Time for alignment: 162.6216
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 540.8672 - loglik: -5.3597e+02 - logprior: -4.8954e+00
Epoch 2/10
25/25 - 6s - loss: 431.3652 - loglik: -4.2975e+02 - logprior: -1.6177e+00
Epoch 3/10
25/25 - 6s - loss: 409.4866 - loglik: -4.0787e+02 - logprior: -1.6202e+00
Epoch 4/10
25/25 - 6s - loss: 409.2530 - loglik: -4.0774e+02 - logprior: -1.5102e+00
Epoch 5/10
25/25 - 6s - loss: 406.5492 - loglik: -4.0504e+02 - logprior: -1.5066e+00
Epoch 6/10
25/25 - 6s - loss: 406.2334 - loglik: -4.0471e+02 - logprior: -1.5249e+00
Epoch 7/10
25/25 - 6s - loss: 405.4559 - loglik: -4.0393e+02 - logprior: -1.5218e+00
Epoch 8/10
25/25 - 6s - loss: 403.9702 - loglik: -4.0245e+02 - logprior: -1.5242e+00
Epoch 9/10
25/25 - 6s - loss: 405.6135 - loglik: -4.0410e+02 - logprior: -1.5183e+00
Fitted a model with MAP estimate = -405.0226
expansions: [(9, 3), (10, 1), (12, 1), (16, 2), (31, 3), (32, 2), (34, 2), (47, 3), (57, 1), (60, 1), (62, 2), (76, 1), (81, 1), (83, 1), (84, 2), (91, 2), (93, 1), (96, 1), (102, 1), (112, 1), (113, 2), (122, 2), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (154, 1), (162, 2), (168, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 405.5490 - loglik: -3.9873e+02 - logprior: -6.8191e+00
Epoch 2/2
25/25 - 7s - loss: 391.9046 - loglik: -3.8928e+02 - logprior: -2.6204e+00
Fitted a model with MAP estimate = -388.1126
expansions: [(0, 3), (214, 1)]
discards: [  8  40  45  61  62  81 142 155 177 206]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 393.7425 - loglik: -3.8938e+02 - logprior: -4.3645e+00
Epoch 2/2
25/25 - 7s - loss: 387.0881 - loglik: -3.8660e+02 - logprior: -4.9272e-01
Fitted a model with MAP estimate = -385.0918
expansions: [(60, 2)]
discards: [  0   2 104]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 394.5822 - loglik: -3.8820e+02 - logprior: -6.3786e+00
Epoch 2/10
25/25 - 7s - loss: 389.6660 - loglik: -3.8822e+02 - logprior: -1.4485e+00
Epoch 3/10
25/25 - 7s - loss: 384.8364 - loglik: -3.8514e+02 - logprior: 0.3062
Epoch 4/10
25/25 - 7s - loss: 383.2321 - loglik: -3.8360e+02 - logprior: 0.3717
Epoch 5/10
25/25 - 7s - loss: 381.1617 - loglik: -3.8161e+02 - logprior: 0.4503
Epoch 6/10
25/25 - 7s - loss: 382.7974 - loglik: -3.8334e+02 - logprior: 0.5416
Fitted a model with MAP estimate = -381.3809
Time for alignment: 164.0330
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 537.0648 - loglik: -5.3223e+02 - logprior: -4.8368e+00
Epoch 2/10
25/25 - 6s - loss: 432.5777 - loglik: -4.3112e+02 - logprior: -1.4548e+00
Epoch 3/10
25/25 - 6s - loss: 412.9333 - loglik: -4.1138e+02 - logprior: -1.5485e+00
Epoch 4/10
25/25 - 6s - loss: 410.1008 - loglik: -4.0867e+02 - logprior: -1.4356e+00
Epoch 5/10
25/25 - 6s - loss: 407.9810 - loglik: -4.0653e+02 - logprior: -1.4499e+00
Epoch 6/10
25/25 - 6s - loss: 406.4196 - loglik: -4.0495e+02 - logprior: -1.4722e+00
Epoch 7/10
25/25 - 6s - loss: 407.1446 - loglik: -4.0561e+02 - logprior: -1.5319e+00
Fitted a model with MAP estimate = -406.2508
expansions: [(9, 4), (11, 1), (31, 3), (32, 2), (34, 2), (47, 3), (55, 1), (57, 1), (59, 1), (74, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (95, 1), (98, 1), (101, 1), (111, 1), (112, 2), (123, 2), (125, 2), (126, 1), (136, 1), (139, 2), (159, 1), (168, 2), (169, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 402.9525 - loglik: -3.9623e+02 - logprior: -6.7231e+00
Epoch 2/2
25/25 - 7s - loss: 389.4224 - loglik: -3.8698e+02 - logprior: -2.4445e+00
Fitted a model with MAP estimate = -386.1726
expansions: [(0, 3)]
discards: [  0  43  60 141]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 394.4155 - loglik: -3.9004e+02 - logprior: -4.3747e+00
Epoch 2/2
25/25 - 7s - loss: 384.3268 - loglik: -3.8381e+02 - logprior: -5.1444e-01
Fitted a model with MAP estimate = -384.4104
expansions: []
discards: [  0   2 105]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 394.4225 - loglik: -3.8802e+02 - logprior: -6.4057e+00
Epoch 2/10
25/25 - 7s - loss: 388.8546 - loglik: -3.8753e+02 - logprior: -1.3287e+00
Epoch 3/10
25/25 - 7s - loss: 385.0950 - loglik: -3.8536e+02 - logprior: 0.2676
Epoch 4/10
25/25 - 7s - loss: 381.6801 - loglik: -3.8203e+02 - logprior: 0.3499
Epoch 5/10
25/25 - 7s - loss: 384.3868 - loglik: -3.8482e+02 - logprior: 0.4320
Fitted a model with MAP estimate = -381.8467
Time for alignment: 145.2068
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 538.2798 - loglik: -5.3341e+02 - logprior: -4.8695e+00
Epoch 2/10
25/25 - 6s - loss: 429.6088 - loglik: -4.2809e+02 - logprior: -1.5219e+00
Epoch 3/10
25/25 - 6s - loss: 411.1415 - loglik: -4.0951e+02 - logprior: -1.6312e+00
Epoch 4/10
25/25 - 6s - loss: 407.7985 - loglik: -4.0626e+02 - logprior: -1.5349e+00
Epoch 5/10
25/25 - 6s - loss: 405.3220 - loglik: -4.0374e+02 - logprior: -1.5812e+00
Epoch 6/10
25/25 - 6s - loss: 405.5558 - loglik: -4.0393e+02 - logprior: -1.6235e+00
Fitted a model with MAP estimate = -405.0644
expansions: [(9, 5), (10, 2), (31, 3), (32, 2), (36, 1), (48, 1), (50, 1), (58, 1), (60, 1), (62, 2), (76, 1), (81, 1), (83, 1), (84, 2), (86, 1), (90, 2), (92, 1), (96, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (160, 1), (162, 2), (168, 2), (169, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 402.3189 - loglik: -3.9548e+02 - logprior: -6.8391e+00
Epoch 2/2
25/25 - 8s - loss: 389.5442 - loglik: -3.8709e+02 - logprior: -2.4561e+00
Fitted a model with MAP estimate = -385.4758
expansions: [(0, 3)]
discards: [  0  11  41  80 106 142 176 205]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 392.6905 - loglik: -3.8830e+02 - logprior: -4.3921e+00
Epoch 2/2
25/25 - 7s - loss: 386.4529 - loglik: -3.8595e+02 - logprior: -5.0006e-01
Fitted a model with MAP estimate = -384.7337
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 394.2920 - loglik: -3.8790e+02 - logprior: -6.3870e+00
Epoch 2/10
25/25 - 7s - loss: 390.1169 - loglik: -3.8875e+02 - logprior: -1.3652e+00
Epoch 3/10
25/25 - 7s - loss: 383.8402 - loglik: -3.8409e+02 - logprior: 0.2503
Epoch 4/10
25/25 - 7s - loss: 382.8084 - loglik: -3.8311e+02 - logprior: 0.3006
Epoch 5/10
25/25 - 7s - loss: 383.1616 - loglik: -3.8354e+02 - logprior: 0.3814
Fitted a model with MAP estimate = -381.6994
Time for alignment: 140.0780
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 8s - loss: 538.6345 - loglik: -5.3375e+02 - logprior: -4.8846e+00
Epoch 2/10
25/25 - 6s - loss: 430.6012 - loglik: -4.2885e+02 - logprior: -1.7465e+00
Epoch 3/10
25/25 - 6s - loss: 410.5025 - loglik: -4.0866e+02 - logprior: -1.8472e+00
Epoch 4/10
25/25 - 6s - loss: 406.5464 - loglik: -4.0476e+02 - logprior: -1.7822e+00
Epoch 5/10
25/25 - 6s - loss: 403.6249 - loglik: -4.0178e+02 - logprior: -1.8452e+00
Epoch 6/10
25/25 - 6s - loss: 405.2341 - loglik: -4.0337e+02 - logprior: -1.8655e+00
Fitted a model with MAP estimate = -403.6922
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 2), (35, 2), (36, 1), (47, 3), (58, 1), (60, 1), (62, 2), (76, 1), (81, 2), (82, 2), (83, 2), (90, 2), (92, 1), (96, 1), (98, 1), (101, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (149, 1), (159, 1), (167, 1), (168, 1), (169, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 221 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 402.0879 - loglik: -3.9526e+02 - logprior: -6.8329e+00
Epoch 2/2
25/25 - 8s - loss: 390.1374 - loglik: -3.8765e+02 - logprior: -2.4900e+00
Fitted a model with MAP estimate = -385.8978
expansions: [(0, 3)]
discards: [  0   9  42  62  63  82 103 145 179]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 393.0532 - loglik: -3.8863e+02 - logprior: -4.4235e+00
Epoch 2/2
25/25 - 7s - loss: 387.7522 - loglik: -3.8719e+02 - logprior: -5.6437e-01
Fitted a model with MAP estimate = -384.9583
expansions: [(61, 2)]
discards: [  0   2  46 104]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 394.1242 - loglik: -3.8770e+02 - logprior: -6.4203e+00
Epoch 2/10
25/25 - 7s - loss: 388.9608 - loglik: -3.8757e+02 - logprior: -1.3908e+00
Epoch 3/10
25/25 - 7s - loss: 385.1037 - loglik: -3.8537e+02 - logprior: 0.2641
Epoch 4/10
25/25 - 7s - loss: 382.5038 - loglik: -3.8285e+02 - logprior: 0.3421
Epoch 5/10
25/25 - 7s - loss: 381.0649 - loglik: -3.8147e+02 - logprior: 0.4060
Epoch 6/10
25/25 - 7s - loss: 381.6994 - loglik: -3.8220e+02 - logprior: 0.5043
Fitted a model with MAP estimate = -381.0628
Time for alignment: 147.9263
Computed alignments with likelihoods: ['-381.1422', '-381.3809', '-381.8467', '-381.6994', '-381.0628']
Best model has likelihood: -381.0628  (prior= 0.5374 )
time for generating output: 0.3005
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9297085476365418
Training of 5 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feafd28b730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49b5db50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb4965a880>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 406.4210 - loglik: -3.8632e+02 - logprior: -2.0101e+01
Epoch 2/10
10/10 - 1s - loss: 340.6413 - loglik: -3.3603e+02 - logprior: -4.6135e+00
Epoch 3/10
10/10 - 1s - loss: 288.7190 - loglik: -2.8623e+02 - logprior: -2.4913e+00
Epoch 4/10
10/10 - 1s - loss: 258.7612 - loglik: -2.5665e+02 - logprior: -2.1152e+00
Epoch 5/10
10/10 - 1s - loss: 245.5648 - loglik: -2.4388e+02 - logprior: -1.6888e+00
Epoch 6/10
10/10 - 1s - loss: 238.4840 - loglik: -2.3701e+02 - logprior: -1.4691e+00
Epoch 7/10
10/10 - 1s - loss: 235.6733 - loglik: -2.3430e+02 - logprior: -1.3750e+00
Epoch 8/10
10/10 - 1s - loss: 234.2513 - loglik: -2.3305e+02 - logprior: -1.1975e+00
Epoch 9/10
10/10 - 1s - loss: 233.6548 - loglik: -2.3258e+02 - logprior: -1.0776e+00
Epoch 10/10
10/10 - 1s - loss: 233.4444 - loglik: -2.3246e+02 - logprior: -9.8289e-01
Fitted a model with MAP estimate = -232.8612
expansions: [(0, 5), (13, 1), (14, 1), (19, 1), (22, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (69, 1), (75, 2), (76, 1), (77, 1), (78, 1), (81, 1), (105, 1), (114, 3), (115, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 254.6593 - loglik: -2.2842e+02 - logprior: -2.6243e+01
Epoch 2/2
10/10 - 2s - loss: 218.8478 - loglik: -2.1158e+02 - logprior: -7.2668e+00
Fitted a model with MAP estimate = -212.6648
expansions: []
discards: [  0  60  63 142 145 147]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 234.4779 - loglik: -2.1162e+02 - logprior: -2.2855e+01
Epoch 2/2
10/10 - 2s - loss: 218.1293 - loglik: -2.0970e+02 - logprior: -8.4271e+00
Fitted a model with MAP estimate = -214.2937
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 229.7049 - loglik: -2.0844e+02 - logprior: -2.1263e+01
Epoch 2/10
10/10 - 2s - loss: 213.4383 - loglik: -2.0800e+02 - logprior: -5.4426e+00
Epoch 3/10
10/10 - 2s - loss: 206.9706 - loglik: -2.0557e+02 - logprior: -1.4043e+00
Epoch 4/10
10/10 - 2s - loss: 206.6414 - loglik: -2.0643e+02 - logprior: -2.1109e-01
Epoch 5/10
10/10 - 2s - loss: 204.3698 - loglik: -2.0488e+02 - logprior: 0.5129
Epoch 6/10
10/10 - 2s - loss: 204.5221 - loglik: -2.0546e+02 - logprior: 0.9365
Fitted a model with MAP estimate = -203.7773
Time for alignment: 50.4166
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.1526 - loglik: -3.8605e+02 - logprior: -2.0105e+01
Epoch 2/10
10/10 - 1s - loss: 340.7810 - loglik: -3.3616e+02 - logprior: -4.6220e+00
Epoch 3/10
10/10 - 1s - loss: 292.1104 - loglik: -2.8960e+02 - logprior: -2.5132e+00
Epoch 4/10
10/10 - 1s - loss: 261.4630 - loglik: -2.5937e+02 - logprior: -2.0898e+00
Epoch 5/10
10/10 - 1s - loss: 248.1176 - loglik: -2.4647e+02 - logprior: -1.6459e+00
Epoch 6/10
10/10 - 1s - loss: 241.9206 - loglik: -2.4042e+02 - logprior: -1.4972e+00
Epoch 7/10
10/10 - 1s - loss: 239.1797 - loglik: -2.3770e+02 - logprior: -1.4774e+00
Epoch 8/10
10/10 - 1s - loss: 237.3182 - loglik: -2.3591e+02 - logprior: -1.4053e+00
Epoch 9/10
10/10 - 1s - loss: 237.2184 - loglik: -2.3586e+02 - logprior: -1.3600e+00
Epoch 10/10
10/10 - 1s - loss: 236.1372 - loglik: -2.3477e+02 - logprior: -1.3703e+00
Fitted a model with MAP estimate = -236.0582
expansions: [(0, 5), (13, 1), (14, 1), (19, 1), (22, 1), (26, 1), (27, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 1), (78, 1), (81, 1), (100, 1), (104, 3), (105, 2), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 254.5967 - loglik: -2.2844e+02 - logprior: -2.6155e+01
Epoch 2/2
10/10 - 2s - loss: 217.6374 - loglik: -2.1052e+02 - logprior: -7.1194e+00
Fitted a model with MAP estimate = -211.6046
expansions: []
discards: [ 0 60 63]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 234.2157 - loglik: -2.1143e+02 - logprior: -2.2788e+01
Epoch 2/2
10/10 - 2s - loss: 217.0858 - loglik: -2.0870e+02 - logprior: -8.3835e+00
Fitted a model with MAP estimate = -214.1503
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 229.9024 - loglik: -2.0867e+02 - logprior: -2.1233e+01
Epoch 2/10
10/10 - 2s - loss: 213.0334 - loglik: -2.0762e+02 - logprior: -5.4166e+00
Epoch 3/10
10/10 - 2s - loss: 207.3274 - loglik: -2.0595e+02 - logprior: -1.3781e+00
Epoch 4/10
10/10 - 2s - loss: 205.4854 - loglik: -2.0530e+02 - logprior: -1.8629e-01
Epoch 5/10
10/10 - 2s - loss: 205.5123 - loglik: -2.0603e+02 - logprior: 0.5205
Fitted a model with MAP estimate = -204.3007
Time for alignment: 48.8202
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 405.9819 - loglik: -3.8588e+02 - logprior: -2.0106e+01
Epoch 2/10
10/10 - 1s - loss: 340.8906 - loglik: -3.3627e+02 - logprior: -4.6231e+00
Epoch 3/10
10/10 - 1s - loss: 289.1306 - loglik: -2.8661e+02 - logprior: -2.5227e+00
Epoch 4/10
10/10 - 1s - loss: 259.7474 - loglik: -2.5764e+02 - logprior: -2.1103e+00
Epoch 5/10
10/10 - 1s - loss: 248.2490 - loglik: -2.4664e+02 - logprior: -1.6100e+00
Epoch 6/10
10/10 - 1s - loss: 242.7173 - loglik: -2.4138e+02 - logprior: -1.3325e+00
Epoch 7/10
10/10 - 1s - loss: 239.9016 - loglik: -2.3859e+02 - logprior: -1.3092e+00
Epoch 8/10
10/10 - 1s - loss: 239.1855 - loglik: -2.3793e+02 - logprior: -1.2585e+00
Epoch 9/10
10/10 - 1s - loss: 238.3065 - loglik: -2.3712e+02 - logprior: -1.1863e+00
Epoch 10/10
10/10 - 1s - loss: 237.6739 - loglik: -2.3650e+02 - logprior: -1.1778e+00
Fitted a model with MAP estimate = -237.7574
expansions: [(0, 5), (13, 1), (14, 1), (16, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 1), (78, 1), (81, 1), (98, 2), (106, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 257.9763 - loglik: -2.3177e+02 - logprior: -2.6202e+01
Epoch 2/2
10/10 - 2s - loss: 219.7694 - loglik: -2.1245e+02 - logprior: -7.3231e+00
Fitted a model with MAP estimate = -212.8719
expansions: [(135, 1)]
discards: [  0  60  63 123]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 234.4533 - loglik: -2.1159e+02 - logprior: -2.2866e+01
Epoch 2/2
10/10 - 2s - loss: 217.8141 - loglik: -2.0942e+02 - logprior: -8.3904e+00
Fitted a model with MAP estimate = -214.2464
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 230.2049 - loglik: -2.0896e+02 - logprior: -2.1250e+01
Epoch 2/10
10/10 - 2s - loss: 212.7346 - loglik: -2.0731e+02 - logprior: -5.4266e+00
Epoch 3/10
10/10 - 2s - loss: 208.1130 - loglik: -2.0673e+02 - logprior: -1.3866e+00
Epoch 4/10
10/10 - 2s - loss: 205.4464 - loglik: -2.0525e+02 - logprior: -1.9843e-01
Epoch 5/10
10/10 - 2s - loss: 205.3846 - loglik: -2.0589e+02 - logprior: 0.5098
Epoch 6/10
10/10 - 2s - loss: 203.2158 - loglik: -2.0416e+02 - logprior: 0.9429
Epoch 7/10
10/10 - 2s - loss: 204.4672 - loglik: -2.0560e+02 - logprior: 1.1285
Fitted a model with MAP estimate = -203.4684
Time for alignment: 51.7578
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 406.1996 - loglik: -3.8610e+02 - logprior: -2.0104e+01
Epoch 2/10
10/10 - 1s - loss: 340.9543 - loglik: -3.3634e+02 - logprior: -4.6149e+00
Epoch 3/10
10/10 - 1s - loss: 289.3421 - loglik: -2.8686e+02 - logprior: -2.4797e+00
Epoch 4/10
10/10 - 1s - loss: 259.9803 - loglik: -2.5787e+02 - logprior: -2.1136e+00
Epoch 5/10
10/10 - 1s - loss: 247.0346 - loglik: -2.4532e+02 - logprior: -1.7167e+00
Epoch 6/10
10/10 - 1s - loss: 242.7712 - loglik: -2.4125e+02 - logprior: -1.5226e+00
Epoch 7/10
10/10 - 1s - loss: 238.9150 - loglik: -2.3739e+02 - logprior: -1.5232e+00
Epoch 8/10
10/10 - 1s - loss: 238.0285 - loglik: -2.3656e+02 - logprior: -1.4689e+00
Epoch 9/10
10/10 - 1s - loss: 236.9596 - loglik: -2.3557e+02 - logprior: -1.3863e+00
Epoch 10/10
10/10 - 1s - loss: 236.7216 - loglik: -2.3534e+02 - logprior: -1.3770e+00
Fitted a model with MAP estimate = -236.2268
expansions: [(0, 5), (13, 1), (14, 1), (15, 1), (22, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (69, 1), (72, 1), (75, 1), (76, 1), (77, 2), (78, 1), (98, 2), (104, 3), (105, 2), (106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 254.7671 - loglik: -2.2850e+02 - logprior: -2.6272e+01
Epoch 2/2
10/10 - 2s - loss: 218.4537 - loglik: -2.1122e+02 - logprior: -7.2348e+00
Fitted a model with MAP estimate = -211.6757
expansions: []
discards: [  0  60  63 123]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 234.0703 - loglik: -2.1124e+02 - logprior: -2.2829e+01
Epoch 2/2
10/10 - 2s - loss: 217.2780 - loglik: -2.0889e+02 - logprior: -8.3887e+00
Fitted a model with MAP estimate = -214.2431
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 229.8738 - loglik: -2.0861e+02 - logprior: -2.1260e+01
Epoch 2/10
10/10 - 2s - loss: 213.2368 - loglik: -2.0779e+02 - logprior: -5.4488e+00
Epoch 3/10
10/10 - 2s - loss: 207.8168 - loglik: -2.0642e+02 - logprior: -1.3966e+00
Epoch 4/10
10/10 - 2s - loss: 206.0068 - loglik: -2.0579e+02 - logprior: -2.1192e-01
Epoch 5/10
10/10 - 2s - loss: 204.5682 - loglik: -2.0508e+02 - logprior: 0.5151
Epoch 6/10
10/10 - 2s - loss: 204.0368 - loglik: -2.0499e+02 - logprior: 0.9523
Epoch 7/10
10/10 - 2s - loss: 203.7325 - loglik: -2.0486e+02 - logprior: 1.1255
Epoch 8/10
10/10 - 2s - loss: 203.4855 - loglik: -2.0478e+02 - logprior: 1.2973
Epoch 9/10
10/10 - 2s - loss: 202.9166 - loglik: -2.0434e+02 - logprior: 1.4221
Epoch 10/10
10/10 - 2s - loss: 203.4197 - loglik: -2.0494e+02 - logprior: 1.5169
Fitted a model with MAP estimate = -202.6999
Time for alignment: 57.3337
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 406.0234 - loglik: -3.8592e+02 - logprior: -2.0106e+01
Epoch 2/10
10/10 - 1s - loss: 340.9699 - loglik: -3.3635e+02 - logprior: -4.6194e+00
Epoch 3/10
10/10 - 1s - loss: 290.0988 - loglik: -2.8760e+02 - logprior: -2.4970e+00
Epoch 4/10
10/10 - 1s - loss: 261.7345 - loglik: -2.5969e+02 - logprior: -2.0479e+00
Epoch 5/10
10/10 - 1s - loss: 247.9120 - loglik: -2.4633e+02 - logprior: -1.5829e+00
Epoch 6/10
10/10 - 1s - loss: 242.5483 - loglik: -2.4120e+02 - logprior: -1.3450e+00
Epoch 7/10
10/10 - 1s - loss: 240.4709 - loglik: -2.3911e+02 - logprior: -1.3599e+00
Epoch 8/10
10/10 - 1s - loss: 239.6836 - loglik: -2.3837e+02 - logprior: -1.3127e+00
Epoch 9/10
10/10 - 1s - loss: 238.3515 - loglik: -2.3715e+02 - logprior: -1.2047e+00
Epoch 10/10
10/10 - 1s - loss: 238.4478 - loglik: -2.3726e+02 - logprior: -1.1892e+00
Fitted a model with MAP estimate = -238.1778
expansions: [(0, 5), (13, 1), (14, 1), (15, 1), (24, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 1), (62, 1), (75, 2), (76, 1), (77, 1), (78, 1), (81, 1), (105, 1), (106, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 257.0421 - loglik: -2.3081e+02 - logprior: -2.6229e+01
Epoch 2/2
10/10 - 2s - loss: 220.1830 - loglik: -2.1297e+02 - logprior: -7.2125e+00
Fitted a model with MAP estimate = -213.0716
expansions: [(133, 1)]
discards: [ 0 60]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 234.5727 - loglik: -2.1171e+02 - logprior: -2.2864e+01
Epoch 2/2
10/10 - 2s - loss: 217.5601 - loglik: -2.0917e+02 - logprior: -8.3896e+00
Fitted a model with MAP estimate = -214.2496
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.2742 - loglik: -2.0902e+02 - logprior: -2.1250e+01
Epoch 2/10
10/10 - 2s - loss: 212.5417 - loglik: -2.0712e+02 - logprior: -5.4252e+00
Epoch 3/10
10/10 - 2s - loss: 208.3309 - loglik: -2.0694e+02 - logprior: -1.3866e+00
Epoch 4/10
10/10 - 2s - loss: 205.3579 - loglik: -2.0515e+02 - logprior: -2.0524e-01
Epoch 5/10
10/10 - 2s - loss: 204.9923 - loglik: -2.0550e+02 - logprior: 0.5048
Epoch 6/10
10/10 - 2s - loss: 203.2850 - loglik: -2.0422e+02 - logprior: 0.9339
Epoch 7/10
10/10 - 2s - loss: 204.4156 - loglik: -2.0553e+02 - logprior: 1.1122
Fitted a model with MAP estimate = -203.4398
Time for alignment: 51.6167
Computed alignments with likelihoods: ['-203.7773', '-204.3007', '-203.4684', '-202.6999', '-203.4398']
Best model has likelihood: -202.6999  (prior= 1.5713 )
time for generating output: 0.1553
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.8853242320819112
Training of 5 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb40a305b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebe051f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb05e02670>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 316.7097 - loglik: -2.4823e+02 - logprior: -6.8480e+01
Epoch 2/10
10/10 - 1s - loss: 235.8535 - loglik: -2.1862e+02 - logprior: -1.7233e+01
Epoch 3/10
10/10 - 1s - loss: 202.6229 - loglik: -1.9525e+02 - logprior: -7.3725e+00
Epoch 4/10
10/10 - 1s - loss: 187.3977 - loglik: -1.8371e+02 - logprior: -3.6828e+00
Epoch 5/10
10/10 - 1s - loss: 180.6198 - loglik: -1.7896e+02 - logprior: -1.6606e+00
Epoch 6/10
10/10 - 1s - loss: 177.4107 - loglik: -1.7689e+02 - logprior: -5.2100e-01
Epoch 7/10
10/10 - 2s - loss: 175.8095 - loglik: -1.7592e+02 - logprior: 0.1152
Epoch 8/10
10/10 - 1s - loss: 174.8164 - loglik: -1.7529e+02 - logprior: 0.4759
Epoch 9/10
10/10 - 1s - loss: 174.0813 - loglik: -1.7479e+02 - logprior: 0.7062
Epoch 10/10
10/10 - 1s - loss: 173.5839 - loglik: -1.7448e+02 - logprior: 0.8932
Fitted a model with MAP estimate = -173.3719
expansions: [(9, 2), (27, 4), (39, 2), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 252.8636 - loglik: -1.7585e+02 - logprior: -7.7012e+01
Epoch 2/2
10/10 - 2s - loss: 199.7111 - loglik: -1.6859e+02 - logprior: -3.1121e+01
Fitted a model with MAP estimate = -190.5709
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 244.0526 - loglik: -1.6835e+02 - logprior: -7.5701e+01
Epoch 2/2
10/10 - 1s - loss: 192.6246 - loglik: -1.6635e+02 - logprior: -2.6277e+01
Fitted a model with MAP estimate = -180.8789
expansions: [(0, 5), (10, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.0228 - loglik: -1.6519e+02 - logprior: -6.0832e+01
Epoch 2/10
10/10 - 2s - loss: 175.8053 - loglik: -1.6093e+02 - logprior: -1.4877e+01
Epoch 3/10
10/10 - 2s - loss: 164.8177 - loglik: -1.5953e+02 - logprior: -5.2901e+00
Epoch 4/10
10/10 - 2s - loss: 160.6687 - loglik: -1.5936e+02 - logprior: -1.3073e+00
Epoch 5/10
10/10 - 2s - loss: 158.6126 - loglik: -1.5945e+02 - logprior: 0.8408
Epoch 6/10
10/10 - 2s - loss: 157.4242 - loglik: -1.5957e+02 - logprior: 2.1475
Epoch 7/10
10/10 - 2s - loss: 156.6944 - loglik: -1.5965e+02 - logprior: 2.9594
Epoch 8/10
10/10 - 2s - loss: 156.2049 - loglik: -1.5972e+02 - logprior: 3.5200
Epoch 9/10
10/10 - 2s - loss: 155.8301 - loglik: -1.5976e+02 - logprior: 3.9294
Epoch 10/10
10/10 - 2s - loss: 155.5035 - loglik: -1.5977e+02 - logprior: 4.2617
Fitted a model with MAP estimate = -155.3153
Time for alignment: 51.6295
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 316.7003 - loglik: -2.4822e+02 - logprior: -6.8479e+01
Epoch 2/10
10/10 - 1s - loss: 235.8282 - loglik: -2.1860e+02 - logprior: -1.7231e+01
Epoch 3/10
10/10 - 1s - loss: 202.7732 - loglik: -1.9539e+02 - logprior: -7.3840e+00
Epoch 4/10
10/10 - 1s - loss: 189.7262 - loglik: -1.8606e+02 - logprior: -3.6699e+00
Epoch 5/10
10/10 - 1s - loss: 184.6397 - loglik: -1.8304e+02 - logprior: -1.6013e+00
Epoch 6/10
10/10 - 2s - loss: 180.6857 - loglik: -1.8013e+02 - logprior: -5.5406e-01
Epoch 7/10
10/10 - 1s - loss: 177.8190 - loglik: -1.7778e+02 - logprior: -4.2254e-02
Epoch 8/10
10/10 - 2s - loss: 176.4165 - loglik: -1.7674e+02 - logprior: 0.3276
Epoch 9/10
10/10 - 1s - loss: 175.6202 - loglik: -1.7618e+02 - logprior: 0.5616
Epoch 10/10
10/10 - 1s - loss: 175.0820 - loglik: -1.7580e+02 - logprior: 0.7166
Fitted a model with MAP estimate = -174.8685
expansions: [(27, 3), (39, 1), (52, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 253.7069 - loglik: -1.7673e+02 - logprior: -7.6979e+01
Epoch 2/2
10/10 - 1s - loss: 201.6718 - loglik: -1.7071e+02 - logprior: -3.0964e+01
Fitted a model with MAP estimate = -192.9951
expansions: [(24, 1), (25, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 247.0261 - loglik: -1.7113e+02 - logprior: -7.5892e+01
Epoch 2/2
10/10 - 2s - loss: 196.3623 - loglik: -1.6875e+02 - logprior: -2.7616e+01
Fitted a model with MAP estimate = -185.5426
expansions: [(0, 3), (5, 2), (7, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 228.1326 - loglik: -1.6723e+02 - logprior: -6.0900e+01
Epoch 2/10
10/10 - 2s - loss: 177.7156 - loglik: -1.6302e+02 - logprior: -1.4696e+01
Epoch 3/10
10/10 - 2s - loss: 167.0714 - loglik: -1.6198e+02 - logprior: -5.0898e+00
Epoch 4/10
10/10 - 2s - loss: 163.1223 - loglik: -1.6197e+02 - logprior: -1.1499e+00
Epoch 5/10
10/10 - 2s - loss: 161.0781 - loglik: -1.6212e+02 - logprior: 1.0373
Epoch 6/10
10/10 - 2s - loss: 159.8738 - loglik: -1.6224e+02 - logprior: 2.3657
Epoch 7/10
10/10 - 2s - loss: 159.0414 - loglik: -1.6221e+02 - logprior: 3.1683
Epoch 8/10
10/10 - 2s - loss: 158.4602 - loglik: -1.6215e+02 - logprior: 3.6853
Epoch 9/10
10/10 - 2s - loss: 158.0453 - loglik: -1.6212e+02 - logprior: 4.0769
Epoch 10/10
10/10 - 2s - loss: 157.7187 - loglik: -1.6211e+02 - logprior: 4.3915
Fitted a model with MAP estimate = -157.5249
Time for alignment: 51.7670
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.6987 - loglik: -2.4822e+02 - logprior: -6.8480e+01
Epoch 2/10
10/10 - 1s - loss: 235.6868 - loglik: -2.1846e+02 - logprior: -1.7230e+01
Epoch 3/10
10/10 - 1s - loss: 202.6017 - loglik: -1.9521e+02 - logprior: -7.3894e+00
Epoch 4/10
10/10 - 1s - loss: 187.9729 - loglik: -1.8416e+02 - logprior: -3.8123e+00
Epoch 5/10
10/10 - 1s - loss: 180.5230 - loglik: -1.7860e+02 - logprior: -1.9189e+00
Epoch 6/10
10/10 - 1s - loss: 176.9710 - loglik: -1.7609e+02 - logprior: -8.7980e-01
Epoch 7/10
10/10 - 1s - loss: 175.4152 - loglik: -1.7516e+02 - logprior: -2.5285e-01
Epoch 8/10
10/10 - 1s - loss: 174.6069 - loglik: -1.7472e+02 - logprior: 0.1103
Epoch 9/10
10/10 - 1s - loss: 174.0966 - loglik: -1.7446e+02 - logprior: 0.3604
Epoch 10/10
10/10 - 1s - loss: 173.7578 - loglik: -1.7433e+02 - logprior: 0.5721
Fitted a model with MAP estimate = -173.5914
expansions: [(14, 2), (25, 1), (26, 2), (27, 2), (39, 1), (52, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.9985 - loglik: -1.7423e+02 - logprior: -7.6766e+01
Epoch 2/2
10/10 - 2s - loss: 197.5031 - loglik: -1.6682e+02 - logprior: -3.0680e+01
Fitted a model with MAP estimate = -188.5555
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 242.0331 - loglik: -1.6688e+02 - logprior: -7.5149e+01
Epoch 2/2
10/10 - 1s - loss: 190.3867 - loglik: -1.6503e+02 - logprior: -2.5352e+01
Fitted a model with MAP estimate = -179.0080
expansions: [(7, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 243.9362 - loglik: -1.6660e+02 - logprior: -7.7339e+01
Epoch 2/10
10/10 - 2s - loss: 195.9351 - loglik: -1.6529e+02 - logprior: -3.0646e+01
Epoch 3/10
10/10 - 2s - loss: 186.1396 - loglik: -1.6521e+02 - logprior: -2.0932e+01
Epoch 4/10
10/10 - 2s - loss: 181.7013 - loglik: -1.6512e+02 - logprior: -1.6584e+01
Epoch 5/10
10/10 - 2s - loss: 177.2221 - loglik: -1.6491e+02 - logprior: -1.2311e+01
Epoch 6/10
10/10 - 2s - loss: 167.5407 - loglik: -1.6462e+02 - logprior: -2.9202e+00
Epoch 7/10
10/10 - 2s - loss: 161.7927 - loglik: -1.6411e+02 - logprior: 2.3145
Epoch 8/10
10/10 - 2s - loss: 160.7387 - loglik: -1.6417e+02 - logprior: 3.4305
Epoch 9/10
10/10 - 2s - loss: 160.2463 - loglik: -1.6430e+02 - logprior: 4.0585
Epoch 10/10
10/10 - 2s - loss: 159.8737 - loglik: -1.6435e+02 - logprior: 4.4793
Fitted a model with MAP estimate = -159.6750
Time for alignment: 50.3866
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.7003 - loglik: -2.4822e+02 - logprior: -6.8480e+01
Epoch 2/10
10/10 - 1s - loss: 235.6936 - loglik: -2.1846e+02 - logprior: -1.7234e+01
Epoch 3/10
10/10 - 1s - loss: 202.4140 - loglik: -1.9503e+02 - logprior: -7.3851e+00
Epoch 4/10
10/10 - 1s - loss: 188.5697 - loglik: -1.8490e+02 - logprior: -3.6693e+00
Epoch 5/10
10/10 - 1s - loss: 181.2524 - loglik: -1.7959e+02 - logprior: -1.6659e+00
Epoch 6/10
10/10 - 1s - loss: 176.0866 - loglik: -1.7542e+02 - logprior: -6.6464e-01
Epoch 7/10
10/10 - 1s - loss: 173.6647 - loglik: -1.7359e+02 - logprior: -7.7238e-02
Epoch 8/10
10/10 - 1s - loss: 172.5355 - loglik: -1.7282e+02 - logprior: 0.2858
Epoch 9/10
10/10 - 1s - loss: 171.8694 - loglik: -1.7239e+02 - logprior: 0.5244
Epoch 10/10
10/10 - 1s - loss: 171.4730 - loglik: -1.7218e+02 - logprior: 0.7064
Fitted a model with MAP estimate = -171.3010
expansions: [(39, 1), (52, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.7936 - loglik: -1.7366e+02 - logprior: -7.7138e+01
Epoch 2/2
10/10 - 1s - loss: 199.2656 - loglik: -1.6824e+02 - logprior: -3.1030e+01
Fitted a model with MAP estimate = -190.5994
expansions: [(25, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 244.5168 - loglik: -1.6839e+02 - logprior: -7.6132e+01
Epoch 2/2
10/10 - 2s - loss: 194.0448 - loglik: -1.6506e+02 - logprior: -2.8988e+01
Fitted a model with MAP estimate = -183.8370
expansions: [(0, 3), (5, 2), (7, 2), (26, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.9076 - loglik: -1.6371e+02 - logprior: -6.1201e+01
Epoch 2/10
10/10 - 2s - loss: 174.5191 - loglik: -1.5960e+02 - logprior: -1.4917e+01
Epoch 3/10
10/10 - 2s - loss: 163.8471 - loglik: -1.5861e+02 - logprior: -5.2379e+00
Epoch 4/10
10/10 - 2s - loss: 159.7106 - loglik: -1.5845e+02 - logprior: -1.2584e+00
Epoch 5/10
10/10 - 2s - loss: 157.6503 - loglik: -1.5860e+02 - logprior: 0.9488
Epoch 6/10
10/10 - 2s - loss: 156.5276 - loglik: -1.5878e+02 - logprior: 2.2522
Epoch 7/10
10/10 - 2s - loss: 155.8338 - loglik: -1.5889e+02 - logprior: 3.0580
Epoch 8/10
10/10 - 2s - loss: 155.2874 - loglik: -1.5889e+02 - logprior: 3.6034
Epoch 9/10
10/10 - 2s - loss: 154.9001 - loglik: -1.5892e+02 - logprior: 4.0151
Epoch 10/10
10/10 - 2s - loss: 154.5790 - loglik: -1.5892e+02 - logprior: 4.3384
Fitted a model with MAP estimate = -154.4012
Time for alignment: 50.8325
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.7242 - loglik: -2.4825e+02 - logprior: -6.8479e+01
Epoch 2/10
10/10 - 1s - loss: 235.7160 - loglik: -2.1848e+02 - logprior: -1.7233e+01
Epoch 3/10
10/10 - 2s - loss: 202.0608 - loglik: -1.9470e+02 - logprior: -7.3630e+00
Epoch 4/10
10/10 - 1s - loss: 187.4755 - loglik: -1.8385e+02 - logprior: -3.6266e+00
Epoch 5/10
10/10 - 1s - loss: 181.1115 - loglik: -1.7953e+02 - logprior: -1.5797e+00
Epoch 6/10
10/10 - 1s - loss: 176.7125 - loglik: -1.7615e+02 - logprior: -5.5901e-01
Epoch 7/10
10/10 - 1s - loss: 173.9111 - loglik: -1.7387e+02 - logprior: -4.0242e-02
Epoch 8/10
10/10 - 1s - loss: 172.5702 - loglik: -1.7289e+02 - logprior: 0.3149
Epoch 9/10
10/10 - 1s - loss: 171.8521 - loglik: -1.7240e+02 - logprior: 0.5502
Epoch 10/10
10/10 - 1s - loss: 171.3022 - loglik: -1.7200e+02 - logprior: 0.7019
Fitted a model with MAP estimate = -171.0940
expansions: [(26, 3), (27, 3), (39, 1), (52, 1), (58, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.8279 - loglik: -1.7304e+02 - logprior: -7.6792e+01
Epoch 2/2
10/10 - 2s - loss: 196.3983 - loglik: -1.6562e+02 - logprior: -3.0779e+01
Fitted a model with MAP estimate = -187.3292
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 241.1331 - loglik: -1.6588e+02 - logprior: -7.5255e+01
Epoch 2/2
10/10 - 1s - loss: 189.5453 - loglik: -1.6417e+02 - logprior: -2.5375e+01
Fitted a model with MAP estimate = -178.0830
expansions: [(0, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.2258 - loglik: -1.6372e+02 - logprior: -6.0506e+01
Epoch 2/10
10/10 - 1s - loss: 176.1651 - loglik: -1.6141e+02 - logprior: -1.4760e+01
Epoch 3/10
10/10 - 1s - loss: 166.2450 - loglik: -1.6098e+02 - logprior: -5.2634e+00
Epoch 4/10
10/10 - 2s - loss: 162.2507 - loglik: -1.6097e+02 - logprior: -1.2786e+00
Epoch 5/10
10/10 - 1s - loss: 160.1687 - loglik: -1.6111e+02 - logprior: 0.9391
Epoch 6/10
10/10 - 2s - loss: 159.0182 - loglik: -1.6126e+02 - logprior: 2.2441
Epoch 7/10
10/10 - 2s - loss: 158.2850 - loglik: -1.6133e+02 - logprior: 3.0462
Epoch 8/10
10/10 - 2s - loss: 157.7578 - loglik: -1.6135e+02 - logprior: 3.5963
Epoch 9/10
10/10 - 2s - loss: 157.3138 - loglik: -1.6131e+02 - logprior: 3.9988
Epoch 10/10
10/10 - 2s - loss: 156.9599 - loglik: -1.6129e+02 - logprior: 4.3290
Fitted a model with MAP estimate = -156.7668
Time for alignment: 50.1419
Computed alignments with likelihoods: ['-155.3153', '-157.5249', '-159.6750', '-154.4012', '-156.7668']
Best model has likelihood: -154.4012  (prior= 4.4995 )
time for generating output: 0.2137
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.3086338074760172
Training of 5 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feafd675df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac8c9c250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaeba33be0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 757.5569 - loglik: -7.4965e+02 - logprior: -7.9100e+00
Epoch 2/10
20/20 - 9s - loss: 687.0620 - loglik: -6.8723e+02 - logprior: 0.1727
Epoch 3/10
20/20 - 9s - loss: 661.9435 - loglik: -6.6210e+02 - logprior: 0.1529
Epoch 4/10
20/20 - 9s - loss: 657.8901 - loglik: -6.5824e+02 - logprior: 0.3522
Epoch 5/10
20/20 - 9s - loss: 650.9339 - loglik: -6.5132e+02 - logprior: 0.3871
Epoch 6/10
20/20 - 9s - loss: 654.1558 - loglik: -6.5459e+02 - logprior: 0.4340
Fitted a model with MAP estimate = -651.7802
expansions: [(0, 3), (21, 5), (25, 1), (49, 1), (51, 1), (52, 3), (82, 1), (86, 12), (87, 1), (92, 1), (93, 1), (115, 2), (117, 1), (147, 1), (153, 2), (154, 1), (175, 1), (207, 2), (208, 1)]
discards: [  0   1   2 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 663.5516 - loglik: -6.5567e+02 - logprior: -7.8850e+00
Epoch 2/2
20/20 - 11s - loss: 643.3904 - loglik: -6.4351e+02 - logprior: 0.1168
Fitted a model with MAP estimate = -642.3627
expansions: [(0, 4)]
discards: [  1   2  21  22  58 142 185 243]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 657.7784 - loglik: -6.4792e+02 - logprior: -9.8535e+00
Epoch 2/2
20/20 - 11s - loss: 642.1680 - loglik: -6.4222e+02 - logprior: 0.0557
Fitted a model with MAP estimate = -640.2364
expansions: [(0, 4), (23, 1), (138, 2)]
discards: [1 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 655.0945 - loglik: -6.4384e+02 - logprior: -1.1254e+01
Epoch 2/10
20/20 - 11s - loss: 639.1443 - loglik: -6.3899e+02 - logprior: -1.5323e-01
Epoch 3/10
20/20 - 11s - loss: 638.1917 - loglik: -6.3959e+02 - logprior: 1.3995
Epoch 4/10
20/20 - 11s - loss: 635.9686 - loglik: -6.3783e+02 - logprior: 1.8614
Epoch 5/10
20/20 - 11s - loss: 634.5939 - loglik: -6.3668e+02 - logprior: 2.0855
Epoch 6/10
20/20 - 11s - loss: 635.0377 - loglik: -6.3730e+02 - logprior: 2.2644
Fitted a model with MAP estimate = -634.4012
Time for alignment: 205.5581
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 757.5995 - loglik: -7.4969e+02 - logprior: -7.9130e+00
Epoch 2/10
20/20 - 9s - loss: 686.7825 - loglik: -6.8704e+02 - logprior: 0.2616
Epoch 3/10
20/20 - 9s - loss: 659.4654 - loglik: -6.5979e+02 - logprior: 0.3225
Epoch 4/10
20/20 - 9s - loss: 653.4952 - loglik: -6.5409e+02 - logprior: 0.5952
Epoch 5/10
20/20 - 9s - loss: 650.2775 - loglik: -6.5094e+02 - logprior: 0.6588
Epoch 6/10
20/20 - 9s - loss: 648.0071 - loglik: -6.4866e+02 - logprior: 0.6560
Epoch 7/10
20/20 - 9s - loss: 649.7109 - loglik: -6.5041e+02 - logprior: 0.7029
Fitted a model with MAP estimate = -648.4572
expansions: [(0, 4), (45, 1), (54, 3), (82, 3), (86, 12), (87, 1), (94, 1), (114, 1), (115, 2), (117, 1), (147, 1), (153, 1), (199, 4), (207, 2), (208, 1)]
discards: [  0   1   2 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 661.9662 - loglik: -6.5399e+02 - logprior: -7.9747e+00
Epoch 2/2
20/20 - 11s - loss: 642.3116 - loglik: -6.4242e+02 - logprior: 0.1119
Fitted a model with MAP estimate = -641.0238
expansions: [(0, 4)]
discards: [  2   4  89  90  95  96 138 240]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 656.1234 - loglik: -6.4616e+02 - logprior: -9.9628e+00
Epoch 2/2
20/20 - 11s - loss: 642.2903 - loglik: -6.4231e+02 - logprior: 0.0202
Fitted a model with MAP estimate = -640.6529
expansions: [(0, 4)]
discards: [ 1  2  3  4  5  6 91]
Re-initialized the encoder parameters.
Fitting a model of length 251 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 656.2034 - loglik: -6.4491e+02 - logprior: -1.1298e+01
Epoch 2/10
20/20 - 10s - loss: 642.3822 - loglik: -6.4232e+02 - logprior: -5.8592e-02
Epoch 3/10
20/20 - 10s - loss: 641.7654 - loglik: -6.4318e+02 - logprior: 1.4185
Epoch 4/10
20/20 - 10s - loss: 638.7095 - loglik: -6.4058e+02 - logprior: 1.8689
Epoch 5/10
20/20 - 10s - loss: 636.6407 - loglik: -6.3873e+02 - logprior: 2.0895
Epoch 6/10
20/20 - 10s - loss: 636.9206 - loglik: -6.3920e+02 - logprior: 2.2782
Fitted a model with MAP estimate = -636.9071
Time for alignment: 208.4446
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 757.3849 - loglik: -7.4948e+02 - logprior: -7.9048e+00
Epoch 2/10
20/20 - 9s - loss: 690.7625 - loglik: -6.9106e+02 - logprior: 0.2982
Epoch 3/10
20/20 - 9s - loss: 661.3078 - loglik: -6.6157e+02 - logprior: 0.2615
Epoch 4/10
20/20 - 9s - loss: 656.0923 - loglik: -6.5659e+02 - logprior: 0.5026
Epoch 5/10
20/20 - 9s - loss: 649.2812 - loglik: -6.4987e+02 - logprior: 0.5899
Epoch 6/10
20/20 - 9s - loss: 651.5853 - loglik: -6.5222e+02 - logprior: 0.6379
Fitted a model with MAP estimate = -649.8691
expansions: [(0, 3), (20, 5), (45, 1), (52, 3), (87, 13), (93, 3), (94, 1), (103, 1), (116, 2), (118, 1), (147, 1), (153, 1), (201, 2), (207, 2), (208, 1)]
discards: [  0   1   2 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 658.9029 - loglik: -6.5100e+02 - logprior: -7.9077e+00
Epoch 2/2
20/20 - 11s - loss: 643.8091 - loglik: -6.4376e+02 - logprior: -5.0632e-02
Fitted a model with MAP estimate = -639.8055
expansions: [(0, 4), (225, 2)]
discards: [  1   2  20  21  22  23  24 116 143 242]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 656.1569 - loglik: -6.4625e+02 - logprior: -9.9060e+00
Epoch 2/2
20/20 - 10s - loss: 640.7885 - loglik: -6.4085e+02 - logprior: 0.0634
Fitted a model with MAP estimate = -639.8918
expansions: [(0, 4), (176, 1)]
discards: [  1   3   4 220 221]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 655.1946 - loglik: -6.4398e+02 - logprior: -1.1217e+01
Epoch 2/10
20/20 - 11s - loss: 642.0546 - loglik: -6.4200e+02 - logprior: -5.9274e-02
Epoch 3/10
20/20 - 10s - loss: 638.4108 - loglik: -6.3987e+02 - logprior: 1.4631
Epoch 4/10
20/20 - 11s - loss: 637.1947 - loglik: -6.3912e+02 - logprior: 1.9271
Epoch 5/10
20/20 - 11s - loss: 636.5461 - loglik: -6.3872e+02 - logprior: 2.1721
Epoch 6/10
20/20 - 11s - loss: 638.3406 - loglik: -6.4069e+02 - logprior: 2.3509
Fitted a model with MAP estimate = -636.0611
Time for alignment: 202.3514
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 758.7817 - loglik: -7.5087e+02 - logprior: -7.9119e+00
Epoch 2/10
20/20 - 9s - loss: 691.1033 - loglik: -6.9130e+02 - logprior: 0.1925
Epoch 3/10
20/20 - 9s - loss: 662.7064 - loglik: -6.6281e+02 - logprior: 0.0995
Epoch 4/10
20/20 - 9s - loss: 655.0970 - loglik: -6.5547e+02 - logprior: 0.3742
Epoch 5/10
20/20 - 9s - loss: 652.8983 - loglik: -6.5336e+02 - logprior: 0.4606
Epoch 6/10
20/20 - 9s - loss: 649.7590 - loglik: -6.5023e+02 - logprior: 0.4689
Epoch 7/10
20/20 - 9s - loss: 650.6116 - loglik: -6.5105e+02 - logprior: 0.4388
Fitted a model with MAP estimate = -648.9377
expansions: [(0, 3), (25, 3), (45, 1), (52, 3), (74, 1), (86, 9), (91, 1), (92, 2), (93, 1), (102, 1), (118, 1), (121, 1), (150, 1), (153, 1), (199, 4), (202, 1), (207, 1), (208, 1)]
discards: [  0 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 657.9799 - loglik: -6.5013e+02 - logprior: -7.8517e+00
Epoch 2/2
20/20 - 11s - loss: 642.2853 - loglik: -6.4227e+02 - logprior: -1.9360e-02
Fitted a model with MAP estimate = -639.3127
expansions: [(0, 4), (24, 4), (101, 1)]
discards: [  1   2   3  27 112]
Re-initialized the encoder parameters.
Fitting a model of length 262 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 654.5099 - loglik: -6.4441e+02 - logprior: -1.0096e+01
Epoch 2/2
20/20 - 11s - loss: 639.3490 - loglik: -6.3929e+02 - logprior: -5.8296e-02
Fitted a model with MAP estimate = -637.2922
expansions: [(0, 4)]
discards: [ 1  2  3  4  6 25 26]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 654.3523 - loglik: -6.4302e+02 - logprior: -1.1328e+01
Epoch 2/10
20/20 - 11s - loss: 639.7970 - loglik: -6.3964e+02 - logprior: -1.6178e-01
Epoch 3/10
20/20 - 11s - loss: 637.4692 - loglik: -6.3885e+02 - logprior: 1.3761
Epoch 4/10
20/20 - 11s - loss: 636.3668 - loglik: -6.3818e+02 - logprior: 1.8174
Epoch 5/10
20/20 - 11s - loss: 634.1284 - loglik: -6.3618e+02 - logprior: 2.0505
Epoch 6/10
20/20 - 11s - loss: 636.7911 - loglik: -6.3899e+02 - logprior: 2.1957
Fitted a model with MAP estimate = -634.0709
Time for alignment: 213.3429
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 757.8905 - loglik: -7.4999e+02 - logprior: -7.9005e+00
Epoch 2/10
20/20 - 9s - loss: 688.7296 - loglik: -6.8894e+02 - logprior: 0.2061
Epoch 3/10
20/20 - 9s - loss: 664.7872 - loglik: -6.6483e+02 - logprior: 0.0470
Epoch 4/10
20/20 - 9s - loss: 652.0110 - loglik: -6.5228e+02 - logprior: 0.2699
Epoch 5/10
20/20 - 9s - loss: 652.1136 - loglik: -6.5248e+02 - logprior: 0.3656
Fitted a model with MAP estimate = -650.2228
expansions: [(0, 5), (11, 2), (12, 1), (13, 1), (42, 1), (51, 1), (52, 1), (63, 1), (87, 8), (94, 1), (95, 1), (117, 2), (119, 1), (122, 1), (148, 1), (175, 6), (208, 1), (209, 1), (210, 2)]
discards: [  1   2   3   4   5 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 256 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 664.0974 - loglik: -6.5301e+02 - logprior: -1.1089e+01
Epoch 2/2
20/20 - 11s - loss: 642.3429 - loglik: -6.4184e+02 - logprior: -4.9922e-01
Fitted a model with MAP estimate = -640.1691
expansions: [(0, 4), (13, 1), (99, 1), (227, 3)]
discards: [  0   1 135 241]
Re-initialized the encoder parameters.
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 650.3997 - loglik: -6.4226e+02 - logprior: -8.1385e+00
Epoch 2/2
20/20 - 11s - loss: 640.7809 - loglik: -6.4098e+02 - logprior: 0.2030
Fitted a model with MAP estimate = -637.8074
expansions: [(0, 3), (54, 1), (103, 1), (176, 1)]
discards: [1 2 3 4 5 6]
Re-initialized the encoder parameters.
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 653.0379 - loglik: -6.4235e+02 - logprior: -1.0688e+01
Epoch 2/10
20/20 - 11s - loss: 639.4036 - loglik: -6.3941e+02 - logprior: 0.0053
Epoch 3/10
20/20 - 11s - loss: 634.4633 - loglik: -6.3594e+02 - logprior: 1.4772
Epoch 4/10
20/20 - 11s - loss: 635.6740 - loglik: -6.3761e+02 - logprior: 1.9335
Fitted a model with MAP estimate = -634.2256
Time for alignment: 173.2529
Computed alignments with likelihoods: ['-634.4012', '-636.9071', '-636.0611', '-634.0709', '-634.2256']
Best model has likelihood: -634.0709  (prior= 2.3616 )
time for generating output: 0.2870
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.6087809663968604
Training of 5 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49226e50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feafd17a040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d3bc400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.7437 - loglik: -1.3150e+02 - logprior: -3.2457e+00
Epoch 2/10
19/19 - 1s - loss: 113.8799 - loglik: -1.1244e+02 - logprior: -1.4369e+00
Epoch 3/10
19/19 - 1s - loss: 106.2055 - loglik: -1.0499e+02 - logprior: -1.2132e+00
Epoch 4/10
19/19 - 1s - loss: 104.0706 - loglik: -1.0288e+02 - logprior: -1.1862e+00
Epoch 5/10
19/19 - 1s - loss: 103.2241 - loglik: -1.0203e+02 - logprior: -1.1926e+00
Epoch 6/10
19/19 - 1s - loss: 102.7767 - loglik: -1.0160e+02 - logprior: -1.1754e+00
Epoch 7/10
19/19 - 1s - loss: 102.7039 - loglik: -1.0155e+02 - logprior: -1.1532e+00
Epoch 8/10
19/19 - 1s - loss: 102.4224 - loglik: -1.0128e+02 - logprior: -1.1447e+00
Epoch 9/10
19/19 - 1s - loss: 102.5337 - loglik: -1.0139e+02 - logprior: -1.1412e+00
Fitted a model with MAP estimate = -100.8827
expansions: [(10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (22, 2), (28, 2), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.9172 - loglik: -9.9547e+01 - logprior: -3.3702e+00
Epoch 2/2
19/19 - 1s - loss: 94.9232 - loglik: -9.3501e+01 - logprior: -1.4225e+00
Fitted a model with MAP estimate = -92.2918
expansions: []
discards: [28]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.5168 - loglik: -9.3178e+01 - logprior: -3.3390e+00
Epoch 2/2
19/19 - 1s - loss: 93.7442 - loglik: -9.2354e+01 - logprior: -1.3899e+00
Fitted a model with MAP estimate = -92.0740
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.7275 - loglik: -9.1505e+01 - logprior: -3.2227e+00
Epoch 2/10
19/19 - 1s - loss: 92.2764 - loglik: -9.0901e+01 - logprior: -1.3750e+00
Epoch 3/10
19/19 - 1s - loss: 91.7863 - loglik: -9.0549e+01 - logprior: -1.2376e+00
Epoch 4/10
19/19 - 1s - loss: 91.3037 - loglik: -9.0093e+01 - logprior: -1.2108e+00
Epoch 5/10
19/19 - 1s - loss: 90.8599 - loglik: -8.9665e+01 - logprior: -1.1952e+00
Epoch 6/10
19/19 - 1s - loss: 90.4845 - loglik: -8.9308e+01 - logprior: -1.1769e+00
Epoch 7/10
19/19 - 1s - loss: 90.6696 - loglik: -8.9495e+01 - logprior: -1.1745e+00
Fitted a model with MAP estimate = -90.4125
Time for alignment: 43.1217
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.7047 - loglik: -1.3147e+02 - logprior: -3.2392e+00
Epoch 2/10
19/19 - 1s - loss: 114.2062 - loglik: -1.1285e+02 - logprior: -1.3543e+00
Epoch 3/10
19/19 - 1s - loss: 107.6462 - loglik: -1.0620e+02 - logprior: -1.4442e+00
Epoch 4/10
19/19 - 1s - loss: 105.9749 - loglik: -1.0459e+02 - logprior: -1.3826e+00
Epoch 5/10
19/19 - 1s - loss: 105.4977 - loglik: -1.0415e+02 - logprior: -1.3522e+00
Epoch 6/10
19/19 - 1s - loss: 105.0638 - loglik: -1.0373e+02 - logprior: -1.3366e+00
Epoch 7/10
19/19 - 1s - loss: 105.0303 - loglik: -1.0371e+02 - logprior: -1.3231e+00
Epoch 8/10
19/19 - 1s - loss: 104.7506 - loglik: -1.0344e+02 - logprior: -1.3137e+00
Epoch 9/10
19/19 - 1s - loss: 104.7670 - loglik: -1.0346e+02 - logprior: -1.3049e+00
Fitted a model with MAP estimate = -103.2203
expansions: [(10, 2), (12, 3), (13, 3), (22, 2), (28, 2), (30, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.3839 - loglik: -1.0218e+02 - logprior: -4.2042e+00
Epoch 2/2
19/19 - 1s - loss: 96.6385 - loglik: -9.4549e+01 - logprior: -2.0899e+00
Fitted a model with MAP estimate = -93.6047
expansions: [(0, 1)]
discards: [ 0  9 17 30 42]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.2833 - loglik: -9.4051e+01 - logprior: -3.2324e+00
Epoch 2/2
19/19 - 1s - loss: 93.6562 - loglik: -9.2262e+01 - logprior: -1.3947e+00
Fitted a model with MAP estimate = -92.0968
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.6184 - loglik: -9.1415e+01 - logprior: -3.2032e+00
Epoch 2/10
19/19 - 1s - loss: 92.3837 - loglik: -9.0991e+01 - logprior: -1.3925e+00
Epoch 3/10
19/19 - 1s - loss: 91.7875 - loglik: -9.0540e+01 - logprior: -1.2477e+00
Epoch 4/10
19/19 - 1s - loss: 91.1471 - loglik: -8.9932e+01 - logprior: -1.2154e+00
Epoch 5/10
19/19 - 1s - loss: 91.0277 - loglik: -8.9826e+01 - logprior: -1.2013e+00
Epoch 6/10
19/19 - 1s - loss: 90.4814 - loglik: -8.9299e+01 - logprior: -1.1828e+00
Epoch 7/10
19/19 - 1s - loss: 90.6215 - loglik: -8.9439e+01 - logprior: -1.1826e+00
Fitted a model with MAP estimate = -90.3823
Time for alignment: 42.3854
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6619 - loglik: -1.3142e+02 - logprior: -3.2384e+00
Epoch 2/10
19/19 - 1s - loss: 114.8326 - loglik: -1.1347e+02 - logprior: -1.3601e+00
Epoch 3/10
19/19 - 1s - loss: 107.8702 - loglik: -1.0642e+02 - logprior: -1.4499e+00
Epoch 4/10
19/19 - 1s - loss: 106.3181 - loglik: -1.0493e+02 - logprior: -1.3856e+00
Epoch 5/10
19/19 - 1s - loss: 105.3869 - loglik: -1.0404e+02 - logprior: -1.3467e+00
Epoch 6/10
19/19 - 1s - loss: 104.9650 - loglik: -1.0362e+02 - logprior: -1.3404e+00
Epoch 7/10
19/19 - 1s - loss: 105.0634 - loglik: -1.0374e+02 - logprior: -1.3222e+00
Fitted a model with MAP estimate = -103.4166
expansions: [(10, 2), (12, 3), (13, 3), (23, 1), (28, 2), (30, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.2645 - loglik: -1.0208e+02 - logprior: -4.1860e+00
Epoch 2/2
19/19 - 1s - loss: 96.7721 - loglik: -9.4749e+01 - logprior: -2.0233e+00
Fitted a model with MAP estimate = -93.7092
expansions: [(0, 1)]
discards: [ 0  9 17 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.1005 - loglik: -9.3897e+01 - logprior: -3.2034e+00
Epoch 2/2
19/19 - 1s - loss: 93.8448 - loglik: -9.2455e+01 - logprior: -1.3898e+00
Fitted a model with MAP estimate = -92.0690
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.6697 - loglik: -9.1477e+01 - logprior: -3.1924e+00
Epoch 2/10
19/19 - 1s - loss: 92.3663 - loglik: -9.0974e+01 - logprior: -1.3925e+00
Epoch 3/10
19/19 - 1s - loss: 91.6035 - loglik: -9.0355e+01 - logprior: -1.2486e+00
Epoch 4/10
19/19 - 1s - loss: 91.3591 - loglik: -9.0140e+01 - logprior: -1.2188e+00
Epoch 5/10
19/19 - 1s - loss: 90.7176 - loglik: -8.9518e+01 - logprior: -1.1998e+00
Epoch 6/10
19/19 - 1s - loss: 90.7117 - loglik: -8.9525e+01 - logprior: -1.1866e+00
Epoch 7/10
19/19 - 1s - loss: 90.5934 - loglik: -8.9420e+01 - logprior: -1.1735e+00
Epoch 8/10
19/19 - 1s - loss: 90.4584 - loglik: -8.9287e+01 - logprior: -1.1716e+00
Epoch 9/10
19/19 - 1s - loss: 90.3618 - loglik: -8.9205e+01 - logprior: -1.1570e+00
Epoch 10/10
19/19 - 1s - loss: 90.2231 - loglik: -8.9071e+01 - logprior: -1.1522e+00
Fitted a model with MAP estimate = -90.2355
Time for alignment: 43.4396
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.7261 - loglik: -1.3149e+02 - logprior: -3.2394e+00
Epoch 2/10
19/19 - 1s - loss: 114.3878 - loglik: -1.1302e+02 - logprior: -1.3719e+00
Epoch 3/10
19/19 - 1s - loss: 107.9486 - loglik: -1.0648e+02 - logprior: -1.4694e+00
Epoch 4/10
19/19 - 1s - loss: 106.3854 - loglik: -1.0501e+02 - logprior: -1.3777e+00
Epoch 5/10
19/19 - 1s - loss: 105.6905 - loglik: -1.0436e+02 - logprior: -1.3300e+00
Epoch 6/10
19/19 - 1s - loss: 105.3552 - loglik: -1.0402e+02 - logprior: -1.3325e+00
Epoch 7/10
19/19 - 1s - loss: 104.9741 - loglik: -1.0366e+02 - logprior: -1.3160e+00
Epoch 8/10
19/19 - 1s - loss: 104.8790 - loglik: -1.0357e+02 - logprior: -1.3100e+00
Epoch 9/10
19/19 - 1s - loss: 104.9182 - loglik: -1.0362e+02 - logprior: -1.2995e+00
Fitted a model with MAP estimate = -103.3299
expansions: [(10, 2), (12, 3), (13, 3), (22, 2), (28, 3), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.8627 - loglik: -1.0265e+02 - logprior: -4.2175e+00
Epoch 2/2
19/19 - 1s - loss: 97.0454 - loglik: -9.4950e+01 - logprior: -2.0956e+00
Fitted a model with MAP estimate = -94.0916
expansions: [(0, 1)]
discards: [ 0  9 17 30 38]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.6724 - loglik: -9.4431e+01 - logprior: -3.2415e+00
Epoch 2/2
19/19 - 1s - loss: 93.7537 - loglik: -9.2355e+01 - logprior: -1.3984e+00
Fitted a model with MAP estimate = -92.0672
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.6801 - loglik: -9.1472e+01 - logprior: -3.2080e+00
Epoch 2/10
19/19 - 1s - loss: 92.3895 - loglik: -9.1003e+01 - logprior: -1.3869e+00
Epoch 3/10
19/19 - 1s - loss: 91.7994 - loglik: -9.0549e+01 - logprior: -1.2508e+00
Epoch 4/10
19/19 - 1s - loss: 91.2335 - loglik: -9.0018e+01 - logprior: -1.2151e+00
Epoch 5/10
19/19 - 1s - loss: 90.7654 - loglik: -8.9568e+01 - logprior: -1.1979e+00
Epoch 6/10
19/19 - 1s - loss: 90.5942 - loglik: -8.9402e+01 - logprior: -1.1926e+00
Epoch 7/10
19/19 - 1s - loss: 90.5784 - loglik: -8.9403e+01 - logprior: -1.1754e+00
Epoch 8/10
19/19 - 1s - loss: 90.0961 - loglik: -8.8924e+01 - logprior: -1.1724e+00
Epoch 9/10
19/19 - 1s - loss: 90.6841 - loglik: -8.9526e+01 - logprior: -1.1583e+00
Fitted a model with MAP estimate = -90.2599
Time for alignment: 54.4021
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.6662 - loglik: -1.3142e+02 - logprior: -3.2416e+00
Epoch 2/10
19/19 - 1s - loss: 114.2169 - loglik: -1.1284e+02 - logprior: -1.3743e+00
Epoch 3/10
19/19 - 1s - loss: 106.6888 - loglik: -1.0530e+02 - logprior: -1.3867e+00
Epoch 4/10
19/19 - 1s - loss: 104.4050 - loglik: -1.0304e+02 - logprior: -1.3667e+00
Epoch 5/10
19/19 - 1s - loss: 103.7908 - loglik: -1.0245e+02 - logprior: -1.3439e+00
Epoch 6/10
19/19 - 1s - loss: 103.2648 - loglik: -1.0189e+02 - logprior: -1.3700e+00
Epoch 7/10
19/19 - 1s - loss: 103.1249 - loglik: -1.0178e+02 - logprior: -1.3422e+00
Epoch 8/10
19/19 - 1s - loss: 103.0546 - loglik: -1.0171e+02 - logprior: -1.3451e+00
Epoch 9/10
19/19 - 1s - loss: 102.9054 - loglik: -1.0157e+02 - logprior: -1.3320e+00
Epoch 10/10
19/19 - 1s - loss: 102.8176 - loglik: -1.0148e+02 - logprior: -1.3344e+00
Fitted a model with MAP estimate = -101.3877
expansions: [(6, 1), (10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 1), (28, 2), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.0485 - loglik: -1.0190e+02 - logprior: -4.1521e+00
Epoch 2/2
19/19 - 1s - loss: 97.0501 - loglik: -9.5150e+01 - logprior: -1.9001e+00
Fitted a model with MAP estimate = -93.7770
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.8930 - loglik: -9.3676e+01 - logprior: -3.2165e+00
Epoch 2/2
19/19 - 1s - loss: 93.7322 - loglik: -9.2350e+01 - logprior: -1.3827e+00
Fitted a model with MAP estimate = -92.0594
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.7525 - loglik: -9.1550e+01 - logprior: -3.2029e+00
Epoch 2/10
19/19 - 1s - loss: 92.2562 - loglik: -9.0872e+01 - logprior: -1.3844e+00
Epoch 3/10
19/19 - 1s - loss: 91.7207 - loglik: -9.0472e+01 - logprior: -1.2491e+00
Epoch 4/10
19/19 - 1s - loss: 91.3211 - loglik: -9.0106e+01 - logprior: -1.2153e+00
Epoch 5/10
19/19 - 1s - loss: 90.8897 - loglik: -8.9696e+01 - logprior: -1.1935e+00
Epoch 6/10
19/19 - 1s - loss: 90.5872 - loglik: -8.9399e+01 - logprior: -1.1878e+00
Epoch 7/10
19/19 - 1s - loss: 90.6909 - loglik: -8.9511e+01 - logprior: -1.1799e+00
Fitted a model with MAP estimate = -90.4004
Time for alignment: 43.1867
Computed alignments with likelihoods: ['-90.4125', '-90.3823', '-90.2355', '-90.2599', '-90.4004']
Best model has likelihood: -90.2355  (prior= -1.1711 )
time for generating output: 0.0886
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9438877755511023
Training of 5 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1b31d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb2798e190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feafd4d3d00>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 652.6252 - loglik: -6.3406e+02 - logprior: -1.8561e+01
Epoch 2/10
15/15 - 6s - loss: 568.8719 - loglik: -5.6860e+02 - logprior: -2.7101e-01
Epoch 3/10
15/15 - 6s - loss: 506.0146 - loglik: -5.0666e+02 - logprior: 0.6487
Epoch 4/10
15/15 - 6s - loss: 486.5273 - loglik: -4.8693e+02 - logprior: 0.4015
Epoch 5/10
15/15 - 6s - loss: 476.1206 - loglik: -4.7618e+02 - logprior: 0.0618
Epoch 6/10
15/15 - 6s - loss: 471.9088 - loglik: -4.7205e+02 - logprior: 0.1418
Epoch 7/10
15/15 - 6s - loss: 472.0296 - loglik: -4.7239e+02 - logprior: 0.3610
Fitted a model with MAP estimate = -469.7094
expansions: [(25, 2), (55, 2), (56, 1), (63, 1), (79, 2), (80, 7), (81, 3), (82, 2), (84, 1), (90, 1), (99, 1), (103, 1), (104, 2), (105, 6), (106, 2), (110, 1), (132, 2), (133, 2), (134, 4), (135, 2), (136, 1), (137, 1), (138, 1), (156, 1), (157, 5), (158, 2), (166, 1), (172, 8)]
discards: [  1   2   3   6 223]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 477.1875 - loglik: -4.6064e+02 - logprior: -1.6549e+01
Epoch 2/2
15/15 - 9s - loss: 438.4357 - loglik: -4.3855e+02 - logprior: 0.1147
Fitted a model with MAP estimate = -435.4146
expansions: [(0, 3), (18, 1), (20, 1), (24, 1), (82, 2), (244, 2)]
discards: [  1 125 134 166 174 231 232 264]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 463.7086 - loglik: -4.3903e+02 - logprior: -2.4675e+01
Epoch 2/2
15/15 - 9s - loss: 436.1972 - loglik: -4.3415e+02 - logprior: -2.0488e+00
Fitted a model with MAP estimate = -430.9244
expansions: [(206, 1), (261, 1)]
discards: [  0   1   2  88 168 234 245]
Re-initialized the encoder parameters.
Fitting a model of length 282 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 450.2827 - loglik: -4.3291e+02 - logprior: -1.7376e+01
Epoch 2/10
15/15 - 9s - loss: 433.6505 - loglik: -4.3524e+02 - logprior: 1.5857
Epoch 3/10
15/15 - 9s - loss: 426.8991 - loglik: -4.3119e+02 - logprior: 4.2940
Epoch 4/10
15/15 - 9s - loss: 427.6888 - loglik: -4.3289e+02 - logprior: 5.2050
Fitted a model with MAP estimate = -424.6994
Time for alignment: 144.4144
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 652.3417 - loglik: -6.3374e+02 - logprior: -1.8599e+01
Epoch 2/10
15/15 - 6s - loss: 570.8639 - loglik: -5.7049e+02 - logprior: -3.7760e-01
Epoch 3/10
15/15 - 6s - loss: 511.1586 - loglik: -5.1162e+02 - logprior: 0.4613
Epoch 4/10
15/15 - 6s - loss: 477.1588 - loglik: -4.7728e+02 - logprior: 0.1193
Epoch 5/10
15/15 - 6s - loss: 471.1183 - loglik: -4.7090e+02 - logprior: -2.1357e-01
Epoch 6/10
15/15 - 6s - loss: 468.6642 - loglik: -4.6855e+02 - logprior: -1.1451e-01
Epoch 7/10
15/15 - 6s - loss: 465.9420 - loglik: -4.6605e+02 - logprior: 0.1045
Epoch 8/10
15/15 - 6s - loss: 468.9533 - loglik: -4.6915e+02 - logprior: 0.1979
Fitted a model with MAP estimate = -466.1472
expansions: [(19, 1), (25, 4), (39, 1), (49, 2), (51, 2), (52, 1), (54, 2), (55, 2), (66, 1), (69, 3), (70, 6), (72, 2), (75, 1), (83, 1), (84, 1), (99, 1), (100, 1), (102, 4), (108, 1), (130, 2), (131, 2), (132, 1), (133, 2), (134, 3), (136, 1), (156, 1), (157, 7), (172, 8), (182, 2), (197, 1)]
discards: [  1   2   3   6 203 223]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 473.4195 - loglik: -4.5689e+02 - logprior: -1.6528e+01
Epoch 2/2
15/15 - 9s - loss: 437.0576 - loglik: -4.3729e+02 - logprior: 0.2305
Fitted a model with MAP estimate = -433.9366
expansions: [(61, 1), (204, 1)]
discards: [  1   2  22 131 166 174 230 231 232 233]
Re-initialized the encoder parameters.
Fitting a model of length 278 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 454.4160 - loglik: -4.3881e+02 - logprior: -1.5609e+01
Epoch 2/2
15/15 - 8s - loss: 433.1359 - loglik: -4.3402e+02 - logprior: 0.8864
Fitted a model with MAP estimate = -431.3747
expansions: [(0, 4), (22, 1), (81, 1), (198, 2)]
discards: [160]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 457.3488 - loglik: -4.3324e+02 - logprior: -2.4107e+01
Epoch 2/10
15/15 - 9s - loss: 433.5210 - loglik: -4.3228e+02 - logprior: -1.2388e+00
Epoch 3/10
15/15 - 9s - loss: 426.3404 - loglik: -4.3011e+02 - logprior: 3.7712
Epoch 4/10
15/15 - 9s - loss: 425.1788 - loglik: -4.3036e+02 - logprior: 5.1841
Epoch 5/10
15/15 - 9s - loss: 424.0328 - loglik: -4.2986e+02 - logprior: 5.8294
Epoch 6/10
15/15 - 9s - loss: 422.7162 - loglik: -4.2892e+02 - logprior: 6.2086
Epoch 7/10
15/15 - 9s - loss: 422.8101 - loglik: -4.2937e+02 - logprior: 6.5642
Fitted a model with MAP estimate = -421.6399
Time for alignment: 176.1139
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 655.0655 - loglik: -6.3652e+02 - logprior: -1.8543e+01
Epoch 2/10
15/15 - 6s - loss: 569.9260 - loglik: -5.6951e+02 - logprior: -4.1814e-01
Epoch 3/10
15/15 - 6s - loss: 509.9077 - loglik: -5.1024e+02 - logprior: 0.3282
Epoch 4/10
15/15 - 6s - loss: 483.0742 - loglik: -4.8302e+02 - logprior: -5.1249e-02
Epoch 5/10
15/15 - 6s - loss: 472.3814 - loglik: -4.7196e+02 - logprior: -4.2020e-01
Epoch 6/10
15/15 - 6s - loss: 473.4978 - loglik: -4.7322e+02 - logprior: -2.7769e-01
Fitted a model with MAP estimate = -471.5870
expansions: [(27, 1), (41, 1), (42, 1), (46, 1), (50, 1), (52, 2), (53, 1), (55, 2), (56, 1), (57, 2), (71, 1), (73, 1), (74, 5), (76, 1), (97, 1), (101, 2), (103, 2), (104, 5), (106, 2), (110, 1), (132, 2), (133, 2), (134, 4), (135, 2), (136, 1), (137, 1), (139, 1), (156, 1), (157, 3), (158, 4), (173, 8), (198, 1)]
discards: [  1   2   3   4 200 223]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 477.9326 - loglik: -4.6104e+02 - logprior: -1.6895e+01
Epoch 2/2
15/15 - 9s - loss: 442.7227 - loglik: -4.4255e+02 - logprior: -1.7570e-01
Fitted a model with MAP estimate = -438.3444
expansions: [(42, 1), (90, 1), (106, 1), (202, 1), (242, 2)]
discards: [124 133 165 173 230 231]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 11s - loss: 453.6532 - loglik: -4.3786e+02 - logprior: -1.5798e+01
Epoch 2/2
15/15 - 9s - loss: 436.8101 - loglik: -4.3767e+02 - logprior: 0.8554
Fitted a model with MAP estimate = -431.7385
expansions: [(22, 1), (94, 1)]
discards: [  1 163 230]
Re-initialized the encoder parameters.
Fitting a model of length 282 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 451.6720 - loglik: -4.3657e+02 - logprior: -1.5099e+01
Epoch 2/10
15/15 - 9s - loss: 431.7871 - loglik: -4.3347e+02 - logprior: 1.6786
Epoch 3/10
15/15 - 9s - loss: 428.1152 - loglik: -4.3239e+02 - logprior: 4.2735
Epoch 4/10
15/15 - 9s - loss: 426.6508 - loglik: -4.3194e+02 - logprior: 5.2866
Epoch 5/10
15/15 - 9s - loss: 426.6248 - loglik: -4.3252e+02 - logprior: 5.8917
Epoch 6/10
15/15 - 9s - loss: 424.6113 - loglik: -4.3098e+02 - logprior: 6.3659
Epoch 7/10
15/15 - 9s - loss: 423.4369 - loglik: -4.3011e+02 - logprior: 6.6753
Epoch 8/10
15/15 - 9s - loss: 424.7773 - loglik: -4.3179e+02 - logprior: 7.0077
Fitted a model with MAP estimate = -423.6636
Time for alignment: 173.1358
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 654.9382 - loglik: -6.3641e+02 - logprior: -1.8530e+01
Epoch 2/10
15/15 - 6s - loss: 568.0876 - loglik: -5.6760e+02 - logprior: -4.8953e-01
Epoch 3/10
15/15 - 6s - loss: 505.9504 - loglik: -5.0614e+02 - logprior: 0.1900
Epoch 4/10
15/15 - 6s - loss: 480.2769 - loglik: -4.8020e+02 - logprior: -7.4782e-02
Epoch 5/10
15/15 - 6s - loss: 469.6526 - loglik: -4.6917e+02 - logprior: -4.8397e-01
Epoch 6/10
15/15 - 6s - loss: 469.4725 - loglik: -4.6910e+02 - logprior: -3.7471e-01
Epoch 7/10
15/15 - 6s - loss: 465.9634 - loglik: -4.6575e+02 - logprior: -2.1778e-01
Epoch 8/10
15/15 - 6s - loss: 467.5555 - loglik: -4.6737e+02 - logprior: -1.8090e-01
Fitted a model with MAP estimate = -465.6057
expansions: [(25, 4), (39, 1), (49, 1), (51, 2), (52, 1), (55, 1), (58, 1), (73, 4), (74, 3), (75, 2), (77, 2), (79, 1), (82, 1), (89, 1), (97, 1), (103, 3), (105, 3), (111, 1), (133, 2), (134, 2), (135, 1), (136, 2), (137, 2), (138, 1), (139, 1), (140, 1), (141, 1), (157, 1), (158, 4), (159, 3), (174, 8), (181, 1), (182, 2)]
discards: [  1   2   3 223]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 469.9486 - loglik: -4.5364e+02 - logprior: -1.6310e+01
Epoch 2/2
15/15 - 9s - loss: 438.8709 - loglik: -4.3950e+02 - logprior: 0.6340
Fitted a model with MAP estimate = -432.2303
expansions: []
discards: [  1 166 174 231 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 279 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 11s - loss: 452.2545 - loglik: -4.3675e+02 - logprior: -1.5501e+01
Epoch 2/2
15/15 - 9s - loss: 436.4926 - loglik: -4.3770e+02 - logprior: 1.2072
Fitted a model with MAP estimate = -431.5337
expansions: [(0, 3), (200, 1), (239, 2)]
discards: [162]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 458.0453 - loglik: -4.3388e+02 - logprior: -2.4170e+01
Epoch 2/10
15/15 - 9s - loss: 433.0361 - loglik: -4.3180e+02 - logprior: -1.2398e+00
Epoch 3/10
15/15 - 9s - loss: 425.3436 - loglik: -4.2922e+02 - logprior: 3.8793
Epoch 4/10
15/15 - 9s - loss: 428.3253 - loglik: -4.3377e+02 - logprior: 5.4491
Fitted a model with MAP estimate = -424.2228
Time for alignment: 149.5323
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 656.2204 - loglik: -6.3765e+02 - logprior: -1.8569e+01
Epoch 2/10
15/15 - 6s - loss: 565.9055 - loglik: -5.6563e+02 - logprior: -2.7943e-01
Epoch 3/10
15/15 - 6s - loss: 511.4294 - loglik: -5.1217e+02 - logprior: 0.7376
Epoch 4/10
15/15 - 6s - loss: 482.3768 - loglik: -4.8268e+02 - logprior: 0.3025
Epoch 5/10
15/15 - 6s - loss: 472.3826 - loglik: -4.7255e+02 - logprior: 0.1691
Epoch 6/10
15/15 - 6s - loss: 471.7594 - loglik: -4.7195e+02 - logprior: 0.1899
Epoch 7/10
15/15 - 6s - loss: 470.0271 - loglik: -4.7050e+02 - logprior: 0.4681
Epoch 8/10
15/15 - 6s - loss: 470.9685 - loglik: -4.7150e+02 - logprior: 0.5342
Fitted a model with MAP estimate = -468.8808
expansions: [(19, 1), (35, 1), (55, 1), (58, 1), (60, 1), (61, 2), (69, 1), (73, 1), (74, 1), (75, 1), (76, 9), (77, 3), (79, 2), (81, 1), (97, 1), (103, 1), (104, 2), (105, 4), (134, 2), (135, 2), (136, 7), (137, 2), (139, 1), (158, 5), (159, 2), (174, 8)]
discards: [  1   2   6 223]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 473.0433 - loglik: -4.5663e+02 - logprior: -1.6408e+01
Epoch 2/2
15/15 - 9s - loss: 438.1506 - loglik: -4.3856e+02 - logprior: 0.4070
Fitted a model with MAP estimate = -434.1297
expansions: [(0, 3), (64, 1), (94, 2)]
discards: [129 168 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 461.9250 - loglik: -4.3735e+02 - logprior: -2.4580e+01
Epoch 2/2
15/15 - 9s - loss: 435.1949 - loglik: -4.3315e+02 - logprior: -2.0486e+00
Fitted a model with MAP estimate = -431.2814
expansions: [(208, 1), (245, 4)]
discards: [  0   1   2 170]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 449.5438 - loglik: -4.3236e+02 - logprior: -1.7185e+01
Epoch 2/10
15/15 - 9s - loss: 430.2366 - loglik: -4.3216e+02 - logprior: 1.9207
Epoch 3/10
15/15 - 9s - loss: 426.6530 - loglik: -4.3136e+02 - logprior: 4.7093
Epoch 4/10
15/15 - 9s - loss: 425.8056 - loglik: -4.3136e+02 - logprior: 5.5583
Epoch 5/10
15/15 - 9s - loss: 418.8676 - loglik: -4.2492e+02 - logprior: 6.0532
Epoch 6/10
15/15 - 9s - loss: 421.4892 - loglik: -4.2798e+02 - logprior: 6.4879
Fitted a model with MAP estimate = -421.1962
Time for alignment: 168.8988
Computed alignments with likelihoods: ['-424.6994', '-421.6399', '-423.6636', '-424.2228', '-421.1962']
Best model has likelihood: -421.1962  (prior= 6.6323 )
time for generating output: 0.3080
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.9150657229524772
Training of 5 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f619340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb497d00d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feada828100>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 227.0260 - loglik: -1.8536e+02 - logprior: -4.1669e+01
Epoch 2/10
10/10 - 1s - loss: 179.8673 - loglik: -1.6866e+02 - logprior: -1.1208e+01
Epoch 3/10
10/10 - 1s - loss: 160.8840 - loglik: -1.5533e+02 - logprior: -5.5493e+00
Epoch 4/10
10/10 - 1s - loss: 152.3611 - loglik: -1.4883e+02 - logprior: -3.5345e+00
Epoch 5/10
10/10 - 1s - loss: 148.2151 - loglik: -1.4572e+02 - logprior: -2.4996e+00
Epoch 6/10
10/10 - 1s - loss: 145.6831 - loglik: -1.4365e+02 - logprior: -2.0307e+00
Epoch 7/10
10/10 - 1s - loss: 144.6967 - loglik: -1.4306e+02 - logprior: -1.6389e+00
Epoch 8/10
10/10 - 1s - loss: 143.9646 - loglik: -1.4267e+02 - logprior: -1.2965e+00
Epoch 9/10
10/10 - 1s - loss: 143.5001 - loglik: -1.4236e+02 - logprior: -1.1375e+00
Epoch 10/10
10/10 - 1s - loss: 142.9716 - loglik: -1.4188e+02 - logprior: -1.0892e+00
Fitted a model with MAP estimate = -142.8106
expansions: [(0, 2), (16, 1), (17, 2), (18, 2), (21, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 194.5428 - loglik: -1.3973e+02 - logprior: -5.4815e+01
Epoch 2/2
10/10 - 1s - loss: 152.0995 - loglik: -1.3534e+02 - logprior: -1.6756e+01
Fitted a model with MAP estimate = -144.4657
expansions: [(3, 1)]
discards: [ 0 20]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 182.3851 - loglik: -1.3484e+02 - logprior: -4.7547e+01
Epoch 2/2
10/10 - 1s - loss: 153.4025 - loglik: -1.3493e+02 - logprior: -1.8471e+01
Fitted a model with MAP estimate = -148.3302
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 177.1109 - loglik: -1.3445e+02 - logprior: -4.2662e+01
Epoch 2/10
10/10 - 1s - loss: 145.7139 - loglik: -1.3404e+02 - logprior: -1.1669e+01
Epoch 3/10
10/10 - 1s - loss: 138.6763 - loglik: -1.3404e+02 - logprior: -4.6362e+00
Epoch 4/10
10/10 - 1s - loss: 135.8287 - loglik: -1.3362e+02 - logprior: -2.2069e+00
Epoch 5/10
10/10 - 1s - loss: 134.4443 - loglik: -1.3327e+02 - logprior: -1.1705e+00
Epoch 6/10
10/10 - 1s - loss: 133.6162 - loglik: -1.3297e+02 - logprior: -6.4574e-01
Epoch 7/10
10/10 - 1s - loss: 133.0458 - loglik: -1.3293e+02 - logprior: -1.1933e-01
Epoch 8/10
10/10 - 1s - loss: 132.8093 - loglik: -1.3308e+02 - logprior: 0.2751
Epoch 9/10
10/10 - 1s - loss: 132.5479 - loglik: -1.3301e+02 - logprior: 0.4667
Epoch 10/10
10/10 - 1s - loss: 132.2561 - loglik: -1.3285e+02 - logprior: 0.5968
Fitted a model with MAP estimate = -132.2491
Time for alignment: 29.7255
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.0456 - loglik: -1.8538e+02 - logprior: -4.1669e+01
Epoch 2/10
10/10 - 1s - loss: 180.2856 - loglik: -1.6908e+02 - logprior: -1.1204e+01
Epoch 3/10
10/10 - 1s - loss: 162.1715 - loglik: -1.5662e+02 - logprior: -5.5562e+00
Epoch 4/10
10/10 - 1s - loss: 153.2569 - loglik: -1.4972e+02 - logprior: -3.5369e+00
Epoch 5/10
10/10 - 1s - loss: 149.0539 - loglik: -1.4657e+02 - logprior: -2.4837e+00
Epoch 6/10
10/10 - 1s - loss: 147.0228 - loglik: -1.4501e+02 - logprior: -2.0162e+00
Epoch 7/10
10/10 - 1s - loss: 145.5773 - loglik: -1.4390e+02 - logprior: -1.6752e+00
Epoch 8/10
10/10 - 1s - loss: 144.6840 - loglik: -1.4327e+02 - logprior: -1.4168e+00
Epoch 9/10
10/10 - 1s - loss: 143.9565 - loglik: -1.4266e+02 - logprior: -1.2940e+00
Epoch 10/10
10/10 - 1s - loss: 143.4621 - loglik: -1.4224e+02 - logprior: -1.2216e+00
Fitted a model with MAP estimate = -143.2862
expansions: [(0, 2), (11, 1), (16, 1), (17, 2), (18, 1), (20, 2), (21, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 194.0004 - loglik: -1.3948e+02 - logprior: -5.4520e+01
Epoch 2/2
10/10 - 1s - loss: 151.0192 - loglik: -1.3442e+02 - logprior: -1.6594e+01
Fitted a model with MAP estimate = -143.6890
expansions: []
discards: [ 0 21]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 181.1473 - loglik: -1.3399e+02 - logprior: -4.7155e+01
Epoch 2/2
10/10 - 1s - loss: 152.3341 - loglik: -1.3403e+02 - logprior: -1.8309e+01
Fitted a model with MAP estimate = -147.4588
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 176.1754 - loglik: -1.3375e+02 - logprior: -4.2425e+01
Epoch 2/10
10/10 - 1s - loss: 144.6150 - loglik: -1.3317e+02 - logprior: -1.1443e+01
Epoch 3/10
10/10 - 1s - loss: 137.6597 - loglik: -1.3323e+02 - logprior: -4.4249e+00
Epoch 4/10
10/10 - 1s - loss: 135.1319 - loglik: -1.3316e+02 - logprior: -1.9674e+00
Epoch 5/10
10/10 - 1s - loss: 133.7109 - loglik: -1.3286e+02 - logprior: -8.5310e-01
Epoch 6/10
10/10 - 1s - loss: 132.8250 - loglik: -1.3250e+02 - logprior: -3.2855e-01
Epoch 7/10
10/10 - 1s - loss: 132.4342 - loglik: -1.3250e+02 - logprior: 0.0619
Epoch 8/10
10/10 - 1s - loss: 132.2107 - loglik: -1.3263e+02 - logprior: 0.4240
Epoch 9/10
10/10 - 1s - loss: 131.9444 - loglik: -1.3260e+02 - logprior: 0.6520
Epoch 10/10
10/10 - 1s - loss: 131.7639 - loglik: -1.3256e+02 - logprior: 0.7948
Fitted a model with MAP estimate = -131.6467
Time for alignment: 28.7423
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.0407 - loglik: -1.8537e+02 - logprior: -4.1669e+01
Epoch 2/10
10/10 - 1s - loss: 180.3376 - loglik: -1.6912e+02 - logprior: -1.1214e+01
Epoch 3/10
10/10 - 1s - loss: 162.1177 - loglik: -1.5651e+02 - logprior: -5.6115e+00
Epoch 4/10
10/10 - 1s - loss: 153.6850 - loglik: -1.5013e+02 - logprior: -3.5573e+00
Epoch 5/10
10/10 - 1s - loss: 148.7274 - loglik: -1.4623e+02 - logprior: -2.4944e+00
Epoch 6/10
10/10 - 1s - loss: 146.2075 - loglik: -1.4417e+02 - logprior: -2.0399e+00
Epoch 7/10
10/10 - 1s - loss: 145.1153 - loglik: -1.4347e+02 - logprior: -1.6435e+00
Epoch 8/10
10/10 - 1s - loss: 144.2729 - loglik: -1.4300e+02 - logprior: -1.2741e+00
Epoch 9/10
10/10 - 1s - loss: 143.9284 - loglik: -1.4281e+02 - logprior: -1.1208e+00
Epoch 10/10
10/10 - 1s - loss: 143.6686 - loglik: -1.4259e+02 - logprior: -1.0759e+00
Fitted a model with MAP estimate = -143.4336
expansions: [(0, 2), (16, 1), (17, 1), (19, 2), (20, 2), (24, 1), (28, 1), (37, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 194.2961 - loglik: -1.3962e+02 - logprior: -5.4676e+01
Epoch 2/2
10/10 - 1s - loss: 151.5061 - loglik: -1.3488e+02 - logprior: -1.6622e+01
Fitted a model with MAP estimate = -143.8624
expansions: [(3, 1)]
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 181.6330 - loglik: -1.3439e+02 - logprior: -4.7239e+01
Epoch 2/2
10/10 - 1s - loss: 152.4143 - loglik: -1.3411e+02 - logprior: -1.8304e+01
Fitted a model with MAP estimate = -147.3601
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 176.1156 - loglik: -1.3364e+02 - logprior: -4.2474e+01
Epoch 2/10
10/10 - 1s - loss: 144.3160 - loglik: -1.3284e+02 - logprior: -1.1480e+01
Epoch 3/10
10/10 - 1s - loss: 137.3605 - loglik: -1.3293e+02 - logprior: -4.4317e+00
Epoch 4/10
10/10 - 1s - loss: 134.4668 - loglik: -1.3249e+02 - logprior: -1.9816e+00
Epoch 5/10
10/10 - 1s - loss: 133.0649 - loglik: -1.3212e+02 - logprior: -9.4062e-01
Epoch 6/10
10/10 - 1s - loss: 132.2630 - loglik: -1.3182e+02 - logprior: -4.3830e-01
Epoch 7/10
10/10 - 1s - loss: 131.8201 - loglik: -1.3189e+02 - logprior: 0.0688
Epoch 8/10
10/10 - 1s - loss: 131.3953 - loglik: -1.3187e+02 - logprior: 0.4717
Epoch 9/10
10/10 - 1s - loss: 131.2384 - loglik: -1.3191e+02 - logprior: 0.6696
Epoch 10/10
10/10 - 1s - loss: 131.0251 - loglik: -1.3182e+02 - logprior: 0.7926
Fitted a model with MAP estimate = -130.9195
Time for alignment: 28.5373
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.0761 - loglik: -1.8541e+02 - logprior: -4.1668e+01
Epoch 2/10
10/10 - 1s - loss: 180.2564 - loglik: -1.6905e+02 - logprior: -1.1206e+01
Epoch 3/10
10/10 - 1s - loss: 162.0327 - loglik: -1.5648e+02 - logprior: -5.5546e+00
Epoch 4/10
10/10 - 1s - loss: 152.5736 - loglik: -1.4902e+02 - logprior: -3.5512e+00
Epoch 5/10
10/10 - 1s - loss: 148.3152 - loglik: -1.4574e+02 - logprior: -2.5761e+00
Epoch 6/10
10/10 - 1s - loss: 146.4313 - loglik: -1.4433e+02 - logprior: -2.0979e+00
Epoch 7/10
10/10 - 1s - loss: 145.1187 - loglik: -1.4343e+02 - logprior: -1.6894e+00
Epoch 8/10
10/10 - 1s - loss: 144.4183 - loglik: -1.4305e+02 - logprior: -1.3698e+00
Epoch 9/10
10/10 - 1s - loss: 143.8154 - loglik: -1.4258e+02 - logprior: -1.2358e+00
Epoch 10/10
10/10 - 1s - loss: 143.2967 - loglik: -1.4212e+02 - logprior: -1.1765e+00
Fitted a model with MAP estimate = -143.2200
expansions: [(0, 3), (16, 1), (17, 1), (19, 1), (20, 1), (22, 1), (25, 1), (27, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 193.3205 - loglik: -1.3878e+02 - logprior: -5.4537e+01
Epoch 2/2
10/10 - 1s - loss: 150.6106 - loglik: -1.3415e+02 - logprior: -1.6461e+01
Fitted a model with MAP estimate = -143.1848
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.1762 - loglik: -1.3403e+02 - logprior: -4.7143e+01
Epoch 2/2
10/10 - 1s - loss: 152.1263 - loglik: -1.3379e+02 - logprior: -1.8335e+01
Fitted a model with MAP estimate = -147.2482
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 177.1334 - loglik: -1.3341e+02 - logprior: -4.3726e+01
Epoch 2/10
10/10 - 1s - loss: 145.4378 - loglik: -1.3293e+02 - logprior: -1.2504e+01
Epoch 3/10
10/10 - 1s - loss: 137.5824 - loglik: -1.3291e+02 - logprior: -4.6747e+00
Epoch 4/10
10/10 - 1s - loss: 134.4582 - loglik: -1.3236e+02 - logprior: -2.1012e+00
Epoch 5/10
10/10 - 1s - loss: 132.9939 - loglik: -1.3195e+02 - logprior: -1.0445e+00
Epoch 6/10
10/10 - 0s - loss: 132.2437 - loglik: -1.3177e+02 - logprior: -4.7747e-01
Epoch 7/10
10/10 - 1s - loss: 131.8965 - loglik: -1.3195e+02 - logprior: 0.0535
Epoch 8/10
10/10 - 1s - loss: 131.4587 - loglik: -1.3189e+02 - logprior: 0.4297
Epoch 9/10
10/10 - 1s - loss: 131.2616 - loglik: -1.3188e+02 - logprior: 0.6160
Epoch 10/10
10/10 - 1s - loss: 131.0154 - loglik: -1.3177e+02 - logprior: 0.7502
Fitted a model with MAP estimate = -130.9820
Time for alignment: 28.6658
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.9400 - loglik: -1.8527e+02 - logprior: -4.1670e+01
Epoch 2/10
10/10 - 1s - loss: 179.8474 - loglik: -1.6864e+02 - logprior: -1.1203e+01
Epoch 3/10
10/10 - 1s - loss: 160.7846 - loglik: -1.5523e+02 - logprior: -5.5580e+00
Epoch 4/10
10/10 - 1s - loss: 151.3656 - loglik: -1.4779e+02 - logprior: -3.5777e+00
Epoch 5/10
10/10 - 1s - loss: 147.4613 - loglik: -1.4491e+02 - logprior: -2.5504e+00
Epoch 6/10
10/10 - 1s - loss: 145.5217 - loglik: -1.4349e+02 - logprior: -2.0269e+00
Epoch 7/10
10/10 - 1s - loss: 144.3023 - loglik: -1.4266e+02 - logprior: -1.6433e+00
Epoch 8/10
10/10 - 1s - loss: 143.6903 - loglik: -1.4239e+02 - logprior: -1.3024e+00
Epoch 9/10
10/10 - 1s - loss: 143.2588 - loglik: -1.4211e+02 - logprior: -1.1537e+00
Epoch 10/10
10/10 - 1s - loss: 142.7921 - loglik: -1.4169e+02 - logprior: -1.0988e+00
Fitted a model with MAP estimate = -142.6593
expansions: [(0, 2), (1, 1), (16, 1), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 193.9848 - loglik: -1.3940e+02 - logprior: -5.4583e+01
Epoch 2/2
10/10 - 1s - loss: 151.6924 - loglik: -1.3516e+02 - logprior: -1.6533e+01
Fitted a model with MAP estimate = -144.3575
expansions: []
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.2193 - loglik: -1.3506e+02 - logprior: -4.7162e+01
Epoch 2/2
10/10 - 1s - loss: 153.4703 - loglik: -1.3512e+02 - logprior: -1.8352e+01
Fitted a model with MAP estimate = -148.2527
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 176.8903 - loglik: -1.3428e+02 - logprior: -4.2615e+01
Epoch 2/10
10/10 - 1s - loss: 145.3027 - loglik: -1.3377e+02 - logprior: -1.1530e+01
Epoch 3/10
10/10 - 1s - loss: 137.3433 - loglik: -1.3292e+02 - logprior: -4.4244e+00
Epoch 4/10
10/10 - 1s - loss: 134.6190 - loglik: -1.3265e+02 - logprior: -1.9694e+00
Epoch 5/10
10/10 - 1s - loss: 133.0055 - loglik: -1.3208e+02 - logprior: -9.2773e-01
Epoch 6/10
10/10 - 1s - loss: 132.2722 - loglik: -1.3185e+02 - logprior: -4.2039e-01
Epoch 7/10
10/10 - 1s - loss: 131.7044 - loglik: -1.3180e+02 - logprior: 0.0923
Epoch 8/10
10/10 - 1s - loss: 131.3659 - loglik: -1.3186e+02 - logprior: 0.4961
Epoch 9/10
10/10 - 1s - loss: 131.3102 - loglik: -1.3200e+02 - logprior: 0.6938
Epoch 10/10
10/10 - 1s - loss: 131.0312 - loglik: -1.3185e+02 - logprior: 0.8189
Fitted a model with MAP estimate = -130.9033
Time for alignment: 29.0504
Computed alignments with likelihoods: ['-132.2491', '-131.6467', '-130.9195', '-130.9820', '-130.9033']
Best model has likelihood: -130.9033  (prior= 0.9007 )
time for generating output: 0.1208
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.9208816049731563
Training of 5 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb059530a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feafd6b3be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f51f760>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.4037 - loglik: -2.3721e+02 - logprior: -1.3189e+01
Epoch 2/10
11/11 - 1s - loss: 222.1126 - loglik: -2.1869e+02 - logprior: -3.4240e+00
Epoch 3/10
11/11 - 1s - loss: 201.4786 - loglik: -1.9933e+02 - logprior: -2.1439e+00
Epoch 4/10
11/11 - 1s - loss: 192.2298 - loglik: -1.9023e+02 - logprior: -1.9953e+00
Epoch 5/10
11/11 - 1s - loss: 188.9684 - loglik: -1.8706e+02 - logprior: -1.9118e+00
Epoch 6/10
11/11 - 1s - loss: 187.4646 - loglik: -1.8568e+02 - logprior: -1.7805e+00
Epoch 7/10
11/11 - 1s - loss: 186.5675 - loglik: -1.8491e+02 - logprior: -1.6533e+00
Epoch 8/10
11/11 - 1s - loss: 186.5101 - loglik: -1.8489e+02 - logprior: -1.6191e+00
Epoch 9/10
11/11 - 1s - loss: 185.9787 - loglik: -1.8435e+02 - logprior: -1.6326e+00
Epoch 10/10
11/11 - 1s - loss: 186.4386 - loglik: -1.8480e+02 - logprior: -1.6397e+00
Fitted a model with MAP estimate = -185.7616
expansions: [(7, 4), (9, 2), (10, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (48, 2), (49, 1), (50, 1), (53, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 199.5949 - loglik: -1.8462e+02 - logprior: -1.4975e+01
Epoch 2/2
11/11 - 1s - loss: 184.4499 - loglik: -1.7799e+02 - logprior: -6.4569e+00
Fitted a model with MAP estimate = -181.6311
expansions: [(0, 2)]
discards: [ 0  8 15]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.4202 - loglik: -1.7661e+02 - logprior: -1.1806e+01
Epoch 2/2
11/11 - 1s - loss: 178.7067 - loglik: -1.7562e+02 - logprior: -3.0851e+00
Fitted a model with MAP estimate = -177.0185
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 191.0873 - loglik: -1.7688e+02 - logprior: -1.4208e+01
Epoch 2/10
11/11 - 1s - loss: 180.4485 - loglik: -1.7615e+02 - logprior: -4.2981e+00
Epoch 3/10
11/11 - 1s - loss: 176.8098 - loglik: -1.7469e+02 - logprior: -2.1243e+00
Epoch 4/10
11/11 - 1s - loss: 176.6998 - loglik: -1.7539e+02 - logprior: -1.3102e+00
Epoch 5/10
11/11 - 1s - loss: 175.8603 - loglik: -1.7479e+02 - logprior: -1.0696e+00
Epoch 6/10
11/11 - 1s - loss: 175.4360 - loglik: -1.7458e+02 - logprior: -8.6035e-01
Epoch 7/10
11/11 - 1s - loss: 175.0385 - loglik: -1.7423e+02 - logprior: -8.1120e-01
Epoch 8/10
11/11 - 1s - loss: 174.9886 - loglik: -1.7423e+02 - logprior: -7.5790e-01
Epoch 9/10
11/11 - 1s - loss: 175.1262 - loglik: -1.7439e+02 - logprior: -7.3653e-01
Fitted a model with MAP estimate = -174.7631
Time for alignment: 47.0771
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.7161 - loglik: -2.3752e+02 - logprior: -1.3195e+01
Epoch 2/10
11/11 - 1s - loss: 221.5838 - loglik: -2.1815e+02 - logprior: -3.4320e+00
Epoch 3/10
11/11 - 1s - loss: 202.7407 - loglik: -2.0061e+02 - logprior: -2.1349e+00
Epoch 4/10
11/11 - 1s - loss: 192.8263 - loglik: -1.9088e+02 - logprior: -1.9483e+00
Epoch 5/10
11/11 - 1s - loss: 189.1207 - loglik: -1.8724e+02 - logprior: -1.8829e+00
Epoch 6/10
11/11 - 1s - loss: 187.0568 - loglik: -1.8525e+02 - logprior: -1.8071e+00
Epoch 7/10
11/11 - 1s - loss: 187.1749 - loglik: -1.8547e+02 - logprior: -1.7045e+00
Fitted a model with MAP estimate = -186.3010
expansions: [(8, 2), (9, 1), (10, 2), (11, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (48, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 199.1129 - loglik: -1.8410e+02 - logprior: -1.5014e+01
Epoch 2/2
11/11 - 1s - loss: 183.4451 - loglik: -1.7704e+02 - logprior: -6.4060e+00
Fitted a model with MAP estimate = -181.4817
expansions: [(0, 2)]
discards: [ 0 15]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.5813 - loglik: -1.7675e+02 - logprior: -1.1827e+01
Epoch 2/2
11/11 - 1s - loss: 178.8250 - loglik: -1.7573e+02 - logprior: -3.0912e+00
Fitted a model with MAP estimate = -177.2239
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 191.5729 - loglik: -1.7723e+02 - logprior: -1.4343e+01
Epoch 2/10
11/11 - 1s - loss: 181.4428 - loglik: -1.7700e+02 - logprior: -4.4387e+00
Epoch 3/10
11/11 - 1s - loss: 175.7693 - loglik: -1.7360e+02 - logprior: -2.1682e+00
Epoch 4/10
11/11 - 1s - loss: 177.6468 - loglik: -1.7633e+02 - logprior: -1.3218e+00
Fitted a model with MAP estimate = -176.0919
Time for alignment: 37.4909
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.6061 - loglik: -2.3741e+02 - logprior: -1.3195e+01
Epoch 2/10
11/11 - 1s - loss: 221.5882 - loglik: -2.1815e+02 - logprior: -3.4349e+00
Epoch 3/10
11/11 - 1s - loss: 199.8475 - loglik: -1.9767e+02 - logprior: -2.1744e+00
Epoch 4/10
11/11 - 1s - loss: 190.3246 - loglik: -1.8828e+02 - logprior: -2.0482e+00
Epoch 5/10
11/11 - 1s - loss: 187.2505 - loglik: -1.8529e+02 - logprior: -1.9569e+00
Epoch 6/10
11/11 - 1s - loss: 186.3027 - loglik: -1.8452e+02 - logprior: -1.7789e+00
Epoch 7/10
11/11 - 1s - loss: 186.3495 - loglik: -1.8470e+02 - logprior: -1.6525e+00
Fitted a model with MAP estimate = -185.4847
expansions: [(11, 4), (12, 2), (24, 1), (25, 1), (31, 1), (32, 1), (34, 1), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 198.6458 - loglik: -1.8369e+02 - logprior: -1.4959e+01
Epoch 2/2
11/11 - 1s - loss: 183.7153 - loglik: -1.7740e+02 - logprior: -6.3126e+00
Fitted a model with MAP estimate = -181.4725
expansions: [(0, 3)]
discards: [ 0 15]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.9630 - loglik: -1.7713e+02 - logprior: -1.1835e+01
Epoch 2/2
11/11 - 1s - loss: 178.8890 - loglik: -1.7577e+02 - logprior: -3.1209e+00
Fitted a model with MAP estimate = -177.3419
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 191.3861 - loglik: -1.7702e+02 - logprior: -1.4363e+01
Epoch 2/10
11/11 - 1s - loss: 180.6984 - loglik: -1.7619e+02 - logprior: -4.5040e+00
Epoch 3/10
11/11 - 1s - loss: 177.1582 - loglik: -1.7502e+02 - logprior: -2.1399e+00
Epoch 4/10
11/11 - 1s - loss: 176.9003 - loglik: -1.7561e+02 - logprior: -1.2910e+00
Epoch 5/10
11/11 - 1s - loss: 175.9655 - loglik: -1.7493e+02 - logprior: -1.0317e+00
Epoch 6/10
11/11 - 1s - loss: 175.4531 - loglik: -1.7461e+02 - logprior: -8.4378e-01
Epoch 7/10
11/11 - 1s - loss: 174.9515 - loglik: -1.7417e+02 - logprior: -7.7974e-01
Epoch 8/10
11/11 - 1s - loss: 176.0640 - loglik: -1.7532e+02 - logprior: -7.3936e-01
Fitted a model with MAP estimate = -175.0766
Time for alignment: 41.3239
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.7044 - loglik: -2.3751e+02 - logprior: -1.3193e+01
Epoch 2/10
11/11 - 1s - loss: 222.4226 - loglik: -2.1900e+02 - logprior: -3.4255e+00
Epoch 3/10
11/11 - 1s - loss: 202.8937 - loglik: -2.0074e+02 - logprior: -2.1503e+00
Epoch 4/10
11/11 - 1s - loss: 191.8868 - loglik: -1.8987e+02 - logprior: -2.0122e+00
Epoch 5/10
11/11 - 1s - loss: 188.6214 - loglik: -1.8668e+02 - logprior: -1.9411e+00
Epoch 6/10
11/11 - 1s - loss: 186.6562 - loglik: -1.8483e+02 - logprior: -1.8234e+00
Epoch 7/10
11/11 - 1s - loss: 186.1443 - loglik: -1.8443e+02 - logprior: -1.7123e+00
Epoch 8/10
11/11 - 1s - loss: 185.4698 - loglik: -1.8377e+02 - logprior: -1.6967e+00
Epoch 9/10
11/11 - 1s - loss: 185.7877 - loglik: -1.8406e+02 - logprior: -1.7232e+00
Fitted a model with MAP estimate = -185.3277
expansions: [(7, 3), (9, 2), (10, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 1), (45, 1), (48, 2), (49, 1), (50, 1), (53, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 198.5211 - loglik: -1.8355e+02 - logprior: -1.4968e+01
Epoch 2/2
11/11 - 1s - loss: 183.9555 - loglik: -1.7760e+02 - logprior: -6.3557e+00
Fitted a model with MAP estimate = -181.3656
expansions: [(0, 2)]
discards: [ 0 14]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 188.0888 - loglik: -1.7632e+02 - logprior: -1.1771e+01
Epoch 2/2
11/11 - 1s - loss: 178.7125 - loglik: -1.7565e+02 - logprior: -3.0656e+00
Fitted a model with MAP estimate = -176.9657
expansions: []
discards: [ 0 62]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 191.8537 - loglik: -1.7765e+02 - logprior: -1.4205e+01
Epoch 2/10
11/11 - 1s - loss: 180.6685 - loglik: -1.7636e+02 - logprior: -4.3093e+00
Epoch 3/10
11/11 - 1s - loss: 177.5363 - loglik: -1.7540e+02 - logprior: -2.1370e+00
Epoch 4/10
11/11 - 1s - loss: 175.8539 - loglik: -1.7454e+02 - logprior: -1.3189e+00
Epoch 5/10
11/11 - 1s - loss: 176.1182 - loglik: -1.7504e+02 - logprior: -1.0757e+00
Fitted a model with MAP estimate = -175.7490
Time for alignment: 40.4657
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 251.1271 - loglik: -2.3794e+02 - logprior: -1.3192e+01
Epoch 2/10
11/11 - 1s - loss: 221.9097 - loglik: -2.1849e+02 - logprior: -3.4229e+00
Epoch 3/10
11/11 - 1s - loss: 200.7325 - loglik: -1.9858e+02 - logprior: -2.1485e+00
Epoch 4/10
11/11 - 1s - loss: 191.6676 - loglik: -1.8965e+02 - logprior: -2.0223e+00
Epoch 5/10
11/11 - 1s - loss: 188.3483 - loglik: -1.8638e+02 - logprior: -1.9642e+00
Epoch 6/10
11/11 - 1s - loss: 186.3598 - loglik: -1.8451e+02 - logprior: -1.8547e+00
Epoch 7/10
11/11 - 1s - loss: 186.0460 - loglik: -1.8430e+02 - logprior: -1.7505e+00
Epoch 8/10
11/11 - 1s - loss: 185.7688 - loglik: -1.8404e+02 - logprior: -1.7281e+00
Epoch 9/10
11/11 - 1s - loss: 185.8576 - loglik: -1.8409e+02 - logprior: -1.7627e+00
Fitted a model with MAP estimate = -185.1533
expansions: [(8, 3), (9, 1), (10, 1), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (49, 1), (50, 3), (53, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 199.0580 - loglik: -1.8409e+02 - logprior: -1.4969e+01
Epoch 2/2
11/11 - 1s - loss: 183.9140 - loglik: -1.7750e+02 - logprior: -6.4137e+00
Fitted a model with MAP estimate = -181.7020
expansions: [(0, 2)]
discards: [0 8]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 187.8426 - loglik: -1.7605e+02 - logprior: -1.1796e+01
Epoch 2/2
11/11 - 1s - loss: 177.7084 - loglik: -1.7463e+02 - logprior: -3.0791e+00
Fitted a model with MAP estimate = -176.9574
expansions: []
discards: [ 0 62]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 191.6060 - loglik: -1.7736e+02 - logprior: -1.4250e+01
Epoch 2/10
11/11 - 1s - loss: 180.3710 - loglik: -1.7603e+02 - logprior: -4.3451e+00
Epoch 3/10
11/11 - 1s - loss: 177.6964 - loglik: -1.7557e+02 - logprior: -2.1300e+00
Epoch 4/10
11/11 - 1s - loss: 176.3421 - loglik: -1.7503e+02 - logprior: -1.3164e+00
Epoch 5/10
11/11 - 1s - loss: 175.6456 - loglik: -1.7459e+02 - logprior: -1.0564e+00
Epoch 6/10
11/11 - 1s - loss: 175.2560 - loglik: -1.7440e+02 - logprior: -8.6002e-01
Epoch 7/10
11/11 - 1s - loss: 176.7335 - loglik: -1.7593e+02 - logprior: -8.0709e-01
Fitted a model with MAP estimate = -175.2651
Time for alignment: 42.3038
Computed alignments with likelihoods: ['-174.7631', '-176.0919', '-175.0766', '-175.7490', '-175.2651']
Best model has likelihood: -174.7631  (prior= -0.7511 )
time for generating output: 0.1307
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.8284762697751873
Training of 5 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f67d310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb16e9ffa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb16e9fbb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 264.5345 - loglik: -2.2388e+02 - logprior: -4.0650e+01
Epoch 2/10
10/10 - 1s - loss: 207.4584 - loglik: -1.9668e+02 - logprior: -1.0778e+01
Epoch 3/10
10/10 - 1s - loss: 177.0481 - loglik: -1.7156e+02 - logprior: -5.4887e+00
Epoch 4/10
10/10 - 1s - loss: 156.3353 - loglik: -1.5254e+02 - logprior: -3.8001e+00
Epoch 5/10
10/10 - 1s - loss: 147.0172 - loglik: -1.4389e+02 - logprior: -3.1305e+00
Epoch 6/10
10/10 - 1s - loss: 144.0326 - loglik: -1.4136e+02 - logprior: -2.6746e+00
Epoch 7/10
10/10 - 1s - loss: 142.8129 - loglik: -1.4045e+02 - logprior: -2.3641e+00
Epoch 8/10
10/10 - 1s - loss: 142.1562 - loglik: -1.4000e+02 - logprior: -2.1551e+00
Epoch 9/10
10/10 - 1s - loss: 141.7412 - loglik: -1.3972e+02 - logprior: -2.0231e+00
Epoch 10/10
10/10 - 1s - loss: 141.4561 - loglik: -1.3951e+02 - logprior: -1.9422e+00
Fitted a model with MAP estimate = -141.3599
expansions: [(7, 2), (10, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 184.5650 - loglik: -1.3893e+02 - logprior: -4.5630e+01
Epoch 2/2
10/10 - 1s - loss: 149.8611 - loglik: -1.3115e+02 - logprior: -1.8712e+01
Fitted a model with MAP estimate = -144.4337
expansions: []
discards: [45 58 65 70]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 172.9966 - loglik: -1.2994e+02 - logprior: -4.3052e+01
Epoch 2/2
10/10 - 1s - loss: 140.7680 - loglik: -1.2828e+02 - logprior: -1.2485e+01
Fitted a model with MAP estimate = -134.2053
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 163.4865 - loglik: -1.2728e+02 - logprior: -3.6207e+01
Epoch 2/10
10/10 - 1s - loss: 136.5452 - loglik: -1.2734e+02 - logprior: -9.2053e+00
Epoch 3/10
10/10 - 1s - loss: 130.4623 - loglik: -1.2662e+02 - logprior: -3.8388e+00
Epoch 4/10
10/10 - 1s - loss: 128.1401 - loglik: -1.2622e+02 - logprior: -1.9200e+00
Epoch 5/10
10/10 - 1s - loss: 126.6135 - loglik: -1.2578e+02 - logprior: -8.3796e-01
Epoch 6/10
10/10 - 1s - loss: 126.0508 - loglik: -1.2607e+02 - logprior: 0.0212
Epoch 7/10
10/10 - 1s - loss: 125.4249 - loglik: -1.2595e+02 - logprior: 0.5233
Epoch 8/10
10/10 - 1s - loss: 125.3845 - loglik: -1.2616e+02 - logprior: 0.7721
Epoch 9/10
10/10 - 1s - loss: 124.9698 - loglik: -1.2596e+02 - logprior: 0.9880
Epoch 10/10
10/10 - 1s - loss: 124.8085 - loglik: -1.2599e+02 - logprior: 1.1795
Fitted a model with MAP estimate = -124.6755
Time for alignment: 33.1657
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 264.8303 - loglik: -2.2418e+02 - logprior: -4.0649e+01
Epoch 2/10
10/10 - 1s - loss: 207.7490 - loglik: -1.9697e+02 - logprior: -1.0779e+01
Epoch 3/10
10/10 - 1s - loss: 177.5907 - loglik: -1.7203e+02 - logprior: -5.5618e+00
Epoch 4/10
10/10 - 1s - loss: 156.0404 - loglik: -1.5201e+02 - logprior: -4.0327e+00
Epoch 5/10
10/10 - 1s - loss: 146.4809 - loglik: -1.4302e+02 - logprior: -3.4637e+00
Epoch 6/10
10/10 - 1s - loss: 142.5538 - loglik: -1.3951e+02 - logprior: -3.0427e+00
Epoch 7/10
10/10 - 1s - loss: 140.7188 - loglik: -1.3801e+02 - logprior: -2.7076e+00
Epoch 8/10
10/10 - 1s - loss: 140.0697 - loglik: -1.3756e+02 - logprior: -2.5096e+00
Epoch 9/10
10/10 - 1s - loss: 139.3400 - loglik: -1.3694e+02 - logprior: -2.3962e+00
Epoch 10/10
10/10 - 1s - loss: 139.3488 - loglik: -1.3703e+02 - logprior: -2.3188e+00
Fitted a model with MAP estimate = -139.1219
expansions: [(3, 1), (4, 1), (5, 1), (10, 1), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 184.6413 - loglik: -1.3897e+02 - logprior: -4.5667e+01
Epoch 2/2
10/10 - 1s - loss: 150.1151 - loglik: -1.3139e+02 - logprior: -1.8727e+01
Fitted a model with MAP estimate = -144.5268
expansions: []
discards: [45 58 65 70]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.2275 - loglik: -1.3010e+02 - logprior: -4.3132e+01
Epoch 2/2
10/10 - 1s - loss: 141.3033 - loglik: -1.2875e+02 - logprior: -1.2557e+01
Fitted a model with MAP estimate = -135.3122
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 164.5297 - loglik: -1.2823e+02 - logprior: -3.6296e+01
Epoch 2/10
10/10 - 1s - loss: 136.9820 - loglik: -1.2782e+02 - logprior: -9.1586e+00
Epoch 3/10
10/10 - 1s - loss: 131.0922 - loglik: -1.2738e+02 - logprior: -3.7085e+00
Epoch 4/10
10/10 - 1s - loss: 128.0630 - loglik: -1.2630e+02 - logprior: -1.7657e+00
Epoch 5/10
10/10 - 1s - loss: 126.8777 - loglik: -1.2604e+02 - logprior: -8.3619e-01
Epoch 6/10
10/10 - 1s - loss: 126.0307 - loglik: -1.2598e+02 - logprior: -5.0404e-02
Epoch 7/10
10/10 - 1s - loss: 125.2888 - loglik: -1.2582e+02 - logprior: 0.5330
Epoch 8/10
10/10 - 1s - loss: 125.3556 - loglik: -1.2617e+02 - logprior: 0.8192
Fitted a model with MAP estimate = -125.0505
Time for alignment: 29.0478
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 264.6628 - loglik: -2.2401e+02 - logprior: -4.0649e+01
Epoch 2/10
10/10 - 1s - loss: 207.7840 - loglik: -1.9701e+02 - logprior: -1.0769e+01
Epoch 3/10
10/10 - 1s - loss: 177.1794 - loglik: -1.7175e+02 - logprior: -5.4301e+00
Epoch 4/10
10/10 - 1s - loss: 155.3836 - loglik: -1.5163e+02 - logprior: -3.7569e+00
Epoch 5/10
10/10 - 1s - loss: 147.0936 - loglik: -1.4404e+02 - logprior: -3.0558e+00
Epoch 6/10
10/10 - 1s - loss: 144.1779 - loglik: -1.4167e+02 - logprior: -2.5070e+00
Epoch 7/10
10/10 - 1s - loss: 142.9906 - loglik: -1.4082e+02 - logprior: -2.1748e+00
Epoch 8/10
10/10 - 1s - loss: 142.2263 - loglik: -1.4020e+02 - logprior: -2.0299e+00
Epoch 9/10
10/10 - 1s - loss: 141.9880 - loglik: -1.4008e+02 - logprior: -1.9121e+00
Epoch 10/10
10/10 - 1s - loss: 141.7012 - loglik: -1.3987e+02 - logprior: -1.8312e+00
Fitted a model with MAP estimate = -141.5440
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 171.6180 - loglik: -1.3471e+02 - logprior: -3.6910e+01
Epoch 2/2
10/10 - 1s - loss: 137.0698 - loglik: -1.2734e+02 - logprior: -9.7282e+00
Fitted a model with MAP estimate = -132.1632
expansions: []
discards: [ 0 46 59 66 71]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 177.2209 - loglik: -1.3197e+02 - logprior: -4.5248e+01
Epoch 2/2
10/10 - 1s - loss: 148.8954 - loglik: -1.3040e+02 - logprior: -1.8498e+01
Fitted a model with MAP estimate = -144.8553
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 164.7668 - loglik: -1.2840e+02 - logprior: -3.6364e+01
Epoch 2/10
10/10 - 1s - loss: 135.8451 - loglik: -1.2677e+02 - logprior: -9.0713e+00
Epoch 3/10
10/10 - 1s - loss: 129.7473 - loglik: -1.2620e+02 - logprior: -3.5503e+00
Epoch 4/10
10/10 - 1s - loss: 127.3949 - loglik: -1.2602e+02 - logprior: -1.3707e+00
Epoch 5/10
10/10 - 1s - loss: 126.2619 - loglik: -1.2601e+02 - logprior: -2.4864e-01
Epoch 6/10
10/10 - 1s - loss: 125.3707 - loglik: -1.2572e+02 - logprior: 0.3460
Epoch 7/10
10/10 - 1s - loss: 125.1274 - loglik: -1.2581e+02 - logprior: 0.6815
Epoch 8/10
10/10 - 1s - loss: 124.6195 - loglik: -1.2554e+02 - logprior: 0.9165
Epoch 9/10
10/10 - 1s - loss: 124.6949 - loglik: -1.2581e+02 - logprior: 1.1106
Fitted a model with MAP estimate = -124.4192
Time for alignment: 30.3829
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 264.7511 - loglik: -2.2410e+02 - logprior: -4.0649e+01
Epoch 2/10
10/10 - 1s - loss: 207.6244 - loglik: -1.9685e+02 - logprior: -1.0773e+01
Epoch 3/10
10/10 - 1s - loss: 177.4568 - loglik: -1.7199e+02 - logprior: -5.4646e+00
Epoch 4/10
10/10 - 1s - loss: 156.1886 - loglik: -1.5243e+02 - logprior: -3.7546e+00
Epoch 5/10
10/10 - 1s - loss: 147.0199 - loglik: -1.4407e+02 - logprior: -2.9517e+00
Epoch 6/10
10/10 - 1s - loss: 144.1505 - loglik: -1.4175e+02 - logprior: -2.3956e+00
Epoch 7/10
10/10 - 1s - loss: 142.7330 - loglik: -1.4068e+02 - logprior: -2.0506e+00
Epoch 8/10
10/10 - 1s - loss: 142.0284 - loglik: -1.4016e+02 - logprior: -1.8644e+00
Epoch 9/10
10/10 - 1s - loss: 141.7488 - loglik: -1.4000e+02 - logprior: -1.7506e+00
Epoch 10/10
10/10 - 1s - loss: 141.5435 - loglik: -1.3988e+02 - logprior: -1.6657e+00
Fitted a model with MAP estimate = -141.3953
expansions: [(12, 4), (13, 2), (15, 1), (26, 1), (27, 1), (30, 3), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 185.1180 - loglik: -1.3954e+02 - logprior: -4.5576e+01
Epoch 2/2
10/10 - 1s - loss: 150.3460 - loglik: -1.3167e+02 - logprior: -1.8675e+01
Fitted a model with MAP estimate = -144.6697
expansions: []
discards: [16 46 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 173.1732 - loglik: -1.3008e+02 - logprior: -4.3093e+01
Epoch 2/2
10/10 - 1s - loss: 140.9908 - loglik: -1.2846e+02 - logprior: -1.2528e+01
Fitted a model with MAP estimate = -134.3049
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 163.6340 - loglik: -1.2742e+02 - logprior: -3.6218e+01
Epoch 2/10
10/10 - 1s - loss: 136.2375 - loglik: -1.2703e+02 - logprior: -9.2092e+00
Epoch 3/10
10/10 - 1s - loss: 130.7995 - loglik: -1.2696e+02 - logprior: -3.8390e+00
Epoch 4/10
10/10 - 1s - loss: 127.9402 - loglik: -1.2602e+02 - logprior: -1.9206e+00
Epoch 5/10
10/10 - 1s - loss: 126.8919 - loglik: -1.2606e+02 - logprior: -8.3587e-01
Epoch 6/10
10/10 - 1s - loss: 126.1196 - loglik: -1.2614e+02 - logprior: 0.0246
Epoch 7/10
10/10 - 1s - loss: 125.5466 - loglik: -1.2607e+02 - logprior: 0.5245
Epoch 8/10
10/10 - 1s - loss: 125.3723 - loglik: -1.2615e+02 - logprior: 0.7815
Epoch 9/10
10/10 - 1s - loss: 124.8134 - loglik: -1.2581e+02 - logprior: 1.0014
Epoch 10/10
10/10 - 1s - loss: 124.8626 - loglik: -1.2605e+02 - logprior: 1.1920
Fitted a model with MAP estimate = -124.6708
Time for alignment: 31.9359
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 264.6138 - loglik: -2.2396e+02 - logprior: -4.0652e+01
Epoch 2/10
10/10 - 1s - loss: 207.6727 - loglik: -1.9689e+02 - logprior: -1.0780e+01
Epoch 3/10
10/10 - 1s - loss: 176.4119 - loglik: -1.7093e+02 - logprior: -5.4784e+00
Epoch 4/10
10/10 - 1s - loss: 154.3997 - loglik: -1.5057e+02 - logprior: -3.8331e+00
Epoch 5/10
10/10 - 1s - loss: 146.8493 - loglik: -1.4380e+02 - logprior: -3.0521e+00
Epoch 6/10
10/10 - 1s - loss: 144.0816 - loglik: -1.4156e+02 - logprior: -2.5264e+00
Epoch 7/10
10/10 - 1s - loss: 142.8279 - loglik: -1.4061e+02 - logprior: -2.2189e+00
Epoch 8/10
10/10 - 1s - loss: 141.9939 - loglik: -1.3993e+02 - logprior: -2.0672e+00
Epoch 9/10
10/10 - 1s - loss: 141.7465 - loglik: -1.3978e+02 - logprior: -1.9707e+00
Epoch 10/10
10/10 - 1s - loss: 141.2243 - loglik: -1.3932e+02 - logprior: -1.9026e+00
Fitted a model with MAP estimate = -141.1894
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 171.1913 - loglik: -1.3429e+02 - logprior: -3.6900e+01
Epoch 2/2
10/10 - 1s - loss: 136.7440 - loglik: -1.2701e+02 - logprior: -9.7293e+00
Fitted a model with MAP estimate = -132.0724
expansions: []
discards: [ 0 46 59 66 71]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 177.0077 - loglik: -1.3177e+02 - logprior: -4.5242e+01
Epoch 2/2
10/10 - 1s - loss: 149.0287 - loglik: -1.3053e+02 - logprior: -1.8501e+01
Fitted a model with MAP estimate = -144.8617
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 164.6828 - loglik: -1.2834e+02 - logprior: -3.6345e+01
Epoch 2/10
10/10 - 1s - loss: 135.9991 - loglik: -1.2693e+02 - logprior: -9.0654e+00
Epoch 3/10
10/10 - 1s - loss: 129.7269 - loglik: -1.2618e+02 - logprior: -3.5444e+00
Epoch 4/10
10/10 - 1s - loss: 127.2900 - loglik: -1.2592e+02 - logprior: -1.3689e+00
Epoch 5/10
10/10 - 1s - loss: 126.2322 - loglik: -1.2599e+02 - logprior: -2.4657e-01
Epoch 6/10
10/10 - 1s - loss: 125.5443 - loglik: -1.2590e+02 - logprior: 0.3521
Epoch 7/10
10/10 - 1s - loss: 125.0393 - loglik: -1.2572e+02 - logprior: 0.6856
Epoch 8/10
10/10 - 1s - loss: 124.6849 - loglik: -1.2560e+02 - logprior: 0.9190
Epoch 9/10
10/10 - 1s - loss: 124.5746 - loglik: -1.2569e+02 - logprior: 1.1120
Epoch 10/10
10/10 - 1s - loss: 124.3822 - loglik: -1.2566e+02 - logprior: 1.2784
Fitted a model with MAP estimate = -124.2595
Time for alignment: 30.6255
Computed alignments with likelihoods: ['-124.6755', '-125.0505', '-124.4192', '-124.6708', '-124.2595']
Best model has likelihood: -124.2595  (prior= 1.3575 )
time for generating output: 0.1287
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.9108348134991119
Training of 5 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feadac72970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac89e2d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebec22b0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.4818 - loglik: -1.6686e+02 - logprior: -5.6267e+00
Epoch 2/10
15/15 - 1s - loss: 144.2959 - loglik: -1.4253e+02 - logprior: -1.7683e+00
Epoch 3/10
15/15 - 1s - loss: 130.4430 - loglik: -1.2867e+02 - logprior: -1.7732e+00
Epoch 4/10
15/15 - 1s - loss: 126.9629 - loglik: -1.2526e+02 - logprior: -1.7003e+00
Epoch 5/10
15/15 - 1s - loss: 126.4648 - loglik: -1.2487e+02 - logprior: -1.5939e+00
Epoch 6/10
15/15 - 1s - loss: 126.1147 - loglik: -1.2450e+02 - logprior: -1.6142e+00
Epoch 7/10
15/15 - 1s - loss: 125.7919 - loglik: -1.2420e+02 - logprior: -1.5911e+00
Epoch 8/10
15/15 - 1s - loss: 125.8292 - loglik: -1.2426e+02 - logprior: -1.5734e+00
Fitted a model with MAP estimate = -125.5387
expansions: [(9, 2), (10, 3), (11, 2), (24, 2), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.9709 - loglik: -1.2999e+02 - logprior: -6.9849e+00
Epoch 2/2
15/15 - 1s - loss: 127.4265 - loglik: -1.2402e+02 - logprior: -3.4111e+00
Fitted a model with MAP estimate = -124.9467
expansions: []
discards: [11 15 31 50]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.2979 - loglik: -1.2221e+02 - logprior: -6.0856e+00
Epoch 2/2
15/15 - 1s - loss: 122.0811 - loglik: -1.2007e+02 - logprior: -2.0114e+00
Fitted a model with MAP estimate = -121.2113
expansions: []
discards: [36 43]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 126.1937 - loglik: -1.2074e+02 - logprior: -5.4564e+00
Epoch 2/10
15/15 - 1s - loss: 121.4097 - loglik: -1.1952e+02 - logprior: -1.8914e+00
Epoch 3/10
15/15 - 1s - loss: 121.0739 - loglik: -1.1965e+02 - logprior: -1.4229e+00
Epoch 4/10
15/15 - 1s - loss: 120.5727 - loglik: -1.1930e+02 - logprior: -1.2769e+00
Epoch 5/10
15/15 - 1s - loss: 120.1694 - loglik: -1.1895e+02 - logprior: -1.2166e+00
Epoch 6/10
15/15 - 1s - loss: 120.1664 - loglik: -1.1899e+02 - logprior: -1.1779e+00
Epoch 7/10
15/15 - 1s - loss: 119.9696 - loglik: -1.1880e+02 - logprior: -1.1731e+00
Epoch 8/10
15/15 - 1s - loss: 119.8533 - loglik: -1.1870e+02 - logprior: -1.1573e+00
Epoch 9/10
15/15 - 1s - loss: 120.1579 - loglik: -1.1902e+02 - logprior: -1.1360e+00
Fitted a model with MAP estimate = -119.8070
Time for alignment: 36.1943
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.4338 - loglik: -1.6681e+02 - logprior: -5.6245e+00
Epoch 2/10
15/15 - 1s - loss: 144.1890 - loglik: -1.4242e+02 - logprior: -1.7684e+00
Epoch 3/10
15/15 - 1s - loss: 131.1394 - loglik: -1.2937e+02 - logprior: -1.7654e+00
Epoch 4/10
15/15 - 1s - loss: 127.4311 - loglik: -1.2574e+02 - logprior: -1.6890e+00
Epoch 5/10
15/15 - 1s - loss: 126.5260 - loglik: -1.2494e+02 - logprior: -1.5853e+00
Epoch 6/10
15/15 - 1s - loss: 126.2442 - loglik: -1.2464e+02 - logprior: -1.5995e+00
Epoch 7/10
15/15 - 1s - loss: 125.9498 - loglik: -1.2438e+02 - logprior: -1.5699e+00
Epoch 8/10
15/15 - 1s - loss: 125.9236 - loglik: -1.2437e+02 - logprior: -1.5544e+00
Epoch 9/10
15/15 - 1s - loss: 125.9014 - loglik: -1.2436e+02 - logprior: -1.5464e+00
Epoch 10/10
15/15 - 1s - loss: 125.7938 - loglik: -1.2426e+02 - logprior: -1.5387e+00
Fitted a model with MAP estimate = -125.6906
expansions: [(9, 2), (10, 3), (11, 2), (12, 1), (27, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.0662 - loglik: -1.2911e+02 - logprior: -6.9596e+00
Epoch 2/2
15/15 - 1s - loss: 126.9746 - loglik: -1.2360e+02 - logprior: -3.3733e+00
Fitted a model with MAP estimate = -124.9273
expansions: [(0, 2)]
discards: [ 0 11 15 45 49]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.9464 - loglik: -1.2282e+02 - logprior: -5.1231e+00
Epoch 2/2
15/15 - 1s - loss: 121.6938 - loglik: -1.2001e+02 - logprior: -1.6791e+00
Fitted a model with MAP estimate = -120.8059
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.5129 - loglik: -1.2293e+02 - logprior: -6.5783e+00
Epoch 2/10
15/15 - 1s - loss: 122.5321 - loglik: -1.2039e+02 - logprior: -2.1408e+00
Epoch 3/10
15/15 - 1s - loss: 121.0466 - loglik: -1.1962e+02 - logprior: -1.4250e+00
Epoch 4/10
15/15 - 1s - loss: 120.4862 - loglik: -1.1923e+02 - logprior: -1.2525e+00
Epoch 5/10
15/15 - 1s - loss: 120.3651 - loglik: -1.1918e+02 - logprior: -1.1873e+00
Epoch 6/10
15/15 - 1s - loss: 120.2378 - loglik: -1.1907e+02 - logprior: -1.1647e+00
Epoch 7/10
15/15 - 1s - loss: 120.0368 - loglik: -1.1889e+02 - logprior: -1.1430e+00
Epoch 8/10
15/15 - 1s - loss: 120.0824 - loglik: -1.1895e+02 - logprior: -1.1312e+00
Fitted a model with MAP estimate = -119.9270
Time for alignment: 36.5126
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.5115 - loglik: -1.6689e+02 - logprior: -5.6257e+00
Epoch 2/10
15/15 - 1s - loss: 145.0242 - loglik: -1.4325e+02 - logprior: -1.7723e+00
Epoch 3/10
15/15 - 1s - loss: 131.5893 - loglik: -1.2982e+02 - logprior: -1.7660e+00
Epoch 4/10
15/15 - 1s - loss: 127.2199 - loglik: -1.2553e+02 - logprior: -1.6913e+00
Epoch 5/10
15/15 - 1s - loss: 126.4264 - loglik: -1.2484e+02 - logprior: -1.5893e+00
Epoch 6/10
15/15 - 1s - loss: 126.1759 - loglik: -1.2457e+02 - logprior: -1.6025e+00
Epoch 7/10
15/15 - 1s - loss: 125.7786 - loglik: -1.2420e+02 - logprior: -1.5774e+00
Epoch 8/10
15/15 - 1s - loss: 125.7955 - loglik: -1.2424e+02 - logprior: -1.5540e+00
Fitted a model with MAP estimate = -125.6765
expansions: [(9, 2), (10, 3), (11, 2), (15, 1), (27, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 135.8979 - loglik: -1.2893e+02 - logprior: -6.9666e+00
Epoch 2/2
15/15 - 1s - loss: 126.7854 - loglik: -1.2342e+02 - logprior: -3.3618e+00
Fitted a model with MAP estimate = -124.5890
expansions: []
discards: [11 15 45 49]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.0355 - loglik: -1.2202e+02 - logprior: -6.0196e+00
Epoch 2/2
15/15 - 1s - loss: 121.9197 - loglik: -1.1996e+02 - logprior: -1.9555e+00
Fitted a model with MAP estimate = -121.1292
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 125.9633 - loglik: -1.2053e+02 - logprior: -5.4284e+00
Epoch 2/10
15/15 - 1s - loss: 121.6216 - loglik: -1.1976e+02 - logprior: -1.8664e+00
Epoch 3/10
15/15 - 1s - loss: 120.9542 - loglik: -1.1955e+02 - logprior: -1.4086e+00
Epoch 4/10
15/15 - 1s - loss: 120.7339 - loglik: -1.1948e+02 - logprior: -1.2498e+00
Epoch 5/10
15/15 - 1s - loss: 120.2354 - loglik: -1.1904e+02 - logprior: -1.1977e+00
Epoch 6/10
15/15 - 1s - loss: 120.1403 - loglik: -1.1897e+02 - logprior: -1.1670e+00
Epoch 7/10
15/15 - 1s - loss: 120.0865 - loglik: -1.1894e+02 - logprior: -1.1474e+00
Epoch 8/10
15/15 - 1s - loss: 120.0568 - loglik: -1.1893e+02 - logprior: -1.1256e+00
Epoch 9/10
15/15 - 1s - loss: 119.8332 - loglik: -1.1871e+02 - logprior: -1.1186e+00
Epoch 10/10
15/15 - 1s - loss: 119.9456 - loglik: -1.1885e+02 - logprior: -1.0968e+00
Fitted a model with MAP estimate = -119.7844
Time for alignment: 36.3882
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 172.4076 - loglik: -1.6678e+02 - logprior: -5.6239e+00
Epoch 2/10
15/15 - 1s - loss: 144.8390 - loglik: -1.4307e+02 - logprior: -1.7679e+00
Epoch 3/10
15/15 - 1s - loss: 130.5621 - loglik: -1.2880e+02 - logprior: -1.7637e+00
Epoch 4/10
15/15 - 1s - loss: 127.1533 - loglik: -1.2546e+02 - logprior: -1.6914e+00
Epoch 5/10
15/15 - 1s - loss: 126.3409 - loglik: -1.2475e+02 - logprior: -1.5877e+00
Epoch 6/10
15/15 - 1s - loss: 126.1034 - loglik: -1.2450e+02 - logprior: -1.6050e+00
Epoch 7/10
15/15 - 1s - loss: 126.0070 - loglik: -1.2442e+02 - logprior: -1.5843e+00
Epoch 8/10
15/15 - 1s - loss: 125.7385 - loglik: -1.2418e+02 - logprior: -1.5600e+00
Epoch 9/10
15/15 - 1s - loss: 125.8353 - loglik: -1.2428e+02 - logprior: -1.5563e+00
Fitted a model with MAP estimate = -125.6519
expansions: [(9, 2), (10, 3), (11, 2), (24, 2), (27, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.7994 - loglik: -1.2982e+02 - logprior: -6.9753e+00
Epoch 2/2
15/15 - 1s - loss: 127.0712 - loglik: -1.2366e+02 - logprior: -3.4107e+00
Fitted a model with MAP estimate = -124.9469
expansions: []
discards: [15 31 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 128.2200 - loglik: -1.2213e+02 - logprior: -6.0869e+00
Epoch 2/2
15/15 - 1s - loss: 121.7196 - loglik: -1.1975e+02 - logprior: -1.9739e+00
Fitted a model with MAP estimate = -120.9291
expansions: []
discards: [11 37]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 126.2621 - loglik: -1.2083e+02 - logprior: -5.4352e+00
Epoch 2/10
15/15 - 1s - loss: 121.9592 - loglik: -1.2008e+02 - logprior: -1.8754e+00
Epoch 3/10
15/15 - 1s - loss: 120.7703 - loglik: -1.1936e+02 - logprior: -1.4090e+00
Epoch 4/10
15/15 - 1s - loss: 120.6526 - loglik: -1.1940e+02 - logprior: -1.2568e+00
Epoch 5/10
15/15 - 1s - loss: 120.3133 - loglik: -1.1912e+02 - logprior: -1.1961e+00
Epoch 6/10
15/15 - 1s - loss: 120.1889 - loglik: -1.1902e+02 - logprior: -1.1674e+00
Epoch 7/10
15/15 - 1s - loss: 119.9822 - loglik: -1.1883e+02 - logprior: -1.1484e+00
Epoch 8/10
15/15 - 1s - loss: 119.9623 - loglik: -1.1883e+02 - logprior: -1.1344e+00
Epoch 9/10
15/15 - 1s - loss: 119.9542 - loglik: -1.1884e+02 - logprior: -1.1095e+00
Epoch 10/10
15/15 - 1s - loss: 119.9825 - loglik: -1.1888e+02 - logprior: -1.1032e+00
Fitted a model with MAP estimate = -119.7967
Time for alignment: 35.5788
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.5651 - loglik: -1.6694e+02 - logprior: -5.6251e+00
Epoch 2/10
15/15 - 1s - loss: 144.1857 - loglik: -1.4241e+02 - logprior: -1.7720e+00
Epoch 3/10
15/15 - 1s - loss: 131.0211 - loglik: -1.2925e+02 - logprior: -1.7671e+00
Epoch 4/10
15/15 - 1s - loss: 127.4861 - loglik: -1.2579e+02 - logprior: -1.6927e+00
Epoch 5/10
15/15 - 1s - loss: 126.2315 - loglik: -1.2463e+02 - logprior: -1.6053e+00
Epoch 6/10
15/15 - 1s - loss: 125.8042 - loglik: -1.2418e+02 - logprior: -1.6279e+00
Epoch 7/10
15/15 - 1s - loss: 125.6812 - loglik: -1.2407e+02 - logprior: -1.6073e+00
Epoch 8/10
15/15 - 1s - loss: 125.3936 - loglik: -1.2380e+02 - logprior: -1.5899e+00
Epoch 9/10
15/15 - 1s - loss: 125.3538 - loglik: -1.2377e+02 - logprior: -1.5856e+00
Epoch 10/10
15/15 - 1s - loss: 125.4311 - loglik: -1.2385e+02 - logprior: -1.5781e+00
Fitted a model with MAP estimate = -125.2337
expansions: [(9, 2), (10, 3), (11, 2), (12, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 135.7211 - loglik: -1.2878e+02 - logprior: -6.9460e+00
Epoch 2/2
15/15 - 1s - loss: 126.8711 - loglik: -1.2352e+02 - logprior: -3.3499e+00
Fitted a model with MAP estimate = -124.8340
expansions: [(0, 2)]
discards: [ 0 11 15 45]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.7776 - loglik: -1.2265e+02 - logprior: -5.1244e+00
Epoch 2/2
15/15 - 1s - loss: 121.5792 - loglik: -1.1989e+02 - logprior: -1.6869e+00
Fitted a model with MAP estimate = -120.8188
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 129.2804 - loglik: -1.2269e+02 - logprior: -6.5938e+00
Epoch 2/10
15/15 - 1s - loss: 122.3261 - loglik: -1.2018e+02 - logprior: -2.1433e+00
Epoch 3/10
15/15 - 1s - loss: 120.8488 - loglik: -1.1942e+02 - logprior: -1.4323e+00
Epoch 4/10
15/15 - 1s - loss: 120.4045 - loglik: -1.1914e+02 - logprior: -1.2633e+00
Epoch 5/10
15/15 - 1s - loss: 120.2663 - loglik: -1.1907e+02 - logprior: -1.1978e+00
Epoch 6/10
15/15 - 1s - loss: 120.0324 - loglik: -1.1886e+02 - logprior: -1.1760e+00
Epoch 7/10
15/15 - 1s - loss: 119.8966 - loglik: -1.1874e+02 - logprior: -1.1558e+00
Epoch 8/10
15/15 - 1s - loss: 119.8550 - loglik: -1.1872e+02 - logprior: -1.1337e+00
Epoch 9/10
15/15 - 1s - loss: 119.8861 - loglik: -1.1876e+02 - logprior: -1.1253e+00
Fitted a model with MAP estimate = -119.6633
Time for alignment: 36.4268
Computed alignments with likelihoods: ['-119.8070', '-119.9270', '-119.7844', '-119.7967', '-119.6633']
Best model has likelihood: -119.6633  (prior= -1.1220 )
time for generating output: 0.1247
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9790322580645161
Training of 5 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb40804a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d54e3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f72fbe0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.9206 - loglik: -1.8359e+02 - logprior: -1.4333e+01
Epoch 2/10
10/10 - 1s - loss: 169.7171 - loglik: -1.6563e+02 - logprior: -4.0841e+00
Epoch 3/10
10/10 - 1s - loss: 151.7482 - loglik: -1.4941e+02 - logprior: -2.3340e+00
Epoch 4/10
10/10 - 2s - loss: 143.2342 - loglik: -1.4138e+02 - logprior: -1.8506e+00
Epoch 5/10
10/10 - 2s - loss: 139.3478 - loglik: -1.3771e+02 - logprior: -1.6337e+00
Epoch 6/10
10/10 - 1s - loss: 135.0589 - loglik: -1.3358e+02 - logprior: -1.4741e+00
Epoch 7/10
10/10 - 2s - loss: 133.8371 - loglik: -1.3236e+02 - logprior: -1.4767e+00
Epoch 8/10
10/10 - 2s - loss: 132.9854 - loglik: -1.3150e+02 - logprior: -1.4837e+00
Epoch 9/10
10/10 - 2s - loss: 132.0935 - loglik: -1.3061e+02 - logprior: -1.4795e+00
Epoch 10/10
10/10 - 2s - loss: 131.7424 - loglik: -1.3026e+02 - logprior: -1.4835e+00
Fitted a model with MAP estimate = -131.6659
expansions: [(0, 6), (28, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 146.3968 - loglik: -1.2880e+02 - logprior: -1.7597e+01
Epoch 2/2
10/10 - 1s - loss: 126.0706 - loglik: -1.2072e+02 - logprior: -5.3467e+00
Fitted a model with MAP estimate = -122.8887
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 138.3738 - loglik: -1.2236e+02 - logprior: -1.6015e+01
Epoch 2/2
10/10 - 2s - loss: 127.0186 - loglik: -1.2035e+02 - logprior: -6.6702e+00
Fitted a model with MAP estimate = -124.7209
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 133.2072 - loglik: -1.1940e+02 - logprior: -1.3806e+01
Epoch 2/10
10/10 - 2s - loss: 122.6140 - loglik: -1.1887e+02 - logprior: -3.7423e+00
Epoch 3/10
10/10 - 1s - loss: 120.5386 - loglik: -1.1858e+02 - logprior: -1.9628e+00
Epoch 4/10
10/10 - 2s - loss: 119.6170 - loglik: -1.1812e+02 - logprior: -1.5003e+00
Epoch 5/10
10/10 - 1s - loss: 118.9654 - loglik: -1.1773e+02 - logprior: -1.2334e+00
Epoch 6/10
10/10 - 2s - loss: 118.9414 - loglik: -1.1793e+02 - logprior: -1.0161e+00
Epoch 7/10
10/10 - 2s - loss: 118.5722 - loglik: -1.1763e+02 - logprior: -9.3897e-01
Epoch 8/10
10/10 - 1s - loss: 118.0548 - loglik: -1.1712e+02 - logprior: -9.3210e-01
Epoch 9/10
10/10 - 1s - loss: 118.0751 - loglik: -1.1716e+02 - logprior: -9.1382e-01
Fitted a model with MAP estimate = -118.2156
Time for alignment: 53.7185
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.9222 - loglik: -1.8359e+02 - logprior: -1.4334e+01
Epoch 2/10
10/10 - 2s - loss: 169.9337 - loglik: -1.6584e+02 - logprior: -4.0939e+00
Epoch 3/10
10/10 - 1s - loss: 152.3410 - loglik: -1.4996e+02 - logprior: -2.3804e+00
Epoch 4/10
10/10 - 2s - loss: 142.2864 - loglik: -1.4032e+02 - logprior: -1.9685e+00
Epoch 5/10
10/10 - 2s - loss: 138.0316 - loglik: -1.3620e+02 - logprior: -1.8314e+00
Epoch 6/10
10/10 - 1s - loss: 134.2462 - loglik: -1.3258e+02 - logprior: -1.6676e+00
Epoch 7/10
10/10 - 1s - loss: 132.7792 - loglik: -1.3114e+02 - logprior: -1.6402e+00
Epoch 8/10
10/10 - 1s - loss: 131.8063 - loglik: -1.3017e+02 - logprior: -1.6384e+00
Epoch 9/10
10/10 - 1s - loss: 131.5771 - loglik: -1.2997e+02 - logprior: -1.6064e+00
Epoch 10/10
10/10 - 2s - loss: 131.0244 - loglik: -1.2944e+02 - logprior: -1.5813e+00
Fitted a model with MAP estimate = -131.0816
expansions: [(0, 6), (24, 1), (30, 1), (32, 1), (33, 1), (36, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 146.4041 - loglik: -1.2879e+02 - logprior: -1.7613e+01
Epoch 2/2
10/10 - 2s - loss: 126.6385 - loglik: -1.2129e+02 - logprior: -5.3445e+00
Fitted a model with MAP estimate = -122.9239
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 138.4103 - loglik: -1.2240e+02 - logprior: -1.6007e+01
Epoch 2/2
10/10 - 1s - loss: 126.9929 - loglik: -1.2033e+02 - logprior: -6.6666e+00
Fitted a model with MAP estimate = -124.7146
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 133.2611 - loglik: -1.1946e+02 - logprior: -1.3804e+01
Epoch 2/10
10/10 - 1s - loss: 122.5020 - loglik: -1.1876e+02 - logprior: -3.7399e+00
Epoch 3/10
10/10 - 2s - loss: 120.6217 - loglik: -1.1867e+02 - logprior: -1.9520e+00
Epoch 4/10
10/10 - 1s - loss: 119.2027 - loglik: -1.1771e+02 - logprior: -1.4922e+00
Epoch 5/10
10/10 - 1s - loss: 119.4332 - loglik: -1.1821e+02 - logprior: -1.2232e+00
Fitted a model with MAP estimate = -118.7629
Time for alignment: 46.2250
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.3329 - loglik: -1.8300e+02 - logprior: -1.4333e+01
Epoch 2/10
10/10 - 1s - loss: 170.1252 - loglik: -1.6604e+02 - logprior: -4.0901e+00
Epoch 3/10
10/10 - 1s - loss: 152.8105 - loglik: -1.5044e+02 - logprior: -2.3687e+00
Epoch 4/10
10/10 - 2s - loss: 143.0930 - loglik: -1.4118e+02 - logprior: -1.9080e+00
Epoch 5/10
10/10 - 2s - loss: 138.1176 - loglik: -1.3643e+02 - logprior: -1.6910e+00
Epoch 6/10
10/10 - 1s - loss: 136.4765 - loglik: -1.3504e+02 - logprior: -1.4361e+00
Epoch 7/10
10/10 - 2s - loss: 135.4541 - loglik: -1.3415e+02 - logprior: -1.3039e+00
Epoch 8/10
10/10 - 2s - loss: 134.3883 - loglik: -1.3314e+02 - logprior: -1.2507e+00
Epoch 9/10
10/10 - 1s - loss: 134.5382 - loglik: -1.3333e+02 - logprior: -1.2087e+00
Fitted a model with MAP estimate = -134.0248
expansions: [(0, 6), (24, 1), (33, 3), (38, 3), (39, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 145.4729 - loglik: -1.2805e+02 - logprior: -1.7421e+01
Epoch 2/2
10/10 - 1s - loss: 125.9520 - loglik: -1.2067e+02 - logprior: -5.2824e+00
Fitted a model with MAP estimate = -122.8040
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 138.1659 - loglik: -1.2217e+02 - logprior: -1.5991e+01
Epoch 2/2
10/10 - 2s - loss: 127.0745 - loglik: -1.2042e+02 - logprior: -6.6574e+00
Fitted a model with MAP estimate = -124.6828
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 132.8902 - loglik: -1.1914e+02 - logprior: -1.3753e+01
Epoch 2/10
10/10 - 1s - loss: 122.9267 - loglik: -1.1920e+02 - logprior: -3.7233e+00
Epoch 3/10
10/10 - 2s - loss: 120.4367 - loglik: -1.1849e+02 - logprior: -1.9475e+00
Epoch 4/10
10/10 - 2s - loss: 119.7132 - loglik: -1.1822e+02 - logprior: -1.4940e+00
Epoch 5/10
10/10 - 2s - loss: 119.2174 - loglik: -1.1800e+02 - logprior: -1.2206e+00
Epoch 6/10
10/10 - 1s - loss: 118.4127 - loglik: -1.1741e+02 - logprior: -1.0043e+00
Epoch 7/10
10/10 - 1s - loss: 118.3837 - loglik: -1.1745e+02 - logprior: -9.3158e-01
Epoch 8/10
10/10 - 1s - loss: 118.4876 - loglik: -1.1756e+02 - logprior: -9.2368e-01
Fitted a model with MAP estimate = -118.2722
Time for alignment: 49.0499
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.6666 - loglik: -1.8333e+02 - logprior: -1.4333e+01
Epoch 2/10
10/10 - 1s - loss: 170.2783 - loglik: -1.6619e+02 - logprior: -4.0920e+00
Epoch 3/10
10/10 - 2s - loss: 152.5477 - loglik: -1.5017e+02 - logprior: -2.3773e+00
Epoch 4/10
10/10 - 2s - loss: 143.1442 - loglik: -1.4121e+02 - logprior: -1.9301e+00
Epoch 5/10
10/10 - 1s - loss: 138.6122 - loglik: -1.3691e+02 - logprior: -1.6981e+00
Epoch 6/10
10/10 - 1s - loss: 136.4659 - loglik: -1.3497e+02 - logprior: -1.4949e+00
Epoch 7/10
10/10 - 1s - loss: 134.5286 - loglik: -1.3312e+02 - logprior: -1.4121e+00
Epoch 8/10
10/10 - 1s - loss: 133.6815 - loglik: -1.3230e+02 - logprior: -1.3847e+00
Epoch 9/10
10/10 - 2s - loss: 132.9499 - loglik: -1.3156e+02 - logprior: -1.3943e+00
Epoch 10/10
10/10 - 2s - loss: 132.8603 - loglik: -1.3147e+02 - logprior: -1.3908e+00
Fitted a model with MAP estimate = -132.6618
expansions: [(0, 6), (24, 1), (33, 3), (36, 2), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 147.2525 - loglik: -1.2965e+02 - logprior: -1.7601e+01
Epoch 2/2
10/10 - 1s - loss: 126.4911 - loglik: -1.2109e+02 - logprior: -5.4020e+00
Fitted a model with MAP estimate = -123.0509
expansions: []
discards: [ 0 46]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 138.5333 - loglik: -1.2251e+02 - logprior: -1.6021e+01
Epoch 2/2
10/10 - 1s - loss: 127.0278 - loglik: -1.2036e+02 - logprior: -6.6667e+00
Fitted a model with MAP estimate = -124.7133
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 133.3184 - loglik: -1.1951e+02 - logprior: -1.3808e+01
Epoch 2/10
10/10 - 2s - loss: 122.3560 - loglik: -1.1862e+02 - logprior: -3.7390e+00
Epoch 3/10
10/10 - 1s - loss: 120.3877 - loglik: -1.1844e+02 - logprior: -1.9526e+00
Epoch 4/10
10/10 - 1s - loss: 119.6538 - loglik: -1.1816e+02 - logprior: -1.4938e+00
Epoch 5/10
10/10 - 1s - loss: 119.2125 - loglik: -1.1799e+02 - logprior: -1.2219e+00
Epoch 6/10
10/10 - 2s - loss: 118.6879 - loglik: -1.1768e+02 - logprior: -1.0036e+00
Epoch 7/10
10/10 - 2s - loss: 118.6939 - loglik: -1.1776e+02 - logprior: -9.2998e-01
Fitted a model with MAP estimate = -118.3692
Time for alignment: 48.2773
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.3393 - loglik: -1.8301e+02 - logprior: -1.4334e+01
Epoch 2/10
10/10 - 2s - loss: 170.7826 - loglik: -1.6668e+02 - logprior: -4.1026e+00
Epoch 3/10
10/10 - 2s - loss: 153.1866 - loglik: -1.5074e+02 - logprior: -2.4456e+00
Epoch 4/10
10/10 - 2s - loss: 141.5428 - loglik: -1.3944e+02 - logprior: -2.1077e+00
Epoch 5/10
10/10 - 2s - loss: 135.2891 - loglik: -1.3334e+02 - logprior: -1.9486e+00
Epoch 6/10
10/10 - 2s - loss: 132.5287 - loglik: -1.3073e+02 - logprior: -1.8009e+00
Epoch 7/10
10/10 - 2s - loss: 131.6250 - loglik: -1.2989e+02 - logprior: -1.7382e+00
Epoch 8/10
10/10 - 2s - loss: 130.6406 - loglik: -1.2897e+02 - logprior: -1.6668e+00
Epoch 9/10
10/10 - 1s - loss: 130.5434 - loglik: -1.2894e+02 - logprior: -1.5991e+00
Epoch 10/10
10/10 - 2s - loss: 130.3811 - loglik: -1.2882e+02 - logprior: -1.5639e+00
Fitted a model with MAP estimate = -130.2318
expansions: [(0, 6), (24, 1), (27, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (47, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 145.9285 - loglik: -1.2833e+02 - logprior: -1.7602e+01
Epoch 2/2
10/10 - 2s - loss: 126.1757 - loglik: -1.2083e+02 - logprior: -5.3445e+00
Fitted a model with MAP estimate = -122.8542
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 138.1315 - loglik: -1.2212e+02 - logprior: -1.6011e+01
Epoch 2/2
10/10 - 1s - loss: 127.3001 - loglik: -1.2063e+02 - logprior: -6.6690e+00
Fitted a model with MAP estimate = -124.6979
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 133.0462 - loglik: -1.1925e+02 - logprior: -1.3801e+01
Epoch 2/10
10/10 - 2s - loss: 122.9682 - loglik: -1.1923e+02 - logprior: -3.7364e+00
Epoch 3/10
10/10 - 2s - loss: 120.2931 - loglik: -1.1834e+02 - logprior: -1.9513e+00
Epoch 4/10
10/10 - 1s - loss: 119.8796 - loglik: -1.1839e+02 - logprior: -1.4864e+00
Epoch 5/10
10/10 - 1s - loss: 118.6864 - loglik: -1.1747e+02 - logprior: -1.2213e+00
Epoch 6/10
10/10 - 1s - loss: 118.5155 - loglik: -1.1751e+02 - logprior: -1.0037e+00
Epoch 7/10
10/10 - 1s - loss: 119.0364 - loglik: -1.1811e+02 - logprior: -9.2570e-01
Fitted a model with MAP estimate = -118.3613
Time for alignment: 50.9677
Computed alignments with likelihoods: ['-118.2156', '-118.7629', '-118.2722', '-118.3692', '-118.3613']
Best model has likelihood: -118.2156  (prior= -0.8994 )
time for generating output: 0.2903
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.997884493336154
Training of 5 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb40b5e3a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d4b4940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb493137c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 529.1776 - loglik: -5.2296e+02 - logprior: -6.2189e+00
Epoch 2/10
23/23 - 4s - loss: 459.0973 - loglik: -4.5840e+02 - logprior: -6.9379e-01
Epoch 3/10
23/23 - 4s - loss: 439.3157 - loglik: -4.3878e+02 - logprior: -5.3415e-01
Epoch 4/10
23/23 - 4s - loss: 434.8560 - loglik: -4.3450e+02 - logprior: -3.5981e-01
Epoch 5/10
23/23 - 4s - loss: 431.4840 - loglik: -4.3107e+02 - logprior: -4.0912e-01
Epoch 6/10
23/23 - 4s - loss: 431.2404 - loglik: -4.3080e+02 - logprior: -4.4340e-01
Epoch 7/10
23/23 - 4s - loss: 429.7811 - loglik: -4.2935e+02 - logprior: -4.2963e-01
Epoch 8/10
23/23 - 4s - loss: 427.9767 - loglik: -4.2755e+02 - logprior: -4.2490e-01
Epoch 9/10
23/23 - 4s - loss: 429.5061 - loglik: -4.2906e+02 - logprior: -4.5010e-01
Fitted a model with MAP estimate = -428.8161
expansions: [(0, 8), (8, 5), (30, 1), (42, 1), (56, 2), (57, 1), (59, 1), (62, 1), (70, 1), (72, 2), (76, 1), (77, 1), (83, 1), (86, 1), (110, 1), (111, 2), (112, 1), (118, 1), (120, 1), (123, 1), (126, 1), (148, 2), (149, 1), (159, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 440.0550 - loglik: -4.3168e+02 - logprior: -8.3784e+00
Epoch 2/2
23/23 - 6s - loss: 422.3108 - loglik: -4.2181e+02 - logprior: -5.0419e-01
Fitted a model with MAP estimate = -420.6702
expansions: [(0, 19)]
discards: [  1   2   3   4   5   6   7   8   9  72  94 140 197 198 199 200 201 202]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 435.8499 - loglik: -4.2734e+02 - logprior: -8.5052e+00
Epoch 2/2
23/23 - 6s - loss: 424.8351 - loglik: -4.2440e+02 - logprior: -4.3625e-01
Fitted a model with MAP estimate = -422.3637
expansions: [(0, 19), (204, 9)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 432.9133 - loglik: -4.2652e+02 - logprior: -6.3936e+00
Epoch 2/10
23/23 - 6s - loss: 419.9738 - loglik: -4.2016e+02 - logprior: 0.1883
Epoch 3/10
23/23 - 6s - loss: 418.6016 - loglik: -4.1940e+02 - logprior: 0.7995
Epoch 4/10
23/23 - 6s - loss: 416.1632 - loglik: -4.1705e+02 - logprior: 0.8888
Epoch 5/10
23/23 - 6s - loss: 412.2505 - loglik: -4.1327e+02 - logprior: 1.0180
Epoch 6/10
23/23 - 6s - loss: 411.7863 - loglik: -4.1289e+02 - logprior: 1.1056
Epoch 7/10
23/23 - 6s - loss: 412.1889 - loglik: -4.1340e+02 - logprior: 1.2109
Fitted a model with MAP estimate = -411.9022
Time for alignment: 140.4506
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 529.7087 - loglik: -5.2350e+02 - logprior: -6.2109e+00
Epoch 2/10
23/23 - 4s - loss: 463.7655 - loglik: -4.6303e+02 - logprior: -7.3675e-01
Epoch 3/10
23/23 - 4s - loss: 443.4396 - loglik: -4.4284e+02 - logprior: -6.0283e-01
Epoch 4/10
23/23 - 4s - loss: 437.4944 - loglik: -4.3703e+02 - logprior: -4.6534e-01
Epoch 5/10
23/23 - 4s - loss: 435.2855 - loglik: -4.3478e+02 - logprior: -5.0242e-01
Epoch 6/10
23/23 - 4s - loss: 431.4070 - loglik: -4.3085e+02 - logprior: -5.5934e-01
Epoch 7/10
23/23 - 4s - loss: 431.1931 - loglik: -4.3061e+02 - logprior: -5.7811e-01
Epoch 8/10
23/23 - 4s - loss: 430.8806 - loglik: -4.3030e+02 - logprior: -5.7602e-01
Epoch 9/10
23/23 - 4s - loss: 430.9372 - loglik: -4.3036e+02 - logprior: -5.7266e-01
Fitted a model with MAP estimate = -430.4813
expansions: [(0, 9), (9, 1), (15, 2), (30, 1), (42, 1), (52, 1), (56, 1), (57, 2), (58, 2), (65, 1), (69, 1), (71, 1), (76, 1), (77, 1), (83, 1), (86, 1), (107, 2), (112, 1), (119, 1), (121, 1), (124, 1), (127, 1), (133, 1), (148, 1), (149, 1), (150, 1), (159, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 441.6178 - loglik: -4.3329e+02 - logprior: -8.3295e+00
Epoch 2/2
23/23 - 6s - loss: 426.1711 - loglik: -4.2568e+02 - logprior: -4.9123e-01
Fitted a model with MAP estimate = -422.6967
expansions: [(0, 19)]
discards: [  1   2   3   4   5   6   7   8   9  25  73  76 197 198 199 200 201 202]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 438.0268 - loglik: -4.2952e+02 - logprior: -8.5062e+00
Epoch 2/2
23/23 - 6s - loss: 424.9791 - loglik: -4.2457e+02 - logprior: -4.1103e-01
Fitted a model with MAP estimate = -423.0788
expansions: [(0, 19), (28, 1), (204, 10)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19 142]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 432.4969 - loglik: -4.2617e+02 - logprior: -6.3312e+00
Epoch 2/10
23/23 - 6s - loss: 421.6210 - loglik: -4.2178e+02 - logprior: 0.1552
Epoch 3/10
23/23 - 6s - loss: 419.2060 - loglik: -4.1992e+02 - logprior: 0.7168
Epoch 4/10
23/23 - 6s - loss: 416.9689 - loglik: -4.1775e+02 - logprior: 0.7855
Epoch 5/10
23/23 - 6s - loss: 412.4282 - loglik: -4.1329e+02 - logprior: 0.8645
Epoch 6/10
23/23 - 6s - loss: 412.9911 - loglik: -4.1400e+02 - logprior: 1.0059
Fitted a model with MAP estimate = -412.3869
Time for alignment: 133.8345
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 529.2246 - loglik: -5.2302e+02 - logprior: -6.1999e+00
Epoch 2/10
23/23 - 4s - loss: 462.0482 - loglik: -4.6138e+02 - logprior: -6.6582e-01
Epoch 3/10
23/23 - 4s - loss: 442.0061 - loglik: -4.4146e+02 - logprior: -5.5073e-01
Epoch 4/10
23/23 - 4s - loss: 436.2163 - loglik: -4.3575e+02 - logprior: -4.6714e-01
Epoch 5/10
23/23 - 4s - loss: 434.7083 - loglik: -4.3423e+02 - logprior: -4.8319e-01
Epoch 6/10
23/23 - 4s - loss: 431.6283 - loglik: -4.3110e+02 - logprior: -5.2395e-01
Epoch 7/10
23/23 - 4s - loss: 432.5920 - loglik: -4.3205e+02 - logprior: -5.3973e-01
Fitted a model with MAP estimate = -431.1877
expansions: [(0, 8), (8, 5), (19, 1), (42, 1), (52, 1), (57, 1), (59, 1), (66, 1), (70, 1), (72, 1), (83, 1), (86, 1), (106, 1), (111, 2), (112, 1), (114, 1), (118, 1), (119, 1), (126, 1), (148, 1), (149, 1), (150, 1), (159, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 199 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 442.0627 - loglik: -4.3385e+02 - logprior: -8.2122e+00
Epoch 2/2
23/23 - 6s - loss: 426.2939 - loglik: -4.2581e+02 - logprior: -4.8501e-01
Fitted a model with MAP estimate = -423.1814
expansions: [(0, 19)]
discards: [  1   2   3   4   5   6   7   8   9  16 136 193 194 195 196 197 198]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 438.6006 - loglik: -4.3011e+02 - logprior: -8.4875e+00
Epoch 2/2
23/23 - 6s - loss: 426.3106 - loglik: -4.2585e+02 - logprior: -4.6307e-01
Fitted a model with MAP estimate = -424.6850
expansions: [(0, 18), (201, 9)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 433.2681 - loglik: -4.2691e+02 - logprior: -6.3590e+00
Epoch 2/10
23/23 - 6s - loss: 425.2548 - loglik: -4.2539e+02 - logprior: 0.1371
Epoch 3/10
23/23 - 6s - loss: 420.1749 - loglik: -4.2094e+02 - logprior: 0.7657
Epoch 4/10
23/23 - 6s - loss: 416.7452 - loglik: -4.1757e+02 - logprior: 0.8249
Epoch 5/10
23/23 - 6s - loss: 416.1097 - loglik: -4.1707e+02 - logprior: 0.9554
Epoch 6/10
23/23 - 6s - loss: 415.2867 - loglik: -4.1635e+02 - logprior: 1.0674
Epoch 7/10
23/23 - 6s - loss: 415.4207 - loglik: -4.1657e+02 - logprior: 1.1515
Fitted a model with MAP estimate = -414.3292
Time for alignment: 128.7581
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 531.2133 - loglik: -5.2501e+02 - logprior: -6.2081e+00
Epoch 2/10
23/23 - 4s - loss: 458.0482 - loglik: -4.5735e+02 - logprior: -6.9637e-01
Epoch 3/10
23/23 - 4s - loss: 439.1078 - loglik: -4.3852e+02 - logprior: -5.8856e-01
Epoch 4/10
23/23 - 4s - loss: 435.6199 - loglik: -4.3513e+02 - logprior: -4.8517e-01
Epoch 5/10
23/23 - 4s - loss: 431.1656 - loglik: -4.3070e+02 - logprior: -4.6683e-01
Epoch 6/10
23/23 - 4s - loss: 432.1863 - loglik: -4.3168e+02 - logprior: -5.0916e-01
Fitted a model with MAP estimate = -430.3641
expansions: [(0, 8), (8, 4), (14, 1), (31, 1), (51, 1), (52, 1), (57, 2), (59, 2), (66, 1), (71, 1), (72, 1), (76, 1), (77, 1), (78, 1), (83, 1), (86, 1), (110, 1), (111, 2), (112, 1), (118, 1), (123, 1), (126, 1), (127, 1), (148, 1), (149, 1), (150, 1), (159, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 439.9721 - loglik: -4.3184e+02 - logprior: -8.1321e+00
Epoch 2/2
23/23 - 6s - loss: 422.7425 - loglik: -4.2224e+02 - logprior: -5.0257e-01
Fitted a model with MAP estimate = -420.1223
expansions: [(0, 20)]
discards: [  1   2   3   4   5   6   7   8   9  10  17  18  19  20  74  77 141 198
 199 200 201 202 203]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 437.2202 - loglik: -4.2870e+02 - logprior: -8.5184e+00
Epoch 2/2
23/23 - 6s - loss: 425.2534 - loglik: -4.2480e+02 - logprior: -4.4916e-01
Fitted a model with MAP estimate = -422.2978
expansions: [(0, 19), (201, 9)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Re-initialized the encoder parameters.
Fitting a model of length 209 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 434.6598 - loglik: -4.2577e+02 - logprior: -8.8853e+00
Epoch 2/10
23/23 - 6s - loss: 424.6335 - loglik: -4.2370e+02 - logprior: -9.3572e-01
Epoch 3/10
23/23 - 6s - loss: 421.0392 - loglik: -4.2148e+02 - logprior: 0.4445
Epoch 4/10
23/23 - 6s - loss: 417.5016 - loglik: -4.1791e+02 - logprior: 0.4098
Epoch 5/10
23/23 - 6s - loss: 415.9434 - loglik: -4.1655e+02 - logprior: 0.6071
Epoch 6/10
23/23 - 6s - loss: 413.8192 - loglik: -4.1481e+02 - logprior: 0.9876
Epoch 7/10
23/23 - 6s - loss: 412.6596 - loglik: -4.1376e+02 - logprior: 1.0972
Epoch 8/10
23/23 - 6s - loss: 415.0170 - loglik: -4.1622e+02 - logprior: 1.1983
Fitted a model with MAP estimate = -413.0379
Time for alignment: 131.2633
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 530.2429 - loglik: -5.2402e+02 - logprior: -6.2185e+00
Epoch 2/10
23/23 - 4s - loss: 459.8916 - loglik: -4.5915e+02 - logprior: -7.4252e-01
Epoch 3/10
23/23 - 4s - loss: 440.7169 - loglik: -4.4010e+02 - logprior: -6.2111e-01
Epoch 4/10
23/23 - 4s - loss: 435.2320 - loglik: -4.3475e+02 - logprior: -4.8162e-01
Epoch 5/10
23/23 - 4s - loss: 433.4506 - loglik: -4.3303e+02 - logprior: -4.1805e-01
Epoch 6/10
23/23 - 4s - loss: 431.5356 - loglik: -4.3109e+02 - logprior: -4.4382e-01
Epoch 7/10
23/23 - 4s - loss: 430.0162 - loglik: -4.2954e+02 - logprior: -4.7825e-01
Epoch 8/10
23/23 - 4s - loss: 431.3162 - loglik: -4.3084e+02 - logprior: -4.7677e-01
Fitted a model with MAP estimate = -430.3996
expansions: [(0, 8), (9, 4), (13, 1), (30, 1), (42, 1), (52, 1), (55, 2), (56, 1), (58, 1), (65, 1), (69, 1), (70, 1), (71, 2), (81, 2), (82, 1), (83, 1), (106, 1), (111, 2), (112, 1), (118, 1), (120, 1), (126, 1), (127, 1), (149, 1), (150, 2), (159, 6)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 9s - loss: 440.2068 - loglik: -4.3192e+02 - logprior: -8.2848e+00
Epoch 2/2
23/23 - 6s - loss: 422.1390 - loglik: -4.2160e+02 - logprior: -5.3709e-01
Fitted a model with MAP estimate = -419.7169
expansions: [(0, 19)]
discards: [  1   2   3   4   5   6   7   8   9  17  71  94 142 199 200 201 202 203
 204]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 435.1262 - loglik: -4.2662e+02 - logprior: -8.5083e+00
Epoch 2/2
23/23 - 6s - loss: 425.9243 - loglik: -4.2550e+02 - logprior: -4.2275e-01
Fitted a model with MAP estimate = -421.9321
expansions: [(0, 20), (205, 9)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 431.6115 - loglik: -4.2523e+02 - logprior: -6.3769e+00
Epoch 2/10
23/23 - 6s - loss: 420.7424 - loglik: -4.2087e+02 - logprior: 0.1248
Epoch 3/10
23/23 - 6s - loss: 418.3437 - loglik: -4.1908e+02 - logprior: 0.7345
Epoch 4/10
23/23 - 6s - loss: 414.7481 - loglik: -4.1556e+02 - logprior: 0.8099
Epoch 5/10
23/23 - 6s - loss: 413.7815 - loglik: -4.1470e+02 - logprior: 0.9196
Epoch 6/10
23/23 - 6s - loss: 411.6769 - loglik: -4.1269e+02 - logprior: 1.0097
Epoch 7/10
23/23 - 6s - loss: 412.0291 - loglik: -4.1315e+02 - logprior: 1.1235
Fitted a model with MAP estimate = -411.6253
Time for alignment: 136.6298
Computed alignments with likelihoods: ['-411.9022', '-412.3869', '-414.3292', '-413.0379', '-411.6253']
Best model has likelihood: -411.6253  (prior= 1.1790 )
time for generating output: 0.2207
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.6870717222476016
Training of 5 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e17cb50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0597c070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5a1fdac0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 250.1764 - loglik: -2.4903e+02 - logprior: -1.1488e+00
Epoch 2/10
29/29 - 3s - loss: 225.6024 - loglik: -2.2479e+02 - logprior: -8.0890e-01
Epoch 3/10
29/29 - 3s - loss: 221.2336 - loglik: -2.2045e+02 - logprior: -7.8660e-01
Epoch 4/10
29/29 - 3s - loss: 219.6707 - loglik: -2.1886e+02 - logprior: -8.1422e-01
Epoch 5/10
29/29 - 3s - loss: 219.1235 - loglik: -2.1832e+02 - logprior: -8.0102e-01
Epoch 6/10
29/29 - 3s - loss: 218.5519 - loglik: -2.1775e+02 - logprior: -7.9723e-01
Epoch 7/10
29/29 - 3s - loss: 219.2770 - loglik: -2.1849e+02 - logprior: -7.9149e-01
Fitted a model with MAP estimate = -200.8825
expansions: [(1, 1), (2, 1), (13, 1), (14, 2), (15, 2), (22, 1), (39, 2), (40, 1), (41, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (53, 1), (55, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 220.3016 - loglik: -2.1917e+02 - logprior: -1.1294e+00
Epoch 2/2
29/29 - 3s - loss: 216.6468 - loglik: -2.1589e+02 - logprior: -7.5194e-01
Fitted a model with MAP estimate = -197.5691
expansions: []
discards: [ 1 48 58 61 67]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 218.5784 - loglik: -2.1756e+02 - logprior: -1.0166e+00
Epoch 2/2
29/29 - 3s - loss: 215.9084 - loglik: -2.1519e+02 - logprior: -7.1433e-01
Fitted a model with MAP estimate = -197.8994
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.7744 - loglik: -1.9710e+02 - logprior: -6.7398e-01
Epoch 2/10
42/42 - 4s - loss: 196.9957 - loglik: -1.9650e+02 - logprior: -4.9145e-01
Epoch 3/10
42/42 - 4s - loss: 196.1630 - loglik: -1.9568e+02 - logprior: -4.8416e-01
Epoch 4/10
42/42 - 4s - loss: 195.7801 - loglik: -1.9530e+02 - logprior: -4.8026e-01
Epoch 5/10
42/42 - 4s - loss: 195.3396 - loglik: -1.9487e+02 - logprior: -4.7332e-01
Epoch 6/10
42/42 - 4s - loss: 195.2292 - loglik: -1.9476e+02 - logprior: -4.7102e-01
Epoch 7/10
42/42 - 4s - loss: 194.8686 - loglik: -1.9440e+02 - logprior: -4.6909e-01
Epoch 8/10
42/42 - 4s - loss: 195.0864 - loglik: -1.9462e+02 - logprior: -4.6175e-01
Fitted a model with MAP estimate = -194.6972
Time for alignment: 110.6406
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 250.1644 - loglik: -2.4901e+02 - logprior: -1.1508e+00
Epoch 2/10
29/29 - 3s - loss: 225.9017 - loglik: -2.2510e+02 - logprior: -8.0511e-01
Epoch 3/10
29/29 - 3s - loss: 221.4631 - loglik: -2.2067e+02 - logprior: -7.9249e-01
Epoch 4/10
29/29 - 3s - loss: 219.1873 - loglik: -2.1837e+02 - logprior: -8.1408e-01
Epoch 5/10
29/29 - 3s - loss: 218.8407 - loglik: -2.1804e+02 - logprior: -8.0249e-01
Epoch 6/10
29/29 - 3s - loss: 218.9534 - loglik: -2.1816e+02 - logprior: -7.8892e-01
Fitted a model with MAP estimate = -200.8276
expansions: [(1, 1), (2, 1), (13, 2), (14, 4), (15, 2), (27, 2), (38, 1), (41, 1), (43, 1), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 219.8927 - loglik: -2.1876e+02 - logprior: -1.1306e+00
Epoch 2/2
29/29 - 3s - loss: 216.3173 - loglik: -2.1557e+02 - logprior: -7.4329e-01
Fitted a model with MAP estimate = -197.6364
expansions: []
discards: [ 1 37 62 68]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 218.1883 - loglik: -2.1717e+02 - logprior: -1.0231e+00
Epoch 2/2
29/29 - 3s - loss: 216.2066 - loglik: -2.1549e+02 - logprior: -7.1585e-01
Fitted a model with MAP estimate = -197.9971
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.7804 - loglik: -1.9711e+02 - logprior: -6.7294e-01
Epoch 2/10
42/42 - 4s - loss: 197.0403 - loglik: -1.9655e+02 - logprior: -4.9115e-01
Epoch 3/10
42/42 - 4s - loss: 196.2144 - loglik: -1.9573e+02 - logprior: -4.8623e-01
Epoch 4/10
42/42 - 4s - loss: 196.0511 - loglik: -1.9557e+02 - logprior: -4.8063e-01
Epoch 5/10
42/42 - 4s - loss: 195.0571 - loglik: -1.9458e+02 - logprior: -4.8013e-01
Epoch 6/10
42/42 - 4s - loss: 195.3241 - loglik: -1.9485e+02 - logprior: -4.7196e-01
Fitted a model with MAP estimate = -194.8118
Time for alignment: 100.0870
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 250.4878 - loglik: -2.4934e+02 - logprior: -1.1487e+00
Epoch 2/10
29/29 - 3s - loss: 225.3128 - loglik: -2.2452e+02 - logprior: -7.9768e-01
Epoch 3/10
29/29 - 3s - loss: 221.2495 - loglik: -2.2047e+02 - logprior: -7.8276e-01
Epoch 4/10
29/29 - 3s - loss: 220.0198 - loglik: -2.1922e+02 - logprior: -7.9637e-01
Epoch 5/10
29/29 - 3s - loss: 218.6072 - loglik: -2.1781e+02 - logprior: -7.9729e-01
Epoch 6/10
29/29 - 3s - loss: 218.8698 - loglik: -2.1809e+02 - logprior: -7.8336e-01
Fitted a model with MAP estimate = -200.9774
expansions: [(1, 1), (2, 1), (13, 2), (14, 3), (39, 2), (41, 1), (42, 1), (43, 1), (44, 2), (45, 2), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 220.3143 - loglik: -2.1919e+02 - logprior: -1.1243e+00
Epoch 2/2
29/29 - 3s - loss: 216.0762 - loglik: -2.1533e+02 - logprior: -7.4185e-01
Fitted a model with MAP estimate = -197.7094
expansions: [(20, 1)]
discards: [ 1 47 57 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 218.4525 - loglik: -2.1744e+02 - logprior: -1.0128e+00
Epoch 2/2
29/29 - 3s - loss: 216.3561 - loglik: -2.1564e+02 - logprior: -7.1149e-01
Fitted a model with MAP estimate = -198.1132
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.7741 - loglik: -1.9710e+02 - logprior: -6.7345e-01
Epoch 2/10
42/42 - 4s - loss: 197.1033 - loglik: -1.9661e+02 - logprior: -4.8899e-01
Epoch 3/10
42/42 - 4s - loss: 196.4604 - loglik: -1.9598e+02 - logprior: -4.8339e-01
Epoch 4/10
42/42 - 4s - loss: 195.9945 - loglik: -1.9551e+02 - logprior: -4.7983e-01
Epoch 5/10
42/42 - 4s - loss: 195.2252 - loglik: -1.9475e+02 - logprior: -4.7273e-01
Epoch 6/10
42/42 - 4s - loss: 195.2984 - loglik: -1.9483e+02 - logprior: -4.6418e-01
Fitted a model with MAP estimate = -194.8993
Time for alignment: 98.2844
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 250.0962 - loglik: -2.4895e+02 - logprior: -1.1494e+00
Epoch 2/10
29/29 - 3s - loss: 224.4984 - loglik: -2.2370e+02 - logprior: -7.9882e-01
Epoch 3/10
29/29 - 3s - loss: 220.2883 - loglik: -2.1951e+02 - logprior: -7.8030e-01
Epoch 4/10
29/29 - 3s - loss: 218.9275 - loglik: -2.1814e+02 - logprior: -7.8785e-01
Epoch 5/10
29/29 - 3s - loss: 218.4866 - loglik: -2.1770e+02 - logprior: -7.8290e-01
Epoch 6/10
29/29 - 3s - loss: 218.2392 - loglik: -2.1747e+02 - logprior: -7.7368e-01
Epoch 7/10
29/29 - 3s - loss: 217.7803 - loglik: -2.1701e+02 - logprior: -7.6900e-01
Epoch 8/10
29/29 - 3s - loss: 218.2251 - loglik: -2.1746e+02 - logprior: -7.6274e-01
Fitted a model with MAP estimate = -200.1797
expansions: [(1, 1), (2, 1), (13, 2), (14, 3), (21, 1), (38, 1), (41, 1), (43, 1), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 219.3056 - loglik: -2.1820e+02 - logprior: -1.1094e+00
Epoch 2/2
29/29 - 3s - loss: 216.2816 - loglik: -2.1556e+02 - logprior: -7.1850e-01
Fitted a model with MAP estimate = -197.5569
expansions: [(18, 1)]
discards: [ 1 56 64]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.9985 - loglik: -2.1699e+02 - logprior: -1.0134e+00
Epoch 2/2
29/29 - 3s - loss: 216.1223 - loglik: -2.1542e+02 - logprior: -7.0727e-01
Fitted a model with MAP estimate = -197.9911
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.6268 - loglik: -1.9696e+02 - logprior: -6.7019e-01
Epoch 2/10
42/42 - 4s - loss: 197.2650 - loglik: -1.9679e+02 - logprior: -4.7945e-01
Epoch 3/10
42/42 - 4s - loss: 196.0898 - loglik: -1.9561e+02 - logprior: -4.7794e-01
Epoch 4/10
42/42 - 4s - loss: 195.9586 - loglik: -1.9549e+02 - logprior: -4.7186e-01
Epoch 5/10
42/42 - 4s - loss: 195.5170 - loglik: -1.9505e+02 - logprior: -4.6849e-01
Epoch 6/10
42/42 - 4s - loss: 195.0087 - loglik: -1.9455e+02 - logprior: -4.6296e-01
Epoch 7/10
42/42 - 4s - loss: 195.1556 - loglik: -1.9469e+02 - logprior: -4.6087e-01
Fitted a model with MAP estimate = -194.7821
Time for alignment: 109.2104
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 250.8425 - loglik: -2.4969e+02 - logprior: -1.1491e+00
Epoch 2/10
29/29 - 3s - loss: 225.4237 - loglik: -2.2462e+02 - logprior: -8.0740e-01
Epoch 3/10
29/29 - 3s - loss: 221.1680 - loglik: -2.2038e+02 - logprior: -7.9067e-01
Epoch 4/10
29/29 - 3s - loss: 219.0649 - loglik: -2.1825e+02 - logprior: -8.1092e-01
Epoch 5/10
29/29 - 3s - loss: 218.6409 - loglik: -2.1784e+02 - logprior: -7.9815e-01
Epoch 6/10
29/29 - 3s - loss: 218.4255 - loglik: -2.1763e+02 - logprior: -7.9050e-01
Epoch 7/10
29/29 - 3s - loss: 218.4718 - loglik: -2.1769e+02 - logprior: -7.8526e-01
Fitted a model with MAP estimate = -200.5382
expansions: [(1, 1), (2, 1), (13, 2), (14, 3), (27, 2), (39, 1), (41, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 220.3560 - loglik: -2.1922e+02 - logprior: -1.1322e+00
Epoch 2/2
29/29 - 3s - loss: 216.0840 - loglik: -2.1534e+02 - logprior: -7.4431e-01
Fitted a model with MAP estimate = -197.5753
expansions: [(18, 1)]
discards: [ 1 34 57 59 66]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 218.2166 - loglik: -2.1720e+02 - logprior: -1.0190e+00
Epoch 2/2
29/29 - 3s - loss: 216.2646 - loglik: -2.1555e+02 - logprior: -7.1264e-01
Fitted a model with MAP estimate = -197.8682
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.5710 - loglik: -1.9690e+02 - logprior: -6.7434e-01
Epoch 2/10
42/42 - 4s - loss: 197.2623 - loglik: -1.9678e+02 - logprior: -4.8654e-01
Epoch 3/10
42/42 - 4s - loss: 196.0607 - loglik: -1.9558e+02 - logprior: -4.7924e-01
Epoch 4/10
42/42 - 4s - loss: 196.1804 - loglik: -1.9570e+02 - logprior: -4.7592e-01
Fitted a model with MAP estimate = -195.3708
Time for alignment: 94.3477
Computed alignments with likelihoods: ['-194.6972', '-194.8118', '-194.8993', '-194.7821', '-195.3708']
Best model has likelihood: -194.6972  (prior= -0.4635 )
time for generating output: 0.1803
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.59268413247652
Training of 5 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb913981f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0581d790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2cda4d00>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.6024 - loglik: -3.3034e+02 - logprior: -4.5264e+01
Epoch 2/10
10/10 - 1s - loss: 297.9644 - loglik: -2.8712e+02 - logprior: -1.0841e+01
Epoch 3/10
10/10 - 1s - loss: 251.0614 - loglik: -2.4615e+02 - logprior: -4.9083e+00
Epoch 4/10
10/10 - 1s - loss: 220.8892 - loglik: -2.1771e+02 - logprior: -3.1840e+00
Epoch 5/10
10/10 - 1s - loss: 210.1905 - loglik: -2.0788e+02 - logprior: -2.3076e+00
Epoch 6/10
10/10 - 1s - loss: 206.3615 - loglik: -2.0462e+02 - logprior: -1.7439e+00
Epoch 7/10
10/10 - 1s - loss: 204.2985 - loglik: -2.0290e+02 - logprior: -1.3939e+00
Epoch 8/10
10/10 - 1s - loss: 203.2155 - loglik: -2.0204e+02 - logprior: -1.1712e+00
Epoch 9/10
10/10 - 1s - loss: 202.4242 - loglik: -2.0139e+02 - logprior: -1.0392e+00
Epoch 10/10
10/10 - 1s - loss: 201.8431 - loglik: -2.0089e+02 - logprior: -9.5690e-01
Fitted a model with MAP estimate = -201.7165
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (67, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 251.2546 - loglik: -1.9998e+02 - logprior: -5.1278e+01
Epoch 2/2
10/10 - 1s - loss: 208.3553 - loglik: -1.8815e+02 - logprior: -2.0209e+01
Fitted a model with MAP estimate = -200.3878
expansions: [(0, 3)]
discards: [ 0  9 18]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.0536 - loglik: -1.8462e+02 - logprior: -4.0436e+01
Epoch 2/2
10/10 - 1s - loss: 190.6979 - loglik: -1.8137e+02 - logprior: -9.3280e+00
Fitted a model with MAP estimate = -185.1245
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 233.0463 - loglik: -1.8366e+02 - logprior: -4.9384e+01
Epoch 2/10
10/10 - 1s - loss: 197.1303 - loglik: -1.8192e+02 - logprior: -1.5215e+01
Epoch 3/10
10/10 - 1s - loss: 185.5575 - loglik: -1.8117e+02 - logprior: -4.3866e+00
Epoch 4/10
10/10 - 1s - loss: 179.5652 - loglik: -1.7892e+02 - logprior: -6.4536e-01
Epoch 5/10
10/10 - 1s - loss: 178.0076 - loglik: -1.7879e+02 - logprior: 0.7794
Epoch 6/10
10/10 - 1s - loss: 176.2796 - loglik: -1.7805e+02 - logprior: 1.7719
Epoch 7/10
10/10 - 1s - loss: 176.0570 - loglik: -1.7853e+02 - logprior: 2.4753
Epoch 8/10
10/10 - 1s - loss: 174.8975 - loglik: -1.7778e+02 - logprior: 2.8838
Epoch 9/10
10/10 - 1s - loss: 174.8663 - loglik: -1.7807e+02 - logprior: 3.2065
Epoch 10/10
10/10 - 1s - loss: 174.1007 - loglik: -1.7763e+02 - logprior: 3.5320
Fitted a model with MAP estimate = -174.2074
Time for alignment: 43.5514
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.6406 - loglik: -3.3038e+02 - logprior: -4.5264e+01
Epoch 2/10
10/10 - 1s - loss: 297.2255 - loglik: -2.8640e+02 - logprior: -1.0826e+01
Epoch 3/10
10/10 - 1s - loss: 249.2269 - loglik: -2.4440e+02 - logprior: -4.8236e+00
Epoch 4/10
10/10 - 1s - loss: 225.2417 - loglik: -2.2238e+02 - logprior: -2.8612e+00
Epoch 5/10
10/10 - 1s - loss: 216.1344 - loglik: -2.1427e+02 - logprior: -1.8689e+00
Epoch 6/10
10/10 - 1s - loss: 212.0537 - loglik: -2.1082e+02 - logprior: -1.2333e+00
Epoch 7/10
10/10 - 1s - loss: 204.7376 - loglik: -2.0377e+02 - logprior: -9.6393e-01
Epoch 8/10
10/10 - 1s - loss: 201.6311 - loglik: -2.0073e+02 - logprior: -8.9671e-01
Epoch 9/10
10/10 - 1s - loss: 200.4281 - loglik: -1.9956e+02 - logprior: -8.6711e-01
Epoch 10/10
10/10 - 1s - loss: 200.0034 - loglik: -1.9923e+02 - logprior: -7.7304e-01
Fitted a model with MAP estimate = -199.6332
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (58, 2), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.0169 - loglik: -1.9974e+02 - logprior: -5.1281e+01
Epoch 2/2
10/10 - 1s - loss: 208.5325 - loglik: -1.8816e+02 - logprior: -2.0371e+01
Fitted a model with MAP estimate = -200.3856
expansions: [(0, 3)]
discards: [  0   9  19  75 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 227.7807 - loglik: -1.8732e+02 - logprior: -4.0463e+01
Epoch 2/2
10/10 - 1s - loss: 192.9384 - loglik: -1.8353e+02 - logprior: -9.4087e+00
Fitted a model with MAP estimate = -187.3156
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 235.7108 - loglik: -1.8589e+02 - logprior: -4.9824e+01
Epoch 2/10
10/10 - 1s - loss: 200.1869 - loglik: -1.8359e+02 - logprior: -1.6592e+01
Epoch 3/10
10/10 - 1s - loss: 187.3171 - loglik: -1.8185e+02 - logprior: -5.4644e+00
Epoch 4/10
10/10 - 1s - loss: 181.9442 - loglik: -1.8104e+02 - logprior: -9.0785e-01
Epoch 5/10
10/10 - 1s - loss: 179.0202 - loglik: -1.7969e+02 - logprior: 0.6717
Epoch 6/10
10/10 - 1s - loss: 177.6378 - loglik: -1.7933e+02 - logprior: 1.6894
Epoch 7/10
10/10 - 1s - loss: 176.8795 - loglik: -1.7928e+02 - logprior: 2.3976
Epoch 8/10
10/10 - 1s - loss: 176.3707 - loglik: -1.7917e+02 - logprior: 2.8010
Epoch 9/10
10/10 - 1s - loss: 175.8369 - loglik: -1.7896e+02 - logprior: 3.1237
Epoch 10/10
10/10 - 1s - loss: 175.5481 - loglik: -1.7899e+02 - logprior: 3.4377
Fitted a model with MAP estimate = -175.3450
Time for alignment: 41.6500
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 375.7061 - loglik: -3.3044e+02 - logprior: -4.5266e+01
Epoch 2/10
10/10 - 1s - loss: 297.7664 - loglik: -2.8693e+02 - logprior: -1.0834e+01
Epoch 3/10
10/10 - 1s - loss: 252.3115 - loglik: -2.4750e+02 - logprior: -4.8094e+00
Epoch 4/10
10/10 - 1s - loss: 227.7520 - loglik: -2.2497e+02 - logprior: -2.7788e+00
Epoch 5/10
10/10 - 1s - loss: 218.3748 - loglik: -2.1660e+02 - logprior: -1.7751e+00
Epoch 6/10
10/10 - 1s - loss: 212.5739 - loglik: -2.1134e+02 - logprior: -1.2389e+00
Epoch 7/10
10/10 - 1s - loss: 206.2281 - loglik: -2.0522e+02 - logprior: -1.0090e+00
Epoch 8/10
10/10 - 1s - loss: 202.9459 - loglik: -2.0210e+02 - logprior: -8.4904e-01
Epoch 9/10
10/10 - 1s - loss: 202.1844 - loglik: -2.0140e+02 - logprior: -7.7996e-01
Epoch 10/10
10/10 - 1s - loss: 201.2246 - loglik: -2.0043e+02 - logprior: -7.9367e-01
Fitted a model with MAP estimate = -200.8670
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (64, 1), (67, 1), (68, 2), (79, 1), (81, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.0050 - loglik: -1.9979e+02 - logprior: -5.1220e+01
Epoch 2/2
10/10 - 1s - loss: 208.2598 - loglik: -1.8817e+02 - logprior: -2.0093e+01
Fitted a model with MAP estimate = -200.2065
expansions: [(0, 3)]
discards: [ 0  9 19]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 225.2866 - loglik: -1.8499e+02 - logprior: -4.0292e+01
Epoch 2/2
10/10 - 1s - loss: 190.4324 - loglik: -1.8117e+02 - logprior: -9.2620e+00
Fitted a model with MAP estimate = -185.0304
expansions: []
discards: [  0   2 103]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 234.2151 - loglik: -1.8451e+02 - logprior: -4.9701e+01
Epoch 2/10
10/10 - 1s - loss: 199.4254 - loglik: -1.8297e+02 - logprior: -1.6455e+01
Epoch 3/10
10/10 - 1s - loss: 186.4156 - loglik: -1.8108e+02 - logprior: -5.3373e+00
Epoch 4/10
10/10 - 1s - loss: 180.5581 - loglik: -1.7974e+02 - logprior: -8.2017e-01
Epoch 5/10
10/10 - 1s - loss: 178.2343 - loglik: -1.7898e+02 - logprior: 0.7496
Epoch 6/10
10/10 - 1s - loss: 177.0648 - loglik: -1.7883e+02 - logprior: 1.7625
Epoch 7/10
10/10 - 1s - loss: 176.0796 - loglik: -1.7856e+02 - logprior: 2.4783
Epoch 8/10
10/10 - 1s - loss: 175.3344 - loglik: -1.7822e+02 - logprior: 2.8873
Epoch 9/10
10/10 - 1s - loss: 175.1319 - loglik: -1.7834e+02 - logprior: 3.2113
Epoch 10/10
10/10 - 1s - loss: 174.5383 - loglik: -1.7807e+02 - logprior: 3.5314
Fitted a model with MAP estimate = -174.4913
Time for alignment: 42.4558
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 375.8232 - loglik: -3.3056e+02 - logprior: -4.5266e+01
Epoch 2/10
10/10 - 1s - loss: 298.2083 - loglik: -2.8736e+02 - logprior: -1.0844e+01
Epoch 3/10
10/10 - 1s - loss: 251.0515 - loglik: -2.4608e+02 - logprior: -4.9690e+00
Epoch 4/10
10/10 - 1s - loss: 223.8003 - loglik: -2.2055e+02 - logprior: -3.2458e+00
Epoch 5/10
10/10 - 1s - loss: 212.5300 - loglik: -2.1029e+02 - logprior: -2.2414e+00
Epoch 6/10
10/10 - 1s - loss: 207.8101 - loglik: -2.0616e+02 - logprior: -1.6503e+00
Epoch 7/10
10/10 - 1s - loss: 204.9778 - loglik: -2.0360e+02 - logprior: -1.3747e+00
Epoch 8/10
10/10 - 1s - loss: 203.7039 - loglik: -2.0248e+02 - logprior: -1.2233e+00
Epoch 9/10
10/10 - 1s - loss: 202.6608 - loglik: -2.0155e+02 - logprior: -1.1095e+00
Epoch 10/10
10/10 - 1s - loss: 201.9412 - loglik: -2.0090e+02 - logprior: -1.0448e+00
Fitted a model with MAP estimate = -201.3272
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (16, 1), (30, 1), (31, 1), (44, 2), (45, 2), (52, 1), (55, 1), (56, 1), (66, 1), (67, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 250.1361 - loglik: -1.9886e+02 - logprior: -5.1277e+01
Epoch 2/2
10/10 - 1s - loss: 207.4535 - loglik: -1.8736e+02 - logprior: -2.0091e+01
Fitted a model with MAP estimate = -199.6155
expansions: [(0, 3)]
discards: [0 9]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 224.6674 - loglik: -1.8426e+02 - logprior: -4.0406e+01
Epoch 2/2
10/10 - 1s - loss: 190.3426 - loglik: -1.8105e+02 - logprior: -9.2893e+00
Fitted a model with MAP estimate = -185.0722
expansions: []
discards: [ 0  2 54 55]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 234.4605 - loglik: -1.8516e+02 - logprior: -4.9301e+01
Epoch 2/10
10/10 - 1s - loss: 198.7892 - loglik: -1.8368e+02 - logprior: -1.5107e+01
Epoch 3/10
10/10 - 1s - loss: 186.6658 - loglik: -1.8233e+02 - logprior: -4.3357e+00
Epoch 4/10
10/10 - 1s - loss: 181.3567 - loglik: -1.8069e+02 - logprior: -6.6218e-01
Epoch 5/10
10/10 - 1s - loss: 179.2570 - loglik: -1.8000e+02 - logprior: 0.7428
Epoch 6/10
10/10 - 1s - loss: 177.6707 - loglik: -1.7942e+02 - logprior: 1.7520
Epoch 7/10
10/10 - 1s - loss: 177.1988 - loglik: -1.7966e+02 - logprior: 2.4576
Epoch 8/10
10/10 - 1s - loss: 176.7050 - loglik: -1.7956e+02 - logprior: 2.8529
Epoch 9/10
10/10 - 1s - loss: 176.1414 - loglik: -1.7931e+02 - logprior: 3.1712
Epoch 10/10
10/10 - 1s - loss: 175.7437 - loglik: -1.7924e+02 - logprior: 3.4932
Fitted a model with MAP estimate = -175.6281
Time for alignment: 42.5011
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.6477 - loglik: -3.3038e+02 - logprior: -4.5264e+01
Epoch 2/10
10/10 - 1s - loss: 296.9537 - loglik: -2.8612e+02 - logprior: -1.0835e+01
Epoch 3/10
10/10 - 1s - loss: 248.5228 - loglik: -2.4367e+02 - logprior: -4.8569e+00
Epoch 4/10
10/10 - 1s - loss: 224.4021 - loglik: -2.2149e+02 - logprior: -2.9106e+00
Epoch 5/10
10/10 - 1s - loss: 214.5217 - loglik: -2.1265e+02 - logprior: -1.8708e+00
Epoch 6/10
10/10 - 1s - loss: 206.9057 - loglik: -2.0561e+02 - logprior: -1.2913e+00
Epoch 7/10
10/10 - 1s - loss: 203.2471 - loglik: -2.0225e+02 - logprior: -1.0014e+00
Epoch 8/10
10/10 - 1s - loss: 201.3879 - loglik: -2.0053e+02 - logprior: -8.5570e-01
Epoch 9/10
10/10 - 1s - loss: 200.6219 - loglik: -1.9980e+02 - logprior: -8.2118e-01
Epoch 10/10
10/10 - 1s - loss: 200.4061 - loglik: -1.9964e+02 - logprior: -7.6622e-01
Fitted a model with MAP estimate = -199.8075
expansions: [(10, 2), (11, 3), (12, 3), (13, 3), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (64, 2), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.4221 - loglik: -1.9924e+02 - logprior: -5.1185e+01
Epoch 2/2
10/10 - 1s - loss: 206.8516 - loglik: -1.8656e+02 - logprior: -2.0295e+01
Fitted a model with MAP estimate = -199.0481
expansions: [(0, 3)]
discards: [  0   9  17  82 104 105]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.2510 - loglik: -1.8581e+02 - logprior: -4.0441e+01
Epoch 2/2
10/10 - 1s - loss: 192.4957 - loglik: -1.8305e+02 - logprior: -9.4484e+00
Fitted a model with MAP estimate = -186.5210
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 234.5976 - loglik: -1.8489e+02 - logprior: -4.9711e+01
Epoch 2/10
10/10 - 1s - loss: 199.0463 - loglik: -1.8298e+02 - logprior: -1.6063e+01
Epoch 3/10
10/10 - 1s - loss: 186.2214 - loglik: -1.8124e+02 - logprior: -4.9840e+00
Epoch 4/10
10/10 - 1s - loss: 180.3190 - loglik: -1.7949e+02 - logprior: -8.2819e-01
Epoch 5/10
10/10 - 1s - loss: 178.6097 - loglik: -1.7931e+02 - logprior: 0.6986
Epoch 6/10
10/10 - 1s - loss: 176.9607 - loglik: -1.7867e+02 - logprior: 1.7111
Epoch 7/10
10/10 - 1s - loss: 176.3264 - loglik: -1.7874e+02 - logprior: 2.4148
Epoch 8/10
10/10 - 1s - loss: 175.5012 - loglik: -1.7832e+02 - logprior: 2.8158
Epoch 9/10
10/10 - 1s - loss: 175.2613 - loglik: -1.7840e+02 - logprior: 3.1404
Epoch 10/10
10/10 - 1s - loss: 174.9516 - loglik: -1.7841e+02 - logprior: 3.4595
Fitted a model with MAP estimate = -174.6960
Time for alignment: 41.9685
Computed alignments with likelihoods: ['-174.2074', '-175.3450', '-174.4913', '-175.6281', '-174.6960']
Best model has likelihood: -174.2074  (prior= 3.6902 )
time for generating output: 0.1402
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.9110195445180689
Training of 5 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb499b5790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feafd38a7c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2ccf81f0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 631.6595 - loglik: -6.0919e+02 - logprior: -2.2467e+01
Epoch 2/10
14/14 - 3s - loss: 537.0585 - loglik: -5.3507e+02 - logprior: -1.9931e+00
Epoch 3/10
14/14 - 3s - loss: 478.5859 - loglik: -4.7803e+02 - logprior: -5.5872e-01
Epoch 4/10
14/14 - 3s - loss: 462.5202 - loglik: -4.6239e+02 - logprior: -1.2790e-01
Epoch 5/10
14/14 - 3s - loss: 457.9993 - loglik: -4.5807e+02 - logprior: 0.0745
Epoch 6/10
14/14 - 3s - loss: 456.5616 - loglik: -4.5678e+02 - logprior: 0.2204
Epoch 7/10
14/14 - 3s - loss: 454.0595 - loglik: -4.5435e+02 - logprior: 0.2943
Epoch 8/10
14/14 - 3s - loss: 455.0551 - loglik: -4.5547e+02 - logprior: 0.4107
Fitted a model with MAP estimate = -454.1678
expansions: [(14, 1), (15, 1), (28, 1), (30, 1), (31, 2), (32, 2), (36, 1), (41, 2), (42, 1), (44, 1), (52, 1), (53, 1), (55, 4), (81, 1), (90, 1), (91, 7), (112, 1), (113, 2), (114, 2), (115, 4), (117, 2), (118, 2), (119, 2), (128, 2), (153, 1), (163, 1), (166, 8), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 478.3823 - loglik: -4.5186e+02 - logprior: -2.6525e+01
Epoch 2/2
14/14 - 5s - loss: 447.7368 - loglik: -4.3963e+02 - logprior: -8.1086e+00
Fitted a model with MAP estimate = -441.9021
expansions: [(0, 4), (69, 2), (148, 1)]
discards: [  0  50  73  74  75 113 142 145 154 159 171 218 219]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 461.4719 - loglik: -4.4223e+02 - logprior: -1.9237e+01
Epoch 2/2
14/14 - 4s - loss: 437.4353 - loglik: -4.3700e+02 - logprior: -4.3275e-01
Fitted a model with MAP estimate = -434.5405
expansions: [(80, 2), (117, 1)]
discards: [  1   2   3  72  77 112 113 114]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 459.7017 - loglik: -4.4116e+02 - logprior: -1.8545e+01
Epoch 2/10
14/14 - 4s - loss: 436.2614 - loglik: -4.3616e+02 - logprior: -1.0603e-01
Epoch 3/10
14/14 - 4s - loss: 433.6967 - loglik: -4.3640e+02 - logprior: 2.7004
Epoch 4/10
14/14 - 4s - loss: 431.2322 - loglik: -4.3511e+02 - logprior: 3.8777
Epoch 5/10
14/14 - 4s - loss: 430.4380 - loglik: -4.3509e+02 - logprior: 4.6558
Epoch 6/10
14/14 - 4s - loss: 429.9685 - loglik: -4.3500e+02 - logprior: 5.0310
Epoch 7/10
14/14 - 4s - loss: 427.9741 - loglik: -4.3337e+02 - logprior: 5.4004
Epoch 8/10
14/14 - 4s - loss: 428.3333 - loglik: -4.3402e+02 - logprior: 5.6899
Fitted a model with MAP estimate = -428.2702
Time for alignment: 104.1074
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 631.7144 - loglik: -6.0925e+02 - logprior: -2.2468e+01
Epoch 2/10
14/14 - 3s - loss: 538.0555 - loglik: -5.3607e+02 - logprior: -1.9891e+00
Epoch 3/10
14/14 - 3s - loss: 478.8535 - loglik: -4.7837e+02 - logprior: -4.8107e-01
Epoch 4/10
14/14 - 3s - loss: 466.9134 - loglik: -4.6702e+02 - logprior: 0.1064
Epoch 5/10
14/14 - 3s - loss: 460.8674 - loglik: -4.6127e+02 - logprior: 0.3988
Epoch 6/10
14/14 - 3s - loss: 458.7287 - loglik: -4.5931e+02 - logprior: 0.5769
Epoch 7/10
14/14 - 3s - loss: 460.0662 - loglik: -4.6076e+02 - logprior: 0.6964
Fitted a model with MAP estimate = -458.4818
expansions: [(15, 1), (16, 2), (28, 1), (29, 2), (30, 2), (31, 2), (41, 2), (42, 1), (51, 1), (52, 1), (53, 1), (54, 4), (80, 1), (90, 1), (113, 1), (114, 2), (115, 2), (116, 3), (118, 1), (119, 2), (120, 3), (130, 2), (164, 1), (166, 9), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 481.3804 - loglik: -4.5492e+02 - logprior: -2.6459e+01
Epoch 2/2
14/14 - 4s - loss: 448.2496 - loglik: -4.4041e+02 - logprior: -7.8428e+00
Fitted a model with MAP estimate = -443.7341
expansions: [(70, 2), (138, 1)]
discards: [  0  51  73  74  75  90 143 152 166 212]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 471.6888 - loglik: -4.4590e+02 - logprior: -2.5793e+01
Epoch 2/2
14/14 - 4s - loss: 448.0731 - loglik: -4.4181e+02 - logprior: -6.2670e+00
Fitted a model with MAP estimate = -442.6669
expansions: [(0, 4)]
discards: [ 0 19 68 70]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 461.5009 - loglik: -4.4276e+02 - logprior: -1.8742e+01
Epoch 2/10
14/14 - 4s - loss: 439.2879 - loglik: -4.3927e+02 - logprior: -2.1824e-02
Epoch 3/10
14/14 - 4s - loss: 436.3371 - loglik: -4.3936e+02 - logprior: 3.0209
Epoch 4/10
14/14 - 4s - loss: 433.7904 - loglik: -4.3796e+02 - logprior: 4.1698
Epoch 5/10
14/14 - 4s - loss: 433.6577 - loglik: -4.3831e+02 - logprior: 4.6563
Epoch 6/10
14/14 - 4s - loss: 432.9199 - loglik: -4.3794e+02 - logprior: 5.0155
Epoch 7/10
14/14 - 4s - loss: 432.9265 - loglik: -4.3829e+02 - logprior: 5.3630
Fitted a model with MAP estimate = -432.1584
Time for alignment: 93.2996
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 630.8270 - loglik: -6.0836e+02 - logprior: -2.2462e+01
Epoch 2/10
14/14 - 3s - loss: 537.6850 - loglik: -5.3571e+02 - logprior: -1.9779e+00
Epoch 3/10
14/14 - 3s - loss: 480.1554 - loglik: -4.7976e+02 - logprior: -3.9895e-01
Epoch 4/10
14/14 - 3s - loss: 463.7644 - loglik: -4.6403e+02 - logprior: 0.2614
Epoch 5/10
14/14 - 3s - loss: 459.6311 - loglik: -4.6001e+02 - logprior: 0.3810
Epoch 6/10
14/14 - 3s - loss: 456.8291 - loglik: -4.5740e+02 - logprior: 0.5679
Epoch 7/10
14/14 - 3s - loss: 456.3918 - loglik: -4.5709e+02 - logprior: 0.7018
Epoch 8/10
14/14 - 3s - loss: 455.5867 - loglik: -4.5640e+02 - logprior: 0.8111
Epoch 9/10
14/14 - 3s - loss: 455.5444 - loglik: -4.5641e+02 - logprior: 0.8680
Epoch 10/10
14/14 - 3s - loss: 455.2604 - loglik: -4.5614e+02 - logprior: 0.8796
Fitted a model with MAP estimate = -454.6877
expansions: [(15, 1), (16, 2), (28, 1), (30, 1), (31, 2), (32, 2), (36, 2), (40, 2), (41, 1), (43, 1), (51, 1), (52, 1), (54, 3), (81, 1), (90, 1), (110, 1), (114, 1), (115, 1), (116, 4), (118, 2), (119, 2), (120, 2), (130, 2), (160, 1), (167, 9)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 477.5799 - loglik: -4.5118e+02 - logprior: -2.6400e+01
Epoch 2/2
14/14 - 4s - loss: 448.4529 - loglik: -4.4088e+02 - logprior: -7.5730e+00
Fitted a model with MAP estimate = -444.4553
expansions: [(0, 4), (141, 1)]
discards: [  0  18  51 165]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 459.3938 - loglik: -4.4050e+02 - logprior: -1.8898e+01
Epoch 2/2
14/14 - 4s - loss: 438.5303 - loglik: -4.3854e+02 - logprior: 0.0047
Fitted a model with MAP estimate = -434.5148
expansions: [(71, 2)]
discards: [ 1  2  3 75 76]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 459.3531 - loglik: -4.4099e+02 - logprior: -1.8362e+01
Epoch 2/10
14/14 - 4s - loss: 437.0576 - loglik: -4.3728e+02 - logprior: 0.2186
Epoch 3/10
14/14 - 4s - loss: 435.7281 - loglik: -4.3871e+02 - logprior: 2.9792
Epoch 4/10
14/14 - 4s - loss: 432.0025 - loglik: -4.3619e+02 - logprior: 4.1915
Epoch 5/10
14/14 - 4s - loss: 431.0578 - loglik: -4.3608e+02 - logprior: 5.0244
Epoch 6/10
14/14 - 4s - loss: 431.2000 - loglik: -4.3662e+02 - logprior: 5.4177
Fitted a model with MAP estimate = -430.4711
Time for alignment: 99.6543
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 630.9448 - loglik: -6.0847e+02 - logprior: -2.2472e+01
Epoch 2/10
14/14 - 3s - loss: 538.1817 - loglik: -5.3620e+02 - logprior: -1.9856e+00
Epoch 3/10
14/14 - 3s - loss: 477.0965 - loglik: -4.7664e+02 - logprior: -4.5992e-01
Epoch 4/10
14/14 - 3s - loss: 460.8295 - loglik: -4.6090e+02 - logprior: 0.0709
Epoch 5/10
14/14 - 3s - loss: 458.0804 - loglik: -4.5844e+02 - logprior: 0.3566
Epoch 6/10
14/14 - 3s - loss: 455.8120 - loglik: -4.5635e+02 - logprior: 0.5376
Epoch 7/10
14/14 - 3s - loss: 455.6910 - loglik: -4.5633e+02 - logprior: 0.6390
Epoch 8/10
14/14 - 3s - loss: 453.0322 - loglik: -4.5376e+02 - logprior: 0.7307
Epoch 9/10
14/14 - 3s - loss: 455.1058 - loglik: -4.5587e+02 - logprior: 0.7652
Fitted a model with MAP estimate = -453.8320
expansions: [(14, 2), (16, 2), (28, 1), (29, 2), (30, 2), (31, 2), (39, 2), (40, 2), (41, 2), (51, 1), (52, 1), (54, 3), (91, 1), (111, 1), (113, 1), (114, 1), (115, 1), (116, 4), (118, 2), (119, 2), (120, 2), (130, 2), (157, 1), (163, 1), (166, 8), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 476.4058 - loglik: -4.5000e+02 - logprior: -2.6409e+01
Epoch 2/2
14/14 - 4s - loss: 449.3935 - loglik: -4.4178e+02 - logprior: -7.6172e+00
Fitted a model with MAP estimate = -444.0656
expansions: [(0, 4), (143, 1)]
discards: [  0  18  19  50 167 212 213]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 459.6712 - loglik: -4.4077e+02 - logprior: -1.8904e+01
Epoch 2/2
14/14 - 4s - loss: 440.0142 - loglik: -4.4001e+02 - logprior: -5.9015e-03
Fitted a model with MAP estimate = -435.1584
expansions: [(72, 2)]
discards: [ 1  2  3 75 76 77]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 459.5904 - loglik: -4.4115e+02 - logprior: -1.8445e+01
Epoch 2/10
14/14 - 4s - loss: 439.3150 - loglik: -4.3947e+02 - logprior: 0.1556
Epoch 3/10
14/14 - 4s - loss: 434.8726 - loglik: -4.3788e+02 - logprior: 3.0115
Epoch 4/10
14/14 - 4s - loss: 432.5972 - loglik: -4.3683e+02 - logprior: 4.2368
Epoch 5/10
14/14 - 4s - loss: 433.6926 - loglik: -4.3873e+02 - logprior: 5.0416
Fitted a model with MAP estimate = -432.0821
Time for alignment: 92.6186
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 631.7766 - loglik: -6.0931e+02 - logprior: -2.2462e+01
Epoch 2/10
14/14 - 3s - loss: 537.3083 - loglik: -5.3532e+02 - logprior: -1.9863e+00
Epoch 3/10
14/14 - 3s - loss: 483.6195 - loglik: -4.8312e+02 - logprior: -4.9962e-01
Epoch 4/10
14/14 - 3s - loss: 464.0553 - loglik: -4.6401e+02 - logprior: -4.2193e-02
Epoch 5/10
14/14 - 3s - loss: 458.5478 - loglik: -4.5868e+02 - logprior: 0.1322
Epoch 6/10
14/14 - 3s - loss: 456.2543 - loglik: -4.5655e+02 - logprior: 0.2991
Epoch 7/10
14/14 - 3s - loss: 456.6664 - loglik: -4.5708e+02 - logprior: 0.4091
Fitted a model with MAP estimate = -456.1220
expansions: [(15, 1), (16, 2), (28, 1), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 2), (52, 1), (53, 1), (54, 4), (62, 1), (80, 1), (90, 1), (110, 1), (112, 1), (115, 3), (116, 2), (119, 1), (120, 1), (121, 3), (125, 1), (158, 1), (164, 1), (167, 9)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 478.4383 - loglik: -4.5199e+02 - logprior: -2.6445e+01
Epoch 2/2
14/14 - 4s - loss: 447.8276 - loglik: -4.4021e+02 - logprior: -7.6223e+00
Fitted a model with MAP estimate = -444.6516
expansions: [(143, 1)]
discards: [ 0 49 54 71 72]
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 467.7336 - loglik: -4.4207e+02 - logprior: -2.5668e+01
Epoch 2/2
14/14 - 4s - loss: 446.7392 - loglik: -4.4089e+02 - logprior: -5.8442e+00
Fitted a model with MAP estimate = -441.1855
expansions: [(0, 4), (68, 3)]
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 459.4737 - loglik: -4.4108e+02 - logprior: -1.8389e+01
Epoch 2/10
14/14 - 4s - loss: 436.5569 - loglik: -4.3686e+02 - logprior: 0.3031
Epoch 3/10
14/14 - 4s - loss: 433.6562 - loglik: -4.3700e+02 - logprior: 3.3450
Epoch 4/10
14/14 - 4s - loss: 430.9743 - loglik: -4.3549e+02 - logprior: 4.5119
Epoch 5/10
14/14 - 4s - loss: 430.9049 - loglik: -4.3594e+02 - logprior: 5.0327
Epoch 6/10
14/14 - 4s - loss: 429.0713 - loglik: -4.3447e+02 - logprior: 5.3983
Epoch 7/10
14/14 - 4s - loss: 430.6123 - loglik: -4.3634e+02 - logprior: 5.7268
Fitted a model with MAP estimate = -429.1496
Time for alignment: 95.3657
Computed alignments with likelihoods: ['-428.2702', '-432.1584', '-430.4711', '-432.0821', '-429.1496']
Best model has likelihood: -428.2702  (prior= 5.8405 )
time for generating output: 0.2138
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.8825561312607945
Training of 5 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49c11dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb409acca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5a87ca00>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 738.2727 - loglik: -6.9767e+02 - logprior: -4.0606e+01
Epoch 2/10
11/11 - 8s - loss: 624.0400 - loglik: -6.2112e+02 - logprior: -2.9236e+00
Epoch 3/10
11/11 - 8s - loss: 536.5923 - loglik: -5.3823e+02 - logprior: 1.6397
Epoch 4/10
11/11 - 8s - loss: 490.3860 - loglik: -4.9280e+02 - logprior: 2.4163
Epoch 5/10
11/11 - 8s - loss: 477.4760 - loglik: -4.8023e+02 - logprior: 2.7577
Epoch 6/10
11/11 - 8s - loss: 468.9535 - loglik: -4.7210e+02 - logprior: 3.1469
Epoch 7/10
11/11 - 8s - loss: 465.7609 - loglik: -4.6912e+02 - logprior: 3.3612
Epoch 8/10
11/11 - 8s - loss: 466.6365 - loglik: -4.7018e+02 - logprior: 3.5430
Fitted a model with MAP estimate = -464.7154
expansions: [(22, 3), (27, 1), (44, 1), (51, 1), (57, 1), (62, 1), (64, 4), (65, 1), (78, 2), (79, 2), (80, 1), (81, 1), (89, 1), (102, 2), (103, 1), (108, 5), (137, 1), (162, 1), (163, 5), (180, 1), (181, 1), (182, 3), (198, 3), (199, 3), (200, 2), (201, 5), (220, 1), (224, 1), (225, 1)]
discards: [  0 210]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 513.2047 - loglik: -4.6607e+02 - logprior: -4.7131e+01
Epoch 2/2
11/11 - 10s - loss: 458.4387 - loglik: -4.4541e+02 - logprior: -1.3027e+01
Fitted a model with MAP estimate = -447.0239
expansions: [(0, 2), (218, 1), (239, 1), (240, 1), (245, 1), (274, 2)]
discards: [  0  74  91 122 130 131 132 292 295]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 477.2456 - loglik: -4.4224e+02 - logprior: -3.5008e+01
Epoch 2/2
11/11 - 11s - loss: 436.4640 - loglik: -4.3626e+02 - logprior: -1.9941e-01
Fitted a model with MAP estimate = -429.7360
expansions: [(22, 3), (194, 1)]
discards: [ 0 75]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 15s - loss: 483.7669 - loglik: -4.3900e+02 - logprior: -4.4767e+01
Epoch 2/10
11/11 - 11s - loss: 441.7310 - loglik: -4.3225e+02 - logprior: -9.4847e+00
Epoch 3/10
11/11 - 10s - loss: 430.8779 - loglik: -4.3408e+02 - logprior: 3.2024
Epoch 4/10
11/11 - 11s - loss: 422.5298 - loglik: -4.3246e+02 - logprior: 9.9267
Epoch 5/10
11/11 - 11s - loss: 416.9675 - loglik: -4.2886e+02 - logprior: 11.8918
Epoch 6/10
11/11 - 11s - loss: 415.4086 - loglik: -4.2832e+02 - logprior: 12.9117
Epoch 7/10
11/11 - 11s - loss: 416.5918 - loglik: -4.3047e+02 - logprior: 13.8778
Fitted a model with MAP estimate = -414.2470
Time for alignment: 215.6870
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 741.3868 - loglik: -7.0079e+02 - logprior: -4.0595e+01
Epoch 2/10
11/11 - 8s - loss: 624.5244 - loglik: -6.2154e+02 - logprior: -2.9866e+00
Epoch 3/10
11/11 - 8s - loss: 540.3725 - loglik: -5.4239e+02 - logprior: 2.0135
Epoch 4/10
11/11 - 9s - loss: 496.8372 - loglik: -5.0014e+02 - logprior: 3.3071
Epoch 5/10
11/11 - 8s - loss: 479.7898 - loglik: -4.8372e+02 - logprior: 3.9299
Epoch 6/10
11/11 - 8s - loss: 475.6312 - loglik: -4.8002e+02 - logprior: 4.3866
Epoch 7/10
11/11 - 8s - loss: 468.0124 - loglik: -4.7266e+02 - logprior: 4.6459
Epoch 8/10
11/11 - 9s - loss: 469.7591 - loglik: -4.7460e+02 - logprior: 4.8396
Fitted a model with MAP estimate = -467.9082
expansions: [(22, 4), (29, 1), (43, 1), (50, 1), (52, 2), (53, 1), (63, 4), (64, 1), (77, 1), (78, 2), (79, 2), (89, 1), (103, 1), (108, 1), (165, 1), (166, 6), (184, 7), (200, 14), (224, 1), (225, 1)]
discards: [  0 207]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 517.3144 - loglik: -4.7028e+02 - logprior: -4.7032e+01
Epoch 2/2
11/11 - 11s - loss: 460.8491 - loglik: -4.4811e+02 - logprior: -1.2738e+01
Fitted a model with MAP estimate = -453.3678
expansions: [(0, 2), (217, 1), (247, 2)]
discards: [  0  59  75  76  93 193 289 291 292]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 486.6696 - loglik: -4.5170e+02 - logprior: -3.4970e+01
Epoch 2/2
11/11 - 10s - loss: 442.9598 - loglik: -4.4280e+02 - logprior: -1.5738e-01
Fitted a model with MAP estimate = -438.5265
expansions: [(22, 2), (270, 2), (288, 1), (289, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 492.0714 - loglik: -4.4712e+02 - logprior: -4.4949e+01
Epoch 2/10
11/11 - 11s - loss: 446.9818 - loglik: -4.3720e+02 - logprior: -9.7809e+00
Epoch 3/10
11/11 - 11s - loss: 435.7262 - loglik: -4.3791e+02 - logprior: 2.1879
Epoch 4/10
11/11 - 10s - loss: 427.8644 - loglik: -4.3768e+02 - logprior: 9.8129
Epoch 5/10
11/11 - 11s - loss: 422.9762 - loglik: -4.3508e+02 - logprior: 12.0990
Epoch 6/10
11/11 - 11s - loss: 422.9619 - loglik: -4.3611e+02 - logprior: 13.1447
Epoch 7/10
11/11 - 10s - loss: 418.1997 - loglik: -4.3222e+02 - logprior: 14.0231
Epoch 8/10
11/11 - 11s - loss: 420.2655 - loglik: -4.3492e+02 - logprior: 14.6496
Fitted a model with MAP estimate = -418.4579
Time for alignment: 223.2245
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 736.1422 - loglik: -6.9553e+02 - logprior: -4.0613e+01
Epoch 2/10
11/11 - 8s - loss: 631.6406 - loglik: -6.2870e+02 - logprior: -2.9357e+00
Epoch 3/10
11/11 - 8s - loss: 541.6829 - loglik: -5.4354e+02 - logprior: 1.8587
Epoch 4/10
11/11 - 7s - loss: 494.9317 - loglik: -4.9772e+02 - logprior: 2.7873
Epoch 5/10
11/11 - 8s - loss: 480.5098 - loglik: -4.8356e+02 - logprior: 3.0458
Epoch 6/10
11/11 - 8s - loss: 474.4506 - loglik: -4.7782e+02 - logprior: 3.3721
Epoch 7/10
11/11 - 9s - loss: 471.1985 - loglik: -4.7488e+02 - logprior: 3.6797
Epoch 8/10
11/11 - 8s - loss: 468.9783 - loglik: -4.7287e+02 - logprior: 3.8923
Epoch 9/10
11/11 - 8s - loss: 467.6400 - loglik: -4.7174e+02 - logprior: 4.1038
Epoch 10/10
11/11 - 8s - loss: 469.1250 - loglik: -4.7346e+02 - logprior: 4.3324
Fitted a model with MAP estimate = -467.9030
expansions: [(23, 1), (38, 1), (49, 2), (50, 1), (52, 2), (61, 1), (63, 1), (65, 1), (78, 3), (79, 2), (90, 1), (103, 1), (104, 1), (108, 1), (109, 1), (164, 1), (165, 6), (182, 7), (198, 3), (199, 4), (200, 2), (201, 5), (219, 1), (222, 1), (224, 1), (225, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 514.4852 - loglik: -4.6747e+02 - logprior: -4.7014e+01
Epoch 2/2
11/11 - 10s - loss: 460.3008 - loglik: -4.4757e+02 - logprior: -1.2733e+01
Fitted a model with MAP estimate = -451.3307
expansions: [(0, 3), (212, 1), (241, 1)]
discards: [  0  91 291]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 477.9642 - loglik: -4.4314e+02 - logprior: -3.4821e+01
Epoch 2/2
11/11 - 11s - loss: 440.5091 - loglik: -4.4055e+02 - logprior: 0.0413
Fitted a model with MAP estimate = -433.4840
expansions: []
discards: [ 1  2 59]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 476.7056 - loglik: -4.4341e+02 - logprior: -3.3296e+01
Epoch 2/10
11/11 - 11s - loss: 438.3470 - loglik: -4.3924e+02 - logprior: 0.8922
Epoch 3/10
11/11 - 11s - loss: 430.5241 - loglik: -4.3824e+02 - logprior: 7.7183
Epoch 4/10
11/11 - 11s - loss: 423.3663 - loglik: -4.3400e+02 - logprior: 10.6289
Epoch 5/10
11/11 - 11s - loss: 426.8426 - loglik: -4.3906e+02 - logprior: 12.2186
Fitted a model with MAP estimate = -422.3762
Time for alignment: 208.5944
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 737.0515 - loglik: -6.9647e+02 - logprior: -4.0586e+01
Epoch 2/10
11/11 - 8s - loss: 630.6453 - loglik: -6.2769e+02 - logprior: -2.9513e+00
Epoch 3/10
11/11 - 8s - loss: 536.9481 - loglik: -5.3898e+02 - logprior: 2.0274
Epoch 4/10
11/11 - 8s - loss: 491.9550 - loglik: -4.9460e+02 - logprior: 2.6474
Epoch 5/10
11/11 - 8s - loss: 477.9332 - loglik: -4.8090e+02 - logprior: 2.9622
Epoch 6/10
11/11 - 8s - loss: 471.8032 - loglik: -4.7515e+02 - logprior: 3.3431
Epoch 7/10
11/11 - 7s - loss: 466.8550 - loglik: -4.7045e+02 - logprior: 3.5961
Epoch 8/10
11/11 - 9s - loss: 464.7911 - loglik: -4.6855e+02 - logprior: 3.7547
Epoch 9/10
11/11 - 8s - loss: 465.2973 - loglik: -4.6926e+02 - logprior: 3.9670
Fitted a model with MAP estimate = -464.2630
expansions: [(22, 4), (23, 1), (50, 1), (52, 1), (54, 2), (57, 1), (62, 1), (64, 4), (65, 1), (78, 3), (79, 2), (90, 1), (91, 1), (103, 1), (108, 4), (109, 1), (136, 1), (153, 2), (154, 1), (170, 3), (181, 1), (182, 1), (183, 3), (199, 13), (221, 1), (223, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 511.8249 - loglik: -4.6472e+02 - logprior: -4.7105e+01
Epoch 2/2
11/11 - 11s - loss: 458.2428 - loglik: -4.4546e+02 - logprior: -1.2778e+01
Fitted a model with MAP estimate = -448.4027
expansions: [(0, 2), (220, 1), (247, 2)]
discards: [  0  61  97 131 132 133 161 162 293 295 296]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 482.4367 - loglik: -4.4722e+02 - logprior: -3.5219e+01
Epoch 2/2
11/11 - 11s - loss: 441.2700 - loglik: -4.4085e+02 - logprior: -4.1719e-01
Fitted a model with MAP estimate = -434.8643
expansions: [(239, 1), (241, 2), (271, 1), (290, 1), (291, 1)]
discards: [ 0 22]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 484.0936 - loglik: -4.3932e+02 - logprior: -4.4772e+01
Epoch 2/10
11/11 - 12s - loss: 449.6773 - loglik: -4.3979e+02 - logprior: -9.8848e+00
Epoch 3/10
11/11 - 10s - loss: 432.8830 - loglik: -4.3453e+02 - logprior: 1.6515
Epoch 4/10
11/11 - 12s - loss: 424.8159 - loglik: -4.3450e+02 - logprior: 9.6791
Epoch 5/10
11/11 - 11s - loss: 418.5946 - loglik: -4.3073e+02 - logprior: 12.1340
Epoch 6/10
11/11 - 11s - loss: 418.8754 - loglik: -4.3214e+02 - logprior: 13.2659
Fitted a model with MAP estimate = -417.3420
Time for alignment: 215.5075
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 738.3438 - loglik: -6.9771e+02 - logprior: -4.0632e+01
Epoch 2/10
11/11 - 8s - loss: 628.3853 - loglik: -6.2540e+02 - logprior: -2.9860e+00
Epoch 3/10
11/11 - 8s - loss: 545.7493 - loglik: -5.4710e+02 - logprior: 1.3467
Epoch 4/10
11/11 - 8s - loss: 496.9987 - loglik: -4.9919e+02 - logprior: 2.1933
Epoch 5/10
11/11 - 8s - loss: 477.9359 - loglik: -4.8023e+02 - logprior: 2.2918
Epoch 6/10
11/11 - 8s - loss: 472.7847 - loglik: -4.7515e+02 - logprior: 2.3655
Epoch 7/10
11/11 - 9s - loss: 467.0364 - loglik: -4.6963e+02 - logprior: 2.5977
Epoch 8/10
11/11 - 8s - loss: 465.3441 - loglik: -4.6816e+02 - logprior: 2.8207
Epoch 9/10
11/11 - 9s - loss: 464.6459 - loglik: -4.6760e+02 - logprior: 2.9559
Epoch 10/10
11/11 - 8s - loss: 463.4188 - loglik: -4.6658e+02 - logprior: 3.1599
Fitted a model with MAP estimate = -463.4221
expansions: [(20, 1), (21, 4), (25, 1), (35, 1), (41, 1), (45, 1), (47, 1), (49, 2), (61, 1), (63, 1), (76, 1), (77, 2), (78, 2), (79, 1), (101, 3), (104, 1), (106, 1), (108, 4), (129, 1), (137, 1), (162, 1), (164, 5), (165, 1), (181, 1), (182, 1), (183, 3), (199, 5), (201, 1), (203, 7), (223, 1), (224, 1), (225, 2)]
discards: [  0 121 130]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 511.9086 - loglik: -4.6457e+02 - logprior: -4.7334e+01
Epoch 2/2
11/11 - 10s - loss: 456.4431 - loglik: -4.4350e+02 - logprior: -1.2945e+01
Fitted a model with MAP estimate = -445.5396
expansions: [(0, 2), (220, 1), (240, 1), (241, 1), (285, 1)]
discards: [  0  59  91 122 123 132 133 134 204 279 297 298]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 480.9102 - loglik: -4.4601e+02 - logprior: -3.4897e+01
Epoch 2/2
11/11 - 11s - loss: 435.9495 - loglik: -4.3576e+02 - logprior: -1.8755e-01
Fitted a model with MAP estimate = -431.1865
expansions: [(24, 1), (233, 1), (242, 1), (293, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 482.3344 - loglik: -4.3766e+02 - logprior: -4.4672e+01
Epoch 2/10
11/11 - 11s - loss: 444.7434 - loglik: -4.3494e+02 - logprior: -9.8049e+00
Epoch 3/10
11/11 - 12s - loss: 430.3515 - loglik: -4.3208e+02 - logprior: 1.7315
Epoch 4/10
11/11 - 11s - loss: 420.9961 - loglik: -4.3078e+02 - logprior: 9.7839
Epoch 5/10
11/11 - 10s - loss: 417.8989 - loglik: -4.3005e+02 - logprior: 12.1538
Epoch 6/10
11/11 - 10s - loss: 414.2362 - loglik: -4.2747e+02 - logprior: 13.2377
Epoch 7/10
11/11 - 12s - loss: 412.6362 - loglik: -4.2675e+02 - logprior: 14.1165
Epoch 8/10
11/11 - 12s - loss: 416.9452 - loglik: -4.3176e+02 - logprior: 14.8150
Fitted a model with MAP estimate = -412.8515
Time for alignment: 243.5743
Computed alignments with likelihoods: ['-414.2470', '-418.4579', '-422.3762', '-417.3420', '-412.8515']
Best model has likelihood: -412.8515  (prior= 15.2107 )
time for generating output: 0.4910
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.9211257817929117
Training of 5 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac8d4f130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49bc35b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac8fe4eb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 815.0490 - loglik: -8.0910e+02 - logprior: -5.9486e+00
Epoch 2/10
21/21 - 15s - loss: 698.5038 - loglik: -6.9787e+02 - logprior: -6.3812e-01
Epoch 3/10
21/21 - 15s - loss: 648.9079 - loglik: -6.4655e+02 - logprior: -2.3595e+00
Epoch 4/10
21/21 - 15s - loss: 639.5858 - loglik: -6.3735e+02 - logprior: -2.2360e+00
Epoch 5/10
21/21 - 15s - loss: 639.9680 - loglik: -6.3781e+02 - logprior: -2.1605e+00
Fitted a model with MAP estimate = -636.8113
expansions: [(13, 1), (14, 2), (15, 1), (52, 2), (54, 2), (56, 1), (61, 2), (62, 2), (64, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 2), (82, 2), (84, 1), (85, 1), (86, 1), (89, 1), (91, 1), (94, 1), (100, 1), (101, 1), (102, 1), (112, 1), (113, 1), (129, 1), (134, 1), (138, 1), (141, 1), (152, 1), (153, 1), (154, 1), (158, 1), (159, 1), (161, 1), (162, 1), (181, 1), (182, 1), (185, 1), (191, 1), (192, 1), (193, 2), (194, 2), (195, 1), (196, 1), (197, 1), (209, 1), (210, 1), (212, 1), (213, 1), (214, 1), (219, 1), (223, 1), (229, 2), (230, 2), (232, 1), (234, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 630.2662 - loglik: -6.2457e+02 - logprior: -5.6971e+00
Epoch 2/2
21/21 - 22s - loss: 611.2041 - loglik: -6.1182e+02 - logprior: 0.6129
Fitted a model with MAP estimate = -605.8099
expansions: [(0, 3), (18, 1), (74, 1), (147, 1)]
discards: [246 293 332]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 618.5564 - loglik: -6.1117e+02 - logprior: -7.3857e+00
Epoch 2/2
21/21 - 23s - loss: 604.7860 - loglik: -6.0622e+02 - logprior: 1.4346
Fitted a model with MAP estimate = -603.4838
expansions: []
discards: [1 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 612.4264 - loglik: -6.0781e+02 - logprior: -4.6180e+00
Epoch 2/10
21/21 - 22s - loss: 607.2162 - loglik: -6.0912e+02 - logprior: 1.9041
Epoch 3/10
21/21 - 22s - loss: 603.7261 - loglik: -6.0633e+02 - logprior: 2.6075
Epoch 4/10
21/21 - 22s - loss: 601.9847 - loglik: -6.0484e+02 - logprior: 2.8565
Epoch 5/10
21/21 - 22s - loss: 600.3874 - loglik: -6.0349e+02 - logprior: 3.1017
Epoch 6/10
21/21 - 23s - loss: 600.5032 - loglik: -6.0387e+02 - logprior: 3.3678
Fitted a model with MAP estimate = -599.6032
Time for alignment: 372.9507
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 816.3832 - loglik: -8.1041e+02 - logprior: -5.9689e+00
Epoch 2/10
21/21 - 15s - loss: 698.6668 - loglik: -6.9798e+02 - logprior: -6.9087e-01
Epoch 3/10
21/21 - 15s - loss: 644.9616 - loglik: -6.4244e+02 - logprior: -2.5182e+00
Epoch 4/10
21/21 - 15s - loss: 641.3338 - loglik: -6.3878e+02 - logprior: -2.5578e+00
Epoch 5/10
21/21 - 15s - loss: 632.7153 - loglik: -6.3031e+02 - logprior: -2.4015e+00
Epoch 6/10
21/21 - 15s - loss: 638.5080 - loglik: -6.3605e+02 - logprior: -2.4585e+00
Fitted a model with MAP estimate = -634.9455
expansions: [(13, 1), (14, 2), (15, 1), (52, 2), (54, 2), (56, 1), (61, 2), (62, 2), (63, 1), (65, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (85, 1), (86, 1), (87, 1), (90, 1), (93, 1), (94, 1), (95, 1), (101, 1), (107, 1), (113, 1), (115, 1), (117, 1), (136, 1), (140, 1), (143, 1), (156, 1), (157, 1), (158, 1), (160, 1), (161, 1), (163, 1), (164, 1), (173, 1), (183, 1), (186, 2), (189, 1), (191, 1), (192, 1), (193, 2), (194, 1), (195, 1), (196, 1), (210, 1), (211, 1), (213, 1), (214, 1), (216, 1), (220, 1), (224, 1), (230, 2), (231, 2), (233, 1), (235, 1), (236, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [2 3]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 630.8350 - loglik: -6.2524e+02 - logprior: -5.5923e+00
Epoch 2/2
21/21 - 22s - loss: 608.6823 - loglik: -6.0951e+02 - logprior: 0.8318
Fitted a model with MAP estimate = -605.1332
expansions: [(0, 3), (18, 1), (74, 1)]
discards: [ 58 293 332]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 617.9865 - loglik: -6.1057e+02 - logprior: -7.4160e+00
Epoch 2/2
21/21 - 23s - loss: 604.3301 - loglik: -6.0577e+02 - logprior: 1.4395
Fitted a model with MAP estimate = -603.3052
expansions: []
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 612.1989 - loglik: -6.0763e+02 - logprior: -4.5657e+00
Epoch 2/10
21/21 - 22s - loss: 607.1409 - loglik: -6.0926e+02 - logprior: 2.1209
Epoch 3/10
21/21 - 22s - loss: 602.1639 - loglik: -6.0490e+02 - logprior: 2.7315
Epoch 4/10
21/21 - 22s - loss: 601.1736 - loglik: -6.0428e+02 - logprior: 3.1071
Epoch 5/10
21/21 - 22s - loss: 601.9816 - loglik: -6.0527e+02 - logprior: 3.2923
Fitted a model with MAP estimate = -599.6338
Time for alignment: 363.7277
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 817.3231 - loglik: -8.1136e+02 - logprior: -5.9665e+00
Epoch 2/10
21/21 - 15s - loss: 695.9414 - loglik: -6.9520e+02 - logprior: -7.3829e-01
Epoch 3/10
21/21 - 15s - loss: 647.8063 - loglik: -6.4514e+02 - logprior: -2.6639e+00
Epoch 4/10
21/21 - 15s - loss: 636.4872 - loglik: -6.3388e+02 - logprior: -2.6107e+00
Epoch 5/10
21/21 - 15s - loss: 635.8929 - loglik: -6.3341e+02 - logprior: -2.4834e+00
Epoch 6/10
21/21 - 15s - loss: 634.0892 - loglik: -6.3152e+02 - logprior: -2.5691e+00
Epoch 7/10
21/21 - 15s - loss: 636.3538 - loglik: -6.3381e+02 - logprior: -2.5391e+00
Fitted a model with MAP estimate = -634.0330
expansions: [(13, 1), (14, 2), (15, 1), (53, 4), (56, 1), (62, 2), (63, 2), (65, 2), (67, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (87, 1), (88, 1), (91, 1), (94, 1), (96, 1), (102, 1), (103, 1), (112, 1), (114, 1), (115, 1), (116, 1), (117, 1), (137, 1), (139, 1), (140, 1), (153, 1), (154, 1), (155, 1), (156, 1), (158, 1), (159, 1), (161, 1), (162, 1), (168, 1), (170, 1), (180, 1), (183, 1), (186, 1), (189, 1), (190, 1), (191, 2), (192, 2), (193, 1), (194, 1), (195, 1), (207, 1), (208, 1), (210, 1), (211, 1), (217, 1), (222, 1), (226, 1), (227, 2), (228, 2), (230, 1), (234, 1), (254, 1), (255, 1), (256, 1), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 632.2938 - loglik: -6.2659e+02 - logprior: -5.7003e+00
Epoch 2/2
21/21 - 22s - loss: 606.5557 - loglik: -6.0739e+02 - logprior: 0.8295
Fitted a model with MAP estimate = -605.1090
expansions: [(0, 3)]
discards: [247 294 335]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 618.9178 - loglik: -6.1160e+02 - logprior: -7.3174e+00
Epoch 2/2
21/21 - 22s - loss: 604.8322 - loglik: -6.0631e+02 - logprior: 1.4790
Fitted a model with MAP estimate = -604.1826
expansions: [(78, 1)]
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 361 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 614.7461 - loglik: -6.1024e+02 - logprior: -4.5087e+00
Epoch 2/10
21/21 - 22s - loss: 607.5580 - loglik: -6.0958e+02 - logprior: 2.0263
Epoch 3/10
21/21 - 22s - loss: 602.0057 - loglik: -6.0496e+02 - logprior: 2.9526
Epoch 4/10
21/21 - 22s - loss: 602.4338 - loglik: -6.0557e+02 - logprior: 3.1312
Fitted a model with MAP estimate = -601.4835
Time for alignment: 355.3040
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 813.9039 - loglik: -8.0796e+02 - logprior: -5.9447e+00
Epoch 2/10
21/21 - 15s - loss: 694.9077 - loglik: -6.9439e+02 - logprior: -5.1987e-01
Epoch 3/10
21/21 - 15s - loss: 647.2241 - loglik: -6.4535e+02 - logprior: -1.8767e+00
Epoch 4/10
21/21 - 15s - loss: 639.7702 - loglik: -6.3799e+02 - logprior: -1.7755e+00
Epoch 5/10
21/21 - 15s - loss: 640.2188 - loglik: -6.3851e+02 - logprior: -1.7112e+00
Fitted a model with MAP estimate = -637.1764
expansions: [(13, 1), (14, 2), (15, 1), (43, 1), (52, 4), (53, 2), (54, 2), (60, 1), (61, 2), (62, 2), (74, 1), (75, 1), (76, 1), (79, 1), (80, 4), (81, 2), (83, 1), (84, 1), (85, 1), (90, 1), (91, 1), (92, 1), (93, 1), (110, 1), (112, 1), (116, 1), (134, 1), (135, 1), (138, 1), (141, 2), (153, 1), (154, 1), (155, 1), (157, 2), (158, 1), (159, 1), (160, 1), (169, 1), (183, 1), (190, 1), (191, 1), (192, 2), (193, 2), (194, 1), (195, 1), (209, 1), (210, 1), (213, 1), (216, 1), (220, 1), (224, 1), (230, 1), (231, 2), (233, 1), (257, 1), (258, 3), (259, 2), (260, 1), (261, 2), (262, 2), (272, 2), (273, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 634.2923 - loglik: -6.2575e+02 - logprior: -8.5387e+00
Epoch 2/2
21/21 - 22s - loss: 610.9032 - loglik: -6.0883e+02 - logprior: -2.0772e+00
Fitted a model with MAP estimate = -608.1079
expansions: [(0, 3), (150, 1)]
discards: [  0  57 181 249]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 613.4512 - loglik: -6.0848e+02 - logprior: -4.9672e+00
Epoch 2/2
21/21 - 22s - loss: 609.5829 - loglik: -6.1118e+02 - logprior: 1.5991
Fitted a model with MAP estimate = -603.7247
expansions: [(327, 1)]
discards: [  1   3 335]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 616.0178 - loglik: -6.1151e+02 - logprior: -4.5036e+00
Epoch 2/10
21/21 - 22s - loss: 603.8696 - loglik: -6.0579e+02 - logprior: 1.9210
Epoch 3/10
21/21 - 22s - loss: 603.7362 - loglik: -6.0635e+02 - logprior: 2.6162
Epoch 4/10
21/21 - 22s - loss: 603.3589 - loglik: -6.0627e+02 - logprior: 2.9086
Epoch 5/10
21/21 - 22s - loss: 601.6395 - loglik: -6.0474e+02 - logprior: 3.0989
Epoch 6/10
21/21 - 22s - loss: 601.4083 - loglik: -6.0467e+02 - logprior: 3.2616
Epoch 7/10
21/21 - 22s - loss: 599.2462 - loglik: -6.0273e+02 - logprior: 3.4822
Epoch 8/10
21/21 - 22s - loss: 599.6201 - loglik: -6.0334e+02 - logprior: 3.7154
Fitted a model with MAP estimate = -599.5560
Time for alignment: 416.6036
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 817.6944 - loglik: -8.1174e+02 - logprior: -5.9558e+00
Epoch 2/10
21/21 - 15s - loss: 693.2089 - loglik: -6.9258e+02 - logprior: -6.2836e-01
Epoch 3/10
21/21 - 15s - loss: 648.8430 - loglik: -6.4668e+02 - logprior: -2.1652e+00
Epoch 4/10
21/21 - 15s - loss: 638.5563 - loglik: -6.3637e+02 - logprior: -2.1885e+00
Epoch 5/10
21/21 - 15s - loss: 637.5168 - loglik: -6.3536e+02 - logprior: -2.1574e+00
Epoch 6/10
21/21 - 15s - loss: 635.4808 - loglik: -6.3327e+02 - logprior: -2.2081e+00
Epoch 7/10
21/21 - 15s - loss: 637.0671 - loglik: -6.3483e+02 - logprior: -2.2409e+00
Fitted a model with MAP estimate = -635.6152
expansions: [(13, 1), (14, 2), (15, 1), (49, 1), (51, 2), (53, 2), (55, 1), (60, 2), (61, 2), (63, 2), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 2), (81, 1), (83, 1), (85, 1), (86, 1), (89, 1), (92, 1), (93, 1), (94, 1), (111, 1), (113, 1), (114, 1), (115, 1), (116, 1), (134, 1), (135, 1), (138, 1), (141, 1), (154, 1), (155, 1), (156, 1), (158, 1), (159, 1), (160, 2), (161, 1), (170, 1), (180, 1), (183, 2), (186, 1), (188, 1), (189, 1), (190, 1), (192, 2), (193, 1), (194, 1), (195, 1), (208, 1), (212, 1), (214, 1), (218, 1), (219, 1), (222, 1), (230, 2), (232, 1), (236, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 630.6478 - loglik: -6.2508e+02 - logprior: -5.5647e+00
Epoch 2/2
21/21 - 22s - loss: 606.7648 - loglik: -6.0774e+02 - logprior: 0.9747
Fitted a model with MAP estimate = -604.8849
expansions: [(0, 3), (18, 1), (74, 1)]
discards: [ 58 248 332]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 616.7486 - loglik: -6.0940e+02 - logprior: -7.3534e+00
Epoch 2/2
21/21 - 22s - loss: 608.8250 - loglik: -6.1034e+02 - logprior: 1.5125
Fitted a model with MAP estimate = -603.1364
expansions: []
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 613.2013 - loglik: -6.0885e+02 - logprior: -4.3491e+00
Epoch 2/10
21/21 - 22s - loss: 607.0824 - loglik: -6.0923e+02 - logprior: 2.1446
Epoch 3/10
21/21 - 22s - loss: 601.5027 - loglik: -6.0426e+02 - logprior: 2.7573
Epoch 4/10
21/21 - 22s - loss: 600.5493 - loglik: -6.0367e+02 - logprior: 3.1174
Epoch 5/10
21/21 - 22s - loss: 598.6071 - loglik: -6.0191e+02 - logprior: 3.3034
Epoch 6/10
21/21 - 22s - loss: 600.7855 - loglik: -6.0430e+02 - logprior: 3.5169
Fitted a model with MAP estimate = -599.1626
Time for alignment: 400.4105
Computed alignments with likelihoods: ['-599.6032', '-599.6338', '-601.4835', '-599.5560', '-599.1626']
Best model has likelihood: -599.1626  (prior= 3.6776 )
time for generating output: 0.3560
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.9636561695385225
Training of 5 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1e4abe0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac03edee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac03ede20>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 838.6666 - loglik: -8.3698e+02 - logprior: -1.6859e+00
Epoch 2/10
39/39 - 20s - loss: 741.0847 - loglik: -7.3977e+02 - logprior: -1.3122e+00
Epoch 3/10
39/39 - 20s - loss: 727.8891 - loglik: -7.2647e+02 - logprior: -1.4203e+00
Epoch 4/10
39/39 - 20s - loss: 725.6566 - loglik: -7.2424e+02 - logprior: -1.4177e+00
Epoch 5/10
39/39 - 20s - loss: 725.0416 - loglik: -7.2362e+02 - logprior: -1.4229e+00
Epoch 6/10
39/39 - 20s - loss: 723.9310 - loglik: -7.2251e+02 - logprior: -1.4201e+00
Epoch 7/10
39/39 - 20s - loss: 724.8602 - loglik: -7.2344e+02 - logprior: -1.4231e+00
Fitted a model with MAP estimate = -623.9944
expansions: [(20, 1), (41, 1), (45, 1), (82, 4), (83, 2), (93, 1), (96, 2), (101, 3), (112, 3), (120, 1), (121, 5), (122, 1), (148, 1), (151, 2), (152, 1), (154, 1), (157, 7), (168, 2), (169, 4), (170, 1), (171, 2), (172, 4), (173, 1), (185, 1), (187, 4), (188, 3), (191, 2), (205, 1), (207, 1), (208, 1), (212, 3), (218, 1), (244, 3)]
discards: [  0  34 105 106 107 108 109 110]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 718.4080 - loglik: -7.1548e+02 - logprior: -2.9309e+00
Epoch 2/2
39/39 - 28s - loss: 704.3456 - loglik: -7.0259e+02 - logprior: -1.7532e+00
Fitted a model with MAP estimate = -605.8693
expansions: [(0, 2), (102, 2), (236, 1)]
discards: [  0 105 210 211 233 241 242 304 305 306]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 706.5405 - loglik: -7.0453e+02 - logprior: -2.0125e+00
Epoch 2/2
39/39 - 27s - loss: 701.2899 - loglik: -7.0059e+02 - logprior: -7.0123e-01
Fitted a model with MAP estimate = -603.7360
expansions: [(210, 1), (242, 1), (302, 3)]
discards: [  0 268 269]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 608.7754 - loglik: -6.0664e+02 - logprior: -2.1329e+00
Epoch 2/10
43/43 - 30s - loss: 599.6407 - loglik: -5.9876e+02 - logprior: -8.8337e-01
Epoch 3/10
43/43 - 30s - loss: 601.7216 - loglik: -6.0119e+02 - logprior: -5.3523e-01
Fitted a model with MAP estimate = -599.5913
Time for alignment: 497.9241
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 838.4272 - loglik: -8.3675e+02 - logprior: -1.6813e+00
Epoch 2/10
39/39 - 20s - loss: 741.4884 - loglik: -7.4033e+02 - logprior: -1.1624e+00
Epoch 3/10
39/39 - 20s - loss: 729.7116 - loglik: -7.2849e+02 - logprior: -1.2251e+00
Epoch 4/10
39/39 - 20s - loss: 726.2106 - loglik: -7.2492e+02 - logprior: -1.2905e+00
Epoch 5/10
39/39 - 20s - loss: 725.0397 - loglik: -7.2374e+02 - logprior: -1.3032e+00
Epoch 6/10
39/39 - 20s - loss: 724.5168 - loglik: -7.2321e+02 - logprior: -1.3095e+00
Epoch 7/10
39/39 - 20s - loss: 723.8387 - loglik: -7.2253e+02 - logprior: -1.3124e+00
Epoch 8/10
39/39 - 20s - loss: 724.3117 - loglik: -7.2300e+02 - logprior: -1.3108e+00
Fitted a model with MAP estimate = -623.2893
expansions: [(28, 1), (42, 1), (83, 4), (84, 2), (98, 2), (103, 3), (119, 1), (121, 1), (122, 1), (123, 1), (124, 3), (148, 11), (151, 1), (153, 2), (154, 1), (169, 2), (170, 4), (171, 3), (172, 2), (173, 2), (174, 1), (179, 3), (185, 5), (186, 2), (187, 1), (190, 5), (206, 1), (207, 1), (224, 1), (244, 3)]
discards: [  0 107 108 109 110 111 112 161 162 163 164 165 166]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 724.0042 - loglik: -7.2123e+02 - logprior: -2.7787e+00
Epoch 2/2
39/39 - 27s - loss: 709.1971 - loglik: -7.0771e+02 - logprior: -1.4843e+00
Fitted a model with MAP estimate = -609.0819
expansions: [(0, 2), (103, 2), (119, 10), (302, 2)]
discards: [  0 106 165 166 167 168 179 189 204 205 299 300 301]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 708.6122 - loglik: -7.0670e+02 - logprior: -1.9145e+00
Epoch 2/2
39/39 - 28s - loss: 700.1432 - loglik: -6.9947e+02 - logprior: -6.7759e-01
Fitted a model with MAP estimate = -602.5488
expansions: []
discards: [  0 104 121 122 123 124 125 126 127 128 303 304]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 611.2136 - loglik: -6.0917e+02 - logprior: -2.0415e+00
Epoch 2/10
43/43 - 28s - loss: 605.3416 - loglik: -6.0484e+02 - logprior: -5.0554e-01
Epoch 3/10
43/43 - 28s - loss: 601.3536 - loglik: -6.0103e+02 - logprior: -3.1877e-01
Epoch 4/10
43/43 - 29s - loss: 602.9357 - loglik: -6.0268e+02 - logprior: -2.6060e-01
Fitted a model with MAP estimate = -601.4460
Time for alignment: 540.7076
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 839.3063 - loglik: -8.3761e+02 - logprior: -1.6989e+00
Epoch 2/10
39/39 - 20s - loss: 742.1292 - loglik: -7.4082e+02 - logprior: -1.3044e+00
Epoch 3/10
39/39 - 20s - loss: 731.0687 - loglik: -7.2970e+02 - logprior: -1.3717e+00
Epoch 4/10
39/39 - 20s - loss: 728.1074 - loglik: -7.2670e+02 - logprior: -1.4054e+00
Epoch 5/10
39/39 - 20s - loss: 727.1987 - loglik: -7.2577e+02 - logprior: -1.4270e+00
Epoch 6/10
39/39 - 20s - loss: 726.7434 - loglik: -7.2534e+02 - logprior: -1.4069e+00
Epoch 7/10
39/39 - 20s - loss: 725.7482 - loglik: -7.2433e+02 - logprior: -1.4134e+00
Epoch 8/10
39/39 - 20s - loss: 725.5690 - loglik: -7.2417e+02 - logprior: -1.4030e+00
Epoch 9/10
39/39 - 20s - loss: 725.6833 - loglik: -7.2428e+02 - logprior: -1.4047e+00
Fitted a model with MAP estimate = -625.4484
expansions: [(14, 1), (19, 1), (40, 1), (56, 1), (81, 6), (83, 1), (93, 2), (95, 1), (100, 2), (105, 1), (106, 3), (110, 1), (111, 3), (119, 1), (120, 1), (121, 1), (122, 3), (123, 2), (146, 1), (149, 6), (152, 3), (168, 1), (169, 2), (170, 4), (171, 2), (172, 2), (174, 1), (175, 1), (180, 5), (185, 4), (186, 2), (187, 2), (188, 1), (191, 3), (192, 1), (193, 1), (207, 1), (208, 1), (212, 3), (244, 3)]
discards: [  0  33 159 160 161 162]
Re-initialized the encoder parameters.
Fitting a model of length 320 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 719.2692 - loglik: -7.1632e+02 - logprior: -2.9541e+00
Epoch 2/2
39/39 - 30s - loss: 701.9386 - loglik: -7.0014e+02 - logprior: -1.8033e+00
Fitted a model with MAP estimate = -603.8347
expansions: [(0, 2), (122, 2), (320, 2)]
discards: [  0 112 129 130 131 151 199 200 201 202 203 204 233 241 242 256 284 285
 317 318 319]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 707.0749 - loglik: -7.0505e+02 - logprior: -2.0246e+00
Epoch 2/2
39/39 - 28s - loss: 700.0410 - loglik: -6.9923e+02 - logprior: -8.0668e-01
Fitted a model with MAP estimate = -602.5355
expansions: [(200, 1)]
discards: [  0 123 124 125 126 127 128 129 196 303 304]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 608.7007 - loglik: -6.0659e+02 - logprior: -2.1122e+00
Epoch 2/10
43/43 - 29s - loss: 608.4084 - loglik: -6.0780e+02 - logprior: -6.0454e-01
Epoch 3/10
43/43 - 28s - loss: 599.9150 - loglik: -5.9952e+02 - logprior: -3.9136e-01
Epoch 4/10
43/43 - 28s - loss: 600.0002 - loglik: -5.9967e+02 - logprior: -3.3425e-01
Fitted a model with MAP estimate = -600.6239
Time for alignment: 570.2030
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 839.9585 - loglik: -8.3826e+02 - logprior: -1.7022e+00
Epoch 2/10
39/39 - 20s - loss: 740.5631 - loglik: -7.3924e+02 - logprior: -1.3194e+00
Epoch 3/10
39/39 - 20s - loss: 728.2202 - loglik: -7.2685e+02 - logprior: -1.3737e+00
Epoch 4/10
39/39 - 20s - loss: 725.8756 - loglik: -7.2448e+02 - logprior: -1.3928e+00
Epoch 5/10
39/39 - 20s - loss: 724.6346 - loglik: -7.2323e+02 - logprior: -1.4014e+00
Epoch 6/10
39/39 - 20s - loss: 723.8880 - loglik: -7.2248e+02 - logprior: -1.4122e+00
Epoch 7/10
39/39 - 20s - loss: 724.0874 - loglik: -7.2265e+02 - logprior: -1.4361e+00
Fitted a model with MAP estimate = -623.5239
expansions: [(14, 1), (41, 1), (76, 1), (81, 4), (82, 2), (92, 1), (93, 3), (95, 2), (104, 1), (106, 2), (107, 1), (109, 1), (110, 1), (111, 3), (119, 1), (120, 1), (121, 1), (122, 3), (123, 2), (141, 1), (147, 1), (149, 2), (153, 1), (159, 8), (168, 3), (169, 6), (170, 3), (171, 2), (173, 1), (174, 1), (186, 1), (187, 5), (188, 1), (191, 2), (200, 1), (207, 1), (208, 1), (212, 4), (225, 1), (244, 3)]
discards: [  0 142 143 144 145]
Re-initialized the encoder parameters.
Fitting a model of length 320 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 713.8251 - loglik: -7.1094e+02 - logprior: -2.8828e+00
Epoch 2/2
39/39 - 30s - loss: 698.1772 - loglik: -6.9642e+02 - logprior: -1.7547e+00
Fitted a model with MAP estimate = -600.5354
expansions: [(0, 2), (122, 1), (133, 1), (198, 2)]
discards: [  0 108 123 124 125 126 127 152 209 224 245 280 281 282 317 318 319]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 703.0411 - loglik: -7.0105e+02 - logprior: -1.9927e+00
Epoch 2/2
39/39 - 28s - loss: 697.2284 - loglik: -6.9653e+02 - logprior: -7.0212e-01
Fitted a model with MAP estimate = -600.4193
expansions: [(120, 1), (121, 2), (122, 1), (252, 2), (309, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 317 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 35s - loss: 602.5439 - loglik: -6.0037e+02 - logprior: -2.1780e+00
Epoch 2/10
43/43 - 32s - loss: 597.1637 - loglik: -5.9647e+02 - logprior: -6.9123e-01
Epoch 3/10
43/43 - 32s - loss: 594.9830 - loglik: -5.9451e+02 - logprior: -4.7107e-01
Epoch 4/10
43/43 - 32s - loss: 594.3524 - loglik: -5.9394e+02 - logprior: -4.0802e-01
Epoch 5/10
43/43 - 32s - loss: 594.9664 - loglik: -5.9441e+02 - logprior: -5.6002e-01
Fitted a model with MAP estimate = -593.5450
Time for alignment: 579.8724
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 842.0750 - loglik: -8.4039e+02 - logprior: -1.6857e+00
Epoch 2/10
39/39 - 20s - loss: 744.3084 - loglik: -7.4293e+02 - logprior: -1.3741e+00
Epoch 3/10
39/39 - 20s - loss: 730.6722 - loglik: -7.2915e+02 - logprior: -1.5207e+00
Epoch 4/10
39/39 - 20s - loss: 728.0068 - loglik: -7.2648e+02 - logprior: -1.5287e+00
Epoch 5/10
39/39 - 20s - loss: 727.2823 - loglik: -7.2574e+02 - logprior: -1.5378e+00
Epoch 6/10
39/39 - 20s - loss: 726.9505 - loglik: -7.2542e+02 - logprior: -1.5271e+00
Epoch 7/10
39/39 - 20s - loss: 726.3259 - loglik: -7.2479e+02 - logprior: -1.5343e+00
Epoch 8/10
39/39 - 20s - loss: 726.3076 - loglik: -7.2477e+02 - logprior: -1.5401e+00
Epoch 9/10
39/39 - 20s - loss: 726.3527 - loglik: -7.2479e+02 - logprior: -1.5614e+00
Fitted a model with MAP estimate = -625.9444
expansions: [(14, 2), (40, 1), (53, 1), (55, 1), (75, 1), (79, 3), (80, 2), (81, 2), (92, 3), (93, 1), (98, 3), (110, 3), (115, 1), (117, 1), (118, 1), (119, 1), (120, 3), (126, 2), (145, 1), (147, 2), (148, 4), (150, 2), (151, 1), (165, 2), (166, 5), (167, 4), (171, 1), (178, 1), (179, 1), (184, 2), (185, 1), (187, 5), (188, 1), (192, 1), (194, 2), (197, 1), (208, 1), (209, 1), (213, 4), (215, 1), (241, 1), (244, 3)]
discards: [  0  33 102 103 104 105 106 107 157 159 180 181]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 722.9503 - loglik: -7.2005e+02 - logprior: -2.9050e+00
Epoch 2/2
39/39 - 28s - loss: 704.8693 - loglik: -7.0321e+02 - logprior: -1.6546e+00
Fitted a model with MAP estimate = -605.4302
expansions: [(0, 2), (208, 1)]
discards: [  0 120 148 176 182 191 192 193 194 195 229 235 236 273 274 309 310 311]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 708.9692 - loglik: -7.0714e+02 - logprior: -1.8295e+00
Epoch 2/2
39/39 - 26s - loss: 702.8925 - loglik: -7.0235e+02 - logprior: -5.4194e-01
Fitted a model with MAP estimate = -604.9626
expansions: [(297, 3)]
discards: [  0 105 199]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 608.9822 - loglik: -6.0697e+02 - logprior: -2.0149e+00
Epoch 2/10
43/43 - 29s - loss: 603.0914 - loglik: -6.0233e+02 - logprior: -7.5968e-01
Epoch 3/10
43/43 - 28s - loss: 602.8373 - loglik: -6.0235e+02 - logprior: -4.8760e-01
Epoch 4/10
43/43 - 29s - loss: 600.7994 - loglik: -6.0043e+02 - logprior: -3.6566e-01
Epoch 5/10
43/43 - 29s - loss: 600.5410 - loglik: -6.0026e+02 - logprior: -2.8064e-01
Epoch 6/10
43/43 - 29s - loss: 600.1799 - loglik: -5.9998e+02 - logprior: -1.9802e-01
Epoch 7/10
43/43 - 29s - loss: 599.8044 - loglik: -5.9949e+02 - logprior: -3.1150e-01
Epoch 8/10
43/43 - 29s - loss: 599.5591 - loglik: -5.9928e+02 - logprior: -2.7988e-01
Epoch 9/10
43/43 - 29s - loss: 600.8121 - loglik: -6.0076e+02 - logprior: -5.4220e-02
Fitted a model with MAP estimate = -599.2424
Time for alignment: 707.6509
Computed alignments with likelihoods: ['-599.5913', '-601.4460', '-600.6239', '-593.5450', '-599.2424']
Best model has likelihood: -593.5450  (prior= -0.6100 )
time for generating output: 0.4231
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.4959943321161916
Training of 5 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feab79b85e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9d8c7c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feadaaabdf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.9738 - loglik: -2.3683e+02 - logprior: -3.8142e+01
Epoch 2/10
10/10 - 1s - loss: 223.6523 - loglik: -2.1358e+02 - logprior: -1.0072e+01
Epoch 3/10
10/10 - 1s - loss: 196.8493 - loglik: -1.9176e+02 - logprior: -5.0897e+00
Epoch 4/10
10/10 - 1s - loss: 181.8648 - loglik: -1.7855e+02 - logprior: -3.3198e+00
Epoch 5/10
10/10 - 1s - loss: 175.2331 - loglik: -1.7276e+02 - logprior: -2.4736e+00
Epoch 6/10
10/10 - 1s - loss: 171.9366 - loglik: -1.6980e+02 - logprior: -2.1380e+00
Epoch 7/10
10/10 - 1s - loss: 169.9299 - loglik: -1.6801e+02 - logprior: -1.9236e+00
Epoch 8/10
10/10 - 1s - loss: 167.3583 - loglik: -1.6557e+02 - logprior: -1.7909e+00
Epoch 9/10
10/10 - 1s - loss: 166.1824 - loglik: -1.6446e+02 - logprior: -1.7232e+00
Epoch 10/10
10/10 - 1s - loss: 165.6726 - loglik: -1.6403e+02 - logprior: -1.6442e+00
Fitted a model with MAP estimate = -165.4131
expansions: [(0, 2), (12, 1), (14, 2), (15, 1), (20, 1), (21, 1), (23, 2), (30, 1), (32, 1), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 212.7000 - loglik: -1.6278e+02 - logprior: -4.9916e+01
Epoch 2/2
10/10 - 1s - loss: 168.8957 - loglik: -1.5382e+02 - logprior: -1.5080e+01
Fitted a model with MAP estimate = -160.4611
expansions: []
discards: [ 0 17 29 69]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 198.7251 - loglik: -1.5535e+02 - logprior: -4.3373e+01
Epoch 2/2
10/10 - 1s - loss: 171.2407 - loglik: -1.5427e+02 - logprior: -1.6972e+01
Fitted a model with MAP estimate = -166.0416
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 193.2841 - loglik: -1.5359e+02 - logprior: -3.9691e+01
Epoch 2/10
10/10 - 1s - loss: 163.4503 - loglik: -1.5257e+02 - logprior: -1.0880e+01
Epoch 3/10
10/10 - 1s - loss: 155.8960 - loglik: -1.5171e+02 - logprior: -4.1836e+00
Epoch 4/10
10/10 - 1s - loss: 152.0105 - loglik: -1.4989e+02 - logprior: -2.1212e+00
Epoch 5/10
10/10 - 1s - loss: 150.4215 - loglik: -1.4933e+02 - logprior: -1.0917e+00
Epoch 6/10
10/10 - 1s - loss: 149.5594 - loglik: -1.4928e+02 - logprior: -2.7920e-01
Epoch 7/10
10/10 - 1s - loss: 149.1385 - loglik: -1.4932e+02 - logprior: 0.1792
Epoch 8/10
10/10 - 1s - loss: 148.4194 - loglik: -1.4883e+02 - logprior: 0.4063
Epoch 9/10
10/10 - 1s - loss: 148.9096 - loglik: -1.4951e+02 - logprior: 0.6008
Fitted a model with MAP estimate = -148.3511
Time for alignment: 41.8470
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.8102 - loglik: -2.3667e+02 - logprior: -3.8143e+01
Epoch 2/10
10/10 - 1s - loss: 223.6469 - loglik: -2.1357e+02 - logprior: -1.0076e+01
Epoch 3/10
10/10 - 1s - loss: 197.1657 - loglik: -1.9207e+02 - logprior: -5.0960e+00
Epoch 4/10
10/10 - 1s - loss: 181.5600 - loglik: -1.7811e+02 - logprior: -3.4517e+00
Epoch 5/10
10/10 - 1s - loss: 174.8674 - loglik: -1.7225e+02 - logprior: -2.6193e+00
Epoch 6/10
10/10 - 1s - loss: 170.7689 - loglik: -1.6850e+02 - logprior: -2.2685e+00
Epoch 7/10
10/10 - 1s - loss: 168.1826 - loglik: -1.6629e+02 - logprior: -1.8902e+00
Epoch 8/10
10/10 - 1s - loss: 167.1308 - loglik: -1.6554e+02 - logprior: -1.5931e+00
Epoch 9/10
10/10 - 1s - loss: 165.9528 - loglik: -1.6442e+02 - logprior: -1.5297e+00
Epoch 10/10
10/10 - 1s - loss: 165.4089 - loglik: -1.6386e+02 - logprior: -1.5488e+00
Fitted a model with MAP estimate = -165.0177
expansions: [(0, 2), (13, 1), (15, 2), (20, 2), (21, 2), (23, 2), (30, 1), (35, 2), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 213.5115 - loglik: -1.6356e+02 - logprior: -4.9949e+01
Epoch 2/2
10/10 - 1s - loss: 169.2098 - loglik: -1.5399e+02 - logprior: -1.5224e+01
Fitted a model with MAP estimate = -160.7705
expansions: []
discards: [ 0 18 32 47 71]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 198.2914 - loglik: -1.5493e+02 - logprior: -4.3361e+01
Epoch 2/2
10/10 - 1s - loss: 171.1472 - loglik: -1.5423e+02 - logprior: -1.6915e+01
Fitted a model with MAP estimate = -165.8145
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 193.1143 - loglik: -1.5343e+02 - logprior: -3.9684e+01
Epoch 2/10
10/10 - 1s - loss: 163.1773 - loglik: -1.5228e+02 - logprior: -1.0896e+01
Epoch 3/10
10/10 - 1s - loss: 155.2350 - loglik: -1.5107e+02 - logprior: -4.1648e+00
Epoch 4/10
10/10 - 1s - loss: 152.0850 - loglik: -1.4997e+02 - logprior: -2.1159e+00
Epoch 5/10
10/10 - 1s - loss: 150.4158 - loglik: -1.4933e+02 - logprior: -1.0837e+00
Epoch 6/10
10/10 - 1s - loss: 149.3323 - loglik: -1.4907e+02 - logprior: -2.6476e-01
Epoch 7/10
10/10 - 1s - loss: 148.7572 - loglik: -1.4895e+02 - logprior: 0.1897
Epoch 8/10
10/10 - 1s - loss: 148.3316 - loglik: -1.4875e+02 - logprior: 0.4200
Epoch 9/10
10/10 - 1s - loss: 148.2456 - loglik: -1.4886e+02 - logprior: 0.6169
Epoch 10/10
10/10 - 1s - loss: 147.9422 - loglik: -1.4874e+02 - logprior: 0.7961
Fitted a model with MAP estimate = -147.8330
Time for alignment: 43.9019
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.9501 - loglik: -2.3681e+02 - logprior: -3.8144e+01
Epoch 2/10
10/10 - 1s - loss: 223.3296 - loglik: -2.1325e+02 - logprior: -1.0078e+01
Epoch 3/10
10/10 - 1s - loss: 197.1374 - loglik: -1.9203e+02 - logprior: -5.1053e+00
Epoch 4/10
10/10 - 1s - loss: 182.9050 - loglik: -1.7947e+02 - logprior: -3.4322e+00
Epoch 5/10
10/10 - 1s - loss: 176.5134 - loglik: -1.7400e+02 - logprior: -2.5172e+00
Epoch 6/10
10/10 - 1s - loss: 172.0518 - loglik: -1.6986e+02 - logprior: -2.1869e+00
Epoch 7/10
10/10 - 1s - loss: 169.6662 - loglik: -1.6764e+02 - logprior: -2.0256e+00
Epoch 8/10
10/10 - 1s - loss: 168.2104 - loglik: -1.6652e+02 - logprior: -1.6916e+00
Epoch 9/10
10/10 - 1s - loss: 167.6646 - loglik: -1.6619e+02 - logprior: -1.4757e+00
Epoch 10/10
10/10 - 1s - loss: 166.9784 - loglik: -1.6553e+02 - logprior: -1.4500e+00
Fitted a model with MAP estimate = -166.8748
expansions: [(0, 2), (12, 1), (15, 2), (20, 1), (21, 1), (26, 1), (30, 1), (35, 2), (46, 1), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 217.2189 - loglik: -1.6703e+02 - logprior: -5.0187e+01
Epoch 2/2
10/10 - 1s - loss: 172.2130 - loglik: -1.5704e+02 - logprior: -1.5176e+01
Fitted a model with MAP estimate = -163.2824
expansions: [(31, 1), (71, 1)]
discards: [ 0 18 44]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 200.7669 - loglik: -1.5704e+02 - logprior: -4.3731e+01
Epoch 2/2
10/10 - 1s - loss: 172.3610 - loglik: -1.5534e+02 - logprior: -1.7020e+01
Fitted a model with MAP estimate = -166.9389
expansions: [(29, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 193.7729 - loglik: -1.5419e+02 - logprior: -3.9581e+01
Epoch 2/10
10/10 - 1s - loss: 164.1278 - loglik: -1.5334e+02 - logprior: -1.0793e+01
Epoch 3/10
10/10 - 1s - loss: 156.1543 - loglik: -1.5202e+02 - logprior: -4.1374e+00
Epoch 4/10
10/10 - 1s - loss: 152.5063 - loglik: -1.5040e+02 - logprior: -2.1031e+00
Epoch 5/10
10/10 - 1s - loss: 151.0379 - loglik: -1.4996e+02 - logprior: -1.0742e+00
Epoch 6/10
10/10 - 1s - loss: 149.8652 - loglik: -1.4961e+02 - logprior: -2.5559e-01
Epoch 7/10
10/10 - 1s - loss: 149.5002 - loglik: -1.4970e+02 - logprior: 0.1988
Epoch 8/10
10/10 - 1s - loss: 149.1292 - loglik: -1.4955e+02 - logprior: 0.4239
Epoch 9/10
10/10 - 1s - loss: 148.8560 - loglik: -1.4948e+02 - logprior: 0.6199
Epoch 10/10
10/10 - 1s - loss: 148.8078 - loglik: -1.4960e+02 - logprior: 0.7929
Fitted a model with MAP estimate = -148.5155
Time for alignment: 43.8291
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.8981 - loglik: -2.3676e+02 - logprior: -3.8141e+01
Epoch 2/10
10/10 - 1s - loss: 223.5290 - loglik: -2.1346e+02 - logprior: -1.0073e+01
Epoch 3/10
10/10 - 1s - loss: 196.9452 - loglik: -1.9186e+02 - logprior: -5.0837e+00
Epoch 4/10
10/10 - 1s - loss: 182.5598 - loglik: -1.7919e+02 - logprior: -3.3659e+00
Epoch 5/10
10/10 - 1s - loss: 175.4089 - loglik: -1.7296e+02 - logprior: -2.4444e+00
Epoch 6/10
10/10 - 1s - loss: 171.9104 - loglik: -1.6988e+02 - logprior: -2.0308e+00
Epoch 7/10
10/10 - 1s - loss: 169.8427 - loglik: -1.6801e+02 - logprior: -1.8347e+00
Epoch 8/10
10/10 - 1s - loss: 167.2538 - loglik: -1.6552e+02 - logprior: -1.7352e+00
Epoch 9/10
10/10 - 1s - loss: 165.9499 - loglik: -1.6424e+02 - logprior: -1.7138e+00
Epoch 10/10
10/10 - 1s - loss: 165.3920 - loglik: -1.6378e+02 - logprior: -1.6168e+00
Fitted a model with MAP estimate = -165.0206
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (23, 3), (30, 1), (35, 2), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 213.5770 - loglik: -1.6380e+02 - logprior: -4.9781e+01
Epoch 2/2
10/10 - 1s - loss: 168.8563 - loglik: -1.5366e+02 - logprior: -1.5198e+01
Fitted a model with MAP estimate = -160.6890
expansions: []
discards: [ 0 18 26 47 71]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 198.0761 - loglik: -1.5473e+02 - logprior: -4.3346e+01
Epoch 2/2
10/10 - 1s - loss: 170.4242 - loglik: -1.5351e+02 - logprior: -1.6918e+01
Fitted a model with MAP estimate = -165.2339
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.6539 - loglik: -1.5301e+02 - logprior: -3.9642e+01
Epoch 2/10
10/10 - 1s - loss: 162.5180 - loglik: -1.5166e+02 - logprior: -1.0853e+01
Epoch 3/10
10/10 - 1s - loss: 155.0138 - loglik: -1.5090e+02 - logprior: -4.1162e+00
Epoch 4/10
10/10 - 1s - loss: 151.5929 - loglik: -1.4953e+02 - logprior: -2.0670e+00
Epoch 5/10
10/10 - 1s - loss: 149.7081 - loglik: -1.4867e+02 - logprior: -1.0377e+00
Epoch 6/10
10/10 - 1s - loss: 149.2046 - loglik: -1.4898e+02 - logprior: -2.2693e-01
Epoch 7/10
10/10 - 1s - loss: 147.9115 - loglik: -1.4815e+02 - logprior: 0.2375
Epoch 8/10
10/10 - 1s - loss: 148.0448 - loglik: -1.4851e+02 - logprior: 0.4641
Fitted a model with MAP estimate = -147.7797
Time for alignment: 41.7570
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.7419 - loglik: -2.3660e+02 - logprior: -3.8143e+01
Epoch 2/10
10/10 - 1s - loss: 223.8182 - loglik: -2.1374e+02 - logprior: -1.0077e+01
Epoch 3/10
10/10 - 1s - loss: 196.6700 - loglik: -1.9155e+02 - logprior: -5.1199e+00
Epoch 4/10
10/10 - 1s - loss: 179.8554 - loglik: -1.7640e+02 - logprior: -3.4541e+00
Epoch 5/10
10/10 - 1s - loss: 173.0617 - loglik: -1.7035e+02 - logprior: -2.7153e+00
Epoch 6/10
10/10 - 1s - loss: 169.5232 - loglik: -1.6713e+02 - logprior: -2.3897e+00
Epoch 7/10
10/10 - 1s - loss: 167.5869 - loglik: -1.6565e+02 - logprior: -1.9407e+00
Epoch 8/10
10/10 - 1s - loss: 166.7294 - loglik: -1.6508e+02 - logprior: -1.6509e+00
Epoch 9/10
10/10 - 1s - loss: 165.6443 - loglik: -1.6402e+02 - logprior: -1.6286e+00
Epoch 10/10
10/10 - 1s - loss: 165.1796 - loglik: -1.6356e+02 - logprior: -1.6182e+00
Fitted a model with MAP estimate = -165.0354
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (30, 1), (38, 1), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 213.3769 - loglik: -1.6331e+02 - logprior: -5.0066e+01
Epoch 2/2
10/10 - 1s - loss: 170.4885 - loglik: -1.5540e+02 - logprior: -1.5088e+01
Fitted a model with MAP estimate = -162.2619
expansions: []
discards: [ 0 18 67]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 200.1902 - loglik: -1.5666e+02 - logprior: -4.3526e+01
Epoch 2/2
10/10 - 1s - loss: 172.3555 - loglik: -1.5538e+02 - logprior: -1.6974e+01
Fitted a model with MAP estimate = -167.4953
expansions: [(29, 3)]
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 194.2921 - loglik: -1.5471e+02 - logprior: -3.9584e+01
Epoch 2/10
10/10 - 1s - loss: 163.4295 - loglik: -1.5261e+02 - logprior: -1.0816e+01
Epoch 3/10
10/10 - 1s - loss: 155.1244 - loglik: -1.5100e+02 - logprior: -4.1242e+00
Epoch 4/10
10/10 - 1s - loss: 151.5148 - loglik: -1.4943e+02 - logprior: -2.0856e+00
Epoch 5/10
10/10 - 1s - loss: 149.5401 - loglik: -1.4849e+02 - logprior: -1.0523e+00
Epoch 6/10
10/10 - 1s - loss: 148.8111 - loglik: -1.4857e+02 - logprior: -2.4264e-01
Epoch 7/10
10/10 - 1s - loss: 148.3226 - loglik: -1.4854e+02 - logprior: 0.2198
Epoch 8/10
10/10 - 1s - loss: 148.0142 - loglik: -1.4845e+02 - logprior: 0.4396
Epoch 9/10
10/10 - 1s - loss: 147.7049 - loglik: -1.4834e+02 - logprior: 0.6366
Epoch 10/10
10/10 - 1s - loss: 147.6495 - loglik: -1.4846e+02 - logprior: 0.8109
Fitted a model with MAP estimate = -147.4302
Time for alignment: 42.4696
Computed alignments with likelihoods: ['-148.3511', '-147.8330', '-148.5155', '-147.7797', '-147.4302']
Best model has likelihood: -147.4302  (prior= 0.8895 )
time for generating output: 0.1668
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.9180559480079118
Training of 5 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaeb9a7fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1ddc220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e1af3d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 302.4799 - loglik: -2.9150e+02 - logprior: -1.0980e+01
Epoch 2/10
19/19 - 6s - loss: 244.4322 - loglik: -2.4220e+02 - logprior: -2.2344e+00
Epoch 3/10
19/19 - 5s - loss: 224.0242 - loglik: -2.2210e+02 - logprior: -1.9210e+00
Epoch 4/10
19/19 - 5s - loss: 218.0529 - loglik: -2.1637e+02 - logprior: -1.6809e+00
Epoch 5/10
19/19 - 5s - loss: 217.6503 - loglik: -2.1610e+02 - logprior: -1.5481e+00
Epoch 6/10
19/19 - 5s - loss: 216.1928 - loglik: -2.1467e+02 - logprior: -1.5244e+00
Epoch 7/10
19/19 - 5s - loss: 215.3648 - loglik: -2.1387e+02 - logprior: -1.4923e+00
Epoch 8/10
19/19 - 5s - loss: 216.2000 - loglik: -2.1473e+02 - logprior: -1.4701e+00
Fitted a model with MAP estimate = -215.6205
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (27, 1), (29, 1), (43, 2), (47, 2), (48, 2), (65, 1), (71, 1), (72, 3), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 224.1566 - loglik: -2.1047e+02 - logprior: -1.3690e+01
Epoch 2/2
19/19 - 5s - loss: 207.3348 - loglik: -2.0292e+02 - logprior: -4.4160e+00
Fitted a model with MAP estimate = -202.8678
expansions: [(0, 2), (64, 4)]
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 209.9647 - loglik: -2.0048e+02 - logprior: -9.4867e+00
Epoch 2/2
19/19 - 5s - loss: 199.9123 - loglik: -1.9865e+02 - logprior: -1.2587e+00
Fitted a model with MAP estimate = -198.6586
expansions: []
discards: [ 0 12 65]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 213.4322 - loglik: -2.0196e+02 - logprior: -1.1473e+01
Epoch 2/10
19/19 - 5s - loss: 201.1870 - loglik: -1.9955e+02 - logprior: -1.6341e+00
Epoch 3/10
19/19 - 5s - loss: 199.0688 - loglik: -1.9878e+02 - logprior: -2.8415e-01
Epoch 4/10
19/19 - 5s - loss: 197.8543 - loglik: -1.9791e+02 - logprior: 0.0548
Epoch 5/10
19/19 - 6s - loss: 198.8952 - loglik: -1.9912e+02 - logprior: 0.2268
Fitted a model with MAP estimate = -197.8714
Time for alignment: 112.4909
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 303.1835 - loglik: -2.9221e+02 - logprior: -1.0975e+01
Epoch 2/10
19/19 - 6s - loss: 244.9887 - loglik: -2.4269e+02 - logprior: -2.2962e+00
Epoch 3/10
19/19 - 6s - loss: 224.4872 - loglik: -2.2231e+02 - logprior: -2.1741e+00
Epoch 4/10
19/19 - 5s - loss: 218.1734 - loglik: -2.1615e+02 - logprior: -2.0226e+00
Epoch 5/10
19/19 - 5s - loss: 216.4788 - loglik: -2.1465e+02 - logprior: -1.8315e+00
Epoch 6/10
19/19 - 5s - loss: 215.5122 - loglik: -2.1367e+02 - logprior: -1.8409e+00
Epoch 7/10
19/19 - 5s - loss: 215.3234 - loglik: -2.1351e+02 - logprior: -1.8137e+00
Epoch 8/10
19/19 - 5s - loss: 215.6707 - loglik: -2.1387e+02 - logprior: -1.7986e+00
Fitted a model with MAP estimate = -215.0453
expansions: [(7, 1), (9, 3), (10, 2), (15, 1), (26, 1), (29, 1), (33, 1), (42, 2), (46, 1), (47, 2), (49, 1), (57, 1), (68, 1), (71, 1), (72, 1), (73, 1), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 223.0393 - loglik: -2.0941e+02 - logprior: -1.3626e+01
Epoch 2/2
19/19 - 6s - loss: 205.8240 - loglik: -2.0142e+02 - logprior: -4.4064e+00
Fitted a model with MAP estimate = -201.5969
expansions: [(0, 2)]
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 210.0049 - loglik: -2.0057e+02 - logprior: -9.4346e+00
Epoch 2/2
19/19 - 5s - loss: 199.7164 - loglik: -1.9846e+02 - logprior: -1.2609e+00
Fitted a model with MAP estimate = -198.8030
expansions: []
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 212.7463 - loglik: -2.0149e+02 - logprior: -1.1261e+01
Epoch 2/10
19/19 - 5s - loss: 200.8772 - loglik: -1.9927e+02 - logprior: -1.6027e+00
Epoch 3/10
19/19 - 5s - loss: 200.3335 - loglik: -1.9999e+02 - logprior: -3.4415e-01
Epoch 4/10
19/19 - 4s - loss: 197.9191 - loglik: -1.9795e+02 - logprior: 0.0355
Epoch 5/10
19/19 - 5s - loss: 198.2700 - loglik: -1.9847e+02 - logprior: 0.1991
Fitted a model with MAP estimate = -197.9835
Time for alignment: 112.8225
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 302.9521 - loglik: -2.9197e+02 - logprior: -1.0977e+01
Epoch 2/10
19/19 - 5s - loss: 241.4477 - loglik: -2.3920e+02 - logprior: -2.2467e+00
Epoch 3/10
19/19 - 5s - loss: 221.2276 - loglik: -2.1922e+02 - logprior: -2.0092e+00
Epoch 4/10
19/19 - 5s - loss: 216.7075 - loglik: -2.1491e+02 - logprior: -1.7945e+00
Epoch 5/10
19/19 - 4s - loss: 214.9196 - loglik: -2.1331e+02 - logprior: -1.6098e+00
Epoch 6/10
19/19 - 5s - loss: 215.1488 - loglik: -2.1354e+02 - logprior: -1.6050e+00
Fitted a model with MAP estimate = -215.0161
expansions: [(7, 1), (9, 1), (10, 2), (12, 1), (15, 1), (27, 1), (29, 1), (43, 1), (47, 3), (48, 4), (49, 1), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 223.2632 - loglik: -2.0965e+02 - logprior: -1.3615e+01
Epoch 2/2
19/19 - 4s - loss: 204.7051 - loglik: -2.0046e+02 - logprior: -4.2406e+00
Fitted a model with MAP estimate = -201.1987
expansions: [(0, 2)]
discards: [ 0 10 60]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 210.0168 - loglik: -2.0059e+02 - logprior: -9.4311e+00
Epoch 2/2
19/19 - 5s - loss: 199.9351 - loglik: -1.9866e+02 - logprior: -1.2733e+00
Fitted a model with MAP estimate = -199.3728
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 213.2804 - loglik: -2.0191e+02 - logprior: -1.1367e+01
Epoch 2/10
19/19 - 6s - loss: 202.4138 - loglik: -2.0078e+02 - logprior: -1.6332e+00
Epoch 3/10
19/19 - 4s - loss: 198.0963 - loglik: -1.9775e+02 - logprior: -3.4135e-01
Epoch 4/10
19/19 - 6s - loss: 200.0970 - loglik: -2.0012e+02 - logprior: 0.0256
Fitted a model with MAP estimate = -198.7434
Time for alignment: 92.7571
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 303.1884 - loglik: -2.9222e+02 - logprior: -1.0973e+01
Epoch 2/10
19/19 - 4s - loss: 240.1951 - loglik: -2.3798e+02 - logprior: -2.2137e+00
Epoch 3/10
19/19 - 4s - loss: 220.2250 - loglik: -2.1836e+02 - logprior: -1.8670e+00
Epoch 4/10
19/19 - 5s - loss: 216.2476 - loglik: -2.1458e+02 - logprior: -1.6720e+00
Epoch 5/10
19/19 - 5s - loss: 216.0610 - loglik: -2.1457e+02 - logprior: -1.4903e+00
Epoch 6/10
19/19 - 4s - loss: 214.2051 - loglik: -2.1273e+02 - logprior: -1.4723e+00
Epoch 7/10
19/19 - 5s - loss: 215.6319 - loglik: -2.1418e+02 - logprior: -1.4516e+00
Fitted a model with MAP estimate = -214.6731
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (27, 1), (29, 1), (30, 1), (47, 3), (48, 4), (49, 2), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 223.5058 - loglik: -2.0986e+02 - logprior: -1.3642e+01
Epoch 2/2
19/19 - 5s - loss: 205.2729 - loglik: -2.0085e+02 - logprior: -4.4206e+00
Fitted a model with MAP estimate = -201.8058
expansions: [(0, 2)]
discards: [ 0 13 61 65]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 208.9635 - loglik: -1.9952e+02 - logprior: -9.4389e+00
Epoch 2/2
19/19 - 5s - loss: 200.7872 - loglik: -1.9952e+02 - logprior: -1.2631e+00
Fitted a model with MAP estimate = -198.9189
expansions: []
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 212.6903 - loglik: -2.0131e+02 - logprior: -1.1380e+01
Epoch 2/10
19/19 - 6s - loss: 201.8468 - loglik: -2.0023e+02 - logprior: -1.6211e+00
Epoch 3/10
19/19 - 4s - loss: 198.6814 - loglik: -1.9837e+02 - logprior: -3.1180e-01
Epoch 4/10
19/19 - 4s - loss: 198.7137 - loglik: -1.9878e+02 - logprior: 0.0659
Fitted a model with MAP estimate = -198.5110
Time for alignment: 97.2743
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 302.9460 - loglik: -2.9197e+02 - logprior: -1.0973e+01
Epoch 2/10
19/19 - 6s - loss: 242.1138 - loglik: -2.3988e+02 - logprior: -2.2340e+00
Epoch 3/10
19/19 - 5s - loss: 222.3670 - loglik: -2.2037e+02 - logprior: -2.0003e+00
Epoch 4/10
19/19 - 4s - loss: 215.6718 - loglik: -2.1385e+02 - logprior: -1.8186e+00
Epoch 5/10
19/19 - 5s - loss: 213.5848 - loglik: -2.1192e+02 - logprior: -1.6662e+00
Epoch 6/10
19/19 - 5s - loss: 215.0125 - loglik: -2.1334e+02 - logprior: -1.6699e+00
Fitted a model with MAP estimate = -213.8029
expansions: [(10, 3), (11, 2), (12, 1), (27, 1), (30, 1), (31, 1), (34, 1), (48, 2), (49, 2), (57, 1), (60, 1), (71, 1), (72, 1), (73, 1), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 223.6428 - loglik: -2.0998e+02 - logprior: -1.3664e+01
Epoch 2/2
19/19 - 5s - loss: 206.9904 - loglik: -2.0270e+02 - logprior: -4.2862e+00
Fitted a model with MAP estimate = -202.6523
expansions: [(0, 2), (64, 4)]
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 210.1968 - loglik: -2.0067e+02 - logprior: -9.5250e+00
Epoch 2/2
19/19 - 5s - loss: 199.1962 - loglik: -1.9787e+02 - logprior: -1.3261e+00
Fitted a model with MAP estimate = -198.7551
expansions: []
discards: [ 0 12 66]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 212.5712 - loglik: -2.0105e+02 - logprior: -1.1518e+01
Epoch 2/10
19/19 - 5s - loss: 201.3702 - loglik: -1.9968e+02 - logprior: -1.6915e+00
Epoch 3/10
19/19 - 6s - loss: 199.7568 - loglik: -1.9938e+02 - logprior: -3.7650e-01
Epoch 4/10
19/19 - 5s - loss: 198.0332 - loglik: -1.9802e+02 - logprior: -8.3163e-03
Epoch 5/10
19/19 - 4s - loss: 198.8590 - loglik: -1.9900e+02 - logprior: 0.1448
Fitted a model with MAP estimate = -197.9767
Time for alignment: 101.5438
Computed alignments with likelihoods: ['-197.8714', '-197.9835', '-198.7434', '-198.5110', '-197.9767']
Best model has likelihood: -197.8714  (prior= 0.2991 )
time for generating output: 0.5244
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.21376267367630492
Training of 5 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb05f754c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebf5e1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d9bf6d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 1102.0728 - loglik: -1.1007e+03 - logprior: -1.3791e+00
Epoch 2/10
49/49 - 41s - loss: 962.8964 - loglik: -9.6325e+02 - logprior: 0.3540
Epoch 3/10
49/49 - 41s - loss: 957.4174 - loglik: -9.5772e+02 - logprior: 0.3021
Epoch 4/10
49/49 - 41s - loss: 953.1282 - loglik: -9.5311e+02 - logprior: -1.7424e-02
Epoch 5/10
49/49 - 41s - loss: 952.9810 - loglik: -9.5298e+02 - logprior: -3.0336e-03
Epoch 6/10
49/49 - 41s - loss: 955.3203 - loglik: -9.5547e+02 - logprior: 0.1524
Fitted a model with MAP estimate = -952.5900
expansions: [(0, 5), (36, 3), (141, 1), (193, 1), (211, 1), (213, 3), (227, 1), (229, 2), (230, 2), (231, 3), (232, 3), (234, 2), (238, 2), (240, 1), (241, 1), (246, 1), (248, 1), (249, 1), (251, 1), (252, 6), (253, 2), (254, 1), (268, 1), (269, 1), (270, 1), (271, 2), (272, 4), (290, 1), (291, 1), (292, 2), (293, 3), (294, 1), (296, 4), (298, 7), (308, 2), (309, 1), (310, 4), (323, 1), (325, 1), (326, 1), (327, 1), (328, 1), (329, 1), (330, 2), (344, 2), (346, 1), (347, 1), (363, 3), (364, 4), (365, 1), (366, 1), (367, 1), (395, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 82]
Re-initialized the encoder parameters.
Fitting a model of length 494 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 61s - loss: 943.0208 - loglik: -9.4063e+02 - logprior: -2.3900e+00
Epoch 2/2
49/49 - 57s - loss: 925.3588 - loglik: -9.2729e+02 - logprior: 1.9277
Fitted a model with MAP estimate = -923.3290
expansions: [(0, 7), (337, 1), (341, 1), (414, 1), (481, 1), (486, 5)]
discards: [  1   2   3   4   5   6  23  24 234 326 365 366 397 436 489 490 491 492
 493]
Re-initialized the encoder parameters.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 935.0843 - loglik: -9.3347e+02 - logprior: -1.6114e+00
Epoch 2/2
49/49 - 56s - loss: 924.3278 - loglik: -9.2743e+02 - logprior: 3.0978
Fitted a model with MAP estimate = -923.1319
expansions: [(0, 7), (488, 1), (489, 1), (491, 7)]
discards: [  0   1   2   3   4   5   6   7  24 322]
Re-initialized the encoder parameters.
Fitting a model of length 497 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 61s - loss: 930.7375 - loglik: -9.3059e+02 - logprior: -1.4870e-01
Epoch 2/10
49/49 - 57s - loss: 923.4852 - loglik: -9.2695e+02 - logprior: 3.4646
Epoch 3/10
49/49 - 57s - loss: 919.5711 - loglik: -9.2367e+02 - logprior: 4.0957
Epoch 4/10
49/49 - 57s - loss: 918.9380 - loglik: -9.2349e+02 - logprior: 4.5547
Epoch 5/10
49/49 - 57s - loss: 918.5726 - loglik: -9.2359e+02 - logprior: 5.0152
Epoch 6/10
49/49 - 57s - loss: 919.1699 - loglik: -9.2459e+02 - logprior: 5.4205
Fitted a model with MAP estimate = -916.3430
Time for alignment: 991.8474
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 45s - loss: 1103.2051 - loglik: -1.1018e+03 - logprior: -1.3792e+00
Epoch 2/10
49/49 - 41s - loss: 958.8467 - loglik: -9.5891e+02 - logprior: 0.0671
Epoch 3/10
49/49 - 41s - loss: 953.1646 - loglik: -9.5324e+02 - logprior: 0.0719
Epoch 4/10
49/49 - 41s - loss: 953.4435 - loglik: -9.5366e+02 - logprior: 0.2165
Fitted a model with MAP estimate = -951.5681
expansions: [(0, 5), (131, 1), (141, 1), (183, 1), (211, 1), (213, 1), (214, 1), (227, 1), (230, 3), (231, 2), (232, 3), (233, 1), (236, 3), (240, 2), (242, 1), (249, 1), (251, 1), (254, 2), (255, 5), (256, 3), (257, 2), (276, 1), (277, 1), (278, 1), (279, 2), (296, 1), (297, 2), (298, 2), (299, 4), (302, 2), (303, 1), (305, 6), (315, 1), (316, 1), (318, 4), (319, 1), (329, 2), (330, 2), (331, 2), (332, 3), (333, 2), (347, 1), (351, 1), (369, 2), (370, 3), (372, 1), (374, 1), (395, 2), (396, 3), (398, 1), (399, 11), (404, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 34 35 93]
Re-initialized the encoder parameters.
Fitting a model of length 499 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 61s - loss: 947.7111 - loglik: -9.4508e+02 - logprior: -2.6353e+00
Epoch 2/2
49/49 - 57s - loss: 921.2796 - loglik: -9.2291e+02 - logprior: 1.6267
Fitted a model with MAP estimate = -922.9692
expansions: [(0, 6), (232, 1), (332, 2), (339, 1), (482, 1), (487, 1), (489, 1)]
discards: [  1   2   3   4   5  23 225 239 272 323 386 437 468 494 495 496 497 498]
Re-initialized the encoder parameters.
Fitting a model of length 494 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 934.5448 - loglik: -9.3281e+02 - logprior: -1.7376e+00
Epoch 2/2
49/49 - 57s - loss: 926.6605 - loglik: -9.2971e+02 - logprior: 3.0522
Fitted a model with MAP estimate = -923.4905
expansions: [(0, 6), (342, 1), (344, 1), (494, 8)]
discards: [  0   1   2   3   4   5   6   7 321 322 329]
Re-initialized the encoder parameters.
Fitting a model of length 499 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 61s - loss: 929.3672 - loglik: -9.2928e+02 - logprior: -9.0693e-02
Epoch 2/10
49/49 - 57s - loss: 923.5711 - loglik: -9.2708e+02 - logprior: 3.5097
Epoch 3/10
49/49 - 57s - loss: 920.3026 - loglik: -9.2433e+02 - logprior: 4.0271
Epoch 4/10
49/49 - 58s - loss: 921.7009 - loglik: -9.2613e+02 - logprior: 4.4257
Fitted a model with MAP estimate = -918.9373
Time for alignment: 800.3797
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 45s - loss: 1099.2261 - loglik: -1.0979e+03 - logprior: -1.3252e+00
Epoch 2/10
49/49 - 41s - loss: 959.0383 - loglik: -9.5922e+02 - logprior: 0.1767
Epoch 3/10
49/49 - 41s - loss: 955.7459 - loglik: -9.5587e+02 - logprior: 0.1211
Epoch 4/10
49/49 - 41s - loss: 954.1096 - loglik: -9.5428e+02 - logprior: 0.1746
Epoch 5/10
49/49 - 41s - loss: 952.6584 - loglik: -9.5280e+02 - logprior: 0.1443
Epoch 6/10
49/49 - 41s - loss: 950.2174 - loglik: -9.5031e+02 - logprior: 0.0919
Epoch 7/10
49/49 - 41s - loss: 955.0463 - loglik: -9.5512e+02 - logprior: 0.0748
Fitted a model with MAP estimate = -950.8954
expansions: [(0, 5), (36, 2), (144, 1), (218, 1), (219, 1), (233, 1), (236, 2), (237, 1), (239, 1), (240, 1), (241, 1), (245, 1), (249, 2), (250, 2), (251, 1), (260, 1), (263, 1), (264, 3), (265, 4), (266, 2), (267, 1), (276, 1), (279, 2), (280, 10), (292, 1), (296, 3), (297, 1), (298, 2), (299, 3), (300, 4), (302, 2), (303, 4), (304, 1), (306, 1), (311, 2), (312, 1), (313, 4), (315, 1), (323, 1), (324, 2), (325, 1), (326, 1), (327, 1), (328, 1), (345, 2), (346, 4), (347, 1), (348, 1), (395, 4), (396, 1), (398, 1), (399, 10), (400, 2), (404, 6)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  19  38  39 103 104 107]
Re-initialized the encoder parameters.
Fitting a model of length 497 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 60s - loss: 942.5851 - loglik: -9.4023e+02 - logprior: -2.3574e+00
Epoch 2/2
49/49 - 57s - loss: 927.9802 - loglik: -9.2997e+02 - logprior: 1.9849
Fitted a model with MAP estimate = -922.4695
expansions: [(0, 7), (26, 2), (299, 1), (320, 2), (479, 1), (481, 1)]
discards: [  1   2   3   4   5  21 224 274 362 409 410 463 491 492 493 494 495 496]
Re-initialized the encoder parameters.
Fitting a model of length 493 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 935.0825 - loglik: -9.3327e+02 - logprior: -1.8106e+00
Epoch 2/2
49/49 - 56s - loss: 927.8076 - loglik: -9.3080e+02 - logprior: 2.9939
Fitted a model with MAP estimate = -922.9729
expansions: [(0, 6), (491, 4)]
discards: [  1   2   3   4   5   6   7  27 329 365]
Re-initialized the encoder parameters.
Fitting a model of length 493 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 59s - loss: 933.6859 - loglik: -9.3223e+02 - logprior: -1.4607e+00
Epoch 2/10
49/49 - 56s - loss: 924.8387 - loglik: -9.2838e+02 - logprior: 3.5441
Epoch 3/10
49/49 - 56s - loss: 917.5700 - loglik: -9.2167e+02 - logprior: 4.0985
Epoch 4/10
49/49 - 57s - loss: 922.7341 - loglik: -9.2738e+02 - logprior: 4.6443
Fitted a model with MAP estimate = -919.1357
Time for alignment: 917.2015
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 1101.1198 - loglik: -1.0998e+03 - logprior: -1.3602e+00
Epoch 2/10
49/49 - 41s - loss: 966.4288 - loglik: -9.6668e+02 - logprior: 0.2538
Epoch 3/10
49/49 - 41s - loss: 957.0935 - loglik: -9.5736e+02 - logprior: 0.2617
Epoch 4/10
49/49 - 41s - loss: 953.1812 - loglik: -9.5339e+02 - logprior: 0.2078
Epoch 5/10
49/49 - 41s - loss: 956.6523 - loglik: -9.5675e+02 - logprior: 0.1005
Fitted a model with MAP estimate = -952.5621
expansions: [(0, 6), (134, 1), (142, 1), (183, 1), (186, 1), (193, 1), (213, 3), (227, 1), (229, 2), (230, 2), (231, 3), (232, 3), (233, 1), (234, 1), (238, 2), (240, 1), (241, 1), (249, 1), (250, 1), (251, 2), (252, 7), (253, 3), (269, 1), (270, 1), (271, 2), (272, 4), (284, 1), (291, 1), (292, 4), (293, 3), (295, 4), (297, 2), (298, 4), (308, 1), (309, 1), (310, 1), (311, 4), (312, 1), (313, 1), (321, 1), (323, 1), (324, 1), (325, 1), (326, 1), (344, 5), (345, 1), (364, 1), (394, 2), (395, 3), (399, 10), (400, 2), (404, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 35 36 37]
Re-initialized the encoder parameters.
Fitting a model of length 501 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 61s - loss: 939.9946 - loglik: -9.3758e+02 - logprior: -2.4168e+00
Epoch 2/2
49/49 - 58s - loss: 928.4015 - loglik: -9.3028e+02 - logprior: 1.8758
Fitted a model with MAP estimate = -922.6346
expansions: [(0, 6), (489, 1), (490, 1)]
discards: [  1   2   3   4   5   6 235 329 341 365 366 412 413 469 497 498 499 500]
Re-initialized the encoder parameters.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 934.7684 - loglik: -9.3314e+02 - logprior: -1.6316e+00
Epoch 2/2
49/49 - 56s - loss: 922.7797 - loglik: -9.2592e+02 - logprior: 3.1385
Fitted a model with MAP estimate = -922.9639
expansions: [(0, 6), (406, 1), (491, 7)]
discards: [  1   2   3   4   5   6 272 327]
Re-initialized the encoder parameters.
Fitting a model of length 497 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 60s - loss: 929.6964 - loglik: -9.2809e+02 - logprior: -1.6104e+00
Epoch 2/10
49/49 - 57s - loss: 925.1724 - loglik: -9.2861e+02 - logprior: 3.4378
Epoch 3/10
49/49 - 57s - loss: 920.3013 - loglik: -9.2441e+02 - logprior: 4.1130
Epoch 4/10
49/49 - 57s - loss: 917.2675 - loglik: -9.2172e+02 - logprior: 4.4550
Epoch 5/10
49/49 - 58s - loss: 920.3575 - loglik: -9.2515e+02 - logprior: 4.7892
Fitted a model with MAP estimate = -916.8226
Time for alignment: 896.2895
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 45s - loss: 1096.8356 - loglik: -1.0956e+03 - logprior: -1.2217e+00
Epoch 2/10
49/49 - 41s - loss: 960.8344 - loglik: -9.6129e+02 - logprior: 0.4575
Epoch 3/10
49/49 - 41s - loss: 952.2374 - loglik: -9.5260e+02 - logprior: 0.3579
Epoch 4/10
49/49 - 41s - loss: 955.1652 - loglik: -9.5548e+02 - logprior: 0.3111
Fitted a model with MAP estimate = -951.6185
expansions: [(0, 5), (109, 1), (133, 1), (141, 1), (205, 1), (214, 1), (234, 3), (235, 1), (236, 1), (237, 1), (238, 1), (247, 1), (250, 1), (260, 1), (261, 1), (262, 2), (263, 4), (264, 3), (265, 1), (266, 1), (279, 2), (280, 9), (286, 1), (296, 3), (297, 2), (298, 1), (300, 1), (301, 2), (303, 1), (307, 4), (319, 2), (320, 9), (321, 1), (329, 1), (330, 1), (331, 2), (332, 1), (333, 2), (346, 2), (347, 4), (348, 1), (349, 1), (365, 1), (366, 2), (395, 3), (396, 1), (398, 1), (399, 11), (400, 1), (404, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 34 35 36 37]
Re-initialized the encoder parameters.
Fitting a model of length 494 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 946.8560 - loglik: -9.4425e+02 - logprior: -2.6061e+00
Epoch 2/2
49/49 - 57s - loss: 923.1064 - loglik: -9.2492e+02 - logprior: 1.8155
Fitted a model with MAP estimate = -923.6214
expansions: [(0, 7), (297, 1), (299, 1), (325, 1), (357, 1), (358, 1), (482, 1), (483, 1)]
discards: [  1   2   3   4   5 405 406 462 489 490 491 492 493]
Re-initialized the encoder parameters.
Fitting a model of length 495 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 61s - loss: 933.9318 - loglik: -9.3217e+02 - logprior: -1.7609e+00
Epoch 2/2
49/49 - 57s - loss: 924.5898 - loglik: -9.2771e+02 - logprior: 3.1238
Fitted a model with MAP estimate = -923.0644
expansions: [(0, 7), (495, 8)]
discards: [  0   1   2   3   4   5   6   7 323]
Re-initialized the encoder parameters.
Fitting a model of length 501 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 61s - loss: 929.5612 - loglik: -9.2938e+02 - logprior: -1.8604e-01
Epoch 2/10
49/49 - 58s - loss: 921.9763 - loglik: -9.2534e+02 - logprior: 3.3670
Epoch 3/10
49/49 - 58s - loss: 917.3081 - loglik: -9.2109e+02 - logprior: 3.7835
Epoch 4/10
49/49 - 58s - loss: 920.5082 - loglik: -9.2491e+02 - logprior: 4.3983
Fitted a model with MAP estimate = -917.8920
Time for alignment: 801.8290
Computed alignments with likelihoods: ['-916.3430', '-918.9373', '-919.1357', '-916.8226', '-917.8920']
Best model has likelihood: -916.3430  (prior= 5.8181 )
time for generating output: 0.4608
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.8026140304081089
Training of 5 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaf4941640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead13ff100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feafd2582e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 478.0448 - loglik: -4.7611e+02 - logprior: -1.9330e+00
Epoch 2/10
39/39 - 8s - loss: 412.2815 - loglik: -4.1077e+02 - logprior: -1.5153e+00
Epoch 3/10
39/39 - 8s - loss: 404.7006 - loglik: -4.0318e+02 - logprior: -1.5196e+00
Epoch 4/10
39/39 - 8s - loss: 402.3183 - loglik: -4.0081e+02 - logprior: -1.5103e+00
Epoch 5/10
39/39 - 8s - loss: 401.6765 - loglik: -4.0016e+02 - logprior: -1.5187e+00
Epoch 6/10
39/39 - 8s - loss: 401.1895 - loglik: -3.9967e+02 - logprior: -1.5211e+00
Epoch 7/10
39/39 - 8s - loss: 400.6790 - loglik: -3.9915e+02 - logprior: -1.5328e+00
Epoch 8/10
39/39 - 8s - loss: 400.3507 - loglik: -3.9882e+02 - logprior: -1.5313e+00
Epoch 9/10
39/39 - 8s - loss: 400.0154 - loglik: -3.9848e+02 - logprior: -1.5336e+00
Epoch 10/10
39/39 - 8s - loss: 399.9656 - loglik: -3.9844e+02 - logprior: -1.5216e+00
Fitted a model with MAP estimate = -400.4962
expansions: [(8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (14, 2), (23, 1), (26, 2), (39, 3), (40, 2), (45, 1), (55, 2), (57, 2), (58, 2), (59, 2), (68, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (100, 1), (104, 2), (105, 3), (107, 2), (109, 1), (118, 1), (122, 1), (124, 2), (126, 2), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 395.9850 - loglik: -3.9306e+02 - logprior: -2.9215e+00
Epoch 2/2
39/39 - 11s - loss: 382.1638 - loglik: -3.8110e+02 - logprior: -1.0651e+00
Fitted a model with MAP estimate = -381.1351
expansions: []
discards: [ 13  19  35  53  72  76  79  81  96 117 138 145]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 384.9259 - loglik: -3.8297e+02 - logprior: -1.9603e+00
Epoch 2/2
39/39 - 10s - loss: 381.2847 - loglik: -3.8060e+02 - logprior: -6.7993e-01
Fitted a model with MAP estimate = -380.9331
expansions: []
discards: [159]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 382.4480 - loglik: -3.8081e+02 - logprior: -1.6361e+00
Epoch 2/10
41/41 - 11s - loss: 380.0729 - loglik: -3.7954e+02 - logprior: -5.2837e-01
Epoch 3/10
41/41 - 10s - loss: 377.7194 - loglik: -3.7721e+02 - logprior: -5.0514e-01
Epoch 4/10
41/41 - 10s - loss: 377.2570 - loglik: -3.7679e+02 - logprior: -4.6977e-01
Epoch 5/10
41/41 - 10s - loss: 376.4273 - loglik: -3.7600e+02 - logprior: -4.2328e-01
Epoch 6/10
41/41 - 10s - loss: 375.8306 - loglik: -3.7547e+02 - logprior: -3.5681e-01
Epoch 7/10
41/41 - 10s - loss: 376.7130 - loglik: -3.7640e+02 - logprior: -3.1143e-01
Fitted a model with MAP estimate = -375.8463
Time for alignment: 255.3027
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 478.3622 - loglik: -4.7644e+02 - logprior: -1.9220e+00
Epoch 2/10
39/39 - 8s - loss: 412.4531 - loglik: -4.1094e+02 - logprior: -1.5131e+00
Epoch 3/10
39/39 - 8s - loss: 404.8821 - loglik: -4.0337e+02 - logprior: -1.5072e+00
Epoch 4/10
39/39 - 8s - loss: 402.2905 - loglik: -4.0079e+02 - logprior: -1.4958e+00
Epoch 5/10
39/39 - 8s - loss: 401.5715 - loglik: -4.0007e+02 - logprior: -1.5024e+00
Epoch 6/10
39/39 - 8s - loss: 401.0406 - loglik: -3.9951e+02 - logprior: -1.5266e+00
Epoch 7/10
39/39 - 8s - loss: 400.4818 - loglik: -3.9895e+02 - logprior: -1.5278e+00
Epoch 8/10
39/39 - 8s - loss: 400.4607 - loglik: -3.9895e+02 - logprior: -1.5148e+00
Epoch 9/10
39/39 - 8s - loss: 400.3409 - loglik: -3.9883e+02 - logprior: -1.5153e+00
Epoch 10/10
39/39 - 8s - loss: 400.3148 - loglik: -3.9881e+02 - logprior: -1.5085e+00
Fitted a model with MAP estimate = -400.3760
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 2), (23, 2), (25, 1), (39, 2), (40, 1), (43, 1), (55, 2), (57, 2), (58, 2), (59, 2), (68, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (100, 2), (102, 2), (103, 2), (104, 2), (106, 1), (109, 1), (118, 1), (122, 2), (126, 2), (129, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 394.9093 - loglik: -3.9198e+02 - logprior: -2.9261e+00
Epoch 2/2
39/39 - 11s - loss: 380.8471 - loglik: -3.7976e+02 - logprior: -1.0890e+00
Fitted a model with MAP estimate = -379.4844
expansions: []
discards: [ 13  28  70  74  77  79  94 115 122 131 136 139 171 175]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 383.5896 - loglik: -3.8166e+02 - logprior: -1.9250e+00
Epoch 2/2
39/39 - 10s - loss: 380.3965 - loglik: -3.7972e+02 - logprior: -6.7448e-01
Fitted a model with MAP estimate = -380.1827
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 381.6182 - loglik: -3.8004e+02 - logprior: -1.5817e+00
Epoch 2/10
41/41 - 10s - loss: 379.3296 - loglik: -3.7884e+02 - logprior: -4.9245e-01
Epoch 3/10
41/41 - 10s - loss: 377.2649 - loglik: -3.7682e+02 - logprior: -4.4812e-01
Epoch 4/10
41/41 - 10s - loss: 377.3307 - loglik: -3.7693e+02 - logprior: -4.0139e-01
Fitted a model with MAP estimate = -376.2421
Time for alignment: 225.1596
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 477.8331 - loglik: -4.7592e+02 - logprior: -1.9103e+00
Epoch 2/10
39/39 - 8s - loss: 410.5456 - loglik: -4.0898e+02 - logprior: -1.5621e+00
Epoch 3/10
39/39 - 8s - loss: 403.8525 - loglik: -4.0227e+02 - logprior: -1.5809e+00
Epoch 4/10
39/39 - 8s - loss: 402.3511 - loglik: -4.0076e+02 - logprior: -1.5903e+00
Epoch 5/10
39/39 - 8s - loss: 401.5244 - loglik: -3.9992e+02 - logprior: -1.6010e+00
Epoch 6/10
39/39 - 8s - loss: 401.3147 - loglik: -3.9971e+02 - logprior: -1.6047e+00
Epoch 7/10
39/39 - 8s - loss: 400.9745 - loglik: -3.9934e+02 - logprior: -1.6296e+00
Epoch 8/10
39/39 - 8s - loss: 400.3847 - loglik: -3.9876e+02 - logprior: -1.6263e+00
Epoch 9/10
39/39 - 8s - loss: 400.9686 - loglik: -3.9936e+02 - logprior: -1.6123e+00
Fitted a model with MAP estimate = -400.9135
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 1), (40, 2), (41, 1), (44, 1), (55, 1), (56, 2), (57, 2), (58, 2), (59, 2), (69, 2), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (95, 1), (100, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 2), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 396.4491 - loglik: -3.9355e+02 - logprior: -2.8975e+00
Epoch 2/2
39/39 - 11s - loss: 383.8145 - loglik: -3.8279e+02 - logprior: -1.0283e+00
Fitted a model with MAP estimate = -382.4625
expansions: []
discards: [ 13  69  72  76  78  90  94 115 158]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 386.1620 - loglik: -3.8420e+02 - logprior: -1.9658e+00
Epoch 2/2
39/39 - 10s - loss: 382.4812 - loglik: -3.8181e+02 - logprior: -6.7104e-01
Fitted a model with MAP estimate = -382.0728
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 171 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 383.7396 - loglik: -3.8211e+02 - logprior: -1.6277e+00
Epoch 2/10
41/41 - 10s - loss: 380.9818 - loglik: -3.8045e+02 - logprior: -5.3252e-01
Epoch 3/10
41/41 - 10s - loss: 379.3068 - loglik: -3.7881e+02 - logprior: -4.9592e-01
Epoch 4/10
41/41 - 11s - loss: 379.0273 - loglik: -3.7857e+02 - logprior: -4.5610e-01
Epoch 5/10
41/41 - 10s - loss: 376.2737 - loglik: -3.7588e+02 - logprior: -3.9671e-01
Epoch 6/10
41/41 - 10s - loss: 379.1772 - loglik: -3.7883e+02 - logprior: -3.4630e-01
Fitted a model with MAP estimate = -377.5473
Time for alignment: 238.1468
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 478.2309 - loglik: -4.7628e+02 - logprior: -1.9534e+00
Epoch 2/10
39/39 - 8s - loss: 412.4761 - loglik: -4.1091e+02 - logprior: -1.5650e+00
Epoch 3/10
39/39 - 8s - loss: 405.4147 - loglik: -4.0387e+02 - logprior: -1.5448e+00
Epoch 4/10
39/39 - 8s - loss: 403.0611 - loglik: -4.0148e+02 - logprior: -1.5786e+00
Epoch 5/10
39/39 - 8s - loss: 402.4848 - loglik: -4.0089e+02 - logprior: -1.5913e+00
Epoch 6/10
39/39 - 8s - loss: 401.6719 - loglik: -4.0007e+02 - logprior: -1.6049e+00
Epoch 7/10
39/39 - 8s - loss: 401.3127 - loglik: -3.9971e+02 - logprior: -1.6010e+00
Epoch 8/10
39/39 - 8s - loss: 401.7619 - loglik: -4.0016e+02 - logprior: -1.5986e+00
Fitted a model with MAP estimate = -401.6292
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 1), (31, 1), (39, 2), (44, 1), (45, 1), (55, 1), (57, 2), (58, 2), (59, 2), (69, 2), (71, 2), (80, 1), (89, 2), (92, 1), (93, 2), (100, 2), (102, 1), (103, 2), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (122, 1), (126, 1), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 398.1144 - loglik: -3.9520e+02 - logprior: -2.9134e+00
Epoch 2/2
39/39 - 10s - loss: 385.4220 - loglik: -3.8438e+02 - logprior: -1.0413e+00
Fitted a model with MAP estimate = -383.6339
expansions: []
discards: [ 13  72  75  77  89  93 114 121 130]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 386.7759 - loglik: -3.8479e+02 - logprior: -1.9837e+00
Epoch 2/2
39/39 - 10s - loss: 383.1620 - loglik: -3.8245e+02 - logprior: -7.0798e-01
Fitted a model with MAP estimate = -382.7115
expansions: []
discards: [127]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 14s - loss: 384.8593 - loglik: -3.8321e+02 - logprior: -1.6519e+00
Epoch 2/10
41/41 - 10s - loss: 380.7733 - loglik: -3.8022e+02 - logprior: -5.5527e-01
Epoch 3/10
41/41 - 10s - loss: 381.1868 - loglik: -3.8069e+02 - logprior: -4.9588e-01
Fitted a model with MAP estimate = -379.6033
Time for alignment: 197.9239
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 479.8486 - loglik: -4.7791e+02 - logprior: -1.9372e+00
Epoch 2/10
39/39 - 8s - loss: 410.7274 - loglik: -4.0913e+02 - logprior: -1.5953e+00
Epoch 3/10
39/39 - 8s - loss: 403.7993 - loglik: -4.0220e+02 - logprior: -1.6029e+00
Epoch 4/10
39/39 - 8s - loss: 402.6451 - loglik: -4.0106e+02 - logprior: -1.5832e+00
Epoch 5/10
39/39 - 8s - loss: 401.8042 - loglik: -4.0022e+02 - logprior: -1.5831e+00
Epoch 6/10
39/39 - 8s - loss: 401.6618 - loglik: -4.0008e+02 - logprior: -1.5856e+00
Epoch 7/10
39/39 - 8s - loss: 400.7828 - loglik: -3.9919e+02 - logprior: -1.5923e+00
Epoch 8/10
39/39 - 8s - loss: 400.7838 - loglik: -3.9921e+02 - logprior: -1.5692e+00
Fitted a model with MAP estimate = -401.0845
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (27, 1), (39, 3), (44, 1), (45, 1), (55, 2), (57, 1), (58, 2), (68, 1), (69, 2), (71, 2), (89, 2), (92, 1), (93, 2), (100, 2), (102, 1), (103, 2), (104, 1), (105, 1), (107, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 2), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 397.0370 - loglik: -3.9414e+02 - logprior: -2.9002e+00
Epoch 2/2
39/39 - 11s - loss: 384.0375 - loglik: -3.8300e+02 - logprior: -1.0401e+00
Fitted a model with MAP estimate = -382.5165
expansions: [(103, 1)]
discards: [ 13  50  69  74  88  92 112 119 128]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 385.9371 - loglik: -3.8396e+02 - logprior: -1.9727e+00
Epoch 2/2
39/39 - 10s - loss: 381.9781 - loglik: -3.8127e+02 - logprior: -7.0642e-01
Fitted a model with MAP estimate = -381.3128
expansions: [(49, 1)]
discards: [126]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 382.6272 - loglik: -3.8101e+02 - logprior: -1.6191e+00
Epoch 2/10
41/41 - 10s - loss: 379.6533 - loglik: -3.7912e+02 - logprior: -5.3809e-01
Epoch 3/10
41/41 - 10s - loss: 377.9567 - loglik: -3.7748e+02 - logprior: -4.7436e-01
Epoch 4/10
41/41 - 11s - loss: 377.5694 - loglik: -3.7714e+02 - logprior: -4.2785e-01
Epoch 5/10
41/41 - 10s - loss: 376.6245 - loglik: -3.7624e+02 - logprior: -3.8377e-01
Epoch 6/10
41/41 - 10s - loss: 376.8946 - loglik: -3.7656e+02 - logprior: -3.3378e-01
Fitted a model with MAP estimate = -376.3207
Time for alignment: 227.2338
Computed alignments with likelihoods: ['-375.8463', '-376.2421', '-377.5473', '-379.6033', '-376.3207']
Best model has likelihood: -375.8463  (prior= -0.2606 )
time for generating output: 0.2868
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.30794430794430794
Training of 5 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac0266a00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac0195e20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feadaea9f40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 170.5905 - loglik: -1.1169e+02 - logprior: -5.8902e+01
Epoch 2/10
10/10 - 0s - loss: 104.6173 - loglik: -8.8042e+01 - logprior: -1.6576e+01
Epoch 3/10
10/10 - 0s - loss: 80.5912 - loglik: -7.2337e+01 - logprior: -8.2545e+00
Epoch 4/10
10/10 - 0s - loss: 70.1232 - loglik: -6.4932e+01 - logprior: -5.1909e+00
Epoch 5/10
10/10 - 0s - loss: 64.8251 - loglik: -6.1167e+01 - logprior: -3.6586e+00
Epoch 6/10
10/10 - 0s - loss: 62.3015 - loglik: -5.9408e+01 - logprior: -2.8931e+00
Epoch 7/10
10/10 - 0s - loss: 61.2747 - loglik: -5.8826e+01 - logprior: -2.4489e+00
Epoch 8/10
10/10 - 0s - loss: 60.6521 - loglik: -5.8549e+01 - logprior: -2.1035e+00
Epoch 9/10
10/10 - 0s - loss: 60.3684 - loglik: -5.8538e+01 - logprior: -1.8300e+00
Epoch 10/10
10/10 - 0s - loss: 60.0528 - loglik: -5.8405e+01 - logprior: -1.6478e+00
Fitted a model with MAP estimate = -59.9496
expansions: [(0, 4), (10, 2), (21, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 136.2631 - loglik: -5.7554e+01 - logprior: -7.8709e+01
Epoch 2/2
10/10 - 0s - loss: 78.7658 - loglik: -5.3570e+01 - logprior: -2.5195e+01
Fitted a model with MAP estimate = -67.5929
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.0329 - loglik: -5.1198e+01 - logprior: -6.5835e+01
Epoch 2/2
10/10 - 0s - loss: 73.4305 - loglik: -5.0320e+01 - logprior: -2.3110e+01
Fitted a model with MAP estimate = -64.5468
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 117.0076 - loglik: -5.1120e+01 - logprior: -6.5887e+01
Epoch 2/10
10/10 - 0s - loss: 78.8939 - loglik: -5.1576e+01 - logprior: -2.7318e+01
Epoch 3/10
10/10 - 0s - loss: 67.3590 - loglik: -5.1446e+01 - logprior: -1.5913e+01
Epoch 4/10
10/10 - 0s - loss: 57.9527 - loglik: -5.1480e+01 - logprior: -6.4730e+00
Epoch 5/10
10/10 - 0s - loss: 54.3275 - loglik: -5.1366e+01 - logprior: -2.9613e+00
Epoch 6/10
10/10 - 0s - loss: 52.8653 - loglik: -5.1016e+01 - logprior: -1.8488e+00
Epoch 7/10
10/10 - 0s - loss: 52.3516 - loglik: -5.1073e+01 - logprior: -1.2788e+00
Epoch 8/10
10/10 - 0s - loss: 51.9491 - loglik: -5.1033e+01 - logprior: -9.1560e-01
Epoch 9/10
10/10 - 0s - loss: 51.7570 - loglik: -5.1084e+01 - logprior: -6.7287e-01
Epoch 10/10
10/10 - 0s - loss: 51.2629 - loglik: -5.0767e+01 - logprior: -4.9569e-01
Fitted a model with MAP estimate = -51.2920
Time for alignment: 25.8301
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 170.6244 - loglik: -1.1172e+02 - logprior: -5.8903e+01
Epoch 2/10
10/10 - 0s - loss: 104.4757 - loglik: -8.7896e+01 - logprior: -1.6579e+01
Epoch 3/10
10/10 - 0s - loss: 79.8261 - loglik: -7.1556e+01 - logprior: -8.2703e+00
Epoch 4/10
10/10 - 0s - loss: 68.6661 - loglik: -6.3420e+01 - logprior: -5.2457e+00
Epoch 5/10
10/10 - 0s - loss: 64.6424 - loglik: -6.0960e+01 - logprior: -3.6820e+00
Epoch 6/10
10/10 - 0s - loss: 63.1396 - loglik: -6.0234e+01 - logprior: -2.9054e+00
Epoch 7/10
10/10 - 0s - loss: 62.2443 - loglik: -5.9822e+01 - logprior: -2.4224e+00
Epoch 8/10
10/10 - 0s - loss: 61.8604 - loglik: -5.9795e+01 - logprior: -2.0650e+00
Epoch 9/10
10/10 - 0s - loss: 61.5346 - loglik: -5.9714e+01 - logprior: -1.8207e+00
Epoch 10/10
10/10 - 0s - loss: 61.3179 - loglik: -5.9613e+01 - logprior: -1.7053e+00
Fitted a model with MAP estimate = -61.0624
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.3235 - loglik: -5.8858e+01 - logprior: -7.8466e+01
Epoch 2/2
10/10 - 0s - loss: 79.0273 - loglik: -5.3684e+01 - logprior: -2.5343e+01
Fitted a model with MAP estimate = -68.1109
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.2633 - loglik: -5.1544e+01 - logprior: -6.5720e+01
Epoch 2/2
10/10 - 0s - loss: 73.7325 - loglik: -5.0649e+01 - logprior: -2.3084e+01
Fitted a model with MAP estimate = -64.8032
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 110.4886 - loglik: -5.1379e+01 - logprior: -5.9109e+01
Epoch 2/10
10/10 - 0s - loss: 68.5919 - loglik: -5.1931e+01 - logprior: -1.6661e+01
Epoch 3/10
10/10 - 0s - loss: 60.3110 - loglik: -5.2681e+01 - logprior: -7.6296e+00
Epoch 4/10
10/10 - 0s - loss: 56.8961 - loglik: -5.2601e+01 - logprior: -4.2947e+00
Epoch 5/10
10/10 - 0s - loss: 55.0067 - loglik: -5.2326e+01 - logprior: -2.6810e+00
Epoch 6/10
10/10 - 0s - loss: 53.5412 - loglik: -5.1740e+01 - logprior: -1.8015e+00
Epoch 7/10
10/10 - 0s - loss: 53.2171 - loglik: -5.1944e+01 - logprior: -1.2727e+00
Epoch 8/10
10/10 - 0s - loss: 52.5868 - loglik: -5.1696e+01 - logprior: -8.9070e-01
Epoch 9/10
10/10 - 0s - loss: 52.5113 - loglik: -5.1905e+01 - logprior: -6.0591e-01
Epoch 10/10
10/10 - 0s - loss: 52.2063 - loglik: -5.1787e+01 - logprior: -4.1954e-01
Fitted a model with MAP estimate = -52.0722
Time for alignment: 24.8522
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.6212 - loglik: -1.1172e+02 - logprior: -5.8902e+01
Epoch 2/10
10/10 - 0s - loss: 104.5382 - loglik: -8.7962e+01 - logprior: -1.6577e+01
Epoch 3/10
10/10 - 0s - loss: 80.1078 - loglik: -7.1846e+01 - logprior: -8.2613e+00
Epoch 4/10
10/10 - 0s - loss: 69.0469 - loglik: -6.3817e+01 - logprior: -5.2303e+00
Epoch 5/10
10/10 - 0s - loss: 64.7648 - loglik: -6.1085e+01 - logprior: -3.6793e+00
Epoch 6/10
10/10 - 0s - loss: 63.1087 - loglik: -6.0193e+01 - logprior: -2.9156e+00
Epoch 7/10
10/10 - 0s - loss: 62.3362 - loglik: -5.9902e+01 - logprior: -2.4341e+00
Epoch 8/10
10/10 - 0s - loss: 61.8362 - loglik: -5.9764e+01 - logprior: -2.0720e+00
Epoch 9/10
10/10 - 0s - loss: 61.6495 - loglik: -5.9827e+01 - logprior: -1.8225e+00
Epoch 10/10
10/10 - 0s - loss: 61.1959 - loglik: -5.9493e+01 - logprior: -1.7029e+00
Fitted a model with MAP estimate = -61.0830
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 137.4525 - loglik: -5.8969e+01 - logprior: -7.8483e+01
Epoch 2/2
10/10 - 0s - loss: 78.9516 - loglik: -5.3604e+01 - logprior: -2.5348e+01
Fitted a model with MAP estimate = -68.1360
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.2217 - loglik: -5.1502e+01 - logprior: -6.5720e+01
Epoch 2/2
10/10 - 0s - loss: 73.8015 - loglik: -5.0718e+01 - logprior: -2.3083e+01
Fitted a model with MAP estimate = -64.7902
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 117.2406 - loglik: -5.1478e+01 - logprior: -6.5762e+01
Epoch 2/10
10/10 - 0s - loss: 79.0349 - loglik: -5.1731e+01 - logprior: -2.7304e+01
Epoch 3/10
10/10 - 0s - loss: 67.8243 - loglik: -5.1765e+01 - logprior: -1.6059e+01
Epoch 4/10
10/10 - 0s - loss: 58.2068 - loglik: -5.1696e+01 - logprior: -6.5103e+00
Epoch 5/10
10/10 - 0s - loss: 54.3367 - loglik: -5.1457e+01 - logprior: -2.8799e+00
Epoch 6/10
10/10 - 0s - loss: 52.9253 - loglik: -5.1164e+01 - logprior: -1.7615e+00
Epoch 7/10
10/10 - 0s - loss: 52.5448 - loglik: -5.1357e+01 - logprior: -1.1882e+00
Epoch 8/10
10/10 - 0s - loss: 51.9259 - loglik: -5.1101e+01 - logprior: -8.2522e-01
Epoch 9/10
10/10 - 0s - loss: 51.6558 - loglik: -5.1081e+01 - logprior: -5.7522e-01
Epoch 10/10
10/10 - 0s - loss: 51.5820 - loglik: -5.1178e+01 - logprior: -4.0352e-01
Fitted a model with MAP estimate = -51.3806
Time for alignment: 25.9993
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.5587 - loglik: -1.1166e+02 - logprior: -5.8902e+01
Epoch 2/10
10/10 - 0s - loss: 104.6711 - loglik: -8.8095e+01 - logprior: -1.6576e+01
Epoch 3/10
10/10 - 0s - loss: 80.1850 - loglik: -7.1925e+01 - logprior: -8.2604e+00
Epoch 4/10
10/10 - 0s - loss: 69.3120 - loglik: -6.4084e+01 - logprior: -5.2285e+00
Epoch 5/10
10/10 - 0s - loss: 64.8520 - loglik: -6.1176e+01 - logprior: -3.6756e+00
Epoch 6/10
10/10 - 0s - loss: 62.9991 - loglik: -6.0082e+01 - logprior: -2.9170e+00
Epoch 7/10
10/10 - 0s - loss: 62.4168 - loglik: -5.9977e+01 - logprior: -2.4396e+00
Epoch 8/10
10/10 - 0s - loss: 61.9084 - loglik: -5.9830e+01 - logprior: -2.0779e+00
Epoch 9/10
10/10 - 0s - loss: 61.6039 - loglik: -5.9778e+01 - logprior: -1.8259e+00
Epoch 10/10
10/10 - 0s - loss: 61.2105 - loglik: -5.9504e+01 - logprior: -1.7067e+00
Fitted a model with MAP estimate = -61.0802
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.3973 - loglik: -5.8905e+01 - logprior: -7.8492e+01
Epoch 2/2
10/10 - 0s - loss: 78.9437 - loglik: -5.3597e+01 - logprior: -2.5347e+01
Fitted a model with MAP estimate = -68.1110
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.1060 - loglik: -5.1391e+01 - logprior: -6.5715e+01
Epoch 2/2
10/10 - 0s - loss: 73.8466 - loglik: -5.0759e+01 - logprior: -2.3088e+01
Fitted a model with MAP estimate = -64.8128
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 110.6143 - loglik: -5.1508e+01 - logprior: -5.9106e+01
Epoch 2/10
10/10 - 0s - loss: 68.5182 - loglik: -5.1863e+01 - logprior: -1.6655e+01
Epoch 3/10
10/10 - 0s - loss: 60.1615 - loglik: -5.2535e+01 - logprior: -7.6262e+00
Epoch 4/10
10/10 - 0s - loss: 56.9738 - loglik: -5.2665e+01 - logprior: -4.3084e+00
Epoch 5/10
10/10 - 0s - loss: 54.8744 - loglik: -5.2204e+01 - logprior: -2.6707e+00
Epoch 6/10
10/10 - 0s - loss: 53.6595 - loglik: -5.1865e+01 - logprior: -1.7946e+00
Epoch 7/10
10/10 - 0s - loss: 53.1528 - loglik: -5.1886e+01 - logprior: -1.2669e+00
Epoch 8/10
10/10 - 0s - loss: 52.5731 - loglik: -5.1691e+01 - logprior: -8.8234e-01
Epoch 9/10
10/10 - 0s - loss: 52.5185 - loglik: -5.1917e+01 - logprior: -6.0128e-01
Epoch 10/10
10/10 - 0s - loss: 52.1121 - loglik: -5.1695e+01 - logprior: -4.1716e-01
Fitted a model with MAP estimate = -52.0660
Time for alignment: 24.0418
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.5277 - loglik: -1.1163e+02 - logprior: -5.8902e+01
Epoch 2/10
10/10 - 0s - loss: 104.6641 - loglik: -8.8087e+01 - logprior: -1.6577e+01
Epoch 3/10
10/10 - 0s - loss: 80.0282 - loglik: -7.1765e+01 - logprior: -8.2628e+00
Epoch 4/10
10/10 - 0s - loss: 69.0729 - loglik: -6.3842e+01 - logprior: -5.2308e+00
Epoch 5/10
10/10 - 0s - loss: 64.7299 - loglik: -6.1051e+01 - logprior: -3.6786e+00
Epoch 6/10
10/10 - 0s - loss: 63.1115 - loglik: -6.0197e+01 - logprior: -2.9147e+00
Epoch 7/10
10/10 - 0s - loss: 62.2411 - loglik: -5.9807e+01 - logprior: -2.4341e+00
Epoch 8/10
10/10 - 0s - loss: 61.9594 - loglik: -5.9888e+01 - logprior: -2.0718e+00
Epoch 9/10
10/10 - 0s - loss: 61.4281 - loglik: -5.9604e+01 - logprior: -1.8245e+00
Epoch 10/10
10/10 - 0s - loss: 61.3358 - loglik: -5.9631e+01 - logprior: -1.7045e+00
Fitted a model with MAP estimate = -61.0734
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.3451 - loglik: -5.8868e+01 - logprior: -7.8477e+01
Epoch 2/2
10/10 - 0s - loss: 79.0732 - loglik: -5.3729e+01 - logprior: -2.5344e+01
Fitted a model with MAP estimate = -68.1257
expansions: [(0, 2)]
discards: [14 32]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.1663 - loglik: -5.1449e+01 - logprior: -6.5717e+01
Epoch 2/2
10/10 - 0s - loss: 73.8519 - loglik: -5.0774e+01 - logprior: -2.3078e+01
Fitted a model with MAP estimate = -64.8080
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 110.5260 - loglik: -5.1422e+01 - logprior: -5.9104e+01
Epoch 2/10
10/10 - 0s - loss: 68.5521 - loglik: -5.1895e+01 - logprior: -1.6657e+01
Epoch 3/10
10/10 - 0s - loss: 60.1734 - loglik: -5.2549e+01 - logprior: -7.6246e+00
Epoch 4/10
10/10 - 0s - loss: 57.0378 - loglik: -5.2728e+01 - logprior: -4.3100e+00
Epoch 5/10
10/10 - 0s - loss: 54.8046 - loglik: -5.2134e+01 - logprior: -2.6709e+00
Epoch 6/10
10/10 - 0s - loss: 53.7539 - loglik: -5.1959e+01 - logprior: -1.7952e+00
Epoch 7/10
10/10 - 0s - loss: 52.9280 - loglik: -5.1659e+01 - logprior: -1.2691e+00
Epoch 8/10
10/10 - 0s - loss: 52.7099 - loglik: -5.1825e+01 - logprior: -8.8533e-01
Epoch 9/10
10/10 - 0s - loss: 52.4448 - loglik: -5.1846e+01 - logprior: -5.9912e-01
Epoch 10/10
10/10 - 0s - loss: 52.1483 - loglik: -5.1731e+01 - logprior: -4.1760e-01
Fitted a model with MAP estimate = -52.0632
Time for alignment: 23.7484
Computed alignments with likelihoods: ['-51.2920', '-52.0722', '-51.3806', '-52.0660', '-52.0632']
Best model has likelihood: -51.2920  (prior= -0.4152 )
time for generating output: 0.0811
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.966183574879227
Training of 5 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb16d27820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebdb40d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebdb41c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 8s - loss: 373.2874 - loglik: -3.7050e+02 - logprior: -2.7904e+00
Epoch 2/10
20/20 - 3s - loss: 330.3592 - loglik: -3.2916e+02 - logprior: -1.2016e+00
Epoch 3/10
20/20 - 3s - loss: 311.3540 - loglik: -3.0993e+02 - logprior: -1.4251e+00
Epoch 4/10
20/20 - 3s - loss: 305.8738 - loglik: -3.0452e+02 - logprior: -1.3564e+00
Epoch 5/10
20/20 - 3s - loss: 304.5086 - loglik: -3.0313e+02 - logprior: -1.3743e+00
Epoch 6/10
20/20 - 3s - loss: 304.2004 - loglik: -3.0286e+02 - logprior: -1.3448e+00
Epoch 7/10
20/20 - 3s - loss: 303.9402 - loglik: -3.0261e+02 - logprior: -1.3340e+00
Epoch 8/10
20/20 - 3s - loss: 303.8723 - loglik: -3.0256e+02 - logprior: -1.3172e+00
Epoch 9/10
20/20 - 4s - loss: 303.6388 - loglik: -3.0233e+02 - logprior: -1.3128e+00
Epoch 10/10
20/20 - 3s - loss: 303.8299 - loglik: -3.0251e+02 - logprior: -1.3198e+00
Fitted a model with MAP estimate = -289.1354
expansions: [(5, 1), (8, 1), (10, 3), (12, 1), (18, 1), (20, 1), (23, 2), (37, 2), (39, 1), (40, 1), (47, 1), (50, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 2), (62, 1), (75, 1), (76, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 302.5359 - loglik: -2.9995e+02 - logprior: -2.5870e+00
Epoch 2/2
40/40 - 5s - loss: 292.5226 - loglik: -2.9176e+02 - logprior: -7.6428e-01
Fitted a model with MAP estimate = -278.7926
expansions: []
discards: [ 12  30  46  74  82 107]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 294.7996 - loglik: -2.9301e+02 - logprior: -1.7858e+00
Epoch 2/2
40/40 - 5s - loss: 291.7318 - loglik: -2.9112e+02 - logprior: -6.1525e-01
Fitted a model with MAP estimate = -279.4023
expansions: [(96, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 10s - loss: 275.3687 - loglik: -2.7435e+02 - logprior: -1.0186e+00
Epoch 2/10
57/57 - 7s - loss: 273.5408 - loglik: -2.7283e+02 - logprior: -7.0701e-01
Epoch 3/10
57/57 - 6s - loss: 272.3855 - loglik: -2.7169e+02 - logprior: -6.9160e-01
Epoch 4/10
57/57 - 7s - loss: 272.0284 - loglik: -2.7136e+02 - logprior: -6.6395e-01
Epoch 5/10
57/57 - 6s - loss: 271.3705 - loglik: -2.7073e+02 - logprior: -6.3927e-01
Epoch 6/10
57/57 - 7s - loss: 271.6251 - loglik: -2.7101e+02 - logprior: -6.1797e-01
Fitted a model with MAP estimate = -271.2410
Time for alignment: 141.8460
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 373.4774 - loglik: -3.7069e+02 - logprior: -2.7920e+00
Epoch 2/10
20/20 - 3s - loss: 329.4018 - loglik: -3.2819e+02 - logprior: -1.2071e+00
Epoch 3/10
20/20 - 3s - loss: 310.7896 - loglik: -3.0934e+02 - logprior: -1.4542e+00
Epoch 4/10
20/20 - 3s - loss: 306.3689 - loglik: -3.0500e+02 - logprior: -1.3695e+00
Epoch 5/10
20/20 - 3s - loss: 305.1083 - loglik: -3.0375e+02 - logprior: -1.3597e+00
Epoch 6/10
20/20 - 3s - loss: 304.3276 - loglik: -3.0302e+02 - logprior: -1.3112e+00
Epoch 7/10
20/20 - 3s - loss: 303.8971 - loglik: -3.0262e+02 - logprior: -1.2740e+00
Epoch 8/10
20/20 - 3s - loss: 304.4445 - loglik: -3.0318e+02 - logprior: -1.2619e+00
Fitted a model with MAP estimate = -289.4540
expansions: [(5, 1), (8, 1), (10, 3), (12, 1), (18, 1), (20, 1), (23, 2), (37, 2), (38, 2), (40, 1), (47, 1), (50, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 2), (62, 1), (75, 1), (76, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1), (103, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 302.4318 - loglik: -2.9982e+02 - logprior: -2.6122e+00
Epoch 2/2
40/40 - 5s - loss: 292.0226 - loglik: -2.9118e+02 - logprior: -8.4282e-01
Fitted a model with MAP estimate = -277.9102
expansions: [(140, 2)]
discards: [ 12  30  47  49  75  83 108 125 138 139]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 294.7500 - loglik: -2.9287e+02 - logprior: -1.8829e+00
Epoch 2/2
40/40 - 5s - loss: 291.5199 - loglik: -2.9080e+02 - logprior: -7.2335e-01
Fitted a model with MAP estimate = -278.9404
expansions: [(96, 1), (132, 2)]
discards: [129 131]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 275.6202 - loglik: -2.7460e+02 - logprior: -1.0181e+00
Epoch 2/10
57/57 - 6s - loss: 273.3518 - loglik: -2.7272e+02 - logprior: -6.3294e-01
Epoch 3/10
57/57 - 6s - loss: 272.6735 - loglik: -2.7205e+02 - logprior: -6.2831e-01
Epoch 4/10
57/57 - 6s - loss: 271.7538 - loglik: -2.7114e+02 - logprior: -6.1177e-01
Epoch 5/10
57/57 - 7s - loss: 271.7366 - loglik: -2.7115e+02 - logprior: -5.8688e-01
Epoch 6/10
57/57 - 7s - loss: 271.9511 - loglik: -2.7139e+02 - logprior: -5.6255e-01
Fitted a model with MAP estimate = -271.4630
Time for alignment: 133.1339
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 373.1609 - loglik: -3.7036e+02 - logprior: -2.7960e+00
Epoch 2/10
20/20 - 3s - loss: 330.8046 - loglik: -3.2957e+02 - logprior: -1.2382e+00
Epoch 3/10
20/20 - 3s - loss: 311.7065 - loglik: -3.1025e+02 - logprior: -1.4608e+00
Epoch 4/10
20/20 - 3s - loss: 306.0537 - loglik: -3.0465e+02 - logprior: -1.4067e+00
Epoch 5/10
20/20 - 3s - loss: 304.5845 - loglik: -3.0320e+02 - logprior: -1.3885e+00
Epoch 6/10
20/20 - 3s - loss: 304.5537 - loglik: -3.0321e+02 - logprior: -1.3401e+00
Epoch 7/10
20/20 - 3s - loss: 303.8022 - loglik: -3.0250e+02 - logprior: -1.3070e+00
Epoch 8/10
20/20 - 3s - loss: 303.8433 - loglik: -3.0255e+02 - logprior: -1.2895e+00
Fitted a model with MAP estimate = -288.8064
expansions: [(5, 1), (8, 1), (10, 2), (12, 2), (18, 1), (23, 2), (26, 2), (37, 2), (39, 1), (40, 1), (47, 1), (50, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 2), (62, 1), (75, 1), (76, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1), (103, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 302.9231 - loglik: -3.0031e+02 - logprior: -2.6146e+00
Epoch 2/2
40/40 - 5s - loss: 291.7241 - loglik: -2.9086e+02 - logprior: -8.6719e-01
Fitted a model with MAP estimate = -277.7620
expansions: [(102, 1)]
discards: [ 12  30  34  47  75  83 108 126 137]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 294.2584 - loglik: -2.9243e+02 - logprior: -1.8329e+00
Epoch 2/2
40/40 - 5s - loss: 291.2789 - loglik: -2.9060e+02 - logprior: -6.8182e-01
Fitted a model with MAP estimate = -278.5683
expansions: []
discards: [93]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 275.7221 - loglik: -2.7470e+02 - logprior: -1.0179e+00
Epoch 2/10
57/57 - 6s - loss: 272.9402 - loglik: -2.7231e+02 - logprior: -6.2693e-01
Epoch 3/10
57/57 - 6s - loss: 273.0819 - loglik: -2.7247e+02 - logprior: -6.1057e-01
Fitted a model with MAP estimate = -272.0051
Time for alignment: 114.0649
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 373.0447 - loglik: -3.7025e+02 - logprior: -2.7906e+00
Epoch 2/10
20/20 - 3s - loss: 330.0298 - loglik: -3.2880e+02 - logprior: -1.2306e+00
Epoch 3/10
20/20 - 3s - loss: 310.9075 - loglik: -3.0947e+02 - logprior: -1.4362e+00
Epoch 4/10
20/20 - 3s - loss: 306.2149 - loglik: -3.0484e+02 - logprior: -1.3723e+00
Epoch 5/10
20/20 - 3s - loss: 304.9909 - loglik: -3.0359e+02 - logprior: -1.4000e+00
Epoch 6/10
20/20 - 3s - loss: 303.9001 - loglik: -3.0254e+02 - logprior: -1.3646e+00
Epoch 7/10
20/20 - 3s - loss: 303.9392 - loglik: -3.0259e+02 - logprior: -1.3515e+00
Fitted a model with MAP estimate = -288.7142
expansions: [(5, 1), (8, 1), (10, 2), (12, 2), (18, 1), (20, 1), (23, 2), (37, 2), (39, 1), (40, 1), (42, 1), (46, 1), (57, 1), (58, 2), (59, 1), (60, 1), (61, 2), (62, 1), (75, 1), (76, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1), (97, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 302.6513 - loglik: -3.0007e+02 - logprior: -2.5850e+00
Epoch 2/2
40/40 - 5s - loss: 292.3154 - loglik: -2.9151e+02 - logprior: -8.1025e-01
Fitted a model with MAP estimate = -277.8631
expansions: [(101, 1)]
discards: [ 12  30  46  74  82 107]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 293.9394 - loglik: -2.9213e+02 - logprior: -1.8069e+00
Epoch 2/2
40/40 - 5s - loss: 290.9140 - loglik: -2.9025e+02 - logprior: -6.6253e-01
Fitted a model with MAP estimate = -278.4908
expansions: []
discards: [93]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 275.0853 - loglik: -2.7404e+02 - logprior: -1.0414e+00
Epoch 2/10
57/57 - 6s - loss: 273.4700 - loglik: -2.7276e+02 - logprior: -7.0982e-01
Epoch 3/10
57/57 - 6s - loss: 272.3106 - loglik: -2.7162e+02 - logprior: -6.8619e-01
Epoch 4/10
57/57 - 6s - loss: 271.7454 - loglik: -2.7108e+02 - logprior: -6.6518e-01
Epoch 5/10
57/57 - 6s - loss: 271.2046 - loglik: -2.7057e+02 - logprior: -6.3686e-01
Epoch 6/10
57/57 - 6s - loss: 271.8227 - loglik: -2.7120e+02 - logprior: -6.1909e-01
Fitted a model with MAP estimate = -271.1789
Time for alignment: 129.8799
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 373.1480 - loglik: -3.7035e+02 - logprior: -2.8003e+00
Epoch 2/10
20/20 - 3s - loss: 328.8398 - loglik: -3.2759e+02 - logprior: -1.2489e+00
Epoch 3/10
20/20 - 3s - loss: 309.3818 - loglik: -3.0790e+02 - logprior: -1.4861e+00
Epoch 4/10
20/20 - 3s - loss: 305.5201 - loglik: -3.0414e+02 - logprior: -1.3827e+00
Epoch 5/10
20/20 - 3s - loss: 303.8970 - loglik: -3.0251e+02 - logprior: -1.3849e+00
Epoch 6/10
20/20 - 3s - loss: 303.9752 - loglik: -3.0261e+02 - logprior: -1.3635e+00
Fitted a model with MAP estimate = -288.4967
expansions: [(5, 1), (8, 1), (10, 2), (12, 2), (19, 1), (24, 2), (31, 1), (37, 2), (39, 1), (40, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 2), (61, 2), (64, 2), (76, 1), (78, 1), (81, 2), (83, 1), (86, 1), (94, 1), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 303.5314 - loglik: -3.0093e+02 - logprior: -2.6054e+00
Epoch 2/2
40/40 - 5s - loss: 292.6203 - loglik: -2.9179e+02 - logprior: -8.3136e-01
Fitted a model with MAP estimate = -278.7484
expansions: [(103, 1), (137, 2)]
discards: [ 12  30  46  74  79  82  88 108]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 294.4631 - loglik: -2.9253e+02 - logprior: -1.9365e+00
Epoch 2/2
40/40 - 5s - loss: 291.1034 - loglik: -2.9038e+02 - logprior: -7.1917e-01
Fitted a model with MAP estimate = -278.5445
expansions: [(132, 2)]
discards: [130 131]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 275.8403 - loglik: -2.7482e+02 - logprior: -1.0195e+00
Epoch 2/10
57/57 - 6s - loss: 272.8068 - loglik: -2.7218e+02 - logprior: -6.2439e-01
Epoch 3/10
57/57 - 7s - loss: 273.0672 - loglik: -2.7243e+02 - logprior: -6.3275e-01
Fitted a model with MAP estimate = -271.9019
Time for alignment: 107.4917
Computed alignments with likelihoods: ['-271.2410', '-271.4630', '-272.0051', '-271.1789', '-271.9019']
Best model has likelihood: -271.1789  (prior= -0.6012 )
time for generating output: 0.3053
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.7135570469798658
Training of 5 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1f72bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2cb80a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1657910>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 688.8391 - loglik: -6.7927e+02 - logprior: -9.5702e+00
Epoch 2/10
19/19 - 6s - loss: 621.2219 - loglik: -6.2066e+02 - logprior: -5.6038e-01
Epoch 3/10
19/19 - 6s - loss: 593.5433 - loglik: -5.9301e+02 - logprior: -5.2885e-01
Epoch 4/10
19/19 - 6s - loss: 587.5684 - loglik: -5.8735e+02 - logprior: -2.1852e-01
Epoch 5/10
19/19 - 6s - loss: 583.5994 - loglik: -5.8359e+02 - logprior: -1.1695e-02
Epoch 6/10
19/19 - 6s - loss: 585.5461 - loglik: -5.8561e+02 - logprior: 0.0591
Fitted a model with MAP estimate = -584.3503
expansions: [(25, 1), (27, 1), (28, 1), (30, 4), (61, 1), (63, 4), (104, 1), (117, 1), (119, 1), (134, 1), (135, 1), (164, 1), (165, 6), (183, 1), (184, 2), (198, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 230 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 596.7454 - loglik: -5.8746e+02 - logprior: -9.2840e+00
Epoch 2/2
19/19 - 7s - loss: 580.4467 - loglik: -5.7973e+02 - logprior: -7.1608e-01
Fitted a model with MAP estimate = -577.8517
expansions: [(74, 1), (102, 5), (103, 2), (211, 1), (212, 1)]
discards: [ 33  34 193 194 225 226 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 591.3270 - loglik: -5.8260e+02 - logprior: -8.7298e+00
Epoch 2/2
19/19 - 7s - loss: 578.8553 - loglik: -5.7887e+02 - logprior: 0.0147
Fitted a model with MAP estimate = -576.3294
expansions: [(231, 8)]
discards: [176]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 590.8876 - loglik: -5.8232e+02 - logprior: -8.5650e+00
Epoch 2/10
19/19 - 8s - loss: 578.8183 - loglik: -5.7892e+02 - logprior: 0.0994
Epoch 3/10
19/19 - 8s - loss: 575.0226 - loglik: -5.7610e+02 - logprior: 1.0761
Epoch 4/10
19/19 - 8s - loss: 574.8546 - loglik: -5.7639e+02 - logprior: 1.5318
Epoch 5/10
19/19 - 8s - loss: 573.8021 - loglik: -5.7560e+02 - logprior: 1.7963
Epoch 6/10
19/19 - 8s - loss: 574.7669 - loglik: -5.7672e+02 - logprior: 1.9532
Fitted a model with MAP estimate = -573.0797
Time for alignment: 151.0811
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 688.8716 - loglik: -6.7929e+02 - logprior: -9.5852e+00
Epoch 2/10
19/19 - 6s - loss: 624.3704 - loglik: -6.2385e+02 - logprior: -5.1626e-01
Epoch 3/10
19/19 - 6s - loss: 593.5275 - loglik: -5.9302e+02 - logprior: -5.1224e-01
Epoch 4/10
19/19 - 6s - loss: 587.4910 - loglik: -5.8716e+02 - logprior: -3.2851e-01
Epoch 5/10
19/19 - 6s - loss: 584.6943 - loglik: -5.8457e+02 - logprior: -1.2438e-01
Epoch 6/10
19/19 - 6s - loss: 586.3514 - loglik: -5.8636e+02 - logprior: 0.0100
Fitted a model with MAP estimate = -584.9220
expansions: [(25, 1), (28, 2), (30, 3), (64, 5), (91, 2), (103, 1), (118, 1), (120, 1), (135, 1), (136, 1), (137, 1), (149, 2), (151, 1), (152, 1), (164, 1), (165, 2), (166, 5), (183, 1), (184, 2), (198, 6)]
discards: [169 170]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 597.1901 - loglik: -5.8798e+02 - logprior: -9.2054e+00
Epoch 2/2
19/19 - 8s - loss: 580.6382 - loglik: -5.7985e+02 - logprior: -7.8810e-01
Fitted a model with MAP estimate = -577.4039
expansions: [(216, 1), (217, 1)]
discards: [ 33  34  35 177 178 179 200 201 202 203 204 230 231 232 233 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 221 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 596.1255 - loglik: -5.8729e+02 - logprior: -8.8355e+00
Epoch 2/2
19/19 - 7s - loss: 580.9929 - loglik: -5.8079e+02 - logprior: -2.0101e-01
Fitted a model with MAP estimate = -579.3062
expansions: [(99, 3), (185, 6), (186, 1)]
discards: [191]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 590.2886 - loglik: -5.8180e+02 - logprior: -8.4877e+00
Epoch 2/10
19/19 - 7s - loss: 577.8149 - loglik: -5.7814e+02 - logprior: 0.3216
Epoch 3/10
19/19 - 7s - loss: 575.3760 - loglik: -5.7676e+02 - logprior: 1.3845
Epoch 4/10
19/19 - 7s - loss: 575.1024 - loglik: -5.7698e+02 - logprior: 1.8739
Epoch 5/10
19/19 - 7s - loss: 573.1945 - loglik: -5.7533e+02 - logprior: 2.1339
Epoch 6/10
19/19 - 7s - loss: 571.6961 - loglik: -5.7403e+02 - logprior: 2.3326
Epoch 7/10
19/19 - 7s - loss: 573.0401 - loglik: -5.7553e+02 - logprior: 2.4938
Fitted a model with MAP estimate = -572.2998
Time for alignment: 153.3239
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 688.7327 - loglik: -6.7915e+02 - logprior: -9.5860e+00
Epoch 2/10
19/19 - 6s - loss: 622.2602 - loglik: -6.2174e+02 - logprior: -5.2240e-01
Epoch 3/10
19/19 - 6s - loss: 593.4826 - loglik: -5.9299e+02 - logprior: -4.8848e-01
Epoch 4/10
19/19 - 6s - loss: 588.7297 - loglik: -5.8856e+02 - logprior: -1.7082e-01
Epoch 5/10
19/19 - 6s - loss: 585.8571 - loglik: -5.8588e+02 - logprior: 0.0277
Epoch 6/10
19/19 - 6s - loss: 585.7864 - loglik: -5.8593e+02 - logprior: 0.1447
Epoch 7/10
19/19 - 6s - loss: 586.3823 - loglik: -5.8656e+02 - logprior: 0.1769
Fitted a model with MAP estimate = -585.3468
expansions: [(25, 1), (28, 1), (44, 6), (45, 1), (65, 3), (100, 4), (119, 1), (121, 1), (136, 2), (137, 2), (148, 2), (150, 4), (151, 1), (152, 1), (163, 2), (164, 1), (165, 6), (184, 2), (185, 2), (198, 5)]
discards: [169]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 596.4313 - loglik: -5.8728e+02 - logprior: -9.1474e+00
Epoch 2/2
19/19 - 8s - loss: 577.3740 - loglik: -5.7657e+02 - logprior: -8.0521e-01
Fitted a model with MAP estimate = -574.7316
expansions: [(120, 1), (227, 1)]
discards: [ 43 114 157 171 174 175 195 208 209 210 211 212 240 241 242 243 244]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 592.0164 - loglik: -5.8329e+02 - logprior: -8.7216e+00
Epoch 2/2
19/19 - 7s - loss: 576.3657 - loglik: -5.7627e+02 - logprior: -9.3554e-02
Fitted a model with MAP estimate = -575.2149
expansions: [(50, 1)]
discards: [194 198 199]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 588.7202 - loglik: -5.8024e+02 - logprior: -8.4764e+00
Epoch 2/10
19/19 - 7s - loss: 577.7356 - loglik: -5.7802e+02 - logprior: 0.2871
Epoch 3/10
19/19 - 7s - loss: 574.7670 - loglik: -5.7607e+02 - logprior: 1.2980
Epoch 4/10
19/19 - 7s - loss: 573.2380 - loglik: -5.7498e+02 - logprior: 1.7380
Epoch 5/10
19/19 - 7s - loss: 572.2443 - loglik: -5.7424e+02 - logprior: 2.0004
Epoch 6/10
19/19 - 7s - loss: 573.4213 - loglik: -5.7561e+02 - logprior: 2.1844
Fitted a model with MAP estimate = -572.3149
Time for alignment: 153.9920
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 689.8034 - loglik: -6.8022e+02 - logprior: -9.5816e+00
Epoch 2/10
19/19 - 6s - loss: 623.0762 - loglik: -6.2248e+02 - logprior: -5.9942e-01
Epoch 3/10
19/19 - 6s - loss: 592.4651 - loglik: -5.9180e+02 - logprior: -6.6664e-01
Epoch 4/10
19/19 - 6s - loss: 587.1893 - loglik: -5.8669e+02 - logprior: -4.9602e-01
Epoch 5/10
19/19 - 6s - loss: 586.2348 - loglik: -5.8588e+02 - logprior: -3.5125e-01
Epoch 6/10
19/19 - 6s - loss: 584.2488 - loglik: -5.8404e+02 - logprior: -2.1137e-01
Epoch 7/10
19/19 - 6s - loss: 584.1872 - loglik: -5.8405e+02 - logprior: -1.3257e-01
Epoch 8/10
19/19 - 6s - loss: 586.5544 - loglik: -5.8644e+02 - logprior: -1.1712e-01
Fitted a model with MAP estimate = -584.1388
expansions: [(25, 1), (26, 1), (27, 1), (28, 1), (30, 3), (64, 3), (91, 2), (92, 2), (93, 1), (101, 7), (103, 1), (104, 1), (117, 1), (119, 1), (134, 1), (135, 1), (146, 2), (149, 4), (150, 1), (163, 1), (166, 6), (182, 1), (184, 2), (185, 2), (198, 5)]
discards: [170 171 172 173 174 175 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 599.7837 - loglik: -5.9056e+02 - logprior: -9.2280e+00
Epoch 2/2
19/19 - 8s - loss: 581.0297 - loglik: -5.8025e+02 - logprior: -7.8126e-01
Fitted a model with MAP estimate = -576.6032
expansions: [(74, 2), (202, 6), (223, 1)]
discards: [ 31  34  35  36 119 120 121 122 175 179 180 203 204 205 206 207 208 236
 237 238 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 591.5278 - loglik: -5.8277e+02 - logprior: -8.7539e+00
Epoch 2/2
19/19 - 7s - loss: 578.4482 - loglik: -5.7830e+02 - logprior: -1.5123e-01
Fitted a model with MAP estimate = -575.2714
expansions: []
discards: [100 118 119 120 121 194 195 196 197 198 199]
Re-initialized the encoder parameters.
Fitting a model of length 217 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 593.2437 - loglik: -5.8472e+02 - logprior: -8.5232e+00
Epoch 2/10
19/19 - 7s - loss: 580.7180 - loglik: -5.8091e+02 - logprior: 0.1903
Epoch 3/10
19/19 - 7s - loss: 577.7704 - loglik: -5.7897e+02 - logprior: 1.1988
Epoch 4/10
19/19 - 7s - loss: 577.5041 - loglik: -5.7915e+02 - logprior: 1.6466
Epoch 5/10
19/19 - 7s - loss: 576.0420 - loglik: -5.7793e+02 - logprior: 1.8906
Epoch 6/10
19/19 - 7s - loss: 576.5224 - loglik: -5.7857e+02 - logprior: 2.0514
Fitted a model with MAP estimate = -575.6615
Time for alignment: 157.0953
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 689.0114 - loglik: -6.7944e+02 - logprior: -9.5737e+00
Epoch 2/10
19/19 - 6s - loss: 620.3301 - loglik: -6.1971e+02 - logprior: -6.1644e-01
Epoch 3/10
19/19 - 6s - loss: 591.6958 - loglik: -5.9107e+02 - logprior: -6.2414e-01
Epoch 4/10
19/19 - 6s - loss: 587.3756 - loglik: -5.8687e+02 - logprior: -5.1030e-01
Epoch 5/10
19/19 - 6s - loss: 586.1437 - loglik: -5.8591e+02 - logprior: -2.3535e-01
Epoch 6/10
19/19 - 6s - loss: 585.3882 - loglik: -5.8522e+02 - logprior: -1.6957e-01
Epoch 7/10
19/19 - 6s - loss: 584.7716 - loglik: -5.8468e+02 - logprior: -9.2423e-02
Epoch 8/10
19/19 - 6s - loss: 583.8690 - loglik: -5.8380e+02 - logprior: -6.7545e-02
Epoch 9/10
19/19 - 6s - loss: 584.7155 - loglik: -5.8466e+02 - logprior: -5.3450e-02
Fitted a model with MAP estimate = -583.9176
expansions: [(25, 1), (28, 2), (64, 2), (91, 6), (92, 2), (103, 1), (117, 3), (118, 1), (134, 1), (135, 1), (144, 1), (149, 1), (150, 4), (151, 1), (166, 4), (182, 1), (184, 2), (185, 2), (198, 5)]
discards: [170 171 172 173 174 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 600.2172 - loglik: -5.9101e+02 - logprior: -9.2118e+00
Epoch 2/2
19/19 - 7s - loss: 582.7400 - loglik: -5.8186e+02 - logprior: -8.8140e-01
Fitted a model with MAP estimate = -579.2210
expansions: [(195, 3), (198, 4), (200, 1), (201, 1), (213, 1)]
discards: [133 172 173 179 180 181 182 183 184 185 188 189 226 227 228 229 230]
Re-initialized the encoder parameters.
Fitting a model of length 224 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 595.7841 - loglik: -5.8716e+02 - logprior: -8.6201e+00
Epoch 2/2
19/19 - 7s - loss: 581.8198 - loglik: -5.8182e+02 - logprior: -4.2349e-03
Fitted a model with MAP estimate = -579.2744
expansions: [(176, 5), (178, 1), (179, 1)]
discards: [190 191]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 591.4212 - loglik: -5.8317e+02 - logprior: -8.2522e+00
Epoch 2/10
19/19 - 7s - loss: 579.2798 - loglik: -5.7967e+02 - logprior: 0.3923
Epoch 3/10
19/19 - 7s - loss: 574.8633 - loglik: -5.7635e+02 - logprior: 1.4858
Epoch 4/10
19/19 - 7s - loss: 575.3111 - loglik: -5.7727e+02 - logprior: 1.9561
Fitted a model with MAP estimate = -573.9696
Time for alignment: 148.2407
Computed alignments with likelihoods: ['-573.0797', '-572.2998', '-572.3149', '-575.2714', '-573.9696']
Best model has likelihood: -572.2998  (prior= 2.5738 )
time for generating output: 0.2714
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.7540709812108559
Training of 5 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebbb1eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaf4d40af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac0767910>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.2998 - loglik: -1.5106e+02 - logprior: -3.2373e+00
Epoch 2/10
19/19 - 1s - loss: 122.2729 - loglik: -1.2073e+02 - logprior: -1.5402e+00
Epoch 3/10
19/19 - 1s - loss: 106.9996 - loglik: -1.0538e+02 - logprior: -1.6167e+00
Epoch 4/10
19/19 - 1s - loss: 103.0950 - loglik: -1.0147e+02 - logprior: -1.6255e+00
Epoch 5/10
19/19 - 1s - loss: 101.9888 - loglik: -1.0041e+02 - logprior: -1.5764e+00
Epoch 6/10
19/19 - 1s - loss: 101.8195 - loglik: -1.0026e+02 - logprior: -1.5574e+00
Epoch 7/10
19/19 - 1s - loss: 101.3065 - loglik: -9.9759e+01 - logprior: -1.5472e+00
Epoch 8/10
19/19 - 1s - loss: 101.6233 - loglik: -1.0008e+02 - logprior: -1.5386e+00
Fitted a model with MAP estimate = -96.8840
expansions: [(5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 2), (28, 1), (29, 1), (30, 2), (31, 2), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.4882 - loglik: -1.0118e+02 - logprior: -3.3120e+00
Epoch 2/2
19/19 - 1s - loss: 95.7470 - loglik: -9.4326e+01 - logprior: -1.4212e+00
Fitted a model with MAP estimate = -90.7553
expansions: []
discards: [20 31 42 44]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 97.9413 - loglik: -9.4766e+01 - logprior: -3.1758e+00
Epoch 2/2
19/19 - 1s - loss: 94.9847 - loglik: -9.3714e+01 - logprior: -1.2709e+00
Fitted a model with MAP estimate = -90.5988
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.3531 - loglik: -8.9768e+01 - logprior: -2.5852e+00
Epoch 2/10
21/21 - 1s - loss: 89.8940 - loglik: -8.8627e+01 - logprior: -1.2665e+00
Epoch 3/10
21/21 - 1s - loss: 89.3382 - loglik: -8.8168e+01 - logprior: -1.1698e+00
Epoch 4/10
21/21 - 1s - loss: 88.5661 - loglik: -8.7411e+01 - logprior: -1.1549e+00
Epoch 5/10
21/21 - 1s - loss: 88.1271 - loglik: -8.6948e+01 - logprior: -1.1795e+00
Epoch 6/10
21/21 - 1s - loss: 87.8127 - loglik: -8.6622e+01 - logprior: -1.1906e+00
Epoch 7/10
21/21 - 1s - loss: 87.2082 - loglik: -8.6038e+01 - logprior: -1.1697e+00
Epoch 8/10
21/21 - 1s - loss: 87.3873 - loglik: -8.6220e+01 - logprior: -1.1674e+00
Fitted a model with MAP estimate = -87.2289
Time for alignment: 45.1500
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.2473 - loglik: -1.5102e+02 - logprior: -3.2283e+00
Epoch 2/10
19/19 - 1s - loss: 119.8465 - loglik: -1.1832e+02 - logprior: -1.5284e+00
Epoch 3/10
19/19 - 1s - loss: 106.2636 - loglik: -1.0465e+02 - logprior: -1.6123e+00
Epoch 4/10
19/19 - 1s - loss: 103.7205 - loglik: -1.0215e+02 - logprior: -1.5669e+00
Epoch 5/10
19/19 - 1s - loss: 102.5865 - loglik: -1.0105e+02 - logprior: -1.5318e+00
Epoch 6/10
19/19 - 1s - loss: 102.3623 - loglik: -1.0085e+02 - logprior: -1.5118e+00
Epoch 7/10
19/19 - 1s - loss: 102.0711 - loglik: -1.0057e+02 - logprior: -1.4989e+00
Epoch 8/10
19/19 - 1s - loss: 101.5995 - loglik: -1.0010e+02 - logprior: -1.5006e+00
Epoch 9/10
19/19 - 1s - loss: 101.4230 - loglik: -9.9923e+01 - logprior: -1.4997e+00
Epoch 10/10
19/19 - 1s - loss: 101.7228 - loglik: -1.0022e+02 - logprior: -1.5029e+00
Fitted a model with MAP estimate = -97.2690
expansions: [(5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.3076 - loglik: -1.0100e+02 - logprior: -3.3033e+00
Epoch 2/2
19/19 - 1s - loss: 95.6953 - loglik: -9.4269e+01 - logprior: -1.4263e+00
Fitted a model with MAP estimate = -90.7095
expansions: []
discards: [20 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.8525 - loglik: -9.4682e+01 - logprior: -3.1704e+00
Epoch 2/2
19/19 - 1s - loss: 94.9316 - loglik: -9.3663e+01 - logprior: -1.2682e+00
Fitted a model with MAP estimate = -90.6690
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 92.4357 - loglik: -8.9861e+01 - logprior: -2.5750e+00
Epoch 2/10
21/21 - 1s - loss: 89.8574 - loglik: -8.8605e+01 - logprior: -1.2520e+00
Epoch 3/10
21/21 - 1s - loss: 89.2744 - loglik: -8.8104e+01 - logprior: -1.1703e+00
Epoch 4/10
21/21 - 1s - loss: 88.4880 - loglik: -8.7331e+01 - logprior: -1.1567e+00
Epoch 5/10
21/21 - 1s - loss: 87.6327 - loglik: -8.6457e+01 - logprior: -1.1753e+00
Epoch 6/10
21/21 - 1s - loss: 88.3813 - loglik: -8.7205e+01 - logprior: -1.1767e+00
Fitted a model with MAP estimate = -87.5037
Time for alignment: 43.3534
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.2766 - loglik: -1.5104e+02 - logprior: -3.2393e+00
Epoch 2/10
19/19 - 1s - loss: 120.4047 - loglik: -1.1886e+02 - logprior: -1.5476e+00
Epoch 3/10
19/19 - 1s - loss: 105.6775 - loglik: -1.0407e+02 - logprior: -1.6074e+00
Epoch 4/10
19/19 - 1s - loss: 102.8326 - loglik: -1.0120e+02 - logprior: -1.6311e+00
Epoch 5/10
19/19 - 1s - loss: 102.2924 - loglik: -1.0072e+02 - logprior: -1.5771e+00
Epoch 6/10
19/19 - 1s - loss: 101.7771 - loglik: -1.0022e+02 - logprior: -1.5536e+00
Epoch 7/10
19/19 - 1s - loss: 101.5268 - loglik: -9.9982e+01 - logprior: -1.5443e+00
Epoch 8/10
19/19 - 1s - loss: 101.2652 - loglik: -9.9730e+01 - logprior: -1.5348e+00
Epoch 9/10
19/19 - 1s - loss: 100.9007 - loglik: -9.9360e+01 - logprior: -1.5403e+00
Epoch 10/10
19/19 - 1s - loss: 100.9246 - loglik: -9.9383e+01 - logprior: -1.5413e+00
Fitted a model with MAP estimate = -96.6509
expansions: [(5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 2), (28, 1), (29, 1), (30, 2), (31, 2), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 104.6005 - loglik: -1.0129e+02 - logprior: -3.3078e+00
Epoch 2/2
19/19 - 1s - loss: 95.7799 - loglik: -9.4357e+01 - logprior: -1.4230e+00
Fitted a model with MAP estimate = -90.7709
expansions: []
discards: [20 31 42 44]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.9624 - loglik: -9.4792e+01 - logprior: -3.1700e+00
Epoch 2/2
19/19 - 1s - loss: 95.0188 - loglik: -9.3745e+01 - logprior: -1.2738e+00
Fitted a model with MAP estimate = -90.6361
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.3121 - loglik: -8.9720e+01 - logprior: -2.5919e+00
Epoch 2/10
21/21 - 1s - loss: 90.0817 - loglik: -8.8828e+01 - logprior: -1.2538e+00
Epoch 3/10
21/21 - 1s - loss: 89.4080 - loglik: -8.8239e+01 - logprior: -1.1693e+00
Epoch 4/10
21/21 - 1s - loss: 88.1102 - loglik: -8.6959e+01 - logprior: -1.1512e+00
Epoch 5/10
21/21 - 1s - loss: 88.2656 - loglik: -8.7087e+01 - logprior: -1.1789e+00
Fitted a model with MAP estimate = -87.7668
Time for alignment: 40.8269
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.0931 - loglik: -1.5086e+02 - logprior: -3.2319e+00
Epoch 2/10
19/19 - 1s - loss: 120.0483 - loglik: -1.1852e+02 - logprior: -1.5305e+00
Epoch 3/10
19/19 - 1s - loss: 106.5600 - loglik: -1.0497e+02 - logprior: -1.5935e+00
Epoch 4/10
19/19 - 1s - loss: 103.6617 - loglik: -1.0212e+02 - logprior: -1.5389e+00
Epoch 5/10
19/19 - 1s - loss: 103.0535 - loglik: -1.0155e+02 - logprior: -1.5008e+00
Epoch 6/10
19/19 - 1s - loss: 102.5241 - loglik: -1.0103e+02 - logprior: -1.4908e+00
Epoch 7/10
19/19 - 1s - loss: 102.2311 - loglik: -1.0074e+02 - logprior: -1.4865e+00
Epoch 8/10
19/19 - 1s - loss: 101.7522 - loglik: -1.0027e+02 - logprior: -1.4853e+00
Epoch 9/10
19/19 - 1s - loss: 101.5978 - loglik: -1.0012e+02 - logprior: -1.4824e+00
Epoch 10/10
19/19 - 1s - loss: 101.2597 - loglik: -9.9776e+01 - logprior: -1.4837e+00
Fitted a model with MAP estimate = -97.4244
expansions: [(5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.9017 - loglik: -1.0059e+02 - logprior: -3.3129e+00
Epoch 2/2
19/19 - 1s - loss: 95.6509 - loglik: -9.4207e+01 - logprior: -1.4435e+00
Fitted a model with MAP estimate = -90.6960
expansions: []
discards: [20 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 97.8681 - loglik: -9.4690e+01 - logprior: -3.1785e+00
Epoch 2/2
19/19 - 1s - loss: 95.0229 - loglik: -9.3735e+01 - logprior: -1.2878e+00
Fitted a model with MAP estimate = -90.5384
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.3842 - loglik: -8.9797e+01 - logprior: -2.5875e+00
Epoch 2/10
21/21 - 1s - loss: 90.0934 - loglik: -8.8831e+01 - logprior: -1.2622e+00
Epoch 3/10
21/21 - 1s - loss: 89.1656 - loglik: -8.7985e+01 - logprior: -1.1805e+00
Epoch 4/10
21/21 - 1s - loss: 88.5675 - loglik: -8.7394e+01 - logprior: -1.1731e+00
Epoch 5/10
21/21 - 1s - loss: 87.8345 - loglik: -8.6649e+01 - logprior: -1.1854e+00
Epoch 6/10
21/21 - 1s - loss: 87.6355 - loglik: -8.6457e+01 - logprior: -1.1782e+00
Epoch 7/10
21/21 - 1s - loss: 87.4427 - loglik: -8.6276e+01 - logprior: -1.1671e+00
Epoch 8/10
21/21 - 1s - loss: 87.2769 - loglik: -8.6123e+01 - logprior: -1.1541e+00
Epoch 9/10
21/21 - 1s - loss: 87.3361 - loglik: -8.6189e+01 - logprior: -1.1468e+00
Fitted a model with MAP estimate = -87.1188
Time for alignment: 47.0093
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.2962 - loglik: -1.5106e+02 - logprior: -3.2350e+00
Epoch 2/10
19/19 - 1s - loss: 121.1320 - loglik: -1.1960e+02 - logprior: -1.5327e+00
Epoch 3/10
19/19 - 1s - loss: 106.6975 - loglik: -1.0506e+02 - logprior: -1.6399e+00
Epoch 4/10
19/19 - 1s - loss: 103.9063 - loglik: -1.0229e+02 - logprior: -1.6122e+00
Epoch 5/10
19/19 - 1s - loss: 103.2358 - loglik: -1.0167e+02 - logprior: -1.5610e+00
Epoch 6/10
19/19 - 1s - loss: 102.6506 - loglik: -1.0111e+02 - logprior: -1.5424e+00
Epoch 7/10
19/19 - 1s - loss: 102.4581 - loglik: -1.0093e+02 - logprior: -1.5318e+00
Epoch 8/10
19/19 - 1s - loss: 102.0455 - loglik: -1.0052e+02 - logprior: -1.5266e+00
Epoch 9/10
19/19 - 1s - loss: 102.1858 - loglik: -1.0065e+02 - logprior: -1.5321e+00
Fitted a model with MAP estimate = -97.7251
expansions: [(5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 2), (28, 1), (29, 2), (30, 2), (31, 2), (32, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 105.6934 - loglik: -1.0237e+02 - logprior: -3.3233e+00
Epoch 2/2
19/19 - 1s - loss: 95.9902 - loglik: -9.4541e+01 - logprior: -1.4492e+00
Fitted a model with MAP estimate = -90.9104
expansions: []
discards: [20 31 39 43 45]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 98.0579 - loglik: -9.4883e+01 - logprior: -3.1745e+00
Epoch 2/2
19/19 - 1s - loss: 95.0295 - loglik: -9.3757e+01 - logprior: -1.2725e+00
Fitted a model with MAP estimate = -90.6128
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 92.2208 - loglik: -8.9630e+01 - logprior: -2.5910e+00
Epoch 2/10
21/21 - 1s - loss: 89.9493 - loglik: -8.8686e+01 - logprior: -1.2630e+00
Epoch 3/10
21/21 - 1s - loss: 89.4197 - loglik: -8.8247e+01 - logprior: -1.1731e+00
Epoch 4/10
21/21 - 1s - loss: 88.6047 - loglik: -8.7450e+01 - logprior: -1.1547e+00
Epoch 5/10
21/21 - 1s - loss: 87.7802 - loglik: -8.6601e+01 - logprior: -1.1789e+00
Epoch 6/10
21/21 - 1s - loss: 87.7529 - loglik: -8.6574e+01 - logprior: -1.1793e+00
Epoch 7/10
21/21 - 1s - loss: 87.8851 - loglik: -8.6718e+01 - logprior: -1.1670e+00
Fitted a model with MAP estimate = -87.3658
Time for alignment: 42.8713
Computed alignments with likelihoods: ['-87.2289', '-87.5037', '-87.7668', '-87.1188', '-87.3658']
Best model has likelihood: -87.1188  (prior= -1.1127 )
time for generating output: 0.1099
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.9452672247263362
Training of 5 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac8d39cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feafd759fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d294d90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 417.3200 - loglik: -4.1263e+02 - logprior: -4.6901e+00
Epoch 2/10
27/27 - 4s - loss: 331.9135 - loglik: -3.3003e+02 - logprior: -1.8841e+00
Epoch 3/10
27/27 - 4s - loss: 316.6840 - loglik: -3.1474e+02 - logprior: -1.9475e+00
Epoch 4/10
27/27 - 4s - loss: 313.8091 - loglik: -3.1191e+02 - logprior: -1.8997e+00
Epoch 5/10
27/27 - 4s - loss: 313.0741 - loglik: -3.1118e+02 - logprior: -1.8917e+00
Epoch 6/10
27/27 - 4s - loss: 312.2353 - loglik: -3.1034e+02 - logprior: -1.8967e+00
Epoch 7/10
27/27 - 4s - loss: 311.8549 - loglik: -3.0996e+02 - logprior: -1.8924e+00
Epoch 8/10
27/27 - 4s - loss: 312.2240 - loglik: -3.1031e+02 - logprior: -1.9128e+00
Fitted a model with MAP estimate = -311.4716
expansions: [(3, 1), (9, 2), (21, 1), (25, 2), (26, 1), (35, 1), (36, 2), (37, 4), (38, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 1), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (83, 2), (100, 1), (101, 1), (102, 1), (103, 2), (104, 2), (105, 1), (113, 1), (114, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 162 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 303.7906 - loglik: -2.9938e+02 - logprior: -4.4141e+00
Epoch 2/2
27/27 - 4s - loss: 289.0067 - loglik: -2.8788e+02 - logprior: -1.1254e+00
Fitted a model with MAP estimate = -287.9281
expansions: []
discards: [ 29  47  48  66 111 137 139]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 294.9185 - loglik: -2.9082e+02 - logprior: -4.0967e+00
Epoch 2/2
27/27 - 4s - loss: 289.4628 - loglik: -2.8877e+02 - logprior: -6.8930e-01
Fitted a model with MAP estimate = -288.7341
expansions: []
discards: [10]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 294.4760 - loglik: -2.9049e+02 - logprior: -3.9883e+00
Epoch 2/10
27/27 - 4s - loss: 290.5414 - loglik: -2.8998e+02 - logprior: -5.5840e-01
Epoch 3/10
27/27 - 4s - loss: 289.0793 - loglik: -2.8882e+02 - logprior: -2.6015e-01
Epoch 4/10
27/27 - 4s - loss: 288.2692 - loglik: -2.8809e+02 - logprior: -1.7603e-01
Epoch 5/10
27/27 - 4s - loss: 288.4044 - loglik: -2.8834e+02 - logprior: -6.1707e-02
Fitted a model with MAP estimate = -287.8167
Time for alignment: 102.3351
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 417.2380 - loglik: -4.1254e+02 - logprior: -4.6955e+00
Epoch 2/10
27/27 - 4s - loss: 336.4375 - loglik: -3.3468e+02 - logprior: -1.7529e+00
Epoch 3/10
27/27 - 4s - loss: 318.3853 - loglik: -3.1659e+02 - logprior: -1.7907e+00
Epoch 4/10
27/27 - 4s - loss: 314.6207 - loglik: -3.1283e+02 - logprior: -1.7956e+00
Epoch 5/10
27/27 - 4s - loss: 312.4094 - loglik: -3.1063e+02 - logprior: -1.7746e+00
Epoch 6/10
27/27 - 4s - loss: 312.3242 - loglik: -3.1056e+02 - logprior: -1.7665e+00
Epoch 7/10
27/27 - 4s - loss: 311.9716 - loglik: -3.1021e+02 - logprior: -1.7588e+00
Epoch 8/10
27/27 - 4s - loss: 311.8755 - loglik: -3.1013e+02 - logprior: -1.7435e+00
Epoch 9/10
27/27 - 4s - loss: 312.0193 - loglik: -3.1027e+02 - logprior: -1.7443e+00
Fitted a model with MAP estimate = -311.6315
expansions: [(0, 2), (9, 2), (21, 1), (25, 2), (26, 1), (35, 1), (36, 1), (37, 4), (39, 1), (41, 2), (42, 1), (47, 1), (68, 1), (71, 1), (72, 1), (73, 1), (78, 1), (79, 1), (81, 1), (93, 2), (102, 1), (103, 2), (104, 2), (105, 1), (114, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 306.0292 - loglik: -2.9961e+02 - logprior: -6.4183e+00
Epoch 2/2
27/27 - 4s - loss: 290.1396 - loglik: -2.8881e+02 - logprior: -1.3273e+00
Fitted a model with MAP estimate = -288.4803
expansions: []
discards: [  0  12  30  47  48 133 135]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 299.1257 - loglik: -2.9277e+02 - logprior: -6.3557e+00
Epoch 2/2
27/27 - 4s - loss: 291.1441 - loglik: -2.8984e+02 - logprior: -1.3064e+00
Fitted a model with MAP estimate = -289.7531
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 294.4962 - loglik: -2.9025e+02 - logprior: -4.2466e+00
Epoch 2/10
27/27 - 4s - loss: 290.3017 - loglik: -2.8948e+02 - logprior: -8.1973e-01
Epoch 3/10
27/27 - 4s - loss: 289.6131 - loglik: -2.8908e+02 - logprior: -5.3412e-01
Epoch 4/10
27/27 - 4s - loss: 288.4108 - loglik: -2.8797e+02 - logprior: -4.4240e-01
Epoch 5/10
27/27 - 4s - loss: 288.7906 - loglik: -2.8845e+02 - logprior: -3.4152e-01
Fitted a model with MAP estimate = -288.0336
Time for alignment: 104.7377
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 416.9663 - loglik: -4.1226e+02 - logprior: -4.7028e+00
Epoch 2/10
27/27 - 4s - loss: 330.7195 - loglik: -3.2883e+02 - logprior: -1.8900e+00
Epoch 3/10
27/27 - 4s - loss: 316.9419 - loglik: -3.1510e+02 - logprior: -1.8459e+00
Epoch 4/10
27/27 - 4s - loss: 314.4856 - loglik: -3.1262e+02 - logprior: -1.8669e+00
Epoch 5/10
27/27 - 4s - loss: 312.4438 - loglik: -3.1054e+02 - logprior: -1.9042e+00
Epoch 6/10
27/27 - 4s - loss: 312.4021 - loglik: -3.1048e+02 - logprior: -1.9202e+00
Epoch 7/10
27/27 - 4s - loss: 311.5908 - loglik: -3.0967e+02 - logprior: -1.9236e+00
Epoch 8/10
27/27 - 4s - loss: 311.3242 - loglik: -3.0938e+02 - logprior: -1.9451e+00
Epoch 9/10
27/27 - 4s - loss: 311.2304 - loglik: -3.0927e+02 - logprior: -1.9646e+00
Epoch 10/10
27/27 - 4s - loss: 311.3976 - loglik: -3.0942e+02 - logprior: -1.9756e+00
Fitted a model with MAP estimate = -310.9410
expansions: [(0, 2), (10, 1), (19, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 3), (38, 1), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 1), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (79, 1), (81, 1), (101, 1), (103, 1), (104, 2), (105, 2), (106, 1), (108, 1), (113, 1), (114, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 162 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 307.5813 - loglik: -3.0103e+02 - logprior: -6.5541e+00
Epoch 2/2
27/27 - 4s - loss: 290.3131 - loglik: -2.8905e+02 - logprior: -1.2611e+00
Fitted a model with MAP estimate = -289.0196
expansions: [(43, 1)]
discards: [  0  30  47  48  67 137 139]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 299.5654 - loglik: -2.9341e+02 - logprior: -6.1548e+00
Epoch 2/2
27/27 - 4s - loss: 291.4920 - loglik: -2.9048e+02 - logprior: -1.0164e+00
Fitted a model with MAP estimate = -290.3963
expansions: []
discards: [45]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 295.8723 - loglik: -2.9182e+02 - logprior: -4.0484e+00
Epoch 2/10
27/27 - 4s - loss: 291.4372 - loglik: -2.9080e+02 - logprior: -6.3874e-01
Epoch 3/10
27/27 - 4s - loss: 289.9338 - loglik: -2.8958e+02 - logprior: -3.5642e-01
Epoch 4/10
27/27 - 4s - loss: 289.7714 - loglik: -2.8951e+02 - logprior: -2.6367e-01
Epoch 5/10
27/27 - 4s - loss: 288.5573 - loglik: -2.8839e+02 - logprior: -1.7211e-01
Epoch 6/10
27/27 - 4s - loss: 289.2282 - loglik: -2.8916e+02 - logprior: -6.3731e-02
Fitted a model with MAP estimate = -288.2924
Time for alignment: 114.3137
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 417.5560 - loglik: -4.1285e+02 - logprior: -4.7054e+00
Epoch 2/10
27/27 - 4s - loss: 334.2375 - loglik: -3.3243e+02 - logprior: -1.8065e+00
Epoch 3/10
27/27 - 4s - loss: 318.1266 - loglik: -3.1628e+02 - logprior: -1.8448e+00
Epoch 4/10
27/27 - 4s - loss: 313.7724 - loglik: -3.1190e+02 - logprior: -1.8773e+00
Epoch 5/10
27/27 - 3s - loss: 312.0719 - loglik: -3.1019e+02 - logprior: -1.8854e+00
Epoch 6/10
27/27 - 4s - loss: 311.0788 - loglik: -3.0918e+02 - logprior: -1.9033e+00
Epoch 7/10
27/27 - 4s - loss: 311.2426 - loglik: -3.0935e+02 - logprior: -1.8914e+00
Fitted a model with MAP estimate = -310.7599
expansions: [(0, 2), (9, 2), (18, 1), (24, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (48, 1), (68, 1), (69, 1), (72, 3), (73, 1), (78, 1), (79, 1), (81, 1), (100, 1), (101, 1), (102, 1), (103, 2), (104, 2), (105, 1), (113, 1), (114, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 305.6691 - loglik: -2.9931e+02 - logprior: -6.3561e+00
Epoch 2/2
27/27 - 4s - loss: 289.4923 - loglik: -2.8817e+02 - logprior: -1.3181e+00
Fitted a model with MAP estimate = -287.8827
expansions: [(43, 1)]
discards: [  0  12  29  46  47  48 135 137]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 299.5198 - loglik: -2.9313e+02 - logprior: -6.3938e+00
Epoch 2/2
27/27 - 4s - loss: 291.5537 - loglik: -2.9021e+02 - logprior: -1.3394e+00
Fitted a model with MAP estimate = -290.1136
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 294.7209 - loglik: -2.9046e+02 - logprior: -4.2626e+00
Epoch 2/10
27/27 - 4s - loss: 290.6013 - loglik: -2.8976e+02 - logprior: -8.4003e-01
Epoch 3/10
27/27 - 4s - loss: 289.5891 - loglik: -2.8903e+02 - logprior: -5.5485e-01
Epoch 4/10
27/27 - 4s - loss: 288.0396 - loglik: -2.8758e+02 - logprior: -4.6189e-01
Epoch 5/10
27/27 - 4s - loss: 288.6048 - loglik: -2.8824e+02 - logprior: -3.6343e-01
Fitted a model with MAP estimate = -288.0301
Time for alignment: 95.5841
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 415.8001 - loglik: -4.1109e+02 - logprior: -4.7144e+00
Epoch 2/10
27/27 - 4s - loss: 332.4112 - loglik: -3.3054e+02 - logprior: -1.8760e+00
Epoch 3/10
27/27 - 4s - loss: 320.3458 - loglik: -3.1857e+02 - logprior: -1.7792e+00
Epoch 4/10
27/27 - 4s - loss: 317.2983 - loglik: -3.1555e+02 - logprior: -1.7462e+00
Epoch 5/10
27/27 - 4s - loss: 316.1973 - loglik: -3.1446e+02 - logprior: -1.7376e+00
Epoch 6/10
27/27 - 4s - loss: 315.7347 - loglik: -3.1395e+02 - logprior: -1.7824e+00
Epoch 7/10
27/27 - 4s - loss: 315.1873 - loglik: -3.1339e+02 - logprior: -1.7967e+00
Epoch 8/10
27/27 - 4s - loss: 315.1635 - loglik: -3.1338e+02 - logprior: -1.7856e+00
Epoch 9/10
27/27 - 4s - loss: 314.9745 - loglik: -3.1319e+02 - logprior: -1.7843e+00
Epoch 10/10
27/27 - 4s - loss: 315.1092 - loglik: -3.1333e+02 - logprior: -1.7759e+00
Fitted a model with MAP estimate = -314.6733
expansions: [(0, 2), (9, 2), (21, 1), (25, 2), (26, 1), (36, 2), (37, 3), (38, 1), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (79, 1), (81, 1), (102, 1), (105, 2), (106, 1), (114, 1), (115, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 310.7011 - loglik: -3.0422e+02 - logprior: -6.4821e+00
Epoch 2/2
27/27 - 4s - loss: 294.9627 - loglik: -2.9359e+02 - logprior: -1.3683e+00
Fitted a model with MAP estimate = -293.3039
expansions: [(43, 1)]
discards: [  0  12  30  47  48  67  89 137]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 304.2834 - loglik: -2.9800e+02 - logprior: -6.2834e+00
Epoch 2/2
27/27 - 4s - loss: 297.1796 - loglik: -2.9598e+02 - logprior: -1.1957e+00
Fitted a model with MAP estimate = -295.2758
expansions: []
discards: [44]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 301.2035 - loglik: -2.9700e+02 - logprior: -4.2037e+00
Epoch 2/10
27/27 - 4s - loss: 295.5398 - loglik: -2.9473e+02 - logprior: -8.1154e-01
Epoch 3/10
27/27 - 4s - loss: 295.1927 - loglik: -2.9466e+02 - logprior: -5.3461e-01
Epoch 4/10
27/27 - 4s - loss: 294.7491 - loglik: -2.9431e+02 - logprior: -4.4071e-01
Epoch 5/10
27/27 - 4s - loss: 294.5955 - loglik: -2.9425e+02 - logprior: -3.4440e-01
Epoch 6/10
27/27 - 4s - loss: 292.8704 - loglik: -2.9262e+02 - logprior: -2.4893e-01
Epoch 7/10
27/27 - 4s - loss: 294.4286 - loglik: -2.9426e+02 - logprior: -1.6865e-01
Fitted a model with MAP estimate = -293.4322
Time for alignment: 116.8465
Computed alignments with likelihoods: ['-287.8167', '-288.0336', '-288.2924', '-287.8827', '-293.3039']
Best model has likelihood: -287.8167  (prior= -0.0135 )
time for generating output: 0.2510
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.4867109634551495
Training of 5 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feab79d60a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f369220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb27ef0b20>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 765.4724 - loglik: -7.6336e+02 - logprior: -2.1099e+00
Epoch 2/10
34/34 - 14s - loss: 641.0957 - loglik: -6.3958e+02 - logprior: -1.5167e+00
Epoch 3/10
34/34 - 14s - loss: 621.3457 - loglik: -6.1957e+02 - logprior: -1.7739e+00
Epoch 4/10
34/34 - 14s - loss: 617.0015 - loglik: -6.1522e+02 - logprior: -1.7858e+00
Epoch 5/10
34/34 - 14s - loss: 614.3854 - loglik: -6.1262e+02 - logprior: -1.7651e+00
Epoch 6/10
34/34 - 14s - loss: 614.6658 - loglik: -6.1290e+02 - logprior: -1.7681e+00
Fitted a model with MAP estimate = -613.6634
expansions: [(13, 1), (14, 1), (16, 2), (30, 1), (31, 2), (35, 1), (49, 1), (52, 2), (56, 5), (66, 1), (67, 1), (68, 1), (93, 1), (94, 1), (95, 2), (96, 2), (99, 1), (100, 1), (102, 1), (103, 1), (104, 1), (107, 1), (111, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (168, 3), (171, 2), (172, 1), (175, 1), (178, 1), (186, 3), (187, 2), (203, 1), (208, 1), (211, 1), (224, 1), (225, 1), (226, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 603.6401 - loglik: -6.0027e+02 - logprior: -3.3714e+00
Epoch 2/2
34/34 - 19s - loss: 585.2418 - loglik: -5.8368e+02 - logprior: -1.5638e+00
Fitted a model with MAP estimate = -580.6903
expansions: [(72, 2), (181, 1), (210, 1), (211, 1), (233, 3)]
discards: [ 34  60  66  67  68  69  70 116 118]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 586.8117 - loglik: -5.8470e+02 - logprior: -2.1098e+00
Epoch 2/2
34/34 - 19s - loss: 577.0301 - loglik: -5.7669e+02 - logprior: -3.4416e-01
Fitted a model with MAP estimate = -575.7743
expansions: [(64, 5), (230, 1)]
discards: [ 65  66 251 252 253]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 583.3994 - loglik: -5.8152e+02 - logprior: -1.8839e+00
Epoch 2/10
34/34 - 19s - loss: 577.0557 - loglik: -5.7689e+02 - logprior: -1.6167e-01
Epoch 3/10
34/34 - 19s - loss: 576.2017 - loglik: -5.7620e+02 - logprior: -1.0444e-03
Epoch 4/10
34/34 - 20s - loss: 573.6947 - loglik: -5.7382e+02 - logprior: 0.1265
Epoch 5/10
34/34 - 20s - loss: 573.7323 - loglik: -5.7394e+02 - logprior: 0.2042
Fitted a model with MAP estimate = -572.3180
Time for alignment: 366.7380
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 767.2062 - loglik: -7.6508e+02 - logprior: -2.1241e+00
Epoch 2/10
34/34 - 14s - loss: 642.4462 - loglik: -6.4088e+02 - logprior: -1.5694e+00
Epoch 3/10
34/34 - 14s - loss: 623.8266 - loglik: -6.2206e+02 - logprior: -1.7632e+00
Epoch 4/10
34/34 - 14s - loss: 619.1004 - loglik: -6.1732e+02 - logprior: -1.7853e+00
Epoch 5/10
34/34 - 14s - loss: 618.1782 - loglik: -6.1640e+02 - logprior: -1.7782e+00
Epoch 6/10
34/34 - 14s - loss: 618.0690 - loglik: -6.1628e+02 - logprior: -1.7851e+00
Epoch 7/10
34/34 - 14s - loss: 617.4889 - loglik: -6.1567e+02 - logprior: -1.8147e+00
Epoch 8/10
34/34 - 14s - loss: 617.5103 - loglik: -6.1570e+02 - logprior: -1.8132e+00
Fitted a model with MAP estimate = -616.8162
expansions: [(9, 1), (12, 1), (14, 1), (15, 2), (16, 1), (17, 4), (18, 1), (22, 1), (29, 1), (30, 2), (41, 1), (48, 1), (51, 2), (55, 4), (56, 1), (64, 1), (66, 1), (70, 1), (89, 1), (93, 1), (94, 2), (96, 2), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 4), (144, 1), (164, 4), (167, 1), (171, 1), (172, 2), (174, 1), (184, 2), (185, 1), (186, 3), (188, 1), (189, 1), (198, 1), (203, 1), (208, 1), (211, 1), (225, 1), (227, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 600.9625 - loglik: -5.9762e+02 - logprior: -3.3466e+00
Epoch 2/2
34/34 - 21s - loss: 580.0202 - loglik: -5.7856e+02 - logprior: -1.4595e+00
Fitted a model with MAP estimate = -574.9901
expansions: [(214, 1), (239, 1), (242, 1)]
discards: [ 23  24  41  67  74  75  76 122 189 269]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 585.6330 - loglik: -5.8350e+02 - logprior: -2.1343e+00
Epoch 2/2
34/34 - 20s - loss: 577.9583 - loglik: -5.7750e+02 - logprior: -4.6003e-01
Fitted a model with MAP estimate = -576.9779
expansions: [(70, 4), (204, 1)]
discards: [22]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 579.7737 - loglik: -5.7784e+02 - logprior: -1.9388e+00
Epoch 2/10
34/34 - 20s - loss: 570.2750 - loglik: -5.7001e+02 - logprior: -2.6853e-01
Epoch 3/10
34/34 - 20s - loss: 569.2244 - loglik: -5.6910e+02 - logprior: -1.2476e-01
Epoch 4/10
34/34 - 20s - loss: 568.1283 - loglik: -5.6811e+02 - logprior: -1.7104e-02
Epoch 5/10
34/34 - 20s - loss: 566.3434 - loglik: -5.6645e+02 - logprior: 0.1050
Epoch 6/10
34/34 - 21s - loss: 566.0817 - loglik: -5.6632e+02 - logprior: 0.2360
Epoch 7/10
34/34 - 20s - loss: 565.7192 - loglik: -5.6609e+02 - logprior: 0.3707
Epoch 8/10
34/34 - 20s - loss: 566.3569 - loglik: -5.6686e+02 - logprior: 0.5042
Fitted a model with MAP estimate = -565.2226
Time for alignment: 470.2204
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 765.6449 - loglik: -7.6353e+02 - logprior: -2.1104e+00
Epoch 2/10
34/34 - 14s - loss: 637.0224 - loglik: -6.3547e+02 - logprior: -1.5527e+00
Epoch 3/10
34/34 - 14s - loss: 620.4902 - loglik: -6.1881e+02 - logprior: -1.6849e+00
Epoch 4/10
34/34 - 14s - loss: 617.7101 - loglik: -6.1605e+02 - logprior: -1.6637e+00
Epoch 5/10
34/34 - 14s - loss: 614.5706 - loglik: -6.1291e+02 - logprior: -1.6582e+00
Epoch 6/10
34/34 - 14s - loss: 614.6458 - loglik: -6.1299e+02 - logprior: -1.6594e+00
Fitted a model with MAP estimate = -614.4376
expansions: [(13, 2), (14, 1), (15, 2), (17, 6), (29, 2), (41, 1), (51, 2), (55, 2), (56, 4), (57, 1), (65, 1), (66, 1), (67, 1), (91, 1), (93, 1), (94, 1), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (129, 1), (134, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (167, 1), (172, 1), (173, 1), (175, 1), (178, 1), (186, 3), (187, 2), (203, 1), (204, 3), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 598.2682 - loglik: -5.9491e+02 - logprior: -3.3617e+00
Epoch 2/2
34/34 - 20s - loss: 579.4194 - loglik: -5.7805e+02 - logprior: -1.3660e+00
Fitted a model with MAP estimate = -574.6539
expansions: [(185, 1), (261, 1)]
discards: [18 72 73 74 75 76]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 584.7068 - loglik: -5.8260e+02 - logprior: -2.1081e+00
Epoch 2/2
34/34 - 19s - loss: 579.5455 - loglik: -5.7922e+02 - logprior: -3.2093e-01
Fitted a model with MAP estimate = -578.5440
expansions: [(71, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 582.8030 - loglik: -5.8085e+02 - logprior: -1.9483e+00
Epoch 2/10
34/34 - 20s - loss: 576.7448 - loglik: -5.7658e+02 - logprior: -1.6312e-01
Epoch 3/10
34/34 - 19s - loss: 573.9094 - loglik: -5.7383e+02 - logprior: -7.5845e-02
Epoch 4/10
34/34 - 20s - loss: 574.5223 - loglik: -5.7447e+02 - logprior: -4.8526e-02
Fitted a model with MAP estimate = -572.1443
Time for alignment: 351.3099
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 19s - loss: 766.8180 - loglik: -7.6472e+02 - logprior: -2.1021e+00
Epoch 2/10
34/34 - 14s - loss: 642.4879 - loglik: -6.4098e+02 - logprior: -1.5120e+00
Epoch 3/10
34/34 - 14s - loss: 623.7795 - loglik: -6.2201e+02 - logprior: -1.7722e+00
Epoch 4/10
34/34 - 14s - loss: 620.6721 - loglik: -6.1892e+02 - logprior: -1.7529e+00
Epoch 5/10
34/34 - 14s - loss: 619.3046 - loglik: -6.1756e+02 - logprior: -1.7456e+00
Epoch 6/10
34/34 - 14s - loss: 617.3794 - loglik: -6.1563e+02 - logprior: -1.7497e+00
Epoch 7/10
34/34 - 14s - loss: 618.8093 - loglik: -6.1705e+02 - logprior: -1.7573e+00
Fitted a model with MAP estimate = -618.1578
expansions: [(9, 1), (12, 1), (16, 2), (23, 1), (27, 1), (29, 1), (30, 1), (39, 1), (48, 1), (51, 2), (55, 2), (56, 2), (68, 1), (93, 1), (94, 1), (96, 2), (99, 1), (100, 1), (102, 1), (103, 1), (104, 2), (109, 1), (129, 1), (134, 1), (139, 2), (141, 1), (142, 3), (143, 1), (164, 1), (167, 1), (171, 1), (172, 1), (173, 1), (175, 1), (178, 1), (184, 4), (185, 2), (187, 2), (203, 1), (208, 1), (211, 1), (224, 1), (225, 1), (226, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 603.0620 - loglik: -5.9974e+02 - logprior: -3.3227e+00
Epoch 2/2
34/34 - 19s - loss: 582.0600 - loglik: -5.8075e+02 - logprior: -1.3107e+00
Fitted a model with MAP estimate = -577.8711
expansions: [(177, 1), (228, 1)]
discards: [ 18  60 250 251 252]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 586.9733 - loglik: -5.8490e+02 - logprior: -2.0734e+00
Epoch 2/2
34/34 - 19s - loss: 581.4072 - loglik: -5.8112e+02 - logprior: -2.8613e-01
Fitted a model with MAP estimate = -579.9199
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 290 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 584.5044 - loglik: -5.8261e+02 - logprior: -1.8921e+00
Epoch 2/10
34/34 - 19s - loss: 581.3387 - loglik: -5.8130e+02 - logprior: -3.5247e-02
Epoch 3/10
34/34 - 19s - loss: 579.0898 - loglik: -5.7918e+02 - logprior: 0.0906
Epoch 4/10
34/34 - 19s - loss: 578.7983 - loglik: -5.7898e+02 - logprior: 0.1803
Epoch 5/10
34/34 - 19s - loss: 576.4783 - loglik: -5.7674e+02 - logprior: 0.2580
Epoch 6/10
34/34 - 19s - loss: 577.0191 - loglik: -5.7741e+02 - logprior: 0.3953
Fitted a model with MAP estimate = -576.1749
Time for alignment: 395.2336
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 767.3637 - loglik: -7.6525e+02 - logprior: -2.1164e+00
Epoch 2/10
34/34 - 14s - loss: 640.2614 - loglik: -6.3871e+02 - logprior: -1.5480e+00
Epoch 3/10
34/34 - 14s - loss: 624.4471 - loglik: -6.2275e+02 - logprior: -1.7007e+00
Epoch 4/10
34/34 - 14s - loss: 620.9024 - loglik: -6.1926e+02 - logprior: -1.6453e+00
Epoch 5/10
34/34 - 14s - loss: 617.4387 - loglik: -6.1579e+02 - logprior: -1.6453e+00
Epoch 6/10
34/34 - 14s - loss: 616.5839 - loglik: -6.1493e+02 - logprior: -1.6518e+00
Epoch 7/10
34/34 - 14s - loss: 617.0982 - loglik: -6.1545e+02 - logprior: -1.6529e+00
Fitted a model with MAP estimate = -616.4452
expansions: [(9, 2), (12, 1), (14, 1), (16, 1), (18, 2), (27, 1), (29, 2), (37, 1), (50, 2), (56, 1), (65, 1), (66, 2), (67, 1), (94, 2), (96, 3), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (110, 1), (134, 1), (139, 1), (142, 3), (143, 1), (144, 1), (147, 1), (164, 3), (171, 1), (172, 1), (173, 1), (178, 1), (188, 1), (193, 1), (202, 2), (203, 1), (208, 1), (226, 1), (228, 1), (229, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 605.8931 - loglik: -6.0256e+02 - logprior: -3.3330e+00
Epoch 2/2
34/34 - 19s - loss: 587.8130 - loglik: -5.8651e+02 - logprior: -1.3047e+00
Fitted a model with MAP estimate = -583.9635
expansions: [(22, 1), (175, 1), (224, 1)]
discards: [  8 112 116 203 250]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 22s - loss: 590.7297 - loglik: -5.8870e+02 - logprior: -2.0267e+00
Epoch 2/2
34/34 - 19s - loss: 584.2255 - loglik: -5.8401e+02 - logprior: -2.1657e-01
Fitted a model with MAP estimate = -583.3181
expansions: []
discards: [80]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 21s - loss: 588.4253 - loglik: -5.8653e+02 - logprior: -1.8954e+00
Epoch 2/10
34/34 - 19s - loss: 584.7077 - loglik: -5.8464e+02 - logprior: -6.7243e-02
Epoch 3/10
34/34 - 18s - loss: 583.2526 - loglik: -5.8338e+02 - logprior: 0.1274
Epoch 4/10
34/34 - 18s - loss: 581.6307 - loglik: -5.8186e+02 - logprior: 0.2283
Epoch 5/10
34/34 - 19s - loss: 581.5747 - loglik: -5.8189e+02 - logprior: 0.3203
Epoch 6/10
34/34 - 19s - loss: 579.7869 - loglik: -5.8027e+02 - logprior: 0.4818
Epoch 7/10
34/34 - 19s - loss: 580.3904 - loglik: -5.8100e+02 - logprior: 0.6120
Fitted a model with MAP estimate = -579.7225
Time for alignment: 409.6271
Computed alignments with likelihoods: ['-572.3180', '-565.2226', '-572.1443', '-576.1749', '-579.7225']
Best model has likelihood: -565.2226  (prior= 0.4886 )
time for generating output: 0.3863
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.7111111111111111
Training of 5 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb40f825e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1f70820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac8b5ee80>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 271.4276 - loglik: -1.8319e+02 - logprior: -8.8242e+01
Epoch 2/10
10/10 - 1s - loss: 182.9143 - loglik: -1.5917e+02 - logprior: -2.3744e+01
Epoch 3/10
10/10 - 1s - loss: 152.7937 - loglik: -1.4153e+02 - logprior: -1.1261e+01
Epoch 4/10
10/10 - 1s - loss: 140.1930 - loglik: -1.3339e+02 - logprior: -6.8011e+00
Epoch 5/10
10/10 - 1s - loss: 133.0538 - loglik: -1.2862e+02 - logprior: -4.4385e+00
Epoch 6/10
10/10 - 1s - loss: 128.1187 - loglik: -1.2483e+02 - logprior: -3.2862e+00
Epoch 7/10
10/10 - 1s - loss: 125.5849 - loglik: -1.2310e+02 - logprior: -2.4824e+00
Epoch 8/10
10/10 - 1s - loss: 124.2997 - loglik: -1.2230e+02 - logprior: -1.9949e+00
Epoch 9/10
10/10 - 1s - loss: 123.5718 - loglik: -1.2185e+02 - logprior: -1.7194e+00
Epoch 10/10
10/10 - 1s - loss: 123.1740 - loglik: -1.2171e+02 - logprior: -1.4676e+00
Fitted a model with MAP estimate = -123.0106
expansions: [(7, 1), (8, 3), (13, 1), (19, 1), (22, 1), (26, 2), (28, 2), (37, 1), (38, 2), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.1217 - loglik: -1.2321e+02 - logprior: -9.8914e+01
Epoch 2/2
10/10 - 1s - loss: 156.3045 - loglik: -1.1573e+02 - logprior: -4.0576e+01
Fitted a model with MAP estimate = -144.6823
expansions: [(0, 1)]
discards: [ 0 32 36]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.2707 - loglik: -1.1252e+02 - logprior: -7.8748e+01
Epoch 2/2
10/10 - 1s - loss: 131.8253 - loglik: -1.1100e+02 - logprior: -2.0824e+01
Fitted a model with MAP estimate = -122.8804
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 188.3206 - loglik: -1.1083e+02 - logprior: -7.7495e+01
Epoch 2/10
10/10 - 1s - loss: 130.7526 - loglik: -1.1047e+02 - logprior: -2.0286e+01
Epoch 3/10
10/10 - 1s - loss: 118.6902 - loglik: -1.1023e+02 - logprior: -8.4647e+00
Epoch 4/10
10/10 - 1s - loss: 113.7848 - loglik: -1.1028e+02 - logprior: -3.5040e+00
Epoch 5/10
10/10 - 1s - loss: 111.2209 - loglik: -1.1047e+02 - logprior: -7.4844e-01
Epoch 6/10
10/10 - 1s - loss: 109.8141 - loglik: -1.1068e+02 - logprior: 0.8618
Epoch 7/10
10/10 - 1s - loss: 108.9732 - loglik: -1.1085e+02 - logprior: 1.8769
Epoch 8/10
10/10 - 1s - loss: 108.4012 - loglik: -1.1094e+02 - logprior: 2.5424
Epoch 9/10
10/10 - 1s - loss: 107.9933 - loglik: -1.1102e+02 - logprior: 3.0288
Epoch 10/10
10/10 - 1s - loss: 107.6693 - loglik: -1.1108e+02 - logprior: 3.4142
Fitted a model with MAP estimate = -107.5074
Time for alignment: 31.2195
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.4276 - loglik: -1.8319e+02 - logprior: -8.8242e+01
Epoch 2/10
10/10 - 1s - loss: 182.9143 - loglik: -1.5917e+02 - logprior: -2.3744e+01
Epoch 3/10
10/10 - 1s - loss: 152.7937 - loglik: -1.4153e+02 - logprior: -1.1261e+01
Epoch 4/10
10/10 - 1s - loss: 140.1930 - loglik: -1.3339e+02 - logprior: -6.8011e+00
Epoch 5/10
10/10 - 1s - loss: 133.0537 - loglik: -1.2862e+02 - logprior: -4.4385e+00
Epoch 6/10
10/10 - 1s - loss: 128.1187 - loglik: -1.2483e+02 - logprior: -3.2862e+00
Epoch 7/10
10/10 - 1s - loss: 125.5849 - loglik: -1.2310e+02 - logprior: -2.4824e+00
Epoch 8/10
10/10 - 1s - loss: 124.2998 - loglik: -1.2230e+02 - logprior: -1.9949e+00
Epoch 9/10
10/10 - 1s - loss: 123.5718 - loglik: -1.2185e+02 - logprior: -1.7194e+00
Epoch 10/10
10/10 - 1s - loss: 123.1739 - loglik: -1.2171e+02 - logprior: -1.4676e+00
Fitted a model with MAP estimate = -123.0107
expansions: [(7, 1), (8, 3), (13, 1), (19, 1), (22, 1), (26, 2), (28, 2), (37, 1), (38, 2), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.1217 - loglik: -1.2321e+02 - logprior: -9.8914e+01
Epoch 2/2
10/10 - 1s - loss: 156.3045 - loglik: -1.1573e+02 - logprior: -4.0576e+01
Fitted a model with MAP estimate = -144.6823
expansions: [(0, 1)]
discards: [ 0 32 36]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.2707 - loglik: -1.1252e+02 - logprior: -7.8748e+01
Epoch 2/2
10/10 - 1s - loss: 131.8253 - loglik: -1.1100e+02 - logprior: -2.0824e+01
Fitted a model with MAP estimate = -122.8804
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 188.3206 - loglik: -1.1083e+02 - logprior: -7.7495e+01
Epoch 2/10
10/10 - 1s - loss: 130.7526 - loglik: -1.1047e+02 - logprior: -2.0286e+01
Epoch 3/10
10/10 - 1s - loss: 118.6903 - loglik: -1.1023e+02 - logprior: -8.4647e+00
Epoch 4/10
10/10 - 1s - loss: 113.7848 - loglik: -1.1028e+02 - logprior: -3.5040e+00
Epoch 5/10
10/10 - 1s - loss: 111.2208 - loglik: -1.1047e+02 - logprior: -7.4844e-01
Epoch 6/10
10/10 - 1s - loss: 109.8141 - loglik: -1.1068e+02 - logprior: 0.8618
Epoch 7/10
10/10 - 1s - loss: 108.9732 - loglik: -1.1085e+02 - logprior: 1.8769
Epoch 8/10
10/10 - 1s - loss: 108.4011 - loglik: -1.1094e+02 - logprior: 2.5424
Epoch 9/10
10/10 - 1s - loss: 107.9932 - loglik: -1.1102e+02 - logprior: 3.0288
Epoch 10/10
10/10 - 1s - loss: 107.6693 - loglik: -1.1108e+02 - logprior: 3.4142
Fitted a model with MAP estimate = -107.5073
Time for alignment: 29.2162
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.4276 - loglik: -1.8319e+02 - logprior: -8.8242e+01
Epoch 2/10
10/10 - 1s - loss: 182.9143 - loglik: -1.5917e+02 - logprior: -2.3744e+01
Epoch 3/10
10/10 - 1s - loss: 152.7937 - loglik: -1.4153e+02 - logprior: -1.1261e+01
Epoch 4/10
10/10 - 1s - loss: 140.1930 - loglik: -1.3339e+02 - logprior: -6.8011e+00
Epoch 5/10
10/10 - 1s - loss: 133.0538 - loglik: -1.2862e+02 - logprior: -4.4385e+00
Epoch 6/10
10/10 - 1s - loss: 128.1187 - loglik: -1.2483e+02 - logprior: -3.2862e+00
Epoch 7/10
10/10 - 1s - loss: 125.5849 - loglik: -1.2310e+02 - logprior: -2.4824e+00
Epoch 8/10
10/10 - 1s - loss: 124.2997 - loglik: -1.2230e+02 - logprior: -1.9949e+00
Epoch 9/10
10/10 - 1s - loss: 123.5718 - loglik: -1.2185e+02 - logprior: -1.7194e+00
Epoch 10/10
10/10 - 1s - loss: 123.1740 - loglik: -1.2171e+02 - logprior: -1.4676e+00
Fitted a model with MAP estimate = -123.0107
expansions: [(7, 1), (8, 3), (13, 1), (19, 1), (22, 1), (26, 2), (28, 2), (37, 1), (38, 2), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.1217 - loglik: -1.2321e+02 - logprior: -9.8914e+01
Epoch 2/2
10/10 - 1s - loss: 156.3045 - loglik: -1.1573e+02 - logprior: -4.0576e+01
Fitted a model with MAP estimate = -144.6823
expansions: [(0, 1)]
discards: [ 0 32 36]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.2707 - loglik: -1.1252e+02 - logprior: -7.8748e+01
Epoch 2/2
10/10 - 1s - loss: 131.8253 - loglik: -1.1100e+02 - logprior: -2.0824e+01
Fitted a model with MAP estimate = -122.8804
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 188.3207 - loglik: -1.1083e+02 - logprior: -7.7495e+01
Epoch 2/10
10/10 - 1s - loss: 130.7526 - loglik: -1.1047e+02 - logprior: -2.0286e+01
Epoch 3/10
10/10 - 1s - loss: 118.6902 - loglik: -1.1023e+02 - logprior: -8.4647e+00
Epoch 4/10
10/10 - 1s - loss: 113.7849 - loglik: -1.1028e+02 - logprior: -3.5040e+00
Epoch 5/10
10/10 - 1s - loss: 111.2209 - loglik: -1.1047e+02 - logprior: -7.4843e-01
Epoch 6/10
10/10 - 1s - loss: 109.8141 - loglik: -1.1068e+02 - logprior: 0.8618
Epoch 7/10
10/10 - 1s - loss: 108.9733 - loglik: -1.1085e+02 - logprior: 1.8769
Epoch 8/10
10/10 - 1s - loss: 108.4012 - loglik: -1.1094e+02 - logprior: 2.5424
Epoch 9/10
10/10 - 1s - loss: 107.9933 - loglik: -1.1102e+02 - logprior: 3.0288
Epoch 10/10
10/10 - 1s - loss: 107.6694 - loglik: -1.1108e+02 - logprior: 3.4142
Fitted a model with MAP estimate = -107.5075
Time for alignment: 28.1861
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.4276 - loglik: -1.8319e+02 - logprior: -8.8242e+01
Epoch 2/10
10/10 - 1s - loss: 182.9143 - loglik: -1.5917e+02 - logprior: -2.3744e+01
Epoch 3/10
10/10 - 1s - loss: 152.7937 - loglik: -1.4153e+02 - logprior: -1.1261e+01
Epoch 4/10
10/10 - 1s - loss: 140.1930 - loglik: -1.3339e+02 - logprior: -6.8011e+00
Epoch 5/10
10/10 - 1s - loss: 133.0538 - loglik: -1.2862e+02 - logprior: -4.4385e+00
Epoch 6/10
10/10 - 1s - loss: 128.1187 - loglik: -1.2483e+02 - logprior: -3.2862e+00
Epoch 7/10
10/10 - 1s - loss: 125.5849 - loglik: -1.2310e+02 - logprior: -2.4824e+00
Epoch 8/10
10/10 - 1s - loss: 124.2998 - loglik: -1.2230e+02 - logprior: -1.9949e+00
Epoch 9/10
10/10 - 1s - loss: 123.5718 - loglik: -1.2185e+02 - logprior: -1.7194e+00
Epoch 10/10
10/10 - 1s - loss: 123.1740 - loglik: -1.2171e+02 - logprior: -1.4676e+00
Fitted a model with MAP estimate = -123.0107
expansions: [(7, 1), (8, 3), (13, 1), (19, 1), (22, 1), (26, 2), (28, 2), (37, 1), (38, 2), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 222.1217 - loglik: -1.2321e+02 - logprior: -9.8914e+01
Epoch 2/2
10/10 - 1s - loss: 156.3046 - loglik: -1.1573e+02 - logprior: -4.0576e+01
Fitted a model with MAP estimate = -144.6824
expansions: [(0, 1)]
discards: [ 0 32 36]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.2707 - loglik: -1.1252e+02 - logprior: -7.8748e+01
Epoch 2/2
10/10 - 1s - loss: 131.8252 - loglik: -1.1100e+02 - logprior: -2.0824e+01
Fitted a model with MAP estimate = -122.8804
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 188.3207 - loglik: -1.1083e+02 - logprior: -7.7495e+01
Epoch 2/10
10/10 - 1s - loss: 130.7526 - loglik: -1.1047e+02 - logprior: -2.0286e+01
Epoch 3/10
10/10 - 1s - loss: 118.6902 - loglik: -1.1023e+02 - logprior: -8.4647e+00
Epoch 4/10
10/10 - 1s - loss: 113.7848 - loglik: -1.1028e+02 - logprior: -3.5040e+00
Epoch 5/10
10/10 - 1s - loss: 111.2209 - loglik: -1.1047e+02 - logprior: -7.4842e-01
Epoch 6/10
10/10 - 1s - loss: 109.8141 - loglik: -1.1068e+02 - logprior: 0.8618
Epoch 7/10
10/10 - 1s - loss: 108.9732 - loglik: -1.1085e+02 - logprior: 1.8769
Epoch 8/10
10/10 - 1s - loss: 108.4011 - loglik: -1.1094e+02 - logprior: 2.5424
Epoch 9/10
10/10 - 1s - loss: 107.9933 - loglik: -1.1102e+02 - logprior: 3.0288
Epoch 10/10
10/10 - 1s - loss: 107.6693 - loglik: -1.1108e+02 - logprior: 3.4142
Fitted a model with MAP estimate = -107.5076
Time for alignment: 29.0609
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.4276 - loglik: -1.8319e+02 - logprior: -8.8242e+01
Epoch 2/10
10/10 - 1s - loss: 182.9143 - loglik: -1.5917e+02 - logprior: -2.3744e+01
Epoch 3/10
10/10 - 1s - loss: 152.7937 - loglik: -1.4153e+02 - logprior: -1.1261e+01
Epoch 4/10
10/10 - 1s - loss: 140.1930 - loglik: -1.3339e+02 - logprior: -6.8011e+00
Epoch 5/10
10/10 - 1s - loss: 133.0538 - loglik: -1.2862e+02 - logprior: -4.4385e+00
Epoch 6/10
10/10 - 1s - loss: 128.1187 - loglik: -1.2483e+02 - logprior: -3.2862e+00
Epoch 7/10
10/10 - 1s - loss: 125.5849 - loglik: -1.2310e+02 - logprior: -2.4824e+00
Epoch 8/10
10/10 - 1s - loss: 124.2998 - loglik: -1.2230e+02 - logprior: -1.9949e+00
Epoch 9/10
10/10 - 1s - loss: 123.5718 - loglik: -1.2185e+02 - logprior: -1.7194e+00
Epoch 10/10
10/10 - 1s - loss: 123.1740 - loglik: -1.2171e+02 - logprior: -1.4676e+00
Fitted a model with MAP estimate = -123.0105
expansions: [(7, 1), (8, 3), (13, 1), (19, 1), (22, 1), (26, 2), (28, 2), (37, 1), (38, 2), (39, 1), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 222.1217 - loglik: -1.2321e+02 - logprior: -9.8914e+01
Epoch 2/2
10/10 - 1s - loss: 156.3045 - loglik: -1.1573e+02 - logprior: -4.0576e+01
Fitted a model with MAP estimate = -144.6823
expansions: [(0, 1)]
discards: [ 0 32 36]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.2707 - loglik: -1.1252e+02 - logprior: -7.8748e+01
Epoch 2/2
10/10 - 1s - loss: 131.8252 - loglik: -1.1100e+02 - logprior: -2.0824e+01
Fitted a model with MAP estimate = -122.8804
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 62 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 188.3206 - loglik: -1.1083e+02 - logprior: -7.7495e+01
Epoch 2/10
10/10 - 1s - loss: 130.7525 - loglik: -1.1047e+02 - logprior: -2.0286e+01
Epoch 3/10
10/10 - 1s - loss: 118.6902 - loglik: -1.1023e+02 - logprior: -8.4647e+00
Epoch 4/10
10/10 - 1s - loss: 113.7848 - loglik: -1.1028e+02 - logprior: -3.5040e+00
Epoch 5/10
10/10 - 1s - loss: 111.2209 - loglik: -1.1047e+02 - logprior: -7.4843e-01
Epoch 6/10
10/10 - 1s - loss: 109.8141 - loglik: -1.1068e+02 - logprior: 0.8618
Epoch 7/10
10/10 - 1s - loss: 108.9732 - loglik: -1.1085e+02 - logprior: 1.8769
Epoch 8/10
10/10 - 1s - loss: 108.4011 - loglik: -1.1094e+02 - logprior: 2.5424
Epoch 9/10
10/10 - 1s - loss: 107.9932 - loglik: -1.1102e+02 - logprior: 3.0288
Epoch 10/10
10/10 - 1s - loss: 107.6693 - loglik: -1.1108e+02 - logprior: 3.4142
Fitted a model with MAP estimate = -107.5073
Time for alignment: 27.4945
Computed alignments with likelihoods: ['-107.5074', '-107.5073', '-107.5075', '-107.5076', '-107.5073']
Best model has likelihood: -107.5073  (prior= 3.5987 )
time for generating output: 0.1036
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.8618337274877206
Training of 5 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaaf3f7610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9dccec40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac04e9fa0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 317.0171 - loglik: -3.0932e+02 - logprior: -7.6945e+00
Epoch 2/10
13/13 - 1s - loss: 284.9683 - loglik: -2.8310e+02 - logprior: -1.8700e+00
Epoch 3/10
13/13 - 1s - loss: 261.2642 - loglik: -2.5964e+02 - logprior: -1.6240e+00
Epoch 4/10
13/13 - 1s - loss: 252.6106 - loglik: -2.5082e+02 - logprior: -1.7885e+00
Epoch 5/10
13/13 - 1s - loss: 251.0197 - loglik: -2.4933e+02 - logprior: -1.6897e+00
Epoch 6/10
13/13 - 1s - loss: 249.9019 - loglik: -2.4828e+02 - logprior: -1.6254e+00
Epoch 7/10
13/13 - 1s - loss: 249.1206 - loglik: -2.4747e+02 - logprior: -1.6521e+00
Epoch 8/10
13/13 - 1s - loss: 249.2277 - loglik: -2.4756e+02 - logprior: -1.6652e+00
Fitted a model with MAP estimate = -248.8166
expansions: [(9, 1), (10, 1), (13, 2), (15, 1), (28, 3), (29, 4), (30, 1), (31, 1), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 258.3305 - loglik: -2.4928e+02 - logprior: -9.0464e+00
Epoch 2/2
13/13 - 2s - loss: 248.1167 - loglik: -2.4410e+02 - logprior: -4.0181e+00
Fitted a model with MAP estimate = -246.2809
expansions: [(0, 2)]
discards: [ 0 14 67 96]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 250.1709 - loglik: -2.4334e+02 - logprior: -6.8335e+00
Epoch 2/2
13/13 - 2s - loss: 244.1880 - loglik: -2.4236e+02 - logprior: -1.8291e+00
Fitted a model with MAP estimate = -243.2756
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.4377 - loglik: -2.4375e+02 - logprior: -8.6881e+00
Epoch 2/10
13/13 - 1s - loss: 246.1094 - loglik: -2.4348e+02 - logprior: -2.6317e+00
Epoch 3/10
13/13 - 2s - loss: 243.5116 - loglik: -2.4225e+02 - logprior: -1.2640e+00
Epoch 4/10
13/13 - 1s - loss: 242.7002 - loglik: -2.4176e+02 - logprior: -9.3950e-01
Epoch 5/10
13/13 - 2s - loss: 242.9362 - loglik: -2.4209e+02 - logprior: -8.4150e-01
Fitted a model with MAP estimate = -242.4067
Time for alignment: 47.7429
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 317.2706 - loglik: -3.0958e+02 - logprior: -7.6951e+00
Epoch 2/10
13/13 - 1s - loss: 283.7897 - loglik: -2.8192e+02 - logprior: -1.8682e+00
Epoch 3/10
13/13 - 1s - loss: 260.4644 - loglik: -2.5883e+02 - logprior: -1.6299e+00
Epoch 4/10
13/13 - 1s - loss: 251.8474 - loglik: -2.5003e+02 - logprior: -1.8127e+00
Epoch 5/10
13/13 - 1s - loss: 249.7359 - loglik: -2.4801e+02 - logprior: -1.7216e+00
Epoch 6/10
13/13 - 1s - loss: 248.9809 - loglik: -2.4735e+02 - logprior: -1.6349e+00
Epoch 7/10
13/13 - 1s - loss: 248.4256 - loglik: -2.4678e+02 - logprior: -1.6418e+00
Epoch 8/10
13/13 - 1s - loss: 248.5601 - loglik: -2.4692e+02 - logprior: -1.6390e+00
Fitted a model with MAP estimate = -248.2370
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 3), (29, 4), (30, 1), (31, 1), (39, 1), (40, 1), (42, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 258.1089 - loglik: -2.4906e+02 - logprior: -9.0440e+00
Epoch 2/2
13/13 - 2s - loss: 247.9143 - loglik: -2.4391e+02 - logprior: -4.0076e+00
Fitted a model with MAP estimate = -246.3151
expansions: [(0, 2)]
discards: [ 0 14 67 96]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 250.0246 - loglik: -2.4320e+02 - logprior: -6.8243e+00
Epoch 2/2
13/13 - 2s - loss: 244.1983 - loglik: -2.4237e+02 - logprior: -1.8265e+00
Fitted a model with MAP estimate = -243.2280
expansions: []
discards: [ 0 92]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.3913 - loglik: -2.4373e+02 - logprior: -8.6662e+00
Epoch 2/10
13/13 - 2s - loss: 245.6889 - loglik: -2.4306e+02 - logprior: -2.6268e+00
Epoch 3/10
13/13 - 1s - loss: 243.9520 - loglik: -2.4269e+02 - logprior: -1.2622e+00
Epoch 4/10
13/13 - 2s - loss: 243.2285 - loglik: -2.4228e+02 - logprior: -9.4910e-01
Epoch 5/10
13/13 - 2s - loss: 242.7864 - loglik: -2.4194e+02 - logprior: -8.5092e-01
Epoch 6/10
13/13 - 2s - loss: 242.3998 - loglik: -2.4157e+02 - logprior: -8.2810e-01
Epoch 7/10
13/13 - 2s - loss: 242.2871 - loglik: -2.4147e+02 - logprior: -8.2005e-01
Epoch 8/10
13/13 - 1s - loss: 242.4208 - loglik: -2.4162e+02 - logprior: -8.0384e-01
Fitted a model with MAP estimate = -242.0482
Time for alignment: 51.0461
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 317.2001 - loglik: -3.0950e+02 - logprior: -7.6959e+00
Epoch 2/10
13/13 - 1s - loss: 285.1182 - loglik: -2.8325e+02 - logprior: -1.8663e+00
Epoch 3/10
13/13 - 1s - loss: 262.3141 - loglik: -2.6070e+02 - logprior: -1.6095e+00
Epoch 4/10
13/13 - 1s - loss: 252.9954 - loglik: -2.5119e+02 - logprior: -1.8098e+00
Epoch 5/10
13/13 - 1s - loss: 250.2814 - loglik: -2.4852e+02 - logprior: -1.7615e+00
Epoch 6/10
13/13 - 1s - loss: 249.1223 - loglik: -2.4745e+02 - logprior: -1.6690e+00
Epoch 7/10
13/13 - 1s - loss: 248.9282 - loglik: -2.4725e+02 - logprior: -1.6756e+00
Epoch 8/10
13/13 - 1s - loss: 248.5547 - loglik: -2.4688e+02 - logprior: -1.6737e+00
Epoch 9/10
13/13 - 1s - loss: 248.2913 - loglik: -2.4665e+02 - logprior: -1.6435e+00
Epoch 10/10
13/13 - 1s - loss: 248.5101 - loglik: -2.4689e+02 - logprior: -1.6246e+00
Fitted a model with MAP estimate = -248.3307
expansions: [(9, 1), (10, 1), (12, 1), (18, 1), (28, 3), (29, 4), (30, 1), (31, 1), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 257.8146 - loglik: -2.4879e+02 - logprior: -9.0279e+00
Epoch 2/2
13/13 - 2s - loss: 247.5188 - loglik: -2.4353e+02 - logprior: -3.9870e+00
Fitted a model with MAP estimate = -246.2331
expansions: [(0, 2)]
discards: [ 0 66 95]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 250.0685 - loglik: -2.4325e+02 - logprior: -6.8209e+00
Epoch 2/2
13/13 - 2s - loss: 244.0182 - loglik: -2.4220e+02 - logprior: -1.8194e+00
Fitted a model with MAP estimate = -243.2460
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 252.3987 - loglik: -2.4375e+02 - logprior: -8.6480e+00
Epoch 2/10
13/13 - 2s - loss: 245.9168 - loglik: -2.4332e+02 - logprior: -2.5921e+00
Epoch 3/10
13/13 - 2s - loss: 243.6255 - loglik: -2.4239e+02 - logprior: -1.2386e+00
Epoch 4/10
13/13 - 2s - loss: 242.7523 - loglik: -2.4181e+02 - logprior: -9.4132e-01
Epoch 5/10
13/13 - 2s - loss: 242.9369 - loglik: -2.4210e+02 - logprior: -8.4155e-01
Fitted a model with MAP estimate = -242.4073
Time for alignment: 47.4938
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.9196 - loglik: -3.0922e+02 - logprior: -7.6987e+00
Epoch 2/10
13/13 - 1s - loss: 283.5605 - loglik: -2.8169e+02 - logprior: -1.8705e+00
Epoch 3/10
13/13 - 1s - loss: 259.9703 - loglik: -2.5836e+02 - logprior: -1.6135e+00
Epoch 4/10
13/13 - 1s - loss: 252.0299 - loglik: -2.5023e+02 - logprior: -1.7955e+00
Epoch 5/10
13/13 - 1s - loss: 250.1134 - loglik: -2.4838e+02 - logprior: -1.7361e+00
Epoch 6/10
13/13 - 1s - loss: 248.7847 - loglik: -2.4713e+02 - logprior: -1.6551e+00
Epoch 7/10
13/13 - 1s - loss: 248.3470 - loglik: -2.4668e+02 - logprior: -1.6674e+00
Epoch 8/10
13/13 - 1s - loss: 248.7813 - loglik: -2.4711e+02 - logprior: -1.6696e+00
Fitted a model with MAP estimate = -247.9569
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (29, 1), (30, 4), (31, 2), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 258.1250 - loglik: -2.4908e+02 - logprior: -9.0468e+00
Epoch 2/2
13/13 - 2s - loss: 247.6823 - loglik: -2.4365e+02 - logprior: -4.0328e+00
Fitted a model with MAP estimate = -246.3224
expansions: [(0, 2)]
discards: [ 0 14 66 95]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 250.1571 - loglik: -2.4332e+02 - logprior: -6.8334e+00
Epoch 2/2
13/13 - 2s - loss: 244.0062 - loglik: -2.4217e+02 - logprior: -1.8360e+00
Fitted a model with MAP estimate = -243.2847
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.6218 - loglik: -2.4397e+02 - logprior: -8.6507e+00
Epoch 2/10
13/13 - 2s - loss: 245.5465 - loglik: -2.4294e+02 - logprior: -2.6027e+00
Epoch 3/10
13/13 - 2s - loss: 243.9349 - loglik: -2.4267e+02 - logprior: -1.2622e+00
Epoch 4/10
13/13 - 2s - loss: 243.3843 - loglik: -2.4243e+02 - logprior: -9.5015e-01
Epoch 5/10
13/13 - 2s - loss: 242.3727 - loglik: -2.4151e+02 - logprior: -8.5961e-01
Epoch 6/10
13/13 - 2s - loss: 242.5228 - loglik: -2.4170e+02 - logprior: -8.2477e-01
Fitted a model with MAP estimate = -242.2732
Time for alignment: 47.3930
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 317.1732 - loglik: -3.0948e+02 - logprior: -7.6942e+00
Epoch 2/10
13/13 - 1s - loss: 285.0442 - loglik: -2.8317e+02 - logprior: -1.8697e+00
Epoch 3/10
13/13 - 1s - loss: 261.8040 - loglik: -2.6019e+02 - logprior: -1.6147e+00
Epoch 4/10
13/13 - 1s - loss: 252.5500 - loglik: -2.5074e+02 - logprior: -1.8052e+00
Epoch 5/10
13/13 - 1s - loss: 250.1102 - loglik: -2.4838e+02 - logprior: -1.7339e+00
Epoch 6/10
13/13 - 1s - loss: 249.0352 - loglik: -2.4739e+02 - logprior: -1.6453e+00
Epoch 7/10
13/13 - 1s - loss: 248.5448 - loglik: -2.4689e+02 - logprior: -1.6551e+00
Epoch 8/10
13/13 - 1s - loss: 248.9863 - loglik: -2.4733e+02 - logprior: -1.6579e+00
Fitted a model with MAP estimate = -248.4114
expansions: [(9, 1), (10, 1), (12, 1), (15, 1), (28, 3), (29, 3), (30, 1), (39, 1), (40, 1), (42, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 257.4726 - loglik: -2.4844e+02 - logprior: -9.0336e+00
Epoch 2/2
13/13 - 2s - loss: 247.7083 - loglik: -2.4373e+02 - logprior: -3.9753e+00
Fitted a model with MAP estimate = -246.2261
expansions: [(0, 2)]
discards: [ 0 64 93]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 250.0910 - loglik: -2.4327e+02 - logprior: -6.8229e+00
Epoch 2/2
13/13 - 1s - loss: 243.8289 - loglik: -2.4199e+02 - logprior: -1.8356e+00
Fitted a model with MAP estimate = -243.3045
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 252.5909 - loglik: -2.4394e+02 - logprior: -8.6461e+00
Epoch 2/10
13/13 - 2s - loss: 245.9138 - loglik: -2.4332e+02 - logprior: -2.5984e+00
Epoch 3/10
13/13 - 1s - loss: 243.3996 - loglik: -2.4213e+02 - logprior: -1.2657e+00
Epoch 4/10
13/13 - 2s - loss: 243.5484 - loglik: -2.4260e+02 - logprior: -9.5182e-01
Fitted a model with MAP estimate = -242.7823
Time for alignment: 43.0245
Computed alignments with likelihoods: ['-242.4067', '-242.0482', '-242.4073', '-242.2732', '-242.7823']
Best model has likelihood: -242.0482  (prior= -0.7581 )
time for generating output: 0.1600
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.9274809160305344
Training of 5 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb27d78cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e6c2670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f9c674c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.7382 - loglik: -3.4716e+02 - logprior: -8.5575e+01
Epoch 2/10
10/10 - 1s - loss: 326.9818 - loglik: -3.0712e+02 - logprior: -1.9865e+01
Epoch 3/10
10/10 - 1s - loss: 273.6293 - loglik: -2.6549e+02 - logprior: -8.1344e+00
Epoch 4/10
10/10 - 1s - loss: 244.0961 - loglik: -2.3906e+02 - logprior: -5.0402e+00
Epoch 5/10
10/10 - 1s - loss: 232.7668 - loglik: -2.2986e+02 - logprior: -2.9090e+00
Epoch 6/10
10/10 - 1s - loss: 227.2221 - loglik: -2.2567e+02 - logprior: -1.5485e+00
Epoch 7/10
10/10 - 1s - loss: 223.8910 - loglik: -2.2298e+02 - logprior: -9.1046e-01
Epoch 8/10
10/10 - 1s - loss: 222.0848 - loglik: -2.2173e+02 - logprior: -3.5699e-01
Epoch 9/10
10/10 - 1s - loss: 220.9970 - loglik: -2.2104e+02 - logprior: 0.0439
Epoch 10/10
10/10 - 1s - loss: 220.2716 - loglik: -2.2056e+02 - logprior: 0.2886
Fitted a model with MAP estimate = -219.9747
expansions: [(8, 2), (13, 2), (14, 1), (24, 2), (29, 1), (40, 2), (41, 2), (47, 1), (48, 2), (50, 1), (61, 1), (62, 1), (63, 1), (68, 2), (77, 1), (79, 1), (80, 2), (81, 2), (82, 1), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 318.6643 - loglik: -2.2274e+02 - logprior: -9.5923e+01
Epoch 2/2
10/10 - 1s - loss: 241.2340 - loglik: -2.0406e+02 - logprior: -3.7173e+01
Fitted a model with MAP estimate = -227.7504
expansions: [(0, 3)]
discards: [  0  29  47  60  87 102 105 117 118]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 276.9320 - loglik: -2.0141e+02 - logprior: -7.5526e+01
Epoch 2/2
10/10 - 1s - loss: 210.9329 - loglik: -1.9467e+02 - logprior: -1.6261e+01
Fitted a model with MAP estimate = -200.4871
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 287.9257 - loglik: -1.9724e+02 - logprior: -9.0690e+01
Epoch 2/10
10/10 - 1s - loss: 219.0099 - loglik: -1.9514e+02 - logprior: -2.3872e+01
Epoch 3/10
10/10 - 1s - loss: 198.4039 - loglik: -1.9344e+02 - logprior: -4.9644e+00
Epoch 4/10
10/10 - 1s - loss: 190.5777 - loglik: -1.9227e+02 - logprior: 1.6914
Epoch 5/10
10/10 - 1s - loss: 186.8722 - loglik: -1.9168e+02 - logprior: 4.8043
Epoch 6/10
10/10 - 1s - loss: 184.8947 - loglik: -1.9146e+02 - logprior: 6.5692
Epoch 7/10
10/10 - 1s - loss: 183.6514 - loglik: -1.9139e+02 - logprior: 7.7369
Epoch 8/10
10/10 - 1s - loss: 182.7223 - loglik: -1.9136e+02 - logprior: 8.6400
Epoch 9/10
10/10 - 1s - loss: 182.0086 - loglik: -1.9142e+02 - logprior: 9.4066
Epoch 10/10
10/10 - 1s - loss: 181.4045 - loglik: -1.9146e+02 - logprior: 10.0598
Fitted a model with MAP estimate = -181.1137
Time for alignment: 45.6403
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.7382 - loglik: -3.4716e+02 - logprior: -8.5575e+01
Epoch 2/10
10/10 - 1s - loss: 326.9818 - loglik: -3.0712e+02 - logprior: -1.9865e+01
Epoch 3/10
10/10 - 1s - loss: 273.6293 - loglik: -2.6549e+02 - logprior: -8.1344e+00
Epoch 4/10
10/10 - 1s - loss: 244.0961 - loglik: -2.3906e+02 - logprior: -5.0402e+00
Epoch 5/10
10/10 - 1s - loss: 232.7668 - loglik: -2.2986e+02 - logprior: -2.9090e+00
Epoch 6/10
10/10 - 1s - loss: 227.2222 - loglik: -2.2567e+02 - logprior: -1.5485e+00
Epoch 7/10
10/10 - 1s - loss: 223.8910 - loglik: -2.2298e+02 - logprior: -9.1046e-01
Epoch 8/10
10/10 - 1s - loss: 222.0848 - loglik: -2.2173e+02 - logprior: -3.5700e-01
Epoch 9/10
10/10 - 1s - loss: 220.9970 - loglik: -2.2104e+02 - logprior: 0.0439
Epoch 10/10
10/10 - 1s - loss: 220.2714 - loglik: -2.2056e+02 - logprior: 0.2886
Fitted a model with MAP estimate = -219.9757
expansions: [(8, 2), (13, 2), (14, 1), (24, 2), (29, 1), (40, 2), (41, 2), (47, 1), (48, 2), (50, 1), (61, 1), (62, 1), (63, 1), (68, 2), (77, 1), (79, 1), (80, 2), (81, 2), (82, 1), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 318.6643 - loglik: -2.2274e+02 - logprior: -9.5923e+01
Epoch 2/2
10/10 - 1s - loss: 241.2340 - loglik: -2.0406e+02 - logprior: -3.7173e+01
Fitted a model with MAP estimate = -227.7504
expansions: [(0, 3)]
discards: [  0  29  47  60  87 102 105 117 118]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 276.9320 - loglik: -2.0141e+02 - logprior: -7.5526e+01
Epoch 2/2
10/10 - 1s - loss: 210.9329 - loglik: -1.9467e+02 - logprior: -1.6260e+01
Fitted a model with MAP estimate = -200.4871
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 287.9261 - loglik: -1.9724e+02 - logprior: -9.0690e+01
Epoch 2/10
10/10 - 1s - loss: 219.0103 - loglik: -1.9514e+02 - logprior: -2.3872e+01
Epoch 3/10
10/10 - 1s - loss: 198.4040 - loglik: -1.9344e+02 - logprior: -4.9638e+00
Epoch 4/10
10/10 - 1s - loss: 190.5779 - loglik: -1.9227e+02 - logprior: 1.6918
Epoch 5/10
10/10 - 1s - loss: 186.8727 - loglik: -1.9168e+02 - logprior: 4.8043
Epoch 6/10
10/10 - 1s - loss: 184.8953 - loglik: -1.9146e+02 - logprior: 6.5691
Epoch 7/10
10/10 - 1s - loss: 183.6519 - loglik: -1.9139e+02 - logprior: 7.7368
Epoch 8/10
10/10 - 1s - loss: 182.7228 - loglik: -1.9136e+02 - logprior: 8.6398
Epoch 9/10
10/10 - 1s - loss: 182.0092 - loglik: -1.9142e+02 - logprior: 9.4062
Epoch 10/10
10/10 - 1s - loss: 181.4051 - loglik: -1.9146e+02 - logprior: 10.0594
Fitted a model with MAP estimate = -181.1144
Time for alignment: 44.0035
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.7382 - loglik: -3.4716e+02 - logprior: -8.5575e+01
Epoch 2/10
10/10 - 1s - loss: 326.9818 - loglik: -3.0712e+02 - logprior: -1.9865e+01
Epoch 3/10
10/10 - 1s - loss: 273.6293 - loglik: -2.6549e+02 - logprior: -8.1344e+00
Epoch 4/10
10/10 - 1s - loss: 244.0961 - loglik: -2.3906e+02 - logprior: -5.0402e+00
Epoch 5/10
10/10 - 1s - loss: 232.7668 - loglik: -2.2986e+02 - logprior: -2.9090e+00
Epoch 6/10
10/10 - 1s - loss: 227.2222 - loglik: -2.2567e+02 - logprior: -1.5485e+00
Epoch 7/10
10/10 - 1s - loss: 223.8911 - loglik: -2.2298e+02 - logprior: -9.1046e-01
Epoch 8/10
10/10 - 1s - loss: 222.0849 - loglik: -2.2173e+02 - logprior: -3.5699e-01
Epoch 9/10
10/10 - 1s - loss: 220.9970 - loglik: -2.2104e+02 - logprior: 0.0439
Epoch 10/10
10/10 - 1s - loss: 220.2715 - loglik: -2.2056e+02 - logprior: 0.2887
Fitted a model with MAP estimate = -219.9751
expansions: [(8, 2), (13, 2), (14, 1), (24, 2), (29, 1), (40, 2), (41, 2), (47, 1), (48, 2), (50, 1), (61, 1), (62, 1), (63, 1), (68, 2), (77, 1), (79, 1), (80, 2), (81, 2), (82, 1), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 318.6642 - loglik: -2.2274e+02 - logprior: -9.5923e+01
Epoch 2/2
10/10 - 1s - loss: 241.2339 - loglik: -2.0406e+02 - logprior: -3.7173e+01
Fitted a model with MAP estimate = -227.7502
expansions: [(0, 3)]
discards: [  0  29  47  60  87 102 105 117 118]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 276.9320 - loglik: -2.0141e+02 - logprior: -7.5526e+01
Epoch 2/2
10/10 - 1s - loss: 210.9329 - loglik: -1.9467e+02 - logprior: -1.6261e+01
Fitted a model with MAP estimate = -200.4871
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 287.9256 - loglik: -1.9724e+02 - logprior: -9.0690e+01
Epoch 2/10
10/10 - 1s - loss: 219.0100 - loglik: -1.9514e+02 - logprior: -2.3872e+01
Epoch 3/10
10/10 - 1s - loss: 198.4040 - loglik: -1.9344e+02 - logprior: -4.9644e+00
Epoch 4/10
10/10 - 1s - loss: 190.5778 - loglik: -1.9227e+02 - logprior: 1.6913
Epoch 5/10
10/10 - 1s - loss: 186.8723 - loglik: -1.9168e+02 - logprior: 4.8041
Epoch 6/10
10/10 - 1s - loss: 184.8949 - loglik: -1.9146e+02 - logprior: 6.5690
Epoch 7/10
10/10 - 1s - loss: 183.6517 - loglik: -1.9139e+02 - logprior: 7.7366
Epoch 8/10
10/10 - 1s - loss: 182.7227 - loglik: -1.9136e+02 - logprior: 8.6397
Epoch 9/10
10/10 - 1s - loss: 182.0091 - loglik: -1.9142e+02 - logprior: 9.4062
Epoch 10/10
10/10 - 1s - loss: 181.4050 - loglik: -1.9146e+02 - logprior: 10.0593
Fitted a model with MAP estimate = -181.1141
Time for alignment: 42.9813
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.7382 - loglik: -3.4716e+02 - logprior: -8.5575e+01
Epoch 2/10
10/10 - 1s - loss: 326.9818 - loglik: -3.0712e+02 - logprior: -1.9865e+01
Epoch 3/10
10/10 - 1s - loss: 273.6293 - loglik: -2.6549e+02 - logprior: -8.1344e+00
Epoch 4/10
10/10 - 1s - loss: 244.0961 - loglik: -2.3906e+02 - logprior: -5.0402e+00
Epoch 5/10
10/10 - 1s - loss: 232.7668 - loglik: -2.2986e+02 - logprior: -2.9090e+00
Epoch 6/10
10/10 - 1s - loss: 227.2222 - loglik: -2.2567e+02 - logprior: -1.5485e+00
Epoch 7/10
10/10 - 1s - loss: 223.8911 - loglik: -2.2298e+02 - logprior: -9.1046e-01
Epoch 8/10
10/10 - 1s - loss: 222.0848 - loglik: -2.2173e+02 - logprior: -3.5699e-01
Epoch 9/10
10/10 - 1s - loss: 220.9969 - loglik: -2.2104e+02 - logprior: 0.0440
Epoch 10/10
10/10 - 1s - loss: 220.2715 - loglik: -2.2056e+02 - logprior: 0.2887
Fitted a model with MAP estimate = -219.9750
expansions: [(8, 2), (13, 2), (14, 1), (24, 2), (29, 1), (40, 2), (41, 2), (47, 1), (48, 2), (50, 1), (61, 1), (62, 1), (63, 1), (68, 2), (77, 1), (79, 1), (80, 2), (81, 2), (82, 1), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 318.6643 - loglik: -2.2274e+02 - logprior: -9.5923e+01
Epoch 2/2
10/10 - 1s - loss: 241.2340 - loglik: -2.0406e+02 - logprior: -3.7173e+01
Fitted a model with MAP estimate = -227.7504
expansions: [(0, 3)]
discards: [  0  29  47  60  87 102 105 117 118]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 276.9320 - loglik: -2.0141e+02 - logprior: -7.5526e+01
Epoch 2/2
10/10 - 1s - loss: 210.9328 - loglik: -1.9467e+02 - logprior: -1.6261e+01
Fitted a model with MAP estimate = -200.4871
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 287.9257 - loglik: -1.9724e+02 - logprior: -9.0690e+01
Epoch 2/10
10/10 - 1s - loss: 219.0100 - loglik: -1.9514e+02 - logprior: -2.3872e+01
Epoch 3/10
10/10 - 1s - loss: 198.4040 - loglik: -1.9344e+02 - logprior: -4.9644e+00
Epoch 4/10
10/10 - 1s - loss: 190.5777 - loglik: -1.9227e+02 - logprior: 1.6913
Epoch 5/10
10/10 - 1s - loss: 186.8723 - loglik: -1.9168e+02 - logprior: 4.8043
Epoch 6/10
10/10 - 1s - loss: 184.8948 - loglik: -1.9146e+02 - logprior: 6.5692
Epoch 7/10
10/10 - 1s - loss: 183.6516 - loglik: -1.9139e+02 - logprior: 7.7368
Epoch 8/10
10/10 - 1s - loss: 182.7225 - loglik: -1.9136e+02 - logprior: 8.6399
Epoch 9/10
10/10 - 1s - loss: 182.0089 - loglik: -1.9142e+02 - logprior: 9.4065
Epoch 10/10
10/10 - 1s - loss: 181.4047 - loglik: -1.9146e+02 - logprior: 10.0596
Fitted a model with MAP estimate = -181.1141
Time for alignment: 44.4451
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.7382 - loglik: -3.4716e+02 - logprior: -8.5575e+01
Epoch 2/10
10/10 - 1s - loss: 326.9818 - loglik: -3.0712e+02 - logprior: -1.9865e+01
Epoch 3/10
10/10 - 1s - loss: 273.6293 - loglik: -2.6549e+02 - logprior: -8.1344e+00
Epoch 4/10
10/10 - 1s - loss: 244.0961 - loglik: -2.3906e+02 - logprior: -5.0402e+00
Epoch 5/10
10/10 - 1s - loss: 232.7668 - loglik: -2.2986e+02 - logprior: -2.9090e+00
Epoch 6/10
10/10 - 1s - loss: 227.2222 - loglik: -2.2567e+02 - logprior: -1.5485e+00
Epoch 7/10
10/10 - 1s - loss: 223.8911 - loglik: -2.2298e+02 - logprior: -9.1046e-01
Epoch 8/10
10/10 - 1s - loss: 222.0847 - loglik: -2.2173e+02 - logprior: -3.5699e-01
Epoch 9/10
10/10 - 1s - loss: 220.9969 - loglik: -2.2104e+02 - logprior: 0.0439
Epoch 10/10
10/10 - 1s - loss: 220.2714 - loglik: -2.2056e+02 - logprior: 0.2886
Fitted a model with MAP estimate = -219.9752
expansions: [(8, 2), (13, 2), (14, 1), (24, 2), (29, 1), (40, 2), (41, 2), (47, 1), (48, 2), (50, 1), (61, 1), (62, 1), (63, 1), (68, 2), (77, 1), (79, 1), (80, 2), (81, 2), (82, 1), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 318.6644 - loglik: -2.2274e+02 - logprior: -9.5923e+01
Epoch 2/2
10/10 - 1s - loss: 241.2341 - loglik: -2.0406e+02 - logprior: -3.7173e+01
Fitted a model with MAP estimate = -227.7505
expansions: [(0, 3)]
discards: [  0  29  47  60  87 102 105 117 118]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 276.9320 - loglik: -2.0141e+02 - logprior: -7.5526e+01
Epoch 2/2
10/10 - 1s - loss: 210.9329 - loglik: -1.9467e+02 - logprior: -1.6261e+01
Fitted a model with MAP estimate = -200.4871
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 287.9257 - loglik: -1.9724e+02 - logprior: -9.0690e+01
Epoch 2/10
10/10 - 1s - loss: 219.0100 - loglik: -1.9514e+02 - logprior: -2.3872e+01
Epoch 3/10
10/10 - 1s - loss: 198.4040 - loglik: -1.9344e+02 - logprior: -4.9643e+00
Epoch 4/10
10/10 - 1s - loss: 190.5777 - loglik: -1.9227e+02 - logprior: 1.6914
Epoch 5/10
10/10 - 1s - loss: 186.8723 - loglik: -1.9168e+02 - logprior: 4.8043
Epoch 6/10
10/10 - 1s - loss: 184.8947 - loglik: -1.9146e+02 - logprior: 6.5692
Epoch 7/10
10/10 - 1s - loss: 183.6513 - loglik: -1.9139e+02 - logprior: 7.7369
Epoch 8/10
10/10 - 1s - loss: 182.7223 - loglik: -1.9136e+02 - logprior: 8.6400
Epoch 9/10
10/10 - 1s - loss: 182.0086 - loglik: -1.9142e+02 - logprior: 9.4066
Epoch 10/10
10/10 - 1s - loss: 181.4045 - loglik: -1.9146e+02 - logprior: 10.0598
Fitted a model with MAP estimate = -181.1138
Time for alignment: 42.6121
Computed alignments with likelihoods: ['-181.1137', '-181.1144', '-181.1141', '-181.1141', '-181.1138']
Best model has likelihood: -181.1137  (prior= 10.3799 )
time for generating output: 0.1313
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.8985074626865671
Training of 5 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feafd26ae80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5a93d7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1bc8a90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.1843 - loglik: -1.3551e+02 - logprior: -3.1678e+01
Epoch 2/10
10/10 - 0s - loss: 123.5152 - loglik: -1.1454e+02 - logprior: -8.9719e+00
Epoch 3/10
10/10 - 0s - loss: 98.9512 - loglik: -9.4064e+01 - logprior: -4.8872e+00
Epoch 4/10
10/10 - 0s - loss: 84.7426 - loglik: -8.0903e+01 - logprior: -3.8391e+00
Epoch 5/10
10/10 - 0s - loss: 78.6664 - loglik: -7.5068e+01 - logprior: -3.5979e+00
Epoch 6/10
10/10 - 0s - loss: 76.4180 - loglik: -7.3279e+01 - logprior: -3.1389e+00
Epoch 7/10
10/10 - 0s - loss: 75.1892 - loglik: -7.2438e+01 - logprior: -2.7508e+00
Epoch 8/10
10/10 - 0s - loss: 74.4726 - loglik: -7.1865e+01 - logprior: -2.6075e+00
Epoch 9/10
10/10 - 0s - loss: 73.8240 - loglik: -7.1309e+01 - logprior: -2.5150e+00
Epoch 10/10
10/10 - 0s - loss: 73.6822 - loglik: -7.1270e+01 - logprior: -2.4118e+00
Fitted a model with MAP estimate = -73.6073
expansions: [(0, 2), (3, 2), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 50 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 112.3218 - loglik: -7.0225e+01 - logprior: -4.2097e+01
Epoch 2/2
10/10 - 0s - loss: 75.9757 - loglik: -6.2351e+01 - logprior: -1.3624e+01
Fitted a model with MAP estimate = -69.6865
expansions: []
discards: [ 0 31 34]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 98.8870 - loglik: -6.2799e+01 - logprior: -3.6088e+01
Epoch 2/2
10/10 - 0s - loss: 77.1026 - loglik: -6.2643e+01 - logprior: -1.4459e+01
Fitted a model with MAP estimate = -72.7057
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 93.6000 - loglik: -6.1622e+01 - logprior: -3.1978e+01
Epoch 2/10
10/10 - 0s - loss: 70.6303 - loglik: -6.1424e+01 - logprior: -9.2066e+00
Epoch 3/10
10/10 - 0s - loss: 65.3988 - loglik: -6.0789e+01 - logprior: -4.6096e+00
Epoch 4/10
10/10 - 0s - loss: 63.3940 - loglik: -6.0245e+01 - logprior: -3.1493e+00
Epoch 5/10
10/10 - 0s - loss: 62.5893 - loglik: -6.0500e+01 - logprior: -2.0897e+00
Epoch 6/10
10/10 - 0s - loss: 61.9683 - loglik: -6.0489e+01 - logprior: -1.4788e+00
Epoch 7/10
10/10 - 0s - loss: 61.5656 - loglik: -6.0329e+01 - logprior: -1.2367e+00
Epoch 8/10
10/10 - 0s - loss: 61.5081 - loglik: -6.0481e+01 - logprior: -1.0273e+00
Epoch 9/10
10/10 - 0s - loss: 61.2359 - loglik: -6.0358e+01 - logprior: -8.7820e-01
Epoch 10/10
10/10 - 0s - loss: 61.1247 - loglik: -6.0325e+01 - logprior: -7.9955e-01
Fitted a model with MAP estimate = -61.1129
Time for alignment: 26.5756
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.1103 - loglik: -1.3543e+02 - logprior: -3.1678e+01
Epoch 2/10
10/10 - 0s - loss: 123.3403 - loglik: -1.1437e+02 - logprior: -8.9745e+00
Epoch 3/10
10/10 - 0s - loss: 98.8083 - loglik: -9.3910e+01 - logprior: -4.8987e+00
Epoch 4/10
10/10 - 0s - loss: 84.4389 - loglik: -8.0556e+01 - logprior: -3.8833e+00
Epoch 5/10
10/10 - 0s - loss: 78.2983 - loglik: -7.4625e+01 - logprior: -3.6736e+00
Epoch 6/10
10/10 - 0s - loss: 75.7391 - loglik: -7.2513e+01 - logprior: -3.2262e+00
Epoch 7/10
10/10 - 0s - loss: 74.5835 - loglik: -7.1774e+01 - logprior: -2.8096e+00
Epoch 8/10
10/10 - 0s - loss: 74.1573 - loglik: -7.1525e+01 - logprior: -2.6319e+00
Epoch 9/10
10/10 - 0s - loss: 73.8099 - loglik: -7.1283e+01 - logprior: -2.5273e+00
Epoch 10/10
10/10 - 0s - loss: 73.6977 - loglik: -7.1286e+01 - logprior: -2.4115e+00
Fitted a model with MAP estimate = -73.5939
expansions: [(0, 2), (3, 2), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 50 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 112.2441 - loglik: -7.0147e+01 - logprior: -4.2097e+01
Epoch 2/2
10/10 - 0s - loss: 76.0307 - loglik: -6.2404e+01 - logprior: -1.3627e+01
Fitted a model with MAP estimate = -69.6800
expansions: []
discards: [ 0 31 34]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 98.9826 - loglik: -6.2896e+01 - logprior: -3.6087e+01
Epoch 2/2
10/10 - 0s - loss: 76.9366 - loglik: -6.2479e+01 - logprior: -1.4457e+01
Fitted a model with MAP estimate = -72.7048
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 93.7424 - loglik: -6.1766e+01 - logprior: -3.1977e+01
Epoch 2/10
10/10 - 0s - loss: 70.5773 - loglik: -6.1369e+01 - logprior: -9.2084e+00
Epoch 3/10
10/10 - 0s - loss: 65.2846 - loglik: -6.0677e+01 - logprior: -4.6077e+00
Epoch 4/10
10/10 - 0s - loss: 63.3860 - loglik: -6.0235e+01 - logprior: -3.1507e+00
Epoch 5/10
10/10 - 0s - loss: 62.6256 - loglik: -6.0537e+01 - logprior: -2.0889e+00
Epoch 6/10
10/10 - 0s - loss: 61.7905 - loglik: -6.0315e+01 - logprior: -1.4759e+00
Epoch 7/10
10/10 - 0s - loss: 61.7659 - loglik: -6.0522e+01 - logprior: -1.2444e+00
Epoch 8/10
10/10 - 1s - loss: 61.3768 - loglik: -6.0353e+01 - logprior: -1.0242e+00
Epoch 9/10
10/10 - 0s - loss: 61.4076 - loglik: -6.0528e+01 - logprior: -8.7981e-01
Fitted a model with MAP estimate = -61.2189
Time for alignment: 25.4791
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.1762 - loglik: -1.3550e+02 - logprior: -3.1678e+01
Epoch 2/10
10/10 - 0s - loss: 123.6531 - loglik: -1.1468e+02 - logprior: -8.9693e+00
Epoch 3/10
10/10 - 0s - loss: 99.4437 - loglik: -9.4572e+01 - logprior: -4.8716e+00
Epoch 4/10
10/10 - 0s - loss: 84.6654 - loglik: -8.0824e+01 - logprior: -3.8418e+00
Epoch 5/10
10/10 - 0s - loss: 78.1603 - loglik: -7.4485e+01 - logprior: -3.6752e+00
Epoch 6/10
10/10 - 0s - loss: 75.6162 - loglik: -7.2346e+01 - logprior: -3.2705e+00
Epoch 7/10
10/10 - 0s - loss: 74.6726 - loglik: -7.1852e+01 - logprior: -2.8211e+00
Epoch 8/10
10/10 - 0s - loss: 74.0616 - loglik: -7.1433e+01 - logprior: -2.6287e+00
Epoch 9/10
10/10 - 0s - loss: 73.9220 - loglik: -7.1388e+01 - logprior: -2.5337e+00
Epoch 10/10
10/10 - 0s - loss: 73.6823 - loglik: -7.1266e+01 - logprior: -2.4162e+00
Fitted a model with MAP estimate = -73.5982
expansions: [(0, 2), (3, 2), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 50 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 112.1489 - loglik: -7.0048e+01 - logprior: -4.2101e+01
Epoch 2/2
10/10 - 0s - loss: 76.1569 - loglik: -6.2528e+01 - logprior: -1.3629e+01
Fitted a model with MAP estimate = -69.6911
expansions: []
discards: [ 0 31 34]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 98.9610 - loglik: -6.2876e+01 - logprior: -3.6085e+01
Epoch 2/2
10/10 - 0s - loss: 76.9769 - loglik: -6.2514e+01 - logprior: -1.4463e+01
Fitted a model with MAP estimate = -72.7114
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 93.7436 - loglik: -6.1763e+01 - logprior: -3.1980e+01
Epoch 2/10
10/10 - 0s - loss: 70.5850 - loglik: -6.1381e+01 - logprior: -9.2038e+00
Epoch 3/10
10/10 - 0s - loss: 65.3387 - loglik: -6.0727e+01 - logprior: -4.6121e+00
Epoch 4/10
10/10 - 0s - loss: 63.4423 - loglik: -6.0297e+01 - logprior: -3.1451e+00
Epoch 5/10
10/10 - 0s - loss: 62.4411 - loglik: -6.0351e+01 - logprior: -2.0906e+00
Epoch 6/10
10/10 - 0s - loss: 62.0278 - loglik: -6.0550e+01 - logprior: -1.4783e+00
Epoch 7/10
10/10 - 0s - loss: 61.6012 - loglik: -6.0362e+01 - logprior: -1.2396e+00
Epoch 8/10
10/10 - 0s - loss: 61.4314 - loglik: -6.0402e+01 - logprior: -1.0296e+00
Epoch 9/10
10/10 - 0s - loss: 61.3284 - loglik: -6.0453e+01 - logprior: -8.7492e-01
Epoch 10/10
10/10 - 0s - loss: 61.1699 - loglik: -6.0369e+01 - logprior: -8.0119e-01
Fitted a model with MAP estimate = -61.1107
Time for alignment: 25.7093
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.1609 - loglik: -1.3548e+02 - logprior: -3.1677e+01
Epoch 2/10
10/10 - 0s - loss: 123.4717 - loglik: -1.1450e+02 - logprior: -8.9690e+00
Epoch 3/10
10/10 - 0s - loss: 98.8751 - loglik: -9.3995e+01 - logprior: -4.8801e+00
Epoch 4/10
10/10 - 0s - loss: 84.4790 - loglik: -8.0625e+01 - logprior: -3.8544e+00
Epoch 5/10
10/10 - 0s - loss: 78.2980 - loglik: -7.4650e+01 - logprior: -3.6482e+00
Epoch 6/10
10/10 - 0s - loss: 75.8404 - loglik: -7.2617e+01 - logprior: -3.2239e+00
Epoch 7/10
10/10 - 0s - loss: 74.7727 - loglik: -7.1966e+01 - logprior: -2.8070e+00
Epoch 8/10
10/10 - 0s - loss: 74.1223 - loglik: -7.1494e+01 - logprior: -2.6286e+00
Epoch 9/10
10/10 - 0s - loss: 73.8657 - loglik: -7.1342e+01 - logprior: -2.5240e+00
Epoch 10/10
10/10 - 0s - loss: 73.7042 - loglik: -7.1288e+01 - logprior: -2.4165e+00
Fitted a model with MAP estimate = -73.5958
expansions: [(0, 2), (3, 2), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 50 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 112.1833 - loglik: -7.0088e+01 - logprior: -4.2095e+01
Epoch 2/2
10/10 - 0s - loss: 76.0990 - loglik: -6.2465e+01 - logprior: -1.3634e+01
Fitted a model with MAP estimate = -69.6911
expansions: []
discards: [ 0 31 34]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 98.9102 - loglik: -6.2825e+01 - logprior: -3.6085e+01
Epoch 2/2
10/10 - 1s - loss: 77.1333 - loglik: -6.2670e+01 - logprior: -1.4463e+01
Fitted a model with MAP estimate = -72.7065
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 93.7967 - loglik: -6.1818e+01 - logprior: -3.1979e+01
Epoch 2/10
10/10 - 0s - loss: 70.4406 - loglik: -6.1239e+01 - logprior: -9.2013e+00
Epoch 3/10
10/10 - 0s - loss: 65.4271 - loglik: -6.0810e+01 - logprior: -4.6176e+00
Epoch 4/10
10/10 - 0s - loss: 63.3908 - loglik: -6.0248e+01 - logprior: -3.1433e+00
Epoch 5/10
10/10 - 0s - loss: 62.3862 - loglik: -6.0295e+01 - logprior: -2.0915e+00
Epoch 6/10
10/10 - 0s - loss: 62.0755 - loglik: -6.0597e+01 - logprior: -1.4787e+00
Epoch 7/10
10/10 - 0s - loss: 61.6671 - loglik: -6.0426e+01 - logprior: -1.2407e+00
Epoch 8/10
10/10 - 0s - loss: 61.4715 - loglik: -6.0439e+01 - logprior: -1.0329e+00
Epoch 9/10
10/10 - 0s - loss: 61.2536 - loglik: -6.0377e+01 - logprior: -8.7626e-01
Epoch 10/10
10/10 - 0s - loss: 61.1941 - loglik: -6.0387e+01 - logprior: -8.0746e-01
Fitted a model with MAP estimate = -61.1158
Time for alignment: 25.1678
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.1664 - loglik: -1.3549e+02 - logprior: -3.1678e+01
Epoch 2/10
10/10 - 0s - loss: 123.6298 - loglik: -1.1466e+02 - logprior: -8.9671e+00
Epoch 3/10
10/10 - 0s - loss: 99.3867 - loglik: -9.4525e+01 - logprior: -4.8613e+00
Epoch 4/10
10/10 - 0s - loss: 85.3890 - loglik: -8.1614e+01 - logprior: -3.7746e+00
Epoch 5/10
10/10 - 0s - loss: 79.1155 - loglik: -7.5600e+01 - logprior: -3.5151e+00
Epoch 6/10
10/10 - 0s - loss: 76.5780 - loglik: -7.3543e+01 - logprior: -3.0345e+00
Epoch 7/10
10/10 - 0s - loss: 75.5450 - loglik: -7.2964e+01 - logprior: -2.5806e+00
Epoch 8/10
10/10 - 0s - loss: 74.6390 - loglik: -7.2255e+01 - logprior: -2.3839e+00
Epoch 9/10
10/10 - 0s - loss: 74.5261 - loglik: -7.2249e+01 - logprior: -2.2768e+00
Epoch 10/10
10/10 - 0s - loss: 74.2287 - loglik: -7.2061e+01 - logprior: -2.1676e+00
Fitted a model with MAP estimate = -74.1025
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 114.6100 - loglik: -7.2379e+01 - logprior: -4.2231e+01
Epoch 2/2
10/10 - 0s - loss: 78.0761 - loglik: -6.4301e+01 - logprior: -1.3775e+01
Fitted a model with MAP estimate = -70.9966
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 91.0767 - loglik: -6.1240e+01 - logprior: -2.9837e+01
Epoch 2/2
10/10 - 0s - loss: 69.2807 - loglik: -6.0842e+01 - logprior: -8.4386e+00
Fitted a model with MAP estimate = -66.1772
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.8256 - loglik: -6.0595e+01 - logprior: -2.8231e+01
Epoch 2/10
10/10 - 0s - loss: 68.5417 - loglik: -6.0547e+01 - logprior: -7.9948e+00
Epoch 3/10
10/10 - 0s - loss: 64.9210 - loglik: -6.0914e+01 - logprior: -4.0072e+00
Epoch 4/10
10/10 - 0s - loss: 63.3214 - loglik: -6.0817e+01 - logprior: -2.5043e+00
Epoch 5/10
10/10 - 0s - loss: 62.5614 - loglik: -6.0760e+01 - logprior: -1.8014e+00
Epoch 6/10
10/10 - 0s - loss: 62.1076 - loglik: -6.0652e+01 - logprior: -1.4553e+00
Epoch 7/10
10/10 - 0s - loss: 61.8476 - loglik: -6.0615e+01 - logprior: -1.2329e+00
Epoch 8/10
10/10 - 0s - loss: 61.7486 - loglik: -6.0715e+01 - logprior: -1.0338e+00
Epoch 9/10
10/10 - 0s - loss: 61.4549 - loglik: -6.0568e+01 - logprior: -8.8658e-01
Epoch 10/10
10/10 - 0s - loss: 61.4748 - loglik: -6.0675e+01 - logprior: -8.0011e-01
Fitted a model with MAP estimate = -61.3744
Time for alignment: 24.7463
Computed alignments with likelihoods: ['-61.1129', '-61.2189', '-61.1107', '-61.1158', '-61.3744']
Best model has likelihood: -61.1107  (prior= -0.7600 )
time for generating output: 0.0875
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 5 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feada840220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feadac2ac10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feada838910>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 445.1491 - loglik: -4.3984e+02 - logprior: -5.3122e+00
Epoch 2/10
15/15 - 3s - loss: 369.0225 - loglik: -3.6776e+02 - logprior: -1.2576e+00
Epoch 3/10
15/15 - 3s - loss: 321.7821 - loglik: -3.2033e+02 - logprior: -1.4528e+00
Epoch 4/10
15/15 - 3s - loss: 306.6914 - loglik: -3.0509e+02 - logprior: -1.6016e+00
Epoch 5/10
15/15 - 3s - loss: 303.4588 - loglik: -3.0196e+02 - logprior: -1.4964e+00
Epoch 6/10
15/15 - 3s - loss: 301.9648 - loglik: -3.0046e+02 - logprior: -1.5095e+00
Epoch 7/10
15/15 - 3s - loss: 300.1093 - loglik: -2.9860e+02 - logprior: -1.5067e+00
Epoch 8/10
15/15 - 3s - loss: 298.8156 - loglik: -2.9731e+02 - logprior: -1.5030e+00
Epoch 9/10
15/15 - 3s - loss: 297.8691 - loglik: -2.9634e+02 - logprior: -1.5263e+00
Epoch 10/10
15/15 - 3s - loss: 297.5016 - loglik: -2.9598e+02 - logprior: -1.5252e+00
Fitted a model with MAP estimate = -297.3553
expansions: [(7, 3), (10, 1), (14, 1), (24, 2), (25, 1), (49, 2), (55, 1), (59, 1), (64, 2), (65, 2), (69, 1), (91, 1), (92, 1), (108, 1), (112, 1), (114, 3), (116, 2), (118, 1), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 300.8749 - loglik: -2.9429e+02 - logprior: -6.5802e+00
Epoch 2/2
15/15 - 4s - loss: 286.0672 - loglik: -2.8301e+02 - logprior: -3.0589e+00
Fitted a model with MAP estimate = -283.8347
expansions: [(0, 2)]
discards: [ 0  7 57 77 84]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 287.6869 - loglik: -2.8272e+02 - logprior: -4.9648e+00
Epoch 2/2
15/15 - 4s - loss: 282.0526 - loglik: -2.8060e+02 - logprior: -1.4476e+00
Fitted a model with MAP estimate = -281.0858
expansions: []
discards: [ 0 65 66]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 292.6775 - loglik: -2.8622e+02 - logprior: -6.4534e+00
Epoch 2/10
15/15 - 4s - loss: 286.7026 - loglik: -2.8461e+02 - logprior: -2.0882e+00
Epoch 3/10
15/15 - 4s - loss: 284.9884 - loglik: -2.8399e+02 - logprior: -9.9409e-01
Epoch 4/10
15/15 - 4s - loss: 281.6497 - loglik: -2.8081e+02 - logprior: -8.4448e-01
Epoch 5/10
15/15 - 4s - loss: 282.1282 - loglik: -2.8133e+02 - logprior: -7.9322e-01
Fitted a model with MAP estimate = -281.1172
Time for alignment: 107.3170
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 446.1015 - loglik: -4.4078e+02 - logprior: -5.3220e+00
Epoch 2/10
15/15 - 3s - loss: 366.8271 - loglik: -3.6555e+02 - logprior: -1.2785e+00
Epoch 3/10
15/15 - 3s - loss: 320.3831 - loglik: -3.1892e+02 - logprior: -1.4619e+00
Epoch 4/10
15/15 - 3s - loss: 307.7608 - loglik: -3.0616e+02 - logprior: -1.6004e+00
Epoch 5/10
15/15 - 3s - loss: 302.2077 - loglik: -3.0072e+02 - logprior: -1.4831e+00
Epoch 6/10
15/15 - 3s - loss: 302.3387 - loglik: -3.0083e+02 - logprior: -1.5080e+00
Fitted a model with MAP estimate = -299.8288
expansions: [(7, 3), (10, 1), (14, 1), (24, 2), (25, 1), (49, 2), (55, 1), (59, 1), (64, 1), (65, 2), (69, 1), (91, 2), (92, 1), (105, 1), (112, 1), (114, 3), (116, 2), (118, 1), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 301.3284 - loglik: -2.9473e+02 - logprior: -6.5944e+00
Epoch 2/2
15/15 - 4s - loss: 287.5320 - loglik: -2.8447e+02 - logprior: -3.0613e+00
Fitted a model with MAP estimate = -284.0514
expansions: [(0, 2)]
discards: [  0   7  57  66  83 107]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 290.2156 - loglik: -2.8525e+02 - logprior: -4.9697e+00
Epoch 2/2
15/15 - 4s - loss: 283.3787 - loglik: -2.8191e+02 - logprior: -1.4724e+00
Fitted a model with MAP estimate = -282.1886
expansions: [(65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 290.8452 - loglik: -2.8436e+02 - logprior: -6.4814e+00
Epoch 2/10
15/15 - 4s - loss: 283.7941 - loglik: -2.8163e+02 - logprior: -2.1606e+00
Epoch 3/10
15/15 - 4s - loss: 280.1903 - loglik: -2.7916e+02 - logprior: -1.0340e+00
Epoch 4/10
15/15 - 4s - loss: 280.2016 - loglik: -2.7934e+02 - logprior: -8.6518e-01
Fitted a model with MAP estimate = -278.8101
Time for alignment: 88.6730
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 445.8266 - loglik: -4.4051e+02 - logprior: -5.3181e+00
Epoch 2/10
15/15 - 3s - loss: 364.6529 - loglik: -3.6338e+02 - logprior: -1.2768e+00
Epoch 3/10
15/15 - 3s - loss: 317.2479 - loglik: -3.1578e+02 - logprior: -1.4705e+00
Epoch 4/10
15/15 - 3s - loss: 305.8806 - loglik: -3.0429e+02 - logprior: -1.5937e+00
Epoch 5/10
15/15 - 3s - loss: 303.3192 - loglik: -3.0189e+02 - logprior: -1.4323e+00
Epoch 6/10
15/15 - 3s - loss: 300.7281 - loglik: -2.9929e+02 - logprior: -1.4403e+00
Epoch 7/10
15/15 - 3s - loss: 299.5821 - loglik: -2.9814e+02 - logprior: -1.4461e+00
Epoch 8/10
15/15 - 3s - loss: 298.6148 - loglik: -2.9717e+02 - logprior: -1.4480e+00
Epoch 9/10
15/15 - 3s - loss: 297.6432 - loglik: -2.9619e+02 - logprior: -1.4532e+00
Epoch 10/10
15/15 - 3s - loss: 297.3517 - loglik: -2.9591e+02 - logprior: -1.4466e+00
Fitted a model with MAP estimate = -297.3168
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (25, 1), (26, 1), (55, 1), (59, 1), (64, 2), (65, 2), (69, 1), (91, 2), (92, 2), (112, 1), (114, 3), (116, 3), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 301.5801 - loglik: -2.9497e+02 - logprior: -6.6129e+00
Epoch 2/2
15/15 - 4s - loss: 285.8346 - loglik: -2.8274e+02 - logprior: -3.0963e+00
Fitted a model with MAP estimate = -284.1404
expansions: [(0, 2)]
discards: [  0   7  76  83 109]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 287.9729 - loglik: -2.8301e+02 - logprior: -4.9677e+00
Epoch 2/2
15/15 - 4s - loss: 282.0370 - loglik: -2.8059e+02 - logprior: -1.4457e+00
Fitted a model with MAP estimate = -281.0397
expansions: []
discards: [ 0 65]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 291.7154 - loglik: -2.8527e+02 - logprior: -6.4450e+00
Epoch 2/10
15/15 - 4s - loss: 285.0269 - loglik: -2.8297e+02 - logprior: -2.0588e+00
Epoch 3/10
15/15 - 4s - loss: 282.3620 - loglik: -2.8135e+02 - logprior: -1.0125e+00
Epoch 4/10
15/15 - 4s - loss: 281.3282 - loglik: -2.8044e+02 - logprior: -8.8371e-01
Epoch 5/10
15/15 - 4s - loss: 279.4597 - loglik: -2.7862e+02 - logprior: -8.3588e-01
Epoch 6/10
15/15 - 4s - loss: 279.3987 - loglik: -2.7859e+02 - logprior: -8.0683e-01
Epoch 7/10
15/15 - 4s - loss: 279.2813 - loglik: -2.7851e+02 - logprior: -7.6883e-01
Epoch 8/10
15/15 - 4s - loss: 278.7120 - loglik: -2.7800e+02 - logprior: -7.1602e-01
Epoch 9/10
15/15 - 4s - loss: 278.5544 - loglik: -2.7788e+02 - logprior: -6.7816e-01
Epoch 10/10
15/15 - 4s - loss: 278.6266 - loglik: -2.7799e+02 - logprior: -6.3933e-01
Fitted a model with MAP estimate = -278.4454
Time for alignment: 126.9263
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 445.5413 - loglik: -4.4023e+02 - logprior: -5.3138e+00
Epoch 2/10
15/15 - 3s - loss: 365.4024 - loglik: -3.6415e+02 - logprior: -1.2564e+00
Epoch 3/10
15/15 - 3s - loss: 320.5977 - loglik: -3.1918e+02 - logprior: -1.4133e+00
Epoch 4/10
15/15 - 3s - loss: 308.5705 - loglik: -3.0706e+02 - logprior: -1.5149e+00
Epoch 5/10
15/15 - 3s - loss: 305.3012 - loglik: -3.0393e+02 - logprior: -1.3746e+00
Epoch 6/10
15/15 - 3s - loss: 303.2416 - loglik: -3.0186e+02 - logprior: -1.3798e+00
Epoch 7/10
15/15 - 3s - loss: 302.4135 - loglik: -3.0103e+02 - logprior: -1.3800e+00
Epoch 8/10
15/15 - 3s - loss: 302.0273 - loglik: -3.0068e+02 - logprior: -1.3471e+00
Epoch 9/10
15/15 - 3s - loss: 301.2070 - loglik: -2.9986e+02 - logprior: -1.3478e+00
Epoch 10/10
15/15 - 3s - loss: 300.7762 - loglik: -2.9942e+02 - logprior: -1.3564e+00
Fitted a model with MAP estimate = -299.8932
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (27, 1), (49, 2), (60, 1), (66, 3), (91, 2), (92, 2), (112, 1), (114, 4), (116, 3), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 301.9817 - loglik: -2.9539e+02 - logprior: -6.5893e+00
Epoch 2/2
15/15 - 4s - loss: 287.6599 - loglik: -2.8458e+02 - logprior: -3.0836e+00
Fitted a model with MAP estimate = -285.4846
expansions: [(0, 2)]
discards: [  0   7  57 107]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 288.2924 - loglik: -2.8331e+02 - logprior: -4.9818e+00
Epoch 2/2
15/15 - 4s - loss: 281.8059 - loglik: -2.8035e+02 - logprior: -1.4576e+00
Fitted a model with MAP estimate = -280.8500
expansions: [(75, 1)]
discards: [ 0 65 66 82]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 294.2973 - loglik: -2.8784e+02 - logprior: -6.4619e+00
Epoch 2/10
15/15 - 4s - loss: 286.5933 - loglik: -2.8449e+02 - logprior: -2.0987e+00
Epoch 3/10
15/15 - 4s - loss: 283.9824 - loglik: -2.8297e+02 - logprior: -1.0147e+00
Epoch 4/10
15/15 - 4s - loss: 282.7841 - loglik: -2.8194e+02 - logprior: -8.3926e-01
Epoch 5/10
15/15 - 4s - loss: 282.6088 - loglik: -2.8181e+02 - logprior: -8.0076e-01
Epoch 6/10
15/15 - 4s - loss: 280.4988 - loglik: -2.7974e+02 - logprior: -7.5695e-01
Epoch 7/10
15/15 - 4s - loss: 281.2417 - loglik: -2.8052e+02 - logprior: -7.2111e-01
Fitted a model with MAP estimate = -280.8082
Time for alignment: 112.9760
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 445.5900 - loglik: -4.4028e+02 - logprior: -5.3149e+00
Epoch 2/10
15/15 - 3s - loss: 365.8984 - loglik: -3.6463e+02 - logprior: -1.2682e+00
Epoch 3/10
15/15 - 3s - loss: 323.1048 - loglik: -3.2171e+02 - logprior: -1.3903e+00
Epoch 4/10
15/15 - 3s - loss: 309.7564 - loglik: -3.0825e+02 - logprior: -1.5094e+00
Epoch 5/10
15/15 - 3s - loss: 305.9008 - loglik: -3.0452e+02 - logprior: -1.3838e+00
Epoch 6/10
15/15 - 3s - loss: 302.3125 - loglik: -3.0089e+02 - logprior: -1.4225e+00
Epoch 7/10
15/15 - 3s - loss: 302.7331 - loglik: -3.0128e+02 - logprior: -1.4494e+00
Fitted a model with MAP estimate = -301.2731
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (25, 1), (49, 2), (60, 1), (66, 3), (69, 1), (91, 2), (92, 2), (112, 1), (114, 2), (115, 1), (117, 3), (118, 1), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 303.0498 - loglik: -2.9648e+02 - logprior: -6.5717e+00
Epoch 2/2
15/15 - 4s - loss: 287.0693 - loglik: -2.8403e+02 - logprior: -3.0346e+00
Fitted a model with MAP estimate = -285.5245
expansions: [(0, 2)]
discards: [  0   7  57  78 108]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 289.3869 - loglik: -2.8441e+02 - logprior: -4.9774e+00
Epoch 2/2
15/15 - 4s - loss: 283.9045 - loglik: -2.8245e+02 - logprior: -1.4591e+00
Fitted a model with MAP estimate = -282.5314
expansions: []
discards: [ 0 81]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 292.6595 - loglik: -2.8618e+02 - logprior: -6.4811e+00
Epoch 2/10
15/15 - 4s - loss: 284.9338 - loglik: -2.8285e+02 - logprior: -2.0810e+00
Epoch 3/10
15/15 - 4s - loss: 284.2388 - loglik: -2.8323e+02 - logprior: -1.0097e+00
Epoch 4/10
15/15 - 4s - loss: 281.4572 - loglik: -2.8059e+02 - logprior: -8.6996e-01
Epoch 5/10
15/15 - 4s - loss: 280.0866 - loglik: -2.7925e+02 - logprior: -8.3365e-01
Epoch 6/10
15/15 - 4s - loss: 279.8246 - loglik: -2.7903e+02 - logprior: -7.9033e-01
Epoch 7/10
15/15 - 4s - loss: 279.9423 - loglik: -2.7919e+02 - logprior: -7.5065e-01
Fitted a model with MAP estimate = -279.8161
Time for alignment: 102.8145
Computed alignments with likelihoods: ['-281.0858', '-278.8101', '-278.4454', '-280.8082', '-279.8161']
Best model has likelihood: -278.4454  (prior= -0.5986 )
time for generating output: 0.1748
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.9180602006688964
Training of 5 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feae366d880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feae35dc190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9ddca7f0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 299.4545 - loglik: -2.9661e+02 - logprior: -2.8482e+00
Epoch 2/10
34/34 - 4s - loss: 209.8683 - loglik: -2.0810e+02 - logprior: -1.7642e+00
Epoch 3/10
34/34 - 4s - loss: 202.3384 - loglik: -2.0067e+02 - logprior: -1.6721e+00
Epoch 4/10
34/34 - 4s - loss: 200.1478 - loglik: -1.9848e+02 - logprior: -1.6635e+00
Epoch 5/10
34/34 - 5s - loss: 200.8423 - loglik: -1.9919e+02 - logprior: -1.6486e+00
Fitted a model with MAP estimate = -199.7940
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (42, 1), (43, 1), (45, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (79, 1), (82, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 187.2841 - loglik: -1.8432e+02 - logprior: -2.9662e+00
Epoch 2/2
34/34 - 5s - loss: 178.4683 - loglik: -1.7724e+02 - logprior: -1.2303e+00
Fitted a model with MAP estimate = -177.0744
expansions: []
discards: [ 33 139]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 180.5629 - loglik: -1.7790e+02 - logprior: -2.6616e+00
Epoch 2/2
34/34 - 5s - loss: 177.7907 - loglik: -1.7668e+02 - logprior: -1.1148e+00
Fitted a model with MAP estimate = -177.4098
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 179.8972 - loglik: -1.7732e+02 - logprior: -2.5776e+00
Epoch 2/10
34/34 - 5s - loss: 178.9808 - loglik: -1.7798e+02 - logprior: -1.0026e+00
Epoch 3/10
34/34 - 5s - loss: 176.5716 - loglik: -1.7566e+02 - logprior: -9.1372e-01
Epoch 4/10
34/34 - 5s - loss: 176.1971 - loglik: -1.7534e+02 - logprior: -8.5607e-01
Epoch 5/10
34/34 - 5s - loss: 176.2406 - loglik: -1.7544e+02 - logprior: -8.0548e-01
Fitted a model with MAP estimate = -175.3803
Time for alignment: 107.1268
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 298.3239 - loglik: -2.9546e+02 - logprior: -2.8618e+00
Epoch 2/10
34/34 - 5s - loss: 208.2761 - loglik: -2.0639e+02 - logprior: -1.8812e+00
Epoch 3/10
34/34 - 5s - loss: 202.7182 - loglik: -2.0090e+02 - logprior: -1.8175e+00
Epoch 4/10
34/34 - 5s - loss: 202.1537 - loglik: -2.0038e+02 - logprior: -1.7768e+00
Epoch 5/10
34/34 - 4s - loss: 200.7092 - loglik: -1.9893e+02 - logprior: -1.7781e+00
Epoch 6/10
34/34 - 5s - loss: 201.6454 - loglik: -1.9985e+02 - logprior: -1.7926e+00
Fitted a model with MAP estimate = -200.3641
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 1), (27, 1), (28, 1), (40, 1), (41, 1), (44, 1), (48, 1), (49, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (108, 2), (110, 1), (111, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 189.1094 - loglik: -1.8610e+02 - logprior: -3.0057e+00
Epoch 2/2
34/34 - 5s - loss: 178.8732 - loglik: -1.7759e+02 - logprior: -1.2857e+00
Fitted a model with MAP estimate = -177.7351
expansions: []
discards: [20]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 180.5960 - loglik: -1.7792e+02 - logprior: -2.6758e+00
Epoch 2/2
34/34 - 5s - loss: 177.8326 - loglik: -1.7671e+02 - logprior: -1.1184e+00
Fitted a model with MAP estimate = -177.4937
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 180.5403 - loglik: -1.7796e+02 - logprior: -2.5776e+00
Epoch 2/10
34/34 - 5s - loss: 177.6288 - loglik: -1.7662e+02 - logprior: -1.0116e+00
Epoch 3/10
34/34 - 5s - loss: 177.8286 - loglik: -1.7691e+02 - logprior: -9.2182e-01
Fitted a model with MAP estimate = -176.5798
Time for alignment: 102.1226
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 300.1929 - loglik: -2.9733e+02 - logprior: -2.8589e+00
Epoch 2/10
34/34 - 4s - loss: 212.6609 - loglik: -2.1089e+02 - logprior: -1.7718e+00
Epoch 3/10
34/34 - 5s - loss: 203.0929 - loglik: -2.0136e+02 - logprior: -1.7287e+00
Epoch 4/10
34/34 - 5s - loss: 201.3722 - loglik: -1.9968e+02 - logprior: -1.6939e+00
Epoch 5/10
34/34 - 4s - loss: 200.7084 - loglik: -1.9902e+02 - logprior: -1.6858e+00
Epoch 6/10
34/34 - 4s - loss: 200.1196 - loglik: -1.9842e+02 - logprior: -1.7022e+00
Epoch 7/10
34/34 - 5s - loss: 200.9239 - loglik: -1.9922e+02 - logprior: -1.7021e+00
Fitted a model with MAP estimate = -199.8515
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (48, 1), (54, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (70, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 187.5940 - loglik: -1.8460e+02 - logprior: -2.9917e+00
Epoch 2/2
34/34 - 5s - loss: 177.3350 - loglik: -1.7609e+02 - logprior: -1.2454e+00
Fitted a model with MAP estimate = -176.4699
expansions: []
discards: [ 34 140]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 180.3691 - loglik: -1.7772e+02 - logprior: -2.6470e+00
Epoch 2/2
34/34 - 5s - loss: 177.2376 - loglik: -1.7614e+02 - logprior: -1.0951e+00
Fitted a model with MAP estimate = -176.8792
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 179.6715 - loglik: -1.7712e+02 - logprior: -2.5482e+00
Epoch 2/10
34/34 - 5s - loss: 177.0630 - loglik: -1.7608e+02 - logprior: -9.8316e-01
Epoch 3/10
34/34 - 5s - loss: 177.1964 - loglik: -1.7631e+02 - logprior: -8.8372e-01
Fitted a model with MAP estimate = -175.9658
Time for alignment: 106.6452
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 300.3003 - loglik: -2.9746e+02 - logprior: -2.8385e+00
Epoch 2/10
34/34 - 5s - loss: 208.5523 - loglik: -2.0671e+02 - logprior: -1.8457e+00
Epoch 3/10
34/34 - 5s - loss: 202.2885 - loglik: -2.0049e+02 - logprior: -1.8028e+00
Epoch 4/10
34/34 - 5s - loss: 199.2713 - loglik: -1.9749e+02 - logprior: -1.7784e+00
Epoch 5/10
34/34 - 5s - loss: 200.2483 - loglik: -1.9846e+02 - logprior: -1.7874e+00
Fitted a model with MAP estimate = -199.5722
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (48, 1), (49, 1), (53, 1), (55, 1), (56, 1), (57, 1), (59, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 187.8330 - loglik: -1.8486e+02 - logprior: -2.9686e+00
Epoch 2/2
34/34 - 5s - loss: 176.9445 - loglik: -1.7571e+02 - logprior: -1.2354e+00
Fitted a model with MAP estimate = -176.5750
expansions: []
discards: [ 34 140]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 180.2294 - loglik: -1.7758e+02 - logprior: -2.6541e+00
Epoch 2/2
34/34 - 5s - loss: 177.2617 - loglik: -1.7615e+02 - logprior: -1.1090e+00
Fitted a model with MAP estimate = -176.9042
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 179.2198 - loglik: -1.7666e+02 - logprior: -2.5643e+00
Epoch 2/10
34/34 - 5s - loss: 177.6797 - loglik: -1.7668e+02 - logprior: -1.0039e+00
Epoch 3/10
34/34 - 5s - loss: 176.5109 - loglik: -1.7560e+02 - logprior: -9.1109e-01
Epoch 4/10
34/34 - 5s - loss: 175.4611 - loglik: -1.7460e+02 - logprior: -8.5895e-01
Epoch 5/10
34/34 - 5s - loss: 175.0785 - loglik: -1.7428e+02 - logprior: -8.0346e-01
Epoch 6/10
34/34 - 5s - loss: 176.0057 - loglik: -1.7527e+02 - logprior: -7.3861e-01
Fitted a model with MAP estimate = -174.7036
Time for alignment: 111.7744
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 301.1324 - loglik: -2.9828e+02 - logprior: -2.8571e+00
Epoch 2/10
34/34 - 4s - loss: 210.9270 - loglik: -2.0918e+02 - logprior: -1.7428e+00
Epoch 3/10
34/34 - 5s - loss: 203.7896 - loglik: -2.0213e+02 - logprior: -1.6599e+00
Epoch 4/10
34/34 - 5s - loss: 201.3143 - loglik: -1.9970e+02 - logprior: -1.6162e+00
Epoch 5/10
34/34 - 5s - loss: 201.8299 - loglik: -2.0022e+02 - logprior: -1.6142e+00
Fitted a model with MAP estimate = -201.0893
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (45, 3), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (78, 1), (82, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 187.8886 - loglik: -1.8491e+02 - logprior: -2.9747e+00
Epoch 2/2
34/34 - 5s - loss: 178.3405 - loglik: -1.7708e+02 - logprior: -1.2578e+00
Fitted a model with MAP estimate = -177.0428
expansions: []
discards: [ 33  58 140]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 7s - loss: 180.5563 - loglik: -1.7791e+02 - logprior: -2.6492e+00
Epoch 2/2
34/34 - 5s - loss: 177.8255 - loglik: -1.7671e+02 - logprior: -1.1115e+00
Fitted a model with MAP estimate = -177.4163
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 180.2578 - loglik: -1.7769e+02 - logprior: -2.5716e+00
Epoch 2/10
34/34 - 5s - loss: 177.5586 - loglik: -1.7656e+02 - logprior: -1.0024e+00
Epoch 3/10
34/34 - 5s - loss: 177.3586 - loglik: -1.7645e+02 - logprior: -9.0838e-01
Epoch 4/10
34/34 - 5s - loss: 176.7866 - loglik: -1.7594e+02 - logprior: -8.5129e-01
Epoch 5/10
34/34 - 5s - loss: 176.0182 - loglik: -1.7522e+02 - logprior: -7.9845e-01
Epoch 6/10
34/34 - 5s - loss: 174.8044 - loglik: -1.7406e+02 - logprior: -7.4646e-01
Epoch 7/10
34/34 - 5s - loss: 175.5674 - loglik: -1.7488e+02 - logprior: -6.8621e-01
Fitted a model with MAP estimate = -175.0382
Time for alignment: 114.9119
Computed alignments with likelihoods: ['-175.3803', '-176.5798', '-175.9658', '-174.7036', '-175.0382']
Best model has likelihood: -174.7036  (prior= -0.7072 )
time for generating output: 0.3085
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.627254076233668
Training of 5 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d4b43d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebd2d790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb16be2340>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 419.8366 - loglik: -3.9913e+02 - logprior: -2.0708e+01
Epoch 2/10
10/10 - 2s - loss: 364.1033 - loglik: -3.5935e+02 - logprior: -4.7516e+00
Epoch 3/10
10/10 - 2s - loss: 324.8961 - loglik: -3.2236e+02 - logprior: -2.5312e+00
Epoch 4/10
10/10 - 2s - loss: 297.5113 - loglik: -2.9549e+02 - logprior: -2.0183e+00
Epoch 5/10
10/10 - 2s - loss: 285.2825 - loglik: -2.8350e+02 - logprior: -1.7842e+00
Epoch 6/10
10/10 - 2s - loss: 277.8988 - loglik: -2.7622e+02 - logprior: -1.6812e+00
Epoch 7/10
10/10 - 2s - loss: 275.4003 - loglik: -2.7375e+02 - logprior: -1.6473e+00
Epoch 8/10
10/10 - 2s - loss: 273.8888 - loglik: -2.7225e+02 - logprior: -1.6347e+00
Epoch 9/10
10/10 - 2s - loss: 272.5223 - loglik: -2.7087e+02 - logprior: -1.6495e+00
Epoch 10/10
10/10 - 2s - loss: 272.3490 - loglik: -2.7073e+02 - logprior: -1.6148e+00
Fitted a model with MAP estimate = -271.8933
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 3), (29, 2), (53, 1), (60, 1), (62, 1), (79, 1), (80, 2), (81, 1), (82, 2), (83, 1), (84, 1), (86, 1), (98, 1), (101, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 287.5684 - loglik: -2.6871e+02 - logprior: -1.8855e+01
Epoch 2/2
10/10 - 2s - loss: 262.7007 - loglik: -2.5814e+02 - logprior: -4.5650e+00
Fitted a model with MAP estimate = -259.0690
expansions: []
discards: [ 0 38 95]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 283.5109 - loglik: -2.6013e+02 - logprior: -2.3385e+01
Epoch 2/2
10/10 - 2s - loss: 266.3011 - loglik: -2.5712e+02 - logprior: -9.1804e+00
Fitted a model with MAP estimate = -263.8858
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 275.1780 - loglik: -2.5671e+02 - logprior: -1.8471e+01
Epoch 2/10
10/10 - 2s - loss: 261.3376 - loglik: -2.5722e+02 - logprior: -4.1153e+00
Epoch 3/10
10/10 - 2s - loss: 256.7688 - loglik: -2.5547e+02 - logprior: -1.2958e+00
Epoch 4/10
10/10 - 2s - loss: 253.7609 - loglik: -2.5343e+02 - logprior: -3.3481e-01
Epoch 5/10
10/10 - 2s - loss: 253.5079 - loglik: -2.5360e+02 - logprior: 0.0915
Epoch 6/10
10/10 - 2s - loss: 252.0389 - loglik: -2.5239e+02 - logprior: 0.3542
Epoch 7/10
10/10 - 2s - loss: 251.5622 - loglik: -2.5216e+02 - logprior: 0.6006
Epoch 8/10
10/10 - 2s - loss: 251.7641 - loglik: -2.5251e+02 - logprior: 0.7479
Fitted a model with MAP estimate = -251.2892
Time for alignment: 61.2559
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.8576 - loglik: -3.9915e+02 - logprior: -2.0707e+01
Epoch 2/10
10/10 - 2s - loss: 364.1545 - loglik: -3.5940e+02 - logprior: -4.7542e+00
Epoch 3/10
10/10 - 2s - loss: 324.1978 - loglik: -3.2170e+02 - logprior: -2.5006e+00
Epoch 4/10
10/10 - 2s - loss: 296.4402 - loglik: -2.9437e+02 - logprior: -2.0669e+00
Epoch 5/10
10/10 - 2s - loss: 284.8845 - loglik: -2.8287e+02 - logprior: -2.0183e+00
Epoch 6/10
10/10 - 2s - loss: 277.1890 - loglik: -2.7514e+02 - logprior: -2.0466e+00
Epoch 7/10
10/10 - 2s - loss: 274.7855 - loglik: -2.7282e+02 - logprior: -1.9701e+00
Epoch 8/10
10/10 - 2s - loss: 273.2994 - loglik: -2.7145e+02 - logprior: -1.8501e+00
Epoch 9/10
10/10 - 2s - loss: 272.7560 - loglik: -2.7099e+02 - logprior: -1.7691e+00
Epoch 10/10
10/10 - 2s - loss: 271.5954 - loglik: -2.6982e+02 - logprior: -1.7724e+00
Fitted a model with MAP estimate = -271.5679
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 2), (62, 1), (79, 3), (80, 1), (81, 2), (87, 1), (97, 1), (98, 1), (101, 1), (102, 1), (109, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 295.3680 - loglik: -2.7168e+02 - logprior: -2.3689e+01
Epoch 2/2
10/10 - 2s - loss: 270.9683 - loglik: -2.6155e+02 - logprior: -9.4195e+00
Fitted a model with MAP estimate = -266.6456
expansions: [(0, 5), (127, 1)]
discards: [ 0 65]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 278.8314 - loglik: -2.6015e+02 - logprior: -1.8677e+01
Epoch 2/2
10/10 - 2s - loss: 261.0215 - loglik: -2.5670e+02 - logprior: -4.3197e+00
Fitted a model with MAP estimate = -258.5030
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 276.1151 - loglik: -2.5774e+02 - logprior: -1.8372e+01
Epoch 2/10
10/10 - 2s - loss: 261.3276 - loglik: -2.5709e+02 - logprior: -4.2370e+00
Epoch 3/10
10/10 - 2s - loss: 257.6001 - loglik: -2.5607e+02 - logprior: -1.5325e+00
Epoch 4/10
10/10 - 2s - loss: 255.2410 - loglik: -2.5458e+02 - logprior: -6.5866e-01
Epoch 5/10
10/10 - 2s - loss: 253.3281 - loglik: -2.5329e+02 - logprior: -3.7847e-02
Epoch 6/10
10/10 - 2s - loss: 253.7823 - loglik: -2.5423e+02 - logprior: 0.4474
Fitted a model with MAP estimate = -252.8717
Time for alignment: 55.9467
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.7553 - loglik: -3.9905e+02 - logprior: -2.0705e+01
Epoch 2/10
10/10 - 2s - loss: 364.9315 - loglik: -3.6019e+02 - logprior: -4.7370e+00
Epoch 3/10
10/10 - 2s - loss: 323.5915 - loglik: -3.2118e+02 - logprior: -2.4070e+00
Epoch 4/10
10/10 - 2s - loss: 296.4777 - loglik: -2.9461e+02 - logprior: -1.8727e+00
Epoch 5/10
10/10 - 2s - loss: 285.3285 - loglik: -2.8355e+02 - logprior: -1.7782e+00
Epoch 6/10
10/10 - 2s - loss: 279.5477 - loglik: -2.7770e+02 - logprior: -1.8434e+00
Epoch 7/10
10/10 - 2s - loss: 276.6491 - loglik: -2.7479e+02 - logprior: -1.8633e+00
Epoch 8/10
10/10 - 2s - loss: 275.8798 - loglik: -2.7403e+02 - logprior: -1.8499e+00
Epoch 9/10
10/10 - 2s - loss: 275.1938 - loglik: -2.7334e+02 - logprior: -1.8551e+00
Epoch 10/10
10/10 - 2s - loss: 273.6927 - loglik: -2.7185e+02 - logprior: -1.8461e+00
Fitted a model with MAP estimate = -273.7719
expansions: [(4, 1), (5, 3), (9, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (44, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (98, 1), (99, 1), (101, 2), (102, 2), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 289.1036 - loglik: -2.7019e+02 - logprior: -1.8917e+01
Epoch 2/2
10/10 - 2s - loss: 264.7516 - loglik: -2.6012e+02 - logprior: -4.6300e+00
Fitted a model with MAP estimate = -260.9405
expansions: []
discards: [  0   7 125]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 285.2154 - loglik: -2.6169e+02 - logprior: -2.3530e+01
Epoch 2/2
10/10 - 2s - loss: 269.1367 - loglik: -2.5975e+02 - logprior: -9.3879e+00
Fitted a model with MAP estimate = -266.1454
expansions: [(0, 5)]
discards: [0 5]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 278.5955 - loglik: -2.5993e+02 - logprior: -1.8664e+01
Epoch 2/10
10/10 - 2s - loss: 262.9766 - loglik: -2.5868e+02 - logprior: -4.2983e+00
Epoch 3/10
10/10 - 2s - loss: 257.9608 - loglik: -2.5649e+02 - logprior: -1.4715e+00
Epoch 4/10
10/10 - 2s - loss: 256.2667 - loglik: -2.5578e+02 - logprior: -4.8866e-01
Epoch 5/10
10/10 - 2s - loss: 255.3704 - loglik: -2.5531e+02 - logprior: -6.4614e-02
Epoch 6/10
10/10 - 2s - loss: 254.1926 - loglik: -2.5440e+02 - logprior: 0.2033
Epoch 7/10
10/10 - 2s - loss: 252.9349 - loglik: -2.5337e+02 - logprior: 0.4334
Epoch 8/10
10/10 - 2s - loss: 253.7723 - loglik: -2.5436e+02 - logprior: 0.5832
Fitted a model with MAP estimate = -253.1983
Time for alignment: 58.6510
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.3520 - loglik: -3.9864e+02 - logprior: -2.0707e+01
Epoch 2/10
10/10 - 2s - loss: 364.1480 - loglik: -3.5940e+02 - logprior: -4.7489e+00
Epoch 3/10
10/10 - 2s - loss: 319.8545 - loglik: -3.1734e+02 - logprior: -2.5145e+00
Epoch 4/10
10/10 - 2s - loss: 291.3322 - loglik: -2.8916e+02 - logprior: -2.1684e+00
Epoch 5/10
10/10 - 2s - loss: 281.6981 - loglik: -2.7962e+02 - logprior: -2.0769e+00
Epoch 6/10
10/10 - 2s - loss: 275.3660 - loglik: -2.7337e+02 - logprior: -1.9922e+00
Epoch 7/10
10/10 - 2s - loss: 274.0243 - loglik: -2.7204e+02 - logprior: -1.9805e+00
Epoch 8/10
10/10 - 2s - loss: 272.6816 - loglik: -2.7071e+02 - logprior: -1.9759e+00
Epoch 9/10
10/10 - 2s - loss: 271.6622 - loglik: -2.6972e+02 - logprior: -1.9431e+00
Epoch 10/10
10/10 - 2s - loss: 270.8667 - loglik: -2.6896e+02 - logprior: -1.9073e+00
Fitted a model with MAP estimate = -271.0883
expansions: [(4, 1), (5, 3), (9, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (60, 1), (78, 1), (79, 1), (80, 2), (81, 1), (82, 2), (83, 1), (84, 1), (86, 1), (98, 1), (101, 2), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 287.5197 - loglik: -2.6863e+02 - logprior: -1.8890e+01
Epoch 2/2
10/10 - 2s - loss: 262.5296 - loglik: -2.5791e+02 - logprior: -4.6218e+00
Fitted a model with MAP estimate = -259.1366
expansions: []
discards: [ 0  7  8 94]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 284.2448 - loglik: -2.6074e+02 - logprior: -2.3509e+01
Epoch 2/2
10/10 - 2s - loss: 266.7579 - loglik: -2.5746e+02 - logprior: -9.2971e+00
Fitted a model with MAP estimate = -264.5004
expansions: [(0, 5), (7, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 276.6180 - loglik: -2.5804e+02 - logprior: -1.8582e+01
Epoch 2/10
10/10 - 2s - loss: 260.1950 - loglik: -2.5603e+02 - logprior: -4.1689e+00
Epoch 3/10
10/10 - 2s - loss: 256.5286 - loglik: -2.5518e+02 - logprior: -1.3533e+00
Epoch 4/10
10/10 - 2s - loss: 254.8146 - loglik: -2.5445e+02 - logprior: -3.6848e-01
Epoch 5/10
10/10 - 2s - loss: 253.2721 - loglik: -2.5332e+02 - logprior: 0.0495
Epoch 6/10
10/10 - 2s - loss: 252.4232 - loglik: -2.5275e+02 - logprior: 0.3225
Epoch 7/10
10/10 - 2s - loss: 252.2681 - loglik: -2.5282e+02 - logprior: 0.5518
Epoch 8/10
10/10 - 2s - loss: 250.4503 - loglik: -2.5115e+02 - logprior: 0.7031
Epoch 9/10
10/10 - 2s - loss: 251.9040 - loglik: -2.5270e+02 - logprior: 0.7940
Fitted a model with MAP estimate = -251.2439
Time for alignment: 60.4173
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.2699 - loglik: -3.9956e+02 - logprior: -2.0707e+01
Epoch 2/10
10/10 - 2s - loss: 363.4205 - loglik: -3.5868e+02 - logprior: -4.7372e+00
Epoch 3/10
10/10 - 2s - loss: 321.5219 - loglik: -3.1909e+02 - logprior: -2.4307e+00
Epoch 4/10
10/10 - 2s - loss: 296.3693 - loglik: -2.9447e+02 - logprior: -1.9031e+00
Epoch 5/10
10/10 - 2s - loss: 284.8784 - loglik: -2.8314e+02 - logprior: -1.7423e+00
Epoch 6/10
10/10 - 2s - loss: 278.5164 - loglik: -2.7690e+02 - logprior: -1.6192e+00
Epoch 7/10
10/10 - 2s - loss: 275.8250 - loglik: -2.7426e+02 - logprior: -1.5673e+00
Epoch 8/10
10/10 - 2s - loss: 273.9263 - loglik: -2.7232e+02 - logprior: -1.6111e+00
Epoch 9/10
10/10 - 2s - loss: 273.5576 - loglik: -2.7194e+02 - logprior: -1.6177e+00
Epoch 10/10
10/10 - 2s - loss: 272.5085 - loglik: -2.7091e+02 - logprior: -1.6024e+00
Fitted a model with MAP estimate = -272.3078
expansions: [(8, 1), (9, 2), (10, 1), (17, 2), (23, 3), (24, 1), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (83, 1), (86, 2), (87, 3), (99, 1), (101, 4), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 287.7833 - loglik: -2.6892e+02 - logprior: -1.8862e+01
Epoch 2/2
10/10 - 2s - loss: 261.9560 - loglik: -2.5733e+02 - logprior: -4.6248e+00
Fitted a model with MAP estimate = -257.0527
expansions: []
discards: [  0 104 108 127 128]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 283.2992 - loglik: -2.5991e+02 - logprior: -2.3390e+01
Epoch 2/2
10/10 - 2s - loss: 266.2833 - loglik: -2.5708e+02 - logprior: -9.2032e+00
Fitted a model with MAP estimate = -263.3444
expansions: [(0, 5)]
discards: [0 8]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 276.7802 - loglik: -2.5826e+02 - logprior: -1.8519e+01
Epoch 2/10
10/10 - 2s - loss: 259.7454 - loglik: -2.5556e+02 - logprior: -4.1868e+00
Epoch 3/10
10/10 - 2s - loss: 256.5302 - loglik: -2.5519e+02 - logprior: -1.3415e+00
Epoch 4/10
10/10 - 2s - loss: 253.8878 - loglik: -2.5353e+02 - logprior: -3.5778e-01
Epoch 5/10
10/10 - 2s - loss: 252.5519 - loglik: -2.5261e+02 - logprior: 0.0596
Epoch 6/10
10/10 - 2s - loss: 252.1536 - loglik: -2.5250e+02 - logprior: 0.3436
Epoch 7/10
10/10 - 2s - loss: 251.2558 - loglik: -2.5184e+02 - logprior: 0.5853
Epoch 8/10
10/10 - 2s - loss: 251.4599 - loglik: -2.5221e+02 - logprior: 0.7460
Fitted a model with MAP estimate = -250.9813
Time for alignment: 59.7613
Computed alignments with likelihoods: ['-251.2892', '-252.8717', '-253.1983', '-251.2439', '-250.9813']
Best model has likelihood: -250.9813  (prior= 0.8046 )
time for generating output: 0.1553
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.8876811594202898
Training of 5 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb05b16b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1299d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2bfda5e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.3433 - loglik: -2.3303e+02 - logprior: -3.7316e+01
Epoch 2/10
10/10 - 1s - loss: 232.2152 - loglik: -2.2265e+02 - logprior: -9.5610e+00
Epoch 3/10
10/10 - 1s - loss: 214.5544 - loglik: -2.1021e+02 - logprior: -4.3474e+00
Epoch 4/10
10/10 - 1s - loss: 203.2543 - loglik: -2.0061e+02 - logprior: -2.6421e+00
Epoch 5/10
10/10 - 1s - loss: 197.0926 - loglik: -1.9497e+02 - logprior: -2.1176e+00
Epoch 6/10
10/10 - 1s - loss: 194.2934 - loglik: -1.9280e+02 - logprior: -1.4912e+00
Epoch 7/10
10/10 - 1s - loss: 192.8972 - loglik: -1.9207e+02 - logprior: -8.2315e-01
Epoch 8/10
10/10 - 1s - loss: 191.8910 - loglik: -1.9131e+02 - logprior: -5.8440e-01
Epoch 9/10
10/10 - 1s - loss: 191.4704 - loglik: -1.9104e+02 - logprior: -4.3284e-01
Epoch 10/10
10/10 - 1s - loss: 191.2741 - loglik: -1.9102e+02 - logprior: -2.5898e-01
Fitted a model with MAP estimate = -191.0756
expansions: [(0, 3), (6, 1), (22, 1), (28, 1), (30, 2), (31, 2), (45, 2), (49, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 239.6496 - loglik: -1.9106e+02 - logprior: -4.8590e+01
Epoch 2/2
10/10 - 1s - loss: 201.5054 - loglik: -1.8718e+02 - logprior: -1.4323e+01
Fitted a model with MAP estimate = -194.3965
expansions: [(33, 1)]
discards: [ 0 36]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 229.3118 - loglik: -1.8727e+02 - logprior: -4.2047e+01
Epoch 2/2
10/10 - 1s - loss: 202.7811 - loglik: -1.8675e+02 - logprior: -1.6034e+01
Fitted a model with MAP estimate = -198.1915
expansions: [(0, 3), (38, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.2637 - loglik: -1.8601e+02 - logprior: -3.7250e+01
Epoch 2/10
10/10 - 1s - loss: 194.6103 - loglik: -1.8532e+02 - logprior: -9.2891e+00
Epoch 3/10
10/10 - 1s - loss: 188.1374 - loglik: -1.8498e+02 - logprior: -3.1546e+00
Epoch 4/10
10/10 - 1s - loss: 185.8434 - loglik: -1.8492e+02 - logprior: -9.1859e-01
Epoch 5/10
10/10 - 1s - loss: 184.8203 - loglik: -1.8496e+02 - logprior: 0.1435
Epoch 6/10
10/10 - 1s - loss: 183.9118 - loglik: -1.8454e+02 - logprior: 0.6288
Epoch 7/10
10/10 - 1s - loss: 183.5700 - loglik: -1.8439e+02 - logprior: 0.8238
Epoch 8/10
10/10 - 1s - loss: 183.3035 - loglik: -1.8433e+02 - logprior: 1.0297
Epoch 9/10
10/10 - 1s - loss: 182.9794 - loglik: -1.8426e+02 - logprior: 1.2787
Epoch 10/10
10/10 - 1s - loss: 182.8011 - loglik: -1.8425e+02 - logprior: 1.4537
Fitted a model with MAP estimate = -182.7407
Time for alignment: 37.8657
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.3970 - loglik: -2.3308e+02 - logprior: -3.7314e+01
Epoch 2/10
10/10 - 1s - loss: 231.9852 - loglik: -2.2242e+02 - logprior: -9.5652e+00
Epoch 3/10
10/10 - 1s - loss: 214.9873 - loglik: -2.1062e+02 - logprior: -4.3713e+00
Epoch 4/10
10/10 - 1s - loss: 203.6318 - loglik: -2.0093e+02 - logprior: -2.7010e+00
Epoch 5/10
10/10 - 1s - loss: 197.1143 - loglik: -1.9487e+02 - logprior: -2.2396e+00
Epoch 6/10
10/10 - 1s - loss: 194.1165 - loglik: -1.9248e+02 - logprior: -1.6323e+00
Epoch 7/10
10/10 - 1s - loss: 192.4471 - loglik: -1.9140e+02 - logprior: -1.0452e+00
Epoch 8/10
10/10 - 1s - loss: 191.6858 - loglik: -1.9086e+02 - logprior: -8.2484e-01
Epoch 9/10
10/10 - 1s - loss: 191.1011 - loglik: -1.9045e+02 - logprior: -6.5352e-01
Epoch 10/10
10/10 - 1s - loss: 190.8503 - loglik: -1.9035e+02 - logprior: -4.9980e-01
Fitted a model with MAP estimate = -190.7409
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (31, 2), (43, 1), (44, 1), (49, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 239.0457 - loglik: -1.9044e+02 - logprior: -4.8602e+01
Epoch 2/2
10/10 - 1s - loss: 201.0733 - loglik: -1.8685e+02 - logprior: -1.4228e+01
Fitted a model with MAP estimate = -194.2864
expansions: [(37, 1), (38, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 228.9635 - loglik: -1.8699e+02 - logprior: -4.1974e+01
Epoch 2/2
10/10 - 1s - loss: 202.4422 - loglik: -1.8647e+02 - logprior: -1.5968e+01
Fitted a model with MAP estimate = -197.9607
expansions: [(0, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 223.1636 - loglik: -1.8589e+02 - logprior: -3.7274e+01
Epoch 2/10
10/10 - 1s - loss: 194.6264 - loglik: -1.8534e+02 - logprior: -9.2847e+00
Epoch 3/10
10/10 - 1s - loss: 188.5046 - loglik: -1.8535e+02 - logprior: -3.1524e+00
Epoch 4/10
10/10 - 1s - loss: 186.1703 - loglik: -1.8524e+02 - logprior: -9.3488e-01
Epoch 5/10
10/10 - 1s - loss: 185.1606 - loglik: -1.8528e+02 - logprior: 0.1189
Epoch 6/10
10/10 - 1s - loss: 184.2305 - loglik: -1.8482e+02 - logprior: 0.5945
Epoch 7/10
10/10 - 1s - loss: 183.8618 - loglik: -1.8463e+02 - logprior: 0.7667
Epoch 8/10
10/10 - 1s - loss: 183.5228 - loglik: -1.8451e+02 - logprior: 0.9864
Epoch 9/10
10/10 - 1s - loss: 183.2883 - loglik: -1.8455e+02 - logprior: 1.2650
Epoch 10/10
10/10 - 1s - loss: 183.0230 - loglik: -1.8447e+02 - logprior: 1.4516
Fitted a model with MAP estimate = -182.9689
Time for alignment: 38.0140
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.3881 - loglik: -2.3307e+02 - logprior: -3.7314e+01
Epoch 2/10
10/10 - 1s - loss: 232.1681 - loglik: -2.2260e+02 - logprior: -9.5648e+00
Epoch 3/10
10/10 - 1s - loss: 215.5265 - loglik: -2.1113e+02 - logprior: -4.3945e+00
Epoch 4/10
10/10 - 1s - loss: 203.9191 - loglik: -2.0115e+02 - logprior: -2.7681e+00
Epoch 5/10
10/10 - 1s - loss: 197.3754 - loglik: -1.9506e+02 - logprior: -2.3165e+00
Epoch 6/10
10/10 - 1s - loss: 194.0944 - loglik: -1.9236e+02 - logprior: -1.7326e+00
Epoch 7/10
10/10 - 1s - loss: 192.4933 - loglik: -1.9139e+02 - logprior: -1.1039e+00
Epoch 8/10
10/10 - 1s - loss: 191.7486 - loglik: -1.9087e+02 - logprior: -8.7829e-01
Epoch 9/10
10/10 - 1s - loss: 191.1468 - loglik: -1.9039e+02 - logprior: -7.5963e-01
Epoch 10/10
10/10 - 1s - loss: 190.8485 - loglik: -1.9022e+02 - logprior: -6.2865e-01
Fitted a model with MAP estimate = -190.7394
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (31, 2), (41, 1), (45, 1), (49, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 239.0528 - loglik: -1.9045e+02 - logprior: -4.8602e+01
Epoch 2/2
10/10 - 1s - loss: 201.0093 - loglik: -1.8678e+02 - logprior: -1.4229e+01
Fitted a model with MAP estimate = -194.3611
expansions: [(37, 1), (38, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 229.0214 - loglik: -1.8704e+02 - logprior: -4.1984e+01
Epoch 2/2
10/10 - 1s - loss: 202.7477 - loglik: -1.8677e+02 - logprior: -1.5976e+01
Fitted a model with MAP estimate = -198.0989
expansions: [(0, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.4664 - loglik: -1.8619e+02 - logprior: -3.7281e+01
Epoch 2/10
10/10 - 1s - loss: 194.5149 - loglik: -1.8522e+02 - logprior: -9.2900e+00
Epoch 3/10
10/10 - 1s - loss: 188.6263 - loglik: -1.8547e+02 - logprior: -3.1593e+00
Epoch 4/10
10/10 - 1s - loss: 186.3125 - loglik: -1.8537e+02 - logprior: -9.4021e-01
Epoch 5/10
10/10 - 1s - loss: 184.9029 - loglik: -1.8504e+02 - logprior: 0.1358
Epoch 6/10
10/10 - 1s - loss: 184.4162 - loglik: -1.8505e+02 - logprior: 0.6368
Epoch 7/10
10/10 - 1s - loss: 183.8476 - loglik: -1.8469e+02 - logprior: 0.8411
Epoch 8/10
10/10 - 1s - loss: 183.6519 - loglik: -1.8469e+02 - logprior: 1.0363
Epoch 9/10
10/10 - 1s - loss: 183.3030 - loglik: -1.8458e+02 - logprior: 1.2799
Epoch 10/10
10/10 - 1s - loss: 183.1507 - loglik: -1.8460e+02 - logprior: 1.4506
Fitted a model with MAP estimate = -183.0453
Time for alignment: 37.0741
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.4227 - loglik: -2.3311e+02 - logprior: -3.7313e+01
Epoch 2/10
10/10 - 1s - loss: 231.9449 - loglik: -2.2238e+02 - logprior: -9.5626e+00
Epoch 3/10
10/10 - 1s - loss: 214.3527 - loglik: -2.1002e+02 - logprior: -4.3317e+00
Epoch 4/10
10/10 - 1s - loss: 202.3607 - loglik: -1.9971e+02 - logprior: -2.6541e+00
Epoch 5/10
10/10 - 1s - loss: 196.5840 - loglik: -1.9447e+02 - logprior: -2.1146e+00
Epoch 6/10
10/10 - 1s - loss: 193.6499 - loglik: -1.9216e+02 - logprior: -1.4934e+00
Epoch 7/10
10/10 - 1s - loss: 192.2268 - loglik: -1.9135e+02 - logprior: -8.7334e-01
Epoch 8/10
10/10 - 1s - loss: 191.9509 - loglik: -1.9131e+02 - logprior: -6.3984e-01
Epoch 9/10
10/10 - 1s - loss: 191.3309 - loglik: -1.9083e+02 - logprior: -4.9752e-01
Epoch 10/10
10/10 - 1s - loss: 191.1549 - loglik: -1.9083e+02 - logprior: -3.2928e-01
Fitted a model with MAP estimate = -190.9867
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (31, 2), (40, 2), (43, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 239.2057 - loglik: -1.9061e+02 - logprior: -4.8595e+01
Epoch 2/2
10/10 - 1s - loss: 201.2670 - loglik: -1.8702e+02 - logprior: -1.4245e+01
Fitted a model with MAP estimate = -194.4256
expansions: [(37, 1), (38, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.1222 - loglik: -1.8714e+02 - logprior: -4.1987e+01
Epoch 2/2
10/10 - 1s - loss: 202.7126 - loglik: -1.8675e+02 - logprior: -1.5967e+01
Fitted a model with MAP estimate = -198.0405
expansions: [(0, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.1381 - loglik: -1.8586e+02 - logprior: -3.7277e+01
Epoch 2/10
10/10 - 1s - loss: 194.5465 - loglik: -1.8525e+02 - logprior: -9.3013e+00
Epoch 3/10
10/10 - 1s - loss: 188.4688 - loglik: -1.8530e+02 - logprior: -3.1735e+00
Epoch 4/10
10/10 - 1s - loss: 185.9810 - loglik: -1.8502e+02 - logprior: -9.5723e-01
Epoch 5/10
10/10 - 1s - loss: 185.0265 - loglik: -1.8512e+02 - logprior: 0.0966
Epoch 6/10
10/10 - 1s - loss: 184.1155 - loglik: -1.8471e+02 - logprior: 0.5910
Epoch 7/10
10/10 - 1s - loss: 183.6730 - loglik: -1.8445e+02 - logprior: 0.7748
Epoch 8/10
10/10 - 1s - loss: 183.4223 - loglik: -1.8441e+02 - logprior: 0.9844
Epoch 9/10
10/10 - 1s - loss: 183.1418 - loglik: -1.8439e+02 - logprior: 1.2530
Epoch 10/10
10/10 - 1s - loss: 182.8103 - loglik: -1.8424e+02 - logprior: 1.4310
Fitted a model with MAP estimate = -182.8167
Time for alignment: 36.3928
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 270.3192 - loglik: -2.3301e+02 - logprior: -3.7314e+01
Epoch 2/10
10/10 - 1s - loss: 232.2438 - loglik: -2.2268e+02 - logprior: -9.5617e+00
Epoch 3/10
10/10 - 1s - loss: 215.1911 - loglik: -2.1082e+02 - logprior: -4.3719e+00
Epoch 4/10
10/10 - 1s - loss: 203.0987 - loglik: -2.0040e+02 - logprior: -2.7003e+00
Epoch 5/10
10/10 - 1s - loss: 197.1156 - loglik: -1.9493e+02 - logprior: -2.1821e+00
Epoch 6/10
10/10 - 1s - loss: 194.2544 - loglik: -1.9272e+02 - logprior: -1.5362e+00
Epoch 7/10
10/10 - 1s - loss: 192.6679 - loglik: -1.9176e+02 - logprior: -9.0445e-01
Epoch 8/10
10/10 - 1s - loss: 191.9969 - loglik: -1.9133e+02 - logprior: -6.6496e-01
Epoch 9/10
10/10 - 1s - loss: 191.1749 - loglik: -1.9067e+02 - logprior: -5.0476e-01
Epoch 10/10
10/10 - 1s - loss: 191.1031 - loglik: -1.9076e+02 - logprior: -3.4040e-01
Fitted a model with MAP estimate = -190.9291
expansions: [(0, 3), (6, 1), (22, 1), (28, 2), (30, 2), (31, 2), (43, 1), (44, 1), (49, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 239.2619 - loglik: -1.9080e+02 - logprior: -4.8458e+01
Epoch 2/2
10/10 - 1s - loss: 200.6955 - loglik: -1.8648e+02 - logprior: -1.4215e+01
Fitted a model with MAP estimate = -194.0026
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 228.8524 - loglik: -1.8687e+02 - logprior: -4.1978e+01
Epoch 2/2
10/10 - 1s - loss: 202.7506 - loglik: -1.8672e+02 - logprior: -1.6032e+01
Fitted a model with MAP estimate = -198.0812
expansions: [(0, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.3359 - loglik: -1.8599e+02 - logprior: -3.7341e+01
Epoch 2/10
10/10 - 1s - loss: 194.8186 - loglik: -1.8547e+02 - logprior: -9.3517e+00
Epoch 3/10
10/10 - 1s - loss: 188.6308 - loglik: -1.8542e+02 - logprior: -3.2063e+00
Epoch 4/10
10/10 - 1s - loss: 186.2645 - loglik: -1.8527e+02 - logprior: -9.9362e-01
Epoch 5/10
10/10 - 1s - loss: 185.2353 - loglik: -1.8533e+02 - logprior: 0.0908
Epoch 6/10
10/10 - 1s - loss: 184.3576 - loglik: -1.8495e+02 - logprior: 0.5968
Epoch 7/10
10/10 - 1s - loss: 183.8596 - loglik: -1.8466e+02 - logprior: 0.8028
Epoch 8/10
10/10 - 1s - loss: 183.7333 - loglik: -1.8474e+02 - logprior: 1.0072
Epoch 9/10
10/10 - 1s - loss: 183.4187 - loglik: -1.8467e+02 - logprior: 1.2560
Epoch 10/10
10/10 - 1s - loss: 183.2281 - loglik: -1.8465e+02 - logprior: 1.4239
Fitted a model with MAP estimate = -183.1448
Time for alignment: 36.8557
Computed alignments with likelihoods: ['-182.7407', '-182.9689', '-183.0453', '-182.8167', '-183.1448']
Best model has likelihood: -182.7407  (prior= 1.5061 )
time for generating output: 0.1733
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.7493472584856397
Training of 5 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feae3760ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb408cab80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb408cac40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 953.9299 - loglik: -9.5268e+02 - logprior: -1.2460e+00
Epoch 2/10
43/43 - 27s - loss: 840.1375 - loglik: -8.3845e+02 - logprior: -1.6919e+00
Epoch 3/10
43/43 - 27s - loss: 831.2342 - loglik: -8.2948e+02 - logprior: -1.7538e+00
Epoch 4/10
43/43 - 27s - loss: 828.1418 - loglik: -8.2642e+02 - logprior: -1.7234e+00
Epoch 5/10
43/43 - 27s - loss: 827.4373 - loglik: -8.2573e+02 - logprior: -1.7090e+00
Epoch 6/10
43/43 - 27s - loss: 826.9662 - loglik: -8.2525e+02 - logprior: -1.7118e+00
Epoch 7/10
43/43 - 27s - loss: 825.8718 - loglik: -8.2420e+02 - logprior: -1.6753e+00
Epoch 8/10
43/43 - 27s - loss: 825.8891 - loglik: -8.2420e+02 - logprior: -1.6930e+00
Fitted a model with MAP estimate = -821.6815
expansions: [(8, 1), (13, 1), (16, 1), (20, 2), (21, 1), (22, 2), (23, 1), (24, 2), (30, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 2), (46, 1), (56, 1), (59, 1), (60, 1), (62, 1), (80, 2), (81, 1), (82, 2), (83, 1), (90, 1), (91, 2), (92, 1), (95, 1), (97, 1), (98, 1), (103, 2), (119, 1), (122, 1), (123, 1), (124, 1), (130, 2), (132, 2), (144, 1), (147, 1), (152, 1), (154, 1), (155, 3), (156, 2), (167, 1), (180, 1), (182, 1), (184, 1), (185, 1), (187, 1), (197, 1), (200, 1), (204, 2), (205, 2), (207, 1), (209, 1), (210, 1), (219, 2), (220, 1), (225, 2), (226, 2), (236, 1), (238, 3), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [  0 222]
Re-initialized the encoder parameters.
Fitting a model of length 370 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 46s - loss: 817.4470 - loglik: -8.1525e+02 - logprior: -2.1984e+00
Epoch 2/2
43/43 - 42s - loss: 800.4799 - loglik: -7.9944e+02 - logprior: -1.0398e+00
Fitted a model with MAP estimate = -796.6852
expansions: [(0, 2)]
discards: [  0  27  28  60 103 107 123 138 172 175 205 208 269 297 299 315 316 357
 360]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 42s - loss: 801.9203 - loglik: -8.0077e+02 - logprior: -1.1454e+00
Epoch 2/2
43/43 - 40s - loss: 797.6039 - loglik: -7.9718e+02 - logprior: -4.2141e-01
Fitted a model with MAP estimate = -796.4889
expansions: [(302, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 59s - loss: 781.8072 - loglik: -7.8089e+02 - logprior: -9.1299e-01
Epoch 2/10
61/61 - 54s - loss: 775.5103 - loglik: -7.7516e+02 - logprior: -3.5173e-01
Epoch 3/10
61/61 - 55s - loss: 774.7734 - loglik: -7.7449e+02 - logprior: -2.8713e-01
Epoch 4/10
61/61 - 55s - loss: 773.0148 - loglik: -7.7275e+02 - logprior: -2.6690e-01
Epoch 5/10
61/61 - 54s - loss: 773.3204 - loglik: -7.7312e+02 - logprior: -2.0166e-01
Fitted a model with MAP estimate = -771.2033
Time for alignment: 956.8743
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 954.8968 - loglik: -9.5364e+02 - logprior: -1.2596e+00
Epoch 2/10
43/43 - 27s - loss: 844.7387 - loglik: -8.4310e+02 - logprior: -1.6355e+00
Epoch 3/10
43/43 - 27s - loss: 834.5281 - loglik: -8.3285e+02 - logprior: -1.6745e+00
Epoch 4/10
43/43 - 27s - loss: 830.5955 - loglik: -8.2896e+02 - logprior: -1.6318e+00
Epoch 5/10
43/43 - 27s - loss: 831.5258 - loglik: -8.2987e+02 - logprior: -1.6537e+00
Fitted a model with MAP estimate = -825.9731
expansions: [(7, 2), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (29, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 2), (46, 1), (57, 1), (59, 1), (60, 1), (62, 1), (80, 1), (81, 1), (82, 2), (83, 1), (90, 1), (91, 2), (95, 1), (96, 1), (101, 1), (103, 2), (121, 1), (122, 1), (126, 1), (128, 1), (130, 1), (133, 1), (144, 1), (147, 1), (149, 1), (152, 1), (155, 2), (156, 2), (167, 1), (180, 1), (182, 1), (184, 1), (185, 1), (186, 1), (187, 1), (196, 1), (204, 2), (205, 3), (207, 1), (208, 1), (209, 1), (219, 2), (220, 1), (221, 1), (224, 2), (236, 1), (238, 3), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 363 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 45s - loss: 815.3683 - loglik: -8.1320e+02 - logprior: -2.1706e+00
Epoch 2/2
43/43 - 41s - loss: 799.5266 - loglik: -7.9843e+02 - logprior: -1.1006e+00
Fitted a model with MAP estimate = -799.1281
expansions: [(0, 2)]
discards: [  0  58 104 134 201 260 261 262 288 291 309 310 351]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 42s - loss: 801.6295 - loglik: -8.0047e+02 - logprior: -1.1572e+00
Epoch 2/2
43/43 - 39s - loss: 799.4073 - loglik: -7.9906e+02 - logprior: -3.4677e-01
Fitted a model with MAP estimate = -800.1292
expansions: [(301, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 58s - loss: 783.2173 - loglik: -7.8251e+02 - logprior: -7.0756e-01
Epoch 2/10
61/61 - 54s - loss: 774.9782 - loglik: -7.7450e+02 - logprior: -4.7326e-01
Epoch 3/10
61/61 - 54s - loss: 774.2843 - loglik: -7.7382e+02 - logprior: -4.6642e-01
Epoch 4/10
61/61 - 54s - loss: 772.8234 - loglik: -7.7241e+02 - logprior: -4.1749e-01
Epoch 5/10
61/61 - 54s - loss: 774.1823 - loglik: -7.7381e+02 - logprior: -3.7584e-01
Fitted a model with MAP estimate = -771.6953
Time for alignment: 867.8605
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 952.3051 - loglik: -9.5108e+02 - logprior: -1.2280e+00
Epoch 2/10
43/43 - 27s - loss: 839.4849 - loglik: -8.3793e+02 - logprior: -1.5578e+00
Epoch 3/10
43/43 - 27s - loss: 830.8083 - loglik: -8.2920e+02 - logprior: -1.6085e+00
Epoch 4/10
43/43 - 27s - loss: 826.0932 - loglik: -8.2446e+02 - logprior: -1.6321e+00
Epoch 5/10
43/43 - 27s - loss: 826.3870 - loglik: -8.2483e+02 - logprior: -1.5570e+00
Fitted a model with MAP estimate = -823.2937
expansions: [(0, 2), (16, 1), (17, 1), (20, 1), (21, 1), (22, 1), (24, 2), (29, 1), (36, 1), (39, 1), (41, 1), (42, 1), (45, 2), (46, 1), (56, 1), (59, 1), (60, 1), (61, 2), (80, 2), (81, 1), (82, 2), (83, 1), (90, 1), (91, 2), (92, 1), (95, 1), (97, 1), (98, 1), (103, 2), (121, 1), (122, 1), (126, 1), (128, 2), (130, 1), (143, 1), (148, 2), (153, 2), (154, 1), (155, 3), (157, 2), (167, 1), (183, 3), (184, 1), (185, 1), (187, 1), (197, 1), (200, 2), (205, 2), (206, 2), (207, 1), (210, 1), (217, 1), (219, 1), (221, 1), (225, 2), (226, 2), (239, 4), (240, 1), (242, 1), (245, 1), (250, 1), (252, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 373 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 814.4772 - loglik: -8.1261e+02 - logprior: -1.8629e+00
Epoch 2/2
43/43 - 43s - loss: 797.5809 - loglik: -7.9666e+02 - logprior: -9.1689e-01
Fitted a model with MAP estimate = -797.8100
expansions: []
discards: [  0   1  59  81 103 107 123 138 168 200 209 239 262 269 270 299 301 317
 318 319 361 363]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 42s - loss: 802.3701 - loglik: -8.0114e+02 - logprior: -1.2260e+00
Epoch 2/2
43/43 - 39s - loss: 798.5086 - loglik: -7.9818e+02 - logprior: -3.2961e-01
Fitted a model with MAP estimate = -797.9417
expansions: [(0, 2), (300, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 355 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 58s - loss: 779.5619 - loglik: -7.7875e+02 - logprior: -8.1192e-01
Epoch 2/10
61/61 - 55s - loss: 777.7494 - loglik: -7.7725e+02 - logprior: -5.0096e-01
Epoch 3/10
61/61 - 55s - loss: 773.7578 - loglik: -7.7329e+02 - logprior: -4.7182e-01
Epoch 4/10
61/61 - 55s - loss: 772.9440 - loglik: -7.7254e+02 - logprior: -3.9997e-01
Epoch 5/10
61/61 - 55s - loss: 773.0246 - loglik: -7.7263e+02 - logprior: -3.9527e-01
Fitted a model with MAP estimate = -771.8450
Time for alignment: 876.0385
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 953.2307 - loglik: -9.5199e+02 - logprior: -1.2448e+00
Epoch 2/10
43/43 - 27s - loss: 841.3912 - loglik: -8.3987e+02 - logprior: -1.5184e+00
Epoch 3/10
43/43 - 27s - loss: 829.8084 - loglik: -8.2825e+02 - logprior: -1.5620e+00
Epoch 4/10
43/43 - 27s - loss: 828.0953 - loglik: -8.2657e+02 - logprior: -1.5257e+00
Epoch 5/10
43/43 - 27s - loss: 825.5272 - loglik: -8.2399e+02 - logprior: -1.5349e+00
Epoch 6/10
43/43 - 27s - loss: 827.5146 - loglik: -8.2602e+02 - logprior: -1.4946e+00
Fitted a model with MAP estimate = -824.1363
expansions: [(0, 2), (16, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 2), (30, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 2), (46, 1), (57, 1), (59, 1), (62, 1), (64, 2), (80, 2), (81, 1), (82, 2), (83, 1), (90, 1), (91, 2), (92, 1), (95, 1), (97, 1), (98, 1), (103, 2), (119, 1), (120, 1), (121, 1), (124, 1), (130, 1), (132, 2), (144, 1), (147, 1), (149, 1), (152, 1), (155, 2), (156, 2), (167, 1), (183, 4), (184, 2), (187, 1), (200, 2), (205, 3), (206, 2), (207, 1), (210, 1), (217, 1), (219, 1), (221, 1), (222, 1), (225, 2), (226, 2), (239, 3), (240, 1), (242, 1), (245, 1), (250, 1), (253, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 373 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 46s - loss: 813.8940 - loglik: -8.1202e+02 - logprior: -1.8751e+00
Epoch 2/2
43/43 - 43s - loss: 797.4693 - loglik: -7.9656e+02 - logprior: -9.0640e-01
Fitted a model with MAP estimate = -797.2948
expansions: []
discards: [  1  28  60  85 104 108 123 139 175 207 238 239 261 269 270 296 300 302
 318 361 363]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 802.4905 - loglik: -8.0129e+02 - logprior: -1.2011e+00
Epoch 2/2
43/43 - 39s - loss: 798.2565 - loglik: -7.9784e+02 - logprior: -4.2003e-01
Fitted a model with MAP estimate = -798.8659
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 781.5673 - loglik: -7.8086e+02 - logprior: -7.0664e-01
Epoch 2/10
61/61 - 54s - loss: 776.5850 - loglik: -7.7613e+02 - logprior: -4.5023e-01
Epoch 3/10
61/61 - 54s - loss: 774.3625 - loglik: -7.7396e+02 - logprior: -4.0351e-01
Epoch 4/10
61/61 - 54s - loss: 773.7206 - loglik: -7.7332e+02 - logprior: -3.9829e-01
Epoch 5/10
61/61 - 54s - loss: 773.0411 - loglik: -7.7271e+02 - logprior: -3.2632e-01
Epoch 6/10
61/61 - 54s - loss: 771.9431 - loglik: -7.7164e+02 - logprior: -3.0498e-01
Epoch 7/10
61/61 - 54s - loss: 771.1403 - loglik: -7.7090e+02 - logprior: -2.4387e-01
Epoch 8/10
61/61 - 54s - loss: 772.7667 - loglik: -7.7259e+02 - logprior: -1.7855e-01
Fitted a model with MAP estimate = -771.3771
Time for alignment: 1064.9668
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 954.0919 - loglik: -9.5286e+02 - logprior: -1.2344e+00
Epoch 2/10
43/43 - 27s - loss: 844.4625 - loglik: -8.4280e+02 - logprior: -1.6584e+00
Epoch 3/10
43/43 - 27s - loss: 832.8779 - loglik: -8.3117e+02 - logprior: -1.7119e+00
Epoch 4/10
43/43 - 27s - loss: 829.6671 - loglik: -8.2801e+02 - logprior: -1.6619e+00
Epoch 5/10
43/43 - 27s - loss: 827.7703 - loglik: -8.2609e+02 - logprior: -1.6802e+00
Epoch 6/10
43/43 - 27s - loss: 827.4406 - loglik: -8.2578e+02 - logprior: -1.6561e+00
Epoch 7/10
43/43 - 27s - loss: 827.3854 - loglik: -8.2573e+02 - logprior: -1.6580e+00
Epoch 8/10
43/43 - 27s - loss: 827.0532 - loglik: -8.2539e+02 - logprior: -1.6617e+00
Epoch 9/10
43/43 - 27s - loss: 827.3638 - loglik: -8.2569e+02 - logprior: -1.6766e+00
Fitted a model with MAP estimate = -826.0281
expansions: [(7, 2), (16, 1), (17, 1), (20, 1), (21, 1), (22, 1), (24, 2), (29, 1), (33, 1), (39, 1), (41, 1), (42, 1), (45, 2), (46, 1), (57, 1), (59, 1), (60, 1), (62, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (96, 1), (98, 1), (99, 1), (101, 1), (104, 2), (121, 1), (122, 1), (125, 1), (128, 2), (130, 1), (143, 1), (148, 1), (150, 1), (153, 2), (156, 2), (157, 2), (168, 1), (181, 1), (184, 1), (185, 1), (186, 1), (188, 1), (189, 1), (205, 1), (206, 3), (207, 2), (208, 1), (211, 1), (218, 1), (220, 2), (221, 1), (222, 1), (225, 2), (226, 2), (239, 4), (240, 1), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 45s - loss: 815.3847 - loglik: -8.1329e+02 - logprior: -2.0981e+00
Epoch 2/2
43/43 - 42s - loss: 799.5944 - loglik: -7.9856e+02 - logprior: -1.0326e+00
Fitted a model with MAP estimate = -795.5762
expansions: [(0, 2)]
discards: [  0  58 109 164 196 203 262 263 264 290 294 296 313 314 356 358]
Re-initialized the encoder parameters.
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 42s - loss: 801.3410 - loglik: -8.0016e+02 - logprior: -1.1790e+00
Epoch 2/2
43/43 - 40s - loss: 798.2480 - loglik: -7.9789e+02 - logprior: -3.6105e-01
Fitted a model with MAP estimate = -797.1332
expansions: []
discards: [  0   1 134]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 782.8043 - loglik: -7.8191e+02 - logprior: -8.9732e-01
Epoch 2/10
61/61 - 54s - loss: 776.1460 - loglik: -7.7582e+02 - logprior: -3.2153e-01
Epoch 3/10
61/61 - 54s - loss: 775.6596 - loglik: -7.7535e+02 - logprior: -3.0790e-01
Epoch 4/10
61/61 - 54s - loss: 773.8328 - loglik: -7.7359e+02 - logprior: -2.4051e-01
Epoch 5/10
61/61 - 54s - loss: 772.5914 - loglik: -7.7242e+02 - logprior: -1.7575e-01
Epoch 6/10
61/61 - 54s - loss: 773.1395 - loglik: -7.7304e+02 - logprior: -1.0448e-01
Fitted a model with MAP estimate = -771.8378
Time for alignment: 1034.1864
Computed alignments with likelihoods: ['-771.2033', '-771.6953', '-771.8450', '-771.3771', '-771.8378']
Best model has likelihood: -771.2033  (prior= -0.1206 )
time for generating output: 0.3779
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.8163553048995418
Training of 5 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb05828d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaf4c91280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaa615f220>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.2114 - loglik: -1.9354e+02 - logprior: -5.6671e+01
Epoch 2/10
10/10 - 1s - loss: 187.5617 - loglik: -1.7195e+02 - logprior: -1.5615e+01
Epoch 3/10
10/10 - 1s - loss: 162.2446 - loglik: -1.5465e+02 - logprior: -7.5925e+00
Epoch 4/10
10/10 - 1s - loss: 153.4438 - loglik: -1.4895e+02 - logprior: -4.4923e+00
Epoch 5/10
10/10 - 1s - loss: 149.7989 - loglik: -1.4677e+02 - logprior: -3.0263e+00
Epoch 6/10
10/10 - 1s - loss: 147.6485 - loglik: -1.4541e+02 - logprior: -2.2390e+00
Epoch 7/10
10/10 - 1s - loss: 146.3803 - loglik: -1.4463e+02 - logprior: -1.7496e+00
Epoch 8/10
10/10 - 1s - loss: 146.2145 - loglik: -1.4474e+02 - logprior: -1.4774e+00
Epoch 9/10
10/10 - 1s - loss: 144.7324 - loglik: -1.4338e+02 - logprior: -1.3538e+00
Epoch 10/10
10/10 - 1s - loss: 145.0459 - loglik: -1.4377e+02 - logprior: -1.2785e+00
Fitted a model with MAP estimate = -144.7216
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.9879 - loglik: -1.4487e+02 - logprior: -6.4123e+01
Epoch 2/2
10/10 - 1s - loss: 169.7060 - loglik: -1.4282e+02 - logprior: -2.6886e+01
Fitted a model with MAP estimate = -163.1342
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.8715 - loglik: -1.4156e+02 - logprior: -5.2312e+01
Epoch 2/2
10/10 - 1s - loss: 154.6581 - loglik: -1.4027e+02 - logprior: -1.4392e+01
Fitted a model with MAP estimate = -149.4500
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 203.7948 - loglik: -1.4166e+02 - logprior: -6.2138e+01
Epoch 2/10
10/10 - 1s - loss: 162.4799 - loglik: -1.4145e+02 - logprior: -2.1030e+01
Epoch 3/10
10/10 - 1s - loss: 149.6024 - loglik: -1.4135e+02 - logprior: -8.2526e+00
Epoch 4/10
10/10 - 1s - loss: 144.3521 - loglik: -1.4052e+02 - logprior: -3.8364e+00
Epoch 5/10
10/10 - 1s - loss: 143.7671 - loglik: -1.4161e+02 - logprior: -2.1580e+00
Epoch 6/10
10/10 - 1s - loss: 141.1768 - loglik: -1.3983e+02 - logprior: -1.3490e+00
Epoch 7/10
10/10 - 1s - loss: 141.3628 - loglik: -1.4058e+02 - logprior: -7.8346e-01
Fitted a model with MAP estimate = -140.7226
Time for alignment: 39.1833
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.2137 - loglik: -1.9354e+02 - logprior: -5.6672e+01
Epoch 2/10
10/10 - 1s - loss: 187.6311 - loglik: -1.7201e+02 - logprior: -1.5625e+01
Epoch 3/10
10/10 - 1s - loss: 163.0122 - loglik: -1.5540e+02 - logprior: -7.6100e+00
Epoch 4/10
10/10 - 1s - loss: 153.2278 - loglik: -1.4876e+02 - logprior: -4.4644e+00
Epoch 5/10
10/10 - 1s - loss: 150.8171 - loglik: -1.4788e+02 - logprior: -2.9386e+00
Epoch 6/10
10/10 - 1s - loss: 148.3059 - loglik: -1.4627e+02 - logprior: -2.0397e+00
Epoch 7/10
10/10 - 1s - loss: 147.1520 - loglik: -1.4566e+02 - logprior: -1.4967e+00
Epoch 8/10
10/10 - 1s - loss: 146.4351 - loglik: -1.4517e+02 - logprior: -1.2611e+00
Epoch 9/10
10/10 - 1s - loss: 146.9049 - loglik: -1.4577e+02 - logprior: -1.1361e+00
Fitted a model with MAP estimate = -146.2635
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.7378 - loglik: -1.4743e+02 - logprior: -6.4311e+01
Epoch 2/2
10/10 - 1s - loss: 173.2727 - loglik: -1.4614e+02 - logprior: -2.7133e+01
Fitted a model with MAP estimate = -167.6376
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 197.5023 - loglik: -1.4497e+02 - logprior: -5.2528e+01
Epoch 2/2
10/10 - 1s - loss: 160.1824 - loglik: -1.4555e+02 - logprior: -1.4628e+01
Fitted a model with MAP estimate = -154.2451
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 207.9156 - loglik: -1.4572e+02 - logprior: -6.2198e+01
Epoch 2/10
10/10 - 1s - loss: 167.0598 - loglik: -1.4624e+02 - logprior: -2.0820e+01
Epoch 3/10
10/10 - 1s - loss: 154.4883 - loglik: -1.4615e+02 - logprior: -8.3344e+00
Epoch 4/10
10/10 - 1s - loss: 148.9504 - loglik: -1.4485e+02 - logprior: -4.0985e+00
Epoch 5/10
10/10 - 1s - loss: 148.5955 - loglik: -1.4614e+02 - logprior: -2.4531e+00
Epoch 6/10
10/10 - 1s - loss: 146.2365 - loglik: -1.4458e+02 - logprior: -1.6572e+00
Epoch 7/10
10/10 - 1s - loss: 146.5837 - loglik: -1.4551e+02 - logprior: -1.0766e+00
Fitted a model with MAP estimate = -145.8416
Time for alignment: 37.2035
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.5975 - loglik: -1.9392e+02 - logprior: -5.6673e+01
Epoch 2/10
10/10 - 1s - loss: 186.3090 - loglik: -1.7068e+02 - logprior: -1.5627e+01
Epoch 3/10
10/10 - 1s - loss: 161.5111 - loglik: -1.5392e+02 - logprior: -7.5925e+00
Epoch 4/10
10/10 - 1s - loss: 152.6924 - loglik: -1.4817e+02 - logprior: -4.5176e+00
Epoch 5/10
10/10 - 1s - loss: 149.0551 - loglik: -1.4594e+02 - logprior: -3.1131e+00
Epoch 6/10
10/10 - 1s - loss: 147.5265 - loglik: -1.4528e+02 - logprior: -2.2475e+00
Epoch 7/10
10/10 - 1s - loss: 145.8657 - loglik: -1.4414e+02 - logprior: -1.7241e+00
Epoch 8/10
10/10 - 1s - loss: 146.4657 - loglik: -1.4497e+02 - logprior: -1.4909e+00
Fitted a model with MAP estimate = -145.5872
expansions: [(11, 1), (12, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.1917 - loglik: -1.4514e+02 - logprior: -6.4052e+01
Epoch 2/2
10/10 - 1s - loss: 168.9019 - loglik: -1.4201e+02 - logprior: -2.6894e+01
Fitted a model with MAP estimate = -163.1341
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.7996 - loglik: -1.4072e+02 - logprior: -5.2076e+01
Epoch 2/2
10/10 - 1s - loss: 155.3574 - loglik: -1.4095e+02 - logprior: -1.4412e+01
Fitted a model with MAP estimate = -149.5091
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 203.9192 - loglik: -1.4156e+02 - logprior: -6.2360e+01
Epoch 2/10
10/10 - 1s - loss: 162.7884 - loglik: -1.4120e+02 - logprior: -2.1590e+01
Epoch 3/10
10/10 - 1s - loss: 150.4634 - loglik: -1.4189e+02 - logprior: -8.5736e+00
Epoch 4/10
10/10 - 1s - loss: 144.5550 - loglik: -1.4061e+02 - logprior: -3.9475e+00
Epoch 5/10
10/10 - 1s - loss: 142.7576 - loglik: -1.4052e+02 - logprior: -2.2348e+00
Epoch 6/10
10/10 - 1s - loss: 141.9742 - loglik: -1.4056e+02 - logprior: -1.4102e+00
Epoch 7/10
10/10 - 1s - loss: 141.1307 - loglik: -1.4031e+02 - logprior: -8.1903e-01
Epoch 8/10
10/10 - 1s - loss: 140.6722 - loglik: -1.4029e+02 - logprior: -3.7949e-01
Epoch 9/10
10/10 - 1s - loss: 140.3994 - loglik: -1.4026e+02 - logprior: -1.4188e-01
Epoch 10/10
10/10 - 1s - loss: 139.7430 - loglik: -1.3977e+02 - logprior: 0.0223
Fitted a model with MAP estimate = -139.8817
Time for alignment: 40.4508
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.1290 - loglik: -1.9346e+02 - logprior: -5.6669e+01
Epoch 2/10
10/10 - 1s - loss: 186.7944 - loglik: -1.7119e+02 - logprior: -1.5609e+01
Epoch 3/10
10/10 - 1s - loss: 161.5824 - loglik: -1.5398e+02 - logprior: -7.6069e+00
Epoch 4/10
10/10 - 1s - loss: 153.0610 - loglik: -1.4861e+02 - logprior: -4.4512e+00
Epoch 5/10
10/10 - 1s - loss: 148.6434 - loglik: -1.4574e+02 - logprior: -2.9050e+00
Epoch 6/10
10/10 - 1s - loss: 147.4104 - loglik: -1.4539e+02 - logprior: -2.0245e+00
Epoch 7/10
10/10 - 1s - loss: 145.7618 - loglik: -1.4428e+02 - logprior: -1.4772e+00
Epoch 8/10
10/10 - 1s - loss: 144.2463 - loglik: -1.4300e+02 - logprior: -1.2497e+00
Epoch 9/10
10/10 - 1s - loss: 145.7178 - loglik: -1.4460e+02 - logprior: -1.1204e+00
Fitted a model with MAP estimate = -144.6335
expansions: [(13, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.7135 - loglik: -1.4456e+02 - logprior: -6.4154e+01
Epoch 2/2
10/10 - 1s - loss: 169.9191 - loglik: -1.4295e+02 - logprior: -2.6970e+01
Fitted a model with MAP estimate = -163.7739
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.5763 - loglik: -1.4101e+02 - logprior: -5.2562e+01
Epoch 2/2
10/10 - 1s - loss: 156.4147 - loglik: -1.4196e+02 - logprior: -1.4456e+01
Fitted a model with MAP estimate = -150.1349
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 206.1527 - loglik: -1.4318e+02 - logprior: -6.2975e+01
Epoch 2/10
10/10 - 1s - loss: 166.9566 - loglik: -1.4292e+02 - logprior: -2.4041e+01
Epoch 3/10
10/10 - 1s - loss: 153.2056 - loglik: -1.4276e+02 - logprior: -1.0442e+01
Epoch 4/10
10/10 - 1s - loss: 145.9807 - loglik: -1.4175e+02 - logprior: -4.2297e+00
Epoch 5/10
10/10 - 1s - loss: 144.1036 - loglik: -1.4184e+02 - logprior: -2.2650e+00
Epoch 6/10
10/10 - 1s - loss: 143.1028 - loglik: -1.4168e+02 - logprior: -1.4253e+00
Epoch 7/10
10/10 - 1s - loss: 142.3519 - loglik: -1.4146e+02 - logprior: -8.8829e-01
Epoch 8/10
10/10 - 1s - loss: 141.4108 - loglik: -1.4093e+02 - logprior: -4.8032e-01
Epoch 9/10
10/10 - 1s - loss: 142.1757 - loglik: -1.4192e+02 - logprior: -2.5542e-01
Fitted a model with MAP estimate = -141.3432
Time for alignment: 40.6142
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.0733 - loglik: -1.9340e+02 - logprior: -5.6668e+01
Epoch 2/10
10/10 - 1s - loss: 186.8317 - loglik: -1.7123e+02 - logprior: -1.5601e+01
Epoch 3/10
10/10 - 1s - loss: 162.1317 - loglik: -1.5453e+02 - logprior: -7.6038e+00
Epoch 4/10
10/10 - 1s - loss: 152.8770 - loglik: -1.4841e+02 - logprior: -4.4626e+00
Epoch 5/10
10/10 - 1s - loss: 150.4167 - loglik: -1.4751e+02 - logprior: -2.9054e+00
Epoch 6/10
10/10 - 1s - loss: 148.1327 - loglik: -1.4615e+02 - logprior: -1.9871e+00
Epoch 7/10
10/10 - 1s - loss: 147.2486 - loglik: -1.4580e+02 - logprior: -1.4488e+00
Epoch 8/10
10/10 - 1s - loss: 146.6448 - loglik: -1.4543e+02 - logprior: -1.2167e+00
Epoch 9/10
10/10 - 1s - loss: 146.4260 - loglik: -1.4536e+02 - logprior: -1.0665e+00
Epoch 10/10
10/10 - 1s - loss: 145.6473 - loglik: -1.4471e+02 - logprior: -9.4044e-01
Fitted a model with MAP estimate = -145.9311
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 212.0491 - loglik: -1.4784e+02 - logprior: -6.4214e+01
Epoch 2/2
10/10 - 1s - loss: 172.6749 - loglik: -1.4563e+02 - logprior: -2.7040e+01
Fitted a model with MAP estimate = -167.5061
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 197.6002 - loglik: -1.4524e+02 - logprior: -5.2358e+01
Epoch 2/2
10/10 - 1s - loss: 160.0677 - loglik: -1.4555e+02 - logprior: -1.4515e+01
Fitted a model with MAP estimate = -154.0816
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 207.9425 - loglik: -1.4581e+02 - logprior: -6.2135e+01
Epoch 2/10
10/10 - 1s - loss: 166.9320 - loglik: -1.4614e+02 - logprior: -2.0796e+01
Epoch 3/10
10/10 - 1s - loss: 154.2869 - loglik: -1.4603e+02 - logprior: -8.2595e+00
Epoch 4/10
10/10 - 1s - loss: 149.2485 - loglik: -1.4525e+02 - logprior: -3.9967e+00
Epoch 5/10
10/10 - 1s - loss: 147.8718 - loglik: -1.4553e+02 - logprior: -2.3429e+00
Epoch 6/10
10/10 - 1s - loss: 146.6032 - loglik: -1.4505e+02 - logprior: -1.5531e+00
Epoch 7/10
10/10 - 1s - loss: 145.7744 - loglik: -1.4480e+02 - logprior: -9.7849e-01
Epoch 8/10
10/10 - 1s - loss: 145.4221 - loglik: -1.4486e+02 - logprior: -5.5985e-01
Epoch 9/10
10/10 - 1s - loss: 145.5956 - loglik: -1.4528e+02 - logprior: -3.1596e-01
Fitted a model with MAP estimate = -145.0223
Time for alignment: 41.1519
Computed alignments with likelihoods: ['-140.7226', '-145.8416', '-139.8817', '-141.3432', '-145.0223']
Best model has likelihood: -139.8817  (prior= 0.1072 )
time for generating output: 0.1006
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.9597780859916782
Training of 5 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49e95580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebd1aac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead10f67f0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.2944 - loglik: -4.7153e+02 - logprior: -6.6761e+01
Epoch 2/10
10/10 - 2s - loss: 438.0024 - loglik: -4.2471e+02 - logprior: -1.3296e+01
Epoch 3/10
10/10 - 2s - loss: 384.6330 - loglik: -3.8044e+02 - logprior: -4.1931e+00
Epoch 4/10
10/10 - 2s - loss: 355.0051 - loglik: -3.5394e+02 - logprior: -1.0690e+00
Epoch 5/10
10/10 - 2s - loss: 342.5375 - loglik: -3.4310e+02 - logprior: 0.5633
Epoch 6/10
10/10 - 2s - loss: 337.4621 - loglik: -3.3871e+02 - logprior: 1.2500
Epoch 7/10
10/10 - 2s - loss: 333.7422 - loglik: -3.3541e+02 - logprior: 1.6677
Epoch 8/10
10/10 - 2s - loss: 332.4830 - loglik: -3.3447e+02 - logprior: 1.9896
Epoch 9/10
10/10 - 2s - loss: 331.3180 - loglik: -3.3354e+02 - logprior: 2.2239
Epoch 10/10
10/10 - 2s - loss: 330.8067 - loglik: -3.3323e+02 - logprior: 2.4260
Fitted a model with MAP estimate = -330.3957
expansions: [(10, 2), (11, 1), (12, 2), (16, 3), (26, 2), (28, 1), (38, 2), (39, 2), (56, 1), (78, 3), (79, 2), (80, 2), (89, 1), (112, 3), (113, 2), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 409.7923 - loglik: -3.3444e+02 - logprior: -7.5352e+01
Epoch 2/2
10/10 - 3s - loss: 345.2607 - loglik: -3.1791e+02 - logprior: -2.7355e+01
Fitted a model with MAP estimate = -332.9514
expansions: [(0, 1)]
discards: [  0  10  97 139]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 372.2185 - loglik: -3.1377e+02 - logprior: -5.8444e+01
Epoch 2/2
10/10 - 2s - loss: 319.3798 - loglik: -3.0872e+02 - logprior: -1.0663e+01
Fitted a model with MAP estimate = -311.0546
expansions: [(20, 1), (111, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 366.6515 - loglik: -3.0943e+02 - logprior: -5.7223e+01
Epoch 2/10
10/10 - 3s - loss: 316.2743 - loglik: -3.0665e+02 - logprior: -9.6248e+00
Epoch 3/10
10/10 - 3s - loss: 305.2723 - loglik: -3.0552e+02 - logprior: 0.2446
Epoch 4/10
10/10 - 3s - loss: 300.3821 - loglik: -3.0476e+02 - logprior: 4.3828
Epoch 5/10
10/10 - 3s - loss: 297.3486 - loglik: -3.0408e+02 - logprior: 6.7303
Epoch 6/10
10/10 - 3s - loss: 295.6818 - loglik: -3.0390e+02 - logprior: 8.2148
Epoch 7/10
10/10 - 3s - loss: 294.8549 - loglik: -3.0402e+02 - logprior: 9.1632
Epoch 8/10
10/10 - 3s - loss: 294.0474 - loglik: -3.0380e+02 - logprior: 9.7481
Epoch 9/10
10/10 - 3s - loss: 293.0510 - loglik: -3.0328e+02 - logprior: 10.2260
Epoch 10/10
10/10 - 3s - loss: 292.8606 - loglik: -3.0368e+02 - logprior: 10.8146
Fitted a model with MAP estimate = -292.3088
Time for alignment: 73.0987
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.4191 - loglik: -4.7165e+02 - logprior: -6.6765e+01
Epoch 2/10
10/10 - 2s - loss: 438.2085 - loglik: -4.2492e+02 - logprior: -1.3288e+01
Epoch 3/10
10/10 - 2s - loss: 385.3354 - loglik: -3.8116e+02 - logprior: -4.1719e+00
Epoch 4/10
10/10 - 2s - loss: 357.8167 - loglik: -3.5669e+02 - logprior: -1.1288e+00
Epoch 5/10
10/10 - 2s - loss: 345.2799 - loglik: -3.4566e+02 - logprior: 0.3807
Epoch 6/10
10/10 - 2s - loss: 338.7890 - loglik: -3.3986e+02 - logprior: 1.0757
Epoch 7/10
10/10 - 2s - loss: 334.7356 - loglik: -3.3627e+02 - logprior: 1.5355
Epoch 8/10
10/10 - 2s - loss: 332.2596 - loglik: -3.3400e+02 - logprior: 1.7402
Epoch 9/10
10/10 - 2s - loss: 331.2458 - loglik: -3.3326e+02 - logprior: 2.0117
Epoch 10/10
10/10 - 2s - loss: 330.4604 - loglik: -3.3269e+02 - logprior: 2.2286
Fitted a model with MAP estimate = -330.2116
expansions: [(10, 2), (11, 2), (12, 2), (16, 3), (26, 1), (27, 1), (28, 1), (37, 1), (38, 2), (39, 2), (56, 1), (79, 6), (89, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 409.7464 - loglik: -3.3449e+02 - logprior: -7.5258e+01
Epoch 2/2
10/10 - 3s - loss: 345.2695 - loglik: -3.1784e+02 - logprior: -2.7431e+01
Fitted a model with MAP estimate = -333.2124
expansions: [(0, 1), (114, 2), (127, 1)]
discards: [ 0 10 12 50 53]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 372.4507 - loglik: -3.1410e+02 - logprior: -5.8353e+01
Epoch 2/2
10/10 - 3s - loss: 318.5997 - loglik: -3.0816e+02 - logprior: -1.0445e+01
Fitted a model with MAP estimate = -310.1139
expansions: []
discards: [111 160]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 366.7880 - loglik: -3.0959e+02 - logprior: -5.7201e+01
Epoch 2/10
10/10 - 3s - loss: 316.0847 - loglik: -3.0643e+02 - logprior: -9.6521e+00
Epoch 3/10
10/10 - 3s - loss: 304.4662 - loglik: -3.0465e+02 - logprior: 0.1846
Epoch 4/10
10/10 - 3s - loss: 299.4567 - loglik: -3.0374e+02 - logprior: 4.2849
Epoch 5/10
10/10 - 3s - loss: 296.4542 - loglik: -3.0303e+02 - logprior: 6.5792
Epoch 6/10
10/10 - 3s - loss: 295.1793 - loglik: -3.0325e+02 - logprior: 8.0739
Epoch 7/10
10/10 - 3s - loss: 294.2021 - loglik: -3.0321e+02 - logprior: 9.0087
Epoch 8/10
10/10 - 3s - loss: 292.8952 - loglik: -3.0247e+02 - logprior: 9.5785
Epoch 9/10
10/10 - 3s - loss: 292.4929 - loglik: -3.0255e+02 - logprior: 10.0566
Epoch 10/10
10/10 - 3s - loss: 291.2845 - loglik: -3.0195e+02 - logprior: 10.6682
Fitted a model with MAP estimate = -291.3986
Time for alignment: 72.2632
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.8337 - loglik: -4.7207e+02 - logprior: -6.6765e+01
Epoch 2/10
10/10 - 2s - loss: 438.5735 - loglik: -4.2527e+02 - logprior: -1.3306e+01
Epoch 3/10
10/10 - 2s - loss: 385.9350 - loglik: -3.8162e+02 - logprior: -4.3116e+00
Epoch 4/10
10/10 - 2s - loss: 357.0540 - loglik: -3.5564e+02 - logprior: -1.4177e+00
Epoch 5/10
10/10 - 2s - loss: 343.8897 - loglik: -3.4374e+02 - logprior: -1.4511e-01
Epoch 6/10
10/10 - 2s - loss: 337.0161 - loglik: -3.3761e+02 - logprior: 0.5939
Epoch 7/10
10/10 - 2s - loss: 332.1977 - loglik: -3.3339e+02 - logprior: 1.1960
Epoch 8/10
10/10 - 2s - loss: 330.0578 - loglik: -3.3147e+02 - logprior: 1.4154
Epoch 9/10
10/10 - 2s - loss: 329.2087 - loglik: -3.3088e+02 - logprior: 1.6686
Epoch 10/10
10/10 - 2s - loss: 328.3657 - loglik: -3.3032e+02 - logprior: 1.9552
Fitted a model with MAP estimate = -328.1726
expansions: [(8, 1), (10, 1), (12, 2), (16, 3), (19, 1), (26, 1), (27, 1), (28, 1), (37, 1), (38, 2), (39, 2), (56, 1), (78, 2), (79, 3), (80, 2), (89, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 406.1124 - loglik: -3.3093e+02 - logprior: -7.5180e+01
Epoch 2/2
10/10 - 3s - loss: 341.3491 - loglik: -3.1430e+02 - logprior: -2.7047e+01
Fitted a model with MAP estimate = -329.9023
expansions: [(0, 1), (127, 1)]
discards: [  0  49  52 140]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 370.0896 - loglik: -3.1179e+02 - logprior: -5.8302e+01
Epoch 2/2
10/10 - 3s - loss: 316.4845 - loglik: -3.0603e+02 - logprior: -1.0451e+01
Fitted a model with MAP estimate = -308.2089
expansions: []
discards: [160]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 364.5010 - loglik: -3.0724e+02 - logprior: -5.7259e+01
Epoch 2/10
10/10 - 3s - loss: 315.4868 - loglik: -3.0587e+02 - logprior: -9.6217e+00
Epoch 3/10
10/10 - 3s - loss: 303.5341 - loglik: -3.0372e+02 - logprior: 0.1875
Epoch 4/10
10/10 - 3s - loss: 298.5298 - loglik: -3.0280e+02 - logprior: 4.2671
Epoch 5/10
10/10 - 3s - loss: 296.3289 - loglik: -3.0295e+02 - logprior: 6.6220
Epoch 6/10
10/10 - 3s - loss: 294.2410 - loglik: -3.0235e+02 - logprior: 8.1131
Epoch 7/10
10/10 - 3s - loss: 293.2173 - loglik: -3.0228e+02 - logprior: 9.0612
Epoch 8/10
10/10 - 3s - loss: 292.4969 - loglik: -3.0215e+02 - logprior: 9.6547
Epoch 9/10
10/10 - 3s - loss: 291.7280 - loglik: -3.0184e+02 - logprior: 10.1078
Epoch 10/10
10/10 - 3s - loss: 290.7358 - loglik: -3.0143e+02 - logprior: 10.6974
Fitted a model with MAP estimate = -290.7167
Time for alignment: 74.2033
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.3400 - loglik: -4.7158e+02 - logprior: -6.6762e+01
Epoch 2/10
10/10 - 2s - loss: 438.0825 - loglik: -4.2478e+02 - logprior: -1.3303e+01
Epoch 3/10
10/10 - 2s - loss: 385.7675 - loglik: -3.8158e+02 - logprior: -4.1912e+00
Epoch 4/10
10/10 - 2s - loss: 357.0131 - loglik: -3.5589e+02 - logprior: -1.1194e+00
Epoch 5/10
10/10 - 2s - loss: 345.3002 - loglik: -3.4588e+02 - logprior: 0.5787
Epoch 6/10
10/10 - 2s - loss: 337.9808 - loglik: -3.3937e+02 - logprior: 1.3853
Epoch 7/10
10/10 - 2s - loss: 334.8105 - loglik: -3.3656e+02 - logprior: 1.7464
Epoch 8/10
10/10 - 2s - loss: 332.9564 - loglik: -3.3492e+02 - logprior: 1.9674
Epoch 9/10
10/10 - 2s - loss: 331.5418 - loglik: -3.3383e+02 - logprior: 2.2871
Epoch 10/10
10/10 - 2s - loss: 330.9568 - loglik: -3.3350e+02 - logprior: 2.5477
Fitted a model with MAP estimate = -330.6232
expansions: [(10, 2), (11, 1), (12, 2), (16, 3), (26, 2), (38, 2), (39, 2), (56, 1), (79, 6), (90, 3), (110, 2), (114, 3), (115, 1), (121, 1), (123, 2), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 410.1738 - loglik: -3.3477e+02 - logprior: -7.5407e+01
Epoch 2/2
10/10 - 3s - loss: 345.1445 - loglik: -3.1766e+02 - logprior: -2.7486e+01
Fitted a model with MAP estimate = -332.5468
expansions: [(0, 2), (21, 1), (34, 1), (126, 1)]
discards: [  0   9 111 112 140]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 370.3493 - loglik: -3.1205e+02 - logprior: -5.8299e+01
Epoch 2/2
10/10 - 3s - loss: 314.8924 - loglik: -3.0460e+02 - logprior: -1.0289e+01
Fitted a model with MAP estimate = -306.2790
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 379.3991 - loglik: -3.0809e+02 - logprior: -7.1309e+01
Epoch 2/10
10/10 - 3s - loss: 322.4966 - loglik: -3.0511e+02 - logprior: -1.7386e+01
Epoch 3/10
10/10 - 3s - loss: 305.0468 - loglik: -3.0355e+02 - logprior: -1.5011e+00
Epoch 4/10
10/10 - 3s - loss: 298.1216 - loglik: -3.0204e+02 - logprior: 3.9170
Epoch 5/10
10/10 - 3s - loss: 294.7653 - loglik: -3.0109e+02 - logprior: 6.3224
Epoch 6/10
10/10 - 3s - loss: 293.1713 - loglik: -3.0094e+02 - logprior: 7.7735
Epoch 7/10
10/10 - 3s - loss: 292.2989 - loglik: -3.0119e+02 - logprior: 8.8877
Epoch 8/10
10/10 - 3s - loss: 291.1474 - loglik: -3.0096e+02 - logprior: 9.8080
Epoch 9/10
10/10 - 3s - loss: 290.7101 - loglik: -3.0121e+02 - logprior: 10.5038
Epoch 10/10
10/10 - 3s - loss: 289.9991 - loglik: -3.0104e+02 - logprior: 11.0434
Fitted a model with MAP estimate = -289.8827
Time for alignment: 71.8150
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.2151 - loglik: -4.7145e+02 - logprior: -6.6761e+01
Epoch 2/10
10/10 - 2s - loss: 438.6484 - loglik: -4.2533e+02 - logprior: -1.3317e+01
Epoch 3/10
10/10 - 2s - loss: 386.8407 - loglik: -3.8251e+02 - logprior: -4.3270e+00
Epoch 4/10
10/10 - 2s - loss: 359.8742 - loglik: -3.5854e+02 - logprior: -1.3349e+00
Epoch 5/10
10/10 - 2s - loss: 346.4814 - loglik: -3.4675e+02 - logprior: 0.2668
Epoch 6/10
10/10 - 2s - loss: 340.0861 - loglik: -3.4116e+02 - logprior: 1.0753
Epoch 7/10
10/10 - 2s - loss: 335.6635 - loglik: -3.3679e+02 - logprior: 1.1275
Epoch 8/10
10/10 - 2s - loss: 333.2322 - loglik: -3.3463e+02 - logprior: 1.3934
Epoch 9/10
10/10 - 2s - loss: 331.0817 - loglik: -3.3272e+02 - logprior: 1.6385
Epoch 10/10
10/10 - 2s - loss: 329.8537 - loglik: -3.3166e+02 - logprior: 1.8108
Fitted a model with MAP estimate = -329.6833
expansions: [(10, 2), (11, 1), (12, 2), (16, 3), (26, 1), (27, 1), (28, 1), (38, 2), (39, 2), (56, 1), (68, 1), (78, 1), (79, 5), (89, 1), (90, 2), (110, 2), (114, 3), (115, 1), (121, 1), (123, 1), (126, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 407.8687 - loglik: -3.3262e+02 - logprior: -7.5251e+01
Epoch 2/2
10/10 - 3s - loss: 341.7984 - loglik: -3.1449e+02 - logprior: -2.7308e+01
Fitted a model with MAP estimate = -329.6043
expansions: [(0, 2), (74, 1), (128, 1)]
discards: [  0  10 114]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 366.7603 - loglik: -3.0844e+02 - logprior: -5.8316e+01
Epoch 2/2
10/10 - 3s - loss: 312.0252 - loglik: -3.0175e+02 - logprior: -1.0278e+01
Fitted a model with MAP estimate = -303.2196
expansions: [(21, 1)]
discards: [  0 142 164]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 377.9965 - loglik: -3.0606e+02 - logprior: -7.1936e+01
Epoch 2/10
10/10 - 3s - loss: 321.7294 - loglik: -3.0292e+02 - logprior: -1.8810e+01
Epoch 3/10
10/10 - 3s - loss: 302.7370 - loglik: -3.0061e+02 - logprior: -2.1268e+00
Epoch 4/10
10/10 - 3s - loss: 295.3234 - loglik: -2.9913e+02 - logprior: 3.8103
Epoch 5/10
10/10 - 3s - loss: 291.7713 - loglik: -2.9808e+02 - logprior: 6.3068
Epoch 6/10
10/10 - 3s - loss: 290.1179 - loglik: -2.9791e+02 - logprior: 7.7889
Epoch 7/10
10/10 - 3s - loss: 288.9549 - loglik: -2.9789e+02 - logprior: 8.9336
Epoch 8/10
10/10 - 3s - loss: 288.0808 - loglik: -2.9795e+02 - logprior: 9.8730
Epoch 9/10
10/10 - 3s - loss: 287.8725 - loglik: -2.9846e+02 - logprior: 10.5845
Epoch 10/10
10/10 - 3s - loss: 287.0708 - loglik: -2.9822e+02 - logprior: 11.1511
Fitted a model with MAP estimate = -286.6374
Time for alignment: 72.7407
Computed alignments with likelihoods: ['-292.3088', '-291.3986', '-290.7167', '-289.8827', '-286.6374']
Best model has likelihood: -286.6374  (prior= 11.4460 )
time for generating output: 0.1987
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.7749535027898327
Training of 5 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb16c29d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e20fbb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb27f3e7c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.6736 - loglik: -2.8275e+02 - logprior: -9.9211e+00
Epoch 2/10
12/12 - 1s - loss: 254.6586 - loglik: -2.5226e+02 - logprior: -2.4017e+00
Epoch 3/10
12/12 - 1s - loss: 227.7603 - loglik: -2.2610e+02 - logprior: -1.6604e+00
Epoch 4/10
12/12 - 1s - loss: 216.9418 - loglik: -2.1529e+02 - logprior: -1.6494e+00
Epoch 5/10
12/12 - 1s - loss: 212.6789 - loglik: -2.1096e+02 - logprior: -1.7154e+00
Epoch 6/10
12/12 - 1s - loss: 210.5439 - loglik: -2.0884e+02 - logprior: -1.7077e+00
Epoch 7/10
12/12 - 1s - loss: 208.5865 - loglik: -2.0690e+02 - logprior: -1.6910e+00
Epoch 8/10
12/12 - 1s - loss: 208.3723 - loglik: -2.0665e+02 - logprior: -1.7205e+00
Epoch 9/10
12/12 - 1s - loss: 207.2713 - loglik: -2.0555e+02 - logprior: -1.7255e+00
Epoch 10/10
12/12 - 1s - loss: 207.8846 - loglik: -2.0617e+02 - logprior: -1.7140e+00
Fitted a model with MAP estimate = -207.1625
expansions: [(6, 3), (10, 2), (11, 2), (12, 2), (13, 1), (21, 1), (36, 3), (49, 1), (50, 3), (52, 1), (59, 6), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 220.6382 - loglik: -2.0916e+02 - logprior: -1.1477e+01
Epoch 2/2
12/12 - 1s - loss: 202.2538 - loglik: -1.9746e+02 - logprior: -4.7919e+00
Fitted a model with MAP estimate = -198.3583
expansions: [(0, 5), (47, 1), (48, 1)]
discards: [ 0 13 18 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 205.5085 - loglik: -1.9630e+02 - logprior: -9.2045e+00
Epoch 2/2
12/12 - 1s - loss: 192.5271 - loglik: -1.9014e+02 - logprior: -2.3904e+00
Fitted a model with MAP estimate = -191.2923
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 200.9364 - loglik: -1.9202e+02 - logprior: -8.9151e+00
Epoch 2/10
12/12 - 1s - loss: 192.4303 - loglik: -1.9001e+02 - logprior: -2.4206e+00
Epoch 3/10
12/12 - 1s - loss: 190.7130 - loglik: -1.8923e+02 - logprior: -1.4858e+00
Epoch 4/10
12/12 - 1s - loss: 187.8128 - loglik: -1.8666e+02 - logprior: -1.1541e+00
Epoch 5/10
12/12 - 1s - loss: 186.6010 - loglik: -1.8556e+02 - logprior: -1.0422e+00
Epoch 6/10
12/12 - 1s - loss: 184.9957 - loglik: -1.8403e+02 - logprior: -9.6451e-01
Epoch 7/10
12/12 - 1s - loss: 184.5211 - loglik: -1.8356e+02 - logprior: -9.5700e-01
Epoch 8/10
12/12 - 1s - loss: 184.0524 - loglik: -1.8313e+02 - logprior: -9.2733e-01
Epoch 9/10
12/12 - 1s - loss: 184.0731 - loglik: -1.8318e+02 - logprior: -8.9565e-01
Fitted a model with MAP estimate = -183.7996
Time for alignment: 52.9710
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.2532 - loglik: -2.8234e+02 - logprior: -9.9168e+00
Epoch 2/10
12/12 - 1s - loss: 254.5527 - loglik: -2.5216e+02 - logprior: -2.3966e+00
Epoch 3/10
12/12 - 1s - loss: 226.4335 - loglik: -2.2479e+02 - logprior: -1.6411e+00
Epoch 4/10
12/12 - 1s - loss: 216.7288 - loglik: -2.1511e+02 - logprior: -1.6224e+00
Epoch 5/10
12/12 - 1s - loss: 212.3159 - loglik: -2.1065e+02 - logprior: -1.6703e+00
Epoch 6/10
12/12 - 1s - loss: 208.5120 - loglik: -2.0686e+02 - logprior: -1.6476e+00
Epoch 7/10
12/12 - 1s - loss: 208.1075 - loglik: -2.0647e+02 - logprior: -1.6348e+00
Epoch 8/10
12/12 - 1s - loss: 206.1445 - loglik: -2.0449e+02 - logprior: -1.6586e+00
Epoch 9/10
12/12 - 1s - loss: 205.1187 - loglik: -2.0345e+02 - logprior: -1.6715e+00
Epoch 10/10
12/12 - 1s - loss: 205.6465 - loglik: -2.0400e+02 - logprior: -1.6486e+00
Fitted a model with MAP estimate = -205.1046
expansions: [(8, 1), (9, 1), (10, 5), (11, 2), (20, 1), (29, 1), (36, 3), (49, 1), (50, 3), (58, 2), (59, 6), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 219.8502 - loglik: -2.0846e+02 - logprior: -1.1391e+01
Epoch 2/2
12/12 - 1s - loss: 200.2748 - loglik: -1.9560e+02 - logprior: -4.6769e+00
Fitted a model with MAP estimate = -196.2038
expansions: [(0, 5)]
discards: [ 0 13 14 48 76 79 80]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 204.9825 - loglik: -1.9593e+02 - logprior: -9.0531e+00
Epoch 2/2
12/12 - 1s - loss: 194.8877 - loglik: -1.9264e+02 - logprior: -2.2504e+00
Fitted a model with MAP estimate = -192.9520
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 202.7037 - loglik: -1.9395e+02 - logprior: -8.7571e+00
Epoch 2/10
12/12 - 1s - loss: 194.2755 - loglik: -1.9206e+02 - logprior: -2.2197e+00
Epoch 3/10
12/12 - 1s - loss: 191.6668 - loglik: -1.9032e+02 - logprior: -1.3438e+00
Epoch 4/10
12/12 - 1s - loss: 190.3149 - loglik: -1.8932e+02 - logprior: -9.9682e-01
Epoch 5/10
12/12 - 1s - loss: 188.2291 - loglik: -1.8731e+02 - logprior: -9.2233e-01
Epoch 6/10
12/12 - 1s - loss: 186.8300 - loglik: -1.8598e+02 - logprior: -8.4922e-01
Epoch 7/10
12/12 - 1s - loss: 186.4539 - loglik: -1.8561e+02 - logprior: -8.4493e-01
Epoch 8/10
12/12 - 1s - loss: 185.5341 - loglik: -1.8470e+02 - logprior: -8.3470e-01
Epoch 9/10
12/12 - 1s - loss: 186.2552 - loglik: -1.8547e+02 - logprior: -7.8862e-01
Fitted a model with MAP estimate = -185.4796
Time for alignment: 50.8304
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.2965 - loglik: -2.8238e+02 - logprior: -9.9188e+00
Epoch 2/10
12/12 - 1s - loss: 254.5384 - loglik: -2.5213e+02 - logprior: -2.4107e+00
Epoch 3/10
12/12 - 1s - loss: 226.8841 - loglik: -2.2521e+02 - logprior: -1.6791e+00
Epoch 4/10
12/12 - 1s - loss: 214.8842 - loglik: -2.1322e+02 - logprior: -1.6619e+00
Epoch 5/10
12/12 - 1s - loss: 212.1804 - loglik: -2.1048e+02 - logprior: -1.7047e+00
Epoch 6/10
12/12 - 1s - loss: 209.0640 - loglik: -2.0741e+02 - logprior: -1.6554e+00
Epoch 7/10
12/12 - 1s - loss: 208.3948 - loglik: -2.0677e+02 - logprior: -1.6284e+00
Epoch 8/10
12/12 - 1s - loss: 207.3027 - loglik: -2.0563e+02 - logprior: -1.6755e+00
Epoch 9/10
12/12 - 1s - loss: 206.3533 - loglik: -2.0466e+02 - logprior: -1.6934e+00
Epoch 10/10
12/12 - 1s - loss: 206.2432 - loglik: -2.0455e+02 - logprior: -1.6904e+00
Fitted a model with MAP estimate = -206.0786
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (10, 3), (11, 2), (21, 1), (36, 4), (49, 1), (50, 3), (58, 2), (59, 6), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 220.7944 - loglik: -2.0940e+02 - logprior: -1.1396e+01
Epoch 2/2
12/12 - 1s - loss: 200.4971 - loglik: -1.9582e+02 - logprior: -4.6796e+00
Fitted a model with MAP estimate = -196.6122
expansions: [(0, 5)]
discards: [ 0 14 15 76 79 80]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 205.0820 - loglik: -1.9602e+02 - logprior: -9.0638e+00
Epoch 2/2
12/12 - 1s - loss: 192.9703 - loglik: -1.9073e+02 - logprior: -2.2408e+00
Fitted a model with MAP estimate = -191.7774
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 200.5413 - loglik: -1.9175e+02 - logprior: -8.7942e+00
Epoch 2/10
12/12 - 1s - loss: 193.4924 - loglik: -1.9125e+02 - logprior: -2.2389e+00
Epoch 3/10
12/12 - 1s - loss: 190.4384 - loglik: -1.8904e+02 - logprior: -1.3959e+00
Epoch 4/10
12/12 - 1s - loss: 189.3771 - loglik: -1.8829e+02 - logprior: -1.0877e+00
Epoch 5/10
12/12 - 1s - loss: 187.4434 - loglik: -1.8645e+02 - logprior: -9.9087e-01
Epoch 6/10
12/12 - 1s - loss: 186.5285 - loglik: -1.8563e+02 - logprior: -8.9698e-01
Epoch 7/10
12/12 - 1s - loss: 184.5753 - loglik: -1.8367e+02 - logprior: -9.0396e-01
Epoch 8/10
12/12 - 1s - loss: 184.0880 - loglik: -1.8322e+02 - logprior: -8.6929e-01
Epoch 9/10
12/12 - 1s - loss: 185.1103 - loglik: -1.8427e+02 - logprior: -8.4471e-01
Fitted a model with MAP estimate = -184.3145
Time for alignment: 52.1196
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.6270 - loglik: -2.8271e+02 - logprior: -9.9194e+00
Epoch 2/10
12/12 - 1s - loss: 253.6103 - loglik: -2.5120e+02 - logprior: -2.4071e+00
Epoch 3/10
12/12 - 1s - loss: 226.3768 - loglik: -2.2471e+02 - logprior: -1.6694e+00
Epoch 4/10
12/12 - 1s - loss: 217.4221 - loglik: -2.1576e+02 - logprior: -1.6601e+00
Epoch 5/10
12/12 - 1s - loss: 211.9569 - loglik: -2.1028e+02 - logprior: -1.6749e+00
Epoch 6/10
12/12 - 1s - loss: 210.1634 - loglik: -2.0854e+02 - logprior: -1.6244e+00
Epoch 7/10
12/12 - 1s - loss: 208.4216 - loglik: -2.0681e+02 - logprior: -1.6104e+00
Epoch 8/10
12/12 - 1s - loss: 207.0373 - loglik: -2.0538e+02 - logprior: -1.6566e+00
Epoch 9/10
12/12 - 1s - loss: 206.2903 - loglik: -2.0461e+02 - logprior: -1.6815e+00
Epoch 10/10
12/12 - 1s - loss: 206.2604 - loglik: -2.0459e+02 - logprior: -1.6693e+00
Fitted a model with MAP estimate = -206.0711
expansions: [(8, 1), (9, 1), (10, 5), (11, 2), (21, 1), (29, 1), (36, 3), (49, 1), (50, 3), (52, 1), (59, 6), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 221.1562 - loglik: -2.0978e+02 - logprior: -1.1373e+01
Epoch 2/2
12/12 - 1s - loss: 200.7396 - loglik: -1.9608e+02 - logprior: -4.6552e+00
Fitted a model with MAP estimate = -197.2066
expansions: [(0, 6)]
discards: [ 0 12 13 48 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 205.7731 - loglik: -1.9663e+02 - logprior: -9.1465e+00
Epoch 2/2
12/12 - 1s - loss: 195.3592 - loglik: -1.9305e+02 - logprior: -2.3130e+00
Fitted a model with MAP estimate = -193.5085
expansions: []
discards: [1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 203.0677 - loglik: -1.9427e+02 - logprior: -8.7943e+00
Epoch 2/10
12/12 - 1s - loss: 194.7149 - loglik: -1.9251e+02 - logprior: -2.2015e+00
Epoch 3/10
12/12 - 1s - loss: 192.6926 - loglik: -1.9135e+02 - logprior: -1.3434e+00
Epoch 4/10
12/12 - 1s - loss: 189.9985 - loglik: -1.8893e+02 - logprior: -1.0708e+00
Epoch 5/10
12/12 - 1s - loss: 188.9712 - loglik: -1.8806e+02 - logprior: -9.1530e-01
Epoch 6/10
12/12 - 1s - loss: 187.7940 - loglik: -1.8694e+02 - logprior: -8.5102e-01
Epoch 7/10
12/12 - 1s - loss: 187.5655 - loglik: -1.8672e+02 - logprior: -8.4222e-01
Epoch 8/10
12/12 - 1s - loss: 185.8849 - loglik: -1.8508e+02 - logprior: -8.0205e-01
Epoch 9/10
12/12 - 1s - loss: 186.4209 - loglik: -1.8564e+02 - logprior: -7.8382e-01
Fitted a model with MAP estimate = -186.1229
Time for alignment: 50.4887
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.5122 - loglik: -2.8259e+02 - logprior: -9.9199e+00
Epoch 2/10
12/12 - 1s - loss: 253.9860 - loglik: -2.5158e+02 - logprior: -2.4080e+00
Epoch 3/10
12/12 - 1s - loss: 228.3310 - loglik: -2.2665e+02 - logprior: -1.6846e+00
Epoch 4/10
12/12 - 1s - loss: 218.3984 - loglik: -2.1670e+02 - logprior: -1.6953e+00
Epoch 5/10
12/12 - 1s - loss: 214.8705 - loglik: -2.1316e+02 - logprior: -1.7082e+00
Epoch 6/10
12/12 - 1s - loss: 212.9045 - loglik: -2.1129e+02 - logprior: -1.6166e+00
Epoch 7/10
12/12 - 1s - loss: 210.9971 - loglik: -2.0942e+02 - logprior: -1.5752e+00
Epoch 8/10
12/12 - 1s - loss: 210.2836 - loglik: -2.0867e+02 - logprior: -1.6129e+00
Epoch 9/10
12/12 - 1s - loss: 209.5875 - loglik: -2.0795e+02 - logprior: -1.6416e+00
Epoch 10/10
12/12 - 1s - loss: 209.7377 - loglik: -2.0810e+02 - logprior: -1.6418e+00
Fitted a model with MAP estimate = -208.9387
expansions: [(7, 2), (8, 1), (9, 1), (10, 3), (11, 2), (21, 1), (37, 4), (49, 1), (50, 3), (58, 2), (59, 6), (61, 1), (64, 1), (67, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 222.7668 - loglik: -2.1137e+02 - logprior: -1.1395e+01
Epoch 2/2
12/12 - 1s - loss: 202.2318 - loglik: -1.9756e+02 - logprior: -4.6698e+00
Fitted a model with MAP estimate = -197.7266
expansions: [(0, 7)]
discards: [ 0 14 15 48 76 79 80]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 206.9113 - loglik: -1.9775e+02 - logprior: -9.1662e+00
Epoch 2/2
12/12 - 1s - loss: 196.0345 - loglik: -1.9369e+02 - logprior: -2.3460e+00
Fitted a model with MAP estimate = -194.2920
expansions: []
discards: [1 2 3 4 5 6]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 203.6213 - loglik: -1.9486e+02 - logprior: -8.7641e+00
Epoch 2/10
12/12 - 1s - loss: 195.5910 - loglik: -1.9345e+02 - logprior: -2.1386e+00
Epoch 3/10
12/12 - 1s - loss: 193.5295 - loglik: -1.9227e+02 - logprior: -1.2551e+00
Epoch 4/10
12/12 - 1s - loss: 191.7112 - loglik: -1.9069e+02 - logprior: -1.0259e+00
Epoch 5/10
12/12 - 1s - loss: 189.2192 - loglik: -1.8831e+02 - logprior: -9.1212e-01
Epoch 6/10
12/12 - 1s - loss: 188.4520 - loglik: -1.8758e+02 - logprior: -8.6838e-01
Epoch 7/10
12/12 - 1s - loss: 187.2073 - loglik: -1.8632e+02 - logprior: -8.8416e-01
Epoch 8/10
12/12 - 1s - loss: 186.3800 - loglik: -1.8554e+02 - logprior: -8.3902e-01
Epoch 9/10
12/12 - 1s - loss: 186.7284 - loglik: -1.8590e+02 - logprior: -8.2904e-01
Fitted a model with MAP estimate = -186.3086
Time for alignment: 50.0621
Computed alignments with likelihoods: ['-183.7996', '-185.4796', '-184.3145', '-186.1229', '-186.3086']
Best model has likelihood: -183.7996  (prior= -0.8658 )
time for generating output: 0.1646
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.8963525809350165
Training of 5 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2ceaeeb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9da9bfd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9aca26ac0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.5385 - loglik: -2.8929e+02 - logprior: -4.0246e+01
Epoch 2/10
10/10 - 1s - loss: 276.7846 - loglik: -2.6668e+02 - logprior: -1.0105e+01
Epoch 3/10
10/10 - 1s - loss: 251.5024 - loglik: -2.4727e+02 - logprior: -4.2298e+00
Epoch 4/10
10/10 - 1s - loss: 237.6727 - loglik: -2.3560e+02 - logprior: -2.0770e+00
Epoch 5/10
10/10 - 1s - loss: 231.5620 - loglik: -2.3054e+02 - logprior: -1.0259e+00
Epoch 6/10
10/10 - 1s - loss: 228.2398 - loglik: -2.2765e+02 - logprior: -5.8849e-01
Epoch 7/10
10/10 - 1s - loss: 226.9649 - loglik: -2.2657e+02 - logprior: -3.9045e-01
Epoch 8/10
10/10 - 1s - loss: 225.4772 - loglik: -2.2530e+02 - logprior: -1.7240e-01
Epoch 9/10
10/10 - 1s - loss: 225.4626 - loglik: -2.2547e+02 - logprior: 0.0067
Epoch 10/10
10/10 - 1s - loss: 224.0767 - loglik: -2.2419e+02 - logprior: 0.1134
Fitted a model with MAP estimate = -224.2376
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (43, 12), (54, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 274.6481 - loglik: -2.2272e+02 - logprior: -5.1927e+01
Epoch 2/2
10/10 - 1s - loss: 232.5529 - loglik: -2.1705e+02 - logprior: -1.5499e+01
Fitted a model with MAP estimate = -224.7508
expansions: [(7, 1)]
discards: [ 0  1 73]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 262.4630 - loglik: -2.1678e+02 - logprior: -4.5684e+01
Epoch 2/2
10/10 - 1s - loss: 233.4224 - loglik: -2.1575e+02 - logprior: -1.7675e+01
Fitted a model with MAP estimate = -228.2973
expansions: [(0, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 255.3559 - loglik: -2.1455e+02 - logprior: -4.0803e+01
Epoch 2/10
10/10 - 1s - loss: 223.8778 - loglik: -2.1369e+02 - logprior: -1.0191e+01
Epoch 3/10
10/10 - 1s - loss: 217.0478 - loglik: -2.1364e+02 - logprior: -3.4043e+00
Epoch 4/10
10/10 - 1s - loss: 214.1322 - loglik: -2.1320e+02 - logprior: -9.3451e-01
Epoch 5/10
10/10 - 1s - loss: 212.9613 - loglik: -2.1326e+02 - logprior: 0.3019
Epoch 6/10
10/10 - 1s - loss: 212.3126 - loglik: -2.1327e+02 - logprior: 0.9605
Epoch 7/10
10/10 - 1s - loss: 211.6596 - loglik: -2.1298e+02 - logprior: 1.3205
Epoch 8/10
10/10 - 1s - loss: 211.2747 - loglik: -2.1284e+02 - logprior: 1.5628
Epoch 9/10
10/10 - 1s - loss: 211.2348 - loglik: -2.1299e+02 - logprior: 1.7558
Epoch 10/10
10/10 - 1s - loss: 211.0130 - loglik: -2.1295e+02 - logprior: 1.9351
Fitted a model with MAP estimate = -210.7568
Time for alignment: 43.9320
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.3378 - loglik: -2.8909e+02 - logprior: -4.0243e+01
Epoch 2/10
10/10 - 1s - loss: 276.9036 - loglik: -2.6680e+02 - logprior: -1.0108e+01
Epoch 3/10
10/10 - 1s - loss: 252.2680 - loglik: -2.4803e+02 - logprior: -4.2410e+00
Epoch 4/10
10/10 - 1s - loss: 238.9001 - loglik: -2.3684e+02 - logprior: -2.0602e+00
Epoch 5/10
10/10 - 1s - loss: 232.5918 - loglik: -2.3164e+02 - logprior: -9.4872e-01
Epoch 6/10
10/10 - 1s - loss: 229.5383 - loglik: -2.2908e+02 - logprior: -4.5804e-01
Epoch 7/10
10/10 - 1s - loss: 227.8800 - loglik: -2.2765e+02 - logprior: -2.2638e-01
Epoch 8/10
10/10 - 1s - loss: 226.8798 - loglik: -2.2684e+02 - logprior: -3.7363e-02
Epoch 9/10
10/10 - 1s - loss: 225.8663 - loglik: -2.2599e+02 - logprior: 0.1278
Epoch 10/10
10/10 - 1s - loss: 225.8432 - loglik: -2.2609e+02 - logprior: 0.2428
Fitted a model with MAP estimate = -225.3319
expansions: [(0, 3), (5, 1), (8, 3), (36, 1), (43, 11), (54, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 275.0571 - loglik: -2.2326e+02 - logprior: -5.1793e+01
Epoch 2/2
10/10 - 1s - loss: 232.5766 - loglik: -2.1714e+02 - logprior: -1.5438e+01
Fitted a model with MAP estimate = -224.5070
expansions: []
discards: [ 0  1 55 56]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 263.0936 - loglik: -2.1737e+02 - logprior: -4.5728e+01
Epoch 2/2
10/10 - 1s - loss: 233.0748 - loglik: -2.1538e+02 - logprior: -1.7697e+01
Fitted a model with MAP estimate = -228.8033
expansions: [(0, 4), (41, 1)]
discards: [ 0 70]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 255.9080 - loglik: -2.1528e+02 - logprior: -4.0624e+01
Epoch 2/10
10/10 - 1s - loss: 223.9124 - loglik: -2.1392e+02 - logprior: -9.9879e+00
Epoch 3/10
10/10 - 1s - loss: 217.0052 - loglik: -2.1386e+02 - logprior: -3.1445e+00
Epoch 4/10
10/10 - 1s - loss: 214.3388 - loglik: -2.1365e+02 - logprior: -6.8580e-01
Epoch 5/10
10/10 - 1s - loss: 213.3680 - loglik: -2.1391e+02 - logprior: 0.5394
Epoch 6/10
10/10 - 1s - loss: 212.0883 - loglik: -2.1330e+02 - logprior: 1.2103
Epoch 7/10
10/10 - 1s - loss: 212.0351 - loglik: -2.1361e+02 - logprior: 1.5720
Epoch 8/10
10/10 - 1s - loss: 211.8209 - loglik: -2.1363e+02 - logprior: 1.8071
Epoch 9/10
10/10 - 1s - loss: 211.0410 - loglik: -2.1305e+02 - logprior: 2.0051
Epoch 10/10
10/10 - 1s - loss: 211.3620 - loglik: -2.1354e+02 - logprior: 2.1804
Fitted a model with MAP estimate = -210.9965
Time for alignment: 41.7445
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.2046 - loglik: -2.8896e+02 - logprior: -4.0243e+01
Epoch 2/10
10/10 - 1s - loss: 277.2995 - loglik: -2.6720e+02 - logprior: -1.0100e+01
Epoch 3/10
10/10 - 1s - loss: 250.6071 - loglik: -2.4639e+02 - logprior: -4.2153e+00
Epoch 4/10
10/10 - 1s - loss: 238.2749 - loglik: -2.3625e+02 - logprior: -2.0242e+00
Epoch 5/10
10/10 - 1s - loss: 232.8223 - loglik: -2.3184e+02 - logprior: -9.8389e-01
Epoch 6/10
10/10 - 1s - loss: 229.6890 - loglik: -2.2912e+02 - logprior: -5.6422e-01
Epoch 7/10
10/10 - 1s - loss: 227.5235 - loglik: -2.2716e+02 - logprior: -3.6372e-01
Epoch 8/10
10/10 - 1s - loss: 226.2771 - loglik: -2.2604e+02 - logprior: -2.3620e-01
Epoch 9/10
10/10 - 1s - loss: 226.0180 - loglik: -2.2591e+02 - logprior: -1.0615e-01
Epoch 10/10
10/10 - 1s - loss: 224.7723 - loglik: -2.2476e+02 - logprior: -1.0693e-02
Fitted a model with MAP estimate = -224.6210
expansions: [(0, 3), (5, 2), (7, 1), (8, 1), (36, 1), (38, 2), (43, 7), (54, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 274.0544 - loglik: -2.2223e+02 - logprior: -5.1820e+01
Epoch 2/2
10/10 - 1s - loss: 231.6585 - loglik: -2.1644e+02 - logprior: -1.5219e+01
Fitted a model with MAP estimate = -223.7005
expansions: []
discards: [ 0  1 11 72]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 262.0137 - loglik: -2.1644e+02 - logprior: -4.5571e+01
Epoch 2/2
10/10 - 1s - loss: 233.7200 - loglik: -2.1617e+02 - logprior: -1.7552e+01
Fitted a model with MAP estimate = -228.5303
expansions: [(0, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 255.6689 - loglik: -2.1500e+02 - logprior: -4.0667e+01
Epoch 2/10
10/10 - 1s - loss: 224.0150 - loglik: -2.1396e+02 - logprior: -1.0059e+01
Epoch 3/10
10/10 - 1s - loss: 217.4461 - loglik: -2.1423e+02 - logprior: -3.2128e+00
Epoch 4/10
10/10 - 1s - loss: 214.6378 - loglik: -2.1388e+02 - logprior: -7.5465e-01
Epoch 5/10
10/10 - 1s - loss: 213.4040 - loglik: -2.1389e+02 - logprior: 0.4870
Epoch 6/10
10/10 - 1s - loss: 212.6318 - loglik: -2.1379e+02 - logprior: 1.1562
Epoch 7/10
10/10 - 1s - loss: 212.3648 - loglik: -2.1389e+02 - logprior: 1.5287
Epoch 8/10
10/10 - 1s - loss: 211.9388 - loglik: -2.1371e+02 - logprior: 1.7664
Epoch 9/10
10/10 - 1s - loss: 211.6856 - loglik: -2.1365e+02 - logprior: 1.9609
Epoch 10/10
10/10 - 1s - loss: 211.5218 - loglik: -2.1366e+02 - logprior: 2.1420
Fitted a model with MAP estimate = -211.3380
Time for alignment: 40.9283
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 329.1141 - loglik: -2.8887e+02 - logprior: -4.0242e+01
Epoch 2/10
10/10 - 1s - loss: 276.6100 - loglik: -2.6651e+02 - logprior: -1.0098e+01
Epoch 3/10
10/10 - 1s - loss: 250.3778 - loglik: -2.4622e+02 - logprior: -4.1568e+00
Epoch 4/10
10/10 - 1s - loss: 237.7648 - loglik: -2.3579e+02 - logprior: -1.9795e+00
Epoch 5/10
10/10 - 1s - loss: 232.0140 - loglik: -2.3106e+02 - logprior: -9.4967e-01
Epoch 6/10
10/10 - 1s - loss: 228.6308 - loglik: -2.2815e+02 - logprior: -4.8031e-01
Epoch 7/10
10/10 - 1s - loss: 227.5472 - loglik: -2.2728e+02 - logprior: -2.6582e-01
Epoch 8/10
10/10 - 1s - loss: 226.4017 - loglik: -2.2636e+02 - logprior: -4.0646e-02
Epoch 9/10
10/10 - 1s - loss: 225.8173 - loglik: -2.2591e+02 - logprior: 0.0952
Epoch 10/10
10/10 - 1s - loss: 225.3891 - loglik: -2.2553e+02 - logprior: 0.1420
Fitted a model with MAP estimate = -224.9556
expansions: [(0, 3), (8, 3), (36, 1), (43, 12), (53, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 274.7981 - loglik: -2.2301e+02 - logprior: -5.1785e+01
Epoch 2/2
10/10 - 1s - loss: 232.6681 - loglik: -2.1736e+02 - logprior: -1.5307e+01
Fitted a model with MAP estimate = -224.8111
expansions: []
discards: [ 0  1 55 56 57 73]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 263.4908 - loglik: -2.1784e+02 - logprior: -4.5646e+01
Epoch 2/2
10/10 - 1s - loss: 234.7066 - loglik: -2.1719e+02 - logprior: -1.7522e+01
Fitted a model with MAP estimate = -229.6809
expansions: [(0, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 256.7504 - loglik: -2.1612e+02 - logprior: -4.0626e+01
Epoch 2/10
10/10 - 1s - loss: 225.3360 - loglik: -2.1530e+02 - logprior: -1.0031e+01
Epoch 3/10
10/10 - 1s - loss: 218.3243 - loglik: -2.1512e+02 - logprior: -3.2021e+00
Epoch 4/10
10/10 - 1s - loss: 215.8966 - loglik: -2.1513e+02 - logprior: -7.6193e-01
Epoch 5/10
10/10 - 1s - loss: 214.4343 - loglik: -2.1492e+02 - logprior: 0.4808
Epoch 6/10
10/10 - 1s - loss: 213.9953 - loglik: -2.1515e+02 - logprior: 1.1524
Epoch 7/10
10/10 - 1s - loss: 213.1758 - loglik: -2.1470e+02 - logprior: 1.5199
Epoch 8/10
10/10 - 1s - loss: 213.0653 - loglik: -2.1481e+02 - logprior: 1.7489
Epoch 9/10
10/10 - 1s - loss: 212.4485 - loglik: -2.1438e+02 - logprior: 1.9356
Epoch 10/10
10/10 - 1s - loss: 212.8518 - loglik: -2.1497e+02 - logprior: 2.1166
Fitted a model with MAP estimate = -212.4172
Time for alignment: 41.3220
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.2796 - loglik: -2.8903e+02 - logprior: -4.0246e+01
Epoch 2/10
10/10 - 1s - loss: 277.4963 - loglik: -2.6739e+02 - logprior: -1.0107e+01
Epoch 3/10
10/10 - 1s - loss: 253.8446 - loglik: -2.4958e+02 - logprior: -4.2620e+00
Epoch 4/10
10/10 - 1s - loss: 240.7379 - loglik: -2.3857e+02 - logprior: -2.1667e+00
Epoch 5/10
10/10 - 1s - loss: 233.6593 - loglik: -2.3252e+02 - logprior: -1.1440e+00
Epoch 6/10
10/10 - 1s - loss: 230.2522 - loglik: -2.2962e+02 - logprior: -6.3495e-01
Epoch 7/10
10/10 - 1s - loss: 228.8158 - loglik: -2.2839e+02 - logprior: -4.2149e-01
Epoch 8/10
10/10 - 1s - loss: 227.5494 - loglik: -2.2728e+02 - logprior: -2.7064e-01
Epoch 9/10
10/10 - 1s - loss: 227.0404 - loglik: -2.2691e+02 - logprior: -1.2825e-01
Epoch 10/10
10/10 - 1s - loss: 226.0086 - loglik: -2.2594e+02 - logprior: -7.2070e-02
Fitted a model with MAP estimate = -226.0674
expansions: [(0, 3), (7, 2), (8, 2), (25, 1), (36, 2), (38, 2), (43, 7), (54, 3), (56, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 276.5984 - loglik: -2.2482e+02 - logprior: -5.1781e+01
Epoch 2/2
10/10 - 1s - loss: 233.6375 - loglik: -2.1812e+02 - logprior: -1.5516e+01
Fitted a model with MAP estimate = -225.3574
expansions: []
discards: [ 0  1  2 48 74 78]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 254.0129 - loglik: -2.1655e+02 - logprior: -3.7459e+01
Epoch 2/2
10/10 - 1s - loss: 224.0669 - loglik: -2.1493e+02 - logprior: -9.1379e+00
Fitted a model with MAP estimate = -219.7445
expansions: [(0, 3), (34, 2)]
discards: [9]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 265.9988 - loglik: -2.1618e+02 - logprior: -4.9817e+01
Epoch 2/10
10/10 - 1s - loss: 228.6211 - loglik: -2.1459e+02 - logprior: -1.4034e+01
Epoch 3/10
10/10 - 1s - loss: 219.2977 - loglik: -2.1408e+02 - logprior: -5.2132e+00
Epoch 4/10
10/10 - 1s - loss: 216.0708 - loglik: -2.1430e+02 - logprior: -1.7712e+00
Epoch 5/10
10/10 - 1s - loss: 213.6363 - loglik: -2.1350e+02 - logprior: -1.3559e-01
Epoch 6/10
10/10 - 1s - loss: 213.1730 - loglik: -2.1385e+02 - logprior: 0.6787
Epoch 7/10
10/10 - 1s - loss: 212.3350 - loglik: -2.1346e+02 - logprior: 1.1209
Epoch 8/10
10/10 - 1s - loss: 211.6510 - loglik: -2.1305e+02 - logprior: 1.3960
Epoch 9/10
10/10 - 1s - loss: 212.4041 - loglik: -2.1404e+02 - logprior: 1.6373
Fitted a model with MAP estimate = -211.6052
Time for alignment: 39.6357
Computed alignments with likelihoods: ['-210.7568', '-210.9965', '-211.3380', '-212.4172', '-211.6052']
Best model has likelihood: -210.7568  (prior= 2.0270 )
time for generating output: 0.1718
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7220241039123305
Training of 5 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f557c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d843bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49d8fa90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 551.0681 - loglik: -4.7328e+02 - logprior: -7.7792e+01
Epoch 2/10
10/10 - 2s - loss: 420.2306 - loglik: -4.0498e+02 - logprior: -1.5247e+01
Epoch 3/10
10/10 - 2s - loss: 344.2172 - loglik: -3.3879e+02 - logprior: -5.4266e+00
Epoch 4/10
10/10 - 2s - loss: 297.0831 - loglik: -2.9356e+02 - logprior: -3.5218e+00
Epoch 5/10
10/10 - 2s - loss: 277.9207 - loglik: -2.7509e+02 - logprior: -2.8340e+00
Epoch 6/10
10/10 - 2s - loss: 269.8401 - loglik: -2.6831e+02 - logprior: -1.5314e+00
Epoch 7/10
10/10 - 2s - loss: 266.5194 - loglik: -2.6593e+02 - logprior: -5.9401e-01
Epoch 8/10
10/10 - 2s - loss: 263.8128 - loglik: -2.6343e+02 - logprior: -3.8337e-01
Epoch 9/10
10/10 - 2s - loss: 263.8590 - loglik: -2.6367e+02 - logprior: -1.8421e-01
Fitted a model with MAP estimate = -262.5707
expansions: [(16, 4), (17, 2), (18, 1), (20, 1), (22, 1), (24, 2), (39, 2), (40, 1), (41, 1), (44, 1), (53, 1), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (117, 1), (118, 1), (124, 3), (126, 1), (128, 2), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 342.1520 - loglik: -2.5391e+02 - logprior: -8.8238e+01
Epoch 2/2
10/10 - 2s - loss: 261.9298 - loglik: -2.3038e+02 - logprior: -3.1553e+01
Fitted a model with MAP estimate = -249.5259
expansions: [(0, 3), (14, 1), (92, 1)]
discards: [  0  20  32  50 124 162]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 294.3820 - loglik: -2.2553e+02 - logprior: -6.8857e+01
Epoch 2/2
10/10 - 2s - loss: 232.1330 - loglik: -2.2059e+02 - logprior: -1.1542e+01
Fitted a model with MAP estimate = -222.7687
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 307.9388 - loglik: -2.2291e+02 - logprior: -8.5025e+01
Epoch 2/10
10/10 - 2s - loss: 247.4815 - loglik: -2.2207e+02 - logprior: -2.5412e+01
Epoch 3/10
10/10 - 2s - loss: 225.1935 - loglik: -2.2091e+02 - logprior: -4.2809e+00
Epoch 4/10
10/10 - 2s - loss: 213.9940 - loglik: -2.1979e+02 - logprior: 5.7996
Epoch 5/10
10/10 - 2s - loss: 211.2805 - loglik: -2.2066e+02 - logprior: 9.3797
Epoch 6/10
10/10 - 2s - loss: 208.9873 - loglik: -2.2017e+02 - logprior: 11.1854
Epoch 7/10
10/10 - 2s - loss: 206.6519 - loglik: -2.1905e+02 - logprior: 12.4026
Epoch 8/10
10/10 - 2s - loss: 205.9628 - loglik: -2.1933e+02 - logprior: 13.3643
Epoch 9/10
10/10 - 2s - loss: 206.7451 - loglik: -2.2093e+02 - logprior: 14.1867
Fitted a model with MAP estimate = -205.2627
Time for alignment: 64.7135
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 551.0284 - loglik: -4.7324e+02 - logprior: -7.7792e+01
Epoch 2/10
10/10 - 2s - loss: 419.9039 - loglik: -4.0465e+02 - logprior: -1.5253e+01
Epoch 3/10
10/10 - 2s - loss: 344.8427 - loglik: -3.3940e+02 - logprior: -5.4453e+00
Epoch 4/10
10/10 - 2s - loss: 296.9985 - loglik: -2.9335e+02 - logprior: -3.6533e+00
Epoch 5/10
10/10 - 2s - loss: 278.4022 - loglik: -2.7543e+02 - logprior: -2.9750e+00
Epoch 6/10
10/10 - 2s - loss: 270.1041 - loglik: -2.6843e+02 - logprior: -1.6720e+00
Epoch 7/10
10/10 - 2s - loss: 265.6152 - loglik: -2.6494e+02 - logprior: -6.7968e-01
Epoch 8/10
10/10 - 2s - loss: 264.2466 - loglik: -2.6388e+02 - logprior: -3.7017e-01
Epoch 9/10
10/10 - 2s - loss: 263.3922 - loglik: -2.6329e+02 - logprior: -1.0487e-01
Epoch 10/10
10/10 - 2s - loss: 262.1264 - loglik: -2.6233e+02 - logprior: 0.2044
Fitted a model with MAP estimate = -261.8257
expansions: [(16, 4), (18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (53, 1), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (117, 1), (118, 1), (124, 3), (126, 1), (128, 1), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 342.0049 - loglik: -2.5375e+02 - logprior: -8.8251e+01
Epoch 2/2
10/10 - 2s - loss: 263.2177 - loglik: -2.3185e+02 - logprior: -3.1369e+01
Fitted a model with MAP estimate = -251.1609
expansions: [(0, 3), (14, 1), (20, 1), (89, 1)]
discards: [  0  47 121]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 295.0035 - loglik: -2.2618e+02 - logprior: -6.8821e+01
Epoch 2/2
10/10 - 2s - loss: 232.1333 - loglik: -2.2078e+02 - logprior: -1.1352e+01
Fitted a model with MAP estimate = -222.2924
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 307.1736 - loglik: -2.2242e+02 - logprior: -8.4754e+01
Epoch 2/10
10/10 - 2s - loss: 245.8628 - loglik: -2.2083e+02 - logprior: -2.5033e+01
Epoch 3/10
10/10 - 2s - loss: 223.8155 - loglik: -2.1995e+02 - logprior: -3.8665e+00
Epoch 4/10
10/10 - 2s - loss: 214.5117 - loglik: -2.2056e+02 - logprior: 6.0472
Epoch 5/10
10/10 - 2s - loss: 209.6438 - loglik: -2.1926e+02 - logprior: 9.6211
Epoch 6/10
10/10 - 2s - loss: 207.8832 - loglik: -2.1930e+02 - logprior: 11.4179
Epoch 7/10
10/10 - 2s - loss: 206.9469 - loglik: -2.1956e+02 - logprior: 12.6094
Epoch 8/10
10/10 - 2s - loss: 206.0819 - loglik: -2.1965e+02 - logprior: 13.5696
Epoch 9/10
10/10 - 2s - loss: 204.9571 - loglik: -2.1936e+02 - logprior: 14.3991
Epoch 10/10
10/10 - 2s - loss: 204.0051 - loglik: -2.1912e+02 - logprior: 15.1114
Fitted a model with MAP estimate = -203.9994
Time for alignment: 68.8966
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 551.1908 - loglik: -4.7340e+02 - logprior: -7.7790e+01
Epoch 2/10
10/10 - 2s - loss: 419.9557 - loglik: -4.0471e+02 - logprior: -1.5246e+01
Epoch 3/10
10/10 - 2s - loss: 344.6007 - loglik: -3.3915e+02 - logprior: -5.4484e+00
Epoch 4/10
10/10 - 2s - loss: 299.3199 - loglik: -2.9589e+02 - logprior: -3.4262e+00
Epoch 5/10
10/10 - 2s - loss: 278.2197 - loglik: -2.7550e+02 - logprior: -2.7193e+00
Epoch 6/10
10/10 - 2s - loss: 271.2191 - loglik: -2.6939e+02 - logprior: -1.8255e+00
Epoch 7/10
10/10 - 2s - loss: 266.6965 - loglik: -2.6569e+02 - logprior: -1.0071e+00
Epoch 8/10
10/10 - 2s - loss: 264.6250 - loglik: -2.6384e+02 - logprior: -7.8774e-01
Epoch 9/10
10/10 - 2s - loss: 262.9643 - loglik: -2.6236e+02 - logprior: -6.0666e-01
Epoch 10/10
10/10 - 2s - loss: 262.6216 - loglik: -2.6239e+02 - logprior: -2.3220e-01
Fitted a model with MAP estimate = -261.9335
expansions: [(17, 4), (19, 1), (21, 1), (22, 1), (23, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (53, 1), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (117, 1), (118, 1), (124, 3), (126, 1), (128, 1), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 341.8021 - loglik: -2.5361e+02 - logprior: -8.8190e+01
Epoch 2/2
10/10 - 2s - loss: 261.7314 - loglik: -2.3033e+02 - logprior: -3.1401e+01
Fitted a model with MAP estimate = -249.4764
expansions: [(0, 3), (90, 1)]
discards: [  0  48  97 123]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 294.4086 - loglik: -2.2538e+02 - logprior: -6.9033e+01
Epoch 2/2
10/10 - 2s - loss: 232.2616 - loglik: -2.2060e+02 - logprior: -1.1658e+01
Fitted a model with MAP estimate = -223.3560
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 308.4444 - loglik: -2.2334e+02 - logprior: -8.5100e+01
Epoch 2/10
10/10 - 2s - loss: 247.7184 - loglik: -2.2244e+02 - logprior: -2.5273e+01
Epoch 3/10
10/10 - 2s - loss: 225.5258 - loglik: -2.2143e+02 - logprior: -4.0912e+00
Epoch 4/10
10/10 - 2s - loss: 215.1219 - loglik: -2.2084e+02 - logprior: 5.7132
Epoch 5/10
10/10 - 2s - loss: 211.6812 - loglik: -2.2094e+02 - logprior: 9.2619
Epoch 6/10
10/10 - 2s - loss: 209.0885 - loglik: -2.2014e+02 - logprior: 11.0543
Epoch 7/10
10/10 - 2s - loss: 207.8272 - loglik: -2.2010e+02 - logprior: 12.2739
Epoch 8/10
10/10 - 2s - loss: 206.8476 - loglik: -2.2006e+02 - logprior: 13.2167
Epoch 9/10
10/10 - 2s - loss: 206.6408 - loglik: -2.2069e+02 - logprior: 14.0466
Epoch 10/10
10/10 - 2s - loss: 205.5381 - loglik: -2.2031e+02 - logprior: 14.7672
Fitted a model with MAP estimate = -205.3034
Time for alignment: 67.4611
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 551.0294 - loglik: -4.7324e+02 - logprior: -7.7790e+01
Epoch 2/10
10/10 - 2s - loss: 420.0597 - loglik: -4.0481e+02 - logprior: -1.5249e+01
Epoch 3/10
10/10 - 2s - loss: 344.5305 - loglik: -3.3910e+02 - logprior: -5.4278e+00
Epoch 4/10
10/10 - 2s - loss: 298.2656 - loglik: -2.9482e+02 - logprior: -3.4439e+00
Epoch 5/10
10/10 - 2s - loss: 278.1006 - loglik: -2.7541e+02 - logprior: -2.6955e+00
Epoch 6/10
10/10 - 2s - loss: 270.1411 - loglik: -2.6850e+02 - logprior: -1.6447e+00
Epoch 7/10
10/10 - 2s - loss: 266.2735 - loglik: -2.6557e+02 - logprior: -7.0004e-01
Epoch 8/10
10/10 - 2s - loss: 264.1208 - loglik: -2.6369e+02 - logprior: -4.2997e-01
Epoch 9/10
10/10 - 2s - loss: 263.2505 - loglik: -2.6314e+02 - logprior: -1.0625e-01
Epoch 10/10
10/10 - 2s - loss: 262.0082 - loglik: -2.6221e+02 - logprior: 0.1998
Fitted a model with MAP estimate = -261.8055
expansions: [(16, 4), (18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (53, 1), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (117, 1), (118, 1), (124, 3), (126, 1), (128, 1), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 342.1790 - loglik: -2.5391e+02 - logprior: -8.8267e+01
Epoch 2/2
10/10 - 2s - loss: 263.0554 - loglik: -2.3168e+02 - logprior: -3.1375e+01
Fitted a model with MAP estimate = -251.1917
expansions: [(0, 3), (14, 1), (20, 1), (89, 1)]
discards: [  0  47 121]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 295.5943 - loglik: -2.2675e+02 - logprior: -6.8843e+01
Epoch 2/2
10/10 - 2s - loss: 231.5361 - loglik: -2.2023e+02 - logprior: -1.1308e+01
Fitted a model with MAP estimate = -222.4574
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 306.5967 - loglik: -2.2184e+02 - logprior: -8.4757e+01
Epoch 2/10
10/10 - 2s - loss: 246.0912 - loglik: -2.2108e+02 - logprior: -2.5007e+01
Epoch 3/10
10/10 - 2s - loss: 224.7905 - loglik: -2.2092e+02 - logprior: -3.8696e+00
Epoch 4/10
10/10 - 2s - loss: 214.3951 - loglik: -2.2045e+02 - logprior: 6.0541
Epoch 5/10
10/10 - 2s - loss: 209.5050 - loglik: -2.1914e+02 - logprior: 9.6307
Epoch 6/10
10/10 - 2s - loss: 207.7816 - loglik: -2.1922e+02 - logprior: 11.4420
Epoch 7/10
10/10 - 2s - loss: 206.8601 - loglik: -2.1949e+02 - logprior: 12.6305
Epoch 8/10
10/10 - 2s - loss: 206.0265 - loglik: -2.1961e+02 - logprior: 13.5880
Epoch 9/10
10/10 - 2s - loss: 205.0125 - loglik: -2.1944e+02 - logprior: 14.4290
Epoch 10/10
10/10 - 2s - loss: 204.6718 - loglik: -2.1982e+02 - logprior: 15.1469
Fitted a model with MAP estimate = -203.9686
Time for alignment: 69.2772
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 551.5104 - loglik: -4.7372e+02 - logprior: -7.7791e+01
Epoch 2/10
10/10 - 2s - loss: 419.4206 - loglik: -4.0417e+02 - logprior: -1.5250e+01
Epoch 3/10
10/10 - 2s - loss: 344.6873 - loglik: -3.3925e+02 - logprior: -5.4343e+00
Epoch 4/10
10/10 - 2s - loss: 296.9927 - loglik: -2.9336e+02 - logprior: -3.6305e+00
Epoch 5/10
10/10 - 2s - loss: 278.2726 - loglik: -2.7515e+02 - logprior: -3.1245e+00
Epoch 6/10
10/10 - 2s - loss: 270.0264 - loglik: -2.6813e+02 - logprior: -1.8973e+00
Epoch 7/10
10/10 - 2s - loss: 266.4439 - loglik: -2.6550e+02 - logprior: -9.3934e-01
Epoch 8/10
10/10 - 2s - loss: 264.0038 - loglik: -2.6323e+02 - logprior: -7.7100e-01
Epoch 9/10
10/10 - 2s - loss: 263.2739 - loglik: -2.6270e+02 - logprior: -5.7250e-01
Epoch 10/10
10/10 - 2s - loss: 262.5482 - loglik: -2.6226e+02 - logprior: -2.8401e-01
Fitted a model with MAP estimate = -261.7404
expansions: [(16, 4), (18, 1), (20, 1), (22, 1), (24, 2), (39, 2), (40, 1), (41, 1), (44, 1), (53, 1), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (117, 1), (118, 1), (121, 1), (123, 1), (124, 1), (126, 1), (128, 1), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 342.3654 - loglik: -2.5407e+02 - logprior: -8.8297e+01
Epoch 2/2
10/10 - 2s - loss: 263.8081 - loglik: -2.3221e+02 - logprior: -3.1597e+01
Fitted a model with MAP estimate = -250.6955
expansions: [(0, 3), (14, 1), (20, 1), (90, 1)]
discards: [  0  30  48 122]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 294.5221 - loglik: -2.2561e+02 - logprior: -6.8915e+01
Epoch 2/2
10/10 - 2s - loss: 233.1204 - loglik: -2.2165e+02 - logprior: -1.1468e+01
Fitted a model with MAP estimate = -222.6367
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 308.0341 - loglik: -2.2311e+02 - logprior: -8.4927e+01
Epoch 2/10
10/10 - 2s - loss: 247.0211 - loglik: -2.2188e+02 - logprior: -2.5138e+01
Epoch 3/10
10/10 - 2s - loss: 224.9032 - loglik: -2.2094e+02 - logprior: -3.9656e+00
Epoch 4/10
10/10 - 2s - loss: 213.9961 - loglik: -2.1990e+02 - logprior: 5.9013
Epoch 5/10
10/10 - 2s - loss: 210.1930 - loglik: -2.1964e+02 - logprior: 9.4470
Epoch 6/10
10/10 - 2s - loss: 208.6580 - loglik: -2.1992e+02 - logprior: 11.2614
Epoch 7/10
10/10 - 2s - loss: 206.9902 - loglik: -2.1946e+02 - logprior: 12.4686
Epoch 8/10
10/10 - 2s - loss: 206.3895 - loglik: -2.1980e+02 - logprior: 13.4094
Epoch 9/10
10/10 - 2s - loss: 205.8458 - loglik: -2.2010e+02 - logprior: 14.2493
Epoch 10/10
10/10 - 2s - loss: 204.5569 - loglik: -2.1952e+02 - logprior: 14.9627
Fitted a model with MAP estimate = -204.5150
Time for alignment: 67.1257
Computed alignments with likelihoods: ['-205.2627', '-203.9994', '-205.3034', '-203.9686', '-204.5150']
Best model has likelihood: -203.9686  (prior= 15.5143 )
time for generating output: 0.1887
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9316143497757847
Training of 5 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaeb9eb070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5a4e8160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebef1070>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 23s - loss: 814.0132 - loglik: -8.1202e+02 - logprior: -1.9975e+00
Epoch 2/10
33/33 - 20s - loss: 723.2042 - loglik: -7.2256e+02 - logprior: -6.4674e-01
Epoch 3/10
33/33 - 21s - loss: 712.5770 - loglik: -7.1203e+02 - logprior: -5.4857e-01
Epoch 4/10
33/33 - 21s - loss: 710.0183 - loglik: -7.0953e+02 - logprior: -4.9250e-01
Epoch 5/10
33/33 - 21s - loss: 709.9662 - loglik: -7.0948e+02 - logprior: -4.9105e-01
Epoch 6/10
33/33 - 21s - loss: 711.7664 - loglik: -7.1129e+02 - logprior: -4.7319e-01
Fitted a model with MAP estimate = -709.7127
expansions: [(0, 5), (10, 2), (11, 1), (35, 4), (72, 1), (73, 1), (74, 3), (78, 1), (91, 1), (113, 1), (116, 1), (118, 1), (134, 2), (155, 2), (164, 4), (181, 2), (204, 1), (210, 1), (220, 1), (221, 4), (230, 4)]
discards: [  3   4 224 225 226 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 30s - loss: 709.8718 - loglik: -7.0702e+02 - logprior: -2.8525e+00
Epoch 2/2
33/33 - 25s - loss: 700.5790 - loglik: -7.0021e+02 - logprior: -3.6703e-01
Fitted a model with MAP estimate = -698.6964
expansions: [(265, 5)]
discards: [  1   5   6   7  16  42 177 189 190 191 210 258 261 262 263 264]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 709.2524 - loglik: -7.0723e+02 - logprior: -2.0179e+00
Epoch 2/2
33/33 - 24s - loss: 700.6927 - loglik: -7.0070e+02 - logprior: 0.0036
Fitted a model with MAP estimate = -700.4078
expansions: [(0, 5), (1, 1), (254, 4)]
discards: [249 250 251 252 253]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 708.3835 - loglik: -7.0536e+02 - logprior: -3.0198e+00
Epoch 2/10
33/33 - 24s - loss: 701.1815 - loglik: -7.0115e+02 - logprior: -3.1823e-02
Epoch 3/10
33/33 - 24s - loss: 699.1701 - loglik: -6.9927e+02 - logprior: 0.1040
Epoch 4/10
33/33 - 25s - loss: 701.7229 - loglik: -7.0190e+02 - logprior: 0.1730
Fitted a model with MAP estimate = -699.0553
Time for alignment: 426.1808
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 23s - loss: 814.8126 - loglik: -8.1280e+02 - logprior: -2.0109e+00
Epoch 2/10
33/33 - 21s - loss: 721.4789 - loglik: -7.2095e+02 - logprior: -5.2448e-01
Epoch 3/10
33/33 - 21s - loss: 713.5003 - loglik: -7.1303e+02 - logprior: -4.6640e-01
Epoch 4/10
33/33 - 21s - loss: 710.0748 - loglik: -7.0963e+02 - logprior: -4.4125e-01
Epoch 5/10
33/33 - 21s - loss: 707.6152 - loglik: -7.0719e+02 - logprior: -4.2299e-01
Epoch 6/10
33/33 - 21s - loss: 711.0588 - loglik: -7.1062e+02 - logprior: -4.3488e-01
Fitted a model with MAP estimate = -708.5785
expansions: [(0, 6), (4, 1), (9, 1), (34, 1), (65, 1), (72, 1), (73, 1), (74, 2), (79, 2), (112, 1), (113, 1), (119, 1), (133, 2), (156, 6), (184, 1), (204, 1), (223, 3), (224, 2), (230, 4)]
discards: [ 63 225 226 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 262 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 710.4607 - loglik: -7.0759e+02 - logprior: -2.8707e+00
Epoch 2/2
33/33 - 25s - loss: 701.7889 - loglik: -7.0151e+02 - logprior: -2.7954e-01
Fitted a model with MAP estimate = -699.7422
expansions: [(0, 5), (253, 1), (262, 5)]
discards: [  1   6   7   8   9 178 179 180 254 255 258 259 260 261]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 708.7525 - loglik: -7.0560e+02 - logprior: -3.1561e+00
Epoch 2/2
33/33 - 24s - loss: 700.8892 - loglik: -7.0072e+02 - logprior: -1.7062e-01
Fitted a model with MAP estimate = -699.7145
expansions: [(74, 1), (259, 5)]
discards: [  1   5   7   8   9  72 254 255 256 257 258]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 707.2252 - loglik: -7.0533e+02 - logprior: -1.8909e+00
Epoch 2/10
33/33 - 23s - loss: 699.2980 - loglik: -6.9943e+02 - logprior: 0.1290
Epoch 3/10
33/33 - 24s - loss: 700.2449 - loglik: -7.0055e+02 - logprior: 0.3093
Fitted a model with MAP estimate = -699.4497
Time for alignment: 399.6392
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 25s - loss: 811.5524 - loglik: -8.0955e+02 - logprior: -2.0059e+00
Epoch 2/10
33/33 - 21s - loss: 719.7785 - loglik: -7.1918e+02 - logprior: -6.0214e-01
Epoch 3/10
33/33 - 21s - loss: 714.7689 - loglik: -7.1430e+02 - logprior: -4.6795e-01
Epoch 4/10
33/33 - 21s - loss: 708.8125 - loglik: -7.0838e+02 - logprior: -4.3235e-01
Epoch 5/10
33/33 - 21s - loss: 709.4539 - loglik: -7.0907e+02 - logprior: -3.8768e-01
Fitted a model with MAP estimate = -709.6310
expansions: [(0, 6), (9, 1), (10, 1), (34, 4), (64, 1), (71, 1), (72, 1), (73, 3), (77, 1), (112, 1), (113, 1), (116, 1), (134, 2), (155, 3), (163, 5), (177, 1), (204, 1), (210, 1), (220, 1), (221, 4), (230, 4)]
discards: [ 62 224 225 226 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 710.1487 - loglik: -7.0733e+02 - logprior: -2.8166e+00
Epoch 2/2
33/33 - 26s - loss: 698.6086 - loglik: -6.9831e+02 - logprior: -2.9978e-01
Fitted a model with MAP estimate = -698.0857
expansions: [(0, 5), (267, 5)]
discards: [  1   2   6   7   8   9  17  43 179 190 191 192 193 194 260 263 264 265
 266]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 708.5663 - loglik: -7.0547e+02 - logprior: -3.0953e+00
Epoch 2/2
33/33 - 24s - loss: 702.0627 - loglik: -7.0188e+02 - logprior: -1.8401e-01
Fitted a model with MAP estimate = -699.6642
expansions: [(73, 1)]
discards: [  1   6   7   8  71 253 254 255 256 257]
Re-initialized the encoder parameters.
Fitting a model of length 249 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 706.9417 - loglik: -7.0500e+02 - logprior: -1.9450e+00
Epoch 2/10
33/33 - 23s - loss: 703.8454 - loglik: -7.0395e+02 - logprior: 0.1052
Epoch 3/10
33/33 - 23s - loss: 701.6366 - loglik: -7.0192e+02 - logprior: 0.2843
Epoch 4/10
33/33 - 23s - loss: 700.1993 - loglik: -7.0054e+02 - logprior: 0.3375
Epoch 5/10
33/33 - 23s - loss: 700.1331 - loglik: -7.0055e+02 - logprior: 0.4177
Epoch 6/10
33/33 - 23s - loss: 701.6322 - loglik: -7.0210e+02 - logprior: 0.4656
Fitted a model with MAP estimate = -699.6429
Time for alignment: 448.7795
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 23s - loss: 814.6074 - loglik: -8.1258e+02 - logprior: -2.0318e+00
Epoch 2/10
33/33 - 20s - loss: 722.2729 - loglik: -7.2165e+02 - logprior: -6.1821e-01
Epoch 3/10
33/33 - 21s - loss: 712.3701 - loglik: -7.1189e+02 - logprior: -4.8323e-01
Epoch 4/10
33/33 - 21s - loss: 707.8884 - loglik: -7.0746e+02 - logprior: -4.2433e-01
Epoch 5/10
33/33 - 21s - loss: 712.4858 - loglik: -7.1207e+02 - logprior: -4.1562e-01
Fitted a model with MAP estimate = -709.5133
expansions: [(0, 6), (9, 1), (10, 1), (34, 4), (44, 1), (72, 1), (73, 1), (74, 2), (79, 2), (109, 1), (113, 1), (119, 1), (133, 2), (155, 5), (181, 2), (204, 1), (214, 1), (220, 1), (221, 4), (230, 4)]
discards: [224 225 226 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 708.8175 - loglik: -7.0594e+02 - logprior: -2.8730e+00
Epoch 2/2
33/33 - 25s - loss: 702.8961 - loglik: -7.0259e+02 - logprior: -3.0853e-01
Fitted a model with MAP estimate = -699.2948
expansions: [(0, 5), (266, 5)]
discards: [  1   2   6   7   8   9  17  43  75 180 181 211 259 262 263 264]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 708.6021 - loglik: -7.0549e+02 - logprior: -3.1089e+00
Epoch 2/2
33/33 - 24s - loss: 703.5989 - loglik: -7.0346e+02 - logprior: -1.4082e-01
Fitted a model with MAP estimate = -700.1112
expansions: [(51, 1), (260, 5)]
discards: [  1   6   7 252 253 254 255 256 257 258 259]
Re-initialized the encoder parameters.
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 708.0227 - loglik: -7.0613e+02 - logprior: -1.8924e+00
Epoch 2/10
33/33 - 24s - loss: 699.7586 - loglik: -6.9985e+02 - logprior: 0.0963
Epoch 3/10
33/33 - 23s - loss: 700.0088 - loglik: -7.0029e+02 - logprior: 0.2826
Fitted a model with MAP estimate = -699.5571
Time for alignment: 381.4781
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 25s - loss: 814.3705 - loglik: -8.1235e+02 - logprior: -2.0209e+00
Epoch 2/10
33/33 - 21s - loss: 719.9660 - loglik: -7.1935e+02 - logprior: -6.2038e-01
Epoch 3/10
33/33 - 21s - loss: 712.5158 - loglik: -7.1198e+02 - logprior: -5.3149e-01
Epoch 4/10
33/33 - 21s - loss: 707.8834 - loglik: -7.0739e+02 - logprior: -4.9214e-01
Epoch 5/10
33/33 - 21s - loss: 708.6181 - loglik: -7.0813e+02 - logprior: -4.8667e-01
Fitted a model with MAP estimate = -707.8501
expansions: [(0, 6), (5, 1), (10, 1), (34, 5), (61, 1), (64, 2), (66, 1), (72, 1), (73, 1), (79, 2), (109, 2), (110, 4), (113, 1), (116, 1), (118, 1), (132, 2), (143, 1), (155, 1), (163, 4), (204, 1), (206, 1), (220, 1), (221, 4), (230, 4)]
discards: [224 225 226 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 711.0240 - loglik: -7.0821e+02 - logprior: -2.8147e+00
Epoch 2/2
33/33 - 27s - loss: 694.8062 - loglik: -6.9452e+02 - logprior: -2.8566e-01
Fitted a model with MAP estimate = -697.6138
expansions: [(0, 5), (273, 5)]
discards: [  1   2   6   7   8   9  17  43  44  76 131 132 133 134 135 198 199 200
 266 267 269 270 271]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 707.0780 - loglik: -7.0399e+02 - logprior: -3.0848e+00
Epoch 2/2
33/33 - 24s - loss: 702.2985 - loglik: -7.0218e+02 - logprior: -1.1563e-01
Fitted a model with MAP estimate = -700.0255
expansions: [(260, 5)]
discards: [  5   6   7   8 253 257 258]
Re-initialized the encoder parameters.
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 708.5611 - loglik: -7.0665e+02 - logprior: -1.9112e+00
Epoch 2/10
33/33 - 24s - loss: 700.8509 - loglik: -7.0089e+02 - logprior: 0.0439
Epoch 3/10
33/33 - 24s - loss: 700.5280 - loglik: -7.0076e+02 - logprior: 0.2281
Epoch 4/10
33/33 - 24s - loss: 700.7176 - loglik: -7.0101e+02 - logprior: 0.2958
Fitted a model with MAP estimate = -699.3081
Time for alignment: 410.6299
Computed alignments with likelihoods: ['-698.6964', '-699.4497', '-698.0857', '-699.2948', '-697.6138']
Best model has likelihood: -697.6138  (prior= -0.1850 )
time for generating output: 0.3038
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.776595744680851
Training of 5 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea13725220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f5d4310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2cc861c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.8367 - loglik: -3.4890e+02 - logprior: -6.1933e+01
Epoch 2/10
10/10 - 1s - loss: 322.0061 - loglik: -3.0767e+02 - logprior: -1.4334e+01
Epoch 3/10
10/10 - 1s - loss: 277.8574 - loglik: -2.7163e+02 - logprior: -6.2266e+00
Epoch 4/10
10/10 - 1s - loss: 253.6350 - loglik: -2.4996e+02 - logprior: -3.6793e+00
Epoch 5/10
10/10 - 1s - loss: 240.9212 - loglik: -2.3853e+02 - logprior: -2.3952e+00
Epoch 6/10
10/10 - 1s - loss: 235.9768 - loglik: -2.3454e+02 - logprior: -1.4405e+00
Epoch 7/10
10/10 - 1s - loss: 233.5113 - loglik: -2.3257e+02 - logprior: -9.3998e-01
Epoch 8/10
10/10 - 1s - loss: 232.0294 - loglik: -2.3136e+02 - logprior: -6.7111e-01
Epoch 9/10
10/10 - 1s - loss: 231.3796 - loglik: -2.3091e+02 - logprior: -4.6553e-01
Epoch 10/10
10/10 - 1s - loss: 230.6228 - loglik: -2.3033e+02 - logprior: -2.9348e-01
Fitted a model with MAP estimate = -230.5285
expansions: [(11, 1), (13, 7), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 303.2591 - loglik: -2.3385e+02 - logprior: -6.9409e+01
Epoch 2/2
10/10 - 1s - loss: 240.3905 - loglik: -2.1422e+02 - logprior: -2.6173e+01
Fitted a model with MAP estimate = -229.0174
expansions: [(0, 4)]
discards: [ 0 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 267.8673 - loglik: -2.1340e+02 - logprior: -5.4466e+01
Epoch 2/2
10/10 - 1s - loss: 219.3790 - loglik: -2.0765e+02 - logprior: -1.1731e+01
Fitted a model with MAP estimate = -212.0789
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 263.8439 - loglik: -2.1026e+02 - logprior: -5.3588e+01
Epoch 2/10
10/10 - 1s - loss: 218.3658 - loglik: -2.0704e+02 - logprior: -1.1323e+01
Epoch 3/10
10/10 - 1s - loss: 208.3715 - loglik: -2.0579e+02 - logprior: -2.5801e+00
Epoch 4/10
10/10 - 1s - loss: 203.2041 - loglik: -2.0416e+02 - logprior: 0.9540
Epoch 5/10
10/10 - 1s - loss: 200.1707 - loglik: -2.0296e+02 - logprior: 2.7851
Epoch 6/10
10/10 - 1s - loss: 198.0851 - loglik: -2.0184e+02 - logprior: 3.7592
Epoch 7/10
10/10 - 1s - loss: 196.8017 - loglik: -2.0124e+02 - logprior: 4.4366
Epoch 8/10
10/10 - 1s - loss: 197.2722 - loglik: -2.0240e+02 - logprior: 5.1233
Fitted a model with MAP estimate = -196.3046
Time for alignment: 46.2433
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.6835 - loglik: -3.4875e+02 - logprior: -6.1933e+01
Epoch 2/10
10/10 - 1s - loss: 321.8933 - loglik: -3.0756e+02 - logprior: -1.4335e+01
Epoch 3/10
10/10 - 1s - loss: 276.9603 - loglik: -2.7072e+02 - logprior: -6.2356e+00
Epoch 4/10
10/10 - 1s - loss: 250.6214 - loglik: -2.4678e+02 - logprior: -3.8454e+00
Epoch 5/10
10/10 - 1s - loss: 238.9128 - loglik: -2.3635e+02 - logprior: -2.5662e+00
Epoch 6/10
10/10 - 1s - loss: 234.7108 - loglik: -2.3307e+02 - logprior: -1.6381e+00
Epoch 7/10
10/10 - 1s - loss: 232.5607 - loglik: -2.3151e+02 - logprior: -1.0498e+00
Epoch 8/10
10/10 - 1s - loss: 231.3815 - loglik: -2.3064e+02 - logprior: -7.4532e-01
Epoch 9/10
10/10 - 1s - loss: 230.5477 - loglik: -2.2996e+02 - logprior: -5.9071e-01
Epoch 10/10
10/10 - 1s - loss: 230.3105 - loglik: -2.2987e+02 - logprior: -4.4129e-01
Fitted a model with MAP estimate = -229.9570
expansions: [(11, 1), (12, 3), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (54, 1), (62, 1), (63, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 301.0444 - loglik: -2.3171e+02 - logprior: -6.9331e+01
Epoch 2/2
10/10 - 1s - loss: 239.0565 - loglik: -2.1300e+02 - logprior: -2.6057e+01
Fitted a model with MAP estimate = -227.6725
expansions: [(0, 5)]
discards: [ 0 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 266.6168 - loglik: -2.1204e+02 - logprior: -5.4578e+01
Epoch 2/2
10/10 - 1s - loss: 218.9542 - loglik: -2.0717e+02 - logprior: -1.1782e+01
Fitted a model with MAP estimate = -210.8882
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 262.2006 - loglik: -2.0863e+02 - logprior: -5.3569e+01
Epoch 2/10
10/10 - 1s - loss: 217.9565 - loglik: -2.0671e+02 - logprior: -1.1248e+01
Epoch 3/10
10/10 - 1s - loss: 207.0658 - loglik: -2.0454e+02 - logprior: -2.5208e+00
Epoch 4/10
10/10 - 1s - loss: 202.3995 - loglik: -2.0341e+02 - logprior: 1.0143
Epoch 5/10
10/10 - 1s - loss: 198.8954 - loglik: -2.0172e+02 - logprior: 2.8265
Epoch 6/10
10/10 - 1s - loss: 197.0487 - loglik: -2.0085e+02 - logprior: 3.8039
Epoch 7/10
10/10 - 1s - loss: 195.9244 - loglik: -2.0042e+02 - logprior: 4.4921
Epoch 8/10
10/10 - 1s - loss: 194.9699 - loglik: -2.0018e+02 - logprior: 5.2101
Epoch 9/10
10/10 - 1s - loss: 195.0150 - loglik: -2.0081e+02 - logprior: 5.7950
Fitted a model with MAP estimate = -194.5023
Time for alignment: 47.5157
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 410.8768 - loglik: -3.4894e+02 - logprior: -6.1936e+01
Epoch 2/10
10/10 - 1s - loss: 322.3685 - loglik: -3.0803e+02 - logprior: -1.4343e+01
Epoch 3/10
10/10 - 1s - loss: 278.3008 - loglik: -2.7197e+02 - logprior: -6.3358e+00
Epoch 4/10
10/10 - 1s - loss: 253.1875 - loglik: -2.4936e+02 - logprior: -3.8313e+00
Epoch 5/10
10/10 - 1s - loss: 239.7360 - loglik: -2.3705e+02 - logprior: -2.6818e+00
Epoch 6/10
10/10 - 1s - loss: 234.0930 - loglik: -2.3198e+02 - logprior: -2.1097e+00
Epoch 7/10
10/10 - 1s - loss: 231.4754 - loglik: -2.2985e+02 - logprior: -1.6270e+00
Epoch 8/10
10/10 - 1s - loss: 231.2739 - loglik: -2.3002e+02 - logprior: -1.2505e+00
Epoch 9/10
10/10 - 1s - loss: 229.8186 - loglik: -2.2882e+02 - logprior: -9.9873e-01
Epoch 10/10
10/10 - 1s - loss: 229.2225 - loglik: -2.2838e+02 - logprior: -8.4252e-01
Fitted a model with MAP estimate = -229.3589
expansions: [(8, 1), (13, 2), (14, 5), (28, 1), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 301.7021 - loglik: -2.3219e+02 - logprior: -6.9511e+01
Epoch 2/2
10/10 - 1s - loss: 239.5244 - loglik: -2.1313e+02 - logprior: -2.6397e+01
Fitted a model with MAP estimate = -227.9480
expansions: [(0, 8)]
discards: [  0  13  14 109]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 267.6400 - loglik: -2.1282e+02 - logprior: -5.4821e+01
Epoch 2/2
10/10 - 2s - loss: 220.1236 - loglik: -2.0812e+02 - logprior: -1.2002e+01
Fitted a model with MAP estimate = -211.7477
expansions: []
discards: [1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 263.0172 - loglik: -2.0940e+02 - logprior: -5.3622e+01
Epoch 2/10
10/10 - 1s - loss: 218.8092 - loglik: -2.0756e+02 - logprior: -1.1248e+01
Epoch 3/10
10/10 - 1s - loss: 207.7819 - loglik: -2.0533e+02 - logprior: -2.4534e+00
Epoch 4/10
10/10 - 1s - loss: 202.8001 - loglik: -2.0386e+02 - logprior: 1.0596
Epoch 5/10
10/10 - 1s - loss: 199.4086 - loglik: -2.0232e+02 - logprior: 2.9071
Epoch 6/10
10/10 - 1s - loss: 197.4362 - loglik: -2.0134e+02 - logprior: 3.9017
Epoch 7/10
10/10 - 1s - loss: 195.8772 - loglik: -2.0054e+02 - logprior: 4.6667
Epoch 8/10
10/10 - 1s - loss: 195.7016 - loglik: -2.0107e+02 - logprior: 5.3720
Epoch 9/10
10/10 - 1s - loss: 195.2571 - loglik: -2.0120e+02 - logprior: 5.9433
Epoch 10/10
10/10 - 1s - loss: 194.5963 - loglik: -2.0096e+02 - logprior: 6.3632
Fitted a model with MAP estimate = -194.4886
Time for alignment: 50.3322
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.6235 - loglik: -3.4869e+02 - logprior: -6.1931e+01
Epoch 2/10
10/10 - 1s - loss: 322.5342 - loglik: -3.0820e+02 - logprior: -1.4337e+01
Epoch 3/10
10/10 - 1s - loss: 278.4466 - loglik: -2.7218e+02 - logprior: -6.2694e+00
Epoch 4/10
10/10 - 1s - loss: 254.5714 - loglik: -2.5091e+02 - logprior: -3.6573e+00
Epoch 5/10
10/10 - 1s - loss: 240.1443 - loglik: -2.3761e+02 - logprior: -2.5345e+00
Epoch 6/10
10/10 - 1s - loss: 234.1884 - loglik: -2.3218e+02 - logprior: -2.0100e+00
Epoch 7/10
10/10 - 1s - loss: 231.9729 - loglik: -2.3056e+02 - logprior: -1.4097e+00
Epoch 8/10
10/10 - 1s - loss: 230.6169 - loglik: -2.2955e+02 - logprior: -1.0630e+00
Epoch 9/10
10/10 - 1s - loss: 230.0653 - loglik: -2.2924e+02 - logprior: -8.3009e-01
Epoch 10/10
10/10 - 1s - loss: 229.3063 - loglik: -2.2864e+02 - logprior: -6.6718e-01
Fitted a model with MAP estimate = -229.2602
expansions: [(8, 1), (12, 3), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (54, 1), (63, 1), (69, 1), (70, 1), (78, 1), (79, 1), (85, 1), (91, 2), (92, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 301.9133 - loglik: -2.3241e+02 - logprior: -6.9499e+01
Epoch 2/2
10/10 - 1s - loss: 238.7290 - loglik: -2.1243e+02 - logprior: -2.6303e+01
Fitted a model with MAP estimate = -227.8435
expansions: [(0, 5)]
discards: [  0  13  14 112]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 266.7379 - loglik: -2.1213e+02 - logprior: -5.4603e+01
Epoch 2/2
10/10 - 1s - loss: 218.7509 - loglik: -2.0698e+02 - logprior: -1.1768e+01
Fitted a model with MAP estimate = -210.8647
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 262.4991 - loglik: -2.0898e+02 - logprior: -5.3519e+01
Epoch 2/10
10/10 - 1s - loss: 217.9113 - loglik: -2.0671e+02 - logprior: -1.1200e+01
Epoch 3/10
10/10 - 1s - loss: 207.5994 - loglik: -2.0513e+02 - logprior: -2.4650e+00
Epoch 4/10
10/10 - 1s - loss: 201.5175 - loglik: -2.0258e+02 - logprior: 1.0642
Epoch 5/10
10/10 - 1s - loss: 198.4384 - loglik: -2.0133e+02 - logprior: 2.8943
Epoch 6/10
10/10 - 1s - loss: 196.7897 - loglik: -2.0065e+02 - logprior: 3.8568
Epoch 7/10
10/10 - 1s - loss: 196.4104 - loglik: -2.0096e+02 - logprior: 4.5492
Epoch 8/10
10/10 - 1s - loss: 195.6314 - loglik: -2.0087e+02 - logprior: 5.2425
Epoch 9/10
10/10 - 1s - loss: 194.5415 - loglik: -2.0036e+02 - logprior: 5.8195
Epoch 10/10
10/10 - 1s - loss: 195.1149 - loglik: -2.0135e+02 - logprior: 6.2365
Fitted a model with MAP estimate = -194.3077
Time for alignment: 48.9493
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.9589 - loglik: -3.4903e+02 - logprior: -6.1932e+01
Epoch 2/10
10/10 - 1s - loss: 321.7057 - loglik: -3.0737e+02 - logprior: -1.4333e+01
Epoch 3/10
10/10 - 1s - loss: 278.1833 - loglik: -2.7197e+02 - logprior: -6.2158e+00
Epoch 4/10
10/10 - 1s - loss: 253.3106 - loglik: -2.4965e+02 - logprior: -3.6598e+00
Epoch 5/10
10/10 - 1s - loss: 239.4149 - loglik: -2.3696e+02 - logprior: -2.4591e+00
Epoch 6/10
10/10 - 1s - loss: 235.5729 - loglik: -2.3391e+02 - logprior: -1.6638e+00
Epoch 7/10
10/10 - 1s - loss: 232.8614 - loglik: -2.3175e+02 - logprior: -1.1080e+00
Epoch 8/10
10/10 - 1s - loss: 231.4871 - loglik: -2.3068e+02 - logprior: -8.0584e-01
Epoch 9/10
10/10 - 1s - loss: 230.5524 - loglik: -2.2989e+02 - logprior: -6.6009e-01
Epoch 10/10
10/10 - 1s - loss: 230.3999 - loglik: -2.2989e+02 - logprior: -5.0600e-01
Fitted a model with MAP estimate = -230.0004
expansions: [(8, 1), (12, 3), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (54, 1), (63, 1), (64, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 300.4554 - loglik: -2.3107e+02 - logprior: -6.9389e+01
Epoch 2/2
10/10 - 1s - loss: 238.6116 - loglik: -2.1246e+02 - logprior: -2.6149e+01
Fitted a model with MAP estimate = -227.2089
expansions: [(0, 5)]
discards: [ 0 13 14]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 266.1386 - loglik: -2.1149e+02 - logprior: -5.4651e+01
Epoch 2/2
10/10 - 2s - loss: 218.3689 - loglik: -2.0657e+02 - logprior: -1.1804e+01
Fitted a model with MAP estimate = -210.8048
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 262.4209 - loglik: -2.0880e+02 - logprior: -5.3626e+01
Epoch 2/10
10/10 - 1s - loss: 217.5376 - loglik: -2.0626e+02 - logprior: -1.1276e+01
Epoch 3/10
10/10 - 1s - loss: 207.7770 - loglik: -2.0523e+02 - logprior: -2.5504e+00
Epoch 4/10
10/10 - 1s - loss: 201.8346 - loglik: -2.0282e+02 - logprior: 0.9897
Epoch 5/10
10/10 - 1s - loss: 198.8742 - loglik: -2.0167e+02 - logprior: 2.7909
Epoch 6/10
10/10 - 1s - loss: 197.0455 - loglik: -2.0080e+02 - logprior: 3.7552
Epoch 7/10
10/10 - 1s - loss: 196.0254 - loglik: -2.0047e+02 - logprior: 4.4442
Epoch 8/10
10/10 - 1s - loss: 195.0643 - loglik: -2.0020e+02 - logprior: 5.1354
Epoch 9/10
10/10 - 1s - loss: 195.5565 - loglik: -2.0126e+02 - logprior: 5.7069
Fitted a model with MAP estimate = -194.6565
Time for alignment: 46.3969
Computed alignments with likelihoods: ['-196.3046', '-194.5023', '-194.4886', '-194.3077', '-194.6565']
Best model has likelihood: -194.3077  (prior= 6.4212 )
time for generating output: 0.1382
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.9289176090468497
Training of 5 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f022310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1a48280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f9d111c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.6046 - loglik: -1.9342e+02 - logprior: -2.1887e+00
Epoch 2/10
22/22 - 1s - loss: 163.0172 - loglik: -1.6170e+02 - logprior: -1.3213e+00
Epoch 3/10
22/22 - 1s - loss: 155.9984 - loglik: -1.5459e+02 - logprior: -1.4099e+00
Epoch 4/10
22/22 - 1s - loss: 154.6990 - loglik: -1.5339e+02 - logprior: -1.3102e+00
Epoch 5/10
22/22 - 1s - loss: 154.2480 - loglik: -1.5293e+02 - logprior: -1.3214e+00
Epoch 6/10
22/22 - 1s - loss: 153.9288 - loglik: -1.5264e+02 - logprior: -1.2914e+00
Epoch 7/10
22/22 - 1s - loss: 154.0948 - loglik: -1.5281e+02 - logprior: -1.2869e+00
Fitted a model with MAP estimate = -152.8388
expansions: [(8, 1), (9, 2), (12, 1), (14, 2), (17, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 2), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 156.6400 - loglik: -1.5379e+02 - logprior: -2.8469e+00
Epoch 2/2
22/22 - 1s - loss: 148.2281 - loglik: -1.4680e+02 - logprior: -1.4305e+00
Fitted a model with MAP estimate = -145.9631
expansions: [(0, 2)]
discards: [ 0  9 17 22 57 66]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 147.8786 - loglik: -1.4577e+02 - logprior: -2.1119e+00
Epoch 2/2
22/22 - 1s - loss: 145.0029 - loglik: -1.4399e+02 - logprior: -1.0140e+00
Fitted a model with MAP estimate = -145.1653
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.4495 - loglik: -1.4431e+02 - logprior: -1.1405e+00
Epoch 2/10
32/32 - 2s - loss: 143.1388 - loglik: -1.4233e+02 - logprior: -8.0476e-01
Epoch 3/10
32/32 - 2s - loss: 142.8646 - loglik: -1.4207e+02 - logprior: -7.8972e-01
Epoch 4/10
32/32 - 2s - loss: 142.9061 - loglik: -1.4213e+02 - logprior: -7.7945e-01
Fitted a model with MAP estimate = -142.3476
Time for alignment: 53.8191
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 195.3008 - loglik: -1.9312e+02 - logprior: -2.1837e+00
Epoch 2/10
22/22 - 1s - loss: 163.3311 - loglik: -1.6202e+02 - logprior: -1.3072e+00
Epoch 3/10
22/22 - 1s - loss: 156.0748 - loglik: -1.5467e+02 - logprior: -1.4010e+00
Epoch 4/10
22/22 - 1s - loss: 154.8024 - loglik: -1.5350e+02 - logprior: -1.3010e+00
Epoch 5/10
22/22 - 1s - loss: 154.1922 - loglik: -1.5288e+02 - logprior: -1.3120e+00
Epoch 6/10
22/22 - 1s - loss: 153.9965 - loglik: -1.5271e+02 - logprior: -1.2884e+00
Epoch 7/10
22/22 - 1s - loss: 153.7679 - loglik: -1.5249e+02 - logprior: -1.2762e+00
Epoch 8/10
22/22 - 1s - loss: 153.5883 - loglik: -1.5231e+02 - logprior: -1.2736e+00
Epoch 9/10
22/22 - 1s - loss: 153.6027 - loglik: -1.5233e+02 - logprior: -1.2690e+00
Fitted a model with MAP estimate = -152.8287
expansions: [(8, 1), (9, 2), (11, 1), (14, 2), (20, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 2), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 156.8018 - loglik: -1.5395e+02 - logprior: -2.8515e+00
Epoch 2/2
22/22 - 1s - loss: 148.2618 - loglik: -1.4681e+02 - logprior: -1.4529e+00
Fitted a model with MAP estimate = -146.1339
expansions: [(0, 2)]
discards: [ 0  9 17 25 57 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 147.8786 - loglik: -1.4577e+02 - logprior: -2.1107e+00
Epoch 2/2
22/22 - 1s - loss: 145.3129 - loglik: -1.4430e+02 - logprior: -1.0134e+00
Fitted a model with MAP estimate = -145.1023
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.5752 - loglik: -1.4443e+02 - logprior: -1.1403e+00
Epoch 2/10
32/32 - 2s - loss: 143.1295 - loglik: -1.4233e+02 - logprior: -8.0019e-01
Epoch 3/10
32/32 - 2s - loss: 143.0154 - loglik: -1.4223e+02 - logprior: -7.8241e-01
Epoch 4/10
32/32 - 2s - loss: 142.4472 - loglik: -1.4167e+02 - logprior: -7.8154e-01
Epoch 5/10
32/32 - 2s - loss: 142.4142 - loglik: -1.4164e+02 - logprior: -7.7803e-01
Epoch 6/10
32/32 - 2s - loss: 142.3009 - loglik: -1.4153e+02 - logprior: -7.7541e-01
Epoch 7/10
32/32 - 2s - loss: 142.1645 - loglik: -1.4140e+02 - logprior: -7.6753e-01
Epoch 8/10
32/32 - 2s - loss: 141.9276 - loglik: -1.4116e+02 - logprior: -7.6848e-01
Epoch 9/10
32/32 - 2s - loss: 142.0793 - loglik: -1.4132e+02 - logprior: -7.6351e-01
Fitted a model with MAP estimate = -141.9839
Time for alignment: 67.2572
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.4706 - loglik: -1.9329e+02 - logprior: -2.1847e+00
Epoch 2/10
22/22 - 1s - loss: 162.8586 - loglik: -1.6155e+02 - logprior: -1.3102e+00
Epoch 3/10
22/22 - 1s - loss: 156.1222 - loglik: -1.5473e+02 - logprior: -1.3938e+00
Epoch 4/10
22/22 - 1s - loss: 154.5712 - loglik: -1.5328e+02 - logprior: -1.2956e+00
Epoch 5/10
22/22 - 1s - loss: 154.2964 - loglik: -1.5299e+02 - logprior: -1.3098e+00
Epoch 6/10
22/22 - 1s - loss: 153.7790 - loglik: -1.5249e+02 - logprior: -1.2848e+00
Epoch 7/10
22/22 - 1s - loss: 153.8731 - loglik: -1.5260e+02 - logprior: -1.2767e+00
Fitted a model with MAP estimate = -152.6890
expansions: [(8, 1), (9, 2), (12, 1), (14, 2), (20, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 2), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 156.7425 - loglik: -1.5390e+02 - logprior: -2.8449e+00
Epoch 2/2
22/22 - 1s - loss: 148.1121 - loglik: -1.4668e+02 - logprior: -1.4274e+00
Fitted a model with MAP estimate = -145.9092
expansions: [(0, 2)]
discards: [ 0  9 17 25 57]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.5851 - loglik: -1.4546e+02 - logprior: -2.1267e+00
Epoch 2/2
22/22 - 1s - loss: 145.4428 - loglik: -1.4441e+02 - logprior: -1.0288e+00
Fitted a model with MAP estimate = -145.1381
expansions: []
discards: [ 0 64]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.3850 - loglik: -1.4425e+02 - logprior: -1.1357e+00
Epoch 2/10
32/32 - 2s - loss: 143.3347 - loglik: -1.4253e+02 - logprior: -8.0156e-01
Epoch 3/10
32/32 - 2s - loss: 142.8009 - loglik: -1.4202e+02 - logprior: -7.8228e-01
Epoch 4/10
32/32 - 2s - loss: 142.4908 - loglik: -1.4171e+02 - logprior: -7.8191e-01
Epoch 5/10
32/32 - 2s - loss: 142.5829 - loglik: -1.4180e+02 - logprior: -7.8045e-01
Fitted a model with MAP estimate = -142.2245
Time for alignment: 53.8768
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 6s - loss: 195.5553 - loglik: -1.9337e+02 - logprior: -2.1844e+00
Epoch 2/10
22/22 - 1s - loss: 163.3747 - loglik: -1.6207e+02 - logprior: -1.3063e+00
Epoch 3/10
22/22 - 1s - loss: 155.8275 - loglik: -1.5442e+02 - logprior: -1.4063e+00
Epoch 4/10
22/22 - 2s - loss: 154.3008 - loglik: -1.5299e+02 - logprior: -1.3102e+00
Epoch 5/10
22/22 - 1s - loss: 154.1175 - loglik: -1.5280e+02 - logprior: -1.3170e+00
Epoch 6/10
22/22 - 1s - loss: 153.9547 - loglik: -1.5267e+02 - logprior: -1.2889e+00
Epoch 7/10
22/22 - 1s - loss: 153.5301 - loglik: -1.5225e+02 - logprior: -1.2781e+00
Epoch 8/10
22/22 - 1s - loss: 153.5470 - loglik: -1.5228e+02 - logprior: -1.2720e+00
Fitted a model with MAP estimate = -152.7264
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 156.1773 - loglik: -1.5334e+02 - logprior: -2.8373e+00
Epoch 2/2
22/22 - 1s - loss: 148.3168 - loglik: -1.4691e+02 - logprior: -1.4103e+00
Fitted a model with MAP estimate = -146.0298
expansions: [(0, 2)]
discards: [ 0  9 17 25 66]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.9847 - loglik: -1.4588e+02 - logprior: -2.1074e+00
Epoch 2/2
22/22 - 1s - loss: 145.2525 - loglik: -1.4424e+02 - logprior: -1.0171e+00
Fitted a model with MAP estimate = -145.1352
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.5635 - loglik: -1.4442e+02 - logprior: -1.1422e+00
Epoch 2/10
32/32 - 2s - loss: 143.1243 - loglik: -1.4232e+02 - logprior: -8.0277e-01
Epoch 3/10
32/32 - 2s - loss: 142.8409 - loglik: -1.4205e+02 - logprior: -7.8674e-01
Epoch 4/10
32/32 - 2s - loss: 142.5112 - loglik: -1.4173e+02 - logprior: -7.8392e-01
Epoch 5/10
32/32 - 2s - loss: 142.4883 - loglik: -1.4171e+02 - logprior: -7.7573e-01
Epoch 6/10
32/32 - 2s - loss: 142.2085 - loglik: -1.4144e+02 - logprior: -7.7251e-01
Epoch 7/10
32/32 - 2s - loss: 142.1399 - loglik: -1.4137e+02 - logprior: -7.7299e-01
Epoch 8/10
32/32 - 2s - loss: 142.0316 - loglik: -1.4127e+02 - logprior: -7.6029e-01
Epoch 9/10
32/32 - 2s - loss: 142.1394 - loglik: -1.4138e+02 - logprior: -7.6436e-01
Fitted a model with MAP estimate = -141.9642
Time for alignment: 65.1215
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.2476 - loglik: -1.9306e+02 - logprior: -2.1848e+00
Epoch 2/10
22/22 - 1s - loss: 162.9115 - loglik: -1.6161e+02 - logprior: -1.2994e+00
Epoch 3/10
22/22 - 1s - loss: 155.4793 - loglik: -1.5409e+02 - logprior: -1.3895e+00
Epoch 4/10
22/22 - 1s - loss: 154.1268 - loglik: -1.5283e+02 - logprior: -1.2921e+00
Epoch 5/10
22/22 - 1s - loss: 153.8495 - loglik: -1.5256e+02 - logprior: -1.2942e+00
Epoch 6/10
22/22 - 1s - loss: 153.5345 - loglik: -1.5226e+02 - logprior: -1.2698e+00
Epoch 7/10
22/22 - 1s - loss: 153.3444 - loglik: -1.5208e+02 - logprior: -1.2607e+00
Epoch 8/10
22/22 - 1s - loss: 153.4063 - loglik: -1.5215e+02 - logprior: -1.2539e+00
Fitted a model with MAP estimate = -152.5819
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (21, 1), (22, 1), (28, 1), (40, 1), (41, 1), (45, 2), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 156.7375 - loglik: -1.5389e+02 - logprior: -2.8448e+00
Epoch 2/2
22/22 - 1s - loss: 148.1313 - loglik: -1.4669e+02 - logprior: -1.4407e+00
Fitted a model with MAP estimate = -145.9517
expansions: [(0, 2)]
discards: [ 0  9 17 25 57 67]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 148.0466 - loglik: -1.4594e+02 - logprior: -2.1102e+00
Epoch 2/2
22/22 - 1s - loss: 145.1972 - loglik: -1.4418e+02 - logprior: -1.0161e+00
Fitted a model with MAP estimate = -145.1170
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.4749 - loglik: -1.4433e+02 - logprior: -1.1404e+00
Epoch 2/10
32/32 - 2s - loss: 143.2524 - loglik: -1.4245e+02 - logprior: -8.0248e-01
Epoch 3/10
32/32 - 2s - loss: 142.7956 - loglik: -1.4202e+02 - logprior: -7.7818e-01
Epoch 4/10
32/32 - 2s - loss: 142.6793 - loglik: -1.4190e+02 - logprior: -7.7906e-01
Epoch 5/10
32/32 - 2s - loss: 142.4074 - loglik: -1.4164e+02 - logprior: -7.7227e-01
Epoch 6/10
32/32 - 2s - loss: 142.2876 - loglik: -1.4151e+02 - logprior: -7.7344e-01
Epoch 7/10
32/32 - 2s - loss: 141.9839 - loglik: -1.4122e+02 - logprior: -7.6679e-01
Epoch 8/10
32/32 - 2s - loss: 142.1171 - loglik: -1.4136e+02 - logprior: -7.5958e-01
Fitted a model with MAP estimate = -142.0168
Time for alignment: 60.7678
Computed alignments with likelihoods: ['-142.3476', '-141.9839', '-142.2245', '-141.9642', '-142.0168']
Best model has likelihood: -141.9642  (prior= -0.7633 )
time for generating output: 0.1290
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.8306355546212599
Training of 5 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f69a1f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb494063d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49406a60>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 462.5834 - loglik: -4.4966e+02 - logprior: -1.2925e+01
Epoch 2/10
17/17 - 4s - loss: 323.9800 - loglik: -3.2262e+02 - logprior: -1.3570e+00
Epoch 3/10
17/17 - 4s - loss: 271.6701 - loglik: -2.7079e+02 - logprior: -8.7774e-01
Epoch 4/10
17/17 - 4s - loss: 259.4437 - loglik: -2.5884e+02 - logprior: -6.0103e-01
Epoch 5/10
17/17 - 3s - loss: 259.1817 - loglik: -2.5870e+02 - logprior: -4.7965e-01
Epoch 6/10
17/17 - 4s - loss: 254.7734 - loglik: -2.5436e+02 - logprior: -4.1682e-01
Epoch 7/10
17/17 - 4s - loss: 255.0587 - loglik: -2.5463e+02 - logprior: -4.2821e-01
Fitted a model with MAP estimate = -254.3591
expansions: [(0, 33), (8, 1), (22, 1), (25, 1), (30, 1), (32, 1), (33, 1), (50, 1), (81, 1), (83, 1), (93, 1), (117, 1), (138, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 207 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 267.2324 - loglik: -2.5081e+02 - logprior: -1.6424e+01
Epoch 2/2
17/17 - 5s - loss: 222.8682 - loglik: -2.2110e+02 - logprior: -1.7720e+00
Fitted a model with MAP estimate = -214.7402
expansions: [(0, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 256.6369 - loglik: -2.4320e+02 - logprior: -1.3439e+01
Epoch 2/2
17/17 - 5s - loss: 230.8206 - loglik: -2.3043e+02 - logprior: -3.8892e-01
Fitted a model with MAP estimate = -224.8126
expansions: [(0, 29)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 252.5433 - loglik: -2.4069e+02 - logprior: -1.1852e+01
Epoch 2/10
17/17 - 5s - loss: 223.9031 - loglik: -2.2388e+02 - logprior: -1.8990e-02
Epoch 3/10
17/17 - 5s - loss: 212.6833 - loglik: -2.1390e+02 - logprior: 1.2156
Epoch 4/10
17/17 - 5s - loss: 210.8907 - loglik: -2.1259e+02 - logprior: 1.7008
Epoch 5/10
17/17 - 5s - loss: 207.9199 - loglik: -2.0984e+02 - logprior: 1.9162
Epoch 6/10
17/17 - 5s - loss: 203.2863 - loglik: -2.0539e+02 - logprior: 2.1082
Epoch 7/10
17/17 - 5s - loss: 206.0295 - loglik: -2.0829e+02 - logprior: 2.2651
Fitted a model with MAP estimate = -205.0727
Time for alignment: 107.0552
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 463.4481 - loglik: -4.5053e+02 - logprior: -1.2916e+01
Epoch 2/10
17/17 - 4s - loss: 322.9346 - loglik: -3.2160e+02 - logprior: -1.3356e+00
Epoch 3/10
17/17 - 4s - loss: 269.5306 - loglik: -2.6863e+02 - logprior: -9.0419e-01
Epoch 4/10
17/17 - 4s - loss: 261.4180 - loglik: -2.6075e+02 - logprior: -6.7082e-01
Epoch 5/10
17/17 - 4s - loss: 255.7749 - loglik: -2.5527e+02 - logprior: -5.0474e-01
Epoch 6/10
17/17 - 4s - loss: 256.3767 - loglik: -2.5593e+02 - logprior: -4.4779e-01
Fitted a model with MAP estimate = -254.4904
expansions: [(0, 35), (2, 1), (8, 1), (25, 1), (32, 1), (34, 1), (51, 1), (58, 1), (81, 1), (82, 1), (89, 1), (117, 1), (137, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 209 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 265.4496 - loglik: -2.4935e+02 - logprior: -1.6099e+01
Epoch 2/2
17/17 - 5s - loss: 220.5073 - loglik: -2.1879e+02 - logprior: -1.7150e+00
Fitted a model with MAP estimate = -210.9792
expansions: [(0, 18)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 256.1919 - loglik: -2.4285e+02 - logprior: -1.3345e+01
Epoch 2/2
17/17 - 5s - loss: 228.7743 - loglik: -2.2838e+02 - logprior: -3.9290e-01
Fitted a model with MAP estimate = -224.9457
expansions: [(0, 30)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 251.6928 - loglik: -2.3987e+02 - logprior: -1.1822e+01
Epoch 2/10
17/17 - 5s - loss: 221.6840 - loglik: -2.2161e+02 - logprior: -7.4604e-02
Epoch 3/10
17/17 - 5s - loss: 210.8788 - loglik: -2.1200e+02 - logprior: 1.1206
Epoch 4/10
17/17 - 5s - loss: 208.7402 - loglik: -2.1036e+02 - logprior: 1.6181
Epoch 5/10
17/17 - 5s - loss: 203.6112 - loglik: -2.0546e+02 - logprior: 1.8456
Epoch 6/10
17/17 - 5s - loss: 205.9872 - loglik: -2.0802e+02 - logprior: 2.0326
Fitted a model with MAP estimate = -204.1492
Time for alignment: 97.9190
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 461.7947 - loglik: -4.4885e+02 - logprior: -1.2949e+01
Epoch 2/10
17/17 - 4s - loss: 312.6330 - loglik: -3.1100e+02 - logprior: -1.6335e+00
Epoch 3/10
17/17 - 4s - loss: 263.1569 - loglik: -2.6156e+02 - logprior: -1.5932e+00
Epoch 4/10
17/17 - 4s - loss: 250.9144 - loglik: -2.4975e+02 - logprior: -1.1644e+00
Epoch 5/10
17/17 - 4s - loss: 250.8760 - loglik: -2.4971e+02 - logprior: -1.1643e+00
Epoch 6/10
17/17 - 4s - loss: 244.8638 - loglik: -2.4367e+02 - logprior: -1.1946e+00
Epoch 7/10
17/17 - 4s - loss: 245.7345 - loglik: -2.4449e+02 - logprior: -1.2414e+00
Fitted a model with MAP estimate = -245.5700
expansions: [(50, 1), (57, 1), (80, 1), (89, 1), (92, 1), (136, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 6s - loss: 273.1317 - loglik: -2.5555e+02 - logprior: -1.7579e+01
Epoch 2/2
17/17 - 4s - loss: 252.6197 - loglik: -2.4635e+02 - logprior: -6.2707e+00
Fitted a model with MAP estimate = -249.0593
expansions: [(0, 19)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 261.1601 - loglik: -2.4762e+02 - logprior: -1.3544e+01
Epoch 2/2
17/17 - 4s - loss: 243.3687 - loglik: -2.4104e+02 - logprior: -2.3270e+00
Fitted a model with MAP estimate = -239.7988
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 260.8461 - loglik: -2.4519e+02 - logprior: -1.5659e+01
Epoch 2/10
17/17 - 4s - loss: 245.9282 - loglik: -2.4432e+02 - logprior: -1.6094e+00
Epoch 3/10
17/17 - 4s - loss: 239.1416 - loglik: -2.3940e+02 - logprior: 0.2634
Epoch 4/10
17/17 - 4s - loss: 239.1633 - loglik: -2.3972e+02 - logprior: 0.5543
Fitted a model with MAP estimate = -236.7035
Time for alignment: 83.8286
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 463.2588 - loglik: -4.5034e+02 - logprior: -1.2917e+01
Epoch 2/10
17/17 - 4s - loss: 320.4208 - loglik: -3.1914e+02 - logprior: -1.2760e+00
Epoch 3/10
17/17 - 4s - loss: 269.7644 - loglik: -2.6896e+02 - logprior: -8.0042e-01
Epoch 4/10
17/17 - 4s - loss: 257.1214 - loglik: -2.5663e+02 - logprior: -4.9036e-01
Epoch 5/10
17/17 - 4s - loss: 253.5246 - loglik: -2.5312e+02 - logprior: -4.0249e-01
Epoch 6/10
17/17 - 4s - loss: 252.8207 - loglik: -2.5247e+02 - logprior: -3.5521e-01
Epoch 7/10
17/17 - 4s - loss: 252.1010 - loglik: -2.5172e+02 - logprior: -3.8004e-01
Epoch 8/10
17/17 - 4s - loss: 248.8200 - loglik: -2.4846e+02 - logprior: -3.6483e-01
Epoch 9/10
17/17 - 4s - loss: 251.8568 - loglik: -2.5149e+02 - logprior: -3.7062e-01
Fitted a model with MAP estimate = -250.3584
expansions: [(0, 32), (2, 1), (8, 1), (25, 2), (26, 1), (33, 1), (50, 1), (81, 1), (83, 1), (93, 1), (128, 1), (137, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 206 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 263.2812 - loglik: -2.4665e+02 - logprior: -1.6634e+01
Epoch 2/2
17/17 - 5s - loss: 222.4239 - loglik: -2.2059e+02 - logprior: -1.8297e+00
Fitted a model with MAP estimate = -211.5126
expansions: [(0, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 254.8998 - loglik: -2.4142e+02 - logprior: -1.3481e+01
Epoch 2/2
17/17 - 4s - loss: 226.7621 - loglik: -2.2641e+02 - logprior: -3.5399e-01
Fitted a model with MAP estimate = -222.5390
expansions: [(0, 29)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 250.9509 - loglik: -2.3916e+02 - logprior: -1.1787e+01
Epoch 2/10
17/17 - 5s - loss: 222.2906 - loglik: -2.2234e+02 - logprior: 0.0464
Epoch 3/10
17/17 - 5s - loss: 210.4767 - loglik: -2.1174e+02 - logprior: 1.2652
Epoch 4/10
17/17 - 5s - loss: 207.4758 - loglik: -2.0925e+02 - logprior: 1.7716
Epoch 5/10
17/17 - 5s - loss: 204.8292 - loglik: -2.0685e+02 - logprior: 2.0168
Epoch 6/10
17/17 - 5s - loss: 204.0819 - loglik: -2.0627e+02 - logprior: 2.1907
Epoch 7/10
17/17 - 5s - loss: 203.1640 - loglik: -2.0553e+02 - logprior: 2.3639
Epoch 8/10
17/17 - 5s - loss: 201.8762 - loglik: -2.0438e+02 - logprior: 2.5073
Epoch 9/10
17/17 - 5s - loss: 202.4952 - loglik: -2.0514e+02 - logprior: 2.6497
Fitted a model with MAP estimate = -201.7716
Time for alignment: 121.9775
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 462.6205 - loglik: -4.4971e+02 - logprior: -1.2913e+01
Epoch 2/10
17/17 - 4s - loss: 324.8713 - loglik: -3.2354e+02 - logprior: -1.3287e+00
Epoch 3/10
17/17 - 4s - loss: 273.3753 - loglik: -2.7266e+02 - logprior: -7.1490e-01
Epoch 4/10
17/17 - 4s - loss: 261.5960 - loglik: -2.6131e+02 - logprior: -2.8740e-01
Epoch 5/10
17/17 - 4s - loss: 258.5335 - loglik: -2.5832e+02 - logprior: -2.1440e-01
Epoch 6/10
17/17 - 4s - loss: 258.4418 - loglik: -2.5829e+02 - logprior: -1.5232e-01
Epoch 7/10
17/17 - 4s - loss: 255.1992 - loglik: -2.5502e+02 - logprior: -1.8402e-01
Epoch 8/10
17/17 - 4s - loss: 255.2941 - loglik: -2.5512e+02 - logprior: -1.7748e-01
Fitted a model with MAP estimate = -255.1653
expansions: [(0, 33), (25, 2), (29, 1), (40, 1), (58, 1), (81, 1), (83, 1), (93, 1), (123, 1), (138, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 270.6731 - loglik: -2.5402e+02 - logprior: -1.6655e+01
Epoch 2/2
17/17 - 5s - loss: 226.0876 - loglik: -2.2414e+02 - logprior: -1.9479e+00
Fitted a model with MAP estimate = -216.5629
expansions: [(0, 18), (34, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 257.1900 - loglik: -2.4369e+02 - logprior: -1.3495e+01
Epoch 2/2
17/17 - 5s - loss: 228.0312 - loglik: -2.2764e+02 - logprior: -3.8822e-01
Fitted a model with MAP estimate = -224.3038
expansions: [(0, 30)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 251.5699 - loglik: -2.3974e+02 - logprior: -1.1833e+01
Epoch 2/10
17/17 - 5s - loss: 219.3135 - loglik: -2.1930e+02 - logprior: -1.1060e-02
Epoch 3/10
17/17 - 5s - loss: 212.5980 - loglik: -2.1375e+02 - logprior: 1.1482
Epoch 4/10
17/17 - 5s - loss: 206.1829 - loglik: -2.0783e+02 - logprior: 1.6500
Epoch 5/10
17/17 - 5s - loss: 204.7194 - loglik: -2.0660e+02 - logprior: 1.8767
Epoch 6/10
17/17 - 5s - loss: 202.9350 - loglik: -2.0499e+02 - logprior: 2.0543
Epoch 7/10
17/17 - 5s - loss: 205.3615 - loglik: -2.0759e+02 - logprior: 2.2284
Fitted a model with MAP estimate = -202.7696
Time for alignment: 108.7119
Computed alignments with likelihoods: ['-205.0727', '-204.1492', '-236.7035', '-201.7716', '-202.7696']
Best model has likelihood: -201.7716  (prior= 2.7608 )
time for generating output: 0.3460
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.39844896248166
Training of 5 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac87f9b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebe9d0a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9d95fca0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 246.3147 - loglik: -2.2605e+02 - logprior: -2.0264e+01
Epoch 2/10
10/10 - 1s - loss: 217.2975 - loglik: -2.1186e+02 - logprior: -5.4344e+00
Epoch 3/10
10/10 - 1s - loss: 199.7734 - loglik: -1.9685e+02 - logprior: -2.9214e+00
Epoch 4/10
10/10 - 1s - loss: 191.2066 - loglik: -1.8891e+02 - logprior: -2.2926e+00
Epoch 5/10
10/10 - 1s - loss: 187.4111 - loglik: -1.8542e+02 - logprior: -1.9947e+00
Epoch 6/10
10/10 - 1s - loss: 185.8592 - loglik: -1.8421e+02 - logprior: -1.6534e+00
Epoch 7/10
10/10 - 1s - loss: 185.0886 - loglik: -1.8366e+02 - logprior: -1.4271e+00
Epoch 8/10
10/10 - 1s - loss: 183.8728 - loglik: -1.8251e+02 - logprior: -1.3651e+00
Epoch 9/10
10/10 - 1s - loss: 184.0863 - loglik: -1.8276e+02 - logprior: -1.3232e+00
Fitted a model with MAP estimate = -183.6592
expansions: [(0, 2), (7, 2), (8, 2), (22, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.2242 - loglik: -1.8292e+02 - logprior: -2.6300e+01
Epoch 2/2
10/10 - 1s - loss: 187.2405 - loglik: -1.7914e+02 - logprior: -8.1038e+00
Fitted a model with MAP estimate = -183.2852
expansions: []
discards: [ 0 10 12 53 55 63]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.8247 - loglik: -1.8054e+02 - logprior: -2.3290e+01
Epoch 2/2
10/10 - 1s - loss: 188.4631 - loglik: -1.7925e+02 - logprior: -9.2098e+00
Fitted a model with MAP estimate = -185.8576
expansions: [(0, 2), (51, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.0539 - loglik: -1.7878e+02 - logprior: -2.0276e+01
Epoch 2/10
10/10 - 1s - loss: 183.3202 - loglik: -1.7789e+02 - logprior: -5.4316e+00
Epoch 3/10
10/10 - 1s - loss: 180.2492 - loglik: -1.7785e+02 - logprior: -2.4020e+00
Epoch 4/10
10/10 - 1s - loss: 178.6163 - loglik: -1.7713e+02 - logprior: -1.4901e+00
Epoch 5/10
10/10 - 1s - loss: 177.8842 - loglik: -1.7672e+02 - logprior: -1.1618e+00
Epoch 6/10
10/10 - 1s - loss: 177.6093 - loglik: -1.7682e+02 - logprior: -7.8831e-01
Epoch 7/10
10/10 - 1s - loss: 177.1166 - loglik: -1.7661e+02 - logprior: -5.0747e-01
Epoch 8/10
10/10 - 1s - loss: 176.6780 - loglik: -1.7629e+02 - logprior: -3.9136e-01
Epoch 9/10
10/10 - 1s - loss: 176.6871 - loglik: -1.7635e+02 - logprior: -3.3810e-01
Fitted a model with MAP estimate = -176.4699
Time for alignment: 37.6707
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.3805 - loglik: -2.2612e+02 - logprior: -2.0265e+01
Epoch 2/10
10/10 - 1s - loss: 216.8944 - loglik: -2.1145e+02 - logprior: -5.4404e+00
Epoch 3/10
10/10 - 1s - loss: 201.3725 - loglik: -1.9843e+02 - logprior: -2.9392e+00
Epoch 4/10
10/10 - 1s - loss: 192.3547 - loglik: -1.9004e+02 - logprior: -2.3171e+00
Epoch 5/10
10/10 - 1s - loss: 188.7350 - loglik: -1.8668e+02 - logprior: -2.0583e+00
Epoch 6/10
10/10 - 1s - loss: 186.6879 - loglik: -1.8502e+02 - logprior: -1.6702e+00
Epoch 7/10
10/10 - 1s - loss: 185.7532 - loglik: -1.8433e+02 - logprior: -1.4183e+00
Epoch 8/10
10/10 - 1s - loss: 185.3279 - loglik: -1.8398e+02 - logprior: -1.3455e+00
Epoch 9/10
10/10 - 1s - loss: 184.7061 - loglik: -1.8341e+02 - logprior: -1.2951e+00
Epoch 10/10
10/10 - 1s - loss: 184.5049 - loglik: -1.8326e+02 - logprior: -1.2486e+00
Fitted a model with MAP estimate = -184.4558
expansions: [(0, 2), (8, 1), (23, 4), (41, 2), (42, 2), (43, 1), (44, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.1292 - loglik: -1.8367e+02 - logprior: -2.6463e+01
Epoch 2/2
10/10 - 1s - loss: 188.7673 - loglik: -1.8060e+02 - logprior: -8.1650e+00
Fitted a model with MAP estimate = -184.4432
expansions: []
discards: [ 0 26 27 51 56]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.0584 - loglik: -1.8076e+02 - logprior: -2.3296e+01
Epoch 2/2
10/10 - 1s - loss: 189.5354 - loglik: -1.8037e+02 - logprior: -9.1608e+00
Fitted a model with MAP estimate = -186.4996
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.9378 - loglik: -1.7966e+02 - logprior: -2.0279e+01
Epoch 2/10
10/10 - 1s - loss: 184.1848 - loglik: -1.7876e+02 - logprior: -5.4221e+00
Epoch 3/10
10/10 - 1s - loss: 181.3444 - loglik: -1.7895e+02 - logprior: -2.3904e+00
Epoch 4/10
10/10 - 1s - loss: 179.5759 - loglik: -1.7810e+02 - logprior: -1.4774e+00
Epoch 5/10
10/10 - 1s - loss: 179.3203 - loglik: -1.7816e+02 - logprior: -1.1613e+00
Epoch 6/10
10/10 - 1s - loss: 178.6676 - loglik: -1.7784e+02 - logprior: -8.2324e-01
Epoch 7/10
10/10 - 1s - loss: 178.5462 - loglik: -1.7803e+02 - logprior: -5.2115e-01
Epoch 8/10
10/10 - 1s - loss: 177.6618 - loglik: -1.7724e+02 - logprior: -4.2581e-01
Epoch 9/10
10/10 - 1s - loss: 177.7675 - loglik: -1.7742e+02 - logprior: -3.4782e-01
Fitted a model with MAP estimate = -177.6404
Time for alignment: 36.0269
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 246.3161 - loglik: -2.2605e+02 - logprior: -2.0265e+01
Epoch 2/10
10/10 - 1s - loss: 217.0953 - loglik: -2.1166e+02 - logprior: -5.4386e+00
Epoch 3/10
10/10 - 1s - loss: 199.2552 - loglik: -1.9629e+02 - logprior: -2.9682e+00
Epoch 4/10
10/10 - 1s - loss: 190.5741 - loglik: -1.8821e+02 - logprior: -2.3602e+00
Epoch 5/10
10/10 - 1s - loss: 187.4239 - loglik: -1.8539e+02 - logprior: -2.0384e+00
Epoch 6/10
10/10 - 1s - loss: 185.9533 - loglik: -1.8430e+02 - logprior: -1.6567e+00
Epoch 7/10
10/10 - 1s - loss: 184.9740 - loglik: -1.8356e+02 - logprior: -1.4156e+00
Epoch 8/10
10/10 - 1s - loss: 184.6730 - loglik: -1.8331e+02 - logprior: -1.3630e+00
Epoch 9/10
10/10 - 1s - loss: 184.1105 - loglik: -1.8278e+02 - logprior: -1.3302e+00
Epoch 10/10
10/10 - 1s - loss: 183.7891 - loglik: -1.8248e+02 - logprior: -1.3105e+00
Fitted a model with MAP estimate = -183.6216
expansions: [(0, 2), (8, 2), (10, 1), (23, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.1248 - loglik: -1.8266e+02 - logprior: -2.6466e+01
Epoch 2/2
10/10 - 1s - loss: 187.2142 - loglik: -1.7908e+02 - logprior: -8.1380e+00
Fitted a model with MAP estimate = -183.1609
expansions: []
discards: [ 0 10 52 54 62]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.3385 - loglik: -1.8001e+02 - logprior: -2.3324e+01
Epoch 2/2
10/10 - 1s - loss: 188.7108 - loglik: -1.7950e+02 - logprior: -9.2115e+00
Fitted a model with MAP estimate = -185.8333
expansions: [(0, 2), (51, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 199.1520 - loglik: -1.7884e+02 - logprior: -2.0313e+01
Epoch 2/10
10/10 - 1s - loss: 183.2903 - loglik: -1.7786e+02 - logprior: -5.4347e+00
Epoch 3/10
10/10 - 1s - loss: 180.1841 - loglik: -1.7780e+02 - logprior: -2.3862e+00
Epoch 4/10
10/10 - 1s - loss: 178.6274 - loglik: -1.7716e+02 - logprior: -1.4683e+00
Epoch 5/10
10/10 - 1s - loss: 177.9246 - loglik: -1.7678e+02 - logprior: -1.1423e+00
Epoch 6/10
10/10 - 1s - loss: 177.5405 - loglik: -1.7676e+02 - logprior: -7.8311e-01
Epoch 7/10
10/10 - 1s - loss: 177.1344 - loglik: -1.7664e+02 - logprior: -4.9212e-01
Epoch 8/10
10/10 - 1s - loss: 176.8114 - loglik: -1.7642e+02 - logprior: -3.8729e-01
Epoch 9/10
10/10 - 1s - loss: 176.5501 - loglik: -1.7623e+02 - logprior: -3.2208e-01
Epoch 10/10
10/10 - 1s - loss: 176.6346 - loglik: -1.7636e+02 - logprior: -2.7713e-01
Fitted a model with MAP estimate = -176.3558
Time for alignment: 37.6454
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.3844 - loglik: -2.2612e+02 - logprior: -2.0263e+01
Epoch 2/10
10/10 - 1s - loss: 216.3232 - loglik: -2.1088e+02 - logprior: -5.4401e+00
Epoch 3/10
10/10 - 1s - loss: 198.3648 - loglik: -1.9543e+02 - logprior: -2.9322e+00
Epoch 4/10
10/10 - 1s - loss: 190.4540 - loglik: -1.8811e+02 - logprior: -2.3470e+00
Epoch 5/10
10/10 - 1s - loss: 187.1982 - loglik: -1.8517e+02 - logprior: -2.0297e+00
Epoch 6/10
10/10 - 1s - loss: 186.1563 - loglik: -1.8453e+02 - logprior: -1.6295e+00
Epoch 7/10
10/10 - 1s - loss: 185.1763 - loglik: -1.8380e+02 - logprior: -1.3772e+00
Epoch 8/10
10/10 - 1s - loss: 184.7341 - loglik: -1.8341e+02 - logprior: -1.3262e+00
Epoch 9/10
10/10 - 1s - loss: 183.9896 - loglik: -1.8270e+02 - logprior: -1.2852e+00
Epoch 10/10
10/10 - 1s - loss: 183.7241 - loglik: -1.8245e+02 - logprior: -1.2732e+00
Fitted a model with MAP estimate = -183.6759
expansions: [(0, 2), (8, 1), (10, 1), (39, 1), (41, 2), (42, 2), (43, 1), (44, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 209.3306 - loglik: -1.8279e+02 - logprior: -2.6540e+01
Epoch 2/2
10/10 - 1s - loss: 187.5370 - loglik: -1.7946e+02 - logprior: -8.0812e+00
Fitted a model with MAP estimate = -183.4799
expansions: []
discards: [ 0 49 54]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.2629 - loglik: -1.8004e+02 - logprior: -2.3226e+01
Epoch 2/2
10/10 - 1s - loss: 188.3599 - loglik: -1.7925e+02 - logprior: -9.1101e+00
Fitted a model with MAP estimate = -185.6403
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.8997 - loglik: -1.7868e+02 - logprior: -2.0224e+01
Epoch 2/10
10/10 - 1s - loss: 183.5765 - loglik: -1.7821e+02 - logprior: -5.3682e+00
Epoch 3/10
10/10 - 1s - loss: 180.2904 - loglik: -1.7793e+02 - logprior: -2.3558e+00
Epoch 4/10
10/10 - 1s - loss: 178.7941 - loglik: -1.7732e+02 - logprior: -1.4735e+00
Epoch 5/10
10/10 - 1s - loss: 178.0432 - loglik: -1.7691e+02 - logprior: -1.1310e+00
Epoch 6/10
10/10 - 1s - loss: 177.3580 - loglik: -1.7658e+02 - logprior: -7.7339e-01
Epoch 7/10
10/10 - 1s - loss: 177.2537 - loglik: -1.7673e+02 - logprior: -5.1994e-01
Epoch 8/10
10/10 - 1s - loss: 177.0178 - loglik: -1.7656e+02 - logprior: -4.5589e-01
Epoch 9/10
10/10 - 1s - loss: 176.5579 - loglik: -1.7616e+02 - logprior: -3.9468e-01
Epoch 10/10
10/10 - 1s - loss: 176.6864 - loglik: -1.7634e+02 - logprior: -3.4392e-01
Fitted a model with MAP estimate = -176.5036
Time for alignment: 36.0347
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.3660 - loglik: -2.2610e+02 - logprior: -2.0264e+01
Epoch 2/10
10/10 - 1s - loss: 217.1886 - loglik: -2.1175e+02 - logprior: -5.4352e+00
Epoch 3/10
10/10 - 1s - loss: 201.0714 - loglik: -1.9815e+02 - logprior: -2.9232e+00
Epoch 4/10
10/10 - 1s - loss: 192.1214 - loglik: -1.8977e+02 - logprior: -2.3506e+00
Epoch 5/10
10/10 - 1s - loss: 187.7925 - loglik: -1.8570e+02 - logprior: -2.0934e+00
Epoch 6/10
10/10 - 1s - loss: 186.0466 - loglik: -1.8435e+02 - logprior: -1.6970e+00
Epoch 7/10
10/10 - 1s - loss: 185.3866 - loglik: -1.8391e+02 - logprior: -1.4721e+00
Epoch 8/10
10/10 - 1s - loss: 184.4668 - loglik: -1.8304e+02 - logprior: -1.4281e+00
Epoch 9/10
10/10 - 1s - loss: 184.4288 - loglik: -1.8303e+02 - logprior: -1.3959e+00
Epoch 10/10
10/10 - 1s - loss: 184.2035 - loglik: -1.8283e+02 - logprior: -1.3736e+00
Fitted a model with MAP estimate = -183.8745
expansions: [(0, 2), (8, 2), (9, 1), (34, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.4960 - loglik: -1.8300e+02 - logprior: -2.6499e+01
Epoch 2/2
10/10 - 1s - loss: 187.1523 - loglik: -1.7902e+02 - logprior: -8.1304e+00
Fitted a model with MAP estimate = -183.2168
expansions: []
discards: [ 0 10 52 54 62]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 203.3693 - loglik: -1.8005e+02 - logprior: -2.3316e+01
Epoch 2/2
10/10 - 1s - loss: 188.8738 - loglik: -1.7966e+02 - logprior: -9.2136e+00
Fitted a model with MAP estimate = -185.7368
expansions: [(0, 2), (51, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 198.9915 - loglik: -1.7870e+02 - logprior: -2.0294e+01
Epoch 2/10
10/10 - 1s - loss: 183.0674 - loglik: -1.7766e+02 - logprior: -5.4041e+00
Epoch 3/10
10/10 - 1s - loss: 180.1873 - loglik: -1.7781e+02 - logprior: -2.3823e+00
Epoch 4/10
10/10 - 1s - loss: 178.4566 - loglik: -1.7700e+02 - logprior: -1.4574e+00
Epoch 5/10
10/10 - 1s - loss: 178.1264 - loglik: -1.7699e+02 - logprior: -1.1345e+00
Epoch 6/10
10/10 - 1s - loss: 177.3750 - loglik: -1.7660e+02 - logprior: -7.7311e-01
Epoch 7/10
10/10 - 1s - loss: 177.2308 - loglik: -1.7676e+02 - logprior: -4.7487e-01
Epoch 8/10
10/10 - 1s - loss: 176.7073 - loglik: -1.7633e+02 - logprior: -3.8082e-01
Epoch 9/10
10/10 - 1s - loss: 176.4952 - loglik: -1.7617e+02 - logprior: -3.2416e-01
Epoch 10/10
10/10 - 1s - loss: 176.3117 - loglik: -1.7604e+02 - logprior: -2.7472e-01
Fitted a model with MAP estimate = -176.3132
Time for alignment: 35.3986
Computed alignments with likelihoods: ['-176.4699', '-177.6404', '-176.3558', '-176.5036', '-176.3132']
Best model has likelihood: -176.3132  (prior= -0.2632 )
time for generating output: 0.1333
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.6666666666666666
Training of 5 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea137c7700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaf4df6f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feada3eab50>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 199.3119 - loglik: -1.8782e+02 - logprior: -1.1491e+01
Epoch 2/10
11/11 - 1s - loss: 157.4783 - loglik: -1.5425e+02 - logprior: -3.2318e+00
Epoch 3/10
11/11 - 1s - loss: 125.4219 - loglik: -1.2304e+02 - logprior: -2.3774e+00
Epoch 4/10
11/11 - 1s - loss: 110.4998 - loglik: -1.0847e+02 - logprior: -2.0316e+00
Epoch 5/10
11/11 - 1s - loss: 105.9714 - loglik: -1.0419e+02 - logprior: -1.7852e+00
Epoch 6/10
11/11 - 1s - loss: 104.0870 - loglik: -1.0230e+02 - logprior: -1.7905e+00
Epoch 7/10
11/11 - 1s - loss: 102.9641 - loglik: -1.0124e+02 - logprior: -1.7232e+00
Epoch 8/10
11/11 - 1s - loss: 102.1477 - loglik: -1.0044e+02 - logprior: -1.7119e+00
Epoch 9/10
11/11 - 1s - loss: 101.3851 - loglik: -9.9670e+01 - logprior: -1.7152e+00
Epoch 10/10
11/11 - 1s - loss: 101.0415 - loglik: -9.9313e+01 - logprior: -1.7283e+00
Fitted a model with MAP estimate = -100.9887
expansions: [(0, 3), (15, 2), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 113.0317 - loglik: -9.9316e+01 - logprior: -1.3716e+01
Epoch 2/2
11/11 - 1s - loss: 95.6388 - loglik: -9.1392e+01 - logprior: -4.2472e+00
Fitted a model with MAP estimate = -92.9846
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.6616 - loglik: -9.2531e+01 - logprior: -1.3131e+01
Epoch 2/2
11/11 - 1s - loss: 96.5056 - loglik: -9.0981e+01 - logprior: -5.5250e+00
Fitted a model with MAP estimate = -94.1642
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.4241 - loglik: -9.0624e+01 - logprior: -1.0800e+01
Epoch 2/10
11/11 - 1s - loss: 93.8424 - loglik: -9.0860e+01 - logprior: -2.9822e+00
Epoch 3/10
11/11 - 1s - loss: 91.9596 - loglik: -9.0185e+01 - logprior: -1.7746e+00
Epoch 4/10
11/11 - 1s - loss: 90.5397 - loglik: -8.8960e+01 - logprior: -1.5797e+00
Epoch 5/10
11/11 - 1s - loss: 90.3299 - loglik: -8.8808e+01 - logprior: -1.5215e+00
Epoch 6/10
11/11 - 1s - loss: 88.9073 - loglik: -8.7530e+01 - logprior: -1.3777e+00
Epoch 7/10
11/11 - 1s - loss: 88.9714 - loglik: -8.7693e+01 - logprior: -1.2784e+00
Fitted a model with MAP estimate = -88.7599
Time for alignment: 32.7743
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 199.3800 - loglik: -1.8789e+02 - logprior: -1.1489e+01
Epoch 2/10
11/11 - 1s - loss: 157.2060 - loglik: -1.5395e+02 - logprior: -3.2593e+00
Epoch 3/10
11/11 - 1s - loss: 124.6920 - loglik: -1.2225e+02 - logprior: -2.4393e+00
Epoch 4/10
11/11 - 1s - loss: 109.7564 - loglik: -1.0770e+02 - logprior: -2.0580e+00
Epoch 5/10
11/11 - 1s - loss: 105.5694 - loglik: -1.0374e+02 - logprior: -1.8329e+00
Epoch 6/10
11/11 - 1s - loss: 104.1794 - loglik: -1.0234e+02 - logprior: -1.8372e+00
Epoch 7/10
11/11 - 1s - loss: 102.9919 - loglik: -1.0122e+02 - logprior: -1.7672e+00
Epoch 8/10
11/11 - 1s - loss: 102.3698 - loglik: -1.0061e+02 - logprior: -1.7646e+00
Epoch 9/10
11/11 - 1s - loss: 101.8157 - loglik: -1.0006e+02 - logprior: -1.7569e+00
Epoch 10/10
11/11 - 1s - loss: 101.7562 - loglik: -9.9992e+01 - logprior: -1.7642e+00
Fitted a model with MAP estimate = -101.4450
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 3), (38, 2), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 112.9688 - loglik: -9.9277e+01 - logprior: -1.3691e+01
Epoch 2/2
11/11 - 1s - loss: 95.7550 - loglik: -9.1592e+01 - logprior: -4.1626e+00
Fitted a model with MAP estimate = -92.8084
expansions: []
discards: [ 0 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.6187 - loglik: -9.2509e+01 - logprior: -1.3110e+01
Epoch 2/2
11/11 - 1s - loss: 96.3420 - loglik: -9.0820e+01 - logprior: -5.5218e+00
Fitted a model with MAP estimate = -94.2009
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 101.6506 - loglik: -9.0842e+01 - logprior: -1.0809e+01
Epoch 2/10
11/11 - 1s - loss: 93.5621 - loglik: -9.0598e+01 - logprior: -2.9641e+00
Epoch 3/10
11/11 - 1s - loss: 91.7500 - loglik: -8.9976e+01 - logprior: -1.7741e+00
Epoch 4/10
11/11 - 1s - loss: 90.7753 - loglik: -8.9209e+01 - logprior: -1.5661e+00
Epoch 5/10
11/11 - 1s - loss: 89.9649 - loglik: -8.8455e+01 - logprior: -1.5099e+00
Epoch 6/10
11/11 - 1s - loss: 89.5382 - loglik: -8.8167e+01 - logprior: -1.3712e+00
Epoch 7/10
11/11 - 1s - loss: 88.8468 - loglik: -8.7577e+01 - logprior: -1.2700e+00
Epoch 8/10
11/11 - 1s - loss: 88.6661 - loglik: -8.7403e+01 - logprior: -1.2635e+00
Epoch 9/10
11/11 - 1s - loss: 88.3287 - loglik: -8.7061e+01 - logprior: -1.2679e+00
Epoch 10/10
11/11 - 1s - loss: 88.4690 - loglik: -8.7226e+01 - logprior: -1.2427e+00
Fitted a model with MAP estimate = -88.3506
Time for alignment: 35.0250
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 199.1134 - loglik: -1.8762e+02 - logprior: -1.1490e+01
Epoch 2/10
11/11 - 1s - loss: 158.1349 - loglik: -1.5488e+02 - logprior: -3.2557e+00
Epoch 3/10
11/11 - 1s - loss: 127.5477 - loglik: -1.2511e+02 - logprior: -2.4425e+00
Epoch 4/10
11/11 - 1s - loss: 112.1319 - loglik: -1.1006e+02 - logprior: -2.0683e+00
Epoch 5/10
11/11 - 1s - loss: 106.3799 - loglik: -1.0457e+02 - logprior: -1.8091e+00
Epoch 6/10
11/11 - 1s - loss: 104.3967 - loglik: -1.0257e+02 - logprior: -1.8236e+00
Epoch 7/10
11/11 - 1s - loss: 103.0449 - loglik: -1.0129e+02 - logprior: -1.7560e+00
Epoch 8/10
11/11 - 1s - loss: 101.8677 - loglik: -1.0012e+02 - logprior: -1.7517e+00
Epoch 9/10
11/11 - 1s - loss: 101.5447 - loglik: -9.9788e+01 - logprior: -1.7564e+00
Epoch 10/10
11/11 - 1s - loss: 101.2030 - loglik: -9.9440e+01 - logprior: -1.7626e+00
Fitted a model with MAP estimate = -101.0776
expansions: [(0, 3), (15, 2), (26, 1), (27, 1), (28, 3), (29, 1), (30, 1), (31, 1), (34, 1), (37, 1), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 111.5433 - loglik: -9.7867e+01 - logprior: -1.3677e+01
Epoch 2/2
11/11 - 1s - loss: 95.3866 - loglik: -9.1250e+01 - logprior: -4.1370e+00
Fitted a model with MAP estimate = -92.7477
expansions: []
discards: [ 0 36]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 105.7346 - loglik: -9.2623e+01 - logprior: -1.3111e+01
Epoch 2/2
11/11 - 1s - loss: 96.2010 - loglik: -9.0693e+01 - logprior: -5.5084e+00
Fitted a model with MAP estimate = -94.1669
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 101.6227 - loglik: -9.0831e+01 - logprior: -1.0792e+01
Epoch 2/10
11/11 - 1s - loss: 93.7589 - loglik: -9.0799e+01 - logprior: -2.9603e+00
Epoch 3/10
11/11 - 1s - loss: 91.6547 - loglik: -8.9889e+01 - logprior: -1.7660e+00
Epoch 4/10
11/11 - 1s - loss: 90.7112 - loglik: -8.9148e+01 - logprior: -1.5628e+00
Epoch 5/10
11/11 - 1s - loss: 89.8868 - loglik: -8.8373e+01 - logprior: -1.5137e+00
Epoch 6/10
11/11 - 1s - loss: 89.4697 - loglik: -8.8104e+01 - logprior: -1.3653e+00
Epoch 7/10
11/11 - 1s - loss: 89.1543 - loglik: -8.7893e+01 - logprior: -1.2617e+00
Epoch 8/10
11/11 - 1s - loss: 88.4710 - loglik: -8.7213e+01 - logprior: -1.2581e+00
Epoch 9/10
11/11 - 1s - loss: 88.7862 - loglik: -8.7523e+01 - logprior: -1.2635e+00
Fitted a model with MAP estimate = -88.4613
Time for alignment: 32.4577
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 199.0353 - loglik: -1.8754e+02 - logprior: -1.1490e+01
Epoch 2/10
11/11 - 1s - loss: 158.2362 - loglik: -1.5500e+02 - logprior: -3.2397e+00
Epoch 3/10
11/11 - 1s - loss: 126.2826 - loglik: -1.2391e+02 - logprior: -2.3772e+00
Epoch 4/10
11/11 - 1s - loss: 112.5595 - loglik: -1.1054e+02 - logprior: -2.0149e+00
Epoch 5/10
11/11 - 1s - loss: 107.9756 - loglik: -1.0619e+02 - logprior: -1.7879e+00
Epoch 6/10
11/11 - 1s - loss: 105.3906 - loglik: -1.0359e+02 - logprior: -1.8049e+00
Epoch 7/10
11/11 - 1s - loss: 103.7849 - loglik: -1.0204e+02 - logprior: -1.7417e+00
Epoch 8/10
11/11 - 1s - loss: 102.8094 - loglik: -1.0106e+02 - logprior: -1.7529e+00
Epoch 9/10
11/11 - 1s - loss: 102.1176 - loglik: -1.0036e+02 - logprior: -1.7576e+00
Epoch 10/10
11/11 - 1s - loss: 102.3787 - loglik: -1.0062e+02 - logprior: -1.7541e+00
Fitted a model with MAP estimate = -101.8549
expansions: [(0, 3), (15, 2), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 113.7844 - loglik: -1.0007e+02 - logprior: -1.3718e+01
Epoch 2/2
11/11 - 1s - loss: 96.0384 - loglik: -9.1802e+01 - logprior: -4.2367e+00
Fitted a model with MAP estimate = -93.0271
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 105.8724 - loglik: -9.2747e+01 - logprior: -1.3126e+01
Epoch 2/2
11/11 - 1s - loss: 96.2598 - loglik: -9.0730e+01 - logprior: -5.5294e+00
Fitted a model with MAP estimate = -94.2179
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 102.0975 - loglik: -9.1291e+01 - logprior: -1.0807e+01
Epoch 2/10
11/11 - 1s - loss: 93.1489 - loglik: -9.0170e+01 - logprior: -2.9784e+00
Epoch 3/10
11/11 - 1s - loss: 91.8997 - loglik: -9.0122e+01 - logprior: -1.7774e+00
Epoch 4/10
11/11 - 1s - loss: 90.4446 - loglik: -8.8877e+01 - logprior: -1.5673e+00
Epoch 5/10
11/11 - 1s - loss: 90.0650 - loglik: -8.8546e+01 - logprior: -1.5191e+00
Epoch 6/10
11/11 - 1s - loss: 89.2550 - loglik: -8.7883e+01 - logprior: -1.3719e+00
Epoch 7/10
11/11 - 1s - loss: 88.9813 - loglik: -8.7706e+01 - logprior: -1.2752e+00
Epoch 8/10
11/11 - 1s - loss: 88.7042 - loglik: -8.7438e+01 - logprior: -1.2659e+00
Epoch 9/10
11/11 - 1s - loss: 88.6021 - loglik: -8.7331e+01 - logprior: -1.2708e+00
Epoch 10/10
11/11 - 1s - loss: 88.4536 - loglik: -8.7209e+01 - logprior: -1.2445e+00
Fitted a model with MAP estimate = -88.3481
Time for alignment: 32.4443
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 199.2001 - loglik: -1.8771e+02 - logprior: -1.1491e+01
Epoch 2/10
11/11 - 1s - loss: 157.6701 - loglik: -1.5441e+02 - logprior: -3.2610e+00
Epoch 3/10
11/11 - 1s - loss: 125.7036 - loglik: -1.2327e+02 - logprior: -2.4348e+00
Epoch 4/10
11/11 - 1s - loss: 112.5900 - loglik: -1.1057e+02 - logprior: -2.0226e+00
Epoch 5/10
11/11 - 1s - loss: 107.5293 - loglik: -1.0575e+02 - logprior: -1.7790e+00
Epoch 6/10
11/11 - 1s - loss: 106.0500 - loglik: -1.0425e+02 - logprior: -1.7951e+00
Epoch 7/10
11/11 - 1s - loss: 104.2913 - loglik: -1.0255e+02 - logprior: -1.7426e+00
Epoch 8/10
11/11 - 1s - loss: 103.5684 - loglik: -1.0181e+02 - logprior: -1.7535e+00
Epoch 9/10
11/11 - 1s - loss: 102.7846 - loglik: -1.0104e+02 - logprior: -1.7489e+00
Epoch 10/10
11/11 - 1s - loss: 102.4635 - loglik: -1.0070e+02 - logprior: -1.7601e+00
Fitted a model with MAP estimate = -102.2668
expansions: [(0, 3), (15, 2), (26, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 114.0415 - loglik: -1.0034e+02 - logprior: -1.3706e+01
Epoch 2/2
11/11 - 1s - loss: 95.7270 - loglik: -9.1507e+01 - logprior: -4.2204e+00
Fitted a model with MAP estimate = -92.9786
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 105.7944 - loglik: -9.2675e+01 - logprior: -1.3120e+01
Epoch 2/2
11/11 - 1s - loss: 96.1481 - loglik: -9.0634e+01 - logprior: -5.5136e+00
Fitted a model with MAP estimate = -94.1742
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 101.5931 - loglik: -9.0788e+01 - logprior: -1.0805e+01
Epoch 2/10
11/11 - 1s - loss: 93.6215 - loglik: -9.0655e+01 - logprior: -2.9669e+00
Epoch 3/10
11/11 - 1s - loss: 91.8910 - loglik: -9.0112e+01 - logprior: -1.7789e+00
Epoch 4/10
11/11 - 1s - loss: 90.5188 - loglik: -8.8950e+01 - logprior: -1.5689e+00
Epoch 5/10
11/11 - 1s - loss: 90.0089 - loglik: -8.8492e+01 - logprior: -1.5172e+00
Epoch 6/10
11/11 - 1s - loss: 89.1105 - loglik: -8.7735e+01 - logprior: -1.3750e+00
Epoch 7/10
11/11 - 1s - loss: 88.9181 - loglik: -8.7641e+01 - logprior: -1.2766e+00
Epoch 8/10
11/11 - 1s - loss: 88.7523 - loglik: -8.7488e+01 - logprior: -1.2642e+00
Epoch 9/10
11/11 - 1s - loss: 88.8110 - loglik: -8.7540e+01 - logprior: -1.2710e+00
Fitted a model with MAP estimate = -88.4511
Time for alignment: 32.6358
Computed alignments with likelihoods: ['-88.7599', '-88.3506', '-88.4613', '-88.3481', '-88.4511']
Best model has likelihood: -88.3481  (prior= -1.2213 )
time for generating output: 0.1050
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 5 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac00d4880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ad779ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2c4e1670>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 171.6591 - loglik: -1.7097e+02 - logprior: -6.8757e-01
Epoch 2/10
42/42 - 3s - loss: 78.6504 - loglik: -7.7971e+01 - logprior: -6.7896e-01
Epoch 3/10
42/42 - 3s - loss: 74.9607 - loglik: -7.4302e+01 - logprior: -6.5882e-01
Epoch 4/10
42/42 - 3s - loss: 74.5553 - loglik: -7.3910e+01 - logprior: -6.4571e-01
Epoch 5/10
42/42 - 3s - loss: 73.5572 - loglik: -7.2922e+01 - logprior: -6.3573e-01
Epoch 6/10
42/42 - 3s - loss: 73.3528 - loglik: -7.2728e+01 - logprior: -6.2517e-01
Epoch 7/10
42/42 - 3s - loss: 73.3950 - loglik: -7.2776e+01 - logprior: -6.1861e-01
Fitted a model with MAP estimate = -72.8102
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (24, 1), (26, 1), (27, 1), (35, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 47.1167 - loglik: -4.6301e+01 - logprior: -8.1536e-01
Epoch 2/2
42/42 - 4s - loss: 33.3457 - loglik: -3.2744e+01 - logprior: -6.0183e-01
Fitted a model with MAP estimate = -32.8306
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 37.0454 - loglik: -3.5982e+01 - logprior: -1.0638e+00
Epoch 2/2
42/42 - 4s - loss: 33.4172 - loglik: -3.2691e+01 - logprior: -7.2635e-01
Fitted a model with MAP estimate = -33.1969
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 33.4138 - loglik: -3.2836e+01 - logprior: -5.7816e-01
Epoch 2/10
59/59 - 5s - loss: 32.0657 - loglik: -3.1586e+01 - logprior: -4.8000e-01
Epoch 3/10
59/59 - 5s - loss: 32.2267 - loglik: -3.1752e+01 - logprior: -4.7440e-01
Fitted a model with MAP estimate = -31.7486
Time for alignment: 136.8037
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 172.0846 - loglik: -1.7140e+02 - logprior: -6.8719e-01
Epoch 2/10
42/42 - 3s - loss: 79.2901 - loglik: -7.8612e+01 - logprior: -6.7804e-01
Epoch 3/10
42/42 - 3s - loss: 75.6646 - loglik: -7.5006e+01 - logprior: -6.5855e-01
Epoch 4/10
42/42 - 3s - loss: 74.9670 - loglik: -7.4320e+01 - logprior: -6.4677e-01
Epoch 5/10
42/42 - 3s - loss: 74.5359 - loglik: -7.3903e+01 - logprior: -6.3289e-01
Epoch 6/10
42/42 - 3s - loss: 73.9958 - loglik: -7.3370e+01 - logprior: -6.2577e-01
Epoch 7/10
42/42 - 3s - loss: 73.4835 - loglik: -7.2862e+01 - logprior: -6.2196e-01
Epoch 8/10
42/42 - 3s - loss: 73.6984 - loglik: -7.3082e+01 - logprior: -6.1604e-01
Fitted a model with MAP estimate = -73.1674
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (28, 1), (29, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 47.2207 - loglik: -4.6359e+01 - logprior: -8.6210e-01
Epoch 2/2
42/42 - 4s - loss: 33.1818 - loglik: -3.2577e+01 - logprior: -6.0499e-01
Fitted a model with MAP estimate = -32.7978
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 32.9035 - loglik: -3.2449e+01 - logprior: -4.5489e-01
Epoch 2/10
59/59 - 5s - loss: 31.5516 - loglik: -3.1162e+01 - logprior: -3.8996e-01
Epoch 3/10
59/59 - 5s - loss: 31.7533 - loglik: -3.1376e+01 - logprior: -3.7732e-01
Fitted a model with MAP estimate = -31.3444
Time for alignment: 107.0391
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 171.1982 - loglik: -1.7051e+02 - logprior: -6.9247e-01
Epoch 2/10
42/42 - 3s - loss: 78.4131 - loglik: -7.7734e+01 - logprior: -6.7958e-01
Epoch 3/10
42/42 - 3s - loss: 75.3187 - loglik: -7.4660e+01 - logprior: -6.5869e-01
Epoch 4/10
42/42 - 3s - loss: 74.9144 - loglik: -7.4268e+01 - logprior: -6.4603e-01
Epoch 5/10
42/42 - 3s - loss: 73.7514 - loglik: -7.3118e+01 - logprior: -6.3383e-01
Epoch 6/10
42/42 - 3s - loss: 73.7063 - loglik: -7.3082e+01 - logprior: -6.2384e-01
Epoch 7/10
42/42 - 3s - loss: 73.4879 - loglik: -7.2869e+01 - logprior: -6.1846e-01
Epoch 8/10
42/42 - 3s - loss: 73.5034 - loglik: -7.2888e+01 - logprior: -6.1587e-01
Fitted a model with MAP estimate = -72.8231
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (35, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 47.4892 - loglik: -4.6619e+01 - logprior: -8.6977e-01
Epoch 2/2
42/42 - 4s - loss: 33.3229 - loglik: -3.2717e+01 - logprior: -6.0604e-01
Fitted a model with MAP estimate = -32.8037
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 36.2728 - loglik: -3.5237e+01 - logprior: -1.0360e+00
Epoch 2/2
42/42 - 4s - loss: 33.2638 - loglik: -3.2540e+01 - logprior: -7.2433e-01
Fitted a model with MAP estimate = -33.1141
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 33.4222 - loglik: -3.2845e+01 - logprior: -5.7744e-01
Epoch 2/10
59/59 - 5s - loss: 31.9944 - loglik: -3.1512e+01 - logprior: -4.8196e-01
Epoch 3/10
59/59 - 5s - loss: 32.2205 - loglik: -3.1749e+01 - logprior: -4.7189e-01
Fitted a model with MAP estimate = -31.7586
Time for alignment: 138.0362
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 171.1142 - loglik: -1.7043e+02 - logprior: -6.8699e-01
Epoch 2/10
42/42 - 3s - loss: 78.2699 - loglik: -7.7589e+01 - logprior: -6.8087e-01
Epoch 3/10
42/42 - 3s - loss: 75.3289 - loglik: -7.4672e+01 - logprior: -6.5728e-01
Epoch 4/10
42/42 - 3s - loss: 74.2846 - loglik: -7.3638e+01 - logprior: -6.4648e-01
Epoch 5/10
42/42 - 3s - loss: 73.6552 - loglik: -7.3020e+01 - logprior: -6.3486e-01
Epoch 6/10
42/42 - 3s - loss: 73.3905 - loglik: -7.2766e+01 - logprior: -6.2479e-01
Epoch 7/10
42/42 - 3s - loss: 73.3184 - loglik: -7.2700e+01 - logprior: -6.1835e-01
Epoch 8/10
42/42 - 3s - loss: 72.9058 - loglik: -7.2291e+01 - logprior: -6.1497e-01
Epoch 9/10
42/42 - 3s - loss: 72.9119 - loglik: -7.2296e+01 - logprior: -6.1633e-01
Fitted a model with MAP estimate = -72.7317
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (24, 1), (26, 1), (27, 1), (35, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 47.7042 - loglik: -4.6884e+01 - logprior: -8.2057e-01
Epoch 2/2
42/42 - 4s - loss: 32.8349 - loglik: -3.2234e+01 - logprior: -6.0134e-01
Fitted a model with MAP estimate = -32.8293
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 37.0220 - loglik: -3.5964e+01 - logprior: -1.0579e+00
Epoch 2/2
42/42 - 4s - loss: 33.6932 - loglik: -3.2964e+01 - logprior: -7.2912e-01
Fitted a model with MAP estimate = -33.1728
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 33.6259 - loglik: -3.3048e+01 - logprior: -5.7757e-01
Epoch 2/10
59/59 - 5s - loss: 31.8320 - loglik: -3.1351e+01 - logprior: -4.8143e-01
Epoch 3/10
59/59 - 5s - loss: 32.2874 - loglik: -3.1816e+01 - logprior: -4.7139e-01
Fitted a model with MAP estimate = -31.7549
Time for alignment: 140.9759
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 171.9972 - loglik: -1.7131e+02 - logprior: -6.9061e-01
Epoch 2/10
42/42 - 3s - loss: 78.5131 - loglik: -7.7835e+01 - logprior: -6.7845e-01
Epoch 3/10
42/42 - 3s - loss: 75.4894 - loglik: -7.4831e+01 - logprior: -6.5846e-01
Epoch 4/10
42/42 - 3s - loss: 74.6306 - loglik: -7.3982e+01 - logprior: -6.4850e-01
Epoch 5/10
42/42 - 3s - loss: 73.7943 - loglik: -7.3160e+01 - logprior: -6.3445e-01
Epoch 6/10
42/42 - 3s - loss: 73.6062 - loglik: -7.2981e+01 - logprior: -6.2544e-01
Epoch 7/10
42/42 - 3s - loss: 73.8983 - loglik: -7.3280e+01 - logprior: -6.1796e-01
Fitted a model with MAP estimate = -72.8590
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (35, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (69, 1), (70, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 47.3117 - loglik: -4.6466e+01 - logprior: -8.4608e-01
Epoch 2/2
42/42 - 4s - loss: 33.4580 - loglik: -3.2856e+01 - logprior: -6.0188e-01
Fitted a model with MAP estimate = -32.8331
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 36.6084 - loglik: -3.5565e+01 - logprior: -1.0433e+00
Epoch 2/2
42/42 - 4s - loss: 33.1392 - loglik: -3.2412e+01 - logprior: -7.2745e-01
Fitted a model with MAP estimate = -33.0853
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 33.0656 - loglik: -3.2485e+01 - logprior: -5.8020e-01
Epoch 2/10
59/59 - 5s - loss: 32.5074 - loglik: -3.2026e+01 - logprior: -4.8174e-01
Epoch 3/10
59/59 - 5s - loss: 32.1992 - loglik: -3.1726e+01 - logprior: -4.7281e-01
Epoch 4/10
59/59 - 5s - loss: 31.8924 - loglik: -3.1427e+01 - logprior: -4.6531e-01
Epoch 5/10
59/59 - 5s - loss: 31.6046 - loglik: -3.1148e+01 - logprior: -4.5649e-01
Epoch 6/10
59/59 - 5s - loss: 31.4599 - loglik: -3.1008e+01 - logprior: -4.5155e-01
Epoch 7/10
59/59 - 5s - loss: 31.3936 - loglik: -3.0947e+01 - logprior: -4.4699e-01
Epoch 8/10
59/59 - 5s - loss: 31.0298 - loglik: -3.0592e+01 - logprior: -4.3820e-01
Epoch 9/10
59/59 - 5s - loss: 31.1640 - loglik: -3.0734e+01 - logprior: -4.3018e-01
Fitted a model with MAP estimate = -30.7568
Time for alignment: 160.7994
Computed alignments with likelihoods: ['-31.7486', '-31.3444', '-31.7586', '-31.7549', '-30.7568']
Best model has likelihood: -30.7568  (prior= -0.4239 )
time for generating output: 0.1378
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.23410013531799728
Training of 5 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49cf0e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe98adb1580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49ccfb80>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 474.5940 - loglik: -4.5127e+02 - logprior: -2.3321e+01
Epoch 2/10
14/14 - 3s - loss: 407.3474 - loglik: -4.0395e+02 - logprior: -3.3960e+00
Epoch 3/10
14/14 - 3s - loss: 370.9310 - loglik: -3.6944e+02 - logprior: -1.4913e+00
Epoch 4/10
14/14 - 3s - loss: 361.4003 - loglik: -3.6026e+02 - logprior: -1.1399e+00
Epoch 5/10
14/14 - 3s - loss: 359.8349 - loglik: -3.5889e+02 - logprior: -9.4682e-01
Epoch 6/10
14/14 - 3s - loss: 354.2301 - loglik: -3.5351e+02 - logprior: -7.1956e-01
Epoch 7/10
14/14 - 3s - loss: 355.2721 - loglik: -3.5464e+02 - logprior: -6.2953e-01
Fitted a model with MAP estimate = -354.4933
expansions: [(10, 1), (11, 1), (15, 1), (16, 3), (17, 1), (36, 2), (38, 2), (43, 1), (66, 1), (67, 1), (77, 1), (79, 1), (80, 3), (101, 2), (102, 2), (111, 3), (112, 1), (113, 2)]
discards: [ 0 45 46]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 383.3748 - loglik: -3.5538e+02 - logprior: -2.7991e+01
Epoch 2/2
14/14 - 3s - loss: 356.8443 - loglik: -3.4691e+02 - logprior: -9.9365e+00
Fitted a model with MAP estimate = -353.4755
expansions: [(0, 24), (55, 3)]
discards: [  0  18  19  20  46  93 118]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 371.3078 - loglik: -3.5016e+02 - logprior: -2.1148e+01
Epoch 2/2
14/14 - 4s - loss: 348.2900 - loglik: -3.4554e+02 - logprior: -2.7539e+00
Fitted a model with MAP estimate = -345.1157
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 370.1606 - loglik: -3.4490e+02 - logprior: -2.5263e+01
Epoch 2/10
14/14 - 3s - loss: 347.7091 - loglik: -3.4423e+02 - logprior: -3.4767e+00
Epoch 3/10
14/14 - 3s - loss: 342.6172 - loglik: -3.4293e+02 - logprior: 0.3121
Epoch 4/10
14/14 - 3s - loss: 341.5622 - loglik: -3.4301e+02 - logprior: 1.4487
Epoch 5/10
14/14 - 3s - loss: 339.2509 - loglik: -3.4114e+02 - logprior: 1.8939
Epoch 6/10
14/14 - 3s - loss: 340.5066 - loglik: -3.4271e+02 - logprior: 2.2002
Fitted a model with MAP estimate = -338.9284
Time for alignment: 75.2544
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 473.0571 - loglik: -4.4972e+02 - logprior: -2.3337e+01
Epoch 2/10
14/14 - 3s - loss: 411.0463 - loglik: -4.0759e+02 - logprior: -3.4609e+00
Epoch 3/10
14/14 - 3s - loss: 372.4123 - loglik: -3.7089e+02 - logprior: -1.5227e+00
Epoch 4/10
14/14 - 3s - loss: 361.1945 - loglik: -3.5997e+02 - logprior: -1.2215e+00
Epoch 5/10
14/14 - 3s - loss: 357.0652 - loglik: -3.5595e+02 - logprior: -1.1155e+00
Epoch 6/10
14/14 - 3s - loss: 354.2398 - loglik: -3.5333e+02 - logprior: -9.1186e-01
Epoch 7/10
14/14 - 3s - loss: 353.9583 - loglik: -3.5313e+02 - logprior: -8.2864e-01
Epoch 8/10
14/14 - 3s - loss: 353.3187 - loglik: -3.5254e+02 - logprior: -7.7369e-01
Epoch 9/10
14/14 - 3s - loss: 351.4512 - loglik: -3.5064e+02 - logprior: -8.0752e-01
Epoch 10/10
14/14 - 3s - loss: 352.9997 - loglik: -3.5222e+02 - logprior: -7.8295e-01
Fitted a model with MAP estimate = -351.8488
expansions: [(5, 1), (11, 1), (16, 4), (17, 1), (34, 1), (36, 1), (38, 1), (43, 1), (45, 1), (66, 1), (67, 1), (77, 1), (79, 5), (102, 1), (108, 1), (110, 4), (111, 1), (113, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 381.9090 - loglik: -3.5372e+02 - logprior: -2.8187e+01
Epoch 2/2
14/14 - 3s - loss: 355.0843 - loglik: -3.4496e+02 - logprior: -1.0124e+01
Fitted a model with MAP estimate = -350.6712
expansions: [(53, 1)]
discards: [ 0  1 76 77]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 374.1086 - loglik: -3.4604e+02 - logprior: -2.8066e+01
Epoch 2/2
14/14 - 3s - loss: 354.3088 - loglik: -3.4443e+02 - logprior: -9.8768e+00
Fitted a model with MAP estimate = -349.9215
expansions: [(2, 1), (52, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 372.0414 - loglik: -3.4478e+02 - logprior: -2.7264e+01
Epoch 2/10
14/14 - 3s - loss: 347.6685 - loglik: -3.4182e+02 - logprior: -5.8440e+00
Epoch 3/10
14/14 - 3s - loss: 341.8794 - loglik: -3.4170e+02 - logprior: -1.7918e-01
Epoch 4/10
14/14 - 3s - loss: 340.6284 - loglik: -3.4175e+02 - logprior: 1.1263
Epoch 5/10
14/14 - 3s - loss: 338.5002 - loglik: -3.3994e+02 - logprior: 1.4397
Epoch 6/10
14/14 - 3s - loss: 338.8195 - loglik: -3.4044e+02 - logprior: 1.6239
Fitted a model with MAP estimate = -337.7867
Time for alignment: 80.1421
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 475.0264 - loglik: -4.5168e+02 - logprior: -2.3343e+01
Epoch 2/10
14/14 - 3s - loss: 407.5232 - loglik: -4.0413e+02 - logprior: -3.3943e+00
Epoch 3/10
14/14 - 3s - loss: 372.5694 - loglik: -3.7113e+02 - logprior: -1.4362e+00
Epoch 4/10
14/14 - 3s - loss: 360.8629 - loglik: -3.5986e+02 - logprior: -1.0012e+00
Epoch 5/10
14/14 - 3s - loss: 358.1435 - loglik: -3.5732e+02 - logprior: -8.2349e-01
Epoch 6/10
14/14 - 3s - loss: 355.4276 - loglik: -3.5476e+02 - logprior: -6.6776e-01
Epoch 7/10
14/14 - 3s - loss: 354.2267 - loglik: -3.5367e+02 - logprior: -5.6102e-01
Epoch 8/10
14/14 - 3s - loss: 353.7178 - loglik: -3.5318e+02 - logprior: -5.4262e-01
Epoch 9/10
14/14 - 3s - loss: 353.7632 - loglik: -3.5321e+02 - logprior: -5.5194e-01
Fitted a model with MAP estimate = -353.1585
expansions: [(12, 1), (16, 5), (17, 1), (19, 1), (36, 1), (38, 1), (43, 1), (45, 1), (66, 1), (67, 1), (74, 1), (79, 1), (80, 3), (101, 2), (102, 2), (111, 3), (112, 1), (113, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 382.5844 - loglik: -3.5465e+02 - logprior: -2.7939e+01
Epoch 2/2
14/14 - 3s - loss: 356.8997 - loglik: -3.4717e+02 - logprior: -9.7267e+00
Fitted a model with MAP estimate = -353.0211
expansions: [(0, 26)]
discards: [  0  11  94 120]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 368.7328 - loglik: -3.4760e+02 - logprior: -2.1132e+01
Epoch 2/2
14/14 - 4s - loss: 348.9515 - loglik: -3.4635e+02 - logprior: -2.6033e+00
Fitted a model with MAP estimate = -344.4084
expansions: [(42, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  36  77 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 372.5974 - loglik: -3.4738e+02 - logprior: -2.5215e+01
Epoch 2/10
14/14 - 3s - loss: 350.4992 - loglik: -3.4703e+02 - logprior: -3.4675e+00
Epoch 3/10
14/14 - 3s - loss: 344.5583 - loglik: -3.4492e+02 - logprior: 0.3598
Epoch 4/10
14/14 - 3s - loss: 342.2203 - loglik: -3.4371e+02 - logprior: 1.4889
Epoch 5/10
14/14 - 3s - loss: 341.5905 - loglik: -3.4354e+02 - logprior: 1.9453
Epoch 6/10
14/14 - 3s - loss: 340.4639 - loglik: -3.4269e+02 - logprior: 2.2211
Epoch 7/10
14/14 - 3s - loss: 339.4116 - loglik: -3.4190e+02 - logprior: 2.4878
Epoch 8/10
14/14 - 3s - loss: 339.3490 - loglik: -3.4206e+02 - logprior: 2.7121
Epoch 9/10
14/14 - 3s - loss: 339.3454 - loglik: -3.4222e+02 - logprior: 2.8768
Epoch 10/10
14/14 - 3s - loss: 339.1005 - loglik: -3.4215e+02 - logprior: 3.0531
Fitted a model with MAP estimate = -338.5656
Time for alignment: 92.9438
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 473.6523 - loglik: -4.5032e+02 - logprior: -2.3334e+01
Epoch 2/10
14/14 - 3s - loss: 411.1047 - loglik: -4.0768e+02 - logprior: -3.4261e+00
Epoch 3/10
14/14 - 3s - loss: 374.2927 - loglik: -3.7279e+02 - logprior: -1.5007e+00
Epoch 4/10
14/14 - 3s - loss: 359.3019 - loglik: -3.5802e+02 - logprior: -1.2859e+00
Epoch 5/10
14/14 - 3s - loss: 358.5825 - loglik: -3.5742e+02 - logprior: -1.1612e+00
Epoch 6/10
14/14 - 3s - loss: 355.0491 - loglik: -3.5416e+02 - logprior: -8.8513e-01
Epoch 7/10
14/14 - 3s - loss: 353.4942 - loglik: -3.5270e+02 - logprior: -7.9363e-01
Epoch 8/10
14/14 - 3s - loss: 354.1148 - loglik: -3.5338e+02 - logprior: -7.3845e-01
Fitted a model with MAP estimate = -353.3437
expansions: [(3, 1), (15, 1), (16, 3), (18, 1), (35, 1), (36, 3), (38, 1), (43, 1), (44, 1), (66, 1), (67, 1), (77, 1), (79, 1), (80, 3), (101, 1), (102, 1), (108, 1), (111, 3), (112, 1), (113, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 380.7693 - loglik: -3.5270e+02 - logprior: -2.8065e+01
Epoch 2/2
14/14 - 3s - loss: 354.1495 - loglik: -3.4423e+02 - logprior: -9.9173e+00
Fitted a model with MAP estimate = -350.9772
expansions: [(0, 24), (5, 1), (56, 1)]
discards: [ 0 43 57 58 96]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 370.9262 - loglik: -3.4980e+02 - logprior: -2.1130e+01
Epoch 2/2
14/14 - 4s - loss: 345.6794 - loglik: -3.4303e+02 - logprior: -2.6470e+00
Fitted a model with MAP estimate = -343.7688
expansions: [(79, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 368.0160 - loglik: -3.4291e+02 - logprior: -2.5110e+01
Epoch 2/10
14/14 - 3s - loss: 345.5852 - loglik: -3.4226e+02 - logprior: -3.3223e+00
Epoch 3/10
14/14 - 3s - loss: 340.0198 - loglik: -3.4051e+02 - logprior: 0.4905
Epoch 4/10
14/14 - 3s - loss: 339.3384 - loglik: -3.4093e+02 - logprior: 1.5887
Epoch 5/10
14/14 - 3s - loss: 336.7188 - loglik: -3.3880e+02 - logprior: 2.0858
Epoch 6/10
14/14 - 3s - loss: 337.0356 - loglik: -3.3943e+02 - logprior: 2.3959
Fitted a model with MAP estimate = -336.1080
Time for alignment: 75.1730
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 474.5854 - loglik: -4.5125e+02 - logprior: -2.3336e+01
Epoch 2/10
14/14 - 3s - loss: 408.3528 - loglik: -4.0490e+02 - logprior: -3.4568e+00
Epoch 3/10
14/14 - 3s - loss: 369.7579 - loglik: -3.6840e+02 - logprior: -1.3533e+00
Epoch 4/10
14/14 - 3s - loss: 360.5338 - loglik: -3.5968e+02 - logprior: -8.5192e-01
Epoch 5/10
14/14 - 3s - loss: 356.6824 - loglik: -3.5602e+02 - logprior: -6.6580e-01
Epoch 6/10
14/14 - 3s - loss: 355.7449 - loglik: -3.5542e+02 - logprior: -3.2419e-01
Epoch 7/10
14/14 - 3s - loss: 353.9597 - loglik: -3.5370e+02 - logprior: -2.5821e-01
Epoch 8/10
14/14 - 3s - loss: 352.8486 - loglik: -3.5262e+02 - logprior: -2.2510e-01
Epoch 9/10
14/14 - 3s - loss: 355.2589 - loglik: -3.5507e+02 - logprior: -1.9298e-01
Fitted a model with MAP estimate = -352.6090
expansions: [(12, 1), (16, 5), (17, 1), (36, 2), (38, 2), (67, 2), (77, 1), (79, 5), (100, 1), (102, 1), (109, 1), (110, 3), (111, 1), (113, 1)]
discards: [ 0  1 45]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 385.9110 - loglik: -3.5798e+02 - logprior: -2.7930e+01
Epoch 2/2
14/14 - 3s - loss: 358.4314 - loglik: -3.4883e+02 - logprior: -9.6058e+00
Fitted a model with MAP estimate = -354.5830
expansions: [(0, 29)]
discards: [ 0 11 45]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 370.6084 - loglik: -3.4953e+02 - logprior: -2.1074e+01
Epoch 2/2
14/14 - 4s - loss: 350.3102 - loglik: -3.4783e+02 - logprior: -2.4789e+00
Fitted a model with MAP estimate = -345.8137
expansions: [(103, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 370.6247 - loglik: -3.4549e+02 - logprior: -2.5137e+01
Epoch 2/10
14/14 - 3s - loss: 348.6844 - loglik: -3.4536e+02 - logprior: -3.3240e+00
Epoch 3/10
14/14 - 3s - loss: 342.1531 - loglik: -3.4264e+02 - logprior: 0.4893
Epoch 4/10
14/14 - 3s - loss: 342.3325 - loglik: -3.4393e+02 - logprior: 1.5996
Fitted a model with MAP estimate = -340.5409
Time for alignment: 72.3132
Computed alignments with likelihoods: ['-338.9284', '-337.7867', '-338.5656', '-336.1080', '-340.5409']
Best model has likelihood: -336.1080  (prior= 2.5385 )
time for generating output: 0.1586
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.9756200084068937
Training of 5 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9daf1be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2bd619a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb27ec9280>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 21s - loss: 855.7019 - loglik: -8.5298e+02 - logprior: -2.7225e+00
Epoch 2/10
29/29 - 19s - loss: 671.0533 - loglik: -6.6938e+02 - logprior: -1.6691e+00
Epoch 3/10
29/29 - 19s - loss: 647.9758 - loglik: -6.4604e+02 - logprior: -1.9370e+00
Epoch 4/10
29/29 - 19s - loss: 640.7570 - loglik: -6.3879e+02 - logprior: -1.9706e+00
Epoch 5/10
29/29 - 19s - loss: 638.3984 - loglik: -6.3632e+02 - logprior: -2.0809e+00
Epoch 6/10
29/29 - 19s - loss: 635.4709 - loglik: -6.3337e+02 - logprior: -2.1001e+00
Epoch 7/10
29/29 - 19s - loss: 635.4899 - loglik: -6.3336e+02 - logprior: -2.1258e+00
Fitted a model with MAP estimate = -635.1258
expansions: [(16, 1), (22, 1), (24, 1), (29, 1), (30, 1), (31, 1), (37, 2), (39, 1), (49, 2), (50, 1), (51, 1), (52, 1), (73, 1), (76, 1), (89, 1), (90, 3), (97, 1), (121, 3), (122, 1), (123, 1), (124, 1), (142, 1), (144, 1), (151, 1), (154, 1), (155, 1), (162, 1), (171, 1), (172, 1), (181, 1), (184, 1), (185, 1), (190, 1), (191, 1), (204, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 1), (262, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 31s - loss: 621.4620 - loglik: -6.1647e+02 - logprior: -4.9935e+00
Epoch 2/2
29/29 - 26s - loss: 597.9656 - loglik: -5.9600e+02 - logprior: -1.9705e+00
Fitted a model with MAP estimate = -597.0795
expansions: [(0, 2), (37, 1)]
discards: [  0  42  58 142 260 299 325 326]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 603.0788 - loglik: -6.0019e+02 - logprior: -2.8884e+00
Epoch 2/2
29/29 - 25s - loss: 596.2910 - loglik: -5.9618e+02 - logprior: -1.0864e-01
Fitted a model with MAP estimate = -594.7486
expansions: [(322, 2)]
discards: [  0 107 324]
Re-initialized the encoder parameters.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 603.5430 - loglik: -5.9931e+02 - logprior: -4.2350e+00
Epoch 2/10
29/29 - 24s - loss: 597.9542 - loglik: -5.9758e+02 - logprior: -3.7345e-01
Epoch 3/10
29/29 - 25s - loss: 594.3997 - loglik: -5.9480e+02 - logprior: 0.3989
Epoch 4/10
29/29 - 25s - loss: 593.7976 - loglik: -5.9430e+02 - logprior: 0.5041
Epoch 5/10
29/29 - 25s - loss: 593.8686 - loglik: -5.9447e+02 - logprior: 0.6018
Fitted a model with MAP estimate = -591.7166
Time for alignment: 471.2661
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 857.4233 - loglik: -8.5469e+02 - logprior: -2.7285e+00
Epoch 2/10
29/29 - 19s - loss: 678.9395 - loglik: -6.7717e+02 - logprior: -1.7663e+00
Epoch 3/10
29/29 - 19s - loss: 651.8185 - loglik: -6.4983e+02 - logprior: -1.9902e+00
Epoch 4/10
29/29 - 19s - loss: 646.2285 - loglik: -6.4425e+02 - logprior: -1.9745e+00
Epoch 5/10
29/29 - 19s - loss: 644.7300 - loglik: -6.4274e+02 - logprior: -1.9947e+00
Epoch 6/10
29/29 - 19s - loss: 641.9991 - loglik: -6.3991e+02 - logprior: -2.0845e+00
Epoch 7/10
29/29 - 19s - loss: 638.7761 - loglik: -6.3664e+02 - logprior: -2.1345e+00
Epoch 8/10
29/29 - 19s - loss: 639.2618 - loglik: -6.3710e+02 - logprior: -2.1589e+00
Fitted a model with MAP estimate = -638.7589
expansions: [(16, 1), (17, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (48, 1), (49, 1), (50, 2), (70, 1), (86, 1), (89, 1), (90, 3), (97, 1), (117, 8), (120, 1), (121, 2), (122, 1), (124, 1), (126, 1), (139, 1), (141, 1), (147, 1), (150, 1), (154, 2), (162, 1), (171, 1), (172, 1), (182, 1), (184, 1), (185, 1), (186, 1), (191, 1), (216, 1), (217, 2), (218, 2), (219, 1), (240, 1), (250, 2), (252, 1), (256, 1), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 347 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 618.9846 - loglik: -6.1374e+02 - logprior: -5.2427e+00
Epoch 2/2
29/29 - 26s - loss: 597.2993 - loglik: -5.9491e+02 - logprior: -2.3850e+00
Fitted a model with MAP estimate = -592.4508
expansions: [(0, 2), (25, 1), (140, 1)]
discards: [  0  39  61 141 142 143 144 145 268 304 333 334]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 600.5774 - loglik: -5.9793e+02 - logprior: -2.6460e+00
Epoch 2/2
29/29 - 25s - loss: 593.7111 - loglik: -5.9368e+02 - logprior: -3.3356e-02
Fitted a model with MAP estimate = -592.1566
expansions: [(142, 3), (327, 2)]
discards: [  0 106]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 600.9853 - loglik: -5.9678e+02 - logprior: -4.2036e+00
Epoch 2/10
29/29 - 26s - loss: 593.3696 - loglik: -5.9266e+02 - logprior: -7.0814e-01
Epoch 3/10
29/29 - 26s - loss: 589.9141 - loglik: -5.9024e+02 - logprior: 0.3232
Epoch 4/10
29/29 - 25s - loss: 590.3087 - loglik: -5.9082e+02 - logprior: 0.5079
Fitted a model with MAP estimate = -588.8757
Time for alignment: 470.5160
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 23s - loss: 858.6080 - loglik: -8.5590e+02 - logprior: -2.7046e+00
Epoch 2/10
29/29 - 18s - loss: 673.4299 - loglik: -6.7192e+02 - logprior: -1.5117e+00
Epoch 3/10
29/29 - 19s - loss: 649.5060 - loglik: -6.4781e+02 - logprior: -1.6985e+00
Epoch 4/10
29/29 - 19s - loss: 639.0547 - loglik: -6.3731e+02 - logprior: -1.7444e+00
Epoch 5/10
29/29 - 19s - loss: 639.1810 - loglik: -6.3739e+02 - logprior: -1.7905e+00
Fitted a model with MAP estimate = -634.8318
expansions: [(16, 1), (17, 1), (24, 1), (25, 1), (28, 1), (29, 1), (30, 1), (31, 2), (35, 1), (41, 2), (48, 2), (49, 1), (50, 1), (65, 1), (76, 1), (120, 2), (121, 2), (123, 1), (124, 1), (128, 1), (142, 1), (144, 1), (151, 1), (153, 1), (154, 1), (155, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 1), (191, 1), (192, 1), (205, 1), (216, 1), (218, 2), (219, 2), (240, 1), (250, 1), (251, 3), (258, 1), (260, 1), (261, 1), (262, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 618.9377 - loglik: -6.1398e+02 - logprior: -4.9541e+00
Epoch 2/2
29/29 - 25s - loss: 596.4863 - loglik: -5.9445e+02 - logprior: -2.0349e+00
Fitted a model with MAP estimate = -592.6708
expansions: [(0, 2)]
discards: [  0  43  50 140 260 323 324]
Re-initialized the encoder parameters.
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 27s - loss: 597.8923 - loglik: -5.9484e+02 - logprior: -3.0567e+00
Epoch 2/2
29/29 - 25s - loss: 592.8431 - loglik: -5.9264e+02 - logprior: -2.0721e-01
Fitted a model with MAP estimate = -590.9900
expansions: [(138, 1), (257, 2), (320, 2)]
discards: [  0 292 322]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 599.8001 - loglik: -5.9541e+02 - logprior: -4.3898e+00
Epoch 2/10
29/29 - 25s - loss: 593.2759 - loglik: -5.9265e+02 - logprior: -6.2285e-01
Epoch 3/10
29/29 - 25s - loss: 590.4155 - loglik: -5.9064e+02 - logprior: 0.2272
Epoch 4/10
29/29 - 25s - loss: 588.2253 - loglik: -5.8864e+02 - logprior: 0.4163
Epoch 5/10
29/29 - 25s - loss: 588.8080 - loglik: -5.8944e+02 - logprior: 0.6321
Fitted a model with MAP estimate = -587.2813
Time for alignment: 431.8826
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 856.1425 - loglik: -8.5340e+02 - logprior: -2.7397e+00
Epoch 2/10
29/29 - 19s - loss: 673.8593 - loglik: -6.7205e+02 - logprior: -1.8079e+00
Epoch 3/10
29/29 - 19s - loss: 647.5083 - loglik: -6.4551e+02 - logprior: -1.9977e+00
Epoch 4/10
29/29 - 19s - loss: 640.3557 - loglik: -6.3828e+02 - logprior: -2.0725e+00
Epoch 5/10
29/29 - 19s - loss: 637.8282 - loglik: -6.3575e+02 - logprior: -2.0755e+00
Epoch 6/10
29/29 - 19s - loss: 637.8131 - loglik: -6.3571e+02 - logprior: -2.0995e+00
Epoch 7/10
29/29 - 19s - loss: 638.0956 - loglik: -6.3599e+02 - logprior: -2.1028e+00
Fitted a model with MAP estimate = -636.9340
expansions: [(16, 1), (17, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (46, 1), (48, 2), (49, 1), (73, 1), (76, 1), (87, 1), (88, 1), (89, 1), (121, 2), (122, 2), (125, 2), (128, 1), (142, 1), (148, 1), (151, 1), (153, 1), (155, 1), (163, 1), (172, 1), (173, 1), (183, 1), (185, 1), (186, 1), (191, 1), (192, 1), (205, 1), (218, 2), (219, 2), (220, 1), (233, 1), (240, 1), (250, 1), (252, 1), (253, 3), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 622.0623 - loglik: -6.1708e+02 - logprior: -4.9775e+00
Epoch 2/2
29/29 - 25s - loss: 601.9830 - loglik: -5.9986e+02 - logprior: -2.1230e+00
Fitted a model with MAP estimate = -596.1228
expansions: [(0, 2), (26, 1), (111, 1), (140, 1)]
discards: [  0  39 141 146 259 300 301 325 326]
Re-initialized the encoder parameters.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 602.4318 - loglik: -5.9958e+02 - logprior: -2.8475e+00
Epoch 2/2
29/29 - 25s - loss: 595.0529 - loglik: -5.9481e+02 - logprior: -2.4201e-01
Fitted a model with MAP estimate = -594.6534
expansions: [(323, 2)]
discards: [  0 325]
Re-initialized the encoder parameters.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 602.2428 - loglik: -5.9788e+02 - logprior: -4.3593e+00
Epoch 2/10
29/29 - 25s - loss: 597.8038 - loglik: -5.9730e+02 - logprior: -5.0031e-01
Epoch 3/10
29/29 - 25s - loss: 594.4421 - loglik: -5.9453e+02 - logprior: 0.0883
Epoch 4/10
29/29 - 25s - loss: 592.5815 - loglik: -5.9309e+02 - logprior: 0.5050
Epoch 5/10
29/29 - 25s - loss: 593.1060 - loglik: -5.9359e+02 - logprior: 0.4884
Fitted a model with MAP estimate = -590.8077
Time for alignment: 472.0357
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 859.7325 - loglik: -8.5702e+02 - logprior: -2.7119e+00
Epoch 2/10
29/29 - 19s - loss: 677.1074 - loglik: -6.7536e+02 - logprior: -1.7453e+00
Epoch 3/10
29/29 - 19s - loss: 647.4542 - loglik: -6.4542e+02 - logprior: -2.0373e+00
Epoch 4/10
29/29 - 19s - loss: 640.7729 - loglik: -6.3881e+02 - logprior: -1.9589e+00
Epoch 5/10
29/29 - 19s - loss: 642.7840 - loglik: -6.4083e+02 - logprior: -1.9509e+00
Fitted a model with MAP estimate = -638.3614
expansions: [(16, 1), (22, 1), (24, 1), (29, 1), (30, 1), (31, 1), (39, 1), (50, 3), (65, 1), (66, 1), (77, 1), (78, 1), (91, 2), (95, 1), (97, 3), (122, 2), (123, 1), (124, 1), (125, 1), (128, 1), (142, 2), (144, 1), (151, 1), (152, 1), (154, 1), (155, 1), (166, 2), (173, 1), (184, 1), (185, 1), (186, 1), (191, 1), (192, 1), (205, 1), (216, 2), (219, 2), (220, 1), (233, 1), (240, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 622.3088 - loglik: -6.1740e+02 - logprior: -4.9135e+00
Epoch 2/2
29/29 - 26s - loss: 597.7819 - loglik: -5.9586e+02 - logprior: -1.9202e+00
Fitted a model with MAP estimate = -594.1046
expansions: [(0, 2), (28, 1), (29, 1)]
discards: [  0  56  57 113 114 168 198 258 301 326 328]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 602.3566 - loglik: -5.9959e+02 - logprior: -2.7635e+00
Epoch 2/2
29/29 - 25s - loss: 597.6385 - loglik: -5.9755e+02 - logprior: -8.9838e-02
Fitted a model with MAP estimate = -595.7072
expansions: [(33, 1), (114, 3), (322, 2)]
discards: [ 0 29]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 604.5197 - loglik: -6.0001e+02 - logprior: -4.5096e+00
Epoch 2/10
29/29 - 25s - loss: 595.3907 - loglik: -5.9478e+02 - logprior: -6.1309e-01
Epoch 3/10
29/29 - 25s - loss: 592.3129 - loglik: -5.9274e+02 - logprior: 0.4229
Epoch 4/10
29/29 - 25s - loss: 589.6639 - loglik: -5.9012e+02 - logprior: 0.4512
Epoch 5/10
29/29 - 25s - loss: 590.2029 - loglik: -5.9090e+02 - logprior: 0.6932
Fitted a model with MAP estimate = -589.7393
Time for alignment: 435.6626
Computed alignments with likelihoods: ['-591.7166', '-588.8757', '-587.2813', '-590.8077', '-589.7393']
Best model has likelihood: -587.2813  (prior= 0.8317 )
time for generating output: 0.3081
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.8975397973950796
Training of 5 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaaf08a6d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb168f5ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb059a8340>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.1650 - loglik: -1.2839e+02 - logprior: -4.8878e+02
Epoch 2/10
10/10 - 0s - loss: 241.0588 - loglik: -1.0603e+02 - logprior: -1.3503e+02
Epoch 3/10
10/10 - 0s - loss: 150.0565 - loglik: -8.7738e+01 - logprior: -6.2319e+01
Epoch 4/10
10/10 - 0s - loss: 109.5290 - loglik: -7.5003e+01 - logprior: -3.4526e+01
Epoch 5/10
10/10 - 0s - loss: 89.9336 - loglik: -7.0216e+01 - logprior: -1.9718e+01
Epoch 6/10
10/10 - 0s - loss: 79.2850 - loglik: -6.9065e+01 - logprior: -1.0220e+01
Epoch 7/10
10/10 - 0s - loss: 72.9057 - loglik: -6.8513e+01 - logprior: -4.3925e+00
Epoch 8/10
10/10 - 0s - loss: 69.0127 - loglik: -6.8352e+01 - logprior: -6.6066e-01
Epoch 9/10
10/10 - 0s - loss: 66.4693 - loglik: -6.8403e+01 - logprior: 1.9340
Epoch 10/10
10/10 - 0s - loss: 64.5853 - loglik: -6.8369e+01 - logprior: 3.7840
Fitted a model with MAP estimate = -63.6946
expansions: [(0, 4), (9, 1), (10, 1), (17, 2), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 45 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 711.6680 - loglik: -6.1990e+01 - logprior: -6.4968e+02
Epoch 2/2
10/10 - 0s - loss: 255.2050 - loglik: -5.2427e+01 - logprior: -2.0278e+02
Fitted a model with MAP estimate = -169.4789
expansions: []
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 602.7275 - loglik: -5.2165e+01 - logprior: -5.5056e+02
Epoch 2/2
10/10 - 0s - loss: 265.6460 - loglik: -5.1422e+01 - logprior: -2.1422e+02
Fitted a model with MAP estimate = -208.7748
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 543.9719 - loglik: -4.9774e+01 - logprior: -4.9420e+02
Epoch 2/10
10/10 - 0s - loss: 184.4596 - loglik: -4.9668e+01 - logprior: -1.3479e+02
Epoch 3/10
10/10 - 0s - loss: 102.2496 - loglik: -5.0368e+01 - logprior: -5.1882e+01
Epoch 4/10
10/10 - 0s - loss: 71.2753 - loglik: -5.0952e+01 - logprior: -2.0323e+01
Epoch 5/10
10/10 - 0s - loss: 55.0728 - loglik: -5.1394e+01 - logprior: -3.6788e+00
Epoch 6/10
10/10 - 0s - loss: 45.6661 - loglik: -5.1668e+01 - logprior: 6.0020
Epoch 7/10
10/10 - 0s - loss: 39.6995 - loglik: -5.1741e+01 - logprior: 12.0414
Epoch 8/10
10/10 - 0s - loss: 35.5641 - loglik: -5.1721e+01 - logprior: 16.1565
Epoch 9/10
10/10 - 0s - loss: 32.5234 - loglik: -5.1788e+01 - logprior: 19.2650
Epoch 10/10
10/10 - 0s - loss: 30.0498 - loglik: -5.1789e+01 - logprior: 21.7396
Fitted a model with MAP estimate = -28.7989
Time for alignment: 24.7170
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 617.1650 - loglik: -1.2839e+02 - logprior: -4.8878e+02
Epoch 2/10
10/10 - 0s - loss: 241.0589 - loglik: -1.0603e+02 - logprior: -1.3503e+02
Epoch 3/10
10/10 - 0s - loss: 150.0565 - loglik: -8.7738e+01 - logprior: -6.2319e+01
Epoch 4/10
10/10 - 0s - loss: 109.5290 - loglik: -7.5003e+01 - logprior: -3.4526e+01
Epoch 5/10
10/10 - 0s - loss: 89.9336 - loglik: -7.0216e+01 - logprior: -1.9718e+01
Epoch 6/10
10/10 - 0s - loss: 79.2851 - loglik: -6.9065e+01 - logprior: -1.0220e+01
Epoch 7/10
10/10 - 0s - loss: 72.9058 - loglik: -6.8513e+01 - logprior: -4.3925e+00
Epoch 8/10
10/10 - 0s - loss: 69.0126 - loglik: -6.8352e+01 - logprior: -6.6068e-01
Epoch 9/10
10/10 - 0s - loss: 66.4694 - loglik: -6.8403e+01 - logprior: 1.9340
Epoch 10/10
10/10 - 0s - loss: 64.5859 - loglik: -6.8370e+01 - logprior: 3.7840
Fitted a model with MAP estimate = -63.6950
expansions: [(0, 4), (9, 1), (10, 1), (17, 2), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 45 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 711.6680 - loglik: -6.1990e+01 - logprior: -6.4968e+02
Epoch 2/2
10/10 - 0s - loss: 255.2050 - loglik: -5.2427e+01 - logprior: -2.0278e+02
Fitted a model with MAP estimate = -169.4789
expansions: []
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 602.7275 - loglik: -5.2165e+01 - logprior: -5.5056e+02
Epoch 2/2
10/10 - 0s - loss: 265.6458 - loglik: -5.1422e+01 - logprior: -2.1422e+02
Fitted a model with MAP estimate = -208.7746
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 543.9717 - loglik: -4.9773e+01 - logprior: -4.9420e+02
Epoch 2/10
10/10 - 0s - loss: 184.4594 - loglik: -4.9668e+01 - logprior: -1.3479e+02
Epoch 3/10
10/10 - 0s - loss: 102.2494 - loglik: -5.0367e+01 - logprior: -5.1882e+01
Epoch 4/10
10/10 - 0s - loss: 71.2751 - loglik: -5.0952e+01 - logprior: -2.0323e+01
Epoch 5/10
10/10 - 0s - loss: 55.0726 - loglik: -5.1394e+01 - logprior: -3.6790e+00
Epoch 6/10
10/10 - 0s - loss: 45.6658 - loglik: -5.1668e+01 - logprior: 6.0018
Epoch 7/10
10/10 - 0s - loss: 39.6993 - loglik: -5.1740e+01 - logprior: 12.0411
Epoch 8/10
10/10 - 0s - loss: 35.5638 - loglik: -5.1720e+01 - logprior: 16.1564
Epoch 9/10
10/10 - 0s - loss: 32.5231 - loglik: -5.1788e+01 - logprior: 19.2649
Epoch 10/10
10/10 - 0s - loss: 30.0495 - loglik: -5.1789e+01 - logprior: 21.7392
Fitted a model with MAP estimate = -28.7987
Time for alignment: 24.9405
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.1650 - loglik: -1.2839e+02 - logprior: -4.8878e+02
Epoch 2/10
10/10 - 0s - loss: 241.0589 - loglik: -1.0603e+02 - logprior: -1.3503e+02
Epoch 3/10
10/10 - 0s - loss: 150.0565 - loglik: -8.7738e+01 - logprior: -6.2319e+01
Epoch 4/10
10/10 - 0s - loss: 109.5290 - loglik: -7.5003e+01 - logprior: -3.4526e+01
Epoch 5/10
10/10 - 0s - loss: 89.9336 - loglik: -7.0216e+01 - logprior: -1.9718e+01
Epoch 6/10
10/10 - 0s - loss: 79.2850 - loglik: -6.9065e+01 - logprior: -1.0220e+01
Epoch 7/10
10/10 - 0s - loss: 72.9058 - loglik: -6.8513e+01 - logprior: -4.3925e+00
Epoch 8/10
10/10 - 0s - loss: 69.0125 - loglik: -6.8352e+01 - logprior: -6.6066e-01
Epoch 9/10
10/10 - 0s - loss: 66.4693 - loglik: -6.8403e+01 - logprior: 1.9340
Epoch 10/10
10/10 - 0s - loss: 64.5856 - loglik: -6.8370e+01 - logprior: 3.7840
Fitted a model with MAP estimate = -63.6946
expansions: [(0, 4), (9, 1), (10, 1), (17, 2), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 45 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 711.6680 - loglik: -6.1990e+01 - logprior: -6.4968e+02
Epoch 2/2
10/10 - 0s - loss: 255.2050 - loglik: -5.2427e+01 - logprior: -2.0278e+02
Fitted a model with MAP estimate = -169.4789
expansions: []
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 602.7275 - loglik: -5.2165e+01 - logprior: -5.5056e+02
Epoch 2/2
10/10 - 0s - loss: 265.6459 - loglik: -5.1422e+01 - logprior: -2.1422e+02
Fitted a model with MAP estimate = -208.7747
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 543.9717 - loglik: -4.9774e+01 - logprior: -4.9420e+02
Epoch 2/10
10/10 - 0s - loss: 184.4595 - loglik: -4.9668e+01 - logprior: -1.3479e+02
Epoch 3/10
10/10 - 0s - loss: 102.2495 - loglik: -5.0368e+01 - logprior: -5.1882e+01
Epoch 4/10
10/10 - 0s - loss: 71.2751 - loglik: -5.0952e+01 - logprior: -2.0323e+01
Epoch 5/10
10/10 - 0s - loss: 55.0726 - loglik: -5.1394e+01 - logprior: -3.6788e+00
Epoch 6/10
10/10 - 0s - loss: 45.6658 - loglik: -5.1668e+01 - logprior: 6.0020
Epoch 7/10
10/10 - 0s - loss: 39.6992 - loglik: -5.1741e+01 - logprior: 12.0413
Epoch 8/10
10/10 - 0s - loss: 35.5637 - loglik: -5.1720e+01 - logprior: 16.1566
Epoch 9/10
10/10 - 0s - loss: 32.5231 - loglik: -5.1788e+01 - logprior: 19.2651
Epoch 10/10
10/10 - 0s - loss: 30.0494 - loglik: -5.1789e+01 - logprior: 21.7396
Fitted a model with MAP estimate = -28.7987
Time for alignment: 23.7398
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.1650 - loglik: -1.2839e+02 - logprior: -4.8878e+02
Epoch 2/10
10/10 - 0s - loss: 241.0589 - loglik: -1.0603e+02 - logprior: -1.3503e+02
Epoch 3/10
10/10 - 0s - loss: 150.0565 - loglik: -8.7738e+01 - logprior: -6.2319e+01
Epoch 4/10
10/10 - 0s - loss: 109.5290 - loglik: -7.5003e+01 - logprior: -3.4526e+01
Epoch 5/10
10/10 - 0s - loss: 89.9337 - loglik: -7.0216e+01 - logprior: -1.9718e+01
Epoch 6/10
10/10 - 0s - loss: 79.2850 - loglik: -6.9065e+01 - logprior: -1.0220e+01
Epoch 7/10
10/10 - 0s - loss: 72.9058 - loglik: -6.8513e+01 - logprior: -4.3925e+00
Epoch 8/10
10/10 - 0s - loss: 69.0126 - loglik: -6.8352e+01 - logprior: -6.6066e-01
Epoch 9/10
10/10 - 0s - loss: 66.4693 - loglik: -6.8403e+01 - logprior: 1.9340
Epoch 10/10
10/10 - 0s - loss: 64.5858 - loglik: -6.8370e+01 - logprior: 3.7840
Fitted a model with MAP estimate = -63.6947
expansions: [(0, 4), (9, 1), (10, 1), (17, 2), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 45 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 711.6680 - loglik: -6.1990e+01 - logprior: -6.4968e+02
Epoch 2/2
10/10 - 0s - loss: 255.2050 - loglik: -5.2427e+01 - logprior: -2.0278e+02
Fitted a model with MAP estimate = -169.4789
expansions: []
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 602.7275 - loglik: -5.2165e+01 - logprior: -5.5056e+02
Epoch 2/2
10/10 - 0s - loss: 265.6458 - loglik: -5.1422e+01 - logprior: -2.1422e+02
Fitted a model with MAP estimate = -208.7746
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 543.9717 - loglik: -4.9773e+01 - logprior: -4.9420e+02
Epoch 2/10
10/10 - 0s - loss: 184.4594 - loglik: -4.9668e+01 - logprior: -1.3479e+02
Epoch 3/10
10/10 - 0s - loss: 102.2495 - loglik: -5.0367e+01 - logprior: -5.1882e+01
Epoch 4/10
10/10 - 0s - loss: 71.2750 - loglik: -5.0952e+01 - logprior: -2.0323e+01
Epoch 5/10
10/10 - 0s - loss: 55.0726 - loglik: -5.1394e+01 - logprior: -3.6789e+00
Epoch 6/10
10/10 - 0s - loss: 45.6657 - loglik: -5.1668e+01 - logprior: 6.0020
Epoch 7/10
10/10 - 0s - loss: 39.6992 - loglik: -5.1741e+01 - logprior: 12.0413
Epoch 8/10
10/10 - 0s - loss: 35.5636 - loglik: -5.1720e+01 - logprior: 16.1566
Epoch 9/10
10/10 - 0s - loss: 32.5230 - loglik: -5.1788e+01 - logprior: 19.2652
Epoch 10/10
10/10 - 0s - loss: 30.0493 - loglik: -5.1789e+01 - logprior: 21.7395
Fitted a model with MAP estimate = -28.7985
Time for alignment: 24.1273
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.1650 - loglik: -1.2839e+02 - logprior: -4.8878e+02
Epoch 2/10
10/10 - 0s - loss: 241.0589 - loglik: -1.0603e+02 - logprior: -1.3503e+02
Epoch 3/10
10/10 - 0s - loss: 150.0565 - loglik: -8.7738e+01 - logprior: -6.2319e+01
Epoch 4/10
10/10 - 0s - loss: 109.5289 - loglik: -7.5003e+01 - logprior: -3.4526e+01
Epoch 5/10
10/10 - 0s - loss: 89.9337 - loglik: -7.0216e+01 - logprior: -1.9718e+01
Epoch 6/10
10/10 - 0s - loss: 79.2851 - loglik: -6.9065e+01 - logprior: -1.0220e+01
Epoch 7/10
10/10 - 0s - loss: 72.9057 - loglik: -6.8513e+01 - logprior: -4.3925e+00
Epoch 8/10
10/10 - 0s - loss: 69.0126 - loglik: -6.8352e+01 - logprior: -6.6067e-01
Epoch 9/10
10/10 - 0s - loss: 66.4693 - loglik: -6.8403e+01 - logprior: 1.9340
Epoch 10/10
10/10 - 0s - loss: 64.5856 - loglik: -6.8370e+01 - logprior: 3.7840
Fitted a model with MAP estimate = -63.6944
expansions: [(0, 4), (9, 1), (10, 1), (17, 2), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 45 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 711.6680 - loglik: -6.1990e+01 - logprior: -6.4968e+02
Epoch 2/2
10/10 - 0s - loss: 255.2050 - loglik: -5.2427e+01 - logprior: -2.0278e+02
Fitted a model with MAP estimate = -169.4789
expansions: []
discards: [ 0 23]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 602.7275 - loglik: -5.2165e+01 - logprior: -5.5056e+02
Epoch 2/2
10/10 - 0s - loss: 265.6459 - loglik: -5.1422e+01 - logprior: -2.1422e+02
Fitted a model with MAP estimate = -208.7747
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 543.9718 - loglik: -4.9774e+01 - logprior: -4.9420e+02
Epoch 2/10
10/10 - 0s - loss: 184.4596 - loglik: -4.9668e+01 - logprior: -1.3479e+02
Epoch 3/10
10/10 - 0s - loss: 102.2498 - loglik: -5.0368e+01 - logprior: -5.1882e+01
Epoch 4/10
10/10 - 0s - loss: 71.2755 - loglik: -5.0952e+01 - logprior: -2.0323e+01
Epoch 5/10
10/10 - 0s - loss: 55.0733 - loglik: -5.1394e+01 - logprior: -3.6792e+00
Epoch 6/10
10/10 - 0s - loss: 45.6667 - loglik: -5.1668e+01 - logprior: 6.0015
Epoch 7/10
10/10 - 0s - loss: 39.7004 - loglik: -5.1741e+01 - logprior: 12.0407
Epoch 8/10
10/10 - 0s - loss: 35.5649 - loglik: -5.1721e+01 - logprior: 16.1557
Epoch 9/10
10/10 - 0s - loss: 32.5244 - loglik: -5.1789e+01 - logprior: 19.2641
Epoch 10/10
10/10 - 0s - loss: 30.0508 - loglik: -5.1789e+01 - logprior: 21.7385
Fitted a model with MAP estimate = -28.8001
Time for alignment: 23.7360
Computed alignments with likelihoods: ['-28.7989', '-28.7987', '-28.7987', '-28.7985', '-28.8001']
Best model has likelihood: -28.7985  (prior= 22.9495 )
time for generating output: 0.0851
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.8356481481481481
Training of 5 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feadaca74f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb40e72130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb16d4ea00>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.6728 - loglik: -4.7349e+02 - logprior: -4.1866e+00
Epoch 2/10
16/16 - 4s - loss: 441.5871 - loglik: -4.4058e+02 - logprior: -1.0055e+00
Epoch 3/10
16/16 - 4s - loss: 417.6883 - loglik: -4.1645e+02 - logprior: -1.2394e+00
Epoch 4/10
16/16 - 4s - loss: 410.9408 - loglik: -4.0966e+02 - logprior: -1.2821e+00
Epoch 5/10
16/16 - 4s - loss: 408.0345 - loglik: -4.0682e+02 - logprior: -1.2109e+00
Epoch 6/10
16/16 - 4s - loss: 406.7518 - loglik: -4.0550e+02 - logprior: -1.2471e+00
Epoch 7/10
16/16 - 4s - loss: 407.0568 - loglik: -4.0584e+02 - logprior: -1.2213e+00
Fitted a model with MAP estimate = -405.9163
expansions: [(13, 1), (14, 1), (22, 1), (23, 1), (28, 3), (42, 1), (44, 1), (48, 2), (51, 1), (54, 1), (56, 2), (57, 1), (69, 1), (73, 1), (74, 3), (94, 4), (95, 2), (96, 1), (98, 1), (116, 3), (119, 1), (122, 1), (125, 1), (129, 1), (139, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 407.6702 - loglik: -4.0386e+02 - logprior: -3.8103e+00
Epoch 2/2
33/33 - 6s - loss: 398.5575 - loglik: -3.9746e+02 - logprior: -1.0927e+00
Fitted a model with MAP estimate = -396.3587
expansions: [(35, 1), (179, 2)]
discards: [  0  32  33  72  94 116 117 145 174 175 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 407.0228 - loglik: -4.0335e+02 - logprior: -3.6720e+00
Epoch 2/2
33/33 - 6s - loss: 400.9261 - loglik: -4.0004e+02 - logprior: -8.8905e-01
Fitted a model with MAP estimate = -399.1297
expansions: [(0, 1), (169, 2)]
discards: [ 66 167 168]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 403.4629 - loglik: -4.0095e+02 - logprior: -2.5123e+00
Epoch 2/10
33/33 - 6s - loss: 398.7799 - loglik: -3.9821e+02 - logprior: -5.7017e-01
Epoch 3/10
33/33 - 6s - loss: 397.0765 - loglik: -3.9666e+02 - logprior: -4.1849e-01
Epoch 4/10
33/33 - 6s - loss: 396.8863 - loglik: -3.9650e+02 - logprior: -3.8168e-01
Epoch 5/10
33/33 - 6s - loss: 394.8049 - loglik: -3.9447e+02 - logprior: -3.3254e-01
Epoch 6/10
33/33 - 6s - loss: 395.6918 - loglik: -3.9539e+02 - logprior: -3.0239e-01
Fitted a model with MAP estimate = -394.8188
Time for alignment: 135.2061
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.8339 - loglik: -4.7365e+02 - logprior: -4.1884e+00
Epoch 2/10
16/16 - 4s - loss: 443.4277 - loglik: -4.4243e+02 - logprior: -9.9798e-01
Epoch 3/10
16/16 - 4s - loss: 419.9533 - loglik: -4.1870e+02 - logprior: -1.2570e+00
Epoch 4/10
16/16 - 4s - loss: 412.6934 - loglik: -4.1146e+02 - logprior: -1.2340e+00
Epoch 5/10
16/16 - 4s - loss: 409.8578 - loglik: -4.0873e+02 - logprior: -1.1297e+00
Epoch 6/10
16/16 - 4s - loss: 408.6485 - loglik: -4.0751e+02 - logprior: -1.1416e+00
Epoch 7/10
16/16 - 4s - loss: 407.0845 - loglik: -4.0594e+02 - logprior: -1.1453e+00
Epoch 8/10
16/16 - 4s - loss: 408.2077 - loglik: -4.0704e+02 - logprior: -1.1687e+00
Fitted a model with MAP estimate = -406.1082
expansions: [(13, 1), (14, 1), (23, 3), (28, 3), (29, 1), (49, 3), (51, 1), (55, 1), (56, 3), (72, 2), (73, 1), (74, 2), (75, 2), (95, 5), (99, 1), (105, 1), (116, 1), (117, 1), (119, 1), (122, 1), (125, 1), (129, 1), (139, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 407.5184 - loglik: -4.0374e+02 - logprior: -3.7743e+00
Epoch 2/2
33/33 - 6s - loss: 398.4144 - loglik: -3.9735e+02 - logprior: -1.0691e+00
Fitted a model with MAP estimate = -395.7813
expansions: [(179, 2)]
discards: [  0  25  33  34  70 175 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 405.0761 - loglik: -4.0148e+02 - logprior: -3.5995e+00
Epoch 2/2
33/33 - 6s - loss: 399.9368 - loglik: -3.9912e+02 - logprior: -8.1217e-01
Fitted a model with MAP estimate = -397.7105
expansions: [(0, 1), (172, 2)]
discards: [ 93 170 171]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 401.5040 - loglik: -3.9902e+02 - logprior: -2.4880e+00
Epoch 2/10
33/33 - 6s - loss: 397.6182 - loglik: -3.9706e+02 - logprior: -5.6288e-01
Epoch 3/10
33/33 - 6s - loss: 395.8407 - loglik: -3.9541e+02 - logprior: -4.2948e-01
Epoch 4/10
33/33 - 6s - loss: 395.2639 - loglik: -3.9488e+02 - logprior: -3.8199e-01
Epoch 5/10
33/33 - 6s - loss: 393.4132 - loglik: -3.9306e+02 - logprior: -3.5108e-01
Epoch 6/10
33/33 - 6s - loss: 394.0822 - loglik: -3.9378e+02 - logprior: -2.9814e-01
Fitted a model with MAP estimate = -393.2515
Time for alignment: 139.3706
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.2477 - loglik: -4.7307e+02 - logprior: -4.1801e+00
Epoch 2/10
16/16 - 4s - loss: 442.5230 - loglik: -4.4151e+02 - logprior: -1.0113e+00
Epoch 3/10
16/16 - 4s - loss: 419.3477 - loglik: -4.1807e+02 - logprior: -1.2813e+00
Epoch 4/10
16/16 - 4s - loss: 411.9995 - loglik: -4.1071e+02 - logprior: -1.2918e+00
Epoch 5/10
16/16 - 4s - loss: 408.5723 - loglik: -4.0734e+02 - logprior: -1.2316e+00
Epoch 6/10
16/16 - 4s - loss: 407.9844 - loglik: -4.0673e+02 - logprior: -1.2573e+00
Epoch 7/10
16/16 - 4s - loss: 406.4785 - loglik: -4.0524e+02 - logprior: -1.2414e+00
Epoch 8/10
16/16 - 4s - loss: 406.7946 - loglik: -4.0554e+02 - logprior: -1.2507e+00
Fitted a model with MAP estimate = -405.6497
expansions: [(13, 1), (14, 1), (23, 3), (28, 2), (30, 1), (43, 1), (45, 1), (49, 2), (50, 1), (51, 2), (57, 1), (58, 2), (69, 1), (71, 1), (72, 2), (73, 2), (94, 4), (95, 1), (99, 1), (102, 1), (116, 1), (117, 1), (119, 1), (122, 1), (125, 1), (129, 1), (139, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 407.8594 - loglik: -4.0403e+02 - logprior: -3.8319e+00
Epoch 2/2
33/33 - 7s - loss: 398.5496 - loglik: -3.9741e+02 - logprior: -1.1416e+00
Fitted a model with MAP estimate = -396.2158
expansions: [(180, 2)]
discards: [  0  25  33  34  62  74 118 175 176 177 178 179]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 406.9626 - loglik: -4.0333e+02 - logprior: -3.6359e+00
Epoch 2/2
33/33 - 6s - loss: 400.5451 - loglik: -3.9973e+02 - logprior: -8.1257e-01
Fitted a model with MAP estimate = -398.8505
expansions: [(0, 1), (69, 2), (170, 2)]
discards: [ 88 168 169]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 402.6232 - loglik: -4.0013e+02 - logprior: -2.4900e+00
Epoch 2/10
33/33 - 6s - loss: 397.9934 - loglik: -3.9743e+02 - logprior: -5.6241e-01
Epoch 3/10
33/33 - 6s - loss: 396.2802 - loglik: -3.9587e+02 - logprior: -4.0864e-01
Epoch 4/10
33/33 - 6s - loss: 395.5738 - loglik: -3.9519e+02 - logprior: -3.7897e-01
Epoch 5/10
33/33 - 6s - loss: 393.2534 - loglik: -3.9291e+02 - logprior: -3.3894e-01
Epoch 6/10
33/33 - 6s - loss: 395.7391 - loglik: -3.9545e+02 - logprior: -2.8902e-01
Fitted a model with MAP estimate = -393.6448
Time for alignment: 138.0140
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 478.0181 - loglik: -4.7384e+02 - logprior: -4.1807e+00
Epoch 2/10
16/16 - 4s - loss: 440.3382 - loglik: -4.3933e+02 - logprior: -1.0119e+00
Epoch 3/10
16/16 - 4s - loss: 419.7505 - loglik: -4.1847e+02 - logprior: -1.2785e+00
Epoch 4/10
16/16 - 4s - loss: 411.5447 - loglik: -4.1027e+02 - logprior: -1.2730e+00
Epoch 5/10
16/16 - 4s - loss: 408.1381 - loglik: -4.0693e+02 - logprior: -1.2039e+00
Epoch 6/10
16/16 - 4s - loss: 407.9220 - loglik: -4.0668e+02 - logprior: -1.2401e+00
Epoch 7/10
16/16 - 4s - loss: 407.3435 - loglik: -4.0612e+02 - logprior: -1.2256e+00
Epoch 8/10
16/16 - 4s - loss: 406.3095 - loglik: -4.0507e+02 - logprior: -1.2443e+00
Epoch 9/10
16/16 - 4s - loss: 406.4626 - loglik: -4.0522e+02 - logprior: -1.2464e+00
Fitted a model with MAP estimate = -405.6997
expansions: [(13, 1), (14, 1), (20, 1), (23, 1), (28, 2), (30, 1), (43, 1), (45, 1), (49, 2), (50, 1), (53, 1), (54, 1), (56, 2), (74, 2), (75, 3), (95, 3), (96, 2), (99, 1), (114, 1), (116, 1), (120, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 408.0059 - loglik: -4.0424e+02 - logprior: -3.7682e+00
Epoch 2/2
33/33 - 6s - loss: 399.1127 - loglik: -3.9807e+02 - logprior: -1.0472e+00
Fitted a model with MAP estimate = -396.5215
expansions: [(176, 2)]
discards: [  0  32  33  69 172 173 174 175]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 406.3153 - loglik: -4.0271e+02 - logprior: -3.6043e+00
Epoch 2/2
33/33 - 6s - loss: 400.0052 - loglik: -3.9917e+02 - logprior: -8.3256e-01
Fitted a model with MAP estimate = -398.2118
expansions: [(0, 1), (170, 3)]
discards: [168 169]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 402.5246 - loglik: -4.0003e+02 - logprior: -2.4901e+00
Epoch 2/10
33/33 - 6s - loss: 397.3129 - loglik: -3.9673e+02 - logprior: -5.8517e-01
Epoch 3/10
33/33 - 6s - loss: 396.7311 - loglik: -3.9630e+02 - logprior: -4.2728e-01
Epoch 4/10
33/33 - 6s - loss: 395.2901 - loglik: -3.9487e+02 - logprior: -4.1653e-01
Epoch 5/10
33/33 - 6s - loss: 394.4294 - loglik: -3.9406e+02 - logprior: -3.6507e-01
Epoch 6/10
33/33 - 6s - loss: 395.0810 - loglik: -3.9476e+02 - logprior: -3.2101e-01
Fitted a model with MAP estimate = -393.6432
Time for alignment: 143.3180
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.5088 - loglik: -4.7333e+02 - logprior: -4.1832e+00
Epoch 2/10
16/16 - 4s - loss: 441.3415 - loglik: -4.4032e+02 - logprior: -1.0179e+00
Epoch 3/10
16/16 - 4s - loss: 418.5040 - loglik: -4.1725e+02 - logprior: -1.2581e+00
Epoch 4/10
16/16 - 4s - loss: 412.0863 - loglik: -4.1085e+02 - logprior: -1.2347e+00
Epoch 5/10
16/16 - 4s - loss: 409.1701 - loglik: -4.0801e+02 - logprior: -1.1564e+00
Epoch 6/10
16/16 - 4s - loss: 407.4240 - loglik: -4.0626e+02 - logprior: -1.1670e+00
Epoch 7/10
16/16 - 4s - loss: 405.8874 - loglik: -4.0475e+02 - logprior: -1.1328e+00
Epoch 8/10
16/16 - 4s - loss: 405.9073 - loglik: -4.0477e+02 - logprior: -1.1397e+00
Fitted a model with MAP estimate = -405.2011
expansions: [(13, 1), (14, 1), (24, 1), (27, 1), (28, 2), (30, 1), (41, 1), (49, 3), (50, 1), (51, 2), (57, 1), (58, 1), (72, 2), (73, 1), (74, 2), (75, 1), (80, 1), (94, 5), (98, 1), (112, 1), (116, 1), (119, 1), (122, 1), (125, 1), (139, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 407.9281 - loglik: -4.0418e+02 - logprior: -3.7494e+00
Epoch 2/2
33/33 - 6s - loss: 399.0393 - loglik: -3.9805e+02 - logprior: -9.9158e-01
Fitted a model with MAP estimate = -396.3892
expansions: [(93, 1), (177, 2)]
discards: [  0  31  32  33  61 172 173 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 406.2790 - loglik: -4.0269e+02 - logprior: -3.5886e+00
Epoch 2/2
33/33 - 6s - loss: 400.5954 - loglik: -3.9978e+02 - logprior: -8.2010e-01
Fitted a model with MAP estimate = -398.5340
expansions: [(0, 1), (30, 2), (170, 3)]
discards: [168 169]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 401.8371 - loglik: -3.9942e+02 - logprior: -2.4220e+00
Epoch 2/10
33/33 - 6s - loss: 397.4496 - loglik: -3.9697e+02 - logprior: -4.8271e-01
Epoch 3/10
33/33 - 6s - loss: 395.8278 - loglik: -3.9551e+02 - logprior: -3.1703e-01
Epoch 4/10
33/33 - 6s - loss: 395.0862 - loglik: -3.9483e+02 - logprior: -2.5919e-01
Epoch 5/10
33/33 - 6s - loss: 394.6513 - loglik: -3.9442e+02 - logprior: -2.3039e-01
Epoch 6/10
33/33 - 6s - loss: 393.4972 - loglik: -3.9332e+02 - logprior: -1.7869e-01
Epoch 7/10
33/33 - 6s - loss: 393.4457 - loglik: -3.9331e+02 - logprior: -1.3531e-01
Epoch 8/10
33/33 - 6s - loss: 393.4351 - loglik: -3.9334e+02 - logprior: -9.0982e-02
Epoch 9/10
33/33 - 6s - loss: 393.1022 - loglik: -3.9306e+02 - logprior: -4.5860e-02
Epoch 10/10
33/33 - 6s - loss: 393.2632 - loglik: -3.9326e+02 - logprior: -6.3800e-04
Fitted a model with MAP estimate = -392.6354
Time for alignment: 163.7028
Computed alignments with likelihoods: ['-394.8188', '-393.2515', '-393.6448', '-393.6432', '-392.6354']
Best model has likelihood: -392.6354  (prior= 0.0085 )
time for generating output: 0.1906
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.7010022271714922
Training of 5 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb27d71af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb3031ed90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9d7da03a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.8332 - loglik: -2.7690e+02 - logprior: -1.4934e+01
Epoch 2/10
10/10 - 1s - loss: 252.6867 - loglik: -2.4880e+02 - logprior: -3.8821e+00
Epoch 3/10
10/10 - 1s - loss: 227.2457 - loglik: -2.2512e+02 - logprior: -2.1264e+00
Epoch 4/10
10/10 - 1s - loss: 212.2593 - loglik: -2.1058e+02 - logprior: -1.6815e+00
Epoch 5/10
10/10 - 1s - loss: 206.4665 - loglik: -2.0494e+02 - logprior: -1.5290e+00
Epoch 6/10
10/10 - 1s - loss: 204.6784 - loglik: -2.0311e+02 - logprior: -1.5683e+00
Epoch 7/10
10/10 - 1s - loss: 202.5903 - loglik: -2.0111e+02 - logprior: -1.4818e+00
Epoch 8/10
10/10 - 1s - loss: 202.3816 - loglik: -2.0095e+02 - logprior: -1.4347e+00
Epoch 9/10
10/10 - 1s - loss: 200.5704 - loglik: -1.9916e+02 - logprior: -1.4130e+00
Epoch 10/10
10/10 - 1s - loss: 201.5228 - loglik: -2.0011e+02 - logprior: -1.4117e+00
Fitted a model with MAP estimate = -201.2746
expansions: [(33, 1), (35, 2), (38, 1), (40, 1), (41, 1), (44, 1), (57, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.2742 - loglik: -2.0795e+02 - logprior: -1.7324e+01
Epoch 2/2
10/10 - 1s - loss: 204.7235 - loglik: -1.9730e+02 - logprior: -7.4263e+00
Fitted a model with MAP estimate = -202.2508
expansions: [(0, 21)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 212.4910 - loglik: -1.9795e+02 - logprior: -1.4536e+01
Epoch 2/2
10/10 - 1s - loss: 190.4161 - loglik: -1.8599e+02 - logprior: -4.4259e+00
Fitted a model with MAP estimate = -187.3148
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 212.0888 - loglik: -1.9684e+02 - logprior: -1.5250e+01
Epoch 2/10
10/10 - 1s - loss: 198.2001 - loglik: -1.9385e+02 - logprior: -4.3516e+00
Epoch 3/10
10/10 - 1s - loss: 195.7226 - loglik: -1.9364e+02 - logprior: -2.0854e+00
Epoch 4/10
10/10 - 1s - loss: 193.5307 - loglik: -1.9222e+02 - logprior: -1.3155e+00
Epoch 5/10
10/10 - 1s - loss: 192.3679 - loglik: -1.9143e+02 - logprior: -9.3848e-01
Epoch 6/10
10/10 - 1s - loss: 191.0105 - loglik: -1.9033e+02 - logprior: -6.8302e-01
Epoch 7/10
10/10 - 1s - loss: 192.7916 - loglik: -1.9227e+02 - logprior: -5.2144e-01
Fitted a model with MAP estimate = -191.3659
Time for alignment: 43.5463
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 290.6015 - loglik: -2.7567e+02 - logprior: -1.4934e+01
Epoch 2/10
10/10 - 1s - loss: 255.5262 - loglik: -2.5165e+02 - logprior: -3.8792e+00
Epoch 3/10
10/10 - 1s - loss: 227.2697 - loglik: -2.2512e+02 - logprior: -2.1529e+00
Epoch 4/10
10/10 - 1s - loss: 213.2098 - loglik: -2.1145e+02 - logprior: -1.7578e+00
Epoch 5/10
10/10 - 1s - loss: 206.5011 - loglik: -2.0492e+02 - logprior: -1.5834e+00
Epoch 6/10
10/10 - 1s - loss: 204.7518 - loglik: -2.0322e+02 - logprior: -1.5328e+00
Epoch 7/10
10/10 - 1s - loss: 203.6534 - loglik: -2.0225e+02 - logprior: -1.3999e+00
Epoch 8/10
10/10 - 1s - loss: 203.6223 - loglik: -2.0226e+02 - logprior: -1.3654e+00
Epoch 9/10
10/10 - 1s - loss: 203.0767 - loglik: -2.0170e+02 - logprior: -1.3785e+00
Epoch 10/10
10/10 - 1s - loss: 202.7559 - loglik: -2.0140e+02 - logprior: -1.3555e+00
Fitted a model with MAP estimate = -202.2403
expansions: [(19, 1), (22, 1), (37, 1), (39, 1), (44, 1), (48, 1), (56, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 226.8928 - loglik: -2.0962e+02 - logprior: -1.7269e+01
Epoch 2/2
10/10 - 1s - loss: 207.6277 - loglik: -2.0023e+02 - logprior: -7.3941e+00
Fitted a model with MAP estimate = -204.7050
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.0698 - loglik: -2.0403e+02 - logprior: -1.7041e+01
Epoch 2/2
10/10 - 1s - loss: 206.3044 - loglik: -1.9987e+02 - logprior: -6.4387e+00
Fitted a model with MAP estimate = -203.1633
expansions: [(0, 22)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 214.8033 - loglik: -2.0034e+02 - logprior: -1.4461e+01
Epoch 2/10
10/10 - 1s - loss: 194.5961 - loglik: -1.9012e+02 - logprior: -4.4743e+00
Epoch 3/10
10/10 - 1s - loss: 188.3921 - loglik: -1.8582e+02 - logprior: -2.5672e+00
Epoch 4/10
10/10 - 1s - loss: 186.5334 - loglik: -1.8462e+02 - logprior: -1.9122e+00
Epoch 5/10
10/10 - 1s - loss: 185.4481 - loglik: -1.8377e+02 - logprior: -1.6765e+00
Epoch 6/10
10/10 - 1s - loss: 184.9393 - loglik: -1.8339e+02 - logprior: -1.5484e+00
Epoch 7/10
10/10 - 1s - loss: 183.4735 - loglik: -1.8205e+02 - logprior: -1.4284e+00
Epoch 8/10
10/10 - 1s - loss: 184.5258 - loglik: -1.8322e+02 - logprior: -1.3029e+00
Fitted a model with MAP estimate = -183.6728
Time for alignment: 45.1958
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 291.7198 - loglik: -2.7679e+02 - logprior: -1.4933e+01
Epoch 2/10
10/10 - 1s - loss: 254.0199 - loglik: -2.5014e+02 - logprior: -3.8787e+00
Epoch 3/10
10/10 - 1s - loss: 225.2659 - loglik: -2.2311e+02 - logprior: -2.1554e+00
Epoch 4/10
10/10 - 1s - loss: 211.2414 - loglik: -2.0943e+02 - logprior: -1.8130e+00
Epoch 5/10
10/10 - 1s - loss: 204.4367 - loglik: -2.0279e+02 - logprior: -1.6513e+00
Epoch 6/10
10/10 - 1s - loss: 202.2954 - loglik: -2.0074e+02 - logprior: -1.5570e+00
Epoch 7/10
10/10 - 1s - loss: 202.8115 - loglik: -2.0135e+02 - logprior: -1.4593e+00
Fitted a model with MAP estimate = -200.9817
expansions: [(20, 1), (21, 1), (37, 1), (39, 1), (54, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.2254 - loglik: -2.0790e+02 - logprior: -1.7321e+01
Epoch 2/2
10/10 - 1s - loss: 207.9645 - loglik: -2.0059e+02 - logprior: -7.3777e+00
Fitted a model with MAP estimate = -204.7629
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.4201 - loglik: -2.0342e+02 - logprior: -1.6999e+01
Epoch 2/2
10/10 - 1s - loss: 205.3883 - loglik: -1.9920e+02 - logprior: -6.1864e+00
Fitted a model with MAP estimate = -202.1967
expansions: [(0, 21)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 93 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 214.3251 - loglik: -1.9983e+02 - logprior: -1.4497e+01
Epoch 2/10
10/10 - 1s - loss: 195.0098 - loglik: -1.9052e+02 - logprior: -4.4916e+00
Epoch 3/10
10/10 - 1s - loss: 189.5133 - loglik: -1.8691e+02 - logprior: -2.6034e+00
Epoch 4/10
10/10 - 1s - loss: 183.8416 - loglik: -1.8192e+02 - logprior: -1.9260e+00
Epoch 5/10
10/10 - 1s - loss: 185.3572 - loglik: -1.8368e+02 - logprior: -1.6769e+00
Fitted a model with MAP estimate = -184.3215
Time for alignment: 39.8366
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.7482 - loglik: -2.7681e+02 - logprior: -1.4934e+01
Epoch 2/10
10/10 - 1s - loss: 251.8199 - loglik: -2.4793e+02 - logprior: -3.8947e+00
Epoch 3/10
10/10 - 1s - loss: 227.4848 - loglik: -2.2531e+02 - logprior: -2.1779e+00
Epoch 4/10
10/10 - 1s - loss: 212.1208 - loglik: -2.1037e+02 - logprior: -1.7492e+00
Epoch 5/10
10/10 - 1s - loss: 206.0846 - loglik: -2.0452e+02 - logprior: -1.5608e+00
Epoch 6/10
10/10 - 1s - loss: 205.8271 - loglik: -2.0432e+02 - logprior: -1.5065e+00
Epoch 7/10
10/10 - 1s - loss: 202.0415 - loglik: -2.0066e+02 - logprior: -1.3784e+00
Epoch 8/10
10/10 - 1s - loss: 202.2227 - loglik: -2.0091e+02 - logprior: -1.3110e+00
Fitted a model with MAP estimate = -202.0958
expansions: [(23, 1), (35, 2), (38, 1), (40, 3), (54, 1), (55, 2), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.1657 - loglik: -2.0789e+02 - logprior: -1.7276e+01
Epoch 2/2
10/10 - 1s - loss: 206.0802 - loglik: -1.9875e+02 - logprior: -7.3316e+00
Fitted a model with MAP estimate = -201.5465
expansions: []
discards: [ 0 44 62]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 219.0835 - loglik: -2.0219e+02 - logprior: -1.6895e+01
Epoch 2/2
10/10 - 1s - loss: 202.3698 - loglik: -1.9620e+02 - logprior: -6.1676e+00
Fitted a model with MAP estimate = -200.0486
expansions: [(0, 21)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 211.6518 - loglik: -1.9718e+02 - logprior: -1.4474e+01
Epoch 2/10
10/10 - 1s - loss: 192.1021 - loglik: -1.8778e+02 - logprior: -4.3196e+00
Epoch 3/10
10/10 - 1s - loss: 186.7686 - loglik: -1.8422e+02 - logprior: -2.5445e+00
Epoch 4/10
10/10 - 1s - loss: 181.9614 - loglik: -1.8017e+02 - logprior: -1.7944e+00
Epoch 5/10
10/10 - 1s - loss: 181.7138 - loglik: -1.8012e+02 - logprior: -1.5934e+00
Epoch 6/10
10/10 - 1s - loss: 181.7862 - loglik: -1.8033e+02 - logprior: -1.4539e+00
Fitted a model with MAP estimate = -181.2730
Time for alignment: 40.2166
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 291.5416 - loglik: -2.7661e+02 - logprior: -1.4931e+01
Epoch 2/10
10/10 - 1s - loss: 253.6919 - loglik: -2.4982e+02 - logprior: -3.8758e+00
Epoch 3/10
10/10 - 1s - loss: 226.6151 - loglik: -2.2443e+02 - logprior: -2.1867e+00
Epoch 4/10
10/10 - 1s - loss: 212.2224 - loglik: -2.1038e+02 - logprior: -1.8433e+00
Epoch 5/10
10/10 - 1s - loss: 206.2094 - loglik: -2.0457e+02 - logprior: -1.6398e+00
Epoch 6/10
10/10 - 1s - loss: 201.7720 - loglik: -2.0025e+02 - logprior: -1.5244e+00
Epoch 7/10
10/10 - 1s - loss: 201.8420 - loglik: -2.0037e+02 - logprior: -1.4698e+00
Fitted a model with MAP estimate = -201.1180
expansions: [(19, 1), (34, 1), (37, 1), (39, 1), (44, 1), (48, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.0013 - loglik: -2.0867e+02 - logprior: -1.7333e+01
Epoch 2/2
10/10 - 1s - loss: 208.7566 - loglik: -2.0133e+02 - logprior: -7.4271e+00
Fitted a model with MAP estimate = -205.6155
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.7746 - loglik: -2.0375e+02 - logprior: -1.7030e+01
Epoch 2/2
10/10 - 1s - loss: 205.4023 - loglik: -1.9929e+02 - logprior: -6.1141e+00
Fitted a model with MAP estimate = -203.1410
expansions: [(0, 21)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 93 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 214.5765 - loglik: -2.0009e+02 - logprior: -1.4485e+01
Epoch 2/10
10/10 - 1s - loss: 194.8642 - loglik: -1.9036e+02 - logprior: -4.5045e+00
Epoch 3/10
10/10 - 1s - loss: 189.4510 - loglik: -1.8688e+02 - logprior: -2.5684e+00
Epoch 4/10
10/10 - 1s - loss: 187.0319 - loglik: -1.8515e+02 - logprior: -1.8850e+00
Epoch 5/10
10/10 - 1s - loss: 185.2525 - loglik: -1.8362e+02 - logprior: -1.6373e+00
Epoch 6/10
10/10 - 1s - loss: 184.0190 - loglik: -1.8249e+02 - logprior: -1.5313e+00
Epoch 7/10
10/10 - 1s - loss: 185.2956 - loglik: -1.8391e+02 - logprior: -1.3837e+00
Fitted a model with MAP estimate = -184.1241
Time for alignment: 38.9704
Computed alignments with likelihoods: ['-187.3148', '-183.6728', '-184.3215', '-181.2730', '-184.1241']
Best model has likelihood: -181.2730  (prior= -1.3645 )
time for generating output: 0.1704
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.3828463713477851
Training of 5 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaaef87430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead12d8b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe98ab74bb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 613.3357 - loglik: -5.6050e+02 - logprior: -5.2834e+01
Epoch 2/10
10/10 - 2s - loss: 516.8699 - loglik: -5.0806e+02 - logprior: -8.8086e+00
Epoch 3/10
10/10 - 2s - loss: 456.0569 - loglik: -4.5442e+02 - logprior: -1.6326e+00
Epoch 4/10
10/10 - 2s - loss: 427.0181 - loglik: -4.2767e+02 - logprior: 0.6521
Epoch 5/10
10/10 - 2s - loss: 412.1301 - loglik: -4.1383e+02 - logprior: 1.6952
Epoch 6/10
10/10 - 2s - loss: 406.1194 - loglik: -4.0842e+02 - logprior: 2.3021
Epoch 7/10
10/10 - 2s - loss: 404.2689 - loglik: -4.0683e+02 - logprior: 2.5661
Epoch 8/10
10/10 - 2s - loss: 401.3354 - loglik: -4.0414e+02 - logprior: 2.8008
Epoch 9/10
10/10 - 2s - loss: 402.1870 - loglik: -4.0518e+02 - logprior: 2.9887
Fitted a model with MAP estimate = -400.9185
expansions: [(0, 5), (19, 1), (23, 2), (26, 2), (29, 3), (43, 1), (44, 2), (46, 1), (52, 2), (76, 3), (81, 2), (97, 1), (104, 1), (106, 3), (113, 2), (114, 1), (145, 3), (151, 2), (152, 1), (154, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 206 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 480.5737 - loglik: -4.1312e+02 - logprior: -6.7451e+01
Epoch 2/2
10/10 - 3s - loss: 408.3753 - loglik: -3.9269e+02 - logprior: -1.5681e+01
Fitted a model with MAP estimate = -393.7207
expansions: []
discards: [  0   1   2   3   4   5  24  70  97 104 186]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 457.9387 - loglik: -3.9792e+02 - logprior: -6.0023e+01
Epoch 2/2
10/10 - 2s - loss: 409.9923 - loglik: -3.8882e+02 - logprior: -2.1177e+01
Fitted a model with MAP estimate = -403.1797
expansions: [(0, 21)]
discards: [  0  87  88 132]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 445.4491 - loglik: -3.9769e+02 - logprior: -4.7757e+01
Epoch 2/10
10/10 - 3s - loss: 396.7886 - loglik: -3.9004e+02 - logprior: -6.7460e+00
Epoch 3/10
10/10 - 3s - loss: 387.1366 - loglik: -3.8866e+02 - logprior: 1.5187
Epoch 4/10
10/10 - 3s - loss: 378.9445 - loglik: -3.8360e+02 - logprior: 4.6550
Epoch 5/10
10/10 - 3s - loss: 372.8114 - loglik: -3.7961e+02 - logprior: 6.7946
Epoch 6/10
10/10 - 3s - loss: 371.1652 - loglik: -3.7920e+02 - logprior: 8.0363
Epoch 7/10
10/10 - 3s - loss: 370.3869 - loglik: -3.7913e+02 - logprior: 8.7467
Epoch 8/10
10/10 - 3s - loss: 367.8711 - loglik: -3.7729e+02 - logprior: 9.4215
Epoch 9/10
10/10 - 3s - loss: 368.8870 - loglik: -3.7877e+02 - logprior: 9.8843
Fitted a model with MAP estimate = -367.4577
Time for alignment: 71.5762
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 613.7930 - loglik: -5.6097e+02 - logprior: -5.2824e+01
Epoch 2/10
10/10 - 2s - loss: 516.1616 - loglik: -5.0733e+02 - logprior: -8.8323e+00
Epoch 3/10
10/10 - 2s - loss: 454.9619 - loglik: -4.5327e+02 - logprior: -1.6915e+00
Epoch 4/10
10/10 - 2s - loss: 422.4857 - loglik: -4.2298e+02 - logprior: 0.4962
Epoch 5/10
10/10 - 2s - loss: 411.2325 - loglik: -4.1283e+02 - logprior: 1.6006
Epoch 6/10
10/10 - 2s - loss: 405.0034 - loglik: -4.0720e+02 - logprior: 2.2006
Epoch 7/10
10/10 - 2s - loss: 402.4804 - loglik: -4.0487e+02 - logprior: 2.3897
Epoch 8/10
10/10 - 2s - loss: 401.1397 - loglik: -4.0352e+02 - logprior: 2.3833
Epoch 9/10
10/10 - 2s - loss: 397.3849 - loglik: -3.9984e+02 - logprior: 2.4527
Epoch 10/10
10/10 - 2s - loss: 398.9683 - loglik: -4.0155e+02 - logprior: 2.5858
Fitted a model with MAP estimate = -398.0096
expansions: [(0, 5), (19, 1), (23, 3), (24, 1), (31, 3), (44, 1), (45, 2), (46, 1), (47, 1), (52, 2), (81, 2), (82, 2), (113, 1), (114, 1), (117, 1), (119, 1), (130, 1), (154, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 197 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 480.8805 - loglik: -4.1277e+02 - logprior: -6.8110e+01
Epoch 2/2
10/10 - 2s - loss: 411.0701 - loglik: -3.9561e+02 - logprior: -1.5461e+01
Fitted a model with MAP estimate = -395.8293
expansions: [(182, 1)]
discards: [  0   1   2   3   4   5  30 104]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 457.3753 - loglik: -3.9783e+02 - logprior: -5.9548e+01
Epoch 2/2
10/10 - 2s - loss: 412.4946 - loglik: -3.9194e+02 - logprior: -2.0553e+01
Fitted a model with MAP estimate = -404.3708
expansions: [(0, 19)]
discards: [  0  64 174]
Re-initialized the encoder parameters.
Fitting a model of length 206 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 445.3033 - loglik: -3.9811e+02 - logprior: -4.7194e+01
Epoch 2/10
10/10 - 2s - loss: 399.1641 - loglik: -3.9286e+02 - logprior: -6.3042e+00
Epoch 3/10
10/10 - 3s - loss: 384.5813 - loglik: -3.8601e+02 - logprior: 1.4293
Epoch 4/10
10/10 - 3s - loss: 378.8368 - loglik: -3.8327e+02 - logprior: 4.4337
Epoch 5/10
10/10 - 3s - loss: 374.6762 - loglik: -3.8125e+02 - logprior: 6.5785
Epoch 6/10
10/10 - 3s - loss: 371.0981 - loglik: -3.7874e+02 - logprior: 7.6449
Epoch 7/10
10/10 - 3s - loss: 370.3347 - loglik: -3.7873e+02 - logprior: 8.3931
Epoch 8/10
10/10 - 3s - loss: 369.9138 - loglik: -3.7891e+02 - logprior: 8.9964
Epoch 9/10
10/10 - 3s - loss: 369.0200 - loglik: -3.7854e+02 - logprior: 9.5187
Epoch 10/10
10/10 - 3s - loss: 366.6288 - loglik: -3.7659e+02 - logprior: 9.9658
Fitted a model with MAP estimate = -367.5595
Time for alignment: 73.6804
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 614.1693 - loglik: -5.6134e+02 - logprior: -5.2832e+01
Epoch 2/10
10/10 - 2s - loss: 515.0977 - loglik: -5.0628e+02 - logprior: -8.8154e+00
Epoch 3/10
10/10 - 2s - loss: 455.2545 - loglik: -4.5369e+02 - logprior: -1.5611e+00
Epoch 4/10
10/10 - 2s - loss: 422.1462 - loglik: -4.2292e+02 - logprior: 0.7704
Epoch 5/10
10/10 - 2s - loss: 410.4298 - loglik: -4.1228e+02 - logprior: 1.8503
Epoch 6/10
10/10 - 2s - loss: 406.8173 - loglik: -4.0934e+02 - logprior: 2.5272
Epoch 7/10
10/10 - 2s - loss: 402.4217 - loglik: -4.0524e+02 - logprior: 2.8154
Epoch 8/10
10/10 - 2s - loss: 401.4226 - loglik: -4.0428e+02 - logprior: 2.8591
Epoch 9/10
10/10 - 2s - loss: 399.4691 - loglik: -4.0239e+02 - logprior: 2.9192
Epoch 10/10
10/10 - 2s - loss: 400.8019 - loglik: -4.0388e+02 - logprior: 3.0756
Fitted a model with MAP estimate = -399.3551
expansions: [(15, 1), (22, 1), (23, 4), (30, 2), (44, 3), (46, 1), (52, 2), (76, 3), (81, 1), (82, 2), (100, 1), (105, 3), (106, 1), (118, 3), (144, 3), (150, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 463.8788 - loglik: -4.1575e+02 - logprior: -4.8129e+01
Epoch 2/2
10/10 - 2s - loss: 401.6970 - loglik: -3.9293e+02 - logprior: -8.7712e+00
Fitted a model with MAP estimate = -390.6894
expansions: []
discards: [  0  15  26  27  32  55  65  92 100 173 174 181 182 183]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 461.0504 - loglik: -4.0078e+02 - logprior: -6.0267e+01
Epoch 2/2
10/10 - 2s - loss: 418.6578 - loglik: -3.9732e+02 - logprior: -2.1340e+01
Fitted a model with MAP estimate = -409.0255
expansions: [(0, 17)]
discards: [ 0 83 84]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 448.8928 - loglik: -4.0179e+02 - logprior: -4.7102e+01
Epoch 2/10
10/10 - 2s - loss: 400.1588 - loglik: -3.9286e+02 - logprior: -7.2945e+00
Epoch 3/10
10/10 - 2s - loss: 387.9310 - loglik: -3.8852e+02 - logprior: 0.5854
Epoch 4/10
10/10 - 2s - loss: 383.7947 - loglik: -3.8766e+02 - logprior: 3.8703
Epoch 5/10
10/10 - 2s - loss: 378.2479 - loglik: -3.8403e+02 - logprior: 5.7792
Epoch 6/10
10/10 - 2s - loss: 375.7089 - loglik: -3.8262e+02 - logprior: 6.9139
Epoch 7/10
10/10 - 2s - loss: 375.0749 - loglik: -3.8271e+02 - logprior: 7.6320
Epoch 8/10
10/10 - 2s - loss: 374.0648 - loglik: -3.8230e+02 - logprior: 8.2331
Epoch 9/10
10/10 - 2s - loss: 372.4476 - loglik: -3.8114e+02 - logprior: 8.6940
Epoch 10/10
10/10 - 2s - loss: 374.3046 - loglik: -3.8347e+02 - logprior: 9.1607
Fitted a model with MAP estimate = -372.1542
Time for alignment: 74.5075
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 614.2179 - loglik: -5.6138e+02 - logprior: -5.2838e+01
Epoch 2/10
10/10 - 2s - loss: 517.7024 - loglik: -5.0887e+02 - logprior: -8.8278e+00
Epoch 3/10
10/10 - 2s - loss: 458.2166 - loglik: -4.5652e+02 - logprior: -1.7003e+00
Epoch 4/10
10/10 - 2s - loss: 426.1713 - loglik: -4.2645e+02 - logprior: 0.2783
Epoch 5/10
10/10 - 2s - loss: 411.0067 - loglik: -4.1207e+02 - logprior: 1.0594
Epoch 6/10
10/10 - 2s - loss: 405.0786 - loglik: -4.0681e+02 - logprior: 1.7335
Epoch 7/10
10/10 - 2s - loss: 400.8888 - loglik: -4.0285e+02 - logprior: 1.9590
Epoch 8/10
10/10 - 2s - loss: 400.8332 - loglik: -4.0287e+02 - logprior: 2.0391
Epoch 9/10
10/10 - 2s - loss: 400.0578 - loglik: -4.0221e+02 - logprior: 2.1508
Epoch 10/10
10/10 - 2s - loss: 397.4398 - loglik: -3.9973e+02 - logprior: 2.2933
Fitted a model with MAP estimate = -398.8251
expansions: [(7, 3), (18, 2), (19, 2), (22, 1), (23, 1), (30, 2), (42, 1), (43, 1), (44, 2), (46, 1), (52, 1), (74, 1), (75, 2), (80, 1), (81, 2), (97, 2), (107, 2), (113, 2), (114, 1), (115, 3), (142, 1), (153, 1), (154, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 474.3010 - loglik: -4.1379e+02 - logprior: -6.0510e+01
Epoch 2/2
10/10 - 2s - loss: 410.7372 - loglik: -3.8960e+02 - logprior: -2.1134e+01
Fitted a model with MAP estimate = -400.7780
expansions: [(0, 15)]
discards: [  0  23  38  58  92  93 101 119 139 145 174 186]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 446.2146 - loglik: -3.9935e+02 - logprior: -4.6867e+01
Epoch 2/2
10/10 - 2s - loss: 398.3768 - loglik: -3.9095e+02 - logprior: -7.4293e+00
Fitted a model with MAP estimate = -389.7574
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 35 51]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 448.8442 - loglik: -3.9510e+02 - logprior: -5.3743e+01
Epoch 2/10
10/10 - 2s - loss: 398.7307 - loglik: -3.8958e+02 - logprior: -9.1516e+00
Epoch 3/10
10/10 - 2s - loss: 386.3450 - loglik: -3.8715e+02 - logprior: 0.8084
Epoch 4/10
10/10 - 2s - loss: 382.0293 - loglik: -3.8662e+02 - logprior: 4.5863
Epoch 5/10
10/10 - 2s - loss: 375.9368 - loglik: -3.8245e+02 - logprior: 6.5144
Epoch 6/10
10/10 - 2s - loss: 372.2999 - loglik: -3.7983e+02 - logprior: 7.5349
Epoch 7/10
10/10 - 2s - loss: 372.3418 - loglik: -3.8057e+02 - logprior: 8.2243
Fitted a model with MAP estimate = -370.8779
Time for alignment: 65.0934
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 613.5008 - loglik: -5.6067e+02 - logprior: -5.2831e+01
Epoch 2/10
10/10 - 2s - loss: 517.0985 - loglik: -5.0828e+02 - logprior: -8.8203e+00
Epoch 3/10
10/10 - 2s - loss: 454.6532 - loglik: -4.5294e+02 - logprior: -1.7142e+00
Epoch 4/10
10/10 - 2s - loss: 420.2477 - loglik: -4.2074e+02 - logprior: 0.4938
Epoch 5/10
10/10 - 2s - loss: 409.0002 - loglik: -4.1064e+02 - logprior: 1.6410
Epoch 6/10
10/10 - 2s - loss: 402.3369 - loglik: -4.0464e+02 - logprior: 2.3017
Epoch 7/10
10/10 - 2s - loss: 402.8230 - loglik: -4.0546e+02 - logprior: 2.6366
Fitted a model with MAP estimate = -400.6909
expansions: [(7, 1), (8, 2), (23, 4), (30, 2), (42, 1), (43, 1), (44, 2), (46, 1), (52, 2), (76, 3), (81, 1), (82, 2), (113, 1), (116, 1), (119, 1), (151, 2), (152, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 195 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 476.3495 - loglik: -4.1556e+02 - logprior: -6.0791e+01
Epoch 2/2
10/10 - 2s - loss: 416.9745 - loglik: -3.9544e+02 - logprior: -2.1536e+01
Fitted a model with MAP estimate = -407.5891
expansions: [(0, 15)]
discards: [  0  56  66  93 101 175]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 448.0390 - loglik: -4.0090e+02 - logprior: -4.7141e+01
Epoch 2/2
10/10 - 2s - loss: 401.4626 - loglik: -3.9395e+02 - logprior: -7.5169e+00
Fitted a model with MAP estimate = -393.5001
expansions: [(179, 4)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13 104]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 451.0360 - loglik: -3.9649e+02 - logprior: -5.4543e+01
Epoch 2/10
10/10 - 2s - loss: 400.7936 - loglik: -3.9097e+02 - logprior: -9.8226e+00
Epoch 3/10
10/10 - 2s - loss: 387.4650 - loglik: -3.8806e+02 - logprior: 0.5917
Epoch 4/10
10/10 - 2s - loss: 379.6584 - loglik: -3.8411e+02 - logprior: 4.4564
Epoch 5/10
10/10 - 2s - loss: 376.4361 - loglik: -3.8281e+02 - logprior: 6.3728
Epoch 6/10
10/10 - 2s - loss: 372.5904 - loglik: -3.7995e+02 - logprior: 7.3616
Epoch 7/10
10/10 - 2s - loss: 371.9585 - loglik: -3.7996e+02 - logprior: 8.0049
Epoch 8/10
10/10 - 2s - loss: 370.9714 - loglik: -3.7957e+02 - logprior: 8.5973
Epoch 9/10
10/10 - 2s - loss: 369.7989 - loglik: -3.7892e+02 - logprior: 9.1252
Epoch 10/10
10/10 - 2s - loss: 368.7930 - loglik: -3.7837e+02 - logprior: 9.5772
Fitted a model with MAP estimate = -368.6214
Time for alignment: 65.7909
Computed alignments with likelihoods: ['-367.4577', '-367.5595', '-372.1542', '-370.8779', '-368.6214']
Best model has likelihood: -367.4577  (prior= 10.1821 )
time for generating output: 0.2284
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.699374511336982
Training of 5 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49863e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead19ab730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e25f6d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.5552 - loglik: -2.3545e+02 - logprior: -3.1022e+00
Epoch 2/10
19/19 - 2s - loss: 209.8210 - loglik: -2.0869e+02 - logprior: -1.1327e+00
Epoch 3/10
19/19 - 2s - loss: 199.4016 - loglik: -1.9821e+02 - logprior: -1.1879e+00
Epoch 4/10
19/19 - 2s - loss: 196.5271 - loglik: -1.9534e+02 - logprior: -1.1888e+00
Epoch 5/10
19/19 - 2s - loss: 195.6126 - loglik: -1.9447e+02 - logprior: -1.1463e+00
Epoch 6/10
19/19 - 1s - loss: 194.5118 - loglik: -1.9337e+02 - logprior: -1.1389e+00
Epoch 7/10
19/19 - 2s - loss: 194.1931 - loglik: -1.9305e+02 - logprior: -1.1418e+00
Epoch 8/10
19/19 - 2s - loss: 193.5610 - loglik: -1.9242e+02 - logprior: -1.1420e+00
Epoch 9/10
19/19 - 2s - loss: 193.5509 - loglik: -1.9243e+02 - logprior: -1.1198e+00
Epoch 10/10
19/19 - 2s - loss: 193.3157 - loglik: -1.9219e+02 - logprior: -1.1258e+00
Fitted a model with MAP estimate = -185.5018
expansions: [(0, 2), (3, 1), (4, 1), (5, 2), (13, 1), (15, 2), (16, 4), (18, 2), (23, 1), (36, 1), (46, 2), (47, 1), (48, 2), (49, 1), (53, 1), (55, 2), (58, 1)]
discards: [7]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 195.5529 - loglik: -1.9142e+02 - logprior: -4.1375e+00
Epoch 2/2
19/19 - 2s - loss: 187.3524 - loglik: -1.8588e+02 - logprior: -1.4694e+00
Fitted a model with MAP estimate = -179.2213
expansions: [(29, 1), (31, 1)]
discards: [ 0  1 24 25 26 27 62]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 190.3486 - loglik: -1.8743e+02 - logprior: -2.9215e+00
Epoch 2/2
19/19 - 2s - loss: 187.2484 - loglik: -1.8627e+02 - logprior: -9.8296e-01
Fitted a model with MAP estimate = -179.8136
expansions: [(0, 2), (22, 3)]
discards: [9]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.4798 - loglik: -1.7915e+02 - logprior: -2.3279e+00
Epoch 2/10
23/23 - 2s - loss: 178.6192 - loglik: -1.7756e+02 - logprior: -1.0586e+00
Epoch 3/10
23/23 - 2s - loss: 177.7884 - loglik: -1.7680e+02 - logprior: -9.8666e-01
Epoch 4/10
23/23 - 2s - loss: 177.4877 - loglik: -1.7655e+02 - logprior: -9.3863e-01
Epoch 5/10
23/23 - 2s - loss: 177.1362 - loglik: -1.7622e+02 - logprior: -9.1412e-01
Epoch 6/10
23/23 - 2s - loss: 177.0207 - loglik: -1.7612e+02 - logprior: -8.9648e-01
Epoch 7/10
23/23 - 2s - loss: 176.8742 - loglik: -1.7599e+02 - logprior: -8.8316e-01
Epoch 8/10
23/23 - 2s - loss: 176.7912 - loglik: -1.7592e+02 - logprior: -8.6846e-01
Epoch 9/10
23/23 - 2s - loss: 176.8895 - loglik: -1.7603e+02 - logprior: -8.5970e-01
Fitted a model with MAP estimate = -176.7461
Time for alignment: 67.6963
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.7963 - loglik: -2.3569e+02 - logprior: -3.1105e+00
Epoch 2/10
19/19 - 2s - loss: 208.2917 - loglik: -2.0716e+02 - logprior: -1.1314e+00
Epoch 3/10
19/19 - 2s - loss: 197.5575 - loglik: -1.9635e+02 - logprior: -1.2028e+00
Epoch 4/10
19/19 - 2s - loss: 195.3351 - loglik: -1.9420e+02 - logprior: -1.1324e+00
Epoch 5/10
19/19 - 2s - loss: 194.4583 - loglik: -1.9335e+02 - logprior: -1.1068e+00
Epoch 6/10
19/19 - 2s - loss: 194.1553 - loglik: -1.9307e+02 - logprior: -1.0821e+00
Epoch 7/10
19/19 - 2s - loss: 194.4276 - loglik: -1.9336e+02 - logprior: -1.0666e+00
Fitted a model with MAP estimate = -185.9771
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (25, 1), (47, 2), (48, 3), (49, 1), (52, 1), (53, 1), (55, 2), (58, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 195.1961 - loglik: -1.9227e+02 - logprior: -2.9299e+00
Epoch 2/2
19/19 - 2s - loss: 187.9186 - loglik: -1.8672e+02 - logprior: -1.1983e+00
Fitted a model with MAP estimate = -180.3905
expansions: [(30, 1)]
discards: [ 1 23 24 25 62]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 191.0027 - loglik: -1.8824e+02 - logprior: -2.7650e+00
Epoch 2/2
19/19 - 2s - loss: 187.9776 - loglik: -1.8703e+02 - logprior: -9.4745e-01
Fitted a model with MAP estimate = -180.5258
expansions: [(0, 2), (22, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.4695 - loglik: -1.7964e+02 - logprior: -1.8263e+00
Epoch 2/10
23/23 - 2s - loss: 178.9063 - loglik: -1.7794e+02 - logprior: -9.7040e-01
Epoch 3/10
23/23 - 2s - loss: 177.8227 - loglik: -1.7687e+02 - logprior: -9.5548e-01
Epoch 4/10
23/23 - 2s - loss: 177.8106 - loglik: -1.7690e+02 - logprior: -9.1496e-01
Epoch 5/10
23/23 - 2s - loss: 177.4638 - loglik: -1.7656e+02 - logprior: -9.0587e-01
Epoch 6/10
23/23 - 2s - loss: 177.3169 - loglik: -1.7643e+02 - logprior: -8.8987e-01
Epoch 7/10
23/23 - 2s - loss: 176.9716 - loglik: -1.7609e+02 - logprior: -8.7717e-01
Epoch 8/10
23/23 - 2s - loss: 177.0724 - loglik: -1.7621e+02 - logprior: -8.6126e-01
Fitted a model with MAP estimate = -177.0136
Time for alignment: 60.2209
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.1987 - loglik: -2.3509e+02 - logprior: -3.1065e+00
Epoch 2/10
19/19 - 2s - loss: 207.3138 - loglik: -2.0617e+02 - logprior: -1.1406e+00
Epoch 3/10
19/19 - 2s - loss: 197.8365 - loglik: -1.9661e+02 - logprior: -1.2251e+00
Epoch 4/10
19/19 - 2s - loss: 194.8955 - loglik: -1.9370e+02 - logprior: -1.1923e+00
Epoch 5/10
19/19 - 2s - loss: 193.7106 - loglik: -1.9252e+02 - logprior: -1.1945e+00
Epoch 6/10
19/19 - 2s - loss: 193.2042 - loglik: -1.9200e+02 - logprior: -1.2050e+00
Epoch 7/10
19/19 - 2s - loss: 192.8540 - loglik: -1.9165e+02 - logprior: -1.2022e+00
Epoch 8/10
19/19 - 2s - loss: 192.5907 - loglik: -1.9140e+02 - logprior: -1.1910e+00
Epoch 9/10
19/19 - 2s - loss: 192.8635 - loglik: -1.9168e+02 - logprior: -1.1842e+00
Fitted a model with MAP estimate = -184.7076
expansions: [(0, 2), (3, 1), (4, 1), (6, 2), (16, 1), (17, 4), (18, 2), (22, 1), (24, 1), (46, 2), (47, 2), (48, 2), (49, 1), (52, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 195.9242 - loglik: -1.9182e+02 - logprior: -4.1056e+00
Epoch 2/2
19/19 - 2s - loss: 187.8740 - loglik: -1.8645e+02 - logprior: -1.4285e+00
Fitted a model with MAP estimate = -179.6050
expansions: []
discards: [ 0  1 12 25 26 27 28 62 64 68]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 191.4250 - loglik: -1.8850e+02 - logprior: -2.9250e+00
Epoch 2/2
19/19 - 2s - loss: 188.1041 - loglik: -1.8713e+02 - logprior: -9.7255e-01
Fitted a model with MAP estimate = -180.3381
expansions: [(0, 2), (24, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.5831 - loglik: -1.7932e+02 - logprior: -2.2663e+00
Epoch 2/10
23/23 - 2s - loss: 178.9586 - loglik: -1.7791e+02 - logprior: -1.0463e+00
Epoch 3/10
23/23 - 2s - loss: 178.6759 - loglik: -1.7769e+02 - logprior: -9.8557e-01
Epoch 4/10
23/23 - 2s - loss: 178.0179 - loglik: -1.7708e+02 - logprior: -9.3902e-01
Epoch 5/10
23/23 - 2s - loss: 177.9657 - loglik: -1.7704e+02 - logprior: -9.2100e-01
Epoch 6/10
23/23 - 2s - loss: 177.5190 - loglik: -1.7661e+02 - logprior: -9.0728e-01
Epoch 7/10
23/23 - 2s - loss: 177.8347 - loglik: -1.7694e+02 - logprior: -8.9337e-01
Fitted a model with MAP estimate = -177.5862
Time for alignment: 61.3293
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 238.6972 - loglik: -2.3559e+02 - logprior: -3.1036e+00
Epoch 2/10
19/19 - 2s - loss: 207.4965 - loglik: -2.0637e+02 - logprior: -1.1242e+00
Epoch 3/10
19/19 - 2s - loss: 197.3806 - loglik: -1.9618e+02 - logprior: -1.2031e+00
Epoch 4/10
19/19 - 2s - loss: 195.4582 - loglik: -1.9433e+02 - logprior: -1.1298e+00
Epoch 5/10
19/19 - 1s - loss: 194.7641 - loglik: -1.9366e+02 - logprior: -1.1018e+00
Epoch 6/10
19/19 - 1s - loss: 194.6233 - loglik: -1.9355e+02 - logprior: -1.0762e+00
Epoch 7/10
19/19 - 2s - loss: 194.2803 - loglik: -1.9321e+02 - logprior: -1.0708e+00
Epoch 8/10
19/19 - 2s - loss: 194.3205 - loglik: -1.9326e+02 - logprior: -1.0654e+00
Fitted a model with MAP estimate = -186.0709
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (47, 2), (48, 3), (49, 1), (52, 1), (53, 1), (55, 2), (58, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 195.0735 - loglik: -1.9213e+02 - logprior: -2.9398e+00
Epoch 2/2
19/19 - 2s - loss: 187.8153 - loglik: -1.8660e+02 - logprior: -1.2161e+00
Fitted a model with MAP estimate = -180.1761
expansions: [(30, 1)]
discards: [ 1 23 24 25 62]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 190.9200 - loglik: -1.8816e+02 - logprior: -2.7562e+00
Epoch 2/2
19/19 - 2s - loss: 187.7596 - loglik: -1.8682e+02 - logprior: -9.4462e-01
Fitted a model with MAP estimate = -180.2923
expansions: [(0, 2)]
discards: [ 0 20]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 181.8082 - loglik: -1.7999e+02 - logprior: -1.8198e+00
Epoch 2/10
23/23 - 2s - loss: 179.1665 - loglik: -1.7820e+02 - logprior: -9.6777e-01
Epoch 3/10
23/23 - 2s - loss: 178.9745 - loglik: -1.7805e+02 - logprior: -9.2884e-01
Epoch 4/10
23/23 - 2s - loss: 178.5199 - loglik: -1.7763e+02 - logprior: -8.9110e-01
Epoch 5/10
23/23 - 2s - loss: 178.2801 - loglik: -1.7740e+02 - logprior: -8.7900e-01
Epoch 6/10
23/23 - 2s - loss: 178.0645 - loglik: -1.7720e+02 - logprior: -8.6248e-01
Epoch 7/10
23/23 - 2s - loss: 178.1357 - loglik: -1.7728e+02 - logprior: -8.5095e-01
Fitted a model with MAP estimate = -177.9745
Time for alignment: 60.5524
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.6953 - loglik: -2.3559e+02 - logprior: -3.1094e+00
Epoch 2/10
19/19 - 2s - loss: 207.2309 - loglik: -2.0610e+02 - logprior: -1.1299e+00
Epoch 3/10
19/19 - 1s - loss: 197.0226 - loglik: -1.9582e+02 - logprior: -1.2024e+00
Epoch 4/10
19/19 - 1s - loss: 195.1498 - loglik: -1.9398e+02 - logprior: -1.1692e+00
Epoch 5/10
19/19 - 2s - loss: 194.2497 - loglik: -1.9312e+02 - logprior: -1.1336e+00
Epoch 6/10
19/19 - 2s - loss: 193.9406 - loglik: -1.9283e+02 - logprior: -1.1087e+00
Epoch 7/10
19/19 - 2s - loss: 193.9087 - loglik: -1.9283e+02 - logprior: -1.0819e+00
Epoch 8/10
19/19 - 2s - loss: 193.6661 - loglik: -1.9258e+02 - logprior: -1.0853e+00
Epoch 9/10
19/19 - 2s - loss: 193.5487 - loglik: -1.9249e+02 - logprior: -1.0634e+00
Epoch 10/10
19/19 - 2s - loss: 193.5465 - loglik: -1.9248e+02 - logprior: -1.0635e+00
Fitted a model with MAP estimate = -185.4344
expansions: [(0, 2), (3, 1), (4, 1), (15, 5), (16, 2), (17, 1), (18, 2), (25, 1), (47, 2), (48, 3), (49, 1), (52, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 196.2065 - loglik: -1.9206e+02 - logprior: -4.1423e+00
Epoch 2/2
19/19 - 2s - loss: 188.0001 - loglik: -1.8660e+02 - logprior: -1.4042e+00
Fitted a model with MAP estimate = -179.7447
expansions: [(9, 2), (31, 1)]
discards: [ 0 22 23 26 63]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 191.8885 - loglik: -1.8802e+02 - logprior: -3.8721e+00
Epoch 2/2
19/19 - 2s - loss: 187.1229 - loglik: -1.8570e+02 - logprior: -1.4243e+00
Fitted a model with MAP estimate = -179.4867
expansions: [(77, 1)]
discards: [ 0  1  9 10 20 21 22 23 24]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 183.1665 - loglik: -1.8093e+02 - logprior: -2.2401e+00
Epoch 2/10
23/23 - 2s - loss: 180.4590 - loglik: -1.7960e+02 - logprior: -8.5880e-01
Epoch 3/10
23/23 - 2s - loss: 179.6376 - loglik: -1.7884e+02 - logprior: -7.9852e-01
Epoch 4/10
23/23 - 2s - loss: 178.9324 - loglik: -1.7817e+02 - logprior: -7.6240e-01
Epoch 5/10
23/23 - 2s - loss: 178.9220 - loglik: -1.7816e+02 - logprior: -7.6346e-01
Epoch 6/10
23/23 - 2s - loss: 178.9151 - loglik: -1.7816e+02 - logprior: -7.5844e-01
Epoch 7/10
23/23 - 2s - loss: 178.7297 - loglik: -1.7799e+02 - logprior: -7.4405e-01
Epoch 8/10
23/23 - 2s - loss: 178.5851 - loglik: -1.7784e+02 - logprior: -7.4727e-01
Epoch 9/10
23/23 - 2s - loss: 178.7572 - loglik: -1.7803e+02 - logprior: -7.2832e-01
Fitted a model with MAP estimate = -178.5407
Time for alignment: 66.1861
Computed alignments with likelihoods: ['-176.7461', '-177.0136', '-177.5862', '-177.9745', '-178.5407']
Best model has likelihood: -176.7461  (prior= -0.8419 )
time for generating output: 0.1247
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.8717357910906298
Training of 5 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2c8049d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9e91608e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ad0db0a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 796.5281 - loglik: -7.9079e+02 - logprior: -5.7379e+00
Epoch 2/10
22/22 - 12s - loss: 686.0281 - loglik: -6.8540e+02 - logprior: -6.2821e-01
Epoch 3/10
22/22 - 12s - loss: 649.7091 - loglik: -6.4810e+02 - logprior: -1.6050e+00
Epoch 4/10
22/22 - 12s - loss: 640.2282 - loglik: -6.3872e+02 - logprior: -1.5131e+00
Epoch 5/10
22/22 - 12s - loss: 641.2056 - loglik: -6.3965e+02 - logprior: -1.5602e+00
Fitted a model with MAP estimate = -639.6773
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (36, 1), (46, 1), (49, 1), (50, 1), (51, 1), (70, 2), (71, 1), (76, 1), (77, 2), (78, 1), (79, 1), (81, 1), (83, 1), (97, 1), (98, 1), (105, 2), (110, 1), (121, 2), (137, 1), (143, 2), (144, 1), (149, 1), (150, 1), (156, 2), (157, 2), (176, 1), (179, 3), (180, 1), (181, 1), (184, 3), (185, 2), (194, 2), (195, 1), (208, 1), (209, 1), (210, 1), (213, 1), (214, 2), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 640.7294 - loglik: -6.3247e+02 - logprior: -8.2601e+00
Epoch 2/2
22/22 - 17s - loss: 622.9399 - loglik: -6.2077e+02 - logprior: -2.1729e+00
Fitted a model with MAP estimate = -618.7024
expansions: [(0, 3)]
discards: [  0  34 173 194 220 230 231]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 627.1620 - loglik: -6.2218e+02 - logprior: -4.9793e+00
Epoch 2/2
22/22 - 16s - loss: 617.9742 - loglik: -6.1892e+02 - logprior: 0.9410
Fitted a model with MAP estimate = -615.9182
expansions: []
discards: [  0   1   2 100]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 631.6479 - loglik: -6.2405e+02 - logprior: -7.6008e+00
Epoch 2/10
22/22 - 16s - loss: 623.5113 - loglik: -6.2186e+02 - logprior: -1.6488e+00
Epoch 3/10
22/22 - 16s - loss: 618.3335 - loglik: -6.1833e+02 - logprior: -7.0518e-03
Epoch 4/10
22/22 - 16s - loss: 613.9607 - loglik: -6.1614e+02 - logprior: 2.1816
Epoch 5/10
22/22 - 16s - loss: 614.4826 - loglik: -6.1684e+02 - logprior: 2.3541
Fitted a model with MAP estimate = -613.5660
Time for alignment: 270.0277
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 797.6180 - loglik: -7.9187e+02 - logprior: -5.7505e+00
Epoch 2/10
22/22 - 12s - loss: 690.2427 - loglik: -6.8966e+02 - logprior: -5.8633e-01
Epoch 3/10
22/22 - 12s - loss: 650.0316 - loglik: -6.4860e+02 - logprior: -1.4358e+00
Epoch 4/10
22/22 - 12s - loss: 642.8840 - loglik: -6.4166e+02 - logprior: -1.2281e+00
Epoch 5/10
22/22 - 12s - loss: 640.4677 - loglik: -6.3922e+02 - logprior: -1.2515e+00
Epoch 6/10
22/22 - 12s - loss: 642.6150 - loglik: -6.4133e+02 - logprior: -1.2898e+00
Fitted a model with MAP estimate = -640.6227
expansions: [(12, 1), (13, 1), (14, 1), (31, 1), (32, 2), (33, 3), (34, 2), (44, 2), (49, 1), (66, 3), (69, 1), (70, 1), (75, 1), (76, 2), (77, 1), (78, 1), (80, 1), (81, 1), (96, 1), (103, 1), (104, 2), (109, 1), (120, 2), (138, 1), (144, 1), (149, 1), (153, 1), (156, 3), (158, 1), (179, 1), (181, 6), (184, 1), (185, 2), (194, 2), (195, 1), (206, 1), (209, 1), (210, 1), (213, 1), (214, 2), (226, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 640.4796 - loglik: -6.3225e+02 - logprior: -8.2300e+00
Epoch 2/2
22/22 - 17s - loss: 624.3347 - loglik: -6.2204e+02 - logprior: -2.2907e+00
Fitted a model with MAP estimate = -619.1111
expansions: [(0, 3)]
discards: [  0  39  55  80 225 226]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 629.1706 - loglik: -6.2414e+02 - logprior: -5.0299e+00
Epoch 2/2
22/22 - 17s - loss: 615.8277 - loglik: -6.1681e+02 - logprior: 0.9807
Fitted a model with MAP estimate = -615.7923
expansions: []
discards: [  0   1   2  37  38 100 229]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 632.4459 - loglik: -6.2476e+02 - logprior: -7.6882e+00
Epoch 2/10
22/22 - 16s - loss: 623.2145 - loglik: -6.2148e+02 - logprior: -1.7308e+00
Epoch 3/10
22/22 - 16s - loss: 618.3107 - loglik: -6.1828e+02 - logprior: -2.9869e-02
Epoch 4/10
22/22 - 16s - loss: 616.0970 - loglik: -6.1824e+02 - logprior: 2.1413
Epoch 5/10
22/22 - 16s - loss: 616.2795 - loglik: -6.1861e+02 - logprior: 2.3302
Fitted a model with MAP estimate = -614.1241
Time for alignment: 280.2979
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 797.2095 - loglik: -7.9146e+02 - logprior: -5.7481e+00
Epoch 2/10
22/22 - 12s - loss: 683.2040 - loglik: -6.8263e+02 - logprior: -5.7708e-01
Epoch 3/10
22/22 - 12s - loss: 648.4975 - loglik: -6.4688e+02 - logprior: -1.6209e+00
Epoch 4/10
22/22 - 12s - loss: 642.2070 - loglik: -6.4068e+02 - logprior: -1.5255e+00
Epoch 5/10
22/22 - 12s - loss: 638.4807 - loglik: -6.3692e+02 - logprior: -1.5651e+00
Epoch 6/10
22/22 - 12s - loss: 639.2900 - loglik: -6.3771e+02 - logprior: -1.5794e+00
Fitted a model with MAP estimate = -637.7490
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (46, 3), (48, 1), (49, 1), (50, 1), (70, 1), (71, 1), (73, 1), (75, 1), (76, 2), (80, 1), (81, 1), (82, 1), (97, 1), (99, 1), (105, 2), (107, 1), (109, 1), (120, 2), (143, 2), (144, 1), (149, 1), (153, 1), (156, 2), (157, 1), (176, 1), (179, 2), (180, 1), (181, 1), (184, 2), (185, 1), (186, 1), (194, 2), (195, 1), (206, 1), (207, 1), (208, 1), (209, 1), (213, 2), (224, 1), (225, 1), (227, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 640.0588 - loglik: -6.3182e+02 - logprior: -8.2346e+00
Epoch 2/2
22/22 - 17s - loss: 620.6062 - loglik: -6.1848e+02 - logprior: -2.1282e+00
Fitted a model with MAP estimate = -618.3348
expansions: [(0, 3)]
discards: [  0  34 173]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 626.7302 - loglik: -6.2178e+02 - logprior: -4.9482e+00
Epoch 2/2
22/22 - 17s - loss: 617.9700 - loglik: -6.1897e+02 - logprior: 0.9966
Fitted a model with MAP estimate = -615.1838
expansions: []
discards: [  0   1   2 228]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 632.1440 - loglik: -6.2457e+02 - logprior: -7.5757e+00
Epoch 2/10
22/22 - 16s - loss: 621.4861 - loglik: -6.1992e+02 - logprior: -1.5685e+00
Epoch 3/10
22/22 - 16s - loss: 614.7400 - loglik: -6.1470e+02 - logprior: -4.2543e-02
Epoch 4/10
22/22 - 16s - loss: 616.4288 - loglik: -6.1867e+02 - logprior: 2.2437
Fitted a model with MAP estimate = -613.3062
Time for alignment: 266.8537
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 799.1353 - loglik: -7.9339e+02 - logprior: -5.7427e+00
Epoch 2/10
22/22 - 12s - loss: 683.8234 - loglik: -6.8316e+02 - logprior: -6.6024e-01
Epoch 3/10
22/22 - 12s - loss: 652.6924 - loglik: -6.5116e+02 - logprior: -1.5358e+00
Epoch 4/10
22/22 - 12s - loss: 640.6017 - loglik: -6.3923e+02 - logprior: -1.3724e+00
Epoch 5/10
22/22 - 12s - loss: 640.9182 - loglik: -6.3956e+02 - logprior: -1.3609e+00
Fitted a model with MAP estimate = -639.3395
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (36, 1), (46, 2), (48, 2), (49, 1), (62, 1), (70, 1), (71, 2), (72, 1), (76, 2), (79, 1), (81, 1), (82, 1), (97, 1), (99, 1), (105, 2), (109, 1), (121, 2), (132, 1), (143, 2), (144, 1), (149, 1), (153, 1), (156, 2), (157, 2), (159, 1), (179, 2), (180, 4), (184, 1), (185, 2), (194, 2), (195, 1), (206, 1), (207, 1), (208, 1), (209, 1), (214, 2), (216, 2), (223, 2), (224, 1), (225, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 319 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 642.7156 - loglik: -6.3436e+02 - logprior: -8.3514e+00
Epoch 2/2
22/22 - 17s - loss: 619.6520 - loglik: -6.1734e+02 - logprior: -2.3104e+00
Fitted a model with MAP estimate = -617.6937
expansions: [(0, 3), (292, 1)]
discards: [  0  34  55 174 224 276 284 285]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 626.2501 - loglik: -6.2125e+02 - logprior: -5.0004e+00
Epoch 2/2
22/22 - 17s - loss: 616.8983 - loglik: -6.1783e+02 - logprior: 0.9282
Fitted a model with MAP estimate = -615.0403
expansions: []
discards: [  0   1   2 230]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 630.0750 - loglik: -6.2248e+02 - logprior: -7.5924e+00
Epoch 2/10
22/22 - 16s - loss: 622.4811 - loglik: -6.2085e+02 - logprior: -1.6344e+00
Epoch 3/10
22/22 - 16s - loss: 616.4501 - loglik: -6.1646e+02 - logprior: 0.0082
Epoch 4/10
22/22 - 16s - loss: 615.1824 - loglik: -6.1735e+02 - logprior: 2.1676
Epoch 5/10
22/22 - 16s - loss: 612.1552 - loglik: -6.1444e+02 - logprior: 2.2863
Epoch 6/10
22/22 - 17s - loss: 615.3869 - loglik: -6.1783e+02 - logprior: 2.4414
Fitted a model with MAP estimate = -612.1119
Time for alignment: 288.4709
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 797.3950 - loglik: -7.9166e+02 - logprior: -5.7390e+00
Epoch 2/10
22/22 - 12s - loss: 683.0567 - loglik: -6.8242e+02 - logprior: -6.4150e-01
Epoch 3/10
22/22 - 12s - loss: 648.3839 - loglik: -6.4682e+02 - logprior: -1.5628e+00
Epoch 4/10
22/22 - 12s - loss: 643.3010 - loglik: -6.4187e+02 - logprior: -1.4343e+00
Epoch 5/10
22/22 - 12s - loss: 638.7720 - loglik: -6.3722e+02 - logprior: -1.5558e+00
Epoch 6/10
22/22 - 12s - loss: 637.6840 - loglik: -6.3612e+02 - logprior: -1.5659e+00
Epoch 7/10
22/22 - 12s - loss: 639.8264 - loglik: -6.3825e+02 - logprior: -1.5779e+00
Fitted a model with MAP estimate = -637.8339
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (36, 1), (46, 2), (48, 2), (49, 1), (50, 1), (70, 1), (71, 1), (76, 1), (77, 2), (78, 1), (79, 1), (81, 1), (83, 1), (97, 1), (99, 1), (105, 2), (109, 1), (121, 2), (137, 1), (143, 2), (144, 1), (149, 1), (150, 1), (152, 1), (156, 1), (157, 3), (158, 1), (179, 1), (181, 4), (184, 2), (185, 3), (193, 2), (206, 1), (207, 1), (208, 1), (209, 1), (212, 1), (213, 2), (214, 1), (223, 2), (224, 1), (225, 1), (236, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 320 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 641.7190 - loglik: -6.3331e+02 - logprior: -8.4068e+00
Epoch 2/2
22/22 - 17s - loss: 620.4179 - loglik: -6.1807e+02 - logprior: -2.3501e+00
Fitted a model with MAP estimate = -617.6942
expansions: [(0, 3), (293, 1)]
discards: [  0  34  55 174 195 196 197 226 273 286 287]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 626.0518 - loglik: -6.2104e+02 - logprior: -5.0137e+00
Epoch 2/2
22/22 - 17s - loss: 618.7222 - loglik: -6.1958e+02 - logprior: 0.8533
Fitted a model with MAP estimate = -615.0861
expansions: []
discards: [  0   1   2  37 100 227 280]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 633.7530 - loglik: -6.2604e+02 - logprior: -7.7126e+00
Epoch 2/10
22/22 - 16s - loss: 620.2575 - loglik: -6.1842e+02 - logprior: -1.8331e+00
Epoch 3/10
22/22 - 16s - loss: 621.2105 - loglik: -6.2103e+02 - logprior: -1.8075e-01
Fitted a model with MAP estimate = -615.0439
Time for alignment: 261.8944
Computed alignments with likelihoods: ['-613.5660', '-614.1241', '-613.3062', '-612.1119', '-615.0439']
Best model has likelihood: -612.1119  (prior= 2.4155 )
time for generating output: 0.3478
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.9071405570761553
Training of 5 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb05ed0f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f1247a00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe982364ee0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 172.5222 - loglik: -8.0128e+01 - logprior: -9.2394e+01
Epoch 2/10
10/10 - 1s - loss: 94.9105 - loglik: -6.8315e+01 - logprior: -2.6595e+01
Epoch 3/10
10/10 - 1s - loss: 73.0598 - loglik: -5.9735e+01 - logprior: -1.3325e+01
Epoch 4/10
10/10 - 1s - loss: 63.2354 - loglik: -5.5011e+01 - logprior: -8.2245e+00
Epoch 5/10
10/10 - 1s - loss: 58.4468 - loglik: -5.2857e+01 - logprior: -5.5903e+00
Epoch 6/10
10/10 - 1s - loss: 56.2798 - loglik: -5.2139e+01 - logprior: -4.1411e+00
Epoch 7/10
10/10 - 1s - loss: 54.9827 - loglik: -5.1663e+01 - logprior: -3.3195e+00
Epoch 8/10
10/10 - 1s - loss: 54.1337 - loglik: -5.1290e+01 - logprior: -2.8433e+00
Epoch 9/10
10/10 - 1s - loss: 53.6130 - loglik: -5.1101e+01 - logprior: -2.5120e+00
Epoch 10/10
10/10 - 1s - loss: 53.2636 - loglik: -5.1033e+01 - logprior: -2.2301e+00
Fitted a model with MAP estimate = -53.1022
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 173.5472 - loglik: -4.9878e+01 - logprior: -1.2367e+02
Epoch 2/2
10/10 - 1s - loss: 86.1488 - loglik: -4.5999e+01 - logprior: -4.0150e+01
Fitted a model with MAP estimate = -68.9792
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 130.4471 - loglik: -4.3484e+01 - logprior: -8.6963e+01
Epoch 2/10
10/10 - 1s - loss: 68.6953 - loglik: -4.3614e+01 - logprior: -2.5082e+01
Epoch 3/10
10/10 - 1s - loss: 56.2739 - loglik: -4.3950e+01 - logprior: -1.2324e+01
Epoch 4/10
10/10 - 1s - loss: 51.0263 - loglik: -4.3842e+01 - logprior: -7.1845e+00
Epoch 5/10
10/10 - 1s - loss: 48.1273 - loglik: -4.3608e+01 - logprior: -4.5191e+00
Epoch 6/10
10/10 - 1s - loss: 46.1217 - loglik: -4.3049e+01 - logprior: -3.0726e+00
Epoch 7/10
10/10 - 1s - loss: 44.5141 - loglik: -4.2261e+01 - logprior: -2.2530e+00
Epoch 8/10
10/10 - 1s - loss: 43.7351 - loglik: -4.2067e+01 - logprior: -1.6681e+00
Epoch 9/10
10/10 - 1s - loss: 43.2470 - loglik: -4.2094e+01 - logprior: -1.1534e+00
Epoch 10/10
10/10 - 1s - loss: 42.9009 - loglik: -4.2146e+01 - logprior: -7.5485e-01
Fitted a model with MAP estimate = -42.7375
Time for alignment: 23.8854
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.5222 - loglik: -8.0128e+01 - logprior: -9.2394e+01
Epoch 2/10
10/10 - 1s - loss: 94.9105 - loglik: -6.8315e+01 - logprior: -2.6595e+01
Epoch 3/10
10/10 - 1s - loss: 73.0598 - loglik: -5.9735e+01 - logprior: -1.3325e+01
Epoch 4/10
10/10 - 1s - loss: 63.2354 - loglik: -5.5011e+01 - logprior: -8.2245e+00
Epoch 5/10
10/10 - 0s - loss: 58.4468 - loglik: -5.2856e+01 - logprior: -5.5903e+00
Epoch 6/10
10/10 - 1s - loss: 56.2798 - loglik: -5.2139e+01 - logprior: -4.1411e+00
Epoch 7/10
10/10 - 1s - loss: 54.9827 - loglik: -5.1663e+01 - logprior: -3.3195e+00
Epoch 8/10
10/10 - 1s - loss: 54.1337 - loglik: -5.1290e+01 - logprior: -2.8433e+00
Epoch 9/10
10/10 - 1s - loss: 53.6130 - loglik: -5.1101e+01 - logprior: -2.5120e+00
Epoch 10/10
10/10 - 1s - loss: 53.2636 - loglik: -5.1034e+01 - logprior: -2.2301e+00
Fitted a model with MAP estimate = -53.1020
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 173.5472 - loglik: -4.9878e+01 - logprior: -1.2367e+02
Epoch 2/2
10/10 - 1s - loss: 86.1488 - loglik: -4.5999e+01 - logprior: -4.0150e+01
Fitted a model with MAP estimate = -68.9792
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 130.4471 - loglik: -4.3484e+01 - logprior: -8.6963e+01
Epoch 2/10
10/10 - 1s - loss: 68.6952 - loglik: -4.3614e+01 - logprior: -2.5082e+01
Epoch 3/10
10/10 - 1s - loss: 56.2737 - loglik: -4.3950e+01 - logprior: -1.2324e+01
Epoch 4/10
10/10 - 1s - loss: 51.0260 - loglik: -4.3842e+01 - logprior: -7.1845e+00
Epoch 5/10
10/10 - 1s - loss: 48.1271 - loglik: -4.3608e+01 - logprior: -4.5191e+00
Epoch 6/10
10/10 - 1s - loss: 46.1214 - loglik: -4.3049e+01 - logprior: -3.0727e+00
Epoch 7/10
10/10 - 1s - loss: 44.5139 - loglik: -4.2261e+01 - logprior: -2.2530e+00
Epoch 8/10
10/10 - 1s - loss: 43.7350 - loglik: -4.2067e+01 - logprior: -1.6680e+00
Epoch 9/10
10/10 - 1s - loss: 43.2470 - loglik: -4.2094e+01 - logprior: -1.1533e+00
Epoch 10/10
10/10 - 1s - loss: 42.9008 - loglik: -4.2146e+01 - logprior: -7.5491e-01
Fitted a model with MAP estimate = -42.7375
Time for alignment: 22.9192
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 172.5222 - loglik: -8.0128e+01 - logprior: -9.2394e+01
Epoch 2/10
10/10 - 1s - loss: 94.9105 - loglik: -6.8315e+01 - logprior: -2.6595e+01
Epoch 3/10
10/10 - 1s - loss: 73.0598 - loglik: -5.9735e+01 - logprior: -1.3325e+01
Epoch 4/10
10/10 - 1s - loss: 63.2354 - loglik: -5.5011e+01 - logprior: -8.2245e+00
Epoch 5/10
10/10 - 1s - loss: 58.4468 - loglik: -5.2857e+01 - logprior: -5.5903e+00
Epoch 6/10
10/10 - 1s - loss: 56.2798 - loglik: -5.2139e+01 - logprior: -4.1411e+00
Epoch 7/10
10/10 - 1s - loss: 54.9827 - loglik: -5.1663e+01 - logprior: -3.3195e+00
Epoch 8/10
10/10 - 1s - loss: 54.1337 - loglik: -5.1290e+01 - logprior: -2.8433e+00
Epoch 9/10
10/10 - 1s - loss: 53.6130 - loglik: -5.1101e+01 - logprior: -2.5120e+00
Epoch 10/10
10/10 - 1s - loss: 53.2635 - loglik: -5.1033e+01 - logprior: -2.2301e+00
Fitted a model with MAP estimate = -53.1020
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 173.5472 - loglik: -4.9878e+01 - logprior: -1.2367e+02
Epoch 2/2
10/10 - 1s - loss: 86.1488 - loglik: -4.5999e+01 - logprior: -4.0150e+01
Fitted a model with MAP estimate = -68.9792
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 130.4472 - loglik: -4.3484e+01 - logprior: -8.6963e+01
Epoch 2/10
10/10 - 1s - loss: 68.6952 - loglik: -4.3614e+01 - logprior: -2.5082e+01
Epoch 3/10
10/10 - 1s - loss: 56.2736 - loglik: -4.3950e+01 - logprior: -1.2324e+01
Epoch 4/10
10/10 - 0s - loss: 51.0260 - loglik: -4.3841e+01 - logprior: -7.1845e+00
Epoch 5/10
10/10 - 0s - loss: 48.1271 - loglik: -4.3608e+01 - logprior: -4.5191e+00
Epoch 6/10
10/10 - 1s - loss: 46.1211 - loglik: -4.3048e+01 - logprior: -3.0727e+00
Epoch 7/10
10/10 - 1s - loss: 44.5139 - loglik: -4.2261e+01 - logprior: -2.2530e+00
Epoch 8/10
10/10 - 1s - loss: 43.7350 - loglik: -4.2067e+01 - logprior: -1.6680e+00
Epoch 9/10
10/10 - 1s - loss: 43.2470 - loglik: -4.2094e+01 - logprior: -1.1533e+00
Epoch 10/10
10/10 - 1s - loss: 42.9008 - loglik: -4.2146e+01 - logprior: -7.5494e-01
Fitted a model with MAP estimate = -42.7374
Time for alignment: 21.2081
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.5222 - loglik: -8.0128e+01 - logprior: -9.2394e+01
Epoch 2/10
10/10 - 1s - loss: 94.9105 - loglik: -6.8315e+01 - logprior: -2.6595e+01
Epoch 3/10
10/10 - 1s - loss: 73.0598 - loglik: -5.9735e+01 - logprior: -1.3325e+01
Epoch 4/10
10/10 - 1s - loss: 63.2354 - loglik: -5.5011e+01 - logprior: -8.2245e+00
Epoch 5/10
10/10 - 1s - loss: 58.4468 - loglik: -5.2856e+01 - logprior: -5.5903e+00
Epoch 6/10
10/10 - 1s - loss: 56.2799 - loglik: -5.2139e+01 - logprior: -4.1411e+00
Epoch 7/10
10/10 - 1s - loss: 54.9827 - loglik: -5.1663e+01 - logprior: -3.3195e+00
Epoch 8/10
10/10 - 1s - loss: 54.1336 - loglik: -5.1290e+01 - logprior: -2.8433e+00
Epoch 9/10
10/10 - 1s - loss: 53.6131 - loglik: -5.1101e+01 - logprior: -2.5120e+00
Epoch 10/10
10/10 - 1s - loss: 53.2635 - loglik: -5.1033e+01 - logprior: -2.2301e+00
Fitted a model with MAP estimate = -53.1020
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.5472 - loglik: -4.9878e+01 - logprior: -1.2367e+02
Epoch 2/2
10/10 - 1s - loss: 86.1488 - loglik: -4.5999e+01 - logprior: -4.0150e+01
Fitted a model with MAP estimate = -68.9792
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 130.4471 - loglik: -4.3484e+01 - logprior: -8.6963e+01
Epoch 2/10
10/10 - 1s - loss: 68.6952 - loglik: -4.3614e+01 - logprior: -2.5082e+01
Epoch 3/10
10/10 - 1s - loss: 56.2738 - loglik: -4.3950e+01 - logprior: -1.2324e+01
Epoch 4/10
10/10 - 1s - loss: 51.0261 - loglik: -4.3842e+01 - logprior: -7.1845e+00
Epoch 5/10
10/10 - 1s - loss: 48.1273 - loglik: -4.3608e+01 - logprior: -4.5191e+00
Epoch 6/10
10/10 - 1s - loss: 46.1218 - loglik: -4.3049e+01 - logprior: -3.0727e+00
Epoch 7/10
10/10 - 1s - loss: 44.5141 - loglik: -4.2261e+01 - logprior: -2.2529e+00
Epoch 8/10
10/10 - 1s - loss: 43.7351 - loglik: -4.2067e+01 - logprior: -1.6680e+00
Epoch 9/10
10/10 - 1s - loss: 43.2470 - loglik: -4.2094e+01 - logprior: -1.1534e+00
Epoch 10/10
10/10 - 1s - loss: 42.9008 - loglik: -4.2146e+01 - logprior: -7.5489e-01
Fitted a model with MAP estimate = -42.7374
Time for alignment: 23.8795
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 172.5222 - loglik: -8.0128e+01 - logprior: -9.2394e+01
Epoch 2/10
10/10 - 1s - loss: 94.9105 - loglik: -6.8315e+01 - logprior: -2.6595e+01
Epoch 3/10
10/10 - 1s - loss: 73.0598 - loglik: -5.9735e+01 - logprior: -1.3325e+01
Epoch 4/10
10/10 - 1s - loss: 63.2354 - loglik: -5.5011e+01 - logprior: -8.2245e+00
Epoch 5/10
10/10 - 1s - loss: 58.4468 - loglik: -5.2856e+01 - logprior: -5.5903e+00
Epoch 6/10
10/10 - 1s - loss: 56.2798 - loglik: -5.2139e+01 - logprior: -4.1411e+00
Epoch 7/10
10/10 - 1s - loss: 54.9827 - loglik: -5.1663e+01 - logprior: -3.3195e+00
Epoch 8/10
10/10 - 1s - loss: 54.1337 - loglik: -5.1290e+01 - logprior: -2.8433e+00
Epoch 9/10
10/10 - 1s - loss: 53.6130 - loglik: -5.1101e+01 - logprior: -2.5120e+00
Epoch 10/10
10/10 - 0s - loss: 53.2635 - loglik: -5.1033e+01 - logprior: -2.2301e+00
Fitted a model with MAP estimate = -53.1020
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 173.5472 - loglik: -4.9878e+01 - logprior: -1.2367e+02
Epoch 2/2
10/10 - 0s - loss: 86.1488 - loglik: -4.5999e+01 - logprior: -4.0150e+01
Fitted a model with MAP estimate = -68.9792
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 130.4472 - loglik: -4.3484e+01 - logprior: -8.6963e+01
Epoch 2/10
10/10 - 1s - loss: 68.6953 - loglik: -4.3614e+01 - logprior: -2.5082e+01
Epoch 3/10
10/10 - 1s - loss: 56.2740 - loglik: -4.3950e+01 - logprior: -1.2324e+01
Epoch 4/10
10/10 - 1s - loss: 51.0263 - loglik: -4.3842e+01 - logprior: -7.1844e+00
Epoch 5/10
10/10 - 1s - loss: 48.1275 - loglik: -4.3608e+01 - logprior: -4.5191e+00
Epoch 6/10
10/10 - 0s - loss: 46.1218 - loglik: -4.3049e+01 - logprior: -3.0725e+00
Epoch 7/10
10/10 - 0s - loss: 44.5140 - loglik: -4.2261e+01 - logprior: -2.2530e+00
Epoch 8/10
10/10 - 1s - loss: 43.7352 - loglik: -4.2067e+01 - logprior: -1.6683e+00
Epoch 9/10
10/10 - 1s - loss: 43.2471 - loglik: -4.2094e+01 - logprior: -1.1534e+00
Epoch 10/10
10/10 - 1s - loss: 42.9010 - loglik: -4.2146e+01 - logprior: -7.5480e-01
Fitted a model with MAP estimate = -42.7377
Time for alignment: 22.6844
Computed alignments with likelihoods: ['-42.7375', '-42.7375', '-42.7374', '-42.7374', '-42.7377']
Best model has likelihood: -42.7374  (prior= -0.5864 )
time for generating output: 0.0981
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9302325581395349
Training of 5 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2c153520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb27c449d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9aca530a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.9620 - loglik: -3.5065e+02 - logprior: -1.2310e+01
Epoch 2/10
11/11 - 2s - loss: 308.6390 - loglik: -3.0569e+02 - logprior: -2.9460e+00
Epoch 3/10
11/11 - 2s - loss: 269.9921 - loglik: -2.6807e+02 - logprior: -1.9265e+00
Epoch 4/10
11/11 - 2s - loss: 250.5400 - loglik: -2.4854e+02 - logprior: -1.9984e+00
Epoch 5/10
11/11 - 2s - loss: 243.4062 - loglik: -2.4131e+02 - logprior: -2.0945e+00
Epoch 6/10
11/11 - 2s - loss: 241.4641 - loglik: -2.3932e+02 - logprior: -2.1475e+00
Epoch 7/10
11/11 - 2s - loss: 238.1438 - loglik: -2.3609e+02 - logprior: -2.0501e+00
Epoch 8/10
11/11 - 2s - loss: 238.3163 - loglik: -2.3633e+02 - logprior: -1.9885e+00
Fitted a model with MAP estimate = -237.4125
expansions: [(8, 2), (9, 3), (10, 1), (12, 2), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 5), (64, 1), (65, 1), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 251.4351 - loglik: -2.3733e+02 - logprior: -1.4110e+01
Epoch 2/2
11/11 - 2s - loss: 226.4616 - loglik: -2.2068e+02 - logprior: -5.7843e+00
Fitted a model with MAP estimate = -222.8643
expansions: [(0, 22)]
discards: [ 0  7 76 77]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 225.1160 - loglik: -2.1768e+02 - logprior: -7.4382e+00
Epoch 2/2
22/22 - 3s - loss: 214.5947 - loglik: -2.1302e+02 - logprior: -1.5777e+00
Fitted a model with MAP estimate = -210.8957
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 231.8947 - loglik: -2.1859e+02 - logprior: -1.3304e+01
Epoch 2/10
11/11 - 2s - loss: 222.2025 - loglik: -2.1878e+02 - logprior: -3.4253e+00
Epoch 3/10
11/11 - 2s - loss: 217.0810 - loglik: -2.1590e+02 - logprior: -1.1854e+00
Epoch 4/10
11/11 - 2s - loss: 215.3424 - loglik: -2.1479e+02 - logprior: -5.4830e-01
Epoch 5/10
11/11 - 2s - loss: 213.4242 - loglik: -2.1309e+02 - logprior: -3.3135e-01
Epoch 6/10
11/11 - 2s - loss: 213.5712 - loglik: -2.1332e+02 - logprior: -2.5302e-01
Fitted a model with MAP estimate = -212.6408
Time for alignment: 64.1059
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.8098 - loglik: -3.5050e+02 - logprior: -1.2309e+01
Epoch 2/10
11/11 - 2s - loss: 308.6282 - loglik: -3.0568e+02 - logprior: -2.9439e+00
Epoch 3/10
11/11 - 2s - loss: 267.0604 - loglik: -2.6517e+02 - logprior: -1.8928e+00
Epoch 4/10
11/11 - 2s - loss: 249.5633 - loglik: -2.4765e+02 - logprior: -1.9134e+00
Epoch 5/10
11/11 - 2s - loss: 245.3863 - loglik: -2.4343e+02 - logprior: -1.9540e+00
Epoch 6/10
11/11 - 2s - loss: 241.0292 - loglik: -2.3905e+02 - logprior: -1.9832e+00
Epoch 7/10
11/11 - 2s - loss: 240.1656 - loglik: -2.3830e+02 - logprior: -1.8631e+00
Epoch 8/10
11/11 - 2s - loss: 238.8871 - loglik: -2.3710e+02 - logprior: -1.7891e+00
Epoch 9/10
11/11 - 2s - loss: 238.8248 - loglik: -2.3702e+02 - logprior: -1.8035e+00
Epoch 10/10
11/11 - 2s - loss: 237.7254 - loglik: -2.3587e+02 - logprior: -1.8505e+00
Fitted a model with MAP estimate = -236.9428
expansions: [(8, 2), (9, 3), (10, 1), (12, 2), (33, 1), (34, 1), (38, 2), (61, 1), (62, 5), (64, 1), (65, 1), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 252.3720 - loglik: -2.3819e+02 - logprior: -1.4183e+01
Epoch 2/2
11/11 - 2s - loss: 228.4593 - loglik: -2.2257e+02 - logprior: -5.8871e+00
Fitted a model with MAP estimate = -224.1820
expansions: [(0, 22)]
discards: [  0   7  75  76 107]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 225.9847 - loglik: -2.1850e+02 - logprior: -7.4820e+00
Epoch 2/2
22/22 - 3s - loss: 214.4541 - loglik: -2.1284e+02 - logprior: -1.6102e+00
Fitted a model with MAP estimate = -210.9176
expansions: [(70, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 234.1423 - loglik: -2.2054e+02 - logprior: -1.3604e+01
Epoch 2/10
11/11 - 2s - loss: 220.4408 - loglik: -2.1654e+02 - logprior: -3.8984e+00
Epoch 3/10
11/11 - 2s - loss: 216.1052 - loglik: -2.1475e+02 - logprior: -1.3573e+00
Epoch 4/10
11/11 - 2s - loss: 214.1415 - loglik: -2.1347e+02 - logprior: -6.6692e-01
Epoch 5/10
11/11 - 2s - loss: 211.8131 - loglik: -2.1134e+02 - logprior: -4.7241e-01
Epoch 6/10
11/11 - 2s - loss: 212.1459 - loglik: -2.1171e+02 - logprior: -4.3283e-01
Fitted a model with MAP estimate = -211.3307
Time for alignment: 66.1915
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.3198 - loglik: -3.5001e+02 - logprior: -1.2311e+01
Epoch 2/10
11/11 - 2s - loss: 310.5060 - loglik: -3.0756e+02 - logprior: -2.9460e+00
Epoch 3/10
11/11 - 2s - loss: 268.4266 - loglik: -2.6650e+02 - logprior: -1.9249e+00
Epoch 4/10
11/11 - 2s - loss: 250.8700 - loglik: -2.4887e+02 - logprior: -2.0006e+00
Epoch 5/10
11/11 - 2s - loss: 243.9971 - loglik: -2.4189e+02 - logprior: -2.1107e+00
Epoch 6/10
11/11 - 2s - loss: 242.0036 - loglik: -2.3984e+02 - logprior: -2.1676e+00
Epoch 7/10
11/11 - 2s - loss: 240.5102 - loglik: -2.3846e+02 - logprior: -2.0516e+00
Epoch 8/10
11/11 - 2s - loss: 238.5285 - loglik: -2.3655e+02 - logprior: -1.9796e+00
Epoch 9/10
11/11 - 2s - loss: 238.6635 - loglik: -2.3667e+02 - logprior: -1.9904e+00
Fitted a model with MAP estimate = -238.1115
expansions: [(8, 1), (9, 3), (10, 1), (12, 2), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (64, 2), (65, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 252.7862 - loglik: -2.3852e+02 - logprior: -1.4265e+01
Epoch 2/2
11/11 - 2s - loss: 228.5016 - loglik: -2.2261e+02 - logprior: -5.8888e+00
Fitted a model with MAP estimate = -224.8118
expansions: [(0, 23)]
discards: [  0  74  82 107]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 8s - loss: 226.7629 - loglik: -2.1927e+02 - logprior: -7.4945e+00
Epoch 2/2
22/22 - 3s - loss: 216.2181 - loglik: -2.1455e+02 - logprior: -1.6725e+00
Fitted a model with MAP estimate = -211.4263
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 233.7408 - loglik: -2.2014e+02 - logprior: -1.3600e+01
Epoch 2/10
11/11 - 2s - loss: 222.6693 - loglik: -2.1892e+02 - logprior: -3.7487e+00
Epoch 3/10
11/11 - 2s - loss: 217.8010 - loglik: -2.1649e+02 - logprior: -1.3091e+00
Epoch 4/10
11/11 - 2s - loss: 215.9207 - loglik: -2.1531e+02 - logprior: -6.1402e-01
Epoch 5/10
11/11 - 2s - loss: 213.9650 - loglik: -2.1359e+02 - logprior: -3.7696e-01
Epoch 6/10
11/11 - 2s - loss: 213.7835 - loglik: -2.1349e+02 - logprior: -2.9460e-01
Epoch 7/10
11/11 - 2s - loss: 214.3437 - loglik: -2.1405e+02 - logprior: -2.9813e-01
Fitted a model with MAP estimate = -212.3922
Time for alignment: 66.9035
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.9604 - loglik: -3.5066e+02 - logprior: -1.2305e+01
Epoch 2/10
11/11 - 2s - loss: 308.6571 - loglik: -3.0571e+02 - logprior: -2.9453e+00
Epoch 3/10
11/11 - 2s - loss: 268.6861 - loglik: -2.6677e+02 - logprior: -1.9116e+00
Epoch 4/10
11/11 - 2s - loss: 250.0629 - loglik: -2.4809e+02 - logprior: -1.9681e+00
Epoch 5/10
11/11 - 2s - loss: 242.9762 - loglik: -2.4089e+02 - logprior: -2.0838e+00
Epoch 6/10
11/11 - 2s - loss: 241.7346 - loglik: -2.3955e+02 - logprior: -2.1839e+00
Epoch 7/10
11/11 - 2s - loss: 238.0236 - loglik: -2.3593e+02 - logprior: -2.0903e+00
Epoch 8/10
11/11 - 2s - loss: 238.9624 - loglik: -2.3693e+02 - logprior: -2.0336e+00
Fitted a model with MAP estimate = -237.4138
expansions: [(8, 2), (9, 3), (10, 2), (11, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (65, 1), (66, 1), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 253.2446 - loglik: -2.3910e+02 - logprior: -1.4149e+01
Epoch 2/2
11/11 - 2s - loss: 227.7035 - loglik: -2.2185e+02 - logprior: -5.8509e+00
Fitted a model with MAP estimate = -224.0867
expansions: [(0, 22)]
discards: [  0   8  76  85 108]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 224.9824 - loglik: -2.1757e+02 - logprior: -7.4162e+00
Epoch 2/2
22/22 - 3s - loss: 213.5241 - loglik: -2.1198e+02 - logprior: -1.5460e+00
Fitted a model with MAP estimate = -210.3251
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 232.9397 - loglik: -2.1967e+02 - logprior: -1.3267e+01
Epoch 2/10
11/11 - 2s - loss: 220.8770 - loglik: -2.1752e+02 - logprior: -3.3618e+00
Epoch 3/10
11/11 - 2s - loss: 215.3233 - loglik: -2.1417e+02 - logprior: -1.1521e+00
Epoch 4/10
11/11 - 2s - loss: 215.3269 - loglik: -2.1481e+02 - logprior: -5.2144e-01
Fitted a model with MAP estimate = -214.2351
Time for alignment: 57.0202
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 363.6683 - loglik: -3.5136e+02 - logprior: -1.2305e+01
Epoch 2/10
11/11 - 2s - loss: 308.3477 - loglik: -3.0540e+02 - logprior: -2.9430e+00
Epoch 3/10
11/11 - 2s - loss: 267.8699 - loglik: -2.6596e+02 - logprior: -1.9131e+00
Epoch 4/10
11/11 - 2s - loss: 249.7017 - loglik: -2.4776e+02 - logprior: -1.9408e+00
Epoch 5/10
11/11 - 2s - loss: 243.6610 - loglik: -2.4161e+02 - logprior: -2.0480e+00
Epoch 6/10
11/11 - 2s - loss: 240.3005 - loglik: -2.3815e+02 - logprior: -2.1521e+00
Epoch 7/10
11/11 - 2s - loss: 239.5578 - loglik: -2.3749e+02 - logprior: -2.0705e+00
Epoch 8/10
11/11 - 2s - loss: 237.2618 - loglik: -2.3524e+02 - logprior: -2.0242e+00
Epoch 9/10
11/11 - 2s - loss: 237.0960 - loglik: -2.3504e+02 - logprior: -2.0527e+00
Epoch 10/10
11/11 - 2s - loss: 235.7907 - loglik: -2.3372e+02 - logprior: -2.0711e+00
Fitted a model with MAP estimate = -236.1193
expansions: [(8, 2), (9, 4), (10, 2), (11, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (65, 1), (66, 1), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 252.3168 - loglik: -2.3814e+02 - logprior: -1.4179e+01
Epoch 2/2
11/11 - 2s - loss: 229.8964 - loglik: -2.2400e+02 - logprior: -5.8988e+00
Fitted a model with MAP estimate = -224.2376
expansions: [(0, 23)]
discards: [  0   8  11  77  86 109]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 226.5323 - loglik: -2.1910e+02 - logprior: -7.4287e+00
Epoch 2/2
22/22 - 3s - loss: 211.6343 - loglik: -2.1006e+02 - logprior: -1.5782e+00
Fitted a model with MAP estimate = -210.0855
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 234.5375 - loglik: -2.2119e+02 - logprior: -1.3352e+01
Epoch 2/10
11/11 - 2s - loss: 219.6002 - loglik: -2.1613e+02 - logprior: -3.4654e+00
Epoch 3/10
11/11 - 2s - loss: 216.3109 - loglik: -2.1512e+02 - logprior: -1.1922e+00
Epoch 4/10
11/11 - 2s - loss: 214.1954 - loglik: -2.1366e+02 - logprior: -5.3762e-01
Epoch 5/10
11/11 - 2s - loss: 214.3331 - loglik: -2.1403e+02 - logprior: -3.0120e-01
Fitted a model with MAP estimate = -213.0949
Time for alignment: 62.7883
Computed alignments with likelihoods: ['-210.8957', '-210.9176', '-211.4263', '-210.3251', '-210.0855']
Best model has likelihood: -210.0855  (prior= -1.3056 )
time for generating output: 0.2484
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.6790697674418604
Training of 5 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9e908cd60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e4b33a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feada7ec430>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 759.2123 - loglik: -7.5756e+02 - logprior: -1.6550e+00
Epoch 2/10
39/39 - 16s - loss: 644.0827 - loglik: -6.4261e+02 - logprior: -1.4775e+00
Epoch 3/10
39/39 - 16s - loss: 632.6621 - loglik: -6.3106e+02 - logprior: -1.6064e+00
Epoch 4/10
39/39 - 16s - loss: 630.3610 - loglik: -6.2875e+02 - logprior: -1.6069e+00
Epoch 5/10
39/39 - 16s - loss: 629.5681 - loglik: -6.2797e+02 - logprior: -1.6029e+00
Epoch 6/10
39/39 - 16s - loss: 629.2339 - loglik: -6.2763e+02 - logprior: -1.6028e+00
Epoch 7/10
39/39 - 16s - loss: 629.3773 - loglik: -6.2778e+02 - logprior: -1.6007e+00
Fitted a model with MAP estimate = -584.9489
expansions: [(12, 2), (14, 1), (16, 1), (20, 1), (36, 1), (37, 1), (40, 1), (45, 3), (46, 1), (59, 3), (60, 2), (62, 4), (65, 1), (66, 2), (67, 2), (68, 2), (90, 1), (122, 1), (124, 10), (125, 2), (129, 1), (133, 1), (137, 1), (138, 1), (139, 1), (149, 1), (151, 1), (157, 2), (164, 1), (167, 3), (168, 5), (169, 2), (180, 2), (181, 1), (193, 1), (194, 1), (206, 1), (209, 1), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [  0 154 155]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 617.5481 - loglik: -6.1487e+02 - logprior: -2.6774e+00
Epoch 2/2
39/39 - 23s - loss: 601.1565 - loglik: -6.0018e+02 - logprior: -9.7990e-01
Fitted a model with MAP estimate = -557.1969
expansions: [(0, 2), (158, 2), (226, 1)]
discards: [  0  73  74  75  87  93 217 239 279]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 602.0244 - loglik: -6.0037e+02 - logprior: -1.6499e+00
Epoch 2/2
39/39 - 23s - loss: 598.3558 - loglik: -5.9775e+02 - logprior: -6.0780e-01
Fitted a model with MAP estimate = -555.6973
expansions: []
discards: [  0 153]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 30s - loss: 556.9835 - loglik: -5.5525e+02 - logprior: -1.7344e+00
Epoch 2/10
45/45 - 26s - loss: 553.1910 - loglik: -5.5261e+02 - logprior: -5.7777e-01
Epoch 3/10
45/45 - 26s - loss: 551.1354 - loglik: -5.5060e+02 - logprior: -5.3713e-01
Epoch 4/10
45/45 - 26s - loss: 548.1100 - loglik: -5.4760e+02 - logprior: -5.1204e-01
Epoch 5/10
45/45 - 26s - loss: 549.8848 - loglik: -5.4947e+02 - logprior: -4.1102e-01
Fitted a model with MAP estimate = -549.0075
Time for alignment: 478.6205
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 757.4774 - loglik: -7.5582e+02 - logprior: -1.6574e+00
Epoch 2/10
39/39 - 16s - loss: 640.3433 - loglik: -6.3877e+02 - logprior: -1.5752e+00
Epoch 3/10
39/39 - 16s - loss: 628.2883 - loglik: -6.2653e+02 - logprior: -1.7562e+00
Epoch 4/10
39/39 - 16s - loss: 626.2836 - loglik: -6.2455e+02 - logprior: -1.7347e+00
Epoch 5/10
39/39 - 16s - loss: 625.8000 - loglik: -6.2408e+02 - logprior: -1.7239e+00
Epoch 6/10
39/39 - 16s - loss: 625.5660 - loglik: -6.2385e+02 - logprior: -1.7154e+00
Epoch 7/10
39/39 - 16s - loss: 624.7923 - loglik: -6.2307e+02 - logprior: -1.7235e+00
Epoch 8/10
39/39 - 16s - loss: 624.3595 - loglik: -6.2264e+02 - logprior: -1.7146e+00
Epoch 9/10
39/39 - 16s - loss: 624.2696 - loglik: -6.2255e+02 - logprior: -1.7204e+00
Epoch 10/10
39/39 - 16s - loss: 624.1249 - loglik: -6.2241e+02 - logprior: -1.7180e+00
Fitted a model with MAP estimate = -579.2104
expansions: [(12, 2), (13, 1), (14, 1), (16, 1), (17, 1), (19, 1), (37, 1), (39, 1), (45, 3), (46, 1), (60, 2), (62, 4), (63, 1), (66, 1), (68, 1), (69, 1), (70, 1), (98, 1), (125, 5), (126, 2), (128, 1), (139, 2), (140, 1), (142, 1), (143, 1), (144, 1), (145, 1), (147, 1), (148, 1), (160, 5), (161, 1), (164, 1), (165, 1), (166, 2), (167, 2), (168, 3), (169, 2), (170, 2), (174, 1), (180, 2), (191, 1), (193, 1), (203, 1), (206, 1), (211, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 614.6364 - loglik: -6.1183e+02 - logprior: -2.8065e+00
Epoch 2/2
39/39 - 24s - loss: 598.9763 - loglik: -5.9793e+02 - logprior: -1.0500e+00
Fitted a model with MAP estimate = -555.5926
expansions: [(0, 2)]
discards: [  0  12 154 155 201 202 203 204 205 206 207 208 209 210 215 241 279 282]
Re-initialized the encoder parameters.
Fitting a model of length 282 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 605.5604 - loglik: -6.0400e+02 - logprior: -1.5635e+00
Epoch 2/2
39/39 - 22s - loss: 600.9259 - loglik: -6.0040e+02 - logprior: -5.2962e-01
Fitted a model with MAP estimate = -557.0805
expansions: [(201, 2), (202, 2), (203, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 558.0085 - loglik: -5.5634e+02 - logprior: -1.6724e+00
Epoch 2/10
45/45 - 25s - loss: 553.2842 - loglik: -5.5278e+02 - logprior: -5.0226e-01
Epoch 3/10
45/45 - 26s - loss: 551.8608 - loglik: -5.5138e+02 - logprior: -4.7890e-01
Epoch 4/10
45/45 - 26s - loss: 548.8285 - loglik: -5.4832e+02 - logprior: -5.0794e-01
Epoch 5/10
45/45 - 26s - loss: 550.5038 - loglik: -5.5013e+02 - logprior: -3.7389e-01
Fitted a model with MAP estimate = -549.5511
Time for alignment: 520.1445
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 755.4699 - loglik: -7.5382e+02 - logprior: -1.6523e+00
Epoch 2/10
39/39 - 16s - loss: 638.8751 - loglik: -6.3736e+02 - logprior: -1.5102e+00
Epoch 3/10
39/39 - 16s - loss: 629.6517 - loglik: -6.2804e+02 - logprior: -1.6083e+00
Epoch 4/10
39/39 - 16s - loss: 627.8109 - loglik: -6.2625e+02 - logprior: -1.5611e+00
Epoch 5/10
39/39 - 16s - loss: 626.2990 - loglik: -6.2472e+02 - logprior: -1.5768e+00
Epoch 6/10
39/39 - 16s - loss: 626.1403 - loglik: -6.2457e+02 - logprior: -1.5664e+00
Epoch 7/10
39/39 - 16s - loss: 625.9883 - loglik: -6.2442e+02 - logprior: -1.5634e+00
Epoch 8/10
39/39 - 16s - loss: 625.7935 - loglik: -6.2423e+02 - logprior: -1.5645e+00
Epoch 9/10
39/39 - 16s - loss: 626.0096 - loglik: -6.2445e+02 - logprior: -1.5630e+00
Fitted a model with MAP estimate = -580.8332
expansions: [(12, 2), (14, 1), (16, 1), (17, 1), (19, 1), (37, 1), (39, 1), (45, 3), (46, 1), (59, 1), (60, 2), (62, 4), (67, 2), (68, 2), (69, 2), (98, 1), (101, 1), (122, 1), (125, 5), (126, 2), (127, 2), (148, 1), (149, 1), (162, 1), (163, 1), (164, 6), (165, 1), (166, 3), (167, 1), (168, 4), (169, 3), (170, 2), (181, 2), (182, 1), (191, 1), (193, 1), (203, 1), (210, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 616.7399 - loglik: -6.1396e+02 - logprior: -2.7821e+00
Epoch 2/2
39/39 - 24s - loss: 599.9207 - loglik: -5.9879e+02 - logprior: -1.1272e+00
Fitted a model with MAP estimate = -556.4400
expansions: [(0, 2), (208, 2), (222, 1)]
discards: [  0  76  86  91 156 157 204 205 206 210 241 281 282]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 603.9034 - loglik: -6.0223e+02 - logprior: -1.6715e+00
Epoch 2/2
39/39 - 23s - loss: 599.1088 - loglik: -5.9846e+02 - logprior: -6.4860e-01
Fitted a model with MAP estimate = -556.8186
expansions: []
discards: [  0 200 201 205]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 558.8094 - loglik: -5.5709e+02 - logprior: -1.7187e+00
Epoch 2/10
45/45 - 26s - loss: 554.3847 - loglik: -5.5381e+02 - logprior: -5.7182e-01
Epoch 3/10
45/45 - 25s - loss: 551.8388 - loglik: -5.5123e+02 - logprior: -6.1225e-01
Epoch 4/10
45/45 - 26s - loss: 552.2809 - loglik: -5.5176e+02 - logprior: -5.2073e-01
Fitted a model with MAP estimate = -550.7711
Time for alignment: 482.3257
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 759.7300 - loglik: -7.5808e+02 - logprior: -1.6476e+00
Epoch 2/10
39/39 - 16s - loss: 644.0593 - loglik: -6.4250e+02 - logprior: -1.5583e+00
Epoch 3/10
39/39 - 16s - loss: 632.3318 - loglik: -6.3059e+02 - logprior: -1.7395e+00
Epoch 4/10
39/39 - 16s - loss: 629.8010 - loglik: -6.2810e+02 - logprior: -1.7027e+00
Epoch 5/10
39/39 - 16s - loss: 629.1134 - loglik: -6.2744e+02 - logprior: -1.6730e+00
Epoch 6/10
39/39 - 16s - loss: 629.0150 - loglik: -6.2735e+02 - logprior: -1.6692e+00
Epoch 7/10
39/39 - 16s - loss: 628.8668 - loglik: -6.2720e+02 - logprior: -1.6654e+00
Epoch 8/10
39/39 - 16s - loss: 628.7330 - loglik: -6.2707e+02 - logprior: -1.6674e+00
Epoch 9/10
39/39 - 16s - loss: 628.5074 - loglik: -6.2684e+02 - logprior: -1.6696e+00
Epoch 10/10
39/39 - 16s - loss: 628.8597 - loglik: -6.2718e+02 - logprior: -1.6747e+00
Fitted a model with MAP estimate = -583.2787
expansions: [(12, 2), (13, 1), (14, 2), (16, 1), (17, 1), (23, 1), (37, 1), (39, 1), (45, 3), (46, 1), (59, 2), (60, 2), (62, 5), (63, 1), (66, 1), (68, 1), (69, 1), (70, 1), (126, 3), (127, 2), (128, 2), (133, 1), (137, 3), (138, 3), (139, 7), (140, 3), (142, 1), (143, 1), (158, 2), (163, 1), (164, 1), (167, 1), (169, 4), (170, 2), (177, 1), (182, 1), (191, 1), (193, 1), (194, 1), (206, 1), (209, 1), (210, 2), (215, 1), (217, 1), (218, 1)]
discards: [  0 155 156]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 616.0223 - loglik: -6.1333e+02 - logprior: -2.6971e+00
Epoch 2/2
39/39 - 24s - loss: 599.3669 - loglik: -5.9835e+02 - logprior: -1.0172e+00
Fitted a model with MAP estimate = -556.6041
expansions: [(0, 2), (227, 1), (230, 1)]
discards: [  0  11  12  75  79  80 158 173 177 182 279]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 601.9055 - loglik: -6.0027e+02 - logprior: -1.6397e+00
Epoch 2/2
39/39 - 23s - loss: 598.0779 - loglik: -5.9748e+02 - logprior: -5.9960e-01
Fitted a model with MAP estimate = -555.8339
expansions: []
discards: [  0 154 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 30s - loss: 557.6981 - loglik: -5.5604e+02 - logprior: -1.6563e+00
Epoch 2/10
45/45 - 25s - loss: 553.5352 - loglik: -5.5299e+02 - logprior: -5.4756e-01
Epoch 3/10
45/45 - 26s - loss: 552.9221 - loglik: -5.5244e+02 - logprior: -4.8676e-01
Epoch 4/10
45/45 - 25s - loss: 550.4626 - loglik: -5.4993e+02 - logprior: -5.3035e-01
Epoch 5/10
45/45 - 26s - loss: 549.6713 - loglik: -5.4929e+02 - logprior: -3.7988e-01
Epoch 6/10
45/45 - 25s - loss: 549.5119 - loglik: -5.4910e+02 - logprior: -4.1254e-01
Epoch 7/10
45/45 - 26s - loss: 550.7830 - loglik: -5.5056e+02 - logprior: -2.2173e-01
Fitted a model with MAP estimate = -549.8676
Time for alignment: 573.5223
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 756.3282 - loglik: -7.5469e+02 - logprior: -1.6364e+00
Epoch 2/10
39/39 - 16s - loss: 636.7479 - loglik: -6.3543e+02 - logprior: -1.3212e+00
Epoch 3/10
39/39 - 16s - loss: 626.6497 - loglik: -6.2520e+02 - logprior: -1.4541e+00
Epoch 4/10
39/39 - 16s - loss: 624.7532 - loglik: -6.2336e+02 - logprior: -1.3977e+00
Epoch 5/10
39/39 - 16s - loss: 624.6137 - loglik: -6.2323e+02 - logprior: -1.3849e+00
Epoch 6/10
39/39 - 16s - loss: 624.3137 - loglik: -6.2294e+02 - logprior: -1.3754e+00
Epoch 7/10
39/39 - 16s - loss: 623.9134 - loglik: -6.2254e+02 - logprior: -1.3778e+00
Epoch 8/10
39/39 - 16s - loss: 624.3510 - loglik: -6.2298e+02 - logprior: -1.3749e+00
Fitted a model with MAP estimate = -579.3417
expansions: [(12, 2), (14, 1), (16, 1), (17, 1), (19, 1), (37, 1), (39, 1), (42, 1), (45, 2), (46, 1), (58, 5), (59, 3), (67, 1), (68, 1), (126, 2), (127, 5), (129, 1), (137, 1), (139, 2), (163, 1), (164, 1), (165, 7), (166, 3), (167, 2), (168, 4), (169, 2), (170, 1), (180, 1), (181, 2), (182, 1), (191, 1), (193, 1), (203, 1), (210, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 615.2581 - loglik: -6.1261e+02 - logprior: -2.6452e+00
Epoch 2/2
39/39 - 23s - loss: 600.2818 - loglik: -5.9932e+02 - logprior: -9.6442e-01
Fitted a model with MAP estimate = -556.2652
expansions: [(0, 2), (72, 1), (148, 1), (212, 1), (222, 1)]
discards: [  0 151 152 153 204 235 274 276]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 602.3422 - loglik: -6.0070e+02 - logprior: -1.6438e+00
Epoch 2/2
39/39 - 23s - loss: 598.7786 - loglik: -5.9819e+02 - logprior: -5.8468e-01
Fitted a model with MAP estimate = -555.4200
expansions: []
discards: [  0 199 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 286 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 28s - loss: 558.4832 - loglik: -5.5683e+02 - logprior: -1.6563e+00
Epoch 2/10
45/45 - 25s - loss: 553.6864 - loglik: -5.5314e+02 - logprior: -5.4198e-01
Epoch 3/10
45/45 - 26s - loss: 552.9858 - loglik: -5.5243e+02 - logprior: -5.5314e-01
Epoch 4/10
45/45 - 25s - loss: 551.3806 - loglik: -5.5088e+02 - logprior: -4.9902e-01
Epoch 5/10
45/45 - 26s - loss: 549.8453 - loglik: -5.4939e+02 - logprior: -4.5986e-01
Epoch 6/10
45/45 - 25s - loss: 551.5844 - loglik: -5.5124e+02 - logprior: -3.4577e-01
Fitted a model with MAP estimate = -550.3218
Time for alignment: 511.3346
Computed alignments with likelihoods: ['-549.0075', '-549.5511', '-550.7711', '-549.8676', '-550.3218']
Best model has likelihood: -549.0075  (prior= -0.5331 )
time for generating output: 0.2553
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.8937479215164615
Training of 5 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feae3531880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2c2515e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feada4f4310>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 446.2516 - loglik: -4.4522e+02 - logprior: -1.0298e+00
Epoch 2/10
30/30 - 7s - loss: 375.4775 - loglik: -3.7437e+02 - logprior: -1.1104e+00
Epoch 3/10
30/30 - 7s - loss: 365.2465 - loglik: -3.6416e+02 - logprior: -1.0868e+00
Epoch 4/10
30/30 - 6s - loss: 362.8468 - loglik: -3.6176e+02 - logprior: -1.0844e+00
Epoch 5/10
30/30 - 6s - loss: 362.8578 - loglik: -3.6180e+02 - logprior: -1.0593e+00
Fitted a model with MAP estimate = -350.3178
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 1), (73, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 353.6288 - loglik: -3.5253e+02 - logprior: -1.1010e+00
Epoch 2/2
61/61 - 11s - loss: 347.3950 - loglik: -3.4658e+02 - logprior: -8.1420e-01
Fitted a model with MAP estimate = -336.2632
expansions: []
discards: [ 25  48  51  93 101 132 146 150 153]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 348.5591 - loglik: -3.4755e+02 - logprior: -1.0120e+00
Epoch 2/2
61/61 - 10s - loss: 347.2707 - loglik: -3.4653e+02 - logprior: -7.4239e-01
Fitted a model with MAP estimate = -336.5780
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 334.7606 - loglik: -3.3410e+02 - logprior: -6.6469e-01
Epoch 2/10
87/87 - 14s - loss: 334.7225 - loglik: -3.3416e+02 - logprior: -5.6152e-01
Epoch 3/10
87/87 - 14s - loss: 333.3166 - loglik: -3.3277e+02 - logprior: -5.4449e-01
Epoch 4/10
87/87 - 14s - loss: 333.6451 - loglik: -3.3311e+02 - logprior: -5.3286e-01
Fitted a model with MAP estimate = -332.9572
Time for alignment: 256.5400
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 446.0970 - loglik: -4.4507e+02 - logprior: -1.0315e+00
Epoch 2/10
30/30 - 6s - loss: 376.4903 - loglik: -3.7540e+02 - logprior: -1.0947e+00
Epoch 3/10
30/30 - 7s - loss: 364.6102 - loglik: -3.6352e+02 - logprior: -1.0890e+00
Epoch 4/10
30/30 - 6s - loss: 362.7639 - loglik: -3.6167e+02 - logprior: -1.0891e+00
Epoch 5/10
30/30 - 7s - loss: 362.6911 - loglik: -3.6163e+02 - logprior: -1.0629e+00
Epoch 6/10
30/30 - 6s - loss: 362.1468 - loglik: -3.6110e+02 - logprior: -1.0483e+00
Epoch 7/10
30/30 - 7s - loss: 361.8638 - loglik: -3.6083e+02 - logprior: -1.0321e+00
Epoch 8/10
30/30 - 7s - loss: 361.9402 - loglik: -3.6091e+02 - logprior: -1.0318e+00
Fitted a model with MAP estimate = -349.9338
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (88, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 354.4232 - loglik: -3.5330e+02 - logprior: -1.1213e+00
Epoch 2/2
61/61 - 11s - loss: 346.8358 - loglik: -3.4602e+02 - logprior: -8.1995e-01
Fitted a model with MAP estimate = -336.1394
expansions: []
discards: [ 25  48  51  57  93  95 103 134 152 155]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 348.4602 - loglik: -3.4744e+02 - logprior: -1.0159e+00
Epoch 2/2
61/61 - 10s - loss: 347.3197 - loglik: -3.4659e+02 - logprior: -7.3220e-01
Fitted a model with MAP estimate = -336.5476
expansions: []
discards: [141]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 18s - loss: 335.1360 - loglik: -3.3448e+02 - logprior: -6.5748e-01
Epoch 2/10
87/87 - 14s - loss: 334.4086 - loglik: -3.3386e+02 - logprior: -5.4731e-01
Epoch 3/10
87/87 - 14s - loss: 333.4168 - loglik: -3.3288e+02 - logprior: -5.3993e-01
Epoch 4/10
87/87 - 14s - loss: 333.3929 - loglik: -3.3287e+02 - logprior: -5.2521e-01
Epoch 5/10
87/87 - 14s - loss: 333.2242 - loglik: -3.3271e+02 - logprior: -5.1203e-01
Epoch 6/10
87/87 - 14s - loss: 332.4146 - loglik: -3.3192e+02 - logprior: -4.9901e-01
Epoch 7/10
87/87 - 14s - loss: 332.7701 - loglik: -3.3228e+02 - logprior: -4.8554e-01
Fitted a model with MAP estimate = -332.6200
Time for alignment: 319.6689
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 446.0725 - loglik: -4.4504e+02 - logprior: -1.0323e+00
Epoch 2/10
30/30 - 6s - loss: 375.5083 - loglik: -3.7441e+02 - logprior: -1.0952e+00
Epoch 3/10
30/30 - 6s - loss: 365.0107 - loglik: -3.6392e+02 - logprior: -1.0860e+00
Epoch 4/10
30/30 - 6s - loss: 362.9098 - loglik: -3.6183e+02 - logprior: -1.0809e+00
Epoch 5/10
30/30 - 7s - loss: 362.7824 - loglik: -3.6172e+02 - logprior: -1.0578e+00
Epoch 6/10
30/30 - 7s - loss: 362.2437 - loglik: -3.6120e+02 - logprior: -1.0428e+00
Epoch 7/10
30/30 - 6s - loss: 362.3907 - loglik: -3.6136e+02 - logprior: -1.0333e+00
Fitted a model with MAP estimate = -350.0725
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (88, 1), (92, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 353.8789 - loglik: -3.5278e+02 - logprior: -1.1022e+00
Epoch 2/2
61/61 - 11s - loss: 347.3562 - loglik: -3.4655e+02 - logprior: -8.0846e-01
Fitted a model with MAP estimate = -336.1612
expansions: []
discards: [ 25  48  51  92  94 102 133 148 151 154]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 348.6696 - loglik: -3.4766e+02 - logprior: -1.0060e+00
Epoch 2/2
61/61 - 10s - loss: 347.2608 - loglik: -3.4653e+02 - logprior: -7.2929e-01
Fitted a model with MAP estimate = -336.4297
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 334.7205 - loglik: -3.3406e+02 - logprior: -6.5861e-01
Epoch 2/10
87/87 - 14s - loss: 334.5783 - loglik: -3.3403e+02 - logprior: -5.5221e-01
Epoch 3/10
87/87 - 14s - loss: 333.4355 - loglik: -3.3290e+02 - logprior: -5.3414e-01
Epoch 4/10
87/87 - 14s - loss: 333.5276 - loglik: -3.3300e+02 - logprior: -5.2746e-01
Fitted a model with MAP estimate = -333.1218
Time for alignment: 270.5466
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 445.7373 - loglik: -4.4470e+02 - logprior: -1.0329e+00
Epoch 2/10
30/30 - 6s - loss: 375.6596 - loglik: -3.7456e+02 - logprior: -1.0988e+00
Epoch 3/10
30/30 - 6s - loss: 364.8078 - loglik: -3.6372e+02 - logprior: -1.0841e+00
Epoch 4/10
30/30 - 6s - loss: 363.7399 - loglik: -3.6266e+02 - logprior: -1.0754e+00
Epoch 5/10
30/30 - 6s - loss: 362.8737 - loglik: -3.6182e+02 - logprior: -1.0510e+00
Epoch 6/10
30/30 - 6s - loss: 362.3739 - loglik: -3.6133e+02 - logprior: -1.0416e+00
Epoch 7/10
30/30 - 7s - loss: 362.4214 - loglik: -3.6139e+02 - logprior: -1.0351e+00
Fitted a model with MAP estimate = -350.2340
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 1), (73, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 353.2762 - loglik: -3.5218e+02 - logprior: -1.1003e+00
Epoch 2/2
61/61 - 11s - loss: 347.9460 - loglik: -3.4713e+02 - logprior: -8.1445e-01
Fitted a model with MAP estimate = -336.3288
expansions: []
discards: [ 25  48  51  93 101 146 149 153]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 348.8457 - loglik: -3.4783e+02 - logprior: -1.0127e+00
Epoch 2/2
61/61 - 10s - loss: 346.9110 - loglik: -3.4617e+02 - logprior: -7.4099e-01
Fitted a model with MAP estimate = -336.2241
expansions: []
discards: [128]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 334.7546 - loglik: -3.3409e+02 - logprior: -6.6465e-01
Epoch 2/10
87/87 - 14s - loss: 334.6211 - loglik: -3.3406e+02 - logprior: -5.5691e-01
Epoch 3/10
87/87 - 14s - loss: 333.4115 - loglik: -3.3287e+02 - logprior: -5.4334e-01
Epoch 4/10
87/87 - 14s - loss: 333.3369 - loglik: -3.3280e+02 - logprior: -5.3520e-01
Epoch 5/10
87/87 - 14s - loss: 333.2329 - loglik: -3.3272e+02 - logprior: -5.1800e-01
Epoch 6/10
87/87 - 14s - loss: 332.6412 - loglik: -3.3213e+02 - logprior: -5.0854e-01
Epoch 7/10
87/87 - 14s - loss: 332.4506 - loglik: -3.3196e+02 - logprior: -4.9299e-01
Epoch 8/10
87/87 - 14s - loss: 333.0345 - loglik: -3.3256e+02 - logprior: -4.7901e-01
Fitted a model with MAP estimate = -332.5740
Time for alignment: 325.2496
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 446.5228 - loglik: -4.4549e+02 - logprior: -1.0318e+00
Epoch 2/10
30/30 - 7s - loss: 375.2134 - loglik: -3.7411e+02 - logprior: -1.1066e+00
Epoch 3/10
30/30 - 6s - loss: 364.8878 - loglik: -3.6380e+02 - logprior: -1.0906e+00
Epoch 4/10
30/30 - 6s - loss: 363.6464 - loglik: -3.6256e+02 - logprior: -1.0830e+00
Epoch 5/10
30/30 - 7s - loss: 363.0126 - loglik: -3.6195e+02 - logprior: -1.0605e+00
Epoch 6/10
30/30 - 7s - loss: 362.7963 - loglik: -3.6175e+02 - logprior: -1.0461e+00
Epoch 7/10
30/30 - 6s - loss: 362.6369 - loglik: -3.6160e+02 - logprior: -1.0374e+00
Epoch 8/10
30/30 - 7s - loss: 362.2021 - loglik: -3.6117e+02 - logprior: -1.0300e+00
Epoch 9/10
30/30 - 7s - loss: 362.3600 - loglik: -3.6133e+02 - logprior: -1.0300e+00
Fitted a model with MAP estimate = -350.2700
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (74, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 15s - loss: 354.5185 - loglik: -3.5341e+02 - logprior: -1.1111e+00
Epoch 2/2
61/61 - 11s - loss: 346.6142 - loglik: -3.4580e+02 - logprior: -8.1721e-01
Fitted a model with MAP estimate = -336.0688
expansions: []
discards: [ 25  48  51  92  95 102 133 148 154]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 348.5216 - loglik: -3.4750e+02 - logprior: -1.0183e+00
Epoch 2/2
61/61 - 10s - loss: 347.4108 - loglik: -3.4666e+02 - logprior: -7.4643e-01
Fitted a model with MAP estimate = -336.3818
expansions: []
discards: [142]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 16s - loss: 335.2731 - loglik: -3.3461e+02 - logprior: -6.6106e-01
Epoch 2/10
87/87 - 14s - loss: 334.5287 - loglik: -3.3397e+02 - logprior: -5.5651e-01
Epoch 3/10
87/87 - 14s - loss: 333.1298 - loglik: -3.3259e+02 - logprior: -5.3574e-01
Epoch 4/10
87/87 - 14s - loss: 333.5315 - loglik: -3.3300e+02 - logprior: -5.3133e-01
Fitted a model with MAP estimate = -332.9819
Time for alignment: 282.4235
Computed alignments with likelihoods: ['-332.9572', '-332.6200', '-333.1218', '-332.5740', '-332.9819']
Best model has likelihood: -332.5740  (prior= -0.4668 )
time for generating output: 0.2513
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.684053651266766
Training of 5 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f2a1640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1692e2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe98b060a60>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 1245.9368 - loglik: -1.2449e+03 - logprior: -1.0569e+00
Epoch 2/10
40/40 - 39s - loss: 1147.2943 - loglik: -1.1475e+03 - logprior: 0.2083
Epoch 3/10
40/40 - 39s - loss: 1141.8657 - loglik: -1.1421e+03 - logprior: 0.2732
Epoch 4/10
40/40 - 39s - loss: 1140.7708 - loglik: -1.1411e+03 - logprior: 0.2935
Epoch 5/10
40/40 - 39s - loss: 1140.2272 - loglik: -1.1405e+03 - logprior: 0.2887
Epoch 6/10
40/40 - 39s - loss: 1139.6741 - loglik: -1.1400e+03 - logprior: 0.2921
Epoch 7/10
40/40 - 39s - loss: 1139.7844 - loglik: -1.1401e+03 - logprior: 0.2871
Fitted a model with MAP estimate = -847.4598
expansions: [(117, 2), (125, 2), (127, 1), (138, 1), (297, 2), (298, 3), (299, 1), (324, 14), (330, 102)]
discards: [  1 190]
Re-initialized the encoder parameters.
Fitting a model of length 456 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 71s - loss: 1065.8845 - loglik: -1.0652e+03 - logprior: -6.9811e-01
Epoch 2/2
80/80 - 67s - loss: 1039.4183 - loglik: -1.0394e+03 - logprior: -1.9590e-02
Fitted a model with MAP estimate = -776.4460
expansions: [(443, 1), (456, 2)]
discards: [117 128 230 231 302 304 338 341]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 69s - loss: 1042.7029 - loglik: -1.0421e+03 - logprior: -6.0889e-01
Epoch 2/2
80/80 - 66s - loss: 1039.1609 - loglik: -1.0391e+03 - logprior: -3.5938e-02
Fitted a model with MAP estimate = -777.9442
expansions: [(228, 1)]
discards: [382 383 434 449 450]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 92s - loss: 770.6446 - loglik: -7.7022e+02 - logprior: -4.2174e-01
Epoch 2/10
56/56 - 89s - loss: 767.0138 - loglik: -7.6687e+02 - logprior: -1.4211e-01
Epoch 3/10
56/56 - 89s - loss: 766.9403 - loglik: -7.6695e+02 - logprior: 0.0058
Epoch 4/10
56/56 - 88s - loss: 768.1625 - loglik: -7.6796e+02 - logprior: -2.0220e-01
Fitted a model with MAP estimate = -764.1650
Time for alignment: 1326.4979
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 1244.9615 - loglik: -1.2440e+03 - logprior: -1.0096e+00
Epoch 2/10
40/40 - 39s - loss: 1145.4822 - loglik: -1.1456e+03 - logprior: 0.1546
Epoch 3/10
40/40 - 39s - loss: 1140.4072 - loglik: -1.1406e+03 - logprior: 0.2186
Epoch 4/10
40/40 - 39s - loss: 1139.3038 - loglik: -1.1395e+03 - logprior: 0.2422
Epoch 5/10
40/40 - 39s - loss: 1138.7452 - loglik: -1.1390e+03 - logprior: 0.2599
Epoch 6/10
40/40 - 39s - loss: 1138.3475 - loglik: -1.1386e+03 - logprior: 0.2322
Epoch 7/10
40/40 - 39s - loss: 1138.6150 - loglik: -1.1389e+03 - logprior: 0.2565
Fitted a model with MAP estimate = -845.5966
expansions: [(62, 1), (71, 1), (117, 2), (125, 2), (126, 1), (172, 1), (296, 2), (297, 6), (298, 4), (317, 1), (318, 8), (319, 4), (329, 10), (330, 89)]
discards: [  1 189 225]
Re-initialized the encoder parameters.
Fitting a model of length 459 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 73s - loss: 1071.3313 - loglik: -1.0703e+03 - logprior: -1.0550e+00
Epoch 2/2
80/80 - 68s - loss: 1044.0310 - loglik: -1.0437e+03 - logprior: -3.7979e-01
Fitted a model with MAP estimate = -779.7396
expansions: [(384, 2), (386, 7), (388, 1), (389, 1), (405, 1), (407, 1), (415, 1), (429, 2)]
discards: [119 129 305 306 338 341 438 439 440 441 442 443 444 445 446 447 448 449
 450 451 452 453 458]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 70s - loss: 1044.1708 - loglik: -1.0436e+03 - logprior: -5.5737e-01
Epoch 2/2
80/80 - 66s - loss: 1039.0521 - loglik: -1.0393e+03 - logprior: 0.2536
Fitted a model with MAP estimate = -777.6640
expansions: [(438, 1), (452, 2)]
discards: [ 71 451]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 98s - loss: 772.9932 - loglik: -7.7271e+02 - logprior: -2.8690e-01
Epoch 2/10
113/113 - 93s - loss: 766.0146 - loglik: -7.6589e+02 - logprior: -1.2130e-01
Epoch 3/10
113/113 - 93s - loss: 765.9189 - loglik: -7.6589e+02 - logprior: -3.0278e-02
Epoch 4/10
113/113 - 93s - loss: 765.9717 - loglik: -7.6607e+02 - logprior: 0.0955
Fitted a model with MAP estimate = -764.7703
Time for alignment: 1352.7853
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 1247.8099 - loglik: -1.2468e+03 - logprior: -1.0123e+00
Epoch 2/10
40/40 - 39s - loss: 1142.1670 - loglik: -1.1422e+03 - logprior: 0.0235
Epoch 3/10
40/40 - 39s - loss: 1135.9442 - loglik: -1.1360e+03 - logprior: 0.1013
Epoch 4/10
40/40 - 39s - loss: 1135.5916 - loglik: -1.1357e+03 - logprior: 0.1357
Epoch 5/10
40/40 - 39s - loss: 1134.2748 - loglik: -1.1344e+03 - logprior: 0.1302
Epoch 6/10
40/40 - 39s - loss: 1134.9768 - loglik: -1.1351e+03 - logprior: 0.1471
Fitted a model with MAP estimate = -843.1697
expansions: [(124, 1), (125, 1), (138, 1), (220, 1), (290, 1), (291, 2), (292, 2), (293, 4), (294, 3), (295, 2), (315, 2), (316, 1), (318, 4), (319, 5), (327, 23), (328, 2), (329, 7), (330, 68)]
discards: [  1 191 224]
Re-initialized the encoder parameters.
Fitting a model of length 457 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 71s - loss: 1070.4088 - loglik: -1.0693e+03 - logprior: -1.1011e+00
Epoch 2/2
80/80 - 68s - loss: 1045.9576 - loglik: -1.0455e+03 - logprior: -4.1824e-01
Fitted a model with MAP estimate = -780.3052
expansions: [(361, 2), (362, 1), (381, 4), (382, 3), (406, 1), (410, 1)]
discards: [294 300 301 302 331 339 341 429 430 431 432 433 434 435 436 437 438 439
 440 455 456]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 67s - loss: 1044.8060 - loglik: -1.0437e+03 - logprior: -1.1469e+00
Epoch 2/2
40/40 - 64s - loss: 1038.3148 - loglik: -1.0386e+03 - logprior: 0.2556
Fitted a model with MAP estimate = -776.6967
expansions: [(448, 2)]
discards: [382 430 431]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 92s - loss: 772.3128 - loglik: -7.7180e+02 - logprior: -5.1455e-01
Epoch 2/10
56/56 - 89s - loss: 768.0294 - loglik: -7.6765e+02 - logprior: -3.7474e-01
Epoch 3/10
56/56 - 88s - loss: 764.9174 - loglik: -7.6470e+02 - logprior: -2.1897e-01
Epoch 4/10
56/56 - 89s - loss: 765.0738 - loglik: -7.6470e+02 - logprior: -3.7688e-01
Fitted a model with MAP estimate = -764.2933
Time for alignment: 1281.5803
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 1246.9913 - loglik: -1.2460e+03 - logprior: -1.0369e+00
Epoch 2/10
40/40 - 39s - loss: 1145.4329 - loglik: -1.1455e+03 - logprior: 0.0265
Epoch 3/10
40/40 - 39s - loss: 1139.9998 - loglik: -1.1401e+03 - logprior: 0.1394
Epoch 4/10
40/40 - 39s - loss: 1139.1647 - loglik: -1.1393e+03 - logprior: 0.1698
Epoch 5/10
40/40 - 39s - loss: 1138.1058 - loglik: -1.1383e+03 - logprior: 0.1864
Epoch 6/10
40/40 - 39s - loss: 1138.6631 - loglik: -1.1388e+03 - logprior: 0.1498
Fitted a model with MAP estimate = -845.1506
expansions: [(25, 1), (118, 1), (123, 1), (124, 1), (137, 1), (291, 3), (292, 2), (293, 7), (294, 5), (318, 4), (319, 5), (329, 10), (330, 88)]
discards: [  1 212 213 214 239]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 70s - loss: 1063.7706 - loglik: -1.0630e+03 - logprior: -7.9045e-01
Epoch 2/2
80/80 - 67s - loss: 1040.4551 - loglik: -1.0404e+03 - logprior: -6.6155e-03
Fitted a model with MAP estimate = -777.3616
expansions: [(441, 1), (454, 2)]
discards: [293 296 307 336 341 389 390 391 437 438]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 67s - loss: 1043.8979 - loglik: -1.0429e+03 - logprior: -9.9226e-01
Epoch 2/2
40/40 - 64s - loss: 1039.4543 - loglik: -1.0394e+03 - logprior: -5.7799e-02
Fitted a model with MAP estimate = -777.0085
expansions: [(433, 1)]
discards: [445 446]
Re-initialized the encoder parameters.
Fitting a model of length 446 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 92s - loss: 770.0916 - loglik: -7.6954e+02 - logprior: -5.5338e-01
Epoch 2/10
56/56 - 88s - loss: 769.6378 - loglik: -7.6950e+02 - logprior: -1.3325e-01
Epoch 3/10
56/56 - 88s - loss: 766.2289 - loglik: -7.6614e+02 - logprior: -9.3587e-02
Epoch 4/10
56/56 - 88s - loss: 763.4246 - loglik: -7.6332e+02 - logprior: -1.0556e-01
Epoch 5/10
56/56 - 88s - loss: 764.8951 - loglik: -7.6473e+02 - logprior: -1.6905e-01
Fitted a model with MAP estimate = -763.9131
Time for alignment: 1365.5432
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 1250.2308 - loglik: -1.2491e+03 - logprior: -1.0865e+00
Epoch 2/10
40/40 - 39s - loss: 1135.7227 - loglik: -1.1358e+03 - logprior: 0.0667
Epoch 3/10
40/40 - 39s - loss: 1130.1396 - loglik: -1.1303e+03 - logprior: 0.1410
Epoch 4/10
40/40 - 39s - loss: 1128.3102 - loglik: -1.1285e+03 - logprior: 0.1681
Epoch 5/10
40/40 - 39s - loss: 1125.6582 - loglik: -1.1258e+03 - logprior: 0.1608
Epoch 6/10
40/40 - 39s - loss: 1125.1812 - loglik: -1.1253e+03 - logprior: 0.1667
Epoch 7/10
40/40 - 39s - loss: 1125.1371 - loglik: -1.1253e+03 - logprior: 0.1629
Epoch 8/10
40/40 - 39s - loss: 1124.4725 - loglik: -1.1246e+03 - logprior: 0.1562
Epoch 9/10
40/40 - 39s - loss: 1124.9635 - loglik: -1.1251e+03 - logprior: 0.1730
Fitted a model with MAP estimate = -837.6933
expansions: [(25, 1), (118, 2), (123, 1), (124, 1), (242, 1), (289, 13), (312, 38), (313, 1), (317, 2), (318, 8), (320, 1), (329, 1), (330, 48)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 67s - loss: 1074.0284 - loglik: -1.0726e+03 - logprior: -1.4168e+00
Epoch 2/2
40/40 - 64s - loss: 1042.0173 - loglik: -1.0418e+03 - logprior: -2.3631e-01
Fitted a model with MAP estimate = -777.3698
expansions: [(345, 1), (350, 1), (351, 1), (353, 1), (435, 1), (447, 2)]
discards: [118 378]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 71s - loss: 1043.3564 - loglik: -1.0427e+03 - logprior: -6.5921e-01
Epoch 2/2
80/80 - 66s - loss: 1039.4158 - loglik: -1.0394e+03 - logprior: -6.3966e-03
Fitted a model with MAP estimate = -778.4268
expansions: [(164, 1)]
discards: [382 432 433 450 451]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 92s - loss: 770.7256 - loglik: -7.7021e+02 - logprior: -5.1696e-01
Epoch 2/10
56/56 - 89s - loss: 768.2977 - loglik: -7.6821e+02 - logprior: -8.6605e-02
Epoch 3/10
56/56 - 89s - loss: 767.6149 - loglik: -7.6733e+02 - logprior: -2.8785e-01
Epoch 4/10
56/56 - 89s - loss: 765.6349 - loglik: -7.6564e+02 - logprior: 0.0057
Epoch 5/10
56/56 - 89s - loss: 765.8545 - loglik: -7.6594e+02 - logprior: 0.0821
Fitted a model with MAP estimate = -763.9444
Time for alignment: 1483.1309
Computed alignments with likelihoods: ['-764.1650', '-764.7703', '-764.2933', '-763.9131', '-763.9444']
Best model has likelihood: -763.9131  (prior= 0.0605 )
time for generating output: 0.4284
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.7566111453835082
Training of 5 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2c9cac40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac8ecb400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feae33ab970>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 449.3855 - loglik: -4.4651e+02 - logprior: -2.8706e+00
Epoch 2/10
19/19 - 4s - loss: 282.1745 - loglik: -2.8058e+02 - logprior: -1.5968e+00
Epoch 3/10
19/19 - 4s - loss: 215.6366 - loglik: -2.1348e+02 - logprior: -2.1556e+00
Epoch 4/10
19/19 - 4s - loss: 206.8488 - loglik: -2.0456e+02 - logprior: -2.2855e+00
Epoch 5/10
19/19 - 4s - loss: 200.7071 - loglik: -1.9851e+02 - logprior: -2.2013e+00
Epoch 6/10
19/19 - 4s - loss: 198.0853 - loglik: -1.9592e+02 - logprior: -2.1664e+00
Epoch 7/10
19/19 - 4s - loss: 194.9022 - loglik: -1.9276e+02 - logprior: -2.1437e+00
Epoch 8/10
19/19 - 4s - loss: 193.5538 - loglik: -1.9143e+02 - logprior: -2.1226e+00
Epoch 9/10
19/19 - 4s - loss: 192.1729 - loglik: -1.9005e+02 - logprior: -2.1212e+00
Epoch 10/10
19/19 - 4s - loss: 191.7248 - loglik: -1.8960e+02 - logprior: -2.1217e+00
Fitted a model with MAP estimate = -187.2263
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (62, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (104, 1), (114, 1), (120, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 181.3076 - loglik: -1.7820e+02 - logprior: -3.1041e+00
Epoch 2/2
19/19 - 5s - loss: 144.4212 - loglik: -1.4311e+02 - logprior: -1.3063e+00
Fitted a model with MAP estimate = -142.9963
expansions: []
discards: [ 75 148 152 157]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 146.0561 - loglik: -1.4304e+02 - logprior: -3.0115e+00
Epoch 2/2
19/19 - 5s - loss: 141.5085 - loglik: -1.4042e+02 - logprior: -1.0927e+00
Fitted a model with MAP estimate = -142.4287
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 143.6711 - loglik: -1.4149e+02 - logprior: -2.1861e+00
Epoch 2/10
22/22 - 6s - loss: 140.7190 - loglik: -1.3969e+02 - logprior: -1.0274e+00
Epoch 3/10
22/22 - 6s - loss: 138.2293 - loglik: -1.3726e+02 - logprior: -9.7246e-01
Epoch 4/10
22/22 - 6s - loss: 135.1724 - loglik: -1.3424e+02 - logprior: -9.3088e-01
Epoch 5/10
22/22 - 6s - loss: 132.7535 - loglik: -1.3179e+02 - logprior: -9.5921e-01
Epoch 6/10
22/22 - 6s - loss: 130.0635 - loglik: -1.2914e+02 - logprior: -9.2362e-01
Epoch 7/10
22/22 - 6s - loss: 129.0229 - loglik: -1.2814e+02 - logprior: -8.8472e-01
Epoch 8/10
22/22 - 6s - loss: 128.4633 - loglik: -1.2762e+02 - logprior: -8.4067e-01
Epoch 9/10
22/22 - 6s - loss: 127.3960 - loglik: -1.2657e+02 - logprior: -8.2548e-01
Epoch 10/10
22/22 - 6s - loss: 129.6975 - loglik: -1.2891e+02 - logprior: -7.9124e-01
Fitted a model with MAP estimate = -128.0146
Time for alignment: 170.0204
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.4530 - loglik: -4.4657e+02 - logprior: -2.8798e+00
Epoch 2/10
19/19 - 4s - loss: 280.4688 - loglik: -2.7888e+02 - logprior: -1.5863e+00
Epoch 3/10
19/19 - 4s - loss: 214.4022 - loglik: -2.1223e+02 - logprior: -2.1709e+00
Epoch 4/10
19/19 - 4s - loss: 205.3108 - loglik: -2.0302e+02 - logprior: -2.2905e+00
Epoch 5/10
19/19 - 4s - loss: 199.3617 - loglik: -1.9717e+02 - logprior: -2.1894e+00
Epoch 6/10
19/19 - 4s - loss: 197.8279 - loglik: -1.9567e+02 - logprior: -2.1584e+00
Epoch 7/10
19/19 - 4s - loss: 194.2956 - loglik: -1.9217e+02 - logprior: -2.1277e+00
Epoch 8/10
19/19 - 4s - loss: 193.3244 - loglik: -1.9121e+02 - logprior: -2.1165e+00
Epoch 9/10
19/19 - 4s - loss: 191.4057 - loglik: -1.8928e+02 - logprior: -2.1216e+00
Epoch 10/10
19/19 - 4s - loss: 192.7706 - loglik: -1.9065e+02 - logprior: -2.1200e+00
Fitted a model with MAP estimate = -186.6971
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (102, 1), (104, 1), (114, 1), (121, 1), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 180.6195 - loglik: -1.7792e+02 - logprior: -2.6993e+00
Epoch 2/2
19/19 - 5s - loss: 144.8102 - loglik: -1.4402e+02 - logprior: -7.8763e-01
Fitted a model with MAP estimate = -143.5561
expansions: []
discards: [ 75 151 156]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 146.0444 - loglik: -1.4343e+02 - logprior: -2.6133e+00
Epoch 2/2
19/19 - 5s - loss: 142.6237 - loglik: -1.4196e+02 - logprior: -6.6223e-01
Fitted a model with MAP estimate = -143.0481
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 144.7037 - loglik: -1.4285e+02 - logprior: -1.8489e+00
Epoch 2/10
22/22 - 6s - loss: 140.7304 - loglik: -1.3988e+02 - logprior: -8.5157e-01
Epoch 3/10
22/22 - 6s - loss: 136.7789 - loglik: -1.3585e+02 - logprior: -9.3228e-01
Epoch 4/10
22/22 - 6s - loss: 137.7760 - loglik: -1.3684e+02 - logprior: -9.4057e-01
Fitted a model with MAP estimate = -133.1381
Time for alignment: 136.7624
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 448.8307 - loglik: -4.4596e+02 - logprior: -2.8719e+00
Epoch 2/10
19/19 - 4s - loss: 280.0956 - loglik: -2.7850e+02 - logprior: -1.5937e+00
Epoch 3/10
19/19 - 4s - loss: 215.3246 - loglik: -2.1316e+02 - logprior: -2.1649e+00
Epoch 4/10
19/19 - 4s - loss: 206.1029 - loglik: -2.0383e+02 - logprior: -2.2701e+00
Epoch 5/10
19/19 - 4s - loss: 200.8598 - loglik: -1.9869e+02 - logprior: -2.1683e+00
Epoch 6/10
19/19 - 4s - loss: 198.3361 - loglik: -1.9619e+02 - logprior: -2.1422e+00
Epoch 7/10
19/19 - 4s - loss: 195.4588 - loglik: -1.9335e+02 - logprior: -2.1098e+00
Epoch 8/10
19/19 - 4s - loss: 193.2971 - loglik: -1.9119e+02 - logprior: -2.1029e+00
Epoch 9/10
19/19 - 4s - loss: 193.7022 - loglik: -1.9160e+02 - logprior: -2.1022e+00
Fitted a model with MAP estimate = -187.5761
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (104, 1), (114, 1), (122, 3), (125, 1), (126, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 179.5531 - loglik: -1.7648e+02 - logprior: -3.0712e+00
Epoch 2/2
19/19 - 5s - loss: 144.6936 - loglik: -1.4345e+02 - logprior: -1.2452e+00
Fitted a model with MAP estimate = -143.1926
expansions: []
discards: [ 75 151]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 146.2582 - loglik: -1.4323e+02 - logprior: -3.0254e+00
Epoch 2/2
19/19 - 5s - loss: 141.1740 - loglik: -1.4007e+02 - logprior: -1.1004e+00
Fitted a model with MAP estimate = -142.8781
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 144.6453 - loglik: -1.4249e+02 - logprior: -2.1559e+00
Epoch 2/10
22/22 - 6s - loss: 140.1957 - loglik: -1.3924e+02 - logprior: -9.5127e-01
Epoch 3/10
22/22 - 6s - loss: 139.1028 - loglik: -1.3814e+02 - logprior: -9.6578e-01
Epoch 4/10
22/22 - 6s - loss: 134.2369 - loglik: -1.3330e+02 - logprior: -9.3594e-01
Epoch 5/10
22/22 - 6s - loss: 132.4003 - loglik: -1.3145e+02 - logprior: -9.5207e-01
Epoch 6/10
22/22 - 6s - loss: 129.8432 - loglik: -1.2892e+02 - logprior: -9.1932e-01
Epoch 7/10
22/22 - 6s - loss: 129.0772 - loglik: -1.2819e+02 - logprior: -8.8544e-01
Epoch 8/10
22/22 - 6s - loss: 128.9379 - loglik: -1.2809e+02 - logprior: -8.5040e-01
Epoch 9/10
22/22 - 6s - loss: 128.2064 - loglik: -1.2739e+02 - logprior: -8.1560e-01
Epoch 10/10
22/22 - 6s - loss: 128.3100 - loglik: -1.2752e+02 - logprior: -7.8872e-01
Fitted a model with MAP estimate = -128.0340
Time for alignment: 164.6844
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 449.0724 - loglik: -4.4620e+02 - logprior: -2.8684e+00
Epoch 2/10
19/19 - 4s - loss: 282.3258 - loglik: -2.8072e+02 - logprior: -1.6037e+00
Epoch 3/10
19/19 - 4s - loss: 214.7940 - loglik: -2.1263e+02 - logprior: -2.1652e+00
Epoch 4/10
19/19 - 4s - loss: 205.1200 - loglik: -2.0283e+02 - logprior: -2.2898e+00
Epoch 5/10
19/19 - 4s - loss: 200.9645 - loglik: -1.9875e+02 - logprior: -2.2109e+00
Epoch 6/10
19/19 - 4s - loss: 196.2503 - loglik: -1.9406e+02 - logprior: -2.1891e+00
Epoch 7/10
19/19 - 4s - loss: 194.2800 - loglik: -1.9212e+02 - logprior: -2.1561e+00
Epoch 8/10
19/19 - 4s - loss: 193.8716 - loglik: -1.9172e+02 - logprior: -2.1487e+00
Epoch 9/10
19/19 - 4s - loss: 192.6786 - loglik: -1.9053e+02 - logprior: -2.1451e+00
Epoch 10/10
19/19 - 4s - loss: 191.3268 - loglik: -1.8918e+02 - logprior: -2.1458e+00
Fitted a model with MAP estimate = -186.7320
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 1), (61, 1), (62, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (104, 1), (114, 1), (122, 3), (125, 1), (126, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 179.5283 - loglik: -1.7646e+02 - logprior: -3.0672e+00
Epoch 2/2
19/19 - 5s - loss: 144.5746 - loglik: -1.4336e+02 - logprior: -1.2143e+00
Fitted a model with MAP estimate = -143.0521
expansions: []
discards: [150]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 146.1111 - loglik: -1.4311e+02 - logprior: -3.0044e+00
Epoch 2/2
19/19 - 5s - loss: 140.7051 - loglik: -1.3962e+02 - logprior: -1.0828e+00
Fitted a model with MAP estimate = -142.1941
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 144.4547 - loglik: -1.4232e+02 - logprior: -2.1347e+00
Epoch 2/10
22/22 - 6s - loss: 139.3276 - loglik: -1.3836e+02 - logprior: -9.6666e-01
Epoch 3/10
22/22 - 6s - loss: 138.5499 - loglik: -1.3760e+02 - logprior: -9.5333e-01
Epoch 4/10
22/22 - 6s - loss: 135.1533 - loglik: -1.3421e+02 - logprior: -9.4582e-01
Epoch 5/10
22/22 - 6s - loss: 132.0697 - loglik: -1.3112e+02 - logprior: -9.4965e-01
Epoch 6/10
22/22 - 6s - loss: 130.3258 - loglik: -1.2942e+02 - logprior: -9.1013e-01
Epoch 7/10
22/22 - 6s - loss: 129.0591 - loglik: -1.2818e+02 - logprior: -8.8155e-01
Epoch 8/10
22/22 - 6s - loss: 127.8422 - loglik: -1.2699e+02 - logprior: -8.5137e-01
Epoch 9/10
22/22 - 6s - loss: 129.0188 - loglik: -1.2821e+02 - logprior: -8.0460e-01
Fitted a model with MAP estimate = -128.1519
Time for alignment: 162.1499
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 449.0027 - loglik: -4.4613e+02 - logprior: -2.8756e+00
Epoch 2/10
19/19 - 4s - loss: 281.2802 - loglik: -2.7968e+02 - logprior: -1.5967e+00
Epoch 3/10
19/19 - 4s - loss: 215.4287 - loglik: -2.1328e+02 - logprior: -2.1507e+00
Epoch 4/10
19/19 - 4s - loss: 205.8851 - loglik: -2.0362e+02 - logprior: -2.2647e+00
Epoch 5/10
19/19 - 4s - loss: 199.5259 - loglik: -1.9733e+02 - logprior: -2.2003e+00
Epoch 6/10
19/19 - 4s - loss: 196.7125 - loglik: -1.9454e+02 - logprior: -2.1703e+00
Epoch 7/10
19/19 - 4s - loss: 194.3159 - loglik: -1.9218e+02 - logprior: -2.1330e+00
Epoch 8/10
19/19 - 4s - loss: 192.9460 - loglik: -1.9082e+02 - logprior: -2.1274e+00
Epoch 9/10
19/19 - 4s - loss: 191.8088 - loglik: -1.8968e+02 - logprior: -2.1241e+00
Epoch 10/10
19/19 - 4s - loss: 191.3957 - loglik: -1.8928e+02 - logprior: -2.1175e+00
Fitted a model with MAP estimate = -186.8647
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (62, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (104, 1), (114, 1), (121, 1), (122, 2), (125, 1), (126, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 180.5020 - loglik: -1.7742e+02 - logprior: -3.0858e+00
Epoch 2/2
19/19 - 5s - loss: 144.0310 - loglik: -1.4274e+02 - logprior: -1.2960e+00
Fitted a model with MAP estimate = -142.8634
expansions: []
discards: [ 50  76 152]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 145.8648 - loglik: -1.4284e+02 - logprior: -3.0232e+00
Epoch 2/2
19/19 - 5s - loss: 141.8250 - loglik: -1.4071e+02 - logprior: -1.1130e+00
Fitted a model with MAP estimate = -142.3552
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 144.3229 - loglik: -1.4213e+02 - logprior: -2.1887e+00
Epoch 2/10
22/22 - 6s - loss: 139.5849 - loglik: -1.3858e+02 - logprior: -1.0041e+00
Epoch 3/10
22/22 - 6s - loss: 139.8779 - loglik: -1.3889e+02 - logprior: -9.8689e-01
Fitted a model with MAP estimate = -136.1844
Time for alignment: 127.3782
Computed alignments with likelihoods: ['-128.0146', '-133.1381', '-128.0340', '-128.1519', '-136.1844']
Best model has likelihood: -128.0146  (prior= -0.7518 )
time for generating output: 0.1836
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 5 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb490be0a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2caafd00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaf4bfee50>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 606.4276 - loglik: -5.7430e+02 - logprior: -3.2124e+01
Epoch 2/10
12/12 - 3s - loss: 525.2679 - loglik: -5.2187e+02 - logprior: -3.3952e+00
Epoch 3/10
12/12 - 3s - loss: 461.0652 - loglik: -4.6039e+02 - logprior: -6.7237e-01
Epoch 4/10
12/12 - 3s - loss: 435.9539 - loglik: -4.3590e+02 - logprior: -5.5961e-02
Epoch 5/10
12/12 - 3s - loss: 428.3147 - loglik: -4.2877e+02 - logprior: 0.4563
Epoch 6/10
12/12 - 3s - loss: 423.6996 - loglik: -4.2444e+02 - logprior: 0.7397
Epoch 7/10
12/12 - 3s - loss: 422.1551 - loglik: -4.2295e+02 - logprior: 0.7914
Epoch 8/10
12/12 - 3s - loss: 420.9630 - loglik: -4.2177e+02 - logprior: 0.8082
Epoch 9/10
12/12 - 3s - loss: 419.5431 - loglik: -4.2045e+02 - logprior: 0.9091
Epoch 10/10
12/12 - 3s - loss: 418.6529 - loglik: -4.1963e+02 - logprior: 0.9753
Fitted a model with MAP estimate = -418.8474
expansions: [(0, 4), (11, 3), (19, 1), (31, 1), (32, 2), (33, 1), (42, 1), (43, 1), (59, 1), (60, 2), (61, 2), (62, 2), (76, 2), (77, 2), (86, 1), (87, 1), (89, 1), (90, 1), (92, 1), (100, 1), (101, 1), (118, 1), (126, 6), (129, 1), (130, 1), (137, 1), (145, 1), (146, 3), (148, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 455.4939 - loglik: -4.1532e+02 - logprior: -4.0169e+01
Epoch 2/2
12/12 - 4s - loss: 411.0782 - loglik: -4.0478e+02 - logprior: -6.2989e+00
Fitted a model with MAP estimate = -403.1496
expansions: []
discards: [  0   1   2   3   5   6   7  39  75  97 133 160 161 191 192 193 223]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 438.2976 - loglik: -4.0839e+02 - logprior: -2.9903e+01
Epoch 2/2
12/12 - 3s - loss: 406.4681 - loglik: -4.0463e+02 - logprior: -1.8388e+00
Fitted a model with MAP estimate = -402.4918
expansions: [(0, 5), (177, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 227 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 443.3481 - loglik: -4.0464e+02 - logprior: -3.8711e+01
Epoch 2/10
12/12 - 4s - loss: 406.2931 - loglik: -4.0195e+02 - logprior: -4.3432e+00
Epoch 3/10
12/12 - 4s - loss: 397.9999 - loglik: -4.0093e+02 - logprior: 2.9325
Epoch 4/10
12/12 - 4s - loss: 395.2005 - loglik: -4.0084e+02 - logprior: 5.6409
Epoch 5/10
12/12 - 4s - loss: 394.0754 - loglik: -4.0094e+02 - logprior: 6.8628
Epoch 6/10
12/12 - 4s - loss: 394.1379 - loglik: -4.0157e+02 - logprior: 7.4346
Fitted a model with MAP estimate = -392.3733
Time for alignment: 87.5756
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 606.5283 - loglik: -5.7440e+02 - logprior: -3.2123e+01
Epoch 2/10
12/12 - 3s - loss: 522.3148 - loglik: -5.1893e+02 - logprior: -3.3853e+00
Epoch 3/10
12/12 - 3s - loss: 461.3434 - loglik: -4.6079e+02 - logprior: -5.5386e-01
Epoch 4/10
12/12 - 3s - loss: 434.8443 - loglik: -4.3509e+02 - logprior: 0.2485
Epoch 5/10
12/12 - 3s - loss: 428.5811 - loglik: -4.2936e+02 - logprior: 0.7746
Epoch 6/10
12/12 - 3s - loss: 424.7250 - loglik: -4.2583e+02 - logprior: 1.1035
Epoch 7/10
12/12 - 3s - loss: 422.9346 - loglik: -4.2411e+02 - logprior: 1.1742
Epoch 8/10
12/12 - 3s - loss: 421.4121 - loglik: -4.2282e+02 - logprior: 1.4086
Epoch 9/10
12/12 - 3s - loss: 420.6897 - loglik: -4.2227e+02 - logprior: 1.5798
Epoch 10/10
12/12 - 3s - loss: 419.1809 - loglik: -4.2081e+02 - logprior: 1.6245
Fitted a model with MAP estimate = -419.5892
expansions: [(0, 4), (10, 1), (20, 1), (31, 2), (32, 1), (45, 1), (48, 1), (60, 2), (61, 1), (62, 1), (63, 1), (75, 2), (76, 2), (77, 2), (86, 1), (87, 1), (90, 2), (101, 1), (102, 1), (127, 2), (129, 3), (130, 4), (131, 1), (145, 1), (146, 3), (151, 1), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 457.9350 - loglik: -4.1784e+02 - logprior: -4.0099e+01
Epoch 2/2
12/12 - 4s - loss: 412.4373 - loglik: -4.0635e+02 - logprior: -6.0826e+00
Fitted a model with MAP estimate = -406.1642
expansions: [(187, 1)]
discards: [  1   2 163 164 165 188 189 219]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 437.5992 - loglik: -4.0959e+02 - logprior: -2.8004e+01
Epoch 2/2
12/12 - 3s - loss: 407.2734 - loglik: -4.0574e+02 - logprior: -1.5305e+00
Fitted a model with MAP estimate = -403.7800
expansions: [(182, 2)]
discards: [89 93]
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 433.6856 - loglik: -4.0690e+02 - logprior: -2.6783e+01
Epoch 2/10
12/12 - 3s - loss: 406.6047 - loglik: -4.0554e+02 - logprior: -1.0654e+00
Epoch 3/10
12/12 - 4s - loss: 400.2062 - loglik: -4.0395e+02 - logprior: 3.7420
Epoch 4/10
12/12 - 4s - loss: 397.5182 - loglik: -4.0327e+02 - logprior: 5.7481
Epoch 5/10
12/12 - 4s - loss: 398.7463 - loglik: -4.0549e+02 - logprior: 6.7414
Fitted a model with MAP estimate = -396.3584
Time for alignment: 82.5258
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 607.6481 - loglik: -5.7552e+02 - logprior: -3.2124e+01
Epoch 2/10
12/12 - 3s - loss: 522.9082 - loglik: -5.1952e+02 - logprior: -3.3928e+00
Epoch 3/10
12/12 - 3s - loss: 464.7227 - loglik: -4.6400e+02 - logprior: -7.2704e-01
Epoch 4/10
12/12 - 3s - loss: 436.1523 - loglik: -4.3599e+02 - logprior: -1.6182e-01
Epoch 5/10
12/12 - 3s - loss: 425.1705 - loglik: -4.2554e+02 - logprior: 0.3692
Epoch 6/10
12/12 - 3s - loss: 424.6519 - loglik: -4.2534e+02 - logprior: 0.6877
Epoch 7/10
12/12 - 3s - loss: 420.8890 - loglik: -4.2168e+02 - logprior: 0.7888
Epoch 8/10
12/12 - 3s - loss: 420.3640 - loglik: -4.2126e+02 - logprior: 0.8930
Epoch 9/10
12/12 - 3s - loss: 419.8769 - loglik: -4.2088e+02 - logprior: 1.0021
Epoch 10/10
12/12 - 3s - loss: 419.2520 - loglik: -4.2034e+02 - logprior: 1.0848
Fitted a model with MAP estimate = -419.3188
expansions: [(11, 3), (19, 1), (31, 1), (32, 2), (40, 1), (42, 1), (45, 1), (61, 2), (62, 1), (63, 1), (64, 1), (76, 3), (77, 2), (78, 1), (86, 1), (90, 1), (91, 2), (101, 1), (102, 1), (121, 1), (127, 1), (129, 2), (130, 4), (137, 1), (145, 1), (146, 3), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 445.8211 - loglik: -4.1739e+02 - logprior: -2.8428e+01
Epoch 2/2
12/12 - 4s - loss: 407.0912 - loglik: -4.0463e+02 - logprior: -2.4575e+00
Fitted a model with MAP estimate = -401.6404
expansions: [(106, 1), (184, 1)]
discards: [ 35 113 159 160 161 185 186 216]
Re-initialized the encoder parameters.
Fitting a model of length 224 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 432.3779 - loglik: -4.0522e+02 - logprior: -2.7162e+01
Epoch 2/2
12/12 - 3s - loss: 404.3186 - loglik: -4.0297e+02 - logprior: -1.3489e+00
Fitted a model with MAP estimate = -399.8510
expansions: []
discards: [70]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 430.4920 - loglik: -4.0389e+02 - logprior: -2.6604e+01
Epoch 2/10
12/12 - 3s - loss: 401.8875 - loglik: -4.0108e+02 - logprior: -8.0814e-01
Epoch 3/10
12/12 - 3s - loss: 396.4696 - loglik: -4.0045e+02 - logprior: 3.9808
Epoch 4/10
12/12 - 3s - loss: 393.9185 - loglik: -3.9987e+02 - logprior: 5.9495
Epoch 5/10
12/12 - 3s - loss: 394.9753 - loglik: -4.0196e+02 - logprior: 6.9815
Fitted a model with MAP estimate = -392.9082
Time for alignment: 84.6492
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 606.9315 - loglik: -5.7481e+02 - logprior: -3.2121e+01
Epoch 2/10
12/12 - 3s - loss: 524.1743 - loglik: -5.2079e+02 - logprior: -3.3806e+00
Epoch 3/10
12/12 - 3s - loss: 464.8382 - loglik: -4.6424e+02 - logprior: -5.9759e-01
Epoch 4/10
12/12 - 3s - loss: 435.1361 - loglik: -4.3531e+02 - logprior: 0.1697
Epoch 5/10
12/12 - 3s - loss: 426.1954 - loglik: -4.2680e+02 - logprior: 0.6064
Epoch 6/10
12/12 - 3s - loss: 423.2209 - loglik: -4.2416e+02 - logprior: 0.9418
Epoch 7/10
12/12 - 3s - loss: 420.6665 - loglik: -4.2170e+02 - logprior: 1.0314
Epoch 8/10
12/12 - 3s - loss: 420.2488 - loglik: -4.2130e+02 - logprior: 1.0544
Epoch 9/10
12/12 - 3s - loss: 418.8635 - loglik: -4.2000e+02 - logprior: 1.1350
Epoch 10/10
12/12 - 3s - loss: 419.3835 - loglik: -4.2059e+02 - logprior: 1.2081
Fitted a model with MAP estimate = -418.4051
expansions: [(0, 3), (11, 3), (19, 1), (31, 1), (32, 2), (33, 1), (42, 1), (46, 1), (59, 1), (62, 2), (63, 2), (75, 2), (76, 2), (77, 2), (86, 1), (87, 1), (89, 1), (90, 1), (101, 1), (102, 2), (121, 2), (126, 4), (127, 1), (129, 2), (130, 1), (137, 1), (145, 1), (146, 3), (148, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 455.3517 - loglik: -4.1524e+02 - logprior: -4.0111e+01
Epoch 2/2
12/12 - 4s - loss: 409.3269 - loglik: -4.0306e+02 - logprior: -6.2688e+00
Fitted a model with MAP estimate = -401.7926
expansions: [(190, 1)]
discards: [  0   1   2   4   5  38 131 151 158 159 191 192 193 223]
Re-initialized the encoder parameters.
Fitting a model of length 224 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 437.1425 - loglik: -4.0736e+02 - logprior: -2.9784e+01
Epoch 2/2
12/12 - 3s - loss: 405.2672 - loglik: -4.0351e+02 - logprior: -1.7610e+00
Fitted a model with MAP estimate = -401.0706
expansions: [(1, 1), (180, 2)]
discards: [86 90]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 430.6515 - loglik: -4.0405e+02 - logprior: -2.6602e+01
Epoch 2/10
12/12 - 3s - loss: 403.7809 - loglik: -4.0287e+02 - logprior: -9.1514e-01
Epoch 3/10
12/12 - 3s - loss: 396.4517 - loglik: -4.0033e+02 - logprior: 3.8767
Epoch 4/10
12/12 - 4s - loss: 393.8012 - loglik: -3.9967e+02 - logprior: 5.8688
Epoch 5/10
12/12 - 3s - loss: 395.5139 - loglik: -4.0222e+02 - logprior: 6.7013
Fitted a model with MAP estimate = -392.6445
Time for alignment: 82.7931
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 607.2392 - loglik: -5.7511e+02 - logprior: -3.2125e+01
Epoch 2/10
12/12 - 3s - loss: 523.3140 - loglik: -5.1995e+02 - logprior: -3.3597e+00
Epoch 3/10
12/12 - 3s - loss: 456.7121 - loglik: -4.5621e+02 - logprior: -5.0679e-01
Epoch 4/10
12/12 - 3s - loss: 433.6519 - loglik: -4.3380e+02 - logprior: 0.1505
Epoch 5/10
12/12 - 3s - loss: 425.0225 - loglik: -4.2578e+02 - logprior: 0.7592
Epoch 6/10
12/12 - 3s - loss: 421.7365 - loglik: -4.2291e+02 - logprior: 1.1730
Epoch 7/10
12/12 - 3s - loss: 419.9546 - loglik: -4.2126e+02 - logprior: 1.3014
Epoch 8/10
12/12 - 3s - loss: 419.0937 - loglik: -4.2054e+02 - logprior: 1.4491
Epoch 9/10
12/12 - 3s - loss: 418.6658 - loglik: -4.2025e+02 - logprior: 1.5821
Epoch 10/10
12/12 - 3s - loss: 418.3213 - loglik: -4.1997e+02 - logprior: 1.6461
Fitted a model with MAP estimate = -418.0829
expansions: [(11, 3), (19, 1), (31, 1), (32, 2), (33, 1), (47, 1), (60, 1), (61, 2), (62, 1), (63, 1), (64, 1), (76, 3), (77, 2), (86, 1), (87, 1), (89, 1), (90, 1), (101, 1), (102, 1), (127, 2), (129, 7), (146, 3), (148, 2), (153, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 445.9684 - loglik: -4.1757e+02 - logprior: -2.8394e+01
Epoch 2/2
12/12 - 4s - loss: 407.7104 - loglik: -4.0521e+02 - logprior: -2.4961e+00
Fitted a model with MAP estimate = -402.9087
expansions: [(94, 1)]
discards: [ 35  71 157 158 159 185 215]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 432.7540 - loglik: -4.0551e+02 - logprior: -2.7243e+01
Epoch 2/2
12/12 - 3s - loss: 403.3451 - loglik: -4.0196e+02 - logprior: -1.3901e+00
Fitted a model with MAP estimate = -400.1491
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 223 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 428.7237 - loglik: -4.0211e+02 - logprior: -2.6611e+01
Epoch 2/10
12/12 - 3s - loss: 403.6193 - loglik: -4.0280e+02 - logprior: -8.2144e-01
Epoch 3/10
12/12 - 3s - loss: 398.3245 - loglik: -4.0235e+02 - logprior: 4.0262
Epoch 4/10
12/12 - 3s - loss: 395.3324 - loglik: -4.0131e+02 - logprior: 5.9809
Epoch 5/10
12/12 - 3s - loss: 394.3313 - loglik: -4.0133e+02 - logprior: 7.0031
Epoch 6/10
12/12 - 3s - loss: 391.9696 - loglik: -3.9953e+02 - logprior: 7.5564
Epoch 7/10
12/12 - 3s - loss: 393.5800 - loglik: -4.0153e+02 - logprior: 7.9459
Fitted a model with MAP estimate = -392.2108
Time for alignment: 88.5154
Computed alignments with likelihoods: ['-392.3733', '-396.3584', '-392.9082', '-392.6445', '-392.2108']
Best model has likelihood: -392.2108  (prior= 8.1805 )
time for generating output: 0.2206
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.9393774636400707
Training of 5 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9d7c05490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1622550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e772700>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 603.5106 - loglik: -5.9624e+02 - logprior: -7.2726e+00
Epoch 2/10
21/21 - 6s - loss: 480.2247 - loglik: -4.7854e+02 - logprior: -1.6851e+00
Epoch 3/10
21/21 - 6s - loss: 446.4622 - loglik: -4.4432e+02 - logprior: -2.1385e+00
Epoch 4/10
21/21 - 6s - loss: 438.2000 - loglik: -4.3625e+02 - logprior: -1.9549e+00
Epoch 5/10
21/21 - 6s - loss: 435.2413 - loglik: -4.3332e+02 - logprior: -1.9187e+00
Epoch 6/10
21/21 - 6s - loss: 433.5396 - loglik: -4.3162e+02 - logprior: -1.9147e+00
Epoch 7/10
21/21 - 6s - loss: 431.5802 - loglik: -4.2964e+02 - logprior: -1.9390e+00
Epoch 8/10
21/21 - 6s - loss: 431.8922 - loglik: -4.2993e+02 - logprior: -1.9641e+00
Fitted a model with MAP estimate = -431.4121
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (37, 1), (38, 1), (39, 2), (40, 1), (50, 1), (62, 1), (63, 3), (65, 1), (70, 1), (75, 5), (76, 2), (77, 2), (96, 1), (99, 2), (115, 1), (116, 1), (119, 1), (121, 4), (146, 1), (147, 1), (150, 1), (158, 1), (161, 1), (163, 2), (164, 1), (165, 1), (166, 1), (167, 1), (168, 2), (171, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 427.8583 - loglik: -4.1796e+02 - logprior: -9.9000e+00
Epoch 2/2
21/21 - 9s - loss: 403.2721 - loglik: -4.0006e+02 - logprior: -3.2116e+00
Fitted a model with MAP estimate = -399.0337
expansions: [(0, 5)]
discards: [  0  13  18  28 106 133 159 220]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 408.7313 - loglik: -4.0217e+02 - logprior: -6.5620e+00
Epoch 2/2
21/21 - 9s - loss: 397.0387 - loglik: -3.9696e+02 - logprior: -8.1647e-02
Fitted a model with MAP estimate = -395.2229
expansions: []
discards: [ 1  2  3  4 82]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 406.0706 - loglik: -3.9997e+02 - logprior: -6.0997e+00
Epoch 2/10
21/21 - 8s - loss: 396.9489 - loglik: -3.9755e+02 - logprior: 0.6048
Epoch 3/10
21/21 - 9s - loss: 394.8274 - loglik: -3.9623e+02 - logprior: 1.3998
Epoch 4/10
21/21 - 9s - loss: 393.7568 - loglik: -3.9546e+02 - logprior: 1.7034
Epoch 5/10
21/21 - 8s - loss: 392.3429 - loglik: -3.9418e+02 - logprior: 1.8415
Epoch 6/10
21/21 - 9s - loss: 391.0152 - loglik: -3.9299e+02 - logprior: 1.9759
Epoch 7/10
21/21 - 9s - loss: 392.2621 - loglik: -3.9439e+02 - logprior: 2.1280
Fitted a model with MAP estimate = -391.0251
Time for alignment: 189.3181
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 10s - loss: 604.7524 - loglik: -5.9747e+02 - logprior: -7.2817e+00
Epoch 2/10
21/21 - 6s - loss: 481.4102 - loglik: -4.7958e+02 - logprior: -1.8281e+00
Epoch 3/10
21/21 - 7s - loss: 444.3018 - loglik: -4.4182e+02 - logprior: -2.4794e+00
Epoch 4/10
21/21 - 6s - loss: 436.4828 - loglik: -4.3414e+02 - logprior: -2.3436e+00
Epoch 5/10
21/21 - 6s - loss: 435.4709 - loglik: -4.3318e+02 - logprior: -2.2937e+00
Epoch 6/10
21/21 - 6s - loss: 432.8970 - loglik: -4.3054e+02 - logprior: -2.3531e+00
Epoch 7/10
21/21 - 6s - loss: 434.0110 - loglik: -4.3164e+02 - logprior: -2.3732e+00
Fitted a model with MAP estimate = -432.9855
expansions: [(14, 1), (16, 1), (17, 2), (19, 2), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (39, 1), (40, 1), (46, 1), (61, 1), (65, 1), (75, 4), (76, 1), (78, 2), (80, 1), (96, 1), (99, 2), (118, 1), (119, 1), (120, 1), (121, 1), (146, 1), (147, 2), (154, 1), (160, 1), (161, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 2), (171, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 435.9253 - loglik: -4.2584e+02 - logprior: -1.0086e+01
Epoch 2/2
21/21 - 9s - loss: 415.7631 - loglik: -4.1260e+02 - logprior: -3.1678e+00
Fitted a model with MAP estimate = -410.1095
expansions: [(0, 5), (94, 1)]
discards: [  0  18  22  23  29 128 212]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 419.1079 - loglik: -4.1239e+02 - logprior: -6.7180e+00
Epoch 2/2
21/21 - 9s - loss: 408.2227 - loglik: -4.0794e+02 - logprior: -2.7998e-01
Fitted a model with MAP estimate = -405.3734
expansions: [(25, 1)]
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 415.2388 - loglik: -4.0897e+02 - logprior: -6.2661e+00
Epoch 2/10
21/21 - 8s - loss: 406.1143 - loglik: -4.0657e+02 - logprior: 0.4576
Epoch 3/10
21/21 - 8s - loss: 402.4483 - loglik: -4.0372e+02 - logprior: 1.2756
Epoch 4/10
21/21 - 8s - loss: 402.7977 - loglik: -4.0437e+02 - logprior: 1.5735
Fitted a model with MAP estimate = -401.1398
Time for alignment: 153.4393
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 604.5785 - loglik: -5.9731e+02 - logprior: -7.2724e+00
Epoch 2/10
21/21 - 6s - loss: 476.9563 - loglik: -4.7515e+02 - logprior: -1.8081e+00
Epoch 3/10
21/21 - 6s - loss: 436.3886 - loglik: -4.3392e+02 - logprior: -2.4680e+00
Epoch 4/10
21/21 - 6s - loss: 428.5421 - loglik: -4.2617e+02 - logprior: -2.3686e+00
Epoch 5/10
21/21 - 6s - loss: 426.0724 - loglik: -4.2373e+02 - logprior: -2.3395e+00
Epoch 6/10
21/21 - 6s - loss: 424.0107 - loglik: -4.2159e+02 - logprior: -2.4171e+00
Epoch 7/10
21/21 - 7s - loss: 425.1258 - loglik: -4.2269e+02 - logprior: -2.4370e+00
Fitted a model with MAP estimate = -423.9035
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (62, 1), (63, 3), (66, 1), (76, 5), (77, 2), (78, 2), (94, 1), (96, 1), (99, 2), (119, 2), (121, 1), (122, 1), (124, 1), (143, 1), (146, 1), (147, 2), (158, 1), (161, 1), (163, 2), (164, 1), (165, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 422.1490 - loglik: -4.1226e+02 - logprior: -9.8883e+00
Epoch 2/2
21/21 - 9s - loss: 396.8908 - loglik: -3.9377e+02 - logprior: -3.1247e+00
Fitted a model with MAP estimate = -393.5608
expansions: [(0, 5)]
discards: [  0  13  18  28  81 106 133 153 218]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 403.8362 - loglik: -3.9732e+02 - logprior: -6.5180e+00
Epoch 2/2
21/21 - 8s - loss: 392.4802 - loglik: -3.9242e+02 - logprior: -5.8051e-02
Fitted a model with MAP estimate = -390.4245
expansions: []
discards: [  1   2   3   4 102]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 401.8269 - loglik: -3.9570e+02 - logprior: -6.1240e+00
Epoch 2/10
21/21 - 8s - loss: 392.2467 - loglik: -3.9278e+02 - logprior: 0.5328
Epoch 3/10
21/21 - 8s - loss: 390.4033 - loglik: -3.9175e+02 - logprior: 1.3451
Epoch 4/10
21/21 - 9s - loss: 389.4724 - loglik: -3.9112e+02 - logprior: 1.6508
Epoch 5/10
21/21 - 8s - loss: 387.2058 - loglik: -3.8898e+02 - logprior: 1.7783
Epoch 6/10
21/21 - 9s - loss: 387.7538 - loglik: -3.8965e+02 - logprior: 1.8958
Fitted a model with MAP estimate = -386.7110
Time for alignment: 172.2843
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 605.9819 - loglik: -5.9870e+02 - logprior: -7.2808e+00
Epoch 2/10
21/21 - 7s - loss: 484.3045 - loglik: -4.8246e+02 - logprior: -1.8475e+00
Epoch 3/10
21/21 - 6s - loss: 444.1900 - loglik: -4.4184e+02 - logprior: -2.3488e+00
Epoch 4/10
21/21 - 7s - loss: 439.0052 - loglik: -4.3684e+02 - logprior: -2.1672e+00
Epoch 5/10
21/21 - 6s - loss: 435.8314 - loglik: -4.3370e+02 - logprior: -2.1363e+00
Epoch 6/10
21/21 - 6s - loss: 436.6794 - loglik: -4.3450e+02 - logprior: -2.1775e+00
Fitted a model with MAP estimate = -435.2129
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (39, 1), (40, 1), (46, 1), (49, 1), (59, 1), (62, 3), (65, 1), (75, 4), (76, 1), (78, 2), (80, 1), (96, 1), (99, 1), (119, 1), (121, 4), (146, 1), (147, 1), (150, 1), (161, 1), (162, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 435.0999 - loglik: -4.2514e+02 - logprior: -9.9617e+00
Epoch 2/2
21/21 - 9s - loss: 409.6808 - loglik: -4.0654e+02 - logprior: -3.1380e+00
Fitted a model with MAP estimate = -406.4257
expansions: [(0, 5), (45, 1), (95, 1)]
discards: [  0  13  18  28  84 153 214]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 413.8232 - loglik: -4.0712e+02 - logprior: -6.6983e+00
Epoch 2/2
21/21 - 9s - loss: 404.0699 - loglik: -4.0380e+02 - logprior: -2.7302e-01
Fitted a model with MAP estimate = -400.8697
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 411.5467 - loglik: -4.0521e+02 - logprior: -6.3384e+00
Epoch 2/10
21/21 - 9s - loss: 402.5541 - loglik: -4.0290e+02 - logprior: 0.3451
Epoch 3/10
21/21 - 8s - loss: 400.6823 - loglik: -4.0184e+02 - logprior: 1.1536
Epoch 4/10
21/21 - 9s - loss: 398.4997 - loglik: -3.9995e+02 - logprior: 1.4541
Epoch 5/10
21/21 - 9s - loss: 397.7919 - loglik: -3.9936e+02 - logprior: 1.5694
Epoch 6/10
21/21 - 8s - loss: 397.6831 - loglik: -3.9938e+02 - logprior: 1.6968
Epoch 7/10
21/21 - 8s - loss: 395.9918 - loglik: -3.9784e+02 - logprior: 1.8507
Epoch 8/10
21/21 - 9s - loss: 396.2718 - loglik: -3.9828e+02 - logprior: 2.0077
Fitted a model with MAP estimate = -395.8261
Time for alignment: 184.6738
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 10s - loss: 603.5070 - loglik: -5.9624e+02 - logprior: -7.2692e+00
Epoch 2/10
21/21 - 6s - loss: 486.1314 - loglik: -4.8427e+02 - logprior: -1.8598e+00
Epoch 3/10
21/21 - 7s - loss: 451.2579 - loglik: -4.4873e+02 - logprior: -2.5272e+00
Epoch 4/10
21/21 - 6s - loss: 441.9204 - loglik: -4.3951e+02 - logprior: -2.4117e+00
Epoch 5/10
21/21 - 6s - loss: 437.5547 - loglik: -4.3511e+02 - logprior: -2.4456e+00
Epoch 6/10
21/21 - 6s - loss: 438.4420 - loglik: -4.3598e+02 - logprior: -2.4611e+00
Fitted a model with MAP estimate = -437.1122
expansions: [(14, 1), (17, 2), (18, 1), (21, 2), (22, 3), (23, 1), (24, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (46, 1), (60, 1), (63, 4), (65, 1), (75, 1), (76, 4), (77, 2), (78, 2), (97, 1), (98, 1), (99, 1), (119, 1), (120, 1), (121, 1), (122, 1), (124, 1), (146, 1), (148, 1), (159, 1), (161, 1), (162, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 1), (179, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 434.7685 - loglik: -4.2492e+02 - logprior: -9.8452e+00
Epoch 2/2
21/21 - 10s - loss: 411.6822 - loglik: -4.0877e+02 - logprior: -2.9116e+00
Fitted a model with MAP estimate = -406.6758
expansions: [(0, 5), (151, 1)]
discards: [  0  13  18  28  46  82 100 107 218]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 11s - loss: 416.2216 - loglik: -4.0964e+02 - logprior: -6.5833e+00
Epoch 2/2
21/21 - 8s - loss: 402.8441 - loglik: -4.0272e+02 - logprior: -1.2538e-01
Fitted a model with MAP estimate = -401.6616
expansions: []
discards: [ 1  2  3  4 85]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 412.5054 - loglik: -4.0628e+02 - logprior: -6.2264e+00
Epoch 2/10
21/21 - 9s - loss: 403.3719 - loglik: -4.0383e+02 - logprior: 0.4617
Epoch 3/10
21/21 - 8s - loss: 402.0130 - loglik: -4.0328e+02 - logprior: 1.2653
Epoch 4/10
21/21 - 9s - loss: 399.4734 - loglik: -4.0103e+02 - logprior: 1.5545
Epoch 5/10
21/21 - 8s - loss: 398.4767 - loglik: -4.0014e+02 - logprior: 1.6657
Epoch 6/10
21/21 - 9s - loss: 398.8613 - loglik: -4.0065e+02 - logprior: 1.7847
Fitted a model with MAP estimate = -397.5270
Time for alignment: 165.7744
Computed alignments with likelihoods: ['-391.0251', '-401.1398', '-386.7110', '-395.8261', '-397.5270']
Best model has likelihood: -386.7110  (prior= 1.9987 )
time for generating output: 0.3728
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.5753153831670118
Training of 5 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feab79fa9d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe98ae257f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe993c17280>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 284.1509 - loglik: -2.8115e+02 - logprior: -3.0044e+00
Epoch 2/10
19/19 - 2s - loss: 255.2740 - loglik: -2.5442e+02 - logprior: -8.5636e-01
Epoch 3/10
19/19 - 2s - loss: 245.6475 - loglik: -2.4481e+02 - logprior: -8.4033e-01
Epoch 4/10
19/19 - 2s - loss: 242.4704 - loglik: -2.4174e+02 - logprior: -7.2997e-01
Epoch 5/10
19/19 - 2s - loss: 241.9939 - loglik: -2.4128e+02 - logprior: -7.1397e-01
Epoch 6/10
19/19 - 2s - loss: 241.1603 - loglik: -2.4045e+02 - logprior: -7.0535e-01
Epoch 7/10
19/19 - 2s - loss: 241.6321 - loglik: -2.4094e+02 - logprior: -6.9291e-01
Fitted a model with MAP estimate = -240.0240
expansions: [(0, 10), (12, 4), (15, 2), (42, 1), (54, 1), (57, 2), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 244.7524 - loglik: -2.4093e+02 - logprior: -3.8238e+00
Epoch 2/2
19/19 - 3s - loss: 239.3736 - loglik: -2.3828e+02 - logprior: -1.0901e+00
Fitted a model with MAP estimate = -237.0163
expansions: [(0, 7), (21, 2)]
discards: [ 1  2  3  4  5  6  7  8  9 22 29 76]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 243.2038 - loglik: -2.3958e+02 - logprior: -3.6252e+00
Epoch 2/2
19/19 - 3s - loss: 239.2233 - loglik: -2.3811e+02 - logprior: -1.1113e+00
Fitted a model with MAP estimate = -237.2473
expansions: [(0, 8)]
discards: [0 1 2 3 7]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 240.7600 - loglik: -2.3766e+02 - logprior: -3.1031e+00
Epoch 2/10
19/19 - 3s - loss: 237.3062 - loglik: -2.3638e+02 - logprior: -9.2647e-01
Epoch 3/10
19/19 - 3s - loss: 236.1785 - loglik: -2.3539e+02 - logprior: -7.8714e-01
Epoch 4/10
19/19 - 3s - loss: 235.9994 - loglik: -2.3525e+02 - logprior: -7.5398e-01
Epoch 5/10
19/19 - 3s - loss: 235.3562 - loglik: -2.3464e+02 - logprior: -7.1534e-01
Epoch 6/10
19/19 - 3s - loss: 235.1622 - loglik: -2.3447e+02 - logprior: -6.9271e-01
Epoch 7/10
19/19 - 3s - loss: 234.7692 - loglik: -2.3409e+02 - logprior: -6.7911e-01
Epoch 8/10
19/19 - 3s - loss: 234.4936 - loglik: -2.3382e+02 - logprior: -6.6919e-01
Epoch 9/10
19/19 - 3s - loss: 234.8553 - loglik: -2.3420e+02 - logprior: -6.5358e-01
Fitted a model with MAP estimate = -234.4779
Time for alignment: 81.4319
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 284.2063 - loglik: -2.8120e+02 - logprior: -3.0017e+00
Epoch 2/10
19/19 - 2s - loss: 253.4270 - loglik: -2.5257e+02 - logprior: -8.5848e-01
Epoch 3/10
19/19 - 2s - loss: 244.5899 - loglik: -2.4373e+02 - logprior: -8.5570e-01
Epoch 4/10
19/19 - 2s - loss: 242.6139 - loglik: -2.4186e+02 - logprior: -7.5030e-01
Epoch 5/10
19/19 - 2s - loss: 242.0960 - loglik: -2.4136e+02 - logprior: -7.3138e-01
Epoch 6/10
19/19 - 2s - loss: 242.1427 - loglik: -2.4143e+02 - logprior: -7.0960e-01
Fitted a model with MAP estimate = -240.6436
expansions: [(0, 10), (12, 2), (20, 1), (56, 1), (57, 2), (58, 3), (61, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 245.3799 - loglik: -2.4170e+02 - logprior: -3.6835e+00
Epoch 2/2
19/19 - 3s - loss: 239.6752 - loglik: -2.3859e+02 - logprior: -1.0848e+00
Fitted a model with MAP estimate = -237.2687
expansions: [(0, 8), (21, 2)]
discards: [ 1  2  3  4  5  6  7  8  9 72 75 82]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 243.5045 - loglik: -2.3986e+02 - logprior: -3.6451e+00
Epoch 2/2
19/19 - 3s - loss: 239.3532 - loglik: -2.3821e+02 - logprior: -1.1421e+00
Fitted a model with MAP estimate = -237.5078
expansions: [(0, 8)]
discards: [0 1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 241.0452 - loglik: -2.3791e+02 - logprior: -3.1337e+00
Epoch 2/10
19/19 - 3s - loss: 237.9285 - loglik: -2.3700e+02 - logprior: -9.2796e-01
Epoch 3/10
19/19 - 3s - loss: 236.6274 - loglik: -2.3587e+02 - logprior: -7.6050e-01
Epoch 4/10
19/19 - 3s - loss: 236.6000 - loglik: -2.3590e+02 - logprior: -6.9853e-01
Epoch 5/10
19/19 - 3s - loss: 235.8127 - loglik: -2.3514e+02 - logprior: -6.6805e-01
Epoch 6/10
19/19 - 3s - loss: 235.1937 - loglik: -2.3454e+02 - logprior: -6.5625e-01
Epoch 7/10
19/19 - 3s - loss: 235.1501 - loglik: -2.3451e+02 - logprior: -6.3806e-01
Epoch 8/10
19/19 - 3s - loss: 235.7048 - loglik: -2.3509e+02 - logprior: -6.1867e-01
Fitted a model with MAP estimate = -234.9015
Time for alignment: 75.5655
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 284.1790 - loglik: -2.8117e+02 - logprior: -3.0077e+00
Epoch 2/10
19/19 - 2s - loss: 254.3512 - loglik: -2.5348e+02 - logprior: -8.6840e-01
Epoch 3/10
19/19 - 2s - loss: 244.5555 - loglik: -2.4368e+02 - logprior: -8.7579e-01
Epoch 4/10
19/19 - 2s - loss: 242.7811 - loglik: -2.4199e+02 - logprior: -7.9300e-01
Epoch 5/10
19/19 - 2s - loss: 242.3305 - loglik: -2.4154e+02 - logprior: -7.9100e-01
Epoch 6/10
19/19 - 2s - loss: 241.9923 - loglik: -2.4122e+02 - logprior: -7.6830e-01
Epoch 7/10
19/19 - 2s - loss: 241.7044 - loglik: -2.4095e+02 - logprior: -7.5356e-01
Epoch 8/10
19/19 - 2s - loss: 241.4810 - loglik: -2.4074e+02 - logprior: -7.3737e-01
Epoch 9/10
19/19 - 2s - loss: 241.6893 - loglik: -2.4097e+02 - logprior: -7.1695e-01
Fitted a model with MAP estimate = -240.4841
expansions: [(0, 11), (12, 1), (16, 1), (20, 1), (21, 2), (23, 1), (55, 1), (57, 2), (58, 2), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 244.9378 - loglik: -2.4126e+02 - logprior: -3.6777e+00
Epoch 2/2
19/19 - 3s - loss: 239.0244 - loglik: -2.3787e+02 - logprior: -1.1573e+00
Fitted a model with MAP estimate = -236.6408
expansions: [(0, 8), (36, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 77 78]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 242.2547 - loglik: -2.3921e+02 - logprior: -3.0442e+00
Epoch 2/2
19/19 - 3s - loss: 238.4104 - loglik: -2.3756e+02 - logprior: -8.5354e-01
Fitted a model with MAP estimate = -236.8305
expansions: [(0, 8)]
discards: [1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 241.5689 - loglik: -2.3809e+02 - logprior: -3.4825e+00
Epoch 2/10
19/19 - 3s - loss: 237.2637 - loglik: -2.3620e+02 - logprior: -1.0595e+00
Epoch 3/10
19/19 - 3s - loss: 236.8793 - loglik: -2.3604e+02 - logprior: -8.3976e-01
Epoch 4/10
19/19 - 3s - loss: 235.6797 - loglik: -2.3497e+02 - logprior: -7.1057e-01
Epoch 5/10
19/19 - 3s - loss: 235.7017 - loglik: -2.3504e+02 - logprior: -6.5790e-01
Fitted a model with MAP estimate = -235.1331
Time for alignment: 73.5049
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 283.6259 - loglik: -2.8063e+02 - logprior: -2.9966e+00
Epoch 2/10
19/19 - 2s - loss: 252.4428 - loglik: -2.5158e+02 - logprior: -8.5935e-01
Epoch 3/10
19/19 - 2s - loss: 244.2106 - loglik: -2.4335e+02 - logprior: -8.5920e-01
Epoch 4/10
19/19 - 2s - loss: 243.1228 - loglik: -2.4238e+02 - logprior: -7.4504e-01
Epoch 5/10
19/19 - 2s - loss: 241.9691 - loglik: -2.4125e+02 - logprior: -7.1911e-01
Epoch 6/10
19/19 - 2s - loss: 241.7914 - loglik: -2.4109e+02 - logprior: -7.0059e-01
Epoch 7/10
19/19 - 2s - loss: 242.4306 - loglik: -2.4175e+02 - logprior: -6.8300e-01
Fitted a model with MAP estimate = -240.7622
expansions: [(0, 11), (16, 1), (20, 4), (55, 1), (57, 2), (58, 2), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 245.0947 - loglik: -2.4142e+02 - logprior: -3.6785e+00
Epoch 2/2
19/19 - 3s - loss: 239.2106 - loglik: -2.3807e+02 - logprior: -1.1398e+00
Fitted a model with MAP estimate = -237.0255
expansions: [(0, 7), (35, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 76 77]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 243.4012 - loglik: -2.3960e+02 - logprior: -3.8013e+00
Epoch 2/2
19/19 - 3s - loss: 239.2932 - loglik: -2.3822e+02 - logprior: -1.0682e+00
Fitted a model with MAP estimate = -237.4170
expansions: [(0, 9)]
discards: [0 1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 240.9857 - loglik: -2.3788e+02 - logprior: -3.1097e+00
Epoch 2/10
19/19 - 3s - loss: 237.5611 - loglik: -2.3663e+02 - logprior: -9.2720e-01
Epoch 3/10
19/19 - 3s - loss: 236.9018 - loglik: -2.3614e+02 - logprior: -7.6497e-01
Epoch 4/10
19/19 - 3s - loss: 236.3168 - loglik: -2.3557e+02 - logprior: -7.4246e-01
Epoch 5/10
19/19 - 3s - loss: 235.3948 - loglik: -2.3469e+02 - logprior: -7.0500e-01
Epoch 6/10
19/19 - 3s - loss: 236.0241 - loglik: -2.3533e+02 - logprior: -6.9595e-01
Fitted a model with MAP estimate = -235.5306
Time for alignment: 71.0611
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 283.6424 - loglik: -2.8064e+02 - logprior: -3.0018e+00
Epoch 2/10
19/19 - 2s - loss: 253.1113 - loglik: -2.5226e+02 - logprior: -8.5552e-01
Epoch 3/10
19/19 - 2s - loss: 244.6790 - loglik: -2.4383e+02 - logprior: -8.4448e-01
Epoch 4/10
19/19 - 2s - loss: 243.0292 - loglik: -2.4227e+02 - logprior: -7.5911e-01
Epoch 5/10
19/19 - 2s - loss: 242.2178 - loglik: -2.4147e+02 - logprior: -7.4372e-01
Epoch 6/10
19/19 - 2s - loss: 241.9092 - loglik: -2.4118e+02 - logprior: -7.2682e-01
Epoch 7/10
19/19 - 2s - loss: 241.4326 - loglik: -2.4072e+02 - logprior: -7.1009e-01
Epoch 8/10
19/19 - 2s - loss: 242.0859 - loglik: -2.4138e+02 - logprior: -7.0270e-01
Fitted a model with MAP estimate = -240.6019
expansions: [(0, 10), (16, 2), (20, 1), (21, 2), (22, 1), (57, 2), (58, 2), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 245.2776 - loglik: -2.4160e+02 - logprior: -3.6748e+00
Epoch 2/2
19/19 - 3s - loss: 239.2719 - loglik: -2.3816e+02 - logprior: -1.1095e+00
Fitted a model with MAP estimate = -237.2990
expansions: [(0, 7), (35, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 75 76]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 243.7671 - loglik: -2.4003e+02 - logprior: -3.7370e+00
Epoch 2/2
19/19 - 3s - loss: 239.7360 - loglik: -2.3852e+02 - logprior: -1.2156e+00
Fitted a model with MAP estimate = -237.6702
expansions: [(0, 7), (34, 1)]
discards: [ 0  1  2  3  4  5  6  7 24]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 241.5335 - loglik: -2.3836e+02 - logprior: -3.1764e+00
Epoch 2/10
19/19 - 3s - loss: 237.6868 - loglik: -2.3675e+02 - logprior: -9.4005e-01
Epoch 3/10
19/19 - 3s - loss: 236.7015 - loglik: -2.3596e+02 - logprior: -7.4387e-01
Epoch 4/10
19/19 - 3s - loss: 236.4596 - loglik: -2.3579e+02 - logprior: -6.7007e-01
Epoch 5/10
19/19 - 3s - loss: 235.5225 - loglik: -2.3490e+02 - logprior: -6.2207e-01
Epoch 6/10
19/19 - 3s - loss: 235.5456 - loglik: -2.3495e+02 - logprior: -5.9183e-01
Fitted a model with MAP estimate = -235.2531
Time for alignment: 73.3538
Computed alignments with likelihoods: ['-234.4779', '-234.9015', '-235.1331', '-235.5306', '-235.2531']
Best model has likelihood: -234.4779  (prior= -0.6662 )
time for generating output: 0.2404
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.5006315789473684
Training of 5 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feab7b17b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebbe2280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaaf6902e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 732.3680 - loglik: -7.2049e+02 - logprior: -1.1878e+01
Epoch 2/10
17/17 - 6s - loss: 556.7417 - loglik: -5.5510e+02 - logprior: -1.6376e+00
Epoch 3/10
17/17 - 7s - loss: 463.3604 - loglik: -4.6049e+02 - logprior: -2.8739e+00
Epoch 4/10
17/17 - 7s - loss: 442.6953 - loglik: -4.3967e+02 - logprior: -3.0223e+00
Epoch 5/10
17/17 - 7s - loss: 435.7303 - loglik: -4.3308e+02 - logprior: -2.6474e+00
Epoch 6/10
17/17 - 6s - loss: 432.1230 - loglik: -4.2951e+02 - logprior: -2.6157e+00
Epoch 7/10
17/17 - 6s - loss: 432.3027 - loglik: -4.2966e+02 - logprior: -2.6470e+00
Fitted a model with MAP estimate = -429.6711
expansions: [(10, 1), (11, 1), (12, 1), (13, 3), (24, 1), (26, 1), (32, 2), (45, 1), (46, 1), (55, 1), (58, 1), (60, 2), (63, 1), (87, 3), (95, 2), (97, 1), (100, 1), (109, 1), (113, 1), (114, 2), (115, 1), (122, 1), (129, 1), (130, 2), (141, 1), (155, 1), (160, 1), (163, 2), (179, 1), (181, 2), (184, 1), (188, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 429.6440 - loglik: -4.1391e+02 - logprior: -1.5733e+01
Epoch 2/2
17/17 - 9s - loss: 391.8423 - loglik: -3.8736e+02 - logprior: -4.4785e+00
Fitted a model with MAP estimate = -388.0403
expansions: [(0, 9)]
discards: [  0  16 104 139 160]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 398.3830 - loglik: -3.8749e+02 - logprior: -1.0891e+01
Epoch 2/2
17/17 - 9s - loss: 382.8978 - loglik: -3.8330e+02 - logprior: 0.3998
Fitted a model with MAP estimate = -380.6846
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 395.7952 - loglik: -3.8577e+02 - logprior: -1.0027e+01
Epoch 2/10
17/17 - 8s - loss: 380.1478 - loglik: -3.8132e+02 - logprior: 1.1771
Epoch 3/10
17/17 - 9s - loss: 381.3538 - loglik: -3.8423e+02 - logprior: 2.8757
Fitted a model with MAP estimate = -378.6196
Time for alignment: 145.6549
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 731.5129 - loglik: -7.1962e+02 - logprior: -1.1894e+01
Epoch 2/10
17/17 - 6s - loss: 548.7911 - loglik: -5.4718e+02 - logprior: -1.6137e+00
Epoch 3/10
17/17 - 6s - loss: 459.7546 - loglik: -4.5683e+02 - logprior: -2.9213e+00
Epoch 4/10
17/17 - 7s - loss: 439.8090 - loglik: -4.3656e+02 - logprior: -3.2495e+00
Epoch 5/10
17/17 - 7s - loss: 434.8035 - loglik: -4.3186e+02 - logprior: -2.9426e+00
Epoch 6/10
17/17 - 6s - loss: 429.3489 - loglik: -4.2657e+02 - logprior: -2.7776e+00
Epoch 7/10
17/17 - 7s - loss: 430.5941 - loglik: -4.2777e+02 - logprior: -2.8191e+00
Fitted a model with MAP estimate = -428.7130
expansions: [(8, 1), (9, 1), (10, 1), (11, 1), (13, 1), (22, 1), (23, 1), (32, 1), (43, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (64, 1), (72, 1), (87, 1), (90, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 1), (130, 3), (131, 2), (140, 1), (141, 1), (160, 1), (161, 1), (163, 1), (180, 1), (182, 2), (185, 1), (189, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 3), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 429.3174 - loglik: -4.1360e+02 - logprior: -1.5713e+01
Epoch 2/2
17/17 - 9s - loss: 390.9958 - loglik: -3.8649e+02 - logprior: -4.5093e+00
Fitted a model with MAP estimate = -388.4013
expansions: [(0, 9)]
discards: [  0 155 158 245]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 398.7886 - loglik: -3.8799e+02 - logprior: -1.0800e+01
Epoch 2/2
17/17 - 9s - loss: 382.6365 - loglik: -3.8306e+02 - logprior: 0.4228
Fitted a model with MAP estimate = -380.5024
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 394.7981 - loglik: -3.8472e+02 - logprior: -1.0079e+01
Epoch 2/10
17/17 - 9s - loss: 382.0034 - loglik: -3.8312e+02 - logprior: 1.1149
Epoch 3/10
17/17 - 8s - loss: 379.5551 - loglik: -3.8246e+02 - logprior: 2.9011
Epoch 4/10
17/17 - 9s - loss: 378.4258 - loglik: -3.8206e+02 - logprior: 3.6350
Epoch 5/10
17/17 - 8s - loss: 375.7457 - loglik: -3.7971e+02 - logprior: 3.9631
Epoch 6/10
17/17 - 9s - loss: 375.2986 - loglik: -3.7945e+02 - logprior: 4.1481
Epoch 7/10
17/17 - 9s - loss: 374.6260 - loglik: -3.7881e+02 - logprior: 4.1843
Epoch 8/10
17/17 - 8s - loss: 371.3003 - loglik: -3.7567e+02 - logprior: 4.3705
Epoch 9/10
17/17 - 8s - loss: 371.2726 - loglik: -3.7589e+02 - logprior: 4.6218
Epoch 10/10
17/17 - 9s - loss: 369.5561 - loglik: -3.7444e+02 - logprior: 4.8857
Fitted a model with MAP estimate = -371.1315
Time for alignment: 204.9882
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 731.6647 - loglik: -7.1977e+02 - logprior: -1.1895e+01
Epoch 2/10
17/17 - 6s - loss: 552.0352 - loglik: -5.5035e+02 - logprior: -1.6870e+00
Epoch 3/10
17/17 - 6s - loss: 459.5713 - loglik: -4.5660e+02 - logprior: -2.9705e+00
Epoch 4/10
17/17 - 7s - loss: 437.6041 - loglik: -4.3412e+02 - logprior: -3.4798e+00
Epoch 5/10
17/17 - 6s - loss: 432.0380 - loglik: -4.2891e+02 - logprior: -3.1307e+00
Epoch 6/10
17/17 - 6s - loss: 427.2279 - loglik: -4.2424e+02 - logprior: -2.9851e+00
Epoch 7/10
17/17 - 7s - loss: 426.1013 - loglik: -4.2314e+02 - logprior: -2.9657e+00
Epoch 8/10
17/17 - 7s - loss: 427.2580 - loglik: -4.2427e+02 - logprior: -2.9851e+00
Fitted a model with MAP estimate = -425.6321
expansions: [(8, 1), (9, 1), (11, 1), (13, 1), (15, 1), (22, 1), (26, 1), (32, 1), (42, 1), (45, 1), (47, 1), (55, 1), (57, 1), (58, 1), (60, 1), (63, 1), (87, 3), (95, 2), (97, 1), (100, 1), (114, 2), (115, 3), (129, 2), (131, 2), (140, 1), (141, 1), (156, 1), (160, 1), (163, 1), (165, 1), (179, 1), (180, 1), (181, 1), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (206, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 429.1363 - loglik: -4.1335e+02 - logprior: -1.5783e+01
Epoch 2/2
17/17 - 9s - loss: 392.9651 - loglik: -3.8840e+02 - logprior: -4.5634e+00
Fitted a model with MAP estimate = -388.3054
expansions: [(0, 9)]
discards: [  0 103 136 156 160]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 398.8007 - loglik: -3.8794e+02 - logprior: -1.0864e+01
Epoch 2/2
17/17 - 9s - loss: 382.4315 - loglik: -3.8286e+02 - logprior: 0.4269
Fitted a model with MAP estimate = -380.4932
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 395.2853 - loglik: -3.8525e+02 - logprior: -1.0036e+01
Epoch 2/10
17/17 - 9s - loss: 382.8909 - loglik: -3.8403e+02 - logprior: 1.1439
Epoch 3/10
17/17 - 9s - loss: 378.1988 - loglik: -3.8111e+02 - logprior: 2.9125
Epoch 4/10
17/17 - 9s - loss: 377.3906 - loglik: -3.8106e+02 - logprior: 3.6718
Epoch 5/10
17/17 - 9s - loss: 376.8072 - loglik: -3.8082e+02 - logprior: 4.0107
Epoch 6/10
17/17 - 9s - loss: 374.5393 - loglik: -3.7875e+02 - logprior: 4.2137
Epoch 7/10
17/17 - 9s - loss: 371.8560 - loglik: -3.7612e+02 - logprior: 4.2666
Epoch 8/10
17/17 - 9s - loss: 373.3228 - loglik: -3.7773e+02 - logprior: 4.4080
Fitted a model with MAP estimate = -371.7790
Time for alignment: 197.4513
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 731.3824 - loglik: -7.1950e+02 - logprior: -1.1885e+01
Epoch 2/10
17/17 - 6s - loss: 550.7833 - loglik: -5.4911e+02 - logprior: -1.6766e+00
Epoch 3/10
17/17 - 7s - loss: 458.5638 - loglik: -4.5570e+02 - logprior: -2.8645e+00
Epoch 4/10
17/17 - 7s - loss: 438.7096 - loglik: -4.3567e+02 - logprior: -3.0358e+00
Epoch 5/10
17/17 - 7s - loss: 434.2723 - loglik: -4.3147e+02 - logprior: -2.8030e+00
Epoch 6/10
17/17 - 6s - loss: 431.7043 - loglik: -4.2904e+02 - logprior: -2.6628e+00
Epoch 7/10
17/17 - 7s - loss: 428.5995 - loglik: -4.2594e+02 - logprior: -2.6580e+00
Epoch 8/10
17/17 - 7s - loss: 430.5933 - loglik: -4.2790e+02 - logprior: -2.6902e+00
Fitted a model with MAP estimate = -428.3853
expansions: [(8, 1), (9, 1), (10, 1), (13, 3), (22, 1), (23, 1), (32, 2), (45, 1), (54, 1), (56, 1), (58, 1), (60, 1), (61, 1), (63, 1), (86, 2), (95, 2), (97, 1), (100, 1), (114, 2), (115, 3), (129, 2), (131, 1), (140, 1), (141, 1), (152, 1), (160, 1), (162, 1), (163, 1), (179, 1), (181, 2), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 428.1302 - loglik: -4.1249e+02 - logprior: -1.5642e+01
Epoch 2/2
17/17 - 9s - loss: 392.3950 - loglik: -3.8793e+02 - logprior: -4.4614e+00
Fitted a model with MAP estimate = -388.0089
expansions: [(0, 10)]
discards: [  0  16 136 156]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 398.6307 - loglik: -3.8780e+02 - logprior: -1.0827e+01
Epoch 2/2
17/17 - 9s - loss: 381.4656 - loglik: -3.8184e+02 - logprior: 0.3787
Fitted a model with MAP estimate = -380.4949
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 395.7314 - loglik: -3.8573e+02 - logprior: -1.0002e+01
Epoch 2/10
17/17 - 9s - loss: 380.5424 - loglik: -3.8174e+02 - logprior: 1.1984
Epoch 3/10
17/17 - 8s - loss: 380.1028 - loglik: -3.8305e+02 - logprior: 2.9511
Epoch 4/10
17/17 - 9s - loss: 378.0141 - loglik: -3.8170e+02 - logprior: 3.6894
Epoch 5/10
17/17 - 8s - loss: 375.8645 - loglik: -3.7989e+02 - logprior: 4.0268
Epoch 6/10
17/17 - 9s - loss: 373.0515 - loglik: -3.7723e+02 - logprior: 4.1825
Epoch 7/10
17/17 - 9s - loss: 374.4820 - loglik: -3.7868e+02 - logprior: 4.1980
Fitted a model with MAP estimate = -372.5345
Time for alignment: 186.2529
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 732.4785 - loglik: -7.2060e+02 - logprior: -1.1882e+01
Epoch 2/10
17/17 - 7s - loss: 550.3398 - loglik: -5.4859e+02 - logprior: -1.7487e+00
Epoch 3/10
17/17 - 7s - loss: 455.4561 - loglik: -4.5228e+02 - logprior: -3.1767e+00
Epoch 4/10
17/17 - 6s - loss: 437.5064 - loglik: -4.3399e+02 - logprior: -3.5206e+00
Epoch 5/10
17/17 - 7s - loss: 429.0433 - loglik: -4.2578e+02 - logprior: -3.2604e+00
Epoch 6/10
17/17 - 7s - loss: 428.0225 - loglik: -4.2487e+02 - logprior: -3.1498e+00
Epoch 7/10
17/17 - 7s - loss: 424.4804 - loglik: -4.2131e+02 - logprior: -3.1722e+00
Epoch 8/10
17/17 - 7s - loss: 425.9349 - loglik: -4.2274e+02 - logprior: -3.1914e+00
Fitted a model with MAP estimate = -423.8230
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (45, 1), (46, 1), (53, 1), (58, 1), (60, 1), (61, 1), (63, 1), (87, 1), (90, 1), (95, 1), (97, 1), (98, 1), (100, 1), (114, 1), (115, 2), (116, 1), (130, 3), (131, 2), (140, 1), (141, 1), (156, 1), (160, 1), (161, 1), (162, 1), (176, 1), (178, 1), (180, 1), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 427.4078 - loglik: -4.1163e+02 - logprior: -1.5778e+01
Epoch 2/2
17/17 - 9s - loss: 392.4354 - loglik: -3.8787e+02 - logprior: -4.5692e+00
Fitted a model with MAP estimate = -388.2534
expansions: [(0, 9)]
discards: [  0 137 156 159]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 398.2691 - loglik: -3.8751e+02 - logprior: -1.0760e+01
Epoch 2/2
17/17 - 9s - loss: 382.5778 - loglik: -3.8302e+02 - logprior: 0.4390
Fitted a model with MAP estimate = -380.5272
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 395.4131 - loglik: -3.8545e+02 - logprior: -9.9670e+00
Epoch 2/10
17/17 - 9s - loss: 381.8415 - loglik: -3.8303e+02 - logprior: 1.1880
Epoch 3/10
17/17 - 8s - loss: 379.5483 - loglik: -3.8248e+02 - logprior: 2.9293
Epoch 4/10
17/17 - 8s - loss: 377.7375 - loglik: -3.8141e+02 - logprior: 3.6762
Epoch 5/10
17/17 - 9s - loss: 376.2523 - loglik: -3.8025e+02 - logprior: 3.9984
Epoch 6/10
17/17 - 9s - loss: 374.7898 - loglik: -3.7896e+02 - logprior: 4.1702
Epoch 7/10
17/17 - 9s - loss: 373.4812 - loglik: -3.7769e+02 - logprior: 4.2091
Epoch 8/10
17/17 - 9s - loss: 371.9149 - loglik: -3.7630e+02 - logprior: 4.3899
Epoch 9/10
17/17 - 9s - loss: 373.4299 - loglik: -3.7805e+02 - logprior: 4.6204
Fitted a model with MAP estimate = -371.4570
Time for alignment: 203.0216
Computed alignments with likelihoods: ['-378.6196', '-371.1315', '-371.7790', '-372.5345', '-371.4570']
Best model has likelihood: -371.1315  (prior= 5.0172 )
time for generating output: 0.2759
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.9436997319034852
Training of 5 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb05d4b7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f006520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e1d3e80>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.6618 - loglik: -1.5964e+02 - logprior: -2.0018e+01
Epoch 2/10
10/10 - 0s - loss: 148.1298 - loglik: -1.4241e+02 - logprior: -5.7204e+00
Epoch 3/10
10/10 - 0s - loss: 130.4779 - loglik: -1.2721e+02 - logprior: -3.2716e+00
Epoch 4/10
10/10 - 0s - loss: 118.5956 - loglik: -1.1583e+02 - logprior: -2.7655e+00
Epoch 5/10
10/10 - 1s - loss: 113.7898 - loglik: -1.1108e+02 - logprior: -2.7055e+00
Epoch 6/10
10/10 - 0s - loss: 111.9157 - loglik: -1.0933e+02 - logprior: -2.5898e+00
Epoch 7/10
10/10 - 1s - loss: 110.9698 - loglik: -1.0854e+02 - logprior: -2.4277e+00
Epoch 8/10
10/10 - 1s - loss: 110.5518 - loglik: -1.0824e+02 - logprior: -2.3096e+00
Epoch 9/10
10/10 - 0s - loss: 110.1700 - loglik: -1.0789e+02 - logprior: -2.2833e+00
Epoch 10/10
10/10 - 1s - loss: 109.6857 - loglik: -1.0741e+02 - logprior: -2.2802e+00
Fitted a model with MAP estimate = -109.6140
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 135.0028 - loglik: -1.1256e+02 - logprior: -2.2440e+01
Epoch 2/2
10/10 - 1s - loss: 115.8163 - loglik: -1.0609e+02 - logprior: -9.7235e+00
Fitted a model with MAP estimate = -112.4265
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 121.6403 - loglik: -1.0384e+02 - logprior: -1.7797e+01
Epoch 2/2
10/10 - 1s - loss: 108.1897 - loglik: -1.0330e+02 - logprior: -4.8848e+00
Fitted a model with MAP estimate = -106.1216
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.8807 - loglik: -1.0460e+02 - logprior: -2.0282e+01
Epoch 2/10
10/10 - 1s - loss: 110.0379 - loglik: -1.0410e+02 - logprior: -5.9399e+00
Epoch 3/10
10/10 - 1s - loss: 106.5093 - loglik: -1.0332e+02 - logprior: -3.1865e+00
Epoch 4/10
10/10 - 1s - loss: 105.1530 - loglik: -1.0307e+02 - logprior: -2.0794e+00
Epoch 5/10
10/10 - 1s - loss: 104.5098 - loglik: -1.0305e+02 - logprior: -1.4638e+00
Epoch 6/10
10/10 - 1s - loss: 103.7866 - loglik: -1.0254e+02 - logprior: -1.2473e+00
Epoch 7/10
10/10 - 1s - loss: 103.9665 - loglik: -1.0292e+02 - logprior: -1.0488e+00
Fitted a model with MAP estimate = -103.6675
Time for alignment: 29.3424
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.5942 - loglik: -1.5958e+02 - logprior: -2.0018e+01
Epoch 2/10
10/10 - 1s - loss: 147.8092 - loglik: -1.4208e+02 - logprior: -5.7275e+00
Epoch 3/10
10/10 - 1s - loss: 129.9947 - loglik: -1.2668e+02 - logprior: -3.3116e+00
Epoch 4/10
10/10 - 1s - loss: 118.6915 - loglik: -1.1591e+02 - logprior: -2.7775e+00
Epoch 5/10
10/10 - 1s - loss: 113.8453 - loglik: -1.1115e+02 - logprior: -2.6928e+00
Epoch 6/10
10/10 - 1s - loss: 112.2891 - loglik: -1.0967e+02 - logprior: -2.6202e+00
Epoch 7/10
10/10 - 1s - loss: 111.4722 - loglik: -1.0902e+02 - logprior: -2.4570e+00
Epoch 8/10
10/10 - 1s - loss: 110.9236 - loglik: -1.0861e+02 - logprior: -2.3131e+00
Epoch 9/10
10/10 - 1s - loss: 110.4870 - loglik: -1.0822e+02 - logprior: -2.2706e+00
Epoch 10/10
10/10 - 1s - loss: 110.2048 - loglik: -1.0794e+02 - logprior: -2.2678e+00
Fitted a model with MAP estimate = -110.1546
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 135.3608 - loglik: -1.1293e+02 - logprior: -2.2431e+01
Epoch 2/2
10/10 - 1s - loss: 115.8474 - loglik: -1.0611e+02 - logprior: -9.7357e+00
Fitted a model with MAP estimate = -112.4594
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 121.7752 - loglik: -1.0397e+02 - logprior: -1.7800e+01
Epoch 2/2
10/10 - 1s - loss: 107.7458 - loglik: -1.0286e+02 - logprior: -4.8846e+00
Fitted a model with MAP estimate = -106.1142
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 124.9243 - loglik: -1.0464e+02 - logprior: -2.0281e+01
Epoch 2/10
10/10 - 1s - loss: 109.8403 - loglik: -1.0390e+02 - logprior: -5.9437e+00
Epoch 3/10
10/10 - 1s - loss: 106.5649 - loglik: -1.0338e+02 - logprior: -3.1866e+00
Epoch 4/10
10/10 - 1s - loss: 105.2597 - loglik: -1.0319e+02 - logprior: -2.0719e+00
Epoch 5/10
10/10 - 1s - loss: 104.5607 - loglik: -1.0309e+02 - logprior: -1.4752e+00
Epoch 6/10
10/10 - 1s - loss: 103.9379 - loglik: -1.0268e+02 - logprior: -1.2558e+00
Epoch 7/10
10/10 - 1s - loss: 103.6114 - loglik: -1.0256e+02 - logprior: -1.0501e+00
Epoch 8/10
10/10 - 1s - loss: 103.6408 - loglik: -1.0266e+02 - logprior: -9.7948e-01
Fitted a model with MAP estimate = -103.5076
Time for alignment: 28.6924
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.7177 - loglik: -1.5970e+02 - logprior: -2.0019e+01
Epoch 2/10
10/10 - 1s - loss: 148.0174 - loglik: -1.4229e+02 - logprior: -5.7240e+00
Epoch 3/10
10/10 - 1s - loss: 130.2664 - loglik: -1.2697e+02 - logprior: -3.2935e+00
Epoch 4/10
10/10 - 1s - loss: 119.1612 - loglik: -1.1644e+02 - logprior: -2.7233e+00
Epoch 5/10
10/10 - 1s - loss: 113.9757 - loglik: -1.1134e+02 - logprior: -2.6334e+00
Epoch 6/10
10/10 - 1s - loss: 112.0776 - loglik: -1.0943e+02 - logprior: -2.6504e+00
Epoch 7/10
10/10 - 1s - loss: 111.0277 - loglik: -1.0850e+02 - logprior: -2.5311e+00
Epoch 8/10
10/10 - 1s - loss: 110.7705 - loglik: -1.0840e+02 - logprior: -2.3672e+00
Epoch 9/10
10/10 - 1s - loss: 110.3076 - loglik: -1.0803e+02 - logprior: -2.2797e+00
Epoch 10/10
10/10 - 1s - loss: 110.2009 - loglik: -1.0793e+02 - logprior: -2.2691e+00
Fitted a model with MAP estimate = -110.1083
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 135.4637 - loglik: -1.1303e+02 - logprior: -2.2434e+01
Epoch 2/2
10/10 - 1s - loss: 115.7895 - loglik: -1.0606e+02 - logprior: -9.7344e+00
Fitted a model with MAP estimate = -112.4457
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 121.6564 - loglik: -1.0386e+02 - logprior: -1.7799e+01
Epoch 2/2
10/10 - 1s - loss: 107.9430 - loglik: -1.0305e+02 - logprior: -4.8891e+00
Fitted a model with MAP estimate = -106.1160
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.9561 - loglik: -1.0468e+02 - logprior: -2.0278e+01
Epoch 2/10
10/10 - 1s - loss: 109.9605 - loglik: -1.0402e+02 - logprior: -5.9400e+00
Epoch 3/10
10/10 - 1s - loss: 106.3488 - loglik: -1.0316e+02 - logprior: -3.1843e+00
Epoch 4/10
10/10 - 1s - loss: 105.2735 - loglik: -1.0320e+02 - logprior: -2.0714e+00
Epoch 5/10
10/10 - 1s - loss: 104.2918 - loglik: -1.0282e+02 - logprior: -1.4730e+00
Epoch 6/10
10/10 - 1s - loss: 104.2235 - loglik: -1.0297e+02 - logprior: -1.2564e+00
Epoch 7/10
10/10 - 1s - loss: 103.6796 - loglik: -1.0263e+02 - logprior: -1.0510e+00
Epoch 8/10
10/10 - 1s - loss: 103.7573 - loglik: -1.0278e+02 - logprior: -9.7931e-01
Fitted a model with MAP estimate = -103.5166
Time for alignment: 28.6022
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.6363 - loglik: -1.5962e+02 - logprior: -2.0019e+01
Epoch 2/10
10/10 - 1s - loss: 148.0926 - loglik: -1.4237e+02 - logprior: -5.7243e+00
Epoch 3/10
10/10 - 1s - loss: 130.2599 - loglik: -1.2697e+02 - logprior: -3.2912e+00
Epoch 4/10
10/10 - 1s - loss: 119.1832 - loglik: -1.1644e+02 - logprior: -2.7399e+00
Epoch 5/10
10/10 - 1s - loss: 113.8326 - loglik: -1.1117e+02 - logprior: -2.6624e+00
Epoch 6/10
10/10 - 1s - loss: 112.2028 - loglik: -1.0956e+02 - logprior: -2.6393e+00
Epoch 7/10
10/10 - 1s - loss: 110.9885 - loglik: -1.0848e+02 - logprior: -2.5126e+00
Epoch 8/10
10/10 - 1s - loss: 110.6788 - loglik: -1.0832e+02 - logprior: -2.3596e+00
Epoch 9/10
10/10 - 1s - loss: 110.4567 - loglik: -1.0818e+02 - logprior: -2.2731e+00
Epoch 10/10
10/10 - 1s - loss: 109.9617 - loglik: -1.0770e+02 - logprior: -2.2627e+00
Fitted a model with MAP estimate = -110.1140
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 135.1984 - loglik: -1.1277e+02 - logprior: -2.2432e+01
Epoch 2/2
10/10 - 1s - loss: 115.9893 - loglik: -1.0625e+02 - logprior: -9.7359e+00
Fitted a model with MAP estimate = -112.4616
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 121.8452 - loglik: -1.0404e+02 - logprior: -1.7801e+01
Epoch 2/2
10/10 - 1s - loss: 107.8405 - loglik: -1.0295e+02 - logprior: -4.8870e+00
Fitted a model with MAP estimate = -106.1116
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 125.0902 - loglik: -1.0480e+02 - logprior: -2.0288e+01
Epoch 2/10
10/10 - 1s - loss: 110.0101 - loglik: -1.0406e+02 - logprior: -5.9453e+00
Epoch 3/10
10/10 - 1s - loss: 106.2958 - loglik: -1.0311e+02 - logprior: -3.1906e+00
Epoch 4/10
10/10 - 1s - loss: 105.1176 - loglik: -1.0304e+02 - logprior: -2.0761e+00
Epoch 5/10
10/10 - 1s - loss: 104.4523 - loglik: -1.0298e+02 - logprior: -1.4747e+00
Epoch 6/10
10/10 - 1s - loss: 104.1160 - loglik: -1.0286e+02 - logprior: -1.2567e+00
Epoch 7/10
10/10 - 1s - loss: 103.8302 - loglik: -1.0278e+02 - logprior: -1.0528e+00
Epoch 8/10
10/10 - 1s - loss: 103.4918 - loglik: -1.0251e+02 - logprior: -9.7927e-01
Epoch 9/10
10/10 - 1s - loss: 103.5021 - loglik: -1.0258e+02 - logprior: -9.1985e-01
Fitted a model with MAP estimate = -103.4223
Time for alignment: 28.6480
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.7888 - loglik: -1.5977e+02 - logprior: -2.0018e+01
Epoch 2/10
10/10 - 1s - loss: 148.0338 - loglik: -1.4231e+02 - logprior: -5.7221e+00
Epoch 3/10
10/10 - 1s - loss: 129.4565 - loglik: -1.2616e+02 - logprior: -3.2933e+00
Epoch 4/10
10/10 - 1s - loss: 118.1401 - loglik: -1.1537e+02 - logprior: -2.7722e+00
Epoch 5/10
10/10 - 0s - loss: 113.5049 - loglik: -1.1080e+02 - logprior: -2.7089e+00
Epoch 6/10
10/10 - 1s - loss: 111.7896 - loglik: -1.0914e+02 - logprior: -2.6521e+00
Epoch 7/10
10/10 - 1s - loss: 110.8492 - loglik: -1.0834e+02 - logprior: -2.5047e+00
Epoch 8/10
10/10 - 1s - loss: 110.7064 - loglik: -1.0836e+02 - logprior: -2.3481e+00
Epoch 9/10
10/10 - 1s - loss: 110.1552 - loglik: -1.0789e+02 - logprior: -2.2687e+00
Epoch 10/10
10/10 - 1s - loss: 110.2690 - loglik: -1.0801e+02 - logprior: -2.2603e+00
Fitted a model with MAP estimate = -110.0548
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 135.2956 - loglik: -1.1286e+02 - logprior: -2.2433e+01
Epoch 2/2
10/10 - 1s - loss: 115.7826 - loglik: -1.0605e+02 - logprior: -9.7361e+00
Fitted a model with MAP estimate = -112.4718
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 121.4543 - loglik: -1.0365e+02 - logprior: -1.7799e+01
Epoch 2/2
10/10 - 1s - loss: 108.3131 - loglik: -1.0342e+02 - logprior: -4.8894e+00
Fitted a model with MAP estimate = -106.1149
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 125.0242 - loglik: -1.0474e+02 - logprior: -2.0282e+01
Epoch 2/10
10/10 - 1s - loss: 109.7654 - loglik: -1.0382e+02 - logprior: -5.9441e+00
Epoch 3/10
10/10 - 1s - loss: 106.7752 - loglik: -1.0359e+02 - logprior: -3.1893e+00
Epoch 4/10
10/10 - 1s - loss: 105.0001 - loglik: -1.0293e+02 - logprior: -2.0715e+00
Epoch 5/10
10/10 - 1s - loss: 104.5845 - loglik: -1.0310e+02 - logprior: -1.4808e+00
Epoch 6/10
10/10 - 1s - loss: 103.9495 - loglik: -1.0269e+02 - logprior: -1.2608e+00
Epoch 7/10
10/10 - 1s - loss: 103.8038 - loglik: -1.0275e+02 - logprior: -1.0515e+00
Epoch 8/10
10/10 - 1s - loss: 103.5517 - loglik: -1.0257e+02 - logprior: -9.8519e-01
Epoch 9/10
10/10 - 1s - loss: 103.4708 - loglik: -1.0255e+02 - logprior: -9.2220e-01
Epoch 10/10
10/10 - 1s - loss: 103.4281 - loglik: -1.0254e+02 - logprior: -8.9120e-01
Fitted a model with MAP estimate = -103.3397
Time for alignment: 29.3636
Computed alignments with likelihoods: ['-103.6675', '-103.5076', '-103.5166', '-103.4223', '-103.3397']
Best model has likelihood: -103.3397  (prior= -0.8756 )
time for generating output: 0.1077
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.9893069306930693
Training of 5 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb4084df70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb49f4de20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac8e7b880>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 467.9981 - loglik: -4.6622e+02 - logprior: -1.7771e+00
Epoch 2/10
39/39 - 7s - loss: 390.4040 - loglik: -3.8938e+02 - logprior: -1.0289e+00
Epoch 3/10
39/39 - 7s - loss: 383.8024 - loglik: -3.8286e+02 - logprior: -9.4191e-01
Epoch 4/10
39/39 - 7s - loss: 381.2218 - loglik: -3.8033e+02 - logprior: -8.8711e-01
Epoch 5/10
39/39 - 7s - loss: 378.9865 - loglik: -3.7811e+02 - logprior: -8.7541e-01
Epoch 6/10
39/39 - 7s - loss: 378.2013 - loglik: -3.7732e+02 - logprior: -8.7899e-01
Epoch 7/10
39/39 - 8s - loss: 377.2281 - loglik: -3.7634e+02 - logprior: -8.9019e-01
Epoch 8/10
39/39 - 7s - loss: 377.3714 - loglik: -3.7647e+02 - logprior: -9.0191e-01
Fitted a model with MAP estimate = -312.6532
expansions: [(0, 19), (10, 1), (15, 1), (18, 1), (28, 1), (29, 1), (34, 3), (44, 3), (49, 1), (70, 1), (71, 1), (87, 1), (88, 1), (89, 1), (102, 2), (107, 2), (125, 8), (126, 1), (130, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 366.0068 - loglik: -3.6330e+02 - logprior: -2.7060e+00
Epoch 2/2
39/39 - 10s - loss: 351.3552 - loglik: -3.4940e+02 - logprior: -1.9541e+00
Fitted a model with MAP estimate = -291.0248
expansions: [(169, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  58 145 180]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 358.0439 - loglik: -3.5504e+02 - logprior: -3.0027e+00
Epoch 2/2
39/39 - 9s - loss: 352.4228 - loglik: -3.5149e+02 - logprior: -9.3154e-01
Fitted a model with MAP estimate = -292.6800
expansions: [(0, 19)]
discards: [149]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 16s - loss: 291.2877 - loglik: -2.8970e+02 - logprior: -1.5902e+00
Epoch 2/10
52/52 - 11s - loss: 282.9362 - loglik: -2.8167e+02 - logprior: -1.2681e+00
Epoch 3/10
52/52 - 13s - loss: 281.7534 - loglik: -2.8052e+02 - logprior: -1.2315e+00
Epoch 4/10
52/52 - 11s - loss: 281.2108 - loglik: -2.8003e+02 - logprior: -1.1839e+00
Epoch 5/10
52/52 - 13s - loss: 279.5083 - loglik: -2.7839e+02 - logprior: -1.1156e+00
Epoch 6/10
52/52 - 12s - loss: 281.2184 - loglik: -2.8015e+02 - logprior: -1.0700e+00
Fitted a model with MAP estimate = -279.7610
Time for alignment: 245.8855
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 467.7350 - loglik: -4.6598e+02 - logprior: -1.7544e+00
Epoch 2/10
39/39 - 7s - loss: 385.5923 - loglik: -3.8454e+02 - logprior: -1.0523e+00
Epoch 3/10
39/39 - 7s - loss: 376.7800 - loglik: -3.7576e+02 - logprior: -1.0221e+00
Epoch 4/10
39/39 - 7s - loss: 374.8011 - loglik: -3.7381e+02 - logprior: -9.9487e-01
Epoch 5/10
39/39 - 8s - loss: 373.8876 - loglik: -3.7290e+02 - logprior: -9.8300e-01
Epoch 6/10
39/39 - 8s - loss: 373.6551 - loglik: -3.7269e+02 - logprior: -9.6094e-01
Epoch 7/10
39/39 - 7s - loss: 373.4150 - loglik: -3.7245e+02 - logprior: -9.6036e-01
Epoch 8/10
39/39 - 7s - loss: 373.0333 - loglik: -3.7210e+02 - logprior: -9.3777e-01
Epoch 9/10
39/39 - 7s - loss: 373.1606 - loglik: -3.7223e+02 - logprior: -9.3409e-01
Fitted a model with MAP estimate = -309.3025
expansions: [(0, 19), (10, 1), (15, 1), (18, 1), (28, 1), (30, 2), (31, 3), (32, 1), (37, 2), (43, 1), (44, 1), (68, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 2), (108, 1), (126, 1), (130, 2), (134, 8)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 361.1287 - loglik: -3.5841e+02 - logprior: -2.7165e+00
Epoch 2/2
39/39 - 10s - loss: 346.0719 - loglik: -3.4420e+02 - logprior: -1.8690e+00
Fitted a model with MAP estimate = -287.1611
expansions: [(105, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17 146
 173 174 176 177 182 183 184]
Re-initialized the encoder parameters.
Fitting a model of length 163 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 354.4444 - loglik: -3.5238e+02 - logprior: -2.0609e+00
Epoch 2/2
39/39 - 9s - loss: 350.2498 - loglik: -3.4949e+02 - logprior: -7.5788e-01
Fitted a model with MAP estimate = -291.0325
expansions: [(0, 17), (156, 1), (157, 1)]
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 287.5576 - loglik: -2.8599e+02 - logprior: -1.5674e+00
Epoch 2/10
52/52 - 12s - loss: 281.3982 - loglik: -2.8015e+02 - logprior: -1.2472e+00
Epoch 3/10
52/52 - 11s - loss: 283.2014 - loglik: -2.8199e+02 - logprior: -1.2126e+00
Fitted a model with MAP estimate = -280.4931
Time for alignment: 213.4497
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 468.6807 - loglik: -4.6691e+02 - logprior: -1.7673e+00
Epoch 2/10
39/39 - 7s - loss: 387.7596 - loglik: -3.8666e+02 - logprior: -1.0982e+00
Epoch 3/10
39/39 - 8s - loss: 379.3878 - loglik: -3.7826e+02 - logprior: -1.1279e+00
Epoch 4/10
39/39 - 7s - loss: 377.9094 - loglik: -3.7681e+02 - logprior: -1.0963e+00
Epoch 5/10
39/39 - 8s - loss: 376.8455 - loglik: -3.7576e+02 - logprior: -1.0870e+00
Epoch 6/10
39/39 - 8s - loss: 377.1427 - loglik: -3.7605e+02 - logprior: -1.0903e+00
Fitted a model with MAP estimate = -310.0842
expansions: [(0, 19), (18, 1), (19, 1), (28, 1), (30, 1), (31, 2), (32, 1), (33, 1), (38, 1), (43, 1), (44, 3), (71, 2), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 2), (108, 1), (124, 6), (126, 2), (127, 1), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 360.4351 - loglik: -3.5776e+02 - logprior: -2.6787e+00
Epoch 2/2
39/39 - 10s - loss: 345.1553 - loglik: -3.4321e+02 - logprior: -1.9477e+00
Fitted a model with MAP estimate = -286.1606
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17 146
 175]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 350.0439 - loglik: -3.4794e+02 - logprior: -2.0996e+00
Epoch 2/2
39/39 - 9s - loss: 346.1598 - loglik: -3.4532e+02 - logprior: -8.4099e-01
Fitted a model with MAP estimate = -289.1304
expansions: [(0, 18)]
discards: [163]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 286.7584 - loglik: -2.8516e+02 - logprior: -1.6002e+00
Epoch 2/10
52/52 - 12s - loss: 280.3352 - loglik: -2.7907e+02 - logprior: -1.2653e+00
Epoch 3/10
52/52 - 13s - loss: 280.6458 - loglik: -2.7942e+02 - logprior: -1.2299e+00
Fitted a model with MAP estimate = -280.0584
Time for alignment: 197.3882
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 467.4666 - loglik: -4.6569e+02 - logprior: -1.7762e+00
Epoch 2/10
39/39 - 7s - loss: 384.9271 - loglik: -3.8383e+02 - logprior: -1.0954e+00
Epoch 3/10
39/39 - 8s - loss: 375.7455 - loglik: -3.7465e+02 - logprior: -1.0970e+00
Epoch 4/10
39/39 - 8s - loss: 373.5812 - loglik: -3.7251e+02 - logprior: -1.0707e+00
Epoch 5/10
39/39 - 8s - loss: 372.3075 - loglik: -3.7124e+02 - logprior: -1.0717e+00
Epoch 6/10
39/39 - 7s - loss: 370.5896 - loglik: -3.6949e+02 - logprior: -1.0970e+00
Epoch 7/10
39/39 - 8s - loss: 370.0359 - loglik: -3.6894e+02 - logprior: -1.0940e+00
Epoch 8/10
39/39 - 8s - loss: 369.4669 - loglik: -3.6836e+02 - logprior: -1.1100e+00
Epoch 9/10
39/39 - 7s - loss: 369.3624 - loglik: -3.6824e+02 - logprior: -1.1226e+00
Epoch 10/10
39/39 - 7s - loss: 369.0818 - loglik: -3.6796e+02 - logprior: -1.1188e+00
Fitted a model with MAP estimate = -306.6193
expansions: [(0, 19), (14, 1), (18, 1), (22, 1), (28, 1), (30, 1), (31, 3), (33, 1), (38, 1), (43, 2), (44, 1), (45, 1), (50, 1), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (102, 1), (104, 1), (107, 1), (114, 1), (125, 8), (130, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 356.3872 - loglik: -3.5368e+02 - logprior: -2.7033e+00
Epoch 2/2
39/39 - 11s - loss: 340.2838 - loglik: -3.3836e+02 - logprior: -1.9272e+00
Fitted a model with MAP estimate = -282.7810
expansions: [(171, 5)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  72
 182]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 342.2315 - loglik: -3.4012e+02 - logprior: -2.1101e+00
Epoch 2/2
39/39 - 10s - loss: 336.4492 - loglik: -3.3562e+02 - logprior: -8.2793e-01
Fitted a model with MAP estimate = -282.6326
expansions: [(0, 18)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 17s - loss: 279.8003 - loglik: -2.7821e+02 - logprior: -1.5867e+00
Epoch 2/10
52/52 - 13s - loss: 275.5413 - loglik: -2.7429e+02 - logprior: -1.2519e+00
Epoch 3/10
52/52 - 12s - loss: 272.6128 - loglik: -2.7141e+02 - logprior: -1.2056e+00
Epoch 4/10
52/52 - 13s - loss: 273.6310 - loglik: -2.7248e+02 - logprior: -1.1475e+00
Fitted a model with MAP estimate = -272.4673
Time for alignment: 241.2698
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 468.1388 - loglik: -4.6642e+02 - logprior: -1.7194e+00
Epoch 2/10
39/39 - 7s - loss: 389.8057 - loglik: -3.8895e+02 - logprior: -8.5560e-01
Epoch 3/10
39/39 - 8s - loss: 382.7892 - loglik: -3.8195e+02 - logprior: -8.3518e-01
Epoch 4/10
39/39 - 8s - loss: 380.9291 - loglik: -3.8008e+02 - logprior: -8.4529e-01
Epoch 5/10
39/39 - 8s - loss: 379.7117 - loglik: -3.7889e+02 - logprior: -8.2284e-01
Epoch 6/10
39/39 - 8s - loss: 379.0786 - loglik: -3.7828e+02 - logprior: -7.9949e-01
Epoch 7/10
39/39 - 7s - loss: 378.2354 - loglik: -3.7743e+02 - logprior: -8.0889e-01
Epoch 8/10
39/39 - 7s - loss: 377.5413 - loglik: -3.7672e+02 - logprior: -8.1656e-01
Epoch 9/10
39/39 - 7s - loss: 377.3917 - loglik: -3.7657e+02 - logprior: -8.1788e-01
Epoch 10/10
39/39 - 8s - loss: 377.0390 - loglik: -3.7622e+02 - logprior: -8.1837e-01
Fitted a model with MAP estimate = -311.2736
expansions: [(0, 20), (15, 1), (18, 1), (28, 1), (30, 1), (31, 5), (32, 1), (37, 2), (44, 2), (45, 1), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 1), (108, 1), (125, 2), (126, 5), (127, 1), (129, 2), (134, 7)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 362.3740 - loglik: -3.5969e+02 - logprior: -2.6806e+00
Epoch 2/2
39/39 - 11s - loss: 345.9447 - loglik: -3.4409e+02 - logprior: -1.8527e+00
Fitted a model with MAP estimate = -285.6503
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18
  57  76 167 168 169 180 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 354.3394 - loglik: -3.5231e+02 - logprior: -2.0336e+00
Epoch 2/2
39/39 - 9s - loss: 350.2571 - loglik: -3.4954e+02 - logprior: -7.1513e-01
Fitted a model with MAP estimate = -291.8495
expansions: [(0, 20)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 286.8127 - loglik: -2.8530e+02 - logprior: -1.5083e+00
Epoch 2/10
52/52 - 13s - loss: 284.7464 - loglik: -2.8359e+02 - logprior: -1.1603e+00
Epoch 3/10
52/52 - 12s - loss: 280.5416 - loglik: -2.7940e+02 - logprior: -1.1391e+00
Epoch 4/10
52/52 - 13s - loss: 281.0554 - loglik: -2.7996e+02 - logprior: -1.0977e+00
Fitted a model with MAP estimate = -280.2476
Time for alignment: 237.1508
Computed alignments with likelihoods: ['-279.7610', '-280.4931', '-280.0584', '-272.4673', '-280.2476']
Best model has likelihood: -272.4673  (prior= -1.0969 )
time for generating output: 0.6033
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.1908753214591866
Training of 5 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d294fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe98b0186d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea022744c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 817.7996 - loglik: -8.1630e+02 - logprior: -1.5019e+00
Epoch 2/10
37/37 - 20s - loss: 736.0149 - loglik: -7.3546e+02 - logprior: -5.5083e-01
Epoch 3/10
37/37 - 20s - loss: 722.0172 - loglik: -7.2153e+02 - logprior: -4.8359e-01
Epoch 4/10
37/37 - 20s - loss: 721.1608 - loglik: -7.2076e+02 - logprior: -4.0168e-01
Epoch 5/10
37/37 - 20s - loss: 716.5336 - loglik: -7.1616e+02 - logprior: -3.7413e-01
Epoch 6/10
37/37 - 20s - loss: 719.8707 - loglik: -7.1951e+02 - logprior: -3.6361e-01
Fitted a model with MAP estimate = -717.3157
expansions: [(0, 3), (30, 1), (32, 4), (48, 1), (65, 1), (75, 2), (91, 13), (98, 2), (133, 1), (197, 13), (210, 2), (239, 1)]
discards: [104 105 106 152 153 156 189]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 724.7515 - loglik: -7.2235e+02 - logprior: -2.3984e+00
Epoch 2/2
37/37 - 24s - loss: 710.5412 - loglik: -7.1002e+02 - logprior: -5.2556e-01
Fitted a model with MAP estimate = -706.8375
expansions: [(0, 3), (37, 2), (38, 1), (39, 1), (106, 5), (107, 3), (178, 2), (291, 2)]
discards: [85]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 715.3827 - loglik: -7.1260e+02 - logprior: -2.7856e+00
Epoch 2/2
37/37 - 26s - loss: 707.1434 - loglik: -7.0668e+02 - logprior: -4.6402e-01
Fitted a model with MAP estimate = -703.4268
expansions: [(261, 1), (309, 2)]
discards: [  0   1   2 191 306 307]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 30s - loss: 712.4389 - loglik: -7.1045e+02 - logprior: -1.9884e+00
Epoch 2/10
37/37 - 26s - loss: 706.5244 - loglik: -7.0629e+02 - logprior: -2.3326e-01
Epoch 3/10
37/37 - 26s - loss: 703.2413 - loglik: -7.0318e+02 - logprior: -6.2317e-02
Epoch 4/10
37/37 - 26s - loss: 701.3222 - loglik: -7.0136e+02 - logprior: 0.0423
Epoch 5/10
37/37 - 26s - loss: 700.8179 - loglik: -7.0095e+02 - logprior: 0.1278
Epoch 6/10
37/37 - 26s - loss: 702.2346 - loglik: -7.0243e+02 - logprior: 0.1913
Fitted a model with MAP estimate = -700.5963
Time for alignment: 512.9635
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 819.1285 - loglik: -8.1763e+02 - logprior: -1.5032e+00
Epoch 2/10
37/37 - 20s - loss: 738.0596 - loglik: -7.3763e+02 - logprior: -4.3132e-01
Epoch 3/10
37/37 - 20s - loss: 727.8598 - loglik: -7.2741e+02 - logprior: -4.5007e-01
Epoch 4/10
37/37 - 20s - loss: 722.8667 - loglik: -7.2244e+02 - logprior: -4.3025e-01
Epoch 5/10
37/37 - 20s - loss: 721.0571 - loglik: -7.2065e+02 - logprior: -4.0938e-01
Epoch 6/10
37/37 - 20s - loss: 722.5082 - loglik: -7.2209e+02 - logprior: -4.2107e-01
Fitted a model with MAP estimate = -720.8178
expansions: [(0, 3), (34, 3), (35, 2), (39, 1), (61, 2), (94, 3), (95, 7), (116, 2), (117, 1), (135, 3), (136, 1), (137, 3), (144, 1), (148, 10), (197, 13), (210, 2), (240, 1), (244, 1)]
discards: [101 102 103 104 153 154 155]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 725.2108 - loglik: -7.2283e+02 - logprior: -2.3779e+00
Epoch 2/2
37/37 - 26s - loss: 708.3373 - loglik: -7.0778e+02 - logprior: -5.5511e-01
Fitted a model with MAP estimate = -703.9115
expansions: [(0, 3), (38, 1), (43, 1), (161, 1), (162, 1), (192, 1), (306, 2)]
discards: [ 70 107 108 109 157 180 181]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 712.8123 - loglik: -7.1017e+02 - logprior: -2.6384e+00
Epoch 2/2
37/37 - 26s - loss: 705.4312 - loglik: -7.0496e+02 - logprior: -4.7233e-01
Fitted a model with MAP estimate = -702.3456
expansions: [(259, 2)]
discards: [  1   2 294 306 307]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 710.3298 - loglik: -7.0860e+02 - logprior: -1.7255e+00
Epoch 2/10
37/37 - 26s - loss: 705.7993 - loglik: -7.0556e+02 - logprior: -2.3898e-01
Epoch 3/10
37/37 - 26s - loss: 699.2375 - loglik: -6.9914e+02 - logprior: -9.2759e-02
Epoch 4/10
37/37 - 26s - loss: 701.2886 - loglik: -7.0129e+02 - logprior: 0.0037
Fitted a model with MAP estimate = -699.2279
Time for alignment: 467.6504
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 820.7258 - loglik: -8.1920e+02 - logprior: -1.5255e+00
Epoch 2/10
37/37 - 20s - loss: 733.2142 - loglik: -7.3263e+02 - logprior: -5.8537e-01
Epoch 3/10
37/37 - 20s - loss: 720.8936 - loglik: -7.2035e+02 - logprior: -5.4085e-01
Epoch 4/10
37/37 - 20s - loss: 716.9739 - loglik: -7.1642e+02 - logprior: -5.5397e-01
Epoch 5/10
37/37 - 20s - loss: 719.4265 - loglik: -7.1891e+02 - logprior: -5.1867e-01
Fitted a model with MAP estimate = -716.6189
expansions: [(0, 3), (30, 1), (32, 4), (35, 1), (65, 1), (71, 2), (90, 17), (98, 1), (138, 1), (160, 5), (198, 14), (243, 1)]
discards: [103 105 121 151 153 154 155 162 163 164 189 190]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 724.2608 - loglik: -7.2184e+02 - logprior: -2.4209e+00
Epoch 2/2
37/37 - 24s - loss: 710.6266 - loglik: -7.1006e+02 - logprior: -5.6835e-01
Fitted a model with MAP estimate = -706.8850
expansions: [(0, 3), (38, 1), (41, 2), (109, 1), (180, 4), (232, 2), (293, 2)]
discards: [186 187 188]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 714.7972 - loglik: -7.1205e+02 - logprior: -2.7493e+00
Epoch 2/2
37/37 - 26s - loss: 705.5406 - loglik: -7.0484e+02 - logprior: -7.0442e-01
Fitted a model with MAP estimate = -703.7867
expansions: [(47, 1)]
discards: [  1   2  88 187 188 189 190 302 303]
Re-initialized the encoder parameters.
Fitting a model of length 297 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 712.6962 - loglik: -7.1086e+02 - logprior: -1.8329e+00
Epoch 2/10
37/37 - 25s - loss: 708.6715 - loglik: -7.0828e+02 - logprior: -3.9261e-01
Epoch 3/10
37/37 - 25s - loss: 703.3232 - loglik: -7.0308e+02 - logprior: -2.3829e-01
Epoch 4/10
37/37 - 25s - loss: 702.4282 - loglik: -7.0228e+02 - logprior: -1.4378e-01
Epoch 5/10
37/37 - 25s - loss: 702.8440 - loglik: -7.0279e+02 - logprior: -5.5513e-02
Fitted a model with MAP estimate = -701.6933
Time for alignment: 461.1129
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 24s - loss: 819.1749 - loglik: -8.1765e+02 - logprior: -1.5228e+00
Epoch 2/10
37/37 - 20s - loss: 735.2972 - loglik: -7.3478e+02 - logprior: -5.2027e-01
Epoch 3/10
37/37 - 20s - loss: 723.5784 - loglik: -7.2303e+02 - logprior: -5.4373e-01
Epoch 4/10
37/37 - 20s - loss: 719.0131 - loglik: -7.1848e+02 - logprior: -5.3620e-01
Epoch 5/10
37/37 - 20s - loss: 718.5478 - loglik: -7.1800e+02 - logprior: -5.5253e-01
Epoch 6/10
37/37 - 20s - loss: 717.3346 - loglik: -7.1677e+02 - logprior: -5.6119e-01
Epoch 7/10
37/37 - 20s - loss: 716.8030 - loglik: -7.1624e+02 - logprior: -5.6741e-01
Epoch 8/10
37/37 - 20s - loss: 716.8110 - loglik: -7.1625e+02 - logprior: -5.5989e-01
Fitted a model with MAP estimate = -716.7666
expansions: [(0, 3), (30, 1), (32, 5), (45, 1), (66, 1), (90, 15), (98, 3), (140, 1), (156, 10), (197, 14), (210, 2), (239, 1), (247, 1)]
discards: [102 104 105 151 152 153 154 157 159 160 161 162 163 164 165 166]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 724.4372 - loglik: -7.2204e+02 - logprior: -2.3959e+00
Epoch 2/2
37/37 - 25s - loss: 708.0153 - loglik: -7.0752e+02 - logprior: -4.9363e-01
Fitted a model with MAP estimate = -704.9844
expansions: [(0, 3), (38, 1), (39, 1), (106, 2), (107, 7), (178, 3), (230, 1)]
discards: [129]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 713.6393 - loglik: -7.1087e+02 - logprior: -2.7677e+00
Epoch 2/2
37/37 - 27s - loss: 705.4921 - loglik: -7.0506e+02 - logprior: -4.3692e-01
Fitted a model with MAP estimate = -702.0675
expansions: [(0, 2), (205, 6), (265, 2), (313, 2)]
discards: [  0   1   2 106 113 114 115 191 192 193 194 195 196 197 198]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 713.1755 - loglik: -7.1109e+02 - logprior: -2.0828e+00
Epoch 2/10
37/37 - 27s - loss: 706.1263 - loglik: -7.0576e+02 - logprior: -3.6760e-01
Epoch 3/10
37/37 - 27s - loss: 703.0035 - loglik: -7.0278e+02 - logprior: -2.2174e-01
Epoch 4/10
37/37 - 27s - loss: 702.6020 - loglik: -7.0250e+02 - logprior: -9.9954e-02
Epoch 5/10
37/37 - 26s - loss: 698.9363 - loglik: -6.9894e+02 - logprior: 0.0061
Epoch 6/10
37/37 - 27s - loss: 699.0140 - loglik: -6.9911e+02 - logprior: 0.0986
Fitted a model with MAP estimate = -699.5205
Time for alignment: 559.7088
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 819.4463 - loglik: -8.1793e+02 - logprior: -1.5118e+00
Epoch 2/10
37/37 - 20s - loss: 734.9572 - loglik: -7.3442e+02 - logprior: -5.3571e-01
Epoch 3/10
37/37 - 20s - loss: 721.5599 - loglik: -7.2105e+02 - logprior: -5.0880e-01
Epoch 4/10
37/37 - 20s - loss: 717.7092 - loglik: -7.1723e+02 - logprior: -4.7578e-01
Epoch 5/10
37/37 - 20s - loss: 715.7674 - loglik: -7.1526e+02 - logprior: -5.0489e-01
Epoch 6/10
37/37 - 20s - loss: 717.7897 - loglik: -7.1709e+02 - logprior: -7.0220e-01
Fitted a model with MAP estimate = -715.8670
expansions: [(0, 3), (30, 2), (32, 2), (34, 3), (53, 1), (66, 1), (90, 16), (98, 1), (132, 1), (157, 5), (197, 13), (210, 2), (240, 1), (247, 1)]
discards: [103 105 140 141 142 151 152 158 161 162 163 164 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 724.6305 - loglik: -7.2221e+02 - logprior: -2.4181e+00
Epoch 2/2
37/37 - 24s - loss: 710.2672 - loglik: -7.0956e+02 - logprior: -7.0471e-01
Fitted a model with MAP estimate = -706.3590
expansions: [(0, 3), (39, 1), (40, 1), (108, 6), (109, 2), (170, 3), (171, 9), (292, 2)]
discards: [ 34 174 175 176 182 183]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 715.1442 - loglik: -7.1242e+02 - logprior: -2.7242e+00
Epoch 2/2
37/37 - 27s - loss: 705.6713 - loglik: -7.0513e+02 - logprior: -5.3853e-01
Fitted a model with MAP estimate = -702.7933
expansions: [(197, 2), (198, 4), (203, 3), (263, 2), (313, 2)]
discards: [  0   1   2 106 114 115 116 183 184 310 311]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 32s - loss: 711.1589 - loglik: -7.0914e+02 - logprior: -2.0140e+00
Epoch 2/10
37/37 - 27s - loss: 703.9684 - loglik: -7.0363e+02 - logprior: -3.3465e-01
Epoch 3/10
37/37 - 27s - loss: 698.5960 - loglik: -6.9840e+02 - logprior: -1.9590e-01
Epoch 4/10
37/37 - 27s - loss: 698.5847 - loglik: -6.9852e+02 - logprior: -6.2007e-02
Epoch 5/10
37/37 - 28s - loss: 698.4598 - loglik: -6.9848e+02 - logprior: 0.0232
Epoch 6/10
37/37 - 27s - loss: 698.4266 - loglik: -6.9853e+02 - logprior: 0.1077
Epoch 7/10
37/37 - 27s - loss: 697.6984 - loglik: -6.9786e+02 - logprior: 0.1639
Epoch 8/10
37/37 - 27s - loss: 696.7758 - loglik: -6.9680e+02 - logprior: 0.0249
Epoch 9/10
37/37 - 27s - loss: 696.9478 - loglik: -6.9721e+02 - logprior: 0.2573
Fitted a model with MAP estimate = -696.4201
Time for alignment: 606.6306
Computed alignments with likelihoods: ['-700.5963', '-699.2279', '-701.6933', '-699.5205', '-696.4201']
Best model has likelihood: -696.4201  (prior= 0.3695 )
time for generating output: 0.2741
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.8488517745302714
Training of 5 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaa60b0280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9acaf2220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb4983c760>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.6526 - loglik: -2.7348e+02 - logprior: -1.1317e+02
Epoch 2/10
10/10 - 1s - loss: 288.8616 - loglik: -2.6148e+02 - logprior: -2.7378e+01
Epoch 3/10
10/10 - 1s - loss: 258.7217 - loglik: -2.4880e+02 - logprior: -9.9255e+00
Epoch 4/10
10/10 - 1s - loss: 243.0109 - loglik: -2.3960e+02 - logprior: -3.4063e+00
Epoch 5/10
10/10 - 1s - loss: 233.8235 - loglik: -2.3363e+02 - logprior: -1.9391e-01
Epoch 6/10
10/10 - 1s - loss: 228.4869 - loglik: -2.2981e+02 - logprior: 1.3205
Epoch 7/10
10/10 - 1s - loss: 226.0174 - loglik: -2.2831e+02 - logprior: 2.2915
Epoch 8/10
10/10 - 1s - loss: 224.5873 - loglik: -2.2761e+02 - logprior: 3.0242
Epoch 9/10
10/10 - 1s - loss: 223.6893 - loglik: -2.2737e+02 - logprior: 3.6818
Epoch 10/10
10/10 - 1s - loss: 223.0629 - loglik: -2.2730e+02 - logprior: 4.2369
Fitted a model with MAP estimate = -222.7721
expansions: [(0, 6), (36, 3), (37, 5), (49, 4), (76, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 374.2124 - loglik: -2.2890e+02 - logprior: -1.4531e+02
Epoch 2/2
10/10 - 1s - loss: 265.8913 - loglik: -2.2497e+02 - logprior: -4.0922e+01
Fitted a model with MAP estimate = -246.5463
expansions: [(0, 5), (59, 3)]
discards: [ 0  1  2  3  4  5 94 95 96 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 341.3565 - loglik: -2.2413e+02 - logprior: -1.1723e+02
Epoch 2/2
10/10 - 1s - loss: 252.5146 - loglik: -2.2330e+02 - logprior: -2.9214e+01
Fitted a model with MAP estimate = -237.0859
expansions: [(0, 4), (44, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 327.5376 - loglik: -2.2281e+02 - logprior: -1.0473e+02
Epoch 2/10
10/10 - 1s - loss: 245.0918 - loglik: -2.2258e+02 - logprior: -2.2512e+01
Epoch 3/10
10/10 - 1s - loss: 228.0968 - loglik: -2.2291e+02 - logprior: -5.1874e+00
Epoch 4/10
10/10 - 1s - loss: 221.1303 - loglik: -2.2309e+02 - logprior: 1.9646
Epoch 5/10
10/10 - 1s - loss: 217.2374 - loglik: -2.2313e+02 - logprior: 5.8906
Epoch 6/10
10/10 - 1s - loss: 214.8029 - loglik: -2.2292e+02 - logprior: 8.1191
Epoch 7/10
10/10 - 1s - loss: 213.1417 - loglik: -2.2260e+02 - logprior: 9.4624
Epoch 8/10
10/10 - 1s - loss: 211.9596 - loglik: -2.2231e+02 - logprior: 10.3478
Epoch 9/10
10/10 - 1s - loss: 211.0546 - loglik: -2.2216e+02 - logprior: 11.1042
Epoch 10/10
10/10 - 1s - loss: 210.2968 - loglik: -2.2213e+02 - logprior: 11.8333
Fitted a model with MAP estimate = -209.9105
Time for alignment: 34.8321
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.6526 - loglik: -2.7348e+02 - logprior: -1.1317e+02
Epoch 2/10
10/10 - 1s - loss: 288.8616 - loglik: -2.6148e+02 - logprior: -2.7378e+01
Epoch 3/10
10/10 - 1s - loss: 258.7217 - loglik: -2.4880e+02 - logprior: -9.9255e+00
Epoch 4/10
10/10 - 1s - loss: 243.0109 - loglik: -2.3960e+02 - logprior: -3.4063e+00
Epoch 5/10
10/10 - 1s - loss: 233.8235 - loglik: -2.3363e+02 - logprior: -1.9391e-01
Epoch 6/10
10/10 - 1s - loss: 228.4869 - loglik: -2.2981e+02 - logprior: 1.3205
Epoch 7/10
10/10 - 1s - loss: 226.0174 - loglik: -2.2831e+02 - logprior: 2.2915
Epoch 8/10
10/10 - 1s - loss: 224.5873 - loglik: -2.2761e+02 - logprior: 3.0243
Epoch 9/10
10/10 - 1s - loss: 223.6893 - loglik: -2.2737e+02 - logprior: 3.6818
Epoch 10/10
10/10 - 1s - loss: 223.0629 - loglik: -2.2730e+02 - logprior: 4.2369
Fitted a model with MAP estimate = -222.7722
expansions: [(0, 6), (36, 3), (37, 5), (49, 4), (76, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 374.2124 - loglik: -2.2890e+02 - logprior: -1.4531e+02
Epoch 2/2
10/10 - 1s - loss: 265.8913 - loglik: -2.2497e+02 - logprior: -4.0922e+01
Fitted a model with MAP estimate = -246.5464
expansions: [(0, 5), (59, 3)]
discards: [ 0  1  2  3  4  5 94 95 96 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 341.3565 - loglik: -2.2413e+02 - logprior: -1.1723e+02
Epoch 2/2
10/10 - 1s - loss: 252.5147 - loglik: -2.2330e+02 - logprior: -2.9214e+01
Fitted a model with MAP estimate = -237.0858
expansions: [(0, 4), (44, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 327.5377 - loglik: -2.2281e+02 - logprior: -1.0473e+02
Epoch 2/10
10/10 - 1s - loss: 245.0919 - loglik: -2.2258e+02 - logprior: -2.2512e+01
Epoch 3/10
10/10 - 1s - loss: 228.0968 - loglik: -2.2291e+02 - logprior: -5.1874e+00
Epoch 4/10
10/10 - 1s - loss: 221.1303 - loglik: -2.2309e+02 - logprior: 1.9646
Epoch 5/10
10/10 - 1s - loss: 217.2374 - loglik: -2.2313e+02 - logprior: 5.8906
Epoch 6/10
10/10 - 1s - loss: 214.8029 - loglik: -2.2292e+02 - logprior: 8.1191
Epoch 7/10
10/10 - 1s - loss: 213.1418 - loglik: -2.2260e+02 - logprior: 9.4624
Epoch 8/10
10/10 - 1s - loss: 211.9597 - loglik: -2.2231e+02 - logprior: 10.3478
Epoch 9/10
10/10 - 1s - loss: 211.0545 - loglik: -2.2216e+02 - logprior: 11.1042
Epoch 10/10
10/10 - 1s - loss: 210.2968 - loglik: -2.2213e+02 - logprior: 11.8334
Fitted a model with MAP estimate = -209.9107
Time for alignment: 40.4223
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.6526 - loglik: -2.7348e+02 - logprior: -1.1317e+02
Epoch 2/10
10/10 - 1s - loss: 288.8616 - loglik: -2.6148e+02 - logprior: -2.7378e+01
Epoch 3/10
10/10 - 1s - loss: 258.7217 - loglik: -2.4880e+02 - logprior: -9.9255e+00
Epoch 4/10
10/10 - 1s - loss: 243.0109 - loglik: -2.3960e+02 - logprior: -3.4063e+00
Epoch 5/10
10/10 - 1s - loss: 233.8235 - loglik: -2.3363e+02 - logprior: -1.9391e-01
Epoch 6/10
10/10 - 1s - loss: 228.4870 - loglik: -2.2981e+02 - logprior: 1.3205
Epoch 7/10
10/10 - 1s - loss: 226.0174 - loglik: -2.2831e+02 - logprior: 2.2915
Epoch 8/10
10/10 - 1s - loss: 224.5873 - loglik: -2.2761e+02 - logprior: 3.0243
Epoch 9/10
10/10 - 1s - loss: 223.6893 - loglik: -2.2737e+02 - logprior: 3.6818
Epoch 10/10
10/10 - 1s - loss: 223.0629 - loglik: -2.2730e+02 - logprior: 4.2369
Fitted a model with MAP estimate = -222.7722
expansions: [(0, 6), (36, 3), (37, 5), (49, 4), (76, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 374.2124 - loglik: -2.2890e+02 - logprior: -1.4531e+02
Epoch 2/2
10/10 - 1s - loss: 265.8913 - loglik: -2.2497e+02 - logprior: -4.0922e+01
Fitted a model with MAP estimate = -246.5463
expansions: [(0, 5), (59, 3)]
discards: [ 0  1  2  3  4  5 94 95 96 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 341.3565 - loglik: -2.2413e+02 - logprior: -1.1723e+02
Epoch 2/2
10/10 - 1s - loss: 252.5146 - loglik: -2.2330e+02 - logprior: -2.9214e+01
Fitted a model with MAP estimate = -237.0858
expansions: [(0, 4), (44, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 327.5376 - loglik: -2.2281e+02 - logprior: -1.0473e+02
Epoch 2/10
10/10 - 1s - loss: 245.0919 - loglik: -2.2258e+02 - logprior: -2.2512e+01
Epoch 3/10
10/10 - 1s - loss: 228.0968 - loglik: -2.2291e+02 - logprior: -5.1874e+00
Epoch 4/10
10/10 - 1s - loss: 221.1304 - loglik: -2.2309e+02 - logprior: 1.9646
Epoch 5/10
10/10 - 1s - loss: 217.2374 - loglik: -2.2313e+02 - logprior: 5.8906
Epoch 6/10
10/10 - 1s - loss: 214.8029 - loglik: -2.2292e+02 - logprior: 8.1191
Epoch 7/10
10/10 - 1s - loss: 213.1417 - loglik: -2.2260e+02 - logprior: 9.4623
Epoch 8/10
10/10 - 1s - loss: 211.9596 - loglik: -2.2231e+02 - logprior: 10.3477
Epoch 9/10
10/10 - 1s - loss: 211.0546 - loglik: -2.2216e+02 - logprior: 11.1042
Epoch 10/10
10/10 - 1s - loss: 210.2968 - loglik: -2.2213e+02 - logprior: 11.8333
Fitted a model with MAP estimate = -209.9103
Time for alignment: 37.5007
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.6526 - loglik: -2.7348e+02 - logprior: -1.1317e+02
Epoch 2/10
10/10 - 1s - loss: 288.8616 - loglik: -2.6148e+02 - logprior: -2.7378e+01
Epoch 3/10
10/10 - 1s - loss: 258.7217 - loglik: -2.4880e+02 - logprior: -9.9255e+00
Epoch 4/10
10/10 - 1s - loss: 243.0109 - loglik: -2.3960e+02 - logprior: -3.4063e+00
Epoch 5/10
10/10 - 1s - loss: 233.8236 - loglik: -2.3363e+02 - logprior: -1.9391e-01
Epoch 6/10
10/10 - 1s - loss: 228.4869 - loglik: -2.2981e+02 - logprior: 1.3205
Epoch 7/10
10/10 - 1s - loss: 226.0174 - loglik: -2.2831e+02 - logprior: 2.2915
Epoch 8/10
10/10 - 1s - loss: 224.5873 - loglik: -2.2761e+02 - logprior: 3.0243
Epoch 9/10
10/10 - 1s - loss: 223.6893 - loglik: -2.2737e+02 - logprior: 3.6818
Epoch 10/10
10/10 - 1s - loss: 223.0629 - loglik: -2.2730e+02 - logprior: 4.2369
Fitted a model with MAP estimate = -222.7721
expansions: [(0, 6), (36, 3), (37, 5), (49, 4), (76, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 374.2124 - loglik: -2.2890e+02 - logprior: -1.4531e+02
Epoch 2/2
10/10 - 1s - loss: 265.8913 - loglik: -2.2497e+02 - logprior: -4.0922e+01
Fitted a model with MAP estimate = -246.5464
expansions: [(0, 5), (59, 3)]
discards: [ 0  1  2  3  4  5 94 95 96 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 341.3565 - loglik: -2.2413e+02 - logprior: -1.1723e+02
Epoch 2/2
10/10 - 1s - loss: 252.5147 - loglik: -2.2330e+02 - logprior: -2.9214e+01
Fitted a model with MAP estimate = -237.0858
expansions: [(0, 4), (44, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 327.5376 - loglik: -2.2281e+02 - logprior: -1.0473e+02
Epoch 2/10
10/10 - 1s - loss: 245.0919 - loglik: -2.2258e+02 - logprior: -2.2512e+01
Epoch 3/10
10/10 - 1s - loss: 228.0968 - loglik: -2.2291e+02 - logprior: -5.1874e+00
Epoch 4/10
10/10 - 1s - loss: 221.1303 - loglik: -2.2309e+02 - logprior: 1.9646
Epoch 5/10
10/10 - 1s - loss: 217.2374 - loglik: -2.2313e+02 - logprior: 5.8906
Epoch 6/10
10/10 - 1s - loss: 214.8029 - loglik: -2.2292e+02 - logprior: 8.1191
Epoch 7/10
10/10 - 1s - loss: 213.1418 - loglik: -2.2260e+02 - logprior: 9.4624
Epoch 8/10
10/10 - 1s - loss: 211.9595 - loglik: -2.2231e+02 - logprior: 10.3478
Epoch 9/10
10/10 - 1s - loss: 211.0544 - loglik: -2.2216e+02 - logprior: 11.1042
Epoch 10/10
10/10 - 1s - loss: 210.2967 - loglik: -2.2213e+02 - logprior: 11.8334
Fitted a model with MAP estimate = -209.9107
Time for alignment: 35.7443
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.6526 - loglik: -2.7348e+02 - logprior: -1.1317e+02
Epoch 2/10
10/10 - 1s - loss: 288.8616 - loglik: -2.6148e+02 - logprior: -2.7378e+01
Epoch 3/10
10/10 - 1s - loss: 258.7217 - loglik: -2.4880e+02 - logprior: -9.9255e+00
Epoch 4/10
10/10 - 1s - loss: 243.0109 - loglik: -2.3960e+02 - logprior: -3.4063e+00
Epoch 5/10
10/10 - 1s - loss: 233.8236 - loglik: -2.3363e+02 - logprior: -1.9391e-01
Epoch 6/10
10/10 - 1s - loss: 228.4870 - loglik: -2.2981e+02 - logprior: 1.3205
Epoch 7/10
10/10 - 1s - loss: 226.0174 - loglik: -2.2831e+02 - logprior: 2.2915
Epoch 8/10
10/10 - 1s - loss: 224.5874 - loglik: -2.2761e+02 - logprior: 3.0243
Epoch 9/10
10/10 - 1s - loss: 223.6893 - loglik: -2.2737e+02 - logprior: 3.6818
Epoch 10/10
10/10 - 1s - loss: 223.0629 - loglik: -2.2730e+02 - logprior: 4.2369
Fitted a model with MAP estimate = -222.7720
expansions: [(0, 6), (36, 3), (37, 5), (49, 4), (76, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 99 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 374.2125 - loglik: -2.2890e+02 - logprior: -1.4531e+02
Epoch 2/2
10/10 - 1s - loss: 265.8913 - loglik: -2.2497e+02 - logprior: -4.0922e+01
Fitted a model with MAP estimate = -246.5463
expansions: [(0, 5), (59, 3)]
discards: [ 0  1  2  3  4  5 94 95 96 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 341.3565 - loglik: -2.2413e+02 - logprior: -1.1723e+02
Epoch 2/2
10/10 - 1s - loss: 252.5147 - loglik: -2.2330e+02 - logprior: -2.9214e+01
Fitted a model with MAP estimate = -237.0859
expansions: [(0, 4), (44, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 327.5377 - loglik: -2.2281e+02 - logprior: -1.0473e+02
Epoch 2/10
10/10 - 1s - loss: 245.0919 - loglik: -2.2258e+02 - logprior: -2.2512e+01
Epoch 3/10
10/10 - 1s - loss: 228.0968 - loglik: -2.2291e+02 - logprior: -5.1875e+00
Epoch 4/10
10/10 - 1s - loss: 221.1303 - loglik: -2.2309e+02 - logprior: 1.9645
Epoch 5/10
10/10 - 1s - loss: 217.2374 - loglik: -2.2313e+02 - logprior: 5.8906
Epoch 6/10
10/10 - 1s - loss: 214.8030 - loglik: -2.2292e+02 - logprior: 8.1191
Epoch 7/10
10/10 - 1s - loss: 213.1418 - loglik: -2.2260e+02 - logprior: 9.4623
Epoch 8/10
10/10 - 1s - loss: 211.9598 - loglik: -2.2231e+02 - logprior: 10.3478
Epoch 9/10
10/10 - 1s - loss: 211.0548 - loglik: -2.2216e+02 - logprior: 11.1041
Epoch 10/10
10/10 - 1s - loss: 210.2969 - loglik: -2.2213e+02 - logprior: 11.8332
Fitted a model with MAP estimate = -209.9105
Time for alignment: 35.9441
Computed alignments with likelihoods: ['-209.9105', '-209.9107', '-209.9103', '-209.9107', '-209.9105']
Best model has likelihood: -209.9103  (prior= 12.2174 )
time for generating output: 0.1353
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.6973079448456992
Training of 5 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb169e0af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb05af9d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9fa19a8b0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 1001.4727 - loglik: -9.9390e+02 - logprior: -7.5763e+00
Epoch 2/10
19/19 - 17s - loss: 873.7552 - loglik: -8.7498e+02 - logprior: 1.2220
Epoch 3/10
19/19 - 17s - loss: 818.5428 - loglik: -8.1914e+02 - logprior: 0.6016
Epoch 4/10
19/19 - 17s - loss: 794.4551 - loglik: -7.9489e+02 - logprior: 0.4362
Epoch 5/10
19/19 - 17s - loss: 800.5354 - loglik: -8.0093e+02 - logprior: 0.3986
Fitted a model with MAP estimate = -792.9832
expansions: [(32, 1), (68, 1), (101, 4), (116, 1), (120, 1), (121, 5), (123, 5), (125, 2), (126, 2), (144, 1), (145, 1), (147, 1), (165, 2), (167, 1), (168, 1), (169, 1), (170, 2), (173, 1), (178, 1), (179, 1), (191, 3), (199, 1), (204, 1), (205, 1), (212, 1), (221, 2), (222, 2), (227, 1), (237, 3), (265, 7), (301, 1), (302, 1), (304, 1), (311, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 802.6690 - loglik: -7.9198e+02 - logprior: -1.0685e+01
Epoch 2/2
19/19 - 22s - loss: 772.1719 - loglik: -7.6984e+02 - logprior: -2.3288e+00
Fitted a model with MAP estimate = -765.1528
expansions: [(0, 2), (226, 1), (251, 1), (284, 1), (376, 1)]
discards: [  0 103 104 129 130 139 200 316]
Re-initialized the encoder parameters.
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 775.8133 - loglik: -7.6966e+02 - logprior: -6.1503e+00
Epoch 2/2
19/19 - 22s - loss: 758.3904 - loglik: -7.6082e+02 - logprior: 2.4268
Fitted a model with MAP estimate = -756.7378
expansions: []
discards: [  0 142 260]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 28s - loss: 778.7982 - loglik: -7.6936e+02 - logprior: -9.4394e+00
Epoch 2/10
19/19 - 22s - loss: 763.4381 - loglik: -7.6376e+02 - logprior: 0.3170
Epoch 3/10
19/19 - 22s - loss: 758.3551 - loglik: -7.6213e+02 - logprior: 3.7705
Epoch 4/10
19/19 - 22s - loss: 750.2657 - loglik: -7.5452e+02 - logprior: 4.2528
Epoch 5/10
19/19 - 22s - loss: 750.5740 - loglik: -7.5507e+02 - logprior: 4.4947
Fitted a model with MAP estimate = -749.7456
Time for alignment: 365.8941
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 999.4105 - loglik: -9.9184e+02 - logprior: -7.5682e+00
Epoch 2/10
19/19 - 17s - loss: 871.4815 - loglik: -8.7250e+02 - logprior: 1.0165
Epoch 3/10
19/19 - 17s - loss: 817.8660 - loglik: -8.1802e+02 - logprior: 0.1569
Epoch 4/10
19/19 - 17s - loss: 796.7710 - loglik: -7.9672e+02 - logprior: -5.3959e-02
Epoch 5/10
19/19 - 17s - loss: 792.2299 - loglik: -7.9219e+02 - logprior: -4.0766e-02
Epoch 6/10
19/19 - 17s - loss: 789.5117 - loglik: -7.8934e+02 - logprior: -1.7130e-01
Epoch 7/10
19/19 - 17s - loss: 789.9482 - loglik: -7.8977e+02 - logprior: -1.7827e-01
Fitted a model with MAP estimate = -788.6561
expansions: [(16, 1), (65, 1), (66, 1), (98, 1), (99, 3), (106, 1), (113, 1), (114, 1), (117, 1), (118, 1), (119, 1), (122, 7), (141, 1), (163, 1), (165, 4), (168, 1), (176, 1), (178, 1), (190, 4), (198, 1), (201, 1), (204, 1), (210, 1), (211, 1), (220, 1), (221, 2), (223, 1), (236, 1), (237, 1), (238, 1), (259, 2), (264, 6), (285, 1), (295, 4), (301, 1), (302, 1), (303, 2), (304, 2), (311, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 801.5873 - loglik: -7.9045e+02 - logprior: -1.1135e+01
Epoch 2/2
19/19 - 23s - loss: 766.3228 - loglik: -7.6285e+02 - logprior: -3.4716e+00
Fitted a model with MAP estimate = -759.2210
expansions: [(0, 2), (349, 3), (380, 1)]
discards: [  0 102 103 188 304 310]
Re-initialized the encoder parameters.
Fitting a model of length 386 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 766.3719 - loglik: -7.5997e+02 - logprior: -6.4008e+00
Epoch 2/2
19/19 - 23s - loss: 751.8124 - loglik: -7.5417e+02 - logprior: 2.3570
Fitted a model with MAP estimate = -747.6453
expansions: []
discards: [  0 348]
Re-initialized the encoder parameters.
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 770.4413 - loglik: -7.6064e+02 - logprior: -9.8053e+00
Epoch 2/10
19/19 - 23s - loss: 755.4222 - loglik: -7.5577e+02 - logprior: 0.3528
Epoch 3/10
19/19 - 23s - loss: 748.3773 - loglik: -7.5213e+02 - logprior: 3.7518
Epoch 4/10
19/19 - 23s - loss: 742.4653 - loglik: -7.4678e+02 - logprior: 4.3121
Epoch 5/10
19/19 - 23s - loss: 742.1097 - loglik: -7.4663e+02 - logprior: 4.5168
Epoch 6/10
19/19 - 23s - loss: 742.5157 - loglik: -7.4727e+02 - logprior: 4.7536
Fitted a model with MAP estimate = -739.8822
Time for alignment: 426.2543
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 996.9354 - loglik: -9.8939e+02 - logprior: -7.5418e+00
Epoch 2/10
19/19 - 17s - loss: 876.6840 - loglik: -8.7791e+02 - logprior: 1.2284
Epoch 3/10
19/19 - 17s - loss: 816.5659 - loglik: -8.1716e+02 - logprior: 0.5905
Epoch 4/10
19/19 - 17s - loss: 799.0468 - loglik: -7.9936e+02 - logprior: 0.3096
Epoch 5/10
19/19 - 17s - loss: 795.6379 - loglik: -7.9596e+02 - logprior: 0.3223
Epoch 6/10
19/19 - 17s - loss: 790.1357 - loglik: -7.9036e+02 - logprior: 0.2271
Epoch 7/10
19/19 - 17s - loss: 792.7291 - loglik: -7.9294e+02 - logprior: 0.2148
Fitted a model with MAP estimate = -790.8133
expansions: [(32, 2), (40, 1), (114, 1), (115, 3), (118, 1), (119, 1), (120, 1), (122, 7), (142, 1), (143, 1), (157, 1), (164, 1), (166, 1), (167, 1), (168, 1), (169, 1), (178, 1), (179, 1), (191, 2), (192, 1), (197, 1), (199, 1), (205, 1), (219, 2), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (238, 3), (264, 1), (265, 6), (303, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 375 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 802.4056 - loglik: -7.9176e+02 - logprior: -1.0645e+01
Epoch 2/2
19/19 - 22s - loss: 773.1159 - loglik: -7.7100e+02 - logprior: -2.1138e+00
Fitted a model with MAP estimate = -765.1860
expansions: [(0, 2), (80, 1), (279, 1), (354, 1)]
discards: [  0  31 118 252]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 775.3761 - loglik: -7.6917e+02 - logprior: -6.2039e+00
Epoch 2/2
19/19 - 22s - loss: 759.6697 - loglik: -7.6207e+02 - logprior: 2.3971
Fitted a model with MAP estimate = -756.0103
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 375 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 777.2316 - loglik: -7.6771e+02 - logprior: -9.5241e+00
Epoch 2/10
19/19 - 22s - loss: 763.8576 - loglik: -7.6433e+02 - logprior: 0.4732
Epoch 3/10
19/19 - 22s - loss: 754.0820 - loglik: -7.5770e+02 - logprior: 3.6182
Epoch 4/10
19/19 - 22s - loss: 749.7199 - loglik: -7.5382e+02 - logprior: 4.0990
Epoch 5/10
19/19 - 22s - loss: 751.5779 - loglik: -7.5604e+02 - logprior: 4.4591
Fitted a model with MAP estimate = -748.6019
Time for alignment: 392.5254
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 1002.1353 - loglik: -9.9458e+02 - logprior: -7.5596e+00
Epoch 2/10
19/19 - 17s - loss: 874.8931 - loglik: -8.7603e+02 - logprior: 1.1401
Epoch 3/10
19/19 - 17s - loss: 816.0583 - loglik: -8.1663e+02 - logprior: 0.5698
Epoch 4/10
19/19 - 17s - loss: 804.9079 - loglik: -8.0523e+02 - logprior: 0.3223
Epoch 5/10
19/19 - 17s - loss: 795.8258 - loglik: -7.9599e+02 - logprior: 0.1638
Epoch 6/10
19/19 - 17s - loss: 789.4709 - loglik: -7.8947e+02 - logprior: 0.0035
Epoch 7/10
19/19 - 17s - loss: 791.2009 - loglik: -7.9115e+02 - logprior: -5.5389e-02
Fitted a model with MAP estimate = -790.4644
expansions: [(32, 1), (46, 1), (67, 1), (68, 1), (95, 3), (96, 1), (97, 2), (111, 2), (112, 1), (116, 1), (117, 3), (120, 6), (140, 1), (142, 1), (143, 1), (162, 1), (164, 1), (165, 1), (166, 2), (175, 2), (176, 1), (177, 1), (189, 2), (190, 1), (202, 1), (205, 1), (212, 1), (217, 1), (221, 1), (222, 2), (223, 1), (226, 1), (237, 3), (264, 6), (295, 3), (301, 1), (302, 1), (304, 1), (311, 3), (312, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 800.0986 - loglik: -7.8887e+02 - logprior: -1.1224e+01
Epoch 2/2
19/19 - 23s - loss: 770.4832 - loglik: -7.6800e+02 - logprior: -2.4798e+00
Fitted a model with MAP estimate = -763.1388
expansions: [(0, 2), (284, 1)]
discards: [  0 102 103 104 131 194 350 351 352]
Re-initialized the encoder parameters.
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 774.2451 - loglik: -7.6798e+02 - logprior: -6.2664e+00
Epoch 2/2
19/19 - 22s - loss: 758.5987 - loglik: -7.6088e+02 - logprior: 2.2801
Fitted a model with MAP estimate = -756.9127
expansions: [(136, 1)]
discards: [  0 119 201]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 779.7377 - loglik: -7.7027e+02 - logprior: -9.4715e+00
Epoch 2/10
19/19 - 22s - loss: 763.4546 - loglik: -7.6391e+02 - logprior: 0.4571
Epoch 3/10
19/19 - 22s - loss: 755.9055 - loglik: -7.5974e+02 - logprior: 3.8367
Epoch 4/10
19/19 - 22s - loss: 752.8333 - loglik: -7.5713e+02 - logprior: 4.2953
Epoch 5/10
19/19 - 22s - loss: 751.8737 - loglik: -7.5638e+02 - logprior: 4.5046
Epoch 6/10
19/19 - 22s - loss: 750.8101 - loglik: -7.5554e+02 - logprior: 4.7321
Epoch 7/10
19/19 - 22s - loss: 749.2054 - loglik: -7.5412e+02 - logprior: 4.9150
Epoch 8/10
19/19 - 22s - loss: 748.3344 - loglik: -7.5363e+02 - logprior: 5.2988
Epoch 9/10
19/19 - 22s - loss: 751.1947 - loglik: -7.5672e+02 - logprior: 5.5211
Fitted a model with MAP estimate = -748.1388
Time for alignment: 485.3660
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 997.7632 - loglik: -9.9023e+02 - logprior: -7.5362e+00
Epoch 2/10
19/19 - 17s - loss: 871.4872 - loglik: -8.7256e+02 - logprior: 1.0773
Epoch 3/10
19/19 - 17s - loss: 817.7115 - loglik: -8.1829e+02 - logprior: 0.5787
Epoch 4/10
19/19 - 17s - loss: 803.0848 - loglik: -8.0356e+02 - logprior: 0.4757
Epoch 5/10
19/19 - 17s - loss: 793.7361 - loglik: -7.9408e+02 - logprior: 0.3488
Epoch 6/10
19/19 - 17s - loss: 788.9062 - loglik: -7.8915e+02 - logprior: 0.2441
Epoch 7/10
19/19 - 17s - loss: 793.6052 - loglik: -7.9377e+02 - logprior: 0.1630
Fitted a model with MAP estimate = -790.2913
expansions: [(51, 1), (58, 1), (68, 1), (96, 1), (97, 1), (98, 2), (112, 1), (113, 3), (116, 1), (118, 1), (120, 2), (121, 6), (141, 1), (149, 1), (163, 1), (165, 1), (166, 1), (169, 1), (176, 2), (177, 1), (180, 1), (190, 4), (205, 1), (219, 1), (220, 1), (222, 1), (223, 2), (225, 1), (239, 1), (244, 1), (260, 1), (266, 8), (295, 2), (301, 2), (302, 1), (304, 1), (311, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 804.3518 - loglik: -7.9308e+02 - logprior: -1.1271e+01
Epoch 2/2
19/19 - 23s - loss: 772.8171 - loglik: -7.6983e+02 - logprior: -2.9916e+00
Fitted a model with MAP estimate = -766.2208
expansions: [(0, 2), (142, 2), (371, 1)]
discards: [  0 102 120 133 203 310 349]
Re-initialized the encoder parameters.
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 777.6704 - loglik: -7.7114e+02 - logprior: -6.5256e+00
Epoch 2/2
19/19 - 22s - loss: 761.8731 - loglik: -7.6406e+02 - logprior: 2.1891
Fitted a model with MAP estimate = -758.8080
expansions: [(178, 1)]
discards: [  0 142 352]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 782.8284 - loglik: -7.7291e+02 - logprior: -9.9211e+00
Epoch 2/10
19/19 - 22s - loss: 761.9894 - loglik: -7.6194e+02 - logprior: -4.5823e-02
Epoch 3/10
19/19 - 22s - loss: 760.6051 - loglik: -7.6405e+02 - logprior: 3.4450
Epoch 4/10
19/19 - 22s - loss: 752.4026 - loglik: -7.5641e+02 - logprior: 4.0100
Epoch 5/10
19/19 - 22s - loss: 751.7469 - loglik: -7.5600e+02 - logprior: 4.2537
Epoch 6/10
19/19 - 22s - loss: 752.1531 - loglik: -7.5656e+02 - logprior: 4.4099
Fitted a model with MAP estimate = -750.4879
Time for alignment: 417.8318
Computed alignments with likelihoods: ['-749.7456', '-739.8822', '-748.6019', '-748.1388', '-750.4879']
Best model has likelihood: -739.8822  (prior= 4.8773 )
time for generating output: 0.4244
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.7273137019230769
Training of 5 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac03caa60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1031df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1031ac0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 369.1052 - loglik: -3.1284e+02 - logprior: -5.6267e+01
Epoch 2/10
10/10 - 1s - loss: 290.0601 - loglik: -2.7667e+02 - logprior: -1.3393e+01
Epoch 3/10
10/10 - 1s - loss: 239.7250 - loglik: -2.3371e+02 - logprior: -6.0127e+00
Epoch 4/10
10/10 - 1s - loss: 212.8492 - loglik: -2.0907e+02 - logprior: -3.7825e+00
Epoch 5/10
10/10 - 1s - loss: 201.9689 - loglik: -1.9932e+02 - logprior: -2.6478e+00
Epoch 6/10
10/10 - 1s - loss: 197.4701 - loglik: -1.9545e+02 - logprior: -2.0152e+00
Epoch 7/10
10/10 - 1s - loss: 195.1487 - loglik: -1.9352e+02 - logprior: -1.6317e+00
Epoch 8/10
10/10 - 1s - loss: 193.5850 - loglik: -1.9214e+02 - logprior: -1.4408e+00
Epoch 9/10
10/10 - 1s - loss: 193.1158 - loglik: -1.9182e+02 - logprior: -1.2997e+00
Epoch 10/10
10/10 - 1s - loss: 192.5986 - loglik: -1.9142e+02 - logprior: -1.1750e+00
Fitted a model with MAP estimate = -192.4449
expansions: [(13, 3), (17, 3), (18, 1), (32, 1), (33, 1), (34, 2), (35, 1), (52, 1), (54, 2), (55, 1), (56, 1), (57, 1), (62, 1), (65, 1), (72, 1), (77, 4), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 249.0101 - loglik: -1.8578e+02 - logprior: -6.3227e+01
Epoch 2/2
10/10 - 1s - loss: 197.8850 - loglik: -1.7344e+02 - logprior: -2.4446e+01
Fitted a model with MAP estimate = -189.0039
expansions: [(0, 2)]
discards: [ 0 20 99]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 219.4914 - loglik: -1.6970e+02 - logprior: -4.9796e+01
Epoch 2/2
10/10 - 1s - loss: 178.3137 - loglik: -1.6741e+02 - logprior: -1.0907e+01
Fitted a model with MAP estimate = -172.4147
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.2112 - loglik: -1.7000e+02 - logprior: -6.0215e+01
Epoch 2/10
10/10 - 1s - loss: 185.8650 - loglik: -1.6910e+02 - logprior: -1.6769e+01
Epoch 3/10
10/10 - 1s - loss: 172.2710 - loglik: -1.6824e+02 - logprior: -4.0267e+00
Epoch 4/10
10/10 - 1s - loss: 167.5560 - loglik: -1.6787e+02 - logprior: 0.3111
Epoch 5/10
10/10 - 1s - loss: 165.3910 - loglik: -1.6753e+02 - logprior: 2.1431
Epoch 6/10
10/10 - 1s - loss: 164.4723 - loglik: -1.6767e+02 - logprior: 3.2017
Epoch 7/10
10/10 - 1s - loss: 163.5134 - loglik: -1.6758e+02 - logprior: 4.0670
Epoch 8/10
10/10 - 1s - loss: 163.2116 - loglik: -1.6792e+02 - logprior: 4.7081
Epoch 9/10
10/10 - 1s - loss: 162.7781 - loglik: -1.6791e+02 - logprior: 5.1336
Epoch 10/10
10/10 - 1s - loss: 162.3286 - loglik: -1.6779e+02 - logprior: 5.4609
Fitted a model with MAP estimate = -162.1799
Time for alignment: 45.1189
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 368.9361 - loglik: -3.1267e+02 - logprior: -5.6266e+01
Epoch 2/10
10/10 - 1s - loss: 290.0060 - loglik: -2.7661e+02 - logprior: -1.3397e+01
Epoch 3/10
10/10 - 1s - loss: 239.9944 - loglik: -2.3405e+02 - logprior: -5.9450e+00
Epoch 4/10
10/10 - 1s - loss: 213.1925 - loglik: -2.0956e+02 - logprior: -3.6335e+00
Epoch 5/10
10/10 - 1s - loss: 203.1274 - loglik: -2.0068e+02 - logprior: -2.4465e+00
Epoch 6/10
10/10 - 1s - loss: 198.3490 - loglik: -1.9640e+02 - logprior: -1.9455e+00
Epoch 7/10
10/10 - 1s - loss: 195.8635 - loglik: -1.9419e+02 - logprior: -1.6702e+00
Epoch 8/10
10/10 - 1s - loss: 194.7755 - loglik: -1.9327e+02 - logprior: -1.5017e+00
Epoch 9/10
10/10 - 1s - loss: 194.0060 - loglik: -1.9272e+02 - logprior: -1.2823e+00
Epoch 10/10
10/10 - 1s - loss: 193.3091 - loglik: -1.9219e+02 - logprior: -1.1213e+00
Fitted a model with MAP estimate = -193.2970
expansions: [(13, 3), (17, 3), (18, 1), (32, 1), (33, 1), (34, 2), (35, 1), (52, 1), (54, 2), (55, 1), (56, 1), (57, 1), (62, 2), (76, 1), (77, 3), (78, 1), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.0750 - loglik: -1.8581e+02 - logprior: -6.3265e+01
Epoch 2/2
10/10 - 1s - loss: 197.6947 - loglik: -1.7321e+02 - logprior: -2.4481e+01
Fitted a model with MAP estimate = -189.0945
expansions: [(0, 2)]
discards: [ 0 20 97 98]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 219.8250 - loglik: -1.7002e+02 - logprior: -4.9802e+01
Epoch 2/2
10/10 - 1s - loss: 178.3132 - loglik: -1.6744e+02 - logprior: -1.0876e+01
Fitted a model with MAP estimate = -172.4679
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 230.3433 - loglik: -1.7006e+02 - logprior: -6.0280e+01
Epoch 2/10
10/10 - 1s - loss: 185.8638 - loglik: -1.6895e+02 - logprior: -1.6910e+01
Epoch 3/10
10/10 - 1s - loss: 172.9052 - loglik: -1.6886e+02 - logprior: -4.0494e+00
Epoch 4/10
10/10 - 1s - loss: 167.3744 - loglik: -1.6771e+02 - logprior: 0.3329
Epoch 5/10
10/10 - 1s - loss: 165.5598 - loglik: -1.6773e+02 - logprior: 2.1749
Epoch 6/10
10/10 - 1s - loss: 164.6545 - loglik: -1.6790e+02 - logprior: 3.2483
Epoch 7/10
10/10 - 1s - loss: 163.8637 - loglik: -1.6797e+02 - logprior: 4.1060
Epoch 8/10
10/10 - 1s - loss: 163.1403 - loglik: -1.6788e+02 - logprior: 4.7352
Epoch 9/10
10/10 - 1s - loss: 163.0436 - loglik: -1.6821e+02 - logprior: 5.1667
Epoch 10/10
10/10 - 1s - loss: 162.5250 - loglik: -1.6802e+02 - logprior: 5.4986
Fitted a model with MAP estimate = -162.3714
Time for alignment: 45.6149
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.8712 - loglik: -3.1260e+02 - logprior: -5.6268e+01
Epoch 2/10
10/10 - 1s - loss: 290.3690 - loglik: -2.7697e+02 - logprior: -1.3400e+01
Epoch 3/10
10/10 - 1s - loss: 240.7959 - loglik: -2.3470e+02 - logprior: -6.0914e+00
Epoch 4/10
10/10 - 1s - loss: 211.6082 - loglik: -2.0753e+02 - logprior: -4.0830e+00
Epoch 5/10
10/10 - 1s - loss: 200.9435 - loglik: -1.9792e+02 - logprior: -3.0282e+00
Epoch 6/10
10/10 - 1s - loss: 196.6025 - loglik: -1.9426e+02 - logprior: -2.3392e+00
Epoch 7/10
10/10 - 1s - loss: 195.1030 - loglik: -1.9327e+02 - logprior: -1.8318e+00
Epoch 8/10
10/10 - 1s - loss: 193.8371 - loglik: -1.9226e+02 - logprior: -1.5737e+00
Epoch 9/10
10/10 - 1s - loss: 193.3588 - loglik: -1.9195e+02 - logprior: -1.4052e+00
Epoch 10/10
10/10 - 1s - loss: 193.0506 - loglik: -1.9179e+02 - logprior: -1.2637e+00
Fitted a model with MAP estimate = -192.8058
expansions: [(11, 1), (12, 1), (13, 1), (17, 1), (18, 3), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 1), (57, 1), (63, 1), (65, 1), (76, 1), (77, 4), (80, 1), (86, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 250.9161 - loglik: -1.8746e+02 - logprior: -6.3460e+01
Epoch 2/2
10/10 - 1s - loss: 199.6546 - loglik: -1.7497e+02 - logprior: -2.4686e+01
Fitted a model with MAP estimate = -190.9158
expansions: [(0, 2)]
discards: [ 0 22 97]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 221.8922 - loglik: -1.7195e+02 - logprior: -4.9945e+01
Epoch 2/2
10/10 - 1s - loss: 181.3346 - loglik: -1.7026e+02 - logprior: -1.1079e+01
Fitted a model with MAP estimate = -174.8121
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 232.2029 - loglik: -1.7182e+02 - logprior: -6.0384e+01
Epoch 2/10
10/10 - 1s - loss: 187.5851 - loglik: -1.7075e+02 - logprior: -1.6833e+01
Epoch 3/10
10/10 - 1s - loss: 174.4128 - loglik: -1.7024e+02 - logprior: -4.1708e+00
Epoch 4/10
10/10 - 1s - loss: 169.7360 - loglik: -1.6988e+02 - logprior: 0.1418
Epoch 5/10
10/10 - 1s - loss: 167.4910 - loglik: -1.6947e+02 - logprior: 1.9783
Epoch 6/10
10/10 - 1s - loss: 166.1505 - loglik: -1.6921e+02 - logprior: 3.0567
Epoch 7/10
10/10 - 1s - loss: 165.7492 - loglik: -1.6966e+02 - logprior: 3.9117
Epoch 8/10
10/10 - 1s - loss: 165.3325 - loglik: -1.6987e+02 - logprior: 4.5413
Epoch 9/10
10/10 - 1s - loss: 164.9095 - loglik: -1.6988e+02 - logprior: 4.9703
Epoch 10/10
10/10 - 1s - loss: 164.4414 - loglik: -1.6973e+02 - logprior: 5.2921
Fitted a model with MAP estimate = -164.2456
Time for alignment: 43.2924
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.7451 - loglik: -3.1248e+02 - logprior: -5.6269e+01
Epoch 2/10
10/10 - 1s - loss: 291.2656 - loglik: -2.7787e+02 - logprior: -1.3395e+01
Epoch 3/10
10/10 - 1s - loss: 247.2315 - loglik: -2.4133e+02 - logprior: -5.8994e+00
Epoch 4/10
10/10 - 1s - loss: 218.3093 - loglik: -2.1492e+02 - logprior: -3.3932e+00
Epoch 5/10
10/10 - 1s - loss: 206.5841 - loglik: -2.0450e+02 - logprior: -2.0845e+00
Epoch 6/10
10/10 - 1s - loss: 200.7002 - loglik: -1.9915e+02 - logprior: -1.5543e+00
Epoch 7/10
10/10 - 1s - loss: 197.1938 - loglik: -1.9591e+02 - logprior: -1.2810e+00
Epoch 8/10
10/10 - 1s - loss: 195.3592 - loglik: -1.9424e+02 - logprior: -1.1236e+00
Epoch 9/10
10/10 - 1s - loss: 194.3407 - loglik: -1.9346e+02 - logprior: -8.8381e-01
Epoch 10/10
10/10 - 1s - loss: 194.0791 - loglik: -1.9338e+02 - logprior: -7.0128e-01
Fitted a model with MAP estimate = -193.7470
expansions: [(13, 3), (17, 6), (30, 1), (33, 1), (34, 1), (54, 2), (55, 1), (56, 1), (57, 1), (60, 1), (62, 1), (76, 1), (77, 4), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 250.7489 - loglik: -1.8744e+02 - logprior: -6.3312e+01
Epoch 2/2
10/10 - 1s - loss: 198.9764 - loglik: -1.7452e+02 - logprior: -2.4455e+01
Fitted a model with MAP estimate = -189.6590
expansions: [(0, 2)]
discards: [ 0 21 98]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 219.7214 - loglik: -1.6987e+02 - logprior: -4.9854e+01
Epoch 2/2
10/10 - 1s - loss: 178.9440 - loglik: -1.6793e+02 - logprior: -1.1010e+01
Fitted a model with MAP estimate = -172.5763
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.4275 - loglik: -1.7016e+02 - logprior: -6.0268e+01
Epoch 2/10
10/10 - 1s - loss: 185.8545 - loglik: -1.6908e+02 - logprior: -1.6775e+01
Epoch 3/10
10/10 - 1s - loss: 172.6012 - loglik: -1.6848e+02 - logprior: -4.1215e+00
Epoch 4/10
10/10 - 1s - loss: 167.6940 - loglik: -1.6787e+02 - logprior: 0.1767
Epoch 5/10
10/10 - 1s - loss: 165.7699 - loglik: -1.6778e+02 - logprior: 2.0085
Epoch 6/10
10/10 - 1s - loss: 164.5390 - loglik: -1.6761e+02 - logprior: 3.0701
Epoch 7/10
10/10 - 1s - loss: 163.8216 - loglik: -1.6775e+02 - logprior: 3.9244
Epoch 8/10
10/10 - 1s - loss: 163.4311 - loglik: -1.6799e+02 - logprior: 4.5597
Epoch 9/10
10/10 - 1s - loss: 162.7279 - loglik: -1.6771e+02 - logprior: 4.9780
Epoch 10/10
10/10 - 1s - loss: 162.8101 - loglik: -1.6810e+02 - logprior: 5.2920
Fitted a model with MAP estimate = -162.3997
Time for alignment: 42.3150
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.9083 - loglik: -3.1264e+02 - logprior: -5.6266e+01
Epoch 2/10
10/10 - 1s - loss: 290.4184 - loglik: -2.7703e+02 - logprior: -1.3385e+01
Epoch 3/10
10/10 - 1s - loss: 239.8487 - loglik: -2.3384e+02 - logprior: -6.0123e+00
Epoch 4/10
10/10 - 1s - loss: 210.9153 - loglik: -2.0703e+02 - logprior: -3.8829e+00
Epoch 5/10
10/10 - 1s - loss: 201.0485 - loglik: -1.9819e+02 - logprior: -2.8545e+00
Epoch 6/10
10/10 - 1s - loss: 197.1325 - loglik: -1.9495e+02 - logprior: -2.1865e+00
Epoch 7/10
10/10 - 1s - loss: 195.4178 - loglik: -1.9374e+02 - logprior: -1.6780e+00
Epoch 8/10
10/10 - 1s - loss: 193.7479 - loglik: -1.9234e+02 - logprior: -1.4030e+00
Epoch 9/10
10/10 - 1s - loss: 193.4482 - loglik: -1.9215e+02 - logprior: -1.2988e+00
Epoch 10/10
10/10 - 1s - loss: 192.8045 - loglik: -1.9156e+02 - logprior: -1.2441e+00
Fitted a model with MAP estimate = -192.6051
expansions: [(11, 1), (12, 1), (13, 1), (17, 3), (18, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (62, 1), (65, 1), (76, 1), (77, 4), (80, 1), (86, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 250.2170 - loglik: -1.8687e+02 - logprior: -6.3346e+01
Epoch 2/2
10/10 - 1s - loss: 199.3839 - loglik: -1.7473e+02 - logprior: -2.4659e+01
Fitted a model with MAP estimate = -190.1560
expansions: [(0, 2)]
discards: [ 0 20 98]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.7532 - loglik: -1.7083e+02 - logprior: -4.9925e+01
Epoch 2/2
10/10 - 1s - loss: 179.5188 - loglik: -1.6848e+02 - logprior: -1.1036e+01
Fitted a model with MAP estimate = -173.5061
expansions: [(69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 231.7688 - loglik: -1.7091e+02 - logprior: -6.0859e+01
Epoch 2/10
10/10 - 1s - loss: 188.1340 - loglik: -1.6982e+02 - logprior: -1.8313e+01
Epoch 3/10
10/10 - 1s - loss: 173.4409 - loglik: -1.6878e+02 - logprior: -4.6603e+00
Epoch 4/10
10/10 - 1s - loss: 168.1834 - loglik: -1.6844e+02 - logprior: 0.2542
Epoch 5/10
10/10 - 1s - loss: 165.7237 - loglik: -1.6789e+02 - logprior: 2.1621
Epoch 6/10
10/10 - 1s - loss: 164.7209 - loglik: -1.6796e+02 - logprior: 3.2402
Epoch 7/10
10/10 - 1s - loss: 164.2275 - loglik: -1.6832e+02 - logprior: 4.0964
Epoch 8/10
10/10 - 1s - loss: 163.3830 - loglik: -1.6812e+02 - logprior: 4.7380
Epoch 9/10
10/10 - 1s - loss: 163.2007 - loglik: -1.6836e+02 - logprior: 5.1572
Epoch 10/10
10/10 - 1s - loss: 162.5091 - loglik: -1.6799e+02 - logprior: 5.4796
Fitted a model with MAP estimate = -162.4907
Time for alignment: 41.9614
Computed alignments with likelihoods: ['-162.1799', '-162.3714', '-164.2456', '-162.3997', '-162.4907']
Best model has likelihood: -162.1799  (prior= 5.6251 )
time for generating output: 0.1496
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9455898499451019
Training of 5 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d850880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead14c96d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9e8d68520>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.7539 - loglik: -3.8869e+02 - logprior: -8.0651e+00
Epoch 2/10
13/13 - 2s - loss: 355.4148 - loglik: -3.5360e+02 - logprior: -1.8138e+00
Epoch 3/10
13/13 - 2s - loss: 327.4981 - loglik: -3.2601e+02 - logprior: -1.4839e+00
Epoch 4/10
13/13 - 2s - loss: 315.6227 - loglik: -3.1397e+02 - logprior: -1.6539e+00
Epoch 5/10
13/13 - 2s - loss: 311.7025 - loglik: -3.1019e+02 - logprior: -1.5084e+00
Epoch 6/10
13/13 - 2s - loss: 308.5235 - loglik: -3.0711e+02 - logprior: -1.4174e+00
Epoch 7/10
13/13 - 2s - loss: 307.3223 - loglik: -3.0588e+02 - logprior: -1.4413e+00
Epoch 8/10
13/13 - 2s - loss: 306.3136 - loglik: -3.0486e+02 - logprior: -1.4543e+00
Epoch 9/10
13/13 - 2s - loss: 305.4315 - loglik: -3.0396e+02 - logprior: -1.4740e+00
Epoch 10/10
13/13 - 2s - loss: 304.2550 - loglik: -3.0274e+02 - logprior: -1.5150e+00
Fitted a model with MAP estimate = -303.7913
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (27, 1), (28, 2), (29, 1), (33, 1), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (83, 1), (88, 3), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 314.1270 - loglik: -3.0470e+02 - logprior: -9.4287e+00
Epoch 2/2
13/13 - 2s - loss: 298.9563 - loglik: -2.9491e+02 - logprior: -4.0440e+00
Fitted a model with MAP estimate = -296.3811
expansions: [(0, 2)]
discards: [ 0 35 73 74 81]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 300.6254 - loglik: -2.9359e+02 - logprior: -7.0376e+00
Epoch 2/2
13/13 - 2s - loss: 293.3925 - loglik: -2.9169e+02 - logprior: -1.7046e+00
Fitted a model with MAP estimate = -291.8619
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 303.5851 - loglik: -2.9475e+02 - logprior: -8.8372e+00
Epoch 2/10
13/13 - 2s - loss: 294.2437 - loglik: -2.9189e+02 - logprior: -2.3515e+00
Epoch 3/10
13/13 - 2s - loss: 292.4570 - loglik: -2.9147e+02 - logprior: -9.8248e-01
Epoch 4/10
13/13 - 2s - loss: 290.2975 - loglik: -2.8973e+02 - logprior: -5.6336e-01
Epoch 5/10
13/13 - 2s - loss: 290.1110 - loglik: -2.8967e+02 - logprior: -4.3736e-01
Epoch 6/10
13/13 - 2s - loss: 288.9922 - loglik: -2.8862e+02 - logprior: -3.6785e-01
Epoch 7/10
13/13 - 2s - loss: 289.3162 - loglik: -2.8897e+02 - logprior: -3.4225e-01
Fitted a model with MAP estimate = -288.8423
Time for alignment: 73.3949
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 397.0663 - loglik: -3.8899e+02 - logprior: -8.0728e+00
Epoch 2/10
13/13 - 2s - loss: 355.8965 - loglik: -3.5409e+02 - logprior: -1.8104e+00
Epoch 3/10
13/13 - 2s - loss: 327.3499 - loglik: -3.2588e+02 - logprior: -1.4748e+00
Epoch 4/10
13/13 - 2s - loss: 316.5968 - loglik: -3.1496e+02 - logprior: -1.6391e+00
Epoch 5/10
13/13 - 2s - loss: 310.8452 - loglik: -3.0932e+02 - logprior: -1.5208e+00
Epoch 6/10
13/13 - 2s - loss: 308.3631 - loglik: -3.0694e+02 - logprior: -1.4201e+00
Epoch 7/10
13/13 - 2s - loss: 307.0085 - loglik: -3.0556e+02 - logprior: -1.4436e+00
Epoch 8/10
13/13 - 2s - loss: 306.5519 - loglik: -3.0510e+02 - logprior: -1.4510e+00
Epoch 9/10
13/13 - 2s - loss: 305.8088 - loglik: -3.0438e+02 - logprior: -1.4310e+00
Epoch 10/10
13/13 - 2s - loss: 305.5222 - loglik: -3.0408e+02 - logprior: -1.4401e+00
Fitted a model with MAP estimate = -305.0247
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (27, 2), (28, 2), (33, 1), (51, 1), (52, 1), (55, 1), (58, 3), (64, 2), (82, 4), (83, 1), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 315.4007 - loglik: -3.0597e+02 - logprior: -9.4270e+00
Epoch 2/2
13/13 - 3s - loss: 300.4012 - loglik: -2.9633e+02 - logprior: -4.0718e+00
Fitted a model with MAP estimate = -297.6178
expansions: [(0, 2)]
discards: [  0  34  73  74  81 103]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 302.0242 - loglik: -2.9500e+02 - logprior: -7.0240e+00
Epoch 2/2
13/13 - 2s - loss: 295.4150 - loglik: -2.9372e+02 - logprior: -1.6907e+00
Fitted a model with MAP estimate = -293.2580
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 304.7026 - loglik: -2.9589e+02 - logprior: -8.8097e+00
Epoch 2/10
13/13 - 2s - loss: 296.3267 - loglik: -2.9400e+02 - logprior: -2.3250e+00
Epoch 3/10
13/13 - 2s - loss: 293.7186 - loglik: -2.9277e+02 - logprior: -9.4451e-01
Epoch 4/10
13/13 - 2s - loss: 291.8567 - loglik: -2.9132e+02 - logprior: -5.3492e-01
Epoch 5/10
13/13 - 2s - loss: 291.4898 - loglik: -2.9111e+02 - logprior: -3.8194e-01
Epoch 6/10
13/13 - 2s - loss: 290.4219 - loglik: -2.9010e+02 - logprior: -3.2448e-01
Epoch 7/10
13/13 - 2s - loss: 290.8024 - loglik: -2.9052e+02 - logprior: -2.8706e-01
Fitted a model with MAP estimate = -290.3054
Time for alignment: 73.4907
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.8067 - loglik: -3.8875e+02 - logprior: -8.0565e+00
Epoch 2/10
13/13 - 2s - loss: 355.5956 - loglik: -3.5379e+02 - logprior: -1.8070e+00
Epoch 3/10
13/13 - 2s - loss: 328.3623 - loglik: -3.2688e+02 - logprior: -1.4797e+00
Epoch 4/10
13/13 - 2s - loss: 314.7431 - loglik: -3.1309e+02 - logprior: -1.6486e+00
Epoch 5/10
13/13 - 2s - loss: 310.6401 - loglik: -3.0906e+02 - logprior: -1.5819e+00
Epoch 6/10
13/13 - 2s - loss: 308.1555 - loglik: -3.0663e+02 - logprior: -1.5256e+00
Epoch 7/10
13/13 - 2s - loss: 306.3491 - loglik: -3.0479e+02 - logprior: -1.5555e+00
Epoch 8/10
13/13 - 2s - loss: 305.6114 - loglik: -3.0403e+02 - logprior: -1.5787e+00
Epoch 9/10
13/13 - 2s - loss: 304.9871 - loglik: -3.0342e+02 - logprior: -1.5687e+00
Epoch 10/10
13/13 - 2s - loss: 304.3444 - loglik: -3.0278e+02 - logprior: -1.5663e+00
Fitted a model with MAP estimate = -303.8182
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 2), (24, 2), (27, 3), (28, 2), (33, 1), (48, 1), (52, 1), (55, 1), (59, 3), (64, 2), (83, 1), (86, 1), (87, 3), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 314.3940 - loglik: -3.0496e+02 - logprior: -9.4290e+00
Epoch 2/2
13/13 - 3s - loss: 299.5642 - loglik: -2.9547e+02 - logprior: -4.0919e+00
Fitted a model with MAP estimate = -296.5850
expansions: [(0, 2)]
discards: [ 0 27 29 37 76 83]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 300.9819 - loglik: -2.9391e+02 - logprior: -7.0708e+00
Epoch 2/2
13/13 - 2s - loss: 292.8515 - loglik: -2.9109e+02 - logprior: -1.7664e+00
Fitted a model with MAP estimate = -291.7298
expansions: [(123, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 303.2903 - loglik: -2.9429e+02 - logprior: -8.9975e+00
Epoch 2/10
13/13 - 2s - loss: 295.1795 - loglik: -2.9264e+02 - logprior: -2.5393e+00
Epoch 3/10
13/13 - 2s - loss: 291.7088 - loglik: -2.9070e+02 - logprior: -1.0105e+00
Epoch 4/10
13/13 - 2s - loss: 290.2452 - loglik: -2.8963e+02 - logprior: -6.1284e-01
Epoch 5/10
13/13 - 2s - loss: 289.2624 - loglik: -2.8881e+02 - logprior: -4.5402e-01
Epoch 6/10
13/13 - 2s - loss: 288.8903 - loglik: -2.8850e+02 - logprior: -3.9149e-01
Epoch 7/10
13/13 - 2s - loss: 288.2478 - loglik: -2.8788e+02 - logprior: -3.6476e-01
Epoch 8/10
13/13 - 2s - loss: 288.1615 - loglik: -2.8782e+02 - logprior: -3.3913e-01
Epoch 9/10
13/13 - 2s - loss: 288.4037 - loglik: -2.8809e+02 - logprior: -3.1130e-01
Fitted a model with MAP estimate = -287.9828
Time for alignment: 77.9325
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.8701 - loglik: -3.8879e+02 - logprior: -8.0777e+00
Epoch 2/10
13/13 - 2s - loss: 357.0639 - loglik: -3.5526e+02 - logprior: -1.8067e+00
Epoch 3/10
13/13 - 2s - loss: 331.1003 - loglik: -3.2970e+02 - logprior: -1.4001e+00
Epoch 4/10
13/13 - 2s - loss: 317.7999 - loglik: -3.1625e+02 - logprior: -1.5540e+00
Epoch 5/10
13/13 - 2s - loss: 311.9969 - loglik: -3.1056e+02 - logprior: -1.4398e+00
Epoch 6/10
13/13 - 2s - loss: 309.8162 - loglik: -3.0844e+02 - logprior: -1.3743e+00
Epoch 7/10
13/13 - 2s - loss: 308.2683 - loglik: -3.0685e+02 - logprior: -1.4181e+00
Epoch 8/10
13/13 - 2s - loss: 306.9709 - loglik: -3.0551e+02 - logprior: -1.4654e+00
Epoch 9/10
13/13 - 2s - loss: 306.7749 - loglik: -3.0530e+02 - logprior: -1.4721e+00
Epoch 10/10
13/13 - 2s - loss: 306.4383 - loglik: -3.0497e+02 - logprior: -1.4729e+00
Fitted a model with MAP estimate = -306.0326
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 2), (48, 1), (52, 1), (54, 1), (55, 1), (63, 2), (88, 3), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 314.3149 - loglik: -3.0493e+02 - logprior: -9.3844e+00
Epoch 2/2
13/13 - 2s - loss: 300.2937 - loglik: -2.9632e+02 - logprior: -3.9750e+00
Fitted a model with MAP estimate = -297.6193
expansions: [(0, 2), (74, 2)]
discards: [ 0 37 39]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 301.2824 - loglik: -2.9423e+02 - logprior: -7.0485e+00
Epoch 2/2
13/13 - 3s - loss: 293.7768 - loglik: -2.9204e+02 - logprior: -1.7375e+00
Fitted a model with MAP estimate = -292.0457
expansions: []
discards: [ 0 73 74]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 304.2434 - loglik: -2.9544e+02 - logprior: -8.8021e+00
Epoch 2/10
13/13 - 2s - loss: 295.5176 - loglik: -2.9317e+02 - logprior: -2.3485e+00
Epoch 3/10
13/13 - 2s - loss: 292.0434 - loglik: -2.9107e+02 - logprior: -9.7394e-01
Epoch 4/10
13/13 - 2s - loss: 291.3274 - loglik: -2.9076e+02 - logprior: -5.6569e-01
Epoch 5/10
13/13 - 2s - loss: 290.9681 - loglik: -2.9055e+02 - logprior: -4.1672e-01
Epoch 6/10
13/13 - 2s - loss: 289.2582 - loglik: -2.8890e+02 - logprior: -3.5808e-01
Epoch 7/10
13/13 - 2s - loss: 290.3089 - loglik: -2.8998e+02 - logprior: -3.2664e-01
Fitted a model with MAP estimate = -289.5247
Time for alignment: 72.2612
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.8275 - loglik: -3.8877e+02 - logprior: -8.0584e+00
Epoch 2/10
13/13 - 2s - loss: 355.5729 - loglik: -3.5377e+02 - logprior: -1.8052e+00
Epoch 3/10
13/13 - 2s - loss: 327.7408 - loglik: -3.2629e+02 - logprior: -1.4493e+00
Epoch 4/10
13/13 - 2s - loss: 316.8802 - loglik: -3.1526e+02 - logprior: -1.6215e+00
Epoch 5/10
13/13 - 2s - loss: 311.6246 - loglik: -3.1010e+02 - logprior: -1.5253e+00
Epoch 6/10
13/13 - 2s - loss: 308.6382 - loglik: -3.0720e+02 - logprior: -1.4422e+00
Epoch 7/10
13/13 - 2s - loss: 308.2199 - loglik: -3.0671e+02 - logprior: -1.5095e+00
Epoch 8/10
13/13 - 2s - loss: 306.6734 - loglik: -3.0513e+02 - logprior: -1.5393e+00
Epoch 9/10
13/13 - 2s - loss: 305.0999 - loglik: -3.0357e+02 - logprior: -1.5312e+00
Epoch 10/10
13/13 - 2s - loss: 304.6766 - loglik: -3.0316e+02 - logprior: -1.5209e+00
Fitted a model with MAP estimate = -304.6559
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 2), (24, 1), (26, 1), (27, 1), (29, 2), (51, 1), (52, 1), (55, 1), (59, 3), (64, 2), (83, 1), (88, 3), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 315.8041 - loglik: -3.0636e+02 - logprior: -9.4426e+00
Epoch 2/2
13/13 - 3s - loss: 300.9426 - loglik: -2.9688e+02 - logprior: -4.0580e+00
Fitted a model with MAP estimate = -298.4037
expansions: [(0, 2), (38, 1)]
discards: [ 0 27 73 80]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 302.5188 - loglik: -2.9550e+02 - logprior: -7.0214e+00
Epoch 2/2
13/13 - 2s - loss: 293.8543 - loglik: -2.9216e+02 - logprior: -1.6945e+00
Fitted a model with MAP estimate = -293.1056
expansions: [(122, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 304.7233 - loglik: -2.9576e+02 - logprior: -8.9629e+00
Epoch 2/10
13/13 - 2s - loss: 295.9341 - loglik: -2.9346e+02 - logprior: -2.4734e+00
Epoch 3/10
13/13 - 2s - loss: 292.5417 - loglik: -2.9160e+02 - logprior: -9.3783e-01
Epoch 4/10
13/13 - 2s - loss: 290.9044 - loglik: -2.9038e+02 - logprior: -5.2238e-01
Epoch 5/10
13/13 - 2s - loss: 290.2084 - loglik: -2.8984e+02 - logprior: -3.7027e-01
Epoch 6/10
13/13 - 2s - loss: 289.1396 - loglik: -2.8884e+02 - logprior: -2.9790e-01
Epoch 7/10
13/13 - 2s - loss: 289.5050 - loglik: -2.8923e+02 - logprior: -2.7281e-01
Fitted a model with MAP estimate = -288.8854
Time for alignment: 72.5534
Computed alignments with likelihoods: ['-288.8423', '-290.3054', '-287.9828', '-289.5247', '-288.8854']
Best model has likelihood: -287.9828  (prior= -0.2833 )
time for generating output: 0.1851
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.8526924788607032
Training of 5 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5a18c2b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f6e17c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb27f22820>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.6211 - loglik: -2.1403e+02 - logprior: -8.5908e+00
Epoch 2/10
13/13 - 1s - loss: 173.9847 - loglik: -1.7173e+02 - logprior: -2.2565e+00
Epoch 3/10
13/13 - 1s - loss: 145.5300 - loglik: -1.4372e+02 - logprior: -1.8123e+00
Epoch 4/10
13/13 - 2s - loss: 137.3146 - loglik: -1.3568e+02 - logprior: -1.6393e+00
Epoch 5/10
13/13 - 1s - loss: 134.7474 - loglik: -1.3320e+02 - logprior: -1.5439e+00
Epoch 6/10
13/13 - 1s - loss: 133.6563 - loglik: -1.3215e+02 - logprior: -1.5035e+00
Epoch 7/10
13/13 - 2s - loss: 133.2122 - loglik: -1.3172e+02 - logprior: -1.4913e+00
Epoch 8/10
13/13 - 1s - loss: 132.9772 - loglik: -1.3151e+02 - logprior: -1.4695e+00
Epoch 9/10
13/13 - 2s - loss: 133.0683 - loglik: -1.3162e+02 - logprior: -1.4508e+00
Fitted a model with MAP estimate = -132.6950
expansions: [(0, 5), (13, 1), (16, 1), (33, 1), (35, 2), (36, 1), (37, 1), (38, 1), (42, 2), (43, 2), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 137.7745 - loglik: -1.2764e+02 - logprior: -1.0131e+01
Epoch 2/2
13/13 - 1s - loss: 120.7867 - loglik: -1.1767e+02 - logprior: -3.1213e+00
Fitted a model with MAP estimate = -118.1266
expansions: [(0, 2)]
discards: [44]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 126.8372 - loglik: -1.1683e+02 - logprior: -1.0004e+01
Epoch 2/2
13/13 - 1s - loss: 117.6810 - loglik: -1.1454e+02 - logprior: -3.1407e+00
Fitted a model with MAP estimate = -116.3260
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 124.2113 - loglik: -1.1520e+02 - logprior: -9.0083e+00
Epoch 2/10
13/13 - 2s - loss: 116.4784 - loglik: -1.1418e+02 - logprior: -2.2971e+00
Epoch 3/10
13/13 - 1s - loss: 115.5405 - loglik: -1.1398e+02 - logprior: -1.5654e+00
Epoch 4/10
13/13 - 1s - loss: 114.9433 - loglik: -1.1360e+02 - logprior: -1.3478e+00
Epoch 5/10
13/13 - 1s - loss: 114.8083 - loglik: -1.1361e+02 - logprior: -1.2009e+00
Epoch 6/10
13/13 - 1s - loss: 114.2164 - loglik: -1.1303e+02 - logprior: -1.1909e+00
Epoch 7/10
13/13 - 1s - loss: 114.0151 - loglik: -1.1286e+02 - logprior: -1.1561e+00
Epoch 8/10
13/13 - 1s - loss: 114.0134 - loglik: -1.1288e+02 - logprior: -1.1287e+00
Epoch 9/10
13/13 - 1s - loss: 113.8863 - loglik: -1.1279e+02 - logprior: -1.0957e+00
Epoch 10/10
13/13 - 1s - loss: 113.9564 - loglik: -1.1290e+02 - logprior: -1.0575e+00
Fitted a model with MAP estimate = -113.6606
Time for alignment: 53.4153
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.5762 - loglik: -2.1399e+02 - logprior: -8.5896e+00
Epoch 2/10
13/13 - 1s - loss: 173.5888 - loglik: -1.7135e+02 - logprior: -2.2425e+00
Epoch 3/10
13/13 - 1s - loss: 145.1973 - loglik: -1.4344e+02 - logprior: -1.7613e+00
Epoch 4/10
13/13 - 1s - loss: 137.3222 - loglik: -1.3575e+02 - logprior: -1.5729e+00
Epoch 5/10
13/13 - 1s - loss: 134.7035 - loglik: -1.3324e+02 - logprior: -1.4648e+00
Epoch 6/10
13/13 - 1s - loss: 133.9495 - loglik: -1.3254e+02 - logprior: -1.4136e+00
Epoch 7/10
13/13 - 1s - loss: 133.1942 - loglik: -1.3182e+02 - logprior: -1.3783e+00
Epoch 8/10
13/13 - 1s - loss: 133.3141 - loglik: -1.3195e+02 - logprior: -1.3690e+00
Fitted a model with MAP estimate = -132.9353
expansions: [(0, 5), (13, 1), (34, 2), (35, 3), (36, 2), (37, 1), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 138.2810 - loglik: -1.2817e+02 - logprior: -1.0109e+01
Epoch 2/2
13/13 - 1s - loss: 121.1974 - loglik: -1.1800e+02 - logprior: -3.1965e+00
Fitted a model with MAP estimate = -118.0364
expansions: [(0, 2)]
discards: [40 48 52]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 126.2378 - loglik: -1.1619e+02 - logprior: -1.0047e+01
Epoch 2/2
13/13 - 2s - loss: 117.4499 - loglik: -1.1430e+02 - logprior: -3.1476e+00
Fitted a model with MAP estimate = -116.0186
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 123.8909 - loglik: -1.1487e+02 - logprior: -9.0178e+00
Epoch 2/10
13/13 - 1s - loss: 116.1783 - loglik: -1.1387e+02 - logprior: -2.3089e+00
Epoch 3/10
13/13 - 1s - loss: 115.1887 - loglik: -1.1362e+02 - logprior: -1.5657e+00
Epoch 4/10
13/13 - 1s - loss: 114.7484 - loglik: -1.1340e+02 - logprior: -1.3436e+00
Epoch 5/10
13/13 - 1s - loss: 114.1436 - loglik: -1.1294e+02 - logprior: -1.2048e+00
Epoch 6/10
13/13 - 1s - loss: 114.2459 - loglik: -1.1306e+02 - logprior: -1.1883e+00
Fitted a model with MAP estimate = -113.8651
Time for alignment: 46.1713
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.1468 - loglik: -2.1355e+02 - logprior: -8.5969e+00
Epoch 2/10
13/13 - 1s - loss: 171.9320 - loglik: -1.6969e+02 - logprior: -2.2470e+00
Epoch 3/10
13/13 - 1s - loss: 144.3142 - loglik: -1.4256e+02 - logprior: -1.7512e+00
Epoch 4/10
13/13 - 1s - loss: 136.9067 - loglik: -1.3538e+02 - logprior: -1.5313e+00
Epoch 5/10
13/13 - 1s - loss: 134.1889 - loglik: -1.3276e+02 - logprior: -1.4298e+00
Epoch 6/10
13/13 - 1s - loss: 133.0536 - loglik: -1.3165e+02 - logprior: -1.3999e+00
Epoch 7/10
13/13 - 2s - loss: 132.9761 - loglik: -1.3159e+02 - logprior: -1.3848e+00
Epoch 8/10
13/13 - 1s - loss: 132.2484 - loglik: -1.3088e+02 - logprior: -1.3647e+00
Epoch 9/10
13/13 - 2s - loss: 132.1394 - loglik: -1.3079e+02 - logprior: -1.3482e+00
Epoch 10/10
13/13 - 1s - loss: 132.4521 - loglik: -1.3112e+02 - logprior: -1.3365e+00
Fitted a model with MAP estimate = -132.0323
expansions: [(0, 5), (13, 1), (16, 1), (36, 2), (37, 3), (38, 2), (43, 3), (44, 2), (45, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 136.2320 - loglik: -1.2610e+02 - logprior: -1.0130e+01
Epoch 2/2
13/13 - 1s - loss: 118.4099 - loglik: -1.1518e+02 - logprior: -3.2320e+00
Fitted a model with MAP estimate = -115.2399
expansions: [(0, 2)]
discards: [50 62 63 64 65]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 126.9197 - loglik: -1.1689e+02 - logprior: -1.0033e+01
Epoch 2/2
13/13 - 1s - loss: 118.4459 - loglik: -1.1528e+02 - logprior: -3.1700e+00
Fitted a model with MAP estimate = -116.6584
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 124.2615 - loglik: -1.1523e+02 - logprior: -9.0365e+00
Epoch 2/10
13/13 - 2s - loss: 116.9832 - loglik: -1.1467e+02 - logprior: -2.3120e+00
Epoch 3/10
13/13 - 1s - loss: 115.7060 - loglik: -1.1413e+02 - logprior: -1.5733e+00
Epoch 4/10
13/13 - 1s - loss: 115.2888 - loglik: -1.1394e+02 - logprior: -1.3502e+00
Epoch 5/10
13/13 - 1s - loss: 114.7671 - loglik: -1.1357e+02 - logprior: -1.1971e+00
Epoch 6/10
13/13 - 1s - loss: 114.7171 - loglik: -1.1353e+02 - logprior: -1.1849e+00
Epoch 7/10
13/13 - 1s - loss: 114.3666 - loglik: -1.1321e+02 - logprior: -1.1519e+00
Epoch 8/10
13/13 - 1s - loss: 114.3611 - loglik: -1.1324e+02 - logprior: -1.1187e+00
Epoch 9/10
13/13 - 2s - loss: 114.0551 - loglik: -1.1297e+02 - logprior: -1.0833e+00
Epoch 10/10
13/13 - 1s - loss: 113.9689 - loglik: -1.1292e+02 - logprior: -1.0467e+00
Fitted a model with MAP estimate = -113.9040
Time for alignment: 54.5296
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.4746 - loglik: -2.1388e+02 - logprior: -8.5910e+00
Epoch 2/10
13/13 - 1s - loss: 173.3662 - loglik: -1.7113e+02 - logprior: -2.2372e+00
Epoch 3/10
13/13 - 1s - loss: 145.2205 - loglik: -1.4346e+02 - logprior: -1.7578e+00
Epoch 4/10
13/13 - 1s - loss: 136.3999 - loglik: -1.3482e+02 - logprior: -1.5840e+00
Epoch 5/10
13/13 - 1s - loss: 134.4335 - loglik: -1.3294e+02 - logprior: -1.4922e+00
Epoch 6/10
13/13 - 2s - loss: 133.7277 - loglik: -1.3228e+02 - logprior: -1.4468e+00
Epoch 7/10
13/13 - 1s - loss: 133.2636 - loglik: -1.3181e+02 - logprior: -1.4520e+00
Epoch 8/10
13/13 - 1s - loss: 132.7654 - loglik: -1.3135e+02 - logprior: -1.4191e+00
Epoch 9/10
13/13 - 1s - loss: 132.5601 - loglik: -1.3115e+02 - logprior: -1.4124e+00
Epoch 10/10
13/13 - 1s - loss: 132.7336 - loglik: -1.3133e+02 - logprior: -1.4020e+00
Fitted a model with MAP estimate = -132.3560
expansions: [(0, 5), (13, 1), (16, 1), (34, 1), (35, 2), (36, 2), (37, 2), (38, 1), (42, 1), (43, 2), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 137.4369 - loglik: -1.2734e+02 - logprior: -1.0100e+01
Epoch 2/2
13/13 - 1s - loss: 120.1156 - loglik: -1.1695e+02 - logprior: -3.1640e+00
Fitted a model with MAP estimate = -117.4107
expansions: [(0, 2)]
discards: [46 60 63]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 126.5357 - loglik: -1.1648e+02 - logprior: -1.0051e+01
Epoch 2/2
13/13 - 1s - loss: 117.7062 - loglik: -1.1452e+02 - logprior: -3.1910e+00
Fitted a model with MAP estimate = -116.2514
expansions: [(0, 2), (63, 1), (64, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 123.9254 - loglik: -1.1488e+02 - logprior: -9.0416e+00
Epoch 2/10
13/13 - 1s - loss: 114.2686 - loglik: -1.1188e+02 - logprior: -2.3922e+00
Epoch 3/10
13/13 - 2s - loss: 113.1893 - loglik: -1.1151e+02 - logprior: -1.6746e+00
Epoch 4/10
13/13 - 2s - loss: 111.6314 - loglik: -1.1016e+02 - logprior: -1.4685e+00
Epoch 5/10
13/13 - 1s - loss: 111.5061 - loglik: -1.1015e+02 - logprior: -1.3544e+00
Epoch 6/10
13/13 - 1s - loss: 110.6599 - loglik: -1.0931e+02 - logprior: -1.3493e+00
Epoch 7/10
13/13 - 1s - loss: 110.7191 - loglik: -1.0941e+02 - logprior: -1.3136e+00
Fitted a model with MAP estimate = -110.4954
Time for alignment: 49.6094
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.2886 - loglik: -2.1369e+02 - logprior: -8.5938e+00
Epoch 2/10
13/13 - 1s - loss: 170.7782 - loglik: -1.6855e+02 - logprior: -2.2302e+00
Epoch 3/10
13/13 - 1s - loss: 142.7782 - loglik: -1.4108e+02 - logprior: -1.7023e+00
Epoch 4/10
13/13 - 1s - loss: 136.3918 - loglik: -1.3488e+02 - logprior: -1.5120e+00
Epoch 5/10
13/13 - 1s - loss: 133.8044 - loglik: -1.3237e+02 - logprior: -1.4357e+00
Epoch 6/10
13/13 - 1s - loss: 133.1134 - loglik: -1.3173e+02 - logprior: -1.3809e+00
Epoch 7/10
13/13 - 2s - loss: 133.0325 - loglik: -1.3168e+02 - logprior: -1.3508e+00
Epoch 8/10
13/13 - 1s - loss: 132.6393 - loglik: -1.3130e+02 - logprior: -1.3353e+00
Epoch 9/10
13/13 - 1s - loss: 132.6368 - loglik: -1.3130e+02 - logprior: -1.3380e+00
Epoch 10/10
13/13 - 1s - loss: 132.1640 - loglik: -1.3086e+02 - logprior: -1.3089e+00
Fitted a model with MAP estimate = -132.3020
expansions: [(0, 5), (13, 1), (34, 1), (36, 2), (37, 3), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 137.0846 - loglik: -1.2693e+02 - logprior: -1.0159e+01
Epoch 2/2
13/13 - 1s - loss: 120.0449 - loglik: -1.1686e+02 - logprior: -3.1871e+00
Fitted a model with MAP estimate = -117.5245
expansions: [(0, 2)]
discards: [50]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 125.4933 - loglik: -1.1548e+02 - logprior: -1.0011e+01
Epoch 2/2
13/13 - 1s - loss: 117.7043 - loglik: -1.1454e+02 - logprior: -3.1693e+00
Fitted a model with MAP estimate = -115.9393
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 123.3951 - loglik: -1.1436e+02 - logprior: -9.0342e+00
Epoch 2/10
13/13 - 1s - loss: 116.5154 - loglik: -1.1420e+02 - logprior: -2.3183e+00
Epoch 3/10
13/13 - 1s - loss: 115.1990 - loglik: -1.1363e+02 - logprior: -1.5739e+00
Epoch 4/10
13/13 - 1s - loss: 114.4859 - loglik: -1.1313e+02 - logprior: -1.3542e+00
Epoch 5/10
13/13 - 1s - loss: 114.1036 - loglik: -1.1290e+02 - logprior: -1.2031e+00
Epoch 6/10
13/13 - 2s - loss: 114.1193 - loglik: -1.1292e+02 - logprior: -1.1955e+00
Fitted a model with MAP estimate = -113.7688
Time for alignment: 49.0836
Computed alignments with likelihoods: ['-113.6606', '-113.8651', '-113.9040', '-110.4954', '-113.7688']
Best model has likelihood: -110.4954  (prior= -1.2876 )
time for generating output: 0.1996
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.508468197214904
Training of 5 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e462040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2cfc9220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaf4d507c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.4390 - loglik: -5.8624e+01 - logprior: -8.1528e-01
Epoch 2/10
41/41 - 1s - loss: 45.6772 - loglik: -4.4863e+01 - logprior: -8.1381e-01
Epoch 3/10
41/41 - 1s - loss: 44.9082 - loglik: -4.4117e+01 - logprior: -7.9074e-01
Epoch 4/10
41/41 - 1s - loss: 44.8657 - loglik: -4.4079e+01 - logprior: -7.8705e-01
Epoch 5/10
41/41 - 1s - loss: 44.7110 - loglik: -4.3928e+01 - logprior: -7.8348e-01
Epoch 6/10
41/41 - 1s - loss: 44.5471 - loglik: -4.3767e+01 - logprior: -7.7986e-01
Epoch 7/10
41/41 - 1s - loss: 44.5481 - loglik: -4.3767e+01 - logprior: -7.8113e-01
Fitted a model with MAP estimate = -44.1271
expansions: [(8, 2), (9, 1), (10, 2), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 45.7646 - loglik: -4.4757e+01 - logprior: -1.0081e+00
Epoch 2/2
41/41 - 1s - loss: 43.4110 - loglik: -4.2635e+01 - logprior: -7.7593e-01
Fitted a model with MAP estimate = -42.8344
expansions: []
discards: [ 8 14 16]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.0876 - loglik: -4.3111e+01 - logprior: -9.7648e-01
Epoch 2/2
41/41 - 1s - loss: 43.4356 - loglik: -4.2694e+01 - logprior: -7.4117e-01
Fitted a model with MAP estimate = -42.8408
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.7416 - loglik: -4.2133e+01 - logprior: -6.0899e-01
Epoch 2/10
58/58 - 2s - loss: 42.2155 - loglik: -4.1711e+01 - logprior: -5.0429e-01
Epoch 3/10
58/58 - 2s - loss: 42.1562 - loglik: -4.1655e+01 - logprior: -5.0129e-01
Epoch 4/10
58/58 - 2s - loss: 41.9056 - loglik: -4.1405e+01 - logprior: -5.0015e-01
Epoch 5/10
58/58 - 2s - loss: 41.8279 - loglik: -4.1330e+01 - logprior: -4.9814e-01
Epoch 6/10
58/58 - 2s - loss: 41.8018 - loglik: -4.1307e+01 - logprior: -4.9456e-01
Epoch 7/10
58/58 - 2s - loss: 41.6141 - loglik: -4.1119e+01 - logprior: -4.9562e-01
Epoch 8/10
58/58 - 2s - loss: 41.6312 - loglik: -4.1137e+01 - logprior: -4.9430e-01
Fitted a model with MAP estimate = -41.5385
Time for alignment: 63.2118
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.2747 - loglik: -5.8460e+01 - logprior: -8.1499e-01
Epoch 2/10
41/41 - 1s - loss: 45.7168 - loglik: -4.4901e+01 - logprior: -8.1573e-01
Epoch 3/10
41/41 - 1s - loss: 44.9851 - loglik: -4.4193e+01 - logprior: -7.9221e-01
Epoch 4/10
41/41 - 1s - loss: 44.8104 - loglik: -4.4025e+01 - logprior: -7.8591e-01
Epoch 5/10
41/41 - 1s - loss: 44.7081 - loglik: -4.3926e+01 - logprior: -7.8214e-01
Epoch 6/10
41/41 - 1s - loss: 44.5901 - loglik: -4.3810e+01 - logprior: -7.8048e-01
Epoch 7/10
41/41 - 1s - loss: 44.4469 - loglik: -4.3666e+01 - logprior: -7.8062e-01
Epoch 8/10
41/41 - 1s - loss: 44.5103 - loglik: -4.3731e+01 - logprior: -7.7968e-01
Fitted a model with MAP estimate = -44.0906
expansions: [(8, 2), (9, 1), (10, 2), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 45.8074 - loglik: -4.4800e+01 - logprior: -1.0075e+00
Epoch 2/2
41/41 - 1s - loss: 43.3544 - loglik: -4.2577e+01 - logprior: -7.7769e-01
Fitted a model with MAP estimate = -42.8223
expansions: []
discards: [ 8 14 16]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.1753 - loglik: -4.3201e+01 - logprior: -9.7410e-01
Epoch 2/2
41/41 - 1s - loss: 43.2989 - loglik: -4.2557e+01 - logprior: -7.4244e-01
Fitted a model with MAP estimate = -42.8394
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.6474 - loglik: -4.2038e+01 - logprior: -6.0977e-01
Epoch 2/10
58/58 - 2s - loss: 42.2862 - loglik: -4.1783e+01 - logprior: -5.0304e-01
Epoch 3/10
58/58 - 2s - loss: 42.2018 - loglik: -4.1700e+01 - logprior: -5.0166e-01
Epoch 4/10
58/58 - 1s - loss: 41.8826 - loglik: -4.1384e+01 - logprior: -4.9871e-01
Epoch 5/10
58/58 - 2s - loss: 41.8274 - loglik: -4.1331e+01 - logprior: -4.9673e-01
Epoch 6/10
58/58 - 2s - loss: 41.8325 - loglik: -4.1337e+01 - logprior: -4.9601e-01
Fitted a model with MAP estimate = -41.6554
Time for alignment: 61.0494
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.3875 - loglik: -5.8558e+01 - logprior: -8.2939e-01
Epoch 2/10
41/41 - 1s - loss: 45.7801 - loglik: -4.4970e+01 - logprior: -8.0962e-01
Epoch 3/10
41/41 - 1s - loss: 45.1024 - loglik: -4.4308e+01 - logprior: -7.9397e-01
Epoch 4/10
41/41 - 1s - loss: 44.9923 - loglik: -4.4203e+01 - logprior: -7.8907e-01
Epoch 5/10
41/41 - 1s - loss: 44.7416 - loglik: -4.3959e+01 - logprior: -7.8252e-01
Epoch 6/10
41/41 - 1s - loss: 44.7865 - loglik: -4.4003e+01 - logprior: -7.8298e-01
Fitted a model with MAP estimate = -44.2985
expansions: [(4, 1), (9, 1), (10, 2), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 45.3597 - loglik: -4.4356e+01 - logprior: -1.0041e+00
Epoch 2/2
41/41 - 1s - loss: 43.4014 - loglik: -4.2637e+01 - logprior: -7.6483e-01
Fitted a model with MAP estimate = -42.7923
expansions: []
discards: [13 15]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.0269 - loglik: -4.3052e+01 - logprior: -9.7460e-01
Epoch 2/2
41/41 - 1s - loss: 43.4184 - loglik: -4.2678e+01 - logprior: -7.4033e-01
Fitted a model with MAP estimate = -42.8268
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.7987 - loglik: -4.2191e+01 - logprior: -6.0813e-01
Epoch 2/10
58/58 - 2s - loss: 42.0994 - loglik: -4.1595e+01 - logprior: -5.0460e-01
Epoch 3/10
58/58 - 2s - loss: 42.2299 - loglik: -4.1728e+01 - logprior: -5.0160e-01
Fitted a model with MAP estimate = -42.0033
Time for alignment: 53.4229
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.0800 - loglik: -5.8255e+01 - logprior: -8.2511e-01
Epoch 2/10
41/41 - 1s - loss: 45.6231 - loglik: -4.4811e+01 - logprior: -8.1180e-01
Epoch 3/10
41/41 - 1s - loss: 44.9286 - loglik: -4.4138e+01 - logprior: -7.9049e-01
Epoch 4/10
41/41 - 1s - loss: 44.8902 - loglik: -4.4103e+01 - logprior: -7.8746e-01
Epoch 5/10
41/41 - 1s - loss: 44.5974 - loglik: -4.3815e+01 - logprior: -7.8203e-01
Epoch 6/10
41/41 - 1s - loss: 44.6492 - loglik: -4.3869e+01 - logprior: -7.8014e-01
Fitted a model with MAP estimate = -44.1123
expansions: [(8, 2), (9, 2), (10, 2), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 27 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 46.0576 - loglik: -4.5042e+01 - logprior: -1.0158e+00
Epoch 2/2
41/41 - 1s - loss: 43.4712 - loglik: -4.2690e+01 - logprior: -7.8091e-01
Fitted a model with MAP estimate = -42.8234
expansions: []
discards: [ 8 12 14 17]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.1713 - loglik: -4.3196e+01 - logprior: -9.7548e-01
Epoch 2/2
41/41 - 1s - loss: 43.3798 - loglik: -4.2639e+01 - logprior: -7.4081e-01
Fitted a model with MAP estimate = -42.8291
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.7239 - loglik: -4.2116e+01 - logprior: -6.0791e-01
Epoch 2/10
58/58 - 2s - loss: 42.1960 - loglik: -4.1691e+01 - logprior: -5.0534e-01
Epoch 3/10
58/58 - 2s - loss: 42.2095 - loglik: -4.1708e+01 - logprior: -5.0189e-01
Fitted a model with MAP estimate = -41.9936
Time for alignment: 53.5692
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.1628 - loglik: -5.8347e+01 - logprior: -8.1550e-01
Epoch 2/10
41/41 - 1s - loss: 45.6384 - loglik: -4.4822e+01 - logprior: -8.1621e-01
Epoch 3/10
41/41 - 1s - loss: 44.9271 - loglik: -4.4133e+01 - logprior: -7.9379e-01
Epoch 4/10
41/41 - 1s - loss: 44.8942 - loglik: -4.4110e+01 - logprior: -7.8437e-01
Epoch 5/10
41/41 - 1s - loss: 44.7091 - loglik: -4.3928e+01 - logprior: -7.8143e-01
Epoch 6/10
41/41 - 1s - loss: 44.5185 - loglik: -4.3738e+01 - logprior: -7.8085e-01
Epoch 7/10
41/41 - 1s - loss: 44.5376 - loglik: -4.3757e+01 - logprior: -7.8103e-01
Fitted a model with MAP estimate = -44.1209
expansions: [(8, 2), (9, 1), (10, 2), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 45.6556 - loglik: -4.4645e+01 - logprior: -1.0103e+00
Epoch 2/2
41/41 - 1s - loss: 43.4514 - loglik: -4.2678e+01 - logprior: -7.7290e-01
Fitted a model with MAP estimate = -42.8293
expansions: []
discards: [ 8 14 16]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.0622 - loglik: -4.3087e+01 - logprior: -9.7517e-01
Epoch 2/2
41/41 - 1s - loss: 43.4177 - loglik: -4.2675e+01 - logprior: -7.4287e-01
Fitted a model with MAP estimate = -42.8559
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 42.6732 - loglik: -4.2066e+01 - logprior: -6.0756e-01
Epoch 2/10
58/58 - 2s - loss: 42.2697 - loglik: -4.1764e+01 - logprior: -5.0540e-01
Epoch 3/10
58/58 - 2s - loss: 42.1930 - loglik: -4.1691e+01 - logprior: -5.0189e-01
Epoch 4/10
58/58 - 2s - loss: 41.8990 - loglik: -4.1399e+01 - logprior: -4.9955e-01
Epoch 5/10
58/58 - 2s - loss: 41.8078 - loglik: -4.1311e+01 - logprior: -4.9730e-01
Epoch 6/10
58/58 - 2s - loss: 41.8330 - loglik: -4.1338e+01 - logprior: -4.9523e-01
Fitted a model with MAP estimate = -41.6504
Time for alignment: 59.8855
Computed alignments with likelihoods: ['-41.5385', '-41.6554', '-42.0033', '-41.9936', '-41.6504']
Best model has likelihood: -41.5385  (prior= -0.4904 )
time for generating output: 0.0785
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.966464502318944
Training of 5 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ad4df280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2ca5c5e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9d7c263a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0189 - loglik: -1.5684e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.2825 - loglik: -1.3449e+02 - logprior: -3.3788e+01
Epoch 3/10
10/10 - 0s - loss: 133.4336 - loglik: -1.1749e+02 - logprior: -1.5943e+01
Epoch 4/10
10/10 - 1s - loss: 116.9538 - loglik: -1.0746e+02 - logprior: -9.4922e+00
Epoch 5/10
10/10 - 1s - loss: 108.7184 - loglik: -1.0282e+02 - logprior: -5.8939e+00
Epoch 6/10
10/10 - 1s - loss: 104.7584 - loglik: -1.0101e+02 - logprior: -3.7484e+00
Epoch 7/10
10/10 - 1s - loss: 102.8629 - loglik: -1.0052e+02 - logprior: -2.3386e+00
Epoch 8/10
10/10 - 1s - loss: 101.8485 - loglik: -1.0039e+02 - logprior: -1.4571e+00
Epoch 9/10
10/10 - 0s - loss: 101.2081 - loglik: -1.0026e+02 - logprior: -9.4996e-01
Epoch 10/10
10/10 - 1s - loss: 100.7597 - loglik: -1.0024e+02 - logprior: -5.2403e-01
Fitted a model with MAP estimate = -100.5622
expansions: [(9, 1), (13, 1), (15, 5), (18, 1), (31, 1), (32, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 237.8632 - loglik: -9.9126e+01 - logprior: -1.3874e+02
Epoch 2/2
10/10 - 0s - loss: 150.2162 - loglik: -9.3370e+01 - logprior: -5.6846e+01
Fitted a model with MAP estimate = -135.3672
expansions: [(0, 2)]
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 201.6833 - loglik: -9.0350e+01 - logprior: -1.1133e+02
Epoch 2/2
10/10 - 0s - loss: 118.7124 - loglik: -8.8905e+01 - logprior: -2.9807e+01
Fitted a model with MAP estimate = -106.5217
expansions: []
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 219.7624 - loglik: -9.1525e+01 - logprior: -1.2824e+02
Epoch 2/10
10/10 - 1s - loss: 126.8722 - loglik: -9.0757e+01 - logprior: -3.6115e+01
Epoch 3/10
10/10 - 1s - loss: 104.7738 - loglik: -9.0707e+01 - logprior: -1.4067e+01
Epoch 4/10
10/10 - 1s - loss: 96.7268 - loglik: -9.0753e+01 - logprior: -5.9740e+00
Epoch 5/10
10/10 - 0s - loss: 92.6059 - loglik: -9.0666e+01 - logprior: -1.9400e+00
Epoch 6/10
10/10 - 1s - loss: 90.2236 - loglik: -9.0545e+01 - logprior: 0.3209
Epoch 7/10
10/10 - 1s - loss: 88.8164 - loglik: -9.0511e+01 - logprior: 1.6944
Epoch 8/10
10/10 - 1s - loss: 87.8983 - loglik: -9.0534e+01 - logprior: 2.6361
Epoch 9/10
10/10 - 0s - loss: 87.2272 - loglik: -9.0605e+01 - logprior: 3.3778
Epoch 10/10
10/10 - 1s - loss: 86.7012 - loglik: -9.0698e+01 - logprior: 3.9967
Fitted a model with MAP estimate = -86.4467
Time for alignment: 28.7953
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0189 - loglik: -1.5684e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.2825 - loglik: -1.3449e+02 - logprior: -3.3788e+01
Epoch 3/10
10/10 - 1s - loss: 133.4336 - loglik: -1.1749e+02 - logprior: -1.5943e+01
Epoch 4/10
10/10 - 1s - loss: 116.9538 - loglik: -1.0746e+02 - logprior: -9.4922e+00
Epoch 5/10
10/10 - 0s - loss: 108.7184 - loglik: -1.0282e+02 - logprior: -5.8939e+00
Epoch 6/10
10/10 - 1s - loss: 104.7584 - loglik: -1.0101e+02 - logprior: -3.7484e+00
Epoch 7/10
10/10 - 1s - loss: 102.8629 - loglik: -1.0052e+02 - logprior: -2.3386e+00
Epoch 8/10
10/10 - 1s - loss: 101.8485 - loglik: -1.0039e+02 - logprior: -1.4571e+00
Epoch 9/10
10/10 - 1s - loss: 101.2081 - loglik: -1.0026e+02 - logprior: -9.4996e-01
Epoch 10/10
10/10 - 1s - loss: 100.7597 - loglik: -1.0024e+02 - logprior: -5.2403e-01
Fitted a model with MAP estimate = -100.5622
expansions: [(9, 1), (13, 1), (15, 5), (18, 1), (31, 1), (32, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 237.8632 - loglik: -9.9126e+01 - logprior: -1.3874e+02
Epoch 2/2
10/10 - 0s - loss: 150.2162 - loglik: -9.3370e+01 - logprior: -5.6846e+01
Fitted a model with MAP estimate = -135.3672
expansions: [(0, 2)]
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 201.6833 - loglik: -9.0351e+01 - logprior: -1.1133e+02
Epoch 2/2
10/10 - 1s - loss: 118.7124 - loglik: -8.8905e+01 - logprior: -2.9808e+01
Fitted a model with MAP estimate = -106.5218
expansions: []
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 219.7625 - loglik: -9.1525e+01 - logprior: -1.2824e+02
Epoch 2/10
10/10 - 1s - loss: 126.8723 - loglik: -9.0757e+01 - logprior: -3.6115e+01
Epoch 3/10
10/10 - 0s - loss: 104.7737 - loglik: -9.0707e+01 - logprior: -1.4067e+01
Epoch 4/10
10/10 - 1s - loss: 96.7266 - loglik: -9.0753e+01 - logprior: -5.9739e+00
Epoch 5/10
10/10 - 1s - loss: 92.6057 - loglik: -9.0666e+01 - logprior: -1.9399e+00
Epoch 6/10
10/10 - 1s - loss: 90.2234 - loglik: -9.0545e+01 - logprior: 0.3211
Epoch 7/10
10/10 - 1s - loss: 88.8162 - loglik: -9.0511e+01 - logprior: 1.6947
Epoch 8/10
10/10 - 1s - loss: 87.8981 - loglik: -9.0534e+01 - logprior: 2.6363
Epoch 9/10
10/10 - 0s - loss: 87.2269 - loglik: -9.0605e+01 - logprior: 3.3782
Epoch 10/10
10/10 - 1s - loss: 86.7009 - loglik: -9.0698e+01 - logprior: 3.9972
Fitted a model with MAP estimate = -86.4463
Time for alignment: 28.0363
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0189 - loglik: -1.5684e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.2825 - loglik: -1.3449e+02 - logprior: -3.3788e+01
Epoch 3/10
10/10 - 1s - loss: 133.4336 - loglik: -1.1749e+02 - logprior: -1.5943e+01
Epoch 4/10
10/10 - 1s - loss: 116.9538 - loglik: -1.0746e+02 - logprior: -9.4922e+00
Epoch 5/10
10/10 - 0s - loss: 108.7184 - loglik: -1.0282e+02 - logprior: -5.8939e+00
Epoch 6/10
10/10 - 1s - loss: 104.7584 - loglik: -1.0101e+02 - logprior: -3.7484e+00
Epoch 7/10
10/10 - 1s - loss: 102.8629 - loglik: -1.0052e+02 - logprior: -2.3386e+00
Epoch 8/10
10/10 - 0s - loss: 101.8485 - loglik: -1.0039e+02 - logprior: -1.4571e+00
Epoch 9/10
10/10 - 1s - loss: 101.2081 - loglik: -1.0026e+02 - logprior: -9.4996e-01
Epoch 10/10
10/10 - 1s - loss: 100.7597 - loglik: -1.0024e+02 - logprior: -5.2403e-01
Fitted a model with MAP estimate = -100.5623
expansions: [(9, 1), (13, 1), (15, 5), (18, 1), (31, 1), (32, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 237.8632 - loglik: -9.9126e+01 - logprior: -1.3874e+02
Epoch 2/2
10/10 - 1s - loss: 150.2162 - loglik: -9.3370e+01 - logprior: -5.6846e+01
Fitted a model with MAP estimate = -135.3672
expansions: [(0, 2)]
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 201.6833 - loglik: -9.0350e+01 - logprior: -1.1133e+02
Epoch 2/2
10/10 - 1s - loss: 118.7124 - loglik: -8.8905e+01 - logprior: -2.9807e+01
Fitted a model with MAP estimate = -106.5217
expansions: []
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 219.7625 - loglik: -9.1525e+01 - logprior: -1.2824e+02
Epoch 2/10
10/10 - 1s - loss: 126.8722 - loglik: -9.0757e+01 - logprior: -3.6115e+01
Epoch 3/10
10/10 - 1s - loss: 104.7738 - loglik: -9.0707e+01 - logprior: -1.4067e+01
Epoch 4/10
10/10 - 1s - loss: 96.7268 - loglik: -9.0753e+01 - logprior: -5.9740e+00
Epoch 5/10
10/10 - 1s - loss: 92.6059 - loglik: -9.0666e+01 - logprior: -1.9400e+00
Epoch 6/10
10/10 - 1s - loss: 90.2236 - loglik: -9.0545e+01 - logprior: 0.3209
Epoch 7/10
10/10 - 1s - loss: 88.8165 - loglik: -9.0511e+01 - logprior: 1.6943
Epoch 8/10
10/10 - 1s - loss: 87.8983 - loglik: -9.0534e+01 - logprior: 2.6360
Epoch 9/10
10/10 - 1s - loss: 87.2272 - loglik: -9.0605e+01 - logprior: 3.3778
Epoch 10/10
10/10 - 1s - loss: 86.7013 - loglik: -9.0698e+01 - logprior: 3.9967
Fitted a model with MAP estimate = -86.4468
Time for alignment: 28.2245
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0189 - loglik: -1.5684e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.2825 - loglik: -1.3449e+02 - logprior: -3.3788e+01
Epoch 3/10
10/10 - 1s - loss: 133.4336 - loglik: -1.1749e+02 - logprior: -1.5943e+01
Epoch 4/10
10/10 - 1s - loss: 116.9538 - loglik: -1.0746e+02 - logprior: -9.4922e+00
Epoch 5/10
10/10 - 1s - loss: 108.7184 - loglik: -1.0282e+02 - logprior: -5.8939e+00
Epoch 6/10
10/10 - 1s - loss: 104.7584 - loglik: -1.0101e+02 - logprior: -3.7484e+00
Epoch 7/10
10/10 - 1s - loss: 102.8629 - loglik: -1.0052e+02 - logprior: -2.3386e+00
Epoch 8/10
10/10 - 1s - loss: 101.8485 - loglik: -1.0039e+02 - logprior: -1.4571e+00
Epoch 9/10
10/10 - 1s - loss: 101.2081 - loglik: -1.0026e+02 - logprior: -9.4996e-01
Epoch 10/10
10/10 - 1s - loss: 100.7597 - loglik: -1.0024e+02 - logprior: -5.2403e-01
Fitted a model with MAP estimate = -100.5622
expansions: [(9, 1), (13, 1), (15, 5), (18, 1), (31, 1), (32, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 237.8632 - loglik: -9.9126e+01 - logprior: -1.3874e+02
Epoch 2/2
10/10 - 0s - loss: 150.2162 - loglik: -9.3370e+01 - logprior: -5.6846e+01
Fitted a model with MAP estimate = -135.3673
expansions: [(0, 2)]
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 201.6831 - loglik: -9.0350e+01 - logprior: -1.1133e+02
Epoch 2/2
10/10 - 1s - loss: 118.7122 - loglik: -8.8905e+01 - logprior: -2.9807e+01
Fitted a model with MAP estimate = -106.5214
expansions: []
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 219.7621 - loglik: -9.1525e+01 - logprior: -1.2824e+02
Epoch 2/10
10/10 - 1s - loss: 126.8716 - loglik: -9.0756e+01 - logprior: -3.6115e+01
Epoch 3/10
10/10 - 1s - loss: 104.7733 - loglik: -9.0707e+01 - logprior: -1.4067e+01
Epoch 4/10
10/10 - 1s - loss: 96.7261 - loglik: -9.0753e+01 - logprior: -5.9736e+00
Epoch 5/10
10/10 - 1s - loss: 92.6048 - loglik: -9.0665e+01 - logprior: -1.9397e+00
Epoch 6/10
10/10 - 1s - loss: 90.2226 - loglik: -9.0544e+01 - logprior: 0.3215
Epoch 7/10
10/10 - 1s - loss: 88.8152 - loglik: -9.0510e+01 - logprior: 1.6952
Epoch 8/10
10/10 - 1s - loss: 87.8971 - loglik: -9.0534e+01 - logprior: 2.6369
Epoch 9/10
10/10 - 1s - loss: 87.2262 - loglik: -9.0605e+01 - logprior: 3.3788
Epoch 10/10
10/10 - 1s - loss: 86.7002 - loglik: -9.0698e+01 - logprior: 3.9976
Fitted a model with MAP estimate = -86.4455
Time for alignment: 27.8590
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0189 - loglik: -1.5684e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 168.2825 - loglik: -1.3449e+02 - logprior: -3.3788e+01
Epoch 3/10
10/10 - 0s - loss: 133.4336 - loglik: -1.1749e+02 - logprior: -1.5943e+01
Epoch 4/10
10/10 - 0s - loss: 116.9538 - loglik: -1.0746e+02 - logprior: -9.4922e+00
Epoch 5/10
10/10 - 1s - loss: 108.7184 - loglik: -1.0282e+02 - logprior: -5.8939e+00
Epoch 6/10
10/10 - 0s - loss: 104.7584 - loglik: -1.0101e+02 - logprior: -3.7484e+00
Epoch 7/10
10/10 - 1s - loss: 102.8629 - loglik: -1.0052e+02 - logprior: -2.3386e+00
Epoch 8/10
10/10 - 1s - loss: 101.8485 - loglik: -1.0039e+02 - logprior: -1.4571e+00
Epoch 9/10
10/10 - 1s - loss: 101.2080 - loglik: -1.0026e+02 - logprior: -9.4996e-01
Epoch 10/10
10/10 - 1s - loss: 100.7597 - loglik: -1.0024e+02 - logprior: -5.2403e-01
Fitted a model with MAP estimate = -100.5623
expansions: [(9, 1), (13, 1), (15, 5), (18, 1), (31, 1), (32, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 237.8632 - loglik: -9.9126e+01 - logprior: -1.3874e+02
Epoch 2/2
10/10 - 0s - loss: 150.2162 - loglik: -9.3370e+01 - logprior: -5.6846e+01
Fitted a model with MAP estimate = -135.3672
expansions: [(0, 2)]
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 201.6833 - loglik: -9.0351e+01 - logprior: -1.1133e+02
Epoch 2/2
10/10 - 0s - loss: 118.7124 - loglik: -8.8905e+01 - logprior: -2.9807e+01
Fitted a model with MAP estimate = -106.5218
expansions: []
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 219.7624 - loglik: -9.1525e+01 - logprior: -1.2824e+02
Epoch 2/10
10/10 - 0s - loss: 126.8722 - loglik: -9.0757e+01 - logprior: -3.6115e+01
Epoch 3/10
10/10 - 0s - loss: 104.7738 - loglik: -9.0707e+01 - logprior: -1.4067e+01
Epoch 4/10
10/10 - 1s - loss: 96.7267 - loglik: -9.0753e+01 - logprior: -5.9739e+00
Epoch 5/10
10/10 - 0s - loss: 92.6058 - loglik: -9.0666e+01 - logprior: -1.9400e+00
Epoch 6/10
10/10 - 1s - loss: 90.2235 - loglik: -9.0544e+01 - logprior: 0.3210
Epoch 7/10
10/10 - 1s - loss: 88.8163 - loglik: -9.0511e+01 - logprior: 1.6945
Epoch 8/10
10/10 - 1s - loss: 87.8981 - loglik: -9.0534e+01 - logprior: 2.6362
Epoch 9/10
10/10 - 1s - loss: 87.2270 - loglik: -9.0605e+01 - logprior: 3.3780
Epoch 10/10
10/10 - 1s - loss: 86.7010 - loglik: -9.0698e+01 - logprior: 3.9969
Fitted a model with MAP estimate = -86.4464
Time for alignment: 27.4664
Computed alignments with likelihoods: ['-86.4467', '-86.4463', '-86.4468', '-86.4455', '-86.4464']
Best model has likelihood: -86.4455  (prior= 4.2957 )
time for generating output: 0.1010
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.9008313539192399
Training of 5 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe993ef4100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feadad645b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaec3e4400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.3482 - loglik: -1.9223e+02 - logprior: -3.1139e+00
Epoch 2/10
19/19 - 1s - loss: 157.6417 - loglik: -1.5649e+02 - logprior: -1.1536e+00
Epoch 3/10
19/19 - 1s - loss: 144.7701 - loglik: -1.4358e+02 - logprior: -1.1924e+00
Epoch 4/10
19/19 - 1s - loss: 142.2654 - loglik: -1.4111e+02 - logprior: -1.1533e+00
Epoch 5/10
19/19 - 1s - loss: 141.4616 - loglik: -1.4035e+02 - logprior: -1.1132e+00
Epoch 6/10
19/19 - 1s - loss: 141.0926 - loglik: -1.3999e+02 - logprior: -1.0988e+00
Epoch 7/10
19/19 - 1s - loss: 140.8033 - loglik: -1.3972e+02 - logprior: -1.0837e+00
Epoch 8/10
19/19 - 1s - loss: 140.7768 - loglik: -1.3970e+02 - logprior: -1.0722e+00
Epoch 9/10
19/19 - 1s - loss: 140.7193 - loglik: -1.3965e+02 - logprior: -1.0659e+00
Epoch 10/10
19/19 - 1s - loss: 140.6796 - loglik: -1.3962e+02 - logprior: -1.0599e+00
Fitted a model with MAP estimate = -140.6616
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (15, 1), (33, 1), (34, 1), (39, 2), (46, 1), (47, 1), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 141.1041 - loglik: -1.3690e+02 - logprior: -4.2084e+00
Epoch 2/2
19/19 - 1s - loss: 131.6722 - loglik: -1.3034e+02 - logprior: -1.3326e+00
Fitted a model with MAP estimate = -131.6410
expansions: [(0, 2)]
discards: [48]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.8116 - loglik: -1.2981e+02 - logprior: -4.0066e+00
Epoch 2/2
19/19 - 1s - loss: 129.4893 - loglik: -1.2829e+02 - logprior: -1.2029e+00
Fitted a model with MAP estimate = -129.8039
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 133.3125 - loglik: -1.2995e+02 - logprior: -3.3591e+00
Epoch 2/10
21/21 - 1s - loss: 129.7934 - loglik: -1.2839e+02 - logprior: -1.3992e+00
Epoch 3/10
21/21 - 1s - loss: 128.7639 - loglik: -1.2770e+02 - logprior: -1.0669e+00
Epoch 4/10
21/21 - 1s - loss: 128.5592 - loglik: -1.2752e+02 - logprior: -1.0349e+00
Epoch 5/10
21/21 - 1s - loss: 127.9510 - loglik: -1.2693e+02 - logprior: -1.0193e+00
Epoch 6/10
21/21 - 1s - loss: 127.9998 - loglik: -1.2699e+02 - logprior: -1.0135e+00
Fitted a model with MAP estimate = -127.7666
Time for alignment: 49.3914
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.4396 - loglik: -1.9232e+02 - logprior: -3.1151e+00
Epoch 2/10
19/19 - 1s - loss: 158.2142 - loglik: -1.5705e+02 - logprior: -1.1682e+00
Epoch 3/10
19/19 - 1s - loss: 142.9646 - loglik: -1.4172e+02 - logprior: -1.2422e+00
Epoch 4/10
19/19 - 1s - loss: 139.9348 - loglik: -1.3871e+02 - logprior: -1.2209e+00
Epoch 5/10
19/19 - 1s - loss: 139.1409 - loglik: -1.3798e+02 - logprior: -1.1611e+00
Epoch 6/10
19/19 - 1s - loss: 138.8079 - loglik: -1.3767e+02 - logprior: -1.1380e+00
Epoch 7/10
19/19 - 1s - loss: 138.7399 - loglik: -1.3763e+02 - logprior: -1.1118e+00
Epoch 8/10
19/19 - 1s - loss: 138.5653 - loglik: -1.3746e+02 - logprior: -1.1024e+00
Epoch 9/10
19/19 - 1s - loss: 138.5683 - loglik: -1.3747e+02 - logprior: -1.0938e+00
Fitted a model with MAP estimate = -138.5792
expansions: [(0, 3), (8, 1), (13, 1), (14, 1), (15, 1), (33, 1), (34, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 141.5432 - loglik: -1.3731e+02 - logprior: -4.2337e+00
Epoch 2/2
19/19 - 1s - loss: 131.2675 - loglik: -1.2988e+02 - logprior: -1.3896e+00
Fitted a model with MAP estimate = -130.9661
expansions: [(0, 2)]
discards: [48 59 64]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.3949 - loglik: -1.2939e+02 - logprior: -4.0094e+00
Epoch 2/2
19/19 - 1s - loss: 128.7660 - loglik: -1.2755e+02 - logprior: -1.2117e+00
Fitted a model with MAP estimate = -128.9689
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 132.7220 - loglik: -1.2936e+02 - logprior: -3.3599e+00
Epoch 2/10
21/21 - 1s - loss: 129.6010 - loglik: -1.2817e+02 - logprior: -1.4277e+00
Epoch 3/10
21/21 - 1s - loss: 128.3339 - loglik: -1.2726e+02 - logprior: -1.0725e+00
Epoch 4/10
21/21 - 1s - loss: 128.1110 - loglik: -1.2708e+02 - logprior: -1.0317e+00
Epoch 5/10
21/21 - 1s - loss: 127.4828 - loglik: -1.2645e+02 - logprior: -1.0300e+00
Epoch 6/10
21/21 - 1s - loss: 127.5064 - loglik: -1.2650e+02 - logprior: -1.0038e+00
Fitted a model with MAP estimate = -127.3086
Time for alignment: 46.8066
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 195.5210 - loglik: -1.9241e+02 - logprior: -3.1146e+00
Epoch 2/10
19/19 - 1s - loss: 158.8277 - loglik: -1.5767e+02 - logprior: -1.1593e+00
Epoch 3/10
19/19 - 1s - loss: 145.3927 - loglik: -1.4420e+02 - logprior: -1.1959e+00
Epoch 4/10
19/19 - 1s - loss: 141.7697 - loglik: -1.4061e+02 - logprior: -1.1614e+00
Epoch 5/10
19/19 - 1s - loss: 141.0280 - loglik: -1.3991e+02 - logprior: -1.1189e+00
Epoch 6/10
19/19 - 1s - loss: 140.6377 - loglik: -1.3953e+02 - logprior: -1.1038e+00
Epoch 7/10
19/19 - 1s - loss: 140.5499 - loglik: -1.3946e+02 - logprior: -1.0883e+00
Epoch 8/10
19/19 - 1s - loss: 140.1751 - loglik: -1.3910e+02 - logprior: -1.0759e+00
Epoch 9/10
19/19 - 1s - loss: 140.3708 - loglik: -1.3930e+02 - logprior: -1.0686e+00
Fitted a model with MAP estimate = -140.2579
expansions: [(0, 3), (11, 2), (12, 1), (14, 2), (19, 1), (33, 1), (34, 1), (39, 2), (46, 1), (47, 1), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.4479 - loglik: -1.3723e+02 - logprior: -4.2206e+00
Epoch 2/2
19/19 - 1s - loss: 131.6403 - loglik: -1.3025e+02 - logprior: -1.3926e+00
Fitted a model with MAP estimate = -131.3314
expansions: [(0, 2)]
discards: [15 20 50]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.9900 - loglik: -1.2997e+02 - logprior: -4.0201e+00
Epoch 2/2
19/19 - 1s - loss: 129.3792 - loglik: -1.2815e+02 - logprior: -1.2332e+00
Fitted a model with MAP estimate = -129.8242
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 133.4083 - loglik: -1.3005e+02 - logprior: -3.3627e+00
Epoch 2/10
21/21 - 1s - loss: 130.0744 - loglik: -1.2866e+02 - logprior: -1.4113e+00
Epoch 3/10
21/21 - 1s - loss: 128.9444 - loglik: -1.2787e+02 - logprior: -1.0701e+00
Epoch 4/10
21/21 - 1s - loss: 128.4820 - loglik: -1.2745e+02 - logprior: -1.0349e+00
Epoch 5/10
21/21 - 1s - loss: 128.4107 - loglik: -1.2738e+02 - logprior: -1.0270e+00
Epoch 6/10
21/21 - 1s - loss: 127.9121 - loglik: -1.2691e+02 - logprior: -1.0043e+00
Epoch 7/10
21/21 - 1s - loss: 127.9978 - loglik: -1.2701e+02 - logprior: -9.8317e-01
Fitted a model with MAP estimate = -127.8087
Time for alignment: 48.5307
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 195.3776 - loglik: -1.9226e+02 - logprior: -3.1146e+00
Epoch 2/10
19/19 - 1s - loss: 155.8916 - loglik: -1.5473e+02 - logprior: -1.1633e+00
Epoch 3/10
19/19 - 1s - loss: 140.8675 - loglik: -1.3962e+02 - logprior: -1.2488e+00
Epoch 4/10
19/19 - 1s - loss: 138.8964 - loglik: -1.3769e+02 - logprior: -1.2047e+00
Epoch 5/10
19/19 - 1s - loss: 138.3100 - loglik: -1.3717e+02 - logprior: -1.1427e+00
Epoch 6/10
19/19 - 1s - loss: 138.0005 - loglik: -1.3689e+02 - logprior: -1.1137e+00
Epoch 7/10
19/19 - 1s - loss: 137.9949 - loglik: -1.3690e+02 - logprior: -1.0948e+00
Epoch 8/10
19/19 - 1s - loss: 137.7321 - loglik: -1.3666e+02 - logprior: -1.0769e+00
Epoch 9/10
19/19 - 1s - loss: 137.9127 - loglik: -1.3684e+02 - logprior: -1.0713e+00
Fitted a model with MAP estimate = -137.8964
expansions: [(0, 3), (13, 2), (14, 1), (19, 1), (34, 1), (40, 2), (44, 1), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.2654 - loglik: -1.3705e+02 - logprior: -4.2104e+00
Epoch 2/2
19/19 - 1s - loss: 132.2972 - loglik: -1.3089e+02 - logprior: -1.4082e+00
Fitted a model with MAP estimate = -132.2037
expansions: [(0, 2)]
discards: [48 59 64]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.2827 - loglik: -1.3028e+02 - logprior: -3.9982e+00
Epoch 2/2
19/19 - 1s - loss: 129.2127 - loglik: -1.2798e+02 - logprior: -1.2303e+00
Fitted a model with MAP estimate = -129.2466
expansions: []
discards: [ 0 14]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 133.6953 - loglik: -1.3033e+02 - logprior: -3.3677e+00
Epoch 2/10
21/21 - 1s - loss: 130.4459 - loglik: -1.2901e+02 - logprior: -1.4373e+00
Epoch 3/10
21/21 - 1s - loss: 129.2061 - loglik: -1.2814e+02 - logprior: -1.0668e+00
Epoch 4/10
21/21 - 1s - loss: 128.9458 - loglik: -1.2791e+02 - logprior: -1.0367e+00
Epoch 5/10
21/21 - 1s - loss: 128.5477 - loglik: -1.2752e+02 - logprior: -1.0231e+00
Epoch 6/10
21/21 - 1s - loss: 128.3682 - loglik: -1.2737e+02 - logprior: -1.0026e+00
Epoch 7/10
21/21 - 1s - loss: 128.3086 - loglik: -1.2732e+02 - logprior: -9.9026e-01
Epoch 8/10
21/21 - 1s - loss: 128.2880 - loglik: -1.2731e+02 - logprior: -9.7467e-01
Epoch 9/10
21/21 - 1s - loss: 128.0252 - loglik: -1.2707e+02 - logprior: -9.5470e-01
Epoch 10/10
21/21 - 1s - loss: 128.1097 - loglik: -1.2718e+02 - logprior: -9.3363e-01
Fitted a model with MAP estimate = -127.9595
Time for alignment: 52.3793
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 194.9130 - loglik: -1.9180e+02 - logprior: -3.1143e+00
Epoch 2/10
19/19 - 1s - loss: 156.3968 - loglik: -1.5524e+02 - logprior: -1.1533e+00
Epoch 3/10
19/19 - 1s - loss: 143.7894 - loglik: -1.4259e+02 - logprior: -1.2014e+00
Epoch 4/10
19/19 - 1s - loss: 141.5305 - loglik: -1.4036e+02 - logprior: -1.1659e+00
Epoch 5/10
19/19 - 1s - loss: 140.7958 - loglik: -1.3968e+02 - logprior: -1.1185e+00
Epoch 6/10
19/19 - 1s - loss: 140.5780 - loglik: -1.3947e+02 - logprior: -1.1036e+00
Epoch 7/10
19/19 - 1s - loss: 140.2968 - loglik: -1.3921e+02 - logprior: -1.0829e+00
Epoch 8/10
19/19 - 1s - loss: 140.3874 - loglik: -1.3932e+02 - logprior: -1.0647e+00
Fitted a model with MAP estimate = -140.0255
expansions: [(0, 4), (12, 1), (13, 1), (14, 1), (15, 1), (34, 1), (37, 2), (39, 2), (46, 1), (47, 1), (52, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 140.9317 - loglik: -1.3679e+02 - logprior: -4.1420e+00
Epoch 2/2
19/19 - 1s - loss: 131.0903 - loglik: -1.2966e+02 - logprior: -1.4291e+00
Fitted a model with MAP estimate = -131.5550
expansions: []
discards: [46 50]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 132.2226 - loglik: -1.2917e+02 - logprior: -3.0555e+00
Epoch 2/2
19/19 - 1s - loss: 128.9495 - loglik: -1.2776e+02 - logprior: -1.1883e+00
Fitted a model with MAP estimate = -129.7520
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 131.0470 - loglik: -1.2863e+02 - logprior: -2.4133e+00
Epoch 2/10
21/21 - 1s - loss: 129.0047 - loglik: -1.2792e+02 - logprior: -1.0830e+00
Epoch 3/10
21/21 - 1s - loss: 128.7442 - loglik: -1.2768e+02 - logprior: -1.0684e+00
Epoch 4/10
21/21 - 1s - loss: 128.2615 - loglik: -1.2725e+02 - logprior: -1.0147e+00
Epoch 5/10
21/21 - 1s - loss: 128.2755 - loglik: -1.2728e+02 - logprior: -9.9193e-01
Fitted a model with MAP estimate = -127.9113
Time for alignment: 45.0656
Computed alignments with likelihoods: ['-127.7666', '-127.3086', '-127.8087', '-127.9595', '-127.9113']
Best model has likelihood: -127.3086  (prior= -0.9655 )
time for generating output: 0.1278
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.9451029320024953
Training of 5 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9d9790a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaaf0f4490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ad935100>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.3362 - loglik: -3.8322e+02 - logprior: -8.1119e+00
Epoch 2/10
13/13 - 2s - loss: 337.9790 - loglik: -3.3611e+02 - logprior: -1.8706e+00
Epoch 3/10
13/13 - 2s - loss: 296.8940 - loglik: -2.9519e+02 - logprior: -1.7060e+00
Epoch 4/10
13/13 - 2s - loss: 283.8438 - loglik: -2.8189e+02 - logprior: -1.9586e+00
Epoch 5/10
13/13 - 2s - loss: 280.7916 - loglik: -2.7873e+02 - logprior: -2.0582e+00
Epoch 6/10
13/13 - 2s - loss: 279.1244 - loglik: -2.7722e+02 - logprior: -1.9090e+00
Epoch 7/10
13/13 - 2s - loss: 278.2293 - loglik: -2.7635e+02 - logprior: -1.8797e+00
Epoch 8/10
13/13 - 2s - loss: 277.8875 - loglik: -2.7600e+02 - logprior: -1.8870e+00
Epoch 9/10
13/13 - 2s - loss: 277.9339 - loglik: -2.7606e+02 - logprior: -1.8786e+00
Fitted a model with MAP estimate = -277.5749
expansions: [(7, 3), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (49, 1), (50, 1), (51, 1), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 284.3062 - loglik: -2.7475e+02 - logprior: -9.5576e+00
Epoch 2/2
13/13 - 2s - loss: 266.2833 - loglik: -2.6239e+02 - logprior: -3.8931e+00
Fitted a model with MAP estimate = -263.4418
expansions: [(0, 3), (10, 1)]
discards: [ 0 85 93]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 267.6555 - loglik: -2.6028e+02 - logprior: -7.3757e+00
Epoch 2/2
13/13 - 2s - loss: 259.4958 - loglik: -2.5773e+02 - logprior: -1.7633e+00
Fitted a model with MAP estimate = -257.6503
expansions: []
discards: [ 0  2 24]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 270.4850 - loglik: -2.6123e+02 - logprior: -9.2577e+00
Epoch 2/10
13/13 - 2s - loss: 261.3346 - loglik: -2.5846e+02 - logprior: -2.8788e+00
Epoch 3/10
13/13 - 2s - loss: 257.8912 - loglik: -2.5691e+02 - logprior: -9.8301e-01
Epoch 4/10
13/13 - 2s - loss: 257.1294 - loglik: -2.5673e+02 - logprior: -4.0288e-01
Epoch 5/10
13/13 - 2s - loss: 256.1870 - loglik: -2.5588e+02 - logprior: -3.0229e-01
Epoch 6/10
13/13 - 2s - loss: 255.5741 - loglik: -2.5531e+02 - logprior: -2.6179e-01
Epoch 7/10
13/13 - 2s - loss: 255.6961 - loglik: -2.5543e+02 - logprior: -2.6658e-01
Fitted a model with MAP estimate = -255.1872
Time for alignment: 70.6993
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 391.7514 - loglik: -3.8365e+02 - logprior: -8.1055e+00
Epoch 2/10
13/13 - 2s - loss: 337.6659 - loglik: -3.3580e+02 - logprior: -1.8695e+00
Epoch 3/10
13/13 - 2s - loss: 296.4446 - loglik: -2.9472e+02 - logprior: -1.7230e+00
Epoch 4/10
13/13 - 2s - loss: 284.4333 - loglik: -2.8250e+02 - logprior: -1.9342e+00
Epoch 5/10
13/13 - 2s - loss: 281.5737 - loglik: -2.7959e+02 - logprior: -1.9867e+00
Epoch 6/10
13/13 - 2s - loss: 279.9496 - loglik: -2.7807e+02 - logprior: -1.8759e+00
Epoch 7/10
13/13 - 2s - loss: 279.3803 - loglik: -2.7751e+02 - logprior: -1.8728e+00
Epoch 8/10
13/13 - 2s - loss: 277.9683 - loglik: -2.7610e+02 - logprior: -1.8713e+00
Epoch 9/10
13/13 - 2s - loss: 278.8588 - loglik: -2.7700e+02 - logprior: -1.8603e+00
Fitted a model with MAP estimate = -278.2787
expansions: [(7, 3), (11, 1), (14, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 285.5436 - loglik: -2.7601e+02 - logprior: -9.5376e+00
Epoch 2/2
13/13 - 2s - loss: 267.2387 - loglik: -2.6331e+02 - logprior: -3.9328e+00
Fitted a model with MAP estimate = -263.8960
expansions: [(0, 3), (9, 1)]
discards: [ 0 65 94]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 267.2151 - loglik: -2.5983e+02 - logprior: -7.3832e+00
Epoch 2/2
13/13 - 2s - loss: 259.3302 - loglik: -2.5757e+02 - logprior: -1.7578e+00
Fitted a model with MAP estimate = -257.3202
expansions: []
discards: [ 0  2 24 89]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 270.6022 - loglik: -2.6137e+02 - logprior: -9.2335e+00
Epoch 2/10
13/13 - 2s - loss: 262.1223 - loglik: -2.5917e+02 - logprior: -2.9557e+00
Epoch 3/10
13/13 - 2s - loss: 257.6177 - loglik: -2.5664e+02 - logprior: -9.7357e-01
Epoch 4/10
13/13 - 2s - loss: 257.4235 - loglik: -2.5705e+02 - logprior: -3.7436e-01
Epoch 5/10
13/13 - 2s - loss: 256.1142 - loglik: -2.5585e+02 - logprior: -2.6543e-01
Epoch 6/10
13/13 - 2s - loss: 256.1068 - loglik: -2.5590e+02 - logprior: -2.1048e-01
Epoch 7/10
13/13 - 2s - loss: 255.0615 - loglik: -2.5484e+02 - logprior: -2.2436e-01
Epoch 8/10
13/13 - 2s - loss: 255.3875 - loglik: -2.5519e+02 - logprior: -2.0007e-01
Fitted a model with MAP estimate = -255.0913
Time for alignment: 74.6087
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.7469 - loglik: -3.8363e+02 - logprior: -8.1131e+00
Epoch 2/10
13/13 - 2s - loss: 338.0981 - loglik: -3.3623e+02 - logprior: -1.8697e+00
Epoch 3/10
13/13 - 2s - loss: 298.8790 - loglik: -2.9717e+02 - logprior: -1.7101e+00
Epoch 4/10
13/13 - 2s - loss: 284.4926 - loglik: -2.8253e+02 - logprior: -1.9599e+00
Epoch 5/10
13/13 - 2s - loss: 280.9090 - loglik: -2.7889e+02 - logprior: -2.0193e+00
Epoch 6/10
13/13 - 2s - loss: 279.6854 - loglik: -2.7779e+02 - logprior: -1.8939e+00
Epoch 7/10
13/13 - 2s - loss: 279.3864 - loglik: -2.7751e+02 - logprior: -1.8803e+00
Epoch 8/10
13/13 - 2s - loss: 277.9778 - loglik: -2.7610e+02 - logprior: -1.8759e+00
Epoch 9/10
13/13 - 2s - loss: 278.3688 - loglik: -2.7650e+02 - logprior: -1.8675e+00
Fitted a model with MAP estimate = -277.9841
expansions: [(7, 3), (11, 1), (14, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (49, 1), (50, 1), (51, 1), (70, 2), (75, 1), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 283.9297 - loglik: -2.7443e+02 - logprior: -9.5028e+00
Epoch 2/2
13/13 - 2s - loss: 266.0457 - loglik: -2.6223e+02 - logprior: -3.8187e+00
Fitted a model with MAP estimate = -263.4052
expansions: [(0, 3), (10, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 267.0306 - loglik: -2.5969e+02 - logprior: -7.3380e+00
Epoch 2/2
13/13 - 3s - loss: 258.8331 - loglik: -2.5708e+02 - logprior: -1.7486e+00
Fitted a model with MAP estimate = -257.2490
expansions: []
discards: [ 0  2 24 89]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 270.0723 - loglik: -2.6083e+02 - logprior: -9.2447e+00
Epoch 2/10
13/13 - 2s - loss: 262.1406 - loglik: -2.5920e+02 - logprior: -2.9430e+00
Epoch 3/10
13/13 - 2s - loss: 258.3875 - loglik: -2.5739e+02 - logprior: -9.9559e-01
Epoch 4/10
13/13 - 2s - loss: 256.8222 - loglik: -2.5644e+02 - logprior: -3.8050e-01
Epoch 5/10
13/13 - 2s - loss: 256.1102 - loglik: -2.5584e+02 - logprior: -2.7319e-01
Epoch 6/10
13/13 - 2s - loss: 256.5297 - loglik: -2.5630e+02 - logprior: -2.3010e-01
Fitted a model with MAP estimate = -255.4633
Time for alignment: 67.6458
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.3447 - loglik: -3.8323e+02 - logprior: -8.1116e+00
Epoch 2/10
13/13 - 2s - loss: 338.1465 - loglik: -3.3628e+02 - logprior: -1.8661e+00
Epoch 3/10
13/13 - 2s - loss: 300.3462 - loglik: -2.9866e+02 - logprior: -1.6829e+00
Epoch 4/10
13/13 - 2s - loss: 285.1027 - loglik: -2.8320e+02 - logprior: -1.9021e+00
Epoch 5/10
13/13 - 2s - loss: 280.6572 - loglik: -2.7863e+02 - logprior: -2.0223e+00
Epoch 6/10
13/13 - 2s - loss: 279.3019 - loglik: -2.7738e+02 - logprior: -1.9178e+00
Epoch 7/10
13/13 - 2s - loss: 278.0584 - loglik: -2.7614e+02 - logprior: -1.9174e+00
Epoch 8/10
13/13 - 2s - loss: 277.2961 - loglik: -2.7538e+02 - logprior: -1.9153e+00
Epoch 9/10
13/13 - 2s - loss: 277.8777 - loglik: -2.7597e+02 - logprior: -1.9028e+00
Fitted a model with MAP estimate = -277.2584
expansions: [(7, 2), (8, 2), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (35, 1), (36, 1), (37, 1), (38, 1), (48, 1), (49, 1), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 284.7088 - loglik: -2.7522e+02 - logprior: -9.4933e+00
Epoch 2/2
13/13 - 2s - loss: 265.7458 - loglik: -2.6195e+02 - logprior: -3.8001e+00
Fitted a model with MAP estimate = -263.3443
expansions: [(0, 3)]
discards: [ 0 86 94]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 268.0842 - loglik: -2.6075e+02 - logprior: -7.3321e+00
Epoch 2/2
13/13 - 2s - loss: 259.3990 - loglik: -2.5769e+02 - logprior: -1.7048e+00
Fitted a model with MAP estimate = -257.7616
expansions: []
discards: [ 0  2 24]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 269.8008 - loglik: -2.6061e+02 - logprior: -9.1866e+00
Epoch 2/10
13/13 - 2s - loss: 261.8848 - loglik: -2.5914e+02 - logprior: -2.7402e+00
Epoch 3/10
13/13 - 2s - loss: 258.7393 - loglik: -2.5784e+02 - logprior: -8.9621e-01
Epoch 4/10
13/13 - 2s - loss: 256.4999 - loglik: -2.5615e+02 - logprior: -3.4818e-01
Epoch 5/10
13/13 - 2s - loss: 255.8870 - loglik: -2.5564e+02 - logprior: -2.5008e-01
Epoch 6/10
13/13 - 2s - loss: 255.7566 - loglik: -2.5553e+02 - logprior: -2.2378e-01
Epoch 7/10
13/13 - 2s - loss: 255.4891 - loglik: -2.5528e+02 - logprior: -2.1120e-01
Epoch 8/10
13/13 - 2s - loss: 255.3851 - loglik: -2.5519e+02 - logprior: -1.9446e-01
Epoch 9/10
13/13 - 2s - loss: 254.8515 - loglik: -2.5470e+02 - logprior: -1.5652e-01
Epoch 10/10
13/13 - 2s - loss: 254.8698 - loglik: -2.5475e+02 - logprior: -1.1867e-01
Fitted a model with MAP estimate = -254.8637
Time for alignment: 76.7756
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.3230 - loglik: -3.8321e+02 - logprior: -8.1123e+00
Epoch 2/10
13/13 - 2s - loss: 338.3167 - loglik: -3.3644e+02 - logprior: -1.8738e+00
Epoch 3/10
13/13 - 2s - loss: 299.2813 - loglik: -2.9756e+02 - logprior: -1.7227e+00
Epoch 4/10
13/13 - 2s - loss: 284.0218 - loglik: -2.8200e+02 - logprior: -2.0177e+00
Epoch 5/10
13/13 - 2s - loss: 279.8253 - loglik: -2.7767e+02 - logprior: -2.1518e+00
Epoch 6/10
13/13 - 2s - loss: 278.3272 - loglik: -2.7626e+02 - logprior: -2.0657e+00
Epoch 7/10
13/13 - 2s - loss: 277.5431 - loglik: -2.7550e+02 - logprior: -2.0479e+00
Epoch 8/10
13/13 - 2s - loss: 276.1685 - loglik: -2.7410e+02 - logprior: -2.0637e+00
Epoch 9/10
13/13 - 2s - loss: 276.4010 - loglik: -2.7436e+02 - logprior: -2.0423e+00
Fitted a model with MAP estimate = -276.2525
expansions: [(7, 2), (8, 2), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (49, 1), (50, 1), (51, 1), (70, 2), (75, 1), (76, 1), (77, 2), (79, 1), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 282.5892 - loglik: -2.7304e+02 - logprior: -9.5458e+00
Epoch 2/2
13/13 - 2s - loss: 265.4204 - loglik: -2.6158e+02 - logprior: -3.8403e+00
Fitted a model with MAP estimate = -263.0618
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 267.1418 - loglik: -2.5973e+02 - logprior: -7.4102e+00
Epoch 2/2
13/13 - 2s - loss: 258.6194 - loglik: -2.5682e+02 - logprior: -1.8017e+00
Fitted a model with MAP estimate = -257.2786
expansions: []
discards: [ 0  2 89]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 269.8332 - loglik: -2.6057e+02 - logprior: -9.2643e+00
Epoch 2/10
13/13 - 2s - loss: 261.6342 - loglik: -2.5875e+02 - logprior: -2.8835e+00
Epoch 3/10
13/13 - 2s - loss: 258.0927 - loglik: -2.5713e+02 - logprior: -9.6683e-01
Epoch 4/10
13/13 - 2s - loss: 256.8701 - loglik: -2.5646e+02 - logprior: -4.1082e-01
Epoch 5/10
13/13 - 2s - loss: 255.6366 - loglik: -2.5534e+02 - logprior: -2.9831e-01
Epoch 6/10
13/13 - 2s - loss: 255.6080 - loglik: -2.5532e+02 - logprior: -2.9067e-01
Epoch 7/10
13/13 - 2s - loss: 255.2038 - loglik: -2.5493e+02 - logprior: -2.7242e-01
Epoch 8/10
13/13 - 2s - loss: 255.4921 - loglik: -2.5524e+02 - logprior: -2.5433e-01
Fitted a model with MAP estimate = -254.9297
Time for alignment: 71.6136
Computed alignments with likelihoods: ['-255.1872', '-255.0913', '-255.4633', '-254.8637', '-254.9297']
Best model has likelihood: -254.8637  (prior= -0.1065 )
time for generating output: 0.1950
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.9292929292929293
Training of 5 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe98b36cee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ad747d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb497632b0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1082.6700 - loglik: -1.0792e+03 - logprior: -3.4332e+00
Epoch 2/10
25/25 - 28s - loss: 824.7851 - loglik: -8.2410e+02 - logprior: -6.8530e-01
Epoch 3/10
25/25 - 28s - loss: 772.0119 - loglik: -7.7059e+02 - logprior: -1.4220e+00
Epoch 4/10
25/25 - 28s - loss: 768.2928 - loglik: -7.6669e+02 - logprior: -1.6032e+00
Epoch 5/10
25/25 - 28s - loss: 763.3704 - loglik: -7.6162e+02 - logprior: -1.7550e+00
Epoch 6/10
25/25 - 29s - loss: 758.2856 - loglik: -7.5677e+02 - logprior: -1.5173e+00
Epoch 7/10
25/25 - 29s - loss: 757.4339 - loglik: -7.5584e+02 - logprior: -1.5965e+00
Epoch 8/10
25/25 - 28s - loss: 760.3287 - loglik: -7.5868e+02 - logprior: -1.6498e+00
Fitted a model with MAP estimate = -757.8138
expansions: [(0, 2), (145, 1), (146, 1), (147, 1), (165, 2), (170, 1), (174, 6), (175, 2), (176, 2), (188, 1), (191, 6), (192, 2), (195, 1), (196, 3), (197, 2), (199, 1), (200, 2), (202, 1), (203, 1), (204, 1), (205, 1), (206, 2), (208, 1), (209, 1), (214, 1), (220, 1), (222, 1), (223, 1), (224, 1), (225, 1), (227, 1), (228, 1), (229, 1), (232, 1), (233, 1), (235, 1), (236, 1), (247, 1), (253, 3), (254, 2), (255, 1), (257, 5), (279, 2), (280, 1), (281, 1), (298, 1), (299, 2), (300, 2), (302, 1), (303, 1), (312, 1), (317, 1), (320, 2), (324, 2), (326, 2), (340, 1), (353, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 463 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 745.2363 - loglik: -7.3906e+02 - logprior: -6.1774e+00
Epoch 2/2
25/25 - 41s - loss: 707.6094 - loglik: -7.0639e+02 - logprior: -1.2224e+00
Fitted a model with MAP estimate = -701.8022
expansions: [(171, 1), (218, 1), (441, 1)]
discards: [  2 185 186 187 188 228 234 246 313 374 402 426 443]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 714.2801 - loglik: -7.1082e+02 - logprior: -3.4577e+00
Epoch 2/2
25/25 - 40s - loss: 702.2625 - loglik: -7.0302e+02 - logprior: 0.7586
Fitted a model with MAP estimate = -700.5763
expansions: [(129, 1), (243, 1), (315, 1)]
discards: [311 431]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 46s - loss: 709.7164 - loglik: -7.0751e+02 - logprior: -2.2065e+00
Epoch 2/10
25/25 - 40s - loss: 703.0617 - loglik: -7.0478e+02 - logprior: 1.7153
Epoch 3/10
25/25 - 40s - loss: 698.8932 - loglik: -7.0111e+02 - logprior: 2.2171
Epoch 4/10
25/25 - 40s - loss: 696.0748 - loglik: -6.9847e+02 - logprior: 2.3977
Epoch 5/10
25/25 - 40s - loss: 696.9034 - loglik: -6.9944e+02 - logprior: 2.5398
Fitted a model with MAP estimate = -695.3162
Time for alignment: 749.3921
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1086.7611 - loglik: -1.0832e+03 - logprior: -3.5149e+00
Epoch 2/10
25/25 - 28s - loss: 829.0458 - loglik: -8.2820e+02 - logprior: -8.4841e-01
Epoch 3/10
25/25 - 29s - loss: 775.3284 - loglik: -7.7373e+02 - logprior: -1.5997e+00
Epoch 4/10
25/25 - 29s - loss: 767.9914 - loglik: -7.6613e+02 - logprior: -1.8612e+00
Epoch 5/10
25/25 - 29s - loss: 761.1364 - loglik: -7.5922e+02 - logprior: -1.9133e+00
Epoch 6/10
25/25 - 29s - loss: 762.1061 - loglik: -7.6014e+02 - logprior: -1.9690e+00
Fitted a model with MAP estimate = -759.5865
expansions: [(0, 2), (46, 1), (106, 1), (131, 1), (133, 2), (134, 1), (135, 1), (161, 1), (162, 2), (166, 1), (167, 1), (171, 7), (172, 3), (173, 1), (174, 1), (182, 1), (184, 1), (185, 1), (186, 4), (187, 1), (188, 1), (193, 1), (194, 1), (195, 1), (197, 1), (199, 1), (202, 1), (204, 1), (205, 1), (208, 1), (209, 1), (215, 1), (220, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (233, 1), (235, 1), (236, 1), (248, 1), (254, 4), (255, 2), (257, 3), (258, 1), (280, 1), (281, 1), (282, 1), (299, 1), (300, 2), (301, 2), (315, 1), (317, 4), (318, 2), (326, 2), (350, 1), (353, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 461 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 745.4906 - loglik: -7.3939e+02 - logprior: -6.1055e+00
Epoch 2/2
25/25 - 41s - loss: 709.0009 - loglik: -7.0786e+02 - logprior: -1.1400e+00
Fitted a model with MAP estimate = -702.7005
expansions: [(250, 1), (398, 1), (439, 1)]
discards: [  2 141 189 190 191 217 315 323 374 441]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 712.5312 - loglik: -7.0876e+02 - logprior: -3.7729e+00
Epoch 2/2
25/25 - 40s - loss: 702.2209 - loglik: -7.0252e+02 - logprior: 0.3013
Fitted a model with MAP estimate = -699.0191
expansions: []
discards: [187 418 432]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 44s - loss: 710.3943 - loglik: -7.0776e+02 - logprior: -2.6307e+00
Epoch 2/10
25/25 - 40s - loss: 701.9999 - loglik: -7.0354e+02 - logprior: 1.5419
Epoch 3/10
25/25 - 40s - loss: 697.8923 - loglik: -6.9982e+02 - logprior: 1.9233
Epoch 4/10
25/25 - 40s - loss: 698.5753 - loglik: -7.0072e+02 - logprior: 2.1451
Fitted a model with MAP estimate = -696.6431
Time for alignment: 651.8846
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1080.6997 - loglik: -1.0773e+03 - logprior: -3.3846e+00
Epoch 2/10
25/25 - 28s - loss: 821.4667 - loglik: -8.2089e+02 - logprior: -5.7297e-01
Epoch 3/10
25/25 - 29s - loss: 774.9679 - loglik: -7.7377e+02 - logprior: -1.1948e+00
Epoch 4/10
25/25 - 28s - loss: 763.6239 - loglik: -7.6221e+02 - logprior: -1.4143e+00
Epoch 5/10
25/25 - 29s - loss: 758.1625 - loglik: -7.5669e+02 - logprior: -1.4692e+00
Epoch 6/10
25/25 - 29s - loss: 756.7263 - loglik: -7.5523e+02 - logprior: -1.5012e+00
Epoch 7/10
25/25 - 29s - loss: 756.7863 - loglik: -7.5524e+02 - logprior: -1.5430e+00
Fitted a model with MAP estimate = -755.8899
expansions: [(0, 2), (146, 3), (165, 3), (175, 5), (176, 2), (177, 2), (193, 6), (194, 1), (198, 4), (199, 1), (201, 1), (203, 1), (204, 1), (206, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (217, 1), (218, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (228, 1), (229, 1), (230, 1), (235, 1), (237, 2), (238, 1), (249, 1), (252, 1), (254, 3), (255, 4), (257, 3), (266, 1), (280, 1), (281, 1), (282, 1), (299, 1), (300, 1), (301, 2), (303, 1), (317, 1), (318, 1), (319, 1), (321, 2), (326, 1), (327, 3), (350, 1), (353, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 739.1167 - loglik: -7.3316e+02 - logprior: -5.9569e+00
Epoch 2/2
25/25 - 41s - loss: 711.5007 - loglik: -7.1084e+02 - logprior: -6.6099e-01
Fitted a model with MAP estimate = -705.5554
expansions: [(172, 1), (436, 1)]
discards: [  2 185 186 187 224 311 319 398 438]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 714.7341 - loglik: -7.1122e+02 - logprior: -3.5118e+00
Epoch 2/2
25/25 - 39s - loss: 707.2539 - loglik: -7.0815e+02 - logprior: 0.8955
Fitted a model with MAP estimate = -702.4410
expansions: [(210, 4)]
discards: [307 429]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 711.1418 - loglik: -7.0866e+02 - logprior: -2.4803e+00
Epoch 2/10
25/25 - 40s - loss: 700.7985 - loglik: -7.0229e+02 - logprior: 1.4931
Epoch 3/10
25/25 - 40s - loss: 700.6019 - loglik: -7.0251e+02 - logprior: 1.9086
Epoch 4/10
25/25 - 40s - loss: 697.0452 - loglik: -6.9917e+02 - logprior: 2.1287
Epoch 5/10
25/25 - 40s - loss: 696.6624 - loglik: -6.9899e+02 - logprior: 2.3275
Epoch 6/10
25/25 - 40s - loss: 697.0066 - loglik: -6.9951e+02 - logprior: 2.5060
Fitted a model with MAP estimate = -694.6963
Time for alignment: 757.9568
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1082.7939 - loglik: -1.0794e+03 - logprior: -3.4211e+00
Epoch 2/10
25/25 - 29s - loss: 820.0521 - loglik: -8.1921e+02 - logprior: -8.4625e-01
Epoch 3/10
25/25 - 29s - loss: 773.7645 - loglik: -7.7209e+02 - logprior: -1.6778e+00
Epoch 4/10
25/25 - 29s - loss: 760.0756 - loglik: -7.5809e+02 - logprior: -1.9861e+00
Epoch 5/10
25/25 - 28s - loss: 752.3203 - loglik: -7.5036e+02 - logprior: -1.9636e+00
Epoch 6/10
25/25 - 28s - loss: 755.0049 - loglik: -7.5295e+02 - logprior: -2.0502e+00
Fitted a model with MAP estimate = -752.3616
expansions: [(0, 2), (126, 1), (132, 1), (145, 1), (165, 3), (170, 1), (174, 5), (175, 2), (176, 2), (177, 1), (178, 1), (189, 1), (190, 1), (191, 5), (192, 2), (194, 1), (195, 1), (196, 2), (197, 1), (199, 1), (200, 1), (202, 1), (204, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (215, 1), (221, 1), (222, 1), (223, 2), (224, 1), (225, 1), (227, 1), (228, 1), (229, 1), (232, 1), (236, 2), (237, 1), (248, 1), (251, 1), (253, 3), (254, 4), (256, 2), (280, 1), (281, 1), (282, 1), (296, 1), (298, 1), (300, 2), (303, 1), (314, 1), (316, 1), (317, 1), (318, 1), (325, 2), (327, 2), (356, 1), (358, 1), (364, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 732.1936 - loglik: -7.2730e+02 - logprior: -4.8972e+00
Epoch 2/2
25/25 - 40s - loss: 705.6147 - loglik: -7.0624e+02 - logprior: 0.6260
Fitted a model with MAP estimate = -701.4190
expansions: [(169, 1)]
discards: [  2 185 186 190 216 314 321]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 708.3383 - loglik: -7.0573e+02 - logprior: -2.6035e+00
Epoch 2/2
25/25 - 40s - loss: 705.3250 - loglik: -7.0661e+02 - logprior: 1.2844
Fitted a model with MAP estimate = -701.1301
expansions: []
discards: [311]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 710.2191 - loglik: -7.0790e+02 - logprior: -2.3147e+00
Epoch 2/10
25/25 - 39s - loss: 702.0762 - loglik: -7.0374e+02 - logprior: 1.6624
Epoch 3/10
25/25 - 40s - loss: 700.9849 - loglik: -7.0312e+02 - logprior: 2.1394
Epoch 4/10
25/25 - 40s - loss: 696.1475 - loglik: -6.9844e+02 - logprior: 2.2878
Epoch 5/10
25/25 - 40s - loss: 698.2024 - loglik: -7.0064e+02 - logprior: 2.4424
Fitted a model with MAP estimate = -695.7164
Time for alignment: 688.6556
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1077.6594 - loglik: -1.0741e+03 - logprior: -3.5214e+00
Epoch 2/10
25/25 - 29s - loss: 829.5215 - loglik: -8.2872e+02 - logprior: -8.0591e-01
Epoch 3/10
25/25 - 29s - loss: 775.6304 - loglik: -7.7411e+02 - logprior: -1.5234e+00
Epoch 4/10
25/25 - 28s - loss: 767.2991 - loglik: -7.6566e+02 - logprior: -1.6359e+00
Epoch 5/10
25/25 - 29s - loss: 763.0543 - loglik: -7.6133e+02 - logprior: -1.7223e+00
Epoch 6/10
25/25 - 28s - loss: 760.9512 - loglik: -7.5920e+02 - logprior: -1.7548e+00
Epoch 7/10
25/25 - 29s - loss: 761.5642 - loglik: -7.5979e+02 - logprior: -1.7731e+00
Fitted a model with MAP estimate = -759.0798
expansions: [(0, 2), (146, 1), (149, 1), (165, 1), (166, 2), (172, 1), (177, 1), (178, 2), (179, 2), (191, 1), (194, 6), (195, 2), (198, 1), (199, 3), (200, 2), (202, 1), (203, 1), (205, 1), (207, 1), (208, 1), (209, 1), (211, 1), (212, 1), (213, 1), (218, 3), (219, 1), (223, 1), (224, 2), (225, 1), (226, 1), (228, 1), (229, 1), (230, 1), (237, 2), (238, 1), (249, 1), (253, 1), (255, 2), (256, 3), (258, 1), (259, 1), (267, 1), (281, 1), (282, 1), (283, 1), (299, 1), (301, 2), (302, 2), (304, 1), (314, 1), (317, 1), (319, 1), (322, 2), (325, 2), (327, 2), (341, 1), (354, 3), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 457 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 742.6205 - loglik: -7.3643e+02 - logprior: -6.1879e+00
Epoch 2/2
25/25 - 40s - loss: 715.3107 - loglik: -7.1440e+02 - logprior: -9.1157e-01
Fitted a model with MAP estimate = -706.1438
expansions: [(216, 1), (403, 1), (434, 2)]
discards: [  1   2 226 367 396 419 435 436]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 714.3752 - loglik: -7.1075e+02 - logprior: -3.6255e+00
Epoch 2/2
25/25 - 40s - loss: 699.1384 - loglik: -6.9984e+02 - logprior: 0.6993
Fitted a model with MAP estimate = -698.3450
expansions: [(0, 2)]
discards: [177 178 179 180 430 431]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 42s - loss: 714.5418 - loglik: -7.0985e+02 - logprior: -4.6890e+00
Epoch 2/10
25/25 - 39s - loss: 705.4422 - loglik: -7.0692e+02 - logprior: 1.4745
Epoch 3/10
25/25 - 39s - loss: 701.2371 - loglik: -7.0322e+02 - logprior: 1.9866
Epoch 4/10
25/25 - 39s - loss: 700.4776 - loglik: -7.0256e+02 - logprior: 2.0807
Epoch 5/10
25/25 - 39s - loss: 698.1177 - loglik: -7.0047e+02 - logprior: 2.3497
Epoch 6/10
25/25 - 39s - loss: 698.6307 - loglik: -7.0121e+02 - logprior: 2.5791
Fitted a model with MAP estimate = -696.6559
Time for alignment: 754.2268
Computed alignments with likelihoods: ['-695.3162', '-696.6431', '-694.6963', '-695.7164', '-696.6559']
Best model has likelihood: -694.6963  (prior= 2.7276 )
time for generating output: 0.3644
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.9029850746268657
Training of 5 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e669c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead192b2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac0081cd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 327.1301 - loglik: -3.2265e+02 - logprior: -4.4799e+00
Epoch 2/10
16/16 - 3s - loss: 254.7381 - loglik: -2.5331e+02 - logprior: -1.4276e+00
Epoch 3/10
16/16 - 3s - loss: 225.2842 - loglik: -2.2351e+02 - logprior: -1.7784e+00
Epoch 4/10
16/16 - 3s - loss: 214.6150 - loglik: -2.1290e+02 - logprior: -1.7138e+00
Epoch 5/10
16/16 - 3s - loss: 212.0938 - loglik: -2.1050e+02 - logprior: -1.5954e+00
Epoch 6/10
16/16 - 3s - loss: 205.2867 - loglik: -2.0365e+02 - logprior: -1.6340e+00
Epoch 7/10
16/16 - 3s - loss: 204.6719 - loglik: -2.0306e+02 - logprior: -1.6071e+00
Epoch 8/10
16/16 - 3s - loss: 202.9768 - loglik: -2.0136e+02 - logprior: -1.6119e+00
Epoch 9/10
16/16 - 3s - loss: 202.4048 - loglik: -2.0080e+02 - logprior: -1.6025e+00
Epoch 10/10
16/16 - 3s - loss: 202.8722 - loglik: -2.0126e+02 - logprior: -1.6118e+00
Fitted a model with MAP estimate = -202.1653
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 1), (18, 1), (25, 14), (79, 1), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 7s - loss: 207.2958 - loglik: -2.0301e+02 - logprior: -4.2853e+00
Epoch 2/2
33/33 - 5s - loss: 196.8271 - loglik: -1.9433e+02 - logprior: -2.4965e+00
Fitted a model with MAP estimate = -191.9502
expansions: [(0, 1), (19, 1), (41, 1), (42, 1), (43, 1)]
discards: [ 0 23 31 32 33 34 35 36]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 200.3399 - loglik: -1.9727e+02 - logprior: -3.0706e+00
Epoch 2/2
33/33 - 5s - loss: 194.2846 - loglik: -1.9291e+02 - logprior: -1.3752e+00
Fitted a model with MAP estimate = -193.1064
expansions: [(0, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 132 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 8s - loss: 197.8221 - loglik: -1.9448e+02 - logprior: -3.3458e+00
Epoch 2/10
33/33 - 4s - loss: 193.7880 - loglik: -1.9240e+02 - logprior: -1.3832e+00
Epoch 3/10
33/33 - 4s - loss: 191.6532 - loglik: -1.9039e+02 - logprior: -1.2663e+00
Epoch 4/10
33/33 - 5s - loss: 190.8434 - loglik: -1.8964e+02 - logprior: -1.2001e+00
Epoch 5/10
33/33 - 5s - loss: 189.4617 - loglik: -1.8833e+02 - logprior: -1.1311e+00
Epoch 6/10
33/33 - 5s - loss: 189.5754 - loglik: -1.8850e+02 - logprior: -1.0775e+00
Fitted a model with MAP estimate = -187.6813
Time for alignment: 116.3582
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 326.6566 - loglik: -3.2216e+02 - logprior: -4.4959e+00
Epoch 2/10
16/16 - 4s - loss: 259.5262 - loglik: -2.5806e+02 - logprior: -1.4693e+00
Epoch 3/10
16/16 - 3s - loss: 219.8005 - loglik: -2.1792e+02 - logprior: -1.8844e+00
Epoch 4/10
16/16 - 4s - loss: 211.0495 - loglik: -2.0912e+02 - logprior: -1.9313e+00
Epoch 5/10
16/16 - 3s - loss: 204.6546 - loglik: -2.0289e+02 - logprior: -1.7639e+00
Epoch 6/10
16/16 - 3s - loss: 202.0069 - loglik: -2.0023e+02 - logprior: -1.7803e+00
Epoch 7/10
16/16 - 3s - loss: 203.2045 - loglik: -2.0142e+02 - logprior: -1.7823e+00
Fitted a model with MAP estimate = -201.3755
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 1), (18, 1), (86, 2), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 207.1982 - loglik: -2.0288e+02 - logprior: -4.3212e+00
Epoch 2/2
33/33 - 5s - loss: 199.6033 - loglik: -1.9725e+02 - logprior: -2.3508e+00
Fitted a model with MAP estimate = -196.6747
expansions: [(0, 1), (19, 1), (31, 13)]
discards: [ 0 23 92]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 198.4764 - loglik: -1.9528e+02 - logprior: -3.1970e+00
Epoch 2/2
33/33 - 5s - loss: 188.7480 - loglik: -1.8717e+02 - logprior: -1.5775e+00
Fitted a model with MAP estimate = -186.9027
expansions: [(0, 1), (32, 2), (34, 1), (35, 1), (37, 1), (38, 3)]
discards: [47 48 49 50 51 52]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 195.4064 - loglik: -1.9196e+02 - logprior: -3.4475e+00
Epoch 2/10
33/33 - 5s - loss: 190.5643 - loglik: -1.8921e+02 - logprior: -1.3579e+00
Epoch 3/10
33/33 - 4s - loss: 189.1093 - loglik: -1.8787e+02 - logprior: -1.2435e+00
Epoch 4/10
33/33 - 4s - loss: 185.6182 - loglik: -1.8442e+02 - logprior: -1.1939e+00
Epoch 5/10
33/33 - 5s - loss: 187.5479 - loglik: -1.8642e+02 - logprior: -1.1242e+00
Fitted a model with MAP estimate = -186.0898
Time for alignment: 100.6850
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 326.1376 - loglik: -3.2165e+02 - logprior: -4.4842e+00
Epoch 2/10
16/16 - 3s - loss: 257.5517 - loglik: -2.5609e+02 - logprior: -1.4665e+00
Epoch 3/10
16/16 - 4s - loss: 221.2766 - loglik: -2.1915e+02 - logprior: -2.1269e+00
Epoch 4/10
16/16 - 4s - loss: 208.0137 - loglik: -2.0585e+02 - logprior: -2.1641e+00
Epoch 5/10
16/16 - 3s - loss: 204.6961 - loglik: -2.0283e+02 - logprior: -1.8624e+00
Epoch 6/10
16/16 - 3s - loss: 199.8340 - loglik: -1.9799e+02 - logprior: -1.8434e+00
Epoch 7/10
16/16 - 4s - loss: 201.2636 - loglik: -1.9944e+02 - logprior: -1.8229e+00
Fitted a model with MAP estimate = -199.5176
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 1), (18, 1), (25, 14), (86, 1), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 202.7686 - loglik: -1.9840e+02 - logprior: -4.3647e+00
Epoch 2/2
33/33 - 5s - loss: 192.7239 - loglik: -1.9020e+02 - logprior: -2.5219e+00
Fitted a model with MAP estimate = -188.3979
expansions: [(0, 1), (19, 1), (40, 2), (41, 1)]
discards: [23 31 32 33 34 35]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 195.8020 - loglik: -1.9252e+02 - logprior: -3.2799e+00
Epoch 2/2
33/33 - 5s - loss: 187.9344 - loglik: -1.8628e+02 - logprior: -1.6537e+00
Fitted a model with MAP estimate = -187.4040
expansions: [(42, 1)]
discards: [37]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 8s - loss: 191.6055 - loglik: -1.8853e+02 - logprior: -3.0754e+00
Epoch 2/10
33/33 - 5s - loss: 187.3113 - loglik: -1.8593e+02 - logprior: -1.3851e+00
Epoch 3/10
33/33 - 5s - loss: 185.1692 - loglik: -1.8386e+02 - logprior: -1.3083e+00
Epoch 4/10
33/33 - 5s - loss: 185.1230 - loglik: -1.8388e+02 - logprior: -1.2458e+00
Epoch 5/10
33/33 - 5s - loss: 184.2244 - loglik: -1.8304e+02 - logprior: -1.1804e+00
Epoch 6/10
33/33 - 5s - loss: 183.7068 - loglik: -1.8259e+02 - logprior: -1.1139e+00
Epoch 7/10
33/33 - 4s - loss: 183.2091 - loglik: -1.8216e+02 - logprior: -1.0500e+00
Epoch 8/10
33/33 - 5s - loss: 183.0181 - loglik: -1.8203e+02 - logprior: -9.9228e-01
Epoch 9/10
33/33 - 5s - loss: 183.0683 - loglik: -1.8215e+02 - logprior: -9.1565e-01
Fitted a model with MAP estimate = -182.7423
Time for alignment: 120.5531
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 326.4471 - loglik: -3.2197e+02 - logprior: -4.4774e+00
Epoch 2/10
16/16 - 3s - loss: 252.9921 - loglik: -2.5153e+02 - logprior: -1.4632e+00
Epoch 3/10
16/16 - 3s - loss: 220.3779 - loglik: -2.1840e+02 - logprior: -1.9809e+00
Epoch 4/10
16/16 - 3s - loss: 208.5655 - loglik: -2.0667e+02 - logprior: -1.8976e+00
Epoch 5/10
16/16 - 3s - loss: 205.9912 - loglik: -2.0425e+02 - logprior: -1.7455e+00
Epoch 6/10
16/16 - 3s - loss: 201.0072 - loglik: -1.9923e+02 - logprior: -1.7794e+00
Epoch 7/10
16/16 - 3s - loss: 201.9662 - loglik: -2.0021e+02 - logprior: -1.7530e+00
Fitted a model with MAP estimate = -200.1068
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 1), (18, 1), (37, 10), (75, 1), (97, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 203.8788 - loglik: -1.9962e+02 - logprior: -4.2600e+00
Epoch 2/2
33/33 - 5s - loss: 192.7980 - loglik: -1.9046e+02 - logprior: -2.3376e+00
Fitted a model with MAP estimate = -191.0881
expansions: [(0, 1), (19, 1), (46, 3), (47, 2)]
discards: [23]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 194.2466 - loglik: -1.9100e+02 - logprior: -3.2456e+00
Epoch 2/2
33/33 - 5s - loss: 189.5570 - loglik: -1.8790e+02 - logprior: -1.6620e+00
Fitted a model with MAP estimate = -187.9275
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 136 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 191.8671 - loglik: -1.8867e+02 - logprior: -3.1971e+00
Epoch 2/10
33/33 - 5s - loss: 189.2344 - loglik: -1.8773e+02 - logprior: -1.5022e+00
Epoch 3/10
33/33 - 5s - loss: 187.5178 - loglik: -1.8610e+02 - logprior: -1.4163e+00
Epoch 4/10
33/33 - 5s - loss: 185.4241 - loglik: -1.8406e+02 - logprior: -1.3628e+00
Epoch 5/10
33/33 - 5s - loss: 186.4413 - loglik: -1.8516e+02 - logprior: -1.2804e+00
Fitted a model with MAP estimate = -185.1662
Time for alignment: 100.2053
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 326.9220 - loglik: -3.2245e+02 - logprior: -4.4721e+00
Epoch 2/10
16/16 - 3s - loss: 255.9565 - loglik: -2.5451e+02 - logprior: -1.4417e+00
Epoch 3/10
16/16 - 3s - loss: 220.8002 - loglik: -2.1909e+02 - logprior: -1.7118e+00
Epoch 4/10
16/16 - 3s - loss: 210.2121 - loglik: -2.0856e+02 - logprior: -1.6489e+00
Epoch 5/10
16/16 - 4s - loss: 204.1895 - loglik: -2.0251e+02 - logprior: -1.6820e+00
Epoch 6/10
16/16 - 4s - loss: 202.3562 - loglik: -2.0074e+02 - logprior: -1.6114e+00
Epoch 7/10
16/16 - 3s - loss: 200.4590 - loglik: -1.9884e+02 - logprior: -1.6203e+00
Epoch 8/10
16/16 - 4s - loss: 201.1067 - loglik: -1.9948e+02 - logprior: -1.6310e+00
Fitted a model with MAP estimate = -200.0551
expansions: [(0, 1), (10, 1), (11, 1), (13, 1), (15, 1), (16, 2), (17, 1), (86, 2), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 204.8735 - loglik: -2.0092e+02 - logprior: -3.9499e+00
Epoch 2/2
33/33 - 5s - loss: 197.0805 - loglik: -1.9548e+02 - logprior: -1.5974e+00
Fitted a model with MAP estimate = -196.0213
expansions: []
discards: [24 94]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 201.1828 - loglik: -1.9791e+02 - logprior: -3.2751e+00
Epoch 2/2
33/33 - 5s - loss: 196.8488 - loglik: -1.9535e+02 - logprior: -1.4953e+00
Fitted a model with MAP estimate = -196.4554
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 8s - loss: 199.9223 - loglik: -1.9674e+02 - logprior: -3.1820e+00
Epoch 2/10
33/33 - 4s - loss: 197.8871 - loglik: -1.9650e+02 - logprior: -1.3854e+00
Epoch 3/10
33/33 - 5s - loss: 194.8387 - loglik: -1.9355e+02 - logprior: -1.2902e+00
Epoch 4/10
33/33 - 5s - loss: 194.7122 - loglik: -1.9351e+02 - logprior: -1.2040e+00
Epoch 5/10
33/33 - 5s - loss: 195.8681 - loglik: -1.9474e+02 - logprior: -1.1280e+00
Fitted a model with MAP estimate = -193.5937
Time for alignment: 104.3296
Computed alignments with likelihoods: ['-187.6813', '-186.0898', '-182.7423', '-185.1662', '-193.5937']
Best model has likelihood: -182.7423  (prior= -0.8739 )
time for generating output: 0.2795
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.5181013676588898
Training of 5 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead10263a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5a925700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d81eeb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 309.1389 - loglik: -3.0602e+02 - logprior: -3.1176e+00
Epoch 2/10
19/19 - 2s - loss: 278.4462 - loglik: -2.7734e+02 - logprior: -1.1083e+00
Epoch 3/10
19/19 - 2s - loss: 263.3601 - loglik: -2.6194e+02 - logprior: -1.4220e+00
Epoch 4/10
19/19 - 2s - loss: 259.4686 - loglik: -2.5806e+02 - logprior: -1.4059e+00
Epoch 5/10
19/19 - 2s - loss: 257.4654 - loglik: -2.5609e+02 - logprior: -1.3751e+00
Epoch 6/10
19/19 - 2s - loss: 257.6015 - loglik: -2.5625e+02 - logprior: -1.3507e+00
Fitted a model with MAP estimate = -241.9674
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 9), (38, 2), (43, 2), (58, 1), (59, 1), (60, 1), (63, 2), (66, 3), (67, 1), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 261.5888 - loglik: -2.5766e+02 - logprior: -3.9249e+00
Epoch 2/2
19/19 - 2s - loss: 251.7950 - loglik: -2.4976e+02 - logprior: -2.0370e+00
Fitted a model with MAP estimate = -236.2151
expansions: [(0, 2), (46, 2)]
discards: [ 0  9 27 54 87 92]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 252.3676 - loglik: -2.4946e+02 - logprior: -2.9099e+00
Epoch 2/2
19/19 - 2s - loss: 248.5606 - loglik: -2.4744e+02 - logprior: -1.1225e+00
Fitted a model with MAP estimate = -234.1809
expansions: []
discards: [ 0 46 47 48]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 237.5451 - loglik: -2.3500e+02 - logprior: -2.5427e+00
Epoch 2/10
23/23 - 3s - loss: 233.4883 - loglik: -2.3246e+02 - logprior: -1.0287e+00
Epoch 3/10
23/23 - 3s - loss: 232.7744 - loglik: -2.3180e+02 - logprior: -9.7467e-01
Epoch 4/10
23/23 - 3s - loss: 231.3741 - loglik: -2.3042e+02 - logprior: -9.5517e-01
Epoch 5/10
23/23 - 3s - loss: 231.7390 - loglik: -2.3081e+02 - logprior: -9.2918e-01
Fitted a model with MAP estimate = -231.1913
Time for alignment: 68.0489
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 309.0515 - loglik: -3.0593e+02 - logprior: -3.1189e+00
Epoch 2/10
19/19 - 2s - loss: 277.3007 - loglik: -2.7616e+02 - logprior: -1.1365e+00
Epoch 3/10
19/19 - 2s - loss: 262.0419 - loglik: -2.6060e+02 - logprior: -1.4434e+00
Epoch 4/10
19/19 - 2s - loss: 258.1257 - loglik: -2.5672e+02 - logprior: -1.4078e+00
Epoch 5/10
19/19 - 2s - loss: 257.0516 - loglik: -2.5570e+02 - logprior: -1.3547e+00
Epoch 6/10
19/19 - 2s - loss: 256.8351 - loglik: -2.5551e+02 - logprior: -1.3260e+00
Epoch 7/10
19/19 - 2s - loss: 256.5236 - loglik: -2.5521e+02 - logprior: -1.3160e+00
Epoch 8/10
19/19 - 2s - loss: 256.2899 - loglik: -2.5499e+02 - logprior: -1.3023e+00
Epoch 9/10
19/19 - 2s - loss: 256.4375 - loglik: -2.5515e+02 - logprior: -1.2893e+00
Fitted a model with MAP estimate = -241.1102
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 9), (38, 2), (39, 2), (42, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 261.9178 - loglik: -2.5802e+02 - logprior: -3.9021e+00
Epoch 2/2
19/19 - 2s - loss: 252.5409 - loglik: -2.5049e+02 - logprior: -2.0551e+00
Fitted a model with MAP estimate = -236.6680
expansions: [(0, 2)]
discards: [ 0  9 27 55 57 80 88]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 252.5210 - loglik: -2.4962e+02 - logprior: -2.9054e+00
Epoch 2/2
19/19 - 2s - loss: 249.3049 - loglik: -2.4817e+02 - logprior: -1.1388e+00
Fitted a model with MAP estimate = -234.7311
expansions: [(45, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 237.0208 - loglik: -2.3446e+02 - logprior: -2.5646e+00
Epoch 2/10
23/23 - 3s - loss: 233.5004 - loglik: -2.3247e+02 - logprior: -1.0347e+00
Epoch 3/10
23/23 - 3s - loss: 231.7720 - loglik: -2.3080e+02 - logprior: -9.7001e-01
Epoch 4/10
23/23 - 3s - loss: 231.7141 - loglik: -2.3079e+02 - logprior: -9.2266e-01
Epoch 5/10
23/23 - 3s - loss: 230.3659 - loglik: -2.2945e+02 - logprior: -9.1515e-01
Epoch 6/10
23/23 - 3s - loss: 231.0715 - loglik: -2.3017e+02 - logprior: -8.9906e-01
Fitted a model with MAP estimate = -230.4953
Time for alignment: 75.8655
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 308.8486 - loglik: -3.0573e+02 - logprior: -3.1161e+00
Epoch 2/10
19/19 - 2s - loss: 276.1840 - loglik: -2.7505e+02 - logprior: -1.1341e+00
Epoch 3/10
19/19 - 2s - loss: 260.8550 - loglik: -2.5939e+02 - logprior: -1.4603e+00
Epoch 4/10
19/19 - 2s - loss: 257.8783 - loglik: -2.5647e+02 - logprior: -1.4056e+00
Epoch 5/10
19/19 - 2s - loss: 256.7042 - loglik: -2.5535e+02 - logprior: -1.3514e+00
Epoch 6/10
19/19 - 2s - loss: 257.1397 - loglik: -2.5581e+02 - logprior: -1.3280e+00
Fitted a model with MAP estimate = -241.2073
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 9), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 261.5858 - loglik: -2.5767e+02 - logprior: -3.9174e+00
Epoch 2/2
19/19 - 2s - loss: 251.6267 - loglik: -2.4958e+02 - logprior: -2.0420e+00
Fitted a model with MAP estimate = -236.1359
expansions: [(0, 2)]
discards: [ 0  9 27 48 54 80 88]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 252.8743 - loglik: -2.4995e+02 - logprior: -2.9219e+00
Epoch 2/2
19/19 - 2s - loss: 249.4909 - loglik: -2.4834e+02 - logprior: -1.1547e+00
Fitted a model with MAP estimate = -234.9591
expansions: [(45, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 237.0569 - loglik: -2.3446e+02 - logprior: -2.5980e+00
Epoch 2/10
23/23 - 3s - loss: 233.0595 - loglik: -2.3200e+02 - logprior: -1.0577e+00
Epoch 3/10
23/23 - 3s - loss: 232.7390 - loglik: -2.3175e+02 - logprior: -9.8468e-01
Epoch 4/10
23/23 - 3s - loss: 231.3384 - loglik: -2.3038e+02 - logprior: -9.5559e-01
Epoch 5/10
23/23 - 3s - loss: 231.5402 - loglik: -2.3060e+02 - logprior: -9.3921e-01
Fitted a model with MAP estimate = -230.8415
Time for alignment: 67.9920
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 308.8751 - loglik: -3.0576e+02 - logprior: -3.1196e+00
Epoch 2/10
19/19 - 2s - loss: 278.3741 - loglik: -2.7724e+02 - logprior: -1.1310e+00
Epoch 3/10
19/19 - 2s - loss: 263.8751 - loglik: -2.6240e+02 - logprior: -1.4719e+00
Epoch 4/10
19/19 - 2s - loss: 259.3450 - loglik: -2.5790e+02 - logprior: -1.4451e+00
Epoch 5/10
19/19 - 2s - loss: 257.7219 - loglik: -2.5632e+02 - logprior: -1.3996e+00
Epoch 6/10
19/19 - 2s - loss: 257.2148 - loglik: -2.5583e+02 - logprior: -1.3845e+00
Epoch 7/10
19/19 - 2s - loss: 257.3308 - loglik: -2.5596e+02 - logprior: -1.3659e+00
Fitted a model with MAP estimate = -241.8294
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 3), (39, 2), (40, 1), (41, 1), (42, 4), (55, 1), (58, 2), (60, 1), (62, 1), (63, 2), (66, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 261.9573 - loglik: -2.5803e+02 - logprior: -3.9262e+00
Epoch 2/2
19/19 - 2s - loss: 253.0568 - loglik: -2.5102e+02 - logprior: -2.0330e+00
Fitted a model with MAP estimate = -237.0759
expansions: [(0, 2), (58, 1), (61, 2)]
discards: [ 0  9 27 50 77 85]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 252.5579 - loglik: -2.4965e+02 - logprior: -2.9035e+00
Epoch 2/2
19/19 - 2s - loss: 248.9405 - loglik: -2.4782e+02 - logprior: -1.1232e+00
Fitted a model with MAP estimate = -234.9920
expansions: [(42, 2)]
discards: [ 0 47 48]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 237.4387 - loglik: -2.3488e+02 - logprior: -2.5580e+00
Epoch 2/10
23/23 - 3s - loss: 233.9883 - loglik: -2.3297e+02 - logprior: -1.0197e+00
Epoch 3/10
23/23 - 3s - loss: 232.3923 - loglik: -2.3144e+02 - logprior: -9.5065e-01
Epoch 4/10
23/23 - 3s - loss: 231.1411 - loglik: -2.3022e+02 - logprior: -9.2203e-01
Epoch 5/10
23/23 - 3s - loss: 231.6009 - loglik: -2.3069e+02 - logprior: -9.1310e-01
Fitted a model with MAP estimate = -231.0233
Time for alignment: 69.8542
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.9828 - loglik: -3.0587e+02 - logprior: -3.1171e+00
Epoch 2/10
19/19 - 2s - loss: 277.1938 - loglik: -2.7606e+02 - logprior: -1.1311e+00
Epoch 3/10
19/19 - 2s - loss: 262.2628 - loglik: -2.6081e+02 - logprior: -1.4488e+00
Epoch 4/10
19/19 - 2s - loss: 258.5359 - loglik: -2.5713e+02 - logprior: -1.4081e+00
Epoch 5/10
19/19 - 2s - loss: 256.8743 - loglik: -2.5551e+02 - logprior: -1.3606e+00
Epoch 6/10
19/19 - 2s - loss: 257.1521 - loglik: -2.5581e+02 - logprior: -1.3394e+00
Fitted a model with MAP estimate = -241.4004
expansions: [(6, 3), (7, 2), (10, 1), (21, 2), (33, 9), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 261.5935 - loglik: -2.5768e+02 - logprior: -3.9173e+00
Epoch 2/2
19/19 - 2s - loss: 251.8255 - loglik: -2.4979e+02 - logprior: -2.0386e+00
Fitted a model with MAP estimate = -236.0333
expansions: [(0, 2), (46, 2)]
discards: [ 0  9 27 54 79 87]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 252.2907 - loglik: -2.4938e+02 - logprior: -2.9155e+00
Epoch 2/2
19/19 - 2s - loss: 248.5450 - loglik: -2.4742e+02 - logprior: -1.1201e+00
Fitted a model with MAP estimate = -234.5483
expansions: []
discards: [ 0 46 47 48 49]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 237.4236 - loglik: -2.3488e+02 - logprior: -2.5417e+00
Epoch 2/10
23/23 - 3s - loss: 233.8826 - loglik: -2.3286e+02 - logprior: -1.0274e+00
Epoch 3/10
23/23 - 3s - loss: 233.1771 - loglik: -2.3219e+02 - logprior: -9.8370e-01
Epoch 4/10
23/23 - 3s - loss: 231.8863 - loglik: -2.3095e+02 - logprior: -9.3840e-01
Epoch 5/10
23/23 - 3s - loss: 231.4833 - loglik: -2.3056e+02 - logprior: -9.2744e-01
Epoch 6/10
23/23 - 3s - loss: 231.7506 - loglik: -2.3084e+02 - logprior: -9.0984e-01
Fitted a model with MAP estimate = -231.2639
Time for alignment: 69.7691
Computed alignments with likelihoods: ['-231.1913', '-230.4953', '-230.8415', '-231.0233', '-231.2639']
Best model has likelihood: -230.4953  (prior= -0.9103 )
time for generating output: 0.1618
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.8351409978308026
Training of 5 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe98af52640>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb0e6294c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb1f38d5e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.7298 - loglik: -3.8723e+02 - logprior: -4.5496e+01
Epoch 2/10
10/10 - 2s - loss: 370.7486 - loglik: -3.6042e+02 - logprior: -1.0327e+01
Epoch 3/10
10/10 - 2s - loss: 334.9588 - loglik: -3.3081e+02 - logprior: -4.1468e+00
Epoch 4/10
10/10 - 2s - loss: 314.8457 - loglik: -3.1293e+02 - logprior: -1.9199e+00
Epoch 5/10
10/10 - 2s - loss: 304.0074 - loglik: -3.0306e+02 - logprior: -9.5232e-01
Epoch 6/10
10/10 - 2s - loss: 299.3916 - loglik: -2.9867e+02 - logprior: -7.2557e-01
Epoch 7/10
10/10 - 2s - loss: 296.8712 - loglik: -2.9645e+02 - logprior: -4.2137e-01
Epoch 8/10
10/10 - 2s - loss: 295.6900 - loglik: -2.9555e+02 - logprior: -1.4069e-01
Epoch 9/10
10/10 - 2s - loss: 295.4471 - loglik: -2.9546e+02 - logprior: 0.0120
Epoch 10/10
10/10 - 2s - loss: 294.9864 - loglik: -2.9509e+02 - logprior: 0.1019
Fitted a model with MAP estimate = -294.8442
expansions: [(12, 1), (17, 1), (20, 1), (27, 1), (28, 4), (48, 1), (58, 2), (59, 1), (62, 2), (64, 1), (70, 1), (72, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 346.4079 - loglik: -2.9561e+02 - logprior: -5.0797e+01
Epoch 2/2
10/10 - 2s - loss: 306.6650 - loglik: -2.8746e+02 - logprior: -1.9207e+01
Fitted a model with MAP estimate = -300.0932
expansions: []
discards: [  0  67  73 107]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 336.8174 - loglik: -2.8712e+02 - logprior: -4.9694e+01
Epoch 2/2
10/10 - 2s - loss: 301.8708 - loglik: -2.8556e+02 - logprior: -1.6314e+01
Fitted a model with MAP estimate = -293.6775
expansions: [(0, 6), (5, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 322.0961 - loglik: -2.8295e+02 - logprior: -3.9145e+01
Epoch 2/10
10/10 - 2s - loss: 287.6556 - loglik: -2.8002e+02 - logprior: -7.6345e+00
Epoch 3/10
10/10 - 2s - loss: 280.0103 - loglik: -2.7879e+02 - logprior: -1.2177e+00
Epoch 4/10
10/10 - 2s - loss: 276.1095 - loglik: -2.7746e+02 - logprior: 1.3509
Epoch 5/10
10/10 - 2s - loss: 274.7935 - loglik: -2.7752e+02 - logprior: 2.7298
Epoch 6/10
10/10 - 2s - loss: 273.7038 - loglik: -2.7726e+02 - logprior: 3.5564
Epoch 7/10
10/10 - 2s - loss: 273.4558 - loglik: -2.7751e+02 - logprior: 4.0580
Epoch 8/10
10/10 - 2s - loss: 273.0910 - loglik: -2.7748e+02 - logprior: 4.3868
Epoch 9/10
10/10 - 2s - loss: 272.6492 - loglik: -2.7729e+02 - logprior: 4.6457
Epoch 10/10
10/10 - 2s - loss: 272.7347 - loglik: -2.7762e+02 - logprior: 4.8886
Fitted a model with MAP estimate = -272.3929
Time for alignment: 65.4508
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 432.8002 - loglik: -3.8731e+02 - logprior: -4.5495e+01
Epoch 2/10
10/10 - 2s - loss: 370.7115 - loglik: -3.6038e+02 - logprior: -1.0329e+01
Epoch 3/10
10/10 - 2s - loss: 336.7358 - loglik: -3.3259e+02 - logprior: -4.1454e+00
Epoch 4/10
10/10 - 2s - loss: 313.8463 - loglik: -3.1186e+02 - logprior: -1.9904e+00
Epoch 5/10
10/10 - 2s - loss: 303.5480 - loglik: -3.0249e+02 - logprior: -1.0556e+00
Epoch 6/10
10/10 - 2s - loss: 298.8565 - loglik: -2.9820e+02 - logprior: -6.5478e-01
Epoch 7/10
10/10 - 2s - loss: 296.9759 - loglik: -2.9670e+02 - logprior: -2.7925e-01
Epoch 8/10
10/10 - 2s - loss: 296.8348 - loglik: -2.9684e+02 - logprior: 0.0045
Epoch 9/10
10/10 - 2s - loss: 296.0468 - loglik: -2.9619e+02 - logprior: 0.1441
Epoch 10/10
10/10 - 2s - loss: 295.4177 - loglik: -2.9563e+02 - logprior: 0.2124
Fitted a model with MAP estimate = -295.4633
expansions: [(11, 3), (12, 1), (17, 1), (18, 1), (29, 3), (41, 1), (48, 1), (58, 2), (59, 1), (62, 2), (64, 1), (68, 1), (72, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 346.9642 - loglik: -2.9625e+02 - logprior: -5.0709e+01
Epoch 2/2
10/10 - 2s - loss: 305.0851 - loglik: -2.8578e+02 - logprior: -1.9301e+01
Fitted a model with MAP estimate = -298.0681
expansions: [(10, 1), (13, 1)]
discards: [ 69  75 109]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 331.1538 - loglik: -2.8262e+02 - logprior: -4.8535e+01
Epoch 2/2
10/10 - 2s - loss: 293.3256 - loglik: -2.8016e+02 - logprior: -1.3161e+01
Fitted a model with MAP estimate = -284.8591
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 319.5792 - loglik: -2.7958e+02 - logprior: -3.9996e+01
Epoch 2/10
10/10 - 2s - loss: 286.7610 - loglik: -2.7892e+02 - logprior: -7.8437e+00
Epoch 3/10
10/10 - 2s - loss: 280.1257 - loglik: -2.7882e+02 - logprior: -1.3078e+00
Epoch 4/10
10/10 - 2s - loss: 276.4355 - loglik: -2.7764e+02 - logprior: 1.2036
Epoch 5/10
10/10 - 2s - loss: 275.0994 - loglik: -2.7750e+02 - logprior: 2.3997
Epoch 6/10
10/10 - 2s - loss: 273.9042 - loglik: -2.7712e+02 - logprior: 3.2120
Epoch 7/10
10/10 - 2s - loss: 273.4691 - loglik: -2.7737e+02 - logprior: 3.8988
Epoch 8/10
10/10 - 2s - loss: 273.2975 - loglik: -2.7765e+02 - logprior: 4.3557
Epoch 9/10
10/10 - 2s - loss: 272.2950 - loglik: -2.7694e+02 - logprior: 4.6483
Epoch 10/10
10/10 - 2s - loss: 273.1184 - loglik: -2.7803e+02 - logprior: 4.9074
Fitted a model with MAP estimate = -272.4507
Time for alignment: 65.0769
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 433.1545 - loglik: -3.8766e+02 - logprior: -4.5495e+01
Epoch 2/10
10/10 - 2s - loss: 370.1389 - loglik: -3.5981e+02 - logprior: -1.0329e+01
Epoch 3/10
10/10 - 2s - loss: 334.0995 - loglik: -3.2997e+02 - logprior: -4.1314e+00
Epoch 4/10
10/10 - 2s - loss: 313.4494 - loglik: -3.1156e+02 - logprior: -1.8904e+00
Epoch 5/10
10/10 - 2s - loss: 303.8735 - loglik: -3.0305e+02 - logprior: -8.2137e-01
Epoch 6/10
10/10 - 2s - loss: 299.6174 - loglik: -2.9927e+02 - logprior: -3.4396e-01
Epoch 7/10
10/10 - 2s - loss: 298.2070 - loglik: -2.9819e+02 - logprior: -1.6193e-02
Epoch 8/10
10/10 - 2s - loss: 297.3492 - loglik: -2.9751e+02 - logprior: 0.1581
Epoch 9/10
10/10 - 2s - loss: 297.2642 - loglik: -2.9755e+02 - logprior: 0.2883
Epoch 10/10
10/10 - 2s - loss: 296.1020 - loglik: -2.9646e+02 - logprior: 0.3607
Fitted a model with MAP estimate = -296.1292
expansions: [(7, 2), (9, 1), (17, 1), (18, 1), (26, 1), (28, 4), (42, 1), (58, 2), (59, 1), (62, 3), (73, 1), (74, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 346.0321 - loglik: -2.9528e+02 - logprior: -5.0756e+01
Epoch 2/2
10/10 - 2s - loss: 305.6638 - loglik: -2.8650e+02 - logprior: -1.9165e+01
Fitted a model with MAP estimate = -298.8250
expansions: [(9, 3)]
discards: [  0  69  76 109]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 334.4716 - loglik: -2.8491e+02 - logprior: -4.9566e+01
Epoch 2/2
10/10 - 2s - loss: 298.7573 - loglik: -2.8193e+02 - logprior: -1.6827e+01
Fitted a model with MAP estimate = -291.2296
expansions: [(0, 3)]
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 319.4303 - loglik: -2.8061e+02 - logprior: -3.8825e+01
Epoch 2/10
10/10 - 2s - loss: 286.2235 - loglik: -2.7870e+02 - logprior: -7.5214e+00
Epoch 3/10
10/10 - 2s - loss: 278.6652 - loglik: -2.7755e+02 - logprior: -1.1111e+00
Epoch 4/10
10/10 - 2s - loss: 275.9655 - loglik: -2.7745e+02 - logprior: 1.4810
Epoch 5/10
10/10 - 2s - loss: 274.0246 - loglik: -2.7692e+02 - logprior: 2.8950
Epoch 6/10
10/10 - 2s - loss: 273.8227 - loglik: -2.7752e+02 - logprior: 3.6973
Epoch 7/10
10/10 - 2s - loss: 272.8396 - loglik: -2.7702e+02 - logprior: 4.1803
Epoch 8/10
10/10 - 2s - loss: 273.0204 - loglik: -2.7753e+02 - logprior: 4.5062
Fitted a model with MAP estimate = -272.5084
Time for alignment: 61.3579
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 433.1719 - loglik: -3.8768e+02 - logprior: -4.5496e+01
Epoch 2/10
10/10 - 2s - loss: 370.2525 - loglik: -3.5992e+02 - logprior: -1.0331e+01
Epoch 3/10
10/10 - 2s - loss: 336.4100 - loglik: -3.3224e+02 - logprior: -4.1732e+00
Epoch 4/10
10/10 - 2s - loss: 315.2020 - loglik: -3.1328e+02 - logprior: -1.9191e+00
Epoch 5/10
10/10 - 2s - loss: 305.2853 - loglik: -3.0442e+02 - logprior: -8.6606e-01
Epoch 6/10
10/10 - 2s - loss: 299.2879 - loglik: -2.9868e+02 - logprior: -6.0654e-01
Epoch 7/10
10/10 - 2s - loss: 297.4589 - loglik: -2.9706e+02 - logprior: -3.9417e-01
Epoch 8/10
10/10 - 2s - loss: 297.0668 - loglik: -2.9699e+02 - logprior: -7.3514e-02
Epoch 9/10
10/10 - 2s - loss: 295.7011 - loglik: -2.9577e+02 - logprior: 0.0669
Epoch 10/10
10/10 - 2s - loss: 296.2537 - loglik: -2.9638e+02 - logprior: 0.1264
Fitted a model with MAP estimate = -295.7124
expansions: [(11, 4), (16, 1), (26, 1), (42, 1), (58, 2), (59, 1), (62, 2), (64, 1), (68, 1), (72, 1), (84, 1), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 347.8094 - loglik: -2.9688e+02 - logprior: -5.0925e+01
Epoch 2/2
10/10 - 2s - loss: 306.9211 - loglik: -2.8758e+02 - logprior: -1.9337e+01
Fitted a model with MAP estimate = -300.5088
expansions: [(10, 1), (12, 1), (33, 2)]
discards: [ 0 65 71]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 337.0014 - loglik: -2.8704e+02 - logprior: -4.9964e+01
Epoch 2/2
10/10 - 2s - loss: 302.1015 - loglik: -2.8449e+02 - logprior: -1.7614e+01
Fitted a model with MAP estimate = -294.4617
expansions: [(0, 3), (12, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 321.1214 - loglik: -2.8211e+02 - logprior: -3.9009e+01
Epoch 2/10
10/10 - 2s - loss: 286.4091 - loglik: -2.7888e+02 - logprior: -7.5316e+00
Epoch 3/10
10/10 - 2s - loss: 278.9481 - loglik: -2.7785e+02 - logprior: -1.0979e+00
Epoch 4/10
10/10 - 2s - loss: 276.5646 - loglik: -2.7807e+02 - logprior: 1.5024
Epoch 5/10
10/10 - 2s - loss: 274.8492 - loglik: -2.7776e+02 - logprior: 2.9066
Epoch 6/10
10/10 - 2s - loss: 274.2083 - loglik: -2.7792e+02 - logprior: 3.7105
Epoch 7/10
10/10 - 2s - loss: 273.5937 - loglik: -2.7778e+02 - logprior: 4.1910
Epoch 8/10
10/10 - 2s - loss: 273.1064 - loglik: -2.7763e+02 - logprior: 4.5230
Epoch 9/10
10/10 - 2s - loss: 272.6793 - loglik: -2.7747e+02 - logprior: 4.7902
Epoch 10/10
10/10 - 2s - loss: 273.1160 - loglik: -2.7814e+02 - logprior: 5.0266
Fitted a model with MAP estimate = -272.5667
Time for alignment: 64.6743
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 432.8402 - loglik: -3.8734e+02 - logprior: -4.5496e+01
Epoch 2/10
10/10 - 2s - loss: 370.8621 - loglik: -3.6053e+02 - logprior: -1.0328e+01
Epoch 3/10
10/10 - 2s - loss: 336.3588 - loglik: -3.3220e+02 - logprior: -4.1598e+00
Epoch 4/10
10/10 - 2s - loss: 315.4301 - loglik: -3.1357e+02 - logprior: -1.8624e+00
Epoch 5/10
10/10 - 2s - loss: 305.5364 - loglik: -3.0478e+02 - logprior: -7.5661e-01
Epoch 6/10
10/10 - 2s - loss: 301.0560 - loglik: -3.0068e+02 - logprior: -3.7836e-01
Epoch 7/10
10/10 - 2s - loss: 298.5730 - loglik: -2.9845e+02 - logprior: -1.2221e-01
Epoch 8/10
10/10 - 2s - loss: 297.6710 - loglik: -2.9775e+02 - logprior: 0.0808
Epoch 9/10
10/10 - 2s - loss: 296.7840 - loglik: -2.9700e+02 - logprior: 0.2142
Epoch 10/10
10/10 - 2s - loss: 297.1566 - loglik: -2.9744e+02 - logprior: 0.2879
Fitted a model with MAP estimate = -296.6082
expansions: [(7, 2), (9, 1), (17, 1), (18, 2), (19, 1), (28, 2), (42, 1), (58, 2), (59, 1), (62, 3), (73, 1), (74, 1), (84, 1), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 346.7236 - loglik: -2.9587e+02 - logprior: -5.0856e+01
Epoch 2/2
10/10 - 2s - loss: 306.1314 - loglik: -2.8689e+02 - logprior: -1.9238e+01
Fitted a model with MAP estimate = -299.2959
expansions: [(9, 3)]
discards: [ 0 21 68 75]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 334.8079 - loglik: -2.8516e+02 - logprior: -4.9647e+01
Epoch 2/2
10/10 - 2s - loss: 299.3807 - loglik: -2.8229e+02 - logprior: -1.7095e+01
Fitted a model with MAP estimate = -292.1049
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 319.9880 - loglik: -2.8114e+02 - logprior: -3.8850e+01
Epoch 2/10
10/10 - 2s - loss: 285.8375 - loglik: -2.7834e+02 - logprior: -7.5018e+00
Epoch 3/10
10/10 - 2s - loss: 279.0638 - loglik: -2.7798e+02 - logprior: -1.0809e+00
Epoch 4/10
10/10 - 2s - loss: 276.4088 - loglik: -2.7792e+02 - logprior: 1.5148
Epoch 5/10
10/10 - 2s - loss: 274.8102 - loglik: -2.7773e+02 - logprior: 2.9151
Epoch 6/10
10/10 - 2s - loss: 273.8754 - loglik: -2.7760e+02 - logprior: 3.7209
Epoch 7/10
10/10 - 2s - loss: 273.2395 - loglik: -2.7744e+02 - logprior: 4.2033
Epoch 8/10
10/10 - 2s - loss: 273.4725 - loglik: -2.7801e+02 - logprior: 4.5348
Fitted a model with MAP estimate = -272.9505
Time for alignment: 60.8684
Computed alignments with likelihoods: ['-272.3929', '-272.4507', '-272.5084', '-272.5667', '-272.9505']
Best model has likelihood: -272.3929  (prior= 5.0089 )
time for generating output: 0.1600
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.9373549883990719
Training of 5 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaebcd5a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f9afb3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe99409e160>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 452.9714 - loglik: -1.8605e+02 - logprior: -2.6692e+02
Epoch 2/10
10/10 - 1s - loss: 230.2417 - loglik: -1.6029e+02 - logprior: -6.9951e+01
Epoch 3/10
10/10 - 1s - loss: 171.1751 - loglik: -1.4045e+02 - logprior: -3.0729e+01
Epoch 4/10
10/10 - 1s - loss: 143.5510 - loglik: -1.2721e+02 - logprior: -1.6346e+01
Epoch 5/10
10/10 - 1s - loss: 129.3254 - loglik: -1.2099e+02 - logprior: -8.3320e+00
Epoch 6/10
10/10 - 1s - loss: 121.2706 - loglik: -1.1821e+02 - logprior: -3.0593e+00
Epoch 7/10
10/10 - 1s - loss: 116.6140 - loglik: -1.1684e+02 - logprior: 0.2214
Epoch 8/10
10/10 - 1s - loss: 113.8904 - loglik: -1.1610e+02 - logprior: 2.2066
Epoch 9/10
10/10 - 1s - loss: 111.9946 - loglik: -1.1564e+02 - logprior: 3.6453
Epoch 10/10
10/10 - 1s - loss: 110.6204 - loglik: -1.1533e+02 - logprior: 4.7114
Fitted a model with MAP estimate = -109.9948
expansions: [(0, 5), (10, 1), (17, 2), (24, 1), (25, 1), (27, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 457.3251 - loglik: -1.0884e+02 - logprior: -3.4848e+02
Epoch 2/2
10/10 - 1s - loss: 202.8927 - loglik: -9.9577e+01 - logprior: -1.0332e+02
Fitted a model with MAP estimate = -155.6956
expansions: [(23, 1)]
discards: [ 0  1  2 51]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 396.9778 - loglik: -9.7221e+01 - logprior: -2.9976e+02
Epoch 2/2
10/10 - 1s - loss: 209.5622 - loglik: -9.5583e+01 - logprior: -1.1398e+02
Fitted a model with MAP estimate = -179.4434
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.6707 - loglik: -9.4632e+01 - logprior: -2.7404e+02
Epoch 2/10
10/10 - 1s - loss: 166.2587 - loglik: -9.3905e+01 - logprior: -7.2354e+01
Epoch 3/10
10/10 - 1s - loss: 116.2521 - loglik: -9.4105e+01 - logprior: -2.2147e+01
Epoch 4/10
10/10 - 1s - loss: 98.1085 - loglik: -9.4341e+01 - logprior: -3.7677e+00
Epoch 5/10
10/10 - 1s - loss: 88.6048 - loglik: -9.4275e+01 - logprior: 5.6706
Epoch 6/10
10/10 - 1s - loss: 82.9577 - loglik: -9.4144e+01 - logprior: 11.1868
Epoch 7/10
10/10 - 1s - loss: 79.3363 - loglik: -9.4015e+01 - logprior: 14.6790
Epoch 8/10
10/10 - 1s - loss: 76.8470 - loglik: -9.3971e+01 - logprior: 17.1236
Epoch 9/10
10/10 - 1s - loss: 75.0071 - loglik: -9.4027e+01 - logprior: 19.0197
Epoch 10/10
10/10 - 1s - loss: 73.5015 - loglik: -9.4100e+01 - logprior: 20.5983
Fitted a model with MAP estimate = -72.7572
Time for alignment: 32.4590
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 452.9714 - loglik: -1.8605e+02 - logprior: -2.6692e+02
Epoch 2/10
10/10 - 1s - loss: 230.2417 - loglik: -1.6029e+02 - logprior: -6.9951e+01
Epoch 3/10
10/10 - 1s - loss: 171.1751 - loglik: -1.4045e+02 - logprior: -3.0729e+01
Epoch 4/10
10/10 - 1s - loss: 143.5510 - loglik: -1.2721e+02 - logprior: -1.6346e+01
Epoch 5/10
10/10 - 1s - loss: 129.3254 - loglik: -1.2099e+02 - logprior: -8.3320e+00
Epoch 6/10
10/10 - 1s - loss: 121.2706 - loglik: -1.1821e+02 - logprior: -3.0593e+00
Epoch 7/10
10/10 - 1s - loss: 116.6139 - loglik: -1.1684e+02 - logprior: 0.2214
Epoch 8/10
10/10 - 1s - loss: 113.8904 - loglik: -1.1610e+02 - logprior: 2.2066
Epoch 9/10
10/10 - 1s - loss: 111.9946 - loglik: -1.1564e+02 - logprior: 3.6453
Epoch 10/10
10/10 - 1s - loss: 110.6205 - loglik: -1.1533e+02 - logprior: 4.7114
Fitted a model with MAP estimate = -109.9948
expansions: [(0, 5), (10, 1), (17, 2), (24, 1), (25, 1), (27, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 457.3251 - loglik: -1.0884e+02 - logprior: -3.4848e+02
Epoch 2/2
10/10 - 1s - loss: 202.8927 - loglik: -9.9577e+01 - logprior: -1.0332e+02
Fitted a model with MAP estimate = -155.6956
expansions: [(23, 1)]
discards: [ 0  1  2 51]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 396.9778 - loglik: -9.7221e+01 - logprior: -2.9976e+02
Epoch 2/2
10/10 - 1s - loss: 209.5622 - loglik: -9.5583e+01 - logprior: -1.1398e+02
Fitted a model with MAP estimate = -179.4434
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 368.6707 - loglik: -9.4632e+01 - logprior: -2.7404e+02
Epoch 2/10
10/10 - 1s - loss: 166.2587 - loglik: -9.3905e+01 - logprior: -7.2354e+01
Epoch 3/10
10/10 - 1s - loss: 116.2521 - loglik: -9.4105e+01 - logprior: -2.2147e+01
Epoch 4/10
10/10 - 1s - loss: 98.1084 - loglik: -9.4341e+01 - logprior: -3.7676e+00
Epoch 5/10
10/10 - 1s - loss: 88.6048 - loglik: -9.4275e+01 - logprior: 5.6707
Epoch 6/10
10/10 - 1s - loss: 82.9577 - loglik: -9.4145e+01 - logprior: 11.1869
Epoch 7/10
10/10 - 1s - loss: 79.3363 - loglik: -9.4015e+01 - logprior: 14.6791
Epoch 8/10
10/10 - 1s - loss: 76.8470 - loglik: -9.3971e+01 - logprior: 17.1237
Epoch 9/10
10/10 - 1s - loss: 75.0071 - loglik: -9.4027e+01 - logprior: 19.0198
Epoch 10/10
10/10 - 1s - loss: 73.5013 - loglik: -9.4100e+01 - logprior: 20.5983
Fitted a model with MAP estimate = -72.7570
Time for alignment: 30.9524
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 452.9714 - loglik: -1.8605e+02 - logprior: -2.6692e+02
Epoch 2/10
10/10 - 1s - loss: 230.2417 - loglik: -1.6029e+02 - logprior: -6.9951e+01
Epoch 3/10
10/10 - 1s - loss: 171.1751 - loglik: -1.4045e+02 - logprior: -3.0729e+01
Epoch 4/10
10/10 - 1s - loss: 143.5510 - loglik: -1.2721e+02 - logprior: -1.6346e+01
Epoch 5/10
10/10 - 1s - loss: 129.3253 - loglik: -1.2099e+02 - logprior: -8.3320e+00
Epoch 6/10
10/10 - 1s - loss: 121.2706 - loglik: -1.1821e+02 - logprior: -3.0593e+00
Epoch 7/10
10/10 - 1s - loss: 116.6139 - loglik: -1.1684e+02 - logprior: 0.2214
Epoch 8/10
10/10 - 1s - loss: 113.8903 - loglik: -1.1610e+02 - logprior: 2.2066
Epoch 9/10
10/10 - 1s - loss: 111.9946 - loglik: -1.1564e+02 - logprior: 3.6453
Epoch 10/10
10/10 - 1s - loss: 110.6204 - loglik: -1.1533e+02 - logprior: 4.7114
Fitted a model with MAP estimate = -109.9948
expansions: [(0, 5), (10, 1), (17, 2), (24, 1), (25, 1), (27, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 457.3251 - loglik: -1.0884e+02 - logprior: -3.4848e+02
Epoch 2/2
10/10 - 1s - loss: 202.8927 - loglik: -9.9577e+01 - logprior: -1.0332e+02
Fitted a model with MAP estimate = -155.6956
expansions: [(23, 1)]
discards: [ 0  1  2 51]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 396.9778 - loglik: -9.7221e+01 - logprior: -2.9976e+02
Epoch 2/2
10/10 - 1s - loss: 209.5622 - loglik: -9.5583e+01 - logprior: -1.1398e+02
Fitted a model with MAP estimate = -179.4434
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 368.6708 - loglik: -9.4632e+01 - logprior: -2.7404e+02
Epoch 2/10
10/10 - 1s - loss: 166.2588 - loglik: -9.3905e+01 - logprior: -7.2354e+01
Epoch 3/10
10/10 - 1s - loss: 116.2522 - loglik: -9.4106e+01 - logprior: -2.2147e+01
Epoch 4/10
10/10 - 1s - loss: 98.1086 - loglik: -9.4341e+01 - logprior: -3.7677e+00
Epoch 5/10
10/10 - 1s - loss: 88.6048 - loglik: -9.4275e+01 - logprior: 5.6705
Epoch 6/10
10/10 - 1s - loss: 82.9580 - loglik: -9.4145e+01 - logprior: 11.1867
Epoch 7/10
10/10 - 1s - loss: 79.3366 - loglik: -9.4016e+01 - logprior: 14.6789
Epoch 8/10
10/10 - 1s - loss: 76.8472 - loglik: -9.3971e+01 - logprior: 17.1235
Epoch 9/10
10/10 - 1s - loss: 75.0073 - loglik: -9.4027e+01 - logprior: 19.0196
Epoch 10/10
10/10 - 1s - loss: 73.5015 - loglik: -9.4100e+01 - logprior: 20.5981
Fitted a model with MAP estimate = -72.7572
Time for alignment: 30.5285
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 452.9714 - loglik: -1.8605e+02 - logprior: -2.6692e+02
Epoch 2/10
10/10 - 1s - loss: 230.2417 - loglik: -1.6029e+02 - logprior: -6.9951e+01
Epoch 3/10
10/10 - 1s - loss: 171.1751 - loglik: -1.4045e+02 - logprior: -3.0729e+01
Epoch 4/10
10/10 - 1s - loss: 143.5510 - loglik: -1.2721e+02 - logprior: -1.6346e+01
Epoch 5/10
10/10 - 1s - loss: 129.3253 - loglik: -1.2099e+02 - logprior: -8.3320e+00
Epoch 6/10
10/10 - 1s - loss: 121.2706 - loglik: -1.1821e+02 - logprior: -3.0593e+00
Epoch 7/10
10/10 - 1s - loss: 116.6139 - loglik: -1.1684e+02 - logprior: 0.2214
Epoch 8/10
10/10 - 1s - loss: 113.8903 - loglik: -1.1610e+02 - logprior: 2.2066
Epoch 9/10
10/10 - 1s - loss: 111.9946 - loglik: -1.1564e+02 - logprior: 3.6453
Epoch 10/10
10/10 - 1s - loss: 110.6204 - loglik: -1.1533e+02 - logprior: 4.7114
Fitted a model with MAP estimate = -109.9949
expansions: [(0, 5), (10, 1), (17, 2), (24, 1), (25, 1), (27, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 457.3251 - loglik: -1.0884e+02 - logprior: -3.4848e+02
Epoch 2/2
10/10 - 1s - loss: 202.8927 - loglik: -9.9577e+01 - logprior: -1.0332e+02
Fitted a model with MAP estimate = -155.6956
expansions: [(23, 1)]
discards: [ 0  1  2 51]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 396.9778 - loglik: -9.7221e+01 - logprior: -2.9976e+02
Epoch 2/2
10/10 - 1s - loss: 209.5622 - loglik: -9.5583e+01 - logprior: -1.1398e+02
Fitted a model with MAP estimate = -179.4434
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 368.6707 - loglik: -9.4632e+01 - logprior: -2.7404e+02
Epoch 2/10
10/10 - 1s - loss: 166.2587 - loglik: -9.3905e+01 - logprior: -7.2354e+01
Epoch 3/10
10/10 - 1s - loss: 116.2521 - loglik: -9.4105e+01 - logprior: -2.2147e+01
Epoch 4/10
10/10 - 1s - loss: 98.1085 - loglik: -9.4341e+01 - logprior: -3.7676e+00
Epoch 5/10
10/10 - 1s - loss: 88.6048 - loglik: -9.4275e+01 - logprior: 5.6707
Epoch 6/10
10/10 - 1s - loss: 82.9577 - loglik: -9.4145e+01 - logprior: 11.1869
Epoch 7/10
10/10 - 1s - loss: 79.3363 - loglik: -9.4015e+01 - logprior: 14.6791
Epoch 8/10
10/10 - 1s - loss: 76.8470 - loglik: -9.3971e+01 - logprior: 17.1236
Epoch 9/10
10/10 - 1s - loss: 75.0070 - loglik: -9.4027e+01 - logprior: 19.0197
Epoch 10/10
10/10 - 1s - loss: 73.5012 - loglik: -9.4099e+01 - logprior: 20.5983
Fitted a model with MAP estimate = -72.7573
Time for alignment: 30.5911
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 452.9714 - loglik: -1.8605e+02 - logprior: -2.6692e+02
Epoch 2/10
10/10 - 1s - loss: 230.2417 - loglik: -1.6029e+02 - logprior: -6.9951e+01
Epoch 3/10
10/10 - 1s - loss: 171.1751 - loglik: -1.4045e+02 - logprior: -3.0729e+01
Epoch 4/10
10/10 - 1s - loss: 143.5510 - loglik: -1.2721e+02 - logprior: -1.6346e+01
Epoch 5/10
10/10 - 1s - loss: 129.3253 - loglik: -1.2099e+02 - logprior: -8.3320e+00
Epoch 6/10
10/10 - 1s - loss: 121.2706 - loglik: -1.1821e+02 - logprior: -3.0593e+00
Epoch 7/10
10/10 - 1s - loss: 116.6139 - loglik: -1.1684e+02 - logprior: 0.2214
Epoch 8/10
10/10 - 1s - loss: 113.8904 - loglik: -1.1610e+02 - logprior: 2.2066
Epoch 9/10
10/10 - 1s - loss: 111.9946 - loglik: -1.1564e+02 - logprior: 3.6453
Epoch 10/10
10/10 - 1s - loss: 110.6205 - loglik: -1.1533e+02 - logprior: 4.7114
Fitted a model with MAP estimate = -109.9947
expansions: [(0, 5), (10, 1), (17, 2), (24, 1), (25, 1), (27, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 457.3251 - loglik: -1.0884e+02 - logprior: -3.4848e+02
Epoch 2/2
10/10 - 1s - loss: 202.8927 - loglik: -9.9577e+01 - logprior: -1.0332e+02
Fitted a model with MAP estimate = -155.6956
expansions: [(23, 1)]
discards: [ 0  1  2 51]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 396.9778 - loglik: -9.7221e+01 - logprior: -2.9976e+02
Epoch 2/2
10/10 - 1s - loss: 209.5622 - loglik: -9.5583e+01 - logprior: -1.1398e+02
Fitted a model with MAP estimate = -179.4434
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 368.6708 - loglik: -9.4632e+01 - logprior: -2.7404e+02
Epoch 2/10
10/10 - 1s - loss: 166.2588 - loglik: -9.3905e+01 - logprior: -7.2354e+01
Epoch 3/10
10/10 - 1s - loss: 116.2522 - loglik: -9.4106e+01 - logprior: -2.2147e+01
Epoch 4/10
10/10 - 1s - loss: 98.1086 - loglik: -9.4341e+01 - logprior: -3.7677e+00
Epoch 5/10
10/10 - 1s - loss: 88.6049 - loglik: -9.4275e+01 - logprior: 5.6705
Epoch 6/10
10/10 - 1s - loss: 82.9579 - loglik: -9.4145e+01 - logprior: 11.1866
Epoch 7/10
10/10 - 1s - loss: 79.3366 - loglik: -9.4016e+01 - logprior: 14.6789
Epoch 8/10
10/10 - 1s - loss: 76.8473 - loglik: -9.3971e+01 - logprior: 17.1234
Epoch 9/10
10/10 - 1s - loss: 75.0075 - loglik: -9.4027e+01 - logprior: 19.0195
Epoch 10/10
10/10 - 1s - loss: 73.5016 - loglik: -9.4100e+01 - logprior: 20.5980
Fitted a model with MAP estimate = -72.7572
Time for alignment: 30.3863
Computed alignments with likelihoods: ['-72.7572', '-72.7570', '-72.7572', '-72.7573', '-72.7572']
Best model has likelihood: -72.7570  (prior= 21.3851 )
time for generating output: 0.1072
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.8444108761329305
Training of 5 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9941ca970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9827163a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5a2ba100>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 689.4869 - loglik: -6.8544e+02 - logprior: -4.0444e+00
Epoch 2/10
26/26 - 9s - loss: 585.1045 - loglik: -5.8374e+02 - logprior: -1.3689e+00
Epoch 3/10
26/26 - 9s - loss: 576.2180 - loglik: -5.7497e+02 - logprior: -1.2493e+00
Epoch 4/10
26/26 - 9s - loss: 572.7825 - loglik: -5.7149e+02 - logprior: -1.2928e+00
Epoch 5/10
26/26 - 9s - loss: 571.0740 - loglik: -5.6972e+02 - logprior: -1.3526e+00
Epoch 6/10
26/26 - 9s - loss: 569.0288 - loglik: -5.6765e+02 - logprior: -1.3780e+00
Epoch 7/10
26/26 - 9s - loss: 570.1477 - loglik: -5.6876e+02 - logprior: -1.3869e+00
Fitted a model with MAP estimate = -569.0165
expansions: [(55, 1), (57, 2), (137, 1), (173, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 575.5177 - loglik: -5.7088e+02 - logprior: -4.6423e+00
Epoch 2/2
26/26 - 9s - loss: 571.0325 - loglik: -5.6987e+02 - logprior: -1.1657e+00
Fitted a model with MAP estimate = -568.5414
expansions: []
discards: [58 59]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 574.8878 - loglik: -5.7040e+02 - logprior: -4.4918e+00
Epoch 2/2
26/26 - 9s - loss: 571.7133 - loglik: -5.7072e+02 - logprior: -9.9753e-01
Fitted a model with MAP estimate = -569.6915
expansions: []
discards: [142]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 576.3699 - loglik: -5.7201e+02 - logprior: -4.3599e+00
Epoch 2/10
26/26 - 9s - loss: 570.4589 - loglik: -5.6959e+02 - logprior: -8.6623e-01
Epoch 3/10
26/26 - 9s - loss: 568.5795 - loglik: -5.6789e+02 - logprior: -6.9195e-01
Epoch 4/10
26/26 - 9s - loss: 569.0597 - loglik: -5.6837e+02 - logprior: -6.8987e-01
Fitted a model with MAP estimate = -566.9370
Time for alignment: 183.1668
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 691.9139 - loglik: -6.8789e+02 - logprior: -4.0233e+00
Epoch 2/10
26/26 - 9s - loss: 591.1509 - loglik: -5.9002e+02 - logprior: -1.1319e+00
Epoch 3/10
26/26 - 9s - loss: 578.6722 - loglik: -5.7759e+02 - logprior: -1.0816e+00
Epoch 4/10
26/26 - 9s - loss: 575.5819 - loglik: -5.7446e+02 - logprior: -1.1208e+00
Epoch 5/10
26/26 - 9s - loss: 575.7158 - loglik: -5.7460e+02 - logprior: -1.1169e+00
Fitted a model with MAP estimate = -574.1880
expansions: [(56, 1), (108, 4), (137, 1), (163, 1), (173, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 207 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 584.1113 - loglik: -5.7751e+02 - logprior: -6.5979e+00
Epoch 2/2
26/26 - 9s - loss: 574.3211 - loglik: -5.7183e+02 - logprior: -2.4952e+00
Fitted a model with MAP estimate = -572.1871
expansions: [(0, 5), (109, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 577.7420 - loglik: -5.7331e+02 - logprior: -4.4369e+00
Epoch 2/2
26/26 - 9s - loss: 570.6884 - loglik: -5.6971e+02 - logprior: -9.8215e-01
Fitted a model with MAP estimate = -569.0503
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 576.4048 - loglik: -5.7219e+02 - logprior: -4.2159e+00
Epoch 2/10
26/26 - 9s - loss: 568.7921 - loglik: -5.6811e+02 - logprior: -6.8404e-01
Epoch 3/10
26/26 - 9s - loss: 568.5244 - loglik: -5.6810e+02 - logprior: -4.2493e-01
Epoch 4/10
26/26 - 9s - loss: 569.3725 - loglik: -5.6903e+02 - logprior: -3.4577e-01
Fitted a model with MAP estimate = -567.1330
Time for alignment: 167.4749
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 686.9342 - loglik: -6.8291e+02 - logprior: -4.0192e+00
Epoch 2/10
26/26 - 9s - loss: 589.9645 - loglik: -5.8868e+02 - logprior: -1.2836e+00
Epoch 3/10
26/26 - 9s - loss: 578.2603 - loglik: -5.7699e+02 - logprior: -1.2722e+00
Epoch 4/10
26/26 - 9s - loss: 575.2001 - loglik: -5.7387e+02 - logprior: -1.3341e+00
Epoch 5/10
26/26 - 9s - loss: 573.1265 - loglik: -5.7176e+02 - logprior: -1.3678e+00
Epoch 6/10
26/26 - 9s - loss: 572.7731 - loglik: -5.7137e+02 - logprior: -1.4032e+00
Epoch 7/10
26/26 - 9s - loss: 571.6418 - loglik: -5.7022e+02 - logprior: -1.4245e+00
Epoch 8/10
26/26 - 9s - loss: 571.8975 - loglik: -5.7046e+02 - logprior: -1.4366e+00
Fitted a model with MAP estimate = -571.0708
expansions: [(173, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 584.9449 - loglik: -5.7826e+02 - logprior: -6.6824e+00
Epoch 2/2
26/26 - 9s - loss: 577.6150 - loglik: -5.7488e+02 - logprior: -2.7361e+00
Fitted a model with MAP estimate = -574.9841
expansions: [(0, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 578.8663 - loglik: -5.7434e+02 - logprior: -4.5281e+00
Epoch 2/2
26/26 - 9s - loss: 575.7366 - loglik: -5.7463e+02 - logprior: -1.1028e+00
Fitted a model with MAP estimate = -572.7123
expansions: []
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 577.9410 - loglik: -5.7359e+02 - logprior: -4.3545e+00
Epoch 2/10
26/26 - 9s - loss: 575.6242 - loglik: -5.7475e+02 - logprior: -8.7621e-01
Epoch 3/10
26/26 - 9s - loss: 573.2874 - loglik: -5.7259e+02 - logprior: -6.9346e-01
Epoch 4/10
26/26 - 9s - loss: 571.0275 - loglik: -5.7033e+02 - logprior: -6.9357e-01
Epoch 5/10
26/26 - 9s - loss: 570.5073 - loglik: -5.6987e+02 - logprior: -6.3517e-01
Epoch 6/10
26/26 - 9s - loss: 570.2204 - loglik: -5.6968e+02 - logprior: -5.4463e-01
Epoch 7/10
26/26 - 9s - loss: 570.2366 - loglik: -5.6979e+02 - logprior: -4.4976e-01
Fitted a model with MAP estimate = -569.4117
Time for alignment: 214.3693
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 689.8378 - loglik: -6.8582e+02 - logprior: -4.0153e+00
Epoch 2/10
26/26 - 9s - loss: 588.4040 - loglik: -5.8738e+02 - logprior: -1.0243e+00
Epoch 3/10
26/26 - 9s - loss: 576.4289 - loglik: -5.7546e+02 - logprior: -9.6720e-01
Epoch 4/10
26/26 - 9s - loss: 573.1186 - loglik: -5.7211e+02 - logprior: -1.0125e+00
Epoch 5/10
26/26 - 9s - loss: 572.2511 - loglik: -5.7123e+02 - logprior: -1.0203e+00
Epoch 6/10
26/26 - 9s - loss: 572.4436 - loglik: -5.7140e+02 - logprior: -1.0431e+00
Fitted a model with MAP estimate = -570.8984
expansions: [(9, 1), (56, 1), (95, 1), (115, 1), (173, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 204 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 581.1425 - loglik: -5.7453e+02 - logprior: -6.6119e+00
Epoch 2/2
26/26 - 9s - loss: 573.7471 - loglik: -5.7120e+02 - logprior: -2.5427e+00
Fitted a model with MAP estimate = -570.7786
expansions: [(0, 6)]
discards: [ 0 96]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 577.7435 - loglik: -5.7331e+02 - logprior: -4.4320e+00
Epoch 2/2
26/26 - 9s - loss: 571.7335 - loglik: -5.7071e+02 - logprior: -1.0274e+00
Fitted a model with MAP estimate = -569.1105
expansions: []
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 578.6648 - loglik: -5.7265e+02 - logprior: -6.0180e+00
Epoch 2/10
26/26 - 9s - loss: 573.0074 - loglik: -5.7227e+02 - logprior: -7.3906e-01
Epoch 3/10
26/26 - 9s - loss: 570.9985 - loglik: -5.7080e+02 - logprior: -1.9980e-01
Epoch 4/10
26/26 - 9s - loss: 568.3641 - loglik: -5.6821e+02 - logprior: -1.5866e-01
Epoch 5/10
26/26 - 9s - loss: 566.6093 - loglik: -5.6654e+02 - logprior: -7.1946e-02
Epoch 6/10
26/26 - 9s - loss: 568.4555 - loglik: -5.6848e+02 - logprior: 0.0215
Fitted a model with MAP estimate = -566.9321
Time for alignment: 189.7960
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 691.6941 - loglik: -6.8765e+02 - logprior: -4.0402e+00
Epoch 2/10
26/26 - 9s - loss: 587.2520 - loglik: -5.8623e+02 - logprior: -1.0227e+00
Epoch 3/10
26/26 - 9s - loss: 575.1943 - loglik: -5.7420e+02 - logprior: -9.9250e-01
Epoch 4/10
26/26 - 9s - loss: 572.1887 - loglik: -5.7120e+02 - logprior: -9.8859e-01
Epoch 5/10
26/26 - 9s - loss: 570.9605 - loglik: -5.6999e+02 - logprior: -9.6797e-01
Epoch 6/10
26/26 - 9s - loss: 571.3913 - loglik: -5.7041e+02 - logprior: -9.8088e-01
Fitted a model with MAP estimate = -569.5781
expansions: [(11, 1), (77, 2), (108, 3), (160, 1), (163, 1), (173, 1)]
discards: [161]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 575.2257 - loglik: -5.7079e+02 - logprior: -4.4364e+00
Epoch 2/2
26/26 - 9s - loss: 568.9662 - loglik: -5.6794e+02 - logprior: -1.0239e+00
Fitted a model with MAP estimate = -566.8162
expansions: [(113, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 208 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 573.9625 - loglik: -5.6750e+02 - logprior: -6.4627e+00
Epoch 2/2
26/26 - 9s - loss: 571.0273 - loglik: -5.6823e+02 - logprior: -2.8002e+00
Fitted a model with MAP estimate = -567.0381
expansions: [(0, 5)]
discards: [  0 168]
Re-initialized the encoder parameters.
Fitting a model of length 211 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 570.5826 - loglik: -5.6631e+02 - logprior: -4.2703e+00
Epoch 2/10
26/26 - 9s - loss: 566.3217 - loglik: -5.6544e+02 - logprior: -8.7683e-01
Epoch 3/10
26/26 - 9s - loss: 564.1649 - loglik: -5.6353e+02 - logprior: -6.3842e-01
Epoch 4/10
26/26 - 9s - loss: 560.4447 - loglik: -5.5985e+02 - logprior: -5.9142e-01
Epoch 5/10
26/26 - 9s - loss: 562.2420 - loglik: -5.6174e+02 - logprior: -5.0147e-01
Fitted a model with MAP estimate = -560.4178
Time for alignment: 183.7409
Computed alignments with likelihoods: ['-566.9370', '-567.1330', '-569.4117', '-566.9321', '-560.4178']
Best model has likelihood: -560.4178  (prior= -0.4630 )
time for generating output: 0.2883
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.646774193548387
Training of 5 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9e9051070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feafd21ccd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb5a30d190>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 422.3626 - loglik: -3.4561e+02 - logprior: -7.6755e+01
Epoch 2/10
10/10 - 1s - loss: 337.0202 - loglik: -3.1936e+02 - logprior: -1.7661e+01
Epoch 3/10
10/10 - 1s - loss: 299.0062 - loglik: -2.9247e+02 - logprior: -6.5362e+00
Epoch 4/10
10/10 - 1s - loss: 280.7948 - loglik: -2.7829e+02 - logprior: -2.5054e+00
Epoch 5/10
10/10 - 1s - loss: 273.8329 - loglik: -2.7343e+02 - logprior: -4.0290e-01
Epoch 6/10
10/10 - 1s - loss: 270.6216 - loglik: -2.7142e+02 - logprior: 0.7942
Epoch 7/10
10/10 - 1s - loss: 268.8375 - loglik: -2.7037e+02 - logprior: 1.5329
Epoch 8/10
10/10 - 1s - loss: 268.0265 - loglik: -2.6984e+02 - logprior: 1.8130
Epoch 9/10
10/10 - 1s - loss: 266.7297 - loglik: -2.6874e+02 - logprior: 2.0107
Epoch 10/10
10/10 - 1s - loss: 266.4000 - loglik: -2.6863e+02 - logprior: 2.2304
Fitted a model with MAP estimate = -266.0263
expansions: [(9, 2), (10, 2), (12, 3), (19, 2), (46, 3), (65, 2), (79, 1), (82, 1), (83, 1), (84, 1), (86, 2), (87, 5), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 350.3480 - loglik: -2.6471e+02 - logprior: -8.5638e+01
Epoch 2/2
10/10 - 1s - loss: 283.3245 - loglik: -2.5072e+02 - logprior: -3.2603e+01
Fitted a model with MAP estimate = -271.8217
expansions: [(0, 3), (16, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 314.0324 - loglik: -2.4673e+02 - logprior: -6.7298e+01
Epoch 2/2
10/10 - 1s - loss: 256.7484 - loglik: -2.4258e+02 - logprior: -1.4167e+01
Fitted a model with MAP estimate = -247.9848
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 325.3802 - loglik: -2.4448e+02 - logprior: -8.0901e+01
Epoch 2/10
10/10 - 1s - loss: 264.1871 - loglik: -2.4317e+02 - logprior: -2.1019e+01
Epoch 3/10
10/10 - 1s - loss: 245.9156 - loglik: -2.4173e+02 - logprior: -4.1877e+00
Epoch 4/10
10/10 - 1s - loss: 239.6633 - loglik: -2.4128e+02 - logprior: 1.6161
Epoch 5/10
10/10 - 1s - loss: 236.7414 - loglik: -2.4105e+02 - logprior: 4.3106
Epoch 6/10
10/10 - 1s - loss: 235.0736 - loglik: -2.4089e+02 - logprior: 5.8154
Epoch 7/10
10/10 - 1s - loss: 233.5820 - loglik: -2.4038e+02 - logprior: 6.7942
Epoch 8/10
10/10 - 1s - loss: 232.6862 - loglik: -2.4027e+02 - logprior: 7.5794
Epoch 9/10
10/10 - 1s - loss: 232.3731 - loglik: -2.4060e+02 - logprior: 8.2246
Epoch 10/10
10/10 - 1s - loss: 231.9884 - loglik: -2.4073e+02 - logprior: 8.7452
Fitted a model with MAP estimate = -231.4683
Time for alignment: 46.0718
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 422.3717 - loglik: -3.4562e+02 - logprior: -7.6755e+01
Epoch 2/10
10/10 - 1s - loss: 337.2276 - loglik: -3.1957e+02 - logprior: -1.7656e+01
Epoch 3/10
10/10 - 1s - loss: 298.8314 - loglik: -2.9226e+02 - logprior: -6.5741e+00
Epoch 4/10
10/10 - 1s - loss: 279.1951 - loglik: -2.7645e+02 - logprior: -2.7492e+00
Epoch 5/10
10/10 - 1s - loss: 271.9492 - loglik: -2.7145e+02 - logprior: -4.9849e-01
Epoch 6/10
10/10 - 1s - loss: 268.5491 - loglik: -2.6928e+02 - logprior: 0.7309
Epoch 7/10
10/10 - 1s - loss: 266.5869 - loglik: -2.6790e+02 - logprior: 1.3122
Epoch 8/10
10/10 - 1s - loss: 264.5099 - loglik: -2.6601e+02 - logprior: 1.4976
Epoch 9/10
10/10 - 1s - loss: 263.7936 - loglik: -2.6548e+02 - logprior: 1.6857
Epoch 10/10
10/10 - 1s - loss: 263.2571 - loglik: -2.6515e+02 - logprior: 1.8966
Fitted a model with MAP estimate = -262.7901
expansions: [(9, 2), (10, 2), (12, 4), (18, 1), (46, 3), (49, 1), (65, 2), (80, 1), (82, 1), (83, 5), (86, 1), (87, 4), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 348.6754 - loglik: -2.6291e+02 - logprior: -8.5769e+01
Epoch 2/2
10/10 - 1s - loss: 283.3539 - loglik: -2.5054e+02 - logprior: -3.2816e+01
Fitted a model with MAP estimate = -272.1072
expansions: [(0, 3), (16, 1), (109, 2)]
discards: [  0  78 101 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 315.0647 - loglik: -2.4761e+02 - logprior: -6.7451e+01
Epoch 2/2
10/10 - 1s - loss: 256.8397 - loglik: -2.4269e+02 - logprior: -1.4150e+01
Fitted a model with MAP estimate = -247.7146
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 325.3421 - loglik: -2.4408e+02 - logprior: -8.1258e+01
Epoch 2/10
10/10 - 1s - loss: 263.8528 - loglik: -2.4223e+02 - logprior: -2.1622e+01
Epoch 3/10
10/10 - 1s - loss: 246.2641 - loglik: -2.4195e+02 - logprior: -4.3105e+00
Epoch 4/10
10/10 - 1s - loss: 239.1862 - loglik: -2.4089e+02 - logprior: 1.6994
Epoch 5/10
10/10 - 1s - loss: 236.1220 - loglik: -2.4052e+02 - logprior: 4.3979
Epoch 6/10
10/10 - 1s - loss: 234.0098 - loglik: -2.3991e+02 - logprior: 5.9007
Epoch 7/10
10/10 - 1s - loss: 233.2526 - loglik: -2.4014e+02 - logprior: 6.8920
Epoch 8/10
10/10 - 1s - loss: 232.8108 - loglik: -2.4048e+02 - logprior: 7.6739
Epoch 9/10
10/10 - 1s - loss: 232.0733 - loglik: -2.4041e+02 - logprior: 8.3394
Epoch 10/10
10/10 - 1s - loss: 231.4175 - loglik: -2.4029e+02 - logprior: 8.8766
Fitted a model with MAP estimate = -231.2650
Time for alignment: 51.3957
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 422.2477 - loglik: -3.4549e+02 - logprior: -7.6754e+01
Epoch 2/10
10/10 - 1s - loss: 337.9077 - loglik: -3.2026e+02 - logprior: -1.7648e+01
Epoch 3/10
10/10 - 1s - loss: 300.8454 - loglik: -2.9425e+02 - logprior: -6.5932e+00
Epoch 4/10
10/10 - 1s - loss: 282.7968 - loglik: -2.8014e+02 - logprior: -2.6616e+00
Epoch 5/10
10/10 - 1s - loss: 274.6264 - loglik: -2.7405e+02 - logprior: -5.7243e-01
Epoch 6/10
10/10 - 1s - loss: 270.5469 - loglik: -2.7111e+02 - logprior: 0.5601
Epoch 7/10
10/10 - 1s - loss: 268.6259 - loglik: -2.6995e+02 - logprior: 1.3221
Epoch 8/10
10/10 - 1s - loss: 267.6867 - loglik: -2.6935e+02 - logprior: 1.6636
Epoch 9/10
10/10 - 1s - loss: 266.3614 - loglik: -2.6817e+02 - logprior: 1.8072
Epoch 10/10
10/10 - 1s - loss: 265.6302 - loglik: -2.6756e+02 - logprior: 1.9273
Fitted a model with MAP estimate = -264.9732
expansions: [(9, 2), (10, 1), (12, 3), (18, 2), (46, 3), (49, 1), (79, 1), (82, 1), (84, 5), (86, 1), (89, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 350.6017 - loglik: -2.6448e+02 - logprior: -8.6124e+01
Epoch 2/2
10/10 - 1s - loss: 287.4198 - loglik: -2.5450e+02 - logprior: -3.2924e+01
Fitted a model with MAP estimate = -276.4166
expansions: [(0, 3), (15, 1), (76, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 318.7473 - loglik: -2.5096e+02 - logprior: -6.7789e+01
Epoch 2/2
10/10 - 1s - loss: 260.9782 - loglik: -2.4641e+02 - logprior: -1.4567e+01
Fitted a model with MAP estimate = -252.0139
expansions: [(13, 1), (114, 2)]
discards: [ 0  1 80 81]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 332.5615 - loglik: -2.4943e+02 - logprior: -8.3132e+01
Epoch 2/10
10/10 - 1s - loss: 271.9756 - loglik: -2.4558e+02 - logprior: -2.6399e+01
Epoch 3/10
10/10 - 1s - loss: 250.2403 - loglik: -2.4341e+02 - logprior: -6.8349e+00
Epoch 4/10
10/10 - 1s - loss: 241.1944 - loglik: -2.4246e+02 - logprior: 1.2688
Epoch 5/10
10/10 - 1s - loss: 237.5592 - loglik: -2.4188e+02 - logprior: 4.3171
Epoch 6/10
10/10 - 1s - loss: 235.8811 - loglik: -2.4175e+02 - logprior: 5.8668
Epoch 7/10
10/10 - 1s - loss: 234.3710 - loglik: -2.4122e+02 - logprior: 6.8507
Epoch 8/10
10/10 - 1s - loss: 234.0449 - loglik: -2.4168e+02 - logprior: 7.6321
Epoch 9/10
10/10 - 1s - loss: 233.2460 - loglik: -2.4154e+02 - logprior: 8.2914
Epoch 10/10
10/10 - 1s - loss: 232.5079 - loglik: -2.4132e+02 - logprior: 8.8152
Fitted a model with MAP estimate = -232.4033
Time for alignment: 49.4389
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 422.4375 - loglik: -3.4569e+02 - logprior: -7.6752e+01
Epoch 2/10
10/10 - 1s - loss: 337.1402 - loglik: -3.1948e+02 - logprior: -1.7661e+01
Epoch 3/10
10/10 - 1s - loss: 297.6481 - loglik: -2.9108e+02 - logprior: -6.5645e+00
Epoch 4/10
10/10 - 1s - loss: 278.9694 - loglik: -2.7630e+02 - logprior: -2.6682e+00
Epoch 5/10
10/10 - 1s - loss: 271.7950 - loglik: -2.7132e+02 - logprior: -4.7601e-01
Epoch 6/10
10/10 - 1s - loss: 268.4753 - loglik: -2.6918e+02 - logprior: 0.7055
Epoch 7/10
10/10 - 1s - loss: 266.9619 - loglik: -2.6838e+02 - logprior: 1.4180
Epoch 8/10
10/10 - 1s - loss: 266.0347 - loglik: -2.6793e+02 - logprior: 1.8966
Epoch 9/10
10/10 - 1s - loss: 265.2857 - loglik: -2.6744e+02 - logprior: 2.1520
Epoch 10/10
10/10 - 1s - loss: 264.8835 - loglik: -2.6724e+02 - logprior: 2.3535
Fitted a model with MAP estimate = -264.5628
expansions: [(9, 2), (10, 1), (12, 4), (18, 2), (37, 2), (45, 1), (46, 1), (49, 1), (55, 1), (64, 3), (79, 1), (83, 3), (86, 1), (87, 4), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 349.2289 - loglik: -2.6382e+02 - logprior: -8.5404e+01
Epoch 2/2
10/10 - 1s - loss: 284.2208 - loglik: -2.5176e+02 - logprior: -3.2458e+01
Fitted a model with MAP estimate = -273.5085
expansions: [(0, 3)]
discards: [  0  45  78 102]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 317.3738 - loglik: -2.4984e+02 - logprior: -6.7533e+01
Epoch 2/2
10/10 - 1s - loss: 258.8318 - loglik: -2.4413e+02 - logprior: -1.4703e+01
Fitted a model with MAP estimate = -249.4791
expansions: [(70, 1), (111, 3)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 327.3847 - loglik: -2.4410e+02 - logprior: -8.3290e+01
Epoch 2/10
10/10 - 1s - loss: 268.3419 - loglik: -2.4124e+02 - logprior: -2.7103e+01
Epoch 3/10
10/10 - 1s - loss: 247.0899 - loglik: -2.3960e+02 - logprior: -7.4946e+00
Epoch 4/10
10/10 - 1s - loss: 237.5819 - loglik: -2.3884e+02 - logprior: 1.2588
Epoch 5/10
10/10 - 1s - loss: 233.7818 - loglik: -2.3822e+02 - logprior: 4.4400
Epoch 6/10
10/10 - 1s - loss: 231.8604 - loglik: -2.3790e+02 - logprior: 6.0371
Epoch 7/10
10/10 - 1s - loss: 230.4020 - loglik: -2.3743e+02 - logprior: 7.0284
Epoch 8/10
10/10 - 1s - loss: 229.3693 - loglik: -2.3717e+02 - logprior: 7.7993
Epoch 9/10
10/10 - 1s - loss: 228.9346 - loglik: -2.3739e+02 - logprior: 8.4505
Epoch 10/10
10/10 - 1s - loss: 228.6285 - loglik: -2.3761e+02 - logprior: 8.9833
Fitted a model with MAP estimate = -228.2795
Time for alignment: 49.8101
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 422.2177 - loglik: -3.4546e+02 - logprior: -7.6753e+01
Epoch 2/10
10/10 - 1s - loss: 337.4859 - loglik: -3.1983e+02 - logprior: -1.7658e+01
Epoch 3/10
10/10 - 1s - loss: 297.1677 - loglik: -2.9061e+02 - logprior: -6.5568e+00
Epoch 4/10
10/10 - 1s - loss: 278.0168 - loglik: -2.7524e+02 - logprior: -2.7760e+00
Epoch 5/10
10/10 - 1s - loss: 270.5410 - loglik: -2.6999e+02 - logprior: -5.5038e-01
Epoch 6/10
10/10 - 1s - loss: 267.2630 - loglik: -2.6807e+02 - logprior: 0.8035
Epoch 7/10
10/10 - 1s - loss: 265.5673 - loglik: -2.6716e+02 - logprior: 1.5929
Epoch 8/10
10/10 - 1s - loss: 264.8727 - loglik: -2.6690e+02 - logprior: 2.0259
Epoch 9/10
10/10 - 1s - loss: 263.9911 - loglik: -2.6634e+02 - logprior: 2.3530
Epoch 10/10
10/10 - 1s - loss: 263.7717 - loglik: -2.6636e+02 - logprior: 2.5908
Fitted a model with MAP estimate = -263.4352
expansions: [(10, 2), (12, 4), (18, 1), (46, 3), (49, 1), (64, 3), (79, 1), (84, 4), (86, 1), (89, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 351.7195 - loglik: -2.6590e+02 - logprior: -8.5815e+01
Epoch 2/2
10/10 - 1s - loss: 289.4176 - loglik: -2.5660e+02 - logprior: -3.2817e+01
Fitted a model with MAP estimate = -277.8754
expansions: [(0, 3), (4, 1), (5, 1), (14, 1), (65, 2)]
discards: [  0  98  99 108]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 318.8695 - loglik: -2.5107e+02 - logprior: -6.7797e+01
Epoch 2/2
10/10 - 1s - loss: 259.6932 - loglik: -2.4524e+02 - logprior: -1.4457e+01
Fitted a model with MAP estimate = -250.6235
expansions: [(111, 3)]
discards: [ 0  1 81]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.8568 - loglik: -2.4686e+02 - logprior: -8.2993e+01
Epoch 2/10
10/10 - 1s - loss: 269.9565 - loglik: -2.4368e+02 - logprior: -2.6276e+01
Epoch 3/10
10/10 - 1s - loss: 248.0634 - loglik: -2.4129e+02 - logprior: -6.7767e+00
Epoch 4/10
10/10 - 1s - loss: 239.1565 - loglik: -2.4049e+02 - logprior: 1.3294
Epoch 5/10
10/10 - 1s - loss: 235.5376 - loglik: -2.3991e+02 - logprior: 4.3773
Epoch 6/10
10/10 - 1s - loss: 233.5952 - loglik: -2.3950e+02 - logprior: 5.9064
Epoch 7/10
10/10 - 1s - loss: 232.1654 - loglik: -2.3901e+02 - logprior: 6.8439
Epoch 8/10
10/10 - 1s - loss: 231.3131 - loglik: -2.3890e+02 - logprior: 7.5829
Epoch 9/10
10/10 - 1s - loss: 230.8093 - loglik: -2.3904e+02 - logprior: 8.2262
Epoch 10/10
10/10 - 1s - loss: 230.3708 - loglik: -2.3912e+02 - logprior: 8.7533
Fitted a model with MAP estimate = -229.9999
Time for alignment: 49.4815
Computed alignments with likelihoods: ['-231.4683', '-231.2650', '-232.4033', '-228.2795', '-229.9999']
Best model has likelihood: -228.2795  (prior= 9.2423 )
time for generating output: 0.1907
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.8014388489208633
Training of 5 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaaf4bc7c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb40fe4e20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f1547220>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 97.1029 - loglik: -9.2668e+01 - logprior: -4.4350e+00
Epoch 2/10
17/17 - 1s - loss: 74.5429 - loglik: -7.2976e+01 - logprior: -1.5669e+00
Epoch 3/10
17/17 - 1s - loss: 64.3761 - loglik: -6.2754e+01 - logprior: -1.6221e+00
Epoch 4/10
17/17 - 1s - loss: 62.2030 - loglik: -6.0555e+01 - logprior: -1.6478e+00
Epoch 5/10
17/17 - 1s - loss: 61.8480 - loglik: -6.0308e+01 - logprior: -1.5402e+00
Epoch 6/10
17/17 - 1s - loss: 61.2693 - loglik: -5.9710e+01 - logprior: -1.5589e+00
Epoch 7/10
17/17 - 1s - loss: 61.3077 - loglik: -5.9776e+01 - logprior: -1.5320e+00
Fitted a model with MAP estimate = -61.2064
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (14, 1), (15, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 69.4012 - loglik: -6.3842e+01 - logprior: -5.5595e+00
Epoch 2/2
17/17 - 1s - loss: 61.0509 - loglik: -5.8412e+01 - logprior: -2.6385e+00
Fitted a model with MAP estimate = -59.1314
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 63.9237 - loglik: -5.8955e+01 - logprior: -4.9692e+00
Epoch 2/2
17/17 - 1s - loss: 58.5453 - loglik: -5.6872e+01 - logprior: -1.6738e+00
Fitted a model with MAP estimate = -57.9412
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.4041 - loglik: -5.7119e+01 - logprior: -4.2852e+00
Epoch 2/10
17/17 - 1s - loss: 58.3323 - loglik: -5.6757e+01 - logprior: -1.5755e+00
Epoch 3/10
17/17 - 1s - loss: 57.6817 - loglik: -5.6338e+01 - logprior: -1.3439e+00
Epoch 4/10
17/17 - 1s - loss: 57.4193 - loglik: -5.6136e+01 - logprior: -1.2829e+00
Epoch 5/10
17/17 - 1s - loss: 57.4725 - loglik: -5.6216e+01 - logprior: -1.2561e+00
Fitted a model with MAP estimate = -57.2493
Time for alignment: 33.0212
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.1262 - loglik: -9.2697e+01 - logprior: -4.4291e+00
Epoch 2/10
17/17 - 1s - loss: 74.7940 - loglik: -7.3245e+01 - logprior: -1.5492e+00
Epoch 3/10
17/17 - 1s - loss: 64.8167 - loglik: -6.3186e+01 - logprior: -1.6306e+00
Epoch 4/10
17/17 - 1s - loss: 63.0831 - loglik: -6.1462e+01 - logprior: -1.6213e+00
Epoch 5/10
17/17 - 1s - loss: 62.5330 - loglik: -6.1003e+01 - logprior: -1.5304e+00
Epoch 6/10
17/17 - 1s - loss: 62.3761 - loglik: -6.0828e+01 - logprior: -1.5480e+00
Epoch 7/10
17/17 - 1s - loss: 62.1643 - loglik: -6.0639e+01 - logprior: -1.5255e+00
Epoch 8/10
17/17 - 1s - loss: 62.0511 - loglik: -6.0536e+01 - logprior: -1.5148e+00
Epoch 9/10
17/17 - 1s - loss: 62.0051 - loglik: -6.0499e+01 - logprior: -1.5062e+00
Epoch 10/10
17/17 - 1s - loss: 61.9510 - loglik: -6.0447e+01 - logprior: -1.5043e+00
Fitted a model with MAP estimate = -61.9450
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 2), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.7860 - loglik: -6.5260e+01 - logprior: -5.5260e+00
Epoch 2/2
17/17 - 1s - loss: 61.6492 - loglik: -5.9109e+01 - logprior: -2.5404e+00
Fitted a model with MAP estimate = -59.3429
expansions: []
discards: [13 16 19]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 62.1146 - loglik: -5.7733e+01 - logprior: -4.3820e+00
Epoch 2/2
17/17 - 1s - loss: 58.3426 - loglik: -5.6738e+01 - logprior: -1.6042e+00
Fitted a model with MAP estimate = -57.8962
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 61.3935 - loglik: -5.7121e+01 - logprior: -4.2727e+00
Epoch 2/10
17/17 - 1s - loss: 58.1839 - loglik: -5.6609e+01 - logprior: -1.5747e+00
Epoch 3/10
17/17 - 1s - loss: 57.8956 - loglik: -5.6545e+01 - logprior: -1.3510e+00
Epoch 4/10
17/17 - 1s - loss: 57.4599 - loglik: -5.6181e+01 - logprior: -1.2793e+00
Epoch 5/10
17/17 - 1s - loss: 57.2141 - loglik: -5.5962e+01 - logprior: -1.2524e+00
Epoch 6/10
17/17 - 1s - loss: 57.3822 - loglik: -5.6155e+01 - logprior: -1.2276e+00
Fitted a model with MAP estimate = -57.1866
Time for alignment: 33.1648
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.1749 - loglik: -9.2743e+01 - logprior: -4.4318e+00
Epoch 2/10
17/17 - 1s - loss: 75.0487 - loglik: -7.3504e+01 - logprior: -1.5446e+00
Epoch 3/10
17/17 - 1s - loss: 64.8035 - loglik: -6.3168e+01 - logprior: -1.6355e+00
Epoch 4/10
17/17 - 1s - loss: 62.5411 - loglik: -6.0898e+01 - logprior: -1.6429e+00
Epoch 5/10
17/17 - 1s - loss: 61.8456 - loglik: -6.0299e+01 - logprior: -1.5469e+00
Epoch 6/10
17/17 - 1s - loss: 61.7699 - loglik: -6.0209e+01 - logprior: -1.5606e+00
Epoch 7/10
17/17 - 1s - loss: 61.4678 - loglik: -5.9930e+01 - logprior: -1.5378e+00
Epoch 8/10
17/17 - 1s - loss: 61.5534 - loglik: -6.0032e+01 - logprior: -1.5212e+00
Fitted a model with MAP estimate = -61.4339
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 2), (14, 1), (15, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.0932 - loglik: -6.4580e+01 - logprior: -5.5128e+00
Epoch 2/2
17/17 - 1s - loss: 61.5509 - loglik: -5.9046e+01 - logprior: -2.5050e+00
Fitted a model with MAP estimate = -59.1294
expansions: []
discards: [13 16 19]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 62.0786 - loglik: -5.7705e+01 - logprior: -4.3735e+00
Epoch 2/2
17/17 - 1s - loss: 58.2243 - loglik: -5.6619e+01 - logprior: -1.6056e+00
Fitted a model with MAP estimate = -57.8938
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.4083 - loglik: -5.7133e+01 - logprior: -4.2752e+00
Epoch 2/10
17/17 - 1s - loss: 58.1867 - loglik: -5.6604e+01 - logprior: -1.5825e+00
Epoch 3/10
17/17 - 1s - loss: 57.8463 - loglik: -5.6508e+01 - logprior: -1.3383e+00
Epoch 4/10
17/17 - 1s - loss: 57.4222 - loglik: -5.6129e+01 - logprior: -1.2930e+00
Epoch 5/10
17/17 - 1s - loss: 57.4203 - loglik: -5.6175e+01 - logprior: -1.2451e+00
Epoch 6/10
17/17 - 1s - loss: 57.2203 - loglik: -5.5993e+01 - logprior: -1.2278e+00
Epoch 7/10
17/17 - 1s - loss: 57.1811 - loglik: -5.5971e+01 - logprior: -1.2102e+00
Epoch 8/10
17/17 - 1s - loss: 57.1374 - loglik: -5.5947e+01 - logprior: -1.1900e+00
Epoch 9/10
17/17 - 1s - loss: 57.1227 - loglik: -5.5933e+01 - logprior: -1.1898e+00
Epoch 10/10
17/17 - 1s - loss: 56.9581 - loglik: -5.5787e+01 - logprior: -1.1707e+00
Fitted a model with MAP estimate = -57.0248
Time for alignment: 33.9736
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 97.1296 - loglik: -9.2695e+01 - logprior: -4.4343e+00
Epoch 2/10
17/17 - 1s - loss: 74.9536 - loglik: -7.3395e+01 - logprior: -1.5582e+00
Epoch 3/10
17/17 - 1s - loss: 64.8942 - loglik: -6.3280e+01 - logprior: -1.6145e+00
Epoch 4/10
17/17 - 1s - loss: 62.8503 - loglik: -6.1211e+01 - logprior: -1.6396e+00
Epoch 5/10
17/17 - 1s - loss: 62.1482 - loglik: -6.0615e+01 - logprior: -1.5331e+00
Epoch 6/10
17/17 - 1s - loss: 61.9689 - loglik: -6.0414e+01 - logprior: -1.5548e+00
Epoch 7/10
17/17 - 1s - loss: 61.8002 - loglik: -6.0273e+01 - logprior: -1.5270e+00
Epoch 8/10
17/17 - 1s - loss: 61.8171 - loglik: -6.0306e+01 - logprior: -1.5114e+00
Fitted a model with MAP estimate = -61.7068
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 2), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.2626 - loglik: -6.4675e+01 - logprior: -5.5872e+00
Epoch 2/2
17/17 - 1s - loss: 61.0955 - loglik: -5.8365e+01 - logprior: -2.7302e+00
Fitted a model with MAP estimate = -59.2334
expansions: [(2, 1)]
discards: [ 0 13 16 19]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 63.8293 - loglik: -5.8900e+01 - logprior: -4.9290e+00
Epoch 2/2
17/17 - 1s - loss: 58.4258 - loglik: -5.6764e+01 - logprior: -1.6613e+00
Fitted a model with MAP estimate = -57.9366
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.4306 - loglik: -5.7147e+01 - logprior: -4.2832e+00
Epoch 2/10
17/17 - 1s - loss: 58.1821 - loglik: -5.6605e+01 - logprior: -1.5771e+00
Epoch 3/10
17/17 - 1s - loss: 57.7368 - loglik: -5.6399e+01 - logprior: -1.3379e+00
Epoch 4/10
17/17 - 1s - loss: 57.4827 - loglik: -5.6198e+01 - logprior: -1.2846e+00
Epoch 5/10
17/17 - 1s - loss: 57.4044 - loglik: -5.6153e+01 - logprior: -1.2513e+00
Epoch 6/10
17/17 - 1s - loss: 57.2121 - loglik: -5.5985e+01 - logprior: -1.2270e+00
Epoch 7/10
17/17 - 1s - loss: 57.1693 - loglik: -5.5959e+01 - logprior: -1.2105e+00
Epoch 8/10
17/17 - 1s - loss: 57.1633 - loglik: -5.5977e+01 - logprior: -1.1866e+00
Epoch 9/10
17/17 - 1s - loss: 57.0954 - loglik: -5.5907e+01 - logprior: -1.1886e+00
Epoch 10/10
17/17 - 1s - loss: 57.1757 - loglik: -5.6003e+01 - logprior: -1.1731e+00
Fitted a model with MAP estimate = -57.0190
Time for alignment: 31.5422
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 97.1238 - loglik: -9.2696e+01 - logprior: -4.4280e+00
Epoch 2/10
17/17 - 1s - loss: 74.9016 - loglik: -7.3371e+01 - logprior: -1.5305e+00
Epoch 3/10
17/17 - 1s - loss: 64.5815 - loglik: -6.2953e+01 - logprior: -1.6282e+00
Epoch 4/10
17/17 - 1s - loss: 63.1177 - loglik: -6.1495e+01 - logprior: -1.6231e+00
Epoch 5/10
17/17 - 1s - loss: 62.6187 - loglik: -6.1090e+01 - logprior: -1.5282e+00
Epoch 6/10
17/17 - 1s - loss: 62.2514 - loglik: -6.0708e+01 - logprior: -1.5439e+00
Epoch 7/10
17/17 - 1s - loss: 62.1471 - loglik: -6.0624e+01 - logprior: -1.5233e+00
Epoch 8/10
17/17 - 1s - loss: 62.1237 - loglik: -6.0613e+01 - logprior: -1.5108e+00
Epoch 9/10
17/17 - 1s - loss: 62.0228 - loglik: -6.0517e+01 - logprior: -1.5057e+00
Epoch 10/10
17/17 - 1s - loss: 61.9579 - loglik: -6.0460e+01 - logprior: -1.4977e+00
Fitted a model with MAP estimate = -61.9413
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 2), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.7239 - loglik: -6.5207e+01 - logprior: -5.5170e+00
Epoch 2/2
17/17 - 1s - loss: 61.7030 - loglik: -5.9165e+01 - logprior: -2.5377e+00
Fitted a model with MAP estimate = -59.3146
expansions: []
discards: [13 16 19]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 62.1948 - loglik: -5.7810e+01 - logprior: -4.3853e+00
Epoch 2/2
17/17 - 1s - loss: 58.1232 - loglik: -5.6515e+01 - logprior: -1.6080e+00
Fitted a model with MAP estimate = -57.8990
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.3096 - loglik: -5.7041e+01 - logprior: -4.2687e+00
Epoch 2/10
17/17 - 1s - loss: 58.2196 - loglik: -5.6637e+01 - logprior: -1.5822e+00
Epoch 3/10
17/17 - 1s - loss: 57.7966 - loglik: -5.6445e+01 - logprior: -1.3515e+00
Epoch 4/10
17/17 - 1s - loss: 57.4902 - loglik: -5.6218e+01 - logprior: -1.2723e+00
Epoch 5/10
17/17 - 1s - loss: 57.3627 - loglik: -5.6115e+01 - logprior: -1.2480e+00
Epoch 6/10
17/17 - 1s - loss: 57.3133 - loglik: -5.6083e+01 - logprior: -1.2299e+00
Epoch 7/10
17/17 - 1s - loss: 57.1325 - loglik: -5.5926e+01 - logprior: -1.2064e+00
Epoch 8/10
17/17 - 1s - loss: 57.1229 - loglik: -5.5925e+01 - logprior: -1.1979e+00
Epoch 9/10
17/17 - 1s - loss: 57.0687 - loglik: -5.5887e+01 - logprior: -1.1816e+00
Epoch 10/10
17/17 - 1s - loss: 57.0600 - loglik: -5.5886e+01 - logprior: -1.1736e+00
Fitted a model with MAP estimate = -57.0178
Time for alignment: 34.7352
Computed alignments with likelihoods: ['-57.2493', '-57.1866', '-57.0248', '-57.0190', '-57.0178']
Best model has likelihood: -57.0178  (prior= -1.1715 )
time for generating output: 0.0916
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.8120416827479737
Training of 5 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d92d550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaa640ab50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2bf2de80>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 193.4653 - loglik: -1.8537e+02 - logprior: -8.1002e+00
Epoch 2/10
13/13 - 1s - loss: 163.4367 - loglik: -1.6119e+02 - logprior: -2.2482e+00
Epoch 3/10
13/13 - 1s - loss: 145.6477 - loglik: -1.4371e+02 - logprior: -1.9348e+00
Epoch 4/10
13/13 - 1s - loss: 139.0393 - loglik: -1.3702e+02 - logprior: -2.0231e+00
Epoch 5/10
13/13 - 1s - loss: 136.3136 - loglik: -1.3442e+02 - logprior: -1.8983e+00
Epoch 6/10
13/13 - 1s - loss: 134.4844 - loglik: -1.3266e+02 - logprior: -1.8275e+00
Epoch 7/10
13/13 - 1s - loss: 134.3234 - loglik: -1.3248e+02 - logprior: -1.8477e+00
Epoch 8/10
13/13 - 1s - loss: 133.5097 - loglik: -1.3168e+02 - logprior: -1.8347e+00
Epoch 9/10
13/13 - 1s - loss: 133.2169 - loglik: -1.3140e+02 - logprior: -1.8199e+00
Epoch 10/10
13/13 - 1s - loss: 132.7987 - loglik: -1.3097e+02 - logprior: -1.8273e+00
Fitted a model with MAP estimate = -132.7477
expansions: [(13, 1), (17, 5), (18, 1), (29, 1), (36, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 143.5404 - loglik: -1.3401e+02 - logprior: -9.5305e+00
Epoch 2/2
13/13 - 1s - loss: 129.5683 - loglik: -1.2519e+02 - logprior: -4.3738e+00
Fitted a model with MAP estimate = -127.7382
expansions: [(0, 2), (11, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.6721 - loglik: -1.2338e+02 - logprior: -7.2965e+00
Epoch 2/2
13/13 - 1s - loss: 124.0691 - loglik: -1.2191e+02 - logprior: -2.1616e+00
Fitted a model with MAP estimate = -122.4638
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 133.1227 - loglik: -1.2381e+02 - logprior: -9.3128e+00
Epoch 2/10
13/13 - 1s - loss: 124.5751 - loglik: -1.2132e+02 - logprior: -3.2523e+00
Epoch 3/10
13/13 - 1s - loss: 121.6586 - loglik: -1.2001e+02 - logprior: -1.6500e+00
Epoch 4/10
13/13 - 1s - loss: 121.1664 - loglik: -1.1990e+02 - logprior: -1.2619e+00
Epoch 5/10
13/13 - 1s - loss: 120.3486 - loglik: -1.1916e+02 - logprior: -1.1838e+00
Epoch 6/10
13/13 - 1s - loss: 119.8369 - loglik: -1.1864e+02 - logprior: -1.1961e+00
Epoch 7/10
13/13 - 1s - loss: 118.8483 - loglik: -1.1763e+02 - logprior: -1.2201e+00
Epoch 8/10
13/13 - 1s - loss: 119.5759 - loglik: -1.1839e+02 - logprior: -1.1901e+00
Fitted a model with MAP estimate = -118.9532
Time for alignment: 38.3030
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 193.6391 - loglik: -1.8554e+02 - logprior: -8.1000e+00
Epoch 2/10
13/13 - 1s - loss: 164.8616 - loglik: -1.6261e+02 - logprior: -2.2544e+00
Epoch 3/10
13/13 - 1s - loss: 148.0000 - loglik: -1.4613e+02 - logprior: -1.8749e+00
Epoch 4/10
13/13 - 1s - loss: 139.8422 - loglik: -1.3784e+02 - logprior: -1.9986e+00
Epoch 5/10
13/13 - 1s - loss: 136.7297 - loglik: -1.3481e+02 - logprior: -1.9175e+00
Epoch 6/10
13/13 - 1s - loss: 135.3710 - loglik: -1.3356e+02 - logprior: -1.8088e+00
Epoch 7/10
13/13 - 1s - loss: 134.2267 - loglik: -1.3240e+02 - logprior: -1.8317e+00
Epoch 8/10
13/13 - 1s - loss: 133.9292 - loglik: -1.3210e+02 - logprior: -1.8277e+00
Epoch 9/10
13/13 - 1s - loss: 133.4992 - loglik: -1.3168e+02 - logprior: -1.8158e+00
Epoch 10/10
13/13 - 1s - loss: 133.2590 - loglik: -1.3145e+02 - logprior: -1.8134e+00
Fitted a model with MAP estimate = -133.1300
expansions: [(12, 1), (13, 1), (17, 5), (18, 1), (29, 1), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 144.1327 - loglik: -1.3460e+02 - logprior: -9.5372e+00
Epoch 2/2
13/13 - 1s - loss: 129.4109 - loglik: -1.2498e+02 - logprior: -4.4267e+00
Fitted a model with MAP estimate = -127.3397
expansions: [(0, 2)]
discards: [ 0 56]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.6180 - loglik: -1.2331e+02 - logprior: -7.3049e+00
Epoch 2/2
13/13 - 1s - loss: 123.7442 - loglik: -1.2159e+02 - logprior: -2.1530e+00
Fitted a model with MAP estimate = -122.4982
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 133.2645 - loglik: -1.2394e+02 - logprior: -9.3272e+00
Epoch 2/10
13/13 - 1s - loss: 125.0300 - loglik: -1.2168e+02 - logprior: -3.3460e+00
Epoch 3/10
13/13 - 1s - loss: 122.3269 - loglik: -1.2066e+02 - logprior: -1.6675e+00
Epoch 4/10
13/13 - 1s - loss: 120.9678 - loglik: -1.1972e+02 - logprior: -1.2493e+00
Epoch 5/10
13/13 - 1s - loss: 119.9784 - loglik: -1.1880e+02 - logprior: -1.1750e+00
Epoch 6/10
13/13 - 1s - loss: 119.4055 - loglik: -1.1820e+02 - logprior: -1.2026e+00
Epoch 7/10
13/13 - 1s - loss: 119.6393 - loglik: -1.1844e+02 - logprior: -1.2026e+00
Fitted a model with MAP estimate = -119.0942
Time for alignment: 36.4881
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.6212 - loglik: -1.8552e+02 - logprior: -8.0990e+00
Epoch 2/10
13/13 - 1s - loss: 163.4270 - loglik: -1.6117e+02 - logprior: -2.2551e+00
Epoch 3/10
13/13 - 1s - loss: 146.5762 - loglik: -1.4463e+02 - logprior: -1.9467e+00
Epoch 4/10
13/13 - 1s - loss: 140.3263 - loglik: -1.3833e+02 - logprior: -1.9981e+00
Epoch 5/10
13/13 - 1s - loss: 137.0106 - loglik: -1.3516e+02 - logprior: -1.8493e+00
Epoch 6/10
13/13 - 1s - loss: 135.3418 - loglik: -1.3355e+02 - logprior: -1.7918e+00
Epoch 7/10
13/13 - 1s - loss: 134.5686 - loglik: -1.3274e+02 - logprior: -1.8322e+00
Epoch 8/10
13/13 - 1s - loss: 133.9379 - loglik: -1.3211e+02 - logprior: -1.8329e+00
Epoch 9/10
13/13 - 1s - loss: 133.5087 - loglik: -1.3169e+02 - logprior: -1.8145e+00
Epoch 10/10
13/13 - 1s - loss: 133.3584 - loglik: -1.3153e+02 - logprior: -1.8261e+00
Fitted a model with MAP estimate = -133.1335
expansions: [(13, 1), (17, 5), (18, 1), (29, 1), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 144.3036 - loglik: -1.3476e+02 - logprior: -9.5453e+00
Epoch 2/2
13/13 - 1s - loss: 129.7353 - loglik: -1.2532e+02 - logprior: -4.4189e+00
Fitted a model with MAP estimate = -127.9095
expansions: [(0, 2), (11, 1)]
discards: [ 0 55]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.9789 - loglik: -1.2368e+02 - logprior: -7.2943e+00
Epoch 2/2
13/13 - 1s - loss: 124.0926 - loglik: -1.2192e+02 - logprior: -2.1726e+00
Fitted a model with MAP estimate = -122.5275
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 133.3790 - loglik: -1.2405e+02 - logprior: -9.3338e+00
Epoch 2/10
13/13 - 1s - loss: 125.0765 - loglik: -1.2174e+02 - logprior: -3.3403e+00
Epoch 3/10
13/13 - 1s - loss: 121.8517 - loglik: -1.2018e+02 - logprior: -1.6673e+00
Epoch 4/10
13/13 - 1s - loss: 121.1741 - loglik: -1.1992e+02 - logprior: -1.2586e+00
Epoch 5/10
13/13 - 1s - loss: 120.1821 - loglik: -1.1900e+02 - logprior: -1.1785e+00
Epoch 6/10
13/13 - 1s - loss: 119.4528 - loglik: -1.1826e+02 - logprior: -1.1947e+00
Epoch 7/10
13/13 - 1s - loss: 119.6935 - loglik: -1.1848e+02 - logprior: -1.2139e+00
Fitted a model with MAP estimate = -119.1090
Time for alignment: 38.3506
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.3357 - loglik: -1.8523e+02 - logprior: -8.1016e+00
Epoch 2/10
13/13 - 1s - loss: 164.1287 - loglik: -1.6187e+02 - logprior: -2.2608e+00
Epoch 3/10
13/13 - 1s - loss: 146.3781 - loglik: -1.4448e+02 - logprior: -1.8973e+00
Epoch 4/10
13/13 - 1s - loss: 139.4470 - loglik: -1.3745e+02 - logprior: -1.9997e+00
Epoch 5/10
13/13 - 1s - loss: 136.4024 - loglik: -1.3449e+02 - logprior: -1.9152e+00
Epoch 6/10
13/13 - 1s - loss: 135.3339 - loglik: -1.3351e+02 - logprior: -1.8199e+00
Epoch 7/10
13/13 - 1s - loss: 134.2035 - loglik: -1.3237e+02 - logprior: -1.8381e+00
Epoch 8/10
13/13 - 1s - loss: 133.8472 - loglik: -1.3201e+02 - logprior: -1.8341e+00
Epoch 9/10
13/13 - 1s - loss: 133.4209 - loglik: -1.3160e+02 - logprior: -1.8251e+00
Epoch 10/10
13/13 - 1s - loss: 133.2531 - loglik: -1.3143e+02 - logprior: -1.8281e+00
Fitted a model with MAP estimate = -133.1294
expansions: [(12, 1), (13, 1), (17, 5), (18, 1), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 143.2700 - loglik: -1.3374e+02 - logprior: -9.5278e+00
Epoch 2/2
13/13 - 1s - loss: 129.1621 - loglik: -1.2478e+02 - logprior: -4.3794e+00
Fitted a model with MAP estimate = -127.2323
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 130.4945 - loglik: -1.2320e+02 - logprior: -7.2989e+00
Epoch 2/2
13/13 - 1s - loss: 123.5894 - loglik: -1.2144e+02 - logprior: -2.1510e+00
Fitted a model with MAP estimate = -122.4087
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 132.6057 - loglik: -1.2327e+02 - logprior: -9.3317e+00
Epoch 2/10
13/13 - 1s - loss: 125.2292 - loglik: -1.2190e+02 - logprior: -3.3274e+00
Epoch 3/10
13/13 - 1s - loss: 122.1141 - loglik: -1.2045e+02 - logprior: -1.6626e+00
Epoch 4/10
13/13 - 1s - loss: 121.0427 - loglik: -1.1977e+02 - logprior: -1.2715e+00
Epoch 5/10
13/13 - 1s - loss: 120.0685 - loglik: -1.1889e+02 - logprior: -1.1779e+00
Epoch 6/10
13/13 - 1s - loss: 119.8993 - loglik: -1.1869e+02 - logprior: -1.2125e+00
Epoch 7/10
13/13 - 1s - loss: 119.2657 - loglik: -1.1805e+02 - logprior: -1.2110e+00
Epoch 8/10
13/13 - 1s - loss: 119.1684 - loglik: -1.1797e+02 - logprior: -1.2009e+00
Epoch 9/10
13/13 - 1s - loss: 118.6913 - loglik: -1.1752e+02 - logprior: -1.1708e+00
Epoch 10/10
13/13 - 1s - loss: 118.9935 - loglik: -1.1784e+02 - logprior: -1.1507e+00
Fitted a model with MAP estimate = -118.7442
Time for alignment: 35.6646
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 193.7214 - loglik: -1.8562e+02 - logprior: -8.0996e+00
Epoch 2/10
13/13 - 1s - loss: 163.8102 - loglik: -1.6155e+02 - logprior: -2.2570e+00
Epoch 3/10
13/13 - 1s - loss: 147.5351 - loglik: -1.4565e+02 - logprior: -1.8895e+00
Epoch 4/10
13/13 - 1s - loss: 139.6559 - loglik: -1.3768e+02 - logprior: -1.9760e+00
Epoch 5/10
13/13 - 1s - loss: 136.9241 - loglik: -1.3506e+02 - logprior: -1.8664e+00
Epoch 6/10
13/13 - 1s - loss: 135.3780 - loglik: -1.3361e+02 - logprior: -1.7700e+00
Epoch 7/10
13/13 - 1s - loss: 134.7500 - loglik: -1.3296e+02 - logprior: -1.7891e+00
Epoch 8/10
13/13 - 1s - loss: 134.2184 - loglik: -1.3244e+02 - logprior: -1.7790e+00
Epoch 9/10
13/13 - 1s - loss: 134.1527 - loglik: -1.3239e+02 - logprior: -1.7654e+00
Epoch 10/10
13/13 - 1s - loss: 133.5039 - loglik: -1.3174e+02 - logprior: -1.7687e+00
Fitted a model with MAP estimate = -133.5319
expansions: [(12, 1), (13, 1), (16, 7), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 143.2597 - loglik: -1.3376e+02 - logprior: -9.4948e+00
Epoch 2/2
13/13 - 1s - loss: 129.2152 - loglik: -1.2487e+02 - logprior: -4.3426e+00
Fitted a model with MAP estimate = -127.1108
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.4280 - loglik: -1.2314e+02 - logprior: -7.2860e+00
Epoch 2/2
13/13 - 1s - loss: 124.1505 - loglik: -1.2198e+02 - logprior: -2.1736e+00
Fitted a model with MAP estimate = -122.9816
expansions: []
discards: [ 0 13]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 134.2615 - loglik: -1.2490e+02 - logprior: -9.3644e+00
Epoch 2/10
13/13 - 1s - loss: 125.2105 - loglik: -1.2178e+02 - logprior: -3.4291e+00
Epoch 3/10
13/13 - 1s - loss: 122.4101 - loglik: -1.2071e+02 - logprior: -1.6967e+00
Epoch 4/10
13/13 - 1s - loss: 121.0118 - loglik: -1.1974e+02 - logprior: -1.2709e+00
Epoch 5/10
13/13 - 1s - loss: 120.2076 - loglik: -1.1903e+02 - logprior: -1.1818e+00
Epoch 6/10
13/13 - 1s - loss: 119.7217 - loglik: -1.1851e+02 - logprior: -1.2125e+00
Epoch 7/10
13/13 - 1s - loss: 119.3276 - loglik: -1.1811e+02 - logprior: -1.2215e+00
Epoch 8/10
13/13 - 1s - loss: 118.8748 - loglik: -1.1768e+02 - logprior: -1.1941e+00
Epoch 9/10
13/13 - 1s - loss: 119.4134 - loglik: -1.1824e+02 - logprior: -1.1762e+00
Fitted a model with MAP estimate = -118.7985
Time for alignment: 37.7092
Computed alignments with likelihoods: ['-118.9532', '-119.0942', '-119.1090', '-118.7442', '-118.7985']
Best model has likelihood: -118.7442  (prior= -1.1358 )
time for generating output: 0.1135
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.9631728045325779
Training of 5 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead12e5d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f168d190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe98ae5abe0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 240.7937 - loglik: -2.2825e+02 - logprior: -1.2542e+01
Epoch 2/10
11/11 - 1s - loss: 208.4460 - loglik: -2.0516e+02 - logprior: -3.2846e+00
Epoch 3/10
11/11 - 1s - loss: 183.7453 - loglik: -1.8166e+02 - logprior: -2.0839e+00
Epoch 4/10
11/11 - 1s - loss: 167.4616 - loglik: -1.6585e+02 - logprior: -1.6068e+00
Epoch 5/10
11/11 - 1s - loss: 162.0664 - loglik: -1.6068e+02 - logprior: -1.3816e+00
Epoch 6/10
11/11 - 1s - loss: 160.1068 - loglik: -1.5881e+02 - logprior: -1.2954e+00
Epoch 7/10
11/11 - 1s - loss: 159.1692 - loglik: -1.5805e+02 - logprior: -1.1230e+00
Epoch 8/10
11/11 - 1s - loss: 158.3463 - loglik: -1.5735e+02 - logprior: -9.9503e-01
Epoch 9/10
11/11 - 1s - loss: 157.4169 - loglik: -1.5655e+02 - logprior: -8.6546e-01
Epoch 10/10
11/11 - 1s - loss: 157.0417 - loglik: -1.5625e+02 - logprior: -7.8791e-01
Fitted a model with MAP estimate = -156.9188
expansions: [(0, 6), (21, 1), (27, 1), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 171.0383 - loglik: -1.5542e+02 - logprior: -1.5615e+01
Epoch 2/2
11/11 - 1s - loss: 150.7302 - loglik: -1.4633e+02 - logprior: -4.3963e+00
Fitted a model with MAP estimate = -147.5813
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.5619 - loglik: -1.4844e+02 - logprior: -1.4121e+01
Epoch 2/2
11/11 - 1s - loss: 152.2115 - loglik: -1.4665e+02 - logprior: -5.5604e+00
Fitted a model with MAP estimate = -149.5549
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.6435 - loglik: -1.4633e+02 - logprior: -1.2309e+01
Epoch 2/10
11/11 - 1s - loss: 148.5770 - loglik: -1.4533e+02 - logprior: -3.2498e+00
Epoch 3/10
11/11 - 1s - loss: 146.0652 - loglik: -1.4435e+02 - logprior: -1.7153e+00
Epoch 4/10
11/11 - 1s - loss: 145.0947 - loglik: -1.4406e+02 - logprior: -1.0333e+00
Epoch 5/10
11/11 - 1s - loss: 143.9147 - loglik: -1.4307e+02 - logprior: -8.4677e-01
Epoch 6/10
11/11 - 1s - loss: 143.9828 - loglik: -1.4331e+02 - logprior: -6.6971e-01
Fitted a model with MAP estimate = -143.5937
Time for alignment: 35.9525
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 241.0312 - loglik: -2.2849e+02 - logprior: -1.2543e+01
Epoch 2/10
11/11 - 1s - loss: 208.7736 - loglik: -2.0549e+02 - logprior: -3.2842e+00
Epoch 3/10
11/11 - 1s - loss: 184.7587 - loglik: -1.8267e+02 - logprior: -2.0893e+00
Epoch 4/10
11/11 - 1s - loss: 168.8071 - loglik: -1.6719e+02 - logprior: -1.6192e+00
Epoch 5/10
11/11 - 1s - loss: 163.6704 - loglik: -1.6227e+02 - logprior: -1.3969e+00
Epoch 6/10
11/11 - 1s - loss: 161.1894 - loglik: -1.5988e+02 - logprior: -1.3105e+00
Epoch 7/10
11/11 - 1s - loss: 160.2103 - loglik: -1.5908e+02 - logprior: -1.1352e+00
Epoch 8/10
11/11 - 1s - loss: 159.6739 - loglik: -1.5864e+02 - logprior: -1.0350e+00
Epoch 9/10
11/11 - 1s - loss: 158.5450 - loglik: -1.5762e+02 - logprior: -9.2170e-01
Epoch 10/10
11/11 - 1s - loss: 158.4324 - loglik: -1.5761e+02 - logprior: -8.1941e-01
Fitted a model with MAP estimate = -158.0497
expansions: [(0, 6), (21, 1), (23, 2), (29, 2), (46, 1), (47, 1), (48, 1), (50, 1), (58, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 172.5523 - loglik: -1.5696e+02 - logprior: -1.5590e+01
Epoch 2/2
11/11 - 1s - loss: 151.4285 - loglik: -1.4698e+02 - logprior: -4.4448e+00
Fitted a model with MAP estimate = -147.8060
expansions: []
discards: [ 0 30 38]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.7849 - loglik: -1.4866e+02 - logprior: -1.4126e+01
Epoch 2/2
11/11 - 1s - loss: 151.8862 - loglik: -1.4633e+02 - logprior: -5.5525e+00
Fitted a model with MAP estimate = -149.5556
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.6428 - loglik: -1.4636e+02 - logprior: -1.2285e+01
Epoch 2/10
11/11 - 1s - loss: 148.5042 - loglik: -1.4525e+02 - logprior: -3.2548e+00
Epoch 3/10
11/11 - 1s - loss: 146.3663 - loglik: -1.4466e+02 - logprior: -1.7036e+00
Epoch 4/10
11/11 - 1s - loss: 144.7267 - loglik: -1.4368e+02 - logprior: -1.0448e+00
Epoch 5/10
11/11 - 1s - loss: 144.4634 - loglik: -1.4363e+02 - logprior: -8.3079e-01
Epoch 6/10
11/11 - 1s - loss: 143.8661 - loglik: -1.4319e+02 - logprior: -6.7617e-01
Epoch 7/10
11/11 - 1s - loss: 143.8279 - loglik: -1.4322e+02 - logprior: -6.0683e-01
Epoch 8/10
11/11 - 1s - loss: 143.0725 - loglik: -1.4251e+02 - logprior: -5.6507e-01
Epoch 9/10
11/11 - 1s - loss: 143.4485 - loglik: -1.4289e+02 - logprior: -5.5380e-01
Fitted a model with MAP estimate = -143.1267
Time for alignment: 37.0834
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 240.9245 - loglik: -2.2838e+02 - logprior: -1.2545e+01
Epoch 2/10
11/11 - 1s - loss: 208.0928 - loglik: -2.0482e+02 - logprior: -3.2733e+00
Epoch 3/10
11/11 - 1s - loss: 180.8527 - loglik: -1.7882e+02 - logprior: -2.0306e+00
Epoch 4/10
11/11 - 1s - loss: 166.5916 - loglik: -1.6506e+02 - logprior: -1.5318e+00
Epoch 5/10
11/11 - 1s - loss: 161.7041 - loglik: -1.6038e+02 - logprior: -1.3231e+00
Epoch 6/10
11/11 - 1s - loss: 160.1753 - loglik: -1.5893e+02 - logprior: -1.2443e+00
Epoch 7/10
11/11 - 1s - loss: 158.3666 - loglik: -1.5731e+02 - logprior: -1.0609e+00
Epoch 8/10
11/11 - 1s - loss: 157.7025 - loglik: -1.5679e+02 - logprior: -9.1064e-01
Epoch 9/10
11/11 - 1s - loss: 157.0709 - loglik: -1.5625e+02 - logprior: -8.2494e-01
Epoch 10/10
11/11 - 1s - loss: 156.9278 - loglik: -1.5617e+02 - logprior: -7.5797e-01
Fitted a model with MAP estimate = -156.6637
expansions: [(0, 6), (22, 1), (23, 2), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 172.4012 - loglik: -1.5680e+02 - logprior: -1.5598e+01
Epoch 2/2
11/11 - 1s - loss: 151.4804 - loglik: -1.4699e+02 - logprior: -4.4940e+00
Fitted a model with MAP estimate = -148.0434
expansions: []
discards: [ 0 30 38 43]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.8617 - loglik: -1.4872e+02 - logprior: -1.4137e+01
Epoch 2/2
11/11 - 1s - loss: 151.9682 - loglik: -1.4641e+02 - logprior: -5.5562e+00
Fitted a model with MAP estimate = -149.5834
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.7794 - loglik: -1.4647e+02 - logprior: -1.2306e+01
Epoch 2/10
11/11 - 1s - loss: 148.7413 - loglik: -1.4548e+02 - logprior: -3.2569e+00
Epoch 3/10
11/11 - 1s - loss: 145.9084 - loglik: -1.4420e+02 - logprior: -1.7075e+00
Epoch 4/10
11/11 - 1s - loss: 144.9007 - loglik: -1.4385e+02 - logprior: -1.0460e+00
Epoch 5/10
11/11 - 1s - loss: 143.9816 - loglik: -1.4314e+02 - logprior: -8.3842e-01
Epoch 6/10
11/11 - 1s - loss: 143.9792 - loglik: -1.4330e+02 - logprior: -6.7732e-01
Epoch 7/10
11/11 - 1s - loss: 143.6200 - loglik: -1.4301e+02 - logprior: -6.1313e-01
Epoch 8/10
11/11 - 1s - loss: 143.2203 - loglik: -1.4265e+02 - logprior: -5.6589e-01
Epoch 9/10
11/11 - 1s - loss: 143.4187 - loglik: -1.4287e+02 - logprior: -5.4447e-01
Fitted a model with MAP estimate = -143.0855
Time for alignment: 37.0404
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 240.9821 - loglik: -2.2844e+02 - logprior: -1.2543e+01
Epoch 2/10
11/11 - 1s - loss: 208.2413 - loglik: -2.0496e+02 - logprior: -3.2833e+00
Epoch 3/10
11/11 - 1s - loss: 183.1607 - loglik: -1.8108e+02 - logprior: -2.0852e+00
Epoch 4/10
11/11 - 1s - loss: 167.1825 - loglik: -1.6558e+02 - logprior: -1.6007e+00
Epoch 5/10
11/11 - 1s - loss: 162.2943 - loglik: -1.6092e+02 - logprior: -1.3781e+00
Epoch 6/10
11/11 - 1s - loss: 160.4059 - loglik: -1.5911e+02 - logprior: -1.2992e+00
Epoch 7/10
11/11 - 1s - loss: 158.8997 - loglik: -1.5777e+02 - logprior: -1.1331e+00
Epoch 8/10
11/11 - 1s - loss: 158.1131 - loglik: -1.5711e+02 - logprior: -1.0040e+00
Epoch 9/10
11/11 - 1s - loss: 157.6148 - loglik: -1.5675e+02 - logprior: -8.6269e-01
Epoch 10/10
11/11 - 1s - loss: 157.0337 - loglik: -1.5625e+02 - logprior: -7.8733e-01
Fitted a model with MAP estimate = -156.8809
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 170.8524 - loglik: -1.5523e+02 - logprior: -1.5620e+01
Epoch 2/2
11/11 - 1s - loss: 150.7904 - loglik: -1.4639e+02 - logprior: -4.3967e+00
Fitted a model with MAP estimate = -147.5798
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.5915 - loglik: -1.4847e+02 - logprior: -1.4117e+01
Epoch 2/2
11/11 - 1s - loss: 152.1395 - loglik: -1.4658e+02 - logprior: -5.5557e+00
Fitted a model with MAP estimate = -149.5176
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.7074 - loglik: -1.4642e+02 - logprior: -1.2288e+01
Epoch 2/10
11/11 - 1s - loss: 148.6684 - loglik: -1.4542e+02 - logprior: -3.2523e+00
Epoch 3/10
11/11 - 1s - loss: 146.0859 - loglik: -1.4438e+02 - logprior: -1.7055e+00
Epoch 4/10
11/11 - 1s - loss: 144.8366 - loglik: -1.4379e+02 - logprior: -1.0485e+00
Epoch 5/10
11/11 - 1s - loss: 144.3881 - loglik: -1.4355e+02 - logprior: -8.3733e-01
Epoch 6/10
11/11 - 1s - loss: 143.8635 - loglik: -1.4318e+02 - logprior: -6.8171e-01
Epoch 7/10
11/11 - 1s - loss: 143.4989 - loglik: -1.4289e+02 - logprior: -6.0491e-01
Epoch 8/10
11/11 - 1s - loss: 143.2787 - loglik: -1.4271e+02 - logprior: -5.6910e-01
Epoch 9/10
11/11 - 1s - loss: 143.3098 - loglik: -1.4276e+02 - logprior: -5.5230e-01
Fitted a model with MAP estimate = -143.0980
Time for alignment: 36.9039
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 240.9050 - loglik: -2.2836e+02 - logprior: -1.2542e+01
Epoch 2/10
11/11 - 1s - loss: 208.6836 - loglik: -2.0540e+02 - logprior: -3.2861e+00
Epoch 3/10
11/11 - 1s - loss: 184.2821 - loglik: -1.8221e+02 - logprior: -2.0671e+00
Epoch 4/10
11/11 - 1s - loss: 168.0747 - loglik: -1.6651e+02 - logprior: -1.5654e+00
Epoch 5/10
11/11 - 1s - loss: 162.4397 - loglik: -1.6111e+02 - logprior: -1.3323e+00
Epoch 6/10
11/11 - 1s - loss: 160.2700 - loglik: -1.5901e+02 - logprior: -1.2627e+00
Epoch 7/10
11/11 - 1s - loss: 158.5045 - loglik: -1.5743e+02 - logprior: -1.0777e+00
Epoch 8/10
11/11 - 1s - loss: 157.7858 - loglik: -1.5686e+02 - logprior: -9.2565e-01
Epoch 9/10
11/11 - 1s - loss: 157.3168 - loglik: -1.5649e+02 - logprior: -8.2434e-01
Epoch 10/10
11/11 - 1s - loss: 156.8116 - loglik: -1.5606e+02 - logprior: -7.5333e-01
Fitted a model with MAP estimate = -156.6971
expansions: [(0, 6), (21, 1), (23, 2), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 171.7490 - loglik: -1.5614e+02 - logprior: -1.5606e+01
Epoch 2/2
11/11 - 1s - loss: 151.0947 - loglik: -1.4665e+02 - logprior: -4.4454e+00
Fitted a model with MAP estimate = -147.7034
expansions: []
discards: [ 0 30 38]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.6920 - loglik: -1.4857e+02 - logprior: -1.4127e+01
Epoch 2/2
11/11 - 1s - loss: 152.0704 - loglik: -1.4651e+02 - logprior: -5.5570e+00
Fitted a model with MAP estimate = -149.5712
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 158.7887 - loglik: -1.4649e+02 - logprior: -1.2300e+01
Epoch 2/10
11/11 - 1s - loss: 148.8450 - loglik: -1.4559e+02 - logprior: -3.2557e+00
Epoch 3/10
11/11 - 1s - loss: 145.9336 - loglik: -1.4423e+02 - logprior: -1.7073e+00
Epoch 4/10
11/11 - 1s - loss: 144.6932 - loglik: -1.4365e+02 - logprior: -1.0472e+00
Epoch 5/10
11/11 - 1s - loss: 144.4694 - loglik: -1.4363e+02 - logprior: -8.3455e-01
Epoch 6/10
11/11 - 1s - loss: 144.0050 - loglik: -1.4333e+02 - logprior: -6.7734e-01
Epoch 7/10
11/11 - 1s - loss: 143.3918 - loglik: -1.4278e+02 - logprior: -6.1545e-01
Epoch 8/10
11/11 - 1s - loss: 143.5259 - loglik: -1.4296e+02 - logprior: -5.6712e-01
Fitted a model with MAP estimate = -143.2371
Time for alignment: 36.1490
Computed alignments with likelihoods: ['-143.5937', '-143.1267', '-143.0855', '-143.0980', '-143.2371']
Best model has likelihood: -143.0855  (prior= -0.5406 )
time for generating output: 0.1257
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 5 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe98b3cbe50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe993a37070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaaeedaa60>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 526.2079 - loglik: -5.2089e+02 - logprior: -5.3220e+00
Epoch 2/10
24/24 - 6s - loss: 376.9998 - loglik: -3.7481e+02 - logprior: -2.1855e+00
Epoch 3/10
24/24 - 6s - loss: 350.6521 - loglik: -3.4809e+02 - logprior: -2.5654e+00
Epoch 4/10
24/24 - 6s - loss: 346.3332 - loglik: -3.4393e+02 - logprior: -2.3996e+00
Epoch 5/10
24/24 - 6s - loss: 344.8982 - loglik: -3.4248e+02 - logprior: -2.4145e+00
Epoch 6/10
24/24 - 6s - loss: 341.4779 - loglik: -3.3903e+02 - logprior: -2.4494e+00
Epoch 7/10
24/24 - 6s - loss: 344.2208 - loglik: -3.4172e+02 - logprior: -2.5000e+00
Fitted a model with MAP estimate = -342.5462
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (66, 1), (76, 1), (78, 1), (87, 1), (88, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 3), (174, 1), (175, 1), (188, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 336.4657 - loglik: -3.2922e+02 - logprior: -7.2463e+00
Epoch 2/2
24/24 - 8s - loss: 315.7479 - loglik: -3.1329e+02 - logprior: -2.4584e+00
Fitted a model with MAP estimate = -314.8269
expansions: [(0, 3), (190, 1), (192, 1)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 317.0587 - loglik: -3.1255e+02 - logprior: -4.5096e+00
Epoch 2/2
24/24 - 8s - loss: 309.2740 - loglik: -3.0921e+02 - logprior: -6.0876e-02
Fitted a model with MAP estimate = -308.4821
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 320.4873 - loglik: -3.1398e+02 - logprior: -6.5088e+00
Epoch 2/10
24/24 - 8s - loss: 312.6344 - loglik: -3.1172e+02 - logprior: -9.1768e-01
Epoch 3/10
24/24 - 8s - loss: 309.7539 - loglik: -3.1074e+02 - logprior: 0.9885
Epoch 4/10
24/24 - 8s - loss: 308.7519 - loglik: -3.0969e+02 - logprior: 0.9373
Epoch 5/10
24/24 - 8s - loss: 306.6361 - loglik: -3.0766e+02 - logprior: 1.0282
Epoch 6/10
24/24 - 8s - loss: 305.5510 - loglik: -3.0669e+02 - logprior: 1.1394
Epoch 7/10
24/24 - 8s - loss: 307.3101 - loglik: -3.0855e+02 - logprior: 1.2438
Fitted a model with MAP estimate = -305.6218
Time for alignment: 169.7605
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 523.8224 - loglik: -5.1850e+02 - logprior: -5.3265e+00
Epoch 2/10
24/24 - 6s - loss: 377.3252 - loglik: -3.7524e+02 - logprior: -2.0897e+00
Epoch 3/10
24/24 - 6s - loss: 350.1023 - loglik: -3.4776e+02 - logprior: -2.3436e+00
Epoch 4/10
24/24 - 6s - loss: 344.6659 - loglik: -3.4246e+02 - logprior: -2.2016e+00
Epoch 5/10
24/24 - 6s - loss: 345.9630 - loglik: -3.4373e+02 - logprior: -2.2313e+00
Fitted a model with MAP estimate = -343.4687
expansions: [(12, 3), (13, 2), (15, 1), (16, 1), (17, 1), (18, 2), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (66, 1), (76, 1), (83, 1), (85, 1), (88, 1), (91, 1), (111, 1), (114, 1), (117, 1), (118, 1), (119, 2), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (153, 2), (154, 4), (155, 1), (158, 1), (171, 1), (172, 1), (173, 3), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 335.0479 - loglik: -3.2783e+02 - logprior: -7.2185e+00
Epoch 2/2
24/24 - 8s - loss: 316.6906 - loglik: -3.1428e+02 - logprior: -2.4088e+00
Fitted a model with MAP estimate = -313.9271
expansions: [(0, 3), (192, 1), (194, 1)]
discards: [  0  24 147]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 316.8980 - loglik: -3.1242e+02 - logprior: -4.4817e+00
Epoch 2/2
24/24 - 8s - loss: 308.9146 - loglik: -3.0888e+02 - logprior: -3.2906e-02
Fitted a model with MAP estimate = -308.2120
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 320.3262 - loglik: -3.1381e+02 - logprior: -6.5178e+00
Epoch 2/10
24/24 - 8s - loss: 312.5373 - loglik: -3.1163e+02 - logprior: -9.1200e-01
Epoch 3/10
24/24 - 8s - loss: 309.7347 - loglik: -3.1081e+02 - logprior: 1.0704
Epoch 4/10
24/24 - 8s - loss: 307.3515 - loglik: -3.0846e+02 - logprior: 1.1086
Epoch 5/10
24/24 - 8s - loss: 309.5305 - loglik: -3.1074e+02 - logprior: 1.2110
Fitted a model with MAP estimate = -306.7193
Time for alignment: 144.0134
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 526.4286 - loglik: -5.2108e+02 - logprior: -5.3527e+00
Epoch 2/10
24/24 - 6s - loss: 375.1620 - loglik: -3.7295e+02 - logprior: -2.2141e+00
Epoch 3/10
24/24 - 6s - loss: 349.6750 - loglik: -3.4717e+02 - logprior: -2.5085e+00
Epoch 4/10
24/24 - 6s - loss: 345.2751 - loglik: -3.4288e+02 - logprior: -2.3957e+00
Epoch 5/10
24/24 - 6s - loss: 343.6654 - loglik: -3.4129e+02 - logprior: -2.3747e+00
Epoch 6/10
24/24 - 6s - loss: 342.9901 - loglik: -3.4061e+02 - logprior: -2.3818e+00
Epoch 7/10
24/24 - 6s - loss: 342.4929 - loglik: -3.4008e+02 - logprior: -2.4100e+00
Epoch 8/10
24/24 - 6s - loss: 342.6997 - loglik: -3.4026e+02 - logprior: -2.4420e+00
Fitted a model with MAP estimate = -341.9317
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 1), (18, 2), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (76, 1), (82, 1), (87, 1), (89, 1), (90, 1), (108, 1), (109, 1), (112, 1), (115, 1), (118, 2), (119, 2), (121, 1), (149, 1), (152, 1), (153, 4), (157, 1), (171, 1), (172, 1), (173, 3), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 335.4513 - loglik: -3.2815e+02 - logprior: -7.3030e+00
Epoch 2/2
24/24 - 8s - loss: 317.5355 - loglik: -3.1505e+02 - logprior: -2.4805e+00
Fitted a model with MAP estimate = -314.4669
expansions: [(0, 3), (191, 1), (193, 1)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 317.4937 - loglik: -3.1299e+02 - logprior: -4.5028e+00
Epoch 2/2
24/24 - 8s - loss: 308.0490 - loglik: -3.0801e+02 - logprior: -3.9862e-02
Fitted a model with MAP estimate = -308.1721
expansions: [(152, 1)]
discards: [ 0  1 26]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 320.4840 - loglik: -3.1396e+02 - logprior: -6.5223e+00
Epoch 2/10
24/24 - 8s - loss: 311.5659 - loglik: -3.1057e+02 - logprior: -9.9478e-01
Epoch 3/10
24/24 - 8s - loss: 309.6454 - loglik: -3.1056e+02 - logprior: 0.9179
Epoch 4/10
24/24 - 8s - loss: 308.6741 - loglik: -3.0961e+02 - logprior: 0.9373
Epoch 5/10
24/24 - 8s - loss: 307.1438 - loglik: -3.0821e+02 - logprior: 1.0658
Epoch 6/10
24/24 - 8s - loss: 306.1793 - loglik: -3.0739e+02 - logprior: 1.2144
Epoch 7/10
24/24 - 8s - loss: 306.9572 - loglik: -3.0833e+02 - logprior: 1.3740
Fitted a model with MAP estimate = -305.7784
Time for alignment: 174.1594
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 525.7446 - loglik: -5.2044e+02 - logprior: -5.3088e+00
Epoch 2/10
24/24 - 6s - loss: 380.2842 - loglik: -3.7816e+02 - logprior: -2.1260e+00
Epoch 3/10
24/24 - 6s - loss: 349.3799 - loglik: -3.4686e+02 - logprior: -2.5208e+00
Epoch 4/10
24/24 - 6s - loss: 346.1231 - loglik: -3.4367e+02 - logprior: -2.4497e+00
Epoch 5/10
24/24 - 6s - loss: 344.9072 - loglik: -3.4246e+02 - logprior: -2.4486e+00
Epoch 6/10
24/24 - 6s - loss: 344.6420 - loglik: -3.4219e+02 - logprior: -2.4561e+00
Epoch 7/10
24/24 - 6s - loss: 342.5229 - loglik: -3.4003e+02 - logprior: -2.4905e+00
Epoch 8/10
24/24 - 6s - loss: 342.8428 - loglik: -3.4033e+02 - logprior: -2.5101e+00
Fitted a model with MAP estimate = -342.7034
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (75, 1), (82, 1), (87, 1), (89, 1), (90, 1), (108, 1), (109, 1), (112, 1), (114, 1), (118, 1), (119, 1), (120, 1), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 4), (174, 1), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 335.8186 - loglik: -3.2860e+02 - logprior: -7.2178e+00
Epoch 2/2
24/24 - 8s - loss: 317.2348 - loglik: -3.1480e+02 - logprior: -2.4391e+00
Fitted a model with MAP estimate = -315.1214
expansions: [(0, 3), (191, 1), (193, 1)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 316.6889 - loglik: -3.1220e+02 - logprior: -4.4839e+00
Epoch 2/2
24/24 - 8s - loss: 310.7271 - loglik: -3.1071e+02 - logprior: -1.4395e-02
Fitted a model with MAP estimate = -308.7987
expansions: []
discards: [ 0  1 26]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 320.4415 - loglik: -3.1395e+02 - logprior: -6.4953e+00
Epoch 2/10
24/24 - 8s - loss: 313.8884 - loglik: -3.1301e+02 - logprior: -8.7844e-01
Epoch 3/10
24/24 - 8s - loss: 311.0534 - loglik: -3.1217e+02 - logprior: 1.1169
Epoch 4/10
24/24 - 8s - loss: 309.2607 - loglik: -3.1038e+02 - logprior: 1.1229
Epoch 5/10
24/24 - 8s - loss: 307.7185 - loglik: -3.0896e+02 - logprior: 1.2424
Epoch 6/10
24/24 - 8s - loss: 308.0558 - loglik: -3.0935e+02 - logprior: 1.2947
Fitted a model with MAP estimate = -307.2563
Time for alignment: 166.0263
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 525.0233 - loglik: -5.1968e+02 - logprior: -5.3391e+00
Epoch 2/10
24/24 - 6s - loss: 376.2581 - loglik: -3.7405e+02 - logprior: -2.2104e+00
Epoch 3/10
24/24 - 6s - loss: 348.7351 - loglik: -3.4620e+02 - logprior: -2.5394e+00
Epoch 4/10
24/24 - 6s - loss: 346.0202 - loglik: -3.4364e+02 - logprior: -2.3801e+00
Epoch 5/10
24/24 - 6s - loss: 344.4408 - loglik: -3.4207e+02 - logprior: -2.3736e+00
Epoch 6/10
24/24 - 6s - loss: 343.5241 - loglik: -3.4115e+02 - logprior: -2.3721e+00
Epoch 7/10
24/24 - 6s - loss: 342.6735 - loglik: -3.4029e+02 - logprior: -2.3818e+00
Epoch 8/10
24/24 - 6s - loss: 341.7305 - loglik: -3.3933e+02 - logprior: -2.4001e+00
Epoch 9/10
24/24 - 6s - loss: 343.0676 - loglik: -3.4062e+02 - logprior: -2.4497e+00
Fitted a model with MAP estimate = -341.8655
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (76, 1), (82, 1), (88, 1), (90, 1), (93, 1), (110, 1), (113, 1), (118, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (155, 2), (158, 1), (173, 1), (174, 3), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 335.4692 - loglik: -3.2823e+02 - logprior: -7.2391e+00
Epoch 2/2
24/24 - 8s - loss: 316.9532 - loglik: -3.1454e+02 - logprior: -2.4112e+00
Fitted a model with MAP estimate = -314.9394
expansions: [(0, 3)]
discards: [ 0 12]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 316.1339 - loglik: -3.1160e+02 - logprior: -4.5367e+00
Epoch 2/2
24/24 - 8s - loss: 311.5489 - loglik: -3.1148e+02 - logprior: -7.1050e-02
Fitted a model with MAP estimate = -309.3391
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 10s - loss: 321.5207 - loglik: -3.1500e+02 - logprior: -6.5172e+00
Epoch 2/10
24/24 - 8s - loss: 313.2912 - loglik: -3.1229e+02 - logprior: -1.0028e+00
Epoch 3/10
24/24 - 8s - loss: 310.1248 - loglik: -3.1108e+02 - logprior: 0.9597
Epoch 4/10
24/24 - 8s - loss: 309.4803 - loglik: -3.1042e+02 - logprior: 0.9414
Epoch 5/10
24/24 - 8s - loss: 308.0858 - loglik: -3.0914e+02 - logprior: 1.0554
Epoch 6/10
24/24 - 8s - loss: 308.1951 - loglik: -3.0939e+02 - logprior: 1.1946
Fitted a model with MAP estimate = -307.1874
Time for alignment: 168.8543
Computed alignments with likelihoods: ['-305.6218', '-306.7193', '-305.7784', '-307.2563', '-307.1874']
Best model has likelihood: -305.6218  (prior= 1.3119 )
time for generating output: 0.2698
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.97874072733852
Training of 5 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb27f13eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feada67f400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1707340>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 303.7811 - loglik: -2.7670e+02 - logprior: -2.7080e+01
Epoch 2/10
10/10 - 1s - loss: 244.6575 - loglik: -2.3774e+02 - logprior: -6.9188e+00
Epoch 3/10
10/10 - 1s - loss: 205.3399 - loglik: -2.0170e+02 - logprior: -3.6444e+00
Epoch 4/10
10/10 - 1s - loss: 187.9889 - loglik: -1.8529e+02 - logprior: -2.7009e+00
Epoch 5/10
10/10 - 1s - loss: 180.0169 - loglik: -1.7780e+02 - logprior: -2.2143e+00
Epoch 6/10
10/10 - 1s - loss: 176.2279 - loglik: -1.7440e+02 - logprior: -1.8230e+00
Epoch 7/10
10/10 - 1s - loss: 174.1999 - loglik: -1.7260e+02 - logprior: -1.6004e+00
Epoch 8/10
10/10 - 1s - loss: 173.1861 - loglik: -1.7160e+02 - logprior: -1.5823e+00
Epoch 9/10
10/10 - 1s - loss: 172.6930 - loglik: -1.7116e+02 - logprior: -1.5368e+00
Epoch 10/10
10/10 - 1s - loss: 171.7412 - loglik: -1.7024e+02 - logprior: -1.5060e+00
Fitted a model with MAP estimate = -171.3989
expansions: [(0, 2), (11, 2), (12, 1), (34, 1), (36, 4), (37, 1), (48, 3), (49, 2), (69, 2), (70, 2), (71, 2), (72, 1), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.1392 - loglik: -1.6870e+02 - logprior: -3.4443e+01
Epoch 2/2
10/10 - 1s - loss: 169.1282 - loglik: -1.5887e+02 - logprior: -1.0255e+01
Fitted a model with MAP estimate = -161.6253
expansions: []
discards: [13]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.2860 - loglik: -1.5589e+02 - logprior: -2.5399e+01
Epoch 2/2
10/10 - 1s - loss: 160.3067 - loglik: -1.5387e+02 - logprior: -6.4403e+00
Fitted a model with MAP estimate = -156.9293
expansions: []
discards: [87]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 178.6319 - loglik: -1.5448e+02 - logprior: -2.4148e+01
Epoch 2/10
10/10 - 1s - loss: 159.5910 - loglik: -1.5350e+02 - logprior: -6.0956e+00
Epoch 3/10
10/10 - 1s - loss: 154.7363 - loglik: -1.5221e+02 - logprior: -2.5298e+00
Epoch 4/10
10/10 - 1s - loss: 152.6592 - loglik: -1.5148e+02 - logprior: -1.1820e+00
Epoch 5/10
10/10 - 1s - loss: 151.2721 - loglik: -1.5071e+02 - logprior: -5.6651e-01
Epoch 6/10
10/10 - 1s - loss: 150.7094 - loglik: -1.5046e+02 - logprior: -2.5147e-01
Epoch 7/10
10/10 - 1s - loss: 149.8779 - loglik: -1.4987e+02 - logprior: -1.0564e-02
Epoch 8/10
10/10 - 1s - loss: 149.2944 - loglik: -1.4949e+02 - logprior: 0.1976
Epoch 9/10
10/10 - 1s - loss: 149.9402 - loglik: -1.5030e+02 - logprior: 0.3574
Fitted a model with MAP estimate = -149.3871
Time for alignment: 41.7817
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 304.0554 - loglik: -2.7698e+02 - logprior: -2.7077e+01
Epoch 2/10
10/10 - 1s - loss: 245.5620 - loglik: -2.3864e+02 - logprior: -6.9225e+00
Epoch 3/10
10/10 - 1s - loss: 206.6303 - loglik: -2.0296e+02 - logprior: -3.6658e+00
Epoch 4/10
10/10 - 1s - loss: 187.2834 - loglik: -1.8450e+02 - logprior: -2.7841e+00
Epoch 5/10
10/10 - 1s - loss: 179.1289 - loglik: -1.7676e+02 - logprior: -2.3726e+00
Epoch 6/10
10/10 - 1s - loss: 175.9322 - loglik: -1.7400e+02 - logprior: -1.9274e+00
Epoch 7/10
10/10 - 1s - loss: 174.2469 - loglik: -1.7260e+02 - logprior: -1.6482e+00
Epoch 8/10
10/10 - 1s - loss: 173.8608 - loglik: -1.7225e+02 - logprior: -1.6069e+00
Epoch 9/10
10/10 - 1s - loss: 172.2384 - loglik: -1.7064e+02 - logprior: -1.6026e+00
Epoch 10/10
10/10 - 1s - loss: 171.9347 - loglik: -1.7034e+02 - logprior: -1.5911e+00
Fitted a model with MAP estimate = -171.8736
expansions: [(0, 2), (1, 1), (11, 2), (12, 1), (34, 1), (36, 4), (50, 2), (67, 1), (69, 2), (70, 1), (71, 2), (72, 1), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.4041 - loglik: -1.7080e+02 - logprior: -3.4603e+01
Epoch 2/2
10/10 - 1s - loss: 171.2661 - loglik: -1.6094e+02 - logprior: -1.0323e+01
Fitted a model with MAP estimate = -164.8211
expansions: [(48, 1), (59, 2)]
discards: [ 0  1 14 88]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 185.2413 - loglik: -1.5957e+02 - logprior: -2.5667e+01
Epoch 2/2
10/10 - 1s - loss: 163.9256 - loglik: -1.5734e+02 - logprior: -6.5858e+00
Fitted a model with MAP estimate = -159.6318
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 180.4044 - loglik: -1.5624e+02 - logprior: -2.4166e+01
Epoch 2/10
10/10 - 1s - loss: 160.3306 - loglik: -1.5420e+02 - logprior: -6.1344e+00
Epoch 3/10
10/10 - 1s - loss: 156.1688 - loglik: -1.5360e+02 - logprior: -2.5692e+00
Epoch 4/10
10/10 - 1s - loss: 153.5739 - loglik: -1.5236e+02 - logprior: -1.2098e+00
Epoch 5/10
10/10 - 1s - loss: 153.1732 - loglik: -1.5261e+02 - logprior: -5.6531e-01
Epoch 6/10
10/10 - 1s - loss: 151.6150 - loglik: -1.5138e+02 - logprior: -2.3739e-01
Epoch 7/10
10/10 - 1s - loss: 151.5680 - loglik: -1.5155e+02 - logprior: -1.9707e-02
Epoch 8/10
10/10 - 1s - loss: 150.8837 - loglik: -1.5106e+02 - logprior: 0.1804
Epoch 9/10
10/10 - 1s - loss: 151.7470 - loglik: -1.5208e+02 - logprior: 0.3363
Fitted a model with MAP estimate = -150.9161
Time for alignment: 38.3300
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.9233 - loglik: -2.7685e+02 - logprior: -2.7077e+01
Epoch 2/10
10/10 - 1s - loss: 244.7796 - loglik: -2.3787e+02 - logprior: -6.9140e+00
Epoch 3/10
10/10 - 1s - loss: 205.9112 - loglik: -2.0227e+02 - logprior: -3.6417e+00
Epoch 4/10
10/10 - 1s - loss: 185.7039 - loglik: -1.8305e+02 - logprior: -2.6587e+00
Epoch 5/10
10/10 - 1s - loss: 179.1865 - loglik: -1.7701e+02 - logprior: -2.1774e+00
Epoch 6/10
10/10 - 1s - loss: 175.9215 - loglik: -1.7418e+02 - logprior: -1.7373e+00
Epoch 7/10
10/10 - 1s - loss: 174.4475 - loglik: -1.7295e+02 - logprior: -1.4976e+00
Epoch 8/10
10/10 - 1s - loss: 173.5312 - loglik: -1.7207e+02 - logprior: -1.4569e+00
Epoch 9/10
10/10 - 1s - loss: 172.4707 - loglik: -1.7104e+02 - logprior: -1.4343e+00
Epoch 10/10
10/10 - 1s - loss: 172.3503 - loglik: -1.7093e+02 - logprior: -1.4224e+00
Fitted a model with MAP estimate = -171.9654
expansions: [(0, 2), (11, 2), (12, 1), (34, 1), (36, 4), (37, 1), (48, 3), (49, 2), (69, 2), (70, 1), (72, 3), (73, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 204.2720 - loglik: -1.6982e+02 - logprior: -3.4452e+01
Epoch 2/2
10/10 - 1s - loss: 169.6464 - loglik: -1.5934e+02 - logprior: -1.0302e+01
Fitted a model with MAP estimate = -162.3277
expansions: []
discards: [13 95]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.4485 - loglik: -1.5613e+02 - logprior: -2.5318e+01
Epoch 2/2
10/10 - 1s - loss: 159.7430 - loglik: -1.5338e+02 - logprior: -6.3617e+00
Fitted a model with MAP estimate = -156.3333
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 177.6134 - loglik: -1.5353e+02 - logprior: -2.4083e+01
Epoch 2/10
10/10 - 1s - loss: 158.2805 - loglik: -1.5225e+02 - logprior: -6.0268e+00
Epoch 3/10
10/10 - 1s - loss: 154.7772 - loglik: -1.5230e+02 - logprior: -2.4764e+00
Epoch 4/10
10/10 - 1s - loss: 152.2252 - loglik: -1.5109e+02 - logprior: -1.1384e+00
Epoch 5/10
10/10 - 1s - loss: 151.0777 - loglik: -1.5056e+02 - logprior: -5.1516e-01
Epoch 6/10
10/10 - 1s - loss: 150.2300 - loglik: -1.5002e+02 - logprior: -2.0632e-01
Epoch 7/10
10/10 - 1s - loss: 149.8085 - loglik: -1.4985e+02 - logprior: 0.0374
Epoch 8/10
10/10 - 1s - loss: 149.4874 - loglik: -1.4974e+02 - logprior: 0.2532
Epoch 9/10
10/10 - 1s - loss: 149.2676 - loglik: -1.4968e+02 - logprior: 0.4087
Epoch 10/10
10/10 - 1s - loss: 149.0919 - loglik: -1.4960e+02 - logprior: 0.5117
Fitted a model with MAP estimate = -149.0457
Time for alignment: 39.0144
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.7323 - loglik: -2.7665e+02 - logprior: -2.7080e+01
Epoch 2/10
10/10 - 1s - loss: 245.1264 - loglik: -2.3821e+02 - logprior: -6.9148e+00
Epoch 3/10
10/10 - 1s - loss: 206.6313 - loglik: -2.0299e+02 - logprior: -3.6386e+00
Epoch 4/10
10/10 - 1s - loss: 187.7228 - loglik: -1.8510e+02 - logprior: -2.6252e+00
Epoch 5/10
10/10 - 1s - loss: 179.8523 - loglik: -1.7771e+02 - logprior: -2.1425e+00
Epoch 6/10
10/10 - 1s - loss: 176.3622 - loglik: -1.7459e+02 - logprior: -1.7705e+00
Epoch 7/10
10/10 - 1s - loss: 174.3017 - loglik: -1.7274e+02 - logprior: -1.5653e+00
Epoch 8/10
10/10 - 1s - loss: 173.0180 - loglik: -1.7149e+02 - logprior: -1.5328e+00
Epoch 9/10
10/10 - 1s - loss: 172.2283 - loglik: -1.7071e+02 - logprior: -1.5138e+00
Epoch 10/10
10/10 - 1s - loss: 171.5838 - loglik: -1.7007e+02 - logprior: -1.5156e+00
Fitted a model with MAP estimate = -171.3716
expansions: [(0, 2), (11, 2), (12, 1), (34, 1), (36, 4), (37, 1), (48, 3), (49, 2), (69, 2), (70, 2), (71, 2), (72, 1), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.7470 - loglik: -1.6932e+02 - logprior: -3.4431e+01
Epoch 2/2
10/10 - 1s - loss: 168.9034 - loglik: -1.5866e+02 - logprior: -1.0245e+01
Fitted a model with MAP estimate = -162.0973
expansions: []
discards: [13]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.5110 - loglik: -1.5614e+02 - logprior: -2.5368e+01
Epoch 2/2
10/10 - 1s - loss: 161.3092 - loglik: -1.5489e+02 - logprior: -6.4203e+00
Fitted a model with MAP estimate = -157.6110
expansions: []
discards: [87]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.2715 - loglik: -1.5514e+02 - logprior: -2.4128e+01
Epoch 2/10
10/10 - 1s - loss: 160.1565 - loglik: -1.5406e+02 - logprior: -6.1008e+00
Epoch 3/10
10/10 - 1s - loss: 155.9895 - loglik: -1.5344e+02 - logprior: -2.5478e+00
Epoch 4/10
10/10 - 1s - loss: 153.4593 - loglik: -1.5229e+02 - logprior: -1.1741e+00
Epoch 5/10
10/10 - 1s - loss: 151.5664 - loglik: -1.5103e+02 - logprior: -5.3368e-01
Epoch 6/10
10/10 - 1s - loss: 150.6366 - loglik: -1.5042e+02 - logprior: -2.1737e-01
Epoch 7/10
10/10 - 1s - loss: 150.8747 - loglik: -1.5090e+02 - logprior: 0.0255
Fitted a model with MAP estimate = -150.2684
Time for alignment: 33.6840
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 303.7837 - loglik: -2.7671e+02 - logprior: -2.7076e+01
Epoch 2/10
10/10 - 1s - loss: 244.9074 - loglik: -2.3799e+02 - logprior: -6.9196e+00
Epoch 3/10
10/10 - 1s - loss: 207.3456 - loglik: -2.0369e+02 - logprior: -3.6587e+00
Epoch 4/10
10/10 - 1s - loss: 189.0072 - loglik: -1.8629e+02 - logprior: -2.7154e+00
Epoch 5/10
10/10 - 1s - loss: 179.8377 - loglik: -1.7752e+02 - logprior: -2.3152e+00
Epoch 6/10
10/10 - 1s - loss: 176.0217 - loglik: -1.7410e+02 - logprior: -1.9220e+00
Epoch 7/10
10/10 - 1s - loss: 174.2834 - loglik: -1.7259e+02 - logprior: -1.6921e+00
Epoch 8/10
10/10 - 1s - loss: 173.5628 - loglik: -1.7189e+02 - logprior: -1.6680e+00
Epoch 9/10
10/10 - 1s - loss: 172.5207 - loglik: -1.7088e+02 - logprior: -1.6391e+00
Epoch 10/10
10/10 - 1s - loss: 171.7929 - loglik: -1.7017e+02 - logprior: -1.6240e+00
Fitted a model with MAP estimate = -171.7039
expansions: [(0, 2), (11, 2), (12, 1), (34, 1), (36, 4), (37, 1), (48, 3), (49, 2), (67, 1), (69, 2), (70, 1), (71, 2), (72, 1), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.9331 - loglik: -1.6948e+02 - logprior: -3.4453e+01
Epoch 2/2
10/10 - 1s - loss: 168.8960 - loglik: -1.5865e+02 - logprior: -1.0251e+01
Fitted a model with MAP estimate = -161.9568
expansions: []
discards: [13 91]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 181.6063 - loglik: -1.5630e+02 - logprior: -2.5310e+01
Epoch 2/2
10/10 - 1s - loss: 160.4109 - loglik: -1.5404e+02 - logprior: -6.3758e+00
Fitted a model with MAP estimate = -157.3149
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 178.4431 - loglik: -1.5434e+02 - logprior: -2.4104e+01
Epoch 2/10
10/10 - 1s - loss: 159.3279 - loglik: -1.5327e+02 - logprior: -6.0553e+00
Epoch 3/10
10/10 - 1s - loss: 154.8102 - loglik: -1.5229e+02 - logprior: -2.5188e+00
Epoch 4/10
10/10 - 1s - loss: 153.3471 - loglik: -1.5218e+02 - logprior: -1.1632e+00
Epoch 5/10
10/10 - 1s - loss: 151.5548 - loglik: -1.5104e+02 - logprior: -5.1512e-01
Epoch 6/10
10/10 - 1s - loss: 151.0497 - loglik: -1.5087e+02 - logprior: -1.8200e-01
Epoch 7/10
10/10 - 1s - loss: 150.5642 - loglik: -1.5062e+02 - logprior: 0.0597
Epoch 8/10
10/10 - 1s - loss: 150.1440 - loglik: -1.5042e+02 - logprior: 0.2743
Epoch 9/10
10/10 - 1s - loss: 149.8846 - loglik: -1.5032e+02 - logprior: 0.4374
Epoch 10/10
10/10 - 1s - loss: 149.8223 - loglik: -1.5036e+02 - logprior: 0.5346
Fitted a model with MAP estimate = -149.7084
Time for alignment: 38.0784
Computed alignments with likelihoods: ['-149.3871', '-150.9161', '-149.0457', '-150.2684', '-149.7084']
Best model has likelihood: -149.0457  (prior= 0.5561 )
time for generating output: 0.1259
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.7911392405063291
Training of 5 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9e8c062b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2d1f3910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feac8c2bd90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 151.0398 - loglik: -1.4595e+02 - logprior: -5.0941e+00
Epoch 2/10
16/16 - 1s - loss: 125.5357 - loglik: -1.2394e+02 - logprior: -1.5923e+00
Epoch 3/10
16/16 - 1s - loss: 113.3396 - loglik: -1.1168e+02 - logprior: -1.6630e+00
Epoch 4/10
16/16 - 1s - loss: 110.0778 - loglik: -1.0843e+02 - logprior: -1.6504e+00
Epoch 5/10
16/16 - 1s - loss: 109.0902 - loglik: -1.0745e+02 - logprior: -1.6407e+00
Epoch 6/10
16/16 - 1s - loss: 108.7777 - loglik: -1.0715e+02 - logprior: -1.6275e+00
Epoch 7/10
16/16 - 1s - loss: 108.6262 - loglik: -1.0705e+02 - logprior: -1.5747e+00
Epoch 8/10
16/16 - 1s - loss: 108.0757 - loglik: -1.0651e+02 - logprior: -1.5675e+00
Epoch 9/10
16/16 - 1s - loss: 108.4655 - loglik: -1.0691e+02 - logprior: -1.5542e+00
Fitted a model with MAP estimate = -108.1473
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 113.6704 - loglik: -1.0736e+02 - logprior: -6.3117e+00
Epoch 2/2
16/16 - 1s - loss: 105.6985 - loglik: -1.0272e+02 - logprior: -2.9747e+00
Fitted a model with MAP estimate = -104.3006
expansions: [(0, 1)]
discards: [ 0 30]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 107.0368 - loglik: -1.0243e+02 - logprior: -4.6094e+00
Epoch 2/2
16/16 - 1s - loss: 103.0048 - loglik: -1.0135e+02 - logprior: -1.6512e+00
Fitted a model with MAP estimate = -102.6072
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 109.1562 - loglik: -1.0286e+02 - logprior: -6.2995e+00
Epoch 2/10
16/16 - 1s - loss: 104.8330 - loglik: -1.0204e+02 - logprior: -2.7940e+00
Epoch 3/10
16/16 - 1s - loss: 102.8824 - loglik: -1.0133e+02 - logprior: -1.5475e+00
Epoch 4/10
16/16 - 1s - loss: 102.9185 - loglik: -1.0165e+02 - logprior: -1.2641e+00
Fitted a model with MAP estimate = -102.0921
Time for alignment: 44.0791
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 150.9125 - loglik: -1.4582e+02 - logprior: -5.0934e+00
Epoch 2/10
16/16 - 1s - loss: 126.4143 - loglik: -1.2481e+02 - logprior: -1.6043e+00
Epoch 3/10
16/16 - 1s - loss: 114.1474 - loglik: -1.1248e+02 - logprior: -1.6723e+00
Epoch 4/10
16/16 - 1s - loss: 109.8782 - loglik: -1.0819e+02 - logprior: -1.6859e+00
Epoch 5/10
16/16 - 1s - loss: 108.9299 - loglik: -1.0726e+02 - logprior: -1.6684e+00
Epoch 6/10
16/16 - 1s - loss: 108.5424 - loglik: -1.0691e+02 - logprior: -1.6364e+00
Epoch 7/10
16/16 - 1s - loss: 108.4026 - loglik: -1.0680e+02 - logprior: -1.5991e+00
Epoch 8/10
16/16 - 1s - loss: 108.1717 - loglik: -1.0658e+02 - logprior: -1.5939e+00
Epoch 9/10
16/16 - 1s - loss: 107.6670 - loglik: -1.0608e+02 - logprior: -1.5866e+00
Epoch 10/10
16/16 - 1s - loss: 108.1054 - loglik: -1.0653e+02 - logprior: -1.5790e+00
Fitted a model with MAP estimate = -107.8241
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 4), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 114.5875 - loglik: -1.0825e+02 - logprior: -6.3385e+00
Epoch 2/2
16/16 - 1s - loss: 105.9490 - loglik: -1.0292e+02 - logprior: -3.0334e+00
Fitted a model with MAP estimate = -104.4486
expansions: [(0, 1)]
discards: [ 0 36]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 106.9677 - loglik: -1.0233e+02 - logprior: -4.6332e+00
Epoch 2/2
16/16 - 1s - loss: 102.9293 - loglik: -1.0129e+02 - logprior: -1.6439e+00
Fitted a model with MAP estimate = -102.4422
expansions: [(3, 1)]
discards: [ 0 32]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 109.1787 - loglik: -1.0290e+02 - logprior: -6.2748e+00
Epoch 2/10
16/16 - 1s - loss: 104.7072 - loglik: -1.0195e+02 - logprior: -2.7558e+00
Epoch 3/10
16/16 - 1s - loss: 102.9946 - loglik: -1.0148e+02 - logprior: -1.5129e+00
Epoch 4/10
16/16 - 1s - loss: 102.3771 - loglik: -1.0113e+02 - logprior: -1.2512e+00
Epoch 5/10
16/16 - 1s - loss: 101.9138 - loglik: -1.0068e+02 - logprior: -1.2306e+00
Epoch 6/10
16/16 - 1s - loss: 101.6115 - loglik: -1.0041e+02 - logprior: -1.2014e+00
Epoch 7/10
16/16 - 1s - loss: 101.2692 - loglik: -1.0008e+02 - logprior: -1.1876e+00
Epoch 8/10
16/16 - 1s - loss: 101.2808 - loglik: -1.0012e+02 - logprior: -1.1635e+00
Fitted a model with MAP estimate = -101.0349
Time for alignment: 49.0016
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 150.7661 - loglik: -1.4568e+02 - logprior: -5.0898e+00
Epoch 2/10
16/16 - 1s - loss: 125.6797 - loglik: -1.2410e+02 - logprior: -1.5821e+00
Epoch 3/10
16/16 - 1s - loss: 113.4971 - loglik: -1.1184e+02 - logprior: -1.6586e+00
Epoch 4/10
16/16 - 1s - loss: 110.4450 - loglik: -1.0880e+02 - logprior: -1.6401e+00
Epoch 5/10
16/16 - 1s - loss: 108.8775 - loglik: -1.0724e+02 - logprior: -1.6340e+00
Epoch 6/10
16/16 - 1s - loss: 108.8252 - loglik: -1.0720e+02 - logprior: -1.6218e+00
Epoch 7/10
16/16 - 1s - loss: 108.6853 - loglik: -1.0711e+02 - logprior: -1.5725e+00
Epoch 8/10
16/16 - 1s - loss: 108.2143 - loglik: -1.0665e+02 - logprior: -1.5638e+00
Epoch 9/10
16/16 - 1s - loss: 108.0324 - loglik: -1.0648e+02 - logprior: -1.5554e+00
Epoch 10/10
16/16 - 1s - loss: 108.3792 - loglik: -1.0683e+02 - logprior: -1.5470e+00
Fitted a model with MAP estimate = -108.0696
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 114.1946 - loglik: -1.0786e+02 - logprior: -6.3361e+00
Epoch 2/2
16/16 - 1s - loss: 105.5469 - loglik: -1.0254e+02 - logprior: -3.0047e+00
Fitted a model with MAP estimate = -104.4411
expansions: [(0, 1)]
discards: [ 0 31 32]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 106.9173 - loglik: -1.0231e+02 - logprior: -4.6048e+00
Epoch 2/2
16/16 - 1s - loss: 103.0256 - loglik: -1.0139e+02 - logprior: -1.6384e+00
Fitted a model with MAP estimate = -102.6904
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 109.2986 - loglik: -1.0303e+02 - logprior: -6.2707e+00
Epoch 2/10
16/16 - 1s - loss: 104.8796 - loglik: -1.0212e+02 - logprior: -2.7634e+00
Epoch 3/10
16/16 - 1s - loss: 102.8012 - loglik: -1.0127e+02 - logprior: -1.5263e+00
Epoch 4/10
16/16 - 1s - loss: 102.3162 - loglik: -1.0105e+02 - logprior: -1.2632e+00
Epoch 5/10
16/16 - 1s - loss: 102.0008 - loglik: -1.0077e+02 - logprior: -1.2313e+00
Epoch 6/10
16/16 - 1s - loss: 101.4622 - loglik: -1.0025e+02 - logprior: -1.2159e+00
Epoch 7/10
16/16 - 1s - loss: 101.4698 - loglik: -1.0027e+02 - logprior: -1.1968e+00
Fitted a model with MAP estimate = -101.1559
Time for alignment: 48.1596
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 151.1050 - loglik: -1.4601e+02 - logprior: -5.0930e+00
Epoch 2/10
16/16 - 1s - loss: 126.0323 - loglik: -1.2442e+02 - logprior: -1.6170e+00
Epoch 3/10
16/16 - 1s - loss: 115.4244 - loglik: -1.1381e+02 - logprior: -1.6180e+00
Epoch 4/10
16/16 - 1s - loss: 111.8178 - loglik: -1.1021e+02 - logprior: -1.6107e+00
Epoch 5/10
16/16 - 1s - loss: 110.3868 - loglik: -1.0880e+02 - logprior: -1.5894e+00
Epoch 6/10
16/16 - 1s - loss: 109.9847 - loglik: -1.0842e+02 - logprior: -1.5618e+00
Epoch 7/10
16/16 - 1s - loss: 109.5815 - loglik: -1.0805e+02 - logprior: -1.5331e+00
Epoch 8/10
16/16 - 1s - loss: 109.9744 - loglik: -1.0845e+02 - logprior: -1.5280e+00
Fitted a model with MAP estimate = -109.5213
expansions: [(3, 1), (6, 1), (14, 1), (15, 3), (17, 1), (23, 6), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 114.9817 - loglik: -1.0864e+02 - logprior: -6.3389e+00
Epoch 2/2
16/16 - 1s - loss: 106.0769 - loglik: -1.0303e+02 - logprior: -3.0485e+00
Fitted a model with MAP estimate = -104.5078
expansions: [(0, 1)]
discards: [ 0 18 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 107.0985 - loglik: -1.0249e+02 - logprior: -4.6113e+00
Epoch 2/2
16/16 - 1s - loss: 103.2126 - loglik: -1.0158e+02 - logprior: -1.6348e+00
Fitted a model with MAP estimate = -102.6537
expansions: [(3, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 106.1762 - loglik: -1.0157e+02 - logprior: -4.6079e+00
Epoch 2/10
16/16 - 1s - loss: 102.8962 - loglik: -1.0131e+02 - logprior: -1.5846e+00
Epoch 3/10
16/16 - 1s - loss: 101.7322 - loglik: -1.0041e+02 - logprior: -1.3210e+00
Epoch 4/10
16/16 - 1s - loss: 101.5523 - loglik: -1.0030e+02 - logprior: -1.2532e+00
Epoch 5/10
16/16 - 1s - loss: 101.4598 - loglik: -1.0023e+02 - logprior: -1.2321e+00
Epoch 6/10
16/16 - 1s - loss: 101.0764 - loglik: -9.9869e+01 - logprior: -1.2078e+00
Epoch 7/10
16/16 - 1s - loss: 100.7485 - loglik: -9.9560e+01 - logprior: -1.1880e+00
Epoch 8/10
16/16 - 1s - loss: 100.8517 - loglik: -9.9680e+01 - logprior: -1.1716e+00
Fitted a model with MAP estimate = -100.7207
Time for alignment: 46.7424
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 150.9937 - loglik: -1.4590e+02 - logprior: -5.0930e+00
Epoch 2/10
16/16 - 1s - loss: 125.1629 - loglik: -1.2356e+02 - logprior: -1.6044e+00
Epoch 3/10
16/16 - 1s - loss: 113.4760 - loglik: -1.1182e+02 - logprior: -1.6523e+00
Epoch 4/10
16/16 - 1s - loss: 109.9731 - loglik: -1.0834e+02 - logprior: -1.6343e+00
Epoch 5/10
16/16 - 1s - loss: 109.3330 - loglik: -1.0773e+02 - logprior: -1.6065e+00
Epoch 6/10
16/16 - 1s - loss: 108.9417 - loglik: -1.0735e+02 - logprior: -1.5884e+00
Epoch 7/10
16/16 - 1s - loss: 108.6170 - loglik: -1.0707e+02 - logprior: -1.5486e+00
Epoch 8/10
16/16 - 1s - loss: 108.8375 - loglik: -1.0729e+02 - logprior: -1.5432e+00
Fitted a model with MAP estimate = -108.4941
expansions: [(3, 1), (6, 1), (13, 1), (15, 2), (17, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 114.1686 - loglik: -1.0783e+02 - logprior: -6.3353e+00
Epoch 2/2
16/16 - 1s - loss: 106.0473 - loglik: -1.0301e+02 - logprior: -3.0368e+00
Fitted a model with MAP estimate = -104.4325
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 107.1418 - loglik: -1.0252e+02 - logprior: -4.6223e+00
Epoch 2/2
16/16 - 1s - loss: 102.9614 - loglik: -1.0131e+02 - logprior: -1.6503e+00
Fitted a model with MAP estimate = -102.6233
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 109.3976 - loglik: -1.0311e+02 - logprior: -6.2876e+00
Epoch 2/10
16/16 - 1s - loss: 104.7496 - loglik: -1.0201e+02 - logprior: -2.7417e+00
Epoch 3/10
16/16 - 1s - loss: 102.9196 - loglik: -1.0140e+02 - logprior: -1.5175e+00
Epoch 4/10
16/16 - 1s - loss: 102.6039 - loglik: -1.0134e+02 - logprior: -1.2619e+00
Epoch 5/10
16/16 - 1s - loss: 101.6168 - loglik: -1.0038e+02 - logprior: -1.2358e+00
Epoch 6/10
16/16 - 1s - loss: 101.5541 - loglik: -1.0035e+02 - logprior: -1.2066e+00
Epoch 7/10
16/16 - 1s - loss: 101.1448 - loglik: -9.9949e+01 - logprior: -1.1954e+00
Epoch 8/10
16/16 - 1s - loss: 101.4121 - loglik: -1.0024e+02 - logprior: -1.1702e+00
Fitted a model with MAP estimate = -101.0007
Time for alignment: 47.2523
Computed alignments with likelihoods: ['-102.0921', '-101.0349', '-101.1559', '-100.7207', '-101.0007']
Best model has likelihood: -100.7207  (prior= -1.1603 )
time for generating output: 0.1046
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.9208400646203554
Training of 5 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9829b8940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feab792de50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feb058aeac0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 588.4745 - loglik: -5.8682e+02 - logprior: -1.6532e+00
Epoch 2/10
39/39 - 9s - loss: 541.1494 - loglik: -5.4028e+02 - logprior: -8.6759e-01
Epoch 3/10
39/39 - 9s - loss: 535.4066 - loglik: -5.3455e+02 - logprior: -8.5811e-01
Epoch 4/10
39/39 - 9s - loss: 532.9329 - loglik: -5.3209e+02 - logprior: -8.4733e-01
Epoch 5/10
39/39 - 9s - loss: 532.0822 - loglik: -5.3124e+02 - logprior: -8.4073e-01
Epoch 6/10
39/39 - 9s - loss: 530.8691 - loglik: -5.3004e+02 - logprior: -8.3094e-01
Epoch 7/10
39/39 - 9s - loss: 528.6667 - loglik: -5.2784e+02 - logprior: -8.2675e-01
Epoch 8/10
39/39 - 9s - loss: 528.2203 - loglik: -5.2739e+02 - logprior: -8.2849e-01
Epoch 9/10
39/39 - 9s - loss: 528.0193 - loglik: -5.2719e+02 - logprior: -8.2565e-01
Epoch 10/10
39/39 - 9s - loss: 527.8965 - loglik: -5.2707e+02 - logprior: -8.2412e-01
Fitted a model with MAP estimate = -485.2109
expansions: [(6, 1), (7, 1), (10, 2), (11, 1), (20, 2), (24, 6), (25, 1), (55, 2), (71, 2), (72, 2), (76, 2), (87, 1), (92, 1), (111, 1), (113, 1), (123, 6), (126, 2), (128, 1)]
discards: [ 0 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 532.1031 - loglik: -5.2950e+02 - logprior: -2.6006e+00
Epoch 2/2
39/39 - 11s - loss: 522.7439 - loglik: -5.2201e+02 - logprior: -7.3394e-01
Fitted a model with MAP estimate = -478.6777
expansions: [(0, 2), (39, 1), (40, 1), (96, 1), (133, 1), (152, 2), (174, 7)]
discards: [  0  13  25  70  89 157 158 159 160]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 525.2393 - loglik: -5.2348e+02 - logprior: -1.7592e+00
Epoch 2/2
39/39 - 11s - loss: 519.3178 - loglik: -5.1862e+02 - logprior: -7.0243e-01
Fitted a model with MAP estimate = -476.6528
expansions: [(176, 5)]
discards: [  0 159 160 161 162 163 164 165 166 167 168 169 170 171]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 17s - loss: 475.1134 - loglik: -4.7393e+02 - logprior: -1.1786e+00
Epoch 2/10
51/51 - 14s - loss: 470.4574 - loglik: -4.7008e+02 - logprior: -3.8091e-01
Epoch 3/10
51/51 - 14s - loss: 468.2083 - loglik: -4.6783e+02 - logprior: -3.7985e-01
Epoch 4/10
51/51 - 14s - loss: 467.2553 - loglik: -4.6689e+02 - logprior: -3.6140e-01
Epoch 5/10
51/51 - 14s - loss: 467.2700 - loglik: -4.6695e+02 - logprior: -3.2248e-01
Fitted a model with MAP estimate = -466.6854
Time for alignment: 282.3024
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 589.4771 - loglik: -5.8784e+02 - logprior: -1.6384e+00
Epoch 2/10
39/39 - 9s - loss: 541.1580 - loglik: -5.4030e+02 - logprior: -8.5731e-01
Epoch 3/10
39/39 - 9s - loss: 533.8694 - loglik: -5.3295e+02 - logprior: -9.1686e-01
Epoch 4/10
39/39 - 9s - loss: 530.9532 - loglik: -5.3002e+02 - logprior: -9.3015e-01
Epoch 5/10
39/39 - 9s - loss: 530.1882 - loglik: -5.2925e+02 - logprior: -9.3630e-01
Epoch 6/10
39/39 - 9s - loss: 530.0232 - loglik: -5.2908e+02 - logprior: -9.4186e-01
Epoch 7/10
39/39 - 9s - loss: 529.7948 - loglik: -5.2886e+02 - logprior: -9.3605e-01
Epoch 8/10
39/39 - 9s - loss: 529.5075 - loglik: -5.2856e+02 - logprior: -9.4395e-01
Epoch 9/10
39/39 - 9s - loss: 529.5240 - loglik: -5.2858e+02 - logprior: -9.4083e-01
Fitted a model with MAP estimate = -487.1355
expansions: [(4, 1), (11, 1), (13, 1), (20, 1), (24, 6), (25, 1), (55, 2), (71, 1), (78, 4), (80, 1), (97, 1), (106, 1), (123, 2), (124, 1), (125, 5), (140, 3), (141, 4), (146, 3)]
discards: [ 91  92  93  94 132 133 134 135 136 137 138]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 533.4005 - loglik: -5.3160e+02 - logprior: -1.7988e+00
Epoch 2/2
39/39 - 11s - loss: 524.1378 - loglik: -5.2342e+02 - logprior: -7.1928e-01
Fitted a model with MAP estimate = -480.0136
expansions: [(37, 1), (38, 1), (84, 1), (91, 1), (92, 2), (147, 1)]
discards: [  0  25  68 143 157]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 530.3561 - loglik: -5.2780e+02 - logprior: -2.5597e+00
Epoch 2/2
39/39 - 11s - loss: 524.8212 - loglik: -5.2389e+02 - logprior: -9.3302e-01
Fitted a model with MAP estimate = -479.5984
expansions: [(0, 2)]
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 17s - loss: 475.5191 - loglik: -4.7451e+02 - logprior: -1.0123e+00
Epoch 2/10
51/51 - 14s - loss: 470.9670 - loglik: -4.7036e+02 - logprior: -6.0594e-01
Epoch 3/10
51/51 - 14s - loss: 470.4568 - loglik: -4.6985e+02 - logprior: -6.0424e-01
Epoch 4/10
51/51 - 14s - loss: 467.9235 - loglik: -4.6733e+02 - logprior: -5.9519e-01
Epoch 5/10
51/51 - 14s - loss: 467.9850 - loglik: -4.6743e+02 - logprior: -5.5751e-01
Fitted a model with MAP estimate = -467.5880
Time for alignment: 271.3392
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 589.0636 - loglik: -5.8740e+02 - logprior: -1.6671e+00
Epoch 2/10
39/39 - 9s - loss: 541.2987 - loglik: -5.4034e+02 - logprior: -9.6275e-01
Epoch 3/10
39/39 - 9s - loss: 533.9056 - loglik: -5.3290e+02 - logprior: -1.0058e+00
Epoch 4/10
39/39 - 9s - loss: 531.3730 - loglik: -5.3037e+02 - logprior: -1.0037e+00
Epoch 5/10
39/39 - 9s - loss: 530.7526 - loglik: -5.2975e+02 - logprior: -9.9924e-01
Epoch 6/10
39/39 - 9s - loss: 530.3681 - loglik: -5.2937e+02 - logprior: -1.0010e+00
Epoch 7/10
39/39 - 9s - loss: 530.4265 - loglik: -5.2942e+02 - logprior: -1.0114e+00
Fitted a model with MAP estimate = -487.4544
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 1), (24, 6), (25, 1), (55, 2), (57, 1), (69, 2), (76, 1), (77, 7), (78, 1), (79, 1), (81, 1), (87, 2), (92, 1), (113, 1), (123, 1), (125, 2), (126, 3), (128, 1), (140, 7), (146, 3)]
discards: [  0 135 136 137]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 533.3265 - loglik: -5.3072e+02 - logprior: -2.6082e+00
Epoch 2/2
39/39 - 12s - loss: 522.8576 - loglik: -5.2210e+02 - logprior: -7.6133e-01
Fitted a model with MAP estimate = -478.2619
expansions: [(0, 2), (37, 1), (38, 1), (173, 2)]
discards: [  0  25  68  96  97  98 104 105 114]
Re-initialized the encoder parameters.
Fitting a model of length 197 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 526.9051 - loglik: -5.2516e+02 - logprior: -1.7491e+00
Epoch 2/2
39/39 - 12s - loss: 521.8147 - loglik: -5.2114e+02 - logprior: -6.7963e-01
Fitted a model with MAP estimate = -478.4106
expansions: []
discards: [168 169 170 171]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 20s - loss: 475.1057 - loglik: -4.7398e+02 - logprior: -1.1258e+00
Epoch 2/10
51/51 - 14s - loss: 470.0530 - loglik: -4.6939e+02 - logprior: -6.5998e-01
Epoch 3/10
51/51 - 14s - loss: 467.2299 - loglik: -4.6662e+02 - logprior: -6.1256e-01
Epoch 4/10
51/51 - 14s - loss: 466.6366 - loglik: -4.6606e+02 - logprior: -5.7529e-01
Epoch 5/10
51/51 - 14s - loss: 465.4770 - loglik: -4.6493e+02 - logprior: -5.4417e-01
Epoch 6/10
51/51 - 14s - loss: 466.2069 - loglik: -4.6571e+02 - logprior: -5.0191e-01
Fitted a model with MAP estimate = -465.5605
Time for alignment: 278.1801
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 589.0084 - loglik: -5.8736e+02 - logprior: -1.6520e+00
Epoch 2/10
39/39 - 9s - loss: 541.6695 - loglik: -5.4074e+02 - logprior: -9.2678e-01
Epoch 3/10
39/39 - 9s - loss: 534.3828 - loglik: -5.3338e+02 - logprior: -1.0035e+00
Epoch 4/10
39/39 - 9s - loss: 531.9455 - loglik: -5.3091e+02 - logprior: -1.0405e+00
Epoch 5/10
39/39 - 9s - loss: 530.9016 - loglik: -5.2986e+02 - logprior: -1.0406e+00
Epoch 6/10
39/39 - 9s - loss: 530.9073 - loglik: -5.2987e+02 - logprior: -1.0388e+00
Fitted a model with MAP estimate = -486.4727
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (20, 2), (23, 6), (25, 1), (27, 1), (55, 2), (70, 5), (75, 1), (76, 3), (82, 1), (87, 2), (111, 2), (113, 1), (123, 2), (124, 3), (125, 4), (140, 13), (146, 3)]
discards: [  0 132 133 134 135 136 137 138]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 533.5809 - loglik: -5.3094e+02 - logprior: -2.6453e+00
Epoch 2/2
39/39 - 12s - loss: 522.0449 - loglik: -5.2118e+02 - logprior: -8.6908e-01
Fitted a model with MAP estimate = -477.1679
expansions: [(0, 2), (38, 1), (137, 1)]
discards: [  0  24  70  89 112 160 161 172 173 174 175 176 177 178 179]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 529.0883 - loglik: -5.2732e+02 - logprior: -1.7716e+00
Epoch 2/2
39/39 - 11s - loss: 523.0850 - loglik: -5.2240e+02 - logprior: -6.8809e-01
Fitted a model with MAP estimate = -478.2283
expansions: [(168, 2), (170, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 18s - loss: 473.7567 - loglik: -4.7249e+02 - logprior: -1.2619e+00
Epoch 2/10
51/51 - 15s - loss: 466.8065 - loglik: -4.6631e+02 - logprior: -4.9407e-01
Epoch 3/10
51/51 - 15s - loss: 465.2252 - loglik: -4.6475e+02 - logprior: -4.7443e-01
Epoch 4/10
51/51 - 15s - loss: 464.4897 - loglik: -4.6405e+02 - logprior: -4.4090e-01
Epoch 5/10
51/51 - 15s - loss: 464.5850 - loglik: -4.6419e+02 - logprior: -3.9985e-01
Fitted a model with MAP estimate = -463.6252
Time for alignment: 254.9784
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 588.4546 - loglik: -5.8680e+02 - logprior: -1.6578e+00
Epoch 2/10
39/39 - 9s - loss: 541.0284 - loglik: -5.4013e+02 - logprior: -8.9870e-01
Epoch 3/10
39/39 - 9s - loss: 534.4363 - loglik: -5.3347e+02 - logprior: -9.6272e-01
Epoch 4/10
39/39 - 9s - loss: 531.9786 - loglik: -5.3102e+02 - logprior: -9.5790e-01
Epoch 5/10
39/39 - 9s - loss: 530.6417 - loglik: -5.2969e+02 - logprior: -9.4935e-01
Epoch 6/10
39/39 - 9s - loss: 529.5425 - loglik: -5.2859e+02 - logprior: -9.5235e-01
Epoch 7/10
39/39 - 9s - loss: 529.1624 - loglik: -5.2822e+02 - logprior: -9.4651e-01
Epoch 8/10
39/39 - 9s - loss: 528.1391 - loglik: -5.2719e+02 - logprior: -9.4881e-01
Epoch 9/10
39/39 - 9s - loss: 527.8615 - loglik: -5.2689e+02 - logprior: -9.6761e-01
Epoch 10/10
39/39 - 9s - loss: 527.5487 - loglik: -5.2658e+02 - logprior: -9.7161e-01
Fitted a model with MAP estimate = -484.6554
expansions: [(4, 1), (11, 1), (13, 1), (21, 1), (24, 6), (25, 1), (54, 1), (55, 1), (72, 2), (76, 3), (83, 2), (92, 1), (106, 1), (110, 1), (112, 1), (122, 2), (123, 3), (124, 2), (125, 5), (139, 6), (140, 2)]
discards: [78 79]
Re-initialized the encoder parameters.
Fitting a model of length 197 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 527.9364 - loglik: -5.2607e+02 - logprior: -1.8669e+00
Epoch 2/2
39/39 - 12s - loss: 519.7006 - loglik: -5.1891e+02 - logprior: -7.9292e-01
Fitted a model with MAP estimate = -476.1263
expansions: [(37, 1), (93, 1), (131, 1), (148, 1), (155, 1), (156, 1)]
discards: [  0  25 100 144 163 164 165]
Re-initialized the encoder parameters.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 526.9960 - loglik: -5.2437e+02 - logprior: -2.6302e+00
Epoch 2/2
39/39 - 12s - loss: 520.4483 - loglik: -5.1940e+02 - logprior: -1.0515e+00
Fitted a model with MAP estimate = -476.4525
expansions: [(0, 2), (37, 1), (145, 1)]
discards: [  0  87  90 161 162 163 164 174 175 176 180]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 17s - loss: 474.5229 - loglik: -4.7346e+02 - logprior: -1.0621e+00
Epoch 2/10
51/51 - 14s - loss: 468.9029 - loglik: -4.6823e+02 - logprior: -6.7517e-01
Epoch 3/10
51/51 - 14s - loss: 467.1586 - loglik: -4.6652e+02 - logprior: -6.4158e-01
Epoch 4/10
51/51 - 14s - loss: 466.7473 - loglik: -4.6614e+02 - logprior: -6.0243e-01
Epoch 5/10
51/51 - 14s - loss: 465.9388 - loglik: -4.6534e+02 - logprior: -6.0158e-01
Epoch 6/10
51/51 - 14s - loss: 465.7272 - loglik: -4.6514e+02 - logprior: -5.8461e-01
Epoch 7/10
51/51 - 14s - loss: 466.2266 - loglik: -4.6569e+02 - logprior: -5.4098e-01
Fitted a model with MAP estimate = -465.3015
Time for alignment: 315.1428
Computed alignments with likelihoods: ['-466.6854', '-467.5880', '-465.5605', '-463.6252', '-465.3015']
Best model has likelihood: -463.6252  (prior= -0.3785 )
time for generating output: 0.2266
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.6659061277705346
Training of 5 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe91c120850>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe91c120d00>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e5b0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11e100>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e940>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e220>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe91c11ed30>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e9a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ed60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ef70>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e580>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11ebe0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e130>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e0d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11efd0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e490>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e850>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11e280>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe91c11ee20> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe91c11ea30>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91c11eeb0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe91c19dee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2c6a3430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fead1e1e5e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feaaee0bc40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fea3c1ac790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fea30b039d0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe91c1a4ac0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True

Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 10s - loss: 492.5955 - loglik: -4.8666e+02 - logprior: -5.9355e+00
Epoch 2/10
14/14 - 3s - loss: 437.7783 - loglik: -4.3653e+02 - logprior: -1.2440e+00
Epoch 3/10
14/14 - 3s - loss: 405.2350 - loglik: -4.0404e+02 - logprior: -1.1935e+00
Epoch 4/10
14/14 - 3s - loss: 392.3372 - loglik: -3.9120e+02 - logprior: -1.1356e+00
Epoch 5/10
14/14 - 3s - loss: 387.8798 - loglik: -3.8675e+02 - logprior: -1.1288e+00
Epoch 6/10
14/14 - 3s - loss: 386.8192 - loglik: -3.8572e+02 - logprior: -1.1021e+00
Epoch 7/10
14/14 - 3s - loss: 384.6786 - loglik: -3.8359e+02 - logprior: -1.0879e+00
Epoch 8/10
14/14 - 3s - loss: 384.9262 - loglik: -3.8385e+02 - logprior: -1.0781e+00
Fitted a model with MAP estimate = -384.1969
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (33, 1), (50, 1), (52, 1), (53, 1), (58, 1), (61, 1), (65, 1), (70, 2), (71, 2), (82, 1), (102, 4), (103, 2), (117, 1), (124, 1), (128, 1), (131, 1), (133, 1), (134, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 381.7042 - loglik: -3.7744e+02 - logprior: -4.2600e+00
Epoch 2/2
29/29 - 5s - loss: 368.2615 - loglik: -3.6729e+02 - logprior: -9.7500e-01
Fitted a model with MAP estimate = -366.4921
expansions: [(127, 1), (128, 1)]
discards: [  0  41  91 131 168]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 375.5517 - loglik: -3.7083e+02 - logprior: -4.7253e+00
Epoch 2/2
29/29 - 5s - loss: 367.8969 - loglik: -3.6717e+02 - logprior: -7.3080e-01
Fitted a model with MAP estimate = -366.5255
expansions: [(96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 373.5738 - loglik: -3.6957e+02 - logprior: -4.0042e+00
Epoch 2/10
29/29 - 5s - loss: 368.0866 - loglik: -3.6787e+02 - logprior: -2.2009e-01
Epoch 3/10
29/29 - 5s - loss: 366.3601 - loglik: -3.6631e+02 - logprior: -5.0086e-02
Epoch 4/10
29/29 - 5s - loss: 364.6635 - loglik: -3.6466e+02 - logprior: -5.3019e-03
Epoch 5/10
29/29 - 5s - loss: 364.4136 - loglik: -3.6449e+02 - logprior: 0.0791
Epoch 6/10
29/29 - 5s - loss: 363.8825 - loglik: -3.6405e+02 - logprior: 0.1651
Epoch 7/10
29/29 - 5s - loss: 364.1117 - loglik: -3.6436e+02 - logprior: 0.2488
Fitted a model with MAP estimate = -363.5080
Time for alignment: 128.7911
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.3720 - loglik: -4.8544e+02 - logprior: -5.9350e+00
Epoch 2/10
14/14 - 3s - loss: 438.3514 - loglik: -4.3711e+02 - logprior: -1.2412e+00
Epoch 3/10
14/14 - 3s - loss: 404.7346 - loglik: -4.0346e+02 - logprior: -1.2770e+00
Epoch 4/10
14/14 - 3s - loss: 391.9292 - loglik: -3.9072e+02 - logprior: -1.2055e+00
Epoch 5/10
14/14 - 3s - loss: 387.8066 - loglik: -3.8661e+02 - logprior: -1.1931e+00
Epoch 6/10
14/14 - 3s - loss: 385.8124 - loglik: -3.8462e+02 - logprior: -1.1894e+00
Epoch 7/10
14/14 - 3s - loss: 384.6746 - loglik: -3.8349e+02 - logprior: -1.1835e+00
Epoch 8/10
14/14 - 3s - loss: 384.5131 - loglik: -3.8334e+02 - logprior: -1.1764e+00
Epoch 9/10
14/14 - 3s - loss: 384.2747 - loglik: -3.8308e+02 - logprior: -1.1930e+00
Epoch 10/10
14/14 - 3s - loss: 382.9694 - loglik: -3.8174e+02 - logprior: -1.2310e+00
Fitted a model with MAP estimate = -383.2566
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (64, 1), (67, 2), (71, 1), (75, 1), (89, 1), (96, 1), (103, 3), (117, 1), (124, 1), (128, 1), (129, 2), (130, 2), (131, 1), (136, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 381.0724 - loglik: -3.7678e+02 - logprior: -4.2952e+00
Epoch 2/2
29/29 - 5s - loss: 366.9909 - loglik: -3.6613e+02 - logprior: -8.5999e-01
Fitted a model with MAP estimate = -365.0256
expansions: [(127, 1), (128, 1), (130, 1)]
discards: [ 41  86 160]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 370.3324 - loglik: -3.6722e+02 - logprior: -3.1113e+00
Epoch 2/2
29/29 - 5s - loss: 365.3054 - loglik: -3.6468e+02 - logprior: -6.2916e-01
Fitted a model with MAP estimate = -363.8152
expansions: [(165, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 11s - loss: 369.0879 - loglik: -3.6611e+02 - logprior: -2.9748e+00
Epoch 2/10
29/29 - 5s - loss: 364.0635 - loglik: -3.6360e+02 - logprior: -4.6490e-01
Epoch 3/10
29/29 - 5s - loss: 363.3264 - loglik: -3.6299e+02 - logprior: -3.3499e-01
Epoch 4/10
29/29 - 5s - loss: 361.5262 - loglik: -3.6124e+02 - logprior: -2.8781e-01
Epoch 5/10
29/29 - 5s - loss: 361.8279 - loglik: -3.6163e+02 - logprior: -2.0031e-01
Fitted a model with MAP estimate = -360.7330
Time for alignment: 123.9655
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.6043 - loglik: -4.8567e+02 - logprior: -5.9323e+00
Epoch 2/10
14/14 - 3s - loss: 437.2194 - loglik: -4.3597e+02 - logprior: -1.2483e+00
Epoch 3/10
14/14 - 3s - loss: 404.1103 - loglik: -4.0289e+02 - logprior: -1.2218e+00
Epoch 4/10
14/14 - 3s - loss: 391.9789 - loglik: -3.9083e+02 - logprior: -1.1455e+00
Epoch 5/10
14/14 - 3s - loss: 387.6995 - loglik: -3.8657e+02 - logprior: -1.1276e+00
Epoch 6/10
14/14 - 3s - loss: 386.4083 - loglik: -3.8530e+02 - logprior: -1.1061e+00
Epoch 7/10
14/14 - 3s - loss: 383.3943 - loglik: -3.8227e+02 - logprior: -1.1264e+00
Epoch 8/10
14/14 - 3s - loss: 383.7988 - loglik: -3.8265e+02 - logprior: -1.1474e+00
Fitted a model with MAP estimate = -383.3124
expansions: [(0, 2), (9, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (58, 1), (66, 2), (90, 2), (102, 4), (103, 2), (117, 1), (124, 1), (128, 1), (129, 2), (130, 2), (131, 1), (133, 2), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 382.4110 - loglik: -3.7812e+02 - logprior: -4.2928e+00
Epoch 2/2
29/29 - 5s - loss: 369.0614 - loglik: -3.6811e+02 - logprior: -9.5493e-01
Fitted a model with MAP estimate = -366.2924
expansions: [(83, 1), (84, 2), (124, 1), (125, 1)]
discards: [ 42 108 128 159]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 370.1363 - loglik: -3.6701e+02 - logprior: -3.1264e+00
Epoch 2/2
29/29 - 5s - loss: 365.5609 - loglik: -3.6486e+02 - logprior: -6.9738e-01
Fitted a model with MAP estimate = -362.9703
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 368.5896 - loglik: -3.6554e+02 - logprior: -3.0491e+00
Epoch 2/10
29/29 - 5s - loss: 363.6228 - loglik: -3.6309e+02 - logprior: -5.3285e-01
Epoch 3/10
29/29 - 5s - loss: 362.3232 - loglik: -3.6190e+02 - logprior: -4.2306e-01
Epoch 4/10
29/29 - 5s - loss: 361.3545 - loglik: -3.6099e+02 - logprior: -3.6287e-01
Epoch 5/10
29/29 - 5s - loss: 360.5080 - loglik: -3.6022e+02 - logprior: -2.8933e-01
Epoch 6/10
29/29 - 5s - loss: 359.6856 - loglik: -3.5948e+02 - logprior: -2.0739e-01
Epoch 7/10
29/29 - 5s - loss: 360.9121 - loglik: -3.6078e+02 - logprior: -1.3342e-01
Fitted a model with MAP estimate = -359.8511
Time for alignment: 127.3832
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 492.1641 - loglik: -4.8623e+02 - logprior: -5.9347e+00
Epoch 2/10
14/14 - 3s - loss: 438.5955 - loglik: -4.3735e+02 - logprior: -1.2417e+00
Epoch 3/10
14/14 - 3s - loss: 405.3474 - loglik: -4.0408e+02 - logprior: -1.2685e+00
Epoch 4/10
14/14 - 3s - loss: 393.5833 - loglik: -3.9234e+02 - logprior: -1.2438e+00
Epoch 5/10
14/14 - 3s - loss: 388.1272 - loglik: -3.8688e+02 - logprior: -1.2450e+00
Epoch 6/10
14/14 - 3s - loss: 384.9317 - loglik: -3.8371e+02 - logprior: -1.2235e+00
Epoch 7/10
14/14 - 3s - loss: 385.3264 - loglik: -3.8414e+02 - logprior: -1.1880e+00
Fitted a model with MAP estimate = -383.9559
expansions: [(0, 2), (9, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (61, 1), (69, 1), (70, 2), (71, 1), (82, 1), (102, 4), (118, 1), (124, 1), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 381.4019 - loglik: -3.7719e+02 - logprior: -4.2143e+00
Epoch 2/2
29/29 - 5s - loss: 368.5221 - loglik: -3.6764e+02 - logprior: -8.8296e-01
Fitted a model with MAP estimate = -366.9651
expansions: [(126, 1), (127, 1), (129, 1)]
discards: [ 41 163]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 372.2115 - loglik: -3.6917e+02 - logprior: -3.0396e+00
Epoch 2/2
29/29 - 5s - loss: 368.2028 - loglik: -3.6764e+02 - logprior: -5.6341e-01
Fitted a model with MAP estimate = -366.2739
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 370.6371 - loglik: -3.6771e+02 - logprior: -2.9275e+00
Epoch 2/10
29/29 - 5s - loss: 367.7520 - loglik: -3.6725e+02 - logprior: -5.0306e-01
Epoch 3/10
29/29 - 5s - loss: 365.5462 - loglik: -3.6517e+02 - logprior: -3.7720e-01
Epoch 4/10
29/29 - 5s - loss: 364.3383 - loglik: -3.6401e+02 - logprior: -3.2435e-01
Epoch 5/10
29/29 - 5s - loss: 363.5875 - loglik: -3.6334e+02 - logprior: -2.4403e-01
Epoch 6/10
29/29 - 5s - loss: 363.8743 - loglik: -3.6371e+02 - logprior: -1.6551e-01
Fitted a model with MAP estimate = -363.0944
Time for alignment: 115.2147
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 491.0844 - loglik: -4.8515e+02 - logprior: -5.9300e+00
Epoch 2/10
14/14 - 3s - loss: 440.1709 - loglik: -4.3891e+02 - logprior: -1.2577e+00
Epoch 3/10
14/14 - 3s - loss: 409.6461 - loglik: -4.0840e+02 - logprior: -1.2489e+00
Epoch 4/10
14/14 - 3s - loss: 395.1169 - loglik: -3.9393e+02 - logprior: -1.1833e+00
Epoch 5/10
14/14 - 3s - loss: 389.9622 - loglik: -3.8879e+02 - logprior: -1.1753e+00
Epoch 6/10
14/14 - 3s - loss: 388.4203 - loglik: -3.8727e+02 - logprior: -1.1535e+00
Epoch 7/10
14/14 - 3s - loss: 385.5602 - loglik: -3.8444e+02 - logprior: -1.1195e+00
Epoch 8/10
14/14 - 4s - loss: 386.4029 - loglik: -3.8530e+02 - logprior: -1.1028e+00
Fitted a model with MAP estimate = -385.6282
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (69, 1), (71, 1), (77, 1), (82, 1), (102, 4), (117, 1), (124, 1), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 179 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 9s - loss: 383.9583 - loglik: -3.7975e+02 - logprior: -4.2048e+00
Epoch 2/2
29/29 - 5s - loss: 371.4747 - loglik: -3.7054e+02 - logprior: -9.3088e-01
Fitted a model with MAP estimate = -369.2728
expansions: [(125, 1), (126, 1), (128, 1)]
discards: [  0  41  96 162]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 378.0140 - loglik: -3.7333e+02 - logprior: -4.6840e+00
Epoch 2/2
29/29 - 5s - loss: 370.1957 - loglik: -3.6948e+02 - logprior: -7.1841e-01
Fitted a model with MAP estimate = -368.8738
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 376.8157 - loglik: -3.7299e+02 - logprior: -3.8247e+00
Epoch 2/10
29/29 - 5s - loss: 370.3263 - loglik: -3.7013e+02 - logprior: -2.0128e-01
Epoch 3/10
29/29 - 5s - loss: 369.0157 - loglik: -3.6897e+02 - logprior: -4.8027e-02
Epoch 4/10
29/29 - 5s - loss: 368.3539 - loglik: -3.6834e+02 - logprior: -1.3116e-02
Epoch 5/10
29/29 - 5s - loss: 366.9542 - loglik: -3.6703e+02 - logprior: 0.0805
Epoch 6/10
29/29 - 5s - loss: 366.6556 - loglik: -3.6681e+02 - logprior: 0.1516
Epoch 7/10
29/29 - 5s - loss: 367.2784 - loglik: -3.6752e+02 - logprior: 0.2368
Fitted a model with MAP estimate = -366.6391
Time for alignment: 125.5386
Computed alignments with likelihoods: ['-363.5080', '-360.7330', '-359.8511', '-363.0944', '-366.6391']
Best model has likelihood: -359.8511  (prior= -0.0960 )
time for generating output: 0.2366
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.8229130948298853
