Training of 3 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fedfbdf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fedfbf10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fedfb8e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fedfb070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fedfb250>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fedfbdc0>, <__main__.SimpleDirichletPrior object at 0x7f19fedeefd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fed9e670>

Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 538.0690 - loglik: -5.3288e+02 - logprior: -5.1461e+00
Epoch 2/10
25/25 - 7s - loss: 427.6297 - loglik: -4.2529e+02 - logprior: -2.1415e+00
Epoch 3/10
25/25 - 7s - loss: 405.1577 - loglik: -4.0180e+02 - logprior: -2.6586e+00
Epoch 4/10
25/25 - 7s - loss: 402.5900 - loglik: -3.9923e+02 - logprior: -2.5051e+00
Epoch 5/10
25/25 - 8s - loss: 400.8001 - loglik: -3.9746e+02 - logprior: -2.4930e+00
Epoch 6/10
25/25 - 8s - loss: 401.3107 - loglik: -3.9799e+02 - logprior: -2.5023e+00
Fitted a model with MAP estimate = -398.9443
expansions: [(9, 5), (10, 2), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (47, 2), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (95, 1), (98, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (163, 1), (167, 1), (168, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 401.2490 - loglik: -3.9297e+02 - logprior: -7.3460e+00
Epoch 2/2
25/25 - 10s - loss: 385.9924 - loglik: -3.8192e+02 - logprior: -3.0767e+00
Fitted a model with MAP estimate = -380.7291
expansions: [(0, 3)]
discards: [  0  11  60  80 175]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 390.3202 - loglik: -3.8443e+02 - logprior: -4.8879e+00
Epoch 2/2
25/25 - 9s - loss: 382.5184 - loglik: -3.8041e+02 - logprior: -1.0360e+00
Fitted a model with MAP estimate = -379.0932
expansions: []
discards: [  0   2 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 394.2051 - loglik: -3.8629e+02 - logprior: -6.8234e+00
Epoch 2/10
25/25 - 9s - loss: 383.1128 - loglik: -3.8015e+02 - logprior: -1.8595e+00
Epoch 3/10
25/25 - 9s - loss: 381.0574 - loglik: -3.7962e+02 - logprior: -2.8077e-01
Epoch 4/10
25/25 - 9s - loss: 378.3448 - loglik: -3.7708e+02 - logprior: -2.0332e-01
Epoch 5/10
25/25 - 9s - loss: 377.3262 - loglik: -3.7626e+02 - logprior: -8.6076e-02
Epoch 6/10
25/25 - 9s - loss: 376.7152 - loglik: -3.7589e+02 - logprior: 0.0536
Epoch 7/10
25/25 - 9s - loss: 376.5014 - loglik: -3.7586e+02 - logprior: 0.2013
Epoch 8/10
25/25 - 9s - loss: 376.0962 - loglik: -3.7562e+02 - logprior: 0.3260
Epoch 9/10
25/25 - 9s - loss: 374.6692 - loglik: -3.7434e+02 - logprior: 0.4546
Epoch 10/10
25/25 - 9s - loss: 376.8839 - loglik: -3.7668e+02 - logprior: 0.5757
Fitted a model with MAP estimate = -374.0752
Time for alignment: 219.3229
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 537.9841 - loglik: -5.3284e+02 - logprior: -5.1016e+00
Epoch 2/10
25/25 - 8s - loss: 428.6576 - loglik: -4.2623e+02 - logprior: -2.2254e+00
Epoch 3/10
25/25 - 8s - loss: 404.7651 - loglik: -4.0143e+02 - logprior: -2.6541e+00
Epoch 4/10
25/25 - 8s - loss: 403.5522 - loglik: -4.0029e+02 - logprior: -2.4959e+00
Epoch 5/10
25/25 - 8s - loss: 400.9950 - loglik: -3.9775e+02 - logprior: -2.4628e+00
Epoch 6/10
25/25 - 8s - loss: 400.3647 - loglik: -3.9712e+02 - logprior: -2.4575e+00
Epoch 7/10
25/25 - 8s - loss: 400.2799 - loglik: -3.9703e+02 - logprior: -2.4617e+00
Epoch 8/10
25/25 - 8s - loss: 398.8819 - loglik: -3.9564e+02 - logprior: -2.4694e+00
Epoch 9/10
25/25 - 8s - loss: 398.8583 - loglik: -3.9564e+02 - logprior: -2.4591e+00
Epoch 10/10
25/25 - 8s - loss: 398.6847 - loglik: -3.9548e+02 - logprior: -2.4596e+00
Fitted a model with MAP estimate = -397.8607
expansions: [(9, 3), (10, 1), (11, 2), (32, 3), (33, 2), (35, 2), (36, 2), (47, 1), (48, 1), (58, 1), (60, 1), (62, 2), (66, 1), (75, 1), (80, 1), (82, 1), (83, 2), (87, 1), (90, 1), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 3), (149, 1), (158, 1), (166, 1), (167, 1)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 221 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 400.2270 - loglik: -3.9408e+02 - logprior: -5.2859e+00
Epoch 2/2
25/25 - 9s - loss: 383.9621 - loglik: -3.8160e+02 - logprior: -1.3540e+00
Fitted a model with MAP estimate = -379.8727
expansions: []
discards: [  9  42  45  48  82 108 144 178 182]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 390.4668 - loglik: -3.8455e+02 - logprior: -4.8635e+00
Epoch 2/2
25/25 - 9s - loss: 384.3752 - loglik: -3.8223e+02 - logprior: -1.0313e+00
Fitted a model with MAP estimate = -380.1378
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 391.1444 - loglik: -3.8533e+02 - logprior: -4.6666e+00
Epoch 2/10
25/25 - 9s - loss: 381.9650 - loglik: -3.8003e+02 - logprior: -8.1272e-01
Epoch 3/10
25/25 - 9s - loss: 380.5678 - loglik: -3.7900e+02 - logprior: -4.1519e-01
Epoch 4/10
25/25 - 9s - loss: 378.9107 - loglik: -3.7759e+02 - logprior: -2.6426e-01
Epoch 5/10
25/25 - 9s - loss: 378.7168 - loglik: -3.7763e+02 - logprior: -1.1887e-01
Epoch 6/10
25/25 - 9s - loss: 376.5109 - loglik: -3.7562e+02 - logprior: 0.0021
Epoch 7/10
25/25 - 9s - loss: 376.0229 - loglik: -3.7532e+02 - logprior: 0.1337
Epoch 8/10
25/25 - 9s - loss: 377.3165 - loglik: -3.7678e+02 - logprior: 0.2586
Fitted a model with MAP estimate = -375.2064
Time for alignment: 231.4730
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 537.3683 - loglik: -5.3220e+02 - logprior: -5.1328e+00
Epoch 2/10
25/25 - 8s - loss: 426.7550 - loglik: -4.2436e+02 - logprior: -2.2032e+00
Epoch 3/10
25/25 - 8s - loss: 406.1144 - loglik: -4.0286e+02 - logprior: -2.5773e+00
Epoch 4/10
25/25 - 8s - loss: 403.2976 - loglik: -4.0013e+02 - logprior: -2.3963e+00
Epoch 5/10
25/25 - 8s - loss: 401.7616 - loglik: -3.9863e+02 - logprior: -2.3712e+00
Epoch 6/10
25/25 - 8s - loss: 402.1409 - loglik: -3.9902e+02 - logprior: -2.3785e+00
Fitted a model with MAP estimate = -399.8137
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 1), (35, 2), (36, 1), (47, 2), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (162, 2), (168, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 220 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 12s - loss: 402.5706 - loglik: -3.9435e+02 - logprior: -7.3338e+00
Epoch 2/2
25/25 - 9s - loss: 385.1558 - loglik: -3.8111e+02 - logprior: -3.1022e+00
Fitted a model with MAP estimate = -380.7555
expansions: [(0, 3)]
discards: [  0   9  44  61  81 143 177 206]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 13s - loss: 391.6079 - loglik: -3.8566e+02 - logprior: -4.9401e+00
Epoch 2/2
25/25 - 10s - loss: 381.9916 - loglik: -3.7982e+02 - logprior: -1.0800e+00
Fitted a model with MAP estimate = -379.1229
expansions: []
discards: [  0   2 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 12s - loss: 392.0545 - loglik: -3.8406e+02 - logprior: -6.8823e+00
Epoch 2/10
25/25 - 9s - loss: 385.3791 - loglik: -3.8232e+02 - logprior: -1.9424e+00
Epoch 3/10
25/25 - 9s - loss: 380.4425 - loglik: -3.7895e+02 - logprior: -3.1552e-01
Epoch 4/10
25/25 - 9s - loss: 378.4619 - loglik: -3.7716e+02 - logprior: -2.3643e-01
Epoch 5/10
25/25 - 9s - loss: 378.6037 - loglik: -3.7756e+02 - logprior: -8.6073e-02
Fitted a model with MAP estimate = -376.1315
Time for alignment: 175.5021
Computed alignments with likelihoods: ['-374.0752', '-375.2064', '-376.1315']
Best model has likelihood: -374.0752
time for generating output: 0.2641
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9235855988243938
Training of 3 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f17a00dce20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fedfb250>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fedfbb80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fedfb6d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fedfb070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fedfbf10>, <__main__.SimpleDirichletPrior object at 0x7f19fd527100>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fed9e670>

Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.7340 - loglik: -3.8627e+02 - logprior: -2.0462e+01
Epoch 2/10
10/10 - 2s - loss: 342.0000 - loglik: -3.3704e+02 - logprior: -4.9544e+00
Epoch 3/10
10/10 - 2s - loss: 290.3867 - loglik: -2.8759e+02 - logprior: -2.7798e+00
Epoch 4/10
10/10 - 2s - loss: 258.6063 - loglik: -2.5597e+02 - logprior: -2.4909e+00
Epoch 5/10
10/10 - 2s - loss: 247.1801 - loglik: -2.4429e+02 - logprior: -2.4354e+00
Epoch 6/10
10/10 - 2s - loss: 242.0631 - loglik: -2.3921e+02 - logprior: -2.2182e+00
Epoch 7/10
10/10 - 2s - loss: 239.7652 - loglik: -2.3721e+02 - logprior: -2.0198e+00
Epoch 8/10
10/10 - 2s - loss: 237.9402 - loglik: -2.3552e+02 - logprior: -1.9863e+00
Epoch 9/10
10/10 - 2s - loss: 238.6010 - loglik: -2.3618e+02 - logprior: -2.0117e+00
Fitted a model with MAP estimate = -237.1448
expansions: [(0, 4), (13, 3), (18, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (62, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 1), (104, 3), (105, 2), (106, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 258.5653 - loglik: -2.3116e+02 - logprior: -2.6944e+01
Epoch 2/2
10/10 - 2s - loss: 219.0238 - loglik: -2.1058e+02 - logprior: -7.9503e+00
Fitted a model with MAP estimate = -211.5495
expansions: []
discards: [ 62 135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.3232 - loglik: -2.0777e+02 - logprior: -1.9141e+01
Epoch 2/2
10/10 - 2s - loss: 209.1845 - loglik: -2.0423e+02 - logprior: -4.5947e+00
Fitted a model with MAP estimate = -206.2319
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 223.6764 - loglik: -2.0525e+02 - logprior: -1.8048e+01
Epoch 2/10
10/10 - 2s - loss: 208.4378 - loglik: -2.0381e+02 - logprior: -4.2262e+00
Epoch 3/10
10/10 - 2s - loss: 204.4906 - loglik: -2.0238e+02 - logprior: -1.6263e+00
Epoch 4/10
10/10 - 2s - loss: 203.2008 - loglik: -2.0193e+02 - logprior: -7.5368e-01
Epoch 5/10
10/10 - 2s - loss: 202.0331 - loglik: -2.0116e+02 - logprior: -3.3542e-01
Epoch 6/10
10/10 - 2s - loss: 200.9139 - loglik: -2.0038e+02 - logprior: -1.6414e-02
Epoch 7/10
10/10 - 2s - loss: 200.9229 - loglik: -2.0065e+02 - logprior: 0.2445
Fitted a model with MAP estimate = -199.7923
Time for alignment: 61.8497
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.5112 - loglik: -3.8604e+02 - logprior: -2.0463e+01
Epoch 2/10
10/10 - 2s - loss: 341.6952 - loglik: -3.3674e+02 - logprior: -4.9552e+00
Epoch 3/10
10/10 - 2s - loss: 291.0031 - loglik: -2.8821e+02 - logprior: -2.7778e+00
Epoch 4/10
10/10 - 2s - loss: 261.4452 - loglik: -2.5879e+02 - logprior: -2.5095e+00
Epoch 5/10
10/10 - 2s - loss: 247.9162 - loglik: -2.4498e+02 - logprior: -2.5056e+00
Epoch 6/10
10/10 - 2s - loss: 242.0923 - loglik: -2.3916e+02 - logprior: -2.3045e+00
Epoch 7/10
10/10 - 2s - loss: 240.0059 - loglik: -2.3742e+02 - logprior: -2.0701e+00
Epoch 8/10
10/10 - 2s - loss: 238.0031 - loglik: -2.3559e+02 - logprior: -2.0033e+00
Epoch 9/10
10/10 - 2s - loss: 238.1095 - loglik: -2.3573e+02 - logprior: -1.9928e+00
Fitted a model with MAP estimate = -236.9465
expansions: [(0, 3), (14, 3), (15, 1), (22, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 1), (104, 3), (105, 2), (106, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 257.1388 - loglik: -2.2981e+02 - logprior: -2.6883e+01
Epoch 2/2
10/10 - 2s - loss: 219.5938 - loglik: -2.1114e+02 - logprior: -7.9655e+00
Fitted a model with MAP estimate = -211.4026
expansions: []
discards: [ 59  62 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 226.8164 - loglik: -2.0729e+02 - logprior: -1.9112e+01
Epoch 2/2
10/10 - 2s - loss: 209.5561 - loglik: -2.0463e+02 - logprior: -4.5542e+00
Fitted a model with MAP estimate = -206.2457
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.2862 - loglik: -2.0589e+02 - logprior: -1.8022e+01
Epoch 2/10
10/10 - 2s - loss: 208.4279 - loglik: -2.0384e+02 - logprior: -4.1992e+00
Epoch 3/10
10/10 - 2s - loss: 204.5174 - loglik: -2.0244e+02 - logprior: -1.6045e+00
Epoch 4/10
10/10 - 2s - loss: 203.5226 - loglik: -2.0228e+02 - logprior: -7.3028e-01
Epoch 5/10
10/10 - 2s - loss: 201.6754 - loglik: -2.0083e+02 - logprior: -3.0956e-01
Epoch 6/10
10/10 - 2s - loss: 200.3045 - loglik: -1.9978e+02 - logprior: 0.0016
Epoch 7/10
10/10 - 2s - loss: 201.0322 - loglik: -2.0077e+02 - logprior: 0.2588
Fitted a model with MAP estimate = -199.7662
Time for alignment: 61.2991
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.5114 - loglik: -3.8604e+02 - logprior: -2.0464e+01
Epoch 2/10
10/10 - 2s - loss: 341.9819 - loglik: -3.3702e+02 - logprior: -4.9552e+00
Epoch 3/10
10/10 - 2s - loss: 287.9264 - loglik: -2.8511e+02 - logprior: -2.8020e+00
Epoch 4/10
10/10 - 2s - loss: 256.6640 - loglik: -2.5392e+02 - logprior: -2.5755e+00
Epoch 5/10
10/10 - 2s - loss: 244.8781 - loglik: -2.4187e+02 - logprior: -2.5471e+00
Epoch 6/10
10/10 - 2s - loss: 239.9447 - loglik: -2.3704e+02 - logprior: -2.3968e+00
Epoch 7/10
10/10 - 2s - loss: 238.2700 - loglik: -2.3558e+02 - logprior: -2.2496e+00
Epoch 8/10
10/10 - 2s - loss: 237.1349 - loglik: -2.3457e+02 - logprior: -2.1654e+00
Epoch 9/10
10/10 - 2s - loss: 236.3782 - loglik: -2.3391e+02 - logprior: -2.1051e+00
Epoch 10/10
10/10 - 2s - loss: 235.5681 - loglik: -2.3312e+02 - logprior: -2.0974e+00
Fitted a model with MAP estimate = -235.3154
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (18, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 2), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 1), (104, 3), (105, 2), (106, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 257.5128 - loglik: -2.3002e+02 - logprior: -2.7079e+01
Epoch 2/2
10/10 - 2s - loss: 219.0922 - loglik: -2.1059e+02 - logprior: -8.0216e+00
Fitted a model with MAP estimate = -211.4808
expansions: []
discards: [ 61 135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 226.6812 - loglik: -2.0710e+02 - logprior: -1.9150e+01
Epoch 2/2
10/10 - 2s - loss: 210.0850 - loglik: -2.0514e+02 - logprior: -4.5777e+00
Fitted a model with MAP estimate = -206.2508
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 223.3763 - loglik: -2.0496e+02 - logprior: -1.8030e+01
Epoch 2/10
10/10 - 2s - loss: 209.4374 - loglik: -2.0483e+02 - logprior: -4.2093e+00
Epoch 3/10
10/10 - 2s - loss: 204.7684 - loglik: -2.0268e+02 - logprior: -1.6125e+00
Epoch 4/10
10/10 - 2s - loss: 202.6601 - loglik: -2.0140e+02 - logprior: -7.4090e-01
Epoch 5/10
10/10 - 2s - loss: 201.5150 - loglik: -2.0067e+02 - logprior: -3.2020e-01
Epoch 6/10
10/10 - 2s - loss: 201.2065 - loglik: -2.0070e+02 - logprior: 0.0012
Epoch 7/10
10/10 - 2s - loss: 201.2705 - loglik: -2.0102e+02 - logprior: 0.2568
Fitted a model with MAP estimate = -199.8184
Time for alignment: 63.5980
Computed alignments with likelihoods: ['-199.7923', '-199.7662', '-199.8184']
Best model has likelihood: -199.7662
time for generating output: 0.2058
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.9092150170648464
Training of 3 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fde49a30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19c7c1c6d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19c7c1c1c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19c7a47520>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19c79e4ee0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fd503f40>, <__main__.SimpleDirichletPrior object at 0x7f19bf2d7880>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe9813a0>

Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.7267 - loglik: -2.4851e+02 - logprior: -6.9210e+01
Epoch 2/10
10/10 - 2s - loss: 236.9226 - loglik: -2.1896e+02 - logprior: -1.7943e+01
Epoch 3/10
10/10 - 2s - loss: 204.0934 - loglik: -1.9591e+02 - logprior: -8.1017e+00
Epoch 4/10
10/10 - 2s - loss: 188.7075 - loglik: -1.8411e+02 - logprior: -4.5631e+00
Epoch 5/10
10/10 - 2s - loss: 181.1024 - loglik: -1.7841e+02 - logprior: -2.6901e+00
Epoch 6/10
10/10 - 2s - loss: 177.6902 - loglik: -1.7599e+02 - logprior: -1.6922e+00
Epoch 7/10
10/10 - 2s - loss: 175.9008 - loglik: -1.7477e+02 - logprior: -1.0661e+00
Epoch 8/10
10/10 - 2s - loss: 174.9964 - loglik: -1.7407e+02 - logprior: -7.3521e-01
Epoch 9/10
10/10 - 2s - loss: 174.5354 - loglik: -1.7382e+02 - logprior: -4.9731e-01
Epoch 10/10
10/10 - 2s - loss: 174.1785 - loglik: -1.7368e+02 - logprior: -3.0966e-01
Fitted a model with MAP estimate = -173.8099
expansions: [(9, 3), (14, 1), (15, 1), (26, 1), (37, 1), (39, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 251.9575 - loglik: -1.7402e+02 - logprior: -7.7798e+01
Epoch 2/2
10/10 - 2s - loss: 199.4541 - loglik: -1.6765e+02 - logprior: -3.1730e+01
Fitted a model with MAP estimate = -190.3901
expansions: [(32, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 239.7651 - loglik: -1.6476e+02 - logprior: -7.4878e+01
Epoch 2/2
10/10 - 2s - loss: 185.4225 - loglik: -1.6237e+02 - logprior: -2.2906e+01
Fitted a model with MAP estimate = -173.8803
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 223.8290 - loglik: -1.6113e+02 - logprior: -6.2533e+01
Epoch 2/10
10/10 - 2s - loss: 176.9662 - loglik: -1.6100e+02 - logprior: -1.5827e+01
Epoch 3/10
10/10 - 2s - loss: 167.4568 - loglik: -1.6126e+02 - logprior: -6.1328e+00
Epoch 4/10
10/10 - 2s - loss: 163.4526 - loglik: -1.6116e+02 - logprior: -2.1824e+00
Epoch 5/10
10/10 - 2s - loss: 161.2768 - loglik: -1.6104e+02 - logprior: -5.5889e-02
Epoch 6/10
10/10 - 2s - loss: 159.8784 - loglik: -1.6086e+02 - logprior: 1.1721
Epoch 7/10
10/10 - 2s - loss: 159.0035 - loglik: -1.6066e+02 - logprior: 1.9046
Epoch 8/10
10/10 - 2s - loss: 158.4799 - loglik: -1.6066e+02 - logprior: 2.4451
Epoch 9/10
10/10 - 2s - loss: 158.1061 - loglik: -1.6071e+02 - logprior: 2.8806
Epoch 10/10
10/10 - 2s - loss: 157.7978 - loglik: -1.6078e+02 - logprior: 3.2669
Fitted a model with MAP estimate = -157.3342
Time for alignment: 67.0786
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.7261 - loglik: -2.4851e+02 - logprior: -6.9210e+01
Epoch 2/10
10/10 - 2s - loss: 237.0317 - loglik: -2.1907e+02 - logprior: -1.7946e+01
Epoch 3/10
10/10 - 2s - loss: 203.5355 - loglik: -1.9536e+02 - logprior: -8.0958e+00
Epoch 4/10
10/10 - 2s - loss: 187.4850 - loglik: -1.8288e+02 - logprior: -4.5693e+00
Epoch 5/10
10/10 - 2s - loss: 181.2307 - loglik: -1.7866e+02 - logprior: -2.5701e+00
Epoch 6/10
10/10 - 2s - loss: 178.6626 - loglik: -1.7721e+02 - logprior: -1.4380e+00
Epoch 7/10
10/10 - 2s - loss: 177.0687 - loglik: -1.7619e+02 - logprior: -8.3472e-01
Epoch 8/10
10/10 - 2s - loss: 176.1371 - loglik: -1.7549e+02 - logprior: -5.0817e-01
Epoch 9/10
10/10 - 2s - loss: 175.6405 - loglik: -1.7517e+02 - logprior: -2.6377e-01
Epoch 10/10
10/10 - 2s - loss: 175.3198 - loglik: -1.7504e+02 - logprior: -9.3693e-02
Fitted a model with MAP estimate = -174.9925
expansions: [(9, 3), (14, 2), (26, 2), (37, 1), (39, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.9276 - loglik: -1.7506e+02 - logprior: -7.7721e+01
Epoch 2/2
10/10 - 2s - loss: 199.7341 - loglik: -1.6797e+02 - logprior: -3.1700e+01
Fitted a model with MAP estimate = -190.5245
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 239.2940 - loglik: -1.6548e+02 - logprior: -7.3705e+01
Epoch 2/10
10/10 - 2s - loss: 185.0266 - loglik: -1.6354e+02 - logprior: -2.1336e+01
Epoch 3/10
10/10 - 2s - loss: 170.4370 - loglik: -1.6275e+02 - logprior: -7.5886e+00
Epoch 4/10
10/10 - 2s - loss: 165.1165 - loglik: -1.6213e+02 - logprior: -2.8639e+00
Epoch 5/10
10/10 - 2s - loss: 162.6405 - loglik: -1.6187e+02 - logprior: -5.8921e-01
Epoch 6/10
10/10 - 2s - loss: 161.0814 - loglik: -1.6160e+02 - logprior: 0.7270
Epoch 7/10
10/10 - 2s - loss: 160.1534 - loglik: -1.6138e+02 - logprior: 1.4816
Epoch 8/10
10/10 - 2s - loss: 159.5930 - loglik: -1.6133e+02 - logprior: 2.0098
Epoch 9/10
10/10 - 2s - loss: 159.2000 - loglik: -1.6134e+02 - logprior: 2.4164
Epoch 10/10
10/10 - 2s - loss: 158.9011 - loglik: -1.6140e+02 - logprior: 2.7768
Fitted a model with MAP estimate = -158.4485
Time for alignment: 57.3783
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.6821 - loglik: -2.4847e+02 - logprior: -6.9210e+01
Epoch 2/10
10/10 - 2s - loss: 236.8528 - loglik: -2.1889e+02 - logprior: -1.7946e+01
Epoch 3/10
10/10 - 2s - loss: 204.5325 - loglik: -1.9635e+02 - logprior: -8.0954e+00
Epoch 4/10
10/10 - 2s - loss: 191.5221 - loglik: -1.8699e+02 - logprior: -4.4907e+00
Epoch 5/10
10/10 - 2s - loss: 185.0706 - loglik: -1.8248e+02 - logprior: -2.5896e+00
Epoch 6/10
10/10 - 2s - loss: 181.0203 - loglik: -1.7947e+02 - logprior: -1.5440e+00
Epoch 7/10
10/10 - 2s - loss: 178.4577 - loglik: -1.7756e+02 - logprior: -8.9106e-01
Epoch 8/10
10/10 - 2s - loss: 176.9700 - loglik: -1.7626e+02 - logprior: -6.0332e-01
Epoch 9/10
10/10 - 2s - loss: 176.0646 - loglik: -1.7541e+02 - logprior: -4.0434e-01
Epoch 10/10
10/10 - 2s - loss: 175.2966 - loglik: -1.7478e+02 - logprior: -2.6286e-01
Fitted a model with MAP estimate = -174.8173
expansions: [(9, 3), (14, 1), (15, 1), (26, 1), (37, 1), (46, 1), (58, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.9134 - loglik: -1.7495e+02 - logprior: -7.7805e+01
Epoch 2/2
10/10 - 2s - loss: 200.0348 - loglik: -1.6822e+02 - logprior: -3.1727e+01
Fitted a model with MAP estimate = -190.9905
expansions: [(32, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 240.6448 - loglik: -1.6585e+02 - logprior: -7.4654e+01
Epoch 2/2
10/10 - 2s - loss: 186.2512 - loglik: -1.6353e+02 - logprior: -2.2575e+01
Fitted a model with MAP estimate = -175.0192
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 225.1036 - loglik: -1.6236e+02 - logprior: -6.2583e+01
Epoch 2/10
10/10 - 2s - loss: 178.3535 - loglik: -1.6217e+02 - logprior: -1.6043e+01
Epoch 3/10
10/10 - 2s - loss: 168.8162 - loglik: -1.6238e+02 - logprior: -6.3784e+00
Epoch 4/10
10/10 - 2s - loss: 164.8645 - loglik: -1.6236e+02 - logprior: -2.4155e+00
Epoch 5/10
10/10 - 2s - loss: 162.7264 - loglik: -1.6226e+02 - logprior: -3.0547e-01
Epoch 6/10
10/10 - 2s - loss: 161.4178 - loglik: -1.6216e+02 - logprior: 0.9210
Epoch 7/10
10/10 - 2s - loss: 160.4813 - loglik: -1.6192e+02 - logprior: 1.6506
Epoch 8/10
10/10 - 2s - loss: 159.9268 - loglik: -1.6182e+02 - logprior: 2.1730
Epoch 9/10
10/10 - 2s - loss: 159.5576 - loglik: -1.6190e+02 - logprior: 2.6193
Epoch 10/10
10/10 - 2s - loss: 159.2419 - loglik: -1.6198e+02 - logprior: 3.0200
Fitted a model with MAP estimate = -158.7747
Time for alignment: 66.6076
Computed alignments with likelihoods: ['-157.3342', '-158.4485', '-158.7747']
Best model has likelihood: -157.3342
time for generating output: 0.2374
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.3642077406549785
Training of 3 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fc8f8df0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19ae5e5940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fd0870a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19be9bd700>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fdbcf3a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1972890e80>, <__main__.SimpleDirichletPrior object at 0x7f197b6cc610>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19feb3fdc0>

Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 757.5638 - loglik: -7.4903e+02 - logprior: -8.4184e+00
Epoch 2/10
20/20 - 11s - loss: 689.3892 - loglik: -6.8837e+02 - logprior: -2.1861e-01
Epoch 3/10
20/20 - 11s - loss: 656.1129 - loglik: -6.5411e+02 - logprior: -8.0860e-01
Epoch 4/10
20/20 - 11s - loss: 641.4982 - loglik: -6.3879e+02 - logprior: -1.1267e+00
Epoch 5/10
20/20 - 11s - loss: 640.4183 - loglik: -6.3772e+02 - logprior: -1.0992e+00
Epoch 6/10
20/20 - 11s - loss: 635.9598 - loglik: -6.3346e+02 - logprior: -1.1462e+00
Epoch 7/10
20/20 - 11s - loss: 634.9343 - loglik: -6.3257e+02 - logprior: -1.1679e+00
Epoch 8/10
20/20 - 11s - loss: 632.6050 - loglik: -6.3030e+02 - logprior: -1.1874e+00
Epoch 9/10
20/20 - 11s - loss: 632.3241 - loglik: -6.3003e+02 - logprior: -1.2175e+00
Epoch 10/10
20/20 - 11s - loss: 631.6926 - loglik: -6.2941e+02 - logprior: -1.1935e+00
Fitted a model with MAP estimate = -630.2087
expansions: [(0, 4), (43, 1), (53, 1), (54, 3), (56, 1), (92, 1), (94, 1), (100, 1), (111, 4), (112, 2), (115, 2), (117, 1), (154, 1), (165, 1), (168, 1), (170, 10), (199, 4), (207, 1), (208, 1)]
discards: [  1 217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 263 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 683.6144 - loglik: -6.6867e+02 - logprior: -1.3612e+01
Epoch 2/2
20/20 - 13s - loss: 640.6225 - loglik: -6.3666e+02 - logprior: -1.9957e+00
Fitted a model with MAP estimate = -632.0444
expansions: [(21, 5), (176, 1), (231, 1)]
discards: [  2   3   4  46  60 126 127 133 192 233]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 260 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 649.2958 - loglik: -6.3840e+02 - logprior: -8.2751e+00
Epoch 2/2
20/20 - 13s - loss: 631.7858 - loglik: -6.2831e+02 - logprior: -6.6941e-01
Fitted a model with MAP estimate = -626.2857
expansions: [(0, 3), (50, 1), (195, 2)]
discards: [  1 126]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 17s - loss: 651.4406 - loglik: -6.3412e+02 - logprior: -1.4127e+01
Epoch 2/10
20/20 - 13s - loss: 633.5436 - loglik: -6.2865e+02 - logprior: -1.8273e+00
Epoch 3/10
20/20 - 13s - loss: 626.8787 - loglik: -6.2440e+02 - logprior: 0.2651
Epoch 4/10
20/20 - 13s - loss: 620.8534 - loglik: -6.1944e+02 - logprior: 0.9694
Epoch 5/10
20/20 - 13s - loss: 620.7404 - loglik: -6.1995e+02 - logprior: 1.2264
Epoch 6/10
20/20 - 13s - loss: 618.6874 - loglik: -6.1837e+02 - logprior: 1.4305
Epoch 7/10
20/20 - 13s - loss: 619.4190 - loglik: -6.1949e+02 - logprior: 1.6549
Fitted a model with MAP estimate = -615.0270
Time for alignment: 297.4400
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 756.2482 - loglik: -7.4772e+02 - logprior: -8.4136e+00
Epoch 2/10
20/20 - 11s - loss: 686.8772 - loglik: -6.8581e+02 - logprior: -2.6675e-01
Epoch 3/10
20/20 - 11s - loss: 654.4550 - loglik: -6.5230e+02 - logprior: -7.0106e-01
Epoch 4/10
20/20 - 10s - loss: 646.2725 - loglik: -6.4336e+02 - logprior: -9.0207e-01
Epoch 5/10
20/20 - 11s - loss: 636.3649 - loglik: -6.3346e+02 - logprior: -1.0234e+00
Epoch 6/10
20/20 - 11s - loss: 638.1005 - loglik: -6.3545e+02 - logprior: -1.1616e+00
Fitted a model with MAP estimate = -633.6589
expansions: [(0, 3), (21, 1), (44, 1), (49, 2), (51, 1), (53, 2), (80, 6), (81, 1), (86, 1), (93, 1), (111, 4), (112, 1), (115, 2), (117, 1), (151, 1), (154, 1), (169, 1), (171, 11), (176, 1), (198, 3), (201, 1), (207, 1), (208, 1)]
discards: [217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 271 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 672.6172 - loglik: -6.5820e+02 - logprior: -1.2663e+01
Epoch 2/2
20/20 - 14s - loss: 640.5051 - loglik: -6.3642e+02 - logprior: -1.6147e+00
Fitted a model with MAP estimate = -629.0660
expansions: []
discards: [  1   2   3   4  54 132 133 140]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 263 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 648.6500 - loglik: -6.3714e+02 - logprior: -8.4459e+00
Epoch 2/2
20/20 - 13s - loss: 631.4874 - loglik: -6.2763e+02 - logprior: -8.4873e-01
Fitted a model with MAP estimate = -626.5286
expansions: [(0, 3), (91, 4), (235, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 271 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 16s - loss: 649.7704 - loglik: -6.3245e+02 - logprior: -1.4074e+01
Epoch 2/10
20/20 - 13s - loss: 631.6945 - loglik: -6.2686e+02 - logprior: -1.7602e+00
Epoch 3/10
20/20 - 14s - loss: 625.6273 - loglik: -6.2287e+02 - logprior: -3.9921e-02
Epoch 4/10
20/20 - 14s - loss: 621.4435 - loglik: -6.1969e+02 - logprior: 0.6310
Epoch 5/10
20/20 - 13s - loss: 621.4174 - loglik: -6.2026e+02 - logprior: 0.8957
Epoch 6/10
20/20 - 13s - loss: 617.7302 - loglik: -6.1704e+02 - logprior: 1.1237
Epoch 7/10
20/20 - 13s - loss: 615.2415 - loglik: -6.1496e+02 - logprior: 1.3583
Epoch 8/10
20/20 - 14s - loss: 613.9457 - loglik: -6.1399e+02 - logprior: 1.5805
Epoch 9/10
20/20 - 14s - loss: 614.2277 - loglik: -6.1457e+02 - logprior: 1.7716
Fitted a model with MAP estimate = -611.6104
Time for alignment: 286.2963
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 756.9634 - loglik: -7.4842e+02 - logprior: -8.4243e+00
Epoch 2/10
20/20 - 11s - loss: 684.7665 - loglik: -6.8369e+02 - logprior: -2.8424e-01
Epoch 3/10
20/20 - 10s - loss: 653.3438 - loglik: -6.5114e+02 - logprior: -7.2911e-01
Epoch 4/10
20/20 - 11s - loss: 641.7043 - loglik: -6.3873e+02 - logprior: -9.5643e-01
Epoch 5/10
20/20 - 11s - loss: 638.5312 - loglik: -6.3586e+02 - logprior: -1.0235e+00
Epoch 6/10
20/20 - 11s - loss: 633.9916 - loglik: -6.3140e+02 - logprior: -1.1488e+00
Epoch 7/10
20/20 - 11s - loss: 633.7448 - loglik: -6.3125e+02 - logprior: -1.2077e+00
Epoch 8/10
20/20 - 11s - loss: 631.7576 - loglik: -6.2932e+02 - logprior: -1.2425e+00
Epoch 9/10
20/20 - 11s - loss: 630.5480 - loglik: -6.2821e+02 - logprior: -1.2312e+00
Epoch 10/10
20/20 - 11s - loss: 629.8881 - loglik: -6.2764e+02 - logprior: -1.2141e+00
Fitted a model with MAP estimate = -629.0133
expansions: [(0, 3), (19, 5), (25, 1), (50, 2), (52, 2), (76, 1), (83, 1), (86, 1), (87, 10), (95, 1), (99, 1), (112, 5), (113, 2), (116, 2), (117, 1), (148, 1), (154, 1), (169, 1), (171, 10), (199, 5), (207, 1), (208, 1)]
discards: [  1  38 217 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 279 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 17s - loss: 684.0159 - loglik: -6.6927e+02 - logprior: -1.3505e+01
Epoch 2/2
20/20 - 14s - loss: 637.6505 - loglik: -6.3381e+02 - logprior: -1.9905e+00
Fitted a model with MAP estimate = -628.6708
expansions: [(106, 1)]
discards: [  2   3  14  15  16  17  18  19  46  58 139 140 141 142 143 149 210]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 263 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 16s - loss: 649.9966 - loglik: -6.3924e+02 - logprior: -8.2694e+00
Epoch 2/2
20/20 - 13s - loss: 634.9249 - loglik: -6.3161e+02 - logprior: -6.4657e-01
Fitted a model with MAP estimate = -627.7712
expansions: [(0, 4), (15, 1), (177, 1), (198, 2)]
discards: [80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 270 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 17s - loss: 651.8048 - loglik: -6.3445e+02 - logprior: -1.4342e+01
Epoch 2/10
20/20 - 13s - loss: 634.8839 - loglik: -6.3009e+02 - logprior: -1.9172e+00
Epoch 3/10
20/20 - 13s - loss: 624.7750 - loglik: -6.2233e+02 - logprior: 0.1637
Epoch 4/10
20/20 - 13s - loss: 625.2717 - loglik: -6.2385e+02 - logprior: 0.8968
Fitted a model with MAP estimate = -618.8624
Time for alignment: 262.9473
Computed alignments with likelihoods: ['-615.0270', '-611.6104', '-618.8624']
Best model has likelihood: -611.6104
time for generating output: 0.4123
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.6497424576894776
Training of 3 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19ae54b310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fd4642b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fd464bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fd464e20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fd4641c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fd464e80>, <__main__.SimpleDirichletPrior object at 0x7f1994a993d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1950c22160>

Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.7208 - loglik: -1.3145e+02 - logprior: -3.2611e+00
Epoch 2/10
19/19 - 2s - loss: 113.1280 - loglik: -1.1169e+02 - logprior: -1.3896e+00
Epoch 3/10
19/19 - 2s - loss: 105.9554 - loglik: -1.0412e+02 - logprior: -1.5599e+00
Epoch 4/10
19/19 - 2s - loss: 103.6904 - loglik: -1.0197e+02 - logprior: -1.4821e+00
Epoch 5/10
19/19 - 2s - loss: 102.5860 - loglik: -1.0093e+02 - logprior: -1.4539e+00
Epoch 6/10
19/19 - 1s - loss: 102.2883 - loglik: -1.0067e+02 - logprior: -1.4516e+00
Epoch 7/10
19/19 - 1s - loss: 102.1102 - loglik: -1.0050e+02 - logprior: -1.4374e+00
Epoch 8/10
19/19 - 1s - loss: 101.9606 - loglik: -1.0037e+02 - logprior: -1.4303e+00
Epoch 9/10
19/19 - 1s - loss: 102.0255 - loglik: -1.0044e+02 - logprior: -1.4273e+00
Fitted a model with MAP estimate = -100.4175
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (23, 1), (27, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 104.5007 - loglik: -1.0002e+02 - logprior: -4.2410e+00
Epoch 2/2
19/19 - 1s - loss: 95.8895 - loglik: -9.3454e+01 - logprior: -2.1591e+00
Fitted a model with MAP estimate = -92.7255
expansions: [(0, 1)]
discards: [ 0  9 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 96.4123 - loglik: -9.3078e+01 - logprior: -3.0875e+00
Epoch 2/2
19/19 - 1s - loss: 93.3890 - loglik: -9.1642e+01 - logprior: -1.4651e+00
Fitted a model with MAP estimate = -91.1985
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.3031 - loglik: -9.0885e+01 - logprior: -3.1517e+00
Epoch 2/10
19/19 - 2s - loss: 91.4109 - loglik: -8.9668e+01 - logprior: -1.4736e+00
Epoch 3/10
19/19 - 2s - loss: 90.8945 - loglik: -8.9185e+01 - logprior: -1.3888e+00
Epoch 4/10
19/19 - 2s - loss: 90.4036 - loglik: -8.8735e+01 - logprior: -1.3507e+00
Epoch 5/10
19/19 - 2s - loss: 90.2677 - loglik: -8.8638e+01 - logprior: -1.3296e+00
Epoch 6/10
19/19 - 1s - loss: 89.9089 - loglik: -8.8328e+01 - logprior: -1.3131e+00
Epoch 7/10
19/19 - 2s - loss: 89.6628 - loglik: -8.8119e+01 - logprior: -1.3026e+00
Epoch 8/10
19/19 - 2s - loss: 89.6647 - loglik: -8.8146e+01 - logprior: -1.2910e+00
Fitted a model with MAP estimate = -89.4059
Time for alignment: 55.9563
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.8678 - loglik: -1.3159e+02 - logprior: -3.2652e+00
Epoch 2/10
19/19 - 1s - loss: 114.2857 - loglik: -1.1284e+02 - logprior: -1.3875e+00
Epoch 3/10
19/19 - 1s - loss: 106.8084 - loglik: -1.0493e+02 - logprior: -1.5550e+00
Epoch 4/10
19/19 - 2s - loss: 104.4870 - loglik: -1.0273e+02 - logprior: -1.4856e+00
Epoch 5/10
19/19 - 2s - loss: 103.8466 - loglik: -1.0215e+02 - logprior: -1.4496e+00
Epoch 6/10
19/19 - 2s - loss: 103.1078 - loglik: -1.0146e+02 - logprior: -1.4564e+00
Epoch 7/10
19/19 - 1s - loss: 102.6305 - loglik: -1.0100e+02 - logprior: -1.4491e+00
Epoch 8/10
19/19 - 2s - loss: 102.2728 - loglik: -1.0065e+02 - logprior: -1.4558e+00
Epoch 9/10
19/19 - 2s - loss: 102.2349 - loglik: -1.0063e+02 - logprior: -1.4534e+00
Epoch 10/10
19/19 - 2s - loss: 101.9911 - loglik: -1.0039e+02 - logprior: -1.4503e+00
Fitted a model with MAP estimate = -100.5463
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (28, 3), (29, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 104.7862 - loglik: -1.0031e+02 - logprior: -4.2556e+00
Epoch 2/2
19/19 - 1s - loss: 96.2476 - loglik: -9.3731e+01 - logprior: -2.1978e+00
Fitted a model with MAP estimate = -92.7932
expansions: [(0, 1)]
discards: [0 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 96.3369 - loglik: -9.2950e+01 - logprior: -3.1126e+00
Epoch 2/2
19/19 - 2s - loss: 93.2727 - loglik: -9.1472e+01 - logprior: -1.5017e+00
Fitted a model with MAP estimate = -90.9671
expansions: []
discards: [29]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.5177 - loglik: -9.1062e+01 - logprior: -3.1453e+00
Epoch 2/10
19/19 - 2s - loss: 91.8524 - loglik: -9.0051e+01 - logprior: -1.4775e+00
Epoch 3/10
19/19 - 2s - loss: 90.7652 - loglik: -8.9055e+01 - logprior: -1.3959e+00
Epoch 4/10
19/19 - 2s - loss: 90.4722 - loglik: -8.8793e+01 - logprior: -1.3598e+00
Epoch 5/10
19/19 - 1s - loss: 90.1409 - loglik: -8.8513e+01 - logprior: -1.3374e+00
Epoch 6/10
19/19 - 1s - loss: 90.0910 - loglik: -8.8510e+01 - logprior: -1.3211e+00
Epoch 7/10
19/19 - 1s - loss: 89.5438 - loglik: -8.8003e+01 - logprior: -1.3064e+00
Epoch 8/10
19/19 - 2s - loss: 89.8354 - loglik: -8.8315e+01 - logprior: -1.2998e+00
Fitted a model with MAP estimate = -89.4178
Time for alignment: 55.8962
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 134.7583 - loglik: -1.3149e+02 - logprior: -3.2612e+00
Epoch 2/10
19/19 - 1s - loss: 113.7635 - loglik: -1.1232e+02 - logprior: -1.3910e+00
Epoch 3/10
19/19 - 2s - loss: 106.5226 - loglik: -1.0464e+02 - logprior: -1.5649e+00
Epoch 4/10
19/19 - 2s - loss: 104.5415 - loglik: -1.0279e+02 - logprior: -1.4987e+00
Epoch 5/10
19/19 - 2s - loss: 103.8202 - loglik: -1.0212e+02 - logprior: -1.4755e+00
Epoch 6/10
19/19 - 1s - loss: 102.7276 - loglik: -1.0107e+02 - logprior: -1.4873e+00
Epoch 7/10
19/19 - 2s - loss: 102.0278 - loglik: -1.0038e+02 - logprior: -1.4912e+00
Epoch 8/10
19/19 - 1s - loss: 101.9451 - loglik: -1.0030e+02 - logprior: -1.4969e+00
Epoch 9/10
19/19 - 1s - loss: 101.8747 - loglik: -1.0024e+02 - logprior: -1.4898e+00
Epoch 10/10
19/19 - 1s - loss: 101.8572 - loglik: -1.0022e+02 - logprior: -1.4853e+00
Fitted a model with MAP estimate = -100.3230
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (23, 1), (28, 2), (29, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 104.3943 - loglik: -9.9932e+01 - logprior: -4.2501e+00
Epoch 2/2
19/19 - 2s - loss: 95.8402 - loglik: -9.3382e+01 - logprior: -2.1703e+00
Fitted a model with MAP estimate = -92.7433
expansions: [(0, 1)]
discards: [ 0  9 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.4259 - loglik: -9.3038e+01 - logprior: -3.1355e+00
Epoch 2/2
19/19 - 2s - loss: 93.2653 - loglik: -9.1489e+01 - logprior: -1.4940e+00
Fitted a model with MAP estimate = -91.0933
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 94.3691 - loglik: -9.0902e+01 - logprior: -3.1982e+00
Epoch 2/10
19/19 - 2s - loss: 91.3819 - loglik: -8.9622e+01 - logprior: -1.4858e+00
Epoch 3/10
19/19 - 1s - loss: 90.8556 - loglik: -8.9156e+01 - logprior: -1.3799e+00
Epoch 4/10
19/19 - 1s - loss: 90.5953 - loglik: -8.8925e+01 - logprior: -1.3563e+00
Epoch 5/10
19/19 - 1s - loss: 89.9866 - loglik: -8.8355e+01 - logprior: -1.3419e+00
Epoch 6/10
19/19 - 1s - loss: 90.1468 - loglik: -8.8565e+01 - logprior: -1.3242e+00
Fitted a model with MAP estimate = -89.5634
Time for alignment: 53.4129
Computed alignments with likelihoods: ['-89.4059', '-89.4178', '-89.5634']
Best model has likelihood: -89.4059
time for generating output: 0.1109
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9679358717434869
Training of 3 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19bef401f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1972f8cd90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1959357fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f197b148400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fc924a60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fdfeee20>, <__main__.SimpleDirichletPrior object at 0x7f19597b3100>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1994bb2ee0>

Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 652.7516 - loglik: -6.3318e+02 - logprior: -1.9505e+01
Epoch 2/10
15/15 - 7s - loss: 571.6125 - loglik: -5.6987e+02 - logprior: -1.4795e+00
Epoch 3/10
15/15 - 8s - loss: 506.0612 - loglik: -5.0451e+02 - logprior: -1.3223e+00
Epoch 4/10
15/15 - 7s - loss: 478.0655 - loglik: -4.7519e+02 - logprior: -2.2675e+00
Epoch 5/10
15/15 - 8s - loss: 469.6363 - loglik: -4.6643e+02 - logprior: -2.5067e+00
Epoch 6/10
15/15 - 8s - loss: 462.2492 - loglik: -4.5922e+02 - logprior: -2.4208e+00
Epoch 7/10
15/15 - 8s - loss: 466.5821 - loglik: -4.6373e+02 - logprior: -2.2641e+00
Fitted a model with MAP estimate = -463.5344
expansions: [(22, 1), (23, 2), (24, 1), (25, 1), (37, 3), (39, 1), (41, 1), (45, 1), (48, 1), (49, 1), (52, 1), (54, 2), (69, 5), (70, 2), (73, 1), (74, 1), (75, 1), (77, 1), (80, 1), (95, 1), (99, 2), (101, 1), (102, 2), (103, 4), (109, 1), (131, 2), (132, 2), (133, 2), (134, 2), (135, 3), (137, 1), (157, 1), (158, 5), (159, 2), (174, 7), (198, 1)]
discards: [  1   2 200 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 472.0641 - loglik: -4.5308e+02 - logprior: -1.8305e+01
Epoch 2/2
15/15 - 10s - loss: 439.2746 - loglik: -4.3702e+02 - logprior: -1.4332e+00
Fitted a model with MAP estimate = -431.1490
expansions: [(0, 3), (18, 1)]
discards: [ 22  87 132 171 175 180 236 237]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 460.8911 - loglik: -4.3370e+02 - logprior: -2.6312e+01
Epoch 2/2
15/15 - 10s - loss: 434.4198 - loglik: -4.2978e+02 - logprior: -3.8450e+00
Fitted a model with MAP estimate = -428.7336
expansions: [(207, 1)]
discards: [  0   1   2 169 233 234]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 279 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 450.9571 - loglik: -4.3106e+02 - logprior: -1.9040e+01
Epoch 2/10
15/15 - 10s - loss: 432.9616 - loglik: -4.3234e+02 - logprior: 0.1916
Epoch 3/10
15/15 - 10s - loss: 428.5274 - loglik: -4.3086e+02 - logprior: 3.0827
Epoch 4/10
15/15 - 10s - loss: 425.3113 - loglik: -4.2865e+02 - logprior: 3.9958
Epoch 5/10
15/15 - 10s - loss: 425.6104 - loglik: -4.2957e+02 - logprior: 4.5555
Fitted a model with MAP estimate = -423.6095
Time for alignment: 176.9571
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 653.6971 - loglik: -6.3413e+02 - logprior: -1.9507e+01
Epoch 2/10
15/15 - 7s - loss: 572.1130 - loglik: -5.7052e+02 - logprior: -1.3324e+00
Epoch 3/10
15/15 - 8s - loss: 510.6070 - loglik: -5.0964e+02 - logprior: -7.1113e-01
Epoch 4/10
15/15 - 8s - loss: 484.4931 - loglik: -4.8248e+02 - logprior: -1.3545e+00
Epoch 5/10
15/15 - 8s - loss: 474.0097 - loglik: -4.7166e+02 - logprior: -1.6231e+00
Epoch 6/10
15/15 - 8s - loss: 469.6527 - loglik: -4.6762e+02 - logprior: -1.4968e+00
Epoch 7/10
15/15 - 8s - loss: 468.8062 - loglik: -4.6702e+02 - logprior: -1.3196e+00
Epoch 8/10
15/15 - 8s - loss: 470.6153 - loglik: -4.6898e+02 - logprior: -1.1790e+00
Fitted a model with MAP estimate = -466.8265
expansions: [(26, 4), (40, 1), (45, 1), (46, 1), (47, 2), (49, 1), (50, 2), (51, 5), (54, 1), (64, 3), (66, 2), (67, 1), (68, 1), (70, 2), (71, 1), (74, 2), (83, 1), (99, 1), (100, 2), (101, 4), (130, 2), (131, 2), (132, 2), (133, 2), (134, 3), (136, 1), (156, 6), (174, 8), (198, 1)]
discards: [  1   2 203 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 476.8253 - loglik: -4.5816e+02 - logprior: -1.8125e+01
Epoch 2/2
15/15 - 10s - loss: 441.3219 - loglik: -4.3902e+02 - logprior: -1.5867e+00
Fitted a model with MAP estimate = -434.4404
expansions: [(61, 1), (62, 1), (65, 1)]
discards: [  4  81  85  86 130 169 175 178 234 235 236]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 278 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 457.8782 - loglik: -4.3985e+02 - logprior: -1.7199e+01
Epoch 2/2
15/15 - 10s - loss: 432.0900 - loglik: -4.3082e+02 - logprior: -4.5091e-01
Fitted a model with MAP estimate = -429.9627
expansions: [(21, 2), (199, 2)]
discards: [164 227]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 280 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 13s - loss: 451.2566 - loglik: -4.3394e+02 - logprior: -1.6422e+01
Epoch 2/10
15/15 - 10s - loss: 430.1357 - loglik: -4.2967e+02 - logprior: 0.3570
Epoch 3/10
15/15 - 10s - loss: 427.1595 - loglik: -4.2943e+02 - logprior: 3.0385
Epoch 4/10
15/15 - 10s - loss: 426.1726 - loglik: -4.2957e+02 - logprior: 4.0807
Epoch 5/10
15/15 - 10s - loss: 424.1205 - loglik: -4.2817e+02 - logprior: 4.6643
Epoch 6/10
15/15 - 10s - loss: 423.9960 - loglik: -4.2860e+02 - logprior: 5.1750
Epoch 7/10
15/15 - 10s - loss: 422.6210 - loglik: -4.2763e+02 - logprior: 5.5733
Epoch 8/10
15/15 - 10s - loss: 422.5315 - loglik: -4.2791e+02 - logprior: 5.9245
Epoch 9/10
15/15 - 10s - loss: 420.8532 - loglik: -4.2659e+02 - logprior: 6.2797
Epoch 10/10
15/15 - 10s - loss: 420.9555 - loglik: -4.2704e+02 - logprior: 6.6255
Fitted a model with MAP estimate = -420.1610
Time for alignment: 232.7070
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 655.2648 - loglik: -6.3569e+02 - logprior: -1.9512e+01
Epoch 2/10
15/15 - 8s - loss: 572.0464 - loglik: -5.7040e+02 - logprior: -1.3639e+00
Epoch 3/10
15/15 - 8s - loss: 514.0799 - loglik: -5.1283e+02 - logprior: -1.0122e+00
Epoch 4/10
15/15 - 7s - loss: 484.6985 - loglik: -4.8229e+02 - logprior: -1.8550e+00
Epoch 5/10
15/15 - 8s - loss: 469.9370 - loglik: -4.6671e+02 - logprior: -2.4889e+00
Epoch 6/10
15/15 - 8s - loss: 468.3389 - loglik: -4.6527e+02 - logprior: -2.4365e+00
Epoch 7/10
15/15 - 7s - loss: 466.1702 - loglik: -4.6326e+02 - logprior: -2.3534e+00
Epoch 8/10
15/15 - 8s - loss: 468.3840 - loglik: -4.6564e+02 - logprior: -2.1951e+00
Fitted a model with MAP estimate = -464.2407
expansions: [(20, 1), (24, 1), (25, 2), (26, 3), (38, 2), (40, 1), (50, 1), (52, 1), (53, 1), (54, 1), (55, 2), (66, 1), (67, 1), (68, 5), (69, 2), (70, 1), (71, 1), (72, 1), (73, 2), (75, 1), (77, 1), (84, 1), (92, 1), (99, 1), (100, 5), (102, 2), (106, 1), (128, 2), (129, 2), (130, 1), (131, 1), (133, 1), (136, 1), (156, 1), (157, 5), (158, 2), (173, 8), (182, 1), (198, 1)]
discards: [  1   2   6   7 203 223]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 14s - loss: 474.4261 - loglik: -4.5577e+02 - logprior: -1.8035e+01
Epoch 2/2
15/15 - 10s - loss: 437.7275 - loglik: -4.3559e+02 - logprior: -1.3510e+00
Fitted a model with MAP estimate = -430.5732
expansions: [(42, 1), (62, 1)]
discards: [ 88  89 138 170 233]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 450.2508 - loglik: -4.3241e+02 - logprior: -1.6924e+01
Epoch 2/2
15/15 - 10s - loss: 431.0620 - loglik: -4.3004e+02 - logprior: -1.8857e-01
Fitted a model with MAP estimate = -426.5269
expansions: [(204, 1)]
discards: [166]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 14s - loss: 448.7374 - loglik: -4.3165e+02 - logprior: -1.6230e+01
Epoch 2/10
15/15 - 10s - loss: 426.5711 - loglik: -4.2631e+02 - logprior: 0.5793
Epoch 3/10
15/15 - 10s - loss: 426.1575 - loglik: -4.2868e+02 - logprior: 3.3011
Epoch 4/10
15/15 - 10s - loss: 423.2778 - loglik: -4.2693e+02 - logprior: 4.3203
Epoch 5/10
15/15 - 10s - loss: 421.2788 - loglik: -4.2553e+02 - logprior: 4.8594
Epoch 6/10
15/15 - 10s - loss: 422.3498 - loglik: -4.2713e+02 - logprior: 5.3342
Fitted a model with MAP estimate = -419.6015
Time for alignment: 196.7669
Computed alignments with likelihoods: ['-423.6095', '-420.1610', '-419.6015']
Best model has likelihood: -419.6015
time for generating output: 0.3671
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.9073137849679811
Training of 3 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fe88db50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fe116d90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19c7ab4a90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19c7ab48e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f197b71e5b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f197b673d90>, <__main__.SimpleDirichletPrior object at 0x7f198b888400>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19c7983ca0>

Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.1395 - loglik: -1.8513e+02 - logprior: -4.1987e+01
Epoch 2/10
10/10 - 1s - loss: 180.6384 - loglik: -1.6910e+02 - logprior: -1.1507e+01
Epoch 3/10
10/10 - 1s - loss: 162.6940 - loglik: -1.5686e+02 - logprior: -5.8326e+00
Epoch 4/10
10/10 - 1s - loss: 153.7620 - loglik: -1.4985e+02 - logprior: -3.8837e+00
Epoch 5/10
10/10 - 1s - loss: 148.8520 - loglik: -1.4580e+02 - logprior: -3.0085e+00
Epoch 6/10
10/10 - 1s - loss: 145.5514 - loglik: -1.4273e+02 - logprior: -2.6100e+00
Epoch 7/10
10/10 - 1s - loss: 144.1606 - loglik: -1.4139e+02 - logprior: -2.4456e+00
Epoch 8/10
10/10 - 1s - loss: 143.0999 - loglik: -1.4053e+02 - logprior: -2.3088e+00
Epoch 9/10
10/10 - 1s - loss: 142.8785 - loglik: -1.4050e+02 - logprior: -2.1475e+00
Epoch 10/10
10/10 - 1s - loss: 142.5550 - loglik: -1.4033e+02 - logprior: -1.9750e+00
Fitted a model with MAP estimate = -142.2544
expansions: [(0, 2), (17, 1), (18, 3), (22, 1), (24, 1), (28, 1), (37, 1), (38, 1), (42, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 195.9730 - loglik: -1.4026e+02 - logprior: -5.5453e+01
Epoch 2/2
10/10 - 1s - loss: 151.5925 - loglik: -1.3403e+02 - logprior: -1.7238e+01
Fitted a model with MAP estimate = -143.3685
expansions: [(3, 1), (18, 1)]
discards: [ 0 21]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.3307 - loglik: -1.3420e+02 - logprior: -4.7847e+01
Epoch 2/2
10/10 - 1s - loss: 152.6271 - loglik: -1.3353e+02 - logprior: -1.8841e+01
Fitted a model with MAP estimate = -147.1141
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.7026 - loglik: -1.3343e+02 - logprior: -4.2994e+01
Epoch 2/10
10/10 - 1s - loss: 144.7147 - loglik: -1.3245e+02 - logprior: -1.2006e+01
Epoch 3/10
10/10 - 1s - loss: 137.6917 - loglik: -1.3237e+02 - logprior: -5.0057e+00
Epoch 4/10
10/10 - 1s - loss: 134.6835 - loglik: -1.3186e+02 - logprior: -2.4999e+00
Epoch 5/10
10/10 - 1s - loss: 133.1939 - loglik: -1.3159e+02 - logprior: -1.2999e+00
Epoch 6/10
10/10 - 1s - loss: 132.3936 - loglik: -1.3134e+02 - logprior: -7.4276e-01
Epoch 7/10
10/10 - 1s - loss: 132.0150 - loglik: -1.3123e+02 - logprior: -4.8117e-01
Epoch 8/10
10/10 - 1s - loss: 131.5850 - loglik: -1.3107e+02 - logprior: -2.0566e-01
Epoch 9/10
10/10 - 1s - loss: 131.5211 - loglik: -1.3131e+02 - logprior: 0.0920
Epoch 10/10
10/10 - 1s - loss: 131.2793 - loglik: -1.3128e+02 - logprior: 0.3059
Fitted a model with MAP estimate = -130.8344
Time for alignment: 35.0581
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.0057 - loglik: -1.8500e+02 - logprior: -4.1988e+01
Epoch 2/10
10/10 - 1s - loss: 180.4763 - loglik: -1.6894e+02 - logprior: -1.1507e+01
Epoch 3/10
10/10 - 1s - loss: 161.0366 - loglik: -1.5522e+02 - logprior: -5.8133e+00
Epoch 4/10
10/10 - 1s - loss: 151.6216 - loglik: -1.4774e+02 - logprior: -3.8358e+00
Epoch 5/10
10/10 - 1s - loss: 147.4917 - loglik: -1.4443e+02 - logprior: -2.8767e+00
Epoch 6/10
10/10 - 1s - loss: 145.7912 - loglik: -1.4311e+02 - logprior: -2.3973e+00
Epoch 7/10
10/10 - 1s - loss: 144.2054 - loglik: -1.4174e+02 - logprior: -2.2080e+00
Epoch 8/10
10/10 - 1s - loss: 143.8958 - loglik: -1.4162e+02 - logprior: -2.0408e+00
Epoch 9/10
10/10 - 1s - loss: 143.2014 - loglik: -1.4117e+02 - logprior: -1.7970e+00
Epoch 10/10
10/10 - 1s - loss: 143.1517 - loglik: -1.4127e+02 - logprior: -1.6364e+00
Fitted a model with MAP estimate = -142.7795
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 1), (24, 1), (37, 1), (43, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 195.2842 - loglik: -1.3968e+02 - logprior: -5.5355e+01
Epoch 2/2
10/10 - 1s - loss: 151.5740 - loglik: -1.3414e+02 - logprior: -1.7128e+01
Fitted a model with MAP estimate = -143.5031
expansions: [(19, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.1511 - loglik: -1.3421e+02 - logprior: -4.7665e+01
Epoch 2/2
10/10 - 1s - loss: 152.2628 - loglik: -1.3307e+02 - logprior: -1.8941e+01
Fitted a model with MAP estimate = -146.8986
expansions: []
discards: [ 0 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.5976 - loglik: -1.3333e+02 - logprior: -4.2972e+01
Epoch 2/10
10/10 - 1s - loss: 144.6952 - loglik: -1.3246e+02 - logprior: -1.1960e+01
Epoch 3/10
10/10 - 1s - loss: 136.9453 - loglik: -1.3166e+02 - logprior: -4.9424e+00
Epoch 4/10
10/10 - 1s - loss: 134.2482 - loglik: -1.3147e+02 - logprior: -2.4505e+00
Epoch 5/10
10/10 - 1s - loss: 132.8153 - loglik: -1.3125e+02 - logprior: -1.2550e+00
Epoch 6/10
10/10 - 1s - loss: 132.1166 - loglik: -1.3112e+02 - logprior: -6.9746e-01
Epoch 7/10
10/10 - 1s - loss: 131.4247 - loglik: -1.3069e+02 - logprior: -4.3310e-01
Epoch 8/10
10/10 - 1s - loss: 131.1650 - loglik: -1.3070e+02 - logprior: -1.6284e-01
Epoch 9/10
10/10 - 1s - loss: 130.9807 - loglik: -1.3082e+02 - logprior: 0.1374
Epoch 10/10
10/10 - 1s - loss: 130.8072 - loglik: -1.3085e+02 - logprior: 0.3526
Fitted a model with MAP estimate = -130.4029
Time for alignment: 35.0757
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.0818 - loglik: -1.8507e+02 - logprior: -4.1986e+01
Epoch 2/10
10/10 - 1s - loss: 180.9077 - loglik: -1.6938e+02 - logprior: -1.1499e+01
Epoch 3/10
10/10 - 1s - loss: 161.2660 - loglik: -1.5544e+02 - logprior: -5.8256e+00
Epoch 4/10
10/10 - 1s - loss: 151.4743 - loglik: -1.4755e+02 - logprior: -3.8783e+00
Epoch 5/10
10/10 - 1s - loss: 147.4845 - loglik: -1.4437e+02 - logprior: -2.9124e+00
Epoch 6/10
10/10 - 1s - loss: 145.5614 - loglik: -1.4279e+02 - logprior: -2.4321e+00
Epoch 7/10
10/10 - 1s - loss: 144.5820 - loglik: -1.4206e+02 - logprior: -2.2186e+00
Epoch 8/10
10/10 - 1s - loss: 144.2891 - loglik: -1.4204e+02 - logprior: -1.9832e+00
Epoch 9/10
10/10 - 1s - loss: 143.7092 - loglik: -1.4166e+02 - logprior: -1.7752e+00
Epoch 10/10
10/10 - 1s - loss: 143.4589 - loglik: -1.4153e+02 - logprior: -1.6367e+00
Fitted a model with MAP estimate = -143.0357
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 2), (24, 1), (28, 1), (37, 1), (43, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 195.6932 - loglik: -1.4010e+02 - logprior: -5.5307e+01
Epoch 2/2
10/10 - 1s - loss: 151.9657 - loglik: -1.3444e+02 - logprior: -1.7194e+01
Fitted a model with MAP estimate = -143.4508
expansions: [(19, 1)]
discards: [ 0 22 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 182.3733 - loglik: -1.3432e+02 - logprior: -4.7731e+01
Epoch 2/2
10/10 - 1s - loss: 152.3901 - loglik: -1.3330e+02 - logprior: -1.8818e+01
Fitted a model with MAP estimate = -146.9495
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.5616 - loglik: -1.3319e+02 - logprior: -4.3073e+01
Epoch 2/10
10/10 - 1s - loss: 144.5365 - loglik: -1.3223e+02 - logprior: -1.2031e+01
Epoch 3/10
10/10 - 1s - loss: 137.1121 - loglik: -1.3181e+02 - logprior: -4.9759e+00
Epoch 4/10
10/10 - 1s - loss: 134.1451 - loglik: -1.3136e+02 - logprior: -2.4611e+00
Epoch 5/10
10/10 - 1s - loss: 132.7539 - loglik: -1.3118e+02 - logprior: -1.2691e+00
Epoch 6/10
10/10 - 1s - loss: 131.9638 - loglik: -1.3095e+02 - logprior: -7.1589e-01
Epoch 7/10
10/10 - 1s - loss: 131.6524 - loglik: -1.3091e+02 - logprior: -4.4655e-01
Epoch 8/10
10/10 - 1s - loss: 131.1073 - loglik: -1.3065e+02 - logprior: -1.5428e-01
Epoch 9/10
10/10 - 1s - loss: 131.1838 - loglik: -1.3103e+02 - logprior: 0.1550
Fitted a model with MAP estimate = -130.6145
Time for alignment: 34.7103
Computed alignments with likelihoods: ['-130.8344', '-130.4029', '-130.6145']
Best model has likelihood: -130.4029
time for generating output: 0.1232
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.9039276631816897
Training of 3 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19a5ac3d90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1972e8f550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198b888f10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1950cf1160>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1950ce4520>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19595b4850>, <__main__.SimpleDirichletPrior object at 0x7f199d1d0460>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe981b80>

Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.8796 - loglik: -2.3754e+02 - logprior: -1.3320e+01
Epoch 2/10
11/11 - 2s - loss: 220.3501 - loglik: -2.1673e+02 - logprior: -3.5529e+00
Epoch 3/10
11/11 - 2s - loss: 200.6185 - loglik: -1.9809e+02 - logprior: -2.2954e+00
Epoch 4/10
11/11 - 2s - loss: 189.2284 - loglik: -1.8639e+02 - logprior: -2.2723e+00
Epoch 5/10
11/11 - 2s - loss: 185.6341 - loglik: -1.8279e+02 - logprior: -2.2819e+00
Epoch 6/10
11/11 - 2s - loss: 185.3378 - loglik: -1.8278e+02 - logprior: -2.1280e+00
Epoch 7/10
11/11 - 2s - loss: 184.4609 - loglik: -1.8211e+02 - logprior: -1.9773e+00
Epoch 8/10
11/11 - 2s - loss: 183.8060 - loglik: -1.8153e+02 - logprior: -1.9375e+00
Epoch 9/10
11/11 - 2s - loss: 183.5650 - loglik: -1.8125e+02 - logprior: -1.9652e+00
Epoch 10/10
11/11 - 2s - loss: 183.1404 - loglik: -1.8075e+02 - logprior: -2.0101e+00
Fitted a model with MAP estimate = -182.5744
expansions: [(8, 2), (9, 2), (10, 3), (17, 1), (24, 1), (25, 1), (31, 1), (32, 1), (35, 1), (45, 1), (48, 1), (50, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 201.3414 - loglik: -1.8560e+02 - logprior: -1.5277e+01
Epoch 2/2
11/11 - 2s - loss: 184.9173 - loglik: -1.7765e+02 - logprior: -6.6290e+00
Fitted a model with MAP estimate = -180.7371
expansions: [(0, 2)]
discards: [ 0 10 11 14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 187.9671 - loglik: -1.7519e+02 - logprior: -1.2067e+01
Epoch 2/2
11/11 - 2s - loss: 176.7379 - loglik: -1.7270e+02 - logprior: -3.3848e+00
Fitted a model with MAP estimate = -174.9617
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 190.6472 - loglik: -1.7558e+02 - logprior: -1.4363e+01
Epoch 2/10
11/11 - 2s - loss: 178.8453 - loglik: -1.7386e+02 - logprior: -4.3149e+00
Epoch 3/10
11/11 - 2s - loss: 175.6896 - loglik: -1.7256e+02 - logprior: -2.4238e+00
Epoch 4/10
11/11 - 2s - loss: 174.3212 - loglik: -1.7190e+02 - logprior: -1.7299e+00
Epoch 5/10
11/11 - 2s - loss: 173.7135 - loglik: -1.7177e+02 - logprior: -1.3298e+00
Epoch 6/10
11/11 - 2s - loss: 173.4652 - loglik: -1.7167e+02 - logprior: -1.2402e+00
Epoch 7/10
11/11 - 2s - loss: 172.8707 - loglik: -1.7122e+02 - logprior: -1.1254e+00
Epoch 8/10
11/11 - 2s - loss: 172.1326 - loglik: -1.7055e+02 - logprior: -1.0905e+00
Epoch 9/10
11/11 - 2s - loss: 172.2100 - loglik: -1.7065e+02 - logprior: -1.0657e+00
Fitted a model with MAP estimate = -171.5805
Time for alignment: 61.1396
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.5505 - loglik: -2.3721e+02 - logprior: -1.3320e+01
Epoch 2/10
11/11 - 2s - loss: 221.3009 - loglik: -2.1768e+02 - logprior: -3.5547e+00
Epoch 3/10
11/11 - 2s - loss: 200.4380 - loglik: -1.9791e+02 - logprior: -2.3050e+00
Epoch 4/10
11/11 - 2s - loss: 189.8767 - loglik: -1.8704e+02 - logprior: -2.2774e+00
Epoch 5/10
11/11 - 2s - loss: 186.6922 - loglik: -1.8383e+02 - logprior: -2.2909e+00
Epoch 6/10
11/11 - 2s - loss: 184.4698 - loglik: -1.8195e+02 - logprior: -2.1269e+00
Epoch 7/10
11/11 - 2s - loss: 184.4938 - loglik: -1.8219e+02 - logprior: -1.9756e+00
Fitted a model with MAP estimate = -183.4577
expansions: [(8, 2), (9, 1), (10, 2), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 199.4324 - loglik: -1.8381e+02 - logprior: -1.5207e+01
Epoch 2/2
11/11 - 2s - loss: 183.9584 - loglik: -1.7685e+02 - logprior: -6.5156e+00
Fitted a model with MAP estimate = -180.6129
expansions: [(0, 2)]
discards: [ 0 10 43]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 187.8333 - loglik: -1.7516e+02 - logprior: -1.2030e+01
Epoch 2/2
11/11 - 2s - loss: 177.8154 - loglik: -1.7383e+02 - logprior: -3.3451e+00
Fitted a model with MAP estimate = -174.8362
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 190.6626 - loglik: -1.7554e+02 - logprior: -1.4440e+01
Epoch 2/10
11/11 - 2s - loss: 179.0113 - loglik: -1.7393e+02 - logprior: -4.4202e+00
Epoch 3/10
11/11 - 2s - loss: 175.8133 - loglik: -1.7264e+02 - logprior: -2.4541e+00
Epoch 4/10
11/11 - 2s - loss: 174.1640 - loglik: -1.7172e+02 - logprior: -1.7536e+00
Epoch 5/10
11/11 - 2s - loss: 173.6641 - loglik: -1.7169e+02 - logprior: -1.3513e+00
Epoch 6/10
11/11 - 2s - loss: 173.4899 - loglik: -1.7164e+02 - logprior: -1.2716e+00
Epoch 7/10
11/11 - 2s - loss: 172.2321 - loglik: -1.7054e+02 - logprior: -1.1573e+00
Epoch 8/10
11/11 - 2s - loss: 172.1557 - loglik: -1.7051e+02 - logprior: -1.1227e+00
Epoch 9/10
11/11 - 2s - loss: 172.6658 - loglik: -1.7105e+02 - logprior: -1.1145e+00
Fitted a model with MAP estimate = -171.4475
Time for alignment: 56.2165
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 250.9112 - loglik: -2.3757e+02 - logprior: -1.3318e+01
Epoch 2/10
11/11 - 2s - loss: 221.1442 - loglik: -2.1752e+02 - logprior: -3.5546e+00
Epoch 3/10
11/11 - 2s - loss: 199.3143 - loglik: -1.9679e+02 - logprior: -2.2966e+00
Epoch 4/10
11/11 - 2s - loss: 189.6325 - loglik: -1.8683e+02 - logprior: -2.2440e+00
Epoch 5/10
11/11 - 2s - loss: 185.7372 - loglik: -1.8293e+02 - logprior: -2.2413e+00
Epoch 6/10
11/11 - 2s - loss: 185.3096 - loglik: -1.8280e+02 - logprior: -2.1017e+00
Epoch 7/10
11/11 - 2s - loss: 184.2739 - loglik: -1.8190e+02 - logprior: -1.9874e+00
Epoch 8/10
11/11 - 2s - loss: 183.0472 - loglik: -1.8068e+02 - logprior: -1.9781e+00
Epoch 9/10
11/11 - 2s - loss: 182.7632 - loglik: -1.8033e+02 - logprior: -2.0211e+00
Epoch 10/10
11/11 - 2s - loss: 182.5260 - loglik: -1.8004e+02 - logprior: -2.0481e+00
Fitted a model with MAP estimate = -181.7950
expansions: [(9, 1), (10, 1), (11, 2), (12, 2), (24, 1), (25, 1), (31, 1), (32, 1), (35, 1), (45, 1), (48, 1), (50, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 199.8331 - loglik: -1.8404e+02 - logprior: -1.5285e+01
Epoch 2/2
11/11 - 2s - loss: 184.2844 - loglik: -1.7703e+02 - logprior: -6.5884e+00
Fitted a model with MAP estimate = -180.0568
expansions: [(0, 2)]
discards: [ 0 15]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 187.2378 - loglik: -1.7443e+02 - logprior: -1.2076e+01
Epoch 2/2
11/11 - 2s - loss: 177.3233 - loglik: -1.7329e+02 - logprior: -3.3929e+00
Fitted a model with MAP estimate = -174.8563
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 190.3366 - loglik: -1.7520e+02 - logprior: -1.4392e+01
Epoch 2/10
11/11 - 2s - loss: 178.7665 - loglik: -1.7372e+02 - logprior: -4.3580e+00
Epoch 3/10
11/11 - 2s - loss: 176.2654 - loglik: -1.7311e+02 - logprior: -2.4328e+00
Epoch 4/10
11/11 - 2s - loss: 174.1833 - loglik: -1.7177e+02 - logprior: -1.7356e+00
Epoch 5/10
11/11 - 2s - loss: 174.3012 - loglik: -1.7235e+02 - logprior: -1.3366e+00
Fitted a model with MAP estimate = -172.7385
Time for alignment: 53.7071
Computed alignments with likelihoods: ['-171.5805', '-171.4475', '-172.7385']
Best model has likelihood: -171.4475
time for generating output: 0.1435
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.8193172356369692
Training of 3 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fccf50a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f198c6d5d90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1994939e50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fcdd43d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198c6e9700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f198c6e9250>, <__main__.SimpleDirichletPrior object at 0x7f1959141af0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f197b550b80>

Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.8618 - loglik: -2.2382e+02 - logprior: -4.1024e+01
Epoch 2/10
10/10 - 1s - loss: 209.0171 - loglik: -1.9787e+02 - logprior: -1.1139e+01
Epoch 3/10
10/10 - 1s - loss: 179.5043 - loglik: -1.7362e+02 - logprior: -5.8723e+00
Epoch 4/10
10/10 - 1s - loss: 157.8901 - loglik: -1.5357e+02 - logprior: -4.3048e+00
Epoch 5/10
10/10 - 1s - loss: 147.8961 - loglik: -1.4407e+02 - logprior: -3.7648e+00
Epoch 6/10
10/10 - 1s - loss: 144.0567 - loglik: -1.4029e+02 - logprior: -3.4854e+00
Epoch 7/10
10/10 - 1s - loss: 142.2749 - loglik: -1.3866e+02 - logprior: -3.1932e+00
Epoch 8/10
10/10 - 1s - loss: 141.6825 - loglik: -1.3835e+02 - logprior: -2.9479e+00
Epoch 9/10
10/10 - 1s - loss: 140.9403 - loglik: -1.3782e+02 - logprior: -2.7794e+00
Epoch 10/10
10/10 - 1s - loss: 140.9960 - loglik: -1.3794e+02 - logprior: -2.7088e+00
Fitted a model with MAP estimate = -140.4077
expansions: [(2, 1), (5, 1), (10, 1), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 172.2606 - loglik: -1.3445e+02 - logprior: -3.7495e+01
Epoch 2/2
10/10 - 1s - loss: 138.3872 - loglik: -1.2785e+02 - logprior: -1.0362e+01
Fitted a model with MAP estimate = -133.2555
expansions: []
discards: [45 58 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 163.4781 - loglik: -1.2686e+02 - logprior: -3.6474e+01
Epoch 2/2
10/10 - 1s - loss: 135.8542 - loglik: -1.2580e+02 - logprior: -9.8901e+00
Fitted a model with MAP estimate = -131.8339
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 176.2140 - loglik: -1.3047e+02 - logprior: -4.5566e+01
Epoch 2/10
10/10 - 1s - loss: 148.6228 - loglik: -1.2951e+02 - logprior: -1.8910e+01
Epoch 3/10
10/10 - 1s - loss: 142.1192 - loglik: -1.2841e+02 - logprior: -1.3456e+01
Epoch 4/10
10/10 - 1s - loss: 139.1949 - loglik: -1.2778e+02 - logprior: -1.1114e+01
Epoch 5/10
10/10 - 1s - loss: 136.3680 - loglik: -1.2769e+02 - logprior: -8.3664e+00
Epoch 6/10
10/10 - 1s - loss: 130.6499 - loglik: -1.2785e+02 - logprior: -2.4695e+00
Epoch 7/10
10/10 - 1s - loss: 128.6901 - loglik: -1.2821e+02 - logprior: -1.3485e-01
Epoch 8/10
10/10 - 1s - loss: 127.9327 - loglik: -1.2782e+02 - logprior: 0.2408
Epoch 9/10
10/10 - 1s - loss: 128.0041 - loglik: -1.2806e+02 - logprior: 0.4315
Fitted a model with MAP estimate = -127.3607
Time for alignment: 38.2441
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 264.8539 - loglik: -2.2381e+02 - logprior: -4.1024e+01
Epoch 2/10
10/10 - 1s - loss: 209.0224 - loglik: -1.9787e+02 - logprior: -1.1136e+01
Epoch 3/10
10/10 - 1s - loss: 177.9004 - loglik: -1.7204e+02 - logprior: -5.8499e+00
Epoch 4/10
10/10 - 1s - loss: 157.1868 - loglik: -1.5302e+02 - logprior: -4.1622e+00
Epoch 5/10
10/10 - 1s - loss: 148.2208 - loglik: -1.4463e+02 - logprior: -3.5383e+00
Epoch 6/10
10/10 - 1s - loss: 144.9683 - loglik: -1.4148e+02 - logprior: -3.2097e+00
Epoch 7/10
10/10 - 1s - loss: 143.8297 - loglik: -1.4043e+02 - logprior: -2.8975e+00
Epoch 8/10
10/10 - 1s - loss: 143.0424 - loglik: -1.3982e+02 - logprior: -2.7059e+00
Epoch 9/10
10/10 - 1s - loss: 142.8347 - loglik: -1.3979e+02 - logprior: -2.5846e+00
Epoch 10/10
10/10 - 1s - loss: 142.3677 - loglik: -1.3942e+02 - logprior: -2.5025e+00
Fitted a model with MAP estimate = -141.6974
expansions: [(2, 1), (5, 2), (13, 1), (16, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 1), (52, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.4287 - loglik: -1.3457e+02 - logprior: -3.7498e+01
Epoch 2/2
10/10 - 1s - loss: 138.3537 - loglik: -1.2779e+02 - logprior: -1.0384e+01
Fitted a model with MAP estimate = -133.2796
expansions: []
discards: [45 58 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 163.5523 - loglik: -1.2691e+02 - logprior: -3.6483e+01
Epoch 2/2
10/10 - 1s - loss: 135.7307 - loglik: -1.2566e+02 - logprior: -9.8893e+00
Fitted a model with MAP estimate = -131.7869
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 176.0975 - loglik: -1.3037e+02 - logprior: -4.5555e+01
Epoch 2/10
10/10 - 1s - loss: 148.7510 - loglik: -1.2965e+02 - logprior: -1.8902e+01
Epoch 3/10
10/10 - 1s - loss: 142.0999 - loglik: -1.2840e+02 - logprior: -1.3445e+01
Epoch 4/10
10/10 - 1s - loss: 139.2024 - loglik: -1.2781e+02 - logprior: -1.1098e+01
Epoch 5/10
10/10 - 1s - loss: 136.6122 - loglik: -1.2805e+02 - logprior: -8.2563e+00
Epoch 6/10
10/10 - 1s - loss: 130.2368 - loglik: -1.2757e+02 - logprior: -2.3393e+00
Epoch 7/10
10/10 - 1s - loss: 128.5615 - loglik: -1.2810e+02 - logprior: -1.2216e-01
Epoch 8/10
10/10 - 1s - loss: 128.1478 - loglik: -1.2804e+02 - logprior: 0.2403
Epoch 9/10
10/10 - 1s - loss: 127.8219 - loglik: -1.2789e+02 - logprior: 0.4338
Epoch 10/10
10/10 - 1s - loss: 127.7198 - loglik: -1.2793e+02 - logprior: 0.5867
Fitted a model with MAP estimate = -127.2245
Time for alignment: 39.9161
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.6931 - loglik: -2.2365e+02 - logprior: -4.1024e+01
Epoch 2/10
10/10 - 1s - loss: 208.9091 - loglik: -1.9776e+02 - logprior: -1.1134e+01
Epoch 3/10
10/10 - 1s - loss: 177.7375 - loglik: -1.7193e+02 - logprior: -5.7960e+00
Epoch 4/10
10/10 - 1s - loss: 156.2582 - loglik: -1.5215e+02 - logprior: -4.0936e+00
Epoch 5/10
10/10 - 1s - loss: 147.8835 - loglik: -1.4431e+02 - logprior: -3.4373e+00
Epoch 6/10
10/10 - 1s - loss: 145.5056 - loglik: -1.4205e+02 - logprior: -3.0929e+00
Epoch 7/10
10/10 - 1s - loss: 144.0706 - loglik: -1.4084e+02 - logprior: -2.8046e+00
Epoch 8/10
10/10 - 1s - loss: 143.3300 - loglik: -1.4031e+02 - logprior: -2.6345e+00
Epoch 9/10
10/10 - 1s - loss: 143.0303 - loglik: -1.4015e+02 - logprior: -2.5078e+00
Epoch 10/10
10/10 - 1s - loss: 142.7926 - loglik: -1.3999e+02 - logprior: -2.4298e+00
Fitted a model with MAP estimate = -142.1771
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (51, 2), (52, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 172.5269 - loglik: -1.3472e+02 - logprior: -3.7475e+01
Epoch 2/2
10/10 - 1s - loss: 137.6877 - loglik: -1.2724e+02 - logprior: -1.0264e+01
Fitted a model with MAP estimate = -132.6214
expansions: []
discards: [ 0 46 59 67 70]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 177.5078 - loglik: -1.3151e+02 - logprior: -4.5864e+01
Epoch 2/2
10/10 - 1s - loss: 149.6594 - loglik: -1.3043e+02 - logprior: -1.9052e+01
Fitted a model with MAP estimate = -145.0971
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 165.1669 - loglik: -1.2786e+02 - logprior: -3.7121e+01
Epoch 2/10
10/10 - 1s - loss: 136.2409 - loglik: -1.2639e+02 - logprior: -9.6911e+00
Epoch 3/10
10/10 - 1s - loss: 130.2163 - loglik: -1.2582e+02 - logprior: -4.1656e+00
Epoch 4/10
10/10 - 1s - loss: 127.8676 - loglik: -1.2562e+02 - logprior: -1.9641e+00
Epoch 5/10
10/10 - 1s - loss: 126.4504 - loglik: -1.2534e+02 - logprior: -8.0632e-01
Epoch 6/10
10/10 - 1s - loss: 126.2482 - loglik: -1.2572e+02 - logprior: -1.9212e-01
Epoch 7/10
10/10 - 1s - loss: 125.6828 - loglik: -1.2548e+02 - logprior: 0.1496
Epoch 8/10
10/10 - 1s - loss: 125.2995 - loglik: -1.2533e+02 - logprior: 0.3811
Epoch 9/10
10/10 - 1s - loss: 125.1175 - loglik: -1.2534e+02 - logprior: 0.5843
Epoch 10/10
10/10 - 1s - loss: 124.9856 - loglik: -1.2537e+02 - logprior: 0.7510
Fitted a model with MAP estimate = -124.5315
Time for alignment: 38.6426
Computed alignments with likelihoods: ['-127.3607', '-127.2245', '-124.5315']
Best model has likelihood: -124.5315
time for generating output: 0.1386
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.9271758436944938
Training of 3 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19c7994ee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f199d7f1430>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f199d7f1250>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fc864f70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fc864bb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fc864a60>, <__main__.SimpleDirichletPrior object at 0x7f19beba4340>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1972f2d5e0>

Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.5909 - loglik: -1.6689e+02 - logprior: -5.6659e+00
Epoch 2/10
15/15 - 1s - loss: 145.3764 - loglik: -1.4353e+02 - logprior: -1.8168e+00
Epoch 3/10
15/15 - 1s - loss: 130.3908 - loglik: -1.2841e+02 - logprior: -1.8716e+00
Epoch 4/10
15/15 - 1s - loss: 126.7338 - loglik: -1.2443e+02 - logprior: -1.8417e+00
Epoch 5/10
15/15 - 1s - loss: 125.6267 - loglik: -1.2346e+02 - logprior: -1.7509e+00
Epoch 6/10
15/15 - 1s - loss: 124.9830 - loglik: -1.2284e+02 - logprior: -1.7623e+00
Epoch 7/10
15/15 - 1s - loss: 124.7501 - loglik: -1.2261e+02 - logprior: -1.7403e+00
Epoch 8/10
15/15 - 1s - loss: 124.4465 - loglik: -1.2229e+02 - logprior: -1.7379e+00
Epoch 9/10
15/15 - 1s - loss: 124.0841 - loglik: -1.2190e+02 - logprior: -1.7400e+00
Epoch 10/10
15/15 - 1s - loss: 123.7396 - loglik: -1.2152e+02 - logprior: -1.7480e+00
Fitted a model with MAP estimate = -122.9986
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 2), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 137.6235 - loglik: -1.3008e+02 - logprior: -7.0431e+00
Epoch 2/2
15/15 - 1s - loss: 126.8245 - loglik: -1.2258e+02 - logprior: -3.6538e+00
Fitted a model with MAP estimate = -124.2493
expansions: [(0, 2)]
discards: [ 0 13 15 31 46 50]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.3978 - loglik: -1.2152e+02 - logprior: -5.2485e+00
Epoch 2/2
15/15 - 1s - loss: 120.7857 - loglik: -1.1846e+02 - logprior: -1.8001e+00
Fitted a model with MAP estimate = -119.1882
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 128.7233 - loglik: -1.2150e+02 - logprior: -6.6321e+00
Epoch 2/10
15/15 - 1s - loss: 121.6424 - loglik: -1.1879e+02 - logprior: -2.3122e+00
Epoch 3/10
15/15 - 1s - loss: 120.1004 - loglik: -1.1791e+02 - logprior: -1.6179e+00
Epoch 4/10
15/15 - 1s - loss: 119.4117 - loglik: -1.1737e+02 - logprior: -1.4653e+00
Epoch 5/10
15/15 - 1s - loss: 119.1080 - loglik: -1.1715e+02 - logprior: -1.4097e+00
Epoch 6/10
15/15 - 1s - loss: 118.8406 - loglik: -1.1691e+02 - logprior: -1.3933e+00
Epoch 7/10
15/15 - 1s - loss: 118.4769 - loglik: -1.1658e+02 - logprior: -1.3762e+00
Epoch 8/10
15/15 - 1s - loss: 118.3589 - loglik: -1.1648e+02 - logprior: -1.3647e+00
Epoch 9/10
15/15 - 1s - loss: 118.2069 - loglik: -1.1636e+02 - logprior: -1.3415e+00
Epoch 10/10
15/15 - 1s - loss: 118.0931 - loglik: -1.1625e+02 - logprior: -1.3262e+00
Fitted a model with MAP estimate = -117.4049
Time for alignment: 46.6604
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 172.4478 - loglik: -1.6675e+02 - logprior: -5.6653e+00
Epoch 2/10
15/15 - 1s - loss: 146.2887 - loglik: -1.4445e+02 - logprior: -1.8111e+00
Epoch 3/10
15/15 - 1s - loss: 131.7469 - loglik: -1.2976e+02 - logprior: -1.8769e+00
Epoch 4/10
15/15 - 1s - loss: 126.3800 - loglik: -1.2402e+02 - logprior: -1.8658e+00
Epoch 5/10
15/15 - 1s - loss: 125.5556 - loglik: -1.2329e+02 - logprior: -1.7886e+00
Epoch 6/10
15/15 - 1s - loss: 124.6194 - loglik: -1.2238e+02 - logprior: -1.8123e+00
Epoch 7/10
15/15 - 1s - loss: 124.5002 - loglik: -1.2227e+02 - logprior: -1.7917e+00
Epoch 8/10
15/15 - 1s - loss: 124.0844 - loglik: -1.2187e+02 - logprior: -1.7724e+00
Epoch 9/10
15/15 - 1s - loss: 124.1030 - loglik: -1.2188e+02 - logprior: -1.7660e+00
Fitted a model with MAP estimate = -123.3994
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (19, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.1704 - loglik: -1.2868e+02 - logprior: -7.0091e+00
Epoch 2/2
15/15 - 1s - loss: 126.3447 - loglik: -1.2223e+02 - logprior: -3.5486e+00
Fitted a model with MAP estimate = -123.8435
expansions: [(0, 2)]
discards: [ 0 13 15 38 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.3901 - loglik: -1.2157e+02 - logprior: -5.2204e+00
Epoch 2/2
15/15 - 1s - loss: 120.9685 - loglik: -1.1869e+02 - logprior: -1.7834e+00
Fitted a model with MAP estimate = -119.3527
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 128.9193 - loglik: -1.2170e+02 - logprior: -6.6305e+00
Epoch 2/10
15/15 - 1s - loss: 121.9107 - loglik: -1.1910e+02 - logprior: -2.2905e+00
Epoch 3/10
15/15 - 1s - loss: 120.1811 - loglik: -1.1801e+02 - logprior: -1.6026e+00
Epoch 4/10
15/15 - 1s - loss: 119.6358 - loglik: -1.1762e+02 - logprior: -1.4510e+00
Epoch 5/10
15/15 - 1s - loss: 119.2569 - loglik: -1.1732e+02 - logprior: -1.3963e+00
Epoch 6/10
15/15 - 1s - loss: 119.0005 - loglik: -1.1709e+02 - logprior: -1.3818e+00
Epoch 7/10
15/15 - 1s - loss: 118.6987 - loglik: -1.1682e+02 - logprior: -1.3691e+00
Epoch 8/10
15/15 - 1s - loss: 118.5986 - loglik: -1.1675e+02 - logprior: -1.3419e+00
Epoch 9/10
15/15 - 1s - loss: 118.3580 - loglik: -1.1653e+02 - logprior: -1.3274e+00
Epoch 10/10
15/15 - 1s - loss: 118.3383 - loglik: -1.1651e+02 - logprior: -1.3181e+00
Fitted a model with MAP estimate = -117.6293
Time for alignment: 45.6974
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.3833 - loglik: -1.6668e+02 - logprior: -5.6650e+00
Epoch 2/10
15/15 - 1s - loss: 145.0175 - loglik: -1.4318e+02 - logprior: -1.8116e+00
Epoch 3/10
15/15 - 1s - loss: 130.1542 - loglik: -1.2822e+02 - logprior: -1.8608e+00
Epoch 4/10
15/15 - 1s - loss: 126.2719 - loglik: -1.2402e+02 - logprior: -1.8382e+00
Epoch 5/10
15/15 - 1s - loss: 125.2480 - loglik: -1.2304e+02 - logprior: -1.7684e+00
Epoch 6/10
15/15 - 1s - loss: 124.6492 - loglik: -1.2246e+02 - logprior: -1.7901e+00
Epoch 7/10
15/15 - 1s - loss: 124.0536 - loglik: -1.2186e+02 - logprior: -1.7753e+00
Epoch 8/10
15/15 - 1s - loss: 123.8914 - loglik: -1.2168e+02 - logprior: -1.7695e+00
Epoch 9/10
15/15 - 1s - loss: 123.5850 - loglik: -1.2136e+02 - logprior: -1.7635e+00
Epoch 10/10
15/15 - 1s - loss: 123.5052 - loglik: -1.2127e+02 - logprior: -1.7589e+00
Fitted a model with MAP estimate = -122.8560
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.3427 - loglik: -1.2884e+02 - logprior: -7.0105e+00
Epoch 2/2
15/15 - 1s - loss: 126.5522 - loglik: -1.2239e+02 - logprior: -3.5728e+00
Fitted a model with MAP estimate = -123.9990
expansions: [(0, 2)]
discards: [ 0 15 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 126.9077 - loglik: -1.2105e+02 - logprior: -5.2504e+00
Epoch 2/2
15/15 - 1s - loss: 120.5532 - loglik: -1.1826e+02 - logprior: -1.7920e+00
Fitted a model with MAP estimate = -118.9996
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 128.6749 - loglik: -1.2144e+02 - logprior: -6.6433e+00
Epoch 2/10
15/15 - 1s - loss: 121.4993 - loglik: -1.1864e+02 - logprior: -2.3115e+00
Epoch 3/10
15/15 - 1s - loss: 119.8483 - loglik: -1.1765e+02 - logprior: -1.6163e+00
Epoch 4/10
15/15 - 1s - loss: 119.3736 - loglik: -1.1732e+02 - logprior: -1.4637e+00
Epoch 5/10
15/15 - 1s - loss: 118.8981 - loglik: -1.1692e+02 - logprior: -1.4127e+00
Epoch 6/10
15/15 - 1s - loss: 118.5576 - loglik: -1.1661e+02 - logprior: -1.3917e+00
Epoch 7/10
15/15 - 1s - loss: 118.3223 - loglik: -1.1641e+02 - logprior: -1.3769e+00
Epoch 8/10
15/15 - 1s - loss: 118.1113 - loglik: -1.1622e+02 - logprior: -1.3665e+00
Epoch 9/10
15/15 - 1s - loss: 118.1014 - loglik: -1.1624e+02 - logprior: -1.3376e+00
Epoch 10/10
15/15 - 1s - loss: 117.7857 - loglik: -1.1593e+02 - logprior: -1.3289e+00
Fitted a model with MAP estimate = -117.1492
Time for alignment: 45.2595
Computed alignments with likelihoods: ['-117.4049', '-117.6293', '-117.1492']
Best model has likelihood: -117.1492
time for generating output: 0.1310
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9790322580645161
Training of 3 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1972f1fd90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f198c492ac0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198baec820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fe240790>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19bf1f3fa0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19bf5c77c0>, <__main__.SimpleDirichletPrior object at 0x7f1961c7bdf0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f197b77a8b0>

Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.7883 - loglik: -1.8334e+02 - logprior: -1.4439e+01
Epoch 2/10
10/10 - 2s - loss: 170.4092 - loglik: -1.6620e+02 - logprior: -4.1873e+00
Epoch 3/10
10/10 - 2s - loss: 153.3545 - loglik: -1.5064e+02 - logprior: -2.5658e+00
Epoch 4/10
10/10 - 2s - loss: 141.1810 - loglik: -1.3857e+02 - logprior: -2.3559e+00
Epoch 5/10
10/10 - 2s - loss: 137.3940 - loglik: -1.3467e+02 - logprior: -2.4004e+00
Epoch 6/10
10/10 - 2s - loss: 135.5874 - loglik: -1.3292e+02 - logprior: -2.4443e+00
Epoch 7/10
10/10 - 2s - loss: 133.7146 - loglik: -1.3119e+02 - logprior: -2.3629e+00
Epoch 8/10
10/10 - 2s - loss: 133.8996 - loglik: -1.3154e+02 - logprior: -2.2234e+00
Fitted a model with MAP estimate = -133.3526
expansions: [(4, 2), (5, 2), (7, 2), (8, 2), (24, 1), (33, 3), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 146.8925 - loglik: -1.3052e+02 - logprior: -1.6209e+01
Epoch 2/2
10/10 - 2s - loss: 128.5279 - loglik: -1.2125e+02 - logprior: -7.0164e+00
Fitted a model with MAP estimate = -125.1811
expansions: [(0, 2)]
discards: [ 0 10 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 131.8137 - loglik: -1.1865e+02 - logprior: -1.2868e+01
Epoch 2/2
10/10 - 3s - loss: 121.2764 - loglik: -1.1733e+02 - logprior: -3.6730e+00
Fitted a model with MAP estimate = -119.5009
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 134.9785 - loglik: -1.1937e+02 - logprior: -1.5266e+01
Epoch 2/10
10/10 - 2s - loss: 123.7884 - loglik: -1.1850e+02 - logprior: -4.9346e+00
Epoch 3/10
10/10 - 2s - loss: 120.4903 - loglik: -1.1741e+02 - logprior: -2.7233e+00
Epoch 4/10
10/10 - 2s - loss: 119.2598 - loglik: -1.1696e+02 - logprior: -1.9742e+00
Epoch 5/10
10/10 - 2s - loss: 118.6427 - loglik: -1.1690e+02 - logprior: -1.4645e+00
Epoch 6/10
10/10 - 2s - loss: 117.8914 - loglik: -1.1630e+02 - logprior: -1.3462e+00
Epoch 7/10
10/10 - 2s - loss: 118.4518 - loglik: -1.1702e+02 - logprior: -1.2188e+00
Fitted a model with MAP estimate = -117.7701
Time for alignment: 60.2743
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 198.1407 - loglik: -1.8370e+02 - logprior: -1.4438e+01
Epoch 2/10
10/10 - 2s - loss: 169.8597 - loglik: -1.6565e+02 - logprior: -4.1828e+00
Epoch 3/10
10/10 - 2s - loss: 153.3408 - loglik: -1.5063e+02 - logprior: -2.5555e+00
Epoch 4/10
10/10 - 2s - loss: 141.7272 - loglik: -1.3914e+02 - logprior: -2.3210e+00
Epoch 5/10
10/10 - 2s - loss: 137.7027 - loglik: -1.3503e+02 - logprior: -2.3579e+00
Epoch 6/10
10/10 - 2s - loss: 135.4926 - loglik: -1.3290e+02 - logprior: -2.3973e+00
Epoch 7/10
10/10 - 2s - loss: 134.5779 - loglik: -1.3212e+02 - logprior: -2.2981e+00
Epoch 8/10
10/10 - 2s - loss: 133.7469 - loglik: -1.3143e+02 - logprior: -2.1763e+00
Epoch 9/10
10/10 - 2s - loss: 133.0472 - loglik: -1.3080e+02 - logprior: -2.1318e+00
Epoch 10/10
10/10 - 2s - loss: 133.0523 - loglik: -1.3080e+02 - logprior: -2.1380e+00
Fitted a model with MAP estimate = -132.6644
expansions: [(4, 2), (5, 2), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (38, 3), (39, 1), (41, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 146.9086 - loglik: -1.3057e+02 - logprior: -1.6194e+01
Epoch 2/2
10/10 - 2s - loss: 128.2100 - loglik: -1.2095e+02 - logprior: -7.0197e+00
Fitted a model with MAP estimate = -125.1418
expansions: [(0, 2)]
discards: [ 0 10 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 131.2315 - loglik: -1.1808e+02 - logprior: -1.2866e+01
Epoch 2/2
10/10 - 2s - loss: 121.7777 - loglik: -1.1781e+02 - logprior: -3.6732e+00
Fitted a model with MAP estimate = -119.4554
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 135.1837 - loglik: -1.1957e+02 - logprior: -1.5267e+01
Epoch 2/10
10/10 - 2s - loss: 123.4743 - loglik: -1.1817e+02 - logprior: -4.9394e+00
Epoch 3/10
10/10 - 2s - loss: 120.7769 - loglik: -1.1770e+02 - logprior: -2.7171e+00
Epoch 4/10
10/10 - 2s - loss: 119.1856 - loglik: -1.1689e+02 - logprior: -1.9738e+00
Epoch 5/10
10/10 - 2s - loss: 118.4115 - loglik: -1.1667e+02 - logprior: -1.4662e+00
Epoch 6/10
10/10 - 2s - loss: 118.4893 - loglik: -1.1690e+02 - logprior: -1.3437e+00
Fitted a model with MAP estimate = -117.9181
Time for alignment: 63.4821
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.9917 - loglik: -1.8355e+02 - logprior: -1.4439e+01
Epoch 2/10
10/10 - 1s - loss: 170.2819 - loglik: -1.6607e+02 - logprior: -4.1848e+00
Epoch 3/10
10/10 - 2s - loss: 153.0854 - loglik: -1.5036e+02 - logprior: -2.5795e+00
Epoch 4/10
10/10 - 2s - loss: 141.2582 - loglik: -1.3857e+02 - logprior: -2.4293e+00
Epoch 5/10
10/10 - 2s - loss: 136.9029 - loglik: -1.3400e+02 - logprior: -2.5714e+00
Epoch 6/10
10/10 - 2s - loss: 133.3622 - loglik: -1.3045e+02 - logprior: -2.6718e+00
Epoch 7/10
10/10 - 2s - loss: 131.9697 - loglik: -1.2915e+02 - logprior: -2.6202e+00
Epoch 8/10
10/10 - 2s - loss: 131.2939 - loglik: -1.2862e+02 - logprior: -2.5016e+00
Epoch 9/10
10/10 - 2s - loss: 131.0080 - loglik: -1.2840e+02 - logprior: -2.4416e+00
Epoch 10/10
10/10 - 2s - loss: 130.4348 - loglik: -1.2787e+02 - logprior: -2.4243e+00
Fitted a model with MAP estimate = -130.2497
expansions: [(4, 2), (5, 1), (7, 2), (9, 2), (13, 1), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 147.1679 - loglik: -1.3077e+02 - logprior: -1.6231e+01
Epoch 2/2
10/10 - 2s - loss: 128.7504 - loglik: -1.2144e+02 - logprior: -7.0336e+00
Fitted a model with MAP estimate = -125.2629
expansions: [(0, 2)]
discards: [ 0  9 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 131.9954 - loglik: -1.1878e+02 - logprior: -1.2869e+01
Epoch 2/2
10/10 - 2s - loss: 121.4183 - loglik: -1.1742e+02 - logprior: -3.6729e+00
Fitted a model with MAP estimate = -119.5682
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 135.2809 - loglik: -1.1970e+02 - logprior: -1.5220e+01
Epoch 2/10
10/10 - 2s - loss: 123.2974 - loglik: -1.1805e+02 - logprior: -4.8799e+00
Epoch 3/10
10/10 - 2s - loss: 120.4690 - loglik: -1.1739e+02 - logprior: -2.7065e+00
Epoch 4/10
10/10 - 2s - loss: 119.6015 - loglik: -1.1730e+02 - logprior: -1.9706e+00
Epoch 5/10
10/10 - 2s - loss: 118.2331 - loglik: -1.1649e+02 - logprior: -1.4612e+00
Epoch 6/10
10/10 - 2s - loss: 118.3796 - loglik: -1.1679e+02 - logprior: -1.3375e+00
Fitted a model with MAP estimate = -117.9142
Time for alignment: 60.1560
Computed alignments with likelihoods: ['-117.7701', '-117.9181', '-117.9142']
Best model has likelihood: -117.7701
time for generating output: 0.2928
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.9966151893378464
Training of 3 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f18b39cf460>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19a5f48a90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19a5f48b20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19a5f48a30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19a5f483a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18b385a970>, <__main__.SimpleDirichletPrior object at 0x7f197b33a5e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18b40470d0>

Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 528.1895 - loglik: -5.2150e+02 - logprior: -6.5186e+00
Epoch 2/10
23/23 - 6s - loss: 455.6355 - loglik: -4.5335e+02 - logprior: -1.1655e+00
Epoch 3/10
23/23 - 6s - loss: 433.0025 - loglik: -4.2974e+02 - logprior: -1.3260e+00
Epoch 4/10
23/23 - 6s - loss: 425.4366 - loglik: -4.2222e+02 - logprior: -1.2596e+00
Epoch 5/10
23/23 - 6s - loss: 423.5154 - loglik: -4.2087e+02 - logprior: -1.2649e+00
Epoch 6/10
23/23 - 6s - loss: 420.9494 - loglik: -4.1853e+02 - logprior: -1.3464e+00
Epoch 7/10
23/23 - 6s - loss: 420.1417 - loglik: -4.1787e+02 - logprior: -1.3731e+00
Epoch 8/10
23/23 - 6s - loss: 419.6033 - loglik: -4.1743e+02 - logprior: -1.3783e+00
Epoch 9/10
23/23 - 6s - loss: 419.0583 - loglik: -4.1694e+02 - logprior: -1.3871e+00
Epoch 10/10
23/23 - 6s - loss: 419.0441 - loglik: -4.1697e+02 - logprior: -1.3962e+00
Fitted a model with MAP estimate = -417.9761
expansions: [(0, 8), (8, 3), (9, 2), (15, 1), (30, 1), (42, 1), (53, 1), (56, 1), (57, 2), (58, 2), (65, 1), (69, 1), (70, 2), (71, 1), (78, 1), (83, 1), (86, 1), (107, 2), (112, 1), (119, 1), (121, 1), (124, 1), (126, 1), (127, 1), (149, 2), (159, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 441.5075 - loglik: -4.3155e+02 - logprior: -9.0056e+00
Epoch 2/2
23/23 - 7s - loss: 415.6052 - loglik: -4.1288e+02 - logprior: -1.1312e+00
Fitted a model with MAP estimate = -408.6853
expansions: [(0, 6), (189, 1)]
discards: [  1   2   3   4   5   6   7   8  78  95 138 199 200 201 202 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 429.8051 - loglik: -4.1811e+02 - logprior: -9.4477e+00
Epoch 2/2
23/23 - 7s - loss: 414.0959 - loglik: -4.1049e+02 - logprior: -1.0677e+00
Fitted a model with MAP estimate = -407.8883
expansions: [(0, 4), (195, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 426.3131 - loglik: -4.1444e+02 - logprior: -9.0578e+00
Epoch 2/10
23/23 - 8s - loss: 413.3387 - loglik: -4.0927e+02 - logprior: -1.3931e+00
Epoch 3/10
23/23 - 8s - loss: 408.6046 - loglik: -4.0642e+02 - logprior: 0.1782
Epoch 4/10
23/23 - 8s - loss: 405.4084 - loglik: -4.0402e+02 - logprior: 0.5696
Epoch 5/10
23/23 - 8s - loss: 404.0064 - loglik: -4.0313e+02 - logprior: 0.7500
Epoch 6/10
23/23 - 7s - loss: 403.1382 - loglik: -4.0266e+02 - logprior: 0.8895
Epoch 7/10
23/23 - 8s - loss: 402.7762 - loglik: -4.0261e+02 - logprior: 1.0244
Epoch 8/10
23/23 - 7s - loss: 400.9325 - loglik: -4.0107e+02 - logprior: 1.1694
Epoch 9/10
23/23 - 8s - loss: 401.8811 - loglik: -4.0228e+02 - logprior: 1.3198
Fitted a model with MAP estimate = -399.5961
Time for alignment: 198.9719
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 527.2081 - loglik: -5.2052e+02 - logprior: -6.5175e+00
Epoch 2/10
23/23 - 6s - loss: 456.8967 - loglik: -4.5464e+02 - logprior: -1.1560e+00
Epoch 3/10
23/23 - 6s - loss: 432.3654 - loglik: -4.2949e+02 - logprior: -1.2742e+00
Epoch 4/10
23/23 - 6s - loss: 425.9631 - loglik: -4.2315e+02 - logprior: -1.1883e+00
Epoch 5/10
23/23 - 6s - loss: 423.3749 - loglik: -4.2094e+02 - logprior: -1.1530e+00
Epoch 6/10
23/23 - 6s - loss: 423.4127 - loglik: -4.2122e+02 - logprior: -1.1772e+00
Fitted a model with MAP estimate = -420.9888
expansions: [(0, 8), (9, 3), (15, 1), (34, 1), (52, 1), (56, 2), (57, 1), (59, 1), (66, 1), (70, 1), (71, 4), (78, 1), (83, 1), (86, 1), (112, 4), (121, 1), (124, 1), (127, 1), (149, 2), (150, 2), (159, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 436.4339 - loglik: -4.2629e+02 - logprior: -8.7655e+00
Epoch 2/2
23/23 - 7s - loss: 415.2725 - loglik: -4.1209e+02 - logprior: -1.2106e+00
Fitted a model with MAP estimate = -408.2618
expansions: [(0, 5), (17, 2)]
discards: [  1   2   3   4   5  71  93 140 197 198 199 200 201]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 427.6285 - loglik: -4.1579e+02 - logprior: -9.3305e+00
Epoch 2/2
23/23 - 7s - loss: 414.4376 - loglik: -4.1068e+02 - logprior: -1.1207e+00
Fitted a model with MAP estimate = -408.0704
expansions: [(0, 4), (196, 5)]
discards: [5 7]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 203 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 11s - loss: 426.8204 - loglik: -4.1444e+02 - logprior: -9.4780e+00
Epoch 2/10
23/23 - 8s - loss: 413.9576 - loglik: -4.0976e+02 - logprior: -1.4839e+00
Epoch 3/10
23/23 - 7s - loss: 407.8702 - loglik: -4.0555e+02 - logprior: 0.0462
Epoch 4/10
23/23 - 8s - loss: 405.9396 - loglik: -4.0445e+02 - logprior: 0.4656
Epoch 5/10
23/23 - 8s - loss: 404.4438 - loglik: -4.0349e+02 - logprior: 0.6613
Epoch 6/10
23/23 - 7s - loss: 402.9357 - loglik: -4.0235e+02 - logprior: 0.8032
Epoch 7/10
23/23 - 8s - loss: 402.4965 - loglik: -4.0223e+02 - logprior: 0.9399
Epoch 8/10
23/23 - 8s - loss: 401.2657 - loglik: -4.0130e+02 - logprior: 1.0969
Epoch 9/10
23/23 - 7s - loss: 401.4136 - loglik: -4.0171e+02 - logprior: 1.2486
Fitted a model with MAP estimate = -399.5969
Time for alignment: 173.1411
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 528.5002 - loglik: -5.2181e+02 - logprior: -6.5221e+00
Epoch 2/10
23/23 - 6s - loss: 454.4940 - loglik: -4.5227e+02 - logprior: -1.1051e+00
Epoch 3/10
23/23 - 6s - loss: 433.5201 - loglik: -4.3035e+02 - logprior: -1.2219e+00
Epoch 4/10
23/23 - 6s - loss: 427.1940 - loglik: -4.2410e+02 - logprior: -1.1838e+00
Epoch 5/10
23/23 - 6s - loss: 424.1878 - loglik: -4.2169e+02 - logprior: -1.1933e+00
Epoch 6/10
23/23 - 6s - loss: 423.3625 - loglik: -4.2109e+02 - logprior: -1.2719e+00
Epoch 7/10
23/23 - 6s - loss: 421.7308 - loglik: -4.1957e+02 - logprior: -1.3376e+00
Epoch 8/10
23/23 - 6s - loss: 422.3630 - loglik: -4.2031e+02 - logprior: -1.3368e+00
Fitted a model with MAP estimate = -420.1662
expansions: [(0, 8), (9, 4), (19, 1), (52, 1), (53, 1), (56, 2), (57, 1), (59, 1), (62, 1), (65, 1), (70, 1), (71, 1), (73, 1), (74, 1), (75, 1), (86, 1), (107, 2), (113, 1), (119, 1), (120, 1), (121, 1), (126, 1), (127, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 11s - loss: 438.5432 - loglik: -4.2868e+02 - logprior: -8.8158e+00
Epoch 2/2
23/23 - 7s - loss: 415.5178 - loglik: -4.1273e+02 - logprior: -1.0985e+00
Fitted a model with MAP estimate = -408.4428
expansions: [(0, 6)]
discards: [  1   2   3   4   5 135 197 198 199 200 201]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 197 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 428.2549 - loglik: -4.1673e+02 - logprior: -9.2635e+00
Epoch 2/2
23/23 - 7s - loss: 413.3727 - loglik: -4.0970e+02 - logprior: -1.1642e+00
Fitted a model with MAP estimate = -408.2030
expansions: [(0, 5), (197, 4)]
discards: [1 5 6 8 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 201 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 12s - loss: 428.4156 - loglik: -4.1567e+02 - logprior: -9.9602e+00
Epoch 2/10
23/23 - 7s - loss: 414.7697 - loglik: -4.1062e+02 - logprior: -1.4898e+00
Epoch 3/10
23/23 - 7s - loss: 408.4850 - loglik: -4.0626e+02 - logprior: 0.1095
Epoch 4/10
23/23 - 7s - loss: 406.8470 - loglik: -4.0549e+02 - logprior: 0.5677
Epoch 5/10
23/23 - 8s - loss: 404.8659 - loglik: -4.0402e+02 - logprior: 0.7835
Epoch 6/10
23/23 - 7s - loss: 403.7314 - loglik: -4.0329e+02 - logprior: 0.9541
Epoch 7/10
23/23 - 7s - loss: 401.4713 - loglik: -4.0135e+02 - logprior: 1.0871
Epoch 8/10
23/23 - 8s - loss: 404.4415 - loglik: -4.0461e+02 - logprior: 1.2431
Fitted a model with MAP estimate = -400.7482
Time for alignment: 178.1398
Computed alignments with likelihoods: ['-399.5961', '-399.5969', '-400.7482']
Best model has likelihood: -399.5961
time for generating output: 0.3008
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.7108268615806305
Training of 3 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19591d6eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f198c1dda60>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19bee42e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1994aff3a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fd7648b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19bf1e3490>, <__main__.SimpleDirichletPrior object at 0x7f19a58dce80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18b40470d0>

Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 249.6181 - loglik: -2.4817e+02 - logprior: -1.1788e+00
Epoch 2/10
29/29 - 4s - loss: 223.6818 - loglik: -2.2186e+02 - logprior: -8.5413e-01
Epoch 3/10
29/29 - 4s - loss: 219.3031 - loglik: -2.1742e+02 - logprior: -8.4706e-01
Epoch 4/10
29/29 - 4s - loss: 217.5466 - loglik: -2.1582e+02 - logprior: -8.6339e-01
Epoch 5/10
29/29 - 4s - loss: 216.4413 - loglik: -2.1473e+02 - logprior: -8.5350e-01
Epoch 6/10
29/29 - 4s - loss: 215.5894 - loglik: -2.1391e+02 - logprior: -8.4926e-01
Epoch 7/10
29/29 - 4s - loss: 215.3257 - loglik: -2.1369e+02 - logprior: -8.4247e-01
Epoch 8/10
29/29 - 4s - loss: 214.4438 - loglik: -2.1270e+02 - logprior: -8.4943e-01
Epoch 9/10
29/29 - 4s - loss: 213.9748 - loglik: -2.1235e+02 - logprior: -8.5104e-01
Epoch 10/10
29/29 - 4s - loss: 213.3925 - loglik: -2.1174e+02 - logprior: -8.6853e-01
Fitted a model with MAP estimate = -205.0626
expansions: [(2, 1), (14, 4), (17, 2), (21, 1), (38, 2), (41, 1), (43, 2), (44, 2), (45, 1), (46, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 223.2254 - loglik: -2.2089e+02 - logprior: -1.2349e+00
Epoch 2/2
29/29 - 4s - loss: 214.4175 - loglik: -2.1193e+02 - logprior: -8.3508e-01
Fitted a model with MAP estimate = -194.2226
expansions: []
discards: [47 55 57]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 215.5152 - loglik: -2.1267e+02 - logprior: -1.1731e+00
Epoch 2/2
29/29 - 4s - loss: 213.8540 - loglik: -2.1160e+02 - logprior: -7.8176e-01
Fitted a model with MAP estimate = -194.5199
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 195.5523 - loglik: -1.9330e+02 - logprior: -7.1883e-01
Epoch 2/10
42/42 - 6s - loss: 194.8811 - loglik: -1.9285e+02 - logprior: -5.3401e-01
Epoch 3/10
42/42 - 5s - loss: 193.3102 - loglik: -1.9158e+02 - logprior: -5.2163e-01
Epoch 4/10
42/42 - 5s - loss: 192.9573 - loglik: -1.9110e+02 - logprior: -5.1208e-01
Epoch 5/10
42/42 - 5s - loss: 192.0155 - loglik: -1.9030e+02 - logprior: -5.0749e-01
Epoch 6/10
42/42 - 6s - loss: 191.2137 - loglik: -1.8939e+02 - logprior: -5.0158e-01
Epoch 7/10
42/42 - 5s - loss: 190.8483 - loglik: -1.8910e+02 - logprior: -4.9559e-01
Epoch 8/10
42/42 - 6s - loss: 189.4929 - loglik: -1.8755e+02 - logprior: -4.9435e-01
Epoch 9/10
42/42 - 5s - loss: 189.2072 - loglik: -1.8727e+02 - logprior: -4.8428e-01
Epoch 10/10
42/42 - 5s - loss: 188.2731 - loglik: -1.8662e+02 - logprior: -4.8369e-01
Fitted a model with MAP estimate = -186.5644
Time for alignment: 175.5755
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 7s - loss: 250.1098 - loglik: -2.4866e+02 - logprior: -1.1795e+00
Epoch 2/10
29/29 - 4s - loss: 222.9227 - loglik: -2.2113e+02 - logprior: -8.8333e-01
Epoch 3/10
29/29 - 4s - loss: 218.2591 - loglik: -2.1640e+02 - logprior: -8.7319e-01
Epoch 4/10
29/29 - 4s - loss: 217.6046 - loglik: -2.1583e+02 - logprior: -8.6996e-01
Epoch 5/10
29/29 - 4s - loss: 216.6180 - loglik: -2.1489e+02 - logprior: -8.5920e-01
Epoch 6/10
29/29 - 4s - loss: 216.0309 - loglik: -2.1434e+02 - logprior: -8.5492e-01
Epoch 7/10
29/29 - 4s - loss: 215.1793 - loglik: -2.1349e+02 - logprior: -8.6880e-01
Epoch 8/10
29/29 - 4s - loss: 214.4004 - loglik: -2.1262e+02 - logprior: -8.7025e-01
Epoch 9/10
29/29 - 4s - loss: 213.7386 - loglik: -2.1208e+02 - logprior: -8.7573e-01
Epoch 10/10
29/29 - 4s - loss: 213.7302 - loglik: -2.1207e+02 - logprior: -8.7869e-01
Fitted a model with MAP estimate = -205.2022
expansions: [(2, 1), (3, 1), (12, 1), (15, 2), (22, 1), (39, 2), (41, 1), (42, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 222.8944 - loglik: -2.2059e+02 - logprior: -1.2151e+00
Epoch 2/2
29/29 - 4s - loss: 213.6442 - loglik: -2.1121e+02 - logprior: -8.2486e-01
Fitted a model with MAP estimate = -193.6450
expansions: [(19, 1), (20, 3)]
discards: [46 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 215.2670 - loglik: -2.1253e+02 - logprior: -1.1641e+00
Epoch 2/2
29/29 - 4s - loss: 213.1731 - loglik: -2.1100e+02 - logprior: -7.7944e-01
Fitted a model with MAP estimate = -194.0038
expansions: [(21, 1)]
discards: [ 1 15 16 17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 195.4967 - loglik: -1.9337e+02 - logprior: -6.6274e-01
Epoch 2/10
42/42 - 5s - loss: 194.6186 - loglik: -1.9265e+02 - logprior: -5.4691e-01
Epoch 3/10
42/42 - 5s - loss: 193.1031 - loglik: -1.9143e+02 - logprior: -5.2079e-01
Epoch 4/10
42/42 - 5s - loss: 192.8581 - loglik: -1.9104e+02 - logprior: -5.1438e-01
Epoch 5/10
42/42 - 5s - loss: 192.0244 - loglik: -1.9036e+02 - logprior: -5.0625e-01
Epoch 6/10
42/42 - 5s - loss: 191.5057 - loglik: -1.8973e+02 - logprior: -4.9997e-01
Epoch 7/10
42/42 - 5s - loss: 190.3806 - loglik: -1.8869e+02 - logprior: -4.9231e-01
Epoch 8/10
42/42 - 5s - loss: 189.5258 - loglik: -1.8761e+02 - logprior: -4.9291e-01
Epoch 9/10
42/42 - 6s - loss: 188.7726 - loglik: -1.8687e+02 - logprior: -4.8820e-01
Epoch 10/10
42/42 - 6s - loss: 188.3636 - loglik: -1.8672e+02 - logprior: -4.8287e-01
Fitted a model with MAP estimate = -186.5625
Time for alignment: 174.4054
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 8s - loss: 249.8218 - loglik: -2.4837e+02 - logprior: -1.1785e+00
Epoch 2/10
29/29 - 4s - loss: 224.4938 - loglik: -2.2263e+02 - logprior: -8.7856e-01
Epoch 3/10
29/29 - 4s - loss: 218.5809 - loglik: -2.1666e+02 - logprior: -8.8044e-01
Epoch 4/10
29/29 - 4s - loss: 216.6714 - loglik: -2.1495e+02 - logprior: -8.9208e-01
Epoch 5/10
29/29 - 4s - loss: 216.8359 - loglik: -2.1511e+02 - logprior: -8.7717e-01
Fitted a model with MAP estimate = -199.6328
expansions: [(5, 1), (13, 3), (14, 2), (27, 2), (38, 1), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 217.9746 - loglik: -2.1570e+02 - logprior: -1.1993e+00
Epoch 2/2
29/29 - 4s - loss: 214.0900 - loglik: -2.1192e+02 - logprior: -8.3389e-01
Fitted a model with MAP estimate = -194.1212
expansions: []
discards: [33 56 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 215.2556 - loglik: -2.1268e+02 - logprior: -1.1741e+00
Epoch 2/2
29/29 - 4s - loss: 213.2917 - loglik: -2.1118e+02 - logprior: -7.8482e-01
Fitted a model with MAP estimate = -194.3342
expansions: [(19, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 194.9083 - loglik: -1.9278e+02 - logprior: -7.1497e-01
Epoch 2/10
42/42 - 6s - loss: 194.8524 - loglik: -1.9291e+02 - logprior: -5.3345e-01
Epoch 3/10
42/42 - 6s - loss: 192.8682 - loglik: -1.9121e+02 - logprior: -5.2773e-01
Epoch 4/10
42/42 - 5s - loss: 192.5236 - loglik: -1.9073e+02 - logprior: -5.1707e-01
Epoch 5/10
42/42 - 5s - loss: 192.0317 - loglik: -1.9039e+02 - logprior: -5.1092e-01
Epoch 6/10
42/42 - 6s - loss: 190.9303 - loglik: -1.8920e+02 - logprior: -5.0622e-01
Epoch 7/10
42/42 - 6s - loss: 190.7333 - loglik: -1.8906e+02 - logprior: -4.9944e-01
Epoch 8/10
42/42 - 6s - loss: 189.7097 - loglik: -1.8780e+02 - logprior: -4.9732e-01
Epoch 9/10
42/42 - 5s - loss: 188.9175 - loglik: -1.8700e+02 - logprior: -4.9856e-01
Epoch 10/10
42/42 - 5s - loss: 188.0433 - loglik: -1.8637e+02 - logprior: -4.9084e-01
Fitted a model with MAP estimate = -186.4075
Time for alignment: 156.8059
Computed alignments with likelihoods: ['-186.5644', '-186.5625', '-186.4075']
Best model has likelihood: -186.4075
time for generating output: 0.2087
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.6139396935244686
Training of 3 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19a59d74c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19bee0a820>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19bf6ade50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19595ea7f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19feb1e910>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19a5f4f2e0>, <__main__.SimpleDirichletPrior object at 0x7f19fd160be0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fea73310>

Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 376.1695 - loglik: -3.3022e+02 - logprior: -4.5934e+01
Epoch 2/10
10/10 - 1s - loss: 298.9145 - loglik: -2.8741e+02 - logprior: -1.1489e+01
Epoch 3/10
10/10 - 1s - loss: 252.6468 - loglik: -2.4699e+02 - logprior: -5.6415e+00
Epoch 4/10
10/10 - 1s - loss: 225.4060 - loglik: -2.2154e+02 - logprior: -3.8544e+00
Epoch 5/10
10/10 - 1s - loss: 211.9922 - loglik: -2.0900e+02 - logprior: -2.9845e+00
Epoch 6/10
10/10 - 1s - loss: 206.3263 - loglik: -2.0351e+02 - logprior: -2.6545e+00
Epoch 7/10
10/10 - 1s - loss: 204.4719 - loglik: -2.0164e+02 - logprior: -2.3907e+00
Epoch 8/10
10/10 - 1s - loss: 203.3687 - loglik: -2.0066e+02 - logprior: -2.1640e+00
Epoch 9/10
10/10 - 1s - loss: 203.0821 - loglik: -2.0056e+02 - logprior: -2.0307e+00
Epoch 10/10
10/10 - 1s - loss: 202.2840 - loglik: -1.9994e+02 - logprior: -1.9251e+00
Fitted a model with MAP estimate = -201.8753
expansions: [(10, 4), (12, 1), (13, 2), (14, 1), (16, 1), (32, 1), (45, 2), (51, 3), (52, 1), (68, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 251.7155 - loglik: -1.9910e+02 - logprior: -5.2265e+01
Epoch 2/2
10/10 - 2s - loss: 207.9884 - loglik: -1.8645e+02 - logprior: -2.1192e+01
Fitted a model with MAP estimate = -200.1867
expansions: [(0, 3)]
discards: [  0  18  55 100 101]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.1388 - loglik: -1.8636e+02 - logprior: -4.1414e+01
Epoch 2/2
10/10 - 2s - loss: 193.0075 - loglik: -1.8235e+02 - logprior: -1.0346e+01
Fitted a model with MAP estimate = -187.3875
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 235.1431 - loglik: -1.8479e+02 - logprior: -5.0034e+01
Epoch 2/10
10/10 - 2s - loss: 198.9610 - loglik: -1.8342e+02 - logprior: -1.5224e+01
Epoch 3/10
10/10 - 2s - loss: 186.9765 - loglik: -1.8158e+02 - logprior: -4.9989e+00
Epoch 4/10
10/10 - 2s - loss: 182.6761 - loglik: -1.8065e+02 - logprior: -1.5594e+00
Epoch 5/10
10/10 - 1s - loss: 180.3352 - loglik: -1.7970e+02 - logprior: -1.8439e-01
Epoch 6/10
10/10 - 2s - loss: 179.1787 - loglik: -1.7932e+02 - logprior: 0.5879
Epoch 7/10
10/10 - 2s - loss: 178.9111 - loglik: -1.7971e+02 - logprior: 1.2377
Epoch 8/10
10/10 - 2s - loss: 177.9242 - loglik: -1.7923e+02 - logprior: 1.7513
Epoch 9/10
10/10 - 2s - loss: 177.2777 - loglik: -1.7893e+02 - logprior: 2.0970
Epoch 10/10
10/10 - 2s - loss: 177.7985 - loglik: -1.7970e+02 - logprior: 2.3474
Fitted a model with MAP estimate = -176.6114
Time for alignment: 54.2029
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 376.0992 - loglik: -3.3015e+02 - logprior: -4.5931e+01
Epoch 2/10
10/10 - 1s - loss: 298.6282 - loglik: -2.8713e+02 - logprior: -1.1485e+01
Epoch 3/10
10/10 - 1s - loss: 249.8458 - loglik: -2.4428e+02 - logprior: -5.5500e+00
Epoch 4/10
10/10 - 1s - loss: 224.7519 - loglik: -2.2108e+02 - logprior: -3.6507e+00
Epoch 5/10
10/10 - 1s - loss: 214.1358 - loglik: -2.1121e+02 - logprior: -2.8265e+00
Epoch 6/10
10/10 - 1s - loss: 207.1621 - loglik: -2.0445e+02 - logprior: -2.3479e+00
Epoch 7/10
10/10 - 1s - loss: 203.5280 - loglik: -2.0090e+02 - logprior: -2.0931e+00
Epoch 8/10
10/10 - 1s - loss: 202.4608 - loglik: -2.0004e+02 - logprior: -1.9178e+00
Epoch 9/10
10/10 - 1s - loss: 201.6299 - loglik: -1.9940e+02 - logprior: -1.7999e+00
Epoch 10/10
10/10 - 1s - loss: 200.9622 - loglik: -1.9886e+02 - logprior: -1.6973e+00
Fitted a model with MAP estimate = -200.5896
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.9514 - loglik: -1.9837e+02 - logprior: -5.2236e+01
Epoch 2/2
10/10 - 2s - loss: 208.7773 - loglik: -1.8729e+02 - logprior: -2.1178e+01
Fitted a model with MAP estimate = -200.2845
expansions: [(0, 3)]
discards: [  0   9  13 102 103]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 228.1971 - loglik: -1.8646e+02 - logprior: -4.1406e+01
Epoch 2/2
10/10 - 2s - loss: 192.6632 - loglik: -1.8185e+02 - logprior: -1.0458e+01
Fitted a model with MAP estimate = -186.5532
expansions: []
discards: [ 0  2 54 55]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 236.2903 - loglik: -1.8556e+02 - logprior: -5.0384e+01
Epoch 2/10
10/10 - 2s - loss: 200.7991 - loglik: -1.8434e+02 - logprior: -1.6156e+01
Epoch 3/10
10/10 - 2s - loss: 188.2525 - loglik: -1.8243e+02 - logprior: -5.4015e+00
Epoch 4/10
10/10 - 1s - loss: 182.7598 - loglik: -1.8066e+02 - logprior: -1.6150e+00
Epoch 5/10
10/10 - 2s - loss: 181.2554 - loglik: -1.8059e+02 - logprior: -1.8726e-01
Epoch 6/10
10/10 - 2s - loss: 179.9283 - loglik: -1.8006e+02 - logprior: 0.5959
Epoch 7/10
10/10 - 1s - loss: 178.8720 - loglik: -1.7966e+02 - logprior: 1.2458
Epoch 8/10
10/10 - 2s - loss: 178.8153 - loglik: -1.8012e+02 - logprior: 1.7588
Epoch 9/10
10/10 - 2s - loss: 178.3223 - loglik: -1.7996e+02 - logprior: 2.1008
Epoch 10/10
10/10 - 2s - loss: 177.7204 - loglik: -1.7961e+02 - logprior: 2.3546
Fitted a model with MAP estimate = -177.1079
Time for alignment: 52.4906
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 375.9080 - loglik: -3.2996e+02 - logprior: -4.5934e+01
Epoch 2/10
10/10 - 1s - loss: 299.1971 - loglik: -2.8770e+02 - logprior: -1.1483e+01
Epoch 3/10
10/10 - 1s - loss: 249.9756 - loglik: -2.4433e+02 - logprior: -5.6285e+00
Epoch 4/10
10/10 - 1s - loss: 220.7834 - loglik: -2.1673e+02 - logprior: -4.0350e+00
Epoch 5/10
10/10 - 1s - loss: 208.9139 - loglik: -2.0539e+02 - logprior: -3.4079e+00
Epoch 6/10
10/10 - 1s - loss: 205.1674 - loglik: -2.0191e+02 - logprior: -2.8865e+00
Epoch 7/10
10/10 - 1s - loss: 203.6778 - loglik: -2.0060e+02 - logprior: -2.5509e+00
Epoch 8/10
10/10 - 1s - loss: 202.9736 - loglik: -2.0013e+02 - logprior: -2.3323e+00
Epoch 9/10
10/10 - 1s - loss: 201.8785 - loglik: -1.9923e+02 - logprior: -2.2011e+00
Epoch 10/10
10/10 - 1s - loss: 202.0833 - loglik: -1.9956e+02 - logprior: -2.1109e+00
Fitted a model with MAP estimate = -201.2378
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 252.0670 - loglik: -1.9939e+02 - logprior: -5.2316e+01
Epoch 2/2
10/10 - 2s - loss: 208.4867 - loglik: -1.8682e+02 - logprior: -2.1341e+01
Fitted a model with MAP estimate = -200.1413
expansions: [(0, 3)]
discards: [  0   9  18 102 103]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.4305 - loglik: -1.8558e+02 - logprior: -4.1521e+01
Epoch 2/2
10/10 - 2s - loss: 192.2759 - loglik: -1.8150e+02 - logprior: -1.0451e+01
Fitted a model with MAP estimate = -186.3770
expansions: []
discards: [ 0  2 54 55]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 236.2672 - loglik: -1.8563e+02 - logprior: -5.0308e+01
Epoch 2/10
10/10 - 2s - loss: 200.0727 - loglik: -1.8380e+02 - logprior: -1.5981e+01
Epoch 3/10
10/10 - 1s - loss: 187.9427 - loglik: -1.8221e+02 - logprior: -5.3126e+00
Epoch 4/10
10/10 - 1s - loss: 182.8569 - loglik: -1.8077e+02 - logprior: -1.6080e+00
Epoch 5/10
10/10 - 2s - loss: 180.9124 - loglik: -1.8023e+02 - logprior: -2.0178e-01
Epoch 6/10
10/10 - 2s - loss: 179.7138 - loglik: -1.7982e+02 - logprior: 0.5755
Epoch 7/10
10/10 - 2s - loss: 178.6858 - loglik: -1.7945e+02 - logprior: 1.2272
Epoch 8/10
10/10 - 2s - loss: 179.0256 - loglik: -1.8030e+02 - logprior: 1.7362
Fitted a model with MAP estimate = -177.7763
Time for alignment: 49.6934
Computed alignments with likelihoods: ['-176.6114', '-177.1079', '-177.7763']
Best model has likelihood: -176.6114
time for generating output: 0.1670
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.9366334428420291
Training of 3 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f198bc9e130>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1994f9df70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fc9a0f40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f195908aca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f195908a7c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fd6fb550>, <__main__.SimpleDirichletPrior object at 0x7f19a5a644c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19feceff70>

Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 631.4307 - loglik: -6.0807e+02 - logprior: -2.3310e+01
Epoch 2/10
14/14 - 4s - loss: 540.6271 - loglik: -5.3771e+02 - logprior: -2.7954e+00
Epoch 3/10
14/14 - 4s - loss: 482.5531 - loglik: -4.8062e+02 - logprior: -1.6553e+00
Epoch 4/10
14/14 - 4s - loss: 459.3635 - loglik: -4.5691e+02 - logprior: -1.6386e+00
Epoch 5/10
14/14 - 4s - loss: 456.9038 - loglik: -4.5451e+02 - logprior: -1.4516e+00
Epoch 6/10
14/14 - 4s - loss: 453.4546 - loglik: -4.5133e+02 - logprior: -1.3215e+00
Epoch 7/10
14/14 - 4s - loss: 451.8203 - loglik: -4.4986e+02 - logprior: -1.2821e+00
Epoch 8/10
14/14 - 4s - loss: 451.9602 - loglik: -4.5007e+02 - logprior: -1.2523e+00
Fitted a model with MAP estimate = -450.6468
expansions: [(14, 1), (15, 1), (16, 2), (28, 1), (29, 1), (30, 2), (31, 1), (40, 2), (41, 2), (42, 1), (50, 1), (51, 1), (52, 1), (53, 3), (80, 1), (89, 1), (114, 3), (115, 2), (116, 2), (119, 1), (120, 1), (121, 3), (125, 1), (155, 1), (164, 1), (166, 2), (167, 8)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 478.7955 - loglik: -4.5003e+02 - logprior: -2.8063e+01
Epoch 2/2
14/14 - 5s - loss: 446.6129 - loglik: -4.3652e+02 - logprior: -9.1761e+00
Fitted a model with MAP estimate = -440.5244
expansions: [(36, 1), (70, 2)]
discards: [ 0 17 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 463.9897 - loglik: -4.3570e+02 - logprior: -2.7193e+01
Epoch 2/2
14/14 - 5s - loss: 442.6526 - loglik: -4.3389e+02 - logprior: -7.5548e+00
Fitted a model with MAP estimate = -434.7006
expansions: [(0, 3)]
discards: [  0  72  73  74 206]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 458.1604 - loglik: -4.3696e+02 - logprior: -1.9872e+01
Epoch 2/10
14/14 - 5s - loss: 434.6145 - loglik: -4.3220e+02 - logprior: -1.1101e+00
Epoch 3/10
14/14 - 5s - loss: 430.4719 - loglik: -4.3132e+02 - logprior: 2.0197
Epoch 4/10
14/14 - 5s - loss: 427.1614 - loglik: -4.2937e+02 - logprior: 3.2177
Epoch 5/10
14/14 - 5s - loss: 427.5154 - loglik: -4.3040e+02 - logprior: 3.7844
Fitted a model with MAP estimate = -425.1554
Time for alignment: 109.4663
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 632.3201 - loglik: -6.0895e+02 - logprior: -2.3322e+01
Epoch 2/10
14/14 - 4s - loss: 541.9430 - loglik: -5.3898e+02 - logprior: -2.8443e+00
Epoch 3/10
14/14 - 4s - loss: 483.6984 - loglik: -4.8166e+02 - logprior: -1.7279e+00
Epoch 4/10
14/14 - 4s - loss: 463.7720 - loglik: -4.6138e+02 - logprior: -1.5682e+00
Epoch 5/10
14/14 - 4s - loss: 456.1228 - loglik: -4.5390e+02 - logprior: -1.3033e+00
Epoch 6/10
14/14 - 4s - loss: 453.3028 - loglik: -4.5130e+02 - logprior: -1.2304e+00
Epoch 7/10
14/14 - 4s - loss: 452.6193 - loglik: -4.5077e+02 - logprior: -1.1994e+00
Epoch 8/10
14/14 - 4s - loss: 451.6902 - loglik: -4.4997e+02 - logprior: -1.1188e+00
Epoch 9/10
14/14 - 4s - loss: 451.6653 - loglik: -4.5000e+02 - logprior: -1.0772e+00
Epoch 10/10
14/14 - 4s - loss: 450.2617 - loglik: -4.4857e+02 - logprior: -1.1126e+00
Fitted a model with MAP estimate = -449.8171
expansions: [(14, 1), (15, 1), (16, 2), (28, 2), (29, 2), (30, 2), (31, 1), (40, 2), (41, 2), (42, 1), (52, 1), (53, 1), (54, 3), (80, 1), (90, 1), (106, 2), (112, 1), (113, 3), (114, 2), (115, 2), (118, 1), (119, 1), (120, 3), (130, 2), (154, 1), (163, 1), (166, 2), (167, 8)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 479.6550 - loglik: -4.5114e+02 - logprior: -2.7902e+01
Epoch 2/2
14/14 - 5s - loss: 446.1494 - loglik: -4.3630e+02 - logprior: -9.0462e+00
Fitted a model with MAP estimate = -439.9394
expansions: [(0, 3), (72, 2)]
discards: [  0  17  51 129 168 208]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 456.7525 - loglik: -4.3540e+02 - logprior: -2.0342e+01
Epoch 2/2
14/14 - 5s - loss: 432.7994 - loglik: -4.3020e+02 - logprior: -1.4473e+00
Fitted a model with MAP estimate = -428.3808
expansions: []
discards: [ 0  1 22 76 77 78]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 464.2924 - loglik: -4.3625e+02 - logprior: -2.6740e+01
Epoch 2/10
14/14 - 5s - loss: 441.0271 - loglik: -4.3271e+02 - logprior: -6.9910e+00
Epoch 3/10
14/14 - 5s - loss: 431.3991 - loglik: -4.3063e+02 - logprior: 0.4090
Epoch 4/10
14/14 - 5s - loss: 426.7673 - loglik: -4.2881e+02 - logprior: 3.0387
Epoch 5/10
14/14 - 5s - loss: 428.1125 - loglik: -4.3106e+02 - logprior: 3.8305
Fitted a model with MAP estimate = -424.9548
Time for alignment: 117.1803
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 630.2396 - loglik: -6.0689e+02 - logprior: -2.3309e+01
Epoch 2/10
14/14 - 4s - loss: 542.6887 - loglik: -5.3976e+02 - logprior: -2.8109e+00
Epoch 3/10
14/14 - 4s - loss: 479.9370 - loglik: -4.7810e+02 - logprior: -1.5094e+00
Epoch 4/10
14/14 - 4s - loss: 461.4282 - loglik: -4.5940e+02 - logprior: -1.1922e+00
Epoch 5/10
14/14 - 4s - loss: 457.0286 - loglik: -4.5514e+02 - logprior: -1.0100e+00
Epoch 6/10
14/14 - 4s - loss: 455.1985 - loglik: -4.5355e+02 - logprior: -9.3164e-01
Epoch 7/10
14/14 - 4s - loss: 452.5922 - loglik: -4.5103e+02 - logprior: -9.2989e-01
Epoch 8/10
14/14 - 4s - loss: 452.7034 - loglik: -4.5117e+02 - logprior: -9.1344e-01
Fitted a model with MAP estimate = -451.5082
expansions: [(14, 1), (15, 1), (28, 1), (29, 2), (30, 3), (31, 1), (41, 2), (42, 1), (44, 1), (52, 1), (53, 1), (54, 3), (91, 6), (92, 2), (113, 1), (114, 3), (115, 2), (116, 3), (118, 2), (119, 2), (120, 3), (154, 1), (164, 1), (166, 2), (167, 8)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 479.5327 - loglik: -4.5087e+02 - logprior: -2.7992e+01
Epoch 2/2
14/14 - 5s - loss: 444.6614 - loglik: -4.3453e+02 - logprior: -9.2714e+00
Fitted a model with MAP estimate = -438.0879
expansions: [(17, 2), (113, 1)]
discards: [  0  50 110 111 149 155 158]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 462.9907 - loglik: -4.3468e+02 - logprior: -2.7218e+01
Epoch 2/2
14/14 - 5s - loss: 440.6566 - loglik: -4.3169e+02 - logprior: -7.7513e+00
Fitted a model with MAP estimate = -432.7577
expansions: [(0, 4), (114, 1)]
discards: [  0  17 108 109 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 456.9087 - loglik: -4.3573e+02 - logprior: -1.9838e+01
Epoch 2/10
14/14 - 5s - loss: 431.6321 - loglik: -4.2917e+02 - logprior: -1.1478e+00
Epoch 3/10
14/14 - 5s - loss: 428.4001 - loglik: -4.2915e+02 - logprior: 1.9044
Epoch 4/10
14/14 - 5s - loss: 425.6944 - loglik: -4.2782e+02 - logprior: 3.1345
Epoch 5/10
14/14 - 5s - loss: 425.3010 - loglik: -4.2813e+02 - logprior: 3.7224
Epoch 6/10
14/14 - 5s - loss: 423.5931 - loglik: -4.2687e+02 - logprior: 4.0801
Epoch 7/10
14/14 - 5s - loss: 424.2009 - loglik: -4.2781e+02 - logprior: 4.3798
Fitted a model with MAP estimate = -421.9936
Time for alignment: 120.2369
Computed alignments with likelihoods: ['-425.1554', '-424.9548', '-421.9936']
Best model has likelihood: -421.9936
time for generating output: 0.2865
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.8765112262521589
Training of 3 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f17a0295f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18b3c36850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18b3c36190>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19bee3d8e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19bee3d5b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1994bca190>, <__main__.SimpleDirichletPrior object at 0x7f19fd4d5370>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fd256790>

Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 741.0718 - loglik: -6.9854e+02 - logprior: -4.2500e+01
Epoch 2/10
11/11 - 9s - loss: 629.7534 - loglik: -6.2489e+02 - logprior: -4.7446e+00
Epoch 3/10
11/11 - 10s - loss: 547.0602 - loglik: -5.4651e+02 - logprior: -4.8790e-01
Epoch 4/10
11/11 - 9s - loss: 494.9023 - loglik: -4.9457e+02 - logprior: -2.3862e-01
Epoch 5/10
11/11 - 10s - loss: 475.3964 - loglik: -4.7458e+02 - logprior: -4.9584e-01
Epoch 6/10
11/11 - 10s - loss: 467.1665 - loglik: -4.6651e+02 - logprior: -1.1490e-01
Epoch 7/10
11/11 - 9s - loss: 469.0407 - loglik: -4.6879e+02 - logprior: 0.2616
Fitted a model with MAP estimate = -464.8174
expansions: [(19, 4), (20, 1), (21, 2), (22, 1), (26, 1), (35, 1), (47, 1), (49, 1), (52, 2), (63, 1), (65, 1), (78, 1), (79, 1), (80, 1), (91, 1), (92, 1), (103, 5), (104, 1), (106, 1), (107, 3), (162, 2), (163, 5), (169, 2), (180, 1), (181, 1), (182, 3), (198, 6), (200, 1), (202, 5), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 514.6987 - loglik: -4.6400e+02 - logprior: -5.0249e+01
Epoch 2/2
11/11 - 12s - loss: 454.7982 - loglik: -4.3871e+02 - logprior: -1.5575e+01
Fitted a model with MAP estimate = -444.1535
expansions: [(0, 2), (223, 1)]
discards: [  0  23  24  25  63 125 126 134 135 198 199]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 477.6277 - loglik: -4.3956e+02 - logprior: -3.7530e+01
Epoch 2/2
11/11 - 13s - loss: 435.8027 - loglik: -4.3235e+02 - logprior: -2.9586e+00
Fitted a model with MAP estimate = -428.8167
expansions: [(237, 1), (242, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 483.4948 - loglik: -4.3556e+02 - logprior: -4.7444e+01
Epoch 2/10
11/11 - 13s - loss: 445.7177 - loglik: -4.3283e+02 - logprior: -1.2416e+01
Epoch 3/10
11/11 - 13s - loss: 429.8700 - loglik: -4.2872e+02 - logprior: -6.5277e-01
Epoch 4/10
11/11 - 12s - loss: 420.3050 - loglik: -4.2669e+02 - logprior: 6.9646
Epoch 5/10
11/11 - 13s - loss: 418.1245 - loglik: -4.2683e+02 - logprior: 9.2660
Epoch 6/10
11/11 - 12s - loss: 413.5259 - loglik: -4.2334e+02 - logprior: 10.3375
Epoch 7/10
11/11 - 12s - loss: 413.8100 - loglik: -4.2455e+02 - logprior: 11.2396
Fitted a model with MAP estimate = -412.8061
Time for alignment: 237.8858
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 739.3965 - loglik: -6.9686e+02 - logprior: -4.2502e+01
Epoch 2/10
11/11 - 10s - loss: 630.3292 - loglik: -6.2547e+02 - logprior: -4.7393e+00
Epoch 3/10
11/11 - 10s - loss: 540.6367 - loglik: -5.3992e+02 - logprior: -6.5398e-01
Epoch 4/10
11/11 - 10s - loss: 487.1192 - loglik: -4.8643e+02 - logprior: -5.3219e-01
Epoch 5/10
11/11 - 9s - loss: 475.4865 - loglik: -4.7449e+02 - logprior: -5.2137e-01
Epoch 6/10
11/11 - 9s - loss: 470.9252 - loglik: -4.7027e+02 - logprior: -8.0379e-02
Epoch 7/10
11/11 - 10s - loss: 464.8506 - loglik: -4.6467e+02 - logprior: 0.2870
Epoch 8/10
11/11 - 9s - loss: 466.7238 - loglik: -4.6697e+02 - logprior: 0.6033
Fitted a model with MAP estimate = -464.2087
expansions: [(22, 5), (37, 1), (48, 1), (50, 1), (62, 1), (64, 1), (66, 1), (79, 3), (80, 2), (91, 1), (93, 1), (103, 4), (104, 1), (107, 5), (128, 1), (133, 1), (136, 1), (160, 2), (161, 6), (180, 1), (181, 1), (182, 2), (196, 1), (197, 4), (200, 2), (201, 6), (220, 1), (222, 2), (224, 1), (225, 1)]
discards: [  0   1 209]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 17s - loss: 515.7621 - loglik: -4.6540e+02 - logprior: -4.9983e+01
Epoch 2/2
11/11 - 14s - loss: 456.8085 - loglik: -4.4094e+02 - logprior: -1.5377e+01
Fitted a model with MAP estimate = -445.8076
expansions: [(0, 3), (244, 3), (246, 1)]
discards: [  0  92 121 128 129 130 158]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 476.7156 - loglik: -4.3888e+02 - logprior: -3.7371e+01
Epoch 2/2
11/11 - 11s - loss: 432.0289 - loglik: -4.2909e+02 - logprior: -2.5648e+00
Fitted a model with MAP estimate = -426.1310
expansions: [(254, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 477.8282 - loglik: -4.3004e+02 - logprior: -4.7417e+01
Epoch 2/10
11/11 - 14s - loss: 445.4628 - loglik: -4.3288e+02 - logprior: -1.2224e+01
Epoch 3/10
11/11 - 11s - loss: 426.3444 - loglik: -4.2587e+02 - logprior: -5.1000e-02
Epoch 4/10
11/11 - 12s - loss: 417.3926 - loglik: -4.2412e+02 - logprior: 7.2740
Epoch 5/10
11/11 - 12s - loss: 412.3304 - loglik: -4.2122e+02 - logprior: 9.4257
Epoch 6/10
11/11 - 14s - loss: 414.1828 - loglik: -4.2419e+02 - logprior: 10.5215
Fitted a model with MAP estimate = -410.7818
Time for alignment: 238.8931
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 739.6873 - loglik: -6.9718e+02 - logprior: -4.2475e+01
Epoch 2/10
11/11 - 10s - loss: 633.2820 - loglik: -6.2844e+02 - logprior: -4.7151e+00
Epoch 3/10
11/11 - 9s - loss: 544.6467 - loglik: -5.4399e+02 - logprior: -5.8852e-01
Epoch 4/10
11/11 - 9s - loss: 490.2629 - loglik: -4.8970e+02 - logprior: -4.4926e-01
Epoch 5/10
11/11 - 9s - loss: 475.3928 - loglik: -4.7428e+02 - logprior: -7.1469e-01
Epoch 6/10
11/11 - 9s - loss: 468.2180 - loglik: -4.6702e+02 - logprior: -6.1977e-01
Epoch 7/10
11/11 - 9s - loss: 468.4820 - loglik: -4.6759e+02 - logprior: -4.0429e-01
Fitted a model with MAP estimate = -464.9244
expansions: [(19, 3), (20, 1), (21, 2), (27, 1), (37, 1), (48, 1), (50, 1), (53, 2), (61, 1), (63, 1), (65, 1), (78, 3), (79, 2), (90, 1), (91, 1), (102, 3), (103, 1), (104, 1), (106, 2), (129, 1), (134, 2), (136, 1), (161, 1), (169, 2), (180, 1), (182, 2), (183, 2), (197, 1), (198, 3), (199, 4), (201, 2), (202, 5), (220, 1), (222, 1), (223, 1), (224, 1), (225, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 515.6707 - loglik: -4.6498e+02 - logprior: -5.0208e+01
Epoch 2/2
11/11 - 13s - loss: 459.7734 - loglik: -4.4366e+02 - logprior: -1.5494e+01
Fitted a model with MAP estimate = -446.2040
expansions: [(0, 2)]
discards: [  0  23  24  62  96 124 125 132 281]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 18s - loss: 478.4656 - loglik: -4.4031e+02 - logprior: -3.7590e+01
Epoch 2/2
11/11 - 12s - loss: 438.7031 - loglik: -4.3531e+02 - logprior: -2.9590e+00
Fitted a model with MAP estimate = -430.3249
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 16s - loss: 487.1853 - loglik: -4.3907e+02 - logprior: -4.7730e+01
Epoch 2/10
11/11 - 12s - loss: 444.7596 - loglik: -4.3165e+02 - logprior: -1.2735e+01
Epoch 3/10
11/11 - 11s - loss: 433.6434 - loglik: -4.3206e+02 - logprior: -1.1319e+00
Epoch 4/10
11/11 - 11s - loss: 421.0902 - loglik: -4.2732e+02 - logprior: 6.7905
Epoch 5/10
11/11 - 13s - loss: 418.2414 - loglik: -4.2673e+02 - logprior: 9.0593
Epoch 6/10
11/11 - 12s - loss: 416.5103 - loglik: -4.2611e+02 - logprior: 10.1345
Epoch 7/10
11/11 - 13s - loss: 416.8838 - loglik: -4.2738e+02 - logprior: 10.9996
Fitted a model with MAP estimate = -414.3168
Time for alignment: 236.3479
Computed alignments with likelihoods: ['-412.8061', '-410.7818', '-414.3168']
Best model has likelihood: -410.7818
time for generating output: 0.5398
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.9198517488996989
Training of 3 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1961dd3a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f199d69e490>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fe187e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193e0d35b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fec72d30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18b3d88f10>, <__main__.SimpleDirichletPrior object at 0x7f19fd4d0550>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fecefa60>

Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 22s - loss: 815.9082 - loglik: -8.0925e+02 - logprior: -6.5255e+00
Epoch 2/10
21/21 - 17s - loss: 687.5290 - loglik: -6.8553e+02 - logprior: -1.4185e+00
Epoch 3/10
21/21 - 17s - loss: 644.8668 - loglik: -6.4020e+02 - logprior: -3.7878e+00
Epoch 4/10
21/21 - 17s - loss: 635.1896 - loglik: -6.3034e+02 - logprior: -3.7928e+00
Epoch 5/10
21/21 - 17s - loss: 629.9711 - loglik: -6.2551e+02 - logprior: -3.5397e+00
Epoch 6/10
21/21 - 17s - loss: 631.7835 - loglik: -6.2731e+02 - logprior: -3.6578e+00
Fitted a model with MAP estimate = -629.2404
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 5), (54, 1), (55, 1), (57, 1), (59, 2), (61, 1), (63, 2), (75, 3), (76, 1), (77, 1), (78, 1), (79, 2), (80, 1), (82, 1), (84, 1), (85, 1), (88, 1), (91, 1), (92, 1), (93, 1), (99, 1), (101, 1), (108, 1), (113, 1), (115, 1), (135, 1), (138, 1), (141, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (180, 2), (181, 1), (183, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 1), (195, 1), (196, 1), (208, 1), (209, 1), (211, 1), (212, 1), (214, 1), (228, 1), (229, 1), (230, 2), (232, 1), (235, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 1), (262, 1), (271, 1), (272, 2), (273, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 630.3287 - loglik: -6.1974e+02 - logprior: -9.5607e+00
Epoch 2/2
21/21 - 25s - loss: 602.0021 - loglik: -5.9762e+02 - logprior: -2.9883e+00
Fitted a model with MAP estimate = -597.6575
expansions: [(0, 2), (77, 1)]
discards: [  0  56  57 231]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 609.5052 - loglik: -6.0171e+02 - logprior: -5.9633e+00
Epoch 2/2
21/21 - 25s - loss: 593.1019 - loglik: -5.9173e+02 - logprior: 0.6485
Fitted a model with MAP estimate = -592.2987
expansions: [(57, 2)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 28s - loss: 604.7225 - loglik: -5.9705e+02 - logprior: -5.3643e+00
Epoch 2/10
21/21 - 25s - loss: 599.2802 - loglik: -5.9822e+02 - logprior: 1.1764
Epoch 3/10
21/21 - 25s - loss: 590.7969 - loglik: -5.9070e+02 - logprior: 1.8095
Epoch 4/10
21/21 - 25s - loss: 591.5577 - loglik: -5.9230e+02 - logprior: 2.2961
Fitted a model with MAP estimate = -588.9066
Time for alignment: 383.8567
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 21s - loss: 815.2787 - loglik: -8.0864e+02 - logprior: -6.5057e+00
Epoch 2/10
21/21 - 17s - loss: 698.3181 - loglik: -6.9644e+02 - logprior: -1.2636e+00
Epoch 3/10
21/21 - 17s - loss: 644.9157 - loglik: -6.4024e+02 - logprior: -3.4000e+00
Epoch 4/10
21/21 - 17s - loss: 632.9767 - loglik: -6.2804e+02 - logprior: -3.5489e+00
Epoch 5/10
21/21 - 17s - loss: 635.2554 - loglik: -6.3081e+02 - logprior: -3.4022e+00
Fitted a model with MAP estimate = -629.1539
expansions: [(13, 1), (14, 2), (15, 1), (33, 1), (34, 1), (51, 3), (53, 2), (54, 1), (57, 1), (60, 1), (61, 1), (62, 1), (63, 4), (64, 1), (73, 1), (74, 1), (79, 1), (80, 2), (81, 2), (85, 1), (86, 1), (89, 1), (91, 1), (94, 1), (100, 1), (101, 1), (102, 1), (109, 1), (111, 1), (113, 1), (115, 1), (135, 1), (136, 1), (137, 1), (143, 1), (151, 1), (152, 1), (153, 1), (154, 1), (156, 3), (157, 1), (158, 1), (159, 1), (168, 1), (178, 1), (181, 1), (188, 1), (189, 1), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (197, 1), (208, 1), (210, 1), (211, 1), (223, 1), (227, 1), (230, 2), (232, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 630.8344 - loglik: -6.2012e+02 - logprior: -9.4051e+00
Epoch 2/2
21/21 - 25s - loss: 602.6636 - loglik: -5.9800e+02 - logprior: -2.9498e+00
Fitted a model with MAP estimate = -598.0384
expansions: [(0, 2)]
discards: [  0  58 207 336]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 608.8149 - loglik: -6.0085e+02 - logprior: -5.9795e+00
Epoch 2/2
21/21 - 25s - loss: 597.7704 - loglik: -5.9644e+02 - logprior: 0.6868
Fitted a model with MAP estimate = -592.4467
expansions: []
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 30s - loss: 606.0806 - loglik: -5.9818e+02 - logprior: -5.6149e+00
Epoch 2/10
21/21 - 24s - loss: 596.9964 - loglik: -5.9594e+02 - logprior: 1.1494
Epoch 3/10
21/21 - 25s - loss: 594.3333 - loglik: -5.9425e+02 - logprior: 1.7881
Epoch 4/10
21/21 - 25s - loss: 590.1958 - loglik: -5.9090e+02 - logprior: 2.2562
Epoch 5/10
21/21 - 25s - loss: 590.3828 - loglik: -5.9156e+02 - logprior: 2.4843
Fitted a model with MAP estimate = -588.6550
Time for alignment: 390.2656
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 20s - loss: 816.8084 - loglik: -8.1017e+02 - logprior: -6.5064e+00
Epoch 2/10
21/21 - 17s - loss: 689.7880 - loglik: -6.8787e+02 - logprior: -1.3394e+00
Epoch 3/10
21/21 - 17s - loss: 643.2751 - loglik: -6.3885e+02 - logprior: -3.6042e+00
Epoch 4/10
21/21 - 17s - loss: 631.0331 - loglik: -6.2629e+02 - logprior: -3.7234e+00
Epoch 5/10
21/21 - 17s - loss: 630.6888 - loglik: -6.2617e+02 - logprior: -3.5914e+00
Epoch 6/10
21/21 - 17s - loss: 630.0178 - loglik: -6.2560e+02 - logprior: -3.5802e+00
Epoch 7/10
21/21 - 17s - loss: 626.2878 - loglik: -6.2189e+02 - logprior: -3.6182e+00
Epoch 8/10
21/21 - 17s - loss: 626.2126 - loglik: -6.2192e+02 - logprior: -3.5499e+00
Epoch 9/10
21/21 - 17s - loss: 627.6538 - loglik: -6.2325e+02 - logprior: -3.6769e+00
Fitted a model with MAP estimate = -625.7099
expansions: [(13, 1), (14, 2), (15, 1), (53, 5), (55, 1), (56, 1), (58, 1), (60, 2), (61, 1), (62, 1), (64, 3), (75, 1), (76, 1), (77, 3), (80, 1), (82, 1), (84, 1), (85, 1), (86, 1), (90, 1), (93, 1), (94, 1), (95, 1), (102, 1), (103, 1), (113, 1), (115, 1), (117, 1), (130, 1), (135, 1), (137, 1), (138, 1), (154, 1), (155, 1), (156, 1), (158, 1), (159, 1), (161, 1), (162, 1), (171, 1), (181, 1), (184, 1), (188, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 2), (195, 1), (196, 1), (208, 1), (209, 1), (212, 1), (215, 1), (219, 1), (223, 1), (229, 1), (230, 2), (232, 1), (234, 1), (256, 1), (257, 3), (258, 1), (260, 2), (261, 1), (272, 2), (273, 2)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 29s - loss: 628.1487 - loglik: -6.2036e+02 - logprior: -6.8516e+00
Epoch 2/2
21/21 - 25s - loss: 599.9609 - loglik: -5.9850e+02 - logprior: -8.6840e-02
Fitted a model with MAP estimate = -595.1112
expansions: []
discards: [ 56  57 251]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 28s - loss: 608.7275 - loglik: -6.0090e+02 - logprior: -6.0318e+00
Epoch 2/2
21/21 - 25s - loss: 599.3076 - loglik: -5.9798e+02 - logprior: 0.6444
Fitted a model with MAP estimate = -593.2091
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 29s - loss: 608.5526 - loglik: -6.0061e+02 - logprior: -5.6293e+00
Epoch 2/10
21/21 - 25s - loss: 594.9751 - loglik: -5.9392e+02 - logprior: 1.1560
Epoch 3/10
21/21 - 25s - loss: 593.9020 - loglik: -5.9393e+02 - logprior: 1.9174
Epoch 4/10
21/21 - 25s - loss: 592.7673 - loglik: -5.9356e+02 - logprior: 2.3375
Epoch 5/10
21/21 - 25s - loss: 591.3796 - loglik: -5.9272e+02 - logprior: 2.6325
Epoch 6/10
21/21 - 24s - loss: 589.2141 - loglik: -5.9098e+02 - logprior: 2.9083
Epoch 7/10
21/21 - 25s - loss: 590.2477 - loglik: -5.9241e+02 - logprior: 3.1883
Fitted a model with MAP estimate = -587.5004
Time for alignment: 505.8186
Computed alignments with likelihoods: ['-588.9066', '-588.6550', '-587.5004']
Best model has likelihood: -587.5004
time for generating output: 0.3838
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.9587046939988116
Training of 3 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f199d6deb20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f199d3d4d90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f199d3faf40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193efd2af0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198baf6df0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fec72d30>, <__main__.SimpleDirichletPrior object at 0x7f1994eb6310>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe1e6820>

Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 839.4275 - loglik: -8.3718e+02 - logprior: -1.8708e+00
Epoch 2/10
39/39 - 23s - loss: 737.0563 - loglik: -7.3366e+02 - logprior: -1.8605e+00
Epoch 3/10
39/39 - 23s - loss: 721.7732 - loglik: -7.1761e+02 - logprior: -1.9834e+00
Epoch 4/10
39/39 - 23s - loss: 718.3281 - loglik: -7.1430e+02 - logprior: -1.9738e+00
Epoch 5/10
39/39 - 23s - loss: 716.4304 - loglik: -7.1260e+02 - logprior: -1.9999e+00
Epoch 6/10
39/39 - 23s - loss: 715.0123 - loglik: -7.1132e+02 - logprior: -2.0502e+00
Epoch 7/10
39/39 - 23s - loss: 714.0612 - loglik: -7.1048e+02 - logprior: -2.0891e+00
Epoch 8/10
39/39 - 23s - loss: 712.4924 - loglik: -7.0898e+02 - logprior: -2.1259e+00
Epoch 9/10
39/39 - 23s - loss: 712.2363 - loglik: -7.0881e+02 - logprior: -2.1327e+00
Epoch 10/10
39/39 - 23s - loss: 711.9196 - loglik: -7.0856e+02 - logprior: -2.1496e+00
Fitted a model with MAP estimate = -614.9046
expansions: [(14, 1), (30, 1), (40, 1), (42, 1), (43, 1), (80, 1), (81, 4), (82, 1), (84, 1), (89, 1), (92, 3), (94, 2), (99, 2), (102, 1), (118, 1), (120, 2), (121, 3), (146, 5), (147, 11), (169, 5), (170, 2), (173, 3), (175, 1), (176, 1), (179, 2), (180, 2), (181, 3), (182, 1), (186, 3), (187, 1), (188, 1), (189, 1), (192, 2), (193, 2), (208, 2), (209, 1), (213, 3), (215, 1), (225, 1), (244, 3)]
discards: [  0 103 104 105 106 107 108 160 161 162 163 164 165 166 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 313 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 726.6823 - loglik: -7.2154e+02 - logprior: -3.3072e+00
Epoch 2/2
39/39 - 33s - loss: 698.2571 - loglik: -6.9315e+02 - logprior: -2.4008e+00
Fitted a model with MAP estimate = -595.3505
expansions: [(0, 2), (123, 9), (169, 1), (211, 1), (221, 1), (313, 2)]
discards: [  0 106 110 178 194 195 224 229 230 276 277 310 311 312]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 697.9767 - loglik: -6.9220e+02 - logprior: -2.4459e+00
Epoch 2/2
39/39 - 33s - loss: 687.4111 - loglik: -6.8308e+02 - logprior: -1.2807e+00
Fitted a model with MAP estimate = -588.4327
expansions: []
discards: [  0  96  97 122 123 124 125 126 127 313 314]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 37s - loss: 602.5870 - loglik: -5.9692e+02 - logprior: -2.4892e+00
Epoch 2/10
43/43 - 34s - loss: 593.9894 - loglik: -5.9038e+02 - logprior: -9.7727e-01
Epoch 3/10
43/43 - 34s - loss: 595.7538 - loglik: -5.9264e+02 - logprior: -7.4277e-01
Fitted a model with MAP estimate = -587.9245
Time for alignment: 641.5113
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 836.6171 - loglik: -8.3442e+02 - logprior: -1.8493e+00
Epoch 2/10
39/39 - 23s - loss: 738.5999 - loglik: -7.3576e+02 - logprior: -1.6136e+00
Epoch 3/10
39/39 - 23s - loss: 723.2697 - loglik: -7.1912e+02 - logprior: -1.9345e+00
Epoch 4/10
39/39 - 23s - loss: 717.9203 - loglik: -7.1373e+02 - logprior: -2.0511e+00
Epoch 5/10
39/39 - 23s - loss: 714.0360 - loglik: -7.1002e+02 - logprior: -2.0743e+00
Epoch 6/10
39/39 - 24s - loss: 712.7548 - loglik: -7.0890e+02 - logprior: -2.0946e+00
Epoch 7/10
39/39 - 23s - loss: 711.5043 - loglik: -7.0776e+02 - logprior: -2.1214e+00
Epoch 8/10
39/39 - 24s - loss: 710.4586 - loglik: -7.0688e+02 - logprior: -2.1361e+00
Epoch 9/10
39/39 - 24s - loss: 710.2114 - loglik: -7.0673e+02 - logprior: -2.1493e+00
Epoch 10/10
39/39 - 23s - loss: 709.6149 - loglik: -7.0621e+02 - logprior: -2.1635e+00
Fitted a model with MAP estimate = -613.0973
expansions: [(14, 1), (30, 1), (54, 1), (82, 1), (83, 3), (84, 1), (86, 1), (95, 3), (97, 2), (105, 1), (107, 3), (108, 2), (111, 1), (120, 1), (121, 1), (122, 1), (123, 4), (124, 1), (142, 1), (152, 1), (153, 1), (158, 1), (159, 1), (160, 2), (161, 4), (162, 1), (169, 7), (170, 2), (173, 2), (174, 3), (175, 2), (176, 1), (177, 1), (178, 1), (179, 7), (180, 2), (185, 1), (186, 1), (187, 1), (190, 2), (191, 1), (195, 1), (206, 1), (207, 1), (211, 4), (244, 3)]
discards: [  0 150]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 327 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 718.4738 - loglik: -7.1318e+02 - logprior: -3.4188e+00
Epoch 2/2
39/39 - 35s - loss: 690.4148 - loglik: -6.8542e+02 - logprior: -2.3692e+00
Fitted a model with MAP estimate = -589.8491
expansions: [(0, 2), (228, 1), (235, 1), (236, 1), (288, 1)]
discards: [  0 116 124 125 126 127 128 129 148 170 178 179 180 181 182 211 222 224
 225 324 325 326]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 699.2008 - loglik: -6.9352e+02 - logprior: -2.4352e+00
Epoch 2/2
39/39 - 32s - loss: 690.0223 - loglik: -6.8575e+02 - logprior: -1.2775e+00
Fitted a model with MAP estimate = -590.7378
expansions: [(124, 4), (208, 1), (246, 1), (311, 3)]
discards: [  0 232 247]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 317 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 41s - loss: 598.9338 - loglik: -5.9321e+02 - logprior: -2.6179e+00
Epoch 2/10
43/43 - 36s - loss: 591.5775 - loglik: -5.8783e+02 - logprior: -1.1729e+00
Epoch 3/10
43/43 - 36s - loss: 589.8222 - loglik: -5.8636e+02 - logprior: -1.1403e+00
Epoch 4/10
43/43 - 36s - loss: 586.7300 - loglik: -5.8379e+02 - logprior: -8.7386e-01
Epoch 5/10
43/43 - 36s - loss: 584.6862 - loglik: -5.8216e+02 - logprior: -6.8554e-01
Epoch 6/10
43/43 - 36s - loss: 585.4063 - loglik: -5.8297e+02 - logprior: -8.0781e-01
Fitted a model with MAP estimate = -582.0750
Time for alignment: 764.6166
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 842.1088 - loglik: -8.3988e+02 - logprior: -1.8533e+00
Epoch 2/10
39/39 - 23s - loss: 738.6334 - loglik: -7.3520e+02 - logprior: -1.7031e+00
Epoch 3/10
39/39 - 23s - loss: 721.7570 - loglik: -7.1738e+02 - logprior: -1.9086e+00
Epoch 4/10
39/39 - 23s - loss: 714.4396 - loglik: -7.1016e+02 - logprior: -2.0147e+00
Epoch 5/10
39/39 - 23s - loss: 711.9516 - loglik: -7.0797e+02 - logprior: -2.0606e+00
Epoch 6/10
39/39 - 23s - loss: 710.1751 - loglik: -7.0645e+02 - logprior: -2.0956e+00
Epoch 7/10
39/39 - 23s - loss: 709.3347 - loglik: -7.0578e+02 - logprior: -2.1100e+00
Epoch 8/10
39/39 - 23s - loss: 708.5018 - loglik: -7.0506e+02 - logprior: -2.1226e+00
Epoch 9/10
39/39 - 24s - loss: 708.2228 - loglik: -7.0486e+02 - logprior: -2.1382e+00
Epoch 10/10
39/39 - 23s - loss: 708.2068 - loglik: -7.0488e+02 - logprior: -2.1570e+00
Fitted a model with MAP estimate = -612.2756
expansions: [(14, 1), (19, 1), (40, 1), (44, 1), (55, 1), (74, 1), (79, 6), (81, 1), (91, 2), (92, 4), (101, 1), (108, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (142, 13), (146, 1), (147, 3), (150, 1), (152, 3), (160, 1), (161, 1), (162, 1), (166, 2), (168, 2), (170, 1), (173, 1), (178, 1), (179, 6), (180, 1), (181, 2), (186, 1), (187, 1), (188, 1), (190, 1), (191, 1), (192, 2), (208, 1), (209, 1), (213, 4), (214, 2), (225, 1), (244, 3)]
discards: [  0  33 102 104]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 718.4106 - loglik: -7.1331e+02 - logprior: -3.3591e+00
Epoch 2/2
39/39 - 35s - loss: 690.9354 - loglik: -6.8619e+02 - logprior: -2.2190e+00
Fitted a model with MAP estimate = -590.6160
expansions: [(0, 2), (121, 6), (326, 2)]
discards: [  0 102 103 166 167 168 169 170 171 194 239 240 287 323 324 325]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 320 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 694.3904 - loglik: -6.8867e+02 - logprior: -2.5586e+00
Epoch 2/2
39/39 - 35s - loss: 685.2964 - loglik: -6.8097e+02 - logprior: -1.3064e+00
Fitted a model with MAP estimate = -587.0839
expansions: [(32, 1), (285, 1)]
discards: [  0 123 124 125 126 127 128 256 318 319]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 40s - loss: 600.5920 - loglik: -5.9499e+02 - logprior: -2.4719e+00
Epoch 2/10
43/43 - 35s - loss: 591.8611 - loglik: -5.8832e+02 - logprior: -9.2736e-01
Epoch 3/10
43/43 - 35s - loss: 589.7866 - loglik: -5.8670e+02 - logprior: -7.2079e-01
Epoch 4/10
43/43 - 35s - loss: 586.2797 - loglik: -5.8356e+02 - logprior: -6.1114e-01
Epoch 5/10
43/43 - 36s - loss: 587.2068 - loglik: -5.8486e+02 - logprior: -4.9410e-01
Fitted a model with MAP estimate = -583.3602
Time for alignment: 730.2972
Computed alignments with likelihoods: ['-587.9245', '-582.0750', '-583.3602']
Best model has likelihood: -582.0750
time for generating output: 0.4991
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.530710120442531
Training of 3 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f193da85b50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fd07b7c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fcfc5820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fcfc5df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19729f36a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18b3956550>, <__main__.SimpleDirichletPrior object at 0x7f198c29c6a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe1e6820>

Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.9872 - loglik: -2.3646e+02 - logprior: -3.8511e+01
Epoch 2/10
10/10 - 2s - loss: 223.9371 - loglik: -2.1348e+02 - logprior: -1.0431e+01
Epoch 3/10
10/10 - 2s - loss: 197.2419 - loglik: -1.9177e+02 - logprior: -5.4657e+00
Epoch 4/10
10/10 - 1s - loss: 182.0478 - loglik: -1.7812e+02 - logprior: -3.8644e+00
Epoch 5/10
10/10 - 2s - loss: 174.5896 - loglik: -1.7125e+02 - logprior: -3.1044e+00
Epoch 6/10
10/10 - 2s - loss: 171.6045 - loglik: -1.6859e+02 - logprior: -2.6462e+00
Epoch 7/10
10/10 - 2s - loss: 170.9792 - loglik: -1.6821e+02 - logprior: -2.4275e+00
Epoch 8/10
10/10 - 2s - loss: 169.4411 - loglik: -1.6682e+02 - logprior: -2.2959e+00
Epoch 9/10
10/10 - 2s - loss: 169.2019 - loglik: -1.6663e+02 - logprior: -2.2530e+00
Epoch 10/10
10/10 - 2s - loss: 168.6641 - loglik: -1.6611e+02 - logprior: -2.2282e+00
Fitted a model with MAP estimate = -168.1151
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (23, 3), (30, 1), (35, 2), (46, 1), (55, 3), (58, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 216.5164 - loglik: -1.6562e+02 - logprior: -5.0580e+01
Epoch 2/2
10/10 - 2s - loss: 171.2246 - loglik: -1.5499e+02 - logprior: -1.5868e+01
Fitted a model with MAP estimate = -161.7578
expansions: []
discards: [ 0 18 26 47 71]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 198.7476 - loglik: -1.5452e+02 - logprior: -4.3920e+01
Epoch 2/2
10/10 - 2s - loss: 171.0559 - loglik: -1.5340e+02 - logprior: -1.7448e+01
Fitted a model with MAP estimate = -165.6602
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 192.8101 - loglik: -1.5244e+02 - logprior: -4.0180e+01
Epoch 2/10
10/10 - 2s - loss: 162.9600 - loglik: -1.5144e+02 - logprior: -1.1333e+01
Epoch 3/10
10/10 - 2s - loss: 155.5096 - loglik: -1.5070e+02 - logprior: -4.5298e+00
Epoch 4/10
10/10 - 2s - loss: 152.4518 - loglik: -1.4980e+02 - logprior: -2.3425e+00
Epoch 5/10
10/10 - 2s - loss: 151.0780 - loglik: -1.4938e+02 - logprior: -1.4162e+00
Epoch 6/10
10/10 - 2s - loss: 150.2257 - loglik: -1.4915e+02 - logprior: -8.0554e-01
Epoch 7/10
10/10 - 2s - loss: 149.9870 - loglik: -1.4945e+02 - logprior: -2.6952e-01
Epoch 8/10
10/10 - 2s - loss: 149.5777 - loglik: -1.4940e+02 - logprior: 0.0845
Epoch 9/10
10/10 - 2s - loss: 149.7937 - loglik: -1.4980e+02 - logprior: 0.2666
Fitted a model with MAP estimate = -149.1032
Time for alignment: 55.4793
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 275.0330 - loglik: -2.3650e+02 - logprior: -3.8512e+01
Epoch 2/10
10/10 - 2s - loss: 223.8161 - loglik: -2.1336e+02 - logprior: -1.0431e+01
Epoch 3/10
10/10 - 2s - loss: 197.6145 - loglik: -1.9214e+02 - logprior: -5.4625e+00
Epoch 4/10
10/10 - 2s - loss: 182.1630 - loglik: -1.7824e+02 - logprior: -3.8387e+00
Epoch 5/10
10/10 - 2s - loss: 175.7295 - loglik: -1.7236e+02 - logprior: -3.0454e+00
Epoch 6/10
10/10 - 2s - loss: 171.8599 - loglik: -1.6882e+02 - logprior: -2.6514e+00
Epoch 7/10
10/10 - 2s - loss: 169.1373 - loglik: -1.6623e+02 - logprior: -2.6218e+00
Epoch 8/10
10/10 - 2s - loss: 168.2237 - loglik: -1.6545e+02 - logprior: -2.5253e+00
Epoch 9/10
10/10 - 2s - loss: 166.9890 - loglik: -1.6444e+02 - logprior: -2.2872e+00
Epoch 10/10
10/10 - 2s - loss: 167.0475 - loglik: -1.6459e+02 - logprior: -2.1771e+00
Fitted a model with MAP estimate = -166.3107
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (24, 3), (30, 1), (38, 1), (46, 1), (55, 3), (58, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 214.0606 - loglik: -1.6313e+02 - logprior: -5.0639e+01
Epoch 2/2
10/10 - 2s - loss: 169.7942 - loglik: -1.5368e+02 - logprior: -1.5770e+01
Fitted a model with MAP estimate = -160.8700
expansions: []
discards: [ 0 18 26 70]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 198.5451 - loglik: -1.5438e+02 - logprior: -4.3889e+01
Epoch 2/2
10/10 - 2s - loss: 170.9408 - loglik: -1.5332e+02 - logprior: -1.7430e+01
Fitted a model with MAP estimate = -165.6149
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 192.7628 - loglik: -1.5241e+02 - logprior: -4.0161e+01
Epoch 2/10
10/10 - 2s - loss: 162.9716 - loglik: -1.5148e+02 - logprior: -1.1319e+01
Epoch 3/10
10/10 - 2s - loss: 155.6341 - loglik: -1.5085e+02 - logprior: -4.5213e+00
Epoch 4/10
10/10 - 2s - loss: 152.6325 - loglik: -1.5000e+02 - logprior: -2.3254e+00
Epoch 5/10
10/10 - 2s - loss: 150.8988 - loglik: -1.4922e+02 - logprior: -1.4035e+00
Epoch 6/10
10/10 - 2s - loss: 150.3776 - loglik: -1.4931e+02 - logprior: -7.9621e-01
Epoch 7/10
10/10 - 2s - loss: 150.0473 - loglik: -1.4953e+02 - logprior: -2.5830e-01
Epoch 8/10
10/10 - 2s - loss: 149.7886 - loglik: -1.4962e+02 - logprior: 0.0871
Epoch 9/10
10/10 - 2s - loss: 149.2726 - loglik: -1.4929e+02 - logprior: 0.2715
Epoch 10/10
10/10 - 2s - loss: 149.3015 - loglik: -1.4946e+02 - logprior: 0.4095
Fitted a model with MAP estimate = -148.9858
Time for alignment: 57.2671
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 275.2464 - loglik: -2.3672e+02 - logprior: -3.8510e+01
Epoch 2/10
10/10 - 2s - loss: 223.8250 - loglik: -2.1337e+02 - logprior: -1.0430e+01
Epoch 3/10
10/10 - 2s - loss: 198.3247 - loglik: -1.9284e+02 - logprior: -5.4745e+00
Epoch 4/10
10/10 - 2s - loss: 183.9243 - loglik: -1.8008e+02 - logprior: -3.7870e+00
Epoch 5/10
10/10 - 2s - loss: 175.0496 - loglik: -1.7177e+02 - logprior: -3.0689e+00
Epoch 6/10
10/10 - 2s - loss: 171.6189 - loglik: -1.6845e+02 - logprior: -2.7727e+00
Epoch 7/10
10/10 - 2s - loss: 169.9917 - loglik: -1.6710e+02 - logprior: -2.5307e+00
Epoch 8/10
10/10 - 2s - loss: 169.2131 - loglik: -1.6649e+02 - logprior: -2.4219e+00
Epoch 9/10
10/10 - 2s - loss: 168.4797 - loglik: -1.6578e+02 - logprior: -2.3987e+00
Epoch 10/10
10/10 - 2s - loss: 168.0773 - loglik: -1.6534e+02 - logprior: -2.4241e+00
Fitted a model with MAP estimate = -167.3256
expansions: [(7, 2), (12, 1), (15, 2), (18, 1), (20, 2), (24, 3), (25, 1), (38, 1), (43, 1), (55, 3), (58, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.0517 - loglik: -1.6625e+02 - logprior: -4.3513e+01
Epoch 2/2
10/10 - 2s - loss: 174.3885 - loglik: -1.5597e+02 - logprior: -1.8115e+01
Fitted a model with MAP estimate = -167.8979
expansions: [(0, 2)]
discards: [ 0 18 25 69]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 187.4375 - loglik: -1.5276e+02 - logprior: -3.4419e+01
Epoch 2/2
10/10 - 2s - loss: 159.3583 - loglik: -1.5004e+02 - logprior: -9.1087e+00
Fitted a model with MAP estimate = -155.2113
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.8006 - loglik: -1.5267e+02 - logprior: -3.9928e+01
Epoch 2/10
10/10 - 2s - loss: 163.4560 - loglik: -1.5208e+02 - logprior: -1.1184e+01
Epoch 3/10
10/10 - 2s - loss: 155.4921 - loglik: -1.5071e+02 - logprior: -4.5115e+00
Epoch 4/10
10/10 - 2s - loss: 152.5192 - loglik: -1.4985e+02 - logprior: -2.3556e+00
Epoch 5/10
10/10 - 2s - loss: 151.0555 - loglik: -1.4934e+02 - logprior: -1.4295e+00
Epoch 6/10
10/10 - 2s - loss: 150.4491 - loglik: -1.4937e+02 - logprior: -8.2093e-01
Epoch 7/10
10/10 - 2s - loss: 149.8395 - loglik: -1.4930e+02 - logprior: -2.8427e-01
Epoch 8/10
10/10 - 2s - loss: 149.7283 - loglik: -1.4955e+02 - logprior: 0.0749
Epoch 9/10
10/10 - 2s - loss: 149.5556 - loglik: -1.4957e+02 - logprior: 0.2681
Epoch 10/10
10/10 - 2s - loss: 149.3571 - loglik: -1.4951e+02 - logprior: 0.4008
Fitted a model with MAP estimate = -149.0002
Time for alignment: 56.3585
Computed alignments with likelihoods: ['-149.1032', '-148.9858', '-149.0002']
Best model has likelihood: -148.9858
time for generating output: 0.2159
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.9222944334557784
Training of 3 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f193e431430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f198c788c40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fe275040>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193e0d2160>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f193e0d2bb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f193efd4910>, <__main__.SimpleDirichletPrior object at 0x7f19fd4d5ca0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18b3d04670>

Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 302.9627 - loglik: -2.9177e+02 - logprior: -1.1184e+01
Epoch 2/10
19/19 - 8s - loss: 243.3318 - loglik: -2.4089e+02 - logprior: -2.3962e+00
Epoch 3/10
19/19 - 5s - loss: 221.1211 - loglik: -2.1874e+02 - logprior: -2.1669e+00
Epoch 4/10
19/19 - 8s - loss: 219.2797 - loglik: -2.1692e+02 - logprior: -2.0339e+00
Epoch 5/10
19/19 - 5s - loss: 214.1651 - loglik: -2.1212e+02 - logprior: -1.7926e+00
Epoch 6/10
19/19 - 9s - loss: 216.0248 - loglik: -2.1393e+02 - logprior: -1.8206e+00
Fitted a model with MAP estimate = -214.6173
expansions: [(10, 3), (11, 2), (12, 1), (21, 1), (26, 1), (29, 1), (34, 1), (48, 5), (49, 1), (57, 1), (72, 5), (73, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 222.9646 - loglik: -2.0870e+02 - logprior: -1.3958e+01
Epoch 2/2
19/19 - 7s - loss: 205.2064 - loglik: -2.0039e+02 - logprior: -4.5208e+00
Fitted a model with MAP estimate = -200.9443
expansions: [(0, 2)]
discards: [ 0 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 208.3680 - loglik: -1.9818e+02 - logprior: -9.8230e+00
Epoch 2/2
19/19 - 7s - loss: 200.3491 - loglik: -1.9835e+02 - logprior: -1.6613e+00
Fitted a model with MAP estimate = -197.5306
expansions: []
discards: [ 0 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 212.6604 - loglik: -2.0066e+02 - logprior: -1.1617e+01
Epoch 2/10
19/19 - 6s - loss: 199.9224 - loglik: -1.9772e+02 - logprior: -1.8583e+00
Epoch 3/10
19/19 - 7s - loss: 199.2351 - loglik: -1.9828e+02 - logprior: -6.1663e-01
Epoch 4/10
19/19 - 7s - loss: 197.5878 - loglik: -1.9701e+02 - logprior: -2.4375e-01
Epoch 5/10
19/19 - 6s - loss: 196.6957 - loglik: -1.9632e+02 - logprior: -4.5828e-02
Epoch 6/10
19/19 - 8s - loss: 198.2754 - loglik: -1.9795e+02 - logprior: 0.0235
Fitted a model with MAP estimate = -196.5091
Time for alignment: 145.7996
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 303.3050 - loglik: -2.9211e+02 - logprior: -1.1187e+01
Epoch 2/10
19/19 - 7s - loss: 240.0137 - loglik: -2.3753e+02 - logprior: -2.4505e+00
Epoch 3/10
19/19 - 8s - loss: 221.3543 - loglik: -2.1880e+02 - logprior: -2.3126e+00
Epoch 4/10
19/19 - 7s - loss: 216.2109 - loglik: -2.1372e+02 - logprior: -2.1580e+00
Epoch 5/10
19/19 - 5s - loss: 214.3322 - loglik: -2.1209e+02 - logprior: -1.9389e+00
Epoch 6/10
19/19 - 7s - loss: 215.0544 - loglik: -2.1283e+02 - logprior: -1.9255e+00
Fitted a model with MAP estimate = -214.3577
expansions: [(10, 3), (11, 2), (12, 1), (21, 1), (27, 1), (29, 2), (30, 1), (48, 6), (49, 1), (57, 1), (60, 1), (72, 4), (73, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 224.0422 - loglik: -2.0972e+02 - logprior: -1.3996e+01
Epoch 2/2
19/19 - 6s - loss: 205.3413 - loglik: -2.0039e+02 - logprior: -4.6546e+00
Fitted a model with MAP estimate = -200.7646
expansions: [(0, 2)]
discards: [ 0 13 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 209.1245 - loglik: -1.9896e+02 - logprior: -9.8105e+00
Epoch 2/2
19/19 - 7s - loss: 198.7869 - loglik: -1.9682e+02 - logprior: -1.6418e+00
Fitted a model with MAP estimate = -197.2674
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 211.2661 - loglik: -1.9925e+02 - logprior: -1.1647e+01
Epoch 2/10
19/19 - 6s - loss: 200.3450 - loglik: -1.9820e+02 - logprior: -1.8211e+00
Epoch 3/10
19/19 - 8s - loss: 198.6559 - loglik: -1.9771e+02 - logprior: -6.0286e-01
Epoch 4/10
19/19 - 7s - loss: 197.7408 - loglik: -1.9717e+02 - logprior: -2.3243e-01
Epoch 5/10
19/19 - 6s - loss: 195.6498 - loglik: -1.9524e+02 - logprior: -6.4423e-02
Epoch 6/10
19/19 - 6s - loss: 196.9999 - loglik: -1.9671e+02 - logprior: 0.0654
Fitted a model with MAP estimate = -195.9891
Time for alignment: 143.6111
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 303.6113 - loglik: -2.9242e+02 - logprior: -1.1190e+01
Epoch 2/10
19/19 - 6s - loss: 240.3227 - loglik: -2.3785e+02 - logprior: -2.4307e+00
Epoch 3/10
19/19 - 7s - loss: 222.4195 - loglik: -2.1987e+02 - logprior: -2.2179e+00
Epoch 4/10
19/19 - 7s - loss: 216.2992 - loglik: -2.1401e+02 - logprior: -1.9934e+00
Epoch 5/10
19/19 - 6s - loss: 214.4561 - loglik: -2.1236e+02 - logprior: -1.8443e+00
Epoch 6/10
19/19 - 7s - loss: 215.6680 - loglik: -2.1358e+02 - logprior: -1.8307e+00
Fitted a model with MAP estimate = -214.4662
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 1), (29, 1), (46, 1), (48, 6), (49, 1), (57, 1), (72, 5), (73, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 222.3521 - loglik: -2.0811e+02 - logprior: -1.3938e+01
Epoch 2/2
19/19 - 8s - loss: 205.3006 - loglik: -2.0047e+02 - logprior: -4.5330e+00
Fitted a model with MAP estimate = -200.6167
expansions: [(0, 2)]
discards: [ 0 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 208.3745 - loglik: -1.9823e+02 - logprior: -9.7966e+00
Epoch 2/2
19/19 - 7s - loss: 198.7611 - loglik: -1.9678e+02 - logprior: -1.6496e+00
Fitted a model with MAP estimate = -197.2729
expansions: []
discards: [ 0 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 211.6882 - loglik: -1.9975e+02 - logprior: -1.1560e+01
Epoch 2/10
19/19 - 7s - loss: 200.8646 - loglik: -1.9871e+02 - logprior: -1.8186e+00
Epoch 3/10
19/19 - 8s - loss: 198.4292 - loglik: -1.9747e+02 - logprior: -6.1735e-01
Epoch 4/10
19/19 - 7s - loss: 197.9684 - loglik: -1.9738e+02 - logprior: -2.5639e-01
Epoch 5/10
19/19 - 6s - loss: 196.9140 - loglik: -1.9653e+02 - logprior: -4.7054e-02
Epoch 6/10
19/19 - 6s - loss: 197.2403 - loglik: -1.9694e+02 - logprior: 0.0555
Fitted a model with MAP estimate = -196.3136
Time for alignment: 144.9049
Computed alignments with likelihoods: ['-196.5091', '-195.9891', '-196.3136']
Best model has likelihood: -195.9891
time for generating output: 0.5759
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.21629740893728877
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19c7eb1fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19bf56f550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19bf56fc70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19bf56f280>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19bf56f580>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fe275130>, <__main__.SimpleDirichletPrior object at 0x7f19c7ffb4c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19949aab80>

Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 52s - loss: 1100.7257 - loglik: -1.0983e+03 - logprior: -1.9868e+00
Epoch 2/10
49/49 - 48s - loss: 949.9966 - loglik: -9.4749e+02 - logprior: -1.3964e+00
Epoch 3/10
49/49 - 48s - loss: 939.6547 - loglik: -9.3584e+02 - logprior: -1.5916e+00
Epoch 4/10
49/49 - 48s - loss: 931.1727 - loglik: -9.2718e+02 - logprior: -1.6406e+00
Epoch 5/10
49/49 - 48s - loss: 933.0701 - loglik: -9.2907e+02 - logprior: -1.7983e+00
Fitted a model with MAP estimate = -926.2517
expansions: [(0, 3), (82, 2), (139, 1), (180, 1), (205, 1), (211, 1), (212, 1), (230, 3), (231, 4), (232, 1), (235, 2), (239, 2), (241, 1), (251, 1), (252, 1), (254, 1), (255, 3), (256, 6), (257, 1), (271, 1), (272, 1), (273, 1), (274, 2), (275, 4), (287, 1), (292, 1), (293, 4), (294, 1), (295, 2), (298, 3), (299, 8), (300, 1), (301, 1), (310, 2), (311, 1), (312, 1), (313, 4), (314, 1), (322, 1), (324, 1), (325, 1), (327, 1), (331, 1), (344, 5), (345, 1), (346, 1), (394, 4), (395, 1), (399, 12), (400, 1), (404, 5)]
discards: [35]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 514 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 73s - loss: 932.1799 - loglik: -9.2532e+02 - logprior: -3.8117e+00
Epoch 2/2
49/49 - 68s - loss: 904.4118 - loglik: -9.0057e+02 - logprior: 0.0744
Fitted a model with MAP estimate = -894.8101
expansions: [(0, 3), (38, 3), (355, 1), (360, 1), (502, 2), (503, 1), (504, 1), (505, 1)]
discards: [  1   2   3   4   5   6  84 338 383 384 480 509 510 511 512 513]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 511 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 921.2757 - loglik: -9.1264e+02 - logprior: -3.3348e+00
Epoch 2/2
49/49 - 68s - loss: 893.8156 - loglik: -8.9021e+02 - logprior: 1.6901
Fitted a model with MAP estimate = -891.7752
expansions: [(0, 3), (511, 4)]
discards: [35 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 516 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 73s - loss: 913.9404 - loglik: -9.0502e+02 - logprior: -2.6977e+00
Epoch 2/10
49/49 - 69s - loss: 897.6450 - loglik: -8.9443e+02 - logprior: 2.4616
Epoch 3/10
49/49 - 69s - loss: 890.9669 - loglik: -8.8965e+02 - logprior: 3.2208
Epoch 4/10
49/49 - 69s - loss: 888.2813 - loglik: -8.8837e+02 - logprior: 3.8593
Epoch 5/10
49/49 - 69s - loss: 885.8621 - loglik: -8.8723e+02 - logprior: 4.5695
Epoch 6/10
49/49 - 69s - loss: 884.4139 - loglik: -8.8691e+02 - logprior: 5.3631
Epoch 7/10
49/49 - 69s - loss: 875.1956 - loglik: -8.7835e+02 - logprior: 5.7596
Epoch 8/10
49/49 - 69s - loss: 874.1589 - loglik: -8.7836e+02 - logprior: 6.8038
Epoch 9/10
49/49 - 69s - loss: 874.5953 - loglik: -8.7907e+02 - logprior: 7.2948
Fitted a model with MAP estimate = -868.3945
Time for alignment: 1334.4812
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 52s - loss: 1101.2632 - loglik: -1.0989e+03 - logprior: -1.9376e+00
Epoch 2/10
49/49 - 49s - loss: 950.9188 - loglik: -9.4815e+02 - logprior: -1.3679e+00
Epoch 3/10
49/49 - 48s - loss: 936.5621 - loglik: -9.3282e+02 - logprior: -1.4764e+00
Epoch 4/10
49/49 - 48s - loss: 933.4112 - loglik: -9.2955e+02 - logprior: -1.5948e+00
Epoch 5/10
49/49 - 48s - loss: 929.2396 - loglik: -9.2532e+02 - logprior: -1.8001e+00
Epoch 6/10
49/49 - 48s - loss: 926.6838 - loglik: -9.2277e+02 - logprior: -1.9223e+00
Epoch 7/10
49/49 - 48s - loss: 927.9792 - loglik: -9.2420e+02 - logprior: -1.9117e+00
Fitted a model with MAP estimate = -923.7546
expansions: [(0, 3), (137, 1), (183, 1), (214, 1), (215, 1), (233, 4), (234, 1), (235, 1), (236, 1), (245, 1), (248, 1), (249, 1), (254, 1), (255, 1), (256, 1), (259, 1), (260, 2), (261, 5), (263, 1), (279, 1), (280, 1), (281, 2), (282, 1), (283, 2), (299, 2), (300, 9), (301, 4), (303, 3), (304, 5), (305, 2), (313, 2), (314, 1), (315, 2), (317, 1), (318, 1), (327, 1), (328, 1), (329, 1), (331, 1), (349, 8), (351, 1), (364, 3), (365, 1), (366, 1), (368, 1), (370, 1), (389, 6), (394, 6), (395, 4)]
discards: [34 35]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 506 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 937.8373 - loglik: -9.3124e+02 - logprior: -3.9028e+00
Epoch 2/2
49/49 - 67s - loss: 906.1097 - loglik: -9.0281e+02 - logprior: 0.3482
Fitted a model with MAP estimate = -896.6250
expansions: [(0, 3), (280, 1), (351, 1), (358, 1), (359, 2), (420, 1), (496, 4)]
discards: [  1   2   3   4   5   6 447 504 505]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 510 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 914.9877 - loglik: -9.0656e+02 - logprior: -3.3406e+00
Epoch 2/2
49/49 - 68s - loss: 905.3287 - loglik: -9.0166e+02 - logprior: 1.5303
Fitted a model with MAP estimate = -892.4257
expansions: [(0, 3), (501, 1), (502, 1), (503, 1), (510, 3)]
discards: [481]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 518 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 74s - loss: 914.5705 - loglik: -9.0548e+02 - logprior: -2.8860e+00
Epoch 2/10
49/49 - 69s - loss: 896.6833 - loglik: -8.9340e+02 - logprior: 2.3904
Epoch 3/10
49/49 - 69s - loss: 894.4810 - loglik: -8.9275e+02 - logprior: 2.7748
Epoch 4/10
49/49 - 70s - loss: 889.2878 - loglik: -8.8955e+02 - logprior: 3.9907
Epoch 5/10
49/49 - 69s - loss: 884.6677 - loglik: -8.8637e+02 - logprior: 4.9077
Epoch 6/10
49/49 - 69s - loss: 880.0051 - loglik: -8.8248e+02 - logprior: 5.3991
Epoch 7/10
49/49 - 69s - loss: 880.5925 - loglik: -8.8325e+02 - logprior: 5.3797
Fitted a model with MAP estimate = -875.3849
Time for alignment: 1289.9746
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 52s - loss: 1104.7717 - loglik: -1.1022e+03 - logprior: -2.1031e+00
Epoch 2/10
49/49 - 48s - loss: 953.7609 - loglik: -9.5077e+02 - logprior: -1.3452e+00
Epoch 3/10
49/49 - 48s - loss: 940.7018 - loglik: -9.3652e+02 - logprior: -1.7508e+00
Epoch 4/10
49/49 - 48s - loss: 934.1857 - loglik: -9.2967e+02 - logprior: -2.1004e+00
Epoch 5/10
49/49 - 48s - loss: 933.4659 - loglik: -9.2901e+02 - logprior: -2.2343e+00
Epoch 6/10
49/49 - 48s - loss: 928.4282 - loglik: -9.2412e+02 - logprior: -2.2550e+00
Epoch 7/10
49/49 - 49s - loss: 928.1610 - loglik: -9.2397e+02 - logprior: -2.2102e+00
Epoch 8/10
49/49 - 49s - loss: 925.7034 - loglik: -9.2158e+02 - logprior: -2.2111e+00
Epoch 9/10
49/49 - 48s - loss: 926.6088 - loglik: -9.2251e+02 - logprior: -2.2202e+00
Fitted a model with MAP estimate = -922.3659
expansions: [(0, 3), (106, 1), (131, 1), (135, 1), (138, 1), (179, 1), (182, 1), (189, 1), (209, 1), (210, 1), (224, 1), (226, 4), (227, 3), (228, 1), (231, 4), (234, 2), (235, 2), (236, 1), (241, 1), (242, 1), (243, 1), (246, 1), (247, 2), (249, 5), (267, 1), (268, 1), (269, 2), (270, 1), (271, 1), (289, 1), (291, 5), (292, 1), (293, 1), (296, 2), (297, 6), (299, 1), (304, 1), (310, 5), (312, 1), (313, 1), (322, 1), (323, 1), (324, 1), (345, 5), (346, 1), (347, 1), (366, 1), (372, 1), (391, 2), (392, 4), (397, 1), (398, 2), (399, 8)]
discards: [34]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 504 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 71s - loss: 950.0903 - loglik: -9.4331e+02 - logprior: -4.0333e+00
Epoch 2/2
49/49 - 67s - loss: 905.3281 - loglik: -9.0142e+02 - logprior: -3.2329e-02
Fitted a model with MAP estimate = -896.6017
expansions: [(0, 3), (241, 1), (281, 1), (445, 1), (488, 1), (489, 1), (496, 4), (498, 1)]
discards: [  2   3   4   5  37 253 477 499 500 501 502 503]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 505 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 70s - loss: 918.6941 - loglik: -9.1019e+02 - logprior: -3.1128e+00
Epoch 2/2
49/49 - 67s - loss: 903.9073 - loglik: -9.0005e+02 - logprior: 1.5137
Fitted a model with MAP estimate = -892.7338
expansions: [(0, 2), (37, 3), (498, 1), (499, 1)]
discards: [  1   2  21 503 504]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 507 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 71s - loss: 916.3963 - loglik: -9.0760e+02 - logprior: -2.5508e+00
Epoch 2/10
49/49 - 67s - loss: 899.8304 - loglik: -8.9755e+02 - logprior: 3.4139
Epoch 3/10
49/49 - 67s - loss: 893.0211 - loglik: -8.9254e+02 - logprior: 4.1054
Epoch 4/10
49/49 - 67s - loss: 890.4990 - loglik: -8.9140e+02 - logprior: 4.6904
Epoch 5/10
49/49 - 68s - loss: 888.3629 - loglik: -8.9051e+02 - logprior: 5.3911
Epoch 6/10
49/49 - 68s - loss: 881.1036 - loglik: -8.8410e+02 - logprior: 5.9478
Epoch 7/10
49/49 - 67s - loss: 880.7778 - loglik: -8.8470e+02 - logprior: 6.7250
Epoch 8/10
49/49 - 67s - loss: 880.2594 - loglik: -8.8472e+02 - logprior: 7.4339
Epoch 9/10
49/49 - 67s - loss: 871.2734 - loglik: -8.7629e+02 - logprior: 8.1803
Epoch 10/10
49/49 - 67s - loss: 871.3444 - loglik: -8.7705e+02 - logprior: 8.7755
Fitted a model with MAP estimate = -866.0365
Time for alignment: 1570.6905
Computed alignments with likelihoods: ['-868.3945', '-875.3849', '-866.0365']
Best model has likelihood: -866.0365
time for generating output: 0.5301
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.8263536943184849
Training of 3 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f197b397d00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1972ccd070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fdfd6670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1972a812b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1972a7bd60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fed71b20>, <__main__.SimpleDirichletPrior object at 0x7f1878010eb0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe47c670>

Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 478.3851 - loglik: -4.7610e+02 - logprior: -2.0381e+00
Epoch 2/10
39/39 - 12s - loss: 408.5694 - loglik: -4.0618e+02 - logprior: -1.8325e+00
Epoch 3/10
39/39 - 12s - loss: 398.7517 - loglik: -3.9602e+02 - logprior: -1.9454e+00
Epoch 4/10
39/39 - 12s - loss: 396.6668 - loglik: -3.9394e+02 - logprior: -1.9107e+00
Epoch 5/10
39/39 - 11s - loss: 395.6700 - loglik: -3.9304e+02 - logprior: -1.9081e+00
Epoch 6/10
39/39 - 11s - loss: 395.0506 - loglik: -3.9247e+02 - logprior: -1.9133e+00
Epoch 7/10
39/39 - 12s - loss: 394.4710 - loglik: -3.9193e+02 - logprior: -1.9114e+00
Epoch 8/10
39/39 - 12s - loss: 394.6284 - loglik: -3.9212e+02 - logprior: -1.9166e+00
Fitted a model with MAP estimate = -395.0024
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 2), (32, 1), (39, 2), (41, 1), (45, 1), (55, 2), (57, 2), (58, 1), (59, 2), (68, 1), (71, 2), (89, 2), (92, 1), (93, 2), (95, 2), (99, 2), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (122, 1), (126, 2), (133, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 395.8083 - loglik: -3.9174e+02 - logprior: -3.1840e+00
Epoch 2/2
39/39 - 14s - loss: 380.1012 - loglik: -3.7769e+02 - logprior: -1.4087e+00
Fitted a model with MAP estimate = -376.9038
expansions: []
discards: [ 13  34  70  73  78  93 113 120 124 130 167]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 383.6316 - loglik: -3.8032e+02 - logprior: -2.2383e+00
Epoch 2/2
39/39 - 14s - loss: 379.4180 - loglik: -3.7742e+02 - logprior: -9.9674e-01
Fitted a model with MAP estimate = -377.5128
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 17s - loss: 381.1821 - loglik: -3.7810e+02 - logprior: -1.8873e+00
Epoch 2/10
41/41 - 14s - loss: 377.7364 - loglik: -3.7582e+02 - logprior: -8.3221e-01
Epoch 3/10
41/41 - 14s - loss: 375.9279 - loglik: -3.7409e+02 - logprior: -7.5958e-01
Epoch 4/10
41/41 - 14s - loss: 374.6254 - loglik: -3.7291e+02 - logprior: -6.9874e-01
Epoch 5/10
41/41 - 14s - loss: 374.7492 - loglik: -3.7317e+02 - logprior: -6.2600e-01
Fitted a model with MAP estimate = -372.4803
Time for alignment: 301.9193
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 478.8270 - loglik: -4.7656e+02 - logprior: -2.0260e+00
Epoch 2/10
39/39 - 11s - loss: 412.2059 - loglik: -4.0967e+02 - logprior: -1.6991e+00
Epoch 3/10
39/39 - 12s - loss: 404.6562 - loglik: -4.0211e+02 - logprior: -1.7052e+00
Epoch 4/10
39/39 - 11s - loss: 402.3728 - loglik: -3.9987e+02 - logprior: -1.6886e+00
Epoch 5/10
39/39 - 11s - loss: 401.3791 - loglik: -3.9895e+02 - logprior: -1.6962e+00
Epoch 6/10
39/39 - 12s - loss: 400.5496 - loglik: -3.9819e+02 - logprior: -1.6998e+00
Epoch 7/10
39/39 - 12s - loss: 400.2072 - loglik: -3.9788e+02 - logprior: -1.7166e+00
Epoch 8/10
39/39 - 12s - loss: 400.3436 - loglik: -3.9804e+02 - logprior: -1.7202e+00
Fitted a model with MAP estimate = -400.6968
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (32, 1), (40, 2), (45, 3), (55, 2), (57, 1), (58, 2), (59, 1), (67, 1), (69, 2), (71, 2), (89, 2), (94, 1), (96, 1), (100, 1), (102, 2), (103, 2), (104, 3), (106, 2), (118, 1), (122, 1), (126, 2), (133, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 399.4584 - loglik: -3.9542e+02 - logprior: -3.1732e+00
Epoch 2/2
39/39 - 14s - loss: 383.0856 - loglik: -3.8054e+02 - logprior: -1.4697e+00
Fitted a model with MAP estimate = -380.3592
expansions: []
discards: [ 13  69  74  89  93 113 132 134 142 167 175]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 385.7915 - loglik: -3.8236e+02 - logprior: -2.2646e+00
Epoch 2/2
39/39 - 14s - loss: 381.6696 - loglik: -3.7952e+02 - logprior: -1.0160e+00
Fitted a model with MAP estimate = -379.8612
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 17s - loss: 383.6537 - loglik: -3.8037e+02 - logprior: -1.9156e+00
Epoch 2/10
41/41 - 14s - loss: 379.8221 - loglik: -3.7774e+02 - logprior: -8.4661e-01
Epoch 3/10
41/41 - 14s - loss: 377.6420 - loglik: -3.7573e+02 - logprior: -7.6620e-01
Epoch 4/10
41/41 - 14s - loss: 376.0105 - loglik: -3.7424e+02 - logprior: -6.9005e-01
Epoch 5/10
41/41 - 14s - loss: 373.5850 - loglik: -3.7199e+02 - logprior: -6.1543e-01
Epoch 6/10
41/41 - 14s - loss: 374.2340 - loglik: -3.7280e+02 - logprior: -5.4638e-01
Fitted a model with MAP estimate = -372.3668
Time for alignment: 315.4769
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 477.1915 - loglik: -4.7486e+02 - logprior: -2.0640e+00
Epoch 2/10
39/39 - 11s - loss: 408.8818 - loglik: -4.0606e+02 - logprior: -1.8403e+00
Epoch 3/10
39/39 - 12s - loss: 399.6359 - loglik: -3.9692e+02 - logprior: -1.9236e+00
Epoch 4/10
39/39 - 11s - loss: 397.5892 - loglik: -3.9494e+02 - logprior: -1.9234e+00
Epoch 5/10
39/39 - 11s - loss: 396.2976 - loglik: -3.9368e+02 - logprior: -1.9344e+00
Epoch 6/10
39/39 - 11s - loss: 395.6529 - loglik: -3.9307e+02 - logprior: -1.9549e+00
Epoch 7/10
39/39 - 12s - loss: 395.0616 - loglik: -3.9252e+02 - logprior: -1.9613e+00
Epoch 8/10
39/39 - 12s - loss: 395.4512 - loglik: -3.9295e+02 - logprior: -1.9608e+00
Fitted a model with MAP estimate = -394.9921
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 2), (23, 1), (26, 2), (40, 2), (45, 1), (46, 1), (55, 1), (56, 1), (57, 2), (58, 2), (59, 1), (69, 2), (71, 2), (80, 1), (89, 2), (90, 2), (93, 1), (95, 2), (99, 2), (103, 2), (104, 1), (105, 1), (106, 1), (109, 1), (118, 2), (119, 1), (122, 1), (126, 1), (133, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 395.3794 - loglik: -3.9138e+02 - logprior: -3.2135e+00
Epoch 2/2
39/39 - 14s - loss: 378.9842 - loglik: -3.7657e+02 - logprior: -1.4958e+00
Fitted a model with MAP estimate = -375.4845
expansions: []
discards: [ 13  28  35  74  76  91  94 116 118 126 132 139 159]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 382.1748 - loglik: -3.7893e+02 - logprior: -2.2272e+00
Epoch 2/2
39/39 - 14s - loss: 377.8765 - loglik: -3.7592e+02 - logprior: -9.8527e-01
Fitted a model with MAP estimate = -376.3195
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 17s - loss: 379.5777 - loglik: -3.7655e+02 - logprior: -1.8731e+00
Epoch 2/10
41/41 - 14s - loss: 376.1500 - loglik: -3.7427e+02 - logprior: -8.0565e-01
Epoch 3/10
41/41 - 14s - loss: 374.2620 - loglik: -3.7248e+02 - logprior: -7.3209e-01
Epoch 4/10
41/41 - 14s - loss: 373.1463 - loglik: -3.7153e+02 - logprior: -6.6091e-01
Epoch 5/10
41/41 - 14s - loss: 373.2203 - loglik: -3.7174e+02 - logprior: -5.8519e-01
Fitted a model with MAP estimate = -370.7999
Time for alignment: 301.6362
Computed alignments with likelihoods: ['-372.4803', '-372.3668', '-370.7999']
Best model has likelihood: -370.7999
time for generating output: 0.3199
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.3001638001638002
Training of 3 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19592ee4f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f195087e070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f193d4a8430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f198c32aa30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19bf545190>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f198bad61c0>, <__main__.SimpleDirichletPrior object at 0x7f193d0c9df0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fcfca040>

Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 170.9426 - loglik: -1.1175e+02 - logprior: -5.9181e+01
Epoch 2/10
10/10 - 1s - loss: 105.9441 - loglik: -8.9085e+01 - logprior: -1.6851e+01
Epoch 3/10
10/10 - 1s - loss: 81.0249 - loglik: -7.2462e+01 - logprior: -8.5289e+00
Epoch 4/10
10/10 - 1s - loss: 70.4637 - loglik: -6.4962e+01 - logprior: -5.4523e+00
Epoch 5/10
10/10 - 1s - loss: 66.1971 - loglik: -6.2281e+01 - logprior: -3.8976e+00
Epoch 6/10
10/10 - 1s - loss: 64.2811 - loglik: -6.1070e+01 - logprior: -3.1617e+00
Epoch 7/10
10/10 - 1s - loss: 62.9166 - loglik: -5.9946e+01 - logprior: -2.7595e+00
Epoch 8/10
10/10 - 1s - loss: 62.0783 - loglik: -5.9297e+01 - logprior: -2.4635e+00
Epoch 9/10
10/10 - 1s - loss: 61.4454 - loglik: -5.8899e+01 - logprior: -2.2561e+00
Epoch 10/10
10/10 - 1s - loss: 61.4279 - loglik: -5.9064e+01 - logprior: -2.1038e+00
Fitted a model with MAP estimate = -60.8864
expansions: [(0, 4), (10, 2), (21, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.4497 - loglik: -5.7535e+01 - logprior: -7.9742e+01
Epoch 2/2
10/10 - 1s - loss: 79.2762 - loglik: -5.3312e+01 - logprior: -2.5890e+01
Fitted a model with MAP estimate = -68.2253
expansions: [(0, 2)]
discards: [14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.3739 - loglik: -5.1055e+01 - logprior: -6.6218e+01
Epoch 2/2
10/10 - 1s - loss: 73.9926 - loglik: -5.0422e+01 - logprior: -2.3489e+01
Fitted a model with MAP estimate = -64.8732
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 117.5560 - loglik: -5.1203e+01 - logprior: -6.6251e+01
Epoch 2/10
10/10 - 1s - loss: 79.1416 - loglik: -5.1396e+01 - logprior: -2.7672e+01
Epoch 3/10
10/10 - 1s - loss: 67.9430 - loglik: -5.1509e+01 - logprior: -1.6278e+01
Epoch 4/10
10/10 - 1s - loss: 58.4403 - loglik: -5.1321e+01 - logprior: -6.8674e+00
Epoch 5/10
10/10 - 1s - loss: 54.9152 - loglik: -5.1302e+01 - logprior: -3.3273e+00
Epoch 6/10
10/10 - 1s - loss: 53.7889 - loglik: -5.1332e+01 - logprior: -2.1637e+00
Epoch 7/10
10/10 - 1s - loss: 53.2271 - loglik: -5.1327e+01 - logprior: -1.5916e+00
Epoch 8/10
10/10 - 1s - loss: 52.9893 - loglik: -5.1423e+01 - logprior: -1.2476e+00
Epoch 9/10
10/10 - 1s - loss: 52.8137 - loglik: -5.1493e+01 - logprior: -9.9644e-01
Epoch 10/10
10/10 - 1s - loss: 52.5709 - loglik: -5.1455e+01 - logprior: -7.8623e-01
Fitted a model with MAP estimate = -52.2145
Time for alignment: 31.0053
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.8155 - loglik: -1.1162e+02 - logprior: -5.9181e+01
Epoch 2/10
10/10 - 1s - loss: 106.0347 - loglik: -8.9176e+01 - logprior: -1.6850e+01
Epoch 3/10
10/10 - 1s - loss: 80.9115 - loglik: -7.2354e+01 - logprior: -8.5228e+00
Epoch 4/10
10/10 - 1s - loss: 70.6446 - loglik: -6.5168e+01 - logprior: -5.4271e+00
Epoch 5/10
10/10 - 1s - loss: 65.6286 - loglik: -6.1798e+01 - logprior: -3.8120e+00
Epoch 6/10
10/10 - 1s - loss: 62.9514 - loglik: -5.9862e+01 - logprior: -3.0561e+00
Epoch 7/10
10/10 - 1s - loss: 61.7536 - loglik: -5.8883e+01 - logprior: -2.6920e+00
Epoch 8/10
10/10 - 1s - loss: 61.4443 - loglik: -5.8708e+01 - logprior: -2.4166e+00
Epoch 9/10
10/10 - 1s - loss: 61.1893 - loglik: -5.8671e+01 - logprior: -2.2101e+00
Epoch 10/10
10/10 - 1s - loss: 60.8783 - loglik: -5.8559e+01 - logprior: -2.0428e+00
Fitted a model with MAP estimate = -60.6274
expansions: [(0, 4), (10, 2), (26, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 137.4261 - loglik: -5.7599e+01 - logprior: -7.9662e+01
Epoch 2/2
10/10 - 1s - loss: 79.4902 - loglik: -5.3366e+01 - logprior: -2.6051e+01
Fitted a model with MAP estimate = -68.4752
expansions: [(0, 2)]
discards: [14 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.3727 - loglik: -5.1026e+01 - logprior: -6.6232e+01
Epoch 2/2
10/10 - 1s - loss: 74.1494 - loglik: -5.0579e+01 - logprior: -2.3484e+01
Fitted a model with MAP estimate = -64.9534
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 117.5626 - loglik: -5.1205e+01 - logprior: -6.6245e+01
Epoch 2/10
10/10 - 1s - loss: 79.3313 - loglik: -5.1571e+01 - logprior: -2.7681e+01
Epoch 3/10
10/10 - 1s - loss: 67.9237 - loglik: -5.1430e+01 - logprior: -1.6354e+01
Epoch 4/10
10/10 - 1s - loss: 58.7141 - loglik: -5.1554e+01 - logprior: -6.8943e+00
Epoch 5/10
10/10 - 1s - loss: 55.0331 - loglik: -5.1420e+01 - logprior: -3.3147e+00
Epoch 6/10
10/10 - 1s - loss: 53.8612 - loglik: -5.1411e+01 - logprior: -2.1706e+00
Epoch 7/10
10/10 - 1s - loss: 53.3144 - loglik: -5.1425e+01 - logprior: -1.5998e+00
Epoch 8/10
10/10 - 1s - loss: 52.9101 - loglik: -5.1336e+01 - logprior: -1.2625e+00
Epoch 9/10
10/10 - 1s - loss: 52.7981 - loglik: -5.1466e+01 - logprior: -1.0118e+00
Epoch 10/10
10/10 - 1s - loss: 52.5968 - loglik: -5.1471e+01 - logprior: -7.9727e-01
Fitted a model with MAP estimate = -52.2500
Time for alignment: 29.7537
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 170.8353 - loglik: -1.1164e+02 - logprior: -5.9181e+01
Epoch 2/10
10/10 - 1s - loss: 105.9759 - loglik: -8.9118e+01 - logprior: -1.6849e+01
Epoch 3/10
10/10 - 1s - loss: 80.9637 - loglik: -7.2403e+01 - logprior: -8.5263e+00
Epoch 4/10
10/10 - 1s - loss: 70.7277 - loglik: -6.5227e+01 - logprior: -5.4514e+00
Epoch 5/10
10/10 - 1s - loss: 66.1166 - loglik: -6.2209e+01 - logprior: -3.8899e+00
Epoch 6/10
10/10 - 1s - loss: 63.9984 - loglik: -6.0876e+01 - logprior: -3.1080e+00
Epoch 7/10
10/10 - 1s - loss: 62.8099 - loglik: -6.0147e+01 - logprior: -2.6498e+00
Epoch 8/10
10/10 - 1s - loss: 62.1689 - loglik: -5.9798e+01 - logprior: -2.3634e+00
Epoch 9/10
10/10 - 1s - loss: 61.7474 - loglik: -5.9591e+01 - logprior: -2.1472e+00
Epoch 10/10
10/10 - 1s - loss: 61.5806 - loglik: -5.9451e+01 - logprior: -2.0324e+00
Fitted a model with MAP estimate = -61.0944
expansions: [(0, 4), (10, 2), (21, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 137.1209 - loglik: -5.7347e+01 - logprior: -7.9655e+01
Epoch 2/2
10/10 - 1s - loss: 79.4562 - loglik: -5.3546e+01 - logprior: -2.5840e+01
Fitted a model with MAP estimate = -68.2885
expansions: [(0, 2)]
discards: [14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 117.3023 - loglik: -5.1011e+01 - logprior: -6.6193e+01
Epoch 2/2
10/10 - 1s - loss: 74.0899 - loglik: -5.0546e+01 - logprior: -2.3470e+01
Fitted a model with MAP estimate = -64.8965
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 117.4179 - loglik: -5.1073e+01 - logprior: -6.6233e+01
Epoch 2/10
10/10 - 1s - loss: 79.3007 - loglik: -5.1541e+01 - logprior: -2.7675e+01
Epoch 3/10
10/10 - 1s - loss: 67.9500 - loglik: -5.1519e+01 - logprior: -1.6288e+01
Epoch 4/10
10/10 - 1s - loss: 58.5603 - loglik: -5.1456e+01 - logprior: -6.8553e+00
Epoch 5/10
10/10 - 1s - loss: 54.9820 - loglik: -5.1376e+01 - logprior: -3.3252e+00
Epoch 6/10
10/10 - 1s - loss: 53.7151 - loglik: -5.1254e+01 - logprior: -2.1662e+00
Epoch 7/10
10/10 - 1s - loss: 53.3959 - loglik: -5.1503e+01 - logprior: -1.5857e+00
Epoch 8/10
10/10 - 1s - loss: 52.7652 - loglik: -5.1202e+01 - logprior: -1.2458e+00
Epoch 9/10
10/10 - 1s - loss: 52.7993 - loglik: -5.1478e+01 - logprior: -9.9635e-01
Fitted a model with MAP estimate = -52.3501
Time for alignment: 27.8506
Computed alignments with likelihoods: ['-52.2145', '-52.2500', '-52.3501']
Best model has likelihood: -52.2145
time for generating output: 0.1005
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.966183574879227
Training of 3 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f193d5f9b20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1880b88af0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19485fac40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18b354bdc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f187823e430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1961cbdb20>, <__main__.SimpleDirichletPrior object at 0x7f198c0b16a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fcfca040>

Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 9s - loss: 372.7210 - loglik: -3.6972e+02 - logprior: -2.8759e+00
Epoch 2/10
20/20 - 4s - loss: 329.5494 - loglik: -3.2748e+02 - logprior: -1.3377e+00
Epoch 3/10
20/20 - 4s - loss: 307.7646 - loglik: -3.0507e+02 - logprior: -1.6411e+00
Epoch 4/10
20/20 - 5s - loss: 302.4451 - loglik: -2.9968e+02 - logprior: -1.6119e+00
Epoch 5/10
20/20 - 4s - loss: 300.1902 - loglik: -2.9758e+02 - logprior: -1.6090e+00
Epoch 6/10
20/20 - 4s - loss: 299.2789 - loglik: -2.9681e+02 - logprior: -1.5666e+00
Epoch 7/10
20/20 - 4s - loss: 297.9160 - loglik: -2.9550e+02 - logprior: -1.5532e+00
Epoch 8/10
20/20 - 4s - loss: 297.8658 - loglik: -2.9554e+02 - logprior: -1.5425e+00
Epoch 9/10
20/20 - 5s - loss: 297.7413 - loglik: -2.9549e+02 - logprior: -1.5356e+00
Epoch 10/10
20/20 - 4s - loss: 296.5611 - loglik: -2.9436e+02 - logprior: -1.5394e+00
Fitted a model with MAP estimate = -290.7340
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 2), (61, 1), (64, 2), (76, 1), (78, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 303.7412 - loglik: -2.9975e+02 - logprior: -2.8836e+00
Epoch 2/2
40/40 - 7s - loss: 288.7028 - loglik: -2.8604e+02 - logprior: -1.1330e+00
Fitted a model with MAP estimate = -272.7944
expansions: []
discards: [  8  13  46  73  86 103]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 291.1004 - loglik: -2.8717e+02 - logprior: -2.0667e+00
Epoch 2/2
40/40 - 7s - loss: 286.9865 - loglik: -2.8448e+02 - logprior: -9.4052e-01
Fitted a model with MAP estimate = -272.8376
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 12s - loss: 271.5134 - loglik: -2.6813e+02 - logprior: -1.2371e+00
Epoch 2/10
57/57 - 9s - loss: 269.8782 - loglik: -2.6707e+02 - logprior: -9.2442e-01
Epoch 3/10
57/57 - 9s - loss: 267.4739 - loglik: -2.6488e+02 - logprior: -8.7986e-01
Epoch 4/10
57/57 - 9s - loss: 265.9013 - loglik: -2.6361e+02 - logprior: -8.4612e-01
Epoch 5/10
57/57 - 9s - loss: 265.2610 - loglik: -2.6313e+02 - logprior: -8.0695e-01
Epoch 6/10
57/57 - 9s - loss: 264.4805 - loglik: -2.6240e+02 - logprior: -7.7461e-01
Epoch 7/10
57/57 - 9s - loss: 263.7525 - loglik: -2.6192e+02 - logprior: -7.3921e-01
Epoch 8/10
57/57 - 9s - loss: 263.3249 - loglik: -2.6156e+02 - logprior: -7.0307e-01
Epoch 9/10
57/57 - 9s - loss: 263.0342 - loglik: -2.6135e+02 - logprior: -6.7112e-01
Epoch 10/10
57/57 - 9s - loss: 262.7180 - loglik: -2.6121e+02 - logprior: -6.3630e-01
Fitted a model with MAP estimate = -261.2077
Time for alignment: 232.5482
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 8s - loss: 372.8077 - loglik: -3.6981e+02 - logprior: -2.8747e+00
Epoch 2/10
20/20 - 4s - loss: 327.2734 - loglik: -3.2517e+02 - logprior: -1.3480e+00
Epoch 3/10
20/20 - 4s - loss: 307.0300 - loglik: -3.0405e+02 - logprior: -1.6478e+00
Epoch 4/10
20/20 - 4s - loss: 302.5117 - loglik: -2.9973e+02 - logprior: -1.6060e+00
Epoch 5/10
20/20 - 4s - loss: 300.3250 - loglik: -2.9767e+02 - logprior: -1.6362e+00
Epoch 6/10
20/20 - 4s - loss: 298.3920 - loglik: -2.9582e+02 - logprior: -1.6213e+00
Epoch 7/10
20/20 - 4s - loss: 297.8656 - loglik: -2.9539e+02 - logprior: -1.6133e+00
Epoch 8/10
20/20 - 4s - loss: 297.4361 - loglik: -2.9506e+02 - logprior: -1.5982e+00
Epoch 9/10
20/20 - 4s - loss: 296.6421 - loglik: -2.9434e+02 - logprior: -1.6001e+00
Epoch 10/10
20/20 - 4s - loss: 296.4456 - loglik: -2.9420e+02 - logprior: -1.5902e+00
Fitted a model with MAP estimate = -290.1808
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 1), (64, 2), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 303.4857 - loglik: -2.9948e+02 - logprior: -2.8846e+00
Epoch 2/2
40/40 - 7s - loss: 288.2650 - loglik: -2.8557e+02 - logprior: -1.1516e+00
Fitted a model with MAP estimate = -272.5232
expansions: []
discards: [  8  13  46  74  86 102 105 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 291.1049 - loglik: -2.8714e+02 - logprior: -2.0711e+00
Epoch 2/2
40/40 - 7s - loss: 286.8762 - loglik: -2.8434e+02 - logprior: -9.4743e-01
Fitted a model with MAP estimate = -272.9551
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 13s - loss: 271.7408 - loglik: -2.6833e+02 - logprior: -1.2448e+00
Epoch 2/10
57/57 - 9s - loss: 269.1405 - loglik: -2.6634e+02 - logprior: -9.2621e-01
Epoch 3/10
57/57 - 9s - loss: 267.2500 - loglik: -2.6462e+02 - logprior: -8.8313e-01
Epoch 4/10
57/57 - 9s - loss: 265.8784 - loglik: -2.6360e+02 - logprior: -8.4310e-01
Epoch 5/10
57/57 - 9s - loss: 265.3571 - loglik: -2.6320e+02 - logprior: -8.1335e-01
Epoch 6/10
57/57 - 9s - loss: 264.2180 - loglik: -2.6212e+02 - logprior: -7.8620e-01
Epoch 7/10
57/57 - 9s - loss: 263.6989 - loglik: -2.6186e+02 - logprior: -7.4504e-01
Epoch 8/10
57/57 - 9s - loss: 262.8310 - loglik: -2.6107e+02 - logprior: -7.1290e-01
Epoch 9/10
57/57 - 9s - loss: 262.7857 - loglik: -2.6108e+02 - logprior: -6.8347e-01
Epoch 10/10
57/57 - 9s - loss: 262.1851 - loglik: -2.6066e+02 - logprior: -6.4573e-01
Fitted a model with MAP estimate = -261.0859
Time for alignment: 232.5365
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 373.0418 - loglik: -3.7004e+02 - logprior: -2.8740e+00
Epoch 2/10
20/20 - 4s - loss: 327.3369 - loglik: -3.2528e+02 - logprior: -1.3360e+00
Epoch 3/10
20/20 - 4s - loss: 308.0266 - loglik: -3.0540e+02 - logprior: -1.6239e+00
Epoch 4/10
20/20 - 4s - loss: 303.0535 - loglik: -3.0040e+02 - logprior: -1.6241e+00
Epoch 5/10
20/20 - 4s - loss: 301.3608 - loglik: -2.9882e+02 - logprior: -1.6424e+00
Epoch 6/10
20/20 - 4s - loss: 300.5901 - loglik: -2.9817e+02 - logprior: -1.5891e+00
Epoch 7/10
20/20 - 5s - loss: 299.7926 - loglik: -2.9743e+02 - logprior: -1.5786e+00
Epoch 8/10
20/20 - 4s - loss: 298.9067 - loglik: -2.9659e+02 - logprior: -1.5828e+00
Epoch 9/10
20/20 - 4s - loss: 298.6105 - loglik: -2.9635e+02 - logprior: -1.5818e+00
Epoch 10/10
20/20 - 4s - loss: 298.4190 - loglik: -2.9621e+02 - logprior: -1.5801e+00
Fitted a model with MAP estimate = -290.9357
expansions: [(5, 1), (8, 1), (10, 2), (13, 1), (19, 1), (21, 1), (22, 1), (31, 1), (35, 1), (36, 2), (37, 1), (39, 1), (46, 1), (56, 2), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (91, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 303.4839 - loglik: -2.9949e+02 - logprior: -2.8795e+00
Epoch 2/2
40/40 - 7s - loss: 288.6234 - loglik: -2.8592e+02 - logprior: -1.1371e+00
Fitted a model with MAP estimate = -272.8373
expansions: []
discards: [ 12  45  71 100 103 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 11s - loss: 291.3378 - loglik: -2.8733e+02 - logprior: -2.0801e+00
Epoch 2/2
40/40 - 7s - loss: 287.2310 - loglik: -2.8465e+02 - logprior: -9.7154e-01
Fitted a model with MAP estimate = -272.7933
expansions: [(119, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 13s - loss: 272.6138 - loglik: -2.6919e+02 - logprior: -1.2165e+00
Epoch 2/10
57/57 - 9s - loss: 268.5027 - loglik: -2.6580e+02 - logprior: -8.5841e-01
Epoch 3/10
57/57 - 9s - loss: 267.4832 - loglik: -2.6495e+02 - logprior: -8.0899e-01
Epoch 4/10
57/57 - 9s - loss: 266.2449 - loglik: -2.6405e+02 - logprior: -7.7260e-01
Epoch 5/10
57/57 - 8s - loss: 265.1497 - loglik: -2.6308e+02 - logprior: -7.4419e-01
Epoch 6/10
57/57 - 9s - loss: 264.8803 - loglik: -2.6286e+02 - logprior: -7.1322e-01
Epoch 7/10
57/57 - 9s - loss: 263.9649 - loglik: -2.6217e+02 - logprior: -6.7600e-01
Epoch 8/10
57/57 - 9s - loss: 263.5120 - loglik: -2.6180e+02 - logprior: -6.4315e-01
Epoch 9/10
57/57 - 9s - loss: 262.8092 - loglik: -2.6116e+02 - logprior: -6.2187e-01
Epoch 10/10
57/57 - 9s - loss: 262.1873 - loglik: -2.6074e+02 - logprior: -5.8396e-01
Fitted a model with MAP estimate = -261.3355
Time for alignment: 228.6921
Computed alignments with likelihoods: ['-261.2077', '-261.0859', '-261.3355']
Best model has likelihood: -261.0859
time for generating output: 0.3370
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.6077852348993289
Training of 3 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19a596e340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19c7c880d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19c7c88b20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19597e98e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19597e9b50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f198c12eb80>, <__main__.SimpleDirichletPrior object at 0x7f1950838fd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f17a0210700>

Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 686.7849 - loglik: -6.7660e+02 - logprior: -1.0067e+01
Epoch 2/10
19/19 - 8s - loss: 617.9536 - loglik: -6.1616e+02 - logprior: -9.8371e-01
Epoch 3/10
19/19 - 8s - loss: 585.5768 - loglik: -5.8290e+02 - logprior: -1.3742e+00
Epoch 4/10
19/19 - 8s - loss: 577.5320 - loglik: -5.7430e+02 - logprior: -1.6331e+00
Epoch 5/10
19/19 - 8s - loss: 571.5109 - loglik: -5.6796e+02 - logprior: -1.6097e+00
Epoch 6/10
19/19 - 8s - loss: 568.4584 - loglik: -5.6481e+02 - logprior: -1.6900e+00
Epoch 7/10
19/19 - 8s - loss: 567.1516 - loglik: -5.6354e+02 - logprior: -1.8256e+00
Epoch 8/10
19/19 - 8s - loss: 565.5909 - loglik: -5.6212e+02 - logprior: -1.8812e+00
Epoch 9/10
19/19 - 8s - loss: 562.9370 - loglik: -5.5957e+02 - logprior: -1.9127e+00
Epoch 10/10
19/19 - 8s - loss: 561.6409 - loglik: -5.5840e+02 - logprior: -1.8920e+00
Fitted a model with MAP estimate = -561.4422
expansions: [(18, 1), (25, 1), (27, 1), (28, 1), (30, 2), (43, 6), (64, 2), (65, 2), (91, 1), (93, 3), (101, 1), (102, 2), (103, 1), (104, 1), (120, 1), (136, 1), (149, 2), (151, 3), (152, 1), (163, 1), (164, 1), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [  2 170 171 172 173 174 175 176 177 178]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 624.5598 - loglik: -6.1208e+02 - logprior: -1.0954e+01
Epoch 2/2
19/19 - 9s - loss: 578.6230 - loglik: -5.7469e+02 - logprior: -1.7391e+00
Fitted a model with MAP estimate = -568.2099
expansions: [(199, 6), (210, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 585.1477 - loglik: -5.7262e+02 - logprior: -9.5669e+00
Epoch 2/2
19/19 - 9s - loss: 568.0844 - loglik: -5.6372e+02 - logprior: -9.3856e-01
Fitted a model with MAP estimate = -559.9293
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 579.8972 - loglik: -5.6653e+02 - logprior: -9.3728e+00
Epoch 2/10
19/19 - 9s - loss: 565.0952 - loglik: -5.6032e+02 - logprior: -6.6413e-01
Epoch 3/10
19/19 - 9s - loss: 559.8563 - loglik: -5.5631e+02 - logprior: 0.1810
Epoch 4/10
19/19 - 9s - loss: 557.3322 - loglik: -5.5471e+02 - logprior: 0.6417
Epoch 5/10
19/19 - 9s - loss: 555.1362 - loglik: -5.5322e+02 - logprior: 0.9572
Epoch 6/10
19/19 - 9s - loss: 549.3063 - loglik: -5.4790e+02 - logprior: 1.1408
Epoch 7/10
19/19 - 9s - loss: 549.2861 - loglik: -5.4837e+02 - logprior: 1.3614
Epoch 8/10
19/19 - 9s - loss: 547.4016 - loglik: -5.4692e+02 - logprior: 1.5640
Epoch 9/10
19/19 - 9s - loss: 544.1877 - loglik: -5.4410e+02 - logprior: 1.7620
Epoch 10/10
19/19 - 9s - loss: 547.8951 - loglik: -5.4808e+02 - logprior: 1.9595
Fitted a model with MAP estimate = -542.3678
Time for alignment: 247.5828
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 687.1575 - loglik: -6.7700e+02 - logprior: -1.0043e+01
Epoch 2/10
19/19 - 8s - loss: 617.7971 - loglik: -6.1585e+02 - logprior: -1.1418e+00
Epoch 3/10
19/19 - 8s - loss: 587.3657 - loglik: -5.8436e+02 - logprior: -1.5049e+00
Epoch 4/10
19/19 - 8s - loss: 576.1595 - loglik: -5.7250e+02 - logprior: -1.5281e+00
Epoch 5/10
19/19 - 8s - loss: 572.8612 - loglik: -5.6936e+02 - logprior: -1.3787e+00
Epoch 6/10
19/19 - 8s - loss: 570.8892 - loglik: -5.6759e+02 - logprior: -1.5014e+00
Epoch 7/10
19/19 - 8s - loss: 565.0595 - loglik: -5.6170e+02 - logprior: -1.6914e+00
Epoch 8/10
19/19 - 8s - loss: 565.2022 - loglik: -5.6184e+02 - logprior: -1.7788e+00
Fitted a model with MAP estimate = -562.2177
expansions: [(25, 1), (27, 1), (28, 2), (44, 7), (64, 4), (90, 7), (91, 1), (102, 1), (118, 2), (119, 2), (133, 1), (134, 1), (149, 1), (153, 1), (154, 1), (166, 3), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [170 171 172 173 174 175 176 177]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 617.4216 - loglik: -6.0473e+02 - logprior: -1.0862e+01
Epoch 2/2
19/19 - 9s - loss: 577.5887 - loglik: -5.7336e+02 - logprior: -1.6108e+00
Fitted a model with MAP estimate = -566.8894
expansions: [(203, 7), (206, 1), (213, 1)]
discards: [ 30 144]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 238 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 584.0411 - loglik: -5.7105e+02 - logprior: -9.5644e+00
Epoch 2/2
19/19 - 9s - loss: 565.5316 - loglik: -5.6080e+02 - logprior: -9.5582e-01
Fitted a model with MAP estimate = -558.2406
expansions: [(44, 2), (77, 1)]
discards: [203 204 205 206 207 208 209]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 581.4670 - loglik: -5.6797e+02 - logprior: -9.2369e+00
Epoch 2/10
19/19 - 9s - loss: 566.2338 - loglik: -5.6136e+02 - logprior: -6.1086e-01
Epoch 3/10
19/19 - 9s - loss: 560.6938 - loglik: -5.5705e+02 - logprior: 0.1855
Epoch 4/10
19/19 - 9s - loss: 557.4211 - loglik: -5.5470e+02 - logprior: 0.6226
Epoch 5/10
19/19 - 9s - loss: 553.0121 - loglik: -5.5106e+02 - logprior: 0.9191
Epoch 6/10
19/19 - 9s - loss: 551.2031 - loglik: -5.4976e+02 - logprior: 1.0740
Epoch 7/10
19/19 - 9s - loss: 549.3049 - loglik: -5.4831e+02 - logprior: 1.2769
Epoch 8/10
19/19 - 9s - loss: 547.6819 - loglik: -5.4713e+02 - logprior: 1.4683
Epoch 9/10
19/19 - 9s - loss: 546.0641 - loglik: -5.4589e+02 - logprior: 1.7024
Epoch 10/10
19/19 - 9s - loss: 544.6421 - loglik: -5.4472e+02 - logprior: 1.9009
Fitted a model with MAP estimate = -542.5821
Time for alignment: 231.0281
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 687.4849 - loglik: -6.7730e+02 - logprior: -1.0068e+01
Epoch 2/10
19/19 - 8s - loss: 619.1249 - loglik: -6.1721e+02 - logprior: -1.1003e+00
Epoch 3/10
19/19 - 8s - loss: 586.6765 - loglik: -5.8349e+02 - logprior: -1.7210e+00
Epoch 4/10
19/19 - 8s - loss: 578.1124 - loglik: -5.7402e+02 - logprior: -1.9735e+00
Epoch 5/10
19/19 - 8s - loss: 573.0825 - loglik: -5.6918e+02 - logprior: -1.7475e+00
Epoch 6/10
19/19 - 8s - loss: 570.4250 - loglik: -5.6668e+02 - logprior: -1.8851e+00
Epoch 7/10
19/19 - 8s - loss: 566.5333 - loglik: -5.6268e+02 - logprior: -2.0660e+00
Epoch 8/10
19/19 - 8s - loss: 565.1326 - loglik: -5.6128e+02 - logprior: -2.2010e+00
Epoch 9/10
19/19 - 8s - loss: 561.9717 - loglik: -5.5829e+02 - logprior: -2.2415e+00
Epoch 10/10
19/19 - 8s - loss: 563.4487 - loglik: -5.5992e+02 - logprior: -2.2332e+00
Fitted a model with MAP estimate = -561.1597
expansions: [(18, 1), (25, 1), (27, 1), (28, 2), (30, 2), (43, 6), (64, 2), (65, 2), (90, 8), (91, 1), (102, 1), (117, 1), (119, 1), (131, 1), (133, 1), (134, 1), (135, 1), (151, 1), (153, 1), (154, 1), (162, 1), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [  2 170 171 172 173 174 175]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 622.9495 - loglik: -6.1051e+02 - logprior: -1.0929e+01
Epoch 2/2
19/19 - 9s - loss: 576.9894 - loglik: -5.7307e+02 - logprior: -1.7201e+00
Fitted a model with MAP estimate = -566.8912
expansions: [(202, 5), (203, 3), (215, 1)]
discards: [111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 583.1548 - loglik: -5.7067e+02 - logprior: -9.4735e+00
Epoch 2/2
19/19 - 9s - loss: 566.5213 - loglik: -5.6221e+02 - logprior: -8.0695e-01
Fitted a model with MAP estimate = -557.8164
expansions: [(35, 1)]
discards: [207 208 209 210 211 212 213 214 215 216]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 584.4734 - loglik: -5.7109e+02 - logprior: -9.2857e+00
Epoch 2/10
19/19 - 9s - loss: 569.9398 - loglik: -5.6509e+02 - logprior: -5.9910e-01
Epoch 3/10
19/19 - 9s - loss: 564.7696 - loglik: -5.6102e+02 - logprior: 0.1385
Epoch 4/10
19/19 - 9s - loss: 559.0276 - loglik: -5.5618e+02 - logprior: 0.5596
Epoch 5/10
19/19 - 9s - loss: 556.5510 - loglik: -5.5440e+02 - logprior: 0.8454
Epoch 6/10
19/19 - 9s - loss: 555.5270 - loglik: -5.5387e+02 - logprior: 1.0219
Epoch 7/10
19/19 - 9s - loss: 549.7417 - loglik: -5.4859e+02 - logprior: 1.2397
Epoch 8/10
19/19 - 9s - loss: 549.6074 - loglik: -5.4895e+02 - logprior: 1.4524
Epoch 9/10
19/19 - 9s - loss: 549.2607 - loglik: -5.4897e+02 - logprior: 1.6692
Epoch 10/10
19/19 - 9s - loss: 545.9025 - loglik: -5.4583e+02 - logprior: 1.8703
Fitted a model with MAP estimate = -544.2913
Time for alignment: 245.5758
Computed alignments with likelihoods: ['-542.3678', '-542.5821', '-544.2913']
Best model has likelihood: -542.3678
time for generating output: 0.3045
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.6947807933194154
Training of 3 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19948343d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19a5db52b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fe9418e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19c7a3c4c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1961f0dd60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1950a5a760>, <__main__.SimpleDirichletPrior object at 0x7f198c0014c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f17a0210700>

Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.4672 - loglik: -1.5120e+02 - logprior: -3.2604e+00
Epoch 2/10
19/19 - 1s - loss: 121.4482 - loglik: -1.1989e+02 - logprior: -1.5307e+00
Epoch 3/10
19/19 - 1s - loss: 106.8024 - loglik: -1.0490e+02 - logprior: -1.6302e+00
Epoch 4/10
19/19 - 1s - loss: 103.4946 - loglik: -1.0150e+02 - logprior: -1.7296e+00
Epoch 5/10
19/19 - 1s - loss: 102.7639 - loglik: -1.0096e+02 - logprior: -1.6054e+00
Epoch 6/10
19/19 - 1s - loss: 102.5407 - loglik: -1.0074e+02 - logprior: -1.6168e+00
Epoch 7/10
19/19 - 1s - loss: 102.0677 - loglik: -1.0030e+02 - logprior: -1.5868e+00
Epoch 8/10
19/19 - 1s - loss: 102.0025 - loglik: -1.0024e+02 - logprior: -1.5787e+00
Epoch 9/10
19/19 - 1s - loss: 101.8132 - loglik: -1.0005e+02 - logprior: -1.5730e+00
Epoch 10/10
19/19 - 1s - loss: 102.1208 - loglik: -1.0036e+02 - logprior: -1.5655e+00
Fitted a model with MAP estimate = -97.4744
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 1), (32, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 107.1256 - loglik: -1.0275e+02 - logprior: -4.2057e+00
Epoch 2/2
19/19 - 1s - loss: 96.9161 - loglik: -9.4372e+01 - logprior: -2.3916e+00
Fitted a model with MAP estimate = -90.9434
expansions: [(3, 1)]
discards: [ 0 20 38 45]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 99.8169 - loglik: -9.5715e+01 - logprior: -3.9548e+00
Epoch 2/2
19/19 - 1s - loss: 95.4030 - loglik: -9.3746e+01 - logprior: -1.4990e+00
Fitted a model with MAP estimate = -90.3613
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 94.3893 - loglik: -9.0548e+01 - logprior: -3.6913e+00
Epoch 2/10
21/21 - 2s - loss: 90.4555 - loglik: -8.8388e+01 - logprior: -1.9101e+00
Epoch 3/10
21/21 - 2s - loss: 89.3154 - loglik: -8.7658e+01 - logprior: -1.4167e+00
Epoch 4/10
21/21 - 2s - loss: 88.1753 - loglik: -8.6575e+01 - logprior: -1.3228e+00
Epoch 5/10
21/21 - 2s - loss: 88.3416 - loglik: -8.6754e+01 - logprior: -1.3004e+00
Fitted a model with MAP estimate = -87.5169
Time for alignment: 53.3432
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.3653 - loglik: -1.5110e+02 - logprior: -3.2586e+00
Epoch 2/10
19/19 - 1s - loss: 121.6161 - loglik: -1.2007e+02 - logprior: -1.5253e+00
Epoch 3/10
19/19 - 1s - loss: 106.8462 - loglik: -1.0494e+02 - logprior: -1.6071e+00
Epoch 4/10
19/19 - 1s - loss: 103.1366 - loglik: -1.0107e+02 - logprior: -1.7580e+00
Epoch 5/10
19/19 - 1s - loss: 102.0500 - loglik: -1.0020e+02 - logprior: -1.6362e+00
Epoch 6/10
19/19 - 1s - loss: 101.7839 - loglik: -9.9938e+01 - logprior: -1.6567e+00
Epoch 7/10
19/19 - 1s - loss: 101.0501 - loglik: -9.9232e+01 - logprior: -1.6392e+00
Epoch 8/10
19/19 - 1s - loss: 101.2717 - loglik: -9.9468e+01 - logprior: -1.6227e+00
Fitted a model with MAP estimate = -96.8630
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 1), (33, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.3454 - loglik: -1.0199e+02 - logprior: -4.2013e+00
Epoch 2/2
19/19 - 1s - loss: 96.4924 - loglik: -9.4061e+01 - logprior: -2.2904e+00
Fitted a model with MAP estimate = -90.5935
expansions: [(3, 1)]
discards: [ 0 20 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.8366 - loglik: -9.5689e+01 - logprior: -3.9971e+00
Epoch 2/2
19/19 - 1s - loss: 95.2820 - loglik: -9.3608e+01 - logprior: -1.5109e+00
Fitted a model with MAP estimate = -90.3275
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 94.3831 - loglik: -9.0539e+01 - logprior: -3.6974e+00
Epoch 2/10
21/21 - 2s - loss: 90.4343 - loglik: -8.8423e+01 - logprior: -1.8507e+00
Epoch 3/10
21/21 - 2s - loss: 88.8908 - loglik: -8.7241e+01 - logprior: -1.4135e+00
Epoch 4/10
21/21 - 2s - loss: 89.0217 - loglik: -8.7439e+01 - logprior: -1.3120e+00
Fitted a model with MAP estimate = -87.8453
Time for alignment: 48.3750
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.4206 - loglik: -1.5115e+02 - logprior: -3.2596e+00
Epoch 2/10
19/19 - 1s - loss: 119.9098 - loglik: -1.1835e+02 - logprior: -1.5371e+00
Epoch 3/10
19/19 - 1s - loss: 106.5803 - loglik: -1.0469e+02 - logprior: -1.6103e+00
Epoch 4/10
19/19 - 1s - loss: 103.4254 - loglik: -1.0147e+02 - logprior: -1.6720e+00
Epoch 5/10
19/19 - 1s - loss: 102.4126 - loglik: -1.0063e+02 - logprior: -1.5620e+00
Epoch 6/10
19/19 - 1s - loss: 102.0426 - loglik: -1.0027e+02 - logprior: -1.5733e+00
Epoch 7/10
19/19 - 1s - loss: 101.3684 - loglik: -9.9625e+01 - logprior: -1.5496e+00
Epoch 8/10
19/19 - 1s - loss: 101.8117 - loglik: -1.0007e+02 - logprior: -1.5381e+00
Fitted a model with MAP estimate = -97.1995
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.2659 - loglik: -1.0190e+02 - logprior: -4.2084e+00
Epoch 2/2
19/19 - 2s - loss: 96.7597 - loglik: -9.4268e+01 - logprior: -2.3500e+00
Fitted a model with MAP estimate = -90.8224
expansions: [(3, 1)]
discards: [ 0 20 36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.8590 - loglik: -9.5736e+01 - logprior: -3.9686e+00
Epoch 2/2
19/19 - 1s - loss: 95.3220 - loglik: -9.3652e+01 - logprior: -1.5043e+00
Fitted a model with MAP estimate = -90.3717
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 94.4159 - loglik: -9.0572e+01 - logprior: -3.6927e+00
Epoch 2/10
21/21 - 2s - loss: 90.8506 - loglik: -8.8791e+01 - logprior: -1.9001e+00
Epoch 3/10
21/21 - 1s - loss: 89.1595 - loglik: -8.7506e+01 - logprior: -1.4076e+00
Epoch 4/10
21/21 - 2s - loss: 88.2302 - loglik: -8.6630e+01 - logprior: -1.3275e+00
Epoch 5/10
21/21 - 2s - loss: 88.3263 - loglik: -8.6737e+01 - logprior: -1.3017e+00
Fitted a model with MAP estimate = -87.4854
Time for alignment: 48.3653
Computed alignments with likelihoods: ['-87.5169', '-87.8453', '-87.4854']
Best model has likelihood: -87.4854
time for generating output: 0.1223
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.9246619446233098
Training of 3 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19c79f7370>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18b3b80f10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fe275190>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fd76c7f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18788d8af0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f193d4880a0>, <__main__.SimpleDirichletPrior object at 0x7f1947f29a00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1994bb2f70>

Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 11s - loss: 417.6062 - loglik: -4.1271e+02 - logprior: -4.8430e+00
Epoch 2/10
27/27 - 5s - loss: 330.2871 - loglik: -3.2790e+02 - logprior: -2.1501e+00
Epoch 3/10
27/27 - 5s - loss: 317.8047 - loglik: -3.1524e+02 - logprior: -2.1239e+00
Epoch 4/10
27/27 - 5s - loss: 315.8176 - loglik: -3.1334e+02 - logprior: -2.0526e+00
Epoch 5/10
27/27 - 5s - loss: 314.5313 - loglik: -3.1203e+02 - logprior: -2.0566e+00
Epoch 6/10
27/27 - 5s - loss: 313.7310 - loglik: -3.1119e+02 - logprior: -2.0683e+00
Epoch 7/10
27/27 - 5s - loss: 313.2460 - loglik: -3.1069e+02 - logprior: -2.0852e+00
Epoch 8/10
27/27 - 5s - loss: 312.9289 - loglik: -3.1038e+02 - logprior: -2.0831e+00
Epoch 9/10
27/27 - 5s - loss: 312.9767 - loglik: -3.1044e+02 - logprior: -2.0716e+00
Fitted a model with MAP estimate = -312.2157
expansions: [(0, 2), (10, 2), (21, 1), (25, 2), (26, 1), (35, 1), (36, 1), (37, 4), (38, 1), (39, 1), (43, 1), (45, 1), (47, 1), (69, 2), (70, 1), (71, 2), (73, 1), (78, 1), (82, 2), (100, 2), (102, 1), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 159 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 309.2098 - loglik: -3.0189e+02 - logprior: -6.7564e+00
Epoch 2/2
27/27 - 6s - loss: 288.2436 - loglik: -2.8583e+02 - logprior: -1.7336e+00
Fitted a model with MAP estimate = -285.5193
expansions: [(130, 1)]
discards: [  0  30  47  48  89 109 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 298.1132 - loglik: -2.9081e+02 - logprior: -6.5704e+00
Epoch 2/2
27/27 - 6s - loss: 288.9487 - loglik: -2.8675e+02 - logprior: -1.5012e+00
Fitted a model with MAP estimate = -286.9447
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 293.0179 - loglik: -2.8784e+02 - logprior: -4.3561e+00
Epoch 2/10
27/27 - 6s - loss: 287.7143 - loglik: -2.8599e+02 - logprior: -9.9631e-01
Epoch 3/10
27/27 - 6s - loss: 286.0785 - loglik: -2.8469e+02 - logprior: -7.2006e-01
Epoch 4/10
27/27 - 6s - loss: 286.3442 - loglik: -2.8517e+02 - logprior: -5.7881e-01
Fitted a model with MAP estimate = -284.6945
Time for alignment: 137.0679
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 418.2227 - loglik: -4.1330e+02 - logprior: -4.8658e+00
Epoch 2/10
27/27 - 5s - loss: 334.8902 - loglik: -3.3235e+02 - logprior: -2.3121e+00
Epoch 3/10
27/27 - 5s - loss: 319.1491 - loglik: -3.1626e+02 - logprior: -2.4324e+00
Epoch 4/10
27/27 - 5s - loss: 316.0486 - loglik: -3.1327e+02 - logprior: -2.3472e+00
Epoch 5/10
27/27 - 5s - loss: 315.4464 - loglik: -3.1274e+02 - logprior: -2.2892e+00
Epoch 6/10
27/27 - 5s - loss: 314.9295 - loglik: -3.1222e+02 - logprior: -2.2985e+00
Epoch 7/10
27/27 - 5s - loss: 313.9967 - loglik: -3.1127e+02 - logprior: -2.3063e+00
Epoch 8/10
27/27 - 5s - loss: 314.0494 - loglik: -3.1133e+02 - logprior: -2.2901e+00
Fitted a model with MAP estimate = -313.2646
expansions: [(0, 2), (9, 1), (10, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (48, 1), (68, 1), (69, 3), (71, 1), (72, 1), (73, 1), (78, 1), (82, 2), (94, 2), (103, 1), (104, 2), (105, 2), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 162 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 11s - loss: 310.1072 - loglik: -3.0274e+02 - logprior: -6.8225e+00
Epoch 2/2
27/27 - 6s - loss: 290.7536 - loglik: -2.8828e+02 - logprior: -1.7987e+00
Fitted a model with MAP estimate = -288.2846
expansions: []
discards: [  0  30  48  49  50 110 137 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 10s - loss: 300.6297 - loglik: -2.9316e+02 - logprior: -6.6979e+00
Epoch 2/2
27/27 - 6s - loss: 292.1802 - loglik: -2.8988e+02 - logprior: -1.6089e+00
Fitted a model with MAP estimate = -289.9532
expansions: []
discards: [11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 297.1707 - loglik: -2.9185e+02 - logprior: -4.5253e+00
Epoch 2/10
27/27 - 5s - loss: 291.7705 - loglik: -2.8990e+02 - logprior: -1.1614e+00
Epoch 3/10
27/27 - 6s - loss: 290.8800 - loglik: -2.8929e+02 - logprior: -8.8855e-01
Epoch 4/10
27/27 - 6s - loss: 289.5056 - loglik: -2.8813e+02 - logprior: -7.4559e-01
Epoch 5/10
27/27 - 6s - loss: 289.4468 - loglik: -2.8820e+02 - logprior: -6.5715e-01
Epoch 6/10
27/27 - 6s - loss: 289.0840 - loglik: -2.8800e+02 - logprior: -5.2091e-01
Epoch 7/10
27/27 - 6s - loss: 288.3596 - loglik: -2.8739e+02 - logprior: -4.3291e-01
Epoch 8/10
27/27 - 6s - loss: 288.4361 - loglik: -2.8758e+02 - logprior: -3.2417e-01
Fitted a model with MAP estimate = -287.7227
Time for alignment: 154.1263
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 418.3531 - loglik: -4.1343e+02 - logprior: -4.8631e+00
Epoch 2/10
27/27 - 5s - loss: 334.5834 - loglik: -3.3215e+02 - logprior: -2.1702e+00
Epoch 3/10
27/27 - 5s - loss: 316.5959 - loglik: -3.1367e+02 - logprior: -2.3514e+00
Epoch 4/10
27/27 - 5s - loss: 311.2015 - loglik: -3.0833e+02 - logprior: -2.3712e+00
Epoch 5/10
27/27 - 5s - loss: 309.2985 - loglik: -3.0654e+02 - logprior: -2.2921e+00
Epoch 6/10
27/27 - 5s - loss: 309.1648 - loglik: -3.0643e+02 - logprior: -2.2686e+00
Epoch 7/10
27/27 - 5s - loss: 308.9830 - loglik: -3.0625e+02 - logprior: -2.2662e+00
Epoch 8/10
27/27 - 5s - loss: 308.6540 - loglik: -3.0591e+02 - logprior: -2.2672e+00
Epoch 9/10
27/27 - 5s - loss: 308.4637 - loglik: -3.0572e+02 - logprior: -2.2665e+00
Epoch 10/10
27/27 - 5s - loss: 308.7811 - loglik: -3.0603e+02 - logprior: -2.2620e+00
Fitted a model with MAP estimate = -307.6761
expansions: [(0, 2), (19, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 3), (38, 1), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (68, 1), (69, 2), (71, 2), (73, 1), (78, 1), (79, 1), (81, 2), (93, 2), (102, 1), (103, 2), (104, 2), (105, 1), (115, 1), (116, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 305.2730 - loglik: -2.9793e+02 - logprior: -6.7470e+00
Epoch 2/2
27/27 - 6s - loss: 287.0066 - loglik: -2.8460e+02 - logprior: -1.6712e+00
Fitted a model with MAP estimate = -283.8898
expansions: []
discards: [  0  29  46  47 108 135 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 9s - loss: 296.1790 - loglik: -2.8879e+02 - logprior: -6.5882e+00
Epoch 2/2
27/27 - 6s - loss: 288.5550 - loglik: -2.8637e+02 - logprior: -1.4871e+00
Fitted a model with MAP estimate = -285.9697
expansions: []
discards: [43]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 9s - loss: 292.6703 - loglik: -2.8753e+02 - logprior: -4.3587e+00
Epoch 2/10
27/27 - 6s - loss: 287.4603 - loglik: -2.8574e+02 - logprior: -1.0118e+00
Epoch 3/10
27/27 - 6s - loss: 286.1808 - loglik: -2.8480e+02 - logprior: -7.1115e-01
Epoch 4/10
27/27 - 6s - loss: 285.7087 - loglik: -2.8448e+02 - logprior: -6.1538e-01
Epoch 5/10
27/27 - 6s - loss: 284.7354 - loglik: -2.8369e+02 - logprior: -4.7047e-01
Epoch 6/10
27/27 - 6s - loss: 284.8808 - loglik: -2.8398e+02 - logprior: -3.5821e-01
Fitted a model with MAP estimate = -283.6790
Time for alignment: 152.1592
Computed alignments with likelihoods: ['-284.6945', '-287.7227', '-283.6790']
Best model has likelihood: -283.6790
time for generating output: 0.3422
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.5465116279069767
Training of 3 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1947f1c940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f196a3b26a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1994d32dc0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19bf65e0d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18b435ffd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18b438ed30>, <__main__.SimpleDirichletPrior object at 0x7f18a259b5e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1994bb2f70>

Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 21s - loss: 765.7438 - loglik: -7.6315e+02 - logprior: -2.3341e+00
Epoch 2/10
34/34 - 16s - loss: 641.0224 - loglik: -6.3783e+02 - logprior: -2.1157e+00
Epoch 3/10
34/34 - 16s - loss: 622.3881 - loglik: -6.1883e+02 - logprior: -2.3024e+00
Epoch 4/10
34/34 - 17s - loss: 617.5120 - loglik: -6.1434e+02 - logprior: -2.2970e+00
Epoch 5/10
34/34 - 16s - loss: 615.1958 - loglik: -6.1211e+02 - logprior: -2.3315e+00
Epoch 6/10
34/34 - 17s - loss: 614.3668 - loglik: -6.1126e+02 - logprior: -2.3617e+00
Epoch 7/10
34/34 - 16s - loss: 614.1564 - loglik: -6.1108e+02 - logprior: -2.3760e+00
Epoch 8/10
34/34 - 16s - loss: 613.2241 - loglik: -6.1019e+02 - logprior: -2.3647e+00
Epoch 9/10
34/34 - 16s - loss: 614.0401 - loglik: -6.1104e+02 - logprior: -2.3592e+00
Fitted a model with MAP estimate = -612.2901
expansions: [(9, 1), (12, 1), (13, 1), (15, 3), (16, 1), (17, 4), (22, 1), (28, 2), (49, 2), (51, 2), (52, 1), (56, 2), (65, 1), (66, 1), (67, 1), (91, 1), (93, 1), (94, 1), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (110, 1), (130, 1), (136, 1), (138, 1), (141, 1), (143, 2), (144, 1), (147, 1), (165, 1), (167, 1), (171, 2), (172, 1), (175, 1), (184, 2), (185, 1), (186, 2), (189, 1), (190, 1), (199, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 26s - loss: 601.0807 - loglik: -5.9634e+02 - logprior: -3.8598e+00
Epoch 2/2
34/34 - 22s - loss: 579.6875 - loglik: -5.7641e+02 - logprior: -2.0410e+00
Fitted a model with MAP estimate = -573.5468
expansions: [(39, 1), (41, 1), (84, 1), (183, 1), (233, 1)]
discards: [19 20 24 62 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 581.9665 - loglik: -5.7778e+02 - logprior: -2.5768e+00
Epoch 2/2
34/34 - 22s - loss: 575.1945 - loglik: -5.7284e+02 - logprior: -8.1204e-01
Fitted a model with MAP estimate = -570.9310
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 298 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 27s - loss: 579.1732 - loglik: -5.7512e+02 - logprior: -2.2796e+00
Epoch 2/10
34/34 - 22s - loss: 572.1446 - loglik: -5.7002e+02 - logprior: -5.1116e-01
Epoch 3/10
34/34 - 22s - loss: 571.4417 - loglik: -5.6978e+02 - logprior: -3.1222e-01
Epoch 4/10
34/34 - 23s - loss: 570.7736 - loglik: -5.6942e+02 - logprior: -2.3342e-01
Epoch 5/10
34/34 - 22s - loss: 569.9901 - loglik: -5.6894e+02 - logprior: -4.6623e-02
Epoch 6/10
34/34 - 22s - loss: 569.3804 - loglik: -5.6862e+02 - logprior: 0.1404
Epoch 7/10
34/34 - 22s - loss: 568.3315 - loglik: -5.6783e+02 - logprior: 0.3466
Epoch 8/10
34/34 - 22s - loss: 568.4343 - loglik: -5.6812e+02 - logprior: 0.4927
Fitted a model with MAP estimate = -567.3572
Time for alignment: 535.9624
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 20s - loss: 766.5966 - loglik: -7.6401e+02 - logprior: -2.3224e+00
Epoch 2/10
34/34 - 16s - loss: 635.8381 - loglik: -6.3296e+02 - logprior: -2.1180e+00
Epoch 3/10
34/34 - 17s - loss: 618.5902 - loglik: -6.1517e+02 - logprior: -2.3527e+00
Epoch 4/10
34/34 - 16s - loss: 612.7842 - loglik: -6.0956e+02 - logprior: -2.2987e+00
Epoch 5/10
34/34 - 17s - loss: 612.8861 - loglik: -6.0985e+02 - logprior: -2.2680e+00
Fitted a model with MAP estimate = -610.7498
expansions: [(9, 1), (12, 1), (14, 1), (15, 2), (16, 1), (18, 2), (27, 1), (29, 3), (52, 1), (54, 1), (64, 1), (66, 1), (67, 1), (91, 1), (93, 1), (94, 2), (95, 2), (98, 1), (100, 1), (101, 1), (102, 1), (104, 1), (109, 1), (110, 1), (130, 1), (136, 1), (138, 1), (141, 1), (144, 1), (165, 1), (168, 1), (171, 2), (172, 1), (178, 1), (179, 1), (188, 1), (193, 1), (202, 2), (203, 1), (208, 1), (211, 1), (224, 1), (225, 1), (228, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 287 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 601.4143 - loglik: -5.9652e+02 - logprior: -3.8509e+00
Epoch 2/2
34/34 - 21s - loss: 583.8309 - loglik: -5.8078e+02 - logprior: -1.7518e+00
Fitted a model with MAP estimate = -578.6591
expansions: [(57, 2), (242, 1)]
discards: [ 39 246]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 586.8549 - loglik: -5.8279e+02 - logprior: -2.4974e+00
Epoch 2/2
34/34 - 21s - loss: 580.3638 - loglik: -5.7810e+02 - logprior: -8.2202e-01
Fitted a model with MAP estimate = -577.7142
expansions: [(247, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 289 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 26s - loss: 584.1411 - loglik: -5.8014e+02 - logprior: -2.3487e+00
Epoch 2/10
34/34 - 22s - loss: 579.7379 - loglik: -5.7774e+02 - logprior: -5.0195e-01
Epoch 3/10
34/34 - 22s - loss: 577.1786 - loglik: -5.7553e+02 - logprior: -3.8158e-01
Epoch 4/10
34/34 - 21s - loss: 576.5433 - loglik: -5.7533e+02 - logprior: -1.3801e-01
Epoch 5/10
34/34 - 21s - loss: 575.5818 - loglik: -5.7459e+02 - logprior: -3.4279e-02
Epoch 6/10
34/34 - 22s - loss: 574.2197 - loglik: -5.7351e+02 - logprior: 0.1679
Epoch 7/10
34/34 - 21s - loss: 575.2073 - loglik: -5.7472e+02 - logprior: 0.3222
Fitted a model with MAP estimate = -573.1731
Time for alignment: 433.2404
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 20s - loss: 764.4754 - loglik: -7.6191e+02 - logprior: -2.3154e+00
Epoch 2/10
34/34 - 16s - loss: 638.9163 - loglik: -6.3638e+02 - logprior: -1.8835e+00
Epoch 3/10
34/34 - 17s - loss: 621.4788 - loglik: -6.1819e+02 - logprior: -2.1166e+00
Epoch 4/10
34/34 - 16s - loss: 617.5542 - loglik: -6.1447e+02 - logprior: -2.1407e+00
Epoch 5/10
34/34 - 16s - loss: 614.8322 - loglik: -6.1184e+02 - logprior: -2.1391e+00
Epoch 6/10
34/34 - 17s - loss: 614.2296 - loglik: -6.1123e+02 - logprior: -2.1925e+00
Epoch 7/10
34/34 - 16s - loss: 614.2639 - loglik: -6.1131e+02 - logprior: -2.1977e+00
Fitted a model with MAP estimate = -612.2881
expansions: [(13, 1), (14, 1), (15, 1), (16, 1), (21, 2), (22, 1), (29, 3), (51, 1), (52, 1), (55, 5), (57, 2), (64, 2), (66, 1), (92, 1), (94, 2), (95, 2), (96, 2), (99, 1), (101, 1), (102, 1), (103, 1), (105, 1), (110, 1), (130, 1), (135, 1), (137, 1), (139, 1), (142, 1), (143, 1), (144, 2), (145, 1), (148, 1), (165, 1), (166, 1), (167, 1), (171, 1), (172, 1), (175, 1), (178, 1), (188, 1), (190, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 602.9283 - loglik: -5.9796e+02 - logprior: -3.8892e+00
Epoch 2/2
34/34 - 22s - loss: 582.4384 - loglik: -5.7898e+02 - logprior: -2.1021e+00
Fitted a model with MAP estimate = -576.6138
expansions: [(37, 1), (69, 1)]
discards: [117 119 122 188]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 292 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 25s - loss: 585.1434 - loglik: -5.8094e+02 - logprior: -2.5794e+00
Epoch 2/2
34/34 - 22s - loss: 577.5989 - loglik: -5.7522e+02 - logprior: -8.8584e-01
Fitted a model with MAP estimate = -575.0507
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 292 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 25s - loss: 582.4490 - loglik: -5.7841e+02 - logprior: -2.3591e+00
Epoch 2/10
34/34 - 22s - loss: 576.9860 - loglik: -5.7491e+02 - logprior: -5.6129e-01
Epoch 3/10
34/34 - 22s - loss: 575.8623 - loglik: -5.7422e+02 - logprior: -3.6786e-01
Epoch 4/10
34/34 - 21s - loss: 573.1611 - loglik: -5.7192e+02 - logprior: -1.6630e-01
Epoch 5/10
34/34 - 22s - loss: 574.4376 - loglik: -5.7349e+02 - logprior: 9.8459e-04
Fitted a model with MAP estimate = -571.7199
Time for alignment: 428.4812
Computed alignments with likelihoods: ['-567.3572', '-573.1731', '-571.7199']
Best model has likelihood: -567.3572
time for generating output: 0.4420
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.5624691358024692
Training of 3 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f18a2469ca0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193d7612e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19bf473640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f194731c5e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f199d6fea30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fd425d60>, <__main__.SimpleDirichletPrior object at 0x7f19bede3e50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1994bb2f70>

Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.9233 - loglik: -1.8302e+02 - logprior: -8.8881e+01
Epoch 2/10
10/10 - 1s - loss: 184.4494 - loglik: -1.6007e+02 - logprior: -2.4363e+01
Epoch 3/10
10/10 - 1s - loss: 153.7512 - loglik: -1.4189e+02 - logprior: -1.1846e+01
Epoch 4/10
10/10 - 1s - loss: 139.7705 - loglik: -1.3251e+02 - logprior: -7.2404e+00
Epoch 5/10
10/10 - 1s - loss: 132.1836 - loglik: -1.2722e+02 - logprior: -4.9431e+00
Epoch 6/10
10/10 - 1s - loss: 128.1549 - loglik: -1.2423e+02 - logprior: -3.7674e+00
Epoch 7/10
10/10 - 1s - loss: 126.5232 - loglik: -1.2328e+02 - logprior: -2.8596e+00
Epoch 8/10
10/10 - 1s - loss: 125.7156 - loglik: -1.2297e+02 - logprior: -2.3292e+00
Epoch 9/10
10/10 - 1s - loss: 125.1694 - loglik: -1.2281e+02 - logprior: -2.0038e+00
Epoch 10/10
10/10 - 1s - loss: 124.7094 - loglik: -1.2258e+02 - logprior: -1.7892e+00
Fitted a model with MAP estimate = -124.1659
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 222.4446 - loglik: -1.2250e+02 - logprior: -9.9647e+01
Epoch 2/2
10/10 - 1s - loss: 157.2163 - loglik: -1.1571e+02 - logprior: -4.1274e+01
Fitted a model with MAP estimate = -145.3262
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.1367 - loglik: -1.1222e+02 - logprior: -7.9715e+01
Epoch 2/2
10/10 - 1s - loss: 131.3573 - loglik: -1.0951e+02 - logprior: -2.1678e+01
Fitted a model with MAP estimate = -121.9191
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 187.7054 - loglik: -1.0927e+02 - logprior: -7.8272e+01
Epoch 2/10
10/10 - 1s - loss: 130.0444 - loglik: -1.0879e+02 - logprior: -2.1082e+01
Epoch 3/10
10/10 - 1s - loss: 117.9456 - loglik: -1.0827e+02 - logprior: -9.3555e+00
Epoch 4/10
10/10 - 1s - loss: 113.1105 - loglik: -1.0837e+02 - logprior: -4.3584e+00
Epoch 5/10
10/10 - 1s - loss: 110.6276 - loglik: -1.0863e+02 - logprior: -1.6189e+00
Epoch 6/10
10/10 - 1s - loss: 109.2711 - loglik: -1.0887e+02 - logprior: -3.0913e-03
Epoch 7/10
10/10 - 1s - loss: 108.4736 - loglik: -1.0903e+02 - logprior: 0.9661
Epoch 8/10
10/10 - 1s - loss: 107.9476 - loglik: -1.0914e+02 - logprior: 1.6181
Epoch 9/10
10/10 - 1s - loss: 107.5444 - loglik: -1.0922e+02 - logprior: 2.1060
Epoch 10/10
10/10 - 1s - loss: 107.2064 - loglik: -1.0923e+02 - logprior: 2.4711
Fitted a model with MAP estimate = -106.5881
Time for alignment: 35.7272
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.9233 - loglik: -1.8302e+02 - logprior: -8.8881e+01
Epoch 2/10
10/10 - 1s - loss: 184.4494 - loglik: -1.6007e+02 - logprior: -2.4363e+01
Epoch 3/10
10/10 - 1s - loss: 153.7512 - loglik: -1.4189e+02 - logprior: -1.1846e+01
Epoch 4/10
10/10 - 1s - loss: 139.7706 - loglik: -1.3251e+02 - logprior: -7.2404e+00
Epoch 5/10
10/10 - 1s - loss: 132.2020 - loglik: -1.2725e+02 - logprior: -4.9376e+00
Epoch 6/10
10/10 - 1s - loss: 128.1697 - loglik: -1.2426e+02 - logprior: -3.7638e+00
Epoch 7/10
10/10 - 1s - loss: 126.5239 - loglik: -1.2329e+02 - logprior: -2.8586e+00
Epoch 8/10
10/10 - 1s - loss: 125.7131 - loglik: -1.2297e+02 - logprior: -2.3307e+00
Epoch 9/10
10/10 - 1s - loss: 125.1462 - loglik: -1.2278e+02 - logprior: -2.0107e+00
Epoch 10/10
10/10 - 1s - loss: 124.6857 - loglik: -1.2255e+02 - logprior: -1.7920e+00
Fitted a model with MAP estimate = -124.1540
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.4528 - loglik: -1.2251e+02 - logprior: -9.9645e+01
Epoch 2/2
10/10 - 1s - loss: 157.2210 - loglik: -1.1571e+02 - logprior: -4.1271e+01
Fitted a model with MAP estimate = -145.3234
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.1390 - loglik: -1.1222e+02 - logprior: -7.9715e+01
Epoch 2/2
10/10 - 1s - loss: 131.3624 - loglik: -1.0952e+02 - logprior: -2.1678e+01
Fitted a model with MAP estimate = -121.9252
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 187.7024 - loglik: -1.0927e+02 - logprior: -7.8273e+01
Epoch 2/10
10/10 - 1s - loss: 130.0450 - loglik: -1.0880e+02 - logprior: -2.1082e+01
Epoch 3/10
10/10 - 1s - loss: 117.9453 - loglik: -1.0827e+02 - logprior: -9.3556e+00
Epoch 4/10
10/10 - 1s - loss: 113.1094 - loglik: -1.0837e+02 - logprior: -4.3600e+00
Epoch 5/10
10/10 - 1s - loss: 110.6254 - loglik: -1.0862e+02 - logprior: -1.6190e+00
Epoch 6/10
10/10 - 1s - loss: 109.2678 - loglik: -1.0887e+02 - logprior: -3.1357e-03
Epoch 7/10
10/10 - 1s - loss: 108.4732 - loglik: -1.0903e+02 - logprior: 0.9650
Epoch 8/10
10/10 - 1s - loss: 107.9477 - loglik: -1.0914e+02 - logprior: 1.6169
Epoch 9/10
10/10 - 1s - loss: 107.5443 - loglik: -1.0922e+02 - logprior: 2.1047
Epoch 10/10
10/10 - 1s - loss: 107.2072 - loglik: -1.0923e+02 - logprior: 2.4698
Fitted a model with MAP estimate = -106.5892
Time for alignment: 34.9578
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 271.9233 - loglik: -1.8302e+02 - logprior: -8.8881e+01
Epoch 2/10
10/10 - 1s - loss: 184.4494 - loglik: -1.6007e+02 - logprior: -2.4363e+01
Epoch 3/10
10/10 - 1s - loss: 153.7513 - loglik: -1.4189e+02 - logprior: -1.1846e+01
Epoch 4/10
10/10 - 1s - loss: 139.7710 - loglik: -1.3251e+02 - logprior: -7.2404e+00
Epoch 5/10
10/10 - 1s - loss: 132.2684 - loglik: -1.2734e+02 - logprior: -4.9134e+00
Epoch 6/10
10/10 - 1s - loss: 128.1600 - loglik: -1.2434e+02 - logprior: -3.7371e+00
Epoch 7/10
10/10 - 1s - loss: 126.3589 - loglik: -1.2316e+02 - logprior: -2.8890e+00
Epoch 8/10
10/10 - 1s - loss: 125.5346 - loglik: -1.2278e+02 - logprior: -2.3567e+00
Epoch 9/10
10/10 - 1s - loss: 125.0022 - loglik: -1.2262e+02 - logprior: -2.0252e+00
Epoch 10/10
10/10 - 1s - loss: 124.6403 - loglik: -1.2251e+02 - logprior: -1.8030e+00
Fitted a model with MAP estimate = -124.1668
expansions: [(7, 2), (8, 4), (16, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.4901 - loglik: -1.2255e+02 - logprior: -9.9647e+01
Epoch 2/2
10/10 - 1s - loss: 157.2300 - loglik: -1.1574e+02 - logprior: -4.1259e+01
Fitted a model with MAP estimate = -145.3102
expansions: [(0, 1), (48, 1)]
discards: [ 0 30 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 192.1386 - loglik: -1.1222e+02 - logprior: -7.9712e+01
Epoch 2/2
10/10 - 1s - loss: 131.3670 - loglik: -1.0953e+02 - logprior: -2.1677e+01
Fitted a model with MAP estimate = -121.9347
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 187.7009 - loglik: -1.0927e+02 - logprior: -7.8275e+01
Epoch 2/10
10/10 - 1s - loss: 130.0488 - loglik: -1.0880e+02 - logprior: -2.1079e+01
Epoch 3/10
10/10 - 1s - loss: 117.9508 - loglik: -1.0828e+02 - logprior: -9.3538e+00
Epoch 4/10
10/10 - 1s - loss: 113.1163 - loglik: -1.0838e+02 - logprior: -4.3564e+00
Epoch 5/10
10/10 - 1s - loss: 110.6320 - loglik: -1.0864e+02 - logprior: -1.6151e+00
Epoch 6/10
10/10 - 1s - loss: 109.2748 - loglik: -1.0888e+02 - logprior: 0.0023
Epoch 7/10
10/10 - 1s - loss: 108.4767 - loglik: -1.0904e+02 - logprior: 0.9745
Epoch 8/10
10/10 - 1s - loss: 107.9497 - loglik: -1.0916e+02 - logprior: 1.6283
Epoch 9/10
10/10 - 1s - loss: 107.5448 - loglik: -1.0923e+02 - logprior: 2.1165
Epoch 10/10
10/10 - 1s - loss: 107.2082 - loglik: -1.0924e+02 - logprior: 2.4846
Fitted a model with MAP estimate = -106.5902
Time for alignment: 33.2333
Computed alignments with likelihoods: ['-106.5881', '-106.5892', '-106.5902']
Best model has likelihood: -106.5881
time for generating output: 0.1335
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.9127706021466254
Training of 3 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1950f95100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19ae06feb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198c3d72b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18b3e58310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1972c0b130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19bf626070>, <__main__.SimpleDirichletPrior object at 0x7f1947595190>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19a5d59c10>

Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.2845 - loglik: -3.0843e+02 - logprior: -7.8120e+00
Epoch 2/10
13/13 - 2s - loss: 285.5707 - loglik: -2.8343e+02 - logprior: -1.9845e+00
Epoch 3/10
13/13 - 2s - loss: 261.8170 - loglik: -2.5980e+02 - logprior: -1.8014e+00
Epoch 4/10
13/13 - 2s - loss: 251.0813 - loglik: -2.4849e+02 - logprior: -2.1620e+00
Epoch 5/10
13/13 - 2s - loss: 247.4541 - loglik: -2.4469e+02 - logprior: -2.1108e+00
Epoch 6/10
13/13 - 2s - loss: 247.4142 - loglik: -2.4479e+02 - logprior: -2.0164e+00
Epoch 7/10
13/13 - 2s - loss: 246.0072 - loglik: -2.4339e+02 - logprior: -2.0160e+00
Epoch 8/10
13/13 - 2s - loss: 245.8749 - loglik: -2.4325e+02 - logprior: -2.0168e+00
Epoch 9/10
13/13 - 2s - loss: 245.3379 - loglik: -2.4275e+02 - logprior: -2.0062e+00
Epoch 10/10
13/13 - 2s - loss: 244.6978 - loglik: -2.4216e+02 - logprior: -1.9917e+00
Fitted a model with MAP estimate = -244.1502
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (21, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 1), (32, 1), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 4), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 261.1370 - loglik: -2.5123e+02 - logprior: -9.3195e+00
Epoch 2/2
13/13 - 2s - loss: 247.4429 - loglik: -2.4232e+02 - logprior: -4.3260e+00
Fitted a model with MAP estimate = -244.4483
expansions: [(0, 2)]
discards: [ 0 14 34 67 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 249.4737 - loglik: -2.4138e+02 - logprior: -7.0946e+00
Epoch 2/2
13/13 - 2s - loss: 242.7639 - loglik: -2.3969e+02 - logprior: -2.1028e+00
Fitted a model with MAP estimate = -240.4378
expansions: []
discards: [ 0 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 251.8818 - loglik: -2.4204e+02 - logprior: -8.8085e+00
Epoch 2/10
13/13 - 2s - loss: 243.8990 - loglik: -2.4004e+02 - logprior: -2.8934e+00
Epoch 3/10
13/13 - 2s - loss: 241.6494 - loglik: -2.3924e+02 - logprior: -1.5386e+00
Epoch 4/10
13/13 - 2s - loss: 240.8217 - loglik: -2.3867e+02 - logprior: -1.2842e+00
Epoch 5/10
13/13 - 2s - loss: 239.8250 - loglik: -2.3781e+02 - logprior: -1.1619e+00
Epoch 6/10
13/13 - 2s - loss: 239.2418 - loglik: -2.3728e+02 - logprior: -1.1337e+00
Epoch 7/10
13/13 - 2s - loss: 238.5302 - loglik: -2.3660e+02 - logprior: -1.1191e+00
Epoch 8/10
13/13 - 2s - loss: 237.9446 - loglik: -2.3604e+02 - logprior: -1.1050e+00
Epoch 9/10
13/13 - 2s - loss: 237.1936 - loglik: -2.3528e+02 - logprior: -1.0948e+00
Epoch 10/10
13/13 - 2s - loss: 236.5108 - loglik: -2.3459e+02 - logprior: -1.0811e+00
Fitted a model with MAP estimate = -235.4128
Time for alignment: 70.2592
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.8431 - loglik: -3.0899e+02 - logprior: -7.8127e+00
Epoch 2/10
13/13 - 2s - loss: 284.3842 - loglik: -2.8225e+02 - logprior: -1.9838e+00
Epoch 3/10
13/13 - 2s - loss: 260.8032 - loglik: -2.5882e+02 - logprior: -1.7813e+00
Epoch 4/10
13/13 - 2s - loss: 250.8401 - loglik: -2.4838e+02 - logprior: -2.0783e+00
Epoch 5/10
13/13 - 2s - loss: 248.4887 - loglik: -2.4588e+02 - logprior: -2.0166e+00
Epoch 6/10
13/13 - 2s - loss: 247.1456 - loglik: -2.4466e+02 - logprior: -1.9114e+00
Epoch 7/10
13/13 - 2s - loss: 246.6793 - loglik: -2.4419e+02 - logprior: -1.9157e+00
Epoch 8/10
13/13 - 2s - loss: 246.0766 - loglik: -2.4358e+02 - logprior: -1.9180e+00
Epoch 9/10
13/13 - 2s - loss: 245.6419 - loglik: -2.4317e+02 - logprior: -1.9013e+00
Epoch 10/10
13/13 - 2s - loss: 245.4961 - loglik: -2.4306e+02 - logprior: -1.8956e+00
Fitted a model with MAP estimate = -244.5684
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 3), (29, 2), (30, 2), (31, 2), (32, 1), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 261.4524 - loglik: -2.5156e+02 - logprior: -9.2911e+00
Epoch 2/2
13/13 - 2s - loss: 247.6233 - loglik: -2.4250e+02 - logprior: -4.3300e+00
Fitted a model with MAP estimate = -244.4590
expansions: [(0, 2)]
discards: [ 0 14 68 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 248.9517 - loglik: -2.4087e+02 - logprior: -7.0784e+00
Epoch 2/2
13/13 - 2s - loss: 242.7912 - loglik: -2.3976e+02 - logprior: -2.0864e+00
Fitted a model with MAP estimate = -240.2875
expansions: []
discards: [ 0 36 37 40 92]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 251.5258 - loglik: -2.4169e+02 - logprior: -8.8301e+00
Epoch 2/10
13/13 - 2s - loss: 244.3939 - loglik: -2.4053e+02 - logprior: -2.9191e+00
Epoch 3/10
13/13 - 2s - loss: 242.0984 - loglik: -2.3966e+02 - logprior: -1.5572e+00
Epoch 4/10
13/13 - 2s - loss: 240.4430 - loglik: -2.3828e+02 - logprior: -1.2921e+00
Epoch 5/10
13/13 - 2s - loss: 240.3255 - loglik: -2.3831e+02 - logprior: -1.1618e+00
Epoch 6/10
13/13 - 2s - loss: 239.0920 - loglik: -2.3710e+02 - logprior: -1.1465e+00
Epoch 7/10
13/13 - 2s - loss: 237.8305 - loglik: -2.3589e+02 - logprior: -1.1157e+00
Epoch 8/10
13/13 - 2s - loss: 238.2790 - loglik: -2.3635e+02 - logprior: -1.1080e+00
Fitted a model with MAP estimate = -236.5179
Time for alignment: 67.1033
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.7121 - loglik: -3.0886e+02 - logprior: -7.8139e+00
Epoch 2/10
13/13 - 2s - loss: 283.8727 - loglik: -2.8174e+02 - logprior: -1.9924e+00
Epoch 3/10
13/13 - 2s - loss: 259.0813 - loglik: -2.5708e+02 - logprior: -1.8195e+00
Epoch 4/10
13/13 - 2s - loss: 249.9955 - loglik: -2.4744e+02 - logprior: -2.1202e+00
Epoch 5/10
13/13 - 2s - loss: 247.9130 - loglik: -2.4528e+02 - logprior: -2.0105e+00
Epoch 6/10
13/13 - 2s - loss: 246.9135 - loglik: -2.4440e+02 - logprior: -1.9166e+00
Epoch 7/10
13/13 - 2s - loss: 245.9518 - loglik: -2.4339e+02 - logprior: -1.9522e+00
Epoch 8/10
13/13 - 2s - loss: 245.2337 - loglik: -2.4266e+02 - logprior: -1.9648e+00
Epoch 9/10
13/13 - 2s - loss: 245.4058 - loglik: -2.4288e+02 - logprior: -1.9517e+00
Fitted a model with MAP estimate = -244.2153
expansions: [(9, 1), (10, 1), (13, 1), (26, 1), (28, 1), (29, 1), (30, 4), (31, 2), (32, 2), (40, 1), (43, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 114 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 260.1423 - loglik: -2.5026e+02 - logprior: -9.2775e+00
Epoch 2/2
13/13 - 2s - loss: 246.9632 - loglik: -2.4189e+02 - logprior: -4.2742e+00
Fitted a model with MAP estimate = -244.1149
expansions: [(0, 2)]
discards: [ 0 43 67 93 96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 248.8801 - loglik: -2.4083e+02 - logprior: -7.0675e+00
Epoch 2/2
13/13 - 2s - loss: 242.1962 - loglik: -2.3919e+02 - logprior: -2.0854e+00
Fitted a model with MAP estimate = -240.2766
expansions: []
discards: [ 0 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 251.6518 - loglik: -2.4184e+02 - logprior: -8.8252e+00
Epoch 2/10
13/13 - 2s - loss: 243.9951 - loglik: -2.4014e+02 - logprior: -2.9154e+00
Epoch 3/10
13/13 - 2s - loss: 241.5113 - loglik: -2.3908e+02 - logprior: -1.5576e+00
Epoch 4/10
13/13 - 2s - loss: 240.9048 - loglik: -2.3876e+02 - logprior: -1.2883e+00
Epoch 5/10
13/13 - 2s - loss: 239.9346 - loglik: -2.3793e+02 - logprior: -1.1592e+00
Epoch 6/10
13/13 - 2s - loss: 238.8165 - loglik: -2.3685e+02 - logprior: -1.1421e+00
Epoch 7/10
13/13 - 2s - loss: 238.7861 - loglik: -2.3686e+02 - logprior: -1.1218e+00
Epoch 8/10
13/13 - 2s - loss: 237.8053 - loglik: -2.3591e+02 - logprior: -1.1024e+00
Epoch 9/10
13/13 - 2s - loss: 237.3731 - loglik: -2.3548e+02 - logprior: -1.0956e+00
Epoch 10/10
13/13 - 2s - loss: 236.4599 - loglik: -2.3456e+02 - logprior: -1.0784e+00
Fitted a model with MAP estimate = -235.4780
Time for alignment: 68.4203
Computed alignments with likelihoods: ['-235.4128', '-236.5179', '-235.4780']
Best model has likelihood: -235.4128
time for generating output: 0.2113
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.9351145038167938
Training of 3 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f193e30c910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1959753850>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19a5960d90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f196a3cea60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f196a3ced30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1961c9ff40>, <__main__.SimpleDirichletPrior object at 0x7f19478613d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe851ca0>

Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 433.8088 - loglik: -3.4692e+02 - logprior: -8.6875e+01
Epoch 2/10
10/10 - 1s - loss: 329.3674 - loglik: -3.0824e+02 - logprior: -2.1123e+01
Epoch 3/10
10/10 - 1s - loss: 278.4341 - loglik: -2.6912e+02 - logprior: -9.2992e+00
Epoch 4/10
10/10 - 1s - loss: 247.1387 - loglik: -2.4135e+02 - logprior: -5.7856e+00
Epoch 5/10
10/10 - 1s - loss: 232.4520 - loglik: -2.2844e+02 - logprior: -3.9590e+00
Epoch 6/10
10/10 - 1s - loss: 225.9694 - loglik: -2.2280e+02 - logprior: -2.8861e+00
Epoch 7/10
10/10 - 1s - loss: 222.7687 - loglik: -2.2030e+02 - logprior: -2.0351e+00
Epoch 8/10
10/10 - 1s - loss: 221.1785 - loglik: -2.1938e+02 - logprior: -1.4039e+00
Epoch 9/10
10/10 - 1s - loss: 220.2854 - loglik: -2.1899e+02 - logprior: -9.6055e-01
Epoch 10/10
10/10 - 1s - loss: 219.7166 - loglik: -2.1878e+02 - logprior: -5.9263e-01
Fitted a model with MAP estimate = -219.1144
expansions: [(13, 3), (14, 1), (24, 1), (29, 1), (30, 1), (40, 1), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 314.4930 - loglik: -2.1657e+02 - logprior: -9.7585e+01
Epoch 2/2
10/10 - 2s - loss: 238.4348 - loglik: -2.0010e+02 - logprior: -3.7938e+01
Fitted a model with MAP estimate = -224.5212
expansions: [(0, 3)]
discards: [  0 110 111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 275.3085 - loglik: -1.9803e+02 - logprior: -7.6912e+01
Epoch 2/2
10/10 - 2s - loss: 211.4426 - loglik: -1.9328e+02 - logprior: -1.7862e+01
Fitted a model with MAP estimate = -201.1592
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 288.0735 - loglik: -1.9653e+02 - logprior: -9.1271e+01
Epoch 2/10
10/10 - 2s - loss: 218.9305 - loglik: -1.9483e+02 - logprior: -2.3882e+01
Epoch 3/10
10/10 - 2s - loss: 199.5082 - loglik: -1.9306e+02 - logprior: -6.1548e+00
Epoch 4/10
10/10 - 2s - loss: 192.1146 - loglik: -1.9176e+02 - logprior: 0.0929
Epoch 5/10
10/10 - 2s - loss: 188.5592 - loglik: -1.9136e+02 - logprior: 3.2405
Epoch 6/10
10/10 - 2s - loss: 186.5869 - loglik: -1.9129e+02 - logprior: 5.1138
Epoch 7/10
10/10 - 2s - loss: 185.3507 - loglik: -1.9124e+02 - logprior: 6.3019
Epoch 8/10
10/10 - 2s - loss: 184.4970 - loglik: -1.9122e+02 - logprior: 7.1415
Epoch 9/10
10/10 - 2s - loss: 183.8343 - loglik: -1.9126e+02 - logprior: 7.8530
Epoch 10/10
10/10 - 2s - loss: 183.2627 - loglik: -1.9135e+02 - logprior: 8.5079
Fitted a model with MAP estimate = -182.5479
Time for alignment: 54.3190
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 433.8089 - loglik: -3.4692e+02 - logprior: -8.6875e+01
Epoch 2/10
10/10 - 1s - loss: 329.3674 - loglik: -3.0824e+02 - logprior: -2.1123e+01
Epoch 3/10
10/10 - 1s - loss: 278.4339 - loglik: -2.6912e+02 - logprior: -9.2993e+00
Epoch 4/10
10/10 - 1s - loss: 247.0394 - loglik: -2.4122e+02 - logprior: -5.8108e+00
Epoch 5/10
10/10 - 1s - loss: 232.1674 - loglik: -2.2801e+02 - logprior: -4.0534e+00
Epoch 6/10
10/10 - 1s - loss: 226.1067 - loglik: -2.2284e+02 - logprior: -2.8978e+00
Epoch 7/10
10/10 - 1s - loss: 222.9933 - loglik: -2.2050e+02 - logprior: -2.0119e+00
Epoch 8/10
10/10 - 1s - loss: 221.4309 - loglik: -2.1959e+02 - logprior: -1.4079e+00
Epoch 9/10
10/10 - 1s - loss: 220.5489 - loglik: -2.1922e+02 - logprior: -9.6115e-01
Epoch 10/10
10/10 - 1s - loss: 219.8664 - loglik: -2.1887e+02 - logprior: -6.2658e-01
Fitted a model with MAP estimate = -219.1799
expansions: [(12, 1), (13, 2), (14, 2), (24, 1), (29, 2), (30, 1), (40, 2), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 315.6901 - loglik: -2.1773e+02 - logprior: -9.7602e+01
Epoch 2/2
10/10 - 2s - loss: 238.3385 - loglik: -1.9963e+02 - logprior: -3.8314e+01
Fitted a model with MAP estimate = -224.0824
expansions: [(0, 2)]
discards: [  0  35  48 113 114]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 273.7367 - loglik: -1.9658e+02 - logprior: -7.6780e+01
Epoch 2/2
10/10 - 2s - loss: 210.0067 - loglik: -1.9214e+02 - logprior: -1.7553e+01
Fitted a model with MAP estimate = -199.7528
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 287.7488 - loglik: -1.9584e+02 - logprior: -9.1621e+01
Epoch 2/10
10/10 - 2s - loss: 218.5835 - loglik: -1.9390e+02 - logprior: -2.4439e+01
Epoch 3/10
10/10 - 2s - loss: 198.6947 - loglik: -1.9212e+02 - logprior: -6.2674e+00
Epoch 4/10
10/10 - 2s - loss: 191.2656 - loglik: -1.9098e+02 - logprior: 0.1514
Epoch 5/10
10/10 - 2s - loss: 187.6729 - loglik: -1.9058e+02 - logprior: 3.3296
Epoch 6/10
10/10 - 2s - loss: 185.6970 - loglik: -1.9048e+02 - logprior: 5.1925
Epoch 7/10
10/10 - 2s - loss: 184.4781 - loglik: -1.9045e+02 - logprior: 6.3874
Epoch 8/10
10/10 - 2s - loss: 183.6261 - loglik: -1.9045e+02 - logprior: 7.2409
Epoch 9/10
10/10 - 2s - loss: 182.9617 - loglik: -1.9050e+02 - logprior: 7.9597
Epoch 10/10
10/10 - 2s - loss: 182.3899 - loglik: -1.9058e+02 - logprior: 8.6200
Fitted a model with MAP estimate = -181.6699
Time for alignment: 54.5777
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 433.8088 - loglik: -3.4692e+02 - logprior: -8.6875e+01
Epoch 2/10
10/10 - 1s - loss: 329.3674 - loglik: -3.0824e+02 - logprior: -2.1123e+01
Epoch 3/10
10/10 - 1s - loss: 278.4341 - loglik: -2.6912e+02 - logprior: -9.2992e+00
Epoch 4/10
10/10 - 1s - loss: 247.1704 - loglik: -2.4139e+02 - logprior: -5.7766e+00
Epoch 5/10
10/10 - 1s - loss: 233.0596 - loglik: -2.2921e+02 - logprior: -3.8340e+00
Epoch 6/10
10/10 - 1s - loss: 226.1493 - loglik: -2.2317e+02 - logprior: -2.8132e+00
Epoch 7/10
10/10 - 1s - loss: 222.7348 - loglik: -2.2033e+02 - logprior: -2.0095e+00
Epoch 8/10
10/10 - 1s - loss: 221.1632 - loglik: -2.1934e+02 - logprior: -1.3972e+00
Epoch 9/10
10/10 - 1s - loss: 220.2485 - loglik: -2.1893e+02 - logprior: -9.4297e-01
Epoch 10/10
10/10 - 1s - loss: 219.6691 - loglik: -2.1874e+02 - logprior: -5.6926e-01
Fitted a model with MAP estimate = -219.0614
expansions: [(13, 3), (14, 1), (24, 1), (29, 1), (30, 1), (40, 1), (41, 1), (55, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 314.2922 - loglik: -2.1641e+02 - logprior: -9.7556e+01
Epoch 2/2
10/10 - 2s - loss: 238.2952 - loglik: -1.9998e+02 - logprior: -3.7938e+01
Fitted a model with MAP estimate = -224.4442
expansions: [(0, 3)]
discards: [  0 110 111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 275.2627 - loglik: -1.9797e+02 - logprior: -7.6923e+01
Epoch 2/2
10/10 - 2s - loss: 211.4527 - loglik: -1.9327e+02 - logprior: -1.7878e+01
Fitted a model with MAP estimate = -201.2236
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 288.2044 - loglik: -1.9662e+02 - logprior: -9.1317e+01
Epoch 2/10
10/10 - 2s - loss: 219.0718 - loglik: -1.9489e+02 - logprior: -2.3965e+01
Epoch 3/10
10/10 - 2s - loss: 199.6371 - loglik: -1.9314e+02 - logprior: -6.2095e+00
Epoch 4/10
10/10 - 2s - loss: 192.2818 - loglik: -1.9191e+02 - logprior: 0.0730
Epoch 5/10
10/10 - 2s - loss: 188.7282 - loglik: -1.9151e+02 - logprior: 3.2241
Epoch 6/10
10/10 - 2s - loss: 186.7476 - loglik: -1.9142e+02 - logprior: 5.0897
Epoch 7/10
10/10 - 2s - loss: 185.5313 - loglik: -1.9141e+02 - logprior: 6.2934
Epoch 8/10
10/10 - 2s - loss: 184.6853 - loglik: -1.9141e+02 - logprior: 7.1408
Epoch 9/10
10/10 - 2s - loss: 184.0224 - loglik: -1.9145e+02 - logprior: 7.8523
Epoch 10/10
10/10 - 2s - loss: 183.4508 - loglik: -1.9153e+02 - logprior: 8.5011
Fitted a model with MAP estimate = -182.7335
Time for alignment: 55.2552
Computed alignments with likelihoods: ['-182.5479', '-181.6699', '-182.7335']
Best model has likelihood: -181.6699
time for generating output: 0.1767
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.8746268656716418
Training of 3 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fe140bb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1972960e20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19c79f7d60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1972e42c70>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1972e42a90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18b4882c70>, <__main__.SimpleDirichletPrior object at 0x7f18b39bbf70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18b322d9d0>

Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 167.4472 - loglik: -1.3559e+02 - logprior: -3.1847e+01
Epoch 2/10
10/10 - 1s - loss: 124.3129 - loglik: -1.1519e+02 - logprior: -9.1208e+00
Epoch 3/10
10/10 - 1s - loss: 98.7579 - loglik: -9.3689e+01 - logprior: -5.0375e+00
Epoch 4/10
10/10 - 1s - loss: 83.8873 - loglik: -7.9921e+01 - logprior: -3.9486e+00
Epoch 5/10
10/10 - 1s - loss: 78.6622 - loglik: -7.5035e+01 - logprior: -3.6219e+00
Epoch 6/10
10/10 - 1s - loss: 76.7998 - loglik: -7.3372e+01 - logprior: -3.3775e+00
Epoch 7/10
10/10 - 1s - loss: 76.0048 - loglik: -7.2820e+01 - logprior: -2.9988e+00
Epoch 8/10
10/10 - 1s - loss: 75.3710 - loglik: -7.2403e+01 - logprior: -2.7165e+00
Epoch 9/10
10/10 - 1s - loss: 75.1969 - loglik: -7.2349e+01 - logprior: -2.6245e+00
Epoch 10/10
10/10 - 1s - loss: 75.1376 - loglik: -7.2357e+01 - logprior: -2.5725e+00
Fitted a model with MAP estimate = -74.7279
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 114.8598 - loglik: -7.2080e+01 - logprior: -4.2593e+01
Epoch 2/2
10/10 - 1s - loss: 78.1983 - loglik: -6.4042e+01 - logprior: -1.4059e+01
Fitted a model with MAP estimate = -71.0403
expansions: []
discards: [30 33]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 91.2130 - loglik: -6.1018e+01 - logprior: -3.0107e+01
Epoch 2/2
10/10 - 1s - loss: 69.3637 - loglik: -6.0584e+01 - logprior: -8.6945e+00
Fitted a model with MAP estimate = -66.2124
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 88.7243 - loglik: -6.0142e+01 - logprior: -2.8486e+01
Epoch 2/10
10/10 - 1s - loss: 68.8022 - loglik: -6.0453e+01 - logprior: -8.2633e+00
Epoch 3/10
10/10 - 1s - loss: 65.0773 - loglik: -6.0664e+01 - logprior: -4.2714e+00
Epoch 4/10
10/10 - 1s - loss: 63.5129 - loglik: -6.0558e+01 - logprior: -2.7862e+00
Epoch 5/10
10/10 - 1s - loss: 62.8851 - loglik: -6.0620e+01 - logprior: -2.0762e+00
Epoch 6/10
10/10 - 1s - loss: 62.3703 - loglik: -6.0436e+01 - logprior: -1.7236e+00
Epoch 7/10
10/10 - 1s - loss: 62.2738 - loglik: -6.0548e+01 - logprior: -1.4967e+00
Epoch 8/10
10/10 - 1s - loss: 62.0835 - loglik: -6.0544e+01 - logprior: -1.2978e+00
Epoch 9/10
10/10 - 1s - loss: 61.9339 - loglik: -6.0534e+01 - logprior: -1.1490e+00
Epoch 10/10
10/10 - 1s - loss: 61.9166 - loglik: -6.0601e+01 - logprior: -1.0559e+00
Fitted a model with MAP estimate = -61.5827
Time for alignment: 31.3516
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.4535 - loglik: -1.3559e+02 - logprior: -3.1848e+01
Epoch 2/10
10/10 - 1s - loss: 124.2595 - loglik: -1.1512e+02 - logprior: -9.1302e+00
Epoch 3/10
10/10 - 1s - loss: 99.0947 - loglik: -9.4011e+01 - logprior: -5.0509e+00
Epoch 4/10
10/10 - 1s - loss: 84.3656 - loglik: -8.0400e+01 - logprior: -3.9482e+00
Epoch 5/10
10/10 - 1s - loss: 79.1641 - loglik: -7.5527e+01 - logprior: -3.6325e+00
Epoch 6/10
10/10 - 1s - loss: 77.0361 - loglik: -7.3605e+01 - logprior: -3.3879e+00
Epoch 7/10
10/10 - 1s - loss: 76.1569 - loglik: -7.2978e+01 - logprior: -2.9948e+00
Epoch 8/10
10/10 - 1s - loss: 75.6028 - loglik: -7.2634e+01 - logprior: -2.7113e+00
Epoch 9/10
10/10 - 1s - loss: 75.1955 - loglik: -7.2356e+01 - logprior: -2.6135e+00
Epoch 10/10
10/10 - 1s - loss: 75.0807 - loglik: -7.2297e+01 - logprior: -2.5750e+00
Fitted a model with MAP estimate = -74.7444
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 114.7133 - loglik: -7.1960e+01 - logprior: -4.2566e+01
Epoch 2/2
10/10 - 1s - loss: 78.3332 - loglik: -6.4179e+01 - logprior: -1.4056e+01
Fitted a model with MAP estimate = -71.0435
expansions: []
discards: [30 33]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 91.2355 - loglik: -6.1041e+01 - logprior: -3.0108e+01
Epoch 2/2
10/10 - 1s - loss: 69.2846 - loglik: -6.0501e+01 - logprior: -8.6988e+00
Fitted a model with MAP estimate = -66.2264
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 88.7677 - loglik: -6.0194e+01 - logprior: -2.8479e+01
Epoch 2/10
10/10 - 1s - loss: 68.8525 - loglik: -6.0498e+01 - logprior: -8.2704e+00
Epoch 3/10
10/10 - 1s - loss: 65.0335 - loglik: -6.0611e+01 - logprior: -4.2761e+00
Epoch 4/10
10/10 - 1s - loss: 63.4463 - loglik: -6.0492e+01 - logprior: -2.7876e+00
Epoch 5/10
10/10 - 1s - loss: 62.7994 - loglik: -6.0530e+01 - logprior: -2.0773e+00
Epoch 6/10
10/10 - 1s - loss: 62.4714 - loglik: -6.0532e+01 - logprior: -1.7281e+00
Epoch 7/10
10/10 - 1s - loss: 62.2077 - loglik: -6.0483e+01 - logprior: -1.4949e+00
Epoch 8/10
10/10 - 1s - loss: 62.1623 - loglik: -6.0616e+01 - logprior: -1.3028e+00
Epoch 9/10
10/10 - 1s - loss: 61.8997 - loglik: -6.0499e+01 - logprior: -1.1523e+00
Epoch 10/10
10/10 - 1s - loss: 61.8297 - loglik: -6.0514e+01 - logprior: -1.0586e+00
Fitted a model with MAP estimate = -61.5703
Time for alignment: 31.5570
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 167.3591 - loglik: -1.3550e+02 - logprior: -3.1847e+01
Epoch 2/10
10/10 - 1s - loss: 124.1129 - loglik: -1.1498e+02 - logprior: -9.1234e+00
Epoch 3/10
10/10 - 1s - loss: 98.4302 - loglik: -9.3351e+01 - logprior: -5.0467e+00
Epoch 4/10
10/10 - 1s - loss: 83.4396 - loglik: -7.9446e+01 - logprior: -3.9718e+00
Epoch 5/10
10/10 - 1s - loss: 78.3129 - loglik: -7.4588e+01 - logprior: -3.6752e+00
Epoch 6/10
10/10 - 1s - loss: 76.6187 - loglik: -7.3015e+01 - logprior: -3.4016e+00
Epoch 7/10
10/10 - 1s - loss: 75.8283 - loglik: -7.2558e+01 - logprior: -2.9854e+00
Epoch 8/10
10/10 - 1s - loss: 75.5408 - loglik: -7.2548e+01 - logprior: -2.7313e+00
Epoch 9/10
10/10 - 1s - loss: 75.0955 - loglik: -7.2230e+01 - logprior: -2.6341e+00
Epoch 10/10
10/10 - 1s - loss: 75.0644 - loglik: -7.2248e+01 - logprior: -2.5825e+00
Fitted a model with MAP estimate = -74.6943
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 114.8755 - loglik: -7.2076e+01 - logprior: -4.2587e+01
Epoch 2/2
10/10 - 1s - loss: 78.2495 - loglik: -6.4065e+01 - logprior: -1.4068e+01
Fitted a model with MAP estimate = -71.0416
expansions: []
discards: [30 33]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 91.1779 - loglik: -6.0974e+01 - logprior: -3.0110e+01
Epoch 2/2
10/10 - 1s - loss: 69.3629 - loglik: -6.0567e+01 - logprior: -8.7073e+00
Fitted a model with MAP estimate = -66.2261
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 88.7724 - loglik: -6.0184e+01 - logprior: -2.8490e+01
Epoch 2/10
10/10 - 1s - loss: 68.8298 - loglik: -6.0483e+01 - logprior: -8.2640e+00
Epoch 3/10
10/10 - 1s - loss: 65.0087 - loglik: -6.0591e+01 - logprior: -4.2743e+00
Epoch 4/10
10/10 - 1s - loss: 63.5180 - loglik: -6.0561e+01 - logprior: -2.7869e+00
Epoch 5/10
10/10 - 1s - loss: 62.8108 - loglik: -6.0541e+01 - logprior: -2.0782e+00
Epoch 6/10
10/10 - 1s - loss: 62.3463 - loglik: -6.0406e+01 - logprior: -1.7255e+00
Epoch 7/10
10/10 - 1s - loss: 62.2132 - loglik: -6.0479e+01 - logprior: -1.5024e+00
Epoch 8/10
10/10 - 1s - loss: 62.0553 - loglik: -6.0516e+01 - logprior: -1.2978e+00
Epoch 9/10
10/10 - 1s - loss: 62.0230 - loglik: -6.0617e+01 - logprior: -1.1538e+00
Epoch 10/10
10/10 - 1s - loss: 61.8333 - loglik: -6.0521e+01 - logprior: -1.0582e+00
Fitted a model with MAP estimate = -61.5627
Time for alignment: 31.8884
Computed alignments with likelihoods: ['-61.5827', '-61.5703', '-61.5627']
Best model has likelihood: -61.5627
time for generating output: 0.1152
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 3 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1899d80700>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19ae19c940>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19ae19c880>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1961ede730>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fe4a1b20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1961ed26a0>, <__main__.SimpleDirichletPrior object at 0x7f198c796850>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19c7983b80>

Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 445.1873 - loglik: -4.3972e+02 - logprior: -5.4540e+00
Epoch 2/10
15/15 - 4s - loss: 367.8916 - loglik: -3.6646e+02 - logprior: -1.4128e+00
Epoch 3/10
15/15 - 4s - loss: 321.6611 - loglik: -3.1999e+02 - logprior: -1.6427e+00
Epoch 4/10
15/15 - 4s - loss: 309.3295 - loglik: -3.0726e+02 - logprior: -1.8182e+00
Epoch 5/10
15/15 - 4s - loss: 303.7967 - loglik: -3.0173e+02 - logprior: -1.7414e+00
Epoch 6/10
15/15 - 4s - loss: 301.0773 - loglik: -2.9904e+02 - logprior: -1.7568e+00
Epoch 7/10
15/15 - 4s - loss: 301.9137 - loglik: -2.9992e+02 - logprior: -1.7410e+00
Fitted a model with MAP estimate = -299.6940
expansions: [(7, 3), (10, 1), (14, 1), (24, 3), (25, 1), (49, 2), (60, 2), (66, 2), (69, 1), (91, 1), (92, 1), (103, 1), (111, 1), (114, 3), (116, 3), (119, 2), (120, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 10s - loss: 304.0241 - loglik: -2.9704e+02 - logprior: -6.7424e+00
Epoch 2/2
15/15 - 5s - loss: 286.9551 - loglik: -2.8325e+02 - logprior: -3.4432e+00
Fitted a model with MAP estimate = -284.6430
expansions: [(0, 2)]
discards: [  0   7  29  58 147]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 288.4896 - loglik: -2.8300e+02 - logprior: -5.2026e+00
Epoch 2/2
15/15 - 5s - loss: 283.8530 - loglik: -2.8177e+02 - logprior: -1.7269e+00
Fitted a model with MAP estimate = -280.7766
expansions: [(77, 1)]
discards: [ 0 81]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 289.9828 - loglik: -2.8294e+02 - logprior: -6.6697e+00
Epoch 2/10
15/15 - 5s - loss: 282.7474 - loglik: -2.7985e+02 - logprior: -2.4872e+00
Epoch 3/10
15/15 - 5s - loss: 280.3157 - loglik: -2.7851e+02 - logprior: -1.3424e+00
Epoch 4/10
15/15 - 5s - loss: 279.4756 - loglik: -2.7787e+02 - logprior: -1.1617e+00
Epoch 5/10
15/15 - 5s - loss: 277.8489 - loglik: -2.7635e+02 - logprior: -1.1096e+00
Epoch 6/10
15/15 - 5s - loss: 277.3333 - loglik: -2.7592e+02 - logprior: -1.0678e+00
Epoch 7/10
15/15 - 5s - loss: 277.6100 - loglik: -2.7628e+02 - logprior: -1.0185e+00
Fitted a model with MAP estimate = -276.6246
Time for alignment: 126.8748
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 445.6726 - loglik: -4.4020e+02 - logprior: -5.4598e+00
Epoch 2/10
15/15 - 4s - loss: 364.3830 - loglik: -3.6295e+02 - logprior: -1.4079e+00
Epoch 3/10
15/15 - 4s - loss: 317.7237 - loglik: -3.1605e+02 - logprior: -1.6378e+00
Epoch 4/10
15/15 - 4s - loss: 304.5137 - loglik: -3.0237e+02 - logprior: -1.8904e+00
Epoch 5/10
15/15 - 4s - loss: 300.7814 - loglik: -2.9872e+02 - logprior: -1.7684e+00
Epoch 6/10
15/15 - 4s - loss: 298.9549 - loglik: -2.9694e+02 - logprior: -1.7603e+00
Epoch 7/10
15/15 - 4s - loss: 297.9092 - loglik: -2.9589e+02 - logprior: -1.7748e+00
Epoch 8/10
15/15 - 4s - loss: 297.1952 - loglik: -2.9519e+02 - logprior: -1.7620e+00
Epoch 9/10
15/15 - 4s - loss: 297.4371 - loglik: -2.9543e+02 - logprior: -1.7641e+00
Fitted a model with MAP estimate = -296.6247
expansions: [(5, 1), (7, 2), (10, 1), (14, 1), (24, 2), (26, 1), (49, 2), (55, 1), (60, 1), (65, 3), (69, 1), (91, 1), (92, 2), (108, 1), (112, 1), (114, 1), (115, 1), (116, 4), (119, 2), (120, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 303.3756 - loglik: -2.9641e+02 - logprior: -6.7361e+00
Epoch 2/2
15/15 - 5s - loss: 287.3272 - loglik: -2.8362e+02 - logprior: -3.4334e+00
Fitted a model with MAP estimate = -283.9525
expansions: [(0, 2)]
discards: [  0   7  57  83 108 148]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 288.4437 - loglik: -2.8297e+02 - logprior: -5.1828e+00
Epoch 2/2
15/15 - 5s - loss: 281.9434 - loglik: -2.7992e+02 - logprior: -1.6787e+00
Fitted a model with MAP estimate = -280.7406
expansions: []
discards: [  0 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 290.3480 - loglik: -2.8339e+02 - logprior: -6.5864e+00
Epoch 2/10
15/15 - 5s - loss: 283.7385 - loglik: -2.8093e+02 - logprior: -2.3896e+00
Epoch 3/10
15/15 - 5s - loss: 283.0226 - loglik: -2.8127e+02 - logprior: -1.3068e+00
Epoch 4/10
15/15 - 5s - loss: 279.3349 - loglik: -2.7778e+02 - logprior: -1.1215e+00
Epoch 5/10
15/15 - 5s - loss: 280.3149 - loglik: -2.7886e+02 - logprior: -1.0608e+00
Fitted a model with MAP estimate = -278.7385
Time for alignment: 124.7034
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 444.9149 - loglik: -4.3944e+02 - logprior: -5.4623e+00
Epoch 2/10
15/15 - 4s - loss: 366.8034 - loglik: -3.6536e+02 - logprior: -1.4277e+00
Epoch 3/10
15/15 - 4s - loss: 320.3867 - loglik: -3.1866e+02 - logprior: -1.6983e+00
Epoch 4/10
15/15 - 4s - loss: 308.0681 - loglik: -3.0598e+02 - logprior: -1.8312e+00
Epoch 5/10
15/15 - 4s - loss: 302.8400 - loglik: -3.0084e+02 - logprior: -1.6942e+00
Epoch 6/10
15/15 - 4s - loss: 301.8097 - loglik: -2.9985e+02 - logprior: -1.7012e+00
Epoch 7/10
15/15 - 4s - loss: 299.9305 - loglik: -2.9800e+02 - logprior: -1.6976e+00
Epoch 8/10
15/15 - 4s - loss: 299.6545 - loglik: -2.9771e+02 - logprior: -1.6941e+00
Epoch 9/10
15/15 - 4s - loss: 298.3015 - loglik: -2.9633e+02 - logprior: -1.7181e+00
Epoch 10/10
15/15 - 4s - loss: 297.1514 - loglik: -2.9519e+02 - logprior: -1.7235e+00
Fitted a model with MAP estimate = -297.5056
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (27, 1), (49, 2), (60, 1), (66, 3), (69, 1), (91, 2), (92, 2), (106, 2), (112, 1), (114, 1), (115, 1), (116, 4), (119, 1), (120, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 302.3770 - loglik: -2.9540e+02 - logprior: -6.7408e+00
Epoch 2/2
15/15 - 5s - loss: 286.7689 - loglik: -2.8307e+02 - logprior: -3.4347e+00
Fitted a model with MAP estimate = -284.3008
expansions: [(0, 2)]
discards: [  0   7  57  78 108 124]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 288.1495 - loglik: -2.8268e+02 - logprior: -5.1768e+00
Epoch 2/2
15/15 - 5s - loss: 283.2259 - loglik: -2.8122e+02 - logprior: -1.6662e+00
Fitted a model with MAP estimate = -281.1712
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 289.3746 - loglik: -2.8243e+02 - logprior: -6.5994e+00
Epoch 2/10
15/15 - 5s - loss: 286.0013 - loglik: -2.8323e+02 - logprior: -2.3731e+00
Epoch 3/10
15/15 - 5s - loss: 281.2820 - loglik: -2.7959e+02 - logprior: -1.2695e+00
Epoch 4/10
15/15 - 5s - loss: 281.6053 - loglik: -2.8009e+02 - logprior: -1.0874e+00
Fitted a model with MAP estimate = -279.6469
Time for alignment: 123.4416
Computed alignments with likelihoods: ['-276.6246', '-278.7385', '-279.6469']
Best model has likelihood: -276.6246
time for generating output: 0.2236
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.9113712374581939
Training of 3 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19a5fd4820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1878907670>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1878907ca0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19508f4af0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19c790c2e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f193d2adf10>, <__main__.SimpleDirichletPrior object at 0x7f19fed0acd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19c7983b80>

Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 298.9211 - loglik: -2.9589e+02 - logprior: -2.9440e+00
Epoch 2/10
34/34 - 7s - loss: 208.2335 - loglik: -2.0628e+02 - logprior: -1.8984e+00
Epoch 3/10
34/34 - 7s - loss: 201.1662 - loglik: -1.9894e+02 - logprior: -1.9267e+00
Epoch 4/10
34/34 - 7s - loss: 199.4160 - loglik: -1.9724e+02 - logprior: -1.8960e+00
Epoch 5/10
34/34 - 6s - loss: 199.8142 - loglik: -1.9760e+02 - logprior: -1.9198e+00
Fitted a model with MAP estimate = -198.6748
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 2), (27, 1), (28, 1), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (54, 1), (55, 2), (56, 1), (63, 1), (64, 1), (78, 1), (81, 1), (84, 1), (107, 1), (108, 1), (110, 1), (111, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 187.4224 - loglik: -1.8397e+02 - logprior: -3.2276e+00
Epoch 2/2
34/34 - 7s - loss: 177.0957 - loglik: -1.7532e+02 - logprior: -1.5181e+00
Fitted a model with MAP estimate = -175.4521
expansions: []
discards: [19 34]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 180.1932 - loglik: -1.7710e+02 - logprior: -2.8488e+00
Epoch 2/2
34/34 - 7s - loss: 176.7031 - loglik: -1.7513e+02 - logprior: -1.2945e+00
Fitted a model with MAP estimate = -175.4855
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 179.5804 - loglik: -1.7660e+02 - logprior: -2.7150e+00
Epoch 2/10
34/34 - 7s - loss: 176.5851 - loglik: -1.7515e+02 - logprior: -1.1567e+00
Epoch 3/10
34/34 - 7s - loss: 175.1627 - loglik: -1.7378e+02 - logprior: -1.0604e+00
Epoch 4/10
34/34 - 7s - loss: 175.6696 - loglik: -1.7437e+02 - logprior: -9.7236e-01
Fitted a model with MAP estimate = -174.0225
Time for alignment: 138.1340
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 11s - loss: 300.8790 - loglik: -2.9786e+02 - logprior: -2.9320e+00
Epoch 2/10
34/34 - 6s - loss: 211.2064 - loglik: -2.0920e+02 - logprior: -1.8720e+00
Epoch 3/10
34/34 - 6s - loss: 201.3674 - loglik: -1.9920e+02 - logprior: -1.8719e+00
Epoch 4/10
34/34 - 7s - loss: 200.7098 - loglik: -1.9858e+02 - logprior: -1.8347e+00
Epoch 5/10
34/34 - 6s - loss: 200.3383 - loglik: -1.9824e+02 - logprior: -1.8100e+00
Epoch 6/10
34/34 - 6s - loss: 199.3512 - loglik: -1.9723e+02 - logprior: -1.8144e+00
Epoch 7/10
34/34 - 6s - loss: 200.1077 - loglik: -1.9797e+02 - logprior: -1.8331e+00
Fitted a model with MAP estimate = -199.2231
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (43, 1), (44, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (77, 1), (82, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 187.9620 - loglik: -1.8444e+02 - logprior: -3.2651e+00
Epoch 2/2
34/34 - 7s - loss: 176.7359 - loglik: -1.7500e+02 - logprior: -1.4739e+00
Fitted a model with MAP estimate = -175.3866
expansions: []
discards: [ 34 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 11s - loss: 179.8625 - loglik: -1.7676e+02 - logprior: -2.8366e+00
Epoch 2/2
34/34 - 7s - loss: 177.2983 - loglik: -1.7573e+02 - logprior: -1.2933e+00
Fitted a model with MAP estimate = -175.6052
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 179.8404 - loglik: -1.7687e+02 - logprior: -2.7121e+00
Epoch 2/10
34/34 - 7s - loss: 176.3755 - loglik: -1.7493e+02 - logprior: -1.1528e+00
Epoch 3/10
34/34 - 7s - loss: 175.5429 - loglik: -1.7415e+02 - logprior: -1.0567e+00
Epoch 4/10
34/34 - 7s - loss: 174.9246 - loglik: -1.7362e+02 - logprior: -9.7207e-01
Epoch 5/10
34/34 - 7s - loss: 174.0971 - loglik: -1.7288e+02 - logprior: -8.9963e-01
Epoch 6/10
34/34 - 7s - loss: 174.4784 - loglik: -1.7334e+02 - logprior: -8.1487e-01
Fitted a model with MAP estimate = -173.6330
Time for alignment: 164.1698
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 299.9274 - loglik: -2.9691e+02 - logprior: -2.9267e+00
Epoch 2/10
34/34 - 7s - loss: 210.4331 - loglik: -2.0842e+02 - logprior: -1.9146e+00
Epoch 3/10
34/34 - 6s - loss: 200.6699 - loglik: -1.9844e+02 - logprior: -1.9316e+00
Epoch 4/10
34/34 - 6s - loss: 200.1869 - loglik: -1.9800e+02 - logprior: -1.9090e+00
Epoch 5/10
34/34 - 6s - loss: 199.4016 - loglik: -1.9720e+02 - logprior: -1.9198e+00
Epoch 6/10
34/34 - 7s - loss: 197.5943 - loglik: -1.9540e+02 - logprior: -1.8967e+00
Epoch 7/10
34/34 - 6s - loss: 198.9908 - loglik: -1.9677e+02 - logprior: -1.9211e+00
Fitted a model with MAP estimate = -198.1822
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (77, 1), (78, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 188.0620 - loglik: -1.8452e+02 - logprior: -3.2898e+00
Epoch 2/2
34/34 - 7s - loss: 176.7711 - loglik: -1.7500e+02 - logprior: -1.5192e+00
Fitted a model with MAP estimate = -175.4823
expansions: []
discards: [ 20  35 140]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 179.9310 - loglik: -1.7685e+02 - logprior: -2.8324e+00
Epoch 2/2
34/34 - 7s - loss: 177.4725 - loglik: -1.7590e+02 - logprior: -1.2919e+00
Fitted a model with MAP estimate = -175.5562
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 10s - loss: 179.1450 - loglik: -1.7617e+02 - logprior: -2.7165e+00
Epoch 2/10
34/34 - 7s - loss: 177.5094 - loglik: -1.7608e+02 - logprior: -1.1488e+00
Epoch 3/10
34/34 - 6s - loss: 175.2602 - loglik: -1.7388e+02 - logprior: -1.0511e+00
Epoch 4/10
34/34 - 7s - loss: 174.6559 - loglik: -1.7336e+02 - logprior: -9.6818e-01
Epoch 5/10
34/34 - 7s - loss: 174.9278 - loglik: -1.7372e+02 - logprior: -8.8142e-01
Fitted a model with MAP estimate = -173.8156
Time for alignment: 157.0789
Computed alignments with likelihoods: ['-174.0225', '-173.6330', '-173.8156']
Best model has likelihood: -173.6330
time for generating output: 0.4845
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.6283338732318324
Training of 3 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f193d3e11c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193e825e50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f193e7f1d60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19474bc490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f199d45cd60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19ae2db070>, <__main__.SimpleDirichletPrior object at 0x7f193e987e80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fec29820>

Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 420.2404 - loglik: -3.9914e+02 - logprior: -2.1085e+01
Epoch 2/10
10/10 - 2s - loss: 364.7806 - loglik: -3.5968e+02 - logprior: -5.1006e+00
Epoch 3/10
10/10 - 2s - loss: 324.2133 - loglik: -3.2139e+02 - logprior: -2.8159e+00
Epoch 4/10
10/10 - 2s - loss: 296.4726 - loglik: -2.9394e+02 - logprior: -2.4334e+00
Epoch 5/10
10/10 - 2s - loss: 283.3684 - loglik: -2.8046e+02 - logprior: -2.5390e+00
Epoch 6/10
10/10 - 2s - loss: 277.8102 - loglik: -2.7472e+02 - logprior: -2.5513e+00
Epoch 7/10
10/10 - 2s - loss: 275.2067 - loglik: -2.7218e+02 - logprior: -2.5635e+00
Epoch 8/10
10/10 - 2s - loss: 274.1657 - loglik: -2.7123e+02 - logprior: -2.5649e+00
Epoch 9/10
10/10 - 2s - loss: 272.4102 - loglik: -2.6957e+02 - logprior: -2.4955e+00
Epoch 10/10
10/10 - 2s - loss: 272.2424 - loglik: -2.6947e+02 - logprior: -2.4223e+00
Fitted a model with MAP estimate = -271.3929
expansions: [(8, 3), (9, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (87, 1), (97, 4), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 289.6025 - loglik: -2.6971e+02 - logprior: -1.9493e+01
Epoch 2/2
10/10 - 2s - loss: 263.8149 - loglik: -2.5812e+02 - logprior: -5.1824e+00
Fitted a model with MAP estimate = -257.9748
expansions: []
discards: [  0 118 119 120 121]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 286.9027 - loglik: -2.6242e+02 - logprior: -2.4015e+01
Epoch 2/2
10/10 - 2s - loss: 268.8215 - loglik: -2.5864e+02 - logprior: -9.7795e+00
Fitted a model with MAP estimate = -265.1721
expansions: [(0, 5), (120, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 277.5180 - loglik: -2.5800e+02 - logprior: -1.9135e+01
Epoch 2/10
10/10 - 3s - loss: 258.8997 - loglik: -2.5378e+02 - logprior: -4.7365e+00
Epoch 3/10
10/10 - 2s - loss: 253.7334 - loglik: -2.5128e+02 - logprior: -1.9856e+00
Epoch 4/10
10/10 - 3s - loss: 250.5410 - loglik: -2.4894e+02 - logprior: -1.0667e+00
Epoch 5/10
10/10 - 2s - loss: 248.9724 - loglik: -2.4786e+02 - logprior: -6.0742e-01
Epoch 6/10
10/10 - 3s - loss: 248.7923 - loglik: -2.4795e+02 - logprior: -3.6812e-01
Epoch 7/10
10/10 - 3s - loss: 247.8937 - loglik: -2.4724e+02 - logprior: -1.9474e-01
Epoch 8/10
10/10 - 2s - loss: 246.7631 - loglik: -2.4630e+02 - logprior: -2.2413e-02
Epoch 9/10
10/10 - 3s - loss: 247.3732 - loglik: -2.4706e+02 - logprior: 0.1254
Fitted a model with MAP estimate = -246.6288
Time for alignment: 75.9158
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 419.9151 - loglik: -3.9882e+02 - logprior: -2.1081e+01
Epoch 2/10
10/10 - 2s - loss: 364.4847 - loglik: -3.5937e+02 - logprior: -5.1100e+00
Epoch 3/10
10/10 - 2s - loss: 320.1656 - loglik: -3.1727e+02 - logprior: -2.8953e+00
Epoch 4/10
10/10 - 2s - loss: 291.3774 - loglik: -2.8874e+02 - logprior: -2.5570e+00
Epoch 5/10
10/10 - 2s - loss: 281.1302 - loglik: -2.7817e+02 - logprior: -2.6444e+00
Epoch 6/10
10/10 - 2s - loss: 276.1404 - loglik: -2.7300e+02 - logprior: -2.6208e+00
Epoch 7/10
10/10 - 2s - loss: 272.5674 - loglik: -2.6959e+02 - logprior: -2.5076e+00
Epoch 8/10
10/10 - 2s - loss: 272.6864 - loglik: -2.6985e+02 - logprior: -2.4700e+00
Fitted a model with MAP estimate = -270.8984
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (60, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (97, 4), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 288.5938 - loglik: -2.6875e+02 - logprior: -1.9462e+01
Epoch 2/2
10/10 - 2s - loss: 262.4517 - loglik: -2.5686e+02 - logprior: -5.1629e+00
Fitted a model with MAP estimate = -257.3102
expansions: []
discards: [  0 119 120 121 122]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 285.7147 - loglik: -2.6129e+02 - logprior: -2.4026e+01
Epoch 2/2
10/10 - 2s - loss: 268.5805 - loglik: -2.5840e+02 - logprior: -9.8197e+00
Fitted a model with MAP estimate = -264.7512
expansions: [(0, 5), (121, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 277.2249 - loglik: -2.5765e+02 - logprior: -1.9197e+01
Epoch 2/10
10/10 - 3s - loss: 258.4834 - loglik: -2.5330e+02 - logprior: -4.8063e+00
Epoch 3/10
10/10 - 3s - loss: 252.7673 - loglik: -2.5023e+02 - logprior: -2.0794e+00
Epoch 4/10
10/10 - 3s - loss: 249.8587 - loglik: -2.4818e+02 - logprior: -1.1412e+00
Epoch 5/10
10/10 - 2s - loss: 248.0393 - loglik: -2.4685e+02 - logprior: -6.7862e-01
Epoch 6/10
10/10 - 3s - loss: 247.7392 - loglik: -2.4683e+02 - logprior: -4.3520e-01
Epoch 7/10
10/10 - 3s - loss: 247.5660 - loglik: -2.4687e+02 - logprior: -2.5198e-01
Epoch 8/10
10/10 - 3s - loss: 246.5104 - loglik: -2.4603e+02 - logprior: -5.7785e-02
Epoch 9/10
10/10 - 3s - loss: 246.5052 - loglik: -2.4617e+02 - logprior: 0.0792
Epoch 10/10
10/10 - 3s - loss: 246.6005 - loglik: -2.4638e+02 - logprior: 0.1880
Fitted a model with MAP estimate = -245.7682
Time for alignment: 73.5375
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 420.1886 - loglik: -3.9909e+02 - logprior: -2.1084e+01
Epoch 2/10
10/10 - 2s - loss: 364.8893 - loglik: -3.5977e+02 - logprior: -5.1203e+00
Epoch 3/10
10/10 - 2s - loss: 322.2278 - loglik: -3.1927e+02 - logprior: -2.9500e+00
Epoch 4/10
10/10 - 2s - loss: 292.8669 - loglik: -2.9028e+02 - logprior: -2.5202e+00
Epoch 5/10
10/10 - 2s - loss: 281.3157 - loglik: -2.7862e+02 - logprior: -2.4945e+00
Epoch 6/10
10/10 - 2s - loss: 276.9644 - loglik: -2.7415e+02 - logprior: -2.4347e+00
Epoch 7/10
10/10 - 2s - loss: 275.9743 - loglik: -2.7318e+02 - logprior: -2.3572e+00
Epoch 8/10
10/10 - 2s - loss: 273.7769 - loglik: -2.7107e+02 - logprior: -2.2902e+00
Epoch 9/10
10/10 - 2s - loss: 273.8904 - loglik: -2.7129e+02 - logprior: -2.2201e+00
Fitted a model with MAP estimate = -273.2156
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 3), (29, 2), (53, 1), (55, 2), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (84, 2), (86, 2), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 290.8173 - loglik: -2.7089e+02 - logprior: -1.9527e+01
Epoch 2/2
10/10 - 2s - loss: 265.7311 - loglik: -2.5998e+02 - logprior: -5.2981e+00
Fitted a model with MAP estimate = -260.3714
expansions: []
discards: [  0  38  67 105 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 285.5320 - loglik: -2.6106e+02 - logprior: -2.4048e+01
Epoch 2/2
10/10 - 2s - loss: 269.0963 - loglik: -2.5889e+02 - logprior: -9.8198e+00
Fitted a model with MAP estimate = -265.2034
expansions: [(0, 5)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 279.2416 - loglik: -2.5974e+02 - logprior: -1.9126e+01
Epoch 2/10
10/10 - 2s - loss: 261.1393 - loglik: -2.5600e+02 - logprior: -4.7596e+00
Epoch 3/10
10/10 - 2s - loss: 257.7786 - loglik: -2.5534e+02 - logprior: -1.9564e+00
Epoch 4/10
10/10 - 2s - loss: 255.3412 - loglik: -2.5385e+02 - logprior: -9.5471e-01
Epoch 5/10
10/10 - 2s - loss: 253.8338 - loglik: -2.5282e+02 - logprior: -5.1354e-01
Epoch 6/10
10/10 - 2s - loss: 253.5792 - loglik: -2.5284e+02 - logprior: -2.6534e-01
Epoch 7/10
10/10 - 3s - loss: 252.6730 - loglik: -2.5212e+02 - logprior: -9.3890e-02
Epoch 8/10
10/10 - 2s - loss: 252.7560 - loglik: -2.5239e+02 - logprior: 0.0791
Fitted a model with MAP estimate = -251.9322
Time for alignment: 71.1416
Computed alignments with likelihoods: ['-246.6288', '-245.7682', '-251.9322']
Best model has likelihood: -245.7682
time for generating output: 0.2006
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.913372859025033
Training of 3 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f18a2db1e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193d0129a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198c64ffd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f198be58a60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1877d1ff70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1877d17f40>, <__main__.SimpleDirichletPrior object at 0x7f19c78ced00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f198b9740d0>

Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.3682 - loglik: -2.3267e+02 - logprior: -3.7672e+01
Epoch 2/10
10/10 - 1s - loss: 230.7834 - loglik: -2.2066e+02 - logprior: -9.8946e+00
Epoch 3/10
10/10 - 1s - loss: 213.7102 - loglik: -2.0860e+02 - logprior: -4.6373e+00
Epoch 4/10
10/10 - 1s - loss: 203.2096 - loglik: -1.9996e+02 - logprior: -2.8561e+00
Epoch 5/10
10/10 - 1s - loss: 196.8236 - loglik: -1.9396e+02 - logprior: -2.3646e+00
Epoch 6/10
10/10 - 1s - loss: 193.6949 - loglik: -1.9073e+02 - logprior: -2.3795e+00
Epoch 7/10
10/10 - 1s - loss: 191.9020 - loglik: -1.8950e+02 - logprior: -1.9492e+00
Epoch 8/10
10/10 - 1s - loss: 190.8302 - loglik: -1.8908e+02 - logprior: -1.4306e+00
Epoch 9/10
10/10 - 1s - loss: 190.1842 - loglik: -1.8859e+02 - logprior: -1.3138e+00
Epoch 10/10
10/10 - 1s - loss: 189.6789 - loglik: -1.8810e+02 - logprior: -1.2907e+00
Fitted a model with MAP estimate = -189.0567
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (30, 3), (40, 2), (43, 1), (57, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 252.2522 - loglik: -2.0217e+02 - logprior: -4.9741e+01
Epoch 2/2
10/10 - 1s - loss: 203.0205 - loglik: -1.8757e+02 - logprior: -1.4929e+01
Fitted a model with MAP estimate = -193.3904
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 230.1908 - loglik: -1.8681e+02 - logprior: -4.2598e+01
Epoch 2/2
10/10 - 1s - loss: 202.1405 - loglik: -1.8458e+02 - logprior: -1.6680e+01
Fitted a model with MAP estimate = -196.5167
expansions: [(0, 2)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 223.3108 - loglik: -1.8434e+02 - logprior: -3.8024e+01
Epoch 2/10
10/10 - 1s - loss: 193.6982 - loglik: -1.8289e+02 - logprior: -1.0004e+01
Epoch 3/10
10/10 - 1s - loss: 186.8634 - loglik: -1.8238e+02 - logprior: -3.8431e+00
Epoch 4/10
10/10 - 1s - loss: 184.1270 - loglik: -1.8199e+02 - logprior: -1.6026e+00
Epoch 5/10
10/10 - 1s - loss: 182.8226 - loglik: -1.8186e+02 - logprior: -4.9078e-01
Epoch 6/10
10/10 - 1s - loss: 181.8211 - loglik: -1.8149e+02 - logprior: 0.0781
Epoch 7/10
10/10 - 1s - loss: 181.2124 - loglik: -1.8122e+02 - logprior: 0.3744
Epoch 8/10
10/10 - 1s - loss: 181.0077 - loglik: -1.8121e+02 - logprior: 0.5369
Epoch 9/10
10/10 - 1s - loss: 180.5280 - loglik: -1.8085e+02 - logprior: 0.6513
Epoch 10/10
10/10 - 1s - loss: 180.2077 - loglik: -1.8066e+02 - logprior: 0.7869
Fitted a model with MAP estimate = -179.8176
Time for alignment: 49.3073
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 270.2753 - loglik: -2.3258e+02 - logprior: -3.7670e+01
Epoch 2/10
10/10 - 1s - loss: 230.9037 - loglik: -2.2079e+02 - logprior: -9.8893e+00
Epoch 3/10
10/10 - 1s - loss: 214.7295 - loglik: -2.0961e+02 - logprior: -4.6234e+00
Epoch 4/10
10/10 - 1s - loss: 203.5114 - loglik: -2.0027e+02 - logprior: -2.8696e+00
Epoch 5/10
10/10 - 1s - loss: 197.5011 - loglik: -1.9482e+02 - logprior: -2.3786e+00
Epoch 6/10
10/10 - 1s - loss: 193.6030 - loglik: -1.9083e+02 - logprior: -2.3834e+00
Epoch 7/10
10/10 - 1s - loss: 191.8961 - loglik: -1.8952e+02 - logprior: -1.9787e+00
Epoch 8/10
10/10 - 1s - loss: 191.1157 - loglik: -1.8930e+02 - logprior: -1.4784e+00
Epoch 9/10
10/10 - 1s - loss: 190.3475 - loglik: -1.8871e+02 - logprior: -1.3207e+00
Epoch 10/10
10/10 - 1s - loss: 189.8274 - loglik: -1.8827e+02 - logprior: -1.2483e+00
Fitted a model with MAP estimate = -189.3805
expansions: [(0, 3), (9, 1), (22, 1), (25, 1), (27, 1), (30, 1), (31, 2), (40, 2), (43, 1), (56, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 248.8889 - loglik: -1.9888e+02 - logprior: -4.9691e+01
Epoch 2/2
10/10 - 1s - loss: 202.1923 - loglik: -1.8677e+02 - logprior: -1.4984e+01
Fitted a model with MAP estimate = -193.4759
expansions: [(39, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.7163 - loglik: -1.8651e+02 - logprior: -4.2560e+01
Epoch 2/2
10/10 - 1s - loss: 201.6968 - loglik: -1.8433e+02 - logprior: -1.6567e+01
Fitted a model with MAP estimate = -196.1555
expansions: [(0, 2)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.0126 - loglik: -1.8416e+02 - logprior: -3.7957e+01
Epoch 2/10
10/10 - 1s - loss: 193.2435 - loglik: -1.8251e+02 - logprior: -9.9326e+00
Epoch 3/10
10/10 - 1s - loss: 186.6294 - loglik: -1.8221e+02 - logprior: -3.7781e+00
Epoch 4/10
10/10 - 1s - loss: 183.9212 - loglik: -1.8188e+02 - logprior: -1.5157e+00
Epoch 5/10
10/10 - 1s - loss: 182.4711 - loglik: -1.8160e+02 - logprior: -4.0900e-01
Epoch 6/10
10/10 - 1s - loss: 181.5787 - loglik: -1.8133e+02 - logprior: 0.1641
Epoch 7/10
10/10 - 1s - loss: 180.9135 - loglik: -1.8100e+02 - logprior: 0.4562
Epoch 8/10
10/10 - 1s - loss: 180.6555 - loglik: -1.8093e+02 - logprior: 0.6214
Epoch 9/10
10/10 - 1s - loss: 180.2378 - loglik: -1.8064e+02 - logprior: 0.7385
Epoch 10/10
10/10 - 1s - loss: 179.9336 - loglik: -1.8047e+02 - logprior: 0.8722
Fitted a model with MAP estimate = -179.5154
Time for alignment: 46.2421
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 270.2560 - loglik: -2.3256e+02 - logprior: -3.7671e+01
Epoch 2/10
10/10 - 1s - loss: 230.9123 - loglik: -2.2079e+02 - logprior: -9.8957e+00
Epoch 3/10
10/10 - 1s - loss: 213.1706 - loglik: -2.0806e+02 - logprior: -4.6414e+00
Epoch 4/10
10/10 - 1s - loss: 202.8580 - loglik: -1.9969e+02 - logprior: -2.8248e+00
Epoch 5/10
10/10 - 1s - loss: 197.1010 - loglik: -1.9460e+02 - logprior: -2.2491e+00
Epoch 6/10
10/10 - 1s - loss: 193.5191 - loglik: -1.9092e+02 - logprior: -2.2554e+00
Epoch 7/10
10/10 - 1s - loss: 191.9145 - loglik: -1.8966e+02 - logprior: -1.8567e+00
Epoch 8/10
10/10 - 1s - loss: 191.0731 - loglik: -1.8935e+02 - logprior: -1.3906e+00
Epoch 9/10
10/10 - 1s - loss: 190.3131 - loglik: -1.8874e+02 - logprior: -1.2567e+00
Epoch 10/10
10/10 - 1s - loss: 189.9489 - loglik: -1.8847e+02 - logprior: -1.1738e+00
Fitted a model with MAP estimate = -189.4337
expansions: [(0, 3), (6, 1), (22, 1), (28, 2), (29, 2), (30, 2), (40, 2), (43, 1), (57, 1), (65, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.0030 - loglik: -1.9902e+02 - logprior: -4.9662e+01
Epoch 2/2
10/10 - 1s - loss: 201.6213 - loglik: -1.8626e+02 - logprior: -1.4915e+01
Fitted a model with MAP estimate = -192.7276
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 229.3487 - loglik: -1.8614e+02 - logprior: -4.2548e+01
Epoch 2/2
10/10 - 1s - loss: 201.5292 - loglik: -1.8411e+02 - logprior: -1.6613e+01
Fitted a model with MAP estimate = -196.2010
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 222.7484 - loglik: -1.8373e+02 - logprior: -3.8114e+01
Epoch 2/10
10/10 - 1s - loss: 192.8039 - loglik: -1.8200e+02 - logprior: -9.9824e+00
Epoch 3/10
10/10 - 1s - loss: 185.9857 - loglik: -1.8161e+02 - logprior: -3.7024e+00
Epoch 4/10
10/10 - 1s - loss: 183.5055 - loglik: -1.8148e+02 - logprior: -1.4644e+00
Epoch 5/10
10/10 - 1s - loss: 181.8443 - loglik: -1.8102e+02 - logprior: -3.5082e-01
Epoch 6/10
10/10 - 1s - loss: 181.3104 - loglik: -1.8112e+02 - logprior: 0.2253
Epoch 7/10
10/10 - 1s - loss: 180.5999 - loglik: -1.8077e+02 - logprior: 0.5369
Epoch 8/10
10/10 - 1s - loss: 180.2001 - loglik: -1.8059e+02 - logprior: 0.7295
Epoch 9/10
10/10 - 1s - loss: 179.8852 - loglik: -1.8043e+02 - logprior: 0.8737
Epoch 10/10
10/10 - 1s - loss: 179.7155 - loglik: -1.8039e+02 - logprior: 1.0080
Fitted a model with MAP estimate = -179.1448
Time for alignment: 46.9673
Computed alignments with likelihoods: ['-179.8176', '-179.5154', '-179.1448']
Best model has likelihood: -179.1448
time for generating output: 0.2895
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.7801566579634465
Training of 3 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f196a04b5b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1877bd9280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18a23f1280>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f199d164ca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f199d1647f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f186f61ecd0>, <__main__.SimpleDirichletPrior object at 0x7f19485983d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe47c700>

Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 950.3030 - loglik: -9.4825e+02 - logprior: -1.4302e+00
Epoch 2/10
43/43 - 31s - loss: 835.3627 - loglik: -8.3148e+02 - logprior: -1.9872e+00
Epoch 3/10
43/43 - 31s - loss: 819.6653 - loglik: -8.1540e+02 - logprior: -2.1110e+00
Epoch 4/10
43/43 - 31s - loss: 815.3479 - loglik: -8.1125e+02 - logprior: -2.0825e+00
Epoch 5/10
43/43 - 31s - loss: 811.7615 - loglik: -8.0788e+02 - logprior: -2.1681e+00
Epoch 6/10
43/43 - 31s - loss: 811.4767 - loglik: -8.0772e+02 - logprior: -2.1756e+00
Epoch 7/10
43/43 - 31s - loss: 810.4358 - loglik: -8.0676e+02 - logprior: -2.1785e+00
Epoch 8/10
43/43 - 31s - loss: 810.3119 - loglik: -8.0671e+02 - logprior: -2.1956e+00
Epoch 9/10
43/43 - 31s - loss: 809.6442 - loglik: -8.0615e+02 - logprior: -2.1909e+00
Epoch 10/10
43/43 - 32s - loss: 809.5369 - loglik: -8.0611e+02 - logprior: -2.2081e+00
Fitted a model with MAP estimate = -819.9953
expansions: [(8, 1), (13, 1), (16, 1), (21, 3), (22, 2), (23, 1), (24, 1), (30, 1), (40, 1), (42, 2), (43, 2), (45, 2), (46, 1), (56, 1), (59, 2), (61, 1), (62, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 2), (93, 1), (95, 1), (97, 1), (98, 1), (103, 2), (119, 1), (121, 1), (123, 1), (124, 1), (130, 2), (132, 2), (148, 2), (149, 1), (154, 1), (155, 3), (157, 2), (181, 1), (183, 3), (184, 1), (186, 1), (187, 1), (188, 2), (203, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (225, 1), (226, 2), (239, 4), (242, 1), (245, 1), (250, 1), (261, 2), (262, 1), (269, 2), (270, 2), (271, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 372 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 53s - loss: 809.9976 - loglik: -8.0521e+02 - logprior: -2.7219e+00
Epoch 2/2
43/43 - 47s - loss: 784.2909 - loglik: -7.7976e+02 - logprior: -1.5891e+00
Fitted a model with MAP estimate = -777.0388
expansions: [(0, 2), (296, 1)]
discards: [  0  27  28  55  79 112 120 138 172 175 204 209 239 249 269 270 271 301
 317 359 360 362]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 48s - loss: 788.5496 - loglik: -7.8310e+02 - logprior: -1.6827e+00
Epoch 2/2
43/43 - 44s - loss: 782.7634 - loglik: -7.7806e+02 - logprior: -8.7354e-01
Fitted a model with MAP estimate = -774.1812
expansions: [(257, 2), (343, 1)]
discards: [ 0  1 58]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 64s - loss: 765.6545 - loglik: -7.5945e+02 - logprior: -1.1999e+00
Epoch 2/10
61/61 - 61s - loss: 757.2113 - loglik: -7.5105e+02 - logprior: -5.9143e-01
Epoch 3/10
61/61 - 61s - loss: 755.3897 - loglik: -7.5104e+02 - logprior: -4.8343e-01
Epoch 4/10
61/61 - 60s - loss: 754.9337 - loglik: -7.5097e+02 - logprior: -3.8139e-01
Epoch 5/10
61/61 - 61s - loss: 751.6068 - loglik: -7.4772e+02 - logprior: -3.0550e-01
Epoch 6/10
61/61 - 61s - loss: 750.3947 - loglik: -7.4710e+02 - logprior: -2.2522e-01
Epoch 7/10
61/61 - 60s - loss: 748.7045 - loglik: -7.4564e+02 - logprior: -1.1905e-01
Epoch 8/10
61/61 - 61s - loss: 749.6251 - loglik: -7.4675e+02 - logprior: -5.2528e-02
Fitted a model with MAP estimate = -745.0649
Time for alignment: 1307.6947
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 36s - loss: 951.2845 - loglik: -9.4923e+02 - logprior: -1.4254e+00
Epoch 2/10
43/43 - 31s - loss: 832.6938 - loglik: -8.2856e+02 - logprior: -2.1625e+00
Epoch 3/10
43/43 - 31s - loss: 819.1345 - loglik: -8.1480e+02 - logprior: -2.2056e+00
Epoch 4/10
43/43 - 31s - loss: 815.7759 - loglik: -8.1172e+02 - logprior: -2.1733e+00
Epoch 5/10
43/43 - 32s - loss: 813.2277 - loglik: -8.0929e+02 - logprior: -2.1778e+00
Epoch 6/10
43/43 - 31s - loss: 814.1536 - loglik: -8.1038e+02 - logprior: -2.2020e+00
Fitted a model with MAP estimate = -813.3369
expansions: [(7, 2), (16, 1), (21, 3), (22, 1), (23, 1), (24, 2), (29, 1), (36, 1), (39, 1), (41, 1), (42, 1), (45, 2), (46, 1), (56, 1), (59, 1), (60, 1), (61, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (97, 1), (98, 1), (103, 2), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (142, 1), (148, 2), (149, 1), (154, 1), (155, 3), (157, 2), (168, 1), (180, 1), (183, 1), (184, 2), (186, 1), (187, 1), (188, 1), (196, 1), (202, 1), (206, 1), (207, 1), (208, 1), (209, 1), (219, 1), (221, 1), (225, 1), (226, 2), (239, 3), (242, 1), (245, 2), (248, 1), (249, 2), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 50s - loss: 804.9583 - loglik: -8.0001e+02 - logprior: -2.6378e+00
Epoch 2/2
43/43 - 46s - loss: 783.0828 - loglik: -7.7852e+02 - logprior: -1.4889e+00
Fitted a model with MAP estimate = -774.7923
expansions: [(0, 1), (288, 1)]
discards: [  0  25  59 110 135 170 200 205 237 293 318 325 352 355]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 788.2107 - loglik: -7.8279e+02 - logprior: -1.5513e+00
Epoch 2/2
43/43 - 44s - loss: 781.2719 - loglik: -7.7653e+02 - logprior: -8.8551e-01
Fitted a model with MAP estimate = -774.4098
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 65s - loss: 763.5175 - loglik: -7.5750e+02 - logprior: -9.8207e-01
Epoch 2/10
61/61 - 61s - loss: 759.5035 - loglik: -7.5321e+02 - logprior: -7.5098e-01
Epoch 3/10
61/61 - 61s - loss: 755.1779 - loglik: -7.5073e+02 - logprior: -6.3743e-01
Epoch 4/10
61/61 - 60s - loss: 754.3555 - loglik: -7.5031e+02 - logprior: -5.5690e-01
Epoch 5/10
61/61 - 61s - loss: 753.6941 - loglik: -7.4969e+02 - logprior: -4.5691e-01
Epoch 6/10
61/61 - 61s - loss: 750.9351 - loglik: -7.4754e+02 - logprior: -3.8024e-01
Epoch 7/10
61/61 - 61s - loss: 748.9377 - loglik: -7.4577e+02 - logprior: -2.5046e-01
Epoch 8/10
61/61 - 60s - loss: 750.8008 - loglik: -7.4783e+02 - logprior: -1.8020e-01
Fitted a model with MAP estimate = -745.6030
Time for alignment: 1176.6063
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 35s - loss: 949.1786 - loglik: -9.4711e+02 - logprior: -1.4379e+00
Epoch 2/10
43/43 - 31s - loss: 831.1625 - loglik: -8.2692e+02 - logprior: -2.1451e+00
Epoch 3/10
43/43 - 31s - loss: 816.3575 - loglik: -8.1207e+02 - logprior: -2.2587e+00
Epoch 4/10
43/43 - 31s - loss: 814.1797 - loglik: -8.1013e+02 - logprior: -2.2383e+00
Epoch 5/10
43/43 - 31s - loss: 812.1607 - loglik: -8.0827e+02 - logprior: -2.2370e+00
Epoch 6/10
43/43 - 31s - loss: 809.9265 - loglik: -8.0618e+02 - logprior: -2.2452e+00
Epoch 7/10
43/43 - 32s - loss: 810.9272 - loglik: -8.0731e+02 - logprior: -2.2428e+00
Fitted a model with MAP estimate = -815.5379
expansions: [(8, 1), (13, 1), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (30, 1), (40, 1), (42, 1), (43, 1), (45, 1), (46, 2), (49, 2), (57, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (88, 1), (91, 1), (96, 1), (98, 2), (99, 1), (104, 2), (120, 1), (122, 1), (124, 1), (129, 2), (131, 1), (133, 2), (145, 1), (148, 1), (150, 1), (153, 2), (156, 2), (157, 2), (168, 1), (181, 1), (183, 1), (185, 2), (186, 1), (187, 2), (188, 2), (197, 2), (200, 1), (203, 1), (204, 1), (206, 1), (207, 1), (208, 1), (209, 1), (219, 1), (221, 1), (225, 1), (226, 2), (239, 4), (242, 1), (245, 1), (250, 1), (257, 1), (260, 2), (269, 2), (270, 2), (271, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 370 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 52s - loss: 807.8554 - loglik: -8.0298e+02 - logprior: -2.6933e+00
Epoch 2/2
43/43 - 47s - loss: 782.9449 - loglik: -7.7838e+02 - logprior: -1.5450e+00
Fitted a model with MAP estimate = -775.9287
expansions: [(0, 2), (294, 1)]
discards: [  0  59  64 110 127 136 166 173 199 206 242 243 244 258 267 299 315 358
 360]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 787.4041 - loglik: -7.8195e+02 - logprior: -1.6230e+00
Epoch 2/2
43/43 - 44s - loss: 781.6724 - loglik: -7.7695e+02 - logprior: -8.6974e-01
Fitted a model with MAP estimate = -775.3175
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 64s - loss: 765.9943 - loglik: -7.5981e+02 - logprior: -1.1854e+00
Epoch 2/10
61/61 - 60s - loss: 758.3976 - loglik: -7.5221e+02 - logprior: -6.1542e-01
Epoch 3/10
61/61 - 61s - loss: 757.3013 - loglik: -7.5293e+02 - logprior: -5.3460e-01
Epoch 4/10
61/61 - 61s - loss: 753.7501 - loglik: -7.4976e+02 - logprior: -4.4796e-01
Epoch 5/10
61/61 - 60s - loss: 753.6967 - loglik: -7.4978e+02 - logprior: -3.4479e-01
Epoch 6/10
61/61 - 61s - loss: 752.5989 - loglik: -7.4932e+02 - logprior: -2.7901e-01
Epoch 7/10
61/61 - 61s - loss: 750.2062 - loglik: -7.4714e+02 - logprior: -1.5305e-01
Epoch 8/10
61/61 - 60s - loss: 750.4112 - loglik: -7.4764e+02 - logprior: -6.1681e-02
Fitted a model with MAP estimate = -746.5127
Time for alignment: 1213.1837
Computed alignments with likelihoods: ['-745.0649', '-745.6030', '-746.5127']
Best model has likelihood: -745.0649
time for generating output: 0.4487
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.7984373164140524
Training of 3 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1880e8ca60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f196196c7f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18a2db26d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193eeb7430>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f193eeb7c70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f193dc21af0>, <__main__.SimpleDirichletPrior object at 0x7f1833e28fa0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19729af790>

Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 250.6057 - loglik: -1.9352e+02 - logprior: -5.7061e+01
Epoch 2/10
10/10 - 1s - loss: 188.7346 - loglik: -1.7273e+02 - logprior: -1.5971e+01
Epoch 3/10
10/10 - 1s - loss: 163.6412 - loglik: -1.5566e+02 - logprior: -7.9844e+00
Epoch 4/10
10/10 - 2s - loss: 153.0611 - loglik: -1.4801e+02 - logprior: -5.0052e+00
Epoch 5/10
10/10 - 2s - loss: 148.7496 - loglik: -1.4478e+02 - logprior: -3.7377e+00
Epoch 6/10
10/10 - 2s - loss: 147.6730 - loglik: -1.4440e+02 - logprior: -2.9147e+00
Epoch 7/10
10/10 - 2s - loss: 147.4801 - loglik: -1.4477e+02 - logprior: -2.3768e+00
Epoch 8/10
10/10 - 1s - loss: 146.4490 - loglik: -1.4401e+02 - logprior: -2.1290e+00
Epoch 9/10
10/10 - 1s - loss: 145.8132 - loglik: -1.4341e+02 - logprior: -2.0501e+00
Epoch 10/10
10/10 - 1s - loss: 146.1702 - loglik: -1.4380e+02 - logprior: -1.9470e+00
Fitted a model with MAP estimate = -145.2256
expansions: [(11, 1), (12, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.5932 - loglik: -1.4546e+02 - logprior: -6.4726e+01
Epoch 2/2
10/10 - 2s - loss: 169.2431 - loglik: -1.4133e+02 - logprior: -2.7522e+01
Fitted a model with MAP estimate = -163.4289
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 193.9273 - loglik: -1.4081e+02 - logprior: -5.2867e+01
Epoch 2/2
10/10 - 2s - loss: 155.8228 - loglik: -1.4066e+02 - logprior: -1.4983e+01
Fitted a model with MAP estimate = -149.7763
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.3343 - loglik: -1.4088e+02 - logprior: -5.1302e+01
Epoch 2/10
10/10 - 2s - loss: 155.0695 - loglik: -1.4018e+02 - logprior: -1.4700e+01
Epoch 3/10
10/10 - 1s - loss: 146.9117 - loglik: -1.3935e+02 - logprior: -7.2477e+00
Epoch 4/10
10/10 - 1s - loss: 144.1029 - loglik: -1.3957e+02 - logprior: -4.2057e+00
Epoch 5/10
10/10 - 2s - loss: 143.3612 - loglik: -1.4047e+02 - logprior: -2.5516e+00
Epoch 6/10
10/10 - 1s - loss: 142.3488 - loglik: -1.4035e+02 - logprior: -1.6207e+00
Epoch 7/10
10/10 - 1s - loss: 140.7526 - loglik: -1.3924e+02 - logprior: -1.0913e+00
Epoch 8/10
10/10 - 1s - loss: 140.9678 - loglik: -1.3977e+02 - logprior: -7.5295e-01
Fitted a model with MAP estimate = -140.3292
Time for alignment: 51.2741
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 250.9452 - loglik: -1.9386e+02 - logprior: -5.7062e+01
Epoch 2/10
10/10 - 2s - loss: 188.4983 - loglik: -1.7250e+02 - logprior: -1.5965e+01
Epoch 3/10
10/10 - 2s - loss: 162.9093 - loglik: -1.5492e+02 - logprior: -7.9832e+00
Epoch 4/10
10/10 - 2s - loss: 152.8799 - loglik: -1.4779e+02 - logprior: -5.0240e+00
Epoch 5/10
10/10 - 2s - loss: 148.6739 - loglik: -1.4465e+02 - logprior: -3.7529e+00
Epoch 6/10
10/10 - 2s - loss: 148.4897 - loglik: -1.4520e+02 - logprior: -2.9300e+00
Epoch 7/10
10/10 - 2s - loss: 147.0047 - loglik: -1.4430e+02 - logprior: -2.3865e+00
Epoch 8/10
10/10 - 1s - loss: 146.4587 - loglik: -1.4407e+02 - logprior: -2.0833e+00
Epoch 9/10
10/10 - 2s - loss: 145.6604 - loglik: -1.4337e+02 - logprior: -1.9646e+00
Epoch 10/10
10/10 - 2s - loss: 145.9579 - loglik: -1.4373e+02 - logprior: -1.8648e+00
Fitted a model with MAP estimate = -145.2747
expansions: [(11, 1), (12, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.1549 - loglik: -1.4512e+02 - logprior: -6.4654e+01
Epoch 2/2
10/10 - 1s - loss: 169.5565 - loglik: -1.4169e+02 - logprior: -2.7488e+01
Fitted a model with MAP estimate = -163.3872
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 193.5387 - loglik: -1.4042e+02 - logprior: -5.2855e+01
Epoch 2/2
10/10 - 2s - loss: 155.7697 - loglik: -1.4062e+02 - logprior: -1.4969e+01
Fitted a model with MAP estimate = -149.7435
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 204.4091 - loglik: -1.4161e+02 - logprior: -6.2640e+01
Epoch 2/10
10/10 - 2s - loss: 162.4631 - loglik: -1.4082e+02 - logprior: -2.1458e+01
Epoch 3/10
10/10 - 1s - loss: 149.5028 - loglik: -1.4042e+02 - logprior: -8.7672e+00
Epoch 4/10
10/10 - 1s - loss: 145.8628 - loglik: -1.4118e+02 - logprior: -4.3523e+00
Epoch 5/10
10/10 - 2s - loss: 143.6341 - loglik: -1.4074e+02 - logprior: -2.5570e+00
Epoch 6/10
10/10 - 2s - loss: 142.5384 - loglik: -1.4046e+02 - logprior: -1.7022e+00
Epoch 7/10
10/10 - 2s - loss: 142.4504 - loglik: -1.4078e+02 - logprior: -1.2570e+00
Epoch 8/10
10/10 - 1s - loss: 141.4588 - loglik: -1.4007e+02 - logprior: -9.4489e-01
Epoch 9/10
10/10 - 2s - loss: 141.3901 - loglik: -1.4025e+02 - logprior: -6.8094e-01
Epoch 10/10
10/10 - 2s - loss: 141.4489 - loglik: -1.4050e+02 - logprior: -4.7044e-01
Fitted a model with MAP estimate = -140.6020
Time for alignment: 54.5141
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.8372 - loglik: -1.9376e+02 - logprior: -5.7059e+01
Epoch 2/10
10/10 - 2s - loss: 187.6534 - loglik: -1.7167e+02 - logprior: -1.5956e+01
Epoch 3/10
10/10 - 1s - loss: 162.6440 - loglik: -1.5467e+02 - logprior: -7.9723e+00
Epoch 4/10
10/10 - 1s - loss: 153.2008 - loglik: -1.4819e+02 - logprior: -4.9933e+00
Epoch 5/10
10/10 - 2s - loss: 149.3163 - loglik: -1.4538e+02 - logprior: -3.7518e+00
Epoch 6/10
10/10 - 2s - loss: 148.1552 - loglik: -1.4482e+02 - logprior: -2.9418e+00
Epoch 7/10
10/10 - 2s - loss: 146.7765 - loglik: -1.4399e+02 - logprior: -2.3937e+00
Epoch 8/10
10/10 - 2s - loss: 146.6313 - loglik: -1.4417e+02 - logprior: -2.1225e+00
Epoch 9/10
10/10 - 2s - loss: 146.6224 - loglik: -1.4428e+02 - logprior: -1.9850e+00
Epoch 10/10
10/10 - 2s - loss: 146.3436 - loglik: -1.4412e+02 - logprior: -1.8368e+00
Fitted a model with MAP estimate = -145.6531
expansions: [(11, 1), (12, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 208.8715 - loglik: -1.4388e+02 - logprior: -6.4651e+01
Epoch 2/2
10/10 - 2s - loss: 170.1938 - loglik: -1.4242e+02 - logprior: -2.7462e+01
Fitted a model with MAP estimate = -163.3003
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 194.4936 - loglik: -1.4141e+02 - logprior: -5.2822e+01
Epoch 2/2
10/10 - 2s - loss: 154.5645 - loglik: -1.3944e+02 - logprior: -1.4941e+01
Fitted a model with MAP estimate = -149.6255
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 204.5590 - loglik: -1.4168e+02 - logprior: -6.2710e+01
Epoch 2/10
10/10 - 1s - loss: 162.6237 - loglik: -1.4069e+02 - logprior: -2.1740e+01
Epoch 3/10
10/10 - 1s - loss: 149.9862 - loglik: -1.4077e+02 - logprior: -8.8988e+00
Epoch 4/10
10/10 - 1s - loss: 145.0429 - loglik: -1.4035e+02 - logprior: -4.3717e+00
Epoch 5/10
10/10 - 2s - loss: 144.4393 - loglik: -1.4154e+02 - logprior: -2.5642e+00
Epoch 6/10
10/10 - 1s - loss: 142.3800 - loglik: -1.4030e+02 - logprior: -1.7015e+00
Epoch 7/10
10/10 - 1s - loss: 142.1812 - loglik: -1.4052e+02 - logprior: -1.2495e+00
Epoch 8/10
10/10 - 1s - loss: 141.8153 - loglik: -1.4043e+02 - logprior: -9.4090e-01
Epoch 9/10
10/10 - 1s - loss: 141.2233 - loglik: -1.4008e+02 - logprior: -6.9005e-01
Epoch 10/10
10/10 - 1s - loss: 141.2595 - loglik: -1.4029e+02 - logprior: -5.0332e-01
Fitted a model with MAP estimate = -140.5579
Time for alignment: 53.8658
Computed alignments with likelihoods: ['-140.3292', '-140.6020', '-140.5579']
Best model has likelihood: -140.3292
time for generating output: 0.1448
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.9472954230235784
Training of 3 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f193d494790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1899b22c40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19470858b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18785b1b50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18785b1190>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fd6e25e0>, <__main__.SimpleDirichletPrior object at 0x7f19a5906b80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe9ec5e0>

Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 540.3561 - loglik: -4.7213e+02 - logprior: -6.8216e+01
Epoch 2/10
10/10 - 2s - loss: 440.6984 - loglik: -4.2596e+02 - logprior: -1.4731e+01
Epoch 3/10
10/10 - 2s - loss: 387.4507 - loglik: -3.8163e+02 - logprior: -5.8187e+00
Epoch 4/10
10/10 - 2s - loss: 358.3617 - loglik: -3.5538e+02 - logprior: -2.9763e+00
Epoch 5/10
10/10 - 2s - loss: 343.9857 - loglik: -3.4198e+02 - logprior: -1.8565e+00
Epoch 6/10
10/10 - 2s - loss: 337.1181 - loglik: -3.3555e+02 - logprior: -1.1715e+00
Epoch 7/10
10/10 - 2s - loss: 333.1504 - loglik: -3.3218e+02 - logprior: -5.3242e-01
Epoch 8/10
10/10 - 2s - loss: 330.4865 - loglik: -3.2999e+02 - logprior: -1.1052e-01
Epoch 9/10
10/10 - 2s - loss: 328.9067 - loglik: -3.2867e+02 - logprior: 0.1534
Epoch 10/10
10/10 - 2s - loss: 327.9380 - loglik: -3.2783e+02 - logprior: 0.3434
Fitted a model with MAP estimate = -327.1629
expansions: [(11, 3), (13, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (51, 1), (69, 2), (77, 3), (79, 2), (80, 2), (89, 1), (90, 2), (101, 1), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 408.7798 - loglik: -3.3077e+02 - logprior: -7.7545e+01
Epoch 2/2
10/10 - 3s - loss: 342.8480 - loglik: -3.1272e+02 - logprior: -2.9661e+01
Fitted a model with MAP estimate = -330.6296
expansions: [(0, 2), (60, 1)]
discards: [  0  11  81 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 370.6352 - loglik: -3.0966e+02 - logprior: -6.0573e+01
Epoch 2/2
10/10 - 3s - loss: 316.7683 - loglik: -3.0396e+02 - logprior: -1.2513e+01
Fitted a model with MAP estimate = -307.9608
expansions: [(127, 1)]
discards: [  0  96 160]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 384.0725 - loglik: -3.0960e+02 - logprior: -7.4189e+01
Epoch 2/10
10/10 - 3s - loss: 326.4338 - loglik: -3.0519e+02 - logprior: -2.0963e+01
Epoch 3/10
10/10 - 3s - loss: 306.8986 - loglik: -3.0214e+02 - logprior: -4.4130e+00
Epoch 4/10
10/10 - 3s - loss: 298.7002 - loglik: -2.9987e+02 - logprior: 1.5957
Epoch 5/10
10/10 - 3s - loss: 295.4448 - loglik: -2.9920e+02 - logprior: 4.2219
Epoch 6/10
10/10 - 3s - loss: 293.5466 - loglik: -2.9876e+02 - logprior: 5.6956
Epoch 7/10
10/10 - 3s - loss: 292.3810 - loglik: -2.9856e+02 - logprior: 6.6730
Epoch 8/10
10/10 - 3s - loss: 291.6537 - loglik: -2.9863e+02 - logprior: 7.4644
Epoch 9/10
10/10 - 3s - loss: 290.5926 - loglik: -2.9826e+02 - logprior: 8.1720
Epoch 10/10
10/10 - 3s - loss: 290.6968 - loglik: -2.9898e+02 - logprior: 8.7866
Fitted a model with MAP estimate = -289.6015
Time for alignment: 89.1000
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 540.3730 - loglik: -4.7215e+02 - logprior: -6.8213e+01
Epoch 2/10
10/10 - 2s - loss: 441.1190 - loglik: -4.2638e+02 - logprior: -1.4735e+01
Epoch 3/10
10/10 - 2s - loss: 388.6823 - loglik: -3.8286e+02 - logprior: -5.8203e+00
Epoch 4/10
10/10 - 2s - loss: 358.3812 - loglik: -3.5534e+02 - logprior: -3.0339e+00
Epoch 5/10
10/10 - 2s - loss: 342.8434 - loglik: -3.4076e+02 - logprior: -1.9608e+00
Epoch 6/10
10/10 - 2s - loss: 334.3521 - loglik: -3.3272e+02 - logprior: -1.2625e+00
Epoch 7/10
10/10 - 2s - loss: 330.3246 - loglik: -3.2927e+02 - logprior: -6.2305e-01
Epoch 8/10
10/10 - 2s - loss: 328.3138 - loglik: -3.2775e+02 - logprior: -1.7671e-01
Epoch 9/10
10/10 - 2s - loss: 327.3648 - loglik: -3.2704e+02 - logprior: 0.0616
Epoch 10/10
10/10 - 2s - loss: 326.0330 - loglik: -3.2577e+02 - logprior: 0.1850
Fitted a model with MAP estimate = -325.2797
expansions: [(11, 3), (12, 1), (16, 2), (19, 2), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (51, 1), (55, 1), (66, 2), (77, 3), (78, 2), (80, 2), (89, 1), (90, 2), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (127, 1), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 406.6874 - loglik: -3.2874e+02 - logprior: -7.7480e+01
Epoch 2/2
10/10 - 3s - loss: 338.5261 - loglik: -3.0839e+02 - logprior: -2.9668e+01
Fitted a model with MAP estimate = -326.4359
expansions: [(0, 1), (19, 1), (20, 1), (62, 1), (130, 1)]
discards: [  0  11  24  25  51 116]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 367.7715 - loglik: -3.0682e+02 - logprior: -6.0554e+01
Epoch 2/2
10/10 - 3s - loss: 312.1925 - loglik: -2.9925e+02 - logprior: -1.2637e+01
Fitted a model with MAP estimate = -302.5872
expansions: [(25, 1), (45, 1), (127, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 176 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 358.8174 - loglik: -2.9957e+02 - logprior: -5.8953e+01
Epoch 2/10
10/10 - 3s - loss: 307.1870 - loglik: -2.9567e+02 - logprior: -1.1240e+01
Epoch 3/10
10/10 - 3s - loss: 294.6447 - loglik: -2.9275e+02 - logprior: -1.5402e+00
Epoch 4/10
10/10 - 3s - loss: 289.3958 - loglik: -2.9157e+02 - logprior: 2.6161
Epoch 5/10
10/10 - 3s - loss: 286.6274 - loglik: -2.9122e+02 - logprior: 5.0645
Epoch 6/10
10/10 - 3s - loss: 284.8127 - loglik: -2.9093e+02 - logprior: 6.5966
Epoch 7/10
10/10 - 3s - loss: 283.7254 - loglik: -2.9087e+02 - logprior: 7.6258
Epoch 8/10
10/10 - 3s - loss: 283.0429 - loglik: -2.9097e+02 - logprior: 8.4115
Epoch 9/10
10/10 - 3s - loss: 282.3152 - loglik: -2.9084e+02 - logprior: 9.0169
Epoch 10/10
10/10 - 3s - loss: 281.7455 - loglik: -2.9075e+02 - logprior: 9.5005
Fitted a model with MAP estimate = -280.7697
Time for alignment: 91.0232
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 540.0548 - loglik: -4.7183e+02 - logprior: -6.8214e+01
Epoch 2/10
10/10 - 2s - loss: 440.8760 - loglik: -4.2614e+02 - logprior: -1.4735e+01
Epoch 3/10
10/10 - 2s - loss: 387.6373 - loglik: -3.8184e+02 - logprior: -5.7912e+00
Epoch 4/10
10/10 - 2s - loss: 356.4250 - loglik: -3.5349e+02 - logprior: -2.9220e+00
Epoch 5/10
10/10 - 2s - loss: 341.4152 - loglik: -3.3934e+02 - logprior: -1.9027e+00
Epoch 6/10
10/10 - 2s - loss: 333.9300 - loglik: -3.3233e+02 - logprior: -1.1503e+00
Epoch 7/10
10/10 - 2s - loss: 330.2094 - loglik: -3.2918e+02 - logprior: -5.0988e-01
Epoch 8/10
10/10 - 2s - loss: 327.4069 - loglik: -3.2676e+02 - logprior: -1.7572e-01
Epoch 9/10
10/10 - 2s - loss: 325.9885 - loglik: -3.2562e+02 - logprior: 0.0906
Epoch 10/10
10/10 - 2s - loss: 325.2506 - loglik: -3.2503e+02 - logprior: 0.2670
Fitted a model with MAP estimate = -323.1838
expansions: [(11, 3), (13, 1), (19, 2), (26, 1), (27, 1), (28, 1), (38, 1), (40, 2), (51, 1), (55, 1), (66, 2), (71, 2), (77, 2), (79, 2), (80, 1), (89, 1), (90, 2), (103, 2), (110, 2), (114, 2), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 404.3282 - loglik: -3.2636e+02 - logprior: -7.7492e+01
Epoch 2/2
10/10 - 3s - loss: 337.0607 - loglik: -3.0666e+02 - logprior: -2.9937e+01
Fitted a model with MAP estimate = -325.0849
expansions: [(0, 2), (49, 1), (60, 1)]
discards: [  0  11  23  46  86 114 128]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 366.4996 - loglik: -3.0552e+02 - logprior: -6.0597e+01
Epoch 2/2
10/10 - 3s - loss: 312.1995 - loglik: -2.9934e+02 - logprior: -1.2588e+01
Fitted a model with MAP estimate = -303.1349
expansions: []
discards: [  0 160]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 377.5438 - loglik: -3.0378e+02 - logprior: -7.3502e+01
Epoch 2/10
10/10 - 3s - loss: 320.0807 - loglik: -3.0029e+02 - logprior: -1.9529e+01
Epoch 3/10
10/10 - 3s - loss: 301.9863 - loglik: -2.9781e+02 - logprior: -3.8473e+00
Epoch 4/10
10/10 - 3s - loss: 294.8339 - loglik: -2.9607e+02 - logprior: 1.6387
Epoch 5/10
10/10 - 3s - loss: 291.0662 - loglik: -2.9485e+02 - logprior: 4.2113
Epoch 6/10
10/10 - 3s - loss: 289.8773 - loglik: -2.9511e+02 - logprior: 5.6722
Epoch 7/10
10/10 - 3s - loss: 288.4334 - loglik: -2.9463e+02 - logprior: 6.6536
Epoch 8/10
10/10 - 3s - loss: 287.8007 - loglik: -2.9479e+02 - logprior: 7.4522
Epoch 9/10
10/10 - 3s - loss: 287.1312 - loglik: -2.9482e+02 - logprior: 8.1526
Epoch 10/10
10/10 - 3s - loss: 286.5057 - loglik: -2.9480e+02 - logprior: 8.7688
Fitted a model with MAP estimate = -285.7708
Time for alignment: 88.7184
Computed alignments with likelihoods: ['-289.6015', '-280.7697', '-285.7708']
Best model has likelihood: -280.7697
time for generating output: 0.2280
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.7799132052076876
Training of 3 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f193dec5940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18b360ae80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fe32cdf0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18a2e64940>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18a2e82f70>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18a2a84430>, <__main__.SimpleDirichletPrior object at 0x7f1880a429d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19bf74b280>

Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 292.7793 - loglik: -2.8272e+02 - logprior: -1.0050e+01
Epoch 2/10
12/12 - 2s - loss: 254.2710 - loglik: -2.5168e+02 - logprior: -2.5398e+00
Epoch 3/10
12/12 - 2s - loss: 226.7890 - loglik: -2.2463e+02 - logprior: -1.8473e+00
Epoch 4/10
12/12 - 2s - loss: 215.4771 - loglik: -2.1296e+02 - logprior: -1.9010e+00
Epoch 5/10
12/12 - 2s - loss: 210.2709 - loglik: -2.0748e+02 - logprior: -1.9697e+00
Epoch 6/10
12/12 - 2s - loss: 207.9585 - loglik: -2.0540e+02 - logprior: -1.8863e+00
Epoch 7/10
12/12 - 2s - loss: 206.6972 - loglik: -2.0442e+02 - logprior: -1.8153e+00
Epoch 8/10
12/12 - 2s - loss: 206.4924 - loglik: -2.0425e+02 - logprior: -1.8214e+00
Epoch 9/10
12/12 - 2s - loss: 206.1914 - loglik: -2.0399e+02 - logprior: -1.8312e+00
Epoch 10/10
12/12 - 2s - loss: 205.4524 - loglik: -2.0327e+02 - logprior: -1.8256e+00
Fitted a model with MAP estimate = -205.2386
expansions: [(6, 3), (10, 3), (13, 1), (21, 1), (36, 4), (49, 1), (50, 3), (52, 1), (59, 6), (61, 1), (64, 1), (70, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 219.4559 - loglik: -2.0731e+02 - logprior: -1.1698e+01
Epoch 2/2
12/12 - 2s - loss: 200.7526 - loglik: -1.9513e+02 - logprior: -5.0291e+00
Fitted a model with MAP estimate = -195.3727
expansions: [(0, 3), (45, 1)]
discards: [ 0 76 77]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 202.8403 - loglik: -1.9305e+02 - logprior: -9.2798e+00
Epoch 2/2
12/12 - 2s - loss: 191.2725 - loglik: -1.8824e+02 - logprior: -2.5871e+00
Fitted a model with MAP estimate = -188.1010
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 203.5843 - loglik: -1.9166e+02 - logprior: -1.1472e+01
Epoch 2/10
12/12 - 2s - loss: 193.2909 - loglik: -1.8878e+02 - logprior: -4.0695e+00
Epoch 3/10
12/12 - 2s - loss: 188.3241 - loglik: -1.8554e+02 - logprior: -2.2119e+00
Epoch 4/10
12/12 - 2s - loss: 185.5435 - loglik: -1.8331e+02 - logprior: -1.5494e+00
Epoch 5/10
12/12 - 2s - loss: 184.2554 - loglik: -1.8223e+02 - logprior: -1.3769e+00
Epoch 6/10
12/12 - 2s - loss: 183.3321 - loglik: -1.8144e+02 - logprior: -1.2921e+00
Epoch 7/10
12/12 - 2s - loss: 182.6278 - loglik: -1.8084e+02 - logprior: -1.2550e+00
Epoch 8/10
12/12 - 2s - loss: 181.5046 - loglik: -1.7980e+02 - logprior: -1.2246e+00
Epoch 9/10
12/12 - 2s - loss: 182.4151 - loglik: -1.8077e+02 - logprior: -1.1923e+00
Fitted a model with MAP estimate = -181.3261
Time for alignment: 66.7380
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 292.7917 - loglik: -2.8272e+02 - logprior: -1.0055e+01
Epoch 2/10
12/12 - 2s - loss: 253.4951 - loglik: -2.5091e+02 - logprior: -2.5387e+00
Epoch 3/10
12/12 - 2s - loss: 225.0068 - loglik: -2.2290e+02 - logprior: -1.8454e+00
Epoch 4/10
12/12 - 2s - loss: 214.2192 - loglik: -2.1156e+02 - logprior: -1.9297e+00
Epoch 5/10
12/12 - 2s - loss: 210.1781 - loglik: -2.0723e+02 - logprior: -1.9877e+00
Epoch 6/10
12/12 - 2s - loss: 208.2702 - loglik: -2.0560e+02 - logprior: -1.9364e+00
Epoch 7/10
12/12 - 2s - loss: 206.2990 - loglik: -2.0385e+02 - logprior: -1.8769e+00
Epoch 8/10
12/12 - 2s - loss: 206.5678 - loglik: -2.0421e+02 - logprior: -1.8841e+00
Fitted a model with MAP estimate = -205.6917
expansions: [(6, 3), (10, 3), (13, 1), (21, 1), (29, 1), (36, 4), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 220.6559 - loglik: -2.0841e+02 - logprior: -1.1712e+01
Epoch 2/2
12/12 - 2s - loss: 199.8592 - loglik: -1.9413e+02 - logprior: -5.0566e+00
Fitted a model with MAP estimate = -194.9021
expansions: [(0, 3), (46, 1)]
discards: [ 0 62 80 81 82]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 203.4753 - loglik: -1.9359e+02 - logprior: -9.2786e+00
Epoch 2/2
12/12 - 2s - loss: 191.1531 - loglik: -1.8797e+02 - logprior: -2.6113e+00
Fitted a model with MAP estimate = -188.1397
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 203.7312 - loglik: -1.9170e+02 - logprior: -1.1511e+01
Epoch 2/10
12/12 - 2s - loss: 193.4291 - loglik: -1.8875e+02 - logprior: -4.1532e+00
Epoch 3/10
12/12 - 2s - loss: 188.0031 - loglik: -1.8517e+02 - logprior: -2.2033e+00
Epoch 4/10
12/12 - 2s - loss: 186.0053 - loglik: -1.8385e+02 - logprior: -1.4778e+00
Epoch 5/10
12/12 - 2s - loss: 184.5302 - loglik: -1.8257e+02 - logprior: -1.3098e+00
Epoch 6/10
12/12 - 2s - loss: 182.6777 - loglik: -1.8083e+02 - logprior: -1.2677e+00
Epoch 7/10
12/12 - 2s - loss: 182.8539 - loglik: -1.8107e+02 - logprior: -1.2445e+00
Fitted a model with MAP estimate = -181.3887
Time for alignment: 59.5082
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 292.8677 - loglik: -2.8280e+02 - logprior: -1.0052e+01
Epoch 2/10
12/12 - 2s - loss: 253.5388 - loglik: -2.5095e+02 - logprior: -2.5354e+00
Epoch 3/10
12/12 - 2s - loss: 225.7146 - loglik: -2.2360e+02 - logprior: -1.8446e+00
Epoch 4/10
12/12 - 2s - loss: 213.3666 - loglik: -2.1068e+02 - logprior: -1.9352e+00
Epoch 5/10
12/12 - 2s - loss: 209.4999 - loglik: -2.0655e+02 - logprior: -2.0068e+00
Epoch 6/10
12/12 - 2s - loss: 207.4988 - loglik: -2.0489e+02 - logprior: -1.9186e+00
Epoch 7/10
12/12 - 2s - loss: 206.6682 - loglik: -2.0429e+02 - logprior: -1.8560e+00
Epoch 8/10
12/12 - 2s - loss: 206.0020 - loglik: -2.0367e+02 - logprior: -1.8678e+00
Epoch 9/10
12/12 - 2s - loss: 204.9870 - loglik: -2.0268e+02 - logprior: -1.8820e+00
Epoch 10/10
12/12 - 2s - loss: 205.5567 - loglik: -2.0327e+02 - logprior: -1.8750e+00
Fitted a model with MAP estimate = -204.6774
expansions: [(6, 3), (10, 3), (13, 1), (21, 1), (32, 1), (36, 4), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 220.4377 - loglik: -2.0822e+02 - logprior: -1.1728e+01
Epoch 2/2
12/12 - 2s - loss: 198.7767 - loglik: -1.9300e+02 - logprior: -5.1104e+00
Fitted a model with MAP estimate = -194.2379
expansions: [(0, 3), (46, 1)]
discards: [ 0 62 80 81 82]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 202.4648 - loglik: -1.9254e+02 - logprior: -9.2938e+00
Epoch 2/2
12/12 - 2s - loss: 189.7960 - loglik: -1.8666e+02 - logprior: -2.6072e+00
Fitted a model with MAP estimate = -186.9833
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 202.9002 - loglik: -1.9088e+02 - logprior: -1.1533e+01
Epoch 2/10
12/12 - 2s - loss: 191.6952 - loglik: -1.8697e+02 - logprior: -4.2404e+00
Epoch 3/10
12/12 - 2s - loss: 187.2951 - loglik: -1.8438e+02 - logprior: -2.2860e+00
Epoch 4/10
12/12 - 2s - loss: 184.6959 - loglik: -1.8244e+02 - logprior: -1.5593e+00
Epoch 5/10
12/12 - 2s - loss: 182.3302 - loglik: -1.8031e+02 - logprior: -1.3618e+00
Epoch 6/10
12/12 - 2s - loss: 182.3676 - loglik: -1.8048e+02 - logprior: -1.3012e+00
Fitted a model with MAP estimate = -180.9713
Time for alignment: 58.3101
Computed alignments with likelihoods: ['-181.3261', '-181.3887', '-180.9713']
Best model has likelihood: -180.9713
time for generating output: 0.2336
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.9088799518446466
Training of 3 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f18b4776520>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1877e03b50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1877e03a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f196a6cfd30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f197b4d1ac0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f193da6a970>, <__main__.SimpleDirichletPrior object at 0x7f18b3461340>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1972f2da60>

Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 329.7339 - loglik: -2.8902e+02 - logprior: -4.0699e+01
Epoch 2/10
10/10 - 1s - loss: 277.3389 - loglik: -2.6679e+02 - logprior: -1.0540e+01
Epoch 3/10
10/10 - 1s - loss: 251.4098 - loglik: -2.4675e+02 - logprior: -4.6560e+00
Epoch 4/10
10/10 - 1s - loss: 237.5466 - loglik: -2.3490e+02 - logprior: -2.5792e+00
Epoch 5/10
10/10 - 1s - loss: 231.8605 - loglik: -2.2984e+02 - logprior: -1.7310e+00
Epoch 6/10
10/10 - 1s - loss: 228.5027 - loglik: -2.2677e+02 - logprior: -1.2901e+00
Epoch 7/10
10/10 - 1s - loss: 227.0710 - loglik: -2.2564e+02 - logprior: -1.0245e+00
Epoch 8/10
10/10 - 1s - loss: 226.0785 - loglik: -2.2479e+02 - logprior: -8.9722e-01
Epoch 9/10
10/10 - 1s - loss: 224.8189 - loglik: -2.2356e+02 - logprior: -8.5113e-01
Epoch 10/10
10/10 - 1s - loss: 224.9415 - loglik: -2.2371e+02 - logprior: -8.2683e-01
Fitted a model with MAP estimate = -224.0759
expansions: [(0, 3), (6, 2), (7, 1), (8, 1), (36, 1), (37, 2), (43, 12), (53, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 275.5357 - loglik: -2.2267e+02 - logprior: -5.2494e+01
Epoch 2/2
10/10 - 1s - loss: 232.2037 - loglik: -2.1568e+02 - logprior: -1.6147e+01
Fitted a model with MAP estimate = -223.2802
expansions: []
discards: [ 0  1  2  9 58 59 60 76]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 254.9788 - loglik: -2.1656e+02 - logprior: -3.8067e+01
Epoch 2/2
10/10 - 1s - loss: 224.9491 - loglik: -2.1470e+02 - logprior: -9.8913e+00
Fitted a model with MAP estimate = -220.0122
expansions: [(0, 3), (39, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 266.1524 - loglik: -2.1502e+02 - logprior: -5.0756e+01
Epoch 2/10
10/10 - 1s - loss: 227.9904 - loglik: -2.1296e+02 - logprior: -1.4682e+01
Epoch 3/10
10/10 - 1s - loss: 218.7146 - loglik: -2.1249e+02 - logprior: -5.8108e+00
Epoch 4/10
10/10 - 1s - loss: 215.1086 - loglik: -2.1240e+02 - logprior: -2.2989e+00
Epoch 5/10
10/10 - 1s - loss: 213.2389 - loglik: -2.1217e+02 - logprior: -6.6765e-01
Epoch 6/10
10/10 - 2s - loss: 212.5508 - loglik: -2.1235e+02 - logprior: 0.1946
Epoch 7/10
10/10 - 1s - loss: 212.2636 - loglik: -2.1252e+02 - logprior: 0.6578
Epoch 8/10
10/10 - 1s - loss: 211.5222 - loglik: -2.1205e+02 - logprior: 0.9326
Epoch 9/10
10/10 - 1s - loss: 212.0312 - loglik: -2.1276e+02 - logprior: 1.1461
Fitted a model with MAP estimate = -210.9187
Time for alignment: 53.4563
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 329.5457 - loglik: -2.8883e+02 - logprior: -4.0701e+01
Epoch 2/10
10/10 - 1s - loss: 276.9476 - loglik: -2.6641e+02 - logprior: -1.0532e+01
Epoch 3/10
10/10 - 1s - loss: 250.6010 - loglik: -2.4595e+02 - logprior: -4.6473e+00
Epoch 4/10
10/10 - 1s - loss: 237.2517 - loglik: -2.3461e+02 - logprior: -2.5859e+00
Epoch 5/10
10/10 - 1s - loss: 231.4109 - loglik: -2.2939e+02 - logprior: -1.7685e+00
Epoch 6/10
10/10 - 1s - loss: 228.5195 - loglik: -2.2681e+02 - logprior: -1.3262e+00
Epoch 7/10
10/10 - 1s - loss: 227.1913 - loglik: -2.2573e+02 - logprior: -1.0868e+00
Epoch 8/10
10/10 - 1s - loss: 225.5466 - loglik: -2.2423e+02 - logprior: -9.7681e-01
Epoch 9/10
10/10 - 1s - loss: 225.1249 - loglik: -2.2389e+02 - logprior: -9.0082e-01
Epoch 10/10
10/10 - 1s - loss: 224.7659 - loglik: -2.2359e+02 - logprior: -8.2461e-01
Fitted a model with MAP estimate = -224.0333
expansions: [(0, 3), (5, 2), (7, 1), (8, 1), (36, 1), (37, 2), (43, 12), (53, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 275.3602 - loglik: -2.2245e+02 - logprior: -5.2554e+01
Epoch 2/2
10/10 - 1s - loss: 232.3960 - loglik: -2.1592e+02 - logprior: -1.6103e+01
Fitted a model with MAP estimate = -223.4207
expansions: []
discards: [ 0  1 11 58 59 60 76]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 263.8077 - loglik: -2.1712e+02 - logprior: -4.6329e+01
Epoch 2/2
10/10 - 1s - loss: 234.6363 - loglik: -2.1589e+02 - logprior: -1.8405e+01
Fitted a model with MAP estimate = -228.7486
expansions: [(0, 3), (40, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 256.6466 - loglik: -2.1479e+02 - logprior: -4.1510e+01
Epoch 2/10
10/10 - 1s - loss: 224.1053 - loglik: -2.1293e+02 - logprior: -1.0850e+01
Epoch 3/10
10/10 - 1s - loss: 217.0789 - loglik: -2.1267e+02 - logprior: -4.0476e+00
Epoch 4/10
10/10 - 1s - loss: 214.3289 - loglik: -2.1242e+02 - logprior: -1.5397e+00
Epoch 5/10
10/10 - 1s - loss: 213.0330 - loglik: -2.1240e+02 - logprior: -2.6999e-01
Epoch 6/10
10/10 - 2s - loss: 212.6039 - loglik: -2.1267e+02 - logprior: 0.4357
Epoch 7/10
10/10 - 1s - loss: 211.9920 - loglik: -2.1246e+02 - logprior: 0.8430
Epoch 8/10
10/10 - 2s - loss: 211.7907 - loglik: -2.1250e+02 - logprior: 1.0919
Epoch 9/10
10/10 - 1s - loss: 211.5320 - loglik: -2.1242e+02 - logprior: 1.2754
Epoch 10/10
10/10 - 1s - loss: 211.2868 - loglik: -2.1233e+02 - logprior: 1.4388
Fitted a model with MAP estimate = -210.7338
Time for alignment: 54.0401
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.5396 - loglik: -2.8882e+02 - logprior: -4.0701e+01
Epoch 2/10
10/10 - 1s - loss: 277.3210 - loglik: -2.6678e+02 - logprior: -1.0530e+01
Epoch 3/10
10/10 - 1s - loss: 249.9173 - loglik: -2.4526e+02 - logprior: -4.6532e+00
Epoch 4/10
10/10 - 1s - loss: 237.5599 - loglik: -2.3482e+02 - logprior: -2.6156e+00
Epoch 5/10
10/10 - 1s - loss: 231.3052 - loglik: -2.2922e+02 - logprior: -1.7457e+00
Epoch 6/10
10/10 - 1s - loss: 228.7972 - loglik: -2.2716e+02 - logprior: -1.2554e+00
Epoch 7/10
10/10 - 1s - loss: 226.9493 - loglik: -2.2554e+02 - logprior: -1.0681e+00
Epoch 8/10
10/10 - 1s - loss: 225.9041 - loglik: -2.2460e+02 - logprior: -9.6052e-01
Epoch 9/10
10/10 - 1s - loss: 225.3038 - loglik: -2.2404e+02 - logprior: -8.9350e-01
Epoch 10/10
10/10 - 1s - loss: 224.5009 - loglik: -2.2326e+02 - logprior: -8.6528e-01
Fitted a model with MAP estimate = -224.0629
expansions: [(0, 3), (6, 2), (7, 1), (8, 1), (36, 2), (37, 2), (43, 9), (53, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 276.0177 - loglik: -2.2304e+02 - logprior: -5.2610e+01
Epoch 2/2
10/10 - 1s - loss: 232.7175 - loglik: -2.1623e+02 - logprior: -1.6094e+01
Fitted a model with MAP estimate = -223.9352
expansions: []
discards: [ 0  1  2  9 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 254.0154 - loglik: -2.1550e+02 - logprior: -3.8140e+01
Epoch 2/2
10/10 - 1s - loss: 223.8101 - loglik: -2.1361e+02 - logprior: -9.8435e+00
Fitted a model with MAP estimate = -219.0393
expansions: [(0, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 265.0314 - loglik: -2.1385e+02 - logprior: -5.0833e+01
Epoch 2/10
10/10 - 1s - loss: 227.9634 - loglik: -2.1286e+02 - logprior: -1.4781e+01
Epoch 3/10
10/10 - 1s - loss: 218.2451 - loglik: -2.1196e+02 - logprior: -5.8901e+00
Epoch 4/10
10/10 - 2s - loss: 215.0121 - loglik: -2.1220e+02 - logprior: -2.4012e+00
Epoch 5/10
10/10 - 1s - loss: 213.0720 - loglik: -2.1192e+02 - logprior: -7.5524e-01
Epoch 6/10
10/10 - 1s - loss: 212.5824 - loglik: -2.1228e+02 - logprior: 0.0983
Epoch 7/10
10/10 - 1s - loss: 211.7791 - loglik: -2.1195e+02 - logprior: 0.5645
Epoch 8/10
10/10 - 1s - loss: 211.6816 - loglik: -2.1213e+02 - logprior: 0.8464
Epoch 9/10
10/10 - 1s - loss: 211.3605 - loglik: -2.1202e+02 - logprior: 1.0556
Epoch 10/10
10/10 - 1s - loss: 211.0604 - loglik: -2.1190e+02 - logprior: 1.2498
Fitted a model with MAP estimate = -210.6286
Time for alignment: 54.1186
Computed alignments with likelihoods: ['-210.9187', '-210.7338', '-210.6286']
Best model has likelihood: -210.6286
time for generating output: 0.2059
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7297290262123539
Training of 3 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fe0ad4c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f187810bc70>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198c096af0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f185f1d5430>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f185f1d5790>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1891314970>, <__main__.SimpleDirichletPrior object at 0x7f19feb20d30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f193e397a60>

Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 553.1483 - loglik: -4.7352e+02 - logprior: -7.9621e+01
Epoch 2/10
10/10 - 3s - loss: 421.3524 - loglik: -4.0427e+02 - logprior: -1.7063e+01
Epoch 3/10
10/10 - 3s - loss: 343.6089 - loglik: -3.3630e+02 - logprior: -7.2189e+00
Epoch 4/10
10/10 - 3s - loss: 296.4830 - loglik: -2.9119e+02 - logprior: -5.2417e+00
Epoch 5/10
10/10 - 3s - loss: 278.2898 - loglik: -2.7363e+02 - logprior: -4.6336e+00
Epoch 6/10
10/10 - 3s - loss: 271.8858 - loglik: -2.6808e+02 - logprior: -3.7485e+00
Epoch 7/10
10/10 - 3s - loss: 268.1430 - loglik: -2.6507e+02 - logprior: -2.8803e+00
Epoch 8/10
10/10 - 3s - loss: 266.0189 - loglik: -2.6309e+02 - logprior: -2.6057e+00
Epoch 9/10
10/10 - 3s - loss: 265.1217 - loglik: -2.6252e+02 - logprior: -2.2856e+00
Epoch 10/10
10/10 - 3s - loss: 264.3306 - loglik: -2.6227e+02 - logprior: -1.8019e+00
Fitted a model with MAP estimate = -263.7703
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 2), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (133, 1), (137, 1), (138, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 348.8829 - loglik: -2.5777e+02 - logprior: -9.0947e+01
Epoch 2/2
10/10 - 2s - loss: 271.9136 - loglik: -2.3731e+02 - logprior: -3.4501e+01
Fitted a model with MAP estimate = -259.0596
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (86, 1)]
discards: [ 43  53  93 116 120]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 301.1928 - loglik: -2.2915e+02 - logprior: -7.1880e+01
Epoch 2/2
10/10 - 2s - loss: 237.0237 - loglik: -2.2267e+02 - logprior: -1.4196e+01
Fitted a model with MAP estimate = -226.5521
expansions: [(19, 1)]
discards: [ 0  1  2 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 311.1758 - loglik: -2.2350e+02 - logprior: -8.7509e+01
Epoch 2/10
10/10 - 2s - loss: 252.2435 - loglik: -2.2220e+02 - logprior: -2.9879e+01
Epoch 3/10
10/10 - 3s - loss: 231.4506 - loglik: -2.2029e+02 - logprior: -1.1067e+01
Epoch 4/10
10/10 - 2s - loss: 217.5127 - loglik: -2.1960e+02 - logprior: 2.2548
Epoch 5/10
10/10 - 2s - loss: 212.9749 - loglik: -2.1966e+02 - logprior: 6.9452
Epoch 6/10
10/10 - 3s - loss: 210.3208 - loglik: -2.1902e+02 - logprior: 8.9663
Epoch 7/10
10/10 - 2s - loss: 208.4246 - loglik: -2.1837e+02 - logprior: 10.2084
Epoch 8/10
10/10 - 3s - loss: 209.5813 - loglik: -2.2042e+02 - logprior: 11.1191
Fitted a model with MAP estimate = -207.4701
Time for alignment: 80.7278
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 553.4378 - loglik: -4.7381e+02 - logprior: -7.9620e+01
Epoch 2/10
10/10 - 3s - loss: 420.8673 - loglik: -4.0378e+02 - logprior: -1.7062e+01
Epoch 3/10
10/10 - 3s - loss: 343.3515 - loglik: -3.3605e+02 - logprior: -7.2047e+00
Epoch 4/10
10/10 - 3s - loss: 296.2292 - loglik: -2.9100e+02 - logprior: -5.1727e+00
Epoch 5/10
10/10 - 3s - loss: 278.1139 - loglik: -2.7347e+02 - logprior: -4.6187e+00
Epoch 6/10
10/10 - 3s - loss: 270.3522 - loglik: -2.6641e+02 - logprior: -3.8459e+00
Epoch 7/10
10/10 - 3s - loss: 266.8965 - loglik: -2.6369e+02 - logprior: -2.9028e+00
Epoch 8/10
10/10 - 3s - loss: 265.5866 - loglik: -2.6274e+02 - logprior: -2.4462e+00
Epoch 9/10
10/10 - 3s - loss: 263.9733 - loglik: -2.6137e+02 - logprior: -2.2477e+00
Epoch 10/10
10/10 - 3s - loss: 263.9913 - loglik: -2.6185e+02 - logprior: -1.8414e+00
Fitted a model with MAP estimate = -263.1181
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 2), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 179 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 348.8279 - loglik: -2.5764e+02 - logprior: -9.0947e+01
Epoch 2/2
10/10 - 3s - loss: 270.9433 - loglik: -2.3625e+02 - logprior: -3.4521e+01
Fitted a model with MAP estimate = -259.0366
expansions: [(0, 3), (14, 1), (15, 4), (16, 2), (86, 1)]
discards: [  0  43  53  93 116 120]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 301.1008 - loglik: -2.2905e+02 - logprior: -7.1830e+01
Epoch 2/2
10/10 - 3s - loss: 236.0656 - loglik: -2.2189e+02 - logprior: -1.3964e+01
Fitted a model with MAP estimate = -224.8870
expansions: []
discards: [ 0  1 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 309.6252 - loglik: -2.2228e+02 - logprior: -8.7134e+01
Epoch 2/10
10/10 - 2s - loss: 249.3083 - loglik: -2.2164e+02 - logprior: -2.7472e+01
Epoch 3/10
10/10 - 2s - loss: 227.0075 - loglik: -2.2041e+02 - logprior: -6.4780e+00
Epoch 4/10
10/10 - 2s - loss: 215.7897 - loglik: -2.1908e+02 - logprior: 3.4616
Epoch 5/10
10/10 - 2s - loss: 212.6178 - loglik: -2.1943e+02 - logprior: 7.0692
Epoch 6/10
10/10 - 2s - loss: 210.3211 - loglik: -2.1906e+02 - logprior: 9.0178
Epoch 7/10
10/10 - 2s - loss: 209.1930 - loglik: -2.1917e+02 - logprior: 10.2539
Epoch 8/10
10/10 - 2s - loss: 208.5464 - loglik: -2.1941e+02 - logprior: 11.1513
Epoch 9/10
10/10 - 3s - loss: 207.0817 - loglik: -2.1867e+02 - logprior: 11.8740
Epoch 10/10
10/10 - 2s - loss: 206.8752 - loglik: -2.1914e+02 - logprior: 12.5423
Fitted a model with MAP estimate = -206.2761
Time for alignment: 86.2268
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 553.3713 - loglik: -4.7375e+02 - logprior: -7.9619e+01
Epoch 2/10
10/10 - 3s - loss: 420.6877 - loglik: -4.0361e+02 - logprior: -1.7060e+01
Epoch 3/10
10/10 - 3s - loss: 343.0147 - loglik: -3.3572e+02 - logprior: -7.2003e+00
Epoch 4/10
10/10 - 3s - loss: 296.5970 - loglik: -2.9143e+02 - logprior: -5.1101e+00
Epoch 5/10
10/10 - 3s - loss: 278.2756 - loglik: -2.7398e+02 - logprior: -4.2750e+00
Epoch 6/10
10/10 - 3s - loss: 271.1557 - loglik: -2.6744e+02 - logprior: -3.6641e+00
Epoch 7/10
10/10 - 3s - loss: 267.4755 - loglik: -2.6437e+02 - logprior: -2.9165e+00
Epoch 8/10
10/10 - 3s - loss: 265.3821 - loglik: -2.6264e+02 - logprior: -2.4261e+00
Epoch 9/10
10/10 - 3s - loss: 264.3795 - loglik: -2.6182e+02 - logprior: -2.2523e+00
Epoch 10/10
10/10 - 3s - loss: 264.3582 - loglik: -2.6215e+02 - logprior: -1.9540e+00
Fitted a model with MAP estimate = -263.1562
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 1), (58, 1), (77, 1), (78, 2), (79, 2), (90, 1), (91, 1), (98, 2), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (133, 1), (141, 1), (142, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 348.3492 - loglik: -2.5721e+02 - logprior: -9.0978e+01
Epoch 2/2
10/10 - 2s - loss: 271.1492 - loglik: -2.3663e+02 - logprior: -3.4416e+01
Fitted a model with MAP estimate = -258.7207
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (85, 1)]
discards: [  0  43  92 115 119]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 302.1799 - loglik: -2.3007e+02 - logprior: -7.1952e+01
Epoch 2/2
10/10 - 3s - loss: 236.8121 - loglik: -2.2246e+02 - logprior: -1.4187e+01
Fitted a model with MAP estimate = -226.8665
expansions: [(18, 1)]
discards: [ 0  1 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 310.9155 - loglik: -2.2354e+02 - logprior: -8.7185e+01
Epoch 2/10
10/10 - 2s - loss: 249.7430 - loglik: -2.2184e+02 - logprior: -2.7733e+01
Epoch 3/10
10/10 - 2s - loss: 226.6425 - loglik: -2.1971e+02 - logprior: -6.8323e+00
Epoch 4/10
10/10 - 2s - loss: 216.3189 - loglik: -2.1950e+02 - logprior: 3.3547
Epoch 5/10
10/10 - 2s - loss: 212.4262 - loglik: -2.1919e+02 - logprior: 7.0197
Epoch 6/10
10/10 - 2s - loss: 208.9054 - loglik: -2.1757e+02 - logprior: 8.9262
Epoch 7/10
10/10 - 2s - loss: 210.2662 - loglik: -2.2015e+02 - logprior: 10.1518
Fitted a model with MAP estimate = -208.2954
Time for alignment: 78.4206
Computed alignments with likelihoods: ['-207.4701', '-206.2761', '-208.2954']
Best model has likelihood: -206.2761
time for generating output: 0.2420
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9142376681614349
Training of 3 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f193dbca580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193d38c3d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f193d38c2b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f189a19d3a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18b3c14a30>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fe0ad4c0>, <__main__.SimpleDirichletPrior object at 0x7f1961ed58e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19beaa9310>

Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 811.9118 - loglik: -8.0940e+02 - logprior: -2.2015e+00
Epoch 2/10
33/33 - 24s - loss: 720.8261 - loglik: -7.1894e+02 - logprior: -9.8636e-01
Epoch 3/10
33/33 - 24s - loss: 706.3382 - loglik: -7.0391e+02 - logprior: -1.0341e+00
Epoch 4/10
33/33 - 24s - loss: 705.1528 - loglik: -7.0280e+02 - logprior: -1.0034e+00
Epoch 5/10
33/33 - 25s - loss: 705.4557 - loglik: -7.0316e+02 - logprior: -1.0179e+00
Fitted a model with MAP estimate = -700.4059
expansions: [(0, 4), (7, 1), (10, 1), (11, 1), (34, 10), (41, 1), (60, 1), (63, 1), (78, 1), (79, 3), (111, 1), (113, 1), (116, 1), (133, 2), (156, 6), (184, 1), (204, 1), (214, 1), (220, 2), (221, 4)]
discards: [  2   3 226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 269 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 706.9797 - loglik: -7.0181e+02 - logprior: -3.4763e+00
Epoch 2/2
33/33 - 30s - loss: 696.6304 - loglik: -6.9380e+02 - logprior: -9.4028e-01
Fitted a model with MAP estimate = -690.8938
expansions: [(0, 3), (269, 3)]
discards: [ 41  42  43  44  45  46 184 185 186 257 266]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 35s - loss: 701.6313 - loglik: -6.9590e+02 - logprior: -3.6067e+00
Epoch 2/2
33/33 - 29s - loss: 694.8249 - loglik: -6.9215e+02 - logprior: -7.4282e-01
Fitted a model with MAP estimate = -690.9785
expansions: []
discards: [  1   2   3   4 259 260 261 262 263]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 32s - loss: 702.4551 - loglik: -6.9791e+02 - logprior: -2.3380e+00
Epoch 2/10
33/33 - 28s - loss: 695.0127 - loglik: -6.9263e+02 - logprior: -3.8239e-01
Epoch 3/10
33/33 - 28s - loss: 691.4265 - loglik: -6.8940e+02 - logprior: -1.5299e-01
Epoch 4/10
33/33 - 28s - loss: 693.1630 - loglik: -6.9143e+02 - logprior: -3.7823e-02
Fitted a model with MAP estimate = -688.6181
Time for alignment: 475.0740
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 29s - loss: 814.5937 - loglik: -8.1206e+02 - logprior: -2.2094e+00
Epoch 2/10
33/33 - 24s - loss: 721.8185 - loglik: -7.1987e+02 - logprior: -9.8913e-01
Epoch 3/10
33/33 - 24s - loss: 708.1797 - loglik: -7.0568e+02 - logprior: -9.8249e-01
Epoch 4/10
33/33 - 24s - loss: 702.2742 - loglik: -6.9985e+02 - logprior: -1.0314e+00
Epoch 5/10
33/33 - 25s - loss: 701.9350 - loglik: -6.9951e+02 - logprior: -1.0624e+00
Epoch 6/10
33/33 - 24s - loss: 699.9962 - loglik: -6.9761e+02 - logprior: -1.0794e+00
Epoch 7/10
33/33 - 24s - loss: 698.9678 - loglik: -6.9663e+02 - logprior: -1.1096e+00
Epoch 8/10
33/33 - 24s - loss: 699.9504 - loglik: -6.9768e+02 - logprior: -1.1271e+00
Fitted a model with MAP estimate = -697.2592
expansions: [(0, 5), (5, 1), (7, 1), (9, 1), (33, 1), (34, 1), (44, 1), (64, 1), (71, 1), (72, 1), (73, 2), (78, 1), (109, 2), (110, 4), (114, 1), (119, 1), (133, 2), (156, 4), (164, 5), (185, 1), (205, 1), (217, 1), (224, 2), (230, 4)]
discards: [ 62 226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 271 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 34s - loss: 710.0009 - loglik: -7.0488e+02 - logprior: -3.4993e+00
Epoch 2/2
33/33 - 30s - loss: 694.0840 - loglik: -6.9103e+02 - logprior: -1.0324e+00
Fitted a model with MAP estimate = -689.8892
expansions: [(52, 1)]
discards: [  1  16 126 128 129 183 184 195 196 197 198 268 269 270]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 702.5408 - loglik: -6.9773e+02 - logprior: -2.5128e+00
Epoch 2/2
33/33 - 28s - loss: 695.4503 - loglik: -6.9272e+02 - logprior: -6.9341e-01
Fitted a model with MAP estimate = -691.6382
expansions: [(15, 1), (189, 5), (252, 2), (258, 4)]
discards: [  5   6   7 125 253 254 257]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 263 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 32s - loss: 701.2822 - loglik: -6.9668e+02 - logprior: -2.3369e+00
Epoch 2/10
33/33 - 29s - loss: 693.9890 - loglik: -6.9154e+02 - logprior: -4.4587e-01
Epoch 3/10
33/33 - 29s - loss: 692.6902 - loglik: -6.9054e+02 - logprior: -2.4006e-01
Epoch 4/10
33/33 - 29s - loss: 688.6862 - loglik: -6.8685e+02 - logprior: -1.3166e-01
Epoch 5/10
33/33 - 29s - loss: 690.5682 - loglik: -6.8893e+02 - logprior: -2.1514e-02
Fitted a model with MAP estimate = -686.0673
Time for alignment: 582.6901
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 30s - loss: 813.0137 - loglik: -8.1050e+02 - logprior: -2.2032e+00
Epoch 2/10
33/33 - 24s - loss: 719.0822 - loglik: -7.1738e+02 - logprior: -9.3566e-01
Epoch 3/10
33/33 - 24s - loss: 705.0324 - loglik: -7.0285e+02 - logprior: -9.5155e-01
Epoch 4/10
33/33 - 24s - loss: 701.1476 - loglik: -6.9892e+02 - logprior: -9.3932e-01
Epoch 5/10
33/33 - 24s - loss: 700.0732 - loglik: -6.9789e+02 - logprior: -9.1359e-01
Epoch 6/10
33/33 - 25s - loss: 702.4318 - loglik: -7.0023e+02 - logprior: -9.2332e-01
Fitted a model with MAP estimate = -698.0798
expansions: [(0, 5), (9, 1), (34, 6), (60, 1), (63, 1), (65, 1), (73, 2), (78, 2), (113, 1), (116, 1), (118, 1), (135, 1), (155, 2), (164, 4), (184, 1), (204, 1), (221, 1), (224, 2), (230, 4)]
discards: [226 227 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 265 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 33s - loss: 709.1977 - loglik: -7.0410e+02 - logprior: -3.3844e+00
Epoch 2/2
33/33 - 29s - loss: 691.2526 - loglik: -6.8832e+02 - logprior: -1.0169e+00
Fitted a model with MAP estimate = -690.6145
expansions: [(265, 4)]
discards: [  1  15  42  43 178 190 191 192 261 262 263 264]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 31s - loss: 702.3357 - loglik: -6.9763e+02 - logprior: -2.5226e+00
Epoch 2/2
33/33 - 28s - loss: 696.6836 - loglik: -6.9397e+02 - logprior: -7.3333e-01
Fitted a model with MAP estimate = -692.1427
expansions: [(253, 1)]
discards: [  5   6   7 254]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 254 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 31s - loss: 702.0212 - loglik: -6.9749e+02 - logprior: -2.2939e+00
Epoch 2/10
33/33 - 28s - loss: 697.7059 - loglik: -6.9540e+02 - logprior: -3.1927e-01
Epoch 3/10
33/33 - 28s - loss: 693.7368 - loglik: -6.9174e+02 - logprior: -1.2836e-01
Epoch 4/10
33/33 - 27s - loss: 690.8091 - loglik: -6.8911e+02 - logprior: -1.0131e-02
Epoch 5/10
33/33 - 28s - loss: 693.5280 - loglik: -6.9204e+02 - logprior: 0.1037
Fitted a model with MAP estimate = -688.9807
Time for alignment: 523.4456
Computed alignments with likelihoods: ['-688.6181', '-686.0673', '-688.9807']
Best model has likelihood: -686.0673
time for generating output: 0.4257
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.7661646863774524
Training of 3 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19487b9670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18a2f34eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18a2f34e50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fde125e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f196a5fa070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18789433a0>, <__main__.SimpleDirichletPrior object at 0x7f18a2f2bf10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19a5de40d0>

Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 411.5577 - loglik: -3.4866e+02 - logprior: -6.2883e+01
Epoch 2/10
10/10 - 2s - loss: 322.7458 - loglik: -3.0748e+02 - logprior: -1.5255e+01
Epoch 3/10
10/10 - 2s - loss: 274.8827 - loglik: -2.6771e+02 - logprior: -7.1088e+00
Epoch 4/10
10/10 - 2s - loss: 249.4289 - loglik: -2.4433e+02 - logprior: -4.8793e+00
Epoch 5/10
10/10 - 2s - loss: 238.8905 - loglik: -2.3425e+02 - logprior: -4.1701e+00
Epoch 6/10
10/10 - 2s - loss: 234.3663 - loglik: -2.3037e+02 - logprior: -3.4600e+00
Epoch 7/10
10/10 - 2s - loss: 232.7629 - loglik: -2.2962e+02 - logprior: -2.7762e+00
Epoch 8/10
10/10 - 2s - loss: 231.3927 - loglik: -2.2869e+02 - logprior: -2.4321e+00
Epoch 9/10
10/10 - 2s - loss: 230.7104 - loglik: -2.2824e+02 - logprior: -2.2079e+00
Epoch 10/10
10/10 - 2s - loss: 229.7949 - loglik: -2.2751e+02 - logprior: -2.0044e+00
Fitted a model with MAP estimate = -229.6407
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (55, 1), (60, 1), (62, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 287.1237 - loglik: -2.2989e+02 - logprior: -5.6897e+01
Epoch 2/2
10/10 - 2s - loss: 223.2690 - loglik: -2.0935e+02 - logprior: -1.3429e+01
Fitted a model with MAP estimate = -211.2150
expansions: []
discards: [  0 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 281.4266 - loglik: -2.1106e+02 - logprior: -6.9914e+01
Epoch 2/2
10/10 - 2s - loss: 233.3623 - loglik: -2.0536e+02 - logprior: -2.7627e+01
Fitted a model with MAP estimate = -225.0618
expansions: [(0, 3)]
discards: [ 0 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 264.4680 - loglik: -2.0858e+02 - logprior: -5.5533e+01
Epoch 2/10
10/10 - 2s - loss: 216.1932 - loglik: -2.0318e+02 - logprior: -1.2652e+01
Epoch 3/10
10/10 - 2s - loss: 205.4690 - loglik: -2.0098e+02 - logprior: -3.9998e+00
Epoch 4/10
10/10 - 2s - loss: 200.3797 - loglik: -1.9953e+02 - logprior: -3.4559e-01
Epoch 5/10
10/10 - 2s - loss: 197.1796 - loglik: -1.9849e+02 - logprior: 1.7509
Epoch 6/10
10/10 - 2s - loss: 195.7987 - loglik: -1.9843e+02 - logprior: 3.0081
Epoch 7/10
10/10 - 2s - loss: 195.9159 - loglik: -1.9937e+02 - logprior: 3.7954
Fitted a model with MAP estimate = -194.4040
Time for alignment: 56.1797
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 411.6053 - loglik: -3.4871e+02 - logprior: -6.2884e+01
Epoch 2/10
10/10 - 2s - loss: 321.9153 - loglik: -3.0665e+02 - logprior: -1.5261e+01
Epoch 3/10
10/10 - 2s - loss: 273.6935 - loglik: -2.6647e+02 - logprior: -7.1535e+00
Epoch 4/10
10/10 - 2s - loss: 249.4945 - loglik: -2.4435e+02 - logprior: -4.9805e+00
Epoch 5/10
10/10 - 2s - loss: 238.1429 - loglik: -2.3369e+02 - logprior: -4.2216e+00
Epoch 6/10
10/10 - 2s - loss: 234.5850 - loglik: -2.3073e+02 - logprior: -3.4502e+00
Epoch 7/10
10/10 - 2s - loss: 232.2466 - loglik: -2.2910e+02 - logprior: -2.7415e+00
Epoch 8/10
10/10 - 2s - loss: 231.7239 - loglik: -2.2898e+02 - logprior: -2.3978e+00
Epoch 9/10
10/10 - 2s - loss: 230.0610 - loglik: -2.2753e+02 - logprior: -2.1945e+00
Epoch 10/10
10/10 - 2s - loss: 229.7978 - loglik: -2.2747e+02 - logprior: -1.9914e+00
Fitted a model with MAP estimate = -229.3268
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (60, 1), (62, 1), (63, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 303.7276 - loglik: -2.3242e+02 - logprior: -7.0929e+01
Epoch 2/2
10/10 - 2s - loss: 240.2407 - loglik: -2.1201e+02 - logprior: -2.7712e+01
Fitted a model with MAP estimate = -228.1585
expansions: [(0, 3)]
discards: [  0 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 266.0356 - loglik: -2.0971e+02 - logprior: -5.5856e+01
Epoch 2/2
10/10 - 2s - loss: 217.0895 - loglik: -2.0351e+02 - logprior: -1.3200e+01
Fitted a model with MAP estimate = -208.2450
expansions: []
discards: [ 0  2 15]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 276.9208 - loglik: -2.0980e+02 - logprior: -6.6757e+01
Epoch 2/10
10/10 - 2s - loss: 224.0168 - loglik: -2.0536e+02 - logprior: -1.8266e+01
Epoch 3/10
10/10 - 2s - loss: 208.0320 - loglik: -2.0240e+02 - logprior: -5.1181e+00
Epoch 4/10
10/10 - 2s - loss: 201.8276 - loglik: -2.0073e+02 - logprior: -5.5446e-01
Epoch 5/10
10/10 - 2s - loss: 199.1421 - loglik: -2.0030e+02 - logprior: 1.6197
Epoch 6/10
10/10 - 2s - loss: 197.1332 - loglik: -1.9957e+02 - logprior: 2.8103
Epoch 7/10
10/10 - 2s - loss: 196.6625 - loglik: -1.9987e+02 - logprior: 3.5574
Epoch 8/10
10/10 - 2s - loss: 195.3924 - loglik: -1.9922e+02 - logprior: 4.1569
Epoch 9/10
10/10 - 2s - loss: 195.3658 - loglik: -1.9974e+02 - logprior: 4.6897
Epoch 10/10
10/10 - 2s - loss: 195.2014 - loglik: -2.0003e+02 - logprior: 5.1434
Fitted a model with MAP estimate = -194.4266
Time for alignment: 60.6433
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 411.4656 - loglik: -3.4857e+02 - logprior: -6.2885e+01
Epoch 2/10
10/10 - 2s - loss: 322.0172 - loglik: -3.0675e+02 - logprior: -1.5261e+01
Epoch 3/10
10/10 - 2s - loss: 274.2507 - loglik: -2.6704e+02 - logprior: -7.1448e+00
Epoch 4/10
10/10 - 2s - loss: 250.0503 - loglik: -2.4509e+02 - logprior: -4.7978e+00
Epoch 5/10
10/10 - 2s - loss: 239.3318 - loglik: -2.3533e+02 - logprior: -3.7496e+00
Epoch 6/10
10/10 - 2s - loss: 234.6642 - loglik: -2.3136e+02 - logprior: -2.8833e+00
Epoch 7/10
10/10 - 2s - loss: 232.8480 - loglik: -2.3026e+02 - logprior: -2.1994e+00
Epoch 8/10
10/10 - 2s - loss: 231.3023 - loglik: -2.2912e+02 - logprior: -1.8887e+00
Epoch 9/10
10/10 - 2s - loss: 230.6900 - loglik: -2.2873e+02 - logprior: -1.6686e+00
Epoch 10/10
10/10 - 2s - loss: 230.2746 - loglik: -2.2854e+02 - logprior: -1.4208e+00
Fitted a model with MAP estimate = -229.6193
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 3), (70, 1), (78, 1), (79, 1), (87, 5), (88, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 286.8875 - loglik: -2.2962e+02 - logprior: -5.6925e+01
Epoch 2/2
10/10 - 2s - loss: 223.0099 - loglik: -2.0898e+02 - logprior: -1.3554e+01
Fitted a model with MAP estimate = -211.4155
expansions: []
discards: [  0  79 110]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 281.5809 - loglik: -2.1122e+02 - logprior: -6.9924e+01
Epoch 2/2
10/10 - 2s - loss: 234.1554 - loglik: -2.0620e+02 - logprior: -2.7612e+01
Fitted a model with MAP estimate = -225.3682
expansions: [(0, 3)]
discards: [ 0 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 264.3589 - loglik: -2.0847e+02 - logprior: -5.5559e+01
Epoch 2/10
10/10 - 2s - loss: 216.9150 - loglik: -2.0395e+02 - logprior: -1.2624e+01
Epoch 3/10
10/10 - 2s - loss: 205.2722 - loglik: -2.0085e+02 - logprior: -3.9532e+00
Epoch 4/10
10/10 - 2s - loss: 200.2627 - loglik: -1.9943e+02 - logprior: -3.1190e-01
Epoch 5/10
10/10 - 2s - loss: 197.4052 - loglik: -1.9871e+02 - logprior: 1.7583
Epoch 6/10
10/10 - 2s - loss: 196.0249 - loglik: -1.9867e+02 - logprior: 3.0168
Epoch 7/10
10/10 - 2s - loss: 195.3125 - loglik: -1.9878e+02 - logprior: 3.8036
Epoch 8/10
10/10 - 2s - loss: 194.3315 - loglik: -1.9837e+02 - logprior: 4.3533
Epoch 9/10
10/10 - 2s - loss: 194.4018 - loglik: -1.9889e+02 - logprior: 4.7960
Fitted a model with MAP estimate = -193.5840
Time for alignment: 57.7079
Computed alignments with likelihoods: ['-194.4040', '-194.4266', '-193.5840']
Best model has likelihood: -193.5840
time for generating output: 0.2923
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.9297253634894992
Training of 3 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19593b7d00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18915dcf10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f184e04a430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18a2e54ac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f184d479dc0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18b3a18580>, <__main__.SimpleDirichletPrior object at 0x7f185eb07760>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f198c6cd5e0>

Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.4056 - loglik: -1.9314e+02 - logprior: -2.2238e+00
Epoch 2/10
22/22 - 2s - loss: 162.4637 - loglik: -1.6070e+02 - logprior: -1.3547e+00
Epoch 3/10
22/22 - 2s - loss: 155.3522 - loglik: -1.5337e+02 - logprior: -1.4740e+00
Epoch 4/10
22/22 - 2s - loss: 153.9696 - loglik: -1.5220e+02 - logprior: -1.3630e+00
Epoch 5/10
22/22 - 2s - loss: 152.8258 - loglik: -1.5106e+02 - logprior: -1.3864e+00
Epoch 6/10
22/22 - 2s - loss: 152.3968 - loglik: -1.5069e+02 - logprior: -1.3683e+00
Epoch 7/10
22/22 - 2s - loss: 152.1402 - loglik: -1.5045e+02 - logprior: -1.3553e+00
Epoch 8/10
22/22 - 2s - loss: 151.8762 - loglik: -1.5018e+02 - logprior: -1.3468e+00
Epoch 9/10
22/22 - 2s - loss: 151.7384 - loglik: -1.5002e+02 - logprior: -1.3446e+00
Epoch 10/10
22/22 - 2s - loss: 151.5399 - loglik: -1.4981e+02 - logprior: -1.3456e+00
Fitted a model with MAP estimate = -154.0825
expansions: [(8, 1), (9, 2), (12, 2), (13, 2), (20, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 157.0961 - loglik: -1.5358e+02 - logprior: -2.9755e+00
Epoch 2/2
22/22 - 2s - loss: 147.5834 - loglik: -1.4528e+02 - logprior: -1.5983e+00
Fitted a model with MAP estimate = -144.7541
expansions: [(0, 2)]
discards: [ 0  9 14 18 26 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 146.9792 - loglik: -1.4415e+02 - logprior: -2.1776e+00
Epoch 2/2
22/22 - 2s - loss: 144.0978 - loglik: -1.4248e+02 - logprior: -1.0747e+00
Fitted a model with MAP estimate = -143.4172
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 144.5666 - loglik: -1.4260e+02 - logprior: -1.2525e+00
Epoch 2/10
32/32 - 3s - loss: 142.2160 - loglik: -1.4062e+02 - logprior: -9.3813e-01
Epoch 3/10
32/32 - 3s - loss: 141.2965 - loglik: -1.3975e+02 - logprior: -9.1242e-01
Epoch 4/10
32/32 - 3s - loss: 140.9982 - loglik: -1.3946e+02 - logprior: -9.1058e-01
Epoch 5/10
32/32 - 3s - loss: 140.7132 - loglik: -1.3917e+02 - logprior: -9.0148e-01
Epoch 6/10
32/32 - 3s - loss: 140.2233 - loglik: -1.3879e+02 - logprior: -8.9518e-01
Epoch 7/10
32/32 - 3s - loss: 139.9499 - loglik: -1.3846e+02 - logprior: -8.9469e-01
Epoch 8/10
32/32 - 3s - loss: 139.4542 - loglik: -1.3805e+02 - logprior: -8.8527e-01
Epoch 9/10
32/32 - 3s - loss: 139.5117 - loglik: -1.3809e+02 - logprior: -8.8017e-01
Fitted a model with MAP estimate = -138.6864
Time for alignment: 89.0102
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 7s - loss: 195.3790 - loglik: -1.9311e+02 - logprior: -2.2250e+00
Epoch 2/10
22/22 - 2s - loss: 162.6360 - loglik: -1.6087e+02 - logprior: -1.3606e+00
Epoch 3/10
22/22 - 2s - loss: 155.5661 - loglik: -1.5361e+02 - logprior: -1.4670e+00
Epoch 4/10
22/22 - 2s - loss: 153.5289 - loglik: -1.5175e+02 - logprior: -1.3714e+00
Epoch 5/10
22/22 - 2s - loss: 152.9410 - loglik: -1.5122e+02 - logprior: -1.3881e+00
Epoch 6/10
22/22 - 2s - loss: 152.5845 - loglik: -1.5092e+02 - logprior: -1.3666e+00
Epoch 7/10
22/22 - 2s - loss: 152.2158 - loglik: -1.5056e+02 - logprior: -1.3500e+00
Epoch 8/10
22/22 - 2s - loss: 152.4590 - loglik: -1.5080e+02 - logprior: -1.3430e+00
Fitted a model with MAP estimate = -152.7945
expansions: [(8, 1), (9, 2), (11, 2), (14, 2), (20, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 156.6234 - loglik: -1.5316e+02 - logprior: -2.9718e+00
Epoch 2/2
22/22 - 2s - loss: 147.2073 - loglik: -1.4505e+02 - logprior: -1.5954e+00
Fitted a model with MAP estimate = -144.7048
expansions: [(0, 2)]
discards: [ 0  9 14 18 26 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 146.9694 - loglik: -1.4421e+02 - logprior: -2.1766e+00
Epoch 2/2
22/22 - 2s - loss: 143.6400 - loglik: -1.4205e+02 - logprior: -1.0786e+00
Fitted a model with MAP estimate = -143.3585
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 144.6437 - loglik: -1.4270e+02 - logprior: -1.2535e+00
Epoch 2/10
32/32 - 3s - loss: 142.0661 - loglik: -1.4049e+02 - logprior: -9.3512e-01
Epoch 3/10
32/32 - 3s - loss: 141.4173 - loglik: -1.3988e+02 - logprior: -9.1297e-01
Epoch 4/10
32/32 - 3s - loss: 140.9793 - loglik: -1.3946e+02 - logprior: -9.0934e-01
Epoch 5/10
32/32 - 3s - loss: 140.7745 - loglik: -1.3924e+02 - logprior: -9.0322e-01
Epoch 6/10
32/32 - 3s - loss: 140.0372 - loglik: -1.3860e+02 - logprior: -8.9787e-01
Epoch 7/10
32/32 - 3s - loss: 139.9343 - loglik: -1.3846e+02 - logprior: -8.9033e-01
Epoch 8/10
32/32 - 3s - loss: 139.6893 - loglik: -1.3829e+02 - logprior: -8.8754e-01
Epoch 9/10
32/32 - 3s - loss: 139.7142 - loglik: -1.3829e+02 - logprior: -8.8489e-01
Fitted a model with MAP estimate = -138.6908
Time for alignment: 86.6998
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.0680 - loglik: -1.9280e+02 - logprior: -2.2230e+00
Epoch 2/10
22/22 - 2s - loss: 162.4454 - loglik: -1.6079e+02 - logprior: -1.3697e+00
Epoch 3/10
22/22 - 2s - loss: 155.1958 - loglik: -1.5312e+02 - logprior: -1.5006e+00
Epoch 4/10
22/22 - 2s - loss: 153.3814 - loglik: -1.5154e+02 - logprior: -1.3990e+00
Epoch 5/10
22/22 - 2s - loss: 152.7338 - loglik: -1.5096e+02 - logprior: -1.3956e+00
Epoch 6/10
22/22 - 2s - loss: 152.3809 - loglik: -1.5067e+02 - logprior: -1.3676e+00
Epoch 7/10
22/22 - 2s - loss: 152.3139 - loglik: -1.5062e+02 - logprior: -1.3566e+00
Epoch 8/10
22/22 - 2s - loss: 152.1805 - loglik: -1.5050e+02 - logprior: -1.3462e+00
Epoch 9/10
22/22 - 2s - loss: 151.9092 - loglik: -1.5021e+02 - logprior: -1.3435e+00
Epoch 10/10
22/22 - 2s - loss: 151.3884 - loglik: -1.4968e+02 - logprior: -1.3440e+00
Fitted a model with MAP estimate = -153.8531
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 156.6680 - loglik: -1.5314e+02 - logprior: -2.9667e+00
Epoch 2/2
22/22 - 2s - loss: 147.1588 - loglik: -1.4487e+02 - logprior: -1.5744e+00
Fitted a model with MAP estimate = -144.7708
expansions: [(0, 2)]
discards: [ 0  9 17 25 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 146.9098 - loglik: -1.4409e+02 - logprior: -2.1752e+00
Epoch 2/2
22/22 - 2s - loss: 143.8512 - loglik: -1.4224e+02 - logprior: -1.0744e+00
Fitted a model with MAP estimate = -143.3286
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 144.4101 - loglik: -1.4245e+02 - logprior: -1.2427e+00
Epoch 2/10
32/32 - 3s - loss: 142.3207 - loglik: -1.4073e+02 - logprior: -9.4230e-01
Epoch 3/10
32/32 - 3s - loss: 141.4042 - loglik: -1.3986e+02 - logprior: -9.0975e-01
Epoch 4/10
32/32 - 3s - loss: 140.9961 - loglik: -1.3947e+02 - logprior: -9.0597e-01
Epoch 5/10
32/32 - 3s - loss: 140.6868 - loglik: -1.3915e+02 - logprior: -8.9907e-01
Epoch 6/10
32/32 - 3s - loss: 140.1339 - loglik: -1.3869e+02 - logprior: -8.9918e-01
Epoch 7/10
32/32 - 3s - loss: 139.9903 - loglik: -1.3851e+02 - logprior: -8.8770e-01
Epoch 8/10
32/32 - 3s - loss: 139.6058 - loglik: -1.3820e+02 - logprior: -8.8771e-01
Epoch 9/10
32/32 - 3s - loss: 139.4654 - loglik: -1.3805e+02 - logprior: -8.7384e-01
Epoch 10/10
32/32 - 3s - loss: 139.5058 - loglik: -1.3807e+02 - logprior: -8.7740e-01
Fitted a model with MAP estimate = -138.4796
Time for alignment: 90.8114
Computed alignments with likelihoods: ['-138.6864', '-138.6908', '-138.4796']
Best model has likelihood: -138.4796
time for generating output: 0.1528
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.8541798051993553
Training of 3 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f184ddb3fa0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f181a11cc10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f181a11c520>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f181a11cd60>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f181a11c9d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f181a0f0280>, <__main__.SimpleDirichletPrior object at 0x7f1877f89d00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19590dea60>

Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 461.1533 - loglik: -4.4767e+02 - logprior: -1.3474e+01
Epoch 2/10
17/17 - 5s - loss: 310.9188 - loglik: -3.0864e+02 - logprior: -2.1929e+00
Epoch 3/10
17/17 - 5s - loss: 262.6928 - loglik: -2.6087e+02 - logprior: -1.7939e+00
Epoch 4/10
17/17 - 5s - loss: 246.9185 - loglik: -2.4490e+02 - logprior: -1.8057e+00
Epoch 5/10
17/17 - 5s - loss: 242.5349 - loglik: -2.4030e+02 - logprior: -1.8434e+00
Epoch 6/10
17/17 - 5s - loss: 242.7345 - loglik: -2.4035e+02 - logprior: -1.9402e+00
Fitted a model with MAP estimate = -240.9717
expansions: [(20, 1), (31, 1), (56, 1), (82, 1), (92, 1), (136, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 267.7222 - loglik: -2.4916e+02 - logprior: -1.8281e+01
Epoch 2/2
17/17 - 5s - loss: 249.5197 - loglik: -2.4256e+02 - logprior: -6.7700e+00
Fitted a model with MAP estimate = -244.7230
expansions: [(0, 16)]
discards: [  0 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 260.3536 - loglik: -2.4598e+02 - logprior: -1.4159e+01
Epoch 2/2
17/17 - 6s - loss: 241.9581 - loglik: -2.3891e+02 - logprior: -2.8058e+00
Fitted a model with MAP estimate = -238.9593
expansions: [(153, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 258.9256 - loglik: -2.4250e+02 - logprior: -1.6171e+01
Epoch 2/10
17/17 - 5s - loss: 242.2376 - loglik: -2.3996e+02 - logprior: -2.0124e+00
Epoch 3/10
17/17 - 5s - loss: 238.1662 - loglik: -2.3766e+02 - logprior: -1.9623e-01
Epoch 4/10
17/17 - 5s - loss: 234.3944 - loglik: -2.3408e+02 - logprior: 0.0531
Epoch 5/10
17/17 - 5s - loss: 232.6339 - loglik: -2.3250e+02 - logprior: 0.2476
Epoch 6/10
17/17 - 5s - loss: 232.7819 - loglik: -2.3273e+02 - logprior: 0.3604
Fitted a model with MAP estimate = -231.0642
Time for alignment: 117.8546
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 462.7926 - loglik: -4.4938e+02 - logprior: -1.3411e+01
Epoch 2/10
17/17 - 5s - loss: 321.4760 - loglik: -3.1973e+02 - logprior: -1.6780e+00
Epoch 3/10
17/17 - 5s - loss: 269.4374 - loglik: -2.6833e+02 - logprior: -1.0866e+00
Epoch 4/10
17/17 - 5s - loss: 258.6631 - loglik: -2.5752e+02 - logprior: -9.4992e-01
Epoch 5/10
17/17 - 5s - loss: 252.3063 - loglik: -2.5097e+02 - logprior: -9.3536e-01
Epoch 6/10
17/17 - 5s - loss: 254.9212 - loglik: -2.5359e+02 - logprior: -8.9475e-01
Fitted a model with MAP estimate = -251.7275
expansions: [(0, 33), (24, 2), (25, 1), (36, 1), (57, 1), (80, 1), (89, 1), (100, 1), (137, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 10s - loss: 267.6895 - loglik: -2.5017e+02 - logprior: -1.7222e+01
Epoch 2/2
17/17 - 6s - loss: 221.9569 - loglik: -2.1907e+02 - logprior: -2.6622e+00
Fitted a model with MAP estimate = -214.1324
expansions: [(0, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 253.6899 - loglik: -2.3930e+02 - logprior: -1.4127e+01
Epoch 2/2
17/17 - 6s - loss: 225.2854 - loglik: -2.2384e+02 - logprior: -1.2003e+00
Fitted a model with MAP estimate = -220.6277
expansions: [(0, 25)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 200 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 250.4832 - loglik: -2.3755e+02 - logprior: -1.2654e+01
Epoch 2/10
17/17 - 6s - loss: 221.2074 - loglik: -2.2013e+02 - logprior: -8.1399e-01
Epoch 3/10
17/17 - 6s - loss: 214.1368 - loglik: -2.1428e+02 - logprior: 0.4795
Epoch 4/10
17/17 - 6s - loss: 209.7400 - loglik: -2.1039e+02 - logprior: 1.0478
Epoch 5/10
17/17 - 6s - loss: 208.1639 - loglik: -2.0899e+02 - logprior: 1.2515
Epoch 6/10
17/17 - 6s - loss: 206.8178 - loglik: -2.0783e+02 - logprior: 1.4509
Epoch 7/10
17/17 - 6s - loss: 206.1404 - loglik: -2.0735e+02 - logprior: 1.6401
Epoch 8/10
17/17 - 6s - loss: 206.5489 - loglik: -2.0798e+02 - logprior: 1.8464
Fitted a model with MAP estimate = -205.0022
Time for alignment: 139.1761
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 462.4113 - loglik: -4.4898e+02 - logprior: -1.3426e+01
Epoch 2/10
17/17 - 5s - loss: 322.6375 - loglik: -3.2089e+02 - logprior: -1.6697e+00
Epoch 3/10
17/17 - 5s - loss: 275.3775 - loglik: -2.7433e+02 - logprior: -1.0165e+00
Epoch 4/10
17/17 - 5s - loss: 261.3772 - loglik: -2.6033e+02 - logprior: -8.5540e-01
Epoch 5/10
17/17 - 5s - loss: 256.7357 - loglik: -2.5552e+02 - logprior: -8.3570e-01
Epoch 6/10
17/17 - 5s - loss: 255.6464 - loglik: -2.5442e+02 - logprior: -8.3014e-01
Epoch 7/10
17/17 - 5s - loss: 253.0316 - loglik: -2.5176e+02 - logprior: -8.6372e-01
Epoch 8/10
17/17 - 5s - loss: 257.2550 - loglik: -2.5597e+02 - logprior: -8.7239e-01
Fitted a model with MAP estimate = -253.6632
expansions: [(0, 33), (24, 2), (25, 1), (32, 1), (49, 1), (64, 1), (82, 1), (92, 1), (136, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 271.7119 - loglik: -2.5382e+02 - logprior: -1.7577e+01
Epoch 2/2
17/17 - 6s - loss: 230.0114 - loglik: -2.2693e+02 - logprior: -2.8451e+00
Fitted a model with MAP estimate = -219.5690
expansions: [(0, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 257.7451 - loglik: -2.4316e+02 - logprior: -1.4306e+01
Epoch 2/2
17/17 - 6s - loss: 228.0617 - loglik: -2.2654e+02 - logprior: -1.2311e+00
Fitted a model with MAP estimate = -222.0325
expansions: [(0, 25)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 200 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 9s - loss: 251.9521 - loglik: -2.3895e+02 - logprior: -1.2692e+01
Epoch 2/10
17/17 - 6s - loss: 222.7225 - loglik: -2.2168e+02 - logprior: -7.7370e-01
Epoch 3/10
17/17 - 6s - loss: 216.3499 - loglik: -2.1651e+02 - logprior: 0.4944
Epoch 4/10
17/17 - 6s - loss: 209.0001 - loglik: -2.0966e+02 - logprior: 1.0766
Epoch 5/10
17/17 - 6s - loss: 209.0967 - loglik: -2.0995e+02 - logprior: 1.2852
Fitted a model with MAP estimate = -208.0026
Time for alignment: 127.5834
Computed alignments with likelihoods: ['-231.0642', '-205.0022', '-208.0026']
Best model has likelihood: -205.0022
time for generating output: 0.4119
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.5336407461748062
Training of 3 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1961ecd490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f197b5671f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1833a787f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fcfcc820>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fcfcc7c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18a27b49d0>, <__main__.SimpleDirichletPrior object at 0x7f18b44dcdc0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19ae68e3a0>

Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 246.0594 - loglik: -2.2558e+02 - logprior: -2.0450e+01
Epoch 2/10
10/10 - 1s - loss: 216.7767 - loglik: -2.1100e+02 - logprior: -5.6049e+00
Epoch 3/10
10/10 - 1s - loss: 199.6091 - loglik: -1.9625e+02 - logprior: -3.0632e+00
Epoch 4/10
10/10 - 1s - loss: 191.1009 - loglik: -1.8824e+02 - logprior: -2.4591e+00
Epoch 5/10
10/10 - 1s - loss: 187.9015 - loglik: -1.8524e+02 - logprior: -2.2828e+00
Epoch 6/10
10/10 - 1s - loss: 186.3376 - loglik: -1.8395e+02 - logprior: -2.1514e+00
Epoch 7/10
10/10 - 1s - loss: 185.1214 - loglik: -1.8305e+02 - logprior: -1.8655e+00
Epoch 8/10
10/10 - 1s - loss: 184.2871 - loglik: -1.8233e+02 - logprior: -1.7563e+00
Epoch 9/10
10/10 - 1s - loss: 183.8743 - loglik: -1.8187e+02 - logprior: -1.8156e+00
Epoch 10/10
10/10 - 1s - loss: 183.2538 - loglik: -1.8122e+02 - logprior: -1.8426e+00
Fitted a model with MAP estimate = -183.1055
expansions: [(0, 2), (8, 1), (21, 2), (23, 1), (41, 2), (42, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.3557 - loglik: -1.8414e+02 - logprior: -2.6981e+01
Epoch 2/2
10/10 - 1s - loss: 187.8048 - loglik: -1.7891e+02 - logprior: -8.5231e+00
Fitted a model with MAP estimate = -183.0431
expansions: []
discards: [ 0 24 53 61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.1384 - loglik: -1.7900e+02 - logprior: -2.3639e+01
Epoch 2/2
10/10 - 1s - loss: 188.5379 - loglik: -1.7860e+02 - logprior: -9.4447e+00
Fitted a model with MAP estimate = -184.9904
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.7287 - loglik: -1.7762e+02 - logprior: -2.0607e+01
Epoch 2/10
10/10 - 1s - loss: 183.2180 - loglik: -1.7707e+02 - logprior: -5.7124e+00
Epoch 3/10
10/10 - 1s - loss: 179.7602 - loglik: -1.7671e+02 - logprior: -2.6335e+00
Epoch 4/10
10/10 - 1s - loss: 178.4995 - loglik: -1.7650e+02 - logprior: -1.6248e+00
Epoch 5/10
10/10 - 1s - loss: 177.5908 - loglik: -1.7604e+02 - logprior: -1.2264e+00
Epoch 6/10
10/10 - 1s - loss: 177.2747 - loglik: -1.7590e+02 - logprior: -1.0691e+00
Epoch 7/10
10/10 - 1s - loss: 176.8905 - loglik: -1.7568e+02 - logprior: -9.1610e-01
Epoch 8/10
10/10 - 1s - loss: 176.3538 - loglik: -1.7535e+02 - logprior: -7.1241e-01
Epoch 9/10
10/10 - 1s - loss: 176.3693 - loglik: -1.7550e+02 - logprior: -5.8539e-01
Fitted a model with MAP estimate = -175.8323
Time for alignment: 46.6243
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 246.0276 - loglik: -2.2555e+02 - logprior: -2.0449e+01
Epoch 2/10
10/10 - 1s - loss: 216.1318 - loglik: -2.1036e+02 - logprior: -5.6040e+00
Epoch 3/10
10/10 - 1s - loss: 198.1501 - loglik: -1.9488e+02 - logprior: -3.0187e+00
Epoch 4/10
10/10 - 1s - loss: 189.8768 - loglik: -1.8721e+02 - logprior: -2.4284e+00
Epoch 5/10
10/10 - 1s - loss: 186.6161 - loglik: -1.8397e+02 - logprior: -2.3319e+00
Epoch 6/10
10/10 - 1s - loss: 184.7923 - loglik: -1.8231e+02 - logprior: -2.1831e+00
Epoch 7/10
10/10 - 1s - loss: 184.3035 - loglik: -1.8215e+02 - logprior: -1.8880e+00
Epoch 8/10
10/10 - 1s - loss: 183.4980 - loglik: -1.8148e+02 - logprior: -1.7622e+00
Epoch 9/10
10/10 - 1s - loss: 183.3062 - loglik: -1.8127e+02 - logprior: -1.7797e+00
Epoch 10/10
10/10 - 1s - loss: 182.9486 - loglik: -1.8093e+02 - logprior: -1.7675e+00
Fitted a model with MAP estimate = -182.6430
expansions: [(0, 2), (7, 2), (8, 2), (34, 1), (41, 2), (42, 2), (43, 1), (44, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.5320 - loglik: -1.8322e+02 - logprior: -2.7037e+01
Epoch 2/2
10/10 - 1s - loss: 187.6059 - loglik: -1.7865e+02 - logprior: -8.5473e+00
Fitted a model with MAP estimate = -182.5926
expansions: []
discards: [ 0 10 12 54 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.9460 - loglik: -1.7880e+02 - logprior: -2.3623e+01
Epoch 2/2
10/10 - 1s - loss: 187.9450 - loglik: -1.7798e+02 - logprior: -9.4777e+00
Fitted a model with MAP estimate = -184.4318
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 198.4812 - loglik: -1.7736e+02 - logprior: -2.0635e+01
Epoch 2/10
10/10 - 1s - loss: 182.2537 - loglik: -1.7609e+02 - logprior: -5.7333e+00
Epoch 3/10
10/10 - 1s - loss: 179.1132 - loglik: -1.7605e+02 - logprior: -2.6564e+00
Epoch 4/10
10/10 - 1s - loss: 177.8221 - loglik: -1.7581e+02 - logprior: -1.6446e+00
Epoch 5/10
10/10 - 1s - loss: 176.9381 - loglik: -1.7536e+02 - logprior: -1.2503e+00
Epoch 6/10
10/10 - 1s - loss: 176.4986 - loglik: -1.7513e+02 - logprior: -1.0704e+00
Epoch 7/10
10/10 - 1s - loss: 176.2701 - loglik: -1.7511e+02 - logprior: -8.7131e-01
Epoch 8/10
10/10 - 1s - loss: 175.8549 - loglik: -1.7488e+02 - logprior: -6.8973e-01
Epoch 9/10
10/10 - 1s - loss: 175.6970 - loglik: -1.7484e+02 - logprior: -5.8539e-01
Epoch 10/10
10/10 - 1s - loss: 175.6441 - loglik: -1.7481e+02 - logprior: -5.6026e-01
Fitted a model with MAP estimate = -175.1888
Time for alignment: 45.7306
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 246.1497 - loglik: -2.2567e+02 - logprior: -2.0452e+01
Epoch 2/10
10/10 - 1s - loss: 216.1547 - loglik: -2.1038e+02 - logprior: -5.6037e+00
Epoch 3/10
10/10 - 1s - loss: 198.3755 - loglik: -1.9507e+02 - logprior: -3.0387e+00
Epoch 4/10
10/10 - 1s - loss: 190.2117 - loglik: -1.8743e+02 - logprior: -2.4389e+00
Epoch 5/10
10/10 - 1s - loss: 186.9262 - loglik: -1.8424e+02 - logprior: -2.3071e+00
Epoch 6/10
10/10 - 1s - loss: 185.4094 - loglik: -1.8295e+02 - logprior: -2.1808e+00
Epoch 7/10
10/10 - 1s - loss: 184.3193 - loglik: -1.8219e+02 - logprior: -1.8905e+00
Epoch 8/10
10/10 - 1s - loss: 184.1581 - loglik: -1.8215e+02 - logprior: -1.7681e+00
Epoch 9/10
10/10 - 1s - loss: 183.4909 - loglik: -1.8145e+02 - logprior: -1.8119e+00
Epoch 10/10
10/10 - 1s - loss: 183.2462 - loglik: -1.8119e+02 - logprior: -1.8474e+00
Fitted a model with MAP estimate = -182.8837
expansions: [(0, 2), (8, 1), (9, 1), (34, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 210.6618 - loglik: -1.8338e+02 - logprior: -2.7030e+01
Epoch 2/2
10/10 - 1s - loss: 186.8925 - loglik: -1.7807e+02 - logprior: -8.4396e+00
Fitted a model with MAP estimate = -182.3265
expansions: []
discards: [ 0 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.4931 - loglik: -1.7844e+02 - logprior: -2.3548e+01
Epoch 2/2
10/10 - 1s - loss: 187.6608 - loglik: -1.7772e+02 - logprior: -9.4536e+00
Fitted a model with MAP estimate = -184.3841
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 198.3643 - loglik: -1.7722e+02 - logprior: -2.0639e+01
Epoch 2/10
10/10 - 1s - loss: 182.5757 - loglik: -1.7637e+02 - logprior: -5.7482e+00
Epoch 3/10
10/10 - 1s - loss: 179.2249 - loglik: -1.7612e+02 - logprior: -2.6725e+00
Epoch 4/10
10/10 - 1s - loss: 177.7619 - loglik: -1.7571e+02 - logprior: -1.6609e+00
Epoch 5/10
10/10 - 1s - loss: 177.2355 - loglik: -1.7563e+02 - logprior: -1.2635e+00
Epoch 6/10
10/10 - 1s - loss: 176.6593 - loglik: -1.7524e+02 - logprior: -1.0960e+00
Epoch 7/10
10/10 - 1s - loss: 176.3733 - loglik: -1.7515e+02 - logprior: -9.1358e-01
Epoch 8/10
10/10 - 1s - loss: 175.9202 - loglik: -1.7491e+02 - logprior: -7.2190e-01
Epoch 9/10
10/10 - 1s - loss: 175.9851 - loglik: -1.7509e+02 - logprior: -6.1058e-01
Fitted a model with MAP estimate = -175.4399
Time for alignment: 45.4615
Computed alignments with likelihoods: ['-175.8323', '-175.1888', '-175.4399']
Best model has likelihood: -175.1888
time for generating output: 0.5346
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.6507462686567164
Training of 3 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19bf62a9a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f185ed90e80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f193d71e730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193d71e8e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1961804a90>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19c79f7d30>, <__main__.SimpleDirichletPrior object at 0x7f186f8fec40>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19beaa9820>

Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 198.9924 - loglik: -1.8739e+02 - logprior: -1.1586e+01
Epoch 2/10
11/11 - 1s - loss: 159.2124 - loglik: -1.5591e+02 - logprior: -3.2978e+00
Epoch 3/10
11/11 - 1s - loss: 128.2055 - loglik: -1.2573e+02 - logprior: -2.4329e+00
Epoch 4/10
11/11 - 1s - loss: 113.4878 - loglik: -1.1113e+02 - logprior: -2.3182e+00
Epoch 5/10
11/11 - 1s - loss: 107.1409 - loglik: -1.0501e+02 - logprior: -2.0599e+00
Epoch 6/10
11/11 - 1s - loss: 104.1991 - loglik: -1.0183e+02 - logprior: -2.0725e+00
Epoch 7/10
11/11 - 1s - loss: 103.4393 - loglik: -1.0095e+02 - logprior: -2.0981e+00
Epoch 8/10
11/11 - 1s - loss: 102.3692 - loglik: -9.9986e+01 - logprior: -2.0378e+00
Epoch 9/10
11/11 - 1s - loss: 102.0492 - loglik: -9.9687e+01 - logprior: -2.0494e+00
Epoch 10/10
11/11 - 1s - loss: 101.7161 - loglik: -9.9344e+01 - logprior: -2.0524e+00
Fitted a model with MAP estimate = -101.5093
expansions: [(0, 3), (15, 2), (27, 1), (28, 1), (29, 2), (30, 2), (31, 3), (34, 1), (37, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 112.5611 - loglik: -9.8395e+01 - logprior: -1.3898e+01
Epoch 2/2
11/11 - 1s - loss: 95.0074 - loglik: -9.0490e+01 - logprior: -4.3770e+00
Fitted a model with MAP estimate = -92.5210
expansions: []
discards: [ 0 36 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 105.4909 - loglik: -9.2050e+01 - logprior: -1.3278e+01
Epoch 2/2
11/11 - 1s - loss: 96.1761 - loglik: -9.0455e+01 - logprior: -5.5564e+00
Fitted a model with MAP estimate = -93.9521
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 103.1073 - loglik: -9.0887e+01 - logprior: -1.2047e+01
Epoch 2/10
11/11 - 1s - loss: 94.7308 - loglik: -9.0985e+01 - logprior: -3.5907e+00
Epoch 3/10
11/11 - 1s - loss: 92.1971 - loglik: -8.9623e+01 - logprior: -2.3463e+00
Epoch 4/10
11/11 - 1s - loss: 91.2770 - loglik: -8.9237e+01 - logprior: -1.7341e+00
Epoch 5/10
11/11 - 1s - loss: 90.0434 - loglik: -8.8233e+01 - logprior: -1.4804e+00
Epoch 6/10
11/11 - 1s - loss: 90.6407 - loglik: -8.8851e+01 - logprior: -1.4363e+00
Fitted a model with MAP estimate = -89.4645
Time for alignment: 38.3240
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 199.2242 - loglik: -1.8762e+02 - logprior: -1.1582e+01
Epoch 2/10
11/11 - 1s - loss: 158.5913 - loglik: -1.5528e+02 - logprior: -3.2983e+00
Epoch 3/10
11/11 - 1s - loss: 126.3116 - loglik: -1.2380e+02 - logprior: -2.4649e+00
Epoch 4/10
11/11 - 1s - loss: 109.8884 - loglik: -1.0739e+02 - logprior: -2.4250e+00
Epoch 5/10
11/11 - 1s - loss: 104.0955 - loglik: -1.0174e+02 - logprior: -2.1735e+00
Epoch 6/10
11/11 - 1s - loss: 102.3746 - loglik: -9.9895e+01 - logprior: -2.1585e+00
Epoch 7/10
11/11 - 1s - loss: 101.5479 - loglik: -9.9085e+01 - logprior: -2.1480e+00
Epoch 8/10
11/11 - 1s - loss: 101.4133 - loglik: -9.9040e+01 - logprior: -2.0695e+00
Epoch 9/10
11/11 - 1s - loss: 101.4788 - loglik: -9.9084e+01 - logprior: -2.0785e+00
Fitted a model with MAP estimate = -100.6537
expansions: [(0, 3), (15, 1), (26, 1), (27, 2), (28, 3), (29, 1), (30, 1), (31, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 111.2615 - loglik: -9.7203e+01 - logprior: -1.3821e+01
Epoch 2/2
11/11 - 1s - loss: 95.4460 - loglik: -9.1037e+01 - logprior: -4.2960e+00
Fitted a model with MAP estimate = -92.8156
expansions: []
discards: [ 0 33]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.6289 - loglik: -9.2201e+01 - logprior: -1.3282e+01
Epoch 2/2
11/11 - 1s - loss: 96.2410 - loglik: -9.0539e+01 - logprior: -5.5445e+00
Fitted a model with MAP estimate = -93.9918
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 101.5597 - loglik: -9.0434e+01 - logprior: -1.0946e+01
Epoch 2/10
11/11 - 1s - loss: 93.0573 - loglik: -8.9776e+01 - logprior: -3.1171e+00
Epoch 3/10
11/11 - 1s - loss: 91.4862 - loglik: -8.9301e+01 - logprior: -1.9489e+00
Epoch 4/10
11/11 - 1s - loss: 90.2807 - loglik: -8.8216e+01 - logprior: -1.7593e+00
Epoch 5/10
11/11 - 1s - loss: 89.4313 - loglik: -8.7406e+01 - logprior: -1.6923e+00
Epoch 6/10
11/11 - 1s - loss: 89.5333 - loglik: -8.7641e+01 - logprior: -1.5416e+00
Fitted a model with MAP estimate = -88.6607
Time for alignment: 38.2779
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 198.9960 - loglik: -1.8739e+02 - logprior: -1.1584e+01
Epoch 2/10
11/11 - 1s - loss: 158.4200 - loglik: -1.5510e+02 - logprior: -3.3118e+00
Epoch 3/10
11/11 - 1s - loss: 125.1818 - loglik: -1.2265e+02 - logprior: -2.4871e+00
Epoch 4/10
11/11 - 1s - loss: 109.3439 - loglik: -1.0689e+02 - logprior: -2.3927e+00
Epoch 5/10
11/11 - 1s - loss: 104.1496 - loglik: -1.0184e+02 - logprior: -2.1761e+00
Epoch 6/10
11/11 - 1s - loss: 101.8678 - loglik: -9.9366e+01 - logprior: -2.2022e+00
Epoch 7/10
11/11 - 1s - loss: 101.0273 - loglik: -9.8464e+01 - logprior: -2.2015e+00
Epoch 8/10
11/11 - 1s - loss: 100.7575 - loglik: -9.8235e+01 - logprior: -2.1790e+00
Epoch 9/10
11/11 - 1s - loss: 100.3877 - loglik: -9.7839e+01 - logprior: -2.2140e+00
Epoch 10/10
11/11 - 1s - loss: 100.0230 - loglik: -9.7480e+01 - logprior: -2.2064e+00
Fitted a model with MAP estimate = -99.7064
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 112.7356 - loglik: -9.8568e+01 - logprior: -1.3925e+01
Epoch 2/2
11/11 - 1s - loss: 95.7717 - loglik: -9.1194e+01 - logprior: -4.4402e+00
Fitted a model with MAP estimate = -92.6627
expansions: []
discards: [ 0 36 39 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 105.5230 - loglik: -9.2053e+01 - logprior: -1.3295e+01
Epoch 2/2
11/11 - 1s - loss: 96.3903 - loglik: -9.0671e+01 - logprior: -5.5546e+00
Fitted a model with MAP estimate = -93.9583
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 103.3489 - loglik: -9.1104e+01 - logprior: -1.2055e+01
Epoch 2/10
11/11 - 1s - loss: 94.4422 - loglik: -9.0681e+01 - logprior: -3.5978e+00
Epoch 3/10
11/11 - 1s - loss: 92.0869 - loglik: -8.9511e+01 - logprior: -2.3469e+00
Epoch 4/10
11/11 - 1s - loss: 91.3531 - loglik: -8.9306e+01 - logprior: -1.7383e+00
Epoch 5/10
11/11 - 1s - loss: 90.7264 - loglik: -8.8909e+01 - logprior: -1.4849e+00
Epoch 6/10
11/11 - 1s - loss: 89.9737 - loglik: -8.8179e+01 - logprior: -1.4420e+00
Epoch 7/10
11/11 - 1s - loss: 89.5107 - loglik: -8.7782e+01 - logprior: -1.3705e+00
Epoch 8/10
11/11 - 1s - loss: 89.9596 - loglik: -8.8220e+01 - logprior: -1.3733e+00
Fitted a model with MAP estimate = -89.0730
Time for alignment: 38.8552
Computed alignments with likelihoods: ['-89.4645', '-88.6607', '-89.0730']
Best model has likelihood: -88.6607
time for generating output: 0.1520
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 3 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f186f4121c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1972e875b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198bdac4f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f198bb1b7c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19bf317100>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19ae2dbb80>, <__main__.SimpleDirichletPrior object at 0x7f19594aa910>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1899a441f0>

Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 171.5436 - loglik: -1.7060e+02 - logprior: -7.3177e-01
Epoch 2/10
42/42 - 5s - loss: 80.8350 - loglik: -7.9906e+01 - logprior: -6.4642e-01
Epoch 3/10
42/42 - 4s - loss: 78.1954 - loglik: -7.7353e+01 - logprior: -6.4328e-01
Epoch 4/10
42/42 - 5s - loss: 77.8772 - loglik: -7.7047e+01 - logprior: -6.2926e-01
Epoch 5/10
42/42 - 4s - loss: 77.3781 - loglik: -7.6558e+01 - logprior: -6.2216e-01
Epoch 6/10
42/42 - 5s - loss: 77.1636 - loglik: -7.6354e+01 - logprior: -6.0870e-01
Epoch 7/10
42/42 - 5s - loss: 77.1440 - loglik: -7.6340e+01 - logprior: -6.0837e-01
Epoch 8/10
42/42 - 5s - loss: 76.8103 - loglik: -7.6004e+01 - logprior: -6.0723e-01
Epoch 9/10
42/42 - 5s - loss: 77.0072 - loglik: -7.6207e+01 - logprior: -6.0403e-01
Fitted a model with MAP estimate = -76.1044
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (44, 1), (47, 1), (48, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 46.1058 - loglik: -4.5001e+01 - logprior: -8.4875e-01
Epoch 2/2
42/42 - 5s - loss: 33.6168 - loglik: -3.2733e+01 - logprior: -6.3285e-01
Fitted a model with MAP estimate = -32.5779
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 37.0411 - loglik: -3.5866e+01 - logprior: -9.2701e-01
Epoch 2/2
42/42 - 5s - loss: 34.0046 - loglik: -3.3255e+01 - logprior: -5.0300e-01
Fitted a model with MAP estimate = -33.5824
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 33.1680 - loglik: -3.2326e+01 - logprior: -6.2749e-01
Epoch 2/10
59/59 - 7s - loss: 32.4465 - loglik: -3.1647e+01 - logprior: -5.6751e-01
Epoch 3/10
59/59 - 7s - loss: 32.2757 - loglik: -3.1483e+01 - logprior: -5.5624e-01
Epoch 4/10
59/59 - 7s - loss: 31.6679 - loglik: -3.0950e+01 - logprior: -5.4871e-01
Epoch 5/10
59/59 - 7s - loss: 31.7503 - loglik: -3.1048e+01 - logprior: -5.3914e-01
Fitted a model with MAP estimate = -31.3340
Time for alignment: 205.6419
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 8s - loss: 171.3187 - loglik: -1.7038e+02 - logprior: -7.3076e-01
Epoch 2/10
42/42 - 4s - loss: 81.3934 - loglik: -8.0431e+01 - logprior: -6.5056e-01
Epoch 3/10
42/42 - 5s - loss: 78.7927 - loglik: -7.7899e+01 - logprior: -6.4328e-01
Epoch 4/10
42/42 - 5s - loss: 78.4997 - loglik: -7.7618e+01 - logprior: -6.3006e-01
Epoch 5/10
42/42 - 5s - loss: 77.9466 - loglik: -7.7090e+01 - logprior: -6.1828e-01
Epoch 6/10
42/42 - 5s - loss: 78.1762 - loglik: -7.7351e+01 - logprior: -5.9751e-01
Fitted a model with MAP estimate = -76.8601
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 9s - loss: 46.4359 - loglik: -4.5329e+01 - logprior: -8.4116e-01
Epoch 2/2
42/42 - 5s - loss: 33.3792 - loglik: -3.2494e+01 - logprior: -6.3407e-01
Fitted a model with MAP estimate = -32.6016
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 37.2060 - loglik: -3.6005e+01 - logprior: -9.5273e-01
Epoch 2/2
42/42 - 5s - loss: 34.2286 - loglik: -3.3483e+01 - logprior: -4.9638e-01
Fitted a model with MAP estimate = -33.6512
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 10s - loss: 33.3095 - loglik: -3.2455e+01 - logprior: -6.4225e-01
Epoch 2/10
59/59 - 7s - loss: 32.2548 - loglik: -3.1451e+01 - logprior: -5.6970e-01
Epoch 3/10
59/59 - 7s - loss: 32.3068 - loglik: -3.1514e+01 - logprior: -5.5844e-01
Fitted a model with MAP estimate = -31.7683
Time for alignment: 178.8402
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 9s - loss: 171.2370 - loglik: -1.7029e+02 - logprior: -7.3491e-01
Epoch 2/10
42/42 - 5s - loss: 81.7601 - loglik: -8.0810e+01 - logprior: -6.5254e-01
Epoch 3/10
42/42 - 5s - loss: 78.7037 - loglik: -7.7825e+01 - logprior: -6.4726e-01
Epoch 4/10
42/42 - 5s - loss: 78.5425 - loglik: -7.7723e+01 - logprior: -6.2662e-01
Epoch 5/10
42/42 - 4s - loss: 77.5380 - loglik: -7.6721e+01 - logprior: -6.2721e-01
Epoch 6/10
42/42 - 4s - loss: 77.8295 - loglik: -7.7024e+01 - logprior: -6.1108e-01
Fitted a model with MAP estimate = -76.8386
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 46.7544 - loglik: -4.5659e+01 - logprior: -8.4076e-01
Epoch 2/2
42/42 - 5s - loss: 33.1202 - loglik: -3.2233e+01 - logprior: -6.3469e-01
Fitted a model with MAP estimate = -32.5655
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 11s - loss: 32.3805 - loglik: -3.1688e+01 - logprior: -4.7325e-01
Epoch 2/10
59/59 - 7s - loss: 32.0261 - loglik: -3.1381e+01 - logprior: -4.1063e-01
Epoch 3/10
59/59 - 7s - loss: 31.7628 - loglik: -3.1128e+01 - logprior: -3.9956e-01
Epoch 4/10
59/59 - 7s - loss: 31.5205 - loglik: -3.0954e+01 - logprior: -3.9154e-01
Epoch 5/10
59/59 - 7s - loss: 31.2681 - loglik: -3.0721e+01 - logprior: -3.8592e-01
Epoch 6/10
59/59 - 7s - loss: 31.4797 - loglik: -3.0937e+01 - logprior: -3.7789e-01
Fitted a model with MAP estimate = -30.7283
Time for alignment: 159.0007
Computed alignments with likelihoods: ['-31.3340', '-31.7683', '-30.7283']
Best model has likelihood: -30.7283
time for generating output: 0.2048
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.29093369418132614
Training of 3 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fddc0940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1880fb7550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1833e36e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1817b6c2b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f194804b460>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1817b75f70>, <__main__.SimpleDirichletPrior object at 0x7f193e31c670>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1934cd0820>

Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 474.0642 - loglik: -4.5014e+02 - logprior: -2.3898e+01
Epoch 2/10
14/14 - 4s - loss: 410.7167 - loglik: -4.0661e+02 - logprior: -4.0606e+00
Epoch 3/10
14/14 - 4s - loss: 370.7970 - loglik: -3.6828e+02 - logprior: -2.3701e+00
Epoch 4/10
14/14 - 4s - loss: 357.9082 - loglik: -3.5498e+02 - logprior: -2.4357e+00
Epoch 5/10
14/14 - 4s - loss: 354.4801 - loglik: -3.5142e+02 - logprior: -2.3525e+00
Epoch 6/10
14/14 - 4s - loss: 351.7783 - loglik: -3.4898e+02 - logprior: -2.1616e+00
Epoch 7/10
14/14 - 4s - loss: 351.8386 - loglik: -3.4920e+02 - logprior: -2.0486e+00
Fitted a model with MAP estimate = -349.5369
expansions: [(10, 1), (11, 1), (16, 5), (18, 1), (35, 1), (36, 3), (38, 2), (41, 1), (42, 1), (44, 1), (45, 1), (66, 3), (76, 1), (78, 2), (79, 4), (100, 1), (102, 2), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 161 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 379.4709 - loglik: -3.5013e+02 - logprior: -2.8708e+01
Epoch 2/2
14/14 - 5s - loss: 352.9403 - loglik: -3.4179e+02 - logprior: -1.0485e+01
Fitted a model with MAP estimate = -347.0356
expansions: [(0, 20)]
discards: [  0  12  13  45  49  62  63  64 100 103 130]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 10s - loss: 370.2531 - loglik: -3.4800e+02 - logprior: -2.1730e+01
Epoch 2/2
14/14 - 5s - loss: 349.1101 - loglik: -3.4487e+02 - logprior: -3.7106e+00
Fitted a model with MAP estimate = -344.3304
expansions: [(31, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 368.8506 - loglik: -3.4273e+02 - logprior: -2.5650e+01
Epoch 2/10
14/14 - 4s - loss: 346.0195 - loglik: -3.4167e+02 - logprior: -3.8188e+00
Epoch 3/10
14/14 - 4s - loss: 340.7439 - loglik: -3.4007e+02 - logprior: -6.6542e-02
Epoch 4/10
14/14 - 4s - loss: 337.7092 - loglik: -3.3808e+02 - logprior: 1.0159
Epoch 5/10
14/14 - 4s - loss: 336.0287 - loglik: -3.3686e+02 - logprior: 1.4724
Epoch 6/10
14/14 - 4s - loss: 335.6446 - loglik: -3.3682e+02 - logprior: 1.8236
Epoch 7/10
14/14 - 4s - loss: 334.9777 - loglik: -3.3644e+02 - logprior: 2.0759
Epoch 8/10
14/14 - 4s - loss: 333.9083 - loglik: -3.3565e+02 - logprior: 2.3278
Epoch 9/10
14/14 - 4s - loss: 333.1823 - loglik: -3.3515e+02 - logprior: 2.5596
Epoch 10/10
14/14 - 4s - loss: 333.5880 - loglik: -3.3572e+02 - logprior: 2.7156
Fitted a model with MAP estimate = -332.4193
Time for alignment: 116.6905
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 474.0464 - loglik: -4.5014e+02 - logprior: -2.3885e+01
Epoch 2/10
14/14 - 4s - loss: 413.2457 - loglik: -4.0917e+02 - logprior: -4.0335e+00
Epoch 3/10
14/14 - 4s - loss: 378.2701 - loglik: -3.7592e+02 - logprior: -2.1310e+00
Epoch 4/10
14/14 - 4s - loss: 362.3358 - loglik: -3.5958e+02 - logprior: -2.0592e+00
Epoch 5/10
14/14 - 4s - loss: 354.1753 - loglik: -3.5122e+02 - logprior: -2.1957e+00
Epoch 6/10
14/14 - 4s - loss: 353.4178 - loglik: -3.5087e+02 - logprior: -2.0030e+00
Epoch 7/10
14/14 - 4s - loss: 350.3762 - loglik: -3.4779e+02 - logprior: -2.0323e+00
Epoch 8/10
14/14 - 4s - loss: 349.8110 - loglik: -3.4724e+02 - logprior: -1.9946e+00
Epoch 9/10
14/14 - 4s - loss: 348.6022 - loglik: -3.4605e+02 - logprior: -1.9706e+00
Epoch 10/10
14/14 - 4s - loss: 350.1692 - loglik: -3.4762e+02 - logprior: -1.9593e+00
Fitted a model with MAP estimate = -348.2538
expansions: [(3, 1), (11, 2), (15, 1), (16, 5), (17, 1), (34, 1), (36, 1), (38, 2), (43, 1), (44, 1), (45, 1), (66, 1), (67, 1), (74, 1), (76, 1), (78, 2), (79, 4), (100, 1), (101, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 159 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 380.7506 - loglik: -3.5139e+02 - logprior: -2.8750e+01
Epoch 2/2
14/14 - 5s - loss: 352.5288 - loglik: -3.4136e+02 - logprior: -1.0480e+01
Fitted a model with MAP estimate = -347.8307
expansions: [(0, 20), (59, 1)]
discards: [  0  11  12  13  14  49  61  62  99 102]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 371.0916 - loglik: -3.4878e+02 - logprior: -2.1759e+01
Epoch 2/2
14/14 - 5s - loss: 348.3516 - loglik: -3.4402e+02 - logprior: -3.7997e+00
Fitted a model with MAP estimate = -344.2085
expansions: [(30, 2), (73, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 75]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 368.3233 - loglik: -3.4186e+02 - logprior: -2.5967e+01
Epoch 2/10
14/14 - 4s - loss: 345.8043 - loglik: -3.4119e+02 - logprior: -4.0996e+00
Epoch 3/10
14/14 - 4s - loss: 339.8600 - loglik: -3.3904e+02 - logprior: -2.1587e-01
Epoch 4/10
14/14 - 4s - loss: 336.8806 - loglik: -3.3709e+02 - logprior: 0.8592
Epoch 5/10
14/14 - 4s - loss: 336.3515 - loglik: -3.3702e+02 - logprior: 1.3136
Epoch 6/10
14/14 - 4s - loss: 333.9914 - loglik: -3.3506e+02 - logprior: 1.6966
Epoch 7/10
14/14 - 4s - loss: 333.0344 - loglik: -3.3447e+02 - logprior: 2.0143
Epoch 8/10
14/14 - 4s - loss: 334.4135 - loglik: -3.3612e+02 - logprior: 2.2759
Fitted a model with MAP estimate = -332.5131
Time for alignment: 118.8076
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 475.1552 - loglik: -4.5124e+02 - logprior: -2.3894e+01
Epoch 2/10
14/14 - 4s - loss: 406.9441 - loglik: -4.0294e+02 - logprior: -3.9729e+00
Epoch 3/10
14/14 - 4s - loss: 368.1549 - loglik: -3.6592e+02 - logprior: -2.1015e+00
Epoch 4/10
14/14 - 4s - loss: 359.6954 - loglik: -3.5731e+02 - logprior: -1.8860e+00
Epoch 5/10
14/14 - 4s - loss: 355.4482 - loglik: -3.5309e+02 - logprior: -1.7183e+00
Epoch 6/10
14/14 - 4s - loss: 353.1808 - loglik: -3.5097e+02 - logprior: -1.6076e+00
Epoch 7/10
14/14 - 4s - loss: 353.4102 - loglik: -3.5122e+02 - logprior: -1.6147e+00
Fitted a model with MAP estimate = -351.7514
expansions: [(12, 1), (16, 6), (17, 1), (36, 3), (38, 2), (43, 1), (44, 1), (66, 1), (67, 1), (77, 1), (79, 2), (80, 4), (101, 1), (102, 3), (111, 3), (112, 1), (113, 2)]
discards: [ 0 45 46]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 155 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 383.6747 - loglik: -3.5428e+02 - logprior: -2.8838e+01
Epoch 2/2
14/14 - 4s - loss: 356.2367 - loglik: -3.4502e+02 - logprior: -1.0669e+01
Fitted a model with MAP estimate = -350.7884
expansions: [(0, 22), (57, 3)]
discards: [  0  12  13  44  48  95  98 125]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 368.5386 - loglik: -3.4609e+02 - logprior: -2.1974e+01
Epoch 2/2
14/14 - 5s - loss: 347.1789 - loglik: -3.4281e+02 - logprior: -3.8602e+00
Fitted a model with MAP estimate = -342.5558
expansions: [(33, 2), (75, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 77 78]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 370.1590 - loglik: -3.4385e+02 - logprior: -2.5825e+01
Epoch 2/10
14/14 - 4s - loss: 346.5934 - loglik: -3.4200e+02 - logprior: -4.0902e+00
Epoch 3/10
14/14 - 4s - loss: 341.8963 - loglik: -3.4087e+02 - logprior: -3.9948e-01
Epoch 4/10
14/14 - 4s - loss: 337.8181 - loglik: -3.3783e+02 - logprior: 0.6821
Epoch 5/10
14/14 - 4s - loss: 337.4280 - loglik: -3.3782e+02 - logprior: 1.1073
Epoch 6/10
14/14 - 4s - loss: 335.6339 - loglik: -3.3649e+02 - logprior: 1.5169
Epoch 7/10
14/14 - 4s - loss: 334.1171 - loglik: -3.3535e+02 - logprior: 1.8218
Epoch 8/10
14/14 - 4s - loss: 335.7092 - loglik: -3.3718e+02 - logprior: 2.0667
Fitted a model with MAP estimate = -333.5691
Time for alignment: 107.9315
Computed alignments with likelihoods: ['-332.4193', '-332.5131', '-333.5691']
Best model has likelihood: -332.4193
time for generating output: 0.2048
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.9709962168978562
Training of 3 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f184e13ae20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1878177970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f196a645d90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1994bf2490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1994bf2730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1946ff8f10>, <__main__.SimpleDirichletPrior object at 0x7f1833aba4c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fd4a6e50>

Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 25s - loss: 858.6881 - loglik: -8.5554e+02 - logprior: -3.0280e+00
Epoch 2/10
29/29 - 21s - loss: 673.6667 - loglik: -6.7103e+02 - logprior: -2.1966e+00
Epoch 3/10
29/29 - 22s - loss: 643.1511 - loglik: -6.3977e+02 - logprior: -2.6101e+00
Epoch 4/10
29/29 - 21s - loss: 637.4849 - loglik: -6.3421e+02 - logprior: -2.6396e+00
Epoch 5/10
29/29 - 21s - loss: 633.7744 - loglik: -6.3061e+02 - logprior: -2.6236e+00
Epoch 6/10
29/29 - 22s - loss: 630.4348 - loglik: -6.2730e+02 - logprior: -2.6142e+00
Epoch 7/10
29/29 - 22s - loss: 631.7992 - loglik: -6.2866e+02 - logprior: -2.6241e+00
Fitted a model with MAP estimate = -630.5005
expansions: [(16, 1), (22, 1), (24, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (48, 1), (49, 1), (50, 2), (77, 1), (87, 1), (89, 1), (90, 3), (94, 1), (119, 2), (120, 2), (121, 2), (124, 2), (125, 1), (128, 1), (142, 2), (144, 1), (151, 1), (155, 2), (163, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 1), (191, 1), (192, 1), (205, 1), (218, 3), (219, 2), (249, 1), (250, 1), (252, 1), (253, 3), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 345 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 34s - loss: 621.1006 - loglik: -6.1459e+02 - logprior: -5.8410e+00
Epoch 2/2
29/29 - 29s - loss: 592.9709 - loglik: -5.8887e+02 - logprior: -3.2971e+00
Fitted a model with MAP estimate = -587.7027
expansions: [(0, 2), (37, 1)]
discards: [  0  60 105 137 143 149 265 304 305 329 330]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 598.1682 - loglik: -5.9373e+02 - logprior: -3.6135e+00
Epoch 2/2
29/29 - 28s - loss: 590.6751 - loglik: -5.8896e+02 - logprior: -9.1104e-01
Fitted a model with MAP estimate = -586.7980
expansions: [(323, 2)]
discards: [  0 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 33s - loss: 599.5408 - loglik: -5.9369e+02 - logprior: -4.9756e+00
Epoch 2/10
29/29 - 28s - loss: 591.0011 - loglik: -5.8911e+02 - logprior: -1.0571e+00
Epoch 3/10
29/29 - 28s - loss: 585.5347 - loglik: -5.8446e+02 - logprior: -2.6221e-01
Epoch 4/10
29/29 - 28s - loss: 584.9565 - loglik: -5.8431e+02 - logprior: 0.1007
Epoch 5/10
29/29 - 28s - loss: 581.3931 - loglik: -5.8080e+02 - logprior: 0.0646
Epoch 6/10
29/29 - 28s - loss: 584.2676 - loglik: -5.8406e+02 - logprior: 0.4142
Fitted a model with MAP estimate = -581.6907
Time for alignment: 557.7265
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 25s - loss: 859.7366 - loglik: -8.5659e+02 - logprior: -3.0141e+00
Epoch 2/10
29/29 - 22s - loss: 674.0196 - loglik: -6.7147e+02 - logprior: -2.2852e+00
Epoch 3/10
29/29 - 22s - loss: 645.9113 - loglik: -6.4252e+02 - logprior: -2.7565e+00
Epoch 4/10
29/29 - 22s - loss: 637.6572 - loglik: -6.3423e+02 - logprior: -2.7642e+00
Epoch 5/10
29/29 - 22s - loss: 633.1980 - loglik: -6.2986e+02 - logprior: -2.7397e+00
Epoch 6/10
29/29 - 22s - loss: 634.9375 - loglik: -6.3163e+02 - logprior: -2.7618e+00
Fitted a model with MAP estimate = -632.4880
expansions: [(16, 1), (22, 1), (24, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 2), (42, 1), (48, 2), (49, 1), (50, 1), (72, 1), (76, 1), (87, 1), (88, 2), (89, 2), (121, 3), (124, 1), (125, 1), (128, 1), (142, 2), (152, 1), (153, 1), (155, 1), (163, 1), (172, 1), (173, 1), (183, 1), (185, 1), (186, 1), (191, 1), (192, 1), (204, 1), (205, 1), (217, 2), (218, 2), (219, 1), (233, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 621.2999 - loglik: -6.1502e+02 - logprior: -5.6017e+00
Epoch 2/2
29/29 - 28s - loss: 595.0204 - loglik: -5.9150e+02 - logprior: -2.7623e+00
Fitted a model with MAP estimate = -590.3566
expansions: [(0, 2)]
discards: [  0  62 105 142 168 261 326]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 599.1616 - loglik: -5.9468e+02 - logprior: -3.5895e+00
Epoch 2/2
29/29 - 28s - loss: 590.7742 - loglik: -5.8902e+02 - logprior: -9.1045e-01
Fitted a model with MAP estimate = -587.7345
expansions: [(107, 1)]
discards: [  0 325]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 31s - loss: 599.4470 - loglik: -5.9356e+02 - logprior: -4.9363e+00
Epoch 2/10
29/29 - 28s - loss: 590.0236 - loglik: -5.8803e+02 - logprior: -1.1347e+00
Epoch 3/10
29/29 - 28s - loss: 588.2252 - loglik: -5.8706e+02 - logprior: -3.5403e-01
Epoch 4/10
29/29 - 28s - loss: 583.1420 - loglik: -5.8243e+02 - logprior: 0.0220
Epoch 5/10
29/29 - 28s - loss: 587.0618 - loglik: -5.8644e+02 - logprior: 0.0381
Fitted a model with MAP estimate = -583.3964
Time for alignment: 505.2136
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 25s - loss: 857.0953 - loglik: -8.5395e+02 - logprior: -3.0294e+00
Epoch 2/10
29/29 - 22s - loss: 674.0027 - loglik: -6.7151e+02 - logprior: -2.1163e+00
Epoch 3/10
29/29 - 22s - loss: 642.9843 - loglik: -6.3982e+02 - logprior: -2.4563e+00
Epoch 4/10
29/29 - 22s - loss: 636.0626 - loglik: -6.3282e+02 - logprior: -2.6058e+00
Epoch 5/10
29/29 - 22s - loss: 632.6816 - loglik: -6.2950e+02 - logprior: -2.6445e+00
Epoch 6/10
29/29 - 21s - loss: 632.5153 - loglik: -6.2936e+02 - logprior: -2.6569e+00
Epoch 7/10
29/29 - 22s - loss: 632.3303 - loglik: -6.2916e+02 - logprior: -2.6739e+00
Epoch 8/10
29/29 - 22s - loss: 630.5516 - loglik: -6.2738e+02 - logprior: -2.6861e+00
Epoch 9/10
29/29 - 21s - loss: 630.1654 - loglik: -6.2700e+02 - logprior: -2.6972e+00
Epoch 10/10
29/29 - 22s - loss: 631.2676 - loglik: -6.2809e+02 - logprior: -2.7029e+00
Fitted a model with MAP estimate = -629.7574
expansions: [(16, 1), (22, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 2), (41, 1), (48, 2), (49, 1), (50, 1), (73, 1), (87, 1), (89, 1), (90, 3), (94, 2), (119, 2), (120, 2), (121, 2), (124, 1), (125, 1), (126, 1), (142, 1), (144, 1), (151, 1), (153, 1), (154, 1), (155, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 2), (190, 1), (191, 1), (205, 1), (217, 2), (218, 2), (219, 1), (234, 1), (248, 1), (249, 1), (251, 1), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 345 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 32s - loss: 617.2462 - loglik: -6.1126e+02 - logprior: -5.4031e+00
Epoch 2/2
29/29 - 29s - loss: 592.2338 - loglik: -5.8878e+02 - logprior: -2.6713e+00
Fitted a model with MAP estimate = -586.3504
expansions: [(0, 2)]
discards: [  0  62 107 146 263 329 330]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 33s - loss: 595.3038 - loglik: -5.9081e+02 - logprior: -3.5901e+00
Epoch 2/2
29/29 - 28s - loss: 584.6882 - loglik: -5.8308e+02 - logprior: -7.3814e-01
Fitted a model with MAP estimate = -583.7286
expansions: [(326, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 32s - loss: 595.2579 - loglik: -5.8933e+02 - logprior: -4.9640e+00
Epoch 2/10
29/29 - 29s - loss: 585.2261 - loglik: -5.8305e+02 - logprior: -1.2836e+00
Epoch 3/10
29/29 - 29s - loss: 581.6850 - loglik: -5.8089e+02 - logprior: 0.0248
Epoch 4/10
29/29 - 29s - loss: 579.6209 - loglik: -5.7890e+02 - logprior: 0.0278
Epoch 5/10
29/29 - 29s - loss: 580.6462 - loglik: -5.8022e+02 - logprior: 0.2366
Fitted a model with MAP estimate = -577.3867
Time for alignment: 596.8515
Computed alignments with likelihoods: ['-581.6907', '-583.3964', '-577.3867']
Best model has likelihood: -577.3867
time for generating output: 0.4174
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.8772793053545586
Training of 3 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1994864d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18564106a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18a2254d30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f166430ab20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f166430a730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f193db1a310>, <__main__.SimpleDirichletPrior object at 0x7f184e13a370>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19a5f11dc0>

Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 619.6025 - loglik: -1.2835e+02 - logprior: -4.9123e+02
Epoch 2/10
10/10 - 1s - loss: 244.2982 - loglik: -1.0685e+02 - logprior: -1.3744e+02
Epoch 3/10
10/10 - 1s - loss: 151.4829 - loglik: -8.6722e+01 - logprior: -6.4726e+01
Epoch 4/10
10/10 - 1s - loss: 110.7556 - loglik: -7.3483e+01 - logprior: -3.7201e+01
Epoch 5/10
10/10 - 1s - loss: 92.3560 - loglik: -6.9824e+01 - logprior: -2.2485e+01
Epoch 6/10
10/10 - 0s - loss: 82.4428 - loglik: -6.9612e+01 - logprior: -1.2798e+01
Epoch 7/10
10/10 - 0s - loss: 76.6507 - loglik: -7.0107e+01 - logprior: -6.5186e+00
Epoch 8/10
10/10 - 0s - loss: 73.1245 - loglik: -7.0407e+01 - logprior: -2.6994e+00
Epoch 9/10
10/10 - 0s - loss: 70.7828 - loglik: -7.0593e+01 - logprior: -1.7940e-01
Epoch 10/10
10/10 - 1s - loss: 69.1118 - loglik: -7.0788e+01 - logprior: 1.6818
Fitted a model with MAP estimate = -68.3576
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 717.3636 - loglik: -6.2120e+01 - logprior: -6.5520e+02
Epoch 2/2
10/10 - 0s - loss: 261.2766 - loglik: -5.4703e+01 - logprior: -2.0650e+02
Fitted a model with MAP estimate = -174.7331
expansions: []
discards: [23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 508.3477 - loglik: -4.9751e+01 - logprior: -4.5850e+02
Epoch 2/2
10/10 - 0s - loss: 173.9915 - loglik: -4.9680e+01 - logprior: -1.2423e+02
Fitted a model with MAP estimate = -124.0613
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 479.7180 - loglik: -4.8781e+01 - logprior: -4.3085e+02
Epoch 2/10
10/10 - 0s - loss: 166.4890 - loglik: -4.9605e+01 - logprior: -1.1678e+02
Epoch 3/10
10/10 - 0s - loss: 100.5902 - loglik: -5.0418e+01 - logprior: -5.0151e+01
Epoch 4/10
10/10 - 1s - loss: 72.5746 - loglik: -5.0996e+01 - logprior: -2.1578e+01
Epoch 5/10
10/10 - 1s - loss: 57.1550 - loglik: -5.1506e+01 - logprior: -5.6466e+00
Epoch 6/10
10/10 - 0s - loss: 48.1475 - loglik: -5.1951e+01 - logprior: 3.8070
Epoch 7/10
10/10 - 1s - loss: 42.5284 - loglik: -5.2286e+01 - logprior: 9.7601
Epoch 8/10
10/10 - 0s - loss: 38.7019 - loglik: -5.2560e+01 - logprior: 13.8611
Epoch 9/10
10/10 - 1s - loss: 35.8533 - loglik: -5.2783e+01 - logprior: 16.9315
Epoch 10/10
10/10 - 1s - loss: 33.5634 - loglik: -5.2963e+01 - logprior: 19.4007
Fitted a model with MAP estimate = -32.4474
Time for alignment: 28.5113
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 619.6025 - loglik: -1.2835e+02 - logprior: -4.9123e+02
Epoch 2/10
10/10 - 0s - loss: 244.2982 - loglik: -1.0685e+02 - logprior: -1.3744e+02
Epoch 3/10
10/10 - 1s - loss: 151.4832 - loglik: -8.6722e+01 - logprior: -6.4726e+01
Epoch 4/10
10/10 - 1s - loss: 110.7556 - loglik: -7.3483e+01 - logprior: -3.7201e+01
Epoch 5/10
10/10 - 1s - loss: 92.3560 - loglik: -6.9824e+01 - logprior: -2.2485e+01
Epoch 6/10
10/10 - 0s - loss: 82.4428 - loglik: -6.9612e+01 - logprior: -1.2798e+01
Epoch 7/10
10/10 - 0s - loss: 76.6507 - loglik: -7.0107e+01 - logprior: -6.5186e+00
Epoch 8/10
10/10 - 1s - loss: 73.1245 - loglik: -7.0407e+01 - logprior: -2.6994e+00
Epoch 9/10
10/10 - 1s - loss: 70.7828 - loglik: -7.0593e+01 - logprior: -1.7940e-01
Epoch 10/10
10/10 - 1s - loss: 69.1117 - loglik: -7.0788e+01 - logprior: 1.6818
Fitted a model with MAP estimate = -68.3575
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 717.3637 - loglik: -6.2120e+01 - logprior: -6.5520e+02
Epoch 2/2
10/10 - 0s - loss: 261.2767 - loglik: -5.4703e+01 - logprior: -2.0650e+02
Fitted a model with MAP estimate = -174.7331
expansions: []
discards: [23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 508.3477 - loglik: -4.9751e+01 - logprior: -4.5850e+02
Epoch 2/2
10/10 - 0s - loss: 173.9915 - loglik: -4.9680e+01 - logprior: -1.2423e+02
Fitted a model with MAP estimate = -124.0613
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 479.7181 - loglik: -4.8781e+01 - logprior: -4.3085e+02
Epoch 2/10
10/10 - 1s - loss: 166.4892 - loglik: -4.9605e+01 - logprior: -1.1678e+02
Epoch 3/10
10/10 - 1s - loss: 100.5903 - loglik: -5.0418e+01 - logprior: -5.0151e+01
Epoch 4/10
10/10 - 0s - loss: 72.5748 - loglik: -5.0996e+01 - logprior: -2.1578e+01
Epoch 5/10
10/10 - 1s - loss: 57.1552 - loglik: -5.1506e+01 - logprior: -5.6469e+00
Epoch 6/10
10/10 - 1s - loss: 48.1476 - loglik: -5.1951e+01 - logprior: 3.8067
Epoch 7/10
10/10 - 1s - loss: 42.5286 - loglik: -5.2286e+01 - logprior: 9.7599
Epoch 8/10
10/10 - 0s - loss: 38.7020 - loglik: -5.2560e+01 - logprior: 13.8609
Epoch 9/10
10/10 - 0s - loss: 35.8533 - loglik: -5.2783e+01 - logprior: 16.9313
Epoch 10/10
10/10 - 0s - loss: 33.5634 - loglik: -5.2963e+01 - logprior: 19.4005
Fitted a model with MAP estimate = -32.4474
Time for alignment: 28.5332
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 619.6025 - loglik: -1.2835e+02 - logprior: -4.9123e+02
Epoch 2/10
10/10 - 1s - loss: 244.2988 - loglik: -1.0685e+02 - logprior: -1.3744e+02
Epoch 3/10
10/10 - 1s - loss: 151.4828 - loglik: -8.6722e+01 - logprior: -6.4726e+01
Epoch 4/10
10/10 - 1s - loss: 110.7552 - loglik: -7.3483e+01 - logprior: -3.7201e+01
Epoch 5/10
10/10 - 1s - loss: 92.3559 - loglik: -6.9824e+01 - logprior: -2.2485e+01
Epoch 6/10
10/10 - 1s - loss: 82.4428 - loglik: -6.9612e+01 - logprior: -1.2798e+01
Epoch 7/10
10/10 - 1s - loss: 76.6507 - loglik: -7.0107e+01 - logprior: -6.5186e+00
Epoch 8/10
10/10 - 1s - loss: 73.1245 - loglik: -7.0407e+01 - logprior: -2.6994e+00
Epoch 9/10
10/10 - 1s - loss: 70.7828 - loglik: -7.0593e+01 - logprior: -1.7940e-01
Epoch 10/10
10/10 - 1s - loss: 69.1118 - loglik: -7.0788e+01 - logprior: 1.6818
Fitted a model with MAP estimate = -68.3576
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 717.3636 - loglik: -6.2120e+01 - logprior: -6.5520e+02
Epoch 2/2
10/10 - 1s - loss: 261.2766 - loglik: -5.4703e+01 - logprior: -2.0650e+02
Fitted a model with MAP estimate = -174.7332
expansions: []
discards: [23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 508.3477 - loglik: -4.9751e+01 - logprior: -4.5850e+02
Epoch 2/2
10/10 - 0s - loss: 173.9915 - loglik: -4.9680e+01 - logprior: -1.2423e+02
Fitted a model with MAP estimate = -124.0612
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 479.7181 - loglik: -4.8781e+01 - logprior: -4.3085e+02
Epoch 2/10
10/10 - 0s - loss: 166.4892 - loglik: -4.9605e+01 - logprior: -1.1678e+02
Epoch 3/10
10/10 - 0s - loss: 100.5904 - loglik: -5.0418e+01 - logprior: -5.0151e+01
Epoch 4/10
10/10 - 0s - loss: 72.5749 - loglik: -5.0996e+01 - logprior: -2.1578e+01
Epoch 5/10
10/10 - 0s - loss: 57.1553 - loglik: -5.1506e+01 - logprior: -5.6468e+00
Epoch 6/10
10/10 - 0s - loss: 48.1478 - loglik: -5.1951e+01 - logprior: 3.8068
Epoch 7/10
10/10 - 0s - loss: 42.5287 - loglik: -5.2286e+01 - logprior: 9.7599
Epoch 8/10
10/10 - 1s - loss: 38.7022 - loglik: -5.2560e+01 - logprior: 13.8609
Epoch 9/10
10/10 - 1s - loss: 35.8535 - loglik: -5.2783e+01 - logprior: 16.9313
Epoch 10/10
10/10 - 1s - loss: 33.5637 - loglik: -5.2963e+01 - logprior: 19.4005
Fitted a model with MAP estimate = -32.4477
Time for alignment: 28.7569
Computed alignments with likelihoods: ['-32.4474', '-32.4474', '-32.4477']
Best model has likelihood: -32.4474
time for generating output: 0.4880
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.7083333333333334
Training of 3 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19ae06b0a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fe16f490>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198c33ec10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193ed3e5e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1880a81be0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1880af0910>, <__main__.SimpleDirichletPrior object at 0x7f186f46ed00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18a3089c10>

Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 476.5376 - loglik: -4.7216e+02 - logprior: -4.2983e+00
Epoch 2/10
16/16 - 5s - loss: 439.8676 - loglik: -4.3813e+02 - logprior: -1.1311e+00
Epoch 3/10
16/16 - 5s - loss: 414.5002 - loglik: -4.1166e+02 - logprior: -1.5528e+00
Epoch 4/10
16/16 - 5s - loss: 403.5785 - loglik: -4.0026e+02 - logprior: -1.6834e+00
Epoch 5/10
16/16 - 5s - loss: 400.7589 - loglik: -3.9775e+02 - logprior: -1.6014e+00
Epoch 6/10
16/16 - 5s - loss: 398.6112 - loglik: -3.9575e+02 - logprior: -1.6713e+00
Epoch 7/10
16/16 - 5s - loss: 396.0286 - loglik: -3.9332e+02 - logprior: -1.6928e+00
Epoch 8/10
16/16 - 5s - loss: 396.1902 - loglik: -3.9357e+02 - logprior: -1.7056e+00
Fitted a model with MAP estimate = -394.8053
expansions: [(15, 4), (20, 1), (23, 1), (28, 2), (30, 1), (41, 1), (48, 4), (50, 2), (56, 1), (58, 2), (69, 1), (72, 2), (74, 3), (94, 3), (95, 1), (96, 2), (99, 1), (114, 1), (117, 2), (120, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 13s - loss: 408.4602 - loglik: -4.0287e+02 - logprior: -4.2195e+00
Epoch 2/2
33/33 - 9s - loss: 390.5529 - loglik: -3.8698e+02 - logprior: -1.6024e+00
Fitted a model with MAP estimate = -384.8525
expansions: [(180, 2)]
discards: [  0  16  17  34  35  63  92  98 150 178 179]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 401.9019 - loglik: -3.9547e+02 - logprior: -4.0297e+00
Epoch 2/2
33/33 - 8s - loss: 393.0288 - loglik: -3.8959e+02 - logprior: -1.3152e+00
Fitted a model with MAP estimate = -387.5356
expansions: [(0, 1), (171, 2)]
discards: [114 169 170]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 397.5636 - loglik: -3.9239e+02 - logprior: -2.7412e+00
Epoch 2/10
33/33 - 8s - loss: 389.7258 - loglik: -3.8666e+02 - logprior: -9.4400e-01
Epoch 3/10
33/33 - 8s - loss: 388.9911 - loglik: -3.8622e+02 - logprior: -7.7105e-01
Epoch 4/10
33/33 - 8s - loss: 385.2998 - loglik: -3.8284e+02 - logprior: -7.0283e-01
Epoch 5/10
33/33 - 8s - loss: 384.5719 - loglik: -3.8242e+02 - logprior: -6.2577e-01
Epoch 6/10
33/33 - 8s - loss: 383.4764 - loglik: -3.8159e+02 - logprior: -5.5951e-01
Epoch 7/10
33/33 - 8s - loss: 381.7502 - loglik: -3.8009e+02 - logprior: -4.8430e-01
Epoch 8/10
33/33 - 8s - loss: 382.3827 - loglik: -3.8095e+02 - logprior: -3.9544e-01
Fitted a model with MAP estimate = -380.5169
Time for alignment: 195.1383
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 477.2739 - loglik: -4.7288e+02 - logprior: -4.3166e+00
Epoch 2/10
16/16 - 5s - loss: 439.5506 - loglik: -4.3778e+02 - logprior: -1.1627e+00
Epoch 3/10
16/16 - 5s - loss: 413.8184 - loglik: -4.1113e+02 - logprior: -1.6006e+00
Epoch 4/10
16/16 - 5s - loss: 403.2068 - loglik: -4.0026e+02 - logprior: -1.7200e+00
Epoch 5/10
16/16 - 5s - loss: 400.2869 - loglik: -3.9740e+02 - logprior: -1.6448e+00
Epoch 6/10
16/16 - 5s - loss: 399.1524 - loglik: -3.9645e+02 - logprior: -1.6537e+00
Epoch 7/10
16/16 - 5s - loss: 398.4652 - loglik: -3.9593e+02 - logprior: -1.6370e+00
Epoch 8/10
16/16 - 5s - loss: 395.8009 - loglik: -3.9332e+02 - logprior: -1.6464e+00
Epoch 9/10
16/16 - 5s - loss: 397.4227 - loglik: -3.9499e+02 - logprior: -1.6491e+00
Fitted a model with MAP estimate = -395.2128
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 4), (45, 1), (48, 3), (49, 1), (53, 1), (56, 2), (69, 1), (72, 2), (74, 3), (93, 1), (94, 1), (95, 5), (105, 1), (113, 1), (116, 2), (119, 1), (122, 1), (125, 1), (139, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 408.4478 - loglik: -4.0304e+02 - logprior: -4.1928e+00
Epoch 2/2
33/33 - 9s - loss: 390.5564 - loglik: -3.8721e+02 - logprior: -1.5919e+00
Fitted a model with MAP estimate = -385.6912
expansions: [(164, 1), (177, 2)]
discards: [  0  32  33  69  89  95 117 147 174 175 176]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 401.4435 - loglik: -3.9526e+02 - logprior: -3.9809e+00
Epoch 2/2
33/33 - 8s - loss: 393.5874 - loglik: -3.9028e+02 - logprior: -1.2751e+00
Fitted a model with MAP estimate = -387.7868
expansions: [(0, 1), (169, 2)]
discards: [167 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 12s - loss: 397.0843 - loglik: -3.9203e+02 - logprior: -2.7119e+00
Epoch 2/10
33/33 - 8s - loss: 390.2028 - loglik: -3.8725e+02 - logprior: -9.1779e-01
Epoch 3/10
33/33 - 8s - loss: 387.0501 - loglik: -3.8433e+02 - logprior: -7.6422e-01
Epoch 4/10
33/33 - 8s - loss: 385.3386 - loglik: -3.8293e+02 - logprior: -6.9291e-01
Epoch 5/10
33/33 - 8s - loss: 383.8986 - loglik: -3.8176e+02 - logprior: -6.4624e-01
Epoch 6/10
33/33 - 8s - loss: 383.1681 - loglik: -3.8128e+02 - logprior: -5.6356e-01
Epoch 7/10
33/33 - 9s - loss: 381.5594 - loglik: -3.7991e+02 - logprior: -4.9882e-01
Epoch 8/10
33/33 - 8s - loss: 381.9194 - loglik: -3.8047e+02 - logprior: -4.2007e-01
Fitted a model with MAP estimate = -379.9486
Time for alignment: 199.8259
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 10s - loss: 476.8009 - loglik: -4.7242e+02 - logprior: -4.3012e+00
Epoch 2/10
16/16 - 5s - loss: 439.5935 - loglik: -4.3785e+02 - logprior: -1.1353e+00
Epoch 3/10
16/16 - 5s - loss: 414.2613 - loglik: -4.1167e+02 - logprior: -1.4979e+00
Epoch 4/10
16/16 - 5s - loss: 403.7164 - loglik: -4.0083e+02 - logprior: -1.6388e+00
Epoch 5/10
16/16 - 5s - loss: 401.7024 - loglik: -3.9884e+02 - logprior: -1.5737e+00
Epoch 6/10
16/16 - 5s - loss: 399.5863 - loglik: -3.9691e+02 - logprior: -1.6051e+00
Epoch 7/10
16/16 - 5s - loss: 398.6330 - loglik: -3.9611e+02 - logprior: -1.5993e+00
Epoch 8/10
16/16 - 5s - loss: 397.8316 - loglik: -3.9533e+02 - logprior: -1.6359e+00
Epoch 9/10
16/16 - 5s - loss: 395.3253 - loglik: -3.9283e+02 - logprior: -1.6596e+00
Epoch 10/10
16/16 - 5s - loss: 396.7937 - loglik: -3.9434e+02 - logprior: -1.6695e+00
Fitted a model with MAP estimate = -395.0053
expansions: [(14, 1), (24, 1), (27, 2), (28, 3), (29, 1), (42, 1), (43, 1), (48, 2), (50, 2), (53, 1), (56, 2), (72, 1), (73, 2), (74, 3), (94, 4), (95, 1), (96, 1), (98, 1), (116, 3), (122, 1), (123, 1), (128, 1), (129, 1), (139, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 408.7892 - loglik: -4.0335e+02 - logprior: -4.2129e+00
Epoch 2/2
33/33 - 9s - loss: 391.7642 - loglik: -3.8834e+02 - logprior: -1.6111e+00
Fitted a model with MAP estimate = -385.4646
expansions: [(177, 2)]
discards: [  0  31  32  62  90  96 147 175 176]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 401.6769 - loglik: -3.9538e+02 - logprior: -4.0309e+00
Epoch 2/2
33/33 - 9s - loss: 393.0334 - loglik: -3.8959e+02 - logprior: -1.3418e+00
Fitted a model with MAP estimate = -387.6633
expansions: [(0, 1), (30, 2), (170, 2)]
discards: [ 67 168 169]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 172 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 13s - loss: 396.5565 - loglik: -3.9141e+02 - logprior: -2.7123e+00
Epoch 2/10
33/33 - 8s - loss: 389.4748 - loglik: -3.8652e+02 - logprior: -9.0769e-01
Epoch 3/10
33/33 - 9s - loss: 386.8236 - loglik: -3.8413e+02 - logprior: -7.4762e-01
Epoch 4/10
33/33 - 8s - loss: 384.6808 - loglik: -3.8228e+02 - logprior: -6.8277e-01
Epoch 5/10
33/33 - 9s - loss: 383.9667 - loglik: -3.8188e+02 - logprior: -6.0750e-01
Epoch 6/10
33/33 - 8s - loss: 383.1393 - loglik: -3.8128e+02 - logprior: -5.5035e-01
Epoch 7/10
33/33 - 8s - loss: 380.4064 - loglik: -3.7878e+02 - logprior: -4.7143e-01
Epoch 8/10
33/33 - 8s - loss: 380.9503 - loglik: -3.7954e+02 - logprior: -3.9385e-01
Fitted a model with MAP estimate = -379.4786
Time for alignment: 205.5388
Computed alignments with likelihoods: ['-380.5169', '-379.9486', '-379.4786']
Best model has likelihood: -379.4786
time for generating output: 0.2336
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.75
Training of 3 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1961bf8460>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f187864b5b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f199d75b1c0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f198bf07cd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198bf07f40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18189a1e20>, <__main__.SimpleDirichletPrior object at 0x7f1819c0af70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fd5115e0>

Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 290.5476 - loglik: -2.7546e+02 - logprior: -1.5085e+01
Epoch 2/10
10/10 - 2s - loss: 254.9692 - loglik: -2.5091e+02 - logprior: -4.0484e+00
Epoch 3/10
10/10 - 2s - loss: 229.0398 - loglik: -2.2665e+02 - logprior: -2.3794e+00
Epoch 4/10
10/10 - 2s - loss: 212.1791 - loglik: -2.0995e+02 - logprior: -2.0677e+00
Epoch 5/10
10/10 - 2s - loss: 207.5489 - loglik: -2.0515e+02 - logprior: -2.0731e+00
Epoch 6/10
10/10 - 2s - loss: 201.7590 - loglik: -1.9928e+02 - logprior: -2.1690e+00
Epoch 7/10
10/10 - 2s - loss: 204.9513 - loglik: -2.0268e+02 - logprior: -2.0767e+00
Fitted a model with MAP estimate = -202.2309
expansions: [(20, 1), (21, 1), (34, 1), (37, 1), (39, 1), (45, 2), (46, 1), (48, 1), (56, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.1279 - loglik: -2.0742e+02 - logprior: -1.7531e+01
Epoch 2/2
10/10 - 2s - loss: 207.8605 - loglik: -2.0006e+02 - logprior: -7.6569e+00
Fitted a model with MAP estimate = -203.8081
expansions: []
discards: [ 0 19 50]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 219.8848 - loglik: -2.0256e+02 - logprior: -1.7180e+01
Epoch 2/2
10/10 - 2s - loss: 204.0412 - loglik: -1.9773e+02 - logprior: -6.2178e+00
Fitted a model with MAP estimate = -202.2821
expansions: [(0, 11), (48, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 212.8354 - loglik: -1.9805e+02 - logprior: -1.4687e+01
Epoch 2/10
10/10 - 2s - loss: 197.7173 - loglik: -1.9289e+02 - logprior: -4.7350e+00
Epoch 3/10
10/10 - 2s - loss: 190.3031 - loglik: -1.8746e+02 - logprior: -2.7199e+00
Epoch 4/10
10/10 - 2s - loss: 188.4478 - loglik: -1.8616e+02 - logprior: -2.0952e+00
Epoch 5/10
10/10 - 2s - loss: 185.7744 - loglik: -1.8363e+02 - logprior: -1.9122e+00
Epoch 6/10
10/10 - 2s - loss: 185.7415 - loglik: -1.8362e+02 - logprior: -1.8687e+00
Epoch 7/10
10/10 - 2s - loss: 183.9827 - loglik: -1.8196e+02 - logprior: -1.7914e+00
Epoch 8/10
10/10 - 2s - loss: 184.1473 - loglik: -1.8218e+02 - logprior: -1.7448e+00
Fitted a model with MAP estimate = -183.8170
Time for alignment: 52.2308
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 292.9511 - loglik: -2.7786e+02 - logprior: -1.5084e+01
Epoch 2/10
10/10 - 2s - loss: 253.0873 - loglik: -2.4905e+02 - logprior: -4.0242e+00
Epoch 3/10
10/10 - 2s - loss: 226.6003 - loglik: -2.2432e+02 - logprior: -2.2739e+00
Epoch 4/10
10/10 - 2s - loss: 212.7742 - loglik: -2.1072e+02 - logprior: -1.9203e+00
Epoch 5/10
10/10 - 2s - loss: 206.8080 - loglik: -2.0455e+02 - logprior: -1.9287e+00
Epoch 6/10
10/10 - 2s - loss: 204.4148 - loglik: -2.0204e+02 - logprior: -1.9973e+00
Epoch 7/10
10/10 - 2s - loss: 202.2437 - loglik: -2.0007e+02 - logprior: -1.8940e+00
Epoch 8/10
10/10 - 2s - loss: 202.6229 - loglik: -2.0057e+02 - logprior: -1.8283e+00
Fitted a model with MAP estimate = -201.7490
expansions: [(10, 2), (19, 2), (36, 3), (46, 2), (47, 1), (48, 1), (56, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 224.9769 - loglik: -2.0720e+02 - logprior: -1.7549e+01
Epoch 2/2
10/10 - 2s - loss: 207.6834 - loglik: -1.9968e+02 - logprior: -7.8189e+00
Fitted a model with MAP estimate = -200.7366
expansions: [(0, 7), (22, 1), (52, 1)]
discards: [ 0  9 40 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.8021 - loglik: -1.9621e+02 - logprior: -1.4439e+01
Epoch 2/2
10/10 - 2s - loss: 196.3350 - loglik: -1.9177e+02 - logprior: -4.4571e+00
Fitted a model with MAP estimate = -191.0086
expansions: []
discards: [0 1 2 3 4 5]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 210.9871 - loglik: -1.9451e+02 - logprior: -1.6378e+01
Epoch 2/10
10/10 - 2s - loss: 197.5322 - loglik: -1.9231e+02 - logprior: -5.1335e+00
Epoch 3/10
10/10 - 2s - loss: 193.4770 - loglik: -1.9076e+02 - logprior: -2.6121e+00
Epoch 4/10
10/10 - 2s - loss: 191.8650 - loglik: -1.8971e+02 - logprior: -1.9611e+00
Epoch 5/10
10/10 - 2s - loss: 189.7081 - loglik: -1.8784e+02 - logprior: -1.6031e+00
Epoch 6/10
10/10 - 2s - loss: 189.6276 - loglik: -1.8798e+02 - logprior: -1.3832e+00
Epoch 7/10
10/10 - 2s - loss: 188.1777 - loglik: -1.8666e+02 - logprior: -1.3004e+00
Epoch 8/10
10/10 - 2s - loss: 188.5397 - loglik: -1.8702e+02 - logprior: -1.2946e+00
Fitted a model with MAP estimate = -187.7766
Time for alignment: 52.4544
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 291.6751 - loglik: -2.7659e+02 - logprior: -1.5079e+01
Epoch 2/10
10/10 - 2s - loss: 252.7115 - loglik: -2.4865e+02 - logprior: -4.0539e+00
Epoch 3/10
10/10 - 2s - loss: 227.7576 - loglik: -2.2529e+02 - logprior: -2.4536e+00
Epoch 4/10
10/10 - 2s - loss: 212.6879 - loglik: -2.1038e+02 - logprior: -2.1626e+00
Epoch 5/10
10/10 - 2s - loss: 206.1877 - loglik: -2.0371e+02 - logprior: -2.1612e+00
Epoch 6/10
10/10 - 2s - loss: 201.8171 - loglik: -1.9914e+02 - logprior: -2.2850e+00
Epoch 7/10
10/10 - 2s - loss: 200.9227 - loglik: -1.9835e+02 - logprior: -2.2459e+00
Epoch 8/10
10/10 - 2s - loss: 199.9482 - loglik: -1.9753e+02 - logprior: -2.1805e+00
Epoch 9/10
10/10 - 2s - loss: 199.3677 - loglik: -1.9701e+02 - logprior: -2.1549e+00
Epoch 10/10
10/10 - 2s - loss: 199.8993 - loglik: -1.9755e+02 - logprior: -2.1542e+00
Fitted a model with MAP estimate = -198.9723
expansions: [(20, 1), (27, 1), (34, 1), (35, 1), (37, 1), (39, 1), (44, 1), (45, 1), (48, 2), (56, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 226.6410 - loglik: -2.0880e+02 - logprior: -1.7607e+01
Epoch 2/2
10/10 - 2s - loss: 204.0977 - loglik: -1.9608e+02 - logprior: -7.6684e+00
Fitted a model with MAP estimate = -200.6397
expansions: [(0, 7)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 209.3929 - loglik: -1.9465e+02 - logprior: -1.4441e+01
Epoch 2/2
10/10 - 2s - loss: 191.7950 - loglik: -1.8721e+02 - logprior: -4.3974e+00
Fitted a model with MAP estimate = -189.0181
expansions: []
discards: [ 1  2  3  4  5 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 206.1844 - loglik: -1.9207e+02 - logprior: -1.3966e+01
Epoch 2/10
10/10 - 2s - loss: 195.5492 - loglik: -1.9123e+02 - logprior: -4.1739e+00
Epoch 3/10
10/10 - 2s - loss: 191.9101 - loglik: -1.8943e+02 - logprior: -2.3065e+00
Epoch 4/10
10/10 - 2s - loss: 191.1655 - loglik: -1.8922e+02 - logprior: -1.6999e+00
Epoch 5/10
10/10 - 2s - loss: 189.8504 - loglik: -1.8804e+02 - logprior: -1.5137e+00
Epoch 6/10
10/10 - 2s - loss: 188.8465 - loglik: -1.8715e+02 - logprior: -1.4206e+00
Epoch 7/10
10/10 - 2s - loss: 187.8681 - loglik: -1.8633e+02 - logprior: -1.2923e+00
Epoch 8/10
10/10 - 2s - loss: 187.4876 - loglik: -1.8605e+02 - logprior: -1.2162e+00
Epoch 9/10
10/10 - 2s - loss: 186.3358 - loglik: -1.8496e+02 - logprior: -1.1730e+00
Epoch 10/10
10/10 - 2s - loss: 187.7356 - loglik: -1.8635e+02 - logprior: -1.1708e+00
Fitted a model with MAP estimate = -186.8197
Time for alignment: 60.0779
Computed alignments with likelihoods: ['-183.8170', '-187.7766', '-186.8197']
Best model has likelihood: -183.8170
time for generating output: 0.2132
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.5753063147973609
Training of 3 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fdefbb80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1819ef0b20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1819ef0ee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1934a322e0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f181a7b30d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f193d660fd0>, <__main__.SimpleDirichletPrior object at 0x7f1833a22be0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f199d41e700>

Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 615.7791 - loglik: -5.6148e+02 - logprior: -5.4286e+01
Epoch 2/10
10/10 - 3s - loss: 518.2056 - loglik: -5.0791e+02 - logprior: -1.0266e+01
Epoch 3/10
10/10 - 3s - loss: 457.1208 - loglik: -4.5379e+02 - logprior: -3.2386e+00
Epoch 4/10
10/10 - 3s - loss: 420.3326 - loglik: -4.1864e+02 - logprior: -1.3968e+00
Epoch 5/10
10/10 - 3s - loss: 405.3209 - loglik: -4.0374e+02 - logprior: -9.4598e-01
Epoch 6/10
10/10 - 3s - loss: 398.4753 - loglik: -3.9740e+02 - logprior: -3.5539e-01
Epoch 7/10
10/10 - 3s - loss: 398.1779 - loglik: -3.9767e+02 - logprior: 0.0207
Epoch 8/10
10/10 - 3s - loss: 396.5839 - loglik: -3.9636e+02 - logprior: 0.2095
Epoch 9/10
10/10 - 3s - loss: 394.6781 - loglik: -3.9460e+02 - logprior: 0.3516
Epoch 10/10
10/10 - 3s - loss: 394.4086 - loglik: -3.9440e+02 - logprior: 0.4538
Fitted a model with MAP estimate = -393.4275
expansions: [(9, 2), (19, 2), (20, 1), (22, 3), (23, 1), (29, 2), (44, 1), (45, 2), (46, 1), (52, 2), (76, 3), (81, 2), (94, 1), (102, 2), (105, 2), (106, 1), (116, 1), (117, 2), (118, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 198 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 477.0491 - loglik: -4.1355e+02 - logprior: -6.2976e+01
Epoch 2/2
10/10 - 3s - loss: 411.3840 - loglik: -3.8712e+02 - logprior: -2.3532e+01
Fitted a model with MAP estimate = -399.7315
expansions: [(0, 10), (175, 3)]
discards: [  0  21  27  66  93 100 124]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 443.2100 - loglik: -3.9322e+02 - logprior: -4.9278e+01
Epoch 2/2
10/10 - 3s - loss: 396.1565 - loglik: -3.8558e+02 - logprior: -1.0040e+01
Fitted a model with MAP estimate = -384.6157
expansions: [(18, 1), (186, 4)]
discards: [  1   2   3   4   5   6   7   8   9  44  63 148 178 179 180]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 438.9672 - loglik: -3.9094e+02 - logprior: -4.7589e+01
Epoch 2/10
10/10 - 3s - loss: 393.6914 - loglik: -3.8470e+02 - logprior: -8.6639e+00
Epoch 3/10
10/10 - 3s - loss: 379.1628 - loglik: -3.7813e+02 - logprior: -6.1545e-01
Epoch 4/10
10/10 - 3s - loss: 372.6153 - loglik: -3.7455e+02 - logprior: 2.5895
Epoch 5/10
10/10 - 3s - loss: 365.8248 - loglik: -3.6942e+02 - logprior: 4.3603
Epoch 6/10
10/10 - 3s - loss: 364.1148 - loglik: -3.6887e+02 - logprior: 5.4716
Epoch 7/10
10/10 - 3s - loss: 362.2376 - loglik: -3.6778e+02 - logprior: 6.2279
Epoch 8/10
10/10 - 3s - loss: 360.3227 - loglik: -3.6647e+02 - logprior: 6.7904
Epoch 9/10
10/10 - 3s - loss: 360.1458 - loglik: -3.6684e+02 - logprior: 7.3032
Epoch 10/10
10/10 - 3s - loss: 359.3854 - loglik: -3.6653e+02 - logprior: 7.7274
Fitted a model with MAP estimate = -358.3002
Time for alignment: 93.8768
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 615.3683 - loglik: -5.6106e+02 - logprior: -5.4288e+01
Epoch 2/10
10/10 - 3s - loss: 518.3795 - loglik: -5.0811e+02 - logprior: -1.0246e+01
Epoch 3/10
10/10 - 3s - loss: 455.4596 - loglik: -4.5225e+02 - logprior: -3.1141e+00
Epoch 4/10
10/10 - 3s - loss: 422.2001 - loglik: -4.2065e+02 - logprior: -1.2456e+00
Epoch 5/10
10/10 - 3s - loss: 408.7460 - loglik: -4.0747e+02 - logprior: -6.4967e-01
Epoch 6/10
10/10 - 3s - loss: 402.0364 - loglik: -4.0111e+02 - logprior: -1.8785e-01
Epoch 7/10
10/10 - 3s - loss: 399.7225 - loglik: -3.9923e+02 - logprior: 0.1247
Epoch 8/10
10/10 - 3s - loss: 397.6167 - loglik: -3.9736e+02 - logprior: 0.2799
Epoch 9/10
10/10 - 3s - loss: 397.2227 - loglik: -3.9715e+02 - logprior: 0.4410
Epoch 10/10
10/10 - 3s - loss: 398.3405 - loglik: -3.9846e+02 - logprior: 0.6423
Fitted a model with MAP estimate = -396.3444
expansions: [(9, 2), (19, 2), (20, 2), (22, 3), (23, 1), (29, 2), (44, 1), (45, 2), (46, 2), (52, 1), (75, 1), (81, 1), (82, 1), (103, 3), (104, 1), (116, 1), (117, 2), (118, 1), (145, 4), (146, 1), (152, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 476.6368 - loglik: -4.1329e+02 - logprior: -6.2777e+01
Epoch 2/2
10/10 - 3s - loss: 415.2248 - loglik: -3.9106e+02 - logprior: -2.3438e+01
Fitted a model with MAP estimate = -400.8623
expansions: [(0, 12), (127, 1), (174, 1)]
discards: [  0  21  24  28  29  60  61 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 207 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 447.2238 - loglik: -3.9741e+02 - logprior: -4.9125e+01
Epoch 2/2
10/10 - 3s - loss: 395.7697 - loglik: -3.8546e+02 - logprior: -9.7634e+00
Fitted a model with MAP estimate = -386.1752
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10 11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 437.6196 - loglik: -3.8962e+02 - logprior: -4.7550e+01
Epoch 2/10
10/10 - 3s - loss: 394.3312 - loglik: -3.8553e+02 - logprior: -8.4352e+00
Epoch 3/10
10/10 - 3s - loss: 380.6275 - loglik: -3.7980e+02 - logprior: -3.7645e-01
Epoch 4/10
10/10 - 3s - loss: 374.8825 - loglik: -3.7701e+02 - logprior: 2.8162
Epoch 5/10
10/10 - 3s - loss: 368.5826 - loglik: -3.7239e+02 - logprior: 4.6168
Epoch 6/10
10/10 - 3s - loss: 366.8689 - loglik: -3.7182e+02 - logprior: 5.7198
Epoch 7/10
10/10 - 3s - loss: 364.8300 - loglik: -3.7060e+02 - logprior: 6.5031
Epoch 8/10
10/10 - 3s - loss: 363.6093 - loglik: -3.7001e+02 - logprior: 7.0944
Epoch 9/10
10/10 - 3s - loss: 362.7492 - loglik: -3.6971e+02 - logprior: 7.6085
Epoch 10/10
10/10 - 3s - loss: 360.8508 - loglik: -3.6829e+02 - logprior: 8.0585
Fitted a model with MAP estimate = -360.9143
Time for alignment: 96.4168
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 615.1573 - loglik: -5.6085e+02 - logprior: -5.4291e+01
Epoch 2/10
10/10 - 3s - loss: 519.2876 - loglik: -5.0901e+02 - logprior: -1.0249e+01
Epoch 3/10
10/10 - 3s - loss: 457.1157 - loglik: -4.5395e+02 - logprior: -3.0645e+00
Epoch 4/10
10/10 - 3s - loss: 420.7607 - loglik: -4.1940e+02 - logprior: -1.1416e+00
Epoch 5/10
10/10 - 3s - loss: 406.4534 - loglik: -4.0552e+02 - logprior: -5.3980e-01
Epoch 6/10
10/10 - 3s - loss: 400.6158 - loglik: -3.9996e+02 - logprior: -1.1805e-01
Epoch 7/10
10/10 - 3s - loss: 397.3188 - loglik: -3.9709e+02 - logprior: 0.2578
Epoch 8/10
10/10 - 3s - loss: 396.5841 - loglik: -3.9660e+02 - logprior: 0.4655
Epoch 9/10
10/10 - 3s - loss: 394.7286 - loglik: -3.9490e+02 - logprior: 0.6366
Epoch 10/10
10/10 - 3s - loss: 394.9316 - loglik: -3.9522e+02 - logprior: 0.7680
Fitted a model with MAP estimate = -393.8432
expansions: [(0, 3), (19, 1), (22, 1), (23, 2), (25, 1), (30, 3), (44, 3), (45, 1), (46, 1), (52, 1), (76, 3), (81, 2), (94, 1), (98, 1), (103, 3), (116, 3), (119, 1), (145, 3), (150, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 482.7243 - loglik: -4.1130e+02 - logprior: -7.0894e+01
Epoch 2/2
10/10 - 3s - loss: 406.3354 - loglik: -3.8731e+02 - logprior: -1.8301e+01
Fitted a model with MAP estimate = -388.4471
expansions: [(129, 1)]
discards: [  0   1   2  28  94 101 144 176 186 187]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 444.6285 - loglik: -3.9275e+02 - logprior: -5.1177e+01
Epoch 2/2
10/10 - 3s - loss: 391.9543 - loglik: -3.8169e+02 - logprior: -9.7405e+00
Fitted a model with MAP estimate = -383.6381
expansions: [(0, 9), (170, 2)]
discards: [34 89]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 455.5978 - loglik: -3.8869e+02 - logprior: -6.6489e+01
Epoch 2/10
10/10 - 3s - loss: 396.0993 - loglik: -3.8068e+02 - logprior: -1.5035e+01
Epoch 3/10
10/10 - 3s - loss: 377.8924 - loglik: -3.7465e+02 - logprior: -2.7750e+00
Epoch 4/10
10/10 - 3s - loss: 368.9756 - loglik: -3.6991e+02 - logprior: 1.5531
Epoch 5/10
10/10 - 3s - loss: 362.2838 - loglik: -3.6557e+02 - logprior: 3.9497
Epoch 6/10
10/10 - 3s - loss: 358.8780 - loglik: -3.6352e+02 - logprior: 5.2653
Epoch 7/10
10/10 - 3s - loss: 358.8628 - loglik: -3.6445e+02 - logprior: 6.2027
Epoch 8/10
10/10 - 3s - loss: 356.2607 - loglik: -3.6250e+02 - logprior: 6.8396
Epoch 9/10
10/10 - 3s - loss: 355.3003 - loglik: -3.6207e+02 - logprior: 7.3732
Epoch 10/10
10/10 - 3s - loss: 355.5912 - loglik: -3.6282e+02 - logprior: 7.8361
Fitted a model with MAP estimate = -354.0264
Time for alignment: 94.1031
Computed alignments with likelihoods: ['-358.3002', '-360.9143', '-354.0264']
Best model has likelihood: -354.0264
time for generating output: 0.4112
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.7734558248631743
Training of 3 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f180e319910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f180f877ee0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f180f5e4a90>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f183d177dc0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f180f5f0e80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19593577c0>, <__main__.SimpleDirichletPrior object at 0x7f19c7b9abe0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe9ec940>

Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.6042 - loglik: -2.3539e+02 - logprior: -3.1604e+00
Epoch 2/10
19/19 - 2s - loss: 208.5370 - loglik: -2.0697e+02 - logprior: -1.2397e+00
Epoch 3/10
19/19 - 2s - loss: 196.7896 - loglik: -1.9483e+02 - logprior: -1.4223e+00
Epoch 4/10
19/19 - 2s - loss: 193.8043 - loglik: -1.9207e+02 - logprior: -1.3524e+00
Epoch 5/10
19/19 - 2s - loss: 192.7992 - loglik: -1.9116e+02 - logprior: -1.3244e+00
Epoch 6/10
19/19 - 2s - loss: 192.3418 - loglik: -1.9075e+02 - logprior: -1.3074e+00
Epoch 7/10
19/19 - 2s - loss: 192.0838 - loglik: -1.9052e+02 - logprior: -1.2846e+00
Epoch 8/10
19/19 - 2s - loss: 192.0031 - loglik: -1.9044e+02 - logprior: -1.2922e+00
Epoch 9/10
19/19 - 2s - loss: 191.9107 - loglik: -1.9038e+02 - logprior: -1.2729e+00
Epoch 10/10
19/19 - 2s - loss: 191.9563 - loglik: -1.9042e+02 - logprior: -1.2763e+00
Fitted a model with MAP estimate = -185.7824
expansions: [(3, 1), (4, 1), (5, 1), (6, 1), (15, 2), (18, 3), (22, 2), (24, 1), (46, 1), (47, 1), (48, 2), (49, 1), (52, 1), (55, 2), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 194.3033 - loglik: -1.9103e+02 - logprior: -2.9076e+00
Epoch 2/2
19/19 - 2s - loss: 187.1942 - loglik: -1.8539e+02 - logprior: -1.1981e+00
Fitted a model with MAP estimate = -178.7169
expansions: [(21, 4)]
discards: [ 0 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 191.8524 - loglik: -1.8733e+02 - logprior: -3.8664e+00
Epoch 2/2
19/19 - 2s - loss: 187.0250 - loglik: -1.8433e+02 - logprior: -2.0737e+00
Fitted a model with MAP estimate = -178.4424
expansions: [(0, 3)]
discards: [ 0 19 20 21 22 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 180.7624 - loglik: -1.7813e+02 - logprior: -1.8679e+00
Epoch 2/10
23/23 - 3s - loss: 178.3503 - loglik: -1.7672e+02 - logprior: -9.9194e-01
Epoch 3/10
23/23 - 3s - loss: 177.1238 - loglik: -1.7553e+02 - logprior: -9.3583e-01
Epoch 4/10
23/23 - 3s - loss: 177.0619 - loglik: -1.7553e+02 - logprior: -8.9546e-01
Epoch 5/10
23/23 - 3s - loss: 176.2682 - loglik: -1.7479e+02 - logprior: -9.0226e-01
Epoch 6/10
23/23 - 3s - loss: 176.3004 - loglik: -1.7483e+02 - logprior: -9.3406e-01
Fitted a model with MAP estimate = -175.2066
Time for alignment: 82.0555
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.5525 - loglik: -2.3533e+02 - logprior: -3.1653e+00
Epoch 2/10
19/19 - 2s - loss: 207.6149 - loglik: -2.0610e+02 - logprior: -1.2334e+00
Epoch 3/10
19/19 - 2s - loss: 196.1293 - loglik: -1.9419e+02 - logprior: -1.3970e+00
Epoch 4/10
19/19 - 2s - loss: 193.5433 - loglik: -1.9182e+02 - logprior: -1.3270e+00
Epoch 5/10
19/19 - 2s - loss: 192.7515 - loglik: -1.9111e+02 - logprior: -1.3244e+00
Epoch 6/10
19/19 - 2s - loss: 192.3010 - loglik: -1.9071e+02 - logprior: -1.3032e+00
Epoch 7/10
19/19 - 2s - loss: 192.1735 - loglik: -1.9059e+02 - logprior: -1.2953e+00
Epoch 8/10
19/19 - 2s - loss: 192.0172 - loglik: -1.9045e+02 - logprior: -1.2927e+00
Epoch 9/10
19/19 - 2s - loss: 191.8968 - loglik: -1.9035e+02 - logprior: -1.2815e+00
Epoch 10/10
19/19 - 2s - loss: 192.1239 - loglik: -1.9058e+02 - logprior: -1.2785e+00
Fitted a model with MAP estimate = -185.9522
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (15, 1), (18, 3), (23, 2), (24, 1), (46, 2), (47, 1), (48, 1), (49, 2), (52, 1), (55, 2), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 196.7408 - loglik: -1.9206e+02 - logprior: -4.2984e+00
Epoch 2/2
19/19 - 2s - loss: 187.5379 - loglik: -1.8534e+02 - logprior: -1.5742e+00
Fitted a model with MAP estimate = -178.8081
expansions: []
discards: [ 0  1  2 33 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 192.3824 - loglik: -1.8786e+02 - logprior: -3.8752e+00
Epoch 2/2
19/19 - 2s - loss: 188.0473 - loglik: -1.8571e+02 - logprior: -1.7361e+00
Fitted a model with MAP estimate = -179.0431
expansions: [(0, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 180.4978 - loglik: -1.7786e+02 - logprior: -1.8814e+00
Epoch 2/10
23/23 - 3s - loss: 177.9426 - loglik: -1.7627e+02 - logprior: -1.0066e+00
Epoch 3/10
23/23 - 3s - loss: 177.0856 - loglik: -1.7545e+02 - logprior: -9.9414e-01
Epoch 4/10
23/23 - 3s - loss: 176.6784 - loglik: -1.7505e+02 - logprior: -9.9560e-01
Epoch 5/10
23/23 - 3s - loss: 176.1038 - loglik: -1.7457e+02 - logprior: -9.8630e-01
Epoch 6/10
23/23 - 3s - loss: 175.6417 - loglik: -1.7416e+02 - logprior: -9.6617e-01
Epoch 7/10
23/23 - 3s - loss: 175.6914 - loglik: -1.7424e+02 - logprior: -9.5113e-01
Fitted a model with MAP estimate = -174.9050
Time for alignment: 82.5084
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 238.5493 - loglik: -2.3534e+02 - logprior: -3.1619e+00
Epoch 2/10
19/19 - 2s - loss: 208.3546 - loglik: -2.0684e+02 - logprior: -1.2489e+00
Epoch 3/10
19/19 - 2s - loss: 196.7924 - loglik: -1.9490e+02 - logprior: -1.4166e+00
Epoch 4/10
19/19 - 2s - loss: 194.1025 - loglik: -1.9233e+02 - logprior: -1.3696e+00
Epoch 5/10
19/19 - 2s - loss: 193.2640 - loglik: -1.9161e+02 - logprior: -1.3286e+00
Epoch 6/10
19/19 - 2s - loss: 192.8617 - loglik: -1.9126e+02 - logprior: -1.3156e+00
Epoch 7/10
19/19 - 2s - loss: 192.7232 - loglik: -1.9115e+02 - logprior: -1.2982e+00
Epoch 8/10
19/19 - 2s - loss: 192.2637 - loglik: -1.9071e+02 - logprior: -1.2862e+00
Epoch 9/10
19/19 - 2s - loss: 192.5120 - loglik: -1.9096e+02 - logprior: -1.2848e+00
Fitted a model with MAP estimate = -186.1452
expansions: [(3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (36, 1), (46, 2), (47, 1), (48, 2), (49, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 88 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 194.3895 - loglik: -1.9110e+02 - logprior: -2.9080e+00
Epoch 2/2
19/19 - 2s - loss: 186.5299 - loglik: -1.8471e+02 - logprior: -1.2247e+00
Fitted a model with MAP estimate = -178.1470
expansions: [(29, 1)]
discards: [ 0 22 23 24 61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 192.3716 - loglik: -1.8781e+02 - logprior: -3.9106e+00
Epoch 2/2
19/19 - 2s - loss: 187.9326 - loglik: -1.8526e+02 - logprior: -2.0808e+00
Fitted a model with MAP estimate = -179.1468
expansions: [(0, 3), (21, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 179.9193 - loglik: -1.7728e+02 - logprior: -1.9067e+00
Epoch 2/10
23/23 - 3s - loss: 177.2289 - loglik: -1.7553e+02 - logprior: -1.0406e+00
Epoch 3/10
23/23 - 3s - loss: 175.7317 - loglik: -1.7400e+02 - logprior: -1.0646e+00
Epoch 4/10
23/23 - 3s - loss: 175.4324 - loglik: -1.7376e+02 - logprior: -1.0349e+00
Epoch 5/10
23/23 - 3s - loss: 174.9157 - loglik: -1.7333e+02 - logprior: -1.0200e+00
Epoch 6/10
23/23 - 3s - loss: 174.7370 - loglik: -1.7320e+02 - logprior: -1.0049e+00
Epoch 7/10
23/23 - 3s - loss: 174.1962 - loglik: -1.7270e+02 - logprior: -9.8829e-01
Epoch 8/10
23/23 - 3s - loss: 174.1101 - loglik: -1.7263e+02 - logprior: -9.7129e-01
Epoch 9/10
23/23 - 3s - loss: 173.8031 - loglik: -1.7233e+02 - logprior: -9.6038e-01
Epoch 10/10
23/23 - 3s - loss: 173.3804 - loglik: -1.7192e+02 - logprior: -9.5030e-01
Fitted a model with MAP estimate = -172.4452
Time for alignment: 87.6847
Computed alignments with likelihoods: ['-175.2066', '-174.9050', '-172.4452']
Best model has likelihood: -172.4452
time for generating output: 0.1581
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.9047619047619048
Training of 3 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f180f3af580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19592ed580>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18181f4310>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1819762fd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1819762dc0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18562833d0>, <__main__.SimpleDirichletPrior object at 0x7f1818cd2040>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18a3089ee0>

Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 796.4488 - loglik: -7.9010e+02 - logprior: -6.2184e+00
Epoch 2/10
22/22 - 14s - loss: 687.2989 - loglik: -6.8561e+02 - logprior: -1.2020e+00
Epoch 3/10
22/22 - 14s - loss: 644.5802 - loglik: -6.4101e+02 - logprior: -2.7291e+00
Epoch 4/10
22/22 - 14s - loss: 636.5931 - loglik: -6.3310e+02 - logprior: -2.5763e+00
Epoch 5/10
22/22 - 14s - loss: 633.1611 - loglik: -6.2979e+02 - logprior: -2.5180e+00
Epoch 6/10
22/22 - 14s - loss: 631.8840 - loglik: -6.2853e+02 - logprior: -2.5256e+00
Epoch 7/10
22/22 - 14s - loss: 632.2427 - loglik: -6.2886e+02 - logprior: -2.5457e+00
Fitted a model with MAP estimate = -630.4944
expansions: [(14, 1), (15, 1), (32, 1), (33, 3), (34, 3), (35, 1), (45, 2), (48, 1), (49, 1), (50, 1), (70, 1), (71, 2), (72, 1), (76, 2), (79, 1), (82, 1), (94, 1), (105, 1), (106, 2), (107, 1), (110, 1), (119, 1), (121, 2), (137, 1), (144, 1), (145, 1), (149, 1), (150, 1), (152, 1), (156, 1), (158, 1), (176, 1), (178, 3), (179, 1), (180, 1), (183, 3), (184, 1), (185, 1), (193, 2), (205, 1), (208, 1), (209, 1), (213, 3), (214, 1), (223, 2), (224, 1), (225, 1), (236, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 317 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 23s - loss: 640.2856 - loglik: -6.2988e+02 - logprior: -9.3326e+00
Epoch 2/2
22/22 - 19s - loss: 618.1763 - loglik: -6.1323e+02 - logprior: -3.5052e+00
Fitted a model with MAP estimate = -610.9588
expansions: [(0, 3), (246, 1)]
discards: [  0  35  40 219 229 283]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 24s - loss: 623.2574 - loglik: -6.1565e+02 - logprior: -5.8289e+00
Epoch 2/2
22/22 - 19s - loss: 609.6342 - loglik: -6.0791e+02 - logprior: 0.0624
Fitted a model with MAP estimate = -606.5257
expansions: []
discards: [  0   1   2 269]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 311 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 22s - loss: 626.3196 - loglik: -6.1587e+02 - logprior: -8.4319e+00
Epoch 2/10
22/22 - 19s - loss: 615.1562 - loglik: -6.1074e+02 - logprior: -2.4767e+00
Epoch 3/10
22/22 - 19s - loss: 610.1710 - loglik: -6.0744e+02 - logprior: -1.0025e+00
Epoch 4/10
22/22 - 19s - loss: 608.6248 - loglik: -6.0859e+02 - logprior: 1.4941
Epoch 5/10
22/22 - 19s - loss: 604.0319 - loglik: -6.0443e+02 - logprior: 1.7595
Epoch 6/10
22/22 - 19s - loss: 603.7314 - loglik: -6.0444e+02 - logprior: 1.9499
Epoch 7/10
22/22 - 19s - loss: 603.4822 - loglik: -6.0446e+02 - logprior: 2.1339
Epoch 8/10
22/22 - 19s - loss: 601.5507 - loglik: -6.0285e+02 - logprior: 2.3947
Epoch 9/10
22/22 - 19s - loss: 601.4880 - loglik: -6.0311e+02 - logprior: 2.6628
Epoch 10/10
22/22 - 19s - loss: 602.8301 - loglik: -6.0467e+02 - logprior: 2.8512
Fitted a model with MAP estimate = -599.5032
Time for alignment: 432.4491
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 798.2086 - loglik: -7.9186e+02 - logprior: -6.2242e+00
Epoch 2/10
22/22 - 14s - loss: 682.0578 - loglik: -6.8036e+02 - logprior: -1.2320e+00
Epoch 3/10
22/22 - 14s - loss: 642.1179 - loglik: -6.3849e+02 - logprior: -2.7820e+00
Epoch 4/10
22/22 - 14s - loss: 637.4153 - loglik: -6.3388e+02 - logprior: -2.5852e+00
Epoch 5/10
22/22 - 14s - loss: 633.3372 - loglik: -6.2984e+02 - logprior: -2.5997e+00
Epoch 6/10
22/22 - 14s - loss: 633.8790 - loglik: -6.3035e+02 - logprior: -2.6583e+00
Fitted a model with MAP estimate = -631.4735
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 3), (35, 2), (44, 1), (48, 1), (49, 1), (50, 1), (61, 1), (69, 1), (70, 1), (72, 1), (76, 2), (79, 1), (81, 1), (83, 1), (99, 1), (104, 1), (105, 2), (106, 1), (109, 1), (118, 1), (120, 2), (143, 2), (144, 1), (149, 1), (153, 1), (156, 3), (176, 1), (179, 3), (180, 1), (181, 1), (184, 3), (185, 1), (186, 1), (194, 2), (206, 1), (209, 1), (210, 1), (214, 3), (215, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 317 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 23s - loss: 641.1115 - loglik: -6.3070e+02 - logprior: -9.2834e+00
Epoch 2/2
22/22 - 19s - loss: 615.1785 - loglik: -6.1021e+02 - logprior: -3.4780e+00
Fitted a model with MAP estimate = -610.8297
expansions: [(0, 3), (248, 1)]
discards: [  0  34  40 176 221 231]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 620.5005 - loglik: -6.1279e+02 - logprior: -5.9080e+00
Epoch 2/2
22/22 - 19s - loss: 613.1961 - loglik: -6.1141e+02 - logprior: 0.0207
Fitted a model with MAP estimate = -606.6929
expansions: []
discards: [0 1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 24s - loss: 625.7737 - loglik: -6.1531e+02 - logprior: -8.4088e+00
Epoch 2/10
22/22 - 19s - loss: 617.1285 - loglik: -6.1265e+02 - logprior: -2.5388e+00
Epoch 3/10
22/22 - 19s - loss: 608.6307 - loglik: -6.0601e+02 - logprior: -8.9236e-01
Epoch 4/10
22/22 - 19s - loss: 606.3917 - loglik: -6.0637e+02 - logprior: 1.4804
Epoch 5/10
22/22 - 19s - loss: 604.9980 - loglik: -6.0537e+02 - logprior: 1.7225
Epoch 6/10
22/22 - 19s - loss: 603.4895 - loglik: -6.0421e+02 - logprior: 1.9262
Epoch 7/10
22/22 - 19s - loss: 604.3798 - loglik: -6.0541e+02 - logprior: 2.1611
Fitted a model with MAP estimate = -601.2398
Time for alignment: 362.8455
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 18s - loss: 796.8543 - loglik: -7.9050e+02 - logprior: -6.2348e+00
Epoch 2/10
22/22 - 14s - loss: 688.1993 - loglik: -6.8647e+02 - logprior: -1.1409e+00
Epoch 3/10
22/22 - 14s - loss: 646.2270 - loglik: -6.4231e+02 - logprior: -2.5740e+00
Epoch 4/10
22/22 - 14s - loss: 637.1741 - loglik: -6.3353e+02 - logprior: -2.4900e+00
Epoch 5/10
22/22 - 14s - loss: 634.6255 - loglik: -6.3108e+02 - logprior: -2.5777e+00
Epoch 6/10
22/22 - 14s - loss: 633.5072 - loglik: -6.2997e+02 - logprior: -2.6158e+00
Epoch 7/10
22/22 - 14s - loss: 632.7580 - loglik: -6.2925e+02 - logprior: -2.6383e+00
Epoch 8/10
22/22 - 14s - loss: 632.2142 - loglik: -6.2876e+02 - logprior: -2.6439e+00
Epoch 9/10
22/22 - 14s - loss: 632.2792 - loglik: -6.2889e+02 - logprior: -2.6311e+00
Fitted a model with MAP estimate = -630.5358
expansions: [(14, 1), (15, 1), (32, 2), (33, 1), (35, 2), (36, 1), (46, 2), (49, 1), (50, 1), (67, 1), (71, 1), (72, 2), (73, 1), (77, 2), (80, 1), (82, 1), (84, 1), (98, 1), (105, 1), (106, 2), (107, 1), (109, 2), (132, 1), (138, 1), (144, 1), (149, 1), (150, 1), (156, 1), (158, 3), (159, 1), (177, 1), (179, 3), (180, 1), (181, 1), (184, 3), (185, 1), (186, 1), (194, 2), (206, 1), (209, 1), (210, 1), (214, 3), (215, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 24s - loss: 642.3865 - loglik: -6.3219e+02 - logprior: -9.2546e+00
Epoch 2/2
22/22 - 19s - loss: 618.1006 - loglik: -6.1339e+02 - logprior: -3.3240e+00
Fitted a model with MAP estimate = -611.4220
expansions: [(0, 3), (246, 1)]
discards: [  0  34 194 219 229]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 623.5551 - loglik: -6.1605e+02 - logprior: -5.7279e+00
Epoch 2/2
22/22 - 19s - loss: 608.1750 - loglik: -6.0651e+02 - logprior: 0.1385
Fitted a model with MAP estimate = -606.3392
expansions: []
discards: [  0   1   2 269]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 22s - loss: 627.1806 - loglik: -6.1674e+02 - logprior: -8.3805e+00
Epoch 2/10
22/22 - 18s - loss: 611.9274 - loglik: -6.0755e+02 - logprior: -2.4463e+00
Epoch 3/10
22/22 - 19s - loss: 613.0298 - loglik: -6.1029e+02 - logprior: -1.0162e+00
Fitted a model with MAP estimate = -605.5420
Time for alignment: 324.8700
Computed alignments with likelihoods: ['-599.5032', '-601.2398', '-605.5420']
Best model has likelihood: -599.5032
time for generating output: 0.3970
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.9164788278860087
Training of 3 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1818cb0820>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f185efaca90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f185efaca30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193da6ae80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f185f1386a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f186f4bd730>, <__main__.SimpleDirichletPrior object at 0x7f193e80acd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18a3089ee0>

Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 172.5583 - loglik: -7.9870e+01 - logprior: -9.2667e+01
Epoch 2/10
10/10 - 1s - loss: 95.4988 - loglik: -6.8621e+01 - logprior: -2.6855e+01
Epoch 3/10
10/10 - 1s - loss: 73.4782 - loglik: -5.9895e+01 - logprior: -1.3573e+01
Epoch 4/10
10/10 - 1s - loss: 64.3450 - loglik: -5.5874e+01 - logprior: -8.4486e+00
Epoch 5/10
10/10 - 1s - loss: 59.6167 - loglik: -5.3775e+01 - logprior: -5.8246e+00
Epoch 6/10
10/10 - 1s - loss: 57.4730 - loglik: -5.3060e+01 - logprior: -4.3859e+00
Epoch 7/10
10/10 - 1s - loss: 56.3178 - loglik: -5.2637e+01 - logprior: -3.5641e+00
Epoch 8/10
10/10 - 1s - loss: 55.6742 - loglik: -5.2445e+01 - logprior: -3.0436e+00
Epoch 9/10
10/10 - 1s - loss: 55.3035 - loglik: -5.2412e+01 - logprior: -2.7299e+00
Epoch 10/10
10/10 - 1s - loss: 55.0587 - loglik: -5.2397e+01 - logprior: -2.5056e+00
Fitted a model with MAP estimate = -54.8016
expansions: [(0, 4), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.3457 - loglik: -4.9153e+01 - logprior: -1.2407e+02
Epoch 2/2
10/10 - 1s - loss: 86.6227 - loglik: -4.6067e+01 - logprior: -4.0509e+01
Fitted a model with MAP estimate = -69.6227
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 130.7952 - loglik: -4.3428e+01 - logprior: -8.7303e+01
Epoch 2/10
10/10 - 1s - loss: 69.2445 - loglik: -4.3778e+01 - logprior: -2.5411e+01
Epoch 3/10
10/10 - 1s - loss: 56.8182 - loglik: -4.4078e+01 - logprior: -1.2665e+01
Epoch 4/10
10/10 - 1s - loss: 51.8218 - loglik: -4.4064e+01 - logprior: -7.5697e+00
Epoch 5/10
10/10 - 1s - loss: 49.2740 - loglik: -4.4217e+01 - logprior: -4.8411e+00
Epoch 6/10
10/10 - 1s - loss: 47.6158 - loglik: -4.4119e+01 - logprior: -3.2896e+00
Epoch 7/10
10/10 - 1s - loss: 46.2728 - loglik: -4.3594e+01 - logprior: -2.4390e+00
Epoch 8/10
10/10 - 1s - loss: 45.5598 - loglik: -4.3404e+01 - logprior: -1.9156e+00
Epoch 9/10
10/10 - 1s - loss: 45.1720 - loglik: -4.3356e+01 - logprior: -1.5689e+00
Epoch 10/10
10/10 - 1s - loss: 44.9132 - loglik: -4.3409e+01 - logprior: -1.2514e+00
Fitted a model with MAP estimate = -44.5381
Time for alignment: 31.6391
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 172.5583 - loglik: -7.9870e+01 - logprior: -9.2667e+01
Epoch 2/10
10/10 - 1s - loss: 95.4988 - loglik: -6.8621e+01 - logprior: -2.6855e+01
Epoch 3/10
10/10 - 1s - loss: 73.4782 - loglik: -5.9895e+01 - logprior: -1.3573e+01
Epoch 4/10
10/10 - 1s - loss: 64.3451 - loglik: -5.5874e+01 - logprior: -8.4486e+00
Epoch 5/10
10/10 - 1s - loss: 59.6170 - loglik: -5.3776e+01 - logprior: -5.8244e+00
Epoch 6/10
10/10 - 1s - loss: 57.4835 - loglik: -5.3078e+01 - logprior: -4.3818e+00
Epoch 7/10
10/10 - 1s - loss: 56.3221 - loglik: -5.2655e+01 - logprior: -3.5612e+00
Epoch 8/10
10/10 - 1s - loss: 55.6874 - loglik: -5.2472e+01 - logprior: -3.0351e+00
Epoch 9/10
10/10 - 1s - loss: 55.3518 - loglik: -5.2490e+01 - logprior: -2.7103e+00
Epoch 10/10
10/10 - 1s - loss: 55.1292 - loglik: -5.2518e+01 - logprior: -2.4793e+00
Fitted a model with MAP estimate = -54.9033
expansions: [(0, 4), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.3185 - loglik: -4.9186e+01 - logprior: -1.2405e+02
Epoch 2/2
10/10 - 1s - loss: 86.6260 - loglik: -4.6096e+01 - logprior: -4.0495e+01
Fitted a model with MAP estimate = -69.6209
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 130.7913 - loglik: -4.3426e+01 - logprior: -8.7295e+01
Epoch 2/10
10/10 - 1s - loss: 69.2424 - loglik: -4.3787e+01 - logprior: -2.5405e+01
Epoch 3/10
10/10 - 1s - loss: 56.8454 - loglik: -4.4137e+01 - logprior: -1.2649e+01
Epoch 4/10
10/10 - 1s - loss: 51.8704 - loglik: -4.4143e+01 - logprior: -7.5471e+00
Epoch 5/10
10/10 - 1s - loss: 49.3432 - loglik: -4.4294e+01 - logprior: -4.8198e+00
Epoch 6/10
10/10 - 1s - loss: 47.6963 - loglik: -4.4219e+01 - logprior: -3.2656e+00
Epoch 7/10
10/10 - 1s - loss: 46.3494 - loglik: -4.3689e+01 - logprior: -2.4118e+00
Epoch 8/10
10/10 - 1s - loss: 45.6237 - loglik: -4.3461e+01 - logprior: -1.9094e+00
Epoch 9/10
10/10 - 1s - loss: 45.2026 - loglik: -4.3382e+01 - logprior: -1.5787e+00
Epoch 10/10
10/10 - 1s - loss: 44.9267 - loglik: -4.3432e+01 - logprior: -1.2545e+00
Fitted a model with MAP estimate = -44.5524
Time for alignment: 30.7511
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 172.5584 - loglik: -7.9870e+01 - logprior: -9.2667e+01
Epoch 2/10
10/10 - 1s - loss: 95.4988 - loglik: -6.8621e+01 - logprior: -2.6855e+01
Epoch 3/10
10/10 - 1s - loss: 73.4782 - loglik: -5.9895e+01 - logprior: -1.3573e+01
Epoch 4/10
10/10 - 1s - loss: 64.3450 - loglik: -5.5874e+01 - logprior: -8.4486e+00
Epoch 5/10
10/10 - 1s - loss: 59.5998 - loglik: -5.3749e+01 - logprior: -5.8321e+00
Epoch 6/10
10/10 - 1s - loss: 57.3613 - loglik: -5.2872e+01 - logprior: -4.4241e+00
Epoch 7/10
10/10 - 1s - loss: 56.2561 - loglik: -5.2480e+01 - logprior: -3.5923e+00
Epoch 8/10
10/10 - 1s - loss: 55.6191 - loglik: -5.2364e+01 - logprior: -3.0621e+00
Epoch 9/10
10/10 - 1s - loss: 55.2748 - loglik: -5.2383e+01 - logprior: -2.7349e+00
Epoch 10/10
10/10 - 1s - loss: 55.0466 - loglik: -5.2377e+01 - logprior: -2.5031e+00
Fitted a model with MAP estimate = -54.7740
expansions: [(0, 4), (13, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 173.3931 - loglik: -4.9179e+01 - logprior: -1.2407e+02
Epoch 2/2
10/10 - 1s - loss: 86.6086 - loglik: -4.6031e+01 - logprior: -4.0519e+01
Fitted a model with MAP estimate = -69.6162
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 130.7952 - loglik: -4.3425e+01 - logprior: -8.7307e+01
Epoch 2/10
10/10 - 1s - loss: 69.2227 - loglik: -4.3758e+01 - logprior: -2.5411e+01
Epoch 3/10
10/10 - 1s - loss: 56.7769 - loglik: -4.4012e+01 - logprior: -1.2675e+01
Epoch 4/10
10/10 - 1s - loss: 51.7992 - loglik: -4.4032e+01 - logprior: -7.5709e+00
Epoch 5/10
10/10 - 1s - loss: 49.2652 - loglik: -4.4216e+01 - logprior: -4.8407e+00
Epoch 6/10
10/10 - 1s - loss: 47.5886 - loglik: -4.4086e+01 - logprior: -3.2922e+00
Epoch 7/10
10/10 - 1s - loss: 46.2621 - loglik: -4.3582e+01 - logprior: -2.4388e+00
Epoch 8/10
10/10 - 1s - loss: 45.5525 - loglik: -4.3398e+01 - logprior: -1.9128e+00
Epoch 9/10
10/10 - 1s - loss: 45.1717 - loglik: -4.3356e+01 - logprior: -1.5684e+00
Epoch 10/10
10/10 - 1s - loss: 44.9131 - loglik: -4.3409e+01 - logprior: -1.2516e+00
Fitted a model with MAP estimate = -44.5400
Time for alignment: 31.0757
Computed alignments with likelihoods: ['-44.5381', '-44.5524', '-44.5400']
Best model has likelihood: -44.5381
time for generating output: 0.1415
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9302325581395349
Training of 3 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f199d17d6d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193d9c2be0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19345f4580>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19a5db8d30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1950fa79d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f199d0e1f10>, <__main__.SimpleDirichletPrior object at 0x7f1961bf8850>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1877aec790>

Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 362.7022 - loglik: -3.5020e+02 - logprior: -1.2483e+01
Epoch 2/10
11/11 - 2s - loss: 310.6577 - loglik: -3.0751e+02 - logprior: -3.1216e+00
Epoch 3/10
11/11 - 3s - loss: 268.7185 - loglik: -2.6650e+02 - logprior: -2.1910e+00
Epoch 4/10
11/11 - 3s - loss: 247.9944 - loglik: -2.4534e+02 - logprior: -2.5084e+00
Epoch 5/10
11/11 - 3s - loss: 241.0005 - loglik: -2.3788e+02 - logprior: -2.7029e+00
Epoch 6/10
11/11 - 2s - loss: 237.8060 - loglik: -2.3452e+02 - logprior: -2.6896e+00
Epoch 7/10
11/11 - 3s - loss: 236.0323 - loglik: -2.3277e+02 - logprior: -2.6494e+00
Epoch 8/10
11/11 - 3s - loss: 234.8829 - loglik: -2.3170e+02 - logprior: -2.6122e+00
Epoch 9/10
11/11 - 2s - loss: 235.1021 - loglik: -2.3197e+02 - logprior: -2.5889e+00
Fitted a model with MAP estimate = -233.8536
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 252.9069 - loglik: -2.3786e+02 - logprior: -1.4579e+01
Epoch 2/2
11/11 - 3s - loss: 230.0845 - loglik: -2.2335e+02 - logprior: -6.3864e+00
Fitted a model with MAP estimate = -222.3153
expansions: [(0, 19)]
discards: [ 0  7  8 75 76 84 87]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 234.7791 - loglik: -2.2224e+02 - logprior: -1.2211e+01
Epoch 2/2
11/11 - 3s - loss: 219.9851 - loglik: -2.1576e+02 - logprior: -3.8782e+00
Fitted a model with MAP estimate = -216.9487
expansions: [(25, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 228.6177 - loglik: -2.1493e+02 - logprior: -1.3335e+01
Epoch 2/10
11/11 - 3s - loss: 219.3798 - loglik: -2.1552e+02 - logprior: -3.4917e+00
Epoch 3/10
11/11 - 3s - loss: 214.1484 - loglik: -2.1200e+02 - logprior: -1.6552e+00
Epoch 4/10
11/11 - 3s - loss: 213.3647 - loglik: -2.1159e+02 - logprior: -1.1874e+00
Epoch 5/10
11/11 - 3s - loss: 210.8152 - loglik: -2.0924e+02 - logprior: -9.8611e-01
Epoch 6/10
11/11 - 3s - loss: 209.9768 - loglik: -2.0851e+02 - logprior: -9.1941e-01
Epoch 7/10
11/11 - 3s - loss: 209.8667 - loglik: -2.0852e+02 - logprior: -8.4405e-01
Epoch 8/10
11/11 - 3s - loss: 209.2766 - loglik: -2.0808e+02 - logprior: -7.4690e-01
Epoch 9/10
11/11 - 3s - loss: 209.2939 - loglik: -2.0815e+02 - logprior: -7.1097e-01
Fitted a model with MAP estimate = -208.6869
Time for alignment: 90.0285
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.9723 - loglik: -3.5046e+02 - logprior: -1.2483e+01
Epoch 2/10
11/11 - 2s - loss: 309.0630 - loglik: -3.0592e+02 - logprior: -3.1183e+00
Epoch 3/10
11/11 - 2s - loss: 269.5179 - loglik: -2.6733e+02 - logprior: -2.1691e+00
Epoch 4/10
11/11 - 2s - loss: 250.4162 - loglik: -2.4787e+02 - logprior: -2.4226e+00
Epoch 5/10
11/11 - 3s - loss: 242.4114 - loglik: -2.3945e+02 - logprior: -2.6035e+00
Epoch 6/10
11/11 - 3s - loss: 240.1150 - loglik: -2.3699e+02 - logprior: -2.6119e+00
Epoch 7/10
11/11 - 3s - loss: 238.0856 - loglik: -2.3499e+02 - logprior: -2.5863e+00
Epoch 8/10
11/11 - 2s - loss: 236.1882 - loglik: -2.3314e+02 - logprior: -2.5554e+00
Epoch 9/10
11/11 - 3s - loss: 235.6430 - loglik: -2.3261e+02 - logprior: -2.5626e+00
Epoch 10/10
11/11 - 3s - loss: 236.3583 - loglik: -2.3332e+02 - logprior: -2.5807e+00
Fitted a model with MAP estimate = -234.9194
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 2), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 255.6074 - loglik: -2.4053e+02 - logprior: -1.4614e+01
Epoch 2/2
11/11 - 3s - loss: 229.7528 - loglik: -2.2288e+02 - logprior: -6.4323e+00
Fitted a model with MAP estimate = -224.9881
expansions: [(0, 19)]
discards: [ 0 10 73 84 87]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 234.5219 - loglik: -2.2191e+02 - logprior: -1.2192e+01
Epoch 2/2
11/11 - 3s - loss: 219.1284 - loglik: -2.1490e+02 - logprior: -3.8015e+00
Fitted a model with MAP estimate = -215.6127
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 92]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 230.4556 - loglik: -2.1749e+02 - logprior: -1.2560e+01
Epoch 2/10
11/11 - 3s - loss: 220.5311 - loglik: -2.1684e+02 - logprior: -3.2969e+00
Epoch 3/10
11/11 - 3s - loss: 216.4622 - loglik: -2.1426e+02 - logprior: -1.6961e+00
Epoch 4/10
11/11 - 3s - loss: 215.9277 - loglik: -2.1410e+02 - logprior: -1.2408e+00
Epoch 5/10
11/11 - 3s - loss: 214.2796 - loglik: -2.1262e+02 - logprior: -1.0829e+00
Epoch 6/10
11/11 - 3s - loss: 212.1461 - loglik: -2.1065e+02 - logprior: -9.3539e-01
Epoch 7/10
11/11 - 3s - loss: 212.3312 - loglik: -2.1100e+02 - logprior: -8.2105e-01
Fitted a model with MAP estimate = -211.6647
Time for alignment: 84.3807
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 363.3374 - loglik: -3.5083e+02 - logprior: -1.2486e+01
Epoch 2/10
11/11 - 2s - loss: 309.8842 - loglik: -3.0674e+02 - logprior: -3.1186e+00
Epoch 3/10
11/11 - 3s - loss: 267.0375 - loglik: -2.6485e+02 - logprior: -2.1654e+00
Epoch 4/10
11/11 - 3s - loss: 247.2825 - loglik: -2.4472e+02 - logprior: -2.4255e+00
Epoch 5/10
11/11 - 2s - loss: 241.4700 - loglik: -2.3854e+02 - logprior: -2.5457e+00
Epoch 6/10
11/11 - 2s - loss: 237.1908 - loglik: -2.3414e+02 - logprior: -2.5294e+00
Epoch 7/10
11/11 - 2s - loss: 237.0238 - loglik: -2.3398e+02 - logprior: -2.5382e+00
Epoch 8/10
11/11 - 3s - loss: 236.7711 - loglik: -2.3373e+02 - logprior: -2.5372e+00
Epoch 9/10
11/11 - 2s - loss: 234.6321 - loglik: -2.3163e+02 - logprior: -2.5292e+00
Epoch 10/10
11/11 - 2s - loss: 235.6403 - loglik: -2.3265e+02 - logprior: -2.5243e+00
Fitted a model with MAP estimate = -234.7182
expansions: [(8, 2), (9, 3), (10, 2), (12, 2), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 256.3069 - loglik: -2.4127e+02 - logprior: -1.4556e+01
Epoch 2/2
11/11 - 3s - loss: 228.1334 - loglik: -2.2119e+02 - logprior: -6.4804e+00
Fitted a model with MAP estimate = -223.7074
expansions: [(0, 15)]
discards: [ 0  8 14 18 77 78 86 89]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 234.5680 - loglik: -2.2201e+02 - logprior: -1.2140e+01
Epoch 2/2
11/11 - 3s - loss: 218.9946 - loglik: -2.1459e+02 - logprior: -4.0131e+00
Fitted a model with MAP estimate = -216.7827
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 229.0484 - loglik: -2.1616e+02 - logprior: -1.2514e+01
Epoch 2/10
11/11 - 3s - loss: 218.2281 - loglik: -2.1456e+02 - logprior: -3.3029e+00
Epoch 3/10
11/11 - 3s - loss: 214.9295 - loglik: -2.1274e+02 - logprior: -1.7387e+00
Epoch 4/10
11/11 - 3s - loss: 213.2513 - loglik: -2.1134e+02 - logprior: -1.3522e+00
Epoch 5/10
11/11 - 3s - loss: 211.4583 - loglik: -2.0954e+02 - logprior: -1.3050e+00
Epoch 6/10
11/11 - 3s - loss: 209.7950 - loglik: -2.0804e+02 - logprior: -1.1952e+00
Epoch 7/10
11/11 - 3s - loss: 210.7398 - loglik: -2.0917e+02 - logprior: -1.0376e+00
Fitted a model with MAP estimate = -208.9364
Time for alignment: 83.0071
Computed alignments with likelihoods: ['-208.6869', '-211.6647', '-208.9364']
Best model has likelihood: -208.6869
time for generating output: 0.2779
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.5569767441860465
Training of 3 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fca007f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1899f10340>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1899f10a60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f180ee7af40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f198c2b4ca0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19487b96d0>, <__main__.SimpleDirichletPrior object at 0x7f184df3e640>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f197b2f7790>

Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 754.5302 - loglik: -7.5224e+02 - logprior: -1.8150e+00
Epoch 2/10
39/39 - 19s - loss: 633.9354 - loglik: -6.3100e+02 - logprior: -1.7832e+00
Epoch 3/10
39/39 - 19s - loss: 621.5681 - loglik: -6.1819e+02 - logprior: -1.9268e+00
Epoch 4/10
39/39 - 19s - loss: 618.8464 - loglik: -6.1562e+02 - logprior: -1.9056e+00
Epoch 5/10
39/39 - 19s - loss: 617.6368 - loglik: -6.1451e+02 - logprior: -1.9089e+00
Epoch 6/10
39/39 - 19s - loss: 616.5641 - loglik: -6.1350e+02 - logprior: -1.9224e+00
Epoch 7/10
39/39 - 19s - loss: 616.1312 - loglik: -6.1311e+02 - logprior: -1.9284e+00
Epoch 8/10
39/39 - 19s - loss: 615.6338 - loglik: -6.1267e+02 - logprior: -1.9408e+00
Epoch 9/10
39/39 - 19s - loss: 615.2482 - loglik: -6.1232e+02 - logprior: -1.9517e+00
Epoch 10/10
39/39 - 19s - loss: 614.8848 - loglik: -6.1200e+02 - logprior: -1.9570e+00
Fitted a model with MAP estimate = -575.6785
expansions: [(12, 4), (13, 1), (16, 1), (17, 1), (36, 2), (46, 3), (47, 2), (59, 1), (60, 2), (62, 5), (67, 1), (68, 1), (69, 1), (71, 1), (91, 1), (120, 2), (125, 1), (126, 5), (127, 1), (128, 2), (138, 3), (139, 2), (141, 1), (142, 1), (143, 1), (144, 1), (161, 5), (162, 1), (165, 4), (167, 4), (168, 3), (169, 3), (180, 2), (191, 1), (193, 1), (194, 1), (210, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 303 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 614.3500 - loglik: -6.0960e+02 - logprior: -3.2431e+00
Epoch 2/2
39/39 - 28s - loss: 591.1163 - loglik: -5.8730e+02 - logprior: -1.7183e+00
Fitted a model with MAP estimate = -546.3526
expansions: [(0, 2), (42, 1), (226, 1)]
discards: [  0  11  12  56  58  78  79 147 161 176 179 211 228 246 286 287]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 594.1860 - loglik: -5.8949e+02 - logprior: -2.0521e+00
Epoch 2/2
39/39 - 27s - loss: 587.7120 - loglik: -5.8419e+02 - logprior: -1.0335e+00
Fitted a model with MAP estimate = -543.6372
expansions: [(203, 2)]
discards: [  0 152 199 200 201]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 550.6985 - loglik: -5.4584e+02 - logprior: -1.9980e+00
Epoch 2/10
45/45 - 30s - loss: 544.4258 - loglik: -5.4115e+02 - logprior: -9.1177e-01
Epoch 3/10
45/45 - 29s - loss: 542.1978 - loglik: -5.3945e+02 - logprior: -7.9237e-01
Epoch 4/10
45/45 - 30s - loss: 541.9113 - loglik: -5.3941e+02 - logprior: -7.6598e-01
Epoch 5/10
45/45 - 29s - loss: 539.0783 - loglik: -5.3697e+02 - logprior: -5.9204e-01
Epoch 6/10
45/45 - 30s - loss: 539.6341 - loglik: -5.3763e+02 - logprior: -5.8069e-01
Fitted a model with MAP estimate = -537.5345
Time for alignment: 635.6442
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 755.2091 - loglik: -7.5290e+02 - logprior: -1.8635e+00
Epoch 2/10
39/39 - 19s - loss: 632.9807 - loglik: -6.2962e+02 - logprior: -2.0822e+00
Epoch 3/10
39/39 - 19s - loss: 621.9352 - loglik: -6.1831e+02 - logprior: -2.1575e+00
Epoch 4/10
39/39 - 19s - loss: 619.4865 - loglik: -6.1605e+02 - logprior: -2.1328e+00
Epoch 5/10
39/39 - 19s - loss: 617.8298 - loglik: -6.1451e+02 - logprior: -2.1367e+00
Epoch 6/10
39/39 - 19s - loss: 617.0947 - loglik: -6.1384e+02 - logprior: -2.1383e+00
Epoch 7/10
39/39 - 19s - loss: 616.3848 - loglik: -6.1318e+02 - logprior: -2.1419e+00
Epoch 8/10
39/39 - 19s - loss: 615.9014 - loglik: -6.1274e+02 - logprior: -2.1531e+00
Epoch 9/10
39/39 - 19s - loss: 615.5138 - loglik: -6.1239e+02 - logprior: -2.1611e+00
Epoch 10/10
39/39 - 19s - loss: 615.1759 - loglik: -6.1209e+02 - logprior: -2.1627e+00
Fitted a model with MAP estimate = -576.6941
expansions: [(12, 4), (13, 1), (16, 1), (20, 1), (36, 1), (37, 1), (39, 1), (46, 2), (47, 2), (59, 3), (62, 4), (63, 1), (66, 1), (67, 1), (68, 1), (70, 1), (93, 2), (97, 2), (118, 4), (119, 2), (120, 7), (121, 2), (122, 2), (123, 1), (130, 1), (132, 1), (136, 2), (139, 1), (150, 1), (157, 2), (158, 2), (163, 1), (164, 1), (167, 4), (168, 3), (169, 2), (181, 2), (182, 1), (191, 2), (193, 1), (194, 1), (203, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 614.6826 - loglik: -6.0984e+02 - logprior: -3.3099e+00
Epoch 2/2
39/39 - 29s - loss: 591.2150 - loglik: -5.8729e+02 - logprior: -1.8115e+00
Fitted a model with MAP estimate = -545.1921
expansions: [(0, 2), (228, 1)]
discards: [  0  11  12  58  75 118 124 149 150 163 185 208 213 231 249 263 289 291]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 292 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 595.0331 - loglik: -5.9029e+02 - logprior: -2.0737e+00
Epoch 2/2
39/39 - 27s - loss: 588.4095 - loglik: -5.8485e+02 - logprior: -1.0436e+00
Fitted a model with MAP estimate = -544.5384
expansions: []
discards: [  0 198 199 200]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 33s - loss: 550.4130 - loglik: -5.4554e+02 - logprior: -1.9930e+00
Epoch 2/10
45/45 - 30s - loss: 544.8322 - loglik: -5.4154e+02 - logprior: -9.1564e-01
Epoch 3/10
45/45 - 30s - loss: 543.1165 - loglik: -5.4031e+02 - logprior: -8.3093e-01
Epoch 4/10
45/45 - 30s - loss: 541.6533 - loglik: -5.3912e+02 - logprior: -7.5453e-01
Epoch 5/10
45/45 - 29s - loss: 539.1826 - loglik: -5.3699e+02 - logprior: -6.4446e-01
Epoch 6/10
45/45 - 30s - loss: 540.6425 - loglik: -5.3868e+02 - logprior: -5.3147e-01
Fitted a model with MAP estimate = -537.8475
Time for alignment: 639.3090
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 756.3312 - loglik: -7.5401e+02 - logprior: -1.8700e+00
Epoch 2/10
39/39 - 19s - loss: 632.2178 - loglik: -6.2875e+02 - logprior: -2.1533e+00
Epoch 3/10
39/39 - 19s - loss: 621.3807 - loglik: -6.1769e+02 - logprior: -2.1932e+00
Epoch 4/10
39/39 - 19s - loss: 619.4876 - loglik: -6.1604e+02 - logprior: -2.1385e+00
Epoch 5/10
39/39 - 19s - loss: 618.1497 - loglik: -6.1483e+02 - logprior: -2.1287e+00
Epoch 6/10
39/39 - 19s - loss: 616.9828 - loglik: -6.1372e+02 - logprior: -2.1444e+00
Epoch 7/10
39/39 - 19s - loss: 616.5818 - loglik: -6.1333e+02 - logprior: -2.1529e+00
Epoch 8/10
39/39 - 19s - loss: 616.0835 - loglik: -6.1288e+02 - logprior: -2.1659e+00
Epoch 9/10
39/39 - 19s - loss: 615.8530 - loglik: -6.1267e+02 - logprior: -2.1762e+00
Epoch 10/10
39/39 - 19s - loss: 615.5649 - loglik: -6.1241e+02 - logprior: -2.1802e+00
Fitted a model with MAP estimate = -576.9112
expansions: [(12, 4), (13, 1), (16, 1), (20, 1), (36, 1), (37, 1), (39, 1), (46, 2), (47, 2), (58, 2), (59, 2), (62, 4), (63, 1), (66, 1), (67, 1), (68, 1), (70, 1), (90, 1), (97, 2), (122, 1), (123, 8), (124, 2), (125, 2), (126, 2), (132, 2), (134, 1), (140, 1), (141, 1), (143, 2), (144, 1), (156, 1), (157, 2), (158, 2), (164, 2), (167, 3), (168, 3), (169, 2), (181, 2), (182, 1), (191, 1), (193, 1), (194, 1), (206, 1), (209, 1), (211, 2), (212, 2), (217, 1), (218, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 305 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 613.7977 - loglik: -6.0899e+02 - logprior: -3.2278e+00
Epoch 2/2
39/39 - 29s - loss: 590.5795 - loglik: -5.8676e+02 - logprior: -1.6668e+00
Fitted a model with MAP estimate = -545.7595
expansions: [(0, 2)]
discards: [  0  11  12  58  76  77 124 158 162 177 193 213 227 230 248 287 289]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 290 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 594.2117 - loglik: -5.8947e+02 - logprior: -2.0515e+00
Epoch 2/2
39/39 - 27s - loss: 588.0027 - loglik: -5.8449e+02 - logprior: -1.0241e+00
Fitted a model with MAP estimate = -544.3000
expansions: []
discards: [  0 152 199 200 201]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 285 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 32s - loss: 551.1324 - loglik: -5.4628e+02 - logprior: -2.0188e+00
Epoch 2/10
45/45 - 29s - loss: 545.1913 - loglik: -5.4193e+02 - logprior: -9.1839e-01
Epoch 3/10
45/45 - 29s - loss: 543.4662 - loglik: -5.4067e+02 - logprior: -8.2075e-01
Epoch 4/10
45/45 - 30s - loss: 542.6787 - loglik: -5.4017e+02 - logprior: -7.3661e-01
Epoch 5/10
45/45 - 28s - loss: 540.8387 - loglik: -5.3863e+02 - logprior: -6.4280e-01
Epoch 6/10
45/45 - 29s - loss: 540.5877 - loglik: -5.3862e+02 - logprior: -5.2212e-01
Epoch 7/10
45/45 - 29s - loss: 540.5515 - loglik: -5.3869e+02 - logprior: -5.0573e-01
Epoch 8/10
45/45 - 29s - loss: 538.8750 - loglik: -5.3728e+02 - logprior: -3.1915e-01
Epoch 9/10
45/45 - 29s - loss: 539.0250 - loglik: -5.3750e+02 - logprior: -2.6757e-01
Fitted a model with MAP estimate = -536.8336
Time for alignment: 719.7479
Computed alignments with likelihoods: ['-537.5345', '-537.8475', '-536.8336']
Best model has likelihood: -536.8336
time for generating output: 0.3126
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.8905886265380778
Training of 3 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f195911bac0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1899f270d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18b35b1a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18190488b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1950cf2040>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f187816fc70>, <__main__.SimpleDirichletPrior object at 0x7f1818e20ee0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f181a17aaf0>

Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 12s - loss: 445.3963 - loglik: -4.4403e+02 - logprior: -1.0879e+00
Epoch 2/10
30/30 - 8s - loss: 373.2802 - loglik: -3.7074e+02 - logprior: -1.1991e+00
Epoch 3/10
30/30 - 8s - loss: 361.5038 - loglik: -3.5905e+02 - logprior: -1.2063e+00
Epoch 4/10
30/30 - 8s - loss: 359.1991 - loglik: -3.5696e+02 - logprior: -1.2242e+00
Epoch 5/10
30/30 - 8s - loss: 357.7570 - loglik: -3.5570e+02 - logprior: -1.2066e+00
Epoch 6/10
30/30 - 8s - loss: 357.3424 - loglik: -3.5540e+02 - logprior: -1.1879e+00
Epoch 7/10
30/30 - 8s - loss: 357.0423 - loglik: -3.5510e+02 - logprior: -1.1779e+00
Epoch 8/10
30/30 - 8s - loss: 356.8889 - loglik: -3.5503e+02 - logprior: -1.1720e+00
Epoch 9/10
30/30 - 8s - loss: 356.3268 - loglik: -3.5452e+02 - logprior: -1.1710e+00
Epoch 10/10
30/30 - 8s - loss: 356.1313 - loglik: -3.5433e+02 - logprior: -1.1701e+00
Fitted a model with MAP estimate = -348.2641
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (26, 2), (33, 1), (34, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (52, 1), (54, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 350.4186 - loglik: -3.4795e+02 - logprior: -1.3116e+00
Epoch 2/2
61/61 - 14s - loss: 342.5404 - loglik: -3.3998e+02 - logprior: -1.0139e+00
Fitted a model with MAP estimate = -329.4981
expansions: []
discards: [ 18  30  52  93  95 133 147 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 170 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 18s - loss: 343.4127 - loglik: -3.4041e+02 - logprior: -1.1936e+00
Epoch 2/2
61/61 - 14s - loss: 341.8730 - loglik: -3.3920e+02 - logprior: -9.2760e-01
Fitted a model with MAP estimate = -329.7382
expansions: []
discards: [ 47 143]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 22s - loss: 330.3105 - loglik: -3.2757e+02 - logprior: -7.9531e-01
Epoch 2/10
87/87 - 19s - loss: 328.7628 - loglik: -3.2605e+02 - logprior: -6.7426e-01
Epoch 3/10
87/87 - 19s - loss: 328.0795 - loglik: -3.2599e+02 - logprior: -6.4094e-01
Epoch 4/10
87/87 - 19s - loss: 327.4142 - loglik: -3.2569e+02 - logprior: -6.0710e-01
Epoch 5/10
87/87 - 19s - loss: 327.0977 - loglik: -3.2544e+02 - logprior: -5.7867e-01
Epoch 6/10
87/87 - 19s - loss: 326.6898 - loglik: -3.2505e+02 - logprior: -5.5009e-01
Epoch 7/10
87/87 - 19s - loss: 326.5719 - loglik: -3.2498e+02 - logprior: -5.2385e-01
Epoch 8/10
87/87 - 19s - loss: 326.4160 - loglik: -3.2487e+02 - logprior: -4.9774e-01
Epoch 9/10
87/87 - 19s - loss: 325.5186 - loglik: -3.2393e+02 - logprior: -4.6614e-01
Epoch 10/10
87/87 - 19s - loss: 325.2726 - loglik: -3.2376e+02 - logprior: -4.4544e-01
Fitted a model with MAP estimate = -323.7121
Time for alignment: 503.7339
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 14s - loss: 446.0413 - loglik: -4.4469e+02 - logprior: -1.0829e+00
Epoch 2/10
30/30 - 8s - loss: 372.7618 - loglik: -3.7062e+02 - logprior: -1.2193e+00
Epoch 3/10
30/30 - 8s - loss: 361.8320 - loglik: -3.5955e+02 - logprior: -1.2145e+00
Epoch 4/10
30/30 - 8s - loss: 359.4326 - loglik: -3.5725e+02 - logprior: -1.2012e+00
Epoch 5/10
30/30 - 8s - loss: 358.9908 - loglik: -3.5693e+02 - logprior: -1.1715e+00
Epoch 6/10
30/30 - 8s - loss: 358.1385 - loglik: -3.5616e+02 - logprior: -1.1552e+00
Epoch 7/10
30/30 - 8s - loss: 358.0551 - loglik: -3.5609e+02 - logprior: -1.1476e+00
Epoch 8/10
30/30 - 8s - loss: 357.1505 - loglik: -3.5528e+02 - logprior: -1.1412e+00
Epoch 9/10
30/30 - 8s - loss: 356.8253 - loglik: -3.5501e+02 - logprior: -1.1391e+00
Epoch 10/10
30/30 - 8s - loss: 357.3342 - loglik: -3.5552e+02 - logprior: -1.1357e+00
Fitted a model with MAP estimate = -348.6050
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (22, 2), (29, 1), (34, 1), (35, 1), (39, 2), (40, 3), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (128, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 18s - loss: 349.9510 - loglik: -3.4740e+02 - logprior: -1.3175e+00
Epoch 2/2
61/61 - 14s - loss: 342.4280 - loglik: -3.3983e+02 - logprior: -1.0121e+00
Fitted a model with MAP estimate = -329.6790
expansions: []
discards: [ 18  26  49  52  93  95 133 147 151 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 343.5474 - loglik: -3.4060e+02 - logprior: -1.1745e+00
Epoch 2/2
61/61 - 14s - loss: 342.0093 - loglik: -3.3936e+02 - logprior: -8.8527e-01
Fitted a model with MAP estimate = -329.6594
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 24s - loss: 329.5267 - loglik: -3.2680e+02 - logprior: -7.8486e-01
Epoch 2/10
87/87 - 19s - loss: 328.9279 - loglik: -3.2622e+02 - logprior: -6.6440e-01
Epoch 3/10
87/87 - 19s - loss: 328.0309 - loglik: -3.2595e+02 - logprior: -6.3533e-01
Epoch 4/10
87/87 - 19s - loss: 327.3423 - loglik: -3.2562e+02 - logprior: -6.0099e-01
Epoch 5/10
87/87 - 19s - loss: 327.4856 - loglik: -3.2584e+02 - logprior: -5.7019e-01
Fitted a model with MAP estimate = -325.3629
Time for alignment: 409.0175
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 12s - loss: 445.9780 - loglik: -4.4462e+02 - logprior: -1.0814e+00
Epoch 2/10
30/30 - 8s - loss: 373.7644 - loglik: -3.7152e+02 - logprior: -1.1982e+00
Epoch 3/10
30/30 - 8s - loss: 360.7136 - loglik: -3.5843e+02 - logprior: -1.2257e+00
Epoch 4/10
30/30 - 8s - loss: 358.1631 - loglik: -3.5598e+02 - logprior: -1.2206e+00
Epoch 5/10
30/30 - 8s - loss: 358.0685 - loglik: -3.5602e+02 - logprior: -1.1941e+00
Epoch 6/10
30/30 - 8s - loss: 357.0953 - loglik: -3.5513e+02 - logprior: -1.1830e+00
Epoch 7/10
30/30 - 8s - loss: 356.9432 - loglik: -3.5500e+02 - logprior: -1.1749e+00
Epoch 8/10
30/30 - 8s - loss: 355.9579 - loglik: -3.5408e+02 - logprior: -1.1710e+00
Epoch 9/10
30/30 - 8s - loss: 355.9687 - loglik: -3.5415e+02 - logprior: -1.1698e+00
Fitted a model with MAP estimate = -346.6792
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (22, 2), (29, 1), (34, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (88, 1), (93, 1), (94, 1), (97, 1), (99, 1), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 177 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 17s - loss: 350.0470 - loglik: -3.4750e+02 - logprior: -1.3057e+00
Epoch 2/2
61/61 - 14s - loss: 342.4099 - loglik: -3.3982e+02 - logprior: -1.0141e+00
Fitted a model with MAP estimate = -329.5911
expansions: []
discards: [ 17  26  52  93  95 147 149 154]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 18s - loss: 343.4381 - loglik: -3.4053e+02 - logprior: -1.1701e+00
Epoch 2/2
61/61 - 14s - loss: 342.2761 - loglik: -3.3964e+02 - logprior: -9.0059e-01
Fitted a model with MAP estimate = -329.7599
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 22s - loss: 329.6418 - loglik: -3.2694e+02 - logprior: -7.8963e-01
Epoch 2/10
87/87 - 19s - loss: 329.1892 - loglik: -3.2649e+02 - logprior: -6.6981e-01
Epoch 3/10
87/87 - 19s - loss: 327.7928 - loglik: -3.2574e+02 - logprior: -6.3798e-01
Epoch 4/10
87/87 - 19s - loss: 327.6686 - loglik: -3.2595e+02 - logprior: -6.0017e-01
Epoch 5/10
87/87 - 19s - loss: 327.1972 - loglik: -3.2556e+02 - logprior: -5.7459e-01
Epoch 6/10
87/87 - 19s - loss: 327.0399 - loglik: -3.2543e+02 - logprior: -5.4772e-01
Epoch 7/10
87/87 - 19s - loss: 326.2542 - loglik: -3.2467e+02 - logprior: -5.1759e-01
Epoch 8/10
87/87 - 19s - loss: 325.5370 - loglik: -3.2401e+02 - logprior: -4.9204e-01
Epoch 9/10
87/87 - 19s - loss: 326.1167 - loglik: -3.2452e+02 - logprior: -4.6483e-01
Fitted a model with MAP estimate = -324.4432
Time for alignment: 472.9607
Computed alignments with likelihoods: ['-323.7121', '-325.3629', '-324.4432']
Best model has likelihood: -323.7121
time for generating output: 0.2885
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.661257382568858
Training of 3 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19597f2430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fd701160>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f199d0be5e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19ae6fe970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19ae6fe280>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18b347b520>, <__main__.SimpleDirichletPrior object at 0x7f19344a8be0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f181a17aaf0>

Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 47s - loss: 1228.9695 - loglik: -1.2266e+03 - logprior: -1.8377e+00
Epoch 2/10
40/40 - 44s - loss: 1093.6320 - loglik: -1.0881e+03 - logprior: -2.8946e+00
Epoch 3/10
40/40 - 44s - loss: 1074.4689 - loglik: -1.0666e+03 - logprior: -3.0156e+00
Epoch 4/10
40/40 - 44s - loss: 1061.9916 - loglik: -1.0532e+03 - logprior: -3.2237e+00
Epoch 5/10
40/40 - 44s - loss: 1052.7805 - loglik: -1.0440e+03 - logprior: -3.3376e+00
Epoch 6/10
40/40 - 44s - loss: 1048.7695 - loglik: -1.0402e+03 - logprior: -3.4676e+00
Epoch 7/10
40/40 - 44s - loss: 1044.5076 - loglik: -1.0366e+03 - logprior: -3.5144e+00
Epoch 8/10
40/40 - 44s - loss: 1042.4954 - loglik: -1.0348e+03 - logprior: -3.5544e+00
Epoch 9/10
40/40 - 44s - loss: 1040.4037 - loglik: -1.0325e+03 - logprior: -3.6096e+00
Epoch 10/10
40/40 - 44s - loss: 1036.6843 - loglik: -1.0287e+03 - logprior: -3.6439e+00
Fitted a model with MAP estimate = -811.5728
expansions: [(118, 1), (124, 1), (214, 2), (231, 7), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  98  99 100 101 102 103 104 105 106 107 108
 109 110 111 112 155 156 157 158 159 160 161 162 163 164 179 180 181 182
 183 184 185 186 187 188 234 235 236 237 238 239 240 241 242 243 244 245
 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263
 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281
 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299
 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317
 318 319 320 321 322 323 324 325 326 327 328 329]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 20s - loss: 1312.8792 - loglik: -1.3033e+03 - logprior: -4.1546e+00
Epoch 2/2
40/40 - 17s - loss: 1237.9265 - loglik: -1.2261e+03 - logprior: -4.0602e+00
Fitted a model with MAP estimate = -883.9104
expansions: [(0, 5), (5, 2), (6, 4), (12, 2), (48, 1), (49, 3), (54, 9), (56, 1), (58, 4), (82, 1), (88, 11), (112, 1)]
discards: [ 43  44 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 23s - loss: 1212.0188 - loglik: -1.1967e+03 - logprior: -4.6602e+00
Epoch 2/2
40/40 - 20s - loss: 1168.2911 - loglik: -1.1515e+03 - logprior: -4.3845e+00
Fitted a model with MAP estimate = -845.7936
expansions: [(14, 1), (15, 2), (16, 1), (17, 1), (62, 1), (123, 2), (156, 1)]
discards: [  0   1   2   3  10  49  50  51 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 31s - loss: 868.3653 - loglik: -8.5125e+02 - logprior: -2.8919e+00
Epoch 2/10
56/56 - 27s - loss: 854.1408 - loglik: -8.3789e+02 - logprior: -2.7845e+00
Epoch 3/10
56/56 - 27s - loss: 849.8218 - loglik: -8.3638e+02 - logprior: -2.7662e+00
Epoch 4/10
56/56 - 27s - loss: 837.3415 - loglik: -8.2471e+02 - logprior: -3.4136e+00
Epoch 5/10
56/56 - 27s - loss: 827.9371 - loglik: -8.1518e+02 - logprior: -3.6679e+00
Epoch 6/10
56/56 - 27s - loss: 823.0165 - loglik: -8.0928e+02 - logprior: -3.7033e+00
Epoch 7/10
56/56 - 27s - loss: 810.2568 - loglik: -7.9844e+02 - logprior: -3.6896e+00
Epoch 8/10
56/56 - 27s - loss: 809.6915 - loglik: -7.9915e+02 - logprior: -3.6610e+00
Epoch 9/10
56/56 - 27s - loss: 803.7814 - loglik: -7.9460e+02 - logprior: -3.6344e+00
Epoch 10/10
56/56 - 27s - loss: 797.8870 - loglik: -7.8976e+02 - logprior: -3.5956e+00
Fitted a model with MAP estimate = -793.5444
Time for alignment: 1006.3096
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 47s - loss: 1240.7844 - loglik: -1.2390e+03 - logprior: -1.2700e+00
Epoch 2/10
40/40 - 44s - loss: 1126.0045 - loglik: -1.1231e+03 - logprior: -2.9897e-01
Epoch 3/10
40/40 - 44s - loss: 1106.4071 - loglik: -1.1016e+03 - logprior: -4.8620e-01
Epoch 4/10
40/40 - 44s - loss: 1097.5671 - loglik: -1.0922e+03 - logprior: -6.1270e-01
Epoch 5/10
40/40 - 44s - loss: 1093.0457 - loglik: -1.0879e+03 - logprior: -6.8211e-01
Epoch 6/10
40/40 - 44s - loss: 1089.9224 - loglik: -1.0854e+03 - logprior: -7.3885e-01
Epoch 7/10
40/40 - 44s - loss: 1087.4504 - loglik: -1.0835e+03 - logprior: -7.7655e-01
Epoch 8/10
40/40 - 44s - loss: 1086.2888 - loglik: -1.0827e+03 - logprior: -8.0508e-01
Epoch 9/10
40/40 - 44s - loss: 1085.3801 - loglik: -1.0821e+03 - logprior: -8.2699e-01
Epoch 10/10
40/40 - 44s - loss: 1084.2775 - loglik: -1.0810e+03 - logprior: -8.5187e-01
Fitted a model with MAP estimate = -829.9203
expansions: [(18, 2), (42, 2), (113, 2), (123, 1), (124, 1), (168, 1), (220, 3), (241, 2), (292, 2), (293, 4), (294, 2), (295, 1), (296, 2), (317, 1), (318, 2), (319, 8), (329, 10), (330, 88)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 463 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 85s - loss: 1071.6475 - loglik: -1.0652e+03 - logprior: -1.8953e+00
Epoch 2/2
80/80 - 80s - loss: 1014.1888 - loglik: -1.0069e+03 - logprior: -1.2650e+00
Fitted a model with MAP estimate = -753.8662
expansions: [(371, 7), (387, 1), (388, 3), (389, 1), (410, 1), (457, 1)]
discards: [ 18  44 117 308 309 310 344 346 430 437 438 439 440 441 442 443 444 445
 446 447 461 462]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 455 on 10507 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
80/80 - 81s - loss: 1015.2272 - loglik: -1.0054e+03 - logprior: -1.1772e+00
Epoch 2/2
80/80 - 78s - loss: 1001.9581 - loglik: -9.9360e+02 - logprior: -3.0703e-01
Fitted a model with MAP estimate = -746.2066
expansions: [(435, 2), (455, 2)]
discards: [194 248 366 388]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 455 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 114s - loss: 750.0488 - loglik: -7.4050e+02 - logprior: -6.0279e-01
Epoch 2/10
113/113 - 109s - loss: 743.0418 - loglik: -7.3578e+02 - logprior: -2.4723e-01
Epoch 3/10
113/113 - 109s - loss: 738.7527 - loglik: -7.3367e+02 - logprior: -2.7651e-02
Epoch 4/10
113/113 - 109s - loss: 738.6542 - loglik: -7.3437e+02 - logprior: 0.2289
Epoch 5/10
113/113 - 109s - loss: 736.4872 - loglik: -7.3276e+02 - logprior: 0.4397
Epoch 6/10
113/113 - 109s - loss: 731.1864 - loglik: -7.2792e+02 - logprior: 0.7153
Epoch 7/10
113/113 - 109s - loss: 730.7895 - loglik: -7.2864e+02 - logprior: 1.1614
Epoch 8/10
113/113 - 109s - loss: 725.7350 - loglik: -7.2435e+02 - logprior: 1.4834
Epoch 9/10
113/113 - 109s - loss: 726.2087 - loglik: -7.2502e+02 - logprior: 1.4859
Fitted a model with MAP estimate = -721.2298
Time for alignment: 2221.4218
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 48s - loss: 1236.8142 - loglik: -1.2351e+03 - logprior: -1.1929e+00
Epoch 2/10
40/40 - 44s - loss: 1132.7947 - loglik: -1.1305e+03 - logprior: 0.0228
Epoch 3/10
40/40 - 44s - loss: 1118.4924 - loglik: -1.1146e+03 - logprior: -1.5681e-01
Epoch 4/10
40/40 - 44s - loss: 1106.1190 - loglik: -1.1008e+03 - logprior: -3.8147e-01
Epoch 5/10
40/40 - 44s - loss: 1098.9601 - loglik: -1.0937e+03 - logprior: -5.0523e-01
Epoch 6/10
40/40 - 44s - loss: 1094.9606 - loglik: -1.0904e+03 - logprior: -5.9964e-01
Epoch 7/10
40/40 - 44s - loss: 1068.9622 - loglik: -1.0616e+03 - logprior: -2.7515e+00
Epoch 8/10
40/40 - 44s - loss: 1053.0615 - loglik: -1.0445e+03 - logprior: -3.6220e+00
Epoch 9/10
40/40 - 44s - loss: 1049.9995 - loglik: -1.0418e+03 - logprior: -3.5656e+00
Epoch 10/10
40/40 - 44s - loss: 1044.7869 - loglik: -1.0364e+03 - logprior: -3.5804e+00
Fitted a model with MAP estimate = -825.7374
expansions: [(118, 1), (330, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  60  61  64  84  85  86  87  88  89  90  91  93  94 160
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269
 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287
 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305
 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323
 324 325 326 327 328 329]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 22s - loss: 1318.3538 - loglik: -1.3091e+03 - logprior: -3.0688e+00
Epoch 2/2
40/40 - 18s - loss: 1244.2753 - loglik: -1.2342e+03 - logprior: -1.4810e+00
Fitted a model with MAP estimate = -889.8224
expansions: [(0, 14), (2, 2), (4, 2), (6, 15), (7, 3), (10, 2), (17, 1), (18, 7), (22, 3), (23, 14), (24, 1), (31, 8), (47, 25), (48, 2), (50, 1), (105, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 30s - loss: 1190.9700 - loglik: -1.1756e+03 - logprior: -4.0094e+00
Epoch 2/2
40/40 - 27s - loss: 1128.1072 - loglik: -1.1123e+03 - logprior: -3.1170e+00
Fitted a model with MAP estimate = -818.9842
expansions: [(0, 2), (17, 1), (127, 1), (128, 1), (129, 2), (130, 2), (131, 5), (135, 2), (138, 2)]
discards: [ 14  15  29  30  31  32  46  74  75  76  77 136 139 140 141 142 143 144
 145 146 147 148 149]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 225 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 40s - loss: 843.4218 - loglik: -8.2747e+02 - logprior: -2.0265e+00
Epoch 2/10
56/56 - 37s - loss: 830.6123 - loglik: -8.1616e+02 - logprior: -1.9375e+00
Epoch 3/10
56/56 - 37s - loss: 818.8134 - loglik: -8.0793e+02 - logprior: -1.8897e+00
Epoch 4/10
56/56 - 37s - loss: 815.3359 - loglik: -8.0631e+02 - logprior: -1.8448e+00
Epoch 5/10
56/56 - 37s - loss: 810.0309 - loglik: -8.0207e+02 - logprior: -1.8030e+00
Epoch 6/10
56/56 - 36s - loss: 806.8138 - loglik: -7.9973e+02 - logprior: -1.7640e+00
Epoch 7/10
56/56 - 37s - loss: 806.1879 - loglik: -8.0002e+02 - logprior: -1.7108e+00
Epoch 8/10
56/56 - 37s - loss: 802.2949 - loglik: -7.9635e+02 - logprior: -1.6498e+00
Epoch 9/10
56/56 - 37s - loss: 800.9362 - loglik: -7.9467e+02 - logprior: -1.5741e+00
Epoch 10/10
56/56 - 37s - loss: 793.7267 - loglik: -7.8736e+02 - logprior: -1.5095e+00
Fitted a model with MAP estimate = -789.9090
Time for alignment: 1135.4425
Computed alignments with likelihoods: ['-793.5444', '-721.2298', '-789.9090']
Best model has likelihood: -721.2298
time for generating output: 0.5354
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.7029405716635821
Training of 3 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1947aa23d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f199d0e10d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19485fa9a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fe7f0820>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1834136070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fe26e460>, <__main__.SimpleDirichletPrior object at 0x7f180f501250>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1899ae01f0>

Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.5634 - loglik: -4.4657e+02 - logprior: -2.9768e+00
Epoch 2/10
19/19 - 5s - loss: 285.6647 - loglik: -2.8401e+02 - logprior: -1.4250e+00
Epoch 3/10
19/19 - 5s - loss: 215.8734 - loglik: -2.1395e+02 - logprior: -1.8238e+00
Epoch 4/10
19/19 - 5s - loss: 202.3927 - loglik: -2.0009e+02 - logprior: -2.1510e+00
Epoch 5/10
19/19 - 5s - loss: 197.1015 - loglik: -1.9437e+02 - logprior: -2.3981e+00
Epoch 6/10
19/19 - 5s - loss: 194.6383 - loglik: -1.9177e+02 - logprior: -2.4595e+00
Epoch 7/10
19/19 - 5s - loss: 192.5967 - loglik: -1.8978e+02 - logprior: -2.3937e+00
Epoch 8/10
19/19 - 5s - loss: 192.7728 - loglik: -1.8999e+02 - logprior: -2.3725e+00
Fitted a model with MAP estimate = -186.0988
expansions: [(7, 1), (8, 2), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (83, 1), (91, 1), (97, 1), (100, 1), (101, 1), (104, 1), (114, 1), (120, 2), (121, 1), (122, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 181.3598 - loglik: -1.7812e+02 - logprior: -2.9092e+00
Epoch 2/2
19/19 - 6s - loss: 144.6417 - loglik: -1.4343e+02 - logprior: -1.0412e+00
Fitted a model with MAP estimate = -143.0931
expansions: []
discards: [ 50  76 149]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 145.5422 - loglik: -1.4255e+02 - logprior: -2.8005e+00
Epoch 2/2
19/19 - 6s - loss: 141.2977 - loglik: -1.4019e+02 - logprior: -8.8637e-01
Fitted a model with MAP estimate = -141.3495
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 13s - loss: 143.6149 - loglik: -1.4142e+02 - logprior: -2.0017e+00
Epoch 2/10
22/22 - 7s - loss: 139.1433 - loglik: -1.3805e+02 - logprior: -9.0656e-01
Epoch 3/10
22/22 - 7s - loss: 136.5448 - loglik: -1.3533e+02 - logprior: -9.5095e-01
Epoch 4/10
22/22 - 7s - loss: 135.1422 - loglik: -1.3350e+02 - logprior: -1.2211e+00
Epoch 5/10
22/22 - 7s - loss: 129.8633 - loglik: -1.2819e+02 - logprior: -1.1620e+00
Epoch 6/10
22/22 - 7s - loss: 129.3570 - loglik: -1.2773e+02 - logprior: -1.1210e+00
Epoch 7/10
22/22 - 7s - loss: 127.6068 - loglik: -1.2605e+02 - logprior: -1.0884e+00
Epoch 8/10
22/22 - 7s - loss: 127.0919 - loglik: -1.2560e+02 - logprior: -1.0565e+00
Epoch 9/10
22/22 - 7s - loss: 125.5415 - loglik: -1.2413e+02 - logprior: -1.0085e+00
Epoch 10/10
22/22 - 7s - loss: 127.0058 - loglik: -1.2564e+02 - logprior: -9.7657e-01
Fitted a model with MAP estimate = -125.7471
Time for alignment: 192.0769
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.5052 - loglik: -4.4649e+02 - logprior: -2.9933e+00
Epoch 2/10
19/19 - 5s - loss: 281.8743 - loglik: -2.8020e+02 - logprior: -1.4309e+00
Epoch 3/10
19/19 - 5s - loss: 213.4558 - loglik: -2.1148e+02 - logprior: -1.8564e+00
Epoch 4/10
19/19 - 5s - loss: 201.0172 - loglik: -1.9866e+02 - logprior: -2.1262e+00
Epoch 5/10
19/19 - 5s - loss: 195.4672 - loglik: -1.9278e+02 - logprior: -2.2772e+00
Epoch 6/10
19/19 - 5s - loss: 193.2256 - loglik: -1.9028e+02 - logprior: -2.4789e+00
Epoch 7/10
19/19 - 5s - loss: 191.6983 - loglik: -1.8884e+02 - logprior: -2.4039e+00
Epoch 8/10
19/19 - 5s - loss: 190.5094 - loglik: -1.8767e+02 - logprior: -2.4091e+00
Epoch 9/10
19/19 - 5s - loss: 191.6613 - loglik: -1.8886e+02 - logprior: -2.4037e+00
Fitted a model with MAP estimate = -184.7563
expansions: [(7, 1), (8, 2), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 1), (61, 1), (62, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (101, 1), (104, 1), (114, 2), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 180.6409 - loglik: -1.7743e+02 - logprior: -2.8988e+00
Epoch 2/2
19/19 - 6s - loss: 144.7937 - loglik: -1.4360e+02 - logprior: -1.0443e+00
Fitted a model with MAP estimate = -143.0735
expansions: []
discards: [ 50 141 152]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 145.7423 - loglik: -1.4278e+02 - logprior: -2.7982e+00
Epoch 2/2
19/19 - 6s - loss: 141.3163 - loglik: -1.4021e+02 - logprior: -8.8965e-01
Fitted a model with MAP estimate = -141.3628
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 143.8770 - loglik: -1.4171e+02 - logprior: -1.9728e+00
Epoch 2/10
22/22 - 7s - loss: 137.7050 - loglik: -1.3664e+02 - logprior: -8.9931e-01
Epoch 3/10
22/22 - 7s - loss: 137.0833 - loglik: -1.3589e+02 - logprior: -9.3930e-01
Epoch 4/10
22/22 - 7s - loss: 135.3893 - loglik: -1.3377e+02 - logprior: -1.1923e+00
Epoch 5/10
22/22 - 7s - loss: 130.5536 - loglik: -1.2888e+02 - logprior: -1.1626e+00
Epoch 6/10
22/22 - 7s - loss: 128.7041 - loglik: -1.2710e+02 - logprior: -1.1126e+00
Epoch 7/10
22/22 - 7s - loss: 126.8942 - loglik: -1.2535e+02 - logprior: -1.0846e+00
Epoch 8/10
22/22 - 7s - loss: 127.4449 - loglik: -1.2597e+02 - logprior: -1.0411e+00
Fitted a model with MAP estimate = -126.1756
Time for alignment: 180.1867
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.1539 - loglik: -4.4615e+02 - logprior: -2.9895e+00
Epoch 2/10
19/19 - 5s - loss: 280.8297 - loglik: -2.7916e+02 - logprior: -1.4417e+00
Epoch 3/10
19/19 - 5s - loss: 212.1083 - loglik: -2.1020e+02 - logprior: -1.8242e+00
Epoch 4/10
19/19 - 5s - loss: 200.1167 - loglik: -1.9784e+02 - logprior: -2.1571e+00
Epoch 5/10
19/19 - 5s - loss: 195.0816 - loglik: -1.9238e+02 - logprior: -2.3618e+00
Epoch 6/10
19/19 - 5s - loss: 192.1656 - loglik: -1.8935e+02 - logprior: -2.4481e+00
Epoch 7/10
19/19 - 5s - loss: 192.1249 - loglik: -1.8937e+02 - logprior: -2.3942e+00
Epoch 8/10
19/19 - 5s - loss: 189.9115 - loglik: -1.8717e+02 - logprior: -2.3973e+00
Epoch 9/10
19/19 - 5s - loss: 190.1292 - loglik: -1.8739e+02 - logprior: -2.3904e+00
Fitted a model with MAP estimate = -184.4074
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (102, 1), (111, 2), (114, 2), (121, 1), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 182.6694 - loglik: -1.7942e+02 - logprior: -2.9453e+00
Epoch 2/2
19/19 - 6s - loss: 144.7901 - loglik: -1.4351e+02 - logprior: -1.1233e+00
Fitted a model with MAP estimate = -142.9637
expansions: []
discards: [ 50  76 138 143 154 159]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 146.4614 - loglik: -1.4350e+02 - logprior: -2.7895e+00
Epoch 2/2
19/19 - 6s - loss: 140.8693 - loglik: -1.3976e+02 - logprior: -8.8767e-01
Fitted a model with MAP estimate = -141.2979
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 142.9776 - loglik: -1.4078e+02 - logprior: -2.0149e+00
Epoch 2/10
22/22 - 7s - loss: 140.2222 - loglik: -1.3918e+02 - logprior: -8.8201e-01
Epoch 3/10
22/22 - 7s - loss: 137.5910 - loglik: -1.3623e+02 - logprior: -1.0964e+00
Epoch 4/10
22/22 - 7s - loss: 132.5259 - loglik: -1.3093e+02 - logprior: -1.1660e+00
Epoch 5/10
22/22 - 7s - loss: 131.9716 - loglik: -1.3030e+02 - logprior: -1.1452e+00
Epoch 6/10
22/22 - 7s - loss: 127.7553 - loglik: -1.2616e+02 - logprior: -1.0989e+00
Epoch 7/10
22/22 - 7s - loss: 126.7094 - loglik: -1.2517e+02 - logprior: -1.0853e+00
Epoch 8/10
22/22 - 7s - loss: 127.3348 - loglik: -1.2586e+02 - logprior: -1.0434e+00
Fitted a model with MAP estimate = -126.1308
Time for alignment: 181.4601
Computed alignments with likelihoods: ['-125.7471', '-126.1756', '-126.1308']
Best model has likelihood: -125.7471
time for generating output: 0.2119
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 3 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19be8ace50>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f182325ee50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f184d312220>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f184d312580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18788135e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f180d3a6520>, <__main__.SimpleDirichletPrior object at 0x7f193dc86d90>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1961ac9280>

Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 609.3304 - loglik: -5.7607e+02 - logprior: -3.3239e+01
Epoch 2/10
12/12 - 4s - loss: 523.7061 - loglik: -5.1920e+02 - logprior: -4.4825e+00
Epoch 3/10
12/12 - 4s - loss: 465.6099 - loglik: -4.6367e+02 - logprior: -1.8374e+00
Epoch 4/10
12/12 - 4s - loss: 435.9517 - loglik: -4.3415e+02 - logprior: -1.4757e+00
Epoch 5/10
12/12 - 4s - loss: 425.8877 - loglik: -4.2418e+02 - logprior: -1.1539e+00
Epoch 6/10
12/12 - 4s - loss: 421.0055 - loglik: -4.1958e+02 - logprior: -8.5233e-01
Epoch 7/10
12/12 - 4s - loss: 423.3692 - loglik: -4.2219e+02 - logprior: -6.4334e-01
Fitted a model with MAP estimate = -419.4632
expansions: [(11, 3), (19, 1), (30, 2), (31, 2), (32, 1), (41, 1), (59, 1), (60, 2), (61, 2), (62, 2), (74, 1), (75, 2), (76, 1), (77, 1), (86, 1), (87, 1), (89, 1), (90, 1), (92, 1), (100, 1), (101, 1), (120, 2), (125, 1), (126, 6), (129, 1), (130, 1), (137, 1), (146, 3), (149, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 455.5660 - loglik: -4.1617e+02 - logprior: -3.8856e+01
Epoch 2/2
12/12 - 4s - loss: 416.1287 - loglik: -4.0265e+02 - logprior: -1.2886e+01
Fitted a model with MAP estimate = -408.9847
expansions: [(0, 3)]
discards: [  0  71 130 149 192 221]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 431.3778 - loglik: -4.0163e+02 - logprior: -2.9142e+01
Epoch 2/2
12/12 - 4s - loss: 401.3751 - loglik: -3.9768e+02 - logprior: -3.1047e+00
Fitted a model with MAP estimate = -396.1048
expansions: []
discards: [  1   2  37 155 156 157]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 429.5005 - loglik: -4.0069e+02 - logprior: -2.8176e+01
Epoch 2/10
12/12 - 4s - loss: 400.9985 - loglik: -3.9791e+02 - logprior: -2.4447e+00
Epoch 3/10
12/12 - 4s - loss: 395.7253 - loglik: -3.9738e+02 - logprior: 2.2871
Epoch 4/10
12/12 - 4s - loss: 392.8883 - loglik: -3.9665e+02 - logprior: 4.3708
Epoch 5/10
12/12 - 4s - loss: 391.0453 - loglik: -3.9593e+02 - logprior: 5.4409
Epoch 6/10
12/12 - 4s - loss: 391.5872 - loglik: -3.9716e+02 - logprior: 6.0860
Fitted a model with MAP estimate = -389.6068
Time for alignment: 96.7284
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 609.0138 - loglik: -5.7576e+02 - logprior: -3.3240e+01
Epoch 2/10
12/12 - 4s - loss: 523.6040 - loglik: -5.1907e+02 - logprior: -4.5066e+00
Epoch 3/10
12/12 - 4s - loss: 464.3665 - loglik: -4.6247e+02 - logprior: -1.7991e+00
Epoch 4/10
12/12 - 4s - loss: 437.8000 - loglik: -4.3635e+02 - logprior: -1.1334e+00
Epoch 5/10
12/12 - 4s - loss: 430.5838 - loglik: -4.2925e+02 - logprior: -7.9171e-01
Epoch 6/10
12/12 - 4s - loss: 424.9889 - loglik: -4.2392e+02 - logprior: -5.2170e-01
Epoch 7/10
12/12 - 4s - loss: 423.0174 - loglik: -4.2200e+02 - logprior: -4.9773e-01
Epoch 8/10
12/12 - 4s - loss: 422.4140 - loglik: -4.2154e+02 - logprior: -3.7241e-01
Epoch 9/10
12/12 - 4s - loss: 421.6964 - loglik: -4.2094e+02 - logprior: -2.7028e-01
Epoch 10/10
12/12 - 4s - loss: 420.3364 - loglik: -4.1961e+02 - logprior: -2.4201e-01
Fitted a model with MAP estimate = -420.1734
expansions: [(11, 3), (21, 1), (30, 1), (31, 3), (42, 1), (45, 1), (48, 1), (60, 1), (61, 1), (62, 1), (63, 1), (77, 2), (78, 2), (87, 1), (91, 1), (93, 1), (102, 1), (103, 2), (121, 1), (127, 1), (129, 1), (131, 2), (132, 1), (138, 2), (139, 2), (146, 3), (148, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 457.4931 - loglik: -4.1806e+02 - logprior: -3.8928e+01
Epoch 2/2
12/12 - 4s - loss: 418.1259 - loglik: -4.0461e+02 - logprior: -1.2936e+01
Fitted a model with MAP estimate = -411.3400
expansions: [(0, 4), (105, 1), (182, 1)]
discards: [  0  36  92 172 185 215]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 433.2642 - loglik: -4.0384e+02 - logprior: -2.8847e+01
Epoch 2/2
12/12 - 4s - loss: 404.6933 - loglik: -4.0147e+02 - logprior: -2.6748e+00
Fitted a model with MAP estimate = -398.6885
expansions: []
discards: [  1   2   3 129 185]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 224 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 432.0206 - loglik: -4.0350e+02 - logprior: -2.7970e+01
Epoch 2/10
12/12 - 4s - loss: 403.8787 - loglik: -4.0096e+02 - logprior: -2.3575e+00
Epoch 3/10
12/12 - 4s - loss: 396.7220 - loglik: -3.9850e+02 - logprior: 2.3543
Epoch 4/10
12/12 - 4s - loss: 396.2319 - loglik: -4.0003e+02 - logprior: 4.3705
Epoch 5/10
12/12 - 4s - loss: 393.8300 - loglik: -3.9876e+02 - logprior: 5.4573
Epoch 6/10
12/12 - 4s - loss: 393.0990 - loglik: -3.9868e+02 - logprior: 6.0742
Epoch 7/10
12/12 - 4s - loss: 392.8358 - loglik: -3.9888e+02 - logprior: 6.5168
Epoch 8/10
12/12 - 4s - loss: 391.8606 - loglik: -3.9831e+02 - logprior: 6.9077
Epoch 9/10
12/12 - 4s - loss: 391.6833 - loglik: -3.9850e+02 - logprior: 7.2648
Epoch 10/10
12/12 - 4s - loss: 390.1021 - loglik: -3.9728e+02 - logprior: 7.6252
Fitted a model with MAP estimate = -390.1119
Time for alignment: 123.0956
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 607.3663 - loglik: -5.7411e+02 - logprior: -3.3238e+01
Epoch 2/10
12/12 - 4s - loss: 524.4528 - loglik: -5.1991e+02 - logprior: -4.5244e+00
Epoch 3/10
12/12 - 4s - loss: 461.6388 - loglik: -4.5948e+02 - logprior: -2.0709e+00
Epoch 4/10
12/12 - 4s - loss: 433.8857 - loglik: -4.3197e+02 - logprior: -1.6338e+00
Epoch 5/10
12/12 - 4s - loss: 427.6143 - loglik: -4.2602e+02 - logprior: -1.0602e+00
Epoch 6/10
12/12 - 4s - loss: 426.5156 - loglik: -4.2517e+02 - logprior: -7.8087e-01
Epoch 7/10
12/12 - 4s - loss: 422.3212 - loglik: -4.2109e+02 - logprior: -6.8307e-01
Epoch 8/10
12/12 - 4s - loss: 422.4946 - loglik: -4.2144e+02 - logprior: -5.3272e-01
Fitted a model with MAP estimate = -421.2123
expansions: [(11, 3), (21, 1), (30, 2), (31, 3), (32, 1), (41, 1), (45, 1), (60, 2), (61, 2), (62, 2), (74, 1), (75, 2), (76, 1), (77, 1), (86, 1), (90, 1), (91, 1), (92, 1), (101, 1), (102, 1), (120, 2), (126, 5), (127, 1), (129, 2), (130, 1), (139, 1), (147, 2), (149, 2), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 458.7389 - loglik: -4.1938e+02 - logprior: -3.8806e+01
Epoch 2/2
12/12 - 4s - loss: 419.4559 - loglik: -4.0605e+02 - logprior: -1.2841e+01
Fitted a model with MAP estimate = -412.5058
expansions: [(0, 4)]
discards: [  0  35  36  72 158 191 221]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 232 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 435.4230 - loglik: -4.0587e+02 - logprior: -2.9013e+01
Epoch 2/2
12/12 - 4s - loss: 405.1396 - loglik: -4.0164e+02 - logprior: -2.9385e+00
Fitted a model with MAP estimate = -400.5872
expansions: []
discards: [  1   2   3 150 156 157 166]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 434.5302 - loglik: -4.0584e+02 - logprior: -2.8111e+01
Epoch 2/10
12/12 - 4s - loss: 405.2301 - loglik: -4.0219e+02 - logprior: -2.4815e+00
Epoch 3/10
12/12 - 4s - loss: 397.9501 - loglik: -3.9955e+02 - logprior: 2.1932
Epoch 4/10
12/12 - 4s - loss: 395.5698 - loglik: -3.9924e+02 - logprior: 4.2497
Epoch 5/10
12/12 - 4s - loss: 393.2458 - loglik: -3.9806e+02 - logprior: 5.3505
Epoch 6/10
12/12 - 4s - loss: 392.5588 - loglik: -3.9803e+02 - logprior: 5.9762
Epoch 7/10
12/12 - 4s - loss: 391.8625 - loglik: -3.9784e+02 - logprior: 6.4591
Epoch 8/10
12/12 - 4s - loss: 391.0059 - loglik: -3.9734e+02 - logprior: 6.8091
Epoch 9/10
12/12 - 4s - loss: 391.2389 - loglik: -3.9798e+02 - logprior: 7.2130
Fitted a model with MAP estimate = -389.9425
Time for alignment: 110.7044
Computed alignments with likelihoods: ['-389.6068', '-390.1119', '-389.9425']
Best model has likelihood: -389.6068
time for generating output: 0.2712
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.9353676770422726
Training of 3 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f18b4469eb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f180dc3f8b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f180ddab430>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f10390d3ca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f184d1ea1c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f103902e2b0>, <__main__.SimpleDirichletPrior object at 0x7f18b4032340>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f193db83a60>

Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 604.4471 - loglik: -5.9676e+02 - logprior: -7.6631e+00
Epoch 2/10
21/21 - 8s - loss: 481.8917 - loglik: -4.7960e+02 - logprior: -2.2291e+00
Epoch 3/10
21/21 - 9s - loss: 444.9219 - loglik: -4.4149e+02 - logprior: -3.0082e+00
Epoch 4/10
21/21 - 8s - loss: 438.3207 - loglik: -4.3507e+02 - logprior: -2.7256e+00
Epoch 5/10
21/21 - 8s - loss: 436.7224 - loglik: -4.3351e+02 - logprior: -2.6934e+00
Epoch 6/10
21/21 - 9s - loss: 433.0775 - loglik: -4.2986e+02 - logprior: -2.7242e+00
Epoch 7/10
21/21 - 8s - loss: 435.2053 - loglik: -4.3195e+02 - logprior: -2.7456e+00
Fitted a model with MAP estimate = -432.9045
expansions: [(17, 1), (18, 1), (19, 1), (20, 3), (21, 2), (22, 1), (36, 1), (37, 1), (39, 2), (40, 1), (50, 1), (60, 1), (63, 3), (66, 1), (76, 7), (78, 1), (97, 1), (100, 2), (102, 1), (115, 1), (116, 1), (119, 1), (121, 1), (122, 1), (147, 1), (148, 2), (159, 1), (161, 1), (162, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 427.4514 - loglik: -4.1949e+02 - logprior: -7.4865e+00
Epoch 2/2
21/21 - 10s - loss: 404.3848 - loglik: -4.0268e+02 - logprior: -1.1784e+00
Fitted a model with MAP estimate = -398.6104
expansions: [(97, 1)]
discards: [ 24  81 131 186 217]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 16s - loss: 412.7273 - loglik: -4.0507e+02 - logprior: -7.0900e+00
Epoch 2/2
21/21 - 10s - loss: 400.1840 - loglik: -3.9899e+02 - logprior: -5.8830e-01
Fitted a model with MAP estimate = -398.4597
expansions: []
discards: [83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 14s - loss: 410.8068 - loglik: -4.0333e+02 - logprior: -6.8471e+00
Epoch 2/10
21/21 - 10s - loss: 399.9503 - loglik: -3.9907e+02 - logprior: -2.6853e-01
Epoch 3/10
21/21 - 10s - loss: 397.9578 - loglik: -3.9783e+02 - logprior: 0.4911
Epoch 4/10
21/21 - 11s - loss: 397.5302 - loglik: -3.9777e+02 - logprior: 0.8362
Epoch 5/10
21/21 - 10s - loss: 395.0011 - loglik: -3.9540e+02 - logprior: 0.9903
Epoch 6/10
21/21 - 10s - loss: 395.0602 - loglik: -3.9558e+02 - logprior: 1.0988
Fitted a model with MAP estimate = -393.1609
Time for alignment: 210.7733
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 14s - loss: 605.1080 - loglik: -5.9742e+02 - logprior: -7.6663e+00
Epoch 2/10
21/21 - 8s - loss: 482.4899 - loglik: -4.8030e+02 - logprior: -2.1362e+00
Epoch 3/10
21/21 - 8s - loss: 444.7021 - loglik: -4.4147e+02 - logprior: -2.6812e+00
Epoch 4/10
21/21 - 8s - loss: 437.6768 - loglik: -4.3452e+02 - logprior: -2.5860e+00
Epoch 5/10
21/21 - 8s - loss: 435.0650 - loglik: -4.3198e+02 - logprior: -2.5692e+00
Epoch 6/10
21/21 - 8s - loss: 432.2161 - loglik: -4.2907e+02 - logprior: -2.6674e+00
Epoch 7/10
21/21 - 9s - loss: 433.7830 - loglik: -4.3055e+02 - logprior: -2.7251e+00
Fitted a model with MAP estimate = -431.8425
expansions: [(16, 2), (19, 1), (20, 2), (21, 3), (22, 1), (35, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (64, 4), (66, 1), (76, 7), (78, 1), (97, 1), (100, 2), (102, 1), (115, 1), (119, 1), (120, 1), (121, 1), (122, 1), (144, 1), (146, 3), (147, 1), (158, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (167, 1), (169, 3), (171, 1), (178, 1), (179, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 424.0431 - loglik: -4.1621e+02 - logprior: -7.3397e+00
Epoch 2/2
21/21 - 10s - loss: 396.5164 - loglik: -3.9498e+02 - logprior: -1.0256e+00
Fitted a model with MAP estimate = -393.7473
expansions: [(97, 1)]
discards: [ 27  81 131 147 180 181 218]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 409.5465 - loglik: -4.0191e+02 - logprior: -7.0845e+00
Epoch 2/2
21/21 - 10s - loss: 397.9186 - loglik: -3.9677e+02 - logprior: -5.5328e-01
Fitted a model with MAP estimate = -395.6688
expansions: []
discards: [16 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 15s - loss: 407.3989 - loglik: -3.9989e+02 - logprior: -6.8642e+00
Epoch 2/10
21/21 - 10s - loss: 398.7745 - loglik: -3.9787e+02 - logprior: -2.5634e-01
Epoch 3/10
21/21 - 9s - loss: 395.5683 - loglik: -3.9538e+02 - logprior: 0.4770
Epoch 4/10
21/21 - 10s - loss: 393.9555 - loglik: -3.9418e+02 - logprior: 0.8469
Epoch 5/10
21/21 - 10s - loss: 394.1142 - loglik: -3.9450e+02 - logprior: 0.9801
Fitted a model with MAP estimate = -391.7395
Time for alignment: 199.9507
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 604.3978 - loglik: -5.9673e+02 - logprior: -7.6388e+00
Epoch 2/10
21/21 - 8s - loss: 480.9079 - loglik: -4.7858e+02 - logprior: -2.2744e+00
Epoch 3/10
21/21 - 8s - loss: 438.4216 - loglik: -4.3461e+02 - logprior: -3.2480e+00
Epoch 4/10
21/21 - 8s - loss: 429.5568 - loglik: -4.2580e+02 - logprior: -3.1938e+00
Epoch 5/10
21/21 - 8s - loss: 428.1854 - loglik: -4.2456e+02 - logprior: -3.1174e+00
Epoch 6/10
21/21 - 9s - loss: 426.7699 - loglik: -4.2315e+02 - logprior: -3.1183e+00
Epoch 7/10
21/21 - 8s - loss: 426.5770 - loglik: -4.2296e+02 - logprior: -3.1037e+00
Epoch 8/10
21/21 - 9s - loss: 425.7911 - loglik: -4.2217e+02 - logprior: -3.1099e+00
Epoch 9/10
21/21 - 8s - loss: 425.9695 - loglik: -4.2236e+02 - logprior: -3.0991e+00
Fitted a model with MAP estimate = -424.7891
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (35, 1), (37, 1), (38, 1), (39, 1), (40, 1), (46, 1), (49, 1), (59, 1), (62, 3), (75, 6), (76, 1), (78, 2), (81, 2), (96, 1), (99, 1), (115, 1), (116, 1), (119, 1), (121, 4), (143, 1), (146, 1), (147, 1), (150, 1), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 245 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 424.7630 - loglik: -4.1372e+02 - logprior: -1.0538e+01
Epoch 2/2
21/21 - 11s - loss: 398.7171 - loglik: -3.9412e+02 - logprior: -4.0599e+00
Fitted a model with MAP estimate = -392.5862
expansions: [(0, 5)]
discards: [  0  18  28  82  83  97 111 159 221]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 402.9718 - loglik: -3.9517e+02 - logprior: -7.1964e+00
Epoch 2/2
21/21 - 11s - loss: 389.2998 - loglik: -3.8796e+02 - logprior: -7.0349e-01
Fitted a model with MAP estimate = -387.0492
expansions: []
discards: [1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 400.6082 - loglik: -3.9326e+02 - logprior: -6.6837e+00
Epoch 2/10
21/21 - 10s - loss: 387.4044 - loglik: -3.8674e+02 - logprior: 0.0139
Epoch 3/10
21/21 - 10s - loss: 386.8610 - loglik: -3.8689e+02 - logprior: 0.7205
Epoch 4/10
21/21 - 10s - loss: 385.3419 - loglik: -3.8578e+02 - logprior: 1.0982
Epoch 5/10
21/21 - 10s - loss: 384.3105 - loglik: -3.8494e+02 - logprior: 1.2492
Epoch 6/10
21/21 - 10s - loss: 383.0848 - loglik: -3.8376e+02 - logprior: 1.2873
Epoch 7/10
21/21 - 10s - loss: 381.5920 - loglik: -3.8241e+02 - logprior: 1.4305
Epoch 8/10
21/21 - 10s - loss: 380.9626 - loglik: -3.8191e+02 - logprior: 1.5731
Epoch 9/10
21/21 - 10s - loss: 381.0351 - loglik: -3.8219e+02 - logprior: 1.7803
Fitted a model with MAP estimate = -379.9997
Time for alignment: 256.7462
Computed alignments with likelihoods: ['-393.1609', '-391.7395', '-379.9997']
Best model has likelihood: -379.9997
time for generating output: 0.4906
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.6187158727170025
Training of 3 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19ae757490>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19ae0cab50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fdbe8340>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f166c4b1bb0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f166c4b16a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1856817490>, <__main__.SimpleDirichletPrior object at 0x7f199d0988e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18b3939dc0>

Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.7032 - loglik: -2.8053e+02 - logprior: -3.0582e+00
Epoch 2/10
19/19 - 3s - loss: 254.0995 - loglik: -2.5249e+02 - logprior: -9.3490e-01
Epoch 3/10
19/19 - 3s - loss: 242.9894 - loglik: -2.4108e+02 - logprior: -9.9149e-01
Epoch 4/10
19/19 - 3s - loss: 239.3582 - loglik: -2.3732e+02 - logprior: -9.3828e-01
Epoch 5/10
19/19 - 3s - loss: 237.6877 - loglik: -2.3582e+02 - logprior: -9.1651e-01
Epoch 6/10
19/19 - 3s - loss: 236.9031 - loglik: -2.3517e+02 - logprior: -9.1195e-01
Epoch 7/10
19/19 - 3s - loss: 235.6490 - loglik: -2.3400e+02 - logprior: -9.0629e-01
Epoch 8/10
19/19 - 3s - loss: 235.6453 - loglik: -2.3401e+02 - logprior: -8.9128e-01
Epoch 9/10
19/19 - 3s - loss: 233.8917 - loglik: -2.3219e+02 - logprior: -8.9126e-01
Epoch 10/10
19/19 - 3s - loss: 233.7381 - loglik: -2.3197e+02 - logprior: -8.9020e-01
Fitted a model with MAP estimate = -231.5673
expansions: [(0, 12), (12, 1), (16, 2), (23, 1), (36, 1), (57, 4), (59, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 253.0015 - loglik: -2.4776e+02 - logprior: -4.0674e+00
Epoch 2/2
19/19 - 4s - loss: 237.0383 - loglik: -2.3394e+02 - logprior: -1.3547e+00
Fitted a model with MAP estimate = -231.4292
expansions: [(0, 5)]
discards: [77 78 82]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 239.2964 - loglik: -2.3337e+02 - logprior: -3.8843e+00
Epoch 2/2
19/19 - 4s - loss: 234.1154 - loglik: -2.3081e+02 - logprior: -1.5435e+00
Fitted a model with MAP estimate = -230.1172
expansions: [(0, 4), (33, 2), (34, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 238.1742 - loglik: -2.3292e+02 - logprior: -3.4530e+00
Epoch 2/10
19/19 - 3s - loss: 233.1940 - loglik: -2.3049e+02 - logprior: -1.1133e+00
Epoch 3/10
19/19 - 4s - loss: 231.4950 - loglik: -2.2897e+02 - logprior: -1.0896e+00
Epoch 4/10
19/19 - 4s - loss: 230.5399 - loglik: -2.2822e+02 - logprior: -9.7334e-01
Epoch 5/10
19/19 - 4s - loss: 228.8866 - loglik: -2.2677e+02 - logprior: -9.1263e-01
Epoch 6/10
19/19 - 4s - loss: 228.5713 - loglik: -2.2660e+02 - logprior: -8.7119e-01
Epoch 7/10
19/19 - 3s - loss: 227.8489 - loglik: -2.2599e+02 - logprior: -8.5608e-01
Epoch 8/10
19/19 - 4s - loss: 227.5906 - loglik: -2.2580e+02 - logprior: -8.2843e-01
Epoch 9/10
19/19 - 4s - loss: 226.8151 - loglik: -2.2502e+02 - logprior: -8.2807e-01
Epoch 10/10
19/19 - 3s - loss: 225.8996 - loglik: -2.2410e+02 - logprior: -8.1796e-01
Fitted a model with MAP estimate = -224.6570
Time for alignment: 119.3463
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.3586 - loglik: -2.8018e+02 - logprior: -3.0685e+00
Epoch 2/10
19/19 - 3s - loss: 252.8262 - loglik: -2.5124e+02 - logprior: -9.2780e-01
Epoch 3/10
19/19 - 3s - loss: 243.5561 - loglik: -2.4175e+02 - logprior: -9.6613e-01
Epoch 4/10
19/19 - 3s - loss: 241.1111 - loglik: -2.3960e+02 - logprior: -8.8819e-01
Epoch 5/10
19/19 - 3s - loss: 238.5817 - loglik: -2.3712e+02 - logprior: -9.1562e-01
Epoch 6/10
19/19 - 3s - loss: 237.2345 - loglik: -2.3556e+02 - logprior: -9.3388e-01
Epoch 7/10
19/19 - 3s - loss: 236.7474 - loglik: -2.3513e+02 - logprior: -9.2272e-01
Epoch 8/10
19/19 - 3s - loss: 235.5184 - loglik: -2.3392e+02 - logprior: -9.1025e-01
Epoch 9/10
19/19 - 3s - loss: 234.8069 - loglik: -2.3318e+02 - logprior: -9.1331e-01
Epoch 10/10
19/19 - 3s - loss: 234.2472 - loglik: -2.3258e+02 - logprior: -9.0937e-01
Fitted a model with MAP estimate = -232.3409
expansions: [(0, 11), (12, 1), (16, 1), (17, 1), (18, 2), (35, 1), (57, 3), (59, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 250.6201 - loglik: -2.4553e+02 - logprior: -4.0264e+00
Epoch 2/2
19/19 - 4s - loss: 237.0658 - loglik: -2.3416e+02 - logprior: -1.3530e+00
Fitted a model with MAP estimate = -232.0503
expansions: [(0, 5)]
discards: [34 76 77 81]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 240.6078 - loglik: -2.3484e+02 - logprior: -3.9226e+00
Epoch 2/2
19/19 - 4s - loss: 234.7723 - loglik: -2.3148e+02 - logprior: -1.5706e+00
Fitted a model with MAP estimate = -231.0654
expansions: [(0, 4)]
discards: [0 1 2 3 4 5 6 7 8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 238.6279 - loglik: -2.3331e+02 - logprior: -3.4694e+00
Epoch 2/10
19/19 - 3s - loss: 233.8273 - loglik: -2.3086e+02 - logprior: -1.2487e+00
Epoch 3/10
19/19 - 4s - loss: 232.8676 - loglik: -2.3025e+02 - logprior: -1.0962e+00
Epoch 4/10
19/19 - 4s - loss: 230.6390 - loglik: -2.2826e+02 - logprior: -9.9568e-01
Epoch 5/10
19/19 - 4s - loss: 230.0028 - loglik: -2.2783e+02 - logprior: -9.4205e-01
Epoch 6/10
19/19 - 3s - loss: 229.3268 - loglik: -2.2729e+02 - logprior: -9.1374e-01
Epoch 7/10
19/19 - 4s - loss: 228.9824 - loglik: -2.2705e+02 - logprior: -8.9165e-01
Epoch 8/10
19/19 - 4s - loss: 227.7468 - loglik: -2.2585e+02 - logprior: -8.8132e-01
Epoch 9/10
19/19 - 4s - loss: 227.0829 - loglik: -2.2522e+02 - logprior: -8.6028e-01
Epoch 10/10
19/19 - 4s - loss: 227.1098 - loglik: -2.2527e+02 - logprior: -8.4305e-01
Fitted a model with MAP estimate = -225.2578
Time for alignment: 119.5239
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 283.8193 - loglik: -2.8063e+02 - logprior: -3.0694e+00
Epoch 2/10
19/19 - 3s - loss: 253.2247 - loglik: -2.5164e+02 - logprior: -9.2951e-01
Epoch 3/10
19/19 - 3s - loss: 242.2845 - loglik: -2.4048e+02 - logprior: -9.6866e-01
Epoch 4/10
19/19 - 3s - loss: 238.5332 - loglik: -2.3665e+02 - logprior: -9.3756e-01
Epoch 5/10
19/19 - 3s - loss: 236.7147 - loglik: -2.3483e+02 - logprior: -9.3128e-01
Epoch 6/10
19/19 - 3s - loss: 235.8857 - loglik: -2.3415e+02 - logprior: -9.2498e-01
Epoch 7/10
19/19 - 3s - loss: 235.2771 - loglik: -2.3365e+02 - logprior: -9.0067e-01
Epoch 8/10
19/19 - 3s - loss: 235.0473 - loglik: -2.3346e+02 - logprior: -8.9435e-01
Epoch 9/10
19/19 - 3s - loss: 234.0977 - loglik: -2.3251e+02 - logprior: -8.9485e-01
Epoch 10/10
19/19 - 3s - loss: 233.4927 - loglik: -2.3185e+02 - logprior: -8.9484e-01
Fitted a model with MAP estimate = -231.4543
expansions: [(0, 12), (12, 1), (15, 4), (18, 1), (54, 1), (57, 4), (59, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 251.4740 - loglik: -2.4636e+02 - logprior: -4.0666e+00
Epoch 2/2
19/19 - 4s - loss: 235.8999 - loglik: -2.3291e+02 - logprior: -1.3953e+00
Fitted a model with MAP estimate = -230.6398
expansions: [(0, 5), (29, 1), (30, 1)]
discards: [ 0  1  2  3 78 79 80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 238.2982 - loglik: -2.3310e+02 - logprior: -3.3221e+00
Epoch 2/2
19/19 - 4s - loss: 233.1888 - loglik: -2.3027e+02 - logprior: -1.2588e+00
Fitted a model with MAP estimate = -229.4962
expansions: [(0, 5)]
discards: [0 1 2 3 4 5 6 7 8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 237.2118 - loglik: -2.3239e+02 - logprior: -3.0847e+00
Epoch 2/10
19/19 - 4s - loss: 233.1774 - loglik: -2.3048e+02 - logprior: -1.1426e+00
Epoch 3/10
19/19 - 4s - loss: 230.8317 - loglik: -2.2839e+02 - logprior: -1.0133e+00
Epoch 4/10
19/19 - 4s - loss: 229.9713 - loglik: -2.2771e+02 - logprior: -9.3086e-01
Epoch 5/10
19/19 - 4s - loss: 229.5978 - loglik: -2.2752e+02 - logprior: -8.8526e-01
Epoch 6/10
19/19 - 4s - loss: 227.7886 - loglik: -2.2584e+02 - logprior: -8.7262e-01
Epoch 7/10
19/19 - 4s - loss: 228.8167 - loglik: -2.2697e+02 - logprior: -8.4343e-01
Fitted a model with MAP estimate = -226.4605
Time for alignment: 111.5573
Computed alignments with likelihoods: ['-224.6570', '-225.2578', '-226.4605']
Best model has likelihood: -224.6570
time for generating output: 0.2618
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.52
Training of 3 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f180dae0070>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f181a419ac0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f181a419e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1817bfbf40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1817bfb2e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f185ef843d0>, <__main__.SimpleDirichletPrior object at 0x7f15ec73f9d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1934b14af0>

Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 733.3306 - loglik: -7.2075e+02 - logprior: -1.2561e+01
Epoch 2/10
17/17 - 8s - loss: 554.5247 - loglik: -5.5215e+02 - logprior: -2.3502e+00
Epoch 3/10
17/17 - 8s - loss: 457.6404 - loglik: -4.5368e+02 - logprior: -3.9399e+00
Epoch 4/10
17/17 - 8s - loss: 435.9464 - loglik: -4.3089e+02 - logprior: -4.7865e+00
Epoch 5/10
17/17 - 8s - loss: 428.7533 - loglik: -4.2359e+02 - logprior: -4.6070e+00
Epoch 6/10
17/17 - 8s - loss: 429.0765 - loglik: -4.2417e+02 - logprior: -4.3566e+00
Fitted a model with MAP estimate = -425.8478
expansions: [(8, 1), (9, 1), (11, 1), (13, 1), (15, 1), (24, 1), (33, 2), (44, 1), (55, 1), (56, 1), (58, 2), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (98, 1), (100, 1), (114, 1), (115, 1), (116, 2), (117, 1), (129, 2), (131, 1), (142, 1), (156, 1), (160, 1), (161, 1), (162, 1), (163, 1), (176, 1), (178, 1), (180, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 430.1076 - loglik: -4.1271e+02 - logprior: -1.6931e+01
Epoch 2/2
17/17 - 10s - loss: 391.1477 - loglik: -3.8506e+02 - logprior: -5.6265e+00
Fitted a model with MAP estimate = -386.1314
expansions: [(0, 10)]
discards: [  0 139 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 16s - loss: 398.2920 - loglik: -3.8589e+02 - logprior: -1.1939e+01
Epoch 2/2
17/17 - 10s - loss: 380.4138 - loglik: -3.7911e+02 - logprior: -8.5418e-01
Fitted a model with MAP estimate = -378.5426
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 14s - loss: 394.7109 - loglik: -3.8308e+02 - logprior: -1.1133e+01
Epoch 2/10
17/17 - 10s - loss: 380.6708 - loglik: -3.8021e+02 - logprior: 0.0236
Epoch 3/10
17/17 - 10s - loss: 377.0367 - loglik: -3.7803e+02 - logprior: 1.5406
Epoch 4/10
17/17 - 10s - loss: 374.9752 - loglik: -3.7670e+02 - logprior: 2.2938
Epoch 5/10
17/17 - 10s - loss: 371.8209 - loglik: -3.7393e+02 - logprior: 2.6720
Epoch 6/10
17/17 - 10s - loss: 372.0430 - loglik: -3.7446e+02 - logprior: 2.9566
Fitted a model with MAP estimate = -370.1470
Time for alignment: 192.1320
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 732.0862 - loglik: -7.1951e+02 - logprior: -1.2561e+01
Epoch 2/10
17/17 - 8s - loss: 549.1292 - loglik: -5.4672e+02 - logprior: -2.3788e+00
Epoch 3/10
17/17 - 8s - loss: 452.0970 - loglik: -4.4789e+02 - logprior: -4.1361e+00
Epoch 4/10
17/17 - 8s - loss: 432.2724 - loglik: -4.2705e+02 - logprior: -4.7407e+00
Epoch 5/10
17/17 - 8s - loss: 429.3734 - loglik: -4.2444e+02 - logprior: -4.3211e+00
Epoch 6/10
17/17 - 8s - loss: 425.3421 - loglik: -4.2067e+02 - logprior: -4.1241e+00
Epoch 7/10
17/17 - 8s - loss: 425.7764 - loglik: -4.2116e+02 - logprior: -4.1293e+00
Fitted a model with MAP estimate = -424.2727
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (45, 1), (54, 1), (56, 1), (58, 1), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 3), (129, 2), (131, 1), (140, 1), (141, 1), (160, 1), (161, 1), (163, 1), (177, 1), (179, 1), (181, 2), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 429.3413 - loglik: -4.1190e+02 - logprior: -1.6930e+01
Epoch 2/2
17/17 - 10s - loss: 391.6758 - loglik: -3.8572e+02 - logprior: -5.5469e+00
Fitted a model with MAP estimate = -386.2449
expansions: [(0, 10)]
discards: [  0 140 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 398.6140 - loglik: -3.8625e+02 - logprior: -1.1927e+01
Epoch 2/2
17/17 - 10s - loss: 381.6394 - loglik: -3.8041e+02 - logprior: -8.0435e-01
Fitted a model with MAP estimate = -378.5444
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 15s - loss: 394.1347 - loglik: -3.8258e+02 - logprior: -1.1092e+01
Epoch 2/10
17/17 - 10s - loss: 381.1260 - loglik: -3.8073e+02 - logprior: 0.0763
Epoch 3/10
17/17 - 10s - loss: 377.2255 - loglik: -3.7832e+02 - logprior: 1.6198
Epoch 4/10
17/17 - 10s - loss: 375.8051 - loglik: -3.7762e+02 - logprior: 2.3684
Epoch 5/10
17/17 - 10s - loss: 373.4077 - loglik: -3.7561e+02 - logprior: 2.7486
Epoch 6/10
17/17 - 10s - loss: 368.5110 - loglik: -3.7096e+02 - logprior: 2.9807
Epoch 7/10
17/17 - 10s - loss: 372.3585 - loglik: -3.7508e+02 - logprior: 3.2442
Fitted a model with MAP estimate = -369.2139
Time for alignment: 211.8624
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 733.7176 - loglik: -7.2115e+02 - logprior: -1.2548e+01
Epoch 2/10
17/17 - 8s - loss: 553.8433 - loglik: -5.5141e+02 - logprior: -2.4095e+00
Epoch 3/10
17/17 - 8s - loss: 456.7876 - loglik: -4.5253e+02 - logprior: -4.1402e+00
Epoch 4/10
17/17 - 8s - loss: 431.9417 - loglik: -4.2655e+02 - logprior: -4.8706e+00
Epoch 5/10
17/17 - 8s - loss: 428.2621 - loglik: -4.2315e+02 - logprior: -4.5610e+00
Epoch 6/10
17/17 - 8s - loss: 425.4248 - loglik: -4.2059e+02 - logprior: -4.3301e+00
Epoch 7/10
17/17 - 8s - loss: 423.2330 - loglik: -4.1847e+02 - logprior: -4.3006e+00
Epoch 8/10
17/17 - 8s - loss: 422.9508 - loglik: -4.1814e+02 - logprior: -4.3348e+00
Epoch 9/10
17/17 - 8s - loss: 423.9901 - loglik: -4.1913e+02 - logprior: -4.3421e+00
Fitted a model with MAP estimate = -422.0300
expansions: [(9, 1), (11, 1), (12, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 2), (45, 1), (46, 1), (53, 1), (55, 1), (57, 1), (59, 1), (60, 1), (62, 1), (85, 1), (86, 1), (94, 2), (96, 1), (99, 1), (113, 1), (114, 1), (115, 1), (122, 1), (128, 1), (130, 1), (141, 1), (155, 1), (160, 1), (162, 1), (163, 1), (176, 1), (179, 1), (181, 1), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 429.3896 - loglik: -4.1209e+02 - logprior: -1.6858e+01
Epoch 2/2
17/17 - 10s - loss: 391.8121 - loglik: -3.8607e+02 - logprior: -5.3217e+00
Fitted a model with MAP estimate = -386.4835
expansions: [(0, 10)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 15s - loss: 398.7088 - loglik: -3.8629e+02 - logprior: -1.2005e+01
Epoch 2/2
17/17 - 10s - loss: 381.5968 - loglik: -3.8041e+02 - logprior: -7.6619e-01
Fitted a model with MAP estimate = -378.7523
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 13s - loss: 394.1363 - loglik: -3.8266e+02 - logprior: -1.1021e+01
Epoch 2/10
17/17 - 10s - loss: 381.5535 - loglik: -3.8119e+02 - logprior: 0.1185
Epoch 3/10
17/17 - 10s - loss: 377.8463 - loglik: -3.7898e+02 - logprior: 1.6670
Epoch 4/10
17/17 - 10s - loss: 375.3079 - loglik: -3.7714e+02 - logprior: 2.3961
Epoch 5/10
17/17 - 10s - loss: 372.3275 - loglik: -3.7455e+02 - logprior: 2.7755
Epoch 6/10
17/17 - 10s - loss: 371.5696 - loglik: -3.7409e+02 - logprior: 3.0491
Epoch 7/10
17/17 - 10s - loss: 373.2940 - loglik: -3.7606e+02 - logprior: 3.2868
Fitted a model with MAP estimate = -369.4088
Time for alignment: 224.6586
Computed alignments with likelihoods: ['-370.1470', '-369.2139', '-369.4088']
Best model has likelihood: -369.2139
time for generating output: 0.3376
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.9498276522405209
Training of 3 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f193d36fbb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f180da8d970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f186f32ed30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18b3b34e50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18b3b34340>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fd985580>, <__main__.SimpleDirichletPrior object at 0x7f183d0a1d00>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1877aec820>

Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.6165 - loglik: -1.5946e+02 - logprior: -2.0141e+01
Epoch 2/10
10/10 - 1s - loss: 148.4449 - loglik: -1.4259e+02 - logprior: -5.8312e+00
Epoch 3/10
10/10 - 1s - loss: 130.3837 - loglik: -1.2698e+02 - logprior: -3.3972e+00
Epoch 4/10
10/10 - 1s - loss: 118.3004 - loglik: -1.1540e+02 - logprior: -2.8910e+00
Epoch 5/10
10/10 - 1s - loss: 113.6854 - loglik: -1.1078e+02 - logprior: -2.8529e+00
Epoch 6/10
10/10 - 1s - loss: 111.6417 - loglik: -1.0859e+02 - logprior: -2.8639e+00
Epoch 7/10
10/10 - 1s - loss: 110.9295 - loglik: -1.0781e+02 - logprior: -2.7571e+00
Epoch 8/10
10/10 - 1s - loss: 110.2143 - loglik: -1.0727e+02 - logprior: -2.5974e+00
Epoch 9/10
10/10 - 1s - loss: 110.0688 - loglik: -1.0728e+02 - logprior: -2.5094e+00
Epoch 10/10
10/10 - 1s - loss: 109.8083 - loglik: -1.0703e+02 - logprior: -2.4948e+00
Fitted a model with MAP estimate = -109.5132
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 135.5213 - loglik: -1.1258e+02 - logprior: -2.2659e+01
Epoch 2/2
10/10 - 1s - loss: 115.8990 - loglik: -1.0583e+02 - logprior: -9.8399e+00
Fitted a model with MAP estimate = -112.3925
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.6890 - loglik: -1.0344e+02 - logprior: -1.8007e+01
Epoch 2/2
10/10 - 1s - loss: 107.5877 - loglik: -1.0220e+02 - logprior: -5.1091e+00
Fitted a model with MAP estimate = -105.3664
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.9754 - loglik: -1.0423e+02 - logprior: -2.0460e+01
Epoch 2/10
10/10 - 1s - loss: 109.5503 - loglik: -1.0320e+02 - logprior: -6.0200e+00
Epoch 3/10
10/10 - 1s - loss: 106.3583 - loglik: -1.0282e+02 - logprior: -3.1536e+00
Epoch 4/10
10/10 - 1s - loss: 105.0180 - loglik: -1.0229e+02 - logprior: -2.3199e+00
Epoch 5/10
10/10 - 1s - loss: 104.2037 - loglik: -1.0207e+02 - logprior: -1.7284e+00
Epoch 6/10
10/10 - 1s - loss: 103.8083 - loglik: -1.0196e+02 - logprior: -1.4686e+00
Epoch 7/10
10/10 - 1s - loss: 103.5806 - loglik: -1.0185e+02 - logprior: -1.3571e+00
Epoch 8/10
10/10 - 1s - loss: 103.4519 - loglik: -1.0187e+02 - logprior: -1.2070e+00
Epoch 9/10
10/10 - 1s - loss: 103.2321 - loglik: -1.0170e+02 - logprior: -1.1456e+00
Epoch 10/10
10/10 - 1s - loss: 103.1392 - loglik: -1.0164e+02 - logprior: -1.1188e+00
Fitted a model with MAP estimate = -102.6643
Time for alignment: 36.0353
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 179.5638 - loglik: -1.5940e+02 - logprior: -2.0141e+01
Epoch 2/10
10/10 - 1s - loss: 148.1909 - loglik: -1.4234e+02 - logprior: -5.8322e+00
Epoch 3/10
10/10 - 1s - loss: 131.0055 - loglik: -1.2762e+02 - logprior: -3.3820e+00
Epoch 4/10
10/10 - 1s - loss: 118.7917 - loglik: -1.1588e+02 - logprior: -2.8820e+00
Epoch 5/10
10/10 - 1s - loss: 112.9824 - loglik: -1.0988e+02 - logprior: -2.8776e+00
Epoch 6/10
10/10 - 1s - loss: 111.7888 - loglik: -1.0856e+02 - logprior: -2.8254e+00
Epoch 7/10
10/10 - 1s - loss: 110.8360 - loglik: -1.0779e+02 - logprior: -2.7050e+00
Epoch 8/10
10/10 - 1s - loss: 109.9081 - loglik: -1.0702e+02 - logprior: -2.5949e+00
Epoch 9/10
10/10 - 1s - loss: 109.7536 - loglik: -1.0691e+02 - logprior: -2.5321e+00
Epoch 10/10
10/10 - 1s - loss: 109.3902 - loglik: -1.0657e+02 - logprior: -2.5023e+00
Fitted a model with MAP estimate = -109.1029
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 135.2985 - loglik: -1.1229e+02 - logprior: -2.2687e+01
Epoch 2/2
10/10 - 1s - loss: 115.8981 - loglik: -1.0578e+02 - logprior: -9.8458e+00
Fitted a model with MAP estimate = -112.2994
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.7155 - loglik: -1.0343e+02 - logprior: -1.8012e+01
Epoch 2/2
10/10 - 1s - loss: 107.6961 - loglik: -1.0230e+02 - logprior: -5.1033e+00
Fitted a model with MAP estimate = -105.3866
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.7482 - loglik: -1.0398e+02 - logprior: -2.0472e+01
Epoch 2/10
10/10 - 1s - loss: 110.0832 - loglik: -1.0373e+02 - logprior: -6.0272e+00
Epoch 3/10
10/10 - 1s - loss: 106.1830 - loglik: -1.0265e+02 - logprior: -3.1513e+00
Epoch 4/10
10/10 - 1s - loss: 104.9898 - loglik: -1.0226e+02 - logprior: -2.3212e+00
Epoch 5/10
10/10 - 1s - loss: 104.2184 - loglik: -1.0209e+02 - logprior: -1.7314e+00
Epoch 6/10
10/10 - 1s - loss: 103.7312 - loglik: -1.0188e+02 - logprior: -1.4650e+00
Epoch 7/10
10/10 - 1s - loss: 103.7518 - loglik: -1.0203e+02 - logprior: -1.3507e+00
Fitted a model with MAP estimate = -103.0755
Time for alignment: 33.2330
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.6237 - loglik: -1.5946e+02 - logprior: -2.0141e+01
Epoch 2/10
10/10 - 1s - loss: 148.2280 - loglik: -1.4237e+02 - logprior: -5.8365e+00
Epoch 3/10
10/10 - 1s - loss: 130.0710 - loglik: -1.2665e+02 - logprior: -3.4209e+00
Epoch 4/10
10/10 - 1s - loss: 118.3013 - loglik: -1.1532e+02 - logprior: -2.9469e+00
Epoch 5/10
10/10 - 1s - loss: 113.1878 - loglik: -1.1005e+02 - logprior: -2.9490e+00
Epoch 6/10
10/10 - 1s - loss: 111.6295 - loglik: -1.0832e+02 - logprior: -2.9266e+00
Epoch 7/10
10/10 - 1s - loss: 110.5603 - loglik: -1.0738e+02 - logprior: -2.7940e+00
Epoch 8/10
10/10 - 1s - loss: 110.4338 - loglik: -1.0744e+02 - logprior: -2.6357e+00
Epoch 9/10
10/10 - 1s - loss: 109.9594 - loglik: -1.0704e+02 - logprior: -2.5533e+00
Epoch 10/10
10/10 - 1s - loss: 110.0325 - loglik: -1.0716e+02 - logprior: -2.5336e+00
Fitted a model with MAP estimate = -109.3826
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 135.6349 - loglik: -1.1261e+02 - logprior: -2.2695e+01
Epoch 2/2
10/10 - 1s - loss: 115.8684 - loglik: -1.0571e+02 - logprior: -9.8612e+00
Fitted a model with MAP estimate = -112.2716
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 121.4849 - loglik: -1.0318e+02 - logprior: -1.8016e+01
Epoch 2/2
10/10 - 1s - loss: 107.8997 - loglik: -1.0247e+02 - logprior: -5.1154e+00
Fitted a model with MAP estimate = -105.3369
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 124.8495 - loglik: -1.0405e+02 - logprior: -2.0469e+01
Epoch 2/10
10/10 - 1s - loss: 109.7715 - loglik: -1.0342e+02 - logprior: -6.0239e+00
Epoch 3/10
10/10 - 1s - loss: 106.3503 - loglik: -1.0280e+02 - logprior: -3.1538e+00
Epoch 4/10
10/10 - 1s - loss: 104.9130 - loglik: -1.0217e+02 - logprior: -2.3260e+00
Epoch 5/10
10/10 - 1s - loss: 104.0542 - loglik: -1.0191e+02 - logprior: -1.7371e+00
Epoch 6/10
10/10 - 1s - loss: 103.9891 - loglik: -1.0213e+02 - logprior: -1.4701e+00
Epoch 7/10
10/10 - 1s - loss: 103.4757 - loglik: -1.0175e+02 - logprior: -1.3489e+00
Epoch 8/10
10/10 - 1s - loss: 103.3536 - loglik: -1.0177e+02 - logprior: -1.2031e+00
Epoch 9/10
10/10 - 1s - loss: 103.2841 - loglik: -1.0176e+02 - logprior: -1.1407e+00
Epoch 10/10
10/10 - 1s - loss: 103.1861 - loglik: -1.0168e+02 - logprior: -1.1133e+00
Fitted a model with MAP estimate = -102.6515
Time for alignment: 36.8337
Computed alignments with likelihoods: ['-102.6643', '-103.0755', '-102.6515']
Best model has likelihood: -102.6515
time for generating output: 0.1333
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.9893069306930693
Training of 3 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f183c6e8f10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f15ecb62310>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1961df3fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f184e145100>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f184e1453a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1972800100>, <__main__.SimpleDirichletPrior object at 0x7f15eee47f70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19c7905af0>

Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 467.6827 - loglik: -4.6582e+02 - logprior: -1.8379e+00
Epoch 2/10
39/39 - 11s - loss: 385.2429 - loglik: -3.8381e+02 - logprior: -1.1789e+00
Epoch 3/10
39/39 - 11s - loss: 377.0460 - loglik: -3.7534e+02 - logprior: -1.1986e+00
Epoch 4/10
39/39 - 11s - loss: 374.6380 - loglik: -3.7293e+02 - logprior: -1.1162e+00
Epoch 5/10
39/39 - 11s - loss: 373.0829 - loglik: -3.7143e+02 - logprior: -1.1044e+00
Epoch 6/10
39/39 - 10s - loss: 372.2171 - loglik: -3.7061e+02 - logprior: -1.1047e+00
Epoch 7/10
39/39 - 11s - loss: 371.4208 - loglik: -3.6986e+02 - logprior: -1.0990e+00
Epoch 8/10
39/39 - 11s - loss: 370.3909 - loglik: -3.6881e+02 - logprior: -1.1131e+00
Epoch 9/10
39/39 - 10s - loss: 370.2181 - loglik: -3.6862e+02 - logprior: -1.1149e+00
Epoch 10/10
39/39 - 11s - loss: 369.8660 - loglik: -3.6824e+02 - logprior: -1.1338e+00
Fitted a model with MAP estimate = -306.6701
expansions: [(0, 10), (10, 1), (15, 1), (18, 1), (28, 1), (30, 1), (32, 4), (33, 1), (38, 1), (43, 1), (44, 1), (45, 1), (60, 1), (72, 1), (89, 1), (90, 2), (91, 2), (103, 2), (107, 1), (108, 1), (127, 11), (134, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 355.3162 - loglik: -3.5186e+02 - logprior: -3.0305e+00
Epoch 2/2
39/39 - 14s - loss: 337.2648 - loglik: -3.3533e+02 - logprior: -1.5648e+00
Fitted a model with MAP estimate = -279.0092
expansions: []
discards: [  1   2   3   4   5   6   7   8 119 184]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 175 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 342.3105 - loglik: -3.3982e+02 - logprior: -2.1172e+00
Epoch 2/2
39/39 - 13s - loss: 338.5991 - loglik: -3.3745e+02 - logprior: -7.8416e-01
Fitted a model with MAP estimate = -282.3112
expansions: [(0, 17), (175, 6)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 198 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 20s - loss: 279.4750 - loglik: -2.7740e+02 - logprior: -1.7952e+00
Epoch 2/10
52/52 - 17s - loss: 273.4287 - loglik: -2.7175e+02 - logprior: -1.4040e+00
Epoch 3/10
52/52 - 18s - loss: 269.7778 - loglik: -2.6803e+02 - logprior: -1.3967e+00
Epoch 4/10
52/52 - 16s - loss: 267.6522 - loglik: -2.6584e+02 - logprior: -1.3780e+00
Epoch 5/10
52/52 - 15s - loss: 266.4178 - loglik: -2.6470e+02 - logprior: -1.3216e+00
Epoch 6/10
52/52 - 19s - loss: 266.8769 - loglik: -2.6521e+02 - logprior: -1.2795e+00
Fitted a model with MAP estimate = -264.9491
Time for alignment: 363.2101
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 469.1767 - loglik: -4.6731e+02 - logprior: -1.8378e+00
Epoch 2/10
39/39 - 10s - loss: 386.4835 - loglik: -3.8504e+02 - logprior: -1.2026e+00
Epoch 3/10
39/39 - 11s - loss: 377.6801 - loglik: -3.7581e+02 - logprior: -1.2711e+00
Epoch 4/10
39/39 - 11s - loss: 375.3968 - loglik: -3.7343e+02 - logprior: -1.2338e+00
Epoch 5/10
39/39 - 11s - loss: 374.2120 - loglik: -3.7225e+02 - logprior: -1.2386e+00
Epoch 6/10
39/39 - 11s - loss: 373.4595 - loglik: -3.7151e+02 - logprior: -1.2557e+00
Epoch 7/10
39/39 - 11s - loss: 372.2531 - loglik: -3.7028e+02 - logprior: -1.2828e+00
Epoch 8/10
39/39 - 11s - loss: 371.8842 - loglik: -3.6993e+02 - logprior: -1.2913e+00
Epoch 9/10
39/39 - 11s - loss: 371.7474 - loglik: -3.6982e+02 - logprior: -1.2893e+00
Epoch 10/10
39/39 - 10s - loss: 370.5740 - loglik: -3.6863e+02 - logprior: -1.2948e+00
Fitted a model with MAP estimate = -305.9656
expansions: [(0, 18), (10, 1), (15, 1), (18, 1), (28, 1), (29, 1), (34, 3), (37, 1), (42, 1), (44, 1), (56, 1), (71, 2), (88, 1), (89, 2), (90, 1), (103, 1), (107, 1), (108, 1), (112, 1), (126, 8), (130, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 363.9243 - loglik: -3.6042e+02 - logprior: -3.0209e+00
Epoch 2/2
39/39 - 13s - loss: 345.8341 - loglik: -3.4331e+02 - logprior: -2.0978e+00
Fitted a model with MAP estimate = -284.6358
expansions: [(54, 3), (185, 25)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  57  58
 125 166 167 168 169 170 171 172 173]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 357.7810 - loglik: -3.5472e+02 - logprior: -2.6395e+00
Epoch 2/2
39/39 - 14s - loss: 344.0789 - loglik: -3.4205e+02 - logprior: -1.5505e+00
Fitted a model with MAP estimate = -284.2332
expansions: [(40, 2), (42, 1), (111, 1)]
discards: [ 44 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166
 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184
 185]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 18s - loss: 294.3282 - loglik: -2.9272e+02 - logprior: -1.2319e+00
Epoch 2/10
52/52 - 13s - loss: 290.3950 - loglik: -2.8929e+02 - logprior: -6.8966e-01
Epoch 3/10
52/52 - 15s - loss: 287.3897 - loglik: -2.8627e+02 - logprior: -6.6132e-01
Epoch 4/10
52/52 - 13s - loss: 286.8326 - loglik: -2.8572e+02 - logprior: -5.9721e-01
Epoch 5/10
52/52 - 14s - loss: 285.3168 - loglik: -2.8430e+02 - logprior: -5.5160e-01
Epoch 6/10
52/52 - 12s - loss: 286.2489 - loglik: -2.8528e+02 - logprior: -4.9356e-01
Fitted a model with MAP estimate = -284.4189
Time for alignment: 347.2569
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 468.3974 - loglik: -4.6652e+02 - logprior: -1.8492e+00
Epoch 2/10
39/39 - 11s - loss: 383.2230 - loglik: -3.8195e+02 - logprior: -1.1604e+00
Epoch 3/10
39/39 - 11s - loss: 373.5697 - loglik: -3.7183e+02 - logprior: -1.2429e+00
Epoch 4/10
39/39 - 10s - loss: 371.3987 - loglik: -3.6961e+02 - logprior: -1.2406e+00
Epoch 5/10
39/39 - 11s - loss: 370.1069 - loglik: -3.6832e+02 - logprior: -1.2252e+00
Epoch 6/10
39/39 - 11s - loss: 369.2780 - loglik: -3.6752e+02 - logprior: -1.2267e+00
Epoch 7/10
39/39 - 10s - loss: 368.4828 - loglik: -3.6672e+02 - logprior: -1.2389e+00
Epoch 8/10
39/39 - 11s - loss: 368.1268 - loglik: -3.6639e+02 - logprior: -1.2415e+00
Epoch 9/10
39/39 - 10s - loss: 368.1587 - loglik: -3.6643e+02 - logprior: -1.2339e+00
Fitted a model with MAP estimate = -304.4296
expansions: [(0, 14), (10, 1), (15, 1), (18, 1), (28, 1), (29, 2), (32, 4), (37, 1), (42, 1), (43, 1), (44, 1), (71, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (102, 1), (106, 1), (107, 1), (125, 4), (127, 2), (134, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 357.7892 - loglik: -3.5444e+02 - logprior: -3.0004e+00
Epoch 2/2
39/39 - 13s - loss: 342.0909 - loglik: -3.4011e+02 - logprior: -1.6601e+00
Fitted a model with MAP estimate = -283.5580
expansions: [(166, 3), (167, 2), (169, 1)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  52  53  54 177 178 179
 180]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 168 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 350.1318 - loglik: -3.4773e+02 - logprior: -2.0734e+00
Epoch 2/2
39/39 - 13s - loss: 344.7155 - loglik: -3.4353e+02 - logprior: -8.0916e-01
Fitted a model with MAP estimate = -286.4475
expansions: [(0, 18), (40, 3), (168, 5)]
discards: [152 153]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 20s - loss: 282.1425 - loglik: -2.8012e+02 - logprior: -1.7328e+00
Epoch 2/10
52/52 - 15s - loss: 274.8137 - loglik: -2.7327e+02 - logprior: -1.2726e+00
Epoch 3/10
52/52 - 18s - loss: 275.3840 - loglik: -2.7385e+02 - logprior: -1.2162e+00
Fitted a model with MAP estimate = -273.1632
Time for alignment: 299.0647
Computed alignments with likelihoods: ['-264.9491', '-284.2332', '-273.1632']
Best model has likelihood: -264.9491
time for generating output: 0.7484
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.23202209734260407
Training of 3 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1947efdbb0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1819c129a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f193472e670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f15eed9d6a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f15eed9dbe0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f182346a520>, <__main__.SimpleDirichletPrior object at 0x7f18a283bd60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1891807dc0>

Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 26s - loss: 816.7014 - loglik: -8.1453e+02 - logprior: -1.7040e+00
Epoch 2/10
37/37 - 23s - loss: 730.2719 - loglik: -7.2801e+02 - logprior: -7.9957e-01
Epoch 3/10
37/37 - 23s - loss: 715.4963 - loglik: -7.1263e+02 - logprior: -1.0599e+00
Epoch 4/10
37/37 - 23s - loss: 706.3453 - loglik: -7.0323e+02 - logprior: -1.1640e+00
Epoch 5/10
37/37 - 23s - loss: 705.7213 - loglik: -7.0285e+02 - logprior: -1.0952e+00
Epoch 6/10
37/37 - 23s - loss: 703.2642 - loglik: -7.0042e+02 - logprior: -1.2849e+00
Epoch 7/10
37/37 - 23s - loss: 701.8327 - loglik: -6.9936e+02 - logprior: -1.1090e+00
Epoch 8/10
37/37 - 23s - loss: 702.2783 - loglik: -6.9998e+02 - logprior: -1.0993e+00
Fitted a model with MAP estimate = -699.6046
expansions: [(0, 4), (34, 3), (35, 2), (39, 1), (61, 2), (76, 2), (93, 11), (116, 1), (117, 1), (135, 5), (136, 1), (197, 12), (210, 4), (240, 1), (242, 1)]
discards: [ 98  99 100 101 103]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 35s - loss: 726.0659 - loglik: -7.2123e+02 - logprior: -3.0882e+00
Epoch 2/2
37/37 - 29s - loss: 697.8367 - loglik: -6.9396e+02 - logprior: -1.3840e+00
Fitted a model with MAP estimate = -689.6076
expansions: [(0, 2), (39, 1), (44, 1), (160, 1), (232, 1)]
discards: [  0  71 114 115 116 254]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 300 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 33s - loss: 706.6443 - loglik: -7.0091e+02 - logprior: -2.4938e+00
Epoch 2/2
37/37 - 29s - loss: 695.2100 - loglik: -6.9099e+02 - logprior: -1.0246e+00
Fitted a model with MAP estimate = -687.7402
expansions: [(0, 2), (91, 1), (92, 1), (235, 2), (239, 2)]
discards: [  1 181 182 183]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 33s - loss: 706.1852 - loglik: -6.9983e+02 - logprior: -2.6957e+00
Epoch 2/10
37/37 - 29s - loss: 695.3801 - loglik: -6.9129e+02 - logprior: -8.7148e-01
Epoch 3/10
37/37 - 29s - loss: 689.5411 - loglik: -6.8588e+02 - logprior: -6.9590e-01
Epoch 4/10
37/37 - 29s - loss: 686.0473 - loglik: -6.8293e+02 - logprior: -5.7632e-01
Epoch 5/10
37/37 - 30s - loss: 684.1842 - loglik: -6.8155e+02 - logprior: -4.5243e-01
Epoch 6/10
37/37 - 30s - loss: 682.8231 - loglik: -6.8055e+02 - logprior: -4.0642e-01
Epoch 7/10
37/37 - 30s - loss: 681.3823 - loglik: -6.7954e+02 - logprior: -2.0834e-01
Epoch 8/10
37/37 - 29s - loss: 681.8611 - loglik: -6.8033e+02 - logprior: -4.9267e-02
Fitted a model with MAP estimate = -678.5064
Time for alignment: 683.6828
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 27s - loss: 816.5200 - loglik: -8.1435e+02 - logprior: -1.6976e+00
Epoch 2/10
37/37 - 23s - loss: 729.2222 - loglik: -7.2672e+02 - logprior: -8.5523e-01
Epoch 3/10
37/37 - 23s - loss: 714.2952 - loglik: -7.1097e+02 - logprior: -1.0137e+00
Epoch 4/10
37/37 - 23s - loss: 706.4036 - loglik: -7.0316e+02 - logprior: -9.9821e-01
Epoch 5/10
37/37 - 23s - loss: 704.7310 - loglik: -7.0184e+02 - logprior: -1.0145e+00
Epoch 6/10
37/37 - 23s - loss: 701.4829 - loglik: -6.9874e+02 - logprior: -1.1360e+00
Epoch 7/10
37/37 - 23s - loss: 702.5333 - loglik: -7.0006e+02 - logprior: -1.0667e+00
Fitted a model with MAP estimate = -699.2162
expansions: [(0, 4), (30, 1), (33, 3), (34, 2), (38, 1), (52, 1), (73, 3), (107, 1), (122, 3), (147, 5), (148, 1), (186, 1), (198, 8), (206, 5), (241, 1), (242, 1)]
discards: [93 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 293 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 725.7041 - loglik: -7.2072e+02 - logprior: -3.0188e+00
Epoch 2/2
37/37 - 28s - loss: 701.1868 - loglik: -6.9711e+02 - logprior: -1.3245e+00
Fitted a model with MAP estimate = -692.5337
expansions: [(0, 2), (37, 1), (44, 1), (103, 12), (139, 1), (166, 1), (226, 4), (242, 3)]
discards: [  0 243 244 245 246 247]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 36s - loss: 708.0006 - loglik: -7.0195e+02 - logprior: -2.5053e+00
Epoch 2/2
37/37 - 31s - loss: 692.6741 - loglik: -6.8812e+02 - logprior: -1.0855e+00
Fitted a model with MAP estimate = -684.6717
expansions: [(113, 4), (234, 1)]
discards: [  1 227]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 34s - loss: 701.9565 - loglik: -6.9575e+02 - logprior: -2.1937e+00
Epoch 2/10
37/37 - 31s - loss: 691.4711 - loglik: -6.8708e+02 - logprior: -7.8685e-01
Epoch 3/10
37/37 - 31s - loss: 687.1041 - loglik: -6.8298e+02 - logprior: -8.6397e-01
Epoch 4/10
37/37 - 31s - loss: 681.5385 - loglik: -6.7811e+02 - logprior: -5.9822e-01
Epoch 5/10
37/37 - 31s - loss: 680.0418 - loglik: -6.7705e+02 - logprior: -5.8594e-01
Epoch 6/10
37/37 - 31s - loss: 679.5303 - loglik: -6.7704e+02 - logprior: -3.7135e-01
Epoch 7/10
37/37 - 31s - loss: 677.3162 - loglik: -6.7512e+02 - logprior: -2.9404e-01
Epoch 8/10
37/37 - 31s - loss: 677.7498 - loglik: -6.7595e+02 - logprior: -9.7037e-02
Fitted a model with MAP estimate = -673.9667
Time for alignment: 677.5967
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 26s - loss: 813.3270 - loglik: -8.1116e+02 - logprior: -1.7017e+00
Epoch 2/10
37/37 - 23s - loss: 726.6898 - loglik: -7.2457e+02 - logprior: -7.0552e-01
Epoch 3/10
37/37 - 23s - loss: 711.6934 - loglik: -7.0910e+02 - logprior: -9.0338e-01
Epoch 4/10
37/37 - 23s - loss: 706.2919 - loglik: -7.0340e+02 - logprior: -9.2991e-01
Epoch 5/10
37/37 - 23s - loss: 703.9731 - loglik: -7.0125e+02 - logprior: -9.6396e-01
Epoch 6/10
37/37 - 23s - loss: 700.4707 - loglik: -6.9795e+02 - logprior: -9.8947e-01
Epoch 7/10
37/37 - 23s - loss: 703.3342 - loglik: -7.0099e+02 - logprior: -9.9708e-01
Fitted a model with MAP estimate = -699.1059
expansions: [(0, 4), (34, 4), (35, 2), (67, 1), (76, 2), (141, 1), (190, 1), (198, 6), (206, 4), (240, 1), (242, 1)]
discards: [ 98 103 104 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 277 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 31s - loss: 725.4825 - loglik: -7.2061e+02 - logprior: -2.9254e+00
Epoch 2/2
37/37 - 26s - loss: 706.1027 - loglik: -7.0234e+02 - logprior: -1.1060e+00
Fitted a model with MAP estimate = -698.4281
expansions: [(0, 2), (43, 1), (44, 1), (88, 1), (103, 13), (105, 10), (166, 1)]
discards: [  0 162 163 164]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 302 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 32s - loss: 711.2543 - loglik: -7.0534e+02 - logprior: -2.4503e+00
Epoch 2/2
37/37 - 29s - loss: 695.1151 - loglik: -6.9067e+02 - logprior: -9.8772e-01
Fitted a model with MAP estimate = -687.8005
expansions: [(0, 2), (93, 1), (126, 1), (239, 1)]
discards: [ 1 90 91]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 304 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 32s - loss: 705.0506 - loglik: -6.9835e+02 - logprior: -2.6577e+00
Epoch 2/10
37/37 - 30s - loss: 694.7907 - loglik: -6.9042e+02 - logprior: -8.1708e-01
Epoch 3/10
37/37 - 29s - loss: 689.3593 - loglik: -6.8552e+02 - logprior: -6.6091e-01
Epoch 4/10
37/37 - 29s - loss: 685.5178 - loglik: -6.8229e+02 - logprior: -5.5404e-01
Epoch 5/10
37/37 - 30s - loss: 684.3752 - loglik: -6.8172e+02 - logprior: -4.4567e-01
Epoch 6/10
37/37 - 30s - loss: 681.8620 - loglik: -6.7945e+02 - logprior: -4.8534e-01
Epoch 7/10
37/37 - 30s - loss: 682.3822 - loglik: -6.8045e+02 - logprior: -2.4739e-01
Fitted a model with MAP estimate = -679.3198
Time for alignment: 621.9053
Computed alignments with likelihoods: ['-678.5064', '-673.9667', '-679.3198']
Best model has likelihood: -673.9667
time for generating output: 0.3092
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.9272094641614474
Training of 3 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fd6e0790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f10390f0280>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f10390d3c10>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1994d23130>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1994d23e50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18786030d0>, <__main__.SimpleDirichletPrior object at 0x7f18193d24f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19beaa9a60>

Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 387.2053 - loglik: -2.7272e+02 - logprior: -1.1446e+02
Epoch 2/10
10/10 - 1s - loss: 290.3579 - loglik: -2.6162e+02 - logprior: -2.8563e+01
Epoch 3/10
10/10 - 1s - loss: 262.0308 - loglik: -2.5084e+02 - logprior: -1.1027e+01
Epoch 4/10
10/10 - 1s - loss: 246.9115 - loglik: -2.4218e+02 - logprior: -4.6850e+00
Epoch 5/10
10/10 - 1s - loss: 238.3399 - loglik: -2.3662e+02 - logprior: -1.6705e+00
Epoch 6/10
10/10 - 1s - loss: 232.4685 - loglik: -2.3194e+02 - logprior: -1.9184e-01
Epoch 7/10
10/10 - 1s - loss: 228.9733 - loglik: -2.2889e+02 - logprior: 0.6445
Epoch 8/10
10/10 - 1s - loss: 227.0066 - loglik: -2.2733e+02 - logprior: 1.2252
Epoch 9/10
10/10 - 1s - loss: 225.7800 - loglik: -2.2656e+02 - logprior: 1.6554
Epoch 10/10
10/10 - 1s - loss: 224.9814 - loglik: -2.2621e+02 - logprior: 2.0494
Fitted a model with MAP estimate = -223.8377
expansions: [(0, 6), (51, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 383.4792 - loglik: -2.3007e+02 - logprior: -1.5266e+02
Epoch 2/2
10/10 - 1s - loss: 270.8438 - loglik: -2.2515e+02 - logprior: -4.4970e+01
Fitted a model with MAP estimate = -249.2423
expansions: [(0, 3), (43, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 346.4983 - loglik: -2.2524e+02 - logprior: -1.2045e+02
Epoch 2/2
10/10 - 1s - loss: 256.4403 - loglik: -2.2302e+02 - logprior: -3.2540e+01
Fitted a model with MAP estimate = -239.3042
expansions: []
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 326.1044 - loglik: -2.2361e+02 - logprior: -1.0158e+02
Epoch 2/10
10/10 - 1s - loss: 248.1902 - loglik: -2.2271e+02 - logprior: -2.4646e+01
Epoch 3/10
10/10 - 1s - loss: 231.7190 - loglik: -2.2240e+02 - logprior: -8.5301e+00
Epoch 4/10
10/10 - 1s - loss: 224.6335 - loglik: -2.2222e+02 - logprior: -1.6517e+00
Epoch 5/10
10/10 - 1s - loss: 220.6753 - loglik: -2.2215e+02 - logprior: 2.1948
Epoch 6/10
10/10 - 1s - loss: 218.3026 - loglik: -2.2207e+02 - logprior: 4.4807
Epoch 7/10
10/10 - 1s - loss: 216.7765 - loglik: -2.2198e+02 - logprior: 5.9216
Epoch 8/10
10/10 - 1s - loss: 215.6890 - loglik: -2.2188e+02 - logprior: 6.9095
Epoch 9/10
10/10 - 1s - loss: 214.8370 - loglik: -2.2177e+02 - logprior: 7.6651
Epoch 10/10
10/10 - 1s - loss: 214.1071 - loglik: -2.2169e+02 - logprior: 8.3165
Fitted a model with MAP estimate = -212.9997
Time for alignment: 48.8099
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.2055 - loglik: -2.7272e+02 - logprior: -1.1446e+02
Epoch 2/10
10/10 - 1s - loss: 290.3581 - loglik: -2.6162e+02 - logprior: -2.8564e+01
Epoch 3/10
10/10 - 1s - loss: 262.0306 - loglik: -2.5083e+02 - logprior: -1.1027e+01
Epoch 4/10
10/10 - 1s - loss: 246.6314 - loglik: -2.4183e+02 - logprior: -4.7408e+00
Epoch 5/10
10/10 - 1s - loss: 237.3369 - loglik: -2.3527e+02 - logprior: -1.8940e+00
Epoch 6/10
10/10 - 1s - loss: 231.9234 - loglik: -2.3110e+02 - logprior: -2.7535e-01
Epoch 7/10
10/10 - 1s - loss: 228.7262 - loglik: -2.2850e+02 - logprior: 0.6080
Epoch 8/10
10/10 - 1s - loss: 226.8550 - loglik: -2.2714e+02 - logprior: 1.1864
Epoch 9/10
10/10 - 1s - loss: 225.7037 - loglik: -2.2646e+02 - logprior: 1.6058
Epoch 10/10
10/10 - 1s - loss: 224.8502 - loglik: -2.2605e+02 - logprior: 1.9955
Fitted a model with MAP estimate = -223.6625
expansions: [(0, 6), (51, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 384.0591 - loglik: -2.3054e+02 - logprior: -1.5280e+02
Epoch 2/2
10/10 - 1s - loss: 270.9860 - loglik: -2.2529e+02 - logprior: -4.4997e+01
Fitted a model with MAP estimate = -249.3105
expansions: [(0, 3), (43, 4), (78, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 96 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 346.4526 - loglik: -2.2538e+02 - logprior: -1.2028e+02
Epoch 2/2
10/10 - 1s - loss: 255.2862 - loglik: -2.2233e+02 - logprior: -3.2057e+01
Fitted a model with MAP estimate = -237.7789
expansions: []
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 324.7396 - loglik: -2.2272e+02 - logprior: -1.0105e+02
Epoch 2/10
10/10 - 1s - loss: 246.5143 - loglik: -2.2151e+02 - logprior: -2.4093e+01
Epoch 3/10
10/10 - 1s - loss: 229.9424 - loglik: -2.2116e+02 - logprior: -7.9245e+00
Epoch 4/10
10/10 - 1s - loss: 222.8288 - loglik: -2.2096e+02 - logprior: -1.0673e+00
Epoch 5/10
10/10 - 1s - loss: 218.8130 - loglik: -2.2083e+02 - logprior: 2.7640
Epoch 6/10
10/10 - 1s - loss: 216.3165 - loglik: -2.2058e+02 - logprior: 5.0193
Epoch 7/10
10/10 - 1s - loss: 214.6484 - loglik: -2.2029e+02 - logprior: 6.4180
Epoch 8/10
10/10 - 1s - loss: 213.4900 - loglik: -2.2012e+02 - logprior: 7.3989
Epoch 9/10
10/10 - 1s - loss: 212.5872 - loglik: -2.1998e+02 - logprior: 8.1575
Epoch 10/10
10/10 - 1s - loss: 211.8433 - loglik: -2.1988e+02 - logprior: 8.7997
Fitted a model with MAP estimate = -210.7085
Time for alignment: 45.2491
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 387.2053 - loglik: -2.7272e+02 - logprior: -1.1446e+02
Epoch 2/10
10/10 - 1s - loss: 290.3581 - loglik: -2.6162e+02 - logprior: -2.8564e+01
Epoch 3/10
10/10 - 1s - loss: 262.0284 - loglik: -2.5083e+02 - logprior: -1.1029e+01
Epoch 4/10
10/10 - 1s - loss: 246.3522 - loglik: -2.4148e+02 - logprior: -4.7879e+00
Epoch 5/10
10/10 - 1s - loss: 236.9305 - loglik: -2.3477e+02 - logprior: -1.8711e+00
Epoch 6/10
10/10 - 1s - loss: 231.5670 - loglik: -2.3070e+02 - logprior: -2.4622e-01
Epoch 7/10
10/10 - 1s - loss: 228.4179 - loglik: -2.2827e+02 - logprior: 0.6303
Epoch 8/10
10/10 - 1s - loss: 226.5321 - loglik: -2.2701e+02 - logprior: 1.2264
Epoch 9/10
10/10 - 1s - loss: 225.3866 - loglik: -2.2636e+02 - logprior: 1.6447
Epoch 10/10
10/10 - 1s - loss: 224.5933 - loglik: -2.2603e+02 - logprior: 2.0672
Fitted a model with MAP estimate = -223.6303
expansions: [(0, 6), (51, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 385.3423 - loglik: -2.3206e+02 - logprior: -1.5265e+02
Epoch 2/2
10/10 - 1s - loss: 271.4111 - loglik: -2.2592e+02 - logprior: -4.4774e+01
Fitted a model with MAP estimate = -249.6750
expansions: [(0, 3), (78, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 346.7064 - loglik: -2.2546e+02 - logprior: -1.2041e+02
Epoch 2/2
10/10 - 1s - loss: 256.4558 - loglik: -2.2304e+02 - logprior: -3.2587e+01
Fitted a model with MAP estimate = -239.3568
expansions: []
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 326.2844 - loglik: -2.2386e+02 - logprior: -1.0157e+02
Epoch 2/10
10/10 - 1s - loss: 248.2623 - loglik: -2.2285e+02 - logprior: -2.4635e+01
Epoch 3/10
10/10 - 1s - loss: 231.7283 - loglik: -2.2242e+02 - logprior: -8.5405e+00
Epoch 4/10
10/10 - 1s - loss: 224.6752 - loglik: -2.2227e+02 - logprior: -1.6530e+00
Epoch 5/10
10/10 - 1s - loss: 220.7437 - loglik: -2.2220e+02 - logprior: 2.1601
Epoch 6/10
10/10 - 1s - loss: 218.3889 - loglik: -2.2211e+02 - logprior: 4.4318
Epoch 7/10
10/10 - 1s - loss: 216.8684 - loglik: -2.2202e+02 - logprior: 5.8716
Epoch 8/10
10/10 - 1s - loss: 215.7477 - loglik: -2.2188e+02 - logprior: 6.8595
Epoch 9/10
10/10 - 1s - loss: 214.8522 - loglik: -2.2172e+02 - logprior: 7.5973
Epoch 10/10
10/10 - 1s - loss: 214.1385 - loglik: -2.2164e+02 - logprior: 8.2328
Fitted a model with MAP estimate = -213.0595
Time for alignment: 45.2964
Computed alignments with likelihoods: ['-212.9997', '-210.7085', '-213.0595']
Best model has likelihood: -210.7085
time for generating output: 0.1573
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.716349310571241
Training of 3 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f197b391df0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f15eee95ca0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18b4561e20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18b3c03a90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18914981f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f15ec2bffd0>, <__main__.SimpleDirichletPrior object at 0x7f180df34550>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe1e6820>

Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 999.8740 - loglik: -9.9145e+02 - logprior: -8.3067e+00
Epoch 2/10
19/19 - 20s - loss: 875.0969 - loglik: -8.7510e+02 - logprior: 0.5994
Epoch 3/10
19/19 - 19s - loss: 814.3416 - loglik: -8.1294e+02 - logprior: -6.2956e-01
Epoch 4/10
19/19 - 20s - loss: 801.2837 - loglik: -7.9924e+02 - logprior: -1.0371e+00
Epoch 5/10
19/19 - 19s - loss: 799.1597 - loglik: -7.9731e+02 - logprior: -9.3677e-01
Epoch 6/10
19/19 - 20s - loss: 793.0890 - loglik: -7.9131e+02 - logprior: -1.0001e+00
Epoch 7/10
19/19 - 19s - loss: 797.4783 - loglik: -7.9564e+02 - logprior: -1.1238e+00
Fitted a model with MAP estimate = -792.7419
expansions: [(14, 1), (32, 1), (110, 1), (123, 15), (124, 2), (142, 1), (157, 1), (167, 1), (168, 1), (169, 1), (170, 1), (173, 1), (174, 1), (176, 3), (190, 4), (197, 1), (198, 1), (204, 1), (218, 2), (219, 1), (220, 1), (221, 1), (222, 1), (223, 2), (236, 2), (237, 2), (262, 1), (263, 3), (301, 2), (302, 5), (303, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 28s - loss: 809.6899 - loglik: -7.9629e+02 - logprior: -1.2480e+01
Epoch 2/2
19/19 - 25s - loss: 775.6988 - loglik: -7.7065e+02 - logprior: -3.7521e+00
Fitted a model with MAP estimate = -764.6383
expansions: [(0, 2), (314, 1)]
discards: [  0 133 134 265]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 31s - loss: 781.9328 - loglik: -7.7246e+02 - logprior: -7.8709e+00
Epoch 2/2
19/19 - 25s - loss: 760.5847 - loglik: -7.5963e+02 - logprior: 0.6175
Fitted a model with MAP estimate = -756.0720
expansions: []
discards: [  0 353]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 782.7414 - loglik: -7.6985e+02 - logprior: -1.1120e+01
Epoch 2/10
19/19 - 25s - loss: 764.0046 - loglik: -7.6143e+02 - logprior: -9.0677e-01
Epoch 3/10
19/19 - 25s - loss: 756.4714 - loglik: -7.5733e+02 - logprior: 2.3207
Epoch 4/10
19/19 - 25s - loss: 755.2314 - loglik: -7.5701e+02 - logprior: 3.0470
Epoch 5/10
19/19 - 25s - loss: 748.5074 - loglik: -7.5084e+02 - logprior: 3.3984
Epoch 6/10
19/19 - 25s - loss: 753.8217 - loglik: -7.5659e+02 - logprior: 3.7046
Fitted a model with MAP estimate = -748.7962
Time for alignment: 469.5248
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 998.5162 - loglik: -9.9010e+02 - logprior: -8.3016e+00
Epoch 2/10
19/19 - 20s - loss: 872.5060 - loglik: -8.7226e+02 - logprior: 0.3788
Epoch 3/10
19/19 - 20s - loss: 809.9193 - loglik: -8.0778e+02 - logprior: -1.0265e+00
Epoch 4/10
19/19 - 20s - loss: 793.0002 - loglik: -7.9001e+02 - logprior: -1.5007e+00
Epoch 5/10
19/19 - 19s - loss: 788.9470 - loglik: -7.8619e+02 - logprior: -1.5495e+00
Epoch 6/10
19/19 - 20s - loss: 782.6868 - loglik: -7.7998e+02 - logprior: -1.6758e+00
Epoch 7/10
19/19 - 20s - loss: 780.6370 - loglik: -7.7798e+02 - logprior: -1.7134e+00
Epoch 8/10
19/19 - 20s - loss: 778.3550 - loglik: -7.7574e+02 - logprior: -1.7445e+00
Epoch 9/10
19/19 - 20s - loss: 777.5870 - loglik: -7.7498e+02 - logprior: -1.7864e+00
Epoch 10/10
19/19 - 20s - loss: 776.4634 - loglik: -7.7392e+02 - logprior: -1.7754e+00
Fitted a model with MAP estimate = -776.2543
expansions: [(32, 1), (65, 1), (67, 1), (98, 1), (99, 3), (113, 1), (114, 3), (117, 1), (119, 1), (123, 8), (125, 2), (143, 1), (145, 1), (158, 1), (164, 2), (167, 1), (168, 1), (169, 1), (170, 1), (174, 1), (178, 2), (181, 1), (190, 1), (191, 2), (199, 1), (205, 1), (212, 2), (221, 4), (224, 1), (237, 4), (259, 1), (264, 2), (266, 3), (273, 1), (293, 2), (294, 7), (300, 1), (301, 6), (303, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 395 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 805.5778 - loglik: -7.9174e+02 - logprior: -1.2889e+01
Epoch 2/2
19/19 - 26s - loss: 753.8003 - loglik: -7.4773e+02 - logprior: -4.6285e+00
Fitted a model with MAP estimate = -746.1841
expansions: [(0, 2), (269, 1), (285, 1)]
discards: [  0 102 103 146 147 189 252 352 353]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 760.5714 - loglik: -7.5078e+02 - logprior: -7.9090e+00
Epoch 2/2
19/19 - 26s - loss: 740.5659 - loglik: -7.3940e+02 - logprior: 0.6716
Fitted a model with MAP estimate = -735.3039
expansions: [(314, 1), (368, 1)]
discards: [  0 118 265]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 389 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 31s - loss: 762.6000 - loglik: -7.4947e+02 - logprior: -1.1112e+01
Epoch 2/10
19/19 - 26s - loss: 744.6024 - loglik: -7.4140e+02 - logprior: -1.3401e+00
Epoch 3/10
19/19 - 26s - loss: 737.5508 - loglik: -7.3823e+02 - logprior: 2.2744
Epoch 4/10
19/19 - 26s - loss: 732.6794 - loglik: -7.3450e+02 - logprior: 3.1852
Epoch 5/10
19/19 - 26s - loss: 731.1028 - loglik: -7.3351e+02 - logprior: 3.5770
Epoch 6/10
19/19 - 26s - loss: 730.5200 - loglik: -7.3322e+02 - logprior: 3.7320
Epoch 7/10
19/19 - 26s - loss: 729.5522 - loglik: -7.3272e+02 - logprior: 4.1021
Epoch 8/10
19/19 - 26s - loss: 728.1306 - loglik: -7.3172e+02 - logprior: 4.4546
Epoch 9/10
19/19 - 26s - loss: 726.8428 - loglik: -7.3080e+02 - logprior: 4.7684
Epoch 10/10
19/19 - 26s - loss: 727.0882 - loglik: -7.3151e+02 - logprior: 5.1877
Fitted a model with MAP estimate = -725.2219
Time for alignment: 645.8205
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 999.2222 - loglik: -9.9079e+02 - logprior: -8.3224e+00
Epoch 2/10
19/19 - 20s - loss: 871.3419 - loglik: -8.7109e+02 - logprior: 0.3447
Epoch 3/10
19/19 - 20s - loss: 814.3680 - loglik: -8.1264e+02 - logprior: -8.7319e-01
Epoch 4/10
19/19 - 20s - loss: 802.7239 - loglik: -8.0004e+02 - logprior: -1.3964e+00
Epoch 5/10
19/19 - 20s - loss: 792.7752 - loglik: -7.9022e+02 - logprior: -1.4153e+00
Epoch 6/10
19/19 - 20s - loss: 792.7817 - loglik: -7.9029e+02 - logprior: -1.5291e+00
Fitted a model with MAP estimate = -788.3773
expansions: [(14, 1), (33, 1), (57, 1), (99, 1), (100, 3), (108, 1), (113, 2), (114, 2), (117, 1), (118, 1), (119, 1), (121, 8), (122, 2), (125, 1), (141, 1), (156, 1), (166, 3), (169, 1), (173, 1), (175, 1), (176, 1), (177, 1), (180, 1), (189, 1), (190, 2), (197, 1), (198, 1), (211, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (237, 1), (238, 1), (242, 2), (260, 1), (264, 8), (294, 4), (301, 2), (302, 4), (303, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 391 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 806.9582 - loglik: -7.9333e+02 - logprior: -1.2382e+01
Epoch 2/2
19/19 - 26s - loss: 763.2073 - loglik: -7.5747e+02 - logprior: -4.0153e+00
Fitted a model with MAP estimate = -754.7873
expansions: [(0, 2)]
discards: [  0 103 104 120 291 316 354 355]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 770.5421 - loglik: -7.6120e+02 - logprior: -7.3693e+00
Epoch 2/2
19/19 - 25s - loss: 748.6614 - loglik: -7.4808e+02 - logprior: 1.1434
Fitted a model with MAP estimate = -744.7752
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 384 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 29s - loss: 770.6776 - loglik: -7.5819e+02 - logprior: -1.0571e+01
Epoch 2/10
19/19 - 25s - loss: 752.0934 - loglik: -7.4975e+02 - logprior: -6.0041e-01
Epoch 3/10
19/19 - 25s - loss: 743.5779 - loglik: -7.4472e+02 - logprior: 2.6344
Epoch 4/10
19/19 - 26s - loss: 741.0050 - loglik: -7.4304e+02 - logprior: 3.3333
Epoch 5/10
19/19 - 25s - loss: 742.7302 - loglik: -7.4523e+02 - logprior: 3.6254
Fitted a model with MAP estimate = -737.8070
Time for alignment: 431.2539
Computed alignments with likelihoods: ['-748.7962', '-725.2219', '-737.8070']
Best model has likelihood: -725.2219
time for generating output: 0.4348
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.7094350961538461
Training of 3 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1959358940>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19a5de0040>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19a5de0400>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f181ab40b50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f181ab40bb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1819a1db80>, <__main__.SimpleDirichletPrior object at 0x7f19341b9ca0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1878129d30>

Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.3148 - loglik: -3.1220e+02 - logprior: -5.7095e+01
Epoch 2/10
10/10 - 1s - loss: 291.1414 - loglik: -2.7692e+02 - logprior: -1.4203e+01
Epoch 3/10
10/10 - 1s - loss: 240.9298 - loglik: -2.3409e+02 - logprior: -6.8169e+00
Epoch 4/10
10/10 - 1s - loss: 212.8812 - loglik: -2.0815e+02 - logprior: -4.6715e+00
Epoch 5/10
10/10 - 1s - loss: 202.4037 - loglik: -1.9884e+02 - logprior: -3.5250e+00
Epoch 6/10
10/10 - 1s - loss: 197.9268 - loglik: -1.9483e+02 - logprior: -3.0045e+00
Epoch 7/10
10/10 - 1s - loss: 195.7147 - loglik: -1.9286e+02 - logprior: -2.5857e+00
Epoch 8/10
10/10 - 1s - loss: 194.8517 - loglik: -1.9225e+02 - logprior: -2.2674e+00
Epoch 9/10
10/10 - 1s - loss: 194.3410 - loglik: -1.9199e+02 - logprior: -2.0689e+00
Epoch 10/10
10/10 - 1s - loss: 193.8813 - loglik: -1.9168e+02 - logprior: -1.9459e+00
Fitted a model with MAP estimate = -193.2260
expansions: [(13, 3), (17, 3), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 250.3461 - loglik: -1.8554e+02 - logprior: -6.4625e+01
Epoch 2/2
10/10 - 2s - loss: 200.0214 - loglik: -1.7397e+02 - logprior: -2.5878e+01
Fitted a model with MAP estimate = -191.2775
expansions: [(0, 2), (69, 1)]
discards: [ 0 20 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 221.4347 - loglik: -1.7026e+02 - logprior: -5.0991e+01
Epoch 2/2
10/10 - 2s - loss: 179.8379 - loglik: -1.6769e+02 - logprior: -1.2015e+01
Fitted a model with MAP estimate = -173.4497
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 230.7380 - loglik: -1.6924e+02 - logprior: -6.1333e+01
Epoch 2/10
10/10 - 2s - loss: 187.0799 - loglik: -1.6906e+02 - logprior: -1.7902e+01
Epoch 3/10
10/10 - 2s - loss: 173.8381 - loglik: -1.6851e+02 - logprior: -5.1617e+00
Epoch 4/10
10/10 - 2s - loss: 168.6871 - loglik: -1.6769e+02 - logprior: -7.7835e-01
Epoch 5/10
10/10 - 2s - loss: 166.8353 - loglik: -1.6774e+02 - logprior: 1.1418
Epoch 6/10
10/10 - 2s - loss: 165.5863 - loglik: -1.6748e+02 - logprior: 2.1631
Epoch 7/10
10/10 - 2s - loss: 164.9749 - loglik: -1.6754e+02 - logprior: 2.8406
Epoch 8/10
10/10 - 2s - loss: 164.6023 - loglik: -1.6772e+02 - logprior: 3.4001
Epoch 9/10
10/10 - 2s - loss: 164.2768 - loglik: -1.6787e+02 - logprior: 3.8801
Epoch 10/10
10/10 - 2s - loss: 163.5806 - loglik: -1.6755e+02 - logprior: 4.2611
Fitted a model with MAP estimate = -163.3250
Time for alignment: 53.5604
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 369.5170 - loglik: -3.1240e+02 - logprior: -5.7097e+01
Epoch 2/10
10/10 - 1s - loss: 291.1051 - loglik: -2.7690e+02 - logprior: -1.4190e+01
Epoch 3/10
10/10 - 1s - loss: 242.5989 - loglik: -2.3576e+02 - logprior: -6.8168e+00
Epoch 4/10
10/10 - 1s - loss: 213.3546 - loglik: -2.0858e+02 - logprior: -4.7133e+00
Epoch 5/10
10/10 - 1s - loss: 202.7869 - loglik: -1.9916e+02 - logprior: -3.6003e+00
Epoch 6/10
10/10 - 1s - loss: 198.1882 - loglik: -1.9519e+02 - logprior: -2.9638e+00
Epoch 7/10
10/10 - 1s - loss: 195.9443 - loglik: -1.9323e+02 - logprior: -2.5838e+00
Epoch 8/10
10/10 - 1s - loss: 194.7930 - loglik: -1.9225e+02 - logprior: -2.2677e+00
Epoch 9/10
10/10 - 1s - loss: 194.5094 - loglik: -1.9209e+02 - logprior: -2.1148e+00
Epoch 10/10
10/10 - 1s - loss: 193.6275 - loglik: -1.9137e+02 - logprior: -1.9928e+00
Fitted a model with MAP estimate = -193.4800
expansions: [(13, 5), (17, 2), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.4380 - loglik: -1.8578e+02 - logprior: -6.4492e+01
Epoch 2/2
10/10 - 2s - loss: 198.6235 - loglik: -1.7282e+02 - logprior: -2.5655e+01
Fitted a model with MAP estimate = -189.9531
expansions: [(0, 2), (70, 1)]
discards: [ 0 21 22 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 221.3694 - loglik: -1.7022e+02 - logprior: -5.0978e+01
Epoch 2/2
10/10 - 2s - loss: 179.8822 - loglik: -1.6772e+02 - logprior: -1.2035e+01
Fitted a model with MAP estimate = -173.4957
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 231.1837 - loglik: -1.6961e+02 - logprior: -6.1403e+01
Epoch 2/10
10/10 - 1s - loss: 187.3255 - loglik: -1.6916e+02 - logprior: -1.8046e+01
Epoch 3/10
10/10 - 2s - loss: 174.1602 - loglik: -1.6878e+02 - logprior: -5.2067e+00
Epoch 4/10
10/10 - 2s - loss: 169.0425 - loglik: -1.6804e+02 - logprior: -7.7741e-01
Epoch 5/10
10/10 - 2s - loss: 167.0021 - loglik: -1.6789e+02 - logprior: 1.1402
Epoch 6/10
10/10 - 2s - loss: 165.8224 - loglik: -1.6771e+02 - logprior: 2.1637
Epoch 7/10
10/10 - 2s - loss: 165.1357 - loglik: -1.6769e+02 - logprior: 2.8422
Epoch 8/10
10/10 - 2s - loss: 164.6574 - loglik: -1.6777e+02 - logprior: 3.4132
Epoch 9/10
10/10 - 2s - loss: 164.4984 - loglik: -1.6809e+02 - logprior: 3.8973
Epoch 10/10
10/10 - 2s - loss: 163.5219 - loglik: -1.6749e+02 - logprior: 4.2857
Fitted a model with MAP estimate = -163.4796
Time for alignment: 52.4206
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 369.5102 - loglik: -3.1239e+02 - logprior: -5.7097e+01
Epoch 2/10
10/10 - 1s - loss: 291.2205 - loglik: -2.7701e+02 - logprior: -1.4193e+01
Epoch 3/10
10/10 - 1s - loss: 241.7386 - loglik: -2.3494e+02 - logprior: -6.7762e+00
Epoch 4/10
10/10 - 1s - loss: 215.1940 - loglik: -2.1041e+02 - logprior: -4.7260e+00
Epoch 5/10
10/10 - 1s - loss: 204.0244 - loglik: -2.0027e+02 - logprior: -3.7121e+00
Epoch 6/10
10/10 - 1s - loss: 198.3136 - loglik: -1.9489e+02 - logprior: -3.2563e+00
Epoch 7/10
10/10 - 1s - loss: 195.6659 - loglik: -1.9256e+02 - logprior: -2.8075e+00
Epoch 8/10
10/10 - 1s - loss: 194.6818 - loglik: -1.9192e+02 - logprior: -2.4598e+00
Epoch 9/10
10/10 - 1s - loss: 193.7314 - loglik: -1.9123e+02 - logprior: -2.2577e+00
Epoch 10/10
10/10 - 1s - loss: 193.8651 - loglik: -1.9152e+02 - logprior: -2.1084e+00
Fitted a model with MAP estimate = -193.1533
expansions: [(13, 5), (17, 2), (19, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (60, 1), (62, 1), (78, 4), (81, 1), (87, 1), (89, 1), (90, 1), (91, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 250.0978 - loglik: -1.8536e+02 - logprior: -6.4551e+01
Epoch 2/2
10/10 - 2s - loss: 198.5437 - loglik: -1.7268e+02 - logprior: -2.5728e+01
Fitted a model with MAP estimate = -189.8511
expansions: [(0, 2), (70, 1)]
discards: [ 0 21 22 99]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 221.4620 - loglik: -1.7031e+02 - logprior: -5.0980e+01
Epoch 2/2
10/10 - 2s - loss: 179.9167 - loglik: -1.6772e+02 - logprior: -1.2066e+01
Fitted a model with MAP estimate = -173.5016
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 231.1884 - loglik: -1.6956e+02 - logprior: -6.1458e+01
Epoch 2/10
10/10 - 2s - loss: 187.2074 - loglik: -1.6898e+02 - logprior: -1.8105e+01
Epoch 3/10
10/10 - 2s - loss: 173.9834 - loglik: -1.6855e+02 - logprior: -5.2600e+00
Epoch 4/10
10/10 - 2s - loss: 168.8329 - loglik: -1.6778e+02 - logprior: -8.3064e-01
Epoch 5/10
10/10 - 2s - loss: 166.9659 - loglik: -1.6781e+02 - logprior: 1.0967
Epoch 6/10
10/10 - 2s - loss: 165.8394 - loglik: -1.6766e+02 - logprior: 2.1019
Epoch 7/10
10/10 - 2s - loss: 164.9379 - loglik: -1.6742e+02 - logprior: 2.7806
Epoch 8/10
10/10 - 2s - loss: 164.4698 - loglik: -1.6751e+02 - logprior: 3.3442
Epoch 9/10
10/10 - 2s - loss: 164.3981 - loglik: -1.6790e+02 - logprior: 3.8100
Epoch 10/10
10/10 - 2s - loss: 163.8174 - loglik: -1.6769e+02 - logprior: 4.1868
Fitted a model with MAP estimate = -163.3718
Time for alignment: 54.3070
Computed alignments with likelihoods: ['-163.3250', '-163.4796', '-163.3718']
Best model has likelihood: -163.3250
time for generating output: 0.1850
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9350982066609735
Training of 3 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f183c483c40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1823540910>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f180e3d9910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f15ec88d3a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f195911b730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f15edd9ec70>, <__main__.SimpleDirichletPrior object at 0x7f181a330910>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1823535310>

Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.7234 - loglik: -3.8846e+02 - logprior: -8.2163e+00
Epoch 2/10
13/13 - 3s - loss: 354.4658 - loglik: -3.5216e+02 - logprior: -1.9904e+00
Epoch 3/10
13/13 - 3s - loss: 325.5229 - loglik: -3.2332e+02 - logprior: -1.7333e+00
Epoch 4/10
13/13 - 3s - loss: 312.3677 - loglik: -3.0973e+02 - logprior: -2.0290e+00
Epoch 5/10
13/13 - 3s - loss: 307.1425 - loglik: -3.0450e+02 - logprior: -1.9601e+00
Epoch 6/10
13/13 - 3s - loss: 304.1851 - loglik: -3.0174e+02 - logprior: -1.8892e+00
Epoch 7/10
13/13 - 3s - loss: 303.5716 - loglik: -3.0113e+02 - logprior: -1.9427e+00
Epoch 8/10
13/13 - 3s - loss: 302.2541 - loglik: -2.9978e+02 - logprior: -1.9937e+00
Epoch 9/10
13/13 - 3s - loss: 301.0229 - loglik: -2.9859e+02 - logprior: -1.9919e+00
Epoch 10/10
13/13 - 3s - loss: 300.4765 - loglik: -2.9808e+02 - logprior: -1.9991e+00
Fitted a model with MAP estimate = -299.9184
expansions: [(6, 1), (19, 1), (22, 1), (24, 1), (25, 1), (26, 1), (27, 3), (28, 2), (33, 1), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 1), (80, 1), (97, 1), (99, 1), (100, 3), (101, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 315.9387 - loglik: -3.0578e+02 - logprior: -9.7135e+00
Epoch 2/2
13/13 - 3s - loss: 296.9482 - loglik: -2.9179e+02 - logprior: -4.5087e+00
Fitted a model with MAP estimate = -292.7808
expansions: [(0, 2)]
discards: [ 0 35 80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 297.5153 - loglik: -2.8925e+02 - logprior: -7.3761e+00
Epoch 2/2
13/13 - 3s - loss: 289.3245 - loglik: -2.8638e+02 - logprior: -2.0280e+00
Fitted a model with MAP estimate = -286.4731
expansions: []
discards: [  0  73 125]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 9s - loss: 301.2296 - loglik: -2.9123e+02 - logprior: -9.0577e+00
Epoch 2/10
13/13 - 3s - loss: 291.4294 - loglik: -2.8783e+02 - logprior: -2.7642e+00
Epoch 3/10
13/13 - 3s - loss: 287.6406 - loglik: -2.8552e+02 - logprior: -1.3145e+00
Epoch 4/10
13/13 - 3s - loss: 287.3494 - loglik: -2.8566e+02 - logprior: -9.6324e-01
Epoch 5/10
13/13 - 3s - loss: 285.6395 - loglik: -2.8423e+02 - logprior: -7.6175e-01
Epoch 6/10
13/13 - 3s - loss: 286.1638 - loglik: -2.8491e+02 - logprior: -6.8215e-01
Fitted a model with MAP estimate = -284.8278
Time for alignment: 87.3613
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.8793 - loglik: -3.8861e+02 - logprior: -8.2257e+00
Epoch 2/10
13/13 - 3s - loss: 355.2817 - loglik: -3.5299e+02 - logprior: -1.9757e+00
Epoch 3/10
13/13 - 3s - loss: 326.7801 - loglik: -3.2444e+02 - logprior: -1.7247e+00
Epoch 4/10
13/13 - 3s - loss: 312.3380 - loglik: -3.0927e+02 - logprior: -2.0721e+00
Epoch 5/10
13/13 - 3s - loss: 307.7412 - loglik: -3.0491e+02 - logprior: -1.9773e+00
Epoch 6/10
13/13 - 3s - loss: 305.0016 - loglik: -3.0256e+02 - logprior: -1.8619e+00
Epoch 7/10
13/13 - 3s - loss: 303.5578 - loglik: -3.0118e+02 - logprior: -1.8937e+00
Epoch 8/10
13/13 - 3s - loss: 303.2179 - loglik: -3.0084e+02 - logprior: -1.9123e+00
Epoch 9/10
13/13 - 3s - loss: 302.1834 - loglik: -2.9987e+02 - logprior: -1.8905e+00
Epoch 10/10
13/13 - 3s - loss: 302.3247 - loglik: -3.0000e+02 - logprior: -1.9042e+00
Fitted a model with MAP estimate = -301.2059
expansions: [(6, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 2), (25, 2), (26, 1), (27, 2), (28, 2), (51, 1), (52, 1), (55, 1), (60, 1), (64, 2), (79, 2), (80, 1), (81, 2), (82, 1), (100, 2), (101, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 319.9339 - loglik: -3.0973e+02 - logprior: -9.7137e+00
Epoch 2/2
13/13 - 3s - loss: 299.4316 - loglik: -2.9402e+02 - logprior: -4.6625e+00
Fitted a model with MAP estimate = -294.4403
expansions: [(0, 2)]
discards: [  0  23  29  38  40  82 100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 299.3180 - loglik: -2.9093e+02 - logprior: -7.4109e+00
Epoch 2/2
13/13 - 3s - loss: 289.9716 - loglik: -2.8686e+02 - logprior: -2.0665e+00
Fitted a model with MAP estimate = -287.1090
expansions: []
discards: [  0 124]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 301.8466 - loglik: -2.9166e+02 - logprior: -9.1170e+00
Epoch 2/10
13/13 - 3s - loss: 291.5578 - loglik: -2.8780e+02 - logprior: -2.8475e+00
Epoch 3/10
13/13 - 3s - loss: 287.7944 - loglik: -2.8552e+02 - logprior: -1.4037e+00
Epoch 4/10
13/13 - 3s - loss: 287.4206 - loglik: -2.8561e+02 - logprior: -1.0491e+00
Epoch 5/10
13/13 - 3s - loss: 285.9871 - loglik: -2.8445e+02 - logprior: -8.5956e-01
Epoch 6/10
13/13 - 3s - loss: 286.3511 - loglik: -2.8497e+02 - logprior: -7.7729e-01
Fitted a model with MAP estimate = -285.0463
Time for alignment: 86.8147
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.4338 - loglik: -3.8817e+02 - logprior: -8.2204e+00
Epoch 2/10
13/13 - 3s - loss: 355.3812 - loglik: -3.5309e+02 - logprior: -1.9761e+00
Epoch 3/10
13/13 - 3s - loss: 327.5239 - loglik: -3.2530e+02 - logprior: -1.7343e+00
Epoch 4/10
13/13 - 3s - loss: 312.5773 - loglik: -3.0971e+02 - logprior: -2.1522e+00
Epoch 5/10
13/13 - 3s - loss: 307.1511 - loglik: -3.0419e+02 - logprior: -2.1504e+00
Epoch 6/10
13/13 - 3s - loss: 305.1098 - loglik: -3.0240e+02 - logprior: -2.0849e+00
Epoch 7/10
13/13 - 3s - loss: 303.5659 - loglik: -3.0094e+02 - logprior: -2.0982e+00
Epoch 8/10
13/13 - 3s - loss: 302.5860 - loglik: -3.0001e+02 - logprior: -2.0779e+00
Epoch 9/10
13/13 - 3s - loss: 302.7535 - loglik: -3.0027e+02 - logprior: -2.0343e+00
Fitted a model with MAP estimate = -301.8407
expansions: [(6, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 2), (25, 2), (26, 1), (27, 2), (28, 2), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (80, 1), (82, 1), (83, 1), (84, 1), (93, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 319.1777 - loglik: -3.0889e+02 - logprior: -9.7805e+00
Epoch 2/2
13/13 - 3s - loss: 300.7356 - loglik: -2.9534e+02 - logprior: -4.6620e+00
Fitted a model with MAP estimate = -296.1598
expansions: [(0, 2)]
discards: [ 0 23 29 38 40 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 301.0772 - loglik: -2.9265e+02 - logprior: -7.4457e+00
Epoch 2/2
13/13 - 3s - loss: 292.5449 - loglik: -2.8946e+02 - logprior: -2.0914e+00
Fitted a model with MAP estimate = -289.6457
expansions: []
discards: [ 0 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 303.6201 - loglik: -2.9349e+02 - logprior: -9.0951e+00
Epoch 2/10
13/13 - 3s - loss: 294.6186 - loglik: -2.9091e+02 - logprior: -2.8164e+00
Epoch 3/10
13/13 - 3s - loss: 290.6522 - loglik: -2.8843e+02 - logprior: -1.3617e+00
Epoch 4/10
13/13 - 3s - loss: 289.9150 - loglik: -2.8815e+02 - logprior: -1.0088e+00
Epoch 5/10
13/13 - 3s - loss: 288.9820 - loglik: -2.8748e+02 - logprior: -8.2613e-01
Epoch 6/10
13/13 - 3s - loss: 288.8836 - loglik: -2.8754e+02 - logprior: -7.3493e-01
Epoch 7/10
13/13 - 3s - loss: 288.1788 - loglik: -2.8694e+02 - logprior: -6.9310e-01
Epoch 8/10
13/13 - 3s - loss: 287.9819 - loglik: -2.8682e+02 - logprior: -6.5762e-01
Epoch 9/10
13/13 - 3s - loss: 287.5751 - loglik: -2.8648e+02 - logprior: -6.1721e-01
Epoch 10/10
13/13 - 3s - loss: 287.6136 - loglik: -2.8658e+02 - logprior: -5.8562e-01
Fitted a model with MAP estimate = -286.9721
Time for alignment: 95.4856
Computed alignments with likelihoods: ['-284.8278', '-285.0463', '-286.9721']
Best model has likelihood: -284.8278
time for generating output: 0.2174
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.8166444147752558
Training of 3 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f15eda7c5e0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f10217dbb80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fca3b670>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fca3bac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f193ed27550>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1818ccc0a0>, <__main__.SimpleDirichletPrior object at 0x7f18198127c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fddc1940>

Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 223.1281 - loglik: -2.1444e+02 - logprior: -8.6843e+00
Epoch 2/10
13/13 - 2s - loss: 175.9618 - loglik: -1.7357e+02 - logprior: -2.3364e+00
Epoch 3/10
13/13 - 2s - loss: 148.9705 - loglik: -1.4698e+02 - logprior: -1.9339e+00
Epoch 4/10
13/13 - 2s - loss: 139.1988 - loglik: -1.3732e+02 - logprior: -1.8547e+00
Epoch 5/10
13/13 - 2s - loss: 135.5919 - loglik: -1.3363e+02 - logprior: -1.7702e+00
Epoch 6/10
13/13 - 2s - loss: 134.4033 - loglik: -1.3232e+02 - logprior: -1.8029e+00
Epoch 7/10
13/13 - 2s - loss: 133.9033 - loglik: -1.3193e+02 - logprior: -1.7453e+00
Epoch 8/10
13/13 - 2s - loss: 133.6661 - loglik: -1.3171e+02 - logprior: -1.7433e+00
Epoch 9/10
13/13 - 2s - loss: 133.3135 - loglik: -1.3138e+02 - logprior: -1.7142e+00
Epoch 10/10
13/13 - 2s - loss: 133.4298 - loglik: -1.3149e+02 - logprior: -1.7069e+00
Fitted a model with MAP estimate = -132.9870
expansions: [(0, 4), (13, 1), (16, 1), (33, 1), (35, 2), (36, 2), (37, 1), (43, 2), (44, 2), (45, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 139.5145 - loglik: -1.2886e+02 - logprior: -1.0460e+01
Epoch 2/2
13/13 - 2s - loss: 121.7108 - loglik: -1.1812e+02 - logprior: -3.3807e+00
Fitted a model with MAP estimate = -117.9305
expansions: [(0, 2), (58, 1)]
discards: [45 59 60 61 62 63]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 130.8346 - loglik: -1.2036e+02 - logprior: -1.0258e+01
Epoch 2/2
13/13 - 2s - loss: 120.9494 - loglik: -1.1730e+02 - logprior: -3.4293e+00
Fitted a model with MAP estimate = -118.5627
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 125.4622 - loglik: -1.1709e+02 - logprior: -8.1406e+00
Epoch 2/10
13/13 - 2s - loss: 118.3334 - loglik: -1.1567e+02 - logprior: -2.4525e+00
Epoch 3/10
13/13 - 2s - loss: 117.5508 - loglik: -1.1537e+02 - logprior: -1.9217e+00
Epoch 4/10
13/13 - 2s - loss: 116.6266 - loglik: -1.1476e+02 - logprior: -1.5817e+00
Epoch 5/10
13/13 - 2s - loss: 116.3793 - loglik: -1.1460e+02 - logprior: -1.5056e+00
Epoch 6/10
13/13 - 2s - loss: 116.2631 - loglik: -1.1453e+02 - logprior: -1.4483e+00
Epoch 7/10
13/13 - 2s - loss: 116.0766 - loglik: -1.1436e+02 - logprior: -1.4244e+00
Epoch 8/10
13/13 - 2s - loss: 115.5931 - loglik: -1.1391e+02 - logprior: -1.3859e+00
Epoch 9/10
13/13 - 2s - loss: 115.8807 - loglik: -1.1423e+02 - logprior: -1.3534e+00
Fitted a model with MAP estimate = -115.3270
Time for alignment: 68.2057
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 222.8427 - loglik: -2.1415e+02 - logprior: -8.6853e+00
Epoch 2/10
13/13 - 2s - loss: 171.9417 - loglik: -1.6958e+02 - logprior: -2.3019e+00
Epoch 3/10
13/13 - 2s - loss: 144.1213 - loglik: -1.4224e+02 - logprior: -1.8052e+00
Epoch 4/10
13/13 - 2s - loss: 137.1764 - loglik: -1.3551e+02 - logprior: -1.6525e+00
Epoch 5/10
13/13 - 2s - loss: 134.2546 - loglik: -1.3256e+02 - logprior: -1.5327e+00
Epoch 6/10
13/13 - 2s - loss: 133.6605 - loglik: -1.3187e+02 - logprior: -1.5467e+00
Epoch 7/10
13/13 - 2s - loss: 133.0931 - loglik: -1.3140e+02 - logprior: -1.4913e+00
Epoch 8/10
13/13 - 2s - loss: 132.9094 - loglik: -1.3121e+02 - logprior: -1.4984e+00
Epoch 9/10
13/13 - 2s - loss: 132.7676 - loglik: -1.3108e+02 - logprior: -1.4796e+00
Epoch 10/10
13/13 - 2s - loss: 132.3122 - loglik: -1.3064e+02 - logprior: -1.4715e+00
Fitted a model with MAP estimate = -132.2673
expansions: [(0, 4), (13, 1), (37, 6), (43, 3), (44, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 137.3203 - loglik: -1.2679e+02 - logprior: -1.0339e+01
Epoch 2/2
13/13 - 2s - loss: 121.8164 - loglik: -1.1842e+02 - logprior: -3.2019e+00
Fitted a model with MAP estimate = -118.8983
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 126.9149 - loglik: -1.1653e+02 - logprior: -1.0207e+01
Epoch 2/2
13/13 - 2s - loss: 118.1934 - loglik: -1.1465e+02 - logprior: -3.3702e+00
Fitted a model with MAP estimate = -116.0095
expansions: []
discards: [57 58]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 81 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 124.2665 - loglik: -1.1599e+02 - logprior: -8.0908e+00
Epoch 2/10
13/13 - 2s - loss: 117.8962 - loglik: -1.1529e+02 - logprior: -2.4231e+00
Epoch 3/10
13/13 - 2s - loss: 116.5767 - loglik: -1.1446e+02 - logprior: -1.8927e+00
Epoch 4/10
13/13 - 2s - loss: 115.9831 - loglik: -1.1420e+02 - logprior: -1.5500e+00
Epoch 5/10
13/13 - 2s - loss: 115.5501 - loglik: -1.1384e+02 - logprior: -1.4683e+00
Epoch 6/10
13/13 - 2s - loss: 115.3383 - loglik: -1.1368e+02 - logprior: -1.4186e+00
Epoch 7/10
13/13 - 2s - loss: 115.0474 - loglik: -1.1338e+02 - logprior: -1.4101e+00
Epoch 8/10
13/13 - 2s - loss: 114.7778 - loglik: -1.1313e+02 - logprior: -1.3899e+00
Epoch 9/10
13/13 - 2s - loss: 114.5901 - loglik: -1.1298e+02 - logprior: -1.3470e+00
Epoch 10/10
13/13 - 2s - loss: 115.0018 - loglik: -1.1342e+02 - logprior: -1.3102e+00
Fitted a model with MAP estimate = -114.3050
Time for alignment: 71.0161
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 222.8460 - loglik: -2.1415e+02 - logprior: -8.6879e+00
Epoch 2/10
13/13 - 2s - loss: 171.9762 - loglik: -1.6961e+02 - logprior: -2.3072e+00
Epoch 3/10
13/13 - 2s - loss: 143.2091 - loglik: -1.4131e+02 - logprior: -1.8284e+00
Epoch 4/10
13/13 - 2s - loss: 136.2375 - loglik: -1.3448e+02 - logprior: -1.7283e+00
Epoch 5/10
13/13 - 2s - loss: 133.8627 - loglik: -1.3202e+02 - logprior: -1.6620e+00
Epoch 6/10
13/13 - 2s - loss: 133.2458 - loglik: -1.3134e+02 - logprior: -1.6673e+00
Epoch 7/10
13/13 - 2s - loss: 132.7073 - loglik: -1.3088e+02 - logprior: -1.6232e+00
Epoch 8/10
13/13 - 2s - loss: 132.3675 - loglik: -1.3054e+02 - logprior: -1.6168e+00
Epoch 9/10
13/13 - 2s - loss: 132.4942 - loglik: -1.3068e+02 - logprior: -1.5971e+00
Fitted a model with MAP estimate = -131.9904
expansions: [(0, 4), (13, 1), (35, 1), (36, 2), (37, 3), (38, 2), (43, 3), (44, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 137.5161 - loglik: -1.2699e+02 - logprior: -1.0344e+01
Epoch 2/2
13/13 - 2s - loss: 121.8985 - loglik: -1.1844e+02 - logprior: -3.2192e+00
Fitted a model with MAP estimate = -118.4417
expansions: [(0, 2)]
discards: [49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 126.6699 - loglik: -1.1623e+02 - logprior: -1.0217e+01
Epoch 2/2
13/13 - 2s - loss: 117.7373 - loglik: -1.1418e+02 - logprior: -3.3637e+00
Fitted a model with MAP estimate = -115.5790
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 122.1983 - loglik: -1.1392e+02 - logprior: -8.0845e+00
Epoch 2/10
13/13 - 2s - loss: 115.7876 - loglik: -1.1318e+02 - logprior: -2.4099e+00
Epoch 3/10
13/13 - 2s - loss: 114.9405 - loglik: -1.1283e+02 - logprior: -1.8744e+00
Epoch 4/10
13/13 - 2s - loss: 113.7667 - loglik: -1.1199e+02 - logprior: -1.5328e+00
Epoch 5/10
13/13 - 2s - loss: 113.9128 - loglik: -1.1221e+02 - logprior: -1.4487e+00
Fitted a model with MAP estimate = -113.2790
Time for alignment: 58.3795
Computed alignments with likelihoods: ['-115.3270', '-114.3050', '-113.2790']
Best model has likelihood: -113.2790
time for generating output: 0.2249
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.5058336469702672
Training of 3 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f180ef50790>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f184daa82b0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f184daa8a30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1020a1dc40>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f181b0580d0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f185eeb3fd0>, <__main__.SimpleDirichletPrior object at 0x7f19fcf0aca0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fddc1940>

Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 59.8400 - loglik: -5.8986e+01 - logprior: -8.4403e-01
Epoch 2/10
41/41 - 2s - loss: 46.4820 - loglik: -4.5553e+01 - logprior: -9.1630e-01
Epoch 3/10
41/41 - 2s - loss: 45.7002 - loglik: -4.4782e+01 - logprior: -8.8254e-01
Epoch 4/10
41/41 - 2s - loss: 45.3176 - loglik: -4.4326e+01 - logprior: -8.8248e-01
Epoch 5/10
41/41 - 2s - loss: 45.1598 - loglik: -4.4139e+01 - logprior: -8.7932e-01
Epoch 6/10
41/41 - 2s - loss: 45.1254 - loglik: -4.4093e+01 - logprior: -8.7741e-01
Epoch 7/10
41/41 - 2s - loss: 44.9768 - loglik: -4.3938e+01 - logprior: -8.7563e-01
Epoch 8/10
41/41 - 2s - loss: 44.9359 - loglik: -4.3885e+01 - logprior: -8.7437e-01
Epoch 9/10
41/41 - 2s - loss: 44.9250 - loglik: -4.3865e+01 - logprior: -8.7538e-01
Epoch 10/10
41/41 - 2s - loss: 44.8565 - loglik: -4.3794e+01 - logprior: -8.7284e-01
Fitted a model with MAP estimate = -44.7303
expansions: [(8, 2), (9, 2), (10, 2), (11, 2), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 27 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 46.0721 - loglik: -4.4842e+01 - logprior: -1.0988e+00
Epoch 2/2
41/41 - 1s - loss: 43.5270 - loglik: -4.2530e+01 - logprior: -8.8578e-01
Fitted a model with MAP estimate = -42.7395
expansions: []
discards: [ 8 12 15 17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 7s - loss: 44.2919 - loglik: -4.3117e+01 - logprior: -1.0603e+00
Epoch 2/2
41/41 - 2s - loss: 43.5634 - loglik: -4.2601e+01 - logprior: -8.4549e-01
Fitted a model with MAP estimate = -42.7462
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 6s - loss: 42.7138 - loglik: -4.1904e+01 - logprior: -6.8401e-01
Epoch 2/10
58/58 - 2s - loss: 42.3335 - loglik: -4.1630e+01 - logprior: -5.8609e-01
Epoch 3/10
58/58 - 2s - loss: 42.2734 - loglik: -4.1573e+01 - logprior: -5.8026e-01
Epoch 4/10
58/58 - 3s - loss: 41.8522 - loglik: -4.1109e+01 - logprior: -5.7519e-01
Epoch 5/10
58/58 - 3s - loss: 41.8014 - loglik: -4.1049e+01 - logprior: -5.7450e-01
Epoch 6/10
58/58 - 2s - loss: 41.8844 - loglik: -4.1130e+01 - logprior: -5.7234e-01
Fitted a model with MAP estimate = -41.3967
Time for alignment: 87.3586
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 59.4140 - loglik: -5.8597e+01 - logprior: -8.0586e-01
Epoch 2/10
41/41 - 2s - loss: 46.3187 - loglik: -4.5568e+01 - logprior: -7.4036e-01
Epoch 3/10
41/41 - 2s - loss: 45.6623 - loglik: -4.4927e+01 - logprior: -6.9510e-01
Epoch 4/10
41/41 - 2s - loss: 45.3501 - loglik: -4.4560e+01 - logprior: -6.9395e-01
Epoch 5/10
41/41 - 2s - loss: 45.1678 - loglik: -4.4344e+01 - logprior: -6.9319e-01
Epoch 6/10
41/41 - 2s - loss: 45.0310 - loglik: -4.4188e+01 - logprior: -6.9112e-01
Epoch 7/10
41/41 - 2s - loss: 45.0437 - loglik: -4.4192e+01 - logprior: -6.8913e-01
Fitted a model with MAP estimate = -44.6196
expansions: [(4, 1), (8, 2), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 47.0075 - loglik: -4.5785e+01 - logprior: -1.1176e+00
Epoch 2/2
41/41 - 1s - loss: 43.5873 - loglik: -4.2623e+01 - logprior: -8.7098e-01
Fitted a model with MAP estimate = -42.6989
expansions: []
discards: [11]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.0674 - loglik: -4.2888e+01 - logprior: -1.0677e+00
Epoch 2/2
41/41 - 2s - loss: 43.5677 - loglik: -4.2605e+01 - logprior: -8.5148e-01
Fitted a model with MAP estimate = -42.6523
expansions: []
discards: [8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.7433 - loglik: -4.1937e+01 - logprior: -6.8475e-01
Epoch 2/10
58/58 - 2s - loss: 42.2826 - loglik: -4.1582e+01 - logprior: -5.8560e-01
Epoch 3/10
58/58 - 2s - loss: 42.2432 - loglik: -4.1551e+01 - logprior: -5.7905e-01
Epoch 4/10
58/58 - 2s - loss: 41.8336 - loglik: -4.1090e+01 - logprior: -5.7761e-01
Epoch 5/10
58/58 - 2s - loss: 41.8856 - loglik: -4.1132e+01 - logprior: -5.7415e-01
Fitted a model with MAP estimate = -41.5298
Time for alignment: 78.8205
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 5s - loss: 59.8497 - loglik: -5.8980e+01 - logprior: -8.5920e-01
Epoch 2/10
41/41 - 2s - loss: 46.4152 - loglik: -4.5440e+01 - logprior: -9.0994e-01
Epoch 3/10
41/41 - 2s - loss: 45.5596 - loglik: -4.4586e+01 - logprior: -8.8396e-01
Epoch 4/10
41/41 - 2s - loss: 45.4328 - loglik: -4.4428e+01 - logprior: -8.8349e-01
Epoch 5/10
41/41 - 2s - loss: 45.1076 - loglik: -4.4081e+01 - logprior: -8.7821e-01
Epoch 6/10
41/41 - 2s - loss: 45.0773 - loglik: -4.4032e+01 - logprior: -8.7605e-01
Epoch 7/10
41/41 - 2s - loss: 44.9223 - loglik: -4.3874e+01 - logprior: -8.7474e-01
Epoch 8/10
41/41 - 2s - loss: 44.9115 - loglik: -4.3848e+01 - logprior: -8.7479e-01
Epoch 9/10
41/41 - 2s - loss: 44.7476 - loglik: -4.3682e+01 - logprior: -8.7476e-01
Epoch 10/10
41/41 - 2s - loss: 44.8111 - loglik: -4.3741e+01 - logprior: -8.7478e-01
Fitted a model with MAP estimate = -44.7115
expansions: [(8, 2), (9, 2), (10, 2), (11, 2), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 27 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 45.9913 - loglik: -4.4756e+01 - logprior: -1.1002e+00
Epoch 2/2
41/41 - 2s - loss: 43.6130 - loglik: -4.2612e+01 - logprior: -8.8472e-01
Fitted a model with MAP estimate = -42.7138
expansions: []
discards: [ 8 13 15 17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 44.2574 - loglik: -4.3081e+01 - logprior: -1.0615e+00
Epoch 2/2
41/41 - 2s - loss: 43.5385 - loglik: -4.2578e+01 - logprior: -8.4273e-01
Fitted a model with MAP estimate = -42.7449
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.7617 - loglik: -4.1948e+01 - logprior: -6.8438e-01
Epoch 2/10
58/58 - 2s - loss: 42.2818 - loglik: -4.1575e+01 - logprior: -5.8697e-01
Epoch 3/10
58/58 - 2s - loss: 42.2647 - loglik: -4.1570e+01 - logprior: -5.7869e-01
Epoch 4/10
58/58 - 2s - loss: 41.8144 - loglik: -4.1070e+01 - logprior: -5.7521e-01
Epoch 5/10
58/58 - 2s - loss: 41.8406 - loglik: -4.1092e+01 - logprior: -5.7388e-01
Fitted a model with MAP estimate = -41.5283
Time for alignment: 83.3868
Computed alignments with likelihoods: ['-41.3967', '-41.5298', '-41.5283']
Best model has likelihood: -41.3967
time for generating output: 0.0942
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.966464502318944
Training of 3 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f197b797100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1038093eb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1a1b43ed30>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f15ed075f10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1819e65a00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19bea03fd0>, <__main__.SimpleDirichletPrior object at 0x7f180ee35d30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1948418ca0>

Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.8616 - loglik: -1.5687e+02 - logprior: -1.2497e+02
Epoch 2/10
10/10 - 1s - loss: 169.6814 - loglik: -1.3509e+02 - logprior: -3.4582e+01
Epoch 3/10
10/10 - 1s - loss: 134.7860 - loglik: -1.1794e+02 - logprior: -1.6825e+01
Epoch 4/10
10/10 - 1s - loss: 118.9485 - loglik: -1.0853e+02 - logprior: -1.0396e+01
Epoch 5/10
10/10 - 1s - loss: 110.9380 - loglik: -1.0408e+02 - logprior: -6.8497e+00
Epoch 6/10
10/10 - 1s - loss: 107.1058 - loglik: -1.0226e+02 - logprior: -4.8291e+00
Epoch 7/10
10/10 - 1s - loss: 104.9942 - loglik: -1.0120e+02 - logprior: -3.6200e+00
Epoch 8/10
10/10 - 1s - loss: 103.9885 - loglik: -1.0087e+02 - logprior: -2.7661e+00
Epoch 9/10
10/10 - 1s - loss: 103.4041 - loglik: -1.0088e+02 - logprior: -2.1869e+00
Epoch 10/10
10/10 - 1s - loss: 102.9984 - loglik: -1.0097e+02 - logprior: -1.7571e+00
Fitted a model with MAP estimate = -102.5886
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 211.9945 - loglik: -9.8162e+01 - logprior: -1.1363e+02
Epoch 2/2
10/10 - 1s - loss: 123.7324 - loglik: -9.1933e+01 - logprior: -3.1688e+01
Fitted a model with MAP estimate = -109.9088
expansions: []
discards: [ 0  9 18 22 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 232.0647 - loglik: -9.3160e+01 - logprior: -1.3879e+02
Epoch 2/2
10/10 - 1s - loss: 149.8642 - loglik: -9.2077e+01 - logprior: -5.7707e+01
Fitted a model with MAP estimate = -136.3123
expansions: [(0, 2)]
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 201.6059 - loglik: -9.0380e+01 - logprior: -1.1114e+02
Epoch 2/10
10/10 - 1s - loss: 119.9272 - loglik: -8.9320e+01 - logprior: -3.0514e+01
Epoch 3/10
10/10 - 1s - loss: 103.1458 - loglik: -8.9249e+01 - logprior: -1.3695e+01
Epoch 4/10
10/10 - 1s - loss: 96.3870 - loglik: -8.9564e+01 - logprior: -6.5797e+00
Epoch 5/10
10/10 - 1s - loss: 92.9136 - loglik: -9.0005e+01 - logprior: -2.6455e+00
Epoch 6/10
10/10 - 1s - loss: 90.9982 - loglik: -9.0385e+01 - logprior: -3.3657e-01
Epoch 7/10
10/10 - 1s - loss: 89.8570 - loglik: -9.0640e+01 - logprior: 1.0774
Epoch 8/10
10/10 - 1s - loss: 89.1240 - loglik: -9.0859e+01 - logprior: 2.0342
Epoch 9/10
10/10 - 1s - loss: 88.5964 - loglik: -9.1011e+01 - logprior: 2.7179
Epoch 10/10
10/10 - 1s - loss: 88.1767 - loglik: -9.1136e+01 - logprior: 3.2635
Fitted a model with MAP estimate = -87.6671
Time for alignment: 32.5712
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 281.8616 - loglik: -1.5687e+02 - logprior: -1.2497e+02
Epoch 2/10
10/10 - 1s - loss: 169.6813 - loglik: -1.3509e+02 - logprior: -3.4582e+01
Epoch 3/10
10/10 - 1s - loss: 134.7861 - loglik: -1.1794e+02 - logprior: -1.6825e+01
Epoch 4/10
10/10 - 1s - loss: 118.9485 - loglik: -1.0853e+02 - logprior: -1.0396e+01
Epoch 5/10
10/10 - 1s - loss: 110.9388 - loglik: -1.0409e+02 - logprior: -6.8492e+00
Epoch 6/10
10/10 - 1s - loss: 107.2262 - loglik: -1.0244e+02 - logprior: -4.7837e+00
Epoch 7/10
10/10 - 1s - loss: 105.1472 - loglik: -1.0151e+02 - logprior: -3.5635e+00
Epoch 8/10
10/10 - 1s - loss: 104.0264 - loglik: -1.0101e+02 - logprior: -2.7359e+00
Epoch 9/10
10/10 - 1s - loss: 103.4315 - loglik: -1.0097e+02 - logprior: -2.1461e+00
Epoch 10/10
10/10 - 1s - loss: 103.0108 - loglik: -1.0104e+02 - logprior: -1.7313e+00
Fitted a model with MAP estimate = -102.6221
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.9219 - loglik: -9.8120e+01 - logprior: -1.1362e+02
Epoch 2/2
10/10 - 1s - loss: 123.6487 - loglik: -9.1859e+01 - logprior: -3.1685e+01
Fitted a model with MAP estimate = -109.7775
expansions: []
discards: [ 0  9 18 22 31]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 231.9692 - loglik: -9.3073e+01 - logprior: -1.3878e+02
Epoch 2/2
10/10 - 1s - loss: 149.8334 - loglik: -9.2051e+01 - logprior: -5.7703e+01
Fitted a model with MAP estimate = -136.3013
expansions: [(0, 2)]
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 201.6069 - loglik: -9.0384e+01 - logprior: -1.1114e+02
Epoch 2/10
10/10 - 1s - loss: 119.9199 - loglik: -8.9308e+01 - logprior: -3.0515e+01
Epoch 3/10
10/10 - 1s - loss: 103.1342 - loglik: -8.9236e+01 - logprior: -1.3698e+01
Epoch 4/10
10/10 - 1s - loss: 96.3843 - loglik: -8.9559e+01 - logprior: -6.5790e+00
Epoch 5/10
10/10 - 1s - loss: 92.9124 - loglik: -9.0005e+01 - logprior: -2.6451e+00
Epoch 6/10
10/10 - 1s - loss: 90.9964 - loglik: -9.0384e+01 - logprior: -3.3543e-01
Epoch 7/10
10/10 - 1s - loss: 89.8546 - loglik: -9.0637e+01 - logprior: 1.0769
Epoch 8/10
10/10 - 1s - loss: 89.1218 - loglik: -9.0856e+01 - logprior: 2.0339
Epoch 9/10
10/10 - 1s - loss: 88.5938 - loglik: -9.1011e+01 - logprior: 2.7202
Epoch 10/10
10/10 - 1s - loss: 88.1737 - loglik: -9.1134e+01 - logprior: 3.2648
Fitted a model with MAP estimate = -87.6647
Time for alignment: 33.8626
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 281.8616 - loglik: -1.5687e+02 - logprior: -1.2497e+02
Epoch 2/10
10/10 - 1s - loss: 169.6813 - loglik: -1.3509e+02 - logprior: -3.4582e+01
Epoch 3/10
10/10 - 1s - loss: 134.7860 - loglik: -1.1794e+02 - logprior: -1.6825e+01
Epoch 4/10
10/10 - 1s - loss: 118.9487 - loglik: -1.0853e+02 - logprior: -1.0396e+01
Epoch 5/10
10/10 - 1s - loss: 110.9381 - loglik: -1.0408e+02 - logprior: -6.8497e+00
Epoch 6/10
10/10 - 1s - loss: 107.1201 - loglik: -1.0228e+02 - logprior: -4.8242e+00
Epoch 7/10
10/10 - 1s - loss: 105.0579 - loglik: -1.0129e+02 - logprior: -3.5834e+00
Epoch 8/10
10/10 - 1s - loss: 104.0883 - loglik: -1.0102e+02 - logprior: -2.7154e+00
Epoch 9/10
10/10 - 1s - loss: 103.5110 - loglik: -1.0106e+02 - logprior: -2.1313e+00
Epoch 10/10
10/10 - 1s - loss: 103.1127 - loglik: -1.0114e+02 - logprior: -1.7032e+00
Fitted a model with MAP estimate = -102.6872
expansions: [(9, 2), (13, 1), (15, 3), (16, 2), (23, 3), (31, 3), (32, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 241.8230 - loglik: -1.0146e+02 - logprior: -1.4019e+02
Epoch 2/2
10/10 - 1s - loss: 153.6925 - loglik: -9.4973e+01 - logprior: -5.8635e+01
Fitted a model with MAP estimate = -138.7679
expansions: [(0, 2)]
discards: [ 0  8 18 21 30]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 204.2918 - loglik: -9.1331e+01 - logprior: -1.1285e+02
Epoch 2/2
10/10 - 1s - loss: 120.7922 - loglik: -8.9551e+01 - logprior: -3.1154e+01
Fitted a model with MAP estimate = -108.3512
expansions: []
discards: [ 0 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 222.6813 - loglik: -9.2043e+01 - logprior: -1.3053e+02
Epoch 2/10
10/10 - 1s - loss: 129.7751 - loglik: -9.1423e+01 - logprior: -3.8283e+01
Epoch 3/10
10/10 - 1s - loss: 106.8675 - loglik: -9.1284e+01 - logprior: -1.5419e+01
Epoch 4/10
10/10 - 1s - loss: 98.7480 - loglik: -9.1323e+01 - logprior: -7.1889e+00
Epoch 5/10
10/10 - 1s - loss: 94.8028 - loglik: -9.1484e+01 - logprior: -3.0655e+00
Epoch 6/10
10/10 - 1s - loss: 92.6188 - loglik: -9.1596e+01 - logprior: -7.4900e-01
Epoch 7/10
10/10 - 1s - loss: 91.3306 - loglik: -9.1679e+01 - logprior: 0.6431
Epoch 8/10
10/10 - 1s - loss: 90.5018 - loglik: -9.1771e+01 - logprior: 1.5692
Epoch 9/10
10/10 - 1s - loss: 89.9074 - loglik: -9.1856e+01 - logprior: 2.2503
Epoch 10/10
10/10 - 1s - loss: 89.4381 - loglik: -9.1933e+01 - logprior: 2.8033
Fitted a model with MAP estimate = -88.8995
Time for alignment: 31.4323
Computed alignments with likelihoods: ['-87.6671', '-87.6647', '-88.8995']
Best model has likelihood: -87.6647
time for generating output: 0.1404
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.8824228028503563
Training of 3 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1039712fd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19fe1a5760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fe1a5cd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f10285c2100>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1818b9b2e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1934326670>, <__main__.SimpleDirichletPrior object at 0x7f15ee01e4c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18b3d04310>

Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.3700 - loglik: -1.9220e+02 - logprior: -3.1580e+00
Epoch 2/10
19/19 - 1s - loss: 155.5153 - loglik: -1.5423e+02 - logprior: -1.2643e+00
Epoch 3/10
19/19 - 2s - loss: 141.2076 - loglik: -1.3963e+02 - logprior: -1.3656e+00
Epoch 4/10
19/19 - 2s - loss: 139.1280 - loglik: -1.3767e+02 - logprior: -1.3439e+00
Epoch 5/10
19/19 - 2s - loss: 138.3884 - loglik: -1.3698e+02 - logprior: -1.2887e+00
Epoch 6/10
19/19 - 2s - loss: 138.3360 - loglik: -1.3697e+02 - logprior: -1.2607e+00
Epoch 7/10
19/19 - 1s - loss: 138.0425 - loglik: -1.3669e+02 - logprior: -1.2379e+00
Epoch 8/10
19/19 - 2s - loss: 138.0991 - loglik: -1.3676e+02 - logprior: -1.2228e+00
Fitted a model with MAP estimate = -138.5226
expansions: [(0, 3), (13, 2), (14, 2), (19, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 1), (50, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 142.5142 - loglik: -1.3807e+02 - logprior: -4.3378e+00
Epoch 2/2
19/19 - 2s - loss: 132.2548 - loglik: -1.3052e+02 - logprior: -1.6130e+00
Fitted a model with MAP estimate = -132.0269
expansions: [(0, 2)]
discards: [19 26 29 51 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.5569 - loglik: -1.3044e+02 - logprior: -3.9756e+00
Epoch 2/2
19/19 - 2s - loss: 129.5934 - loglik: -1.2791e+02 - logprior: -1.5138e+00
Fitted a model with MAP estimate = -129.1179
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 8s - loss: 132.9635 - loglik: -1.2934e+02 - logprior: -3.3972e+00
Epoch 2/10
21/21 - 2s - loss: 130.1317 - loglik: -1.2805e+02 - logprior: -1.8801e+00
Epoch 3/10
21/21 - 2s - loss: 128.2684 - loglik: -1.2669e+02 - logprior: -1.3335e+00
Epoch 4/10
21/21 - 2s - loss: 127.5662 - loglik: -1.2613e+02 - logprior: -1.1715e+00
Epoch 5/10
21/21 - 2s - loss: 127.2767 - loglik: -1.2585e+02 - logprior: -1.1630e+00
Epoch 6/10
21/21 - 2s - loss: 126.9325 - loglik: -1.2554e+02 - logprior: -1.1428e+00
Epoch 7/10
21/21 - 2s - loss: 126.8147 - loglik: -1.2544e+02 - logprior: -1.1247e+00
Epoch 8/10
21/21 - 2s - loss: 126.7015 - loglik: -1.2536e+02 - logprior: -1.1037e+00
Epoch 9/10
21/21 - 2s - loss: 126.7858 - loglik: -1.2546e+02 - logprior: -1.0862e+00
Fitted a model with MAP estimate = -126.3258
Time for alignment: 63.1415
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.3530 - loglik: -1.9219e+02 - logprior: -3.1574e+00
Epoch 2/10
19/19 - 2s - loss: 154.9233 - loglik: -1.5363e+02 - logprior: -1.2650e+00
Epoch 3/10
19/19 - 2s - loss: 141.1344 - loglik: -1.3957e+02 - logprior: -1.3690e+00
Epoch 4/10
19/19 - 2s - loss: 138.9982 - loglik: -1.3755e+02 - logprior: -1.3360e+00
Epoch 5/10
19/19 - 2s - loss: 138.5365 - loglik: -1.3715e+02 - logprior: -1.2808e+00
Epoch 6/10
19/19 - 2s - loss: 138.1926 - loglik: -1.3682e+02 - logprior: -1.2640e+00
Epoch 7/10
19/19 - 2s - loss: 138.1336 - loglik: -1.3680e+02 - logprior: -1.2341e+00
Epoch 8/10
19/19 - 2s - loss: 137.9777 - loglik: -1.3664e+02 - logprior: -1.2225e+00
Epoch 9/10
19/19 - 2s - loss: 137.9848 - loglik: -1.3665e+02 - logprior: -1.2173e+00
Fitted a model with MAP estimate = -138.5047
expansions: [(0, 3), (13, 2), (14, 2), (19, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 79 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 143.1617 - loglik: -1.3866e+02 - logprior: -4.3938e+00
Epoch 2/2
19/19 - 2s - loss: 132.4053 - loglik: -1.3063e+02 - logprior: -1.6561e+00
Fitted a model with MAP estimate = -132.0876
expansions: [(0, 2)]
discards: [19 26 29 51 62 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 134.5743 - loglik: -1.3045e+02 - logprior: -3.9979e+00
Epoch 2/2
19/19 - 2s - loss: 129.0580 - loglik: -1.2738e+02 - logprior: -1.4997e+00
Fitted a model with MAP estimate = -128.7166
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 132.8296 - loglik: -1.2909e+02 - logprior: -3.4991e+00
Epoch 2/10
21/21 - 2s - loss: 129.7409 - loglik: -1.2764e+02 - logprior: -1.8840e+00
Epoch 3/10
21/21 - 2s - loss: 128.0369 - loglik: -1.2652e+02 - logprior: -1.2601e+00
Epoch 4/10
21/21 - 2s - loss: 127.6921 - loglik: -1.2627e+02 - logprior: -1.1622e+00
Epoch 5/10
21/21 - 2s - loss: 127.0848 - loglik: -1.2568e+02 - logprior: -1.1567e+00
Epoch 6/10
21/21 - 2s - loss: 127.1120 - loglik: -1.2572e+02 - logprior: -1.1411e+00
Fitted a model with MAP estimate = -126.5611
Time for alignment: 59.5707
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 195.5166 - loglik: -1.9235e+02 - logprior: -3.1592e+00
Epoch 2/10
19/19 - 2s - loss: 155.0442 - loglik: -1.5374e+02 - logprior: -1.2793e+00
Epoch 3/10
19/19 - 2s - loss: 141.4038 - loglik: -1.3984e+02 - logprior: -1.3757e+00
Epoch 4/10
19/19 - 2s - loss: 139.7112 - loglik: -1.3825e+02 - logprior: -1.3444e+00
Epoch 5/10
19/19 - 2s - loss: 139.1416 - loglik: -1.3773e+02 - logprior: -1.2982e+00
Epoch 6/10
19/19 - 1s - loss: 138.6989 - loglik: -1.3731e+02 - logprior: -1.2813e+00
Epoch 7/10
19/19 - 2s - loss: 138.6123 - loglik: -1.3724e+02 - logprior: -1.2601e+00
Epoch 8/10
19/19 - 2s - loss: 138.3563 - loglik: -1.3700e+02 - logprior: -1.2461e+00
Epoch 9/10
19/19 - 1s - loss: 138.3923 - loglik: -1.3703e+02 - logprior: -1.2386e+00
Fitted a model with MAP estimate = -138.8120
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (15, 1), (20, 2), (33, 1), (39, 2), (46, 1), (47, 1), (50, 2), (51, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.7835 - loglik: -1.3731e+02 - logprior: -4.3709e+00
Epoch 2/2
19/19 - 2s - loss: 131.7826 - loglik: -1.3010e+02 - logprior: -1.5580e+00
Fitted a model with MAP estimate = -131.5465
expansions: [(0, 2)]
discards: [27 49 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 133.8322 - loglik: -1.2974e+02 - logprior: -3.9571e+00
Epoch 2/2
19/19 - 2s - loss: 128.8868 - loglik: -1.2722e+02 - logprior: -1.4902e+00
Fitted a model with MAP estimate = -129.0504
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 133.5529 - loglik: -1.2981e+02 - logprior: -3.5039e+00
Epoch 2/10
21/21 - 2s - loss: 130.3464 - loglik: -1.2826e+02 - logprior: -1.8738e+00
Epoch 3/10
21/21 - 2s - loss: 128.5823 - loglik: -1.2707e+02 - logprior: -1.2495e+00
Epoch 4/10
21/21 - 2s - loss: 127.8733 - loglik: -1.2645e+02 - logprior: -1.1555e+00
Epoch 5/10
21/21 - 2s - loss: 127.4756 - loglik: -1.2606e+02 - logprior: -1.1618e+00
Epoch 6/10
21/21 - 2s - loss: 127.3497 - loglik: -1.2597e+02 - logprior: -1.1405e+00
Epoch 7/10
21/21 - 2s - loss: 127.2481 - loglik: -1.2589e+02 - logprior: -1.1190e+00
Epoch 8/10
21/21 - 2s - loss: 127.0815 - loglik: -1.2575e+02 - logprior: -1.0975e+00
Epoch 9/10
21/21 - 2s - loss: 126.9033 - loglik: -1.2558e+02 - logprior: -1.0863e+00
Epoch 10/10
21/21 - 2s - loss: 127.0600 - loglik: -1.2576e+02 - logprior: -1.0631e+00
Fitted a model with MAP estimate = -126.6876
Time for alignment: 65.0550
Computed alignments with likelihoods: ['-126.3258', '-126.5611', '-126.6876']
Best model has likelihood: -126.3258
time for generating output: 0.1639
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.9451029320024953
Training of 3 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f185ec00730>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1899ba5be0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fd735ee0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19474c1700>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f180dbc4e80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f15efa54880>, <__main__.SimpleDirichletPrior object at 0x7f1959558970>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18b42efee0>

Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 390.8376 - loglik: -3.8253e+02 - logprior: -8.2716e+00
Epoch 2/10
13/13 - 2s - loss: 338.0248 - loglik: -3.3585e+02 - logprior: -2.0470e+00
Epoch 3/10
13/13 - 2s - loss: 296.8285 - loglik: -2.9457e+02 - logprior: -2.0048e+00
Epoch 4/10
13/13 - 2s - loss: 282.5516 - loglik: -2.7972e+02 - logprior: -2.3604e+00
Epoch 5/10
13/13 - 2s - loss: 278.0823 - loglik: -2.7518e+02 - logprior: -2.3712e+00
Epoch 6/10
13/13 - 2s - loss: 276.6967 - loglik: -2.7400e+02 - logprior: -2.2566e+00
Epoch 7/10
13/13 - 3s - loss: 276.3507 - loglik: -2.7371e+02 - logprior: -2.2380e+00
Epoch 8/10
13/13 - 2s - loss: 275.4162 - loglik: -2.7281e+02 - logprior: -2.2354e+00
Epoch 9/10
13/13 - 2s - loss: 274.6984 - loglik: -2.7213e+02 - logprior: -2.2324e+00
Epoch 10/10
13/13 - 2s - loss: 274.1256 - loglik: -2.7157e+02 - logprior: -2.2283e+00
Fitted a model with MAP estimate = -274.0747
expansions: [(7, 2), (8, 2), (14, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (24, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 3), (75, 1), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 283.3506 - loglik: -2.7315e+02 - logprior: -9.8064e+00
Epoch 2/2
13/13 - 3s - loss: 263.8890 - loglik: -2.5897e+02 - logprior: -4.3391e+00
Fitted a model with MAP estimate = -259.8560
expansions: [(0, 3)]
discards: [  0  88  89 128]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 266.6792 - loglik: -2.5831e+02 - logprior: -7.6289e+00
Epoch 2/2
13/13 - 3s - loss: 256.4826 - loglik: -2.5371e+02 - logprior: -2.0448e+00
Fitted a model with MAP estimate = -254.2411
expansions: []
discards: [ 0  2 24 78]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 269.0806 - loglik: -2.5875e+02 - logprior: -9.5094e+00
Epoch 2/10
13/13 - 3s - loss: 258.7500 - loglik: -2.5486e+02 - logprior: -3.1476e+00
Epoch 3/10
13/13 - 3s - loss: 255.5559 - loglik: -2.5341e+02 - logprior: -1.3898e+00
Epoch 4/10
13/13 - 3s - loss: 254.4463 - loglik: -2.5292e+02 - logprior: -8.4383e-01
Epoch 5/10
13/13 - 3s - loss: 252.9976 - loglik: -2.5160e+02 - logprior: -7.9632e-01
Epoch 6/10
13/13 - 3s - loss: 252.3930 - loglik: -2.5111e+02 - logprior: -7.5710e-01
Epoch 7/10
13/13 - 3s - loss: 252.3353 - loglik: -2.5113e+02 - logprior: -7.3724e-01
Epoch 8/10
13/13 - 3s - loss: 251.5098 - loglik: -2.5036e+02 - logprior: -7.1250e-01
Epoch 9/10
13/13 - 3s - loss: 251.7283 - loglik: -2.5067e+02 - logprior: -6.6125e-01
Fitted a model with MAP estimate = -251.1568
Time for alignment: 93.9992
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.2314 - loglik: -3.8292e+02 - logprior: -8.2703e+00
Epoch 2/10
13/13 - 2s - loss: 337.6596 - loglik: -3.3549e+02 - logprior: -2.0405e+00
Epoch 3/10
13/13 - 2s - loss: 296.3024 - loglik: -2.9406e+02 - logprior: -1.9918e+00
Epoch 4/10
13/13 - 3s - loss: 281.3579 - loglik: -2.7846e+02 - logprior: -2.4163e+00
Epoch 5/10
13/13 - 2s - loss: 278.9930 - loglik: -2.7596e+02 - logprior: -2.4943e+00
Epoch 6/10
13/13 - 2s - loss: 276.5304 - loglik: -2.7369e+02 - logprior: -2.3954e+00
Epoch 7/10
13/13 - 2s - loss: 275.8686 - loglik: -2.7308e+02 - logprior: -2.3701e+00
Epoch 8/10
13/13 - 2s - loss: 275.3873 - loglik: -2.7263e+02 - logprior: -2.3733e+00
Epoch 9/10
13/13 - 3s - loss: 275.3345 - loglik: -2.7260e+02 - logprior: -2.3663e+00
Epoch 10/10
13/13 - 2s - loss: 275.1930 - loglik: -2.7247e+02 - logprior: -2.3657e+00
Fitted a model with MAP estimate = -274.5747
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (49, 1), (50, 1), (51, 1), (70, 3), (75, 1), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 282.7037 - loglik: -2.7244e+02 - logprior: -9.8599e+00
Epoch 2/2
13/13 - 3s - loss: 264.2220 - loglik: -2.5930e+02 - logprior: -4.3398e+00
Fitted a model with MAP estimate = -260.0220
expansions: [(0, 3)]
discards: [  0  86  87 126]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 266.5021 - loglik: -2.5814e+02 - logprior: -7.6449e+00
Epoch 2/2
13/13 - 3s - loss: 256.9716 - loglik: -2.5418e+02 - logprior: -2.0736e+00
Fitted a model with MAP estimate = -254.4529
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 267.9300 - loglik: -2.5758e+02 - logprior: -9.5777e+00
Epoch 2/10
13/13 - 3s - loss: 259.3443 - loglik: -2.5540e+02 - logprior: -3.2223e+00
Epoch 3/10
13/13 - 3s - loss: 255.0895 - loglik: -2.5286e+02 - logprior: -1.4831e+00
Epoch 4/10
13/13 - 3s - loss: 254.1291 - loglik: -2.5256e+02 - logprior: -8.9504e-01
Epoch 5/10
13/13 - 3s - loss: 253.1694 - loglik: -2.5173e+02 - logprior: -8.4147e-01
Epoch 6/10
13/13 - 3s - loss: 252.5007 - loglik: -2.5117e+02 - logprior: -8.0271e-01
Epoch 7/10
13/13 - 3s - loss: 251.9746 - loglik: -2.5072e+02 - logprior: -7.7728e-01
Epoch 8/10
13/13 - 3s - loss: 252.0390 - loglik: -2.5086e+02 - logprior: -7.4685e-01
Fitted a model with MAP estimate = -251.3028
Time for alignment: 89.5789
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 391.1032 - loglik: -3.8279e+02 - logprior: -8.2766e+00
Epoch 2/10
13/13 - 2s - loss: 337.2458 - loglik: -3.3508e+02 - logprior: -2.0373e+00
Epoch 3/10
13/13 - 2s - loss: 296.4565 - loglik: -2.9413e+02 - logprior: -1.9728e+00
Epoch 4/10
13/13 - 2s - loss: 282.3120 - loglik: -2.7923e+02 - logprior: -2.3272e+00
Epoch 5/10
13/13 - 2s - loss: 278.6526 - loglik: -2.7566e+02 - logprior: -2.3538e+00
Epoch 6/10
13/13 - 2s - loss: 277.0296 - loglik: -2.7426e+02 - logprior: -2.2863e+00
Epoch 7/10
13/13 - 2s - loss: 276.0860 - loglik: -2.7337e+02 - logprior: -2.2843e+00
Epoch 8/10
13/13 - 2s - loss: 276.2968 - loglik: -2.7363e+02 - logprior: -2.2973e+00
Fitted a model with MAP estimate = -275.2186
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 2), (53, 1), (70, 3), (75, 2), (76, 4), (97, 1), (98, 2), (99, 3), (100, 1), (101, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 284.8683 - loglik: -2.7458e+02 - logprior: -9.8276e+00
Epoch 2/2
13/13 - 3s - loss: 264.3392 - loglik: -2.5932e+02 - logprior: -4.3599e+00
Fitted a model with MAP estimate = -260.1788
expansions: [(0, 3)]
discards: [  0  65  87  88  94 128]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 266.7391 - loglik: -2.5835e+02 - logprior: -7.6360e+00
Epoch 2/2
13/13 - 3s - loss: 256.6247 - loglik: -2.5385e+02 - logprior: -2.0360e+00
Fitted a model with MAP estimate = -254.4677
expansions: []
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 267.9882 - loglik: -2.5769e+02 - logprior: -9.5061e+00
Epoch 2/10
13/13 - 3s - loss: 259.4363 - loglik: -2.5563e+02 - logprior: -3.0719e+00
Epoch 3/10
13/13 - 3s - loss: 255.5467 - loglik: -2.5341e+02 - logprior: -1.3901e+00
Epoch 4/10
13/13 - 3s - loss: 254.1292 - loglik: -2.5260e+02 - logprior: -8.5347e-01
Epoch 5/10
13/13 - 3s - loss: 252.6494 - loglik: -2.5127e+02 - logprior: -7.8249e-01
Epoch 6/10
13/13 - 3s - loss: 252.6700 - loglik: -2.5139e+02 - logprior: -7.5767e-01
Fitted a model with MAP estimate = -251.7727
Time for alignment: 79.7527
Computed alignments with likelihoods: ['-251.1568', '-251.3028', '-251.7727']
Best model has likelihood: -251.1568
time for generating output: 0.2043
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.9459136822773186
Training of 3 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f100cee7460>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1934f5e760>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1934f5e100>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f196a1109d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f181b05d130>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f100cf61430>, <__main__.SimpleDirichletPrior object at 0x7f181803e040>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f18b42efee0>

Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 36s - loss: 1086.6469 - loglik: -1.0826e+03 - logprior: -3.9562e+00
Epoch 2/10
25/25 - 32s - loss: 824.2214 - loglik: -8.2257e+02 - logprior: -1.5073e+00
Epoch 3/10
25/25 - 32s - loss: 766.9344 - loglik: -7.6334e+02 - logprior: -2.8576e+00
Epoch 4/10
25/25 - 32s - loss: 753.8873 - loglik: -7.5003e+02 - logprior: -3.0217e+00
Epoch 5/10
25/25 - 32s - loss: 752.9988 - loglik: -7.4912e+02 - logprior: -3.0092e+00
Epoch 6/10
25/25 - 32s - loss: 751.3661 - loglik: -7.4741e+02 - logprior: -3.0671e+00
Epoch 7/10
25/25 - 32s - loss: 749.6511 - loglik: -7.4565e+02 - logprior: -3.1399e+00
Epoch 8/10
25/25 - 32s - loss: 747.6984 - loglik: -7.4371e+02 - logprior: -3.1597e+00
Epoch 9/10
25/25 - 32s - loss: 750.5382 - loglik: -7.4658e+02 - logprior: -3.1568e+00
Fitted a model with MAP estimate = -747.1458
expansions: [(3, 1), (52, 1), (125, 1), (131, 1), (139, 1), (142, 1), (162, 2), (163, 2), (169, 1), (173, 4), (174, 2), (175, 2), (190, 1), (191, 1), (192, 4), (193, 1), (194, 1), (197, 1), (198, 1), (199, 1), (200, 1), (202, 1), (203, 2), (206, 1), (208, 1), (209, 1), (212, 1), (213, 1), (218, 1), (219, 1), (223, 1), (224, 1), (225, 2), (226, 1), (227, 1), (228, 2), (229, 1), (230, 1), (237, 2), (238, 1), (250, 1), (251, 1), (255, 3), (256, 2), (258, 5), (266, 1), (281, 1), (282, 1), (299, 1), (300, 1), (301, 2), (318, 5), (319, 2), (324, 1), (327, 1), (352, 2), (356, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 48s - loss: 734.8078 - loglik: -7.2875e+02 - logprior: -5.1022e+00
Epoch 2/2
25/25 - 45s - loss: 703.2612 - loglik: -7.0060e+02 - logprior: -1.3905e+00
Fitted a model with MAP estimate = -694.7523
expansions: [(248, 1), (433, 1)]
discards: [169 186 187 215 319 320 434 435]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 709.8193 - loglik: -7.0398e+02 - logprior: -4.2808e+00
Epoch 2/2
25/25 - 44s - loss: 699.6022 - loglik: -6.9791e+02 - logprior: -2.4999e-02
Fitted a model with MAP estimate = -694.1955
expansions: []
discards: [190]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 48s - loss: 707.2849 - loglik: -7.0138e+02 - logprior: -4.0695e+00
Epoch 2/10
25/25 - 44s - loss: 699.6368 - loglik: -6.9829e+02 - logprior: 0.4624
Epoch 3/10
25/25 - 44s - loss: 694.1700 - loglik: -6.9355e+02 - logprior: 1.0612
Epoch 4/10
25/25 - 44s - loss: 691.5514 - loglik: -6.9137e+02 - logprior: 1.3081
Epoch 5/10
25/25 - 44s - loss: 692.8582 - loglik: -6.9313e+02 - logprior: 1.6025
Fitted a model with MAP estimate = -689.3159
Time for alignment: 847.6432
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 35s - loss: 1081.2168 - loglik: -1.0773e+03 - logprior: -3.8739e+00
Epoch 2/10
25/25 - 32s - loss: 819.4213 - loglik: -8.1758e+02 - logprior: -1.6079e+00
Epoch 3/10
25/25 - 32s - loss: 764.1516 - loglik: -7.6045e+02 - logprior: -2.8677e+00
Epoch 4/10
25/25 - 32s - loss: 757.3100 - loglik: -7.5341e+02 - logprior: -3.0008e+00
Epoch 5/10
25/25 - 32s - loss: 752.8816 - loglik: -7.4895e+02 - logprior: -3.0366e+00
Epoch 6/10
25/25 - 32s - loss: 748.5538 - loglik: -7.4447e+02 - logprior: -3.2008e+00
Epoch 7/10
25/25 - 32s - loss: 745.6372 - loglik: -7.4151e+02 - logprior: -3.2491e+00
Epoch 8/10
25/25 - 32s - loss: 746.5145 - loglik: -7.4240e+02 - logprior: -3.2720e+00
Fitted a model with MAP estimate = -745.6125
expansions: [(52, 1), (144, 1), (147, 1), (163, 1), (164, 2), (170, 1), (174, 5), (175, 2), (176, 2), (177, 1), (178, 1), (191, 1), (192, 4), (193, 1), (194, 1), (196, 1), (197, 1), (198, 2), (199, 1), (201, 1), (202, 1), (204, 1), (205, 1), (207, 1), (208, 1), (211, 1), (212, 1), (214, 1), (216, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (234, 1), (236, 1), (237, 1), (248, 1), (255, 2), (256, 3), (258, 1), (259, 1), (267, 1), (281, 1), (282, 1), (283, 1), (300, 1), (301, 1), (302, 2), (304, 1), (318, 2), (319, 2), (321, 2), (326, 1), (327, 3), (354, 3), (358, 1), (364, 1), (366, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 456 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 734.4536 - loglik: -7.2786e+02 - logprior: -5.5643e+00
Epoch 2/2
25/25 - 44s - loss: 696.0853 - loglik: -6.9276e+02 - logprior: -2.0163e+00
Fitted a model with MAP estimate = -691.2912
expansions: [(127, 1), (246, 1)]
discards: [183 184 185 213 395 434 435]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 47s - loss: 705.1512 - loglik: -6.9961e+02 - logprior: -4.0001e+00
Epoch 2/2
25/25 - 44s - loss: 695.6335 - loglik: -6.9413e+02 - logprior: 0.1067
Fitted a model with MAP estimate = -689.7660
expansions: [(430, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 47s - loss: 701.7211 - loglik: -6.9561e+02 - logprior: -4.3604e+00
Epoch 2/10
25/25 - 44s - loss: 692.4302 - loglik: -6.9080e+02 - logprior: 0.0667
Epoch 3/10
25/25 - 44s - loss: 688.6228 - loglik: -6.8789e+02 - logprior: 0.8709
Epoch 4/10
25/25 - 44s - loss: 684.0605 - loglik: -6.8380e+02 - logprior: 1.1542
Epoch 5/10
25/25 - 44s - loss: 684.6206 - loglik: -6.8481e+02 - logprior: 1.4459
Fitted a model with MAP estimate = -681.4424
Time for alignment: 816.0373
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 37s - loss: 1081.4136 - loglik: -1.0774e+03 - logprior: -3.9510e+00
Epoch 2/10
25/25 - 32s - loss: 819.5168 - loglik: -8.1767e+02 - logprior: -1.7741e+00
Epoch 3/10
25/25 - 32s - loss: 760.2823 - loglik: -7.5639e+02 - logprior: -3.3714e+00
Epoch 4/10
25/25 - 32s - loss: 746.8143 - loglik: -7.4242e+02 - logprior: -3.6784e+00
Epoch 5/10
25/25 - 32s - loss: 747.2033 - loglik: -7.4274e+02 - logprior: -3.6320e+00
Fitted a model with MAP estimate = -743.4395
expansions: [(3, 1), (126, 1), (135, 1), (145, 1), (164, 2), (165, 1), (169, 1), (171, 1), (175, 4), (176, 2), (177, 2), (178, 1), (179, 1), (190, 1), (191, 1), (192, 4), (193, 1), (194, 1), (197, 1), (198, 1), (199, 1), (200, 1), (202, 1), (203, 2), (204, 1), (206, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (214, 1), (216, 1), (222, 1), (223, 1), (224, 2), (226, 1), (227, 1), (228, 2), (229, 1), (230, 1), (237, 2), (238, 1), (249, 1), (251, 1), (252, 1), (254, 2), (255, 1), (256, 1), (258, 6), (281, 1), (282, 1), (283, 1), (297, 1), (299, 1), (300, 1), (301, 2), (303, 1), (313, 1), (316, 1), (318, 1), (321, 2), (324, 1), (325, 1), (327, 2), (352, 2), (354, 1), (355, 1), (360, 1), (363, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 463 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 49s - loss: 729.1240 - loglik: -7.2303e+02 - logprior: -5.0859e+00
Epoch 2/2
25/25 - 46s - loss: 696.1241 - loglik: -6.9366e+02 - logprior: -1.2537e+00
Fitted a model with MAP estimate = -689.6855
expansions: []
discards: [169 186 187 215 269 403 439 440]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 48s - loss: 702.5615 - loglik: -6.9711e+02 - logprior: -3.9936e+00
Epoch 2/2
25/25 - 44s - loss: 697.4141 - loglik: -6.9568e+02 - logprior: -1.6476e-01
Fitted a model with MAP estimate = -689.7534
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 49s - loss: 700.6077 - loglik: -6.9553e+02 - logprior: -3.3351e+00
Epoch 2/10
25/25 - 44s - loss: 695.7260 - loglik: -6.9465e+02 - logprior: 0.6259
Epoch 3/10
25/25 - 44s - loss: 690.7644 - loglik: -6.9005e+02 - logprior: 0.8771
Epoch 4/10
25/25 - 44s - loss: 686.4001 - loglik: -6.8617e+02 - logprior: 1.1827
Epoch 5/10
25/25 - 44s - loss: 685.9979 - loglik: -6.8638e+02 - logprior: 1.6604
Epoch 6/10
25/25 - 45s - loss: 685.7908 - loglik: -6.8661e+02 - logprior: 1.9582
Epoch 7/10
25/25 - 44s - loss: 683.7964 - loglik: -6.8503e+02 - logprior: 2.2656
Epoch 8/10
25/25 - 45s - loss: 683.6594 - loglik: -6.8524e+02 - logprior: 2.5543
Epoch 9/10
25/25 - 44s - loss: 684.2011 - loglik: -6.8613e+02 - logprior: 2.8427
Fitted a model with MAP estimate = -681.3577
Time for alignment: 907.5060
Computed alignments with likelihoods: ['-689.3159', '-681.4424', '-681.3577']
Best model has likelihood: -681.3577
time for generating output: 0.4792
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.9222262183060601
Training of 3 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fe30b970>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f199d1cde20>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1817b849d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1014738580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1833b13970>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f15ec7334c0>, <__main__.SimpleDirichletPrior object at 0x7f15ecddbd60>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f180f5495e0>

Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 326.5529 - loglik: -3.2196e+02 - logprior: -4.5909e+00
Epoch 2/10
16/16 - 5s - loss: 256.2095 - loglik: -2.5464e+02 - logprior: -1.5574e+00
Epoch 3/10
16/16 - 4s - loss: 225.2518 - loglik: -2.2319e+02 - logprior: -1.9033e+00
Epoch 4/10
16/16 - 4s - loss: 213.2652 - loglik: -2.1079e+02 - logprior: -1.9838e+00
Epoch 5/10
16/16 - 4s - loss: 206.0172 - loglik: -2.0347e+02 - logprior: -1.9083e+00
Epoch 6/10
16/16 - 5s - loss: 200.7727 - loglik: -1.9816e+02 - logprior: -1.9615e+00
Epoch 7/10
16/16 - 4s - loss: 199.7981 - loglik: -1.9721e+02 - logprior: -1.9556e+00
Epoch 8/10
16/16 - 4s - loss: 197.8599 - loglik: -1.9538e+02 - logprior: -1.9523e+00
Epoch 9/10
16/16 - 5s - loss: 198.2016 - loglik: -1.9572e+02 - logprior: -1.9512e+00
Fitted a model with MAP estimate = -197.0590
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 1), (70, 1), (76, 1), (97, 1), (98, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 209.3988 - loglik: -2.0430e+02 - logprior: -4.3579e+00
Epoch 2/2
33/33 - 6s - loss: 196.6596 - loglik: -1.9378e+02 - logprior: -2.3635e+00
Fitted a model with MAP estimate = -193.0531
expansions: [(0, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 197.9381 - loglik: -1.9439e+02 - logprior: -3.2614e+00
Epoch 2/2
33/33 - 6s - loss: 192.7754 - loglik: -1.9089e+02 - logprior: -1.5839e+00
Fitted a model with MAP estimate = -191.1154
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 13s - loss: 197.2329 - loglik: -1.9387e+02 - logprior: -3.0967e+00
Epoch 2/10
33/33 - 6s - loss: 191.9681 - loglik: -1.9027e+02 - logprior: -1.4045e+00
Epoch 3/10
33/33 - 6s - loss: 191.4186 - loglik: -1.8970e+02 - logprior: -1.3277e+00
Epoch 4/10
33/33 - 7s - loss: 189.4825 - loglik: -1.8784e+02 - logprior: -1.2178e+00
Epoch 5/10
33/33 - 6s - loss: 188.3300 - loglik: -1.8678e+02 - logprior: -1.1334e+00
Epoch 6/10
33/33 - 6s - loss: 187.7724 - loglik: -1.8631e+02 - logprior: -1.0597e+00
Epoch 7/10
33/33 - 6s - loss: 187.8799 - loglik: -1.8647e+02 - logprior: -9.7720e-01
Fitted a model with MAP estimate = -186.7425
Time for alignment: 154.4305
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 326.6815 - loglik: -3.2210e+02 - logprior: -4.5766e+00
Epoch 2/10
16/16 - 4s - loss: 255.0633 - loglik: -2.5350e+02 - logprior: -1.5558e+00
Epoch 3/10
16/16 - 4s - loss: 220.4991 - loglik: -2.1838e+02 - logprior: -1.9840e+00
Epoch 4/10
16/16 - 5s - loss: 211.0196 - loglik: -2.0864e+02 - logprior: -1.9885e+00
Epoch 5/10
16/16 - 4s - loss: 205.1845 - loglik: -2.0274e+02 - logprior: -1.9111e+00
Epoch 6/10
16/16 - 5s - loss: 202.5940 - loglik: -2.0005e+02 - logprior: -1.9224e+00
Epoch 7/10
16/16 - 5s - loss: 197.3984 - loglik: -1.9492e+02 - logprior: -1.9319e+00
Epoch 8/10
16/16 - 5s - loss: 197.9826 - loglik: -1.9557e+02 - logprior: -1.9089e+00
Fitted a model with MAP estimate = -197.0691
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (69, 1), (82, 1), (97, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 215.0665 - loglik: -2.0857e+02 - logprior: -5.8562e+00
Epoch 2/2
16/16 - 5s - loss: 200.1549 - loglik: -1.9637e+02 - logprior: -3.0137e+00
Fitted a model with MAP estimate = -196.8991
expansions: [(0, 2), (19, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 198.6934 - loglik: -1.9511e+02 - logprior: -3.2033e+00
Epoch 2/2
33/33 - 6s - loss: 192.3615 - loglik: -1.9044e+02 - logprior: -1.6347e+00
Fitted a model with MAP estimate = -190.9353
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 196.0728 - loglik: -1.9267e+02 - logprior: -3.1615e+00
Epoch 2/10
33/33 - 7s - loss: 192.2615 - loglik: -1.9050e+02 - logprior: -1.4834e+00
Epoch 3/10
33/33 - 7s - loss: 191.8362 - loglik: -1.9006e+02 - logprior: -1.3935e+00
Epoch 4/10
33/33 - 6s - loss: 188.4617 - loglik: -1.8674e+02 - logprior: -1.2973e+00
Epoch 5/10
33/33 - 7s - loss: 187.3699 - loglik: -1.8574e+02 - logprior: -1.2101e+00
Epoch 6/10
33/33 - 7s - loss: 188.8069 - loglik: -1.8722e+02 - logprior: -1.1389e+00
Fitted a model with MAP estimate = -186.7756
Time for alignment: 139.5934
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 327.4056 - loglik: -3.2282e+02 - logprior: -4.5821e+00
Epoch 2/10
16/16 - 4s - loss: 254.5141 - loglik: -2.5295e+02 - logprior: -1.5602e+00
Epoch 3/10
16/16 - 4s - loss: 222.7577 - loglik: -2.2065e+02 - logprior: -1.9502e+00
Epoch 4/10
16/16 - 4s - loss: 211.1570 - loglik: -2.0876e+02 - logprior: -1.9803e+00
Epoch 5/10
16/16 - 5s - loss: 206.7275 - loglik: -2.0427e+02 - logprior: -1.8974e+00
Epoch 6/10
16/16 - 4s - loss: 203.2740 - loglik: -2.0078e+02 - logprior: -1.9008e+00
Epoch 7/10
16/16 - 4s - loss: 199.2120 - loglik: -1.9679e+02 - logprior: -1.8933e+00
Epoch 8/10
16/16 - 4s - loss: 200.1240 - loglik: -1.9774e+02 - logprior: -1.8914e+00
Fitted a model with MAP estimate = -199.2334
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 3), (72, 1), (97, 1), (98, 1)]
discards: [0 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 120 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 216.1807 - loglik: -2.0973e+02 - logprior: -5.8437e+00
Epoch 2/2
16/16 - 5s - loss: 201.3443 - loglik: -1.9760e+02 - logprior: -3.0020e+00
Fitted a model with MAP estimate = -199.5655
expansions: [(0, 2), (19, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 200.7041 - loglik: -1.9716e+02 - logprior: -3.1991e+00
Epoch 2/2
33/33 - 6s - loss: 194.7861 - loglik: -1.9289e+02 - logprior: -1.6039e+00
Fitted a model with MAP estimate = -192.4730
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 198.0772 - loglik: -1.9470e+02 - logprior: -3.1401e+00
Epoch 2/10
33/33 - 6s - loss: 194.0973 - loglik: -1.9235e+02 - logprior: -1.4557e+00
Epoch 3/10
33/33 - 6s - loss: 190.6552 - loglik: -1.8888e+02 - logprior: -1.3795e+00
Epoch 4/10
33/33 - 7s - loss: 190.9762 - loglik: -1.8928e+02 - logprior: -1.2666e+00
Fitted a model with MAP estimate = -188.5539
Time for alignment: 124.4955
Computed alignments with likelihoods: ['-186.7425', '-186.7756', '-188.5539']
Best model has likelihood: -186.7425
time for generating output: 0.3325
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.5365684195129087
Training of 3 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f10211440d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18a27b79a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f15ecbc7c70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1856549e20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1856549880>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f10216c64f0>, <__main__.SimpleDirichletPrior object at 0x7f1039a43df0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f180f5495e0>

Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.7296 - loglik: -3.0544e+02 - logprior: -3.1865e+00
Epoch 2/10
19/19 - 3s - loss: 278.1726 - loglik: -2.7628e+02 - logprior: -1.2253e+00
Epoch 3/10
19/19 - 3s - loss: 262.5645 - loglik: -2.5988e+02 - logprior: -1.5923e+00
Epoch 4/10
19/19 - 3s - loss: 257.5178 - loglik: -2.5494e+02 - logprior: -1.5672e+00
Epoch 5/10
19/19 - 3s - loss: 255.3687 - loglik: -2.5304e+02 - logprior: -1.5246e+00
Epoch 6/10
19/19 - 3s - loss: 254.2022 - loglik: -2.5196e+02 - logprior: -1.5239e+00
Epoch 7/10
19/19 - 3s - loss: 253.9225 - loglik: -2.5178e+02 - logprior: -1.5037e+00
Epoch 8/10
19/19 - 3s - loss: 252.5552 - loglik: -2.5045e+02 - logprior: -1.4888e+00
Epoch 9/10
19/19 - 3s - loss: 253.0262 - loglik: -2.5097e+02 - logprior: -1.4779e+00
Fitted a model with MAP estimate = -241.1374
expansions: [(6, 3), (7, 2), (10, 2), (21, 2), (33, 10), (38, 2), (43, 2), (55, 2), (58, 2), (60, 1), (63, 2), (65, 2), (66, 1), (67, 1), (69, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 264.4162 - loglik: -2.5949e+02 - logprior: -4.1610e+00
Epoch 2/2
19/19 - 3s - loss: 249.1464 - loglik: -2.4551e+02 - logprior: -2.4252e+00
Fitted a model with MAP estimate = -232.0221
expansions: [(0, 2)]
discards: [ 0  9 14 28 45 46 47 48 49 50 57 83 91 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 252.0764 - loglik: -2.4755e+02 - logprior: -3.0885e+00
Epoch 2/2
19/19 - 3s - loss: 247.1719 - loglik: -2.4461e+02 - logprior: -1.3012e+00
Fitted a model with MAP estimate = -231.2892
expansions: [(43, 7)]
discards: [ 0 68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 235.9383 - loglik: -2.3168e+02 - logprior: -2.7731e+00
Epoch 2/10
23/23 - 4s - loss: 230.8839 - loglik: -2.2822e+02 - logprior: -1.2967e+00
Epoch 3/10
23/23 - 4s - loss: 228.4143 - loglik: -2.2591e+02 - logprior: -1.1936e+00
Epoch 4/10
23/23 - 4s - loss: 228.1920 - loglik: -2.2577e+02 - logprior: -1.1570e+00
Epoch 5/10
23/23 - 4s - loss: 227.0423 - loglik: -2.2475e+02 - logprior: -1.1418e+00
Epoch 6/10
23/23 - 4s - loss: 226.4580 - loglik: -2.2430e+02 - logprior: -1.1197e+00
Epoch 7/10
23/23 - 4s - loss: 225.5304 - loglik: -2.2347e+02 - logprior: -1.1038e+00
Epoch 8/10
23/23 - 4s - loss: 225.7984 - loglik: -2.2378e+02 - logprior: -1.0974e+00
Fitted a model with MAP estimate = -223.9807
Time for alignment: 106.5078
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.6355 - loglik: -3.0535e+02 - logprior: -3.1861e+00
Epoch 2/10
19/19 - 3s - loss: 275.7791 - loglik: -2.7391e+02 - logprior: -1.2218e+00
Epoch 3/10
19/19 - 3s - loss: 259.5867 - loglik: -2.5710e+02 - logprior: -1.6145e+00
Epoch 4/10
19/19 - 3s - loss: 256.5870 - loglik: -2.5420e+02 - logprior: -1.5515e+00
Epoch 5/10
19/19 - 3s - loss: 255.0680 - loglik: -2.5288e+02 - logprior: -1.4897e+00
Epoch 6/10
19/19 - 3s - loss: 254.6088 - loglik: -2.5252e+02 - logprior: -1.4596e+00
Epoch 7/10
19/19 - 3s - loss: 253.9668 - loglik: -2.5192e+02 - logprior: -1.4350e+00
Epoch 8/10
19/19 - 3s - loss: 253.5116 - loglik: -2.5151e+02 - logprior: -1.4298e+00
Epoch 9/10
19/19 - 3s - loss: 252.9596 - loglik: -2.5100e+02 - logprior: -1.4207e+00
Epoch 10/10
19/19 - 3s - loss: 253.3379 - loglik: -2.5138e+02 - logprior: -1.4168e+00
Fitted a model with MAP estimate = -240.5771
expansions: [(6, 3), (7, 2), (10, 2), (39, 9), (58, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 261.2104 - loglik: -2.5733e+02 - logprior: -3.1456e+00
Epoch 2/2
19/19 - 3s - loss: 249.2200 - loglik: -2.4670e+02 - logprior: -1.3568e+00
Fitted a model with MAP estimate = -232.0633
expansions: []
discards: [ 0 15 55 56 75 81 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.3642 - loglik: -2.4986e+02 - logprior: -4.0946e+00
Epoch 2/2
19/19 - 3s - loss: 250.8442 - loglik: -2.4734e+02 - logprior: -2.2675e+00
Fitted a model with MAP estimate = -233.9735
expansions: [(0, 2), (54, 1)]
discards: [0 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 98 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 234.6477 - loglik: -2.3108e+02 - logprior: -2.0956e+00
Epoch 2/10
23/23 - 3s - loss: 231.7083 - loglik: -2.2920e+02 - logprior: -1.1266e+00
Epoch 3/10
23/23 - 3s - loss: 230.4845 - loglik: -2.2809e+02 - logprior: -1.1137e+00
Epoch 4/10
23/23 - 4s - loss: 228.7581 - loglik: -2.2640e+02 - logprior: -1.0604e+00
Epoch 5/10
23/23 - 3s - loss: 228.6014 - loglik: -2.2638e+02 - logprior: -1.0494e+00
Epoch 6/10
23/23 - 4s - loss: 227.9217 - loglik: -2.2584e+02 - logprior: -1.0305e+00
Epoch 7/10
23/23 - 4s - loss: 227.0074 - loglik: -2.2502e+02 - logprior: -1.0163e+00
Epoch 8/10
23/23 - 4s - loss: 226.8039 - loglik: -2.2489e+02 - logprior: -1.0047e+00
Epoch 9/10
23/23 - 3s - loss: 226.5674 - loglik: -2.2474e+02 - logprior: -9.9035e-01
Epoch 10/10
23/23 - 4s - loss: 225.7865 - loglik: -2.2403e+02 - logprior: -9.7637e-01
Fitted a model with MAP estimate = -225.0256
Time for alignment: 112.4969
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.4655 - loglik: -3.0518e+02 - logprior: -3.1850e+00
Epoch 2/10
19/19 - 3s - loss: 276.9707 - loglik: -2.7509e+02 - logprior: -1.2188e+00
Epoch 3/10
19/19 - 3s - loss: 260.7559 - loglik: -2.5809e+02 - logprior: -1.5876e+00
Epoch 4/10
19/19 - 3s - loss: 256.7896 - loglik: -2.5427e+02 - logprior: -1.5707e+00
Epoch 5/10
19/19 - 3s - loss: 254.7378 - loglik: -2.5245e+02 - logprior: -1.5319e+00
Epoch 6/10
19/19 - 3s - loss: 253.8066 - loglik: -2.5162e+02 - logprior: -1.5113e+00
Epoch 7/10
19/19 - 3s - loss: 253.4027 - loglik: -2.5127e+02 - logprior: -1.4908e+00
Epoch 8/10
19/19 - 3s - loss: 253.1083 - loglik: -2.5103e+02 - logprior: -1.4755e+00
Epoch 9/10
19/19 - 3s - loss: 252.5798 - loglik: -2.5052e+02 - logprior: -1.4746e+00
Epoch 10/10
19/19 - 3s - loss: 252.7788 - loglik: -2.5075e+02 - logprior: -1.4724e+00
Fitted a model with MAP estimate = -240.8434
expansions: [(6, 3), (7, 2), (10, 2), (21, 2), (33, 10), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 261.4796 - loglik: -2.5758e+02 - logprior: -3.1886e+00
Epoch 2/2
19/19 - 3s - loss: 247.4211 - loglik: -2.4481e+02 - logprior: -1.4711e+00
Fitted a model with MAP estimate = -230.6724
expansions: []
discards: [ 0 15 29 46 47 48 49 50 51 58 83 89 91]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 254.3188 - loglik: -2.4881e+02 - logprior: -4.1230e+00
Epoch 2/2
19/19 - 3s - loss: 249.3499 - loglik: -2.4582e+02 - logprior: -2.2963e+00
Fitted a model with MAP estimate = -233.0922
expansions: [(0, 2), (43, 7)]
discards: [0 9]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 106 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 233.9904 - loglik: -2.3038e+02 - logprior: -2.1396e+00
Epoch 2/10
23/23 - 4s - loss: 229.5271 - loglik: -2.2698e+02 - logprior: -1.1528e+00
Epoch 3/10
23/23 - 4s - loss: 229.1065 - loglik: -2.2666e+02 - logprior: -1.1398e+00
Epoch 4/10
23/23 - 4s - loss: 227.4368 - loglik: -2.2506e+02 - logprior: -1.0927e+00
Epoch 5/10
23/23 - 4s - loss: 226.5845 - loglik: -2.2435e+02 - logprior: -1.0783e+00
Epoch 6/10
23/23 - 4s - loss: 226.1437 - loglik: -2.2405e+02 - logprior: -1.0610e+00
Epoch 7/10
23/23 - 4s - loss: 225.2789 - loglik: -2.2327e+02 - logprior: -1.0447e+00
Epoch 8/10
23/23 - 4s - loss: 224.9707 - loglik: -2.2303e+02 - logprior: -1.0296e+00
Epoch 9/10
23/23 - 4s - loss: 224.8760 - loglik: -2.2300e+02 - logprior: -1.0157e+00
Epoch 10/10
23/23 - 4s - loss: 223.7065 - loglik: -2.2188e+02 - logprior: -9.9993e-01
Fitted a model with MAP estimate = -223.0654
Time for alignment: 115.7222
Computed alignments with likelihoods: ['-223.9807', '-225.0256', '-223.0654']
Best model has likelihood: -223.0654
time for generating output: 0.1973
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.7299349240780911
Training of 3 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1934da76d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1934da7af0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19590eeca0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f19590ee070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1000b591f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1000a198b0>, <__main__.SimpleDirichletPrior object at 0x7f1817aac580>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1028283940>

Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 433.3581 - loglik: -3.8710e+02 - logprior: -4.6229e+01
Epoch 2/10
10/10 - 2s - loss: 370.7990 - loglik: -3.5965e+02 - logprior: -1.1027e+01
Epoch 3/10
10/10 - 2s - loss: 333.1379 - loglik: -3.2817e+02 - logprior: -4.8572e+00
Epoch 4/10
10/10 - 2s - loss: 310.7544 - loglik: -3.0766e+02 - logprior: -2.9606e+00
Epoch 5/10
10/10 - 2s - loss: 302.2084 - loglik: -2.9968e+02 - logprior: -2.1788e+00
Epoch 6/10
10/10 - 2s - loss: 298.5406 - loglik: -2.9639e+02 - logprior: -1.7289e+00
Epoch 7/10
10/10 - 2s - loss: 296.8520 - loglik: -2.9512e+02 - logprior: -1.3555e+00
Epoch 8/10
10/10 - 2s - loss: 295.2007 - loglik: -2.9373e+02 - logprior: -1.1046e+00
Epoch 9/10
10/10 - 2s - loss: 295.1740 - loglik: -2.9378e+02 - logprior: -1.0092e+00
Epoch 10/10
10/10 - 2s - loss: 294.3294 - loglik: -2.9300e+02 - logprior: -9.3054e-01
Fitted a model with MAP estimate = -293.9688
expansions: [(7, 2), (10, 1), (17, 1), (18, 1), (29, 3), (43, 1), (49, 2), (59, 4), (61, 1), (73, 1), (85, 1), (90, 2), (91, 3), (92, 2), (93, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 347.8112 - loglik: -2.9547e+02 - logprior: -5.1950e+01
Epoch 2/2
10/10 - 3s - loss: 306.5158 - loglik: -2.8579e+02 - logprior: -2.0260e+01
Fitted a model with MAP estimate = -298.5202
expansions: [(9, 3)]
discards: [ 57  70 108]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 332.0275 - loglik: -2.8185e+02 - logprior: -4.9621e+01
Epoch 2/2
10/10 - 3s - loss: 293.4259 - loglik: -2.7857e+02 - logprior: -1.4338e+01
Fitted a model with MAP estimate = -284.0927
expansions: []
discards: [38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 319.7310 - loglik: -2.7817e+02 - logprior: -4.1039e+01
Epoch 2/10
10/10 - 3s - loss: 286.6168 - loglik: -2.7726e+02 - logprior: -8.8609e+00
Epoch 3/10
10/10 - 3s - loss: 279.5168 - loglik: -2.7678e+02 - logprior: -2.2935e+00
Epoch 4/10
10/10 - 3s - loss: 275.9756 - loglik: -2.7595e+02 - logprior: 0.3654
Epoch 5/10
10/10 - 3s - loss: 274.4310 - loglik: -2.7583e+02 - logprior: 1.7426
Epoch 6/10
10/10 - 3s - loss: 274.0008 - loglik: -2.7615e+02 - logprior: 2.4764
Epoch 7/10
10/10 - 3s - loss: 272.7048 - loglik: -2.7536e+02 - logprior: 2.9785
Epoch 8/10
10/10 - 3s - loss: 272.8863 - loglik: -2.7600e+02 - logprior: 3.4306
Fitted a model with MAP estimate = -272.0417
Time for alignment: 74.9838
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 432.7309 - loglik: -3.8647e+02 - logprior: -4.6231e+01
Epoch 2/10
10/10 - 2s - loss: 371.3499 - loglik: -3.6019e+02 - logprior: -1.1033e+01
Epoch 3/10
10/10 - 2s - loss: 336.0334 - loglik: -3.3099e+02 - logprior: -4.9254e+00
Epoch 4/10
10/10 - 2s - loss: 314.3040 - loglik: -3.1105e+02 - logprior: -3.0685e+00
Epoch 5/10
10/10 - 2s - loss: 303.2616 - loglik: -3.0064e+02 - logprior: -2.1769e+00
Epoch 6/10
10/10 - 2s - loss: 298.6148 - loglik: -2.9648e+02 - logprior: -1.6584e+00
Epoch 7/10
10/10 - 2s - loss: 297.1552 - loglik: -2.9548e+02 - logprior: -1.3078e+00
Epoch 8/10
10/10 - 2s - loss: 295.6531 - loglik: -2.9422e+02 - logprior: -1.1283e+00
Epoch 9/10
10/10 - 2s - loss: 295.0275 - loglik: -2.9374e+02 - logprior: -9.8411e-01
Epoch 10/10
10/10 - 2s - loss: 294.6740 - loglik: -2.9346e+02 - logprior: -8.8981e-01
Fitted a model with MAP estimate = -294.2040
expansions: [(11, 4), (12, 1), (13, 1), (18, 1), (27, 1), (28, 3), (42, 1), (59, 3), (62, 1), (71, 1), (73, 1), (85, 1), (90, 2), (91, 6)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 348.5823 - loglik: -2.9637e+02 - logprior: -5.1839e+01
Epoch 2/2
10/10 - 3s - loss: 305.1730 - loglik: -2.8461e+02 - logprior: -2.0063e+01
Fitted a model with MAP estimate = -296.2627
expansions: [(10, 1)]
discards: [ 71 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 330.7632 - loglik: -2.8081e+02 - logprior: -4.9328e+01
Epoch 2/2
10/10 - 3s - loss: 292.5670 - loglik: -2.7804e+02 - logprior: -1.3954e+01
Fitted a model with MAP estimate = -283.6764
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 319.2719 - loglik: -2.7778e+02 - logprior: -4.0924e+01
Epoch 2/10
10/10 - 3s - loss: 286.2008 - loglik: -2.7687e+02 - logprior: -8.8064e+00
Epoch 3/10
10/10 - 3s - loss: 278.8988 - loglik: -2.7618e+02 - logprior: -2.2674e+00
Epoch 4/10
10/10 - 3s - loss: 276.2042 - loglik: -2.7619e+02 - logprior: 0.3967
Epoch 5/10
10/10 - 3s - loss: 274.0771 - loglik: -2.7548e+02 - logprior: 1.7606
Epoch 6/10
10/10 - 3s - loss: 273.7028 - loglik: -2.7585e+02 - logprior: 2.4886
Epoch 7/10
10/10 - 3s - loss: 272.8339 - loglik: -2.7550e+02 - logprior: 2.9964
Epoch 8/10
10/10 - 3s - loss: 272.1501 - loglik: -2.7528e+02 - logprior: 3.4612
Epoch 9/10
10/10 - 3s - loss: 272.1492 - loglik: -2.7566e+02 - logprior: 3.8372
Epoch 10/10
10/10 - 3s - loss: 271.5610 - loglik: -2.7535e+02 - logprior: 4.1192
Fitted a model with MAP estimate = -271.1395
Time for alignment: 80.3609
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 433.0662 - loglik: -3.8681e+02 - logprior: -4.6230e+01
Epoch 2/10
10/10 - 2s - loss: 371.6501 - loglik: -3.6051e+02 - logprior: -1.1011e+01
Epoch 3/10
10/10 - 2s - loss: 334.9704 - loglik: -3.3010e+02 - logprior: -4.7394e+00
Epoch 4/10
10/10 - 2s - loss: 311.4559 - loglik: -3.0847e+02 - logprior: -2.8535e+00
Epoch 5/10
10/10 - 2s - loss: 302.7288 - loglik: -3.0026e+02 - logprior: -2.1229e+00
Epoch 6/10
10/10 - 2s - loss: 298.5056 - loglik: -2.9642e+02 - logprior: -1.6310e+00
Epoch 7/10
10/10 - 2s - loss: 296.9499 - loglik: -2.9529e+02 - logprior: -1.2293e+00
Epoch 8/10
10/10 - 2s - loss: 295.7869 - loglik: -2.9445e+02 - logprior: -9.3767e-01
Epoch 9/10
10/10 - 2s - loss: 295.3467 - loglik: -2.9415e+02 - logprior: -7.8923e-01
Epoch 10/10
10/10 - 2s - loss: 294.4682 - loglik: -2.9335e+02 - logprior: -6.9872e-01
Fitted a model with MAP estimate = -294.2098
expansions: [(10, 4), (17, 1), (18, 1), (29, 3), (42, 2), (48, 1), (59, 3), (62, 1), (65, 1), (73, 1), (91, 2), (92, 6), (93, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 348.8391 - loglik: -2.9662e+02 - logprior: -5.1809e+01
Epoch 2/2
10/10 - 3s - loss: 305.5213 - loglik: -2.8490e+02 - logprior: -2.0131e+01
Fitted a model with MAP estimate = -297.3533
expansions: [(10, 2)]
discards: [ 71 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 331.4846 - loglik: -2.8138e+02 - logprior: -4.9516e+01
Epoch 2/2
10/10 - 3s - loss: 292.5708 - loglik: -2.7785e+02 - logprior: -1.4173e+01
Fitted a model with MAP estimate = -283.4047
expansions: []
discards: [37 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 319.5946 - loglik: -2.7810e+02 - logprior: -4.0945e+01
Epoch 2/10
10/10 - 3s - loss: 286.4851 - loglik: -2.7715e+02 - logprior: -8.7987e+00
Epoch 3/10
10/10 - 3s - loss: 279.4080 - loglik: -2.7670e+02 - logprior: -2.2384e+00
Epoch 4/10
10/10 - 3s - loss: 276.0414 - loglik: -2.7605e+02 - logprior: 0.4129
Epoch 5/10
10/10 - 3s - loss: 274.0230 - loglik: -2.7546e+02 - logprior: 1.7886
Epoch 6/10
10/10 - 3s - loss: 273.9727 - loglik: -2.7616e+02 - logprior: 2.5196
Epoch 7/10
10/10 - 3s - loss: 272.4734 - loglik: -2.7516e+02 - logprior: 3.0201
Epoch 8/10
10/10 - 3s - loss: 272.6793 - loglik: -2.7582e+02 - logprior: 3.4741
Fitted a model with MAP estimate = -271.8372
Time for alignment: 76.7060
Computed alignments with likelihoods: ['-272.0417', '-271.1395', '-271.8372']
Best model has likelihood: -271.1395
time for generating output: 0.2017
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.9017788089713844
Training of 3 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19fed1e7f0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f193ed6a6a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f188105e6a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1817ed5460>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fdedcb80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19470859d0>, <__main__.SimpleDirichletPrior object at 0x7f180db5fe20>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19485545e0>

Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 455.1663 - loglik: -1.8612e+02 - logprior: -2.6904e+02
Epoch 2/10
10/10 - 1s - loss: 232.4902 - loglik: -1.6043e+02 - logprior: -7.2058e+01
Epoch 3/10
10/10 - 1s - loss: 172.7882 - loglik: -1.3991e+02 - logprior: -3.2881e+01
Epoch 4/10
10/10 - 1s - loss: 144.8600 - loglik: -1.2604e+02 - logprior: -1.8823e+01
Epoch 5/10
10/10 - 1s - loss: 131.2326 - loglik: -1.2009e+02 - logprior: -1.1140e+01
Epoch 6/10
10/10 - 1s - loss: 123.4703 - loglik: -1.1727e+02 - logprior: -6.0606e+00
Epoch 7/10
10/10 - 1s - loss: 118.5020 - loglik: -1.1526e+02 - logprior: -2.8447e+00
Epoch 8/10
10/10 - 1s - loss: 115.6979 - loglik: -1.1462e+02 - logprior: -7.0675e-01
Epoch 9/10
10/10 - 1s - loss: 114.0852 - loglik: -1.1468e+02 - logprior: 0.8899
Epoch 10/10
10/10 - 1s - loss: 112.9382 - loglik: -1.1469e+02 - logprior: 2.0714
Fitted a model with MAP estimate = -112.1038
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 467.5982 - loglik: -1.0966e+02 - logprior: -3.5768e+02
Epoch 2/2
10/10 - 1s - loss: 206.6830 - loglik: -9.7138e+01 - logprior: -1.0941e+02
Fitted a model with MAP estimate = -158.0099
expansions: []
discards: [ 0 21 37 48 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 400.4959 - loglik: -9.7193e+01 - logprior: -3.0317e+02
Epoch 2/2
10/10 - 1s - loss: 212.6907 - loglik: -9.5571e+01 - logprior: -1.1705e+02
Fitted a model with MAP estimate = -182.7579
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 372.3291 - loglik: -9.4720e+01 - logprior: -2.7754e+02
Epoch 2/10
10/10 - 1s - loss: 169.8931 - loglik: -9.4084e+01 - logprior: -7.5756e+01
Epoch 3/10
10/10 - 1s - loss: 119.1082 - loglik: -9.4158e+01 - logprior: -2.4913e+01
Epoch 4/10
10/10 - 1s - loss: 100.8028 - loglik: -9.4212e+01 - logprior: -6.4423e+00
Epoch 5/10
10/10 - 1s - loss: 91.4939 - loglik: -9.4181e+01 - logprior: 2.9703
Epoch 6/10
10/10 - 1s - loss: 86.0514 - loglik: -9.4294e+01 - logprior: 8.5278
Epoch 7/10
10/10 - 1s - loss: 82.6304 - loglik: -9.4447e+01 - logprior: 12.0987
Epoch 8/10
10/10 - 1s - loss: 80.2791 - loglik: -9.4643e+01 - logprior: 14.6424
Epoch 9/10
10/10 - 1s - loss: 78.4737 - loglik: -9.4795e+01 - logprior: 16.5923
Epoch 10/10
10/10 - 1s - loss: 76.9780 - loglik: -9.4883e+01 - logprior: 18.1849
Fitted a model with MAP estimate = -75.9520
Time for alignment: 37.0688
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 455.1665 - loglik: -1.8612e+02 - logprior: -2.6904e+02
Epoch 2/10
10/10 - 1s - loss: 232.4904 - loglik: -1.6043e+02 - logprior: -7.2059e+01
Epoch 3/10
10/10 - 1s - loss: 172.7876 - loglik: -1.3991e+02 - logprior: -3.2880e+01
Epoch 4/10
10/10 - 1s - loss: 144.8596 - loglik: -1.2604e+02 - logprior: -1.8823e+01
Epoch 5/10
10/10 - 1s - loss: 131.1845 - loglik: -1.2002e+02 - logprior: -1.1160e+01
Epoch 6/10
10/10 - 1s - loss: 123.3536 - loglik: -1.1710e+02 - logprior: -6.0968e+00
Epoch 7/10
10/10 - 1s - loss: 118.3707 - loglik: -1.1512e+02 - logprior: -2.8867e+00
Epoch 8/10
10/10 - 1s - loss: 115.5542 - loglik: -1.1449e+02 - logprior: -7.1002e-01
Epoch 9/10
10/10 - 1s - loss: 113.9521 - loglik: -1.1454e+02 - logprior: 0.8791
Epoch 10/10
10/10 - 1s - loss: 112.8788 - loglik: -1.1468e+02 - logprior: 2.0831
Fitted a model with MAP estimate = -112.1143
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 467.6506 - loglik: -1.0973e+02 - logprior: -3.5767e+02
Epoch 2/2
10/10 - 1s - loss: 206.7083 - loglik: -9.7191e+01 - logprior: -1.0940e+02
Fitted a model with MAP estimate = -158.0644
expansions: []
discards: [ 0 21 37 48 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 400.4868 - loglik: -9.7231e+01 - logprior: -3.0317e+02
Epoch 2/2
10/10 - 1s - loss: 212.6957 - loglik: -9.5631e+01 - logprior: -1.1702e+02
Fitted a model with MAP estimate = -182.8000
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 372.3135 - loglik: -9.4738e+01 - logprior: -2.7753e+02
Epoch 2/10
10/10 - 1s - loss: 169.8986 - loglik: -9.4116e+01 - logprior: -7.5742e+01
Epoch 3/10
10/10 - 1s - loss: 119.1121 - loglik: -9.4200e+01 - logprior: -2.4882e+01
Epoch 4/10
10/10 - 1s - loss: 100.7565 - loglik: -9.4142e+01 - logprior: -6.4742e+00
Epoch 5/10
10/10 - 1s - loss: 91.4188 - loglik: -9.4109e+01 - logprior: 2.9667
Epoch 6/10
10/10 - 1s - loss: 86.0187 - loglik: -9.4300e+01 - logprior: 8.5644
Epoch 7/10
10/10 - 1s - loss: 82.6159 - loglik: -9.4491e+01 - logprior: 12.1385
Epoch 8/10
10/10 - 1s - loss: 80.2599 - loglik: -9.4638e+01 - logprior: 14.6591
Epoch 9/10
10/10 - 1s - loss: 78.4561 - loglik: -9.4779e+01 - logprior: 16.6056
Epoch 10/10
10/10 - 1s - loss: 76.9596 - loglik: -9.4892e+01 - logprior: 18.2146
Fitted a model with MAP estimate = -75.9296
Time for alignment: 36.7447
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 455.1663 - loglik: -1.8612e+02 - logprior: -2.6904e+02
Epoch 2/10
10/10 - 1s - loss: 232.4906 - loglik: -1.6043e+02 - logprior: -7.2059e+01
Epoch 3/10
10/10 - 1s - loss: 172.7878 - loglik: -1.3991e+02 - logprior: -3.2881e+01
Epoch 4/10
10/10 - 1s - loss: 144.8599 - loglik: -1.2604e+02 - logprior: -1.8823e+01
Epoch 5/10
10/10 - 1s - loss: 131.2549 - loglik: -1.2012e+02 - logprior: -1.1128e+01
Epoch 6/10
10/10 - 1s - loss: 123.4741 - loglik: -1.1735e+02 - logprior: -6.0321e+00
Epoch 7/10
10/10 - 1s - loss: 118.3893 - loglik: -1.1520e+02 - logprior: -2.8562e+00
Epoch 8/10
10/10 - 1s - loss: 115.5656 - loglik: -1.1449e+02 - logprior: -7.0927e-01
Epoch 9/10
10/10 - 1s - loss: 113.9556 - loglik: -1.1453e+02 - logprior: 0.8732
Epoch 10/10
10/10 - 1s - loss: 112.8799 - loglik: -1.1467e+02 - logprior: 2.0715
Fitted a model with MAP estimate = -112.1143
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 467.6263 - loglik: -1.0970e+02 - logprior: -3.5768e+02
Epoch 2/2
10/10 - 1s - loss: 206.6871 - loglik: -9.7149e+01 - logprior: -1.0941e+02
Fitted a model with MAP estimate = -158.0288
expansions: []
discards: [ 0 21 37 48 53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 400.4896 - loglik: -9.7205e+01 - logprior: -3.0317e+02
Epoch 2/2
10/10 - 1s - loss: 212.6904 - loglik: -9.5590e+01 - logprior: -1.1704e+02
Fitted a model with MAP estimate = -182.7659
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 372.3188 - loglik: -9.4719e+01 - logprior: -2.7753e+02
Epoch 2/10
10/10 - 1s - loss: 169.8924 - loglik: -9.4088e+01 - logprior: -7.5754e+01
Epoch 3/10
10/10 - 1s - loss: 119.1122 - loglik: -9.4170e+01 - logprior: -2.4905e+01
Epoch 4/10
10/10 - 1s - loss: 100.8142 - loglik: -9.4233e+01 - logprior: -6.4302e+00
Epoch 5/10
10/10 - 1s - loss: 91.5180 - loglik: -9.4221e+01 - logprior: 2.9910
Epoch 6/10
10/10 - 1s - loss: 86.0827 - loglik: -9.4357e+01 - logprior: 8.5591
Epoch 7/10
10/10 - 1s - loss: 82.6298 - loglik: -9.4461e+01 - logprior: 12.1166
Epoch 8/10
10/10 - 1s - loss: 80.2718 - loglik: -9.4650e+01 - logprior: 14.6616
Epoch 9/10
10/10 - 1s - loss: 78.4582 - loglik: -9.4816e+01 - logprior: 16.6287
Epoch 10/10
10/10 - 1s - loss: 76.9571 - loglik: -9.4911e+01 - logprior: 18.2308
Fitted a model with MAP estimate = -75.9300
Time for alignment: 36.4224
Computed alignments with likelihoods: ['-75.9520', '-75.9296', '-75.9300']
Best model has likelihood: -75.9296
time for generating output: 0.1418
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.8474320241691843
Training of 3 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f100c7dff70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f15ecbd2550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f100c39d280>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1020d8bac0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f15eda861f0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1014102f40>, <__main__.SimpleDirichletPrior object at 0x7f1817ed5370>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f100c446f70>

Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 690.5986 - loglik: -6.8617e+02 - logprior: -4.3181e+00
Epoch 2/10
26/26 - 11s - loss: 588.2308 - loglik: -5.8678e+02 - logprior: -1.3413e+00
Epoch 3/10
26/26 - 11s - loss: 574.3835 - loglik: -5.7247e+02 - logprior: -1.4679e+00
Epoch 4/10
26/26 - 11s - loss: 569.7899 - loglik: -5.6775e+02 - logprior: -1.4850e+00
Epoch 5/10
26/26 - 11s - loss: 571.0841 - loglik: -5.6903e+02 - logprior: -1.4547e+00
Fitted a model with MAP estimate = -567.7684
expansions: [(11, 1), (43, 1), (94, 2), (161, 1), (174, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 206 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 578.0809 - loglik: -5.7256e+02 - logprior: -4.8535e+00
Epoch 2/2
26/26 - 12s - loss: 569.5549 - loglik: -5.6748e+02 - logprior: -1.4412e+00
Fitted a model with MAP estimate = -566.6079
expansions: []
discards: [96 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 14s - loss: 575.5149 - loglik: -5.7030e+02 - logprior: -4.6408e+00
Epoch 2/2
26/26 - 11s - loss: 570.7447 - loglik: -5.6899e+02 - logprior: -1.1388e+00
Fitted a model with MAP estimate = -567.5461
expansions: [(96, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 206 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 17s - loss: 575.0403 - loglik: -5.7005e+02 - logprior: -4.4225e+00
Epoch 2/10
26/26 - 11s - loss: 568.3795 - loglik: -5.6681e+02 - logprior: -9.5701e-01
Epoch 3/10
26/26 - 12s - loss: 565.5198 - loglik: -5.6414e+02 - logprior: -6.6163e-01
Epoch 4/10
26/26 - 11s - loss: 564.5722 - loglik: -5.6322e+02 - logprior: -5.9872e-01
Epoch 5/10
26/26 - 11s - loss: 563.8602 - loglik: -5.6265e+02 - logprior: -4.6592e-01
Epoch 6/10
26/26 - 11s - loss: 561.1893 - loglik: -5.6014e+02 - logprior: -3.2154e-01
Epoch 7/10
26/26 - 11s - loss: 561.3558 - loglik: -5.6045e+02 - logprior: -1.8362e-01
Fitted a model with MAP estimate = -560.3823
Time for alignment: 239.8614
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 691.2702 - loglik: -6.8684e+02 - logprior: -4.3101e+00
Epoch 2/10
26/26 - 11s - loss: 586.5956 - loglik: -5.8516e+02 - logprior: -1.3098e+00
Epoch 3/10
26/26 - 11s - loss: 571.3416 - loglik: -5.6949e+02 - logprior: -1.3932e+00
Epoch 4/10
26/26 - 11s - loss: 567.4941 - loglik: -5.6542e+02 - logprior: -1.4122e+00
Epoch 5/10
26/26 - 11s - loss: 564.8209 - loglik: -5.6264e+02 - logprior: -1.4661e+00
Epoch 6/10
26/26 - 11s - loss: 563.8809 - loglik: -5.6168e+02 - logprior: -1.4776e+00
Epoch 7/10
26/26 - 11s - loss: 563.3608 - loglik: -5.6116e+02 - logprior: -1.4943e+00
Epoch 8/10
26/26 - 11s - loss: 561.9962 - loglik: -5.5980e+02 - logprior: -1.4941e+00
Epoch 9/10
26/26 - 11s - loss: 562.2138 - loglik: -5.6005e+02 - logprior: -1.4887e+00
Fitted a model with MAP estimate = -561.4276
expansions: [(0, 3), (10, 1), (156, 1), (173, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 206 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 577.8204 - loglik: -5.7043e+02 - logprior: -6.5716e+00
Epoch 2/2
26/26 - 11s - loss: 565.8305 - loglik: -5.6318e+02 - logprior: -1.7168e+00
Fitted a model with MAP estimate = -562.5940
expansions: []
discards: [0 1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 16s - loss: 577.2242 - loglik: -5.6975e+02 - logprior: -6.7018e+00
Epoch 2/2
26/26 - 11s - loss: 568.1200 - loglik: -5.6556e+02 - logprior: -1.7695e+00
Fitted a model with MAP estimate = -564.5715
expansions: [(0, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 205 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 572.6774 - loglik: -5.6750e+02 - logprior: -4.4037e+00
Epoch 2/10
26/26 - 11s - loss: 566.4040 - loglik: -5.6473e+02 - logprior: -9.0451e-01
Epoch 3/10
26/26 - 11s - loss: 563.2328 - loglik: -5.6161e+02 - logprior: -7.0575e-01
Epoch 4/10
26/26 - 11s - loss: 561.5827 - loglik: -5.6010e+02 - logprior: -5.7565e-01
Epoch 5/10
26/26 - 11s - loss: 560.8238 - loglik: -5.5953e+02 - logprior: -4.5580e-01
Epoch 6/10
26/26 - 11s - loss: 558.1409 - loglik: -5.5703e+02 - logprior: -3.2315e-01
Epoch 7/10
26/26 - 11s - loss: 559.5140 - loglik: -5.5857e+02 - logprior: -1.9190e-01
Fitted a model with MAP estimate = -557.4672
Time for alignment: 282.7884
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 14s - loss: 690.9743 - loglik: -6.8658e+02 - logprior: -4.2833e+00
Epoch 2/10
26/26 - 11s - loss: 594.0096 - loglik: -5.9260e+02 - logprior: -1.3032e+00
Epoch 3/10
26/26 - 11s - loss: 576.1810 - loglik: -5.7432e+02 - logprior: -1.4659e+00
Epoch 4/10
26/26 - 11s - loss: 571.8383 - loglik: -5.6985e+02 - logprior: -1.4931e+00
Epoch 5/10
26/26 - 11s - loss: 569.0697 - loglik: -5.6700e+02 - logprior: -1.5132e+00
Epoch 6/10
26/26 - 11s - loss: 569.2236 - loglik: -5.6709e+02 - logprior: -1.5367e+00
Fitted a model with MAP estimate = -567.7640
expansions: [(43, 1), (108, 3), (141, 1), (173, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 206 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 580.7810 - loglik: -5.7525e+02 - logprior: -4.8770e+00
Epoch 2/2
26/26 - 11s - loss: 569.9459 - loglik: -5.6785e+02 - logprior: -1.4735e+00
Fitted a model with MAP estimate = -568.5612
expansions: [(165, 4)]
discards: [  0 110 111]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 207 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 580.5656 - loglik: -5.7323e+02 - logprior: -6.7889e+00
Epoch 2/2
26/26 - 12s - loss: 573.7743 - loglik: -5.6997e+02 - logprior: -3.1765e+00
Fitted a model with MAP estimate = -570.3620
expansions: [(0, 6)]
discards: [  0 164 165 166 167 168]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 207 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 579.5019 - loglik: -5.7446e+02 - logprior: -4.4787e+00
Epoch 2/10
26/26 - 12s - loss: 569.5437 - loglik: -5.6797e+02 - logprior: -9.5880e-01
Epoch 3/10
26/26 - 12s - loss: 568.0608 - loglik: -5.6662e+02 - logprior: -7.4065e-01
Epoch 4/10
26/26 - 12s - loss: 567.2728 - loglik: -5.6580e+02 - logprior: -7.3835e-01
Epoch 5/10
26/26 - 12s - loss: 564.3429 - loglik: -5.6298e+02 - logprior: -6.3056e-01
Epoch 6/10
26/26 - 12s - loss: 564.4659 - loglik: -5.6323e+02 - logprior: -5.3325e-01
Fitted a model with MAP estimate = -562.4755
Time for alignment: 240.3886
Computed alignments with likelihoods: ['-560.3823', '-557.4672', '-562.4755']
Best model has likelihood: -557.4672
time for generating output: 0.3422
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.6238709677419355
Training of 3 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f198c1e4550>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f180f91cd30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f180e1b2b20>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f197b496a90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f197b4968e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19c7c8d970>, <__main__.SimpleDirichletPrior object at 0x7f189149cf10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe58d820>

Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 422.8947 - loglik: -3.4497e+02 - logprior: -7.7897e+01
Epoch 2/10
10/10 - 2s - loss: 337.9657 - loglik: -3.1908e+02 - logprior: -1.8761e+01
Epoch 3/10
10/10 - 2s - loss: 299.5953 - loglik: -2.9186e+02 - logprior: -7.6311e+00
Epoch 4/10
10/10 - 2s - loss: 279.1892 - loglik: -2.7514e+02 - logprior: -3.9043e+00
Epoch 5/10
10/10 - 2s - loss: 271.2217 - loglik: -2.6873e+02 - logprior: -2.1217e+00
Epoch 6/10
10/10 - 2s - loss: 267.7446 - loglik: -2.6624e+02 - logprior: -1.0678e+00
Epoch 7/10
10/10 - 2s - loss: 265.4478 - loglik: -2.6476e+02 - logprior: -3.1633e-01
Epoch 8/10
10/10 - 2s - loss: 263.3268 - loglik: -2.6306e+02 - logprior: 0.1005
Epoch 9/10
10/10 - 2s - loss: 261.9161 - loglik: -2.6193e+02 - logprior: 0.3913
Epoch 10/10
10/10 - 2s - loss: 260.7674 - loglik: -2.6104e+02 - logprior: 0.6483
Fitted a model with MAP estimate = -260.3101
expansions: [(5, 1), (6, 1), (10, 2), (12, 4), (18, 2), (36, 4), (45, 3), (54, 2), (59, 1), (63, 4), (83, 3), (86, 1), (88, 4), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 347.7517 - loglik: -2.6039e+02 - logprior: -8.6976e+01
Epoch 2/2
10/10 - 2s - loss: 278.8867 - loglik: -2.4462e+02 - logprior: -3.3790e+01
Fitted a model with MAP estimate = -265.9930
expansions: [(0, 2)]
discards: [  0  46 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 310.6953 - loglik: -2.4145e+02 - logprior: -6.8639e+01
Epoch 2/2
10/10 - 2s - loss: 251.5633 - loglik: -2.3538e+02 - logprior: -1.5527e+01
Fitted a model with MAP estimate = -241.6604
expansions: [(116, 3)]
discards: [ 0 46 78]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 324.1475 - loglik: -2.3930e+02 - logprior: -8.4295e+01
Epoch 2/10
10/10 - 2s - loss: 263.2828 - loglik: -2.3508e+02 - logprior: -2.7719e+01
Epoch 3/10
10/10 - 2s - loss: 240.9162 - loglik: -2.3201e+02 - logprior: -8.3207e+00
Epoch 4/10
10/10 - 2s - loss: 231.7458 - loglik: -2.3116e+02 - logprior: -2.3696e-02
Epoch 5/10
10/10 - 2s - loss: 228.2086 - loglik: -2.3084e+02 - logprior: 3.1331
Epoch 6/10
10/10 - 2s - loss: 226.1118 - loglik: -2.3039e+02 - logprior: 4.7612
Epoch 7/10
10/10 - 2s - loss: 225.0530 - loglik: -2.3035e+02 - logprior: 5.7657
Epoch 8/10
10/10 - 2s - loss: 224.2211 - loglik: -2.3025e+02 - logprior: 6.4864
Epoch 9/10
10/10 - 2s - loss: 223.4189 - loglik: -2.3004e+02 - logprior: 7.0613
Epoch 10/10
10/10 - 2s - loss: 223.2635 - loglik: -2.3043e+02 - logprior: 7.6021
Fitted a model with MAP estimate = -222.3523
Time for alignment: 63.7278
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 423.2450 - loglik: -3.4532e+02 - logprior: -7.7897e+01
Epoch 2/10
10/10 - 2s - loss: 337.7291 - loglik: -3.1886e+02 - logprior: -1.8746e+01
Epoch 3/10
10/10 - 2s - loss: 297.8351 - loglik: -2.9015e+02 - logprior: -7.5855e+00
Epoch 4/10
10/10 - 2s - loss: 279.0601 - loglik: -2.7512e+02 - logprior: -3.8075e+00
Epoch 5/10
10/10 - 2s - loss: 269.8018 - loglik: -2.6732e+02 - logprior: -2.1317e+00
Epoch 6/10
10/10 - 2s - loss: 265.3349 - loglik: -2.6362e+02 - logprior: -1.2575e+00
Epoch 7/10
10/10 - 2s - loss: 263.3088 - loglik: -2.6243e+02 - logprior: -5.0100e-01
Epoch 8/10
10/10 - 2s - loss: 262.0401 - loglik: -2.6189e+02 - logprior: 0.1690
Epoch 9/10
10/10 - 2s - loss: 261.2211 - loglik: -2.6143e+02 - logprior: 0.5362
Epoch 10/10
10/10 - 2s - loss: 260.6896 - loglik: -2.6112e+02 - logprior: 0.7930
Fitted a model with MAP estimate = -259.9124
expansions: [(5, 3), (6, 2), (12, 4), (18, 2), (36, 4), (45, 2), (48, 3), (63, 3), (78, 1), (82, 2), (86, 4), (88, 3), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 348.1537 - loglik: -2.6081e+02 - logprior: -8.6973e+01
Epoch 2/2
10/10 - 2s - loss: 278.3504 - loglik: -2.4384e+02 - logprior: -3.4058e+01
Fitted a model with MAP estimate = -265.4265
expansions: [(0, 2), (61, 1)]
discards: [  0   8 105]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 309.1898 - loglik: -2.3984e+02 - logprior: -6.8786e+01
Epoch 2/2
10/10 - 2s - loss: 250.5287 - loglik: -2.3452e+02 - logprior: -1.5399e+01
Fitted a model with MAP estimate = -240.1959
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 321.2515 - loglik: -2.3809e+02 - logprior: -8.2654e+01
Epoch 2/10
10/10 - 2s - loss: 258.4821 - loglik: -2.3492e+02 - logprior: -2.3134e+01
Epoch 3/10
10/10 - 2s - loss: 239.2048 - loglik: -2.3297e+02 - logprior: -5.7141e+00
Epoch 4/10
10/10 - 2s - loss: 232.3515 - loglik: -2.3226e+02 - logprior: 0.4224
Epoch 5/10
10/10 - 2s - loss: 229.3330 - loglik: -2.3217e+02 - logprior: 3.2920
Epoch 6/10
10/10 - 2s - loss: 227.2745 - loglik: -2.3173e+02 - logprior: 4.8985
Epoch 7/10
10/10 - 2s - loss: 226.3110 - loglik: -2.3174e+02 - logprior: 5.8824
Epoch 8/10
10/10 - 2s - loss: 225.0438 - loglik: -2.3119e+02 - logprior: 6.5816
Epoch 9/10
10/10 - 2s - loss: 224.6434 - loglik: -2.3139e+02 - logprior: 7.1777
Epoch 10/10
10/10 - 2s - loss: 224.3123 - loglik: -2.3160e+02 - logprior: 7.7127
Fitted a model with MAP estimate = -223.5132
Time for alignment: 62.2849
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 423.0952 - loglik: -3.4517e+02 - logprior: -7.7898e+01
Epoch 2/10
10/10 - 2s - loss: 337.9149 - loglik: -3.1904e+02 - logprior: -1.8750e+01
Epoch 3/10
10/10 - 2s - loss: 299.1213 - loglik: -2.9143e+02 - logprior: -7.5976e+00
Epoch 4/10
10/10 - 2s - loss: 278.7312 - loglik: -2.7450e+02 - logprior: -4.0977e+00
Epoch 5/10
10/10 - 2s - loss: 269.9589 - loglik: -2.6704e+02 - logprior: -2.5490e+00
Epoch 6/10
10/10 - 2s - loss: 265.9749 - loglik: -2.6398e+02 - logprior: -1.5258e+00
Epoch 7/10
10/10 - 2s - loss: 263.4296 - loglik: -2.6212e+02 - logprior: -8.9619e-01
Epoch 8/10
10/10 - 2s - loss: 262.4532 - loglik: -2.6177e+02 - logprior: -3.0676e-01
Epoch 9/10
10/10 - 2s - loss: 261.3701 - loglik: -2.6105e+02 - logprior: 0.0742
Epoch 10/10
10/10 - 2s - loss: 260.8765 - loglik: -2.6079e+02 - logprior: 0.3276
Fitted a model with MAP estimate = -260.1598
expansions: [(5, 2), (6, 1), (7, 1), (10, 1), (12, 3), (19, 2), (36, 4), (45, 1), (46, 1), (49, 3), (64, 2), (68, 1), (83, 4), (86, 1), (88, 4), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 350.9149 - loglik: -2.6314e+02 - logprior: -8.7360e+01
Epoch 2/2
10/10 - 2s - loss: 282.9701 - loglik: -2.4807e+02 - logprior: -3.4389e+01
Fitted a model with MAP estimate = -269.8337
expansions: [(0, 2), (17, 1), (64, 1)]
discards: [  0   9  45  46  84  85 106 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 317.0947 - loglik: -2.4741e+02 - logprior: -6.9063e+01
Epoch 2/2
10/10 - 2s - loss: 257.4816 - loglik: -2.4090e+02 - logprior: -1.5970e+01
Fitted a model with MAP estimate = -247.1798
expansions: [(81, 2), (112, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 328.1377 - loglik: -2.4288e+02 - logprior: -8.4703e+01
Epoch 2/10
10/10 - 2s - loss: 267.2785 - loglik: -2.3794e+02 - logprior: -2.8806e+01
Epoch 3/10
10/10 - 2s - loss: 245.1663 - loglik: -2.3530e+02 - logprior: -9.3046e+00
Epoch 4/10
10/10 - 2s - loss: 234.1497 - loglik: -2.3328e+02 - logprior: -3.2419e-01
Epoch 5/10
10/10 - 2s - loss: 230.4950 - loglik: -2.3293e+02 - logprior: 2.9387
Epoch 6/10
10/10 - 2s - loss: 228.4725 - loglik: -2.3260e+02 - logprior: 4.6035
Epoch 7/10
10/10 - 2s - loss: 227.7789 - loglik: -2.3294e+02 - logprior: 5.6171
Epoch 8/10
10/10 - 2s - loss: 226.0708 - loglik: -2.3199e+02 - logprior: 6.3617
Epoch 9/10
10/10 - 2s - loss: 226.3786 - loglik: -2.3291e+02 - logprior: 6.9705
Fitted a model with MAP estimate = -225.2134
Time for alignment: 59.7213
Computed alignments with likelihoods: ['-222.3523', '-223.5132', '-225.2134']
Best model has likelihood: -222.3523
time for generating output: 0.2302
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.7489208633093525
Training of 3 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f186f857f70>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f10209ec700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f15ef4f9d60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f10209f0fa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f100095bc40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f103837dc70>, <__main__.SimpleDirichletPrior object at 0x7f1880a86070>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe58d820>

Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 97.1282 - loglik: -9.2667e+01 - logprior: -4.4408e+00
Epoch 2/10
17/17 - 1s - loss: 74.6686 - loglik: -7.3104e+01 - logprior: -1.5531e+00
Epoch 3/10
17/17 - 1s - loss: 65.0272 - loglik: -6.3334e+01 - logprior: -1.6860e+00
Epoch 4/10
17/17 - 1s - loss: 63.5786 - loglik: -6.1819e+01 - logprior: -1.6590e+00
Epoch 5/10
17/17 - 1s - loss: 63.1353 - loglik: -6.1439e+01 - logprior: -1.5792e+00
Epoch 6/10
17/17 - 1s - loss: 62.7896 - loglik: -6.1072e+01 - logprior: -1.5920e+00
Epoch 7/10
17/17 - 1s - loss: 62.9236 - loglik: -6.1202e+01 - logprior: -1.5717e+00
Fitted a model with MAP estimate = -62.5656
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.1334 - loglik: -6.4500e+01 - logprior: -5.5566e+00
Epoch 2/2
17/17 - 1s - loss: 61.6006 - loglik: -5.8912e+01 - logprior: -2.5661e+00
Fitted a model with MAP estimate = -59.1820
expansions: []
discards: [13 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.2275 - loglik: -5.7755e+01 - logprior: -4.3787e+00
Epoch 2/2
17/17 - 1s - loss: 58.4901 - loglik: -5.6678e+01 - logprior: -1.7051e+00
Fitted a model with MAP estimate = -58.0641
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 61.5345 - loglik: -5.7169e+01 - logprior: -4.2738e+00
Epoch 2/10
17/17 - 1s - loss: 58.5274 - loglik: -5.6741e+01 - logprior: -1.6830e+00
Epoch 3/10
17/17 - 1s - loss: 58.0501 - loglik: -5.6432e+01 - logprior: -1.4683e+00
Epoch 4/10
17/17 - 1s - loss: 57.8156 - loglik: -5.6238e+01 - logprior: -1.4012e+00
Epoch 5/10
17/17 - 1s - loss: 57.7860 - loglik: -5.6221e+01 - logprior: -1.3672e+00
Epoch 6/10
17/17 - 1s - loss: 57.4451 - loglik: -5.5894e+01 - logprior: -1.3439e+00
Epoch 7/10
17/17 - 1s - loss: 57.5193 - loglik: -5.5979e+01 - logprior: -1.3227e+00
Fitted a model with MAP estimate = -57.2139
Time for alignment: 39.9961
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 97.0853 - loglik: -9.2621e+01 - logprior: -4.4436e+00
Epoch 2/10
17/17 - 1s - loss: 74.7840 - loglik: -7.3212e+01 - logprior: -1.5605e+00
Epoch 3/10
17/17 - 1s - loss: 65.1065 - loglik: -6.3383e+01 - logprior: -1.6962e+00
Epoch 4/10
17/17 - 1s - loss: 63.4074 - loglik: -6.1609e+01 - logprior: -1.6663e+00
Epoch 5/10
17/17 - 1s - loss: 63.0502 - loglik: -6.1341e+01 - logprior: -1.5859e+00
Epoch 6/10
17/17 - 1s - loss: 62.9364 - loglik: -6.1193e+01 - logprior: -1.5995e+00
Epoch 7/10
17/17 - 1s - loss: 62.7607 - loglik: -6.1030e+01 - logprior: -1.5763e+00
Epoch 8/10
17/17 - 1s - loss: 62.6990 - loglik: -6.0964e+01 - logprior: -1.5637e+00
Epoch 9/10
17/17 - 1s - loss: 62.5695 - loglik: -6.0827e+01 - logprior: -1.5594e+00
Epoch 10/10
17/17 - 1s - loss: 62.6805 - loglik: -6.0939e+01 - logprior: -1.5479e+00
Fitted a model with MAP estimate = -62.3738
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.2962 - loglik: -6.4622e+01 - logprior: -5.5561e+00
Epoch 2/2
17/17 - 1s - loss: 62.0347 - loglik: -5.9277e+01 - logprior: -2.6283e+00
Fitted a model with MAP estimate = -59.4585
expansions: []
discards: [13 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.3667 - loglik: -5.7858e+01 - logprior: -4.4041e+00
Epoch 2/2
17/17 - 1s - loss: 58.5020 - loglik: -5.6688e+01 - logprior: -1.7049e+00
Fitted a model with MAP estimate = -58.0679
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 61.6315 - loglik: -5.7259e+01 - logprior: -4.2767e+00
Epoch 2/10
17/17 - 1s - loss: 58.4076 - loglik: -5.6621e+01 - logprior: -1.6794e+00
Epoch 3/10
17/17 - 1s - loss: 58.1323 - loglik: -5.6518e+01 - logprior: -1.4626e+00
Epoch 4/10
17/17 - 1s - loss: 57.6796 - loglik: -5.6107e+01 - logprior: -1.3993e+00
Epoch 5/10
17/17 - 1s - loss: 57.7366 - loglik: -5.6168e+01 - logprior: -1.3686e+00
Fitted a model with MAP estimate = -57.3834
Time for alignment: 42.0929
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 97.1909 - loglik: -9.2728e+01 - logprior: -4.4429e+00
Epoch 2/10
17/17 - 1s - loss: 75.4917 - loglik: -7.3924e+01 - logprior: -1.5564e+00
Epoch 3/10
17/17 - 1s - loss: 64.9672 - loglik: -6.3260e+01 - logprior: -1.6899e+00
Epoch 4/10
17/17 - 1s - loss: 63.0942 - loglik: -6.1267e+01 - logprior: -1.6911e+00
Epoch 5/10
17/17 - 1s - loss: 62.4812 - loglik: -6.0740e+01 - logprior: -1.5979e+00
Epoch 6/10
17/17 - 1s - loss: 62.4436 - loglik: -6.0674e+01 - logprior: -1.6126e+00
Epoch 7/10
17/17 - 1s - loss: 62.3077 - loglik: -6.0553e+01 - logprior: -1.5885e+00
Epoch 8/10
17/17 - 1s - loss: 62.1516 - loglik: -6.0397e+01 - logprior: -1.5748e+00
Epoch 9/10
17/17 - 1s - loss: 62.1018 - loglik: -6.0340e+01 - logprior: -1.5659e+00
Epoch 10/10
17/17 - 1s - loss: 62.0193 - loglik: -6.0259e+01 - logprior: -1.5597e+00
Fitted a model with MAP estimate = -61.8539
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (14, 1), (15, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 69.8457 - loglik: -6.4153e+01 - logprior: -5.5520e+00
Epoch 2/2
17/17 - 1s - loss: 61.9815 - loglik: -5.9218e+01 - logprior: -2.6330e+00
Fitted a model with MAP estimate = -59.4760
expansions: []
discards: [13 16]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.3864 - loglik: -5.7879e+01 - logprior: -4.4053e+00
Epoch 2/2
17/17 - 1s - loss: 58.6274 - loglik: -5.6819e+01 - logprior: -1.7047e+00
Fitted a model with MAP estimate = -58.0900
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 61.4808 - loglik: -5.7114e+01 - logprior: -4.2742e+00
Epoch 2/10
17/17 - 1s - loss: 58.6122 - loglik: -5.6817e+01 - logprior: -1.6860e+00
Epoch 3/10
17/17 - 1s - loss: 57.9041 - loglik: -5.6299e+01 - logprior: -1.4591e+00
Epoch 4/10
17/17 - 1s - loss: 57.8141 - loglik: -5.6233e+01 - logprior: -1.4029e+00
Epoch 5/10
17/17 - 1s - loss: 57.6856 - loglik: -5.6112e+01 - logprior: -1.3745e+00
Epoch 6/10
17/17 - 1s - loss: 57.7681 - loglik: -5.6217e+01 - logprior: -1.3408e+00
Fitted a model with MAP estimate = -57.3030
Time for alignment: 40.8392
Computed alignments with likelihoods: ['-57.2139', '-57.3834', '-57.3030']
Best model has likelihood: -57.2139
time for generating output: 0.1266
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.7830953299884215
Training of 3 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f1934e693a0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f180f254d30>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f196a630e80>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f10393729a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1039372d60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f15ed9d9100>, <__main__.SimpleDirichletPrior object at 0x7f1039d0f0a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fe58d820>

Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.6776 - loglik: -1.8550e+02 - logprior: -8.1672e+00
Epoch 2/10
13/13 - 1s - loss: 163.8297 - loglik: -1.6154e+02 - logprior: -2.2904e+00
Epoch 3/10
13/13 - 1s - loss: 146.3886 - loglik: -1.4430e+02 - logprior: -2.0195e+00
Epoch 4/10
13/13 - 1s - loss: 139.3620 - loglik: -1.3688e+02 - logprior: -2.1628e+00
Epoch 5/10
13/13 - 1s - loss: 136.0701 - loglik: -1.3366e+02 - logprior: -2.0543e+00
Epoch 6/10
13/13 - 1s - loss: 134.6843 - loglik: -1.3245e+02 - logprior: -1.9755e+00
Epoch 7/10
13/13 - 1s - loss: 133.2779 - loglik: -1.3105e+02 - logprior: -2.0075e+00
Epoch 8/10
13/13 - 1s - loss: 133.6665 - loglik: -1.3149e+02 - logprior: -1.9881e+00
Fitted a model with MAP estimate = -133.0499
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (32, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 143.3338 - loglik: -1.3340e+02 - logprior: -9.6894e+00
Epoch 2/2
13/13 - 1s - loss: 129.3841 - loglik: -1.2439e+02 - logprior: -4.6500e+00
Fitted a model with MAP estimate = -126.6071
expansions: [(0, 2)]
discards: [ 0 23 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.6431 - loglik: -1.2288e+02 - logprior: -7.4731e+00
Epoch 2/2
13/13 - 1s - loss: 123.5624 - loglik: -1.2098e+02 - logprior: -2.3617e+00
Fitted a model with MAP estimate = -121.9556
expansions: [(17, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 133.5957 - loglik: -1.2393e+02 - logprior: -9.4326e+00
Epoch 2/10
13/13 - 1s - loss: 125.4829 - loglik: -1.2152e+02 - logprior: -3.7007e+00
Epoch 3/10
13/13 - 1s - loss: 121.9106 - loglik: -1.1967e+02 - logprior: -1.8837e+00
Epoch 4/10
13/13 - 1s - loss: 120.1046 - loglik: -1.1824e+02 - logprior: -1.5250e+00
Epoch 5/10
13/13 - 1s - loss: 119.6785 - loglik: -1.1800e+02 - logprior: -1.3880e+00
Epoch 6/10
13/13 - 1s - loss: 119.1292 - loglik: -1.1748e+02 - logprior: -1.3923e+00
Epoch 7/10
13/13 - 1s - loss: 118.9249 - loglik: -1.1727e+02 - logprior: -1.4170e+00
Epoch 8/10
13/13 - 1s - loss: 118.7295 - loglik: -1.1715e+02 - logprior: -1.3602e+00
Epoch 9/10
13/13 - 1s - loss: 118.1549 - loglik: -1.1660e+02 - logprior: -1.3475e+00
Epoch 10/10
13/13 - 1s - loss: 118.8730 - loglik: -1.1735e+02 - logprior: -1.3156e+00
Fitted a model with MAP estimate = -118.1697
Time for alignment: 45.0615
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 193.9209 - loglik: -1.8574e+02 - logprior: -8.1687e+00
Epoch 2/10
13/13 - 1s - loss: 164.1898 - loglik: -1.6190e+02 - logprior: -2.2888e+00
Epoch 3/10
13/13 - 1s - loss: 146.3852 - loglik: -1.4433e+02 - logprior: -1.9821e+00
Epoch 4/10
13/13 - 1s - loss: 138.7099 - loglik: -1.3629e+02 - logprior: -2.1345e+00
Epoch 5/10
13/13 - 1s - loss: 136.1001 - loglik: -1.3378e+02 - logprior: -2.0596e+00
Epoch 6/10
13/13 - 1s - loss: 133.9929 - loglik: -1.3181e+02 - logprior: -1.9685e+00
Epoch 7/10
13/13 - 1s - loss: 133.8341 - loglik: -1.3167e+02 - logprior: -1.9897e+00
Epoch 8/10
13/13 - 1s - loss: 133.5542 - loglik: -1.3141e+02 - logprior: -1.9758e+00
Epoch 9/10
13/13 - 1s - loss: 133.3291 - loglik: -1.3121e+02 - logprior: -1.9608e+00
Epoch 10/10
13/13 - 1s - loss: 133.1810 - loglik: -1.3107e+02 - logprior: -1.9539e+00
Fitted a model with MAP estimate = -132.9276
expansions: [(12, 1), (17, 5), (18, 1), (21, 1), (32, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 142.7715 - loglik: -1.3287e+02 - logprior: -9.6876e+00
Epoch 2/2
13/13 - 1s - loss: 129.0496 - loglik: -1.2407e+02 - logprior: -4.6239e+00
Fitted a model with MAP estimate = -126.7481
expansions: [(0, 2)]
discards: [ 0 55]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.6678 - loglik: -1.2290e+02 - logprior: -7.4728e+00
Epoch 2/2
13/13 - 1s - loss: 123.4272 - loglik: -1.2083e+02 - logprior: -2.3583e+00
Fitted a model with MAP estimate = -121.9980
expansions: [(17, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 133.5284 - loglik: -1.2383e+02 - logprior: -9.4271e+00
Epoch 2/10
13/13 - 1s - loss: 125.3641 - loglik: -1.2147e+02 - logprior: -3.6451e+00
Epoch 3/10
13/13 - 1s - loss: 121.5087 - loglik: -1.1931e+02 - logprior: -1.8655e+00
Epoch 4/10
13/13 - 1s - loss: 120.0373 - loglik: -1.1818e+02 - logprior: -1.5156e+00
Epoch 5/10
13/13 - 1s - loss: 120.0474 - loglik: -1.1835e+02 - logprior: -1.3902e+00
Fitted a model with MAP estimate = -119.0228
Time for alignment: 42.2419
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.8939 - loglik: -1.8572e+02 - logprior: -8.1681e+00
Epoch 2/10
13/13 - 1s - loss: 163.7486 - loglik: -1.6146e+02 - logprior: -2.2873e+00
Epoch 3/10
13/13 - 1s - loss: 145.4977 - loglik: -1.4342e+02 - logprior: -1.9962e+00
Epoch 4/10
13/13 - 1s - loss: 138.7523 - loglik: -1.3630e+02 - logprior: -2.1622e+00
Epoch 5/10
13/13 - 1s - loss: 136.1203 - loglik: -1.3378e+02 - logprior: -2.0617e+00
Epoch 6/10
13/13 - 1s - loss: 134.7625 - loglik: -1.3256e+02 - logprior: -1.9662e+00
Epoch 7/10
13/13 - 1s - loss: 134.2625 - loglik: -1.3209e+02 - logprior: -1.9781e+00
Epoch 8/10
13/13 - 1s - loss: 133.7218 - loglik: -1.3158e+02 - logprior: -1.9653e+00
Epoch 9/10
13/13 - 1s - loss: 133.4875 - loglik: -1.3136e+02 - logprior: -1.9483e+00
Epoch 10/10
13/13 - 1s - loss: 134.0974 - loglik: -1.3198e+02 - logprior: -1.9450e+00
Fitted a model with MAP estimate = -133.3900
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 143.9599 - loglik: -1.3404e+02 - logprior: -9.6923e+00
Epoch 2/2
13/13 - 1s - loss: 129.0723 - loglik: -1.2407e+02 - logprior: -4.6509e+00
Fitted a model with MAP estimate = -126.7451
expansions: [(0, 2)]
discards: [ 0 23 56]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 131.1665 - loglik: -1.2339e+02 - logprior: -7.4696e+00
Epoch 2/2
13/13 - 1s - loss: 123.1345 - loglik: -1.2055e+02 - logprior: -2.3558e+00
Fitted a model with MAP estimate = -121.9846
expansions: [(17, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 133.5446 - loglik: -1.2384e+02 - logprior: -9.4370e+00
Epoch 2/10
13/13 - 1s - loss: 125.3234 - loglik: -1.2139e+02 - logprior: -3.6845e+00
Epoch 3/10
13/13 - 1s - loss: 121.4143 - loglik: -1.1920e+02 - logprior: -1.8791e+00
Epoch 4/10
13/13 - 1s - loss: 120.5612 - loglik: -1.1870e+02 - logprior: -1.5134e+00
Epoch 5/10
13/13 - 1s - loss: 119.5947 - loglik: -1.1792e+02 - logprior: -1.3755e+00
Epoch 6/10
13/13 - 1s - loss: 118.8869 - loglik: -1.1724e+02 - logprior: -1.3890e+00
Epoch 7/10
13/13 - 1s - loss: 119.2469 - loglik: -1.1761e+02 - logprior: -1.4030e+00
Fitted a model with MAP estimate = -118.5078
Time for alignment: 43.5079
Computed alignments with likelihoods: ['-118.1697', '-119.0228', '-118.5078']
Best model has likelihood: -118.1697
time for generating output: 0.1435
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.9730878186968839
Training of 3 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f15ef0e9640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1899b8a130>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19475bc460>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1039023e80>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f187810b700>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f18a27ee760>, <__main__.SimpleDirichletPrior object at 0x7f15efaff1f0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fd1d73a0>

Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 240.9414 - loglik: -2.2826e+02 - logprior: -1.2675e+01
Epoch 2/10
11/11 - 1s - loss: 208.2651 - loglik: -2.0488e+02 - logprior: -3.3768e+00
Epoch 3/10
11/11 - 1s - loss: 182.7579 - loglik: -1.8043e+02 - logprior: -2.2015e+00
Epoch 4/10
11/11 - 1s - loss: 167.2491 - loglik: -1.6489e+02 - logprior: -1.9697e+00
Epoch 5/10
11/11 - 1s - loss: 161.2790 - loglik: -1.5905e+02 - logprior: -1.7231e+00
Epoch 6/10
11/11 - 1s - loss: 159.1318 - loglik: -1.5717e+02 - logprior: -1.6064e+00
Epoch 7/10
11/11 - 1s - loss: 157.3938 - loglik: -1.5574e+02 - logprior: -1.3854e+00
Epoch 8/10
11/11 - 1s - loss: 156.8129 - loglik: -1.5532e+02 - logprior: -1.2536e+00
Epoch 9/10
11/11 - 1s - loss: 156.6428 - loglik: -1.5526e+02 - logprior: -1.1610e+00
Epoch 10/10
11/11 - 1s - loss: 156.1296 - loglik: -1.5478e+02 - logprior: -1.1215e+00
Fitted a model with MAP estimate = -155.8724
expansions: [(0, 6), (21, 1), (27, 1), (28, 1), (29, 2), (31, 2), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 171.9202 - loglik: -1.5574e+02 - logprior: -1.5914e+01
Epoch 2/2
11/11 - 1s - loss: 150.9940 - loglik: -1.4590e+02 - logprior: -4.6987e+00
Fitted a model with MAP estimate = -146.8861
expansions: []
discards: [ 0 38 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 162.4636 - loglik: -1.4777e+02 - logprior: -1.4341e+01
Epoch 2/2
11/11 - 1s - loss: 151.2627 - loglik: -1.4531e+02 - logprior: -5.6817e+00
Fitted a model with MAP estimate = -148.2288
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 158.1345 - loglik: -1.4536e+02 - logprior: -1.2500e+01
Epoch 2/10
11/11 - 1s - loss: 147.5597 - loglik: -1.4398e+02 - logprior: -3.3191e+00
Epoch 3/10
11/11 - 1s - loss: 145.2299 - loglik: -1.4290e+02 - logprior: -2.0337e+00
Epoch 4/10
11/11 - 1s - loss: 143.8966 - loglik: -1.4218e+02 - logprior: -1.4035e+00
Epoch 5/10
11/11 - 1s - loss: 143.3493 - loglik: -1.4197e+02 - logprior: -1.0811e+00
Epoch 6/10
11/11 - 1s - loss: 142.8442 - loglik: -1.4156e+02 - logprior: -1.0045e+00
Epoch 7/10
11/11 - 1s - loss: 142.6748 - loglik: -1.4153e+02 - logprior: -8.7604e-01
Epoch 8/10
11/11 - 1s - loss: 142.2878 - loglik: -1.4119e+02 - logprior: -8.4203e-01
Epoch 9/10
11/11 - 1s - loss: 142.2421 - loglik: -1.4118e+02 - logprior: -8.0154e-01
Epoch 10/10
11/11 - 1s - loss: 141.8319 - loglik: -1.4081e+02 - logprior: -7.6650e-01
Fitted a model with MAP estimate = -141.6933
Time for alignment: 46.8041
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 241.0692 - loglik: -2.2839e+02 - logprior: -1.2674e+01
Epoch 2/10
11/11 - 1s - loss: 207.8766 - loglik: -2.0449e+02 - logprior: -3.3842e+00
Epoch 3/10
11/11 - 1s - loss: 181.7498 - loglik: -1.7939e+02 - logprior: -2.2265e+00
Epoch 4/10
11/11 - 1s - loss: 166.1188 - loglik: -1.6379e+02 - logprior: -1.9998e+00
Epoch 5/10
11/11 - 1s - loss: 160.8009 - loglik: -1.5860e+02 - logprior: -1.7835e+00
Epoch 6/10
11/11 - 1s - loss: 158.2880 - loglik: -1.5624e+02 - logprior: -1.6607e+00
Epoch 7/10
11/11 - 1s - loss: 157.0995 - loglik: -1.5539e+02 - logprior: -1.4411e+00
Epoch 8/10
11/11 - 1s - loss: 156.2474 - loglik: -1.5471e+02 - logprior: -1.3153e+00
Epoch 9/10
11/11 - 1s - loss: 156.1767 - loglik: -1.5474e+02 - logprior: -1.2179e+00
Epoch 10/10
11/11 - 1s - loss: 155.7437 - loglik: -1.5435e+02 - logprior: -1.1744e+00
Fitted a model with MAP estimate = -155.4432
expansions: [(0, 6), (22, 1), (27, 1), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 171.0994 - loglik: -1.5493e+02 - logprior: -1.5910e+01
Epoch 2/2
11/11 - 1s - loss: 150.5356 - loglik: -1.4557e+02 - logprior: -4.5999e+00
Fitted a model with MAP estimate = -146.6339
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 161.9890 - loglik: -1.4736e+02 - logprior: -1.4321e+01
Epoch 2/2
11/11 - 1s - loss: 150.8869 - loglik: -1.4494e+02 - logprior: -5.6752e+00
Fitted a model with MAP estimate = -148.1684
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 157.9206 - loglik: -1.4519e+02 - logprior: -1.2487e+01
Epoch 2/10
11/11 - 1s - loss: 147.8748 - loglik: -1.4431e+02 - logprior: -3.3116e+00
Epoch 3/10
11/11 - 1s - loss: 144.9862 - loglik: -1.4264e+02 - logprior: -2.0373e+00
Epoch 4/10
11/11 - 1s - loss: 144.2524 - loglik: -1.4255e+02 - logprior: -1.3888e+00
Epoch 5/10
11/11 - 1s - loss: 142.8271 - loglik: -1.4146e+02 - logprior: -1.0727e+00
Epoch 6/10
11/11 - 1s - loss: 143.1115 - loglik: -1.4183e+02 - logprior: -1.0034e+00
Fitted a model with MAP estimate = -142.3861
Time for alignment: 41.4272
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 240.9851 - loglik: -2.2831e+02 - logprior: -1.2673e+01
Epoch 2/10
11/11 - 1s - loss: 208.7432 - loglik: -2.0536e+02 - logprior: -3.3774e+00
Epoch 3/10
11/11 - 1s - loss: 183.3057 - loglik: -1.8099e+02 - logprior: -2.1904e+00
Epoch 4/10
11/11 - 1s - loss: 166.3331 - loglik: -1.6401e+02 - logprior: -2.0293e+00
Epoch 5/10
11/11 - 1s - loss: 161.2370 - loglik: -1.5899e+02 - logprior: -1.8252e+00
Epoch 6/10
11/11 - 1s - loss: 158.9164 - loglik: -1.5687e+02 - logprior: -1.6537e+00
Epoch 7/10
11/11 - 1s - loss: 158.1463 - loglik: -1.5644e+02 - logprior: -1.4129e+00
Epoch 8/10
11/11 - 1s - loss: 157.7004 - loglik: -1.5618e+02 - logprior: -1.2722e+00
Epoch 9/10
11/11 - 1s - loss: 156.7549 - loglik: -1.5532e+02 - logprior: -1.1898e+00
Epoch 10/10
11/11 - 1s - loss: 156.8423 - loglik: -1.5544e+02 - logprior: -1.1543e+00
Fitted a model with MAP estimate = -156.3560
expansions: [(0, 6), (21, 1), (27, 1), (29, 2), (32, 1), (33, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 172.0562 - loglik: -1.5586e+02 - logprior: -1.5910e+01
Epoch 2/2
11/11 - 1s - loss: 150.7616 - loglik: -1.4572e+02 - logprior: -4.6475e+00
Fitted a model with MAP estimate = -146.6868
expansions: []
discards: [ 0 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 162.4651 - loglik: -1.4779e+02 - logprior: -1.4332e+01
Epoch 2/2
11/11 - 1s - loss: 151.4811 - loglik: -1.4550e+02 - logprior: -5.6887e+00
Fitted a model with MAP estimate = -148.4146
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.3347 - loglik: -1.4556e+02 - logprior: -1.2504e+01
Epoch 2/10
11/11 - 1s - loss: 148.0226 - loglik: -1.4443e+02 - logprior: -3.3204e+00
Epoch 3/10
11/11 - 1s - loss: 144.7343 - loglik: -1.4238e+02 - logprior: -2.0347e+00
Epoch 4/10
11/11 - 1s - loss: 144.0116 - loglik: -1.4230e+02 - logprior: -1.3911e+00
Epoch 5/10
11/11 - 1s - loss: 143.2523 - loglik: -1.4188e+02 - logprior: -1.0756e+00
Epoch 6/10
11/11 - 1s - loss: 142.9998 - loglik: -1.4172e+02 - logprior: -1.0031e+00
Epoch 7/10
11/11 - 1s - loss: 142.4317 - loglik: -1.4130e+02 - logprior: -8.6604e-01
Epoch 8/10
11/11 - 1s - loss: 142.1964 - loglik: -1.4109e+02 - logprior: -8.4075e-01
Epoch 9/10
11/11 - 1s - loss: 142.4888 - loglik: -1.4145e+02 - logprior: -7.8878e-01
Fitted a model with MAP estimate = -141.8266
Time for alignment: 44.3129
Computed alignments with likelihoods: ['-141.6933', '-142.3861', '-141.8266']
Best model has likelihood: -141.6933
time for generating output: 0.1448
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 3 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f160c647c40>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18340fde50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18340fda60>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f100079fcd0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f185692aaf0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1817d29a60>, <__main__.SimpleDirichletPrior object at 0x7f10482a5c70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19c7e57e50>

Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 527.0377 - loglik: -5.2140e+02 - logprior: -5.6236e+00
Epoch 2/10
24/24 - 7s - loss: 381.4300 - loglik: -3.7879e+02 - logprior: -2.5884e+00
Epoch 3/10
24/24 - 7s - loss: 347.2751 - loglik: -3.4364e+02 - logprior: -3.2161e+00
Epoch 4/10
24/24 - 8s - loss: 343.6267 - loglik: -3.4014e+02 - logprior: -3.0294e+00
Epoch 5/10
24/24 - 8s - loss: 341.5245 - loglik: -3.3811e+02 - logprior: -2.9936e+00
Epoch 6/10
24/24 - 7s - loss: 341.1149 - loglik: -3.3773e+02 - logprior: -2.9753e+00
Epoch 7/10
24/24 - 7s - loss: 341.9992 - loglik: -3.3858e+02 - logprior: -3.0137e+00
Fitted a model with MAP estimate = -340.5708
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (76, 1), (82, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (149, 1), (152, 2), (153, 1), (154, 3), (158, 2), (173, 2), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 335.1532 - loglik: -3.2688e+02 - logprior: -7.8116e+00
Epoch 2/2
24/24 - 9s - loss: 314.2531 - loglik: -3.1046e+02 - logprior: -3.1408e+00
Fitted a model with MAP estimate = -310.1038
expansions: [(0, 2)]
discards: [ 0 12 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 15s - loss: 316.2558 - loglik: -3.1041e+02 - logprior: -5.1312e+00
Epoch 2/2
24/24 - 9s - loss: 308.1616 - loglik: -3.0668e+02 - logprior: -7.3013e-01
Fitted a model with MAP estimate = -305.2467
expansions: [(150, 1), (193, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 314.9016 - loglik: -3.0932e+02 - logprior: -4.8073e+00
Epoch 2/10
24/24 - 9s - loss: 306.0971 - loglik: -3.0498e+02 - logprior: -3.6324e-01
Epoch 3/10
24/24 - 9s - loss: 302.9998 - loglik: -3.0239e+02 - logprior: 0.1331
Epoch 4/10
24/24 - 9s - loss: 302.5539 - loglik: -3.0223e+02 - logprior: 0.3230
Epoch 5/10
24/24 - 9s - loss: 302.1432 - loglik: -3.0206e+02 - logprior: 0.5036
Epoch 6/10
24/24 - 9s - loss: 302.4013 - loglik: -3.0253e+02 - logprior: 0.6819
Fitted a model with MAP estimate = -300.4839
Time for alignment: 192.8997
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 528.1529 - loglik: -5.2250e+02 - logprior: -5.6411e+00
Epoch 2/10
24/24 - 7s - loss: 381.5903 - loglik: -3.7902e+02 - logprior: -2.5356e+00
Epoch 3/10
24/24 - 7s - loss: 348.7178 - loglik: -3.4529e+02 - logprior: -3.1462e+00
Epoch 4/10
24/24 - 8s - loss: 344.0545 - loglik: -3.4064e+02 - logprior: -2.9566e+00
Epoch 5/10
24/24 - 7s - loss: 342.4575 - loglik: -3.3907e+02 - logprior: -2.9637e+00
Epoch 6/10
24/24 - 7s - loss: 341.5356 - loglik: -3.3817e+02 - logprior: -2.9573e+00
Epoch 7/10
24/24 - 7s - loss: 341.2792 - loglik: -3.3790e+02 - logprior: -2.9624e+00
Epoch 8/10
24/24 - 7s - loss: 340.8902 - loglik: -3.3751e+02 - logprior: -2.9679e+00
Epoch 9/10
24/24 - 8s - loss: 340.4223 - loglik: -3.3704e+02 - logprior: -2.9682e+00
Epoch 10/10
24/24 - 7s - loss: 340.5864 - loglik: -3.3719e+02 - logprior: -2.9866e+00
Fitted a model with MAP estimate = -339.6852
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 1), (18, 1), (35, 2), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (60, 1), (64, 1), (66, 1), (76, 1), (83, 1), (85, 1), (88, 1), (91, 1), (109, 1), (111, 1), (114, 1), (116, 1), (119, 2), (120, 2), (121, 1), (122, 1), (123, 1), (153, 2), (154, 3), (155, 1), (156, 1), (158, 1), (172, 1), (174, 3), (186, 1), (187, 1), (190, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 333.9299 - loglik: -3.2564e+02 - logprior: -7.8302e+00
Epoch 2/2
24/24 - 9s - loss: 316.2888 - loglik: -3.1247e+02 - logprior: -3.2276e+00
Fitted a model with MAP estimate = -311.7539
expansions: [(0, 2), (193, 1), (217, 1)]
discards: [  0  13 149]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 13s - loss: 317.6997 - loglik: -3.1189e+02 - logprior: -5.1299e+00
Epoch 2/2
24/24 - 9s - loss: 307.0225 - loglik: -3.0566e+02 - logprior: -6.2018e-01
Fitted a model with MAP estimate = -304.1892
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 313.6284 - loglik: -3.0811e+02 - logprior: -4.7340e+00
Epoch 2/10
24/24 - 9s - loss: 306.7221 - loglik: -3.0567e+02 - logprior: -2.8185e-01
Epoch 3/10
24/24 - 9s - loss: 303.9240 - loglik: -3.0340e+02 - logprior: 0.2208
Epoch 4/10
24/24 - 9s - loss: 302.6276 - loglik: -3.0239e+02 - logprior: 0.4110
Epoch 5/10
24/24 - 9s - loss: 301.9621 - loglik: -3.0197e+02 - logprior: 0.5978
Epoch 6/10
24/24 - 9s - loss: 301.2241 - loglik: -3.0147e+02 - logprior: 0.7843
Epoch 7/10
24/24 - 9s - loss: 300.5712 - loglik: -3.0103e+02 - logprior: 0.9875
Epoch 8/10
24/24 - 9s - loss: 300.5781 - loglik: -3.0125e+02 - logprior: 1.1775
Fitted a model with MAP estimate = -299.6508
Time for alignment: 232.3904
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 10s - loss: 525.1193 - loglik: -5.1948e+02 - logprior: -5.6237e+00
Epoch 2/10
24/24 - 8s - loss: 384.4501 - loglik: -3.8192e+02 - logprior: -2.4802e+00
Epoch 3/10
24/24 - 7s - loss: 348.6601 - loglik: -3.4532e+02 - logprior: -2.9776e+00
Epoch 4/10
24/24 - 7s - loss: 345.1746 - loglik: -3.4193e+02 - logprior: -2.7967e+00
Epoch 5/10
24/24 - 8s - loss: 343.4836 - loglik: -3.4025e+02 - logprior: -2.7939e+00
Epoch 6/10
24/24 - 7s - loss: 342.7390 - loglik: -3.3950e+02 - logprior: -2.8088e+00
Epoch 7/10
24/24 - 7s - loss: 342.4738 - loglik: -3.3921e+02 - logprior: -2.8351e+00
Epoch 8/10
24/24 - 7s - loss: 342.1198 - loglik: -3.3886e+02 - logprior: -2.8344e+00
Epoch 9/10
24/24 - 7s - loss: 341.2327 - loglik: -3.3797e+02 - logprior: -2.8497e+00
Epoch 10/10
24/24 - 8s - loss: 342.7597 - loglik: -3.3950e+02 - logprior: -2.8591e+00
Fitted a model with MAP estimate = -340.8650
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (48, 2), (49, 2), (63, 1), (65, 1), (78, 1), (85, 1), (88, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (153, 2), (154, 5), (158, 1), (172, 1), (173, 3), (174, 1), (175, 1), (187, 1), (190, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 14s - loss: 335.9866 - loglik: -3.2769e+02 - logprior: -7.8276e+00
Epoch 2/2
24/24 - 9s - loss: 313.3599 - loglik: -3.0952e+02 - logprior: -3.2095e+00
Fitted a model with MAP estimate = -310.4049
expansions: [(0, 2), (192, 1)]
discards: [ 0 12 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 316.0610 - loglik: -3.1034e+02 - logprior: -5.0356e+00
Epoch 2/2
24/24 - 9s - loss: 306.1205 - loglik: -3.0476e+02 - logprior: -6.2299e-01
Fitted a model with MAP estimate = -304.2647
expansions: [(151, 1)]
discards: [25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 13s - loss: 313.3843 - loglik: -3.0789e+02 - logprior: -4.7243e+00
Epoch 2/10
24/24 - 9s - loss: 307.2224 - loglik: -3.0617e+02 - logprior: -2.9370e-01
Epoch 3/10
24/24 - 9s - loss: 303.4984 - loglik: -3.0296e+02 - logprior: 0.1988
Epoch 4/10
24/24 - 9s - loss: 301.6571 - loglik: -3.0140e+02 - logprior: 0.4008
Epoch 5/10
24/24 - 9s - loss: 303.0546 - loglik: -3.0302e+02 - logprior: 0.5642
Fitted a model with MAP estimate = -300.6367
Time for alignment: 205.4630
Computed alignments with likelihoods: ['-300.4839', '-299.6508', '-300.6367']
Best model has likelihood: -299.6508
time for generating output: 0.3086
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.9638140039804596
Training of 3 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f15ef062af0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18a27e9dc0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19a5b09640>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1833af67c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f10001c86a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1020f2bf10>, <__main__.SimpleDirichletPrior object at 0x7f193de54a30>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f1891811a60>

Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 304.0266 - loglik: -2.7659e+02 - logprior: -2.7413e+01
Epoch 2/10
10/10 - 1s - loss: 247.7590 - loglik: -2.4053e+02 - logprior: -7.2178e+00
Epoch 3/10
10/10 - 1s - loss: 211.2607 - loglik: -2.0733e+02 - logprior: -3.9291e+00
Epoch 4/10
10/10 - 1s - loss: 192.7113 - loglik: -1.8978e+02 - logprior: -2.9211e+00
Epoch 5/10
10/10 - 1s - loss: 182.9374 - loglik: -1.8019e+02 - logprior: -2.6653e+00
Epoch 6/10
10/10 - 1s - loss: 178.7797 - loglik: -1.7595e+02 - logprior: -2.5578e+00
Epoch 7/10
10/10 - 1s - loss: 176.7733 - loglik: -1.7408e+02 - logprior: -2.2919e+00
Epoch 8/10
10/10 - 1s - loss: 175.9724 - loglik: -1.7346e+02 - logprior: -2.1295e+00
Epoch 9/10
10/10 - 1s - loss: 175.4032 - loglik: -1.7298e+02 - logprior: -2.0956e+00
Epoch 10/10
10/10 - 1s - loss: 175.3187 - loglik: -1.7293e+02 - logprior: -2.0695e+00
Fitted a model with MAP estimate = -174.7806
expansions: [(0, 3), (11, 1), (12, 1), (34, 1), (35, 4), (36, 2), (50, 2), (60, 1), (69, 1), (70, 1), (72, 3), (73, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 205.8524 - loglik: -1.7090e+02 - logprior: -3.4658e+01
Epoch 2/2
10/10 - 1s - loss: 170.8220 - loglik: -1.5999e+02 - logprior: -1.0520e+01
Fitted a model with MAP estimate = -163.3858
expansions: []
discards: [ 0 44]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 191.0657 - loglik: -1.5946e+02 - logprior: -3.1290e+01
Epoch 2/2
10/10 - 1s - loss: 170.0181 - loglik: -1.5723e+02 - logprior: -1.2550e+01
Fitted a model with MAP estimate = -165.9001
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 101 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 187.1339 - loglik: -1.5747e+02 - logprior: -2.9424e+01
Epoch 2/10
10/10 - 1s - loss: 165.7931 - loglik: -1.5695e+02 - logprior: -8.6101e+00
Epoch 3/10
10/10 - 1s - loss: 158.8891 - loglik: -1.5517e+02 - logprior: -3.3845e+00
Epoch 4/10
10/10 - 1s - loss: 157.1383 - loglik: -1.5500e+02 - logprior: -1.7388e+00
Epoch 5/10
10/10 - 1s - loss: 155.0535 - loglik: -1.5353e+02 - logprior: -1.1343e+00
Epoch 6/10
10/10 - 1s - loss: 154.5479 - loglik: -1.5328e+02 - logprior: -8.7800e-01
Epoch 7/10
10/10 - 1s - loss: 154.3975 - loglik: -1.5346e+02 - logprior: -5.4794e-01
Epoch 8/10
10/10 - 1s - loss: 153.5185 - loglik: -1.5287e+02 - logprior: -2.7258e-01
Epoch 9/10
10/10 - 1s - loss: 153.9592 - loglik: -1.5344e+02 - logprior: -1.4928e-01
Fitted a model with MAP estimate = -153.1205
Time for alignment: 47.9606
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.8520 - loglik: -2.7642e+02 - logprior: -2.7412e+01
Epoch 2/10
10/10 - 1s - loss: 247.3468 - loglik: -2.4011e+02 - logprior: -7.2207e+00
Epoch 3/10
10/10 - 1s - loss: 210.6831 - loglik: -2.0675e+02 - logprior: -3.9255e+00
Epoch 4/10
10/10 - 1s - loss: 190.1970 - loglik: -1.8722e+02 - logprior: -2.9627e+00
Epoch 5/10
10/10 - 1s - loss: 181.3369 - loglik: -1.7841e+02 - logprior: -2.7874e+00
Epoch 6/10
10/10 - 1s - loss: 176.7853 - loglik: -1.7373e+02 - logprior: -2.6881e+00
Epoch 7/10
10/10 - 1s - loss: 175.2449 - loglik: -1.7235e+02 - logprior: -2.4547e+00
Epoch 8/10
10/10 - 1s - loss: 174.2105 - loglik: -1.7150e+02 - logprior: -2.3113e+00
Epoch 9/10
10/10 - 1s - loss: 173.1913 - loglik: -1.7054e+02 - logprior: -2.2720e+00
Epoch 10/10
10/10 - 1s - loss: 173.1683 - loglik: -1.7050e+02 - logprior: -2.2806e+00
Fitted a model with MAP estimate = -172.4992
expansions: [(0, 3), (5, 1), (10, 1), (24, 1), (35, 4), (36, 2), (48, 2), (49, 2), (58, 1), (69, 1), (70, 1), (72, 2), (73, 1), (74, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 204.4947 - loglik: -1.6930e+02 - logprior: -3.4811e+01
Epoch 2/2
10/10 - 1s - loss: 169.0614 - loglik: -1.5784e+02 - logprior: -1.0810e+01
Fitted a model with MAP estimate = -162.0879
expansions: []
discards: [ 0 46 92]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 190.1848 - loglik: -1.5835e+02 - logprior: -3.1434e+01
Epoch 2/2
10/10 - 1s - loss: 169.1275 - loglik: -1.5609e+02 - logprior: -1.2732e+01
Fitted a model with MAP estimate = -164.4819
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 185.5099 - loglik: -1.5571e+02 - logprior: -2.9523e+01
Epoch 2/10
10/10 - 1s - loss: 163.2841 - loglik: -1.5434e+02 - logprior: -8.6957e+00
Epoch 3/10
10/10 - 1s - loss: 156.7723 - loglik: -1.5292e+02 - logprior: -3.4909e+00
Epoch 4/10
10/10 - 1s - loss: 153.6548 - loglik: -1.5144e+02 - logprior: -1.8068e+00
Epoch 5/10
10/10 - 1s - loss: 152.3320 - loglik: -1.5085e+02 - logprior: -1.0792e+00
Epoch 6/10
10/10 - 1s - loss: 151.4617 - loglik: -1.5031e+02 - logprior: -7.4709e-01
Epoch 7/10
10/10 - 1s - loss: 151.2566 - loglik: -1.5030e+02 - logprior: -5.4256e-01
Epoch 8/10
10/10 - 1s - loss: 151.0555 - loglik: -1.5028e+02 - logprior: -3.6234e-01
Epoch 9/10
10/10 - 1s - loss: 150.4009 - loglik: -1.4978e+02 - logprior: -2.0224e-01
Epoch 10/10
10/10 - 1s - loss: 150.2812 - loglik: -1.4980e+02 - logprior: -6.3280e-02
Fitted a model with MAP estimate = -149.8610
Time for alignment: 48.2234
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 304.3434 - loglik: -2.7691e+02 - logprior: -2.7411e+01
Epoch 2/10
10/10 - 1s - loss: 247.6671 - loglik: -2.4042e+02 - logprior: -7.2265e+00
Epoch 3/10
10/10 - 1s - loss: 210.2934 - loglik: -2.0633e+02 - logprior: -3.9575e+00
Epoch 4/10
10/10 - 1s - loss: 187.9175 - loglik: -1.8491e+02 - logprior: -2.9756e+00
Epoch 5/10
10/10 - 1s - loss: 179.1209 - loglik: -1.7618e+02 - logprior: -2.7213e+00
Epoch 6/10
10/10 - 1s - loss: 176.5751 - loglik: -1.7361e+02 - logprior: -2.5699e+00
Epoch 7/10
10/10 - 1s - loss: 175.1258 - loglik: -1.7245e+02 - logprior: -2.2900e+00
Epoch 8/10
10/10 - 1s - loss: 174.7202 - loglik: -1.7227e+02 - logprior: -2.1144e+00
Epoch 9/10
10/10 - 1s - loss: 174.5591 - loglik: -1.7216e+02 - logprior: -2.0718e+00
Epoch 10/10
10/10 - 1s - loss: 173.5938 - loglik: -1.7120e+02 - logprior: -2.0702e+00
Fitted a model with MAP estimate = -173.5922
expansions: [(0, 3), (11, 1), (12, 1), (34, 1), (36, 4), (47, 2), (50, 2), (60, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 205.5100 - loglik: -1.7045e+02 - logprior: -3.4724e+01
Epoch 2/2
10/10 - 1s - loss: 171.8260 - loglik: -1.6081e+02 - logprior: -1.0623e+01
Fitted a model with MAP estimate = -163.3481
expansions: [(59, 1)]
discards: [ 0 89]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 191.1562 - loglik: -1.5948e+02 - logprior: -3.1322e+01
Epoch 2/2
10/10 - 1s - loss: 168.7742 - loglik: -1.5585e+02 - logprior: -1.2658e+01
Fitted a model with MAP estimate = -164.5117
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 185.8924 - loglik: -1.5615e+02 - logprior: -2.9487e+01
Epoch 2/10
10/10 - 1s - loss: 163.4711 - loglik: -1.5459e+02 - logprior: -8.6535e+00
Epoch 3/10
10/10 - 1s - loss: 157.4984 - loglik: -1.5376e+02 - logprior: -3.4161e+00
Epoch 4/10
10/10 - 1s - loss: 154.4096 - loglik: -1.5224e+02 - logprior: -1.7700e+00
Epoch 5/10
10/10 - 1s - loss: 153.6483 - loglik: -1.5210e+02 - logprior: -1.1524e+00
Epoch 6/10
10/10 - 1s - loss: 152.4793 - loglik: -1.5119e+02 - logprior: -8.9335e-01
Epoch 7/10
10/10 - 1s - loss: 152.1038 - loglik: -1.5114e+02 - logprior: -5.6877e-01
Epoch 8/10
10/10 - 1s - loss: 151.7205 - loglik: -1.5103e+02 - logprior: -2.8906e-01
Epoch 9/10
10/10 - 1s - loss: 151.6288 - loglik: -1.5108e+02 - logprior: -1.5193e-01
Epoch 10/10
10/10 - 1s - loss: 151.2951 - loglik: -1.5083e+02 - logprior: -7.2629e-02
Fitted a model with MAP estimate = -150.7540
Time for alignment: 48.4490
Computed alignments with likelihoods: ['-153.1205', '-149.8610', '-150.7540']
Best model has likelihood: -149.8610
time for generating output: 0.1500
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.8
Training of 3 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f19590a65b0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18340f73d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f100cc49be0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18565703a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f15efac9f10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f1028767fd0>, <__main__.SimpleDirichletPrior object at 0x7f1881058f10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f180e1e7af0>

Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 151.3783 - loglik: -1.4624e+02 - logprior: -5.1336e+00
Epoch 2/10
16/16 - 2s - loss: 125.1777 - loglik: -1.2353e+02 - logprior: -1.5911e+00
Epoch 3/10
16/16 - 2s - loss: 113.8044 - loglik: -1.1187e+02 - logprior: -1.7129e+00
Epoch 4/10
16/16 - 2s - loss: 109.9686 - loglik: -1.0789e+02 - logprior: -1.7912e+00
Epoch 5/10
16/16 - 2s - loss: 108.6929 - loglik: -1.0667e+02 - logprior: -1.7528e+00
Epoch 6/10
16/16 - 2s - loss: 107.8260 - loglik: -1.0584e+02 - logprior: -1.7155e+00
Epoch 7/10
16/16 - 2s - loss: 107.1922 - loglik: -1.0524e+02 - logprior: -1.6770e+00
Epoch 8/10
16/16 - 2s - loss: 107.8792 - loglik: -1.0593e+02 - logprior: -1.6687e+00
Fitted a model with MAP estimate = -106.9897
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (14, 1), (16, 1), (23, 7)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 113.9275 - loglik: -1.0729e+02 - logprior: -6.3259e+00
Epoch 2/2
16/16 - 2s - loss: 104.9283 - loglik: -1.0142e+02 - logprior: -3.1398e+00
Fitted a model with MAP estimate = -103.4209
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.4669 - loglik: -1.0144e+02 - logprior: -4.7107e+00
Epoch 2/2
16/16 - 2s - loss: 102.0861 - loglik: -1.0004e+02 - logprior: -1.7416e+00
Fitted a model with MAP estimate = -101.1081
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 108.5915 - loglik: -1.0200e+02 - logprior: -6.2899e+00
Epoch 2/10
16/16 - 2s - loss: 104.0398 - loglik: -1.0087e+02 - logprior: -2.8677e+00
Epoch 3/10
16/16 - 2s - loss: 101.9190 - loglik: -9.9854e+01 - logprior: -1.6691e+00
Epoch 4/10
16/16 - 2s - loss: 101.1225 - loglik: -9.9254e+01 - logprior: -1.4260e+00
Epoch 5/10
16/16 - 2s - loss: 100.0586 - loglik: -9.8206e+01 - logprior: -1.4275e+00
Epoch 6/10
16/16 - 2s - loss: 99.7903 - loglik: -9.7968e+01 - logprior: -1.4145e+00
Epoch 7/10
16/16 - 2s - loss: 99.7000 - loglik: -9.7923e+01 - logprior: -1.3927e+00
Epoch 8/10
16/16 - 2s - loss: 99.0569 - loglik: -9.7316e+01 - logprior: -1.3677e+00
Epoch 9/10
16/16 - 2s - loss: 99.0106 - loglik: -9.7300e+01 - logprior: -1.3481e+00
Epoch 10/10
16/16 - 2s - loss: 99.0340 - loglik: -9.7340e+01 - logprior: -1.3347e+00
Fitted a model with MAP estimate = -98.5311
Time for alignment: 65.8812
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.3255 - loglik: -1.4618e+02 - logprior: -5.1374e+00
Epoch 2/10
16/16 - 2s - loss: 128.4464 - loglik: -1.2676e+02 - logprior: -1.6090e+00
Epoch 3/10
16/16 - 2s - loss: 118.2208 - loglik: -1.1619e+02 - logprior: -1.7194e+00
Epoch 4/10
16/16 - 2s - loss: 113.8765 - loglik: -1.1172e+02 - logprior: -1.8076e+00
Epoch 5/10
16/16 - 2s - loss: 111.1913 - loglik: -1.0907e+02 - logprior: -1.7897e+00
Epoch 6/10
16/16 - 2s - loss: 110.3049 - loglik: -1.0823e+02 - logprior: -1.7610e+00
Epoch 7/10
16/16 - 2s - loss: 109.8072 - loglik: -1.0777e+02 - logprior: -1.7159e+00
Epoch 8/10
16/16 - 2s - loss: 109.5700 - loglik: -1.0754e+02 - logprior: -1.7026e+00
Epoch 9/10
16/16 - 2s - loss: 109.4168 - loglik: -1.0740e+02 - logprior: -1.6869e+00
Epoch 10/10
16/16 - 2s - loss: 109.4594 - loglik: -1.0745e+02 - logprior: -1.6850e+00
Fitted a model with MAP estimate = -108.9283
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (16, 3), (22, 1), (24, 2), (33, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 115.7100 - loglik: -1.0898e+02 - logprior: -6.3707e+00
Epoch 2/2
16/16 - 2s - loss: 105.7739 - loglik: -1.0210e+02 - logprior: -3.2101e+00
Fitted a model with MAP estimate = -103.8767
expansions: [(0, 1)]
discards: [ 0 21]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 107.0886 - loglik: -1.0204e+02 - logprior: -4.6984e+00
Epoch 2/2
16/16 - 2s - loss: 102.4344 - loglik: -1.0039e+02 - logprior: -1.7182e+00
Fitted a model with MAP estimate = -101.6168
expansions: [(3, 1), (31, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 109.3749 - loglik: -1.0276e+02 - logprior: -6.2806e+00
Epoch 2/10
16/16 - 2s - loss: 104.2635 - loglik: -1.0105e+02 - logprior: -2.9200e+00
Epoch 3/10
16/16 - 2s - loss: 101.8780 - loglik: -9.9780e+01 - logprior: -1.6958e+00
Epoch 4/10
16/16 - 2s - loss: 101.1206 - loglik: -9.9280e+01 - logprior: -1.4006e+00
Epoch 5/10
16/16 - 2s - loss: 100.6956 - loglik: -9.8878e+01 - logprior: -1.3910e+00
Epoch 6/10
16/16 - 2s - loss: 99.8656 - loglik: -9.8079e+01 - logprior: -1.3761e+00
Epoch 7/10
16/16 - 2s - loss: 99.8929 - loglik: -9.8139e+01 - logprior: -1.3627e+00
Fitted a model with MAP estimate = -99.0628
Time for alignment: 61.1832
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 151.3150 - loglik: -1.4618e+02 - logprior: -5.1347e+00
Epoch 2/10
16/16 - 2s - loss: 125.7771 - loglik: -1.2411e+02 - logprior: -1.6089e+00
Epoch 3/10
16/16 - 2s - loss: 114.5224 - loglik: -1.1243e+02 - logprior: -1.7724e+00
Epoch 4/10
16/16 - 2s - loss: 110.4879 - loglik: -1.0838e+02 - logprior: -1.8292e+00
Epoch 5/10
16/16 - 2s - loss: 109.0481 - loglik: -1.0702e+02 - logprior: -1.7665e+00
Epoch 6/10
16/16 - 2s - loss: 108.5862 - loglik: -1.0659e+02 - logprior: -1.7396e+00
Epoch 7/10
16/16 - 2s - loss: 107.8137 - loglik: -1.0584e+02 - logprior: -1.7065e+00
Epoch 8/10
16/16 - 2s - loss: 107.9895 - loglik: -1.0602e+02 - logprior: -1.6965e+00
Fitted a model with MAP estimate = -107.3867
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 6), (25, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 114.6419 - loglik: -1.0799e+02 - logprior: -6.3377e+00
Epoch 2/2
16/16 - 2s - loss: 105.3259 - loglik: -1.0183e+02 - logprior: -3.1533e+00
Fitted a model with MAP estimate = -103.5977
expansions: [(0, 1)]
discards: [ 0 30 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.6972 - loglik: -1.0175e+02 - logprior: -4.6840e+00
Epoch 2/2
16/16 - 2s - loss: 102.3740 - loglik: -1.0034e+02 - logprior: -1.7324e+00
Fitted a model with MAP estimate = -101.3628
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 109.0325 - loglik: -1.0244e+02 - logprior: -6.2766e+00
Epoch 2/10
16/16 - 2s - loss: 104.2117 - loglik: -1.0107e+02 - logprior: -2.8293e+00
Epoch 3/10
16/16 - 2s - loss: 102.0813 - loglik: -1.0005e+02 - logprior: -1.6392e+00
Epoch 4/10
16/16 - 2s - loss: 100.7973 - loglik: -9.8934e+01 - logprior: -1.4122e+00
Epoch 5/10
16/16 - 2s - loss: 100.4729 - loglik: -9.8627e+01 - logprior: -1.4161e+00
Epoch 6/10
16/16 - 2s - loss: 99.9820 - loglik: -9.8175e+01 - logprior: -1.3952e+00
Epoch 7/10
16/16 - 2s - loss: 99.3982 - loglik: -9.7629e+01 - logprior: -1.3666e+00
Epoch 8/10
16/16 - 2s - loss: 99.4722 - loglik: -9.7731e+01 - logprior: -1.3556e+00
Fitted a model with MAP estimate = -98.8324
Time for alignment: 59.3787
Computed alignments with likelihoods: ['-98.5311', '-99.0628', '-98.8324']
Best model has likelihood: -98.5311
time for generating output: 0.1419
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.9159935379644588
Training of 3 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f18234173d0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1899ee5df0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19fe27ccd0>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f181aa70400>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f19484445e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19fcf0ab20>, <__main__.SimpleDirichletPrior object at 0x7f186f4f93d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f180e1e7af0>

Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 586.7502 - loglik: -5.8447e+02 - logprior: -1.7781e+00
Epoch 2/10
39/39 - 12s - loss: 536.5344 - loglik: -5.3374e+02 - logprior: -1.1427e+00
Epoch 3/10
39/39 - 12s - loss: 525.8532 - loglik: -5.2223e+02 - logprior: -1.2285e+00
Epoch 4/10
39/39 - 12s - loss: 520.5741 - loglik: -5.1701e+02 - logprior: -1.2773e+00
Epoch 5/10
39/39 - 12s - loss: 517.9848 - loglik: -5.1462e+02 - logprior: -1.3046e+00
Epoch 6/10
39/39 - 12s - loss: 516.2309 - loglik: -5.1311e+02 - logprior: -1.3212e+00
Epoch 7/10
39/39 - 13s - loss: 515.2404 - loglik: -5.1228e+02 - logprior: -1.3283e+00
Epoch 8/10
39/39 - 12s - loss: 514.2416 - loglik: -5.1138e+02 - logprior: -1.3518e+00
Epoch 9/10
39/39 - 12s - loss: 513.3994 - loglik: -5.1064e+02 - logprior: -1.3611e+00
Epoch 10/10
39/39 - 12s - loss: 512.7572 - loglik: -5.1010e+02 - logprior: -1.3567e+00
Fitted a model with MAP estimate = -493.9543
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (24, 4), (25, 2), (55, 1), (56, 2), (70, 1), (74, 12), (77, 2), (86, 1), (87, 2), (90, 1), (93, 1), (109, 1), (118, 1), (120, 1), (122, 5), (123, 2), (125, 1)]
discards: [  2 136 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 198 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 542.1631 - loglik: -5.3784e+02 - logprior: -2.2427e+00
Epoch 2/2
39/39 - 15s - loss: 516.3120 - loglik: -5.1205e+02 - logprior: -1.2163e+00
Fitted a model with MAP estimate = -466.8719
expansions: [(2, 1), (41, 2), (160, 1), (162, 1), (181, 8)]
discards: [  0  24  68 105 118 154 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 204 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 521.1432 - loglik: -5.1423e+02 - logprior: -3.0019e+00
Epoch 2/2
39/39 - 16s - loss: 511.6951 - loglik: -5.0642e+02 - logprior: -1.4267e+00
Fitted a model with MAP estimate = -464.2175
expansions: [(2, 1), (28, 1), (103, 5)]
discards: [ 0 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 209 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 26s - loss: 466.9869 - loglik: -4.6066e+02 - logprior: -1.5491e+00
Epoch 2/10
51/51 - 21s - loss: 459.9769 - loglik: -4.5502e+02 - logprior: -8.0668e-01
Epoch 3/10
51/51 - 21s - loss: 456.2578 - loglik: -4.5160e+02 - logprior: -7.8961e-01
Epoch 4/10
51/51 - 20s - loss: 453.0848 - loglik: -4.4868e+02 - logprior: -8.8338e-01
Epoch 5/10
51/51 - 21s - loss: 450.8425 - loglik: -4.4711e+02 - logprior: -8.2061e-01
Epoch 6/10
51/51 - 20s - loss: 448.4038 - loglik: -4.4506e+02 - logprior: -7.7269e-01
Epoch 7/10
51/51 - 20s - loss: 447.3520 - loglik: -4.4440e+02 - logprior: -7.0473e-01
Epoch 8/10
51/51 - 21s - loss: 446.0626 - loglik: -4.4344e+02 - logprior: -6.5150e-01
Epoch 9/10
51/51 - 20s - loss: 444.9275 - loglik: -4.4266e+02 - logprior: -5.9092e-01
Epoch 10/10
51/51 - 20s - loss: 445.3227 - loglik: -4.4320e+02 - logprior: -5.4014e-01
Fitted a model with MAP estimate = -442.3573
Time for alignment: 500.7057
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 588.8178 - loglik: -5.8653e+02 - logprior: -1.7846e+00
Epoch 2/10
39/39 - 12s - loss: 535.7100 - loglik: -5.3228e+02 - logprior: -1.2566e+00
Epoch 3/10
39/39 - 12s - loss: 523.2341 - loglik: -5.1917e+02 - logprior: -1.3561e+00
Epoch 4/10
39/39 - 12s - loss: 519.1497 - loglik: -5.1540e+02 - logprior: -1.3409e+00
Epoch 5/10
39/39 - 12s - loss: 517.0143 - loglik: -5.1358e+02 - logprior: -1.3620e+00
Epoch 6/10
39/39 - 12s - loss: 515.3182 - loglik: -5.1217e+02 - logprior: -1.3687e+00
Epoch 7/10
39/39 - 12s - loss: 514.2083 - loglik: -5.1127e+02 - logprior: -1.3947e+00
Epoch 8/10
39/39 - 12s - loss: 513.5176 - loglik: -5.1069e+02 - logprior: -1.4288e+00
Epoch 9/10
39/39 - 13s - loss: 512.9008 - loglik: -5.1016e+02 - logprior: -1.4512e+00
Epoch 10/10
39/39 - 12s - loss: 512.2242 - loglik: -5.0956e+02 - logprior: -1.4532e+00
Fitted a model with MAP estimate = -492.9106
expansions: [(6, 1), (7, 1), (11, 1), (13, 1), (21, 2), (22, 1), (24, 1), (27, 1), (30, 1), (54, 1), (55, 1), (70, 2), (73, 14), (76, 3), (77, 1), (80, 1), (121, 1), (123, 5), (124, 1), (125, 4), (127, 1)]
discards: [  2 134 135 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 541.7308 - loglik: -5.3760e+02 - logprior: -2.2494e+00
Epoch 2/2
39/39 - 15s - loss: 516.1717 - loglik: -5.1218e+02 - logprior: -1.1712e+00
Fitted a model with MAP estimate = -469.1112
expansions: [(2, 1), (166, 1)]
discards: [  0 154 155]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 522.7345 - loglik: -5.1597e+02 - logprior: -3.0135e+00
Epoch 2/2
39/39 - 15s - loss: 515.0530 - loglik: -5.0969e+02 - logprior: -1.3836e+00
Fitted a model with MAP estimate = -467.9509
expansions: [(0, 2), (159, 1)]
discards: [  0  92 160]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 195 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 22s - loss: 468.9262 - loglik: -4.6250e+02 - logprior: -1.3712e+00
Epoch 2/10
51/51 - 19s - loss: 463.5136 - loglik: -4.5830e+02 - logprior: -9.3012e-01
Epoch 3/10
51/51 - 19s - loss: 459.2945 - loglik: -4.5437e+02 - logprior: -8.6558e-01
Epoch 4/10
51/51 - 19s - loss: 455.7692 - loglik: -4.5135e+02 - logprior: -8.0155e-01
Epoch 5/10
51/51 - 19s - loss: 454.0481 - loglik: -4.5033e+02 - logprior: -7.4796e-01
Epoch 6/10
51/51 - 19s - loss: 451.2370 - loglik: -4.4791e+02 - logprior: -6.9219e-01
Epoch 7/10
51/51 - 19s - loss: 450.2794 - loglik: -4.4738e+02 - logprior: -6.3066e-01
Epoch 8/10
51/51 - 19s - loss: 448.9055 - loglik: -4.4635e+02 - logprior: -5.7354e-01
Epoch 9/10
51/51 - 19s - loss: 448.5544 - loglik: -4.4634e+02 - logprior: -5.1010e-01
Epoch 10/10
51/51 - 19s - loss: 447.8834 - loglik: -4.4582e+02 - logprior: -4.4435e-01
Fitted a model with MAP estimate = -445.4447
Time for alignment: 481.3990
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 586.5748 - loglik: -5.8431e+02 - logprior: -1.7722e+00
Epoch 2/10
39/39 - 12s - loss: 535.8911 - loglik: -5.3294e+02 - logprior: -1.2481e+00
Epoch 3/10
39/39 - 12s - loss: 525.4360 - loglik: -5.2169e+02 - logprior: -1.2976e+00
Epoch 4/10
39/39 - 12s - loss: 521.4974 - loglik: -5.1784e+02 - logprior: -1.2801e+00
Epoch 5/10
39/39 - 12s - loss: 518.9742 - loglik: -5.1551e+02 - logprior: -1.3047e+00
Epoch 6/10
39/39 - 12s - loss: 517.2032 - loglik: -5.1398e+02 - logprior: -1.3124e+00
Epoch 7/10
39/39 - 12s - loss: 515.7808 - loglik: -5.1281e+02 - logprior: -1.3124e+00
Epoch 8/10
39/39 - 12s - loss: 514.8442 - loglik: -5.1202e+02 - logprior: -1.3386e+00
Epoch 9/10
39/39 - 12s - loss: 513.9982 - loglik: -5.1130e+02 - logprior: -1.3566e+00
Epoch 10/10
39/39 - 12s - loss: 513.0704 - loglik: -5.1043e+02 - logprior: -1.3560e+00
Fitted a model with MAP estimate = -493.7765
expansions: [(6, 1), (7, 1), (10, 2), (11, 1), (21, 3), (24, 3), (25, 2), (54, 1), (55, 1), (70, 2), (72, 1), (73, 1), (74, 16), (75, 1), (106, 1), (108, 1), (120, 4), (125, 1)]
discards: [  2  95  96  97 138]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 544.9988 - loglik: -5.4075e+02 - logprior: -2.2934e+00
Epoch 2/2
39/39 - 15s - loss: 519.1638 - loglik: -5.1503e+02 - logprior: -1.3060e+00
Fitted a model with MAP estimate = -470.3002
expansions: [(2, 1), (42, 2), (156, 5)]
discards: [  0  12  31  84  91  98  99 100 108 109 110 111 112 113 164 165 166 167
 168 169 170 171 172 173 174 175 176 177 178 179]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 527.2697 - loglik: -5.2078e+02 - logprior: -2.7933e+00
Epoch 2/2
39/39 - 14s - loss: 518.3001 - loglik: -5.1354e+02 - logprior: -1.1795e+00
Fitted a model with MAP estimate = -469.1838
expansions: [(2, 1), (149, 1), (150, 3), (152, 2), (153, 9)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 21s - loss: 469.9379 - loglik: -4.6407e+02 - logprior: -1.4655e+00
Epoch 2/10
51/51 - 19s - loss: 462.5257 - loglik: -4.5784e+02 - logprior: -8.6479e-01
Epoch 3/10
51/51 - 18s - loss: 459.2719 - loglik: -4.5488e+02 - logprior: -8.0739e-01
Epoch 4/10
51/51 - 18s - loss: 456.3055 - loglik: -4.5223e+02 - logprior: -7.6248e-01
Epoch 5/10
51/51 - 18s - loss: 453.8958 - loglik: -4.5036e+02 - logprior: -7.2296e-01
Epoch 6/10
51/51 - 19s - loss: 451.2166 - loglik: -4.4801e+02 - logprior: -6.7342e-01
Epoch 7/10
51/51 - 18s - loss: 450.5045 - loglik: -4.4767e+02 - logprior: -6.3038e-01
Epoch 8/10
51/51 - 18s - loss: 450.5811 - loglik: -4.4809e+02 - logprior: -5.8418e-01
Fitted a model with MAP estimate = -447.1782
Time for alignment: 429.5403
Computed alignments with likelihoods: ['-442.3573', '-445.4447', '-447.1782']
Best model has likelihood: -442.3573
time for generating output: 0.2993
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.6760104302477183
Training of 3 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f17a01b8a00>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f17a01b83d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a01636d0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01635b0>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163f40>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f17a0163100>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01630a0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163910>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163730>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01632b0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163640>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01636a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163550>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01631f0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a01633a0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a0163ee0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f17a0163460> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f180da71910>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f18a2e644c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f18a2e64a00>, <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f1014680760>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f17a01e2ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f17a00e3580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f1833d032e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7f1a1b41c160> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7f18b7b0b430>, <function make_default_emission_matrix at 0x7f18b7b0b430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f19ae0d4e20>, <__main__.SimpleDirichletPrior object at 0x7f10146e6340>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f19fcf120d0>

Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 490.5177 - loglik: -4.8435e+02 - logprior: -6.1051e+00
Epoch 2/10
14/14 - 4s - loss: 436.6322 - loglik: -4.3476e+02 - logprior: -1.4260e+00
Epoch 3/10
14/14 - 4s - loss: 400.7080 - loglik: -3.9820e+02 - logprior: -1.5561e+00
Epoch 4/10
14/14 - 4s - loss: 387.4002 - loglik: -3.8448e+02 - logprior: -1.6532e+00
Epoch 5/10
14/14 - 4s - loss: 383.5350 - loglik: -3.8085e+02 - logprior: -1.6479e+00
Epoch 6/10
14/14 - 4s - loss: 380.9771 - loglik: -3.7861e+02 - logprior: -1.6131e+00
Epoch 7/10
14/14 - 4s - loss: 379.5263 - loglik: -3.7729e+02 - logprior: -1.6125e+00
Epoch 8/10
14/14 - 4s - loss: 378.8955 - loglik: -3.7673e+02 - logprior: -1.6290e+00
Epoch 9/10
14/14 - 4s - loss: 378.5112 - loglik: -3.7633e+02 - logprior: -1.6571e+00
Epoch 10/10
14/14 - 4s - loss: 376.5152 - loglik: -3.7431e+02 - logprior: -1.6960e+00
Fitted a model with MAP estimate = -376.8228
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (69, 1), (70, 2), (71, 2), (72, 1), (76, 1), (81, 1), (102, 5), (118, 1), (120, 2), (128, 1), (131, 1), (133, 1), (134, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 381.7724 - loglik: -3.7624e+02 - logprior: -4.7063e+00
Epoch 2/2
29/29 - 7s - loss: 363.6119 - loglik: -3.6099e+02 - logprior: -1.4809e+00
Fitted a model with MAP estimate = -358.8814
expansions: [(89, 1), (129, 1), (133, 1)]
discards: [ 41  92  94 151]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 11s - loss: 368.0188 - loglik: -3.6328e+02 - logprior: -3.5061e+00
Epoch 2/2
29/29 - 7s - loss: 361.0701 - loglik: -3.5897e+02 - logprior: -1.0406e+00
Fitted a model with MAP estimate = -358.2989
expansions: []
discards: [  0 114]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 370.8577 - loglik: -3.6458e+02 - logprior: -5.0388e+00
Epoch 2/10
29/29 - 7s - loss: 363.0924 - loglik: -3.5996e+02 - logprior: -2.0386e+00
Epoch 3/10
29/29 - 7s - loss: 359.9381 - loglik: -3.5827e+02 - logprior: -5.8728e-01
Epoch 4/10
29/29 - 7s - loss: 359.3497 - loglik: -3.5794e+02 - logprior: -4.3780e-01
Epoch 5/10
29/29 - 7s - loss: 356.3911 - loglik: -3.5518e+02 - logprior: -3.5344e-01
Epoch 6/10
29/29 - 7s - loss: 356.6871 - loglik: -3.5567e+02 - logprior: -2.4616e-01
Fitted a model with MAP estimate = -355.2128
Time for alignment: 160.8433
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 491.6651 - loglik: -4.8551e+02 - logprior: -6.1018e+00
Epoch 2/10
14/14 - 4s - loss: 437.0891 - loglik: -4.3522e+02 - logprior: -1.4176e+00
Epoch 3/10
14/14 - 4s - loss: 399.0363 - loglik: -3.9654e+02 - logprior: -1.5660e+00
Epoch 4/10
14/14 - 4s - loss: 387.6596 - loglik: -3.8480e+02 - logprior: -1.6387e+00
Epoch 5/10
14/14 - 4s - loss: 383.0352 - loglik: -3.8038e+02 - logprior: -1.6269e+00
Epoch 6/10
14/14 - 4s - loss: 382.1190 - loglik: -3.7976e+02 - logprior: -1.6070e+00
Epoch 7/10
14/14 - 4s - loss: 380.1891 - loglik: -3.7798e+02 - logprior: -1.5927e+00
Epoch 8/10
14/14 - 4s - loss: 379.1959 - loglik: -3.7704e+02 - logprior: -1.6040e+00
Epoch 9/10
14/14 - 4s - loss: 378.5434 - loglik: -3.7642e+02 - logprior: -1.5997e+00
Epoch 10/10
14/14 - 4s - loss: 378.4125 - loglik: -3.7631e+02 - logprior: -1.5945e+00
Fitted a model with MAP estimate = -377.7128
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (61, 1), (69, 1), (71, 1), (77, 1), (82, 1), (102, 5), (117, 1), (120, 2), (128, 1), (131, 1), (133, 3), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 12s - loss: 382.5592 - loglik: -3.7706e+02 - logprior: -4.6548e+00
Epoch 2/2
29/29 - 7s - loss: 364.8907 - loglik: -3.6232e+02 - logprior: -1.3742e+00
Fitted a model with MAP estimate = -360.3424
expansions: [(125, 1), (129, 1), (165, 1), (170, 1)]
discards: [ 41 147]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 368.2534 - loglik: -3.6349e+02 - logprior: -3.4555e+00
Epoch 2/2
29/29 - 7s - loss: 361.2397 - loglik: -3.5917e+02 - logprior: -9.5599e-01
Fitted a model with MAP estimate = -358.2886
expansions: [(2, 1)]
discards: [ 0 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 370.0555 - loglik: -3.6380e+02 - logprior: -4.9867e+00
Epoch 2/10
29/29 - 7s - loss: 363.2581 - loglik: -3.6012e+02 - logprior: -2.0275e+00
Epoch 3/10
29/29 - 7s - loss: 359.9581 - loglik: -3.5827e+02 - logprior: -5.7407e-01
Epoch 4/10
29/29 - 7s - loss: 358.0272 - loglik: -3.5658e+02 - logprior: -4.8056e-01
Epoch 5/10
29/29 - 7s - loss: 356.6552 - loglik: -3.5553e+02 - logprior: -2.8411e-01
Epoch 6/10
29/29 - 7s - loss: 357.1229 - loglik: -3.5608e+02 - logprior: -3.1118e-01
Fitted a model with MAP estimate = -354.7533
Time for alignment: 160.0720
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 491.5437 - loglik: -4.8537e+02 - logprior: -6.1130e+00
Epoch 2/10
14/14 - 4s - loss: 434.9593 - loglik: -4.3309e+02 - logprior: -1.4242e+00
Epoch 3/10
14/14 - 4s - loss: 399.4403 - loglik: -3.9693e+02 - logprior: -1.5603e+00
Epoch 4/10
14/14 - 4s - loss: 387.4278 - loglik: -3.8459e+02 - logprior: -1.5780e+00
Epoch 5/10
14/14 - 4s - loss: 382.7977 - loglik: -3.8022e+02 - logprior: -1.5549e+00
Epoch 6/10
14/14 - 4s - loss: 381.4075 - loglik: -3.7908e+02 - logprior: -1.5477e+00
Epoch 7/10
14/14 - 4s - loss: 380.4767 - loglik: -3.7837e+02 - logprior: -1.5077e+00
Epoch 8/10
14/14 - 4s - loss: 379.0470 - loglik: -3.7699e+02 - logprior: -1.5241e+00
Epoch 9/10
14/14 - 4s - loss: 378.2543 - loglik: -3.7623e+02 - logprior: -1.5527e+00
Epoch 10/10
14/14 - 4s - loss: 378.8826 - loglik: -3.7689e+02 - logprior: -1.5593e+00
Fitted a model with MAP estimate = -377.4190
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 1), (69, 1), (71, 3), (74, 1), (76, 1), (102, 6), (117, 1), (120, 2), (127, 2), (128, 1), (131, 1), (133, 1), (134, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 12s - loss: 380.2801 - loglik: -3.7482e+02 - logprior: -4.6677e+00
Epoch 2/2
29/29 - 7s - loss: 361.2069 - loglik: -3.5861e+02 - logprior: -1.4377e+00
Fitted a model with MAP estimate = -357.6182
expansions: [(132, 1), (160, 1)]
discards: [ 41  91  98 150]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 10s - loss: 367.6227 - loglik: -3.6291e+02 - logprior: -3.4958e+00
Epoch 2/2
29/29 - 7s - loss: 360.4064 - loglik: -3.5830e+02 - logprior: -1.0169e+00
Fitted a model with MAP estimate = -357.7898
expansions: [(96, 1)]
discards: [90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 366.1872 - loglik: -3.6162e+02 - logprior: -3.3154e+00
Epoch 2/10
29/29 - 7s - loss: 360.4163 - loglik: -3.5851e+02 - logprior: -7.7798e-01
Epoch 3/10
29/29 - 7s - loss: 358.1641 - loglik: -3.5641e+02 - logprior: -6.3971e-01
Epoch 4/10
29/29 - 7s - loss: 356.6578 - loglik: -3.5515e+02 - logprior: -5.2597e-01
Epoch 5/10
29/29 - 7s - loss: 357.0130 - loglik: -3.5574e+02 - logprior: -4.2506e-01
Fitted a model with MAP estimate = -354.4119
Time for alignment: 153.9672
Computed alignments with likelihoods: ['-355.2128', '-354.7533', '-354.4119']
Best model has likelihood: -354.4119
time for generating output: 0.3043
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.8121309101992609
