Training of 3 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb3bd13370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb3bd13fa0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 771.0783 - loglik: -7.6955e+02 - logprior: -1.5239e+00
Epoch 2/10
39/39 - 36s - loss: 645.1856 - loglik: -6.4335e+02 - logprior: -1.8364e+00
Epoch 3/10
39/39 - 38s - loss: 635.3918 - loglik: -6.3359e+02 - logprior: -1.8059e+00
Epoch 4/10
39/39 - 40s - loss: 632.7476 - loglik: -6.3099e+02 - logprior: -1.7621e+00
Epoch 5/10
39/39 - 41s - loss: 631.3514 - loglik: -6.2957e+02 - logprior: -1.7797e+00
Epoch 6/10
39/39 - 40s - loss: 630.6675 - loglik: -6.2887e+02 - logprior: -1.7981e+00
Epoch 7/10
39/39 - 40s - loss: 630.8436 - loglik: -6.2903e+02 - logprior: -1.8104e+00
Fitted a model with MAP estimate = -630.1482
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (46, 1), (52, 5), (55, 1), (61, 1), (62, 1), (63, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (83, 1), (84, 1), (85, 1), (86, 1), (91, 1), (95, 1), (98, 1), (99, 1), (100, 1), (113, 1), (118, 1), (119, 1), (120, 1), (121, 1), (135, 1), (140, 1), (142, 1), (143, 1), (158, 1), (159, 1), (160, 1), (162, 1), (164, 2), (165, 1), (173, 1), (184, 1), (188, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (214, 1), (217, 1), (218, 1), (220, 1), (229, 1), (235, 1), (236, 2), (262, 1), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (279, 2), (280, 3)]
discards: [1]
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 620.5377 - loglik: -6.1897e+02 - logprior: -1.5694e+00
Epoch 2/2
39/39 - 61s - loss: 609.3112 - loglik: -6.0891e+02 - logprior: -4.0354e-01
Fitted a model with MAP estimate = -606.6054
expansions: [(185, 1), (203, 1)]
discards: [249 335 336]
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 611.6159 - loglik: -6.1039e+02 - logprior: -1.2295e+00
Epoch 2/2
39/39 - 75s - loss: 608.1402 - loglik: -6.0804e+02 - logprior: -1.0479e-01
Fitted a model with MAP estimate = -605.5795
expansions: []
discards: []
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 80s - loss: 610.8052 - loglik: -6.0983e+02 - logprior: -9.7795e-01
Epoch 2/10
39/39 - 76s - loss: 607.2250 - loglik: -6.0735e+02 - logprior: 0.1231
Epoch 3/10
39/39 - 78s - loss: 605.3146 - loglik: -6.0561e+02 - logprior: 0.2933
Epoch 4/10
39/39 - 84s - loss: 602.7532 - loglik: -6.0311e+02 - logprior: 0.3596
Epoch 5/10
39/39 - 86s - loss: 601.3818 - loglik: -6.0185e+02 - logprior: 0.4704
Epoch 6/10
39/39 - 79s - loss: 600.3857 - loglik: -6.0099e+02 - logprior: 0.6048
Epoch 7/10
39/39 - 82s - loss: 600.4645 - loglik: -6.0111e+02 - logprior: 0.6482
Fitted a model with MAP estimate = -600.0359
Time for alignment: 1389.8523
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 770.0081 - loglik: -7.6847e+02 - logprior: -1.5428e+00
Epoch 2/10
39/39 - 41s - loss: 646.3256 - loglik: -6.4441e+02 - logprior: -1.9183e+00
Epoch 3/10
39/39 - 39s - loss: 635.4626 - loglik: -6.3357e+02 - logprior: -1.8924e+00
Epoch 4/10
39/39 - 40s - loss: 633.5103 - loglik: -6.3167e+02 - logprior: -1.8391e+00
Epoch 5/10
39/39 - 40s - loss: 631.4102 - loglik: -6.2955e+02 - logprior: -1.8630e+00
Epoch 6/10
39/39 - 41s - loss: 630.8058 - loglik: -6.2892e+02 - logprior: -1.8879e+00
Epoch 7/10
39/39 - 42s - loss: 631.1849 - loglik: -6.2925e+02 - logprior: -1.9375e+00
Fitted a model with MAP estimate = -630.4066
expansions: [(12, 2), (13, 1), (14, 1), (45, 1), (51, 3), (55, 1), (61, 1), (62, 1), (63, 1), (64, 1), (66, 1), (77, 1), (78, 1), (79, 1), (83, 1), (84, 1), (85, 1), (87, 1), (89, 1), (90, 1), (97, 1), (98, 1), (99, 1), (105, 1), (111, 1), (119, 1), (121, 1), (134, 1), (140, 1), (141, 1), (142, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (172, 1), (183, 1), (184, 1), (188, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (214, 1), (215, 1), (217, 1), (218, 1), (220, 1), (229, 1), (235, 1), (236, 2), (238, 1), (263, 1), (264, 1), (265, 1), (266, 3), (267, 2), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 80s - loss: 620.9307 - loglik: -6.1935e+02 - logprior: -1.5818e+00
Epoch 2/2
39/39 - 76s - loss: 608.8665 - loglik: -6.0839e+02 - logprior: -4.7830e-01
Fitted a model with MAP estimate = -606.4805
expansions: [(19, 1), (77, 1), (182, 1)]
discards: [143 247]
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 612.2819 - loglik: -6.1110e+02 - logprior: -1.1796e+00
Epoch 2/2
39/39 - 80s - loss: 607.9934 - loglik: -6.0792e+02 - logprior: -7.5298e-02
Fitted a model with MAP estimate = -605.5675
expansions: []
discards: [334 335]
Fitting a model of length 363 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 85s - loss: 611.5383 - loglik: -6.1056e+02 - logprior: -9.8063e-01
Epoch 2/10
39/39 - 78s - loss: 607.4440 - loglik: -6.0758e+02 - logprior: 0.1383
Epoch 3/10
39/39 - 75s - loss: 605.6572 - loglik: -6.0593e+02 - logprior: 0.2750
Epoch 4/10
39/39 - 75s - loss: 603.2820 - loglik: -6.0370e+02 - logprior: 0.4141
Epoch 5/10
39/39 - 84s - loss: 601.4930 - loglik: -6.0194e+02 - logprior: 0.4518
Epoch 6/10
39/39 - 89s - loss: 601.5136 - loglik: -6.0207e+02 - logprior: 0.5546
Fitted a model with MAP estimate = -600.5641
Time for alignment: 1391.9950
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 769.7654 - loglik: -7.6823e+02 - logprior: -1.5326e+00
Epoch 2/10
39/39 - 57s - loss: 646.1920 - loglik: -6.4428e+02 - logprior: -1.9115e+00
Epoch 3/10
39/39 - 55s - loss: 635.7461 - loglik: -6.3394e+02 - logprior: -1.8089e+00
Epoch 4/10
39/39 - 52s - loss: 632.9161 - loglik: -6.3114e+02 - logprior: -1.7752e+00
Epoch 5/10
39/39 - 51s - loss: 631.3018 - loglik: -6.2950e+02 - logprior: -1.7986e+00
Epoch 6/10
39/39 - 52s - loss: 630.8924 - loglik: -6.2907e+02 - logprior: -1.8208e+00
Epoch 7/10
39/39 - 49s - loss: 630.4427 - loglik: -6.2859e+02 - logprior: -1.8551e+00
Epoch 8/10
39/39 - 53s - loss: 631.1788 - loglik: -6.2931e+02 - logprior: -1.8685e+00
Fitted a model with MAP estimate = -630.0756
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (46, 1), (51, 3), (55, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (77, 1), (78, 1), (79, 1), (82, 1), (83, 1), (84, 1), (85, 1), (89, 1), (90, 1), (97, 1), (99, 1), (102, 1), (105, 1), (117, 1), (120, 1), (121, 1), (135, 1), (140, 1), (143, 1), (146, 1), (157, 1), (158, 1), (161, 1), (163, 1), (164, 1), (166, 1), (167, 1), (173, 1), (174, 1), (185, 1), (188, 2), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (213, 1), (214, 1), (217, 1), (219, 1), (227, 1), (228, 2), (235, 3), (263, 1), (264, 1), (265, 1), (266, 3), (267, 2), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 620.4354 - loglik: -6.1889e+02 - logprior: -1.5442e+00
Epoch 2/2
39/39 - 70s - loss: 608.3948 - loglik: -6.0793e+02 - logprior: -4.6431e-01
Fitted a model with MAP estimate = -605.6352
expansions: [(16, 1)]
discards: [249 286]
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 80s - loss: 611.4283 - loglik: -6.1027e+02 - logprior: -1.1576e+00
Epoch 2/2
39/39 - 71s - loss: 607.5023 - loglik: -6.0744e+02 - logprior: -6.0671e-02
Fitted a model with MAP estimate = -605.4068
expansions: []
discards: [335 336]
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 76s - loss: 610.9370 - loglik: -6.0996e+02 - logprior: -9.7961e-01
Epoch 2/10
39/39 - 72s - loss: 607.3765 - loglik: -6.0750e+02 - logprior: 0.1233
Epoch 3/10
39/39 - 69s - loss: 605.1518 - loglik: -6.0545e+02 - logprior: 0.2992
Epoch 4/10
39/39 - 68s - loss: 602.9363 - loglik: -6.0335e+02 - logprior: 0.4131
Epoch 5/10
39/39 - 73s - loss: 601.3103 - loglik: -6.0179e+02 - logprior: 0.4800
Epoch 6/10
39/39 - 83s - loss: 601.1038 - loglik: -6.0171e+02 - logprior: 0.6066
Epoch 7/10
39/39 - 87s - loss: 600.4019 - loglik: -6.0113e+02 - logprior: 0.7261
Epoch 8/10
39/39 - 91s - loss: 601.1013 - loglik: -6.0192e+02 - logprior: 0.8170
Fitted a model with MAP estimate = -599.9258
Time for alignment: 1629.7789
Computed alignments with likelihoods: ['-600.0359', '-600.5641', '-599.9258']
Best model has likelihood: -599.9258
SP score = 0.9434
Training of 3 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb199a9190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb199d7700>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 149.9902 - loglik: -1.4679e+02 - logprior: -3.1968e+00
Epoch 2/10
19/19 - 1s - loss: 128.6447 - loglik: -1.2727e+02 - logprior: -1.3761e+00
Epoch 3/10
19/19 - 1s - loss: 119.6812 - loglik: -1.1810e+02 - logprior: -1.5824e+00
Epoch 4/10
19/19 - 1s - loss: 118.1937 - loglik: -1.1674e+02 - logprior: -1.4562e+00
Epoch 5/10
19/19 - 1s - loss: 117.5797 - loglik: -1.1614e+02 - logprior: -1.4352e+00
Epoch 6/10
19/19 - 1s - loss: 117.4411 - loglik: -1.1604e+02 - logprior: -1.4029e+00
Epoch 7/10
19/19 - 1s - loss: 117.3141 - loglik: -1.1592e+02 - logprior: -1.3902e+00
Epoch 8/10
19/19 - 1s - loss: 117.1848 - loglik: -1.1580e+02 - logprior: -1.3831e+00
Epoch 9/10
19/19 - 1s - loss: 117.1438 - loglik: -1.1576e+02 - logprior: -1.3793e+00
Epoch 10/10
19/19 - 1s - loss: 117.1863 - loglik: -1.1581e+02 - logprior: -1.3719e+00
Fitted a model with MAP estimate = -116.9747
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (28, 2), (29, 2), (30, 2), (31, 2), (32, 3)]
discards: [0]
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.6475 - loglik: -1.1851e+02 - logprior: -4.1389e+00
Epoch 2/2
19/19 - 1s - loss: 114.0857 - loglik: -1.1195e+02 - logprior: -2.1338e+00
Fitted a model with MAP estimate = -112.6184
expansions: [(0, 2)]
discards: [ 0 22 34 36 40 45]
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.7662 - loglik: -1.1076e+02 - logprior: -3.0104e+00
Epoch 2/2
19/19 - 1s - loss: 110.6531 - loglik: -1.0945e+02 - logprior: -1.1985e+00
Fitted a model with MAP estimate = -110.0460
expansions: []
discards: [0]
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.3075 - loglik: -1.1157e+02 - logprior: -3.7397e+00
Epoch 2/10
19/19 - 1s - loss: 111.2181 - loglik: -1.0983e+02 - logprior: -1.3864e+00
Epoch 3/10
19/19 - 1s - loss: 110.5227 - loglik: -1.0930e+02 - logprior: -1.2214e+00
Epoch 4/10
19/19 - 1s - loss: 110.1924 - loglik: -1.0901e+02 - logprior: -1.1842e+00
Epoch 5/10
19/19 - 1s - loss: 109.7509 - loglik: -1.0858e+02 - logprior: -1.1750e+00
Epoch 6/10
19/19 - 1s - loss: 109.5414 - loglik: -1.0838e+02 - logprior: -1.1566e+00
Epoch 7/10
19/19 - 1s - loss: 109.4951 - loglik: -1.0834e+02 - logprior: -1.1542e+00
Epoch 8/10
19/19 - 1s - loss: 109.4230 - loglik: -1.0828e+02 - logprior: -1.1402e+00
Epoch 9/10
19/19 - 1s - loss: 109.3412 - loglik: -1.0822e+02 - logprior: -1.1187e+00
Epoch 10/10
19/19 - 1s - loss: 109.3245 - loglik: -1.0821e+02 - logprior: -1.1120e+00
Fitted a model with MAP estimate = -109.2585
Time for alignment: 42.8375
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0173 - loglik: -1.4682e+02 - logprior: -3.1993e+00
Epoch 2/10
19/19 - 1s - loss: 128.3927 - loglik: -1.2700e+02 - logprior: -1.3913e+00
Epoch 3/10
19/19 - 1s - loss: 119.5149 - loglik: -1.1792e+02 - logprior: -1.5926e+00
Epoch 4/10
19/19 - 1s - loss: 117.9474 - loglik: -1.1647e+02 - logprior: -1.4738e+00
Epoch 5/10
19/19 - 1s - loss: 117.4097 - loglik: -1.1595e+02 - logprior: -1.4575e+00
Epoch 6/10
19/19 - 1s - loss: 117.1369 - loglik: -1.1571e+02 - logprior: -1.4264e+00
Epoch 7/10
19/19 - 1s - loss: 117.1233 - loglik: -1.1571e+02 - logprior: -1.4165e+00
Epoch 8/10
19/19 - 1s - loss: 117.0367 - loglik: -1.1563e+02 - logprior: -1.4070e+00
Epoch 9/10
19/19 - 1s - loss: 116.9854 - loglik: -1.1558e+02 - logprior: -1.4039e+00
Epoch 10/10
19/19 - 1s - loss: 116.9298 - loglik: -1.1553e+02 - logprior: -1.3953e+00
Fitted a model with MAP estimate = -116.7784
expansions: [(6, 1), (7, 1), (8, 1), (16, 1), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.7986 - loglik: -1.1768e+02 - logprior: -4.1215e+00
Epoch 2/2
19/19 - 1s - loss: 113.9010 - loglik: -1.1183e+02 - logprior: -2.0715e+00
Fitted a model with MAP estimate = -112.5138
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.6711 - loglik: -1.1067e+02 - logprior: -3.0004e+00
Epoch 2/2
19/19 - 1s - loss: 110.5957 - loglik: -1.0940e+02 - logprior: -1.1978e+00
Fitted a model with MAP estimate = -110.0525
expansions: []
discards: [0]
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 115.2908 - loglik: -1.1154e+02 - logprior: -3.7469e+00
Epoch 2/10
19/19 - 1s - loss: 111.2337 - loglik: -1.0985e+02 - logprior: -1.3879e+00
Epoch 3/10
19/19 - 1s - loss: 110.5172 - loglik: -1.0930e+02 - logprior: -1.2213e+00
Epoch 4/10
19/19 - 1s - loss: 110.1252 - loglik: -1.0893e+02 - logprior: -1.1903e+00
Epoch 5/10
19/19 - 1s - loss: 109.7395 - loglik: -1.0856e+02 - logprior: -1.1745e+00
Epoch 6/10
19/19 - 1s - loss: 109.6351 - loglik: -1.0847e+02 - logprior: -1.1654e+00
Epoch 7/10
19/19 - 1s - loss: 109.4870 - loglik: -1.0834e+02 - logprior: -1.1503e+00
Epoch 8/10
19/19 - 1s - loss: 109.2952 - loglik: -1.0816e+02 - logprior: -1.1365e+00
Epoch 9/10
19/19 - 1s - loss: 109.4508 - loglik: -1.0832e+02 - logprior: -1.1272e+00
Fitted a model with MAP estimate = -109.2840
Time for alignment: 41.9412
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0030 - loglik: -1.4680e+02 - logprior: -3.1989e+00
Epoch 2/10
19/19 - 1s - loss: 128.2819 - loglik: -1.2692e+02 - logprior: -1.3631e+00
Epoch 3/10
19/19 - 1s - loss: 119.8808 - loglik: -1.1835e+02 - logprior: -1.5271e+00
Epoch 4/10
19/19 - 1s - loss: 118.2191 - loglik: -1.1680e+02 - logprior: -1.4161e+00
Epoch 5/10
19/19 - 1s - loss: 117.6639 - loglik: -1.1627e+02 - logprior: -1.3986e+00
Epoch 6/10
19/19 - 1s - loss: 117.4042 - loglik: -1.1603e+02 - logprior: -1.3719e+00
Epoch 7/10
19/19 - 1s - loss: 117.3763 - loglik: -1.1602e+02 - logprior: -1.3597e+00
Epoch 8/10
19/19 - 1s - loss: 117.2691 - loglik: -1.1592e+02 - logprior: -1.3520e+00
Epoch 9/10
19/19 - 1s - loss: 117.1806 - loglik: -1.1583e+02 - logprior: -1.3466e+00
Epoch 10/10
19/19 - 1s - loss: 117.2423 - loglik: -1.1590e+02 - logprior: -1.3434e+00
Fitted a model with MAP estimate = -117.0272
expansions: [(6, 3), (15, 1), (19, 2), (20, 1), (27, 2), (28, 2), (30, 1), (31, 2), (32, 2)]
discards: [0]
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.6255 - loglik: -1.1751e+02 - logprior: -4.1120e+00
Epoch 2/2
19/19 - 1s - loss: 113.9697 - loglik: -1.1185e+02 - logprior: -2.1155e+00
Fitted a model with MAP estimate = -112.6031
expansions: [(0, 2)]
discards: [ 0 23 34 36 42]
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.6142 - loglik: -1.1061e+02 - logprior: -3.0010e+00
Epoch 2/2
19/19 - 1s - loss: 110.6587 - loglik: -1.0946e+02 - logprior: -1.1990e+00
Fitted a model with MAP estimate = -110.0435
expansions: []
discards: [0]
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.3998 - loglik: -1.1164e+02 - logprior: -3.7598e+00
Epoch 2/10
19/19 - 1s - loss: 111.2517 - loglik: -1.0986e+02 - logprior: -1.3956e+00
Epoch 3/10
19/19 - 1s - loss: 110.4878 - loglik: -1.0927e+02 - logprior: -1.2212e+00
Epoch 4/10
19/19 - 1s - loss: 110.1172 - loglik: -1.0893e+02 - logprior: -1.1876e+00
Epoch 5/10
19/19 - 1s - loss: 109.8873 - loglik: -1.0871e+02 - logprior: -1.1757e+00
Epoch 6/10
19/19 - 1s - loss: 109.5067 - loglik: -1.0835e+02 - logprior: -1.1603e+00
Epoch 7/10
19/19 - 1s - loss: 109.5201 - loglik: -1.0837e+02 - logprior: -1.1506e+00
Fitted a model with MAP estimate = -109.3458
Time for alignment: 40.2035
Computed alignments with likelihoods: ['-109.2585', '-109.2840', '-109.3458']
Best model has likelihood: -109.2585
SP score = 0.6381
Training of 3 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdae6d46c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdaf794bfd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.2066 - loglik: -3.5720e+02 - logprior: -3.0041e+00
Epoch 2/10
19/19 - 3s - loss: 302.0317 - loglik: -3.0087e+02 - logprior: -1.1638e+00
Epoch 3/10
19/19 - 3s - loss: 278.7118 - loglik: -2.7745e+02 - logprior: -1.2611e+00
Epoch 4/10
19/19 - 3s - loss: 273.8290 - loglik: -2.7263e+02 - logprior: -1.1954e+00
Epoch 5/10
19/19 - 3s - loss: 271.6099 - loglik: -2.7045e+02 - logprior: -1.1594e+00
Epoch 6/10
19/19 - 3s - loss: 269.9854 - loglik: -2.6883e+02 - logprior: -1.1529e+00
Epoch 7/10
19/19 - 4s - loss: 269.0819 - loglik: -2.6793e+02 - logprior: -1.1485e+00
Epoch 8/10
19/19 - 3s - loss: 269.4510 - loglik: -2.6831e+02 - logprior: -1.1390e+00
Fitted a model with MAP estimate = -268.8774
expansions: [(12, 2), (14, 4), (27, 1), (28, 2), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (94, 1), (99, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 269.5431 - loglik: -2.6570e+02 - logprior: -3.8433e+00
Epoch 2/2
19/19 - 4s - loss: 259.3646 - loglik: -2.5748e+02 - logprior: -1.8886e+00
Fitted a model with MAP estimate = -257.4606
expansions: [(0, 3)]
discards: [ 0 12 17 38 76]
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 260.8348 - loglik: -2.5812e+02 - logprior: -2.7149e+00
Epoch 2/2
19/19 - 5s - loss: 257.3572 - loglik: -2.5639e+02 - logprior: -9.6486e-01
Fitted a model with MAP estimate = -256.5450
expansions: []
discards: [0 2]
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 262.1109 - loglik: -2.5840e+02 - logprior: -3.7078e+00
Epoch 2/10
19/19 - 5s - loss: 258.6866 - loglik: -2.5732e+02 - logprior: -1.3653e+00
Epoch 3/10
19/19 - 5s - loss: 256.6179 - loglik: -2.5593e+02 - logprior: -6.8374e-01
Epoch 4/10
19/19 - 5s - loss: 256.0381 - loglik: -2.5541e+02 - logprior: -6.3061e-01
Epoch 5/10
19/19 - 5s - loss: 255.4644 - loglik: -2.5484e+02 - logprior: -6.2259e-01
Epoch 6/10
19/19 - 5s - loss: 255.4478 - loglik: -2.5486e+02 - logprior: -5.8700e-01
Epoch 7/10
19/19 - 5s - loss: 254.7434 - loglik: -2.5417e+02 - logprior: -5.7333e-01
Epoch 8/10
19/19 - 5s - loss: 254.6832 - loglik: -2.5415e+02 - logprior: -5.3776e-01
Epoch 9/10
19/19 - 5s - loss: 254.7490 - loglik: -2.5427e+02 - logprior: -4.7915e-01
Fitted a model with MAP estimate = -254.4820
Time for alignment: 123.5461
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 360.4462 - loglik: -3.5744e+02 - logprior: -3.0060e+00
Epoch 2/10
19/19 - 3s - loss: 306.1308 - loglik: -3.0496e+02 - logprior: -1.1736e+00
Epoch 3/10
19/19 - 3s - loss: 279.6067 - loglik: -2.7832e+02 - logprior: -1.2837e+00
Epoch 4/10
19/19 - 3s - loss: 272.0252 - loglik: -2.7072e+02 - logprior: -1.3065e+00
Epoch 5/10
19/19 - 3s - loss: 270.5063 - loglik: -2.6926e+02 - logprior: -1.2443e+00
Epoch 6/10
19/19 - 3s - loss: 268.2847 - loglik: -2.6707e+02 - logprior: -1.2113e+00
Epoch 7/10
19/19 - 4s - loss: 268.1399 - loglik: -2.6696e+02 - logprior: -1.1816e+00
Epoch 8/10
19/19 - 4s - loss: 267.8571 - loglik: -2.6668e+02 - logprior: -1.1739e+00
Epoch 9/10
19/19 - 3s - loss: 268.0291 - loglik: -2.6687e+02 - logprior: -1.1571e+00
Fitted a model with MAP estimate = -267.3843
expansions: [(12, 2), (14, 4), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (60, 1), (70, 1), (71, 1), (93, 1), (101, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 269.4137 - loglik: -2.6557e+02 - logprior: -3.8470e+00
Epoch 2/2
19/19 - 5s - loss: 259.4923 - loglik: -2.5762e+02 - logprior: -1.8761e+00
Fitted a model with MAP estimate = -257.9185
expansions: [(0, 3)]
discards: [ 0 12 17 38]
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 260.9792 - loglik: -2.5828e+02 - logprior: -2.7004e+00
Epoch 2/2
19/19 - 5s - loss: 257.3520 - loglik: -2.5638e+02 - logprior: -9.6731e-01
Fitted a model with MAP estimate = -256.6593
expansions: []
discards: [0 2]
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 262.4201 - loglik: -2.5873e+02 - logprior: -3.6906e+00
Epoch 2/10
19/19 - 5s - loss: 258.6897 - loglik: -2.5728e+02 - logprior: -1.4095e+00
Epoch 3/10
19/19 - 5s - loss: 257.0672 - loglik: -2.5638e+02 - logprior: -6.8434e-01
Epoch 4/10
19/19 - 5s - loss: 256.4906 - loglik: -2.5589e+02 - logprior: -5.9908e-01
Epoch 5/10
19/19 - 5s - loss: 255.8762 - loglik: -2.5531e+02 - logprior: -5.6865e-01
Epoch 6/10
19/19 - 5s - loss: 255.3603 - loglik: -2.5481e+02 - logprior: -5.4820e-01
Epoch 7/10
19/19 - 5s - loss: 255.4729 - loglik: -2.5495e+02 - logprior: -5.1931e-01
Fitted a model with MAP estimate = -255.0377
Time for alignment: 122.2656
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.2872 - loglik: -3.5728e+02 - logprior: -3.0098e+00
Epoch 2/10
19/19 - 4s - loss: 302.3611 - loglik: -3.0117e+02 - logprior: -1.1924e+00
Epoch 3/10
19/19 - 4s - loss: 277.6948 - loglik: -2.7637e+02 - logprior: -1.3240e+00
Epoch 4/10
19/19 - 4s - loss: 272.6364 - loglik: -2.7134e+02 - logprior: -1.2991e+00
Epoch 5/10
19/19 - 4s - loss: 270.3184 - loglik: -2.6909e+02 - logprior: -1.2330e+00
Epoch 6/10
19/19 - 4s - loss: 269.5794 - loglik: -2.6837e+02 - logprior: -1.2141e+00
Epoch 7/10
19/19 - 4s - loss: 269.2264 - loglik: -2.6804e+02 - logprior: -1.1827e+00
Epoch 8/10
19/19 - 4s - loss: 268.7032 - loglik: -2.6754e+02 - logprior: -1.1653e+00
Epoch 9/10
19/19 - 4s - loss: 268.3351 - loglik: -2.6717e+02 - logprior: -1.1618e+00
Epoch 10/10
19/19 - 4s - loss: 268.2543 - loglik: -2.6709e+02 - logprior: -1.1678e+00
Fitted a model with MAP estimate = -268.1535
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (94, 1), (97, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 269.8393 - loglik: -2.6600e+02 - logprior: -3.8351e+00
Epoch 2/2
19/19 - 6s - loss: 260.2253 - loglik: -2.5838e+02 - logprior: -1.8418e+00
Fitted a model with MAP estimate = -258.5345
expansions: [(0, 3)]
discards: [ 0 37 75]
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 260.3246 - loglik: -2.5765e+02 - logprior: -2.6790e+00
Epoch 2/2
19/19 - 6s - loss: 256.6380 - loglik: -2.5569e+02 - logprior: -9.4836e-01
Fitted a model with MAP estimate = -255.9456
expansions: []
discards: [ 0  2 17]
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 262.0083 - loglik: -2.5836e+02 - logprior: -3.6451e+00
Epoch 2/10
19/19 - 5s - loss: 258.0795 - loglik: -2.5676e+02 - logprior: -1.3192e+00
Epoch 3/10
19/19 - 6s - loss: 256.9778 - loglik: -2.5635e+02 - logprior: -6.3070e-01
Epoch 4/10
19/19 - 6s - loss: 255.5155 - loglik: -2.5495e+02 - logprior: -5.7004e-01
Epoch 5/10
19/19 - 6s - loss: 255.7255 - loglik: -2.5517e+02 - logprior: -5.5615e-01
Fitted a model with MAP estimate = -254.9902
Time for alignment: 131.2243
Computed alignments with likelihoods: ['-254.4820', '-255.0377', '-254.9902']
Best model has likelihood: -254.4820
SP score = 0.7351
Training of 3 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb08c44a00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb3bbe6130>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.7876 - loglik: -2.7172e+02 - logprior: -3.0708e+00
Epoch 2/10
19/19 - 2s - loss: 236.8084 - loglik: -2.3556e+02 - logprior: -1.2443e+00
Epoch 3/10
19/19 - 2s - loss: 222.3965 - loglik: -2.2096e+02 - logprior: -1.4374e+00
Epoch 4/10
19/19 - 2s - loss: 219.4003 - loglik: -2.1787e+02 - logprior: -1.5313e+00
Epoch 5/10
19/19 - 3s - loss: 218.5377 - loglik: -2.1708e+02 - logprior: -1.4573e+00
Epoch 6/10
19/19 - 3s - loss: 217.8357 - loglik: -2.1639e+02 - logprior: -1.4431e+00
Epoch 7/10
19/19 - 3s - loss: 217.7745 - loglik: -2.1635e+02 - logprior: -1.4223e+00
Epoch 8/10
19/19 - 3s - loss: 217.5170 - loglik: -2.1610e+02 - logprior: -1.4201e+00
Epoch 9/10
19/19 - 3s - loss: 217.7518 - loglik: -2.1634e+02 - logprior: -1.4121e+00
Fitted a model with MAP estimate = -217.1440
expansions: [(5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (16, 1), (17, 1), (18, 1), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: [0]
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 219.6434 - loglik: -2.1573e+02 - logprior: -3.9174e+00
Epoch 2/2
19/19 - 3s - loss: 209.2955 - loglik: -2.0719e+02 - logprior: -2.1064e+00
Fitted a model with MAP estimate = -207.4103
expansions: [(0, 2)]
discards: [ 0 45 74 81 83 84 95]
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 210.3447 - loglik: -2.0744e+02 - logprior: -2.9055e+00
Epoch 2/2
19/19 - 3s - loss: 206.9800 - loglik: -2.0587e+02 - logprior: -1.1051e+00
Fitted a model with MAP estimate = -206.0413
expansions: []
discards: [0 1]
Fitting a model of length 98 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 211.7554 - loglik: -2.0796e+02 - logprior: -3.7921e+00
Epoch 2/10
19/19 - 3s - loss: 208.8124 - loglik: -2.0700e+02 - logprior: -1.8144e+00
Epoch 3/10
19/19 - 3s - loss: 206.9625 - loglik: -2.0590e+02 - logprior: -1.0620e+00
Epoch 4/10
19/19 - 3s - loss: 206.3800 - loglik: -2.0556e+02 - logprior: -8.2298e-01
Epoch 5/10
19/19 - 3s - loss: 205.8197 - loglik: -2.0499e+02 - logprior: -8.2638e-01
Epoch 6/10
19/19 - 3s - loss: 205.3670 - loglik: -2.0456e+02 - logprior: -8.0196e-01
Epoch 7/10
19/19 - 3s - loss: 205.4299 - loglik: -2.0466e+02 - logprior: -7.7461e-01
Fitted a model with MAP estimate = -205.1392
Time for alignment: 88.8900
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.6103 - loglik: -2.7153e+02 - logprior: -3.0785e+00
Epoch 2/10
19/19 - 2s - loss: 237.1761 - loglik: -2.3592e+02 - logprior: -1.2539e+00
Epoch 3/10
19/19 - 2s - loss: 221.2626 - loglik: -2.1981e+02 - logprior: -1.4572e+00
Epoch 4/10
19/19 - 2s - loss: 217.3176 - loglik: -2.1588e+02 - logprior: -1.4393e+00
Epoch 5/10
19/19 - 3s - loss: 215.6456 - loglik: -2.1427e+02 - logprior: -1.3804e+00
Epoch 6/10
19/19 - 3s - loss: 215.2494 - loglik: -2.1389e+02 - logprior: -1.3579e+00
Epoch 7/10
19/19 - 3s - loss: 215.4393 - loglik: -2.1412e+02 - logprior: -1.3233e+00
Fitted a model with MAP estimate = -214.6731
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 2), (64, 2), (70, 1), (71, 2), (77, 1)]
discards: []
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 217.3571 - loglik: -2.1357e+02 - logprior: -3.7911e+00
Epoch 2/2
19/19 - 4s - loss: 207.1822 - loglik: -2.0586e+02 - logprior: -1.3217e+00
Fitted a model with MAP estimate = -205.6445
expansions: []
discards: [ 0 46 75 79 85 96]
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 211.6351 - loglik: -2.0766e+02 - logprior: -3.9794e+00
Epoch 2/2
19/19 - 3s - loss: 207.3997 - loglik: -2.0589e+02 - logprior: -1.5136e+00
Fitted a model with MAP estimate = -206.0651
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 208.4292 - loglik: -2.0557e+02 - logprior: -2.8548e+00
Epoch 2/10
19/19 - 3s - loss: 206.0836 - loglik: -2.0503e+02 - logprior: -1.0531e+00
Epoch 3/10
19/19 - 3s - loss: 205.1797 - loglik: -2.0418e+02 - logprior: -1.0041e+00
Epoch 4/10
19/19 - 3s - loss: 205.0289 - loglik: -2.0407e+02 - logprior: -9.5987e-01
Epoch 5/10
19/19 - 3s - loss: 204.6054 - loglik: -2.0369e+02 - logprior: -9.1338e-01
Epoch 6/10
19/19 - 3s - loss: 204.1445 - loglik: -2.0324e+02 - logprior: -8.9952e-01
Epoch 7/10
19/19 - 4s - loss: 204.1895 - loglik: -2.0331e+02 - logprior: -8.8045e-01
Fitted a model with MAP estimate = -203.9110
Time for alignment: 83.8131
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 274.6191 - loglik: -2.7155e+02 - logprior: -3.0691e+00
Epoch 2/10
19/19 - 2s - loss: 236.3689 - loglik: -2.3512e+02 - logprior: -1.2467e+00
Epoch 3/10
19/19 - 3s - loss: 221.9417 - loglik: -2.2039e+02 - logprior: -1.5529e+00
Epoch 4/10
19/19 - 3s - loss: 219.0205 - loglik: -2.1745e+02 - logprior: -1.5714e+00
Epoch 5/10
19/19 - 3s - loss: 218.0629 - loglik: -2.1659e+02 - logprior: -1.4762e+00
Epoch 6/10
19/19 - 2s - loss: 217.5009 - loglik: -2.1602e+02 - logprior: -1.4766e+00
Epoch 7/10
19/19 - 3s - loss: 217.3017 - loglik: -2.1584e+02 - logprior: -1.4642e+00
Epoch 8/10
19/19 - 3s - loss: 216.8619 - loglik: -2.1541e+02 - logprior: -1.4538e+00
Epoch 9/10
19/19 - 3s - loss: 217.3757 - loglik: -2.1593e+02 - logprior: -1.4457e+00
Fitted a model with MAP estimate = -216.6678
expansions: [(5, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (28, 1), (29, 1), (33, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: [0]
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 219.0377 - loglik: -2.1512e+02 - logprior: -3.9179e+00
Epoch 2/2
19/19 - 4s - loss: 208.7887 - loglik: -2.0670e+02 - logprior: -2.0838e+00
Fitted a model with MAP estimate = -207.2602
expansions: [(0, 2)]
discards: [ 0 73 78 81 83 94]
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 209.9978 - loglik: -2.0712e+02 - logprior: -2.8738e+00
Epoch 2/2
19/19 - 3s - loss: 207.1317 - loglik: -2.0606e+02 - logprior: -1.0744e+00
Fitted a model with MAP estimate = -205.9978
expansions: []
discards: [0]
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 210.8293 - loglik: -2.0702e+02 - logprior: -3.8140e+00
Epoch 2/10
19/19 - 3s - loss: 207.7945 - loglik: -2.0621e+02 - logprior: -1.5875e+00
Epoch 3/10
19/19 - 3s - loss: 206.3362 - loglik: -2.0542e+02 - logprior: -9.2090e-01
Epoch 4/10
19/19 - 3s - loss: 205.9474 - loglik: -2.0504e+02 - logprior: -9.0779e-01
Epoch 5/10
19/19 - 3s - loss: 205.0817 - loglik: -2.0421e+02 - logprior: -8.6738e-01
Epoch 6/10
19/19 - 3s - loss: 205.3826 - loglik: -2.0452e+02 - logprior: -8.6584e-01
Fitted a model with MAP estimate = -204.8764
Time for alignment: 84.6951
Computed alignments with likelihoods: ['-205.1392', '-203.9110', '-204.8764']
Best model has likelihood: -203.9110
SP score = 0.6639
Training of 3 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb3b157fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb19b1dd00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.5035 - loglik: -1.7329e+02 - logprior: -3.2168e+00
Epoch 2/10
19/19 - 1s - loss: 134.0067 - loglik: -1.3254e+02 - logprior: -1.4624e+00
Epoch 3/10
19/19 - 1s - loss: 117.5068 - loglik: -1.1609e+02 - logprior: -1.4207e+00
Epoch 4/10
19/19 - 1s - loss: 114.8086 - loglik: -1.1339e+02 - logprior: -1.4182e+00
Epoch 5/10
19/19 - 1s - loss: 114.2967 - loglik: -1.1292e+02 - logprior: -1.3795e+00
Epoch 6/10
19/19 - 1s - loss: 113.5867 - loglik: -1.1222e+02 - logprior: -1.3638e+00
Epoch 7/10
19/19 - 1s - loss: 113.5018 - loglik: -1.1215e+02 - logprior: -1.3563e+00
Epoch 8/10
19/19 - 1s - loss: 113.4835 - loglik: -1.1214e+02 - logprior: -1.3475e+00
Epoch 9/10
19/19 - 1s - loss: 113.3726 - loglik: -1.1203e+02 - logprior: -1.3402e+00
Epoch 10/10
19/19 - 1s - loss: 113.1588 - loglik: -1.1182e+02 - logprior: -1.3371e+00
Fitted a model with MAP estimate = -113.2024
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.8820 - loglik: -1.0970e+02 - logprior: -4.1776e+00
Epoch 2/2
19/19 - 1s - loss: 104.9392 - loglik: -1.0368e+02 - logprior: -1.2604e+00
Fitted a model with MAP estimate = -103.7158
expansions: []
discards: [ 0 38]
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.2357 - loglik: -1.0521e+02 - logprior: -4.0298e+00
Epoch 2/2
19/19 - 1s - loss: 105.0578 - loglik: -1.0357e+02 - logprior: -1.4851e+00
Fitted a model with MAP estimate = -104.2797
expansions: []
discards: []
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.5061 - loglik: -1.0433e+02 - logprior: -3.1807e+00
Epoch 2/10
19/19 - 1s - loss: 104.5020 - loglik: -1.0312e+02 - logprior: -1.3806e+00
Epoch 3/10
19/19 - 1s - loss: 104.0025 - loglik: -1.0273e+02 - logprior: -1.2769e+00
Epoch 4/10
19/19 - 1s - loss: 103.3054 - loglik: -1.0208e+02 - logprior: -1.2291e+00
Epoch 5/10
19/19 - 1s - loss: 103.3806 - loglik: -1.0219e+02 - logprior: -1.1944e+00
Fitted a model with MAP estimate = -102.7406
Time for alignment: 42.6057
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.2522 - loglik: -1.7303e+02 - logprior: -3.2180e+00
Epoch 2/10
19/19 - 1s - loss: 133.8968 - loglik: -1.3245e+02 - logprior: -1.4488e+00
Epoch 3/10
19/19 - 1s - loss: 117.7028 - loglik: -1.1628e+02 - logprior: -1.4222e+00
Epoch 4/10
19/19 - 1s - loss: 115.0122 - loglik: -1.1357e+02 - logprior: -1.4409e+00
Epoch 5/10
19/19 - 1s - loss: 113.9577 - loglik: -1.1257e+02 - logprior: -1.3920e+00
Epoch 6/10
19/19 - 1s - loss: 113.5983 - loglik: -1.1223e+02 - logprior: -1.3641e+00
Epoch 7/10
19/19 - 1s - loss: 113.3401 - loglik: -1.1199e+02 - logprior: -1.3530e+00
Epoch 8/10
19/19 - 1s - loss: 113.2080 - loglik: -1.1187e+02 - logprior: -1.3413e+00
Epoch 9/10
19/19 - 1s - loss: 113.2040 - loglik: -1.1187e+02 - logprior: -1.3364e+00
Epoch 10/10
19/19 - 1s - loss: 113.1105 - loglik: -1.1178e+02 - logprior: -1.3334e+00
Fitted a model with MAP estimate = -113.1021
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (38, 1), (40, 1), (41, 1)]
discards: []
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.9476 - loglik: -1.0977e+02 - logprior: -4.1789e+00
Epoch 2/2
19/19 - 1s - loss: 105.0950 - loglik: -1.0384e+02 - logprior: -1.2570e+00
Fitted a model with MAP estimate = -103.8285
expansions: []
discards: [ 0 38]
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.4541 - loglik: -1.0541e+02 - logprior: -4.0466e+00
Epoch 2/2
19/19 - 1s - loss: 105.0987 - loglik: -1.0359e+02 - logprior: -1.5102e+00
Fitted a model with MAP estimate = -104.4262
expansions: []
discards: []
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.6950 - loglik: -1.0453e+02 - logprior: -3.1642e+00
Epoch 2/10
19/19 - 1s - loss: 104.6836 - loglik: -1.0331e+02 - logprior: -1.3743e+00
Epoch 3/10
19/19 - 1s - loss: 104.2265 - loglik: -1.0295e+02 - logprior: -1.2716e+00
Epoch 4/10
19/19 - 1s - loss: 103.4078 - loglik: -1.0218e+02 - logprior: -1.2246e+00
Epoch 5/10
19/19 - 1s - loss: 103.3185 - loglik: -1.0212e+02 - logprior: -1.1988e+00
Epoch 6/10
19/19 - 1s - loss: 102.7163 - loglik: -1.0154e+02 - logprior: -1.1794e+00
Epoch 7/10
19/19 - 1s - loss: 102.5746 - loglik: -1.0141e+02 - logprior: -1.1602e+00
Epoch 8/10
19/19 - 1s - loss: 102.5925 - loglik: -1.0145e+02 - logprior: -1.1421e+00
Fitted a model with MAP estimate = -102.4375
Time for alignment: 46.2458
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.4424 - loglik: -1.7322e+02 - logprior: -3.2178e+00
Epoch 2/10
19/19 - 1s - loss: 133.7448 - loglik: -1.3230e+02 - logprior: -1.4471e+00
Epoch 3/10
19/19 - 1s - loss: 117.8991 - loglik: -1.1648e+02 - logprior: -1.4179e+00
Epoch 4/10
19/19 - 1s - loss: 115.1891 - loglik: -1.1375e+02 - logprior: -1.4428e+00
Epoch 5/10
19/19 - 1s - loss: 114.4899 - loglik: -1.1308e+02 - logprior: -1.4086e+00
Epoch 6/10
19/19 - 1s - loss: 114.0810 - loglik: -1.1271e+02 - logprior: -1.3701e+00
Epoch 7/10
19/19 - 1s - loss: 114.0141 - loglik: -1.1265e+02 - logprior: -1.3606e+00
Epoch 8/10
19/19 - 1s - loss: 113.4922 - loglik: -1.1214e+02 - logprior: -1.3502e+00
Epoch 9/10
19/19 - 1s - loss: 113.6866 - loglik: -1.1234e+02 - logprior: -1.3435e+00
Fitted a model with MAP estimate = -113.4768
expansions: [(0, 2), (8, 1), (15, 1), (19, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.8979 - loglik: -1.0977e+02 - logprior: -4.1297e+00
Epoch 2/2
19/19 - 1s - loss: 104.8402 - loglik: -1.0358e+02 - logprior: -1.2602e+00
Fitted a model with MAP estimate = -103.8295
expansions: []
discards: [ 0 38]
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.4196 - loglik: -1.0537e+02 - logprior: -4.0515e+00
Epoch 2/2
19/19 - 1s - loss: 105.2584 - loglik: -1.0376e+02 - logprior: -1.5029e+00
Fitted a model with MAP estimate = -104.4111
expansions: []
discards: []
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.5613 - loglik: -1.0440e+02 - logprior: -3.1612e+00
Epoch 2/10
19/19 - 1s - loss: 104.7821 - loglik: -1.0341e+02 - logprior: -1.3771e+00
Epoch 3/10
19/19 - 1s - loss: 104.0729 - loglik: -1.0280e+02 - logprior: -1.2744e+00
Epoch 4/10
19/19 - 1s - loss: 103.4760 - loglik: -1.0225e+02 - logprior: -1.2303e+00
Epoch 5/10
19/19 - 1s - loss: 103.0625 - loglik: -1.0186e+02 - logprior: -1.1981e+00
Epoch 6/10
19/19 - 1s - loss: 102.7746 - loglik: -1.0159e+02 - logprior: -1.1817e+00
Epoch 7/10
19/19 - 1s - loss: 102.8048 - loglik: -1.0164e+02 - logprior: -1.1638e+00
Fitted a model with MAP estimate = -102.4839
Time for alignment: 43.7907
Computed alignments with likelihoods: ['-102.7406', '-102.4375', '-102.4839']
Best model has likelihood: -102.4375
SP score = 0.8142
Training of 3 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb19e97bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb1179f730>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 616.3973 - loglik: -6.1382e+02 - logprior: -2.5785e+00
Epoch 2/10
19/19 - 18s - loss: 520.4869 - loglik: -5.1936e+02 - logprior: -1.1289e+00
Epoch 3/10
19/19 - 18s - loss: 488.7797 - loglik: -4.8759e+02 - logprior: -1.1872e+00
Epoch 4/10
19/19 - 19s - loss: 479.0585 - loglik: -4.7796e+02 - logprior: -1.1027e+00
Epoch 5/10
19/19 - 18s - loss: 475.1877 - loglik: -4.7409e+02 - logprior: -1.0939e+00
Epoch 6/10
19/19 - 17s - loss: 473.6173 - loglik: -4.7257e+02 - logprior: -1.0492e+00
Epoch 7/10
19/19 - 18s - loss: 474.9214 - loglik: -4.7390e+02 - logprior: -1.0249e+00
Fitted a model with MAP estimate = -471.9752
expansions: [(25, 1), (30, 3), (53, 1), (57, 1), (58, 1), (61, 2), (62, 1), (75, 2), (83, 1), (92, 3), (105, 1), (106, 1)]
discards: []
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 24s - loss: 476.2430 - loglik: -4.7345e+02 - logprior: -2.7917e+00
Epoch 2/2
19/19 - 22s - loss: 467.3078 - loglik: -4.6631e+02 - logprior: -1.0016e+00
Fitted a model with MAP estimate = -464.8094
expansions: []
discards: [  0  33  85 106]
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 24s - loss: 474.1320 - loglik: -4.7044e+02 - logprior: -3.6947e+00
Epoch 2/2
19/19 - 21s - loss: 468.9345 - loglik: -4.6712e+02 - logprior: -1.8152e+00
Fitted a model with MAP estimate = -466.7213
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 186 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 24s - loss: 468.7809 - loglik: -4.6607e+02 - logprior: -2.7086e+00
Epoch 2/10
19/19 - 22s - loss: 464.5066 - loglik: -4.6364e+02 - logprior: -8.6735e-01
Epoch 3/10
19/19 - 22s - loss: 464.6063 - loglik: -4.6386e+02 - logprior: -7.4490e-01
Fitted a model with MAP estimate = -463.0779
Time for alignment: 375.0672
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 616.3793 - loglik: -6.1381e+02 - logprior: -2.5736e+00
Epoch 2/10
19/19 - 19s - loss: 520.2761 - loglik: -5.1909e+02 - logprior: -1.1881e+00
Epoch 3/10
19/19 - 18s - loss: 487.2463 - loglik: -4.8602e+02 - logprior: -1.2283e+00
Epoch 4/10
19/19 - 18s - loss: 479.5790 - loglik: -4.7847e+02 - logprior: -1.1113e+00
Epoch 5/10
19/19 - 16s - loss: 473.6561 - loglik: -4.7256e+02 - logprior: -1.0951e+00
Epoch 6/10
19/19 - 16s - loss: 473.9819 - loglik: -4.7291e+02 - logprior: -1.0727e+00
Fitted a model with MAP estimate = -471.3420
expansions: [(25, 1), (30, 4), (56, 2), (57, 1), (75, 1), (81, 1), (101, 1), (102, 1), (123, 3)]
discards: [2]
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 19s - loss: 476.9564 - loglik: -4.7432e+02 - logprior: -2.6361e+00
Epoch 2/2
19/19 - 17s - loss: 469.0916 - loglik: -4.6805e+02 - logprior: -1.0388e+00
Fitted a model with MAP estimate = -465.4509
expansions: []
discards: [ 33 135]
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 20s - loss: 471.6085 - loglik: -4.6877e+02 - logprior: -2.8373e+00
Epoch 2/2
19/19 - 20s - loss: 467.3337 - loglik: -4.6636e+02 - logprior: -9.6953e-01
Fitted a model with MAP estimate = -465.3055
expansions: [(2, 1)]
discards: [0]
Fitting a model of length 183 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 471.8636 - loglik: -4.6821e+02 - logprior: -3.6585e+00
Epoch 2/10
19/19 - 18s - loss: 468.2290 - loglik: -4.6629e+02 - logprior: -1.9368e+00
Epoch 3/10
19/19 - 17s - loss: 465.8006 - loglik: -4.6416e+02 - logprior: -1.6365e+00
Epoch 4/10
19/19 - 16s - loss: 466.2278 - loglik: -4.6529e+02 - logprior: -9.3295e-01
Fitted a model with MAP estimate = -464.2443
Time for alignment: 338.7259
Fitting a model of length 171 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 617.8403 - loglik: -6.1527e+02 - logprior: -2.5745e+00
Epoch 2/10
19/19 - 14s - loss: 527.2048 - loglik: -5.2606e+02 - logprior: -1.1472e+00
Epoch 3/10
19/19 - 13s - loss: 488.0421 - loglik: -4.8676e+02 - logprior: -1.2818e+00
Epoch 4/10
19/19 - 13s - loss: 477.6750 - loglik: -4.7649e+02 - logprior: -1.1871e+00
Epoch 5/10
19/19 - 14s - loss: 474.8980 - loglik: -4.7375e+02 - logprior: -1.1455e+00
Epoch 6/10
19/19 - 14s - loss: 472.8613 - loglik: -4.7176e+02 - logprior: -1.1041e+00
Epoch 7/10
19/19 - 14s - loss: 474.3301 - loglik: -4.7326e+02 - logprior: -1.0667e+00
Fitted a model with MAP estimate = -471.5641
expansions: [(25, 1), (30, 4), (80, 1), (81, 1), (82, 1), (83, 1), (101, 1), (103, 1), (105, 1), (106, 1)]
discards: [2]
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 20s - loss: 477.6009 - loglik: -4.7499e+02 - logprior: -2.6083e+00
Epoch 2/2
19/19 - 18s - loss: 469.5398 - loglik: -4.6856e+02 - logprior: -9.8440e-01
Fitted a model with MAP estimate = -466.5164
expansions: []
discards: []
Fitting a model of length 183 on 10036 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 468.9572 - loglik: -4.6617e+02 - logprior: -2.7898e+00
Epoch 2/10
19/19 - 20s - loss: 466.4109 - loglik: -4.6548e+02 - logprior: -9.2722e-01
Epoch 3/10
19/19 - 18s - loss: 464.4464 - loglik: -4.6365e+02 - logprior: -7.9972e-01
Epoch 4/10
19/19 - 17s - loss: 463.4755 - loglik: -4.6272e+02 - logprior: -7.5570e-01
Epoch 5/10
19/19 - 16s - loss: 462.1706 - loglik: -4.6146e+02 - logprior: -7.1434e-01
Epoch 6/10
19/19 - 16s - loss: 461.5094 - loglik: -4.6084e+02 - logprior: -6.7034e-01
Epoch 7/10
19/19 - 15s - loss: 462.2878 - loglik: -4.6165e+02 - logprior: -6.3820e-01
Fitted a model with MAP estimate = -461.5965
Time for alignment: 312.3936
Computed alignments with likelihoods: ['-463.0779', '-464.2443', '-461.5965']
Best model has likelihood: -461.5965
SP score = 0.7515
Training of 3 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb2a9206a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb1179ebb0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.8737 - loglik: -1.3966e+02 - logprior: -3.2143e+00
Epoch 2/10
19/19 - 1s - loss: 113.9417 - loglik: -1.1251e+02 - logprior: -1.4283e+00
Epoch 3/10
19/19 - 1s - loss: 104.5745 - loglik: -1.0299e+02 - logprior: -1.5874e+00
Epoch 4/10
19/19 - 1s - loss: 102.5625 - loglik: -1.0112e+02 - logprior: -1.4460e+00
Epoch 5/10
19/19 - 1s - loss: 101.8415 - loglik: -1.0040e+02 - logprior: -1.4369e+00
Epoch 6/10
19/19 - 1s - loss: 101.6509 - loglik: -1.0023e+02 - logprior: -1.4214e+00
Epoch 7/10
19/19 - 1s - loss: 101.3816 - loglik: -9.9977e+01 - logprior: -1.4048e+00
Epoch 8/10
19/19 - 1s - loss: 101.6431 - loglik: -1.0024e+02 - logprior: -1.4008e+00
Fitted a model with MAP estimate = -101.2480
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 1), (27, 1), (28, 2), (29, 2), (31, 2)]
discards: [0]
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.0629 - loglik: -1.0390e+02 - logprior: -4.1623e+00
Epoch 2/2
19/19 - 1s - loss: 100.0773 - loglik: -9.7940e+01 - logprior: -2.1378e+00
Fitted a model with MAP estimate = -98.3303
expansions: [(0, 1)]
discards: [ 0  8 17 26 38 40 44]
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.2694 - loglik: -9.8083e+01 - logprior: -3.1867e+00
Epoch 2/2
19/19 - 1s - loss: 97.4238 - loglik: -9.5976e+01 - logprior: -1.4482e+00
Fitted a model with MAP estimate = -96.9824
expansions: []
discards: []
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.6921 - loglik: -9.6444e+01 - logprior: -3.2483e+00
Epoch 2/10
19/19 - 1s - loss: 97.1485 - loglik: -9.5715e+01 - logprior: -1.4337e+00
Epoch 3/10
19/19 - 1s - loss: 96.8534 - loglik: -9.5507e+01 - logprior: -1.3468e+00
Epoch 4/10
19/19 - 1s - loss: 96.5466 - loglik: -9.5252e+01 - logprior: -1.2945e+00
Epoch 5/10
19/19 - 1s - loss: 96.6117 - loglik: -9.5352e+01 - logprior: -1.2598e+00
Fitted a model with MAP estimate = -96.3057
Time for alignment: 37.5188
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.7775 - loglik: -1.3956e+02 - logprior: -3.2140e+00
Epoch 2/10
19/19 - 1s - loss: 113.4905 - loglik: -1.1206e+02 - logprior: -1.4291e+00
Epoch 3/10
19/19 - 1s - loss: 104.3604 - loglik: -1.0277e+02 - logprior: -1.5879e+00
Epoch 4/10
19/19 - 1s - loss: 102.5359 - loglik: -1.0109e+02 - logprior: -1.4418e+00
Epoch 5/10
19/19 - 1s - loss: 101.9745 - loglik: -1.0054e+02 - logprior: -1.4306e+00
Epoch 6/10
19/19 - 1s - loss: 101.6789 - loglik: -1.0026e+02 - logprior: -1.4183e+00
Epoch 7/10
19/19 - 1s - loss: 101.4894 - loglik: -1.0009e+02 - logprior: -1.4033e+00
Epoch 8/10
19/19 - 1s - loss: 101.5017 - loglik: -1.0011e+02 - logprior: -1.3946e+00
Fitted a model with MAP estimate = -101.3719
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 1), (27, 1), (28, 2), (29, 2), (31, 2)]
discards: [0]
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.1007 - loglik: -1.0395e+02 - logprior: -4.1536e+00
Epoch 2/2
19/19 - 1s - loss: 100.0967 - loglik: -9.7966e+01 - logprior: -2.1310e+00
Fitted a model with MAP estimate = -98.4239
expansions: [(0, 1)]
discards: [ 0  8 17 26 44]
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.8155 - loglik: -9.7610e+01 - logprior: -3.2051e+00
Epoch 2/2
19/19 - 1s - loss: 96.9166 - loglik: -9.5452e+01 - logprior: -1.4644e+00
Fitted a model with MAP estimate = -96.3479
expansions: []
discards: [36 37]
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.8211 - loglik: -9.6579e+01 - logprior: -3.2420e+00
Epoch 2/10
19/19 - 1s - loss: 97.2273 - loglik: -9.5791e+01 - logprior: -1.4367e+00
Epoch 3/10
19/19 - 1s - loss: 96.7909 - loglik: -9.5453e+01 - logprior: -1.3380e+00
Epoch 4/10
19/19 - 1s - loss: 96.6162 - loglik: -9.5325e+01 - logprior: -1.2917e+00
Epoch 5/10
19/19 - 1s - loss: 96.4187 - loglik: -9.5161e+01 - logprior: -1.2581e+00
Epoch 6/10
19/19 - 1s - loss: 96.2935 - loglik: -9.5051e+01 - logprior: -1.2424e+00
Epoch 7/10
19/19 - 1s - loss: 96.2332 - loglik: -9.5002e+01 - logprior: -1.2311e+00
Epoch 8/10
19/19 - 1s - loss: 96.3007 - loglik: -9.5087e+01 - logprior: -1.2137e+00
Fitted a model with MAP estimate = -96.1621
Time for alignment: 39.8627
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.8972 - loglik: -1.3968e+02 - logprior: -3.2136e+00
Epoch 2/10
19/19 - 1s - loss: 114.0107 - loglik: -1.1259e+02 - logprior: -1.4232e+00
Epoch 3/10
19/19 - 1s - loss: 104.1015 - loglik: -1.0252e+02 - logprior: -1.5846e+00
Epoch 4/10
19/19 - 1s - loss: 101.8882 - loglik: -1.0045e+02 - logprior: -1.4407e+00
Epoch 5/10
19/19 - 1s - loss: 101.2800 - loglik: -9.9848e+01 - logprior: -1.4323e+00
Epoch 6/10
19/19 - 1s - loss: 100.9125 - loglik: -9.9495e+01 - logprior: -1.4178e+00
Epoch 7/10
19/19 - 1s - loss: 100.9323 - loglik: -9.9528e+01 - logprior: -1.4042e+00
Fitted a model with MAP estimate = -100.7815
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.6770 - loglik: -1.0353e+02 - logprior: -4.1517e+00
Epoch 2/2
19/19 - 1s - loss: 99.9237 - loglik: -9.7810e+01 - logprior: -2.1133e+00
Fitted a model with MAP estimate = -98.2356
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 44]
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.8944 - loglik: -9.7692e+01 - logprior: -3.2024e+00
Epoch 2/2
19/19 - 1s - loss: 96.9647 - loglik: -9.5504e+01 - logprior: -1.4609e+00
Fitted a model with MAP estimate = -96.5473
expansions: []
discards: [36]
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.7713 - loglik: -9.6529e+01 - logprior: -3.2419e+00
Epoch 2/10
19/19 - 1s - loss: 97.2320 - loglik: -9.5796e+01 - logprior: -1.4357e+00
Epoch 3/10
19/19 - 1s - loss: 96.8199 - loglik: -9.5480e+01 - logprior: -1.3401e+00
Epoch 4/10
19/19 - 1s - loss: 96.6281 - loglik: -9.5336e+01 - logprior: -1.2919e+00
Epoch 5/10
19/19 - 1s - loss: 96.4062 - loglik: -9.5147e+01 - logprior: -1.2589e+00
Epoch 6/10
19/19 - 1s - loss: 96.1674 - loglik: -9.4924e+01 - logprior: -1.2436e+00
Epoch 7/10
19/19 - 1s - loss: 96.3549 - loglik: -9.5133e+01 - logprior: -1.2223e+00
Fitted a model with MAP estimate = -96.1684
Time for alignment: 38.4170
Computed alignments with likelihoods: ['-96.3057', '-96.1621', '-96.1684']
Best model has likelihood: -96.1621
SP score = 0.7730
Training of 3 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdaf788ff10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdad5e8bf40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 457.2245 - loglik: -4.5436e+02 - logprior: -2.8645e+00
Epoch 2/10
19/19 - 6s - loss: 381.4829 - loglik: -3.8012e+02 - logprior: -1.3600e+00
Epoch 3/10
19/19 - 6s - loss: 351.1792 - loglik: -3.4946e+02 - logprior: -1.7227e+00
Epoch 4/10
19/19 - 6s - loss: 345.1384 - loglik: -3.4336e+02 - logprior: -1.7809e+00
Epoch 5/10
19/19 - 6s - loss: 342.9923 - loglik: -3.4130e+02 - logprior: -1.6928e+00
Epoch 6/10
19/19 - 7s - loss: 341.4981 - loglik: -3.3985e+02 - logprior: -1.6507e+00
Epoch 7/10
19/19 - 7s - loss: 341.4239 - loglik: -3.3979e+02 - logprior: -1.6301e+00
Epoch 8/10
19/19 - 7s - loss: 340.2719 - loglik: -3.3866e+02 - logprior: -1.6155e+00
Epoch 9/10
19/19 - 7s - loss: 340.3129 - loglik: -3.3871e+02 - logprior: -1.5980e+00
Fitted a model with MAP estimate = -337.0379
expansions: [(0, 2), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (22, 1), (30, 1), (43, 2), (44, 1), (47, 1), (48, 1), (50, 1), (57, 1), (66, 4), (68, 1), (71, 1), (80, 1), (87, 2), (88, 2), (89, 1), (99, 1), (100, 1), (112, 1), (113, 2), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 335.3026 - loglik: -3.3160e+02 - logprior: -3.7061e+00
Epoch 2/2
19/19 - 10s - loss: 318.0952 - loglik: -3.1697e+02 - logprior: -1.1251e+00
Fitted a model with MAP estimate = -312.9328
expansions: []
discards: [  0  84 114 146]
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 322.2544 - loglik: -3.1844e+02 - logprior: -3.8147e+00
Epoch 2/2
19/19 - 11s - loss: 317.5073 - loglik: -3.1593e+02 - logprior: -1.5754e+00
Fitted a model with MAP estimate = -312.6715
expansions: [(0, 2), (27, 1)]
discards: [ 0 53]
Fitting a model of length 186 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 315.7110 - loglik: -3.1300e+02 - logprior: -2.7094e+00
Epoch 2/10
19/19 - 12s - loss: 311.7898 - loglik: -3.1094e+02 - logprior: -8.4569e-01
Epoch 3/10
19/19 - 12s - loss: 310.5006 - loglik: -3.0975e+02 - logprior: -7.5111e-01
Epoch 4/10
19/19 - 12s - loss: 310.3624 - loglik: -3.0966e+02 - logprior: -6.9900e-01
Epoch 5/10
19/19 - 12s - loss: 309.2013 - loglik: -3.0854e+02 - logprior: -6.6059e-01
Epoch 6/10
19/19 - 12s - loss: 307.7485 - loglik: -3.0711e+02 - logprior: -6.3931e-01
Epoch 7/10
19/19 - 13s - loss: 308.8902 - loglik: -3.0828e+02 - logprior: -6.1049e-01
Fitted a model with MAP estimate = -307.5969
Time for alignment: 246.6511
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 457.9604 - loglik: -4.5510e+02 - logprior: -2.8627e+00
Epoch 2/10
19/19 - 7s - loss: 381.1630 - loglik: -3.7981e+02 - logprior: -1.3515e+00
Epoch 3/10
19/19 - 8s - loss: 349.8738 - loglik: -3.4823e+02 - logprior: -1.6420e+00
Epoch 4/10
19/19 - 8s - loss: 341.8961 - loglik: -3.4015e+02 - logprior: -1.7440e+00
Epoch 5/10
19/19 - 8s - loss: 338.9167 - loglik: -3.3721e+02 - logprior: -1.7056e+00
Epoch 6/10
19/19 - 8s - loss: 337.6660 - loglik: -3.3600e+02 - logprior: -1.6708e+00
Epoch 7/10
19/19 - 8s - loss: 336.7621 - loglik: -3.3510e+02 - logprior: -1.6667e+00
Epoch 8/10
19/19 - 8s - loss: 336.0128 - loglik: -3.3434e+02 - logprior: -1.6757e+00
Epoch 9/10
19/19 - 8s - loss: 335.8048 - loglik: -3.3414e+02 - logprior: -1.6691e+00
Epoch 10/10
19/19 - 8s - loss: 336.8285 - loglik: -3.3517e+02 - logprior: -1.6553e+00
Fitted a model with MAP estimate = -332.5828
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (71, 1), (87, 2), (88, 2), (89, 1), (90, 1), (100, 1), (111, 1), (113, 1), (114, 2), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Fitting a model of length 186 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 331.7585 - loglik: -3.2800e+02 - logprior: -3.7542e+00
Epoch 2/2
19/19 - 12s - loss: 315.6538 - loglik: -3.1460e+02 - logprior: -1.0494e+00
Fitted a model with MAP estimate = -310.6909
expansions: []
discards: [  0  55 113]
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 320.0176 - loglik: -3.1624e+02 - logprior: -3.7826e+00
Epoch 2/2
19/19 - 12s - loss: 315.3218 - loglik: -3.1374e+02 - logprior: -1.5812e+00
Fitted a model with MAP estimate = -310.8858
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 184 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 15s - loss: 313.8669 - loglik: -3.1116e+02 - logprior: -2.7098e+00
Epoch 2/10
19/19 - 12s - loss: 310.4006 - loglik: -3.0957e+02 - logprior: -8.3396e-01
Epoch 3/10
19/19 - 12s - loss: 309.5498 - loglik: -3.0880e+02 - logprior: -7.5247e-01
Epoch 4/10
19/19 - 12s - loss: 309.3331 - loglik: -3.0864e+02 - logprior: -6.9108e-01
Epoch 5/10
19/19 - 12s - loss: 307.1357 - loglik: -3.0648e+02 - logprior: -6.5692e-01
Epoch 6/10
19/19 - 12s - loss: 307.1811 - loglik: -3.0656e+02 - logprior: -6.2259e-01
Fitted a model with MAP estimate = -306.7872
Time for alignment: 267.3019
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 458.0651 - loglik: -4.5521e+02 - logprior: -2.8580e+00
Epoch 2/10
19/19 - 8s - loss: 378.9210 - loglik: -3.7755e+02 - logprior: -1.3757e+00
Epoch 3/10
19/19 - 8s - loss: 348.4650 - loglik: -3.4674e+02 - logprior: -1.7261e+00
Epoch 4/10
19/19 - 8s - loss: 340.9214 - loglik: -3.3915e+02 - logprior: -1.7727e+00
Epoch 5/10
19/19 - 8s - loss: 340.2548 - loglik: -3.3855e+02 - logprior: -1.7037e+00
Epoch 6/10
19/19 - 8s - loss: 338.4935 - loglik: -3.3683e+02 - logprior: -1.6644e+00
Epoch 7/10
19/19 - 8s - loss: 338.6518 - loglik: -3.3701e+02 - logprior: -1.6425e+00
Fitted a model with MAP estimate = -334.9956
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 2), (21, 1), (23, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (58, 1), (66, 3), (68, 1), (71, 2), (87, 2), (88, 2), (89, 1), (99, 1), (100, 1), (112, 1), (113, 2), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Fitting a model of length 191 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 333.5977 - loglik: -3.2992e+02 - logprior: -3.6782e+00
Epoch 2/2
19/19 - 13s - loss: 317.6049 - loglik: -3.1644e+02 - logprior: -1.1689e+00
Fitted a model with MAP estimate = -312.1222
expansions: []
discards: [  0  26  55  87  95 116 148]
Fitting a model of length 184 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 322.5029 - loglik: -3.1861e+02 - logprior: -3.8976e+00
Epoch 2/2
19/19 - 12s - loss: 316.5668 - loglik: -3.1488e+02 - logprior: -1.6867e+00
Fitted a model with MAP estimate = -312.2013
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 185 on 10140 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 314.9513 - loglik: -3.1217e+02 - logprior: -2.7772e+00
Epoch 2/10
19/19 - 11s - loss: 311.9310 - loglik: -3.1104e+02 - logprior: -8.9123e-01
Epoch 3/10
19/19 - 11s - loss: 310.9249 - loglik: -3.1012e+02 - logprior: -8.0772e-01
Epoch 4/10
19/19 - 11s - loss: 310.4355 - loglik: -3.0970e+02 - logprior: -7.3692e-01
Epoch 5/10
19/19 - 11s - loss: 307.5665 - loglik: -3.0684e+02 - logprior: -7.2169e-01
Epoch 6/10
19/19 - 11s - loss: 309.0887 - loglik: -3.0841e+02 - logprior: -6.7838e-01
Fitted a model with MAP estimate = -307.9757
Time for alignment: 235.9996
Computed alignments with likelihoods: ['-307.5969', '-306.7872', '-307.9757']
Best model has likelihood: -306.7872
SP score = 0.2639
Training of 3 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdaf7b08b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb3b066fd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 904.3651 - loglik: -9.0297e+02 - logprior: -1.3961e+00
Epoch 2/10
39/39 - 65s - loss: 724.8888 - loglik: -7.2319e+02 - logprior: -1.7026e+00
Epoch 3/10
39/39 - 65s - loss: 707.9995 - loglik: -7.0631e+02 - logprior: -1.6892e+00
Epoch 4/10
39/39 - 62s - loss: 702.8954 - loglik: -7.0126e+02 - logprior: -1.6348e+00
Epoch 5/10
39/39 - 52s - loss: 700.8243 - loglik: -6.9912e+02 - logprior: -1.7009e+00
Epoch 6/10
39/39 - 52s - loss: 699.7192 - loglik: -6.9807e+02 - logprior: -1.6505e+00
Epoch 7/10
39/39 - 59s - loss: 698.5741 - loglik: -6.9677e+02 - logprior: -1.8035e+00
Epoch 8/10
39/39 - 59s - loss: 699.3989 - loglik: -6.9772e+02 - logprior: -1.6752e+00
Fitted a model with MAP estimate = -697.4842
expansions: [(0, 15), (25, 1), (46, 1), (55, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (95, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (149, 1), (151, 1), (155, 1), (161, 2), (162, 2), (163, 1), (165, 1), (166, 1), (182, 2), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 3), (259, 1), (260, 2), (261, 8), (279, 2), (280, 1), (282, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 78s - loss: 682.9039 - loglik: -6.8062e+02 - logprior: -2.2869e+00
Epoch 2/2
39/39 - 91s - loss: 659.4130 - loglik: -6.5853e+02 - logprior: -8.8029e-01
Fitted a model with MAP estimate = -655.1148
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  11  12 112 158 163 210 235 244
 334 336 362]
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 664.1810 - loglik: -6.6277e+02 - logprior: -1.4159e+00
Epoch 2/2
39/39 - 86s - loss: 658.3000 - loglik: -6.5795e+02 - logprior: -3.5392e-01
Fitted a model with MAP estimate = -655.5673
expansions: []
discards: []
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 98s - loss: 662.1984 - loglik: -6.6102e+02 - logprior: -1.1786e+00
Epoch 2/10
39/39 - 95s - loss: 657.5598 - loglik: -6.5748e+02 - logprior: -7.8778e-02
Epoch 3/10
39/39 - 96s - loss: 655.4592 - loglik: -6.5561e+02 - logprior: 0.1547
Epoch 4/10
39/39 - 83s - loss: 651.6573 - loglik: -6.5191e+02 - logprior: 0.2501
Epoch 5/10
39/39 - 75s - loss: 649.3336 - loglik: -6.4974e+02 - logprior: 0.4100
Epoch 6/10
39/39 - 78s - loss: 648.7265 - loglik: -6.4934e+02 - logprior: 0.6156
Epoch 7/10
39/39 - 74s - loss: 648.0302 - loglik: -6.4877e+02 - logprior: 0.7378
Epoch 8/10
39/39 - 77s - loss: 647.7235 - loglik: -6.4873e+02 - logprior: 1.0025
Epoch 9/10
39/39 - 75s - loss: 647.8694 - loglik: -6.4894e+02 - logprior: 1.0677
Fitted a model with MAP estimate = -647.1962
Time for alignment: 1935.0536
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 904.1445 - loglik: -9.0276e+02 - logprior: -1.3886e+00
Epoch 2/10
39/39 - 54s - loss: 725.5928 - loglik: -7.2390e+02 - logprior: -1.6971e+00
Epoch 3/10
39/39 - 55s - loss: 710.0109 - loglik: -7.0828e+02 - logprior: -1.7319e+00
Epoch 4/10
39/39 - 55s - loss: 705.5507 - loglik: -7.0380e+02 - logprior: -1.7508e+00
Epoch 5/10
39/39 - 54s - loss: 702.6547 - loglik: -7.0083e+02 - logprior: -1.8242e+00
Epoch 6/10
39/39 - 55s - loss: 701.0655 - loglik: -6.9926e+02 - logprior: -1.8056e+00
Epoch 7/10
39/39 - 54s - loss: 700.9335 - loglik: -6.9916e+02 - logprior: -1.7740e+00
Epoch 8/10
39/39 - 56s - loss: 700.1573 - loglik: -6.9833e+02 - logprior: -1.8238e+00
Epoch 9/10
39/39 - 55s - loss: 700.1658 - loglik: -6.9838e+02 - logprior: -1.7898e+00
Fitted a model with MAP estimate = -699.5102
expansions: [(0, 5), (41, 1), (42, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (79, 1), (83, 2), (84, 1), (85, 2), (90, 1), (91, 1), (93, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (173, 1), (186, 1), (188, 2), (189, 1), (190, 1), (194, 1), (205, 1), (207, 2), (208, 2), (209, 3), (227, 1), (228, 1), (229, 1), (230, 4), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 89s - loss: 680.6248 - loglik: -6.7845e+02 - logprior: -2.1758e+00
Epoch 2/2
39/39 - 81s - loss: 659.1769 - loglik: -6.5840e+02 - logprior: -7.7729e-01
Fitted a model with MAP estimate = -654.6984
expansions: []
discards: [  1   2   3 102 148 153 232 260 261 291 326 353 368 369]
Fitting a model of length 375 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 665.5241 - loglik: -6.6421e+02 - logprior: -1.3127e+00
Epoch 2/2
39/39 - 66s - loss: 659.7531 - loglik: -6.5969e+02 - logprior: -6.0368e-02
Fitted a model with MAP estimate = -656.6929
expansions: [(0, 5)]
discards: []
Fitting a model of length 380 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 664.4606 - loglik: -6.6261e+02 - logprior: -1.8509e+00
Epoch 2/10
39/39 - 74s - loss: 658.2472 - loglik: -6.5830e+02 - logprior: 0.0560
Epoch 3/10
39/39 - 76s - loss: 654.7668 - loglik: -6.5502e+02 - logprior: 0.2488
Epoch 4/10
39/39 - 80s - loss: 651.9581 - loglik: -6.5233e+02 - logprior: 0.3718
Epoch 5/10
39/39 - 84s - loss: 650.0127 - loglik: -6.5058e+02 - logprior: 0.5713
Epoch 6/10
39/39 - 80s - loss: 648.8530 - loglik: -6.4935e+02 - logprior: 0.4968
Epoch 7/10
39/39 - 84s - loss: 648.2682 - loglik: -6.4910e+02 - logprior: 0.8286
Epoch 8/10
39/39 - 80s - loss: 647.0351 - loglik: -6.4812e+02 - logprior: 1.0849
Epoch 9/10
39/39 - 74s - loss: 648.2723 - loglik: -6.4934e+02 - logprior: 1.0679
Fitted a model with MAP estimate = -647.1230
Time for alignment: 1850.8773
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 905.5892 - loglik: -9.0418e+02 - logprior: -1.4070e+00
Epoch 2/10
39/39 - 42s - loss: 725.6464 - loglik: -7.2382e+02 - logprior: -1.8253e+00
Epoch 3/10
39/39 - 44s - loss: 709.4701 - loglik: -7.0781e+02 - logprior: -1.6580e+00
Epoch 4/10
39/39 - 45s - loss: 705.0786 - loglik: -7.0347e+02 - logprior: -1.6068e+00
Epoch 5/10
39/39 - 46s - loss: 703.2504 - loglik: -7.0166e+02 - logprior: -1.5905e+00
Epoch 6/10
39/39 - 47s - loss: 701.3874 - loglik: -6.9979e+02 - logprior: -1.5957e+00
Epoch 7/10
39/39 - 47s - loss: 700.5583 - loglik: -6.9890e+02 - logprior: -1.6556e+00
Epoch 8/10
39/39 - 46s - loss: 700.7039 - loglik: -6.9902e+02 - logprior: -1.6796e+00
Fitted a model with MAP estimate = -699.6857
expansions: [(0, 15), (16, 1), (42, 1), (45, 1), (60, 1), (62, 2), (64, 1), (65, 1), (71, 1), (72, 1), (80, 1), (82, 2), (83, 1), (84, 1), (90, 1), (91, 1), (92, 1), (112, 1), (113, 1), (118, 1), (122, 2), (124, 3), (142, 1), (144, 1), (146, 1), (148, 1), (150, 1), (154, 1), (157, 1), (159, 1), (161, 1), (162, 1), (164, 1), (165, 1), (181, 1), (188, 1), (189, 1), (190, 1), (197, 1), (205, 2), (207, 1), (208, 2), (209, 2), (227, 1), (230, 1), (231, 3), (232, 1), (258, 1), (259, 1), (260, 8), (278, 2), (279, 1), (280, 1), (284, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 83s - loss: 683.4562 - loglik: -6.8121e+02 - logprior: -2.2468e+00
Epoch 2/2
39/39 - 79s - loss: 660.0143 - loglik: -6.5915e+02 - logprior: -8.6874e-01
Fitted a model with MAP estimate = -655.8838
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  11  12 112 158 163 269 333 360
 375 376]
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 665.1522 - loglik: -6.6375e+02 - logprior: -1.4061e+00
Epoch 2/2
39/39 - 83s - loss: 659.7393 - loglik: -6.5951e+02 - logprior: -2.3001e-01
Fitted a model with MAP estimate = -656.3588
expansions: []
discards: []
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 80s - loss: 663.5749 - loglik: -6.6239e+02 - logprior: -1.1815e+00
Epoch 2/10
39/39 - 82s - loss: 658.5256 - loglik: -6.5850e+02 - logprior: -2.1422e-02
Epoch 3/10
39/39 - 93s - loss: 655.8385 - loglik: -6.5608e+02 - logprior: 0.2403
Epoch 4/10
39/39 - 92s - loss: 652.3212 - loglik: -6.5272e+02 - logprior: 0.4003
Epoch 5/10
39/39 - 92s - loss: 650.9258 - loglik: -6.5149e+02 - logprior: 0.5604
Epoch 6/10
39/39 - 87s - loss: 649.3207 - loglik: -6.5002e+02 - logprior: 0.6970
Epoch 7/10
39/39 - 81s - loss: 648.9884 - loglik: -6.4965e+02 - logprior: 0.6665
Epoch 8/10
39/39 - 80s - loss: 648.9946 - loglik: -6.4980e+02 - logprior: 0.8089
Fitted a model with MAP estimate = -648.3874
Time for alignment: 1719.2876
Computed alignments with likelihoods: ['-647.1962', '-647.1230', '-648.3874']
Best model has likelihood: -647.1230
SP score = 0.9420
Training of 3 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb00715fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdade7dd400>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 690.7502 - loglik: -6.8815e+02 - logprior: -2.5967e+00
Epoch 2/10
19/19 - 20s - loss: 604.3810 - loglik: -6.0320e+02 - logprior: -1.1816e+00
Epoch 3/10
19/19 - 21s - loss: 557.9814 - loglik: -5.5645e+02 - logprior: -1.5275e+00
Epoch 4/10
19/19 - 22s - loss: 548.7145 - loglik: -5.4704e+02 - logprior: -1.6752e+00
Epoch 5/10
19/19 - 22s - loss: 546.7554 - loglik: -5.4506e+02 - logprior: -1.6940e+00
Epoch 6/10
19/19 - 24s - loss: 544.0938 - loglik: -5.4242e+02 - logprior: -1.6771e+00
Epoch 7/10
19/19 - 23s - loss: 544.9612 - loglik: -5.4333e+02 - logprior: -1.6336e+00
Fitted a model with MAP estimate = -543.3122
expansions: [(7, 1), (8, 1), (33, 1), (40, 1), (81, 5), (89, 1), (90, 2), (92, 2), (116, 1), (117, 8), (118, 2), (119, 1), (120, 1), (127, 2), (131, 3), (132, 2), (135, 1), (136, 1), (139, 1), (141, 3), (142, 8), (143, 1), (145, 1), (147, 1), (156, 1), (159, 2), (160, 1), (171, 5)]
discards: [  0 149 150 151 152 161 162 163 164 165 166 167 168 169 175 176 177 178
 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196
 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Fitting a model of length 221 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 574.9827 - loglik: -5.7140e+02 - logprior: -3.5833e+00
Epoch 2/2
19/19 - 24s - loss: 559.8566 - loglik: -5.5816e+02 - logprior: -1.6983e+00
Fitted a model with MAP estimate = -556.3657
expansions: [(0, 2), (211, 2), (218, 8), (220, 14), (221, 7)]
discards: [  0  80  81  82 133 134 135 136 161 183 184 185 207]
Fitting a model of length 241 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 30s - loss: 556.8010 - loglik: -5.5392e+02 - logprior: -2.8786e+00
Epoch 2/2
19/19 - 26s - loss: 540.1479 - loglik: -5.3911e+02 - logprior: -1.0368e+00
Fitted a model with MAP estimate = -537.8664
expansions: [(85, 3), (197, 1), (201, 1), (214, 4), (241, 5)]
discards: [  0   1   2   3 215 216 217 218 219 220 221 222 223 224 225 226 227 228
 229 230 231 232 233 234 235 236 237 238 239 240]
Fitting a model of length 225 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 555.2468 - loglik: -5.5167e+02 - logprior: -3.5725e+00
Epoch 2/10
19/19 - 22s - loss: 548.4052 - loglik: -5.4698e+02 - logprior: -1.4299e+00
Epoch 3/10
19/19 - 22s - loss: 544.1829 - loglik: -5.4365e+02 - logprior: -5.2865e-01
Epoch 4/10
19/19 - 22s - loss: 542.6238 - loglik: -5.4232e+02 - logprior: -2.9906e-01
Epoch 5/10
19/19 - 23s - loss: 540.5886 - loglik: -5.4034e+02 - logprior: -2.5049e-01
Epoch 6/10
19/19 - 22s - loss: 539.6490 - loglik: -5.3944e+02 - logprior: -2.1322e-01
Epoch 7/10
19/19 - 22s - loss: 540.1254 - loglik: -5.3994e+02 - logprior: -1.8138e-01
Fitted a model with MAP estimate = -539.4094
Time for alignment: 543.1537
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 690.4211 - loglik: -6.8782e+02 - logprior: -2.5989e+00
Epoch 2/10
19/19 - 21s - loss: 602.7999 - loglik: -6.0164e+02 - logprior: -1.1603e+00
Epoch 3/10
19/19 - 22s - loss: 557.9791 - loglik: -5.5653e+02 - logprior: -1.4537e+00
Epoch 4/10
19/19 - 22s - loss: 549.1147 - loglik: -5.4750e+02 - logprior: -1.6141e+00
Epoch 5/10
19/19 - 23s - loss: 546.3027 - loglik: -5.4471e+02 - logprior: -1.5919e+00
Epoch 6/10
19/19 - 23s - loss: 544.4634 - loglik: -5.4291e+02 - logprior: -1.5486e+00
Epoch 7/10
19/19 - 23s - loss: 544.5308 - loglik: -5.4298e+02 - logprior: -1.5533e+00
Fitted a model with MAP estimate = -543.7210
expansions: [(4, 1), (6, 1), (33, 1), (44, 1), (89, 1), (90, 2), (92, 2), (116, 1), (117, 7), (118, 2), (119, 1), (120, 2), (128, 2), (130, 3), (131, 2), (134, 1), (135, 1), (137, 1), (140, 3), (141, 8), (142, 1), (144, 1), (145, 2), (152, 1), (155, 1), (158, 3), (159, 1), (170, 1), (171, 6), (184, 1)]
discards: [  0 149 160 161 162 163 164 165 166 167 168 186 187 188 189 190 191 192
 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210
 211]
Fitting a model of length 236 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 564.6187 - loglik: -5.6103e+02 - logprior: -3.5869e+00
Epoch 2/2
19/19 - 20s - loss: 550.8099 - loglik: -5.4926e+02 - logprior: -1.5533e+00
Fitted a model with MAP estimate = -547.5962
expansions: [(204, 7), (236, 21)]
discards: [  0   1 128 129 130 177 178 179 194 195 196 197 206 207 209 210 211 212
 213]
Fitting a model of length 245 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 22s - loss: 552.2856 - loglik: -5.4863e+02 - logprior: -3.6601e+00
Epoch 2/2
19/19 - 20s - loss: 540.0421 - loglik: -5.3855e+02 - logprior: -1.4963e+00
Fitted a model with MAP estimate = -536.2321
expansions: [(0, 2), (2, 2), (84, 3), (194, 4), (203, 1), (204, 4), (223, 4), (245, 3)]
discards: [  0 150 185 195 196 197 198 199 200 201 209 224 225 226 227 228 229 230
 231 232 233 234 235 236 237 238 239 240 241 242 243 244]
Fitting a model of length 236 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 552.2302 - loglik: -5.4949e+02 - logprior: -2.7371e+00
Epoch 2/10
19/19 - 24s - loss: 542.1013 - loglik: -5.4121e+02 - logprior: -8.9099e-01
Epoch 3/10
19/19 - 24s - loss: 540.9563 - loglik: -5.4031e+02 - logprior: -6.4941e-01
Epoch 4/10
19/19 - 24s - loss: 535.6234 - loglik: -5.3506e+02 - logprior: -5.6504e-01
Epoch 5/10
19/19 - 24s - loss: 539.1731 - loglik: -5.3864e+02 - logprior: -5.3242e-01
Fitted a model with MAP estimate = -536.5684
Time for alignment: 474.3855
Fitting a model of length 212 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 689.7870 - loglik: -6.8718e+02 - logprior: -2.6023e+00
Epoch 2/10
19/19 - 19s - loss: 598.2098 - loglik: -5.9710e+02 - logprior: -1.1138e+00
Epoch 3/10
19/19 - 20s - loss: 556.0949 - loglik: -5.5475e+02 - logprior: -1.3426e+00
Epoch 4/10
19/19 - 20s - loss: 545.9002 - loglik: -5.4436e+02 - logprior: -1.5376e+00
Epoch 5/10
19/19 - 19s - loss: 544.3624 - loglik: -5.4277e+02 - logprior: -1.5897e+00
Epoch 6/10
19/19 - 20s - loss: 543.5793 - loglik: -5.4198e+02 - logprior: -1.5974e+00
Epoch 7/10
19/19 - 21s - loss: 541.1163 - loglik: -5.3953e+02 - logprior: -1.5826e+00
Epoch 8/10
19/19 - 22s - loss: 543.1533 - loglik: -5.4158e+02 - logprior: -1.5777e+00
Fitted a model with MAP estimate = -541.1774
expansions: [(4, 1), (6, 1), (33, 1), (83, 2), (87, 1), (121, 1), (122, 10), (123, 2), (124, 2), (130, 1), (131, 2), (133, 8), (135, 1), (136, 1), (138, 1), (141, 11), (142, 1), (144, 1), (146, 1), (155, 7), (158, 1), (159, 3), (160, 1), (171, 4)]
discards: [  0 148 149 150 151 152 161 162 163 164 165 166 167 168 169 174 175 176
 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194
 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 573.7537 - loglik: -5.7020e+02 - logprior: -3.5548e+00
Epoch 2/2
19/19 - 25s - loss: 556.2397 - loglik: -5.5462e+02 - logprior: -1.6215e+00
Fitted a model with MAP estimate = -553.2361
expansions: [(87, 4), (209, 1), (223, 1), (224, 36)]
discards: [  0   1  84 131 132 133 134 139 158 159 181 182 201 202 216 217 218]
Fitting a model of length 249 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 31s - loss: 552.9316 - loglik: -5.4949e+02 - logprior: -3.4380e+00
Epoch 2/2
19/19 - 29s - loss: 538.5874 - loglik: -5.3731e+02 - logprior: -1.2814e+00
Fitted a model with MAP estimate = -533.3389
expansions: [(0, 2), (2, 2), (84, 3), (193, 1), (249, 10)]
discards: [  0  79  80  81 224 225 226 227 228 229 230 231 232 233 234 235 236 237
 238 239 240 241 242 243 244 245 246 247 248]
Fitting a model of length 238 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 30s - loss: 548.4030 - loglik: -5.4570e+02 - logprior: -2.7009e+00
Epoch 2/10
19/19 - 25s - loss: 540.0657 - loglik: -5.3925e+02 - logprior: -8.1970e-01
Epoch 3/10
19/19 - 24s - loss: 536.9014 - loglik: -5.3636e+02 - logprior: -5.4416e-01
Epoch 4/10
19/19 - 25s - loss: 535.0647 - loglik: -5.3463e+02 - logprior: -4.3601e-01
Epoch 5/10
19/19 - 25s - loss: 535.3681 - loglik: -5.3497e+02 - logprior: -3.9613e-01
Fitted a model with MAP estimate = -534.2201
Time for alignment: 531.6491
Computed alignments with likelihoods: ['-537.8664', '-536.2321', '-533.3389']
Best model has likelihood: -533.3389
SP score = 0.8960
Training of 3 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb00144fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdabc0c6f40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.7189 - loglik: -2.6659e+02 - logprior: -3.1278e+00
Epoch 2/10
19/19 - 2s - loss: 200.7618 - loglik: -1.9942e+02 - logprior: -1.3463e+00
Epoch 3/10
19/19 - 2s - loss: 176.6001 - loglik: -1.7488e+02 - logprior: -1.7152e+00
Epoch 4/10
19/19 - 2s - loss: 172.7384 - loglik: -1.7110e+02 - logprior: -1.6388e+00
Epoch 5/10
19/19 - 2s - loss: 171.3446 - loglik: -1.6975e+02 - logprior: -1.5933e+00
Epoch 6/10
19/19 - 2s - loss: 170.9113 - loglik: -1.6933e+02 - logprior: -1.5845e+00
Epoch 7/10
19/19 - 2s - loss: 170.4084 - loglik: -1.6884e+02 - logprior: -1.5658e+00
Epoch 8/10
19/19 - 2s - loss: 170.0004 - loglik: -1.6844e+02 - logprior: -1.5568e+00
Epoch 9/10
19/19 - 3s - loss: 170.1268 - loglik: -1.6857e+02 - logprior: -1.5564e+00
Fitted a model with MAP estimate = -169.8097
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (44, 1), (46, 3), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 168.3406 - loglik: -1.6439e+02 - logprior: -3.9512e+00
Epoch 2/2
19/19 - 3s - loss: 156.4919 - loglik: -1.5446e+02 - logprior: -2.0299e+00
Fitted a model with MAP estimate = -154.8542
expansions: [(0, 2)]
discards: [ 0 42 60]
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.4507 - loglik: -1.5355e+02 - logprior: -2.8976e+00
Epoch 2/2
19/19 - 3s - loss: 152.5855 - loglik: -1.5149e+02 - logprior: -1.0924e+00
Fitted a model with MAP estimate = -151.6875
expansions: []
discards: [0]
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 158.7848 - loglik: -1.5495e+02 - logprior: -3.8370e+00
Epoch 2/10
19/19 - 3s - loss: 153.9565 - loglik: -1.5250e+02 - logprior: -1.4589e+00
Epoch 3/10
19/19 - 3s - loss: 152.5531 - loglik: -1.5155e+02 - logprior: -1.0013e+00
Epoch 4/10
19/19 - 4s - loss: 151.5636 - loglik: -1.5057e+02 - logprior: -9.9231e-01
Epoch 5/10
19/19 - 3s - loss: 151.2255 - loglik: -1.5027e+02 - logprior: -9.5206e-01
Epoch 6/10
19/19 - 3s - loss: 150.9177 - loglik: -1.4996e+02 - logprior: -9.5670e-01
Epoch 7/10
19/19 - 3s - loss: 150.6258 - loglik: -1.4970e+02 - logprior: -9.2608e-01
Epoch 8/10
19/19 - 3s - loss: 150.5764 - loglik: -1.4965e+02 - logprior: -9.2672e-01
Epoch 9/10
19/19 - 4s - loss: 150.2285 - loglik: -1.4933e+02 - logprior: -8.9549e-01
Epoch 10/10
19/19 - 4s - loss: 150.6535 - loglik: -1.4978e+02 - logprior: -8.7534e-01
Fitted a model with MAP estimate = -150.3047
Time for alignment: 97.9071
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 269.7136 - loglik: -2.6659e+02 - logprior: -3.1240e+00
Epoch 2/10
19/19 - 3s - loss: 200.1308 - loglik: -1.9878e+02 - logprior: -1.3462e+00
Epoch 3/10
19/19 - 3s - loss: 176.7515 - loglik: -1.7504e+02 - logprior: -1.7110e+00
Epoch 4/10
19/19 - 3s - loss: 173.2703 - loglik: -1.7163e+02 - logprior: -1.6405e+00
Epoch 5/10
19/19 - 3s - loss: 171.7023 - loglik: -1.7012e+02 - logprior: -1.5827e+00
Epoch 6/10
19/19 - 3s - loss: 171.2052 - loglik: -1.6963e+02 - logprior: -1.5725e+00
Epoch 7/10
19/19 - 3s - loss: 170.8526 - loglik: -1.6930e+02 - logprior: -1.5500e+00
Epoch 8/10
19/19 - 3s - loss: 170.4585 - loglik: -1.6892e+02 - logprior: -1.5412e+00
Epoch 9/10
19/19 - 3s - loss: 170.7490 - loglik: -1.6921e+02 - logprior: -1.5423e+00
Fitted a model with MAP estimate = -170.2771
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 169.2240 - loglik: -1.6528e+02 - logprior: -3.9444e+00
Epoch 2/2
19/19 - 3s - loss: 157.0719 - loglik: -1.5502e+02 - logprior: -2.0502e+00
Fitted a model with MAP estimate = -155.2391
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 157.2851 - loglik: -1.5436e+02 - logprior: -2.9202e+00
Epoch 2/2
19/19 - 4s - loss: 152.8031 - loglik: -1.5169e+02 - logprior: -1.1135e+00
Fitted a model with MAP estimate = -152.0488
expansions: []
discards: [0]
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.2007 - loglik: -1.5532e+02 - logprior: -3.8782e+00
Epoch 2/10
19/19 - 3s - loss: 154.4930 - loglik: -1.5297e+02 - logprior: -1.5230e+00
Epoch 3/10
19/19 - 3s - loss: 152.5995 - loglik: -1.5157e+02 - logprior: -1.0252e+00
Epoch 4/10
19/19 - 3s - loss: 151.8679 - loglik: -1.5088e+02 - logprior: -9.8778e-01
Epoch 5/10
19/19 - 4s - loss: 151.4035 - loglik: -1.5042e+02 - logprior: -9.7954e-01
Epoch 6/10
19/19 - 3s - loss: 151.2735 - loglik: -1.5032e+02 - logprior: -9.5421e-01
Epoch 7/10
19/19 - 3s - loss: 151.1735 - loglik: -1.5022e+02 - logprior: -9.4909e-01
Epoch 8/10
19/19 - 4s - loss: 150.5072 - loglik: -1.4957e+02 - logprior: -9.3505e-01
Epoch 9/10
19/19 - 3s - loss: 150.4687 - loglik: -1.4956e+02 - logprior: -9.0943e-01
Epoch 10/10
19/19 - 3s - loss: 150.7636 - loglik: -1.4986e+02 - logprior: -9.0009e-01
Fitted a model with MAP estimate = -150.5068
Time for alignment: 101.2301
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 269.8842 - loglik: -2.6676e+02 - logprior: -3.1207e+00
Epoch 2/10
19/19 - 3s - loss: 200.3443 - loglik: -1.9900e+02 - logprior: -1.3464e+00
Epoch 3/10
19/19 - 2s - loss: 176.7130 - loglik: -1.7498e+02 - logprior: -1.7294e+00
Epoch 4/10
19/19 - 2s - loss: 171.9825 - loglik: -1.7031e+02 - logprior: -1.6729e+00
Epoch 5/10
19/19 - 3s - loss: 170.9365 - loglik: -1.6934e+02 - logprior: -1.6005e+00
Epoch 6/10
19/19 - 3s - loss: 170.4085 - loglik: -1.6883e+02 - logprior: -1.5793e+00
Epoch 7/10
19/19 - 3s - loss: 169.9829 - loglik: -1.6842e+02 - logprior: -1.5596e+00
Epoch 8/10
19/19 - 3s - loss: 170.2169 - loglik: -1.6867e+02 - logprior: -1.5497e+00
Fitted a model with MAP estimate = -169.7740
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 2), (71, 2), (72, 1), (77, 1)]
discards: [2]
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 166.0298 - loglik: -1.6308e+02 - logprior: -2.9468e+00
Epoch 2/2
19/19 - 3s - loss: 154.1348 - loglik: -1.5293e+02 - logprior: -1.2078e+00
Fitted a model with MAP estimate = -152.7855
expansions: []
discards: [42 59 60]
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 156.5665 - loglik: -1.5363e+02 - logprior: -2.9360e+00
Epoch 2/2
19/19 - 3s - loss: 152.9766 - loglik: -1.5184e+02 - logprior: -1.1380e+00
Fitted a model with MAP estimate = -152.3366
expansions: []
discards: [ 0 90]
Fitting a model of length 101 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 160.8720 - loglik: -1.5704e+02 - logprior: -3.8275e+00
Epoch 2/10
19/19 - 3s - loss: 156.9807 - loglik: -1.5504e+02 - logprior: -1.9378e+00
Epoch 3/10
19/19 - 3s - loss: 156.3958 - loglik: -1.5460e+02 - logprior: -1.7949e+00
Epoch 4/10
19/19 - 4s - loss: 155.1018 - loglik: -1.5374e+02 - logprior: -1.3664e+00
Epoch 5/10
19/19 - 3s - loss: 153.8456 - loglik: -1.5303e+02 - logprior: -8.1520e-01
Epoch 6/10
19/19 - 4s - loss: 151.8929 - loglik: -1.5095e+02 - logprior: -9.4230e-01
Epoch 7/10
19/19 - 3s - loss: 151.4831 - loglik: -1.5053e+02 - logprior: -9.5454e-01
Epoch 8/10
19/19 - 3s - loss: 151.3640 - loglik: -1.5042e+02 - logprior: -9.4131e-01
Epoch 9/10
19/19 - 3s - loss: 151.3243 - loglik: -1.5038e+02 - logprior: -9.4863e-01
Epoch 10/10
19/19 - 3s - loss: 151.1061 - loglik: -1.5019e+02 - logprior: -9.1799e-01
Fitted a model with MAP estimate = -151.0967
Time for alignment: 96.7310
Computed alignments with likelihoods: ['-150.3047', '-150.5068', '-151.0967']
Best model has likelihood: -150.3047
SP score = 0.9177
Training of 3 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdabc6552b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdaf786c040>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.8679 - loglik: -2.2874e+02 - logprior: -3.1267e+00
Epoch 2/10
19/19 - 1s - loss: 196.2732 - loglik: -1.9497e+02 - logprior: -1.3072e+00
Epoch 3/10
19/19 - 1s - loss: 184.3296 - loglik: -1.8296e+02 - logprior: -1.3665e+00
Epoch 4/10
19/19 - 1s - loss: 181.2741 - loglik: -1.7993e+02 - logprior: -1.3408e+00
Epoch 5/10
19/19 - 1s - loss: 180.0619 - loglik: -1.7874e+02 - logprior: -1.3228e+00
Epoch 6/10
19/19 - 1s - loss: 179.8791 - loglik: -1.7858e+02 - logprior: -1.3008e+00
Epoch 7/10
19/19 - 1s - loss: 179.4326 - loglik: -1.7814e+02 - logprior: -1.2888e+00
Epoch 8/10
19/19 - 1s - loss: 179.0192 - loglik: -1.7773e+02 - logprior: -1.2900e+00
Epoch 9/10
19/19 - 1s - loss: 179.2508 - loglik: -1.7797e+02 - logprior: -1.2835e+00
Fitted a model with MAP estimate = -179.0602
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (22, 1), (23, 1), (32, 1), (34, 1), (35, 1), (49, 1), (50, 2), (53, 2), (55, 3)]
discards: [0]
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.2332 - loglik: -1.7933e+02 - logprior: -3.9003e+00
Epoch 2/2
19/19 - 2s - loss: 176.1436 - loglik: -1.7406e+02 - logprior: -2.0832e+00
Fitted a model with MAP estimate = -174.8885
expansions: [(0, 2)]
discards: [ 0 64 69]
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 176.9396 - loglik: -1.7403e+02 - logprior: -2.9094e+00
Epoch 2/2
19/19 - 2s - loss: 173.4152 - loglik: -1.7226e+02 - logprior: -1.1513e+00
Fitted a model with MAP estimate = -172.7043
expansions: []
discards: [0]
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 177.3822 - loglik: -1.7372e+02 - logprior: -3.6605e+00
Epoch 2/10
19/19 - 2s - loss: 173.7512 - loglik: -1.7248e+02 - logprior: -1.2747e+00
Epoch 3/10
19/19 - 2s - loss: 172.8271 - loglik: -1.7173e+02 - logprior: -1.0993e+00
Epoch 4/10
19/19 - 2s - loss: 172.2855 - loglik: -1.7122e+02 - logprior: -1.0636e+00
Epoch 5/10
19/19 - 2s - loss: 171.4860 - loglik: -1.7045e+02 - logprior: -1.0371e+00
Epoch 6/10
19/19 - 2s - loss: 171.1878 - loglik: -1.7016e+02 - logprior: -1.0236e+00
Epoch 7/10
19/19 - 2s - loss: 171.1239 - loglik: -1.7011e+02 - logprior: -1.0121e+00
Epoch 8/10
19/19 - 2s - loss: 170.7537 - loglik: -1.6976e+02 - logprior: -9.9627e-01
Epoch 9/10
19/19 - 2s - loss: 170.7302 - loglik: -1.6974e+02 - logprior: -9.8560e-01
Epoch 10/10
19/19 - 2s - loss: 170.5958 - loglik: -1.6963e+02 - logprior: -9.6599e-01
Fitted a model with MAP estimate = -170.6694
Time for alignment: 58.9730
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.9990 - loglik: -2.2887e+02 - logprior: -3.1295e+00
Epoch 2/10
19/19 - 1s - loss: 198.5592 - loglik: -1.9725e+02 - logprior: -1.3118e+00
Epoch 3/10
19/19 - 1s - loss: 185.3521 - loglik: -1.8399e+02 - logprior: -1.3618e+00
Epoch 4/10
19/19 - 1s - loss: 182.3311 - loglik: -1.8099e+02 - logprior: -1.3380e+00
Epoch 5/10
19/19 - 1s - loss: 180.5249 - loglik: -1.7918e+02 - logprior: -1.3434e+00
Epoch 6/10
19/19 - 1s - loss: 180.2245 - loglik: -1.7890e+02 - logprior: -1.3281e+00
Epoch 7/10
19/19 - 1s - loss: 179.7808 - loglik: -1.7844e+02 - logprior: -1.3378e+00
Epoch 8/10
19/19 - 1s - loss: 179.4777 - loglik: -1.7814e+02 - logprior: -1.3334e+00
Epoch 9/10
19/19 - 1s - loss: 179.3877 - loglik: -1.7806e+02 - logprior: -1.3277e+00
Epoch 10/10
19/19 - 1s - loss: 179.7923 - loglik: -1.7846e+02 - logprior: -1.3289e+00
Fitted a model with MAP estimate = -179.4234
expansions: [(7, 2), (8, 2), (9, 2), (22, 1), (23, 1), (24, 1), (31, 2), (34, 1), (35, 1), (37, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 183.1608 - loglik: -1.7926e+02 - logprior: -3.9025e+00
Epoch 2/2
19/19 - 2s - loss: 175.8687 - loglik: -1.7377e+02 - logprior: -2.1034e+00
Fitted a model with MAP estimate = -174.3768
expansions: [(0, 2), (10, 1)]
discards: [ 0 40 72]
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 176.4645 - loglik: -1.7360e+02 - logprior: -2.8654e+00
Epoch 2/2
19/19 - 2s - loss: 173.1947 - loglik: -1.7211e+02 - logprior: -1.0839e+00
Fitted a model with MAP estimate = -172.4511
expansions: []
discards: [0]
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.1483 - loglik: -1.7353e+02 - logprior: -3.6182e+00
Epoch 2/10
19/19 - 2s - loss: 173.6434 - loglik: -1.7242e+02 - logprior: -1.2231e+00
Epoch 3/10
19/19 - 2s - loss: 172.5420 - loglik: -1.7152e+02 - logprior: -1.0233e+00
Epoch 4/10
19/19 - 2s - loss: 171.7956 - loglik: -1.7081e+02 - logprior: -9.8253e-01
Epoch 5/10
19/19 - 2s - loss: 171.4784 - loglik: -1.7052e+02 - logprior: -9.5995e-01
Epoch 6/10
19/19 - 2s - loss: 170.9296 - loglik: -1.6997e+02 - logprior: -9.5752e-01
Epoch 7/10
19/19 - 2s - loss: 170.7265 - loglik: -1.6979e+02 - logprior: -9.3186e-01
Epoch 8/10
19/19 - 2s - loss: 170.3950 - loglik: -1.6946e+02 - logprior: -9.3371e-01
Epoch 9/10
19/19 - 2s - loss: 170.7499 - loglik: -1.6984e+02 - logprior: -9.1375e-01
Fitted a model with MAP estimate = -170.3916
Time for alignment: 57.8957
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.7034 - loglik: -2.2857e+02 - logprior: -3.1321e+00
Epoch 2/10
19/19 - 1s - loss: 195.7355 - loglik: -1.9443e+02 - logprior: -1.3080e+00
Epoch 3/10
19/19 - 1s - loss: 184.1661 - loglik: -1.8281e+02 - logprior: -1.3530e+00
Epoch 4/10
19/19 - 1s - loss: 181.3051 - loglik: -1.7999e+02 - logprior: -1.3162e+00
Epoch 5/10
19/19 - 1s - loss: 180.3460 - loglik: -1.7905e+02 - logprior: -1.2951e+00
Epoch 6/10
19/19 - 1s - loss: 179.8837 - loglik: -1.7861e+02 - logprior: -1.2703e+00
Epoch 7/10
19/19 - 1s - loss: 179.9832 - loglik: -1.7871e+02 - logprior: -1.2693e+00
Fitted a model with MAP estimate = -179.5033
expansions: [(7, 2), (8, 3), (9, 2), (23, 1), (32, 2), (33, 1), (34, 1), (35, 2), (47, 1), (49, 1), (50, 2), (53, 2), (55, 3)]
discards: [0]
Fitting a model of length 87 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.4570 - loglik: -1.7953e+02 - logprior: -3.9238e+00
Epoch 2/2
19/19 - 2s - loss: 176.2745 - loglik: -1.7415e+02 - logprior: -2.1258e+00
Fitted a model with MAP estimate = -174.6875
expansions: [(0, 2), (10, 1)]
discards: [ 0 40 45 66 70 75]
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 177.3435 - loglik: -1.7447e+02 - logprior: -2.8762e+00
Epoch 2/2
19/19 - 2s - loss: 173.7949 - loglik: -1.7270e+02 - logprior: -1.0989e+00
Fitted a model with MAP estimate = -173.1391
expansions: []
discards: [0]
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.8435 - loglik: -1.7423e+02 - logprior: -3.6171e+00
Epoch 2/10
19/19 - 2s - loss: 174.2219 - loglik: -1.7300e+02 - logprior: -1.2209e+00
Epoch 3/10
19/19 - 2s - loss: 173.2141 - loglik: -1.7219e+02 - logprior: -1.0273e+00
Epoch 4/10
19/19 - 2s - loss: 172.5850 - loglik: -1.7160e+02 - logprior: -9.8959e-01
Epoch 5/10
19/19 - 2s - loss: 171.9053 - loglik: -1.7095e+02 - logprior: -9.5944e-01
Epoch 6/10
19/19 - 2s - loss: 171.7456 - loglik: -1.7079e+02 - logprior: -9.5412e-01
Epoch 7/10
19/19 - 2s - loss: 171.0395 - loglik: -1.7009e+02 - logprior: -9.4555e-01
Epoch 8/10
19/19 - 2s - loss: 171.1771 - loglik: -1.7025e+02 - logprior: -9.2751e-01
Fitted a model with MAP estimate = -171.0816
Time for alignment: 52.5716
Computed alignments with likelihoods: ['-170.6694', '-170.3916', '-171.0816']
Best model has likelihood: -170.3916
SP score = 0.8143
Training of 3 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdae6b57430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdaf7a466a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 666.7059 - loglik: -6.6407e+02 - logprior: -2.6335e+00
Epoch 2/10
19/19 - 18s - loss: 620.2991 - loglik: -6.1938e+02 - logprior: -9.2329e-01
Epoch 3/10
19/19 - 19s - loss: 596.7707 - loglik: -5.9567e+02 - logprior: -1.1045e+00
Epoch 4/10
19/19 - 20s - loss: 587.9497 - loglik: -5.8678e+02 - logprior: -1.1681e+00
Epoch 5/10
19/19 - 20s - loss: 584.9654 - loglik: -5.8379e+02 - logprior: -1.1793e+00
Epoch 6/10
19/19 - 22s - loss: 581.0518 - loglik: -5.7990e+02 - logprior: -1.1495e+00
Epoch 7/10
19/19 - 22s - loss: 580.2470 - loglik: -5.7908e+02 - logprior: -1.1694e+00
Epoch 8/10
19/19 - 21s - loss: 576.1002 - loglik: -5.7490e+02 - logprior: -1.1997e+00
Epoch 9/10
19/19 - 19s - loss: 576.8072 - loglik: -5.7559e+02 - logprior: -1.2176e+00
Fitted a model with MAP estimate = -575.2500
expansions: [(23, 1), (24, 5), (28, 1), (47, 6), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (83, 2), (84, 2), (96, 1), (103, 1), (104, 3), (105, 1), (108, 1), (109, 1), (120, 1), (123, 1), (126, 1), (135, 1), (139, 1), (150, 1), (155, 1), (156, 3), (161, 1), (163, 1), (167, 1), (168, 1), (169, 1), (170, 1), (176, 2), (179, 1)]
discards: [0]
Fitting a model of length 242 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 615.1833 - loglik: -6.1146e+02 - logprior: -3.7194e+00
Epoch 2/2
19/19 - 23s - loss: 588.3068 - loglik: -5.8666e+02 - logprior: -1.6440e+00
Fitted a model with MAP estimate = -584.8797
expansions: [(0, 3), (31, 3), (242, 6)]
discards: [  0  26  27  28 104 128 132 187 188 197 225 239 240 241]
Fitting a model of length 240 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 589.4305 - loglik: -5.8677e+02 - logprior: -2.6653e+00
Epoch 2/2
19/19 - 22s - loss: 583.7883 - loglik: -5.8303e+02 - logprior: -7.5823e-01
Fitted a model with MAP estimate = -581.4748
expansions: [(57, 1), (62, 1), (185, 2)]
discards: [  0   2  58  81 235 236 237 238 239]
Fitting a model of length 235 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 24s - loss: 589.3884 - loglik: -5.8595e+02 - logprior: -3.4398e+00
Epoch 2/10
19/19 - 22s - loss: 585.1816 - loglik: -5.8410e+02 - logprior: -1.0849e+00
Epoch 3/10
19/19 - 22s - loss: 582.0442 - loglik: -5.8178e+02 - logprior: -2.5934e-01
Epoch 4/10
19/19 - 22s - loss: 578.8597 - loglik: -5.7871e+02 - logprior: -1.4706e-01
Epoch 5/10
19/19 - 23s - loss: 577.0212 - loglik: -5.7687e+02 - logprior: -1.5479e-01
Epoch 6/10
19/19 - 22s - loss: 571.2062 - loglik: -5.7108e+02 - logprior: -1.3084e-01
Epoch 7/10
19/19 - 22s - loss: 568.9561 - loglik: -5.6882e+02 - logprior: -1.3965e-01
Epoch 8/10
19/19 - 21s - loss: 565.6626 - loglik: -5.6549e+02 - logprior: -1.7200e-01
Epoch 9/10
19/19 - 21s - loss: 564.2326 - loglik: -5.6405e+02 - logprior: -1.7761e-01
Epoch 10/10
19/19 - 20s - loss: 563.4135 - loglik: -5.6320e+02 - logprior: -2.1555e-01
Fitted a model with MAP estimate = -562.5806
Time for alignment: 595.3163
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 667.1359 - loglik: -6.6451e+02 - logprior: -2.6263e+00
Epoch 2/10
19/19 - 14s - loss: 621.6600 - loglik: -6.2072e+02 - logprior: -9.3513e-01
Epoch 3/10
19/19 - 14s - loss: 597.8267 - loglik: -5.9673e+02 - logprior: -1.0936e+00
Epoch 4/10
19/19 - 14s - loss: 588.9186 - loglik: -5.8779e+02 - logprior: -1.1283e+00
Epoch 5/10
19/19 - 14s - loss: 585.9080 - loglik: -5.8478e+02 - logprior: -1.1316e+00
Epoch 6/10
19/19 - 14s - loss: 580.5121 - loglik: -5.7940e+02 - logprior: -1.1146e+00
Epoch 7/10
19/19 - 14s - loss: 580.9438 - loglik: -5.7982e+02 - logprior: -1.1226e+00
Fitted a model with MAP estimate = -577.8237
expansions: [(23, 1), (24, 5), (48, 5), (49, 1), (60, 1), (62, 2), (63, 2), (65, 1), (80, 1), (81, 2), (82, 2), (83, 3), (103, 1), (104, 4), (105, 2), (109, 1), (120, 1), (123, 1), (126, 1), (131, 1), (139, 1), (145, 1), (147, 1), (150, 1), (155, 1), (156, 2), (161, 1), (163, 1), (164, 1), (166, 1), (167, 1), (169, 1), (176, 2), (179, 1)]
discards: [0]
Fitting a model of length 245 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 22s - loss: 602.5082 - loglik: -5.9898e+02 - logprior: -3.5258e+00
Epoch 2/2
19/19 - 19s - loss: 587.2871 - loglik: -5.8577e+02 - logprior: -1.5162e+00
Fitted a model with MAP estimate = -584.0318
expansions: [(0, 3), (31, 3), (179, 1)]
discards: [  0  26  27  28  98 101 128 132 134 243 244]
Fitting a model of length 241 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 21s - loss: 588.2293 - loglik: -5.8560e+02 - logprior: -2.6304e+00
Epoch 2/2
19/19 - 18s - loss: 583.5234 - loglik: -5.8287e+02 - logprior: -6.5257e-01
Fitted a model with MAP estimate = -581.3202
expansions: [(62, 1), (241, 9)]
discards: [  0   2  59  60  81 225 226]
Fitting a model of length 244 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 589.2791 - loglik: -5.8587e+02 - logprior: -3.4071e+00
Epoch 2/10
19/19 - 19s - loss: 584.1029 - loglik: -5.8281e+02 - logprior: -1.2893e+00
Epoch 3/10
19/19 - 19s - loss: 581.0640 - loglik: -5.8057e+02 - logprior: -4.9576e-01
Epoch 4/10
19/19 - 19s - loss: 578.1639 - loglik: -5.7775e+02 - logprior: -4.1007e-01
Epoch 5/10
19/19 - 19s - loss: 574.6690 - loglik: -5.7424e+02 - logprior: -4.3088e-01
Epoch 6/10
19/19 - 19s - loss: 571.2214 - loglik: -5.7077e+02 - logprior: -4.4679e-01
Epoch 7/10
19/19 - 19s - loss: 567.9103 - loglik: -5.6746e+02 - logprior: -4.5401e-01
Epoch 8/10
19/19 - 20s - loss: 565.6423 - loglik: -5.6516e+02 - logprior: -4.8123e-01
Epoch 9/10
19/19 - 20s - loss: 562.9457 - loglik: -5.6244e+02 - logprior: -5.1068e-01
Epoch 10/10
19/19 - 20s - loss: 561.8104 - loglik: -5.6127e+02 - logprior: -5.3657e-01
Fitted a model with MAP estimate = -561.5878
Time for alignment: 464.3619
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 16s - loss: 667.1234 - loglik: -6.6449e+02 - logprior: -2.6337e+00
Epoch 2/10
19/19 - 14s - loss: 620.0532 - loglik: -6.1913e+02 - logprior: -9.2528e-01
Epoch 3/10
19/19 - 14s - loss: 597.1007 - loglik: -5.9602e+02 - logprior: -1.0795e+00
Epoch 4/10
19/19 - 14s - loss: 588.1001 - loglik: -5.8694e+02 - logprior: -1.1578e+00
Epoch 5/10
19/19 - 15s - loss: 584.7754 - loglik: -5.8362e+02 - logprior: -1.1536e+00
Epoch 6/10
19/19 - 15s - loss: 581.6547 - loglik: -5.8052e+02 - logprior: -1.1358e+00
Epoch 7/10
19/19 - 15s - loss: 580.1663 - loglik: -5.7899e+02 - logprior: -1.1731e+00
Epoch 8/10
19/19 - 15s - loss: 576.8783 - loglik: -5.7569e+02 - logprior: -1.1849e+00
Epoch 9/10
19/19 - 15s - loss: 574.9724 - loglik: -5.7374e+02 - logprior: -1.2286e+00
Epoch 10/10
19/19 - 15s - loss: 576.0779 - loglik: -5.7485e+02 - logprior: -1.2279e+00
Fitted a model with MAP estimate = -574.9900
expansions: [(23, 1), (24, 6), (28, 1), (47, 5), (49, 1), (60, 1), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (83, 2), (84, 2), (104, 1), (105, 4), (106, 2), (110, 1), (124, 1), (125, 1), (127, 1), (134, 1), (135, 1), (148, 1), (150, 1), (155, 1), (156, 3), (161, 1), (163, 1), (165, 1), (168, 1), (169, 1), (170, 1), (179, 1)]
discards: [0]
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 619.6863 - loglik: -6.1593e+02 - logprior: -3.7535e+00
Epoch 2/2
19/19 - 23s - loss: 589.7601 - loglik: -5.8806e+02 - logprior: -1.7013e+00
Fitted a model with MAP estimate = -585.3184
expansions: [(0, 3), (32, 3), (243, 6)]
discards: [  0  26  27  28  29 106 133 135 200 240 241 242]
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 29s - loss: 589.2920 - loglik: -5.8663e+02 - logprior: -2.6654e+00
Epoch 2/2
19/19 - 29s - loss: 583.5867 - loglik: -5.8285e+02 - logprior: -7.3323e-01
Fitted a model with MAP estimate = -581.2497
expansions: [(28, 3), (189, 1), (223, 1)]
discards: [  0   2  36  37  38  57  58  60 130 238 239 240 241 242]
Fitting a model of length 234 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 590.3713 - loglik: -5.8698e+02 - logprior: -3.3901e+00
Epoch 2/10
19/19 - 24s - loss: 585.2078 - loglik: -5.8421e+02 - logprior: -1.0020e+00
Epoch 3/10
19/19 - 26s - loss: 581.5609 - loglik: -5.8133e+02 - logprior: -2.2804e-01
Epoch 4/10
19/19 - 27s - loss: 579.6478 - loglik: -5.7954e+02 - logprior: -1.0650e-01
Epoch 5/10
19/19 - 25s - loss: 576.2896 - loglik: -5.7619e+02 - logprior: -1.0309e-01
Epoch 6/10
19/19 - 23s - loss: 571.2960 - loglik: -5.7121e+02 - logprior: -8.6707e-02
Epoch 7/10
19/19 - 23s - loss: 570.0990 - loglik: -5.7000e+02 - logprior: -9.6798e-02
Epoch 8/10
19/19 - 24s - loss: 565.8488 - loglik: -5.6573e+02 - logprior: -1.1800e-01
Epoch 9/10
19/19 - 27s - loss: 563.6776 - loglik: -5.6354e+02 - logprior: -1.3553e-01
Epoch 10/10
19/19 - 27s - loss: 562.2905 - loglik: -5.6213e+02 - logprior: -1.5609e-01
Fitted a model with MAP estimate = -562.6308
Time for alignment: 616.4551
Computed alignments with likelihoods: ['-562.5806', '-561.5878', '-562.6308']
Best model has likelihood: -561.5878
SP score = 0.5337
Training of 3 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb19d32a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdade488d90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.1413 - loglik: -1.5092e+02 - logprior: -3.2245e+00
Epoch 2/10
19/19 - 1s - loss: 123.7481 - loglik: -1.2226e+02 - logprior: -1.4905e+00
Epoch 3/10
19/19 - 1s - loss: 111.5731 - loglik: -1.1005e+02 - logprior: -1.5202e+00
Epoch 4/10
19/19 - 1s - loss: 108.6005 - loglik: -1.0702e+02 - logprior: -1.5776e+00
Epoch 5/10
19/19 - 1s - loss: 107.3053 - loglik: -1.0581e+02 - logprior: -1.4919e+00
Epoch 6/10
19/19 - 1s - loss: 107.0265 - loglik: -1.0552e+02 - logprior: -1.5046e+00
Epoch 7/10
19/19 - 1s - loss: 107.0700 - loglik: -1.0559e+02 - logprior: -1.4767e+00
Fitted a model with MAP estimate = -106.6764
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.9587 - loglik: -1.0390e+02 - logprior: -3.0557e+00
Epoch 2/2
19/19 - 1s - loss: 99.7467 - loglik: -9.8412e+01 - logprior: -1.3349e+00
Fitted a model with MAP estimate = -99.0066
expansions: []
discards: [ 0 21 37 40]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 105.4989 - loglik: -1.0143e+02 - logprior: -4.0683e+00
Epoch 2/2
19/19 - 1s - loss: 101.5226 - loglik: -9.9254e+01 - logprior: -2.2688e+00
Fitted a model with MAP estimate = -100.4300
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 104.1450 - loglik: -1.0024e+02 - logprior: -3.9053e+00
Epoch 2/10
19/19 - 1s - loss: 100.2757 - loglik: -9.8794e+01 - logprior: -1.4820e+00
Epoch 3/10
19/19 - 1s - loss: 99.2799 - loglik: -9.8033e+01 - logprior: -1.2464e+00
Epoch 4/10
19/19 - 1s - loss: 98.9731 - loglik: -9.7755e+01 - logprior: -1.2182e+00
Epoch 5/10
19/19 - 1s - loss: 98.5217 - loglik: -9.7338e+01 - logprior: -1.1836e+00
Epoch 6/10
19/19 - 1s - loss: 98.5259 - loglik: -9.7360e+01 - logprior: -1.1663e+00
Fitted a model with MAP estimate = -98.2049
Time for alignment: 38.5238
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 153.9791 - loglik: -1.5075e+02 - logprior: -3.2290e+00
Epoch 2/10
19/19 - 1s - loss: 124.4542 - loglik: -1.2294e+02 - logprior: -1.5131e+00
Epoch 3/10
19/19 - 1s - loss: 111.6097 - loglik: -1.0998e+02 - logprior: -1.6307e+00
Epoch 4/10
19/19 - 1s - loss: 107.9426 - loglik: -1.0625e+02 - logprior: -1.6895e+00
Epoch 5/10
19/19 - 1s - loss: 107.1002 - loglik: -1.0547e+02 - logprior: -1.6285e+00
Epoch 6/10
19/19 - 1s - loss: 106.8014 - loglik: -1.0521e+02 - logprior: -1.5958e+00
Epoch 7/10
19/19 - 1s - loss: 106.5454 - loglik: -1.0498e+02 - logprior: -1.5670e+00
Epoch 8/10
19/19 - 1s - loss: 106.4418 - loglik: -1.0489e+02 - logprior: -1.5550e+00
Epoch 9/10
19/19 - 1s - loss: 106.5192 - loglik: -1.0497e+02 - logprior: -1.5454e+00
Fitted a model with MAP estimate = -106.3433
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 2)]
discards: []
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 108.1674 - loglik: -1.0488e+02 - logprior: -3.2917e+00
Epoch 2/2
19/19 - 1s - loss: 100.4386 - loglik: -9.8971e+01 - logprior: -1.4672e+00
Fitted a model with MAP estimate = -99.5027
expansions: []
discards: [20 38 41]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.4376 - loglik: -9.9260e+01 - logprior: -3.1776e+00
Epoch 2/2
19/19 - 1s - loss: 99.6543 - loglik: -9.8307e+01 - logprior: -1.3476e+00
Fitted a model with MAP estimate = -99.0988
expansions: []
discards: []
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 102.1240 - loglik: -9.8992e+01 - logprior: -3.1321e+00
Epoch 2/10
19/19 - 1s - loss: 99.4920 - loglik: -9.8174e+01 - logprior: -1.3185e+00
Epoch 3/10
19/19 - 1s - loss: 98.8840 - loglik: -9.7667e+01 - logprior: -1.2175e+00
Epoch 4/10
19/19 - 1s - loss: 98.6645 - loglik: -9.7492e+01 - logprior: -1.1727e+00
Epoch 5/10
19/19 - 1s - loss: 98.2241 - loglik: -9.7076e+01 - logprior: -1.1479e+00
Epoch 6/10
19/19 - 1s - loss: 98.0366 - loglik: -9.6902e+01 - logprior: -1.1345e+00
Epoch 7/10
19/19 - 1s - loss: 97.8908 - loglik: -9.6771e+01 - logprior: -1.1198e+00
Epoch 8/10
19/19 - 1s - loss: 97.6489 - loglik: -9.6549e+01 - logprior: -1.0998e+00
Epoch 9/10
19/19 - 1s - loss: 97.9031 - loglik: -9.6815e+01 - logprior: -1.0880e+00
Fitted a model with MAP estimate = -97.6715
Time for alignment: 43.2958
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.0392 - loglik: -1.5081e+02 - logprior: -3.2263e+00
Epoch 2/10
19/19 - 1s - loss: 124.3699 - loglik: -1.2288e+02 - logprior: -1.4936e+00
Epoch 3/10
19/19 - 1s - loss: 110.8195 - loglik: -1.0925e+02 - logprior: -1.5652e+00
Epoch 4/10
19/19 - 1s - loss: 107.3828 - loglik: -1.0577e+02 - logprior: -1.6177e+00
Epoch 5/10
19/19 - 1s - loss: 106.2205 - loglik: -1.0467e+02 - logprior: -1.5509e+00
Epoch 6/10
19/19 - 1s - loss: 105.8284 - loglik: -1.0428e+02 - logprior: -1.5520e+00
Epoch 7/10
19/19 - 1s - loss: 105.5399 - loglik: -1.0402e+02 - logprior: -1.5221e+00
Epoch 8/10
19/19 - 1s - loss: 105.6539 - loglik: -1.0414e+02 - logprior: -1.5128e+00
Fitted a model with MAP estimate = -105.4250
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (14, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.8932 - loglik: -1.0382e+02 - logprior: -3.0700e+00
Epoch 2/2
19/19 - 1s - loss: 99.8475 - loglik: -9.8525e+01 - logprior: -1.3220e+00
Fitted a model with MAP estimate = -99.0067
expansions: []
discards: [22 37 40]
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.7658 - loglik: -9.8798e+01 - logprior: -2.9681e+00
Epoch 2/2
19/19 - 1s - loss: 99.0148 - loglik: -9.7813e+01 - logprior: -1.2022e+00
Fitted a model with MAP estimate = -98.6483
expansions: []
discards: []
Fitting a model of length 58 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 101.3047 - loglik: -9.8362e+01 - logprior: -2.9423e+00
Epoch 2/10
19/19 - 1s - loss: 98.9784 - loglik: -9.7804e+01 - logprior: -1.1745e+00
Epoch 3/10
19/19 - 1s - loss: 98.5475 - loglik: -9.7405e+01 - logprior: -1.1423e+00
Epoch 4/10
19/19 - 1s - loss: 98.1196 - loglik: -9.7017e+01 - logprior: -1.1029e+00
Epoch 5/10
19/19 - 1s - loss: 97.7233 - loglik: -9.6655e+01 - logprior: -1.0687e+00
Epoch 6/10
19/19 - 1s - loss: 97.5682 - loglik: -9.6513e+01 - logprior: -1.0556e+00
Epoch 7/10
19/19 - 1s - loss: 97.5539 - loglik: -9.6510e+01 - logprior: -1.0443e+00
Epoch 8/10
19/19 - 1s - loss: 97.2811 - loglik: -9.6257e+01 - logprior: -1.0239e+00
Epoch 9/10
19/19 - 1s - loss: 97.1412 - loglik: -9.6126e+01 - logprior: -1.0154e+00
Epoch 10/10
19/19 - 1s - loss: 97.5211 - loglik: -9.6523e+01 - logprior: -9.9766e-01
Fitted a model with MAP estimate = -97.2225
Time for alignment: 43.4230
Computed alignments with likelihoods: ['-98.2049', '-97.6715', '-97.2225']
Best model has likelihood: -97.2225
SP score = 0.9630
Training of 3 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb000e9e20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdade6acd60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 365.8485 - loglik: -3.6286e+02 - logprior: -2.9924e+00
Epoch 2/10
19/19 - 3s - loss: 337.2708 - loglik: -3.3624e+02 - logprior: -1.0332e+00
Epoch 3/10
19/19 - 3s - loss: 325.3082 - loglik: -3.2405e+02 - logprior: -1.2606e+00
Epoch 4/10
19/19 - 3s - loss: 321.6607 - loglik: -3.2052e+02 - logprior: -1.1428e+00
Epoch 5/10
19/19 - 3s - loss: 319.2289 - loglik: -3.1807e+02 - logprior: -1.1631e+00
Epoch 6/10
19/19 - 3s - loss: 317.6461 - loglik: -3.1648e+02 - logprior: -1.1675e+00
Epoch 7/10
19/19 - 4s - loss: 316.0832 - loglik: -3.1492e+02 - logprior: -1.1634e+00
Epoch 8/10
19/19 - 4s - loss: 315.4383 - loglik: -3.1428e+02 - logprior: -1.1614e+00
Epoch 9/10
19/19 - 3s - loss: 315.2063 - loglik: -3.1404e+02 - logprior: -1.1673e+00
Epoch 10/10
19/19 - 3s - loss: 314.6575 - loglik: -3.1348e+02 - logprior: -1.1794e+00
Fitted a model with MAP estimate = -314.2535
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (23, 2), (27, 1), (29, 1), (32, 1), (51, 2), (54, 4), (74, 1), (75, 3), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 333.0807 - loglik: -3.2936e+02 - logprior: -3.7176e+00
Epoch 2/2
19/19 - 5s - loss: 318.4473 - loglik: -3.1654e+02 - logprior: -1.9120e+00
Fitted a model with MAP estimate = -316.0304
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 317.6127 - loglik: -3.1483e+02 - logprior: -2.7822e+00
Epoch 2/2
19/19 - 5s - loss: 314.4329 - loglik: -3.1345e+02 - logprior: -9.7798e-01
Fitted a model with MAP estimate = -313.3074
expansions: []
discards: [  1  94  95  96 109]
Fitting a model of length 127 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 317.9317 - loglik: -3.1534e+02 - logprior: -2.5890e+00
Epoch 2/10
19/19 - 5s - loss: 314.9188 - loglik: -3.1402e+02 - logprior: -8.9976e-01
Epoch 3/10
19/19 - 5s - loss: 313.6325 - loglik: -3.1273e+02 - logprior: -9.0104e-01
Epoch 4/10
19/19 - 5s - loss: 312.5699 - loglik: -3.1176e+02 - logprior: -8.1420e-01
Epoch 5/10
19/19 - 5s - loss: 310.9945 - loglik: -3.1020e+02 - logprior: -7.9823e-01
Epoch 6/10
19/19 - 5s - loss: 309.5277 - loglik: -3.0875e+02 - logprior: -7.7730e-01
Epoch 7/10
19/19 - 5s - loss: 308.0471 - loglik: -3.0728e+02 - logprior: -7.6557e-01
Epoch 8/10
19/19 - 5s - loss: 307.5398 - loglik: -3.0679e+02 - logprior: -7.4678e-01
Epoch 9/10
19/19 - 5s - loss: 305.9872 - loglik: -3.0525e+02 - logprior: -7.4122e-01
Epoch 10/10
19/19 - 5s - loss: 306.3585 - loglik: -3.0563e+02 - logprior: -7.2980e-01
Fitted a model with MAP estimate = -305.5914
Time for alignment: 141.7007
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 365.9343 - loglik: -3.6294e+02 - logprior: -2.9933e+00
Epoch 2/10
19/19 - 4s - loss: 338.1785 - loglik: -3.3715e+02 - logprior: -1.0300e+00
Epoch 3/10
19/19 - 4s - loss: 325.5989 - loglik: -3.2434e+02 - logprior: -1.2562e+00
Epoch 4/10
19/19 - 4s - loss: 320.8127 - loglik: -3.1963e+02 - logprior: -1.1806e+00
Epoch 5/10
19/19 - 4s - loss: 318.6034 - loglik: -3.1737e+02 - logprior: -1.2300e+00
Epoch 6/10
19/19 - 4s - loss: 317.3254 - loglik: -3.1609e+02 - logprior: -1.2372e+00
Epoch 7/10
19/19 - 4s - loss: 315.6340 - loglik: -3.1441e+02 - logprior: -1.2259e+00
Epoch 8/10
19/19 - 4s - loss: 315.5730 - loglik: -3.1435e+02 - logprior: -1.2217e+00
Epoch 9/10
19/19 - 4s - loss: 314.7639 - loglik: -3.1354e+02 - logprior: -1.2284e+00
Epoch 10/10
19/19 - 4s - loss: 314.2940 - loglik: -3.1306e+02 - logprior: -1.2339e+00
Fitted a model with MAP estimate = -314.1486
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (24, 2), (27, 1), (28, 1), (29, 1), (31, 1), (35, 2), (54, 4), (74, 1), (75, 3), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 333.0287 - loglik: -3.2931e+02 - logprior: -3.7189e+00
Epoch 2/2
19/19 - 6s - loss: 318.1828 - loglik: -3.1621e+02 - logprior: -1.9763e+00
Fitted a model with MAP estimate = -315.7708
expansions: [(0, 2)]
discards: [  0  48 110]
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 317.3206 - loglik: -3.1453e+02 - logprior: -2.7876e+00
Epoch 2/2
19/19 - 6s - loss: 314.1586 - loglik: -3.1317e+02 - logprior: -9.8846e-01
Fitted a model with MAP estimate = -313.0408
expansions: [(32, 1), (33, 1)]
discards: [24 25 26 94 95 96]
Fitting a model of length 127 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 317.8422 - loglik: -3.1512e+02 - logprior: -2.7188e+00
Epoch 2/10
19/19 - 5s - loss: 314.7838 - loglik: -3.1387e+02 - logprior: -9.1140e-01
Epoch 3/10
19/19 - 5s - loss: 313.5995 - loglik: -3.1275e+02 - logprior: -8.5213e-01
Epoch 4/10
19/19 - 5s - loss: 312.5504 - loglik: -3.1174e+02 - logprior: -8.1505e-01
Epoch 5/10
19/19 - 5s - loss: 310.8488 - loglik: -3.1007e+02 - logprior: -7.7874e-01
Epoch 6/10
19/19 - 5s - loss: 309.5804 - loglik: -3.0882e+02 - logprior: -7.5879e-01
Epoch 7/10
19/19 - 5s - loss: 308.4037 - loglik: -3.0766e+02 - logprior: -7.4442e-01
Epoch 8/10
19/19 - 5s - loss: 307.3586 - loglik: -3.0663e+02 - logprior: -7.2559e-01
Epoch 9/10
19/19 - 4s - loss: 306.3889 - loglik: -3.0567e+02 - logprior: -7.1691e-01
Epoch 10/10
19/19 - 5s - loss: 305.8448 - loglik: -3.0514e+02 - logprior: -7.0162e-01
Fitted a model with MAP estimate = -305.7618
Time for alignment: 148.9622
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 365.6437 - loglik: -3.6264e+02 - logprior: -2.9988e+00
Epoch 2/10
19/19 - 3s - loss: 336.9693 - loglik: -3.3592e+02 - logprior: -1.0527e+00
Epoch 3/10
19/19 - 4s - loss: 324.7927 - loglik: -3.2351e+02 - logprior: -1.2836e+00
Epoch 4/10
19/19 - 4s - loss: 320.6891 - loglik: -3.1949e+02 - logprior: -1.2023e+00
Epoch 5/10
19/19 - 4s - loss: 318.3501 - loglik: -3.1713e+02 - logprior: -1.2236e+00
Epoch 6/10
19/19 - 3s - loss: 316.6302 - loglik: -3.1542e+02 - logprior: -1.2121e+00
Epoch 7/10
19/19 - 4s - loss: 315.0322 - loglik: -3.1382e+02 - logprior: -1.2098e+00
Epoch 8/10
19/19 - 4s - loss: 314.6497 - loglik: -3.1344e+02 - logprior: -1.2126e+00
Epoch 9/10
19/19 - 4s - loss: 314.4686 - loglik: -3.1326e+02 - logprior: -1.2132e+00
Epoch 10/10
19/19 - 4s - loss: 313.6827 - loglik: -3.1246e+02 - logprior: -1.2207e+00
Fitted a model with MAP estimate = -313.4597
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (24, 3), (27, 1), (29, 1), (32, 1), (52, 2), (53, 2), (73, 1), (74, 1), (75, 1), (76, 3), (82, 2), (84, 1), (86, 1), (87, 1)]
discards: [0]
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 333.1781 - loglik: -3.2946e+02 - logprior: -3.7209e+00
Epoch 2/2
19/19 - 5s - loss: 318.3287 - loglik: -3.1639e+02 - logprior: -1.9419e+00
Fitted a model with MAP estimate = -315.7985
expansions: [(0, 2)]
discards: [  0 106]
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 317.2033 - loglik: -3.1445e+02 - logprior: -2.7544e+00
Epoch 2/2
19/19 - 5s - loss: 313.9343 - loglik: -3.1299e+02 - logprior: -9.4684e-01
Fitted a model with MAP estimate = -312.9173
expansions: []
discards: [24 25 26 67]
Fitting a model of length 127 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 317.5993 - loglik: -3.1490e+02 - logprior: -2.7003e+00
Epoch 2/10
19/19 - 4s - loss: 314.5536 - loglik: -3.1365e+02 - logprior: -9.0714e-01
Epoch 3/10
19/19 - 4s - loss: 313.4714 - loglik: -3.1263e+02 - logprior: -8.3873e-01
Epoch 4/10
19/19 - 4s - loss: 312.3303 - loglik: -3.1152e+02 - logprior: -8.0730e-01
Epoch 5/10
19/19 - 4s - loss: 310.8091 - loglik: -3.1003e+02 - logprior: -7.7658e-01
Epoch 6/10
19/19 - 5s - loss: 309.2198 - loglik: -3.0845e+02 - logprior: -7.6732e-01
Epoch 7/10
19/19 - 4s - loss: 307.6067 - loglik: -3.0685e+02 - logprior: -7.5428e-01
Epoch 8/10
19/19 - 4s - loss: 307.2749 - loglik: -3.0653e+02 - logprior: -7.3989e-01
Epoch 9/10
19/19 - 4s - loss: 306.4795 - loglik: -3.0575e+02 - logprior: -7.2873e-01
Epoch 10/10
19/19 - 4s - loss: 305.7841 - loglik: -3.0507e+02 - logprior: -7.1214e-01
Fitted a model with MAP estimate = -305.3750
Time for alignment: 136.5463
Computed alignments with likelihoods: ['-305.5914', '-305.7618', '-305.3750']
Best model has likelihood: -305.3750
SP score = 0.8198
Training of 3 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb2ab390d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb19fbefd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 919.3524 - loglik: -9.1790e+02 - logprior: -1.4519e+00
Epoch 2/10
39/39 - 43s - loss: 824.2354 - loglik: -8.2273e+02 - logprior: -1.5011e+00
Epoch 3/10
39/39 - 38s - loss: 813.8376 - loglik: -8.1233e+02 - logprior: -1.5055e+00
Epoch 4/10
39/39 - 40s - loss: 809.4951 - loglik: -8.0797e+02 - logprior: -1.5286e+00
Epoch 5/10
39/39 - 45s - loss: 806.7361 - loglik: -8.0516e+02 - logprior: -1.5781e+00
Epoch 6/10
39/39 - 49s - loss: 805.5297 - loglik: -8.0393e+02 - logprior: -1.6028e+00
Epoch 7/10
39/39 - 51s - loss: 805.1596 - loglik: -8.0351e+02 - logprior: -1.6479e+00
Epoch 8/10
39/39 - 49s - loss: 805.2447 - loglik: -8.0363e+02 - logprior: -1.6135e+00
Fitted a model with MAP estimate = -796.7876
expansions: [(13, 1), (16, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (30, 1), (40, 1), (42, 1), (43, 2), (45, 1), (46, 1), (56, 1), (59, 1), (60, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (91, 2), (93, 2), (95, 1), (97, 1), (98, 1), (103, 2), (119, 1), (130, 1), (145, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 2), (168, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 3), (207, 1), (208, 1), (209, 1), (211, 1), (220, 1), (222, 1), (228, 2), (240, 2), (241, 2), (242, 1), (245, 1), (258, 1), (261, 1), (262, 1), (263, 1), (269, 1), (270, 2), (271, 1), (272, 1)]
discards: []
Fitting a model of length 356 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 78s - loss: 810.4424 - loglik: -8.0891e+02 - logprior: -1.5313e+00
Epoch 2/2
39/39 - 66s - loss: 792.0597 - loglik: -7.9158e+02 - logprior: -4.8430e-01
Fitted a model with MAP estimate = -781.7841
expansions: [(158, 2)]
discards: [ 29 116 120 134 192 257 258 302 312 343]
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 794.9669 - loglik: -7.9370e+02 - logprior: -1.2666e+00
Epoch 2/2
39/39 - 60s - loss: 790.2861 - loglik: -7.8998e+02 - logprior: -3.0983e-01
Fitted a model with MAP estimate = -779.9657
expansions: [(156, 1)]
discards: [119]
Fitting a model of length 348 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 75s - loss: 787.1280 - loglik: -7.8596e+02 - logprior: -1.1704e+00
Epoch 2/10
39/39 - 62s - loss: 782.3154 - loglik: -7.8222e+02 - logprior: -1.0007e-01
Epoch 3/10
39/39 - 54s - loss: 778.7089 - loglik: -7.7874e+02 - logprior: 0.0275
Epoch 4/10
39/39 - 54s - loss: 775.4904 - loglik: -7.7558e+02 - logprior: 0.0884
Epoch 5/10
39/39 - 52s - loss: 771.8329 - loglik: -7.7194e+02 - logprior: 0.1106
Epoch 6/10
39/39 - 51s - loss: 771.2996 - loglik: -7.7150e+02 - logprior: 0.1972
Epoch 7/10
39/39 - 50s - loss: 770.6055 - loglik: -7.7088e+02 - logprior: 0.2712
Epoch 8/10
39/39 - 49s - loss: 770.0901 - loglik: -7.7042e+02 - logprior: 0.3342
Epoch 9/10
39/39 - 48s - loss: 769.1874 - loglik: -7.6961e+02 - logprior: 0.4233
Epoch 10/10
39/39 - 47s - loss: 768.8749 - loglik: -7.6935e+02 - logprior: 0.4730
Fitted a model with MAP estimate = -767.9150
Time for alignment: 1489.4453
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 920.0765 - loglik: -9.1864e+02 - logprior: -1.4398e+00
Epoch 2/10
39/39 - 34s - loss: 826.6989 - loglik: -8.2528e+02 - logprior: -1.4192e+00
Epoch 3/10
39/39 - 34s - loss: 814.2133 - loglik: -8.1274e+02 - logprior: -1.4774e+00
Epoch 4/10
39/39 - 35s - loss: 810.2252 - loglik: -8.0876e+02 - logprior: -1.4701e+00
Epoch 5/10
39/39 - 34s - loss: 807.4047 - loglik: -8.0589e+02 - logprior: -1.5184e+00
Epoch 6/10
39/39 - 35s - loss: 806.3646 - loglik: -8.0485e+02 - logprior: -1.5099e+00
Epoch 7/10
39/39 - 35s - loss: 806.5556 - loglik: -8.0498e+02 - logprior: -1.5741e+00
Fitted a model with MAP estimate = -798.1618
expansions: [(0, 2), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (25, 1), (30, 1), (36, 1), (39, 1), (41, 1), (42, 1), (44, 2), (45, 1), (55, 1), (58, 1), (61, 1), (64, 1), (79, 1), (80, 1), (81, 2), (86, 1), (87, 1), (95, 1), (97, 1), (102, 1), (103, 2), (119, 1), (124, 2), (134, 1), (145, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 2), (182, 1), (184, 1), (186, 1), (187, 1), (188, 1), (189, 2), (198, 1), (206, 4), (207, 1), (208, 1), (209, 1), (213, 1), (220, 1), (222, 1), (228, 2), (241, 2), (243, 1), (246, 1), (251, 1), (258, 1), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Fitting a model of length 358 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 808.8720 - loglik: -8.0683e+02 - logprior: -2.0390e+00
Epoch 2/2
39/39 - 48s - loss: 790.4078 - loglik: -7.8982e+02 - logprior: -5.9252e-01
Fitted a model with MAP estimate = -779.9821
expansions: [(160, 1)]
discards: [  0  32 105 121 134 194 237 261 320 345]
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 795.9052 - loglik: -7.9366e+02 - logprior: -2.2439e+00
Epoch 2/2
39/39 - 59s - loss: 789.8604 - loglik: -7.8947e+02 - logprior: -3.8544e-01
Fitted a model with MAP estimate = -779.4552
expansions: [(0, 2), (255, 1)]
discards: [0]
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 785.0316 - loglik: -7.8396e+02 - logprior: -1.0726e+00
Epoch 2/10
39/39 - 60s - loss: 781.2220 - loglik: -7.8117e+02 - logprior: -5.4460e-02
Epoch 3/10
39/39 - 59s - loss: 778.1669 - loglik: -7.7821e+02 - logprior: 0.0433
Epoch 4/10
39/39 - 54s - loss: 773.8146 - loglik: -7.7395e+02 - logprior: 0.1311
Epoch 5/10
39/39 - 52s - loss: 771.6622 - loglik: -7.7183e+02 - logprior: 0.1712
Epoch 6/10
39/39 - 50s - loss: 770.3961 - loglik: -7.7061e+02 - logprior: 0.2159
Epoch 7/10
39/39 - 45s - loss: 769.7198 - loglik: -7.7000e+02 - logprior: 0.2776
Epoch 8/10
39/39 - 43s - loss: 769.0877 - loglik: -7.6949e+02 - logprior: 0.3992
Epoch 9/10
39/39 - 43s - loss: 768.0669 - loglik: -7.6848e+02 - logprior: 0.4139
Epoch 10/10
39/39 - 43s - loss: 768.1981 - loglik: -7.6877e+02 - logprior: 0.5689
Fitted a model with MAP estimate = -767.0002
Time for alignment: 1227.6194
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 918.7438 - loglik: -9.1730e+02 - logprior: -1.4404e+00
Epoch 2/10
39/39 - 30s - loss: 825.8325 - loglik: -8.2452e+02 - logprior: -1.3114e+00
Epoch 3/10
39/39 - 31s - loss: 814.6921 - loglik: -8.1334e+02 - logprior: -1.3473e+00
Epoch 4/10
39/39 - 32s - loss: 809.8124 - loglik: -8.0844e+02 - logprior: -1.3745e+00
Epoch 5/10
39/39 - 34s - loss: 806.9525 - loglik: -8.0554e+02 - logprior: -1.4138e+00
Epoch 6/10
39/39 - 35s - loss: 805.8163 - loglik: -8.0442e+02 - logprior: -1.3974e+00
Epoch 7/10
39/39 - 36s - loss: 805.8421 - loglik: -8.0439e+02 - logprior: -1.4563e+00
Fitted a model with MAP estimate = -797.6910
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 3), (24, 2), (25, 1), (30, 1), (40, 1), (42, 1), (43, 2), (44, 2), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 2), (101, 1), (103, 2), (121, 1), (134, 1), (145, 1), (148, 1), (150, 1), (153, 2), (156, 2), (157, 2), (159, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 2), (211, 1), (220, 1), (222, 1), (226, 2), (227, 1), (240, 2), (242, 1), (251, 1), (258, 1), (261, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Fitting a model of length 356 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 810.3884 - loglik: -8.0841e+02 - logprior: -1.9744e+00
Epoch 2/2
39/39 - 60s - loss: 793.3268 - loglik: -7.9280e+02 - logprior: -5.2570e-01
Fitted a model with MAP estimate = -782.9390
expansions: [(307, 1)]
discards: [  0  30  33  34 106 134 192 199 259 266 289 319 343]
Fitting a model of length 344 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 799.7122 - loglik: -7.9737e+02 - logprior: -2.3454e+00
Epoch 2/2
39/39 - 56s - loss: 793.3829 - loglik: -7.9300e+02 - logprior: -3.7974e-01
Fitted a model with MAP estimate = -782.9835
expansions: [(0, 2), (152, 2)]
discards: [0]
Fitting a model of length 347 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 787.1953 - loglik: -7.8609e+02 - logprior: -1.1057e+00
Epoch 2/10
39/39 - 43s - loss: 782.9975 - loglik: -7.8290e+02 - logprior: -9.3110e-02
Epoch 3/10
39/39 - 43s - loss: 779.3210 - loglik: -7.7936e+02 - logprior: 0.0363
Epoch 4/10
39/39 - 44s - loss: 775.8026 - loglik: -7.7586e+02 - logprior: 0.0555
Epoch 5/10
39/39 - 48s - loss: 773.0417 - loglik: -7.7317e+02 - logprior: 0.1249
Epoch 6/10
39/39 - 51s - loss: 771.5677 - loglik: -7.7178e+02 - logprior: 0.2171
Epoch 7/10
39/39 - 52s - loss: 771.8475 - loglik: -7.7210e+02 - logprior: 0.2536
Fitted a model with MAP estimate = -770.0207
Time for alignment: 1066.8311
Computed alignments with likelihoods: ['-767.9150', '-767.0002', '-770.0207']
Best model has likelihood: -767.0002
SP score = 0.4456
Training of 3 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdad5a5c040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb00057f10>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 891.8201 - loglik: -8.9034e+02 - logprior: -1.4827e+00
Epoch 2/10
39/39 - 63s - loss: 755.1893 - loglik: -7.5378e+02 - logprior: -1.4085e+00
Epoch 3/10
39/39 - 61s - loss: 744.5051 - loglik: -7.4305e+02 - logprior: -1.4517e+00
Epoch 4/10
39/39 - 61s - loss: 742.1908 - loglik: -7.4072e+02 - logprior: -1.4752e+00
Epoch 5/10
39/39 - 66s - loss: 740.9586 - loglik: -7.3946e+02 - logprior: -1.4944e+00
Epoch 6/10
39/39 - 66s - loss: 740.6458 - loglik: -7.3914e+02 - logprior: -1.5041e+00
Epoch 7/10
39/39 - 63s - loss: 740.2358 - loglik: -7.3872e+02 - logprior: -1.5144e+00
Epoch 8/10
39/39 - 62s - loss: 739.3974 - loglik: -7.3784e+02 - logprior: -1.5620e+00
Epoch 9/10
39/39 - 66s - loss: 739.9301 - loglik: -7.3835e+02 - logprior: -1.5758e+00
Fitted a model with MAP estimate = -738.6437
expansions: [(20, 2), (21, 1), (64, 2), (98, 1), (101, 2), (104, 1), (106, 1), (123, 1), (142, 1), (144, 1), (145, 1), (146, 3), (147, 2), (148, 1), (163, 2), (164, 2), (165, 2), (169, 1), (178, 3), (179, 4), (182, 1), (183, 5), (185, 1), (187, 1), (189, 1), (190, 2), (191, 1), (192, 1), (205, 2), (213, 3), (221, 1), (222, 2), (223, 1), (228, 1), (230, 1), (235, 1), (236, 2), (243, 1), (244, 2), (245, 2), (248, 1), (249, 1), (253, 1), (254, 2), (257, 1), (267, 1), (277, 1), (278, 1), (279, 1), (282, 3), (283, 1), (289, 1), (290, 2), (292, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 194 195 196 197 198 209 210 211]
Fitting a model of length 407 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 95s - loss: 730.8527 - loglik: -7.2916e+02 - logprior: -1.6892e+00
Epoch 2/2
39/39 - 92s - loss: 716.4221 - loglik: -7.1595e+02 - logprior: -4.6791e-01
Fitted a model with MAP estimate = -713.6316
expansions: [(249, 4), (298, 1), (355, 1)]
discards: [ 67  68 161 163 207 208 219 220 254 266 286 299 300 350 351 352 362]
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 722.8291 - loglik: -7.2150e+02 - logprior: -1.3340e+00
Epoch 2/2
39/39 - 58s - loss: 717.1938 - loglik: -7.1714e+02 - logprior: -5.7107e-02
Fitted a model with MAP estimate = -714.4949
expansions: [(67, 1), (69, 1)]
discards: [227 242 243 244 245 246 247 293]
Fitting a model of length 390 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 721.6973 - loglik: -7.2067e+02 - logprior: -1.0275e+00
Epoch 2/10
39/39 - 54s - loss: 717.1774 - loglik: -7.1736e+02 - logprior: 0.1864
Epoch 3/10
39/39 - 54s - loss: 715.4572 - loglik: -7.1581e+02 - logprior: 0.3576
Epoch 4/10
39/39 - 54s - loss: 713.5762 - loglik: -7.1402e+02 - logprior: 0.4429
Epoch 5/10
39/39 - 54s - loss: 712.6998 - loglik: -7.1330e+02 - logprior: 0.6052
Epoch 6/10
39/39 - 54s - loss: 712.9360 - loglik: -7.1369e+02 - logprior: 0.7555
Fitted a model with MAP estimate = -712.1270
Time for alignment: 1524.5647
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 894.6578 - loglik: -8.9318e+02 - logprior: -1.4823e+00
Epoch 2/10
39/39 - 40s - loss: 757.6038 - loglik: -7.5606e+02 - logprior: -1.5390e+00
Epoch 3/10
39/39 - 40s - loss: 745.0889 - loglik: -7.4332e+02 - logprior: -1.7694e+00
Epoch 4/10
39/39 - 40s - loss: 742.6940 - loglik: -7.4095e+02 - logprior: -1.7480e+00
Epoch 5/10
39/39 - 40s - loss: 741.7722 - loglik: -7.3993e+02 - logprior: -1.8457e+00
Epoch 6/10
39/39 - 40s - loss: 741.0060 - loglik: -7.3921e+02 - logprior: -1.7967e+00
Epoch 7/10
39/39 - 40s - loss: 741.0710 - loglik: -7.3923e+02 - logprior: -1.8398e+00
Fitted a model with MAP estimate = -740.0732
expansions: [(9, 1), (19, 1), (21, 1), (31, 1), (66, 1), (77, 1), (100, 1), (103, 2), (104, 1), (107, 1), (113, 1), (118, 1), (119, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 3), (141, 2), (157, 2), (158, 1), (159, 1), (160, 1), (165, 1), (174, 1), (175, 5), (180, 1), (181, 1), (186, 1), (187, 1), (188, 1), (189, 4), (190, 2), (212, 1), (220, 2), (221, 1), (222, 1), (227, 1), (235, 3), (241, 1), (243, 2), (244, 2), (247, 1), (253, 1), (254, 2), (257, 1), (269, 1), (277, 1), (278, 1), (279, 1), (282, 1), (283, 4), (289, 1), (298, 1), (315, 1), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1)]
discards: [  1 198 199 200 201 202 203 204 205 206 207 208 209]
Fitting a model of length 395 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 729.0831 - loglik: -7.2751e+02 - logprior: -1.5726e+00
Epoch 2/2
39/39 - 54s - loss: 715.0691 - loglik: -7.1455e+02 - logprior: -5.1414e-01
Fitted a model with MAP estimate = -712.5255
expansions: [(110, 1), (247, 2), (287, 1)]
discards: [159 205 206 230 231 232 254 274 288 289 340 341]
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 56s - loss: 720.5659 - loglik: -7.1932e+02 - logprior: -1.2483e+00
Epoch 2/2
39/39 - 53s - loss: 716.1732 - loglik: -7.1615e+02 - logprior: -2.7021e-02
Fitted a model with MAP estimate = -714.0267
expansions: [(164, 1)]
discards: [284]
Fitting a model of length 387 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 718.7140 - loglik: -7.1764e+02 - logprior: -1.0727e+00
Epoch 2/10
39/39 - 53s - loss: 715.5790 - loglik: -7.1583e+02 - logprior: 0.2509
Epoch 3/10
39/39 - 52s - loss: 714.3306 - loglik: -7.1473e+02 - logprior: 0.4025
Epoch 4/10
39/39 - 53s - loss: 712.7499 - loglik: -7.1317e+02 - logprior: 0.4217
Epoch 5/10
39/39 - 52s - loss: 711.7350 - loglik: -7.1239e+02 - logprior: 0.6536
Epoch 6/10
39/39 - 53s - loss: 711.4548 - loglik: -7.1221e+02 - logprior: 0.7516
Epoch 7/10
39/39 - 53s - loss: 712.4357 - loglik: -7.1334e+02 - logprior: 0.9023
Fitted a model with MAP estimate = -711.0789
Time for alignment: 1106.5469
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 891.9358 - loglik: -8.9042e+02 - logprior: -1.5135e+00
Epoch 2/10
39/39 - 40s - loss: 756.6100 - loglik: -7.5517e+02 - logprior: -1.4428e+00
Epoch 3/10
39/39 - 40s - loss: 743.6444 - loglik: -7.4207e+02 - logprior: -1.5777e+00
Epoch 4/10
39/39 - 40s - loss: 741.4216 - loglik: -7.3983e+02 - logprior: -1.5901e+00
Epoch 5/10
39/39 - 40s - loss: 740.4064 - loglik: -7.3878e+02 - logprior: -1.6218e+00
Epoch 6/10
39/39 - 40s - loss: 739.7185 - loglik: -7.3811e+02 - logprior: -1.6096e+00
Epoch 7/10
39/39 - 40s - loss: 739.8058 - loglik: -7.3812e+02 - logprior: -1.6855e+00
Fitted a model with MAP estimate = -738.3430
expansions: [(9, 1), (19, 1), (21, 1), (31, 1), (66, 1), (68, 1), (100, 1), (103, 2), (104, 1), (107, 1), (113, 1), (118, 1), (119, 1), (137, 2), (138, 1), (139, 1), (140, 3), (141, 2), (142, 1), (157, 2), (158, 1), (159, 1), (160, 1), (165, 1), (174, 1), (175, 4), (178, 4), (179, 1), (181, 1), (183, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (202, 2), (209, 2), (217, 1), (218, 1), (219, 1), (227, 1), (232, 1), (233, 2), (236, 1), (239, 1), (241, 2), (242, 2), (244, 1), (248, 1), (249, 1), (266, 1), (276, 1), (278, 3), (281, 3), (282, 3), (291, 1), (296, 1), (313, 1), (316, 1), (319, 1), (321, 1)]
discards: [  1 191 192 193 194 195]
Fitting a model of length 404 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 729.1609 - loglik: -7.2753e+02 - logprior: -1.6317e+00
Epoch 2/2
39/39 - 56s - loss: 716.7430 - loglik: -7.1641e+02 - logprior: -3.3737e-01
Fitted a model with MAP estimate = -714.1465
expansions: [(110, 1), (232, 4)]
discards: [159 161 207 216 243 253 283 298 299 342 343 349]
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 722.0322 - loglik: -7.2076e+02 - logprior: -1.2685e+00
Epoch 2/2
39/39 - 55s - loss: 717.1497 - loglik: -7.1708e+02 - logprior: -7.0670e-02
Fitted a model with MAP estimate = -714.6433
expansions: [(231, 1), (232, 1)]
discards: [214 215 233 234 235 236 237]
Fitting a model of length 392 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 722.0865 - loglik: -7.2107e+02 - logprior: -1.0159e+00
Epoch 2/10
39/39 - 54s - loss: 716.9450 - loglik: -7.1717e+02 - logprior: 0.2243
Epoch 3/10
39/39 - 54s - loss: 714.1394 - loglik: -7.1454e+02 - logprior: 0.4000
Epoch 4/10
39/39 - 54s - loss: 713.2523 - loglik: -7.1376e+02 - logprior: 0.5121
Epoch 5/10
39/39 - 54s - loss: 712.8389 - loglik: -7.1345e+02 - logprior: 0.6065
Epoch 6/10
39/39 - 54s - loss: 712.1974 - loglik: -7.1297e+02 - logprior: 0.7707
Epoch 7/10
39/39 - 54s - loss: 712.3030 - loglik: -7.1309e+02 - logprior: 0.7869
Fitted a model with MAP estimate = -711.6975
Time for alignment: 1127.1556
Computed alignments with likelihoods: ['-712.1270', '-711.0789', '-711.6975']
Best model has likelihood: -711.0789
SP score = 0.8106
Training of 3 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdad5f42bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb000f7910>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 252.4137 - loglik: -2.4934e+02 - logprior: -3.0708e+00
Epoch 2/10
19/19 - 2s - loss: 221.8103 - loglik: -2.2059e+02 - logprior: -1.2236e+00
Epoch 3/10
19/19 - 1s - loss: 209.2168 - loglik: -2.0785e+02 - logprior: -1.3646e+00
Epoch 4/10
19/19 - 2s - loss: 207.0765 - loglik: -2.0582e+02 - logprior: -1.2561e+00
Epoch 5/10
19/19 - 2s - loss: 206.5198 - loglik: -2.0527e+02 - logprior: -1.2525e+00
Epoch 6/10
19/19 - 2s - loss: 206.0008 - loglik: -2.0477e+02 - logprior: -1.2315e+00
Epoch 7/10
19/19 - 2s - loss: 206.0323 - loglik: -2.0481e+02 - logprior: -1.2216e+00
Fitted a model with MAP estimate = -205.8360
expansions: [(8, 4), (9, 3), (19, 1), (26, 1), (40, 2), (41, 3), (43, 2), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.5648 - loglik: -2.0366e+02 - logprior: -3.9090e+00
Epoch 2/2
19/19 - 2s - loss: 200.3757 - loglik: -1.9831e+02 - logprior: -2.0658e+00
Fitted a model with MAP estimate = -198.8499
expansions: [(0, 2)]
discards: [ 0 13 49 52 67]
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 200.6137 - loglik: -1.9775e+02 - logprior: -2.8605e+00
Epoch 2/2
19/19 - 2s - loss: 197.4720 - loglik: -1.9640e+02 - logprior: -1.0723e+00
Fitted a model with MAP estimate = -196.7638
expansions: [(54, 1)]
discards: [0]
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 201.8436 - loglik: -1.9815e+02 - logprior: -3.6939e+00
Epoch 2/10
19/19 - 2s - loss: 197.5696 - loglik: -1.9633e+02 - logprior: -1.2350e+00
Epoch 3/10
19/19 - 2s - loss: 196.7577 - loglik: -1.9577e+02 - logprior: -9.9114e-01
Epoch 4/10
19/19 - 2s - loss: 196.2260 - loglik: -1.9528e+02 - logprior: -9.4722e-01
Epoch 5/10
19/19 - 2s - loss: 196.0605 - loglik: -1.9513e+02 - logprior: -9.3183e-01
Epoch 6/10
19/19 - 2s - loss: 195.6700 - loglik: -1.9475e+02 - logprior: -9.1822e-01
Epoch 7/10
19/19 - 2s - loss: 195.6070 - loglik: -1.9471e+02 - logprior: -9.0166e-01
Epoch 8/10
19/19 - 2s - loss: 195.2470 - loglik: -1.9435e+02 - logprior: -8.9873e-01
Epoch 9/10
19/19 - 2s - loss: 195.2375 - loglik: -1.9436e+02 - logprior: -8.7395e-01
Epoch 10/10
19/19 - 2s - loss: 195.3067 - loglik: -1.9444e+02 - logprior: -8.6459e-01
Fitted a model with MAP estimate = -195.1827
Time for alignment: 59.5146
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.3279 - loglik: -2.4926e+02 - logprior: -3.0683e+00
Epoch 2/10
19/19 - 2s - loss: 222.0194 - loglik: -2.2080e+02 - logprior: -1.2178e+00
Epoch 3/10
19/19 - 2s - loss: 209.7882 - loglik: -2.0841e+02 - logprior: -1.3752e+00
Epoch 4/10
19/19 - 2s - loss: 205.2093 - loglik: -2.0390e+02 - logprior: -1.3050e+00
Epoch 5/10
19/19 - 1s - loss: 204.3278 - loglik: -2.0299e+02 - logprior: -1.3399e+00
Epoch 6/10
19/19 - 2s - loss: 203.9934 - loglik: -2.0268e+02 - logprior: -1.3172e+00
Epoch 7/10
19/19 - 2s - loss: 203.7359 - loglik: -2.0243e+02 - logprior: -1.3059e+00
Epoch 8/10
19/19 - 1s - loss: 203.9618 - loglik: -2.0266e+02 - logprior: -1.2994e+00
Fitted a model with MAP estimate = -203.5728
expansions: [(8, 4), (9, 2), (12, 1), (23, 1), (24, 1), (34, 1), (39, 1), (41, 2), (43, 2), (48, 1), (49, 2), (58, 1), (59, 1), (60, 2)]
discards: [0]
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 207.1085 - loglik: -2.0321e+02 - logprior: -3.8956e+00
Epoch 2/2
19/19 - 2s - loss: 200.2812 - loglik: -1.9824e+02 - logprior: -2.0451e+00
Fitted a model with MAP estimate = -198.8784
expansions: [(0, 2)]
discards: [ 0 12 65]
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.5058 - loglik: -1.9764e+02 - logprior: -2.8626e+00
Epoch 2/2
19/19 - 2s - loss: 197.3660 - loglik: -1.9629e+02 - logprior: -1.0743e+00
Fitted a model with MAP estimate = -196.7332
expansions: []
discards: [ 0 51]
Fitting a model of length 89 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 201.9103 - loglik: -1.9822e+02 - logprior: -3.6909e+00
Epoch 2/10
19/19 - 2s - loss: 197.7755 - loglik: -1.9656e+02 - logprior: -1.2183e+00
Epoch 3/10
19/19 - 2s - loss: 197.0053 - loglik: -1.9601e+02 - logprior: -9.9537e-01
Epoch 4/10
19/19 - 2s - loss: 196.4407 - loglik: -1.9550e+02 - logprior: -9.4281e-01
Epoch 5/10
19/19 - 2s - loss: 196.0923 - loglik: -1.9516e+02 - logprior: -9.3371e-01
Epoch 6/10
19/19 - 2s - loss: 195.7166 - loglik: -1.9480e+02 - logprior: -9.1437e-01
Epoch 7/10
19/19 - 2s - loss: 196.0041 - loglik: -1.9510e+02 - logprior: -9.0746e-01
Fitted a model with MAP estimate = -195.5696
Time for alignment: 55.2353
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.2482 - loglik: -2.4918e+02 - logprior: -3.0710e+00
Epoch 2/10
19/19 - 2s - loss: 221.2344 - loglik: -2.2000e+02 - logprior: -1.2386e+00
Epoch 3/10
19/19 - 2s - loss: 209.2047 - loglik: -2.0782e+02 - logprior: -1.3887e+00
Epoch 4/10
19/19 - 2s - loss: 206.6482 - loglik: -2.0536e+02 - logprior: -1.2854e+00
Epoch 5/10
19/19 - 2s - loss: 205.9418 - loglik: -2.0464e+02 - logprior: -1.2985e+00
Epoch 6/10
19/19 - 2s - loss: 205.5378 - loglik: -2.0427e+02 - logprior: -1.2717e+00
Epoch 7/10
19/19 - 2s - loss: 205.4521 - loglik: -2.0419e+02 - logprior: -1.2657e+00
Epoch 8/10
19/19 - 2s - loss: 205.5789 - loglik: -2.0432e+02 - logprior: -1.2613e+00
Fitted a model with MAP estimate = -205.2547
expansions: [(8, 1), (9, 2), (10, 3), (12, 1), (19, 1), (20, 1), (37, 2), (39, 1), (41, 2), (43, 2), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 207.4609 - loglik: -2.0356e+02 - logprior: -3.9044e+00
Epoch 2/2
19/19 - 2s - loss: 200.3800 - loglik: -1.9830e+02 - logprior: -2.0754e+00
Fitted a model with MAP estimate = -198.9460
expansions: [(0, 2)]
discards: [ 0 13 45 67]
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.4630 - loglik: -1.9759e+02 - logprior: -2.8691e+00
Epoch 2/2
19/19 - 2s - loss: 197.3846 - loglik: -1.9632e+02 - logprior: -1.0696e+00
Fitted a model with MAP estimate = -196.7200
expansions: []
discards: [ 0 51]
Fitting a model of length 89 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 201.8951 - loglik: -1.9821e+02 - logprior: -3.6847e+00
Epoch 2/10
19/19 - 2s - loss: 197.7312 - loglik: -1.9650e+02 - logprior: -1.2282e+00
Epoch 3/10
19/19 - 2s - loss: 197.0342 - loglik: -1.9604e+02 - logprior: -9.9264e-01
Epoch 4/10
19/19 - 2s - loss: 196.4553 - loglik: -1.9550e+02 - logprior: -9.5341e-01
Epoch 5/10
19/19 - 2s - loss: 196.2196 - loglik: -1.9529e+02 - logprior: -9.3132e-01
Epoch 6/10
19/19 - 2s - loss: 195.7494 - loglik: -1.9483e+02 - logprior: -9.2041e-01
Epoch 7/10
19/19 - 2s - loss: 195.6479 - loglik: -1.9474e+02 - logprior: -9.0520e-01
Epoch 8/10
19/19 - 2s - loss: 195.7843 - loglik: -1.9489e+02 - logprior: -8.9235e-01
Fitted a model with MAP estimate = -195.5134
Time for alignment: 57.6897
Computed alignments with likelihoods: ['-195.1827', '-195.5696', '-195.5134']
Best model has likelihood: -195.1827
SP score = 0.8383
Training of 3 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb2abfa700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdab382d520>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.5830 - loglik: -2.9854e+02 - logprior: -3.0384e+00
Epoch 2/10
19/19 - 2s - loss: 272.3362 - loglik: -2.7110e+02 - logprior: -1.2382e+00
Epoch 3/10
19/19 - 2s - loss: 258.7120 - loglik: -2.5747e+02 - logprior: -1.2406e+00
Epoch 4/10
19/19 - 2s - loss: 256.3000 - loglik: -2.5518e+02 - logprior: -1.1234e+00
Epoch 5/10
19/19 - 2s - loss: 255.5346 - loglik: -2.5444e+02 - logprior: -1.0930e+00
Epoch 6/10
19/19 - 2s - loss: 254.6736 - loglik: -2.5360e+02 - logprior: -1.0716e+00
Epoch 7/10
19/19 - 2s - loss: 254.6465 - loglik: -2.5357e+02 - logprior: -1.0771e+00
Epoch 8/10
19/19 - 2s - loss: 254.3822 - loglik: -2.5332e+02 - logprior: -1.0620e+00
Epoch 9/10
19/19 - 2s - loss: 254.1293 - loglik: -2.5306e+02 - logprior: -1.0732e+00
Epoch 10/10
19/19 - 2s - loss: 254.2229 - loglik: -2.5316e+02 - logprior: -1.0591e+00
Fitted a model with MAP estimate = -253.5093
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (32, 1), (38, 2), (41, 2), (42, 1), (52, 1), (53, 2), (54, 1), (74, 1), (75, 2), (76, 2), (77, 4), (78, 2)]
discards: []
Fitting a model of length 114 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 259.7770 - loglik: -2.5565e+02 - logprior: -4.1317e+00
Epoch 2/2
19/19 - 2s - loss: 253.3416 - loglik: -2.5211e+02 - logprior: -1.2295e+00
Fitted a model with MAP estimate = -251.5521
expansions: []
discards: [ 1 70]
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 254.8545 - loglik: -2.5205e+02 - logprior: -2.8045e+00
Epoch 2/2
19/19 - 2s - loss: 252.3668 - loglik: -2.5129e+02 - logprior: -1.0810e+00
Fitted a model with MAP estimate = -251.1261
expansions: []
discards: []
Fitting a model of length 112 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 254.0757 - loglik: -2.5133e+02 - logprior: -2.7484e+00
Epoch 2/10
19/19 - 2s - loss: 251.6220 - loglik: -2.5055e+02 - logprior: -1.0760e+00
Epoch 3/10
19/19 - 2s - loss: 250.9442 - loglik: -2.5000e+02 - logprior: -9.4283e-01
Epoch 4/10
19/19 - 2s - loss: 250.4168 - loglik: -2.4951e+02 - logprior: -9.0326e-01
Epoch 5/10
19/19 - 2s - loss: 249.5724 - loglik: -2.4868e+02 - logprior: -8.9275e-01
Epoch 6/10
19/19 - 2s - loss: 249.0703 - loglik: -2.4820e+02 - logprior: -8.7239e-01
Epoch 7/10
19/19 - 2s - loss: 248.4739 - loglik: -2.4761e+02 - logprior: -8.6302e-01
Epoch 8/10
19/19 - 2s - loss: 248.5763 - loglik: -2.4773e+02 - logprior: -8.4208e-01
Fitted a model with MAP estimate = -248.3459
Time for alignment: 74.0917
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 301.8546 - loglik: -2.9883e+02 - logprior: -3.0285e+00
Epoch 2/10
19/19 - 2s - loss: 271.6205 - loglik: -2.7040e+02 - logprior: -1.2181e+00
Epoch 3/10
19/19 - 2s - loss: 259.6372 - loglik: -2.5845e+02 - logprior: -1.1855e+00
Epoch 4/10
19/19 - 2s - loss: 257.1667 - loglik: -2.5610e+02 - logprior: -1.0710e+00
Epoch 5/10
19/19 - 2s - loss: 256.1611 - loglik: -2.5511e+02 - logprior: -1.0559e+00
Epoch 6/10
19/19 - 2s - loss: 254.9225 - loglik: -2.5386e+02 - logprior: -1.0575e+00
Epoch 7/10
19/19 - 2s - loss: 255.3100 - loglik: -2.5424e+02 - logprior: -1.0684e+00
Fitted a model with MAP estimate = -254.1466
expansions: [(0, 2), (20, 4), (21, 2), (22, 1), (23, 1), (41, 3), (42, 2), (44, 1), (45, 1), (53, 1), (54, 1), (74, 1), (75, 3), (76, 2), (77, 3), (78, 1)]
discards: []
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 259.5251 - loglik: -2.5552e+02 - logprior: -4.0003e+00
Epoch 2/2
19/19 - 2s - loss: 253.7420 - loglik: -2.5252e+02 - logprior: -1.2217e+00
Fitted a model with MAP estimate = -251.9454
expansions: []
discards: [ 1 27]
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.3563 - loglik: -2.5250e+02 - logprior: -2.8588e+00
Epoch 2/2
19/19 - 2s - loss: 252.6896 - loglik: -2.5160e+02 - logprior: -1.0945e+00
Fitted a model with MAP estimate = -251.2421
expansions: []
discards: [58 98]
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 254.2338 - loglik: -2.5143e+02 - logprior: -2.8024e+00
Epoch 2/10
19/19 - 2s - loss: 251.6463 - loglik: -2.5065e+02 - logprior: -1.0006e+00
Epoch 3/10
19/19 - 2s - loss: 251.0537 - loglik: -2.5018e+02 - logprior: -8.7699e-01
Epoch 4/10
19/19 - 2s - loss: 250.3290 - loglik: -2.4947e+02 - logprior: -8.5419e-01
Epoch 5/10
19/19 - 2s - loss: 249.7156 - loglik: -2.4889e+02 - logprior: -8.2214e-01
Epoch 6/10
19/19 - 2s - loss: 248.9707 - loglik: -2.4817e+02 - logprior: -7.9794e-01
Epoch 7/10
19/19 - 2s - loss: 248.5207 - loglik: -2.4774e+02 - logprior: -7.7700e-01
Epoch 8/10
19/19 - 2s - loss: 248.5851 - loglik: -2.4783e+02 - logprior: -7.5856e-01
Fitted a model with MAP estimate = -248.2563
Time for alignment: 67.3083
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 301.7196 - loglik: -2.9869e+02 - logprior: -3.0343e+00
Epoch 2/10
19/19 - 2s - loss: 270.4443 - loglik: -2.6921e+02 - logprior: -1.2310e+00
Epoch 3/10
19/19 - 2s - loss: 259.0451 - loglik: -2.5784e+02 - logprior: -1.2083e+00
Epoch 4/10
19/19 - 2s - loss: 256.4436 - loglik: -2.5534e+02 - logprior: -1.1082e+00
Epoch 5/10
19/19 - 2s - loss: 255.4944 - loglik: -2.5440e+02 - logprior: -1.0898e+00
Epoch 6/10
19/19 - 2s - loss: 254.9348 - loglik: -2.5384e+02 - logprior: -1.0928e+00
Epoch 7/10
19/19 - 2s - loss: 254.8609 - loglik: -2.5377e+02 - logprior: -1.0911e+00
Epoch 8/10
19/19 - 2s - loss: 254.1085 - loglik: -2.5302e+02 - logprior: -1.0881e+00
Epoch 9/10
19/19 - 2s - loss: 254.2462 - loglik: -2.5316e+02 - logprior: -1.0851e+00
Fitted a model with MAP estimate = -253.6905
expansions: [(0, 2), (7, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (39, 2), (41, 3), (42, 1), (44, 1), (45, 1), (51, 1), (52, 1), (53, 1), (75, 3), (76, 2), (77, 2)]
discards: []
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 259.7630 - loglik: -2.5568e+02 - logprior: -4.0785e+00
Epoch 2/2
19/19 - 2s - loss: 253.8287 - loglik: -2.5260e+02 - logprior: -1.2315e+00
Fitted a model with MAP estimate = -252.0707
expansions: []
discards: [1]
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 255.3336 - loglik: -2.5250e+02 - logprior: -2.8313e+00
Epoch 2/2
19/19 - 2s - loss: 252.7271 - loglik: -2.5167e+02 - logprior: -1.0617e+00
Fitted a model with MAP estimate = -251.4368
expansions: [(52, 1)]
discards: [59]
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 254.2623 - loglik: -2.5152e+02 - logprior: -2.7441e+00
Epoch 2/10
19/19 - 2s - loss: 251.6187 - loglik: -2.5056e+02 - logprior: -1.0613e+00
Epoch 3/10
19/19 - 2s - loss: 251.4292 - loglik: -2.5051e+02 - logprior: -9.1652e-01
Epoch 4/10
19/19 - 2s - loss: 250.2234 - loglik: -2.4934e+02 - logprior: -8.8625e-01
Epoch 5/10
19/19 - 2s - loss: 249.7494 - loglik: -2.4887e+02 - logprior: -8.7730e-01
Epoch 6/10
19/19 - 2s - loss: 249.3066 - loglik: -2.4844e+02 - logprior: -8.6176e-01
Epoch 7/10
19/19 - 2s - loss: 249.1030 - loglik: -2.4826e+02 - logprior: -8.4189e-01
Epoch 8/10
19/19 - 2s - loss: 248.4512 - loglik: -2.4762e+02 - logprior: -8.3134e-01
Epoch 9/10
19/19 - 2s - loss: 248.5887 - loglik: -2.4778e+02 - logprior: -8.1338e-01
Fitted a model with MAP estimate = -248.3800
Time for alignment: 72.8668
Computed alignments with likelihoods: ['-248.3459', '-248.2563', '-248.3800']
Best model has likelihood: -248.2563
SP score = 0.8839
Training of 3 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdad5f38ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb08bea580>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.3043 - loglik: -1.8311e+02 - logprior: -3.1959e+00
Epoch 2/10
19/19 - 1s - loss: 154.5858 - loglik: -1.5315e+02 - logprior: -1.4382e+00
Epoch 3/10
19/19 - 1s - loss: 143.3814 - loglik: -1.4177e+02 - logprior: -1.6126e+00
Epoch 4/10
19/19 - 1s - loss: 139.7369 - loglik: -1.3823e+02 - logprior: -1.5083e+00
Epoch 5/10
19/19 - 1s - loss: 138.6769 - loglik: -1.3713e+02 - logprior: -1.5476e+00
Epoch 6/10
19/19 - 1s - loss: 137.9394 - loglik: -1.3640e+02 - logprior: -1.5362e+00
Epoch 7/10
19/19 - 1s - loss: 137.7713 - loglik: -1.3624e+02 - logprior: -1.5341e+00
Epoch 8/10
19/19 - 1s - loss: 137.5653 - loglik: -1.3604e+02 - logprior: -1.5253e+00
Epoch 9/10
19/19 - 1s - loss: 137.6032 - loglik: -1.3608e+02 - logprior: -1.5226e+00
Fitted a model with MAP estimate = -137.3396
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 1), (23, 3), (24, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 140.0435 - loglik: -1.3597e+02 - logprior: -4.0689e+00
Epoch 2/2
19/19 - 1s - loss: 131.1115 - loglik: -1.2907e+02 - logprior: -2.0406e+00
Fitted a model with MAP estimate = -129.4698
expansions: [(0, 2)]
discards: [ 0 29]
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 130.6422 - loglik: -1.2765e+02 - logprior: -2.9888e+00
Epoch 2/2
19/19 - 1s - loss: 127.3673 - loglik: -1.2621e+02 - logprior: -1.1537e+00
Fitted a model with MAP estimate = -126.7678
expansions: []
discards: [0]
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 132.8852 - loglik: -1.2909e+02 - logprior: -3.7908e+00
Epoch 2/10
19/19 - 1s - loss: 128.3404 - loglik: -1.2699e+02 - logprior: -1.3536e+00
Epoch 3/10
19/19 - 1s - loss: 126.9932 - loglik: -1.2586e+02 - logprior: -1.1349e+00
Epoch 4/10
19/19 - 1s - loss: 126.7265 - loglik: -1.2561e+02 - logprior: -1.1198e+00
Epoch 5/10
19/19 - 1s - loss: 125.9707 - loglik: -1.2487e+02 - logprior: -1.0998e+00
Epoch 6/10
19/19 - 1s - loss: 125.2495 - loglik: -1.2416e+02 - logprior: -1.0914e+00
Epoch 7/10
19/19 - 1s - loss: 125.0394 - loglik: -1.2396e+02 - logprior: -1.0833e+00
Epoch 8/10
19/19 - 1s - loss: 125.5686 - loglik: -1.2449e+02 - logprior: -1.0776e+00
Fitted a model with MAP estimate = -124.9544
Time for alignment: 45.5980
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.4041 - loglik: -1.8321e+02 - logprior: -3.1973e+00
Epoch 2/10
19/19 - 1s - loss: 153.8360 - loglik: -1.5218e+02 - logprior: -1.6601e+00
Epoch 3/10
19/19 - 1s - loss: 142.3435 - loglik: -1.4078e+02 - logprior: -1.5622e+00
Epoch 4/10
19/19 - 1s - loss: 139.5015 - loglik: -1.3791e+02 - logprior: -1.5898e+00
Epoch 5/10
19/19 - 1s - loss: 138.5356 - loglik: -1.3698e+02 - logprior: -1.5555e+00
Epoch 6/10
19/19 - 1s - loss: 137.9410 - loglik: -1.3639e+02 - logprior: -1.5518e+00
Epoch 7/10
19/19 - 1s - loss: 138.0348 - loglik: -1.3649e+02 - logprior: -1.5425e+00
Fitted a model with MAP estimate = -137.5731
expansions: [(16, 2), (17, 2), (18, 1), (21, 1), (22, 2), (23, 1), (24, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 137.0806 - loglik: -1.3376e+02 - logprior: -3.3166e+00
Epoch 2/2
19/19 - 1s - loss: 129.0434 - loglik: -1.2766e+02 - logprior: -1.3854e+00
Fitted a model with MAP estimate = -127.7814
expansions: []
discards: [21 28]
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 131.1234 - loglik: -1.2793e+02 - logprior: -3.1920e+00
Epoch 2/2
19/19 - 1s - loss: 128.2525 - loglik: -1.2694e+02 - logprior: -1.3118e+00
Fitted a model with MAP estimate = -127.5539
expansions: []
discards: []
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 130.8847 - loglik: -1.2773e+02 - logprior: -3.1585e+00
Epoch 2/10
19/19 - 1s - loss: 127.9222 - loglik: -1.2663e+02 - logprior: -1.2923e+00
Epoch 3/10
19/19 - 1s - loss: 127.4307 - loglik: -1.2628e+02 - logprior: -1.1557e+00
Epoch 4/10
19/19 - 1s - loss: 126.7125 - loglik: -1.2559e+02 - logprior: -1.1203e+00
Epoch 5/10
19/19 - 1s - loss: 125.9971 - loglik: -1.2489e+02 - logprior: -1.1052e+00
Epoch 6/10
19/19 - 1s - loss: 125.8760 - loglik: -1.2478e+02 - logprior: -1.0913e+00
Epoch 7/10
19/19 - 1s - loss: 125.5304 - loglik: -1.2445e+02 - logprior: -1.0841e+00
Epoch 8/10
19/19 - 1s - loss: 125.1017 - loglik: -1.2403e+02 - logprior: -1.0695e+00
Epoch 9/10
19/19 - 1s - loss: 125.4664 - loglik: -1.2440e+02 - logprior: -1.0616e+00
Fitted a model with MAP estimate = -125.1254
Time for alignment: 45.0812
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 186.5240 - loglik: -1.8333e+02 - logprior: -3.1971e+00
Epoch 2/10
19/19 - 1s - loss: 155.1736 - loglik: -1.5360e+02 - logprior: -1.5717e+00
Epoch 3/10
19/19 - 1s - loss: 143.1945 - loglik: -1.4157e+02 - logprior: -1.6217e+00
Epoch 4/10
19/19 - 1s - loss: 138.4012 - loglik: -1.3678e+02 - logprior: -1.6200e+00
Epoch 5/10
19/19 - 1s - loss: 136.3634 - loglik: -1.3475e+02 - logprior: -1.6159e+00
Epoch 6/10
19/19 - 1s - loss: 135.5239 - loglik: -1.3390e+02 - logprior: -1.6222e+00
Epoch 7/10
19/19 - 1s - loss: 135.1176 - loglik: -1.3350e+02 - logprior: -1.6201e+00
Epoch 8/10
19/19 - 1s - loss: 134.9023 - loglik: -1.3329e+02 - logprior: -1.6130e+00
Epoch 9/10
19/19 - 1s - loss: 135.1969 - loglik: -1.3359e+02 - logprior: -1.6113e+00
Fitted a model with MAP estimate = -134.8407
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (23, 1), (30, 1), (33, 1), (34, 1), (38, 1), (41, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 135.7377 - loglik: -1.3244e+02 - logprior: -3.2994e+00
Epoch 2/2
19/19 - 1s - loss: 128.6241 - loglik: -1.2729e+02 - logprior: -1.3347e+00
Fitted a model with MAP estimate = -127.7655
expansions: []
discards: []
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.8683 - loglik: -1.2769e+02 - logprior: -3.1794e+00
Epoch 2/10
19/19 - 1s - loss: 128.1773 - loglik: -1.2687e+02 - logprior: -1.3030e+00
Epoch 3/10
19/19 - 1s - loss: 127.5635 - loglik: -1.2639e+02 - logprior: -1.1731e+00
Epoch 4/10
19/19 - 1s - loss: 126.6014 - loglik: -1.2547e+02 - logprior: -1.1343e+00
Epoch 5/10
19/19 - 1s - loss: 126.3336 - loglik: -1.2521e+02 - logprior: -1.1249e+00
Epoch 6/10
19/19 - 1s - loss: 125.6197 - loglik: -1.2451e+02 - logprior: -1.1128e+00
Epoch 7/10
19/19 - 1s - loss: 125.6888 - loglik: -1.2458e+02 - logprior: -1.1105e+00
Fitted a model with MAP estimate = -125.2938
Time for alignment: 36.4693
Computed alignments with likelihoods: ['-124.9544', '-125.1254', '-125.2938']
Best model has likelihood: -124.9544
SP score = 0.9695
Training of 3 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fd8ec1be1f0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fd8ec1bec40>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1bee80>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec131ee0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131f10>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131490>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fd8ec1319a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1318b0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131fa0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131850>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131820>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a30>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a00>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1316d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131a90>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131760>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec131790>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1313a0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fd8ec131b80> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fd8ec131130>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fd8ec1312e0> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fd8ec069820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdae6bb0a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fdb08e70b20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 788.6680 - loglik: -7.8626e+02 - logprior: -2.4064e+00
Epoch 2/10
19/19 - 17s - loss: 678.2357 - loglik: -6.7730e+02 - logprior: -9.3510e-01
Epoch 3/10
19/19 - 17s - loss: 623.3693 - loglik: -6.2183e+02 - logprior: -1.5421e+00
Epoch 4/10
19/19 - 17s - loss: 609.9534 - loglik: -6.0830e+02 - logprior: -1.6552e+00
Epoch 5/10
19/19 - 17s - loss: 604.6932 - loglik: -6.0296e+02 - logprior: -1.7370e+00
Epoch 6/10
19/19 - 17s - loss: 602.0091 - loglik: -6.0025e+02 - logprior: -1.7545e+00
Epoch 7/10
19/19 - 16s - loss: 600.0387 - loglik: -5.9830e+02 - logprior: -1.7370e+00
Epoch 8/10
19/19 - 17s - loss: 598.5285 - loglik: -5.9677e+02 - logprior: -1.7535e+00
Epoch 9/10
19/19 - 17s - loss: 599.5204 - loglik: -5.9775e+02 - logprior: -1.7694e+00
Fitted a model with MAP estimate = -597.7809
expansions: [(10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (31, 1), (32, 1), (46, 1), (47, 1), (48, 1), (51, 1), (67, 1), (68, 1), (69, 1), (70, 1), (72, 1), (78, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (131, 1), (132, 1), (135, 1), (137, 1), (140, 3), (148, 1), (149, 1), (154, 1), (156, 4), (163, 2), (164, 3), (173, 2), (177, 1), (180, 1), (183, 1), (185, 1), (192, 2), (193, 1), (194, 1), (196, 1), (197, 1), (211, 1), (212, 3), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Fitting a model of length 313 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 589.7360 - loglik: -5.8720e+02 - logprior: -2.5314e+00
Epoch 2/2
39/39 - 27s - loss: 572.4617 - loglik: -5.7103e+02 - logprior: -1.4322e+00
Fitted a model with MAP estimate = -568.2929
expansions: [(0, 3), (148, 1), (191, 1), (198, 1), (207, 1)]
discards: [  0 174 222 247 277]
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 572.5504 - loglik: -5.7108e+02 - logprior: -1.4657e+00
Epoch 2/2
39/39 - 27s - loss: 567.2867 - loglik: -5.6688e+02 - logprior: -4.1140e-01
Fitted a model with MAP estimate = -564.4583
expansions: []
discards: [0 1]
Fitting a model of length 313 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 573.0933 - loglik: -5.7094e+02 - logprior: -2.1518e+00
Epoch 2/10
39/39 - 27s - loss: 566.0660 - loglik: -5.6578e+02 - logprior: -2.8131e-01
Epoch 3/10
39/39 - 27s - loss: 563.4201 - loglik: -5.6343e+02 - logprior: 0.0142
Epoch 4/10
39/39 - 27s - loss: 561.1118 - loglik: -5.6123e+02 - logprior: 0.1162
Epoch 5/10
39/39 - 27s - loss: 558.7447 - loglik: -5.5894e+02 - logprior: 0.2000
Epoch 6/10
39/39 - 27s - loss: 558.1831 - loglik: -5.5848e+02 - logprior: 0.3006
Epoch 7/10
39/39 - 27s - loss: 557.7487 - loglik: -5.5817e+02 - logprior: 0.4200
Epoch 8/10
39/39 - 27s - loss: 557.9287 - loglik: -5.5846e+02 - logprior: 0.5362
Fitted a model with MAP estimate = -557.2809
Time for alignment: 622.1645
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 788.3869 - loglik: -7.8599e+02 - logprior: -2.4012e+00
Epoch 2/10
19/19 - 16s - loss: 676.8384 - loglik: -6.7590e+02 - logprior: -9.3527e-01
Epoch 3/10
19/19 - 17s - loss: 622.5331 - loglik: -6.2105e+02 - logprior: -1.4858e+00
Epoch 4/10
19/19 - 17s - loss: 608.3443 - loglik: -6.0676e+02 - logprior: -1.5885e+00
Epoch 5/10
19/19 - 17s - loss: 602.1714 - loglik: -6.0049e+02 - logprior: -1.6806e+00
Epoch 6/10
19/19 - 17s - loss: 600.1125 - loglik: -5.9843e+02 - logprior: -1.6853e+00
Epoch 7/10
19/19 - 17s - loss: 597.8355 - loglik: -5.9615e+02 - logprior: -1.6867e+00
Epoch 8/10
19/19 - 17s - loss: 598.3473 - loglik: -5.9667e+02 - logprior: -1.6762e+00
Fitted a model with MAP estimate = -596.7177
expansions: [(8, 1), (9, 1), (10, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (30, 1), (34, 1), (37, 1), (44, 1), (45, 1), (48, 1), (51, 1), (67, 1), (68, 1), (69, 2), (70, 1), (76, 1), (93, 1), (115, 1), (116, 1), (117, 1), (121, 5), (132, 1), (136, 1), (138, 1), (148, 1), (150, 1), (154, 3), (163, 1), (164, 1), (165, 1), (166, 1), (174, 2), (175, 1), (180, 1), (183, 1), (185, 2), (193, 1), (194, 1), (197, 1), (212, 1), (213, 3), (214, 3), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 591.5872 - loglik: -5.8910e+02 - logprior: -2.4849e+00
Epoch 2/2
39/39 - 26s - loss: 575.8432 - loglik: -5.7444e+02 - logprior: -1.4058e+00
Fitted a model with MAP estimate = -570.5325
expansions: [(147, 1), (148, 1), (242, 1)]
discards: [ 44 232 270]
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 575.9775 - loglik: -5.7438e+02 - logprior: -1.5986e+00
Epoch 2/2
39/39 - 26s - loss: 571.7645 - loglik: -5.7136e+02 - logprior: -4.0346e-01
Fitted a model with MAP estimate = -568.4534
expansions: []
discards: []
Fitting a model of length 306 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 574.1134 - loglik: -5.7275e+02 - logprior: -1.3645e+00
Epoch 2/10
39/39 - 26s - loss: 570.2672 - loglik: -5.7008e+02 - logprior: -1.8769e-01
Epoch 3/10
39/39 - 26s - loss: 568.2571 - loglik: -5.6821e+02 - logprior: -5.1365e-02
Epoch 4/10
39/39 - 26s - loss: 565.0807 - loglik: -5.6516e+02 - logprior: 0.0756
Epoch 5/10
39/39 - 26s - loss: 563.5215 - loglik: -5.6369e+02 - logprior: 0.1725
Epoch 6/10
39/39 - 26s - loss: 562.2242 - loglik: -5.6253e+02 - logprior: 0.3077
Epoch 7/10
39/39 - 26s - loss: 562.7439 - loglik: -5.6318e+02 - logprior: 0.4348
Fitted a model with MAP estimate = -561.9089
Time for alignment: 564.5556
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 788.5894 - loglik: -7.8618e+02 - logprior: -2.4058e+00
Epoch 2/10
19/19 - 16s - loss: 677.2121 - loglik: -6.7628e+02 - logprior: -9.3117e-01
Epoch 3/10
19/19 - 17s - loss: 626.1392 - loglik: -6.2465e+02 - logprior: -1.4940e+00
Epoch 4/10
19/19 - 17s - loss: 609.2572 - loglik: -6.0768e+02 - logprior: -1.5767e+00
Epoch 5/10
19/19 - 17s - loss: 605.1735 - loglik: -6.0358e+02 - logprior: -1.5914e+00
Epoch 6/10
19/19 - 17s - loss: 603.0984 - loglik: -6.0155e+02 - logprior: -1.5534e+00
Epoch 7/10
19/19 - 17s - loss: 599.9295 - loglik: -5.9837e+02 - logprior: -1.5553e+00
Epoch 8/10
19/19 - 17s - loss: 601.3697 - loglik: -5.9982e+02 - logprior: -1.5468e+00
Fitted a model with MAP estimate = -599.5264
expansions: [(10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (33, 1), (34, 1), (37, 1), (44, 1), (45, 1), (48, 1), (51, 1), (67, 1), (70, 3), (76, 1), (86, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 1), (134, 1), (148, 2), (149, 1), (154, 2), (163, 1), (164, 2), (174, 2), (175, 1), (179, 1), (185, 1), (186, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 3), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 593.9317 - loglik: -5.9142e+02 - logprior: -2.5121e+00
Epoch 2/2
39/39 - 26s - loss: 578.3600 - loglik: -5.7695e+02 - logprior: -1.4083e+00
Fitted a model with MAP estimate = -573.6302
expansions: [(84, 1), (146, 1)]
discards: [179 200 237]
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 578.8365 - loglik: -5.7725e+02 - logprior: -1.5830e+00
Epoch 2/2
39/39 - 25s - loss: 574.4193 - loglik: -5.7401e+02 - logprior: -4.1014e-01
Fitted a model with MAP estimate = -571.5012
expansions: [(201, 1)]
discards: []
Fitting a model of length 302 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 576.6900 - loglik: -5.7535e+02 - logprior: -1.3409e+00
Epoch 2/10
39/39 - 26s - loss: 572.7932 - loglik: -5.7261e+02 - logprior: -1.8269e-01
Epoch 3/10
39/39 - 26s - loss: 570.3710 - loglik: -5.7033e+02 - logprior: -3.7489e-02
Epoch 4/10
39/39 - 26s - loss: 567.6978 - loglik: -5.6779e+02 - logprior: 0.0888
Epoch 5/10
39/39 - 26s - loss: 565.6117 - loglik: -5.6581e+02 - logprior: 0.2000
Epoch 6/10
Training of 3 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48b5dde2e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4896b65580>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 18s - loss: 789.0794 - loglik: -7.8668e+02 - logprior: -2.4024e+00
Epoch 2/10
19/19 - 16s - loss: 676.8679 - loglik: -6.7593e+02 - logprior: -9.3915e-01
Epoch 3/10
19/19 - 16s - loss: 625.5043 - loglik: -6.2399e+02 - logprior: -1.5096e+00
Epoch 4/10
19/19 - 16s - loss: 611.8301 - loglik: -6.1025e+02 - logprior: -1.5831e+00
Epoch 5/10
19/19 - 16s - loss: 605.6951 - loglik: -6.0404e+02 - logprior: -1.6554e+00
Epoch 6/10
19/19 - 15s - loss: 601.4826 - loglik: -5.9982e+02 - logprior: -1.6673e+00
Epoch 7/10
19/19 - 16s - loss: 601.6551 - loglik: -5.9997e+02 - logprior: -1.6817e+00
Fitted a model with MAP estimate = -599.0778
expansions: [(10, 1), (11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (18, 1), (35, 2), (42, 1), (46, 1), (47, 1), (48, 1), (51, 1), (67, 1), (68, 1), (69, 1), (70, 1), (72, 1), (76, 1), (93, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (131, 1), (132, 1), (136, 1), (138, 1), (148, 1), (152, 1), (154, 3), (163, 3), (165, 3), (166, 1), (174, 2), (175, 1), (180, 1), (185, 1), (186, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 592.3500 - loglik: -5.8986e+02 - logprior: -2.4889e+00
Epoch 2/2
39/39 - 25s - loss: 575.6700 - loglik: -5.7423e+02 - logprior: -1.4449e+00
Fitted a model with MAP estimate = -570.5641
expansions: [(0, 3), (149, 1), (196, 2), (205, 2)]
discards: [  0  44 245 270]
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 574.6647 - loglik: -5.7312e+02 - logprior: -1.5491e+00
Epoch 2/2
39/39 - 26s - loss: 568.8159 - loglik: -5.6832e+02 - logprior: -5.0007e-01
Fitted a model with MAP estimate = -565.7348
expansions: [(210, 1)]
discards: [  0   1 192]
Fitting a model of length 313 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 575.4007 - loglik: -5.7315e+02 - logprior: -2.2461e+00
Epoch 2/10
39/39 - 26s - loss: 569.0156 - loglik: -5.6864e+02 - logprior: -3.7461e-01
Epoch 3/10
39/39 - 26s - loss: 565.3575 - loglik: -5.6531e+02 - logprior: -4.8343e-02
Epoch 4/10
39/39 - 26s - loss: 563.2253 - loglik: -5.6328e+02 - logprior: 0.0573
Epoch 5/10
39/39 - 26s - loss: 560.6142 - loglik: -5.6076e+02 - logprior: 0.1480
Epoch 6/10
39/39 - 26s - loss: 559.7090 - loglik: -5.5996e+02 - logprior: 0.2548
Epoch 7/10
39/39 - 26s - loss: 560.5959 - loglik: -5.6098e+02 - logprior: 0.3876
Fitted a model with MAP estimate = -559.3694
Time for alignment: 537.7657
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 789.1350 - loglik: -7.8673e+02 - logprior: -2.4051e+00
Epoch 2/10
19/19 - 16s - loss: 676.0118 - loglik: -6.7509e+02 - logprior: -9.2650e-01
Epoch 3/10
19/19 - 16s - loss: 623.7781 - loglik: -6.2230e+02 - logprior: -1.4750e+00
Epoch 4/10
19/19 - 16s - loss: 608.7941 - loglik: -6.0725e+02 - logprior: -1.5456e+00
Epoch 5/10
19/19 - 16s - loss: 605.3039 - loglik: -6.0370e+02 - logprior: -1.6017e+00
Epoch 6/10
19/19 - 16s - loss: 601.1144 - loglik: -5.9952e+02 - logprior: -1.5976e+00
Epoch 7/10
19/19 - 16s - loss: 600.0085 - loglik: -5.9840e+02 - logprior: -1.6053e+00
Epoch 8/10
19/19 - 16s - loss: 601.8005 - loglik: -6.0019e+02 - logprior: -1.6103e+00
Fitted a model with MAP estimate = -598.8456
expansions: [(8, 1), (10, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (21, 1), (37, 1), (44, 1), (45, 1), (48, 1), (51, 1), (67, 1), (68, 1), (69, 2), (71, 1), (72, 1), (76, 1), (86, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 1), (135, 1), (148, 2), (149, 1), (163, 1), (164, 2), (176, 2), (177, 1), (184, 1), (185, 1), (186, 1), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 593.6289 - loglik: -5.9112e+02 - logprior: -2.5097e+00
Epoch 2/2
39/39 - 24s - loss: 578.3422 - loglik: -5.7694e+02 - logprior: -1.4056e+00
Fitted a model with MAP estimate = -573.1133
expansions: [(43, 1), (146, 2), (190, 2), (238, 1)]
discards: [ 86 182 201 261]
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 577.5410 - loglik: -5.7594e+02 - logprior: -1.5987e+00
Epoch 2/2
39/39 - 25s - loss: 572.4918 - loglik: -5.7207e+02 - logprior: -4.2430e-01
Fitted a model with MAP estimate = -569.5220
expansions: []
discards: [ 44 146 193 194]
Fitting a model of length 300 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 577.8824 - loglik: -5.7655e+02 - logprior: -1.3352e+00
Epoch 2/10
39/39 - 24s - loss: 573.9027 - loglik: -5.7374e+02 - logprior: -1.5835e-01
Epoch 3/10
39/39 - 24s - loss: 571.9197 - loglik: -5.7190e+02 - logprior: -1.8865e-02
Epoch 4/10
39/39 - 24s - loss: 568.7426 - loglik: -5.6884e+02 - logprior: 0.0987
Epoch 5/10
39/39 - 24s - loss: 567.4375 - loglik: -5.6761e+02 - logprior: 0.1749
Epoch 6/10
39/39 - 24s - loss: 567.0679 - loglik: -5.6737e+02 - logprior: 0.3064
Epoch 7/10
39/39 - 24s - loss: 565.9264 - loglik: -5.6635e+02 - logprior: 0.4285
Epoch 8/10
39/39 - 24s - loss: 565.8329 - loglik: -5.6639e+02 - logprior: 0.5532
Epoch 9/10
39/39 - 24s - loss: 566.5981 - loglik: -5.6727e+02 - logprior: 0.6711
Fitted a model with MAP estimate = -565.5086
Time for alignment: 581.8345
Fitting a model of length 243 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 789.1586 - loglik: -7.8676e+02 - logprior: -2.4027e+00
Epoch 2/10
19/19 - 16s - loss: 673.5752 - loglik: -6.7263e+02 - logprior: -9.4320e-01
Epoch 3/10
19/19 - 16s - loss: 623.0754 - loglik: -6.2147e+02 - logprior: -1.6006e+00
Epoch 4/10
19/19 - 16s - loss: 608.8801 - loglik: -6.0713e+02 - logprior: -1.7463e+00
Epoch 5/10
19/19 - 16s - loss: 602.0109 - loglik: -6.0025e+02 - logprior: -1.7601e+00
Epoch 6/10
19/19 - 16s - loss: 599.0909 - loglik: -5.9734e+02 - logprior: -1.7534e+00
Epoch 7/10
19/19 - 16s - loss: 598.7241 - loglik: -5.9697e+02 - logprior: -1.7560e+00
Epoch 8/10
19/19 - 16s - loss: 598.0240 - loglik: -5.9625e+02 - logprior: -1.7697e+00
Epoch 9/10
19/19 - 16s - loss: 597.1455 - loglik: -5.9535e+02 - logprior: -1.7934e+00
Epoch 10/10
19/19 - 16s - loss: 595.8721 - loglik: -5.9407e+02 - logprior: -1.8049e+00
Fitted a model with MAP estimate = -595.5124
expansions: [(11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (32, 2), (34, 1), (35, 2), (42, 1), (46, 1), (47, 1), (52, 1), (53, 1), (67, 1), (68, 1), (69, 1), (71, 1), (72, 1), (76, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 2), (135, 2), (137, 1), (140, 1), (147, 1), (148, 1), (149, 1), (153, 1), (154, 1), (163, 1), (165, 2), (166, 1), (175, 1), (176, 1), (179, 1), (183, 1), (185, 1), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (213, 3), (227, 1), (230, 1), (231, 1)]
discards: [0]
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 590.0271 - loglik: -5.8754e+02 - logprior: -2.4904e+00
Epoch 2/2
39/39 - 25s - loss: 573.3198 - loglik: -5.7190e+02 - logprior: -1.4195e+00
Fitted a model with MAP estimate = -568.9094
expansions: [(0, 3), (150, 1), (226, 1), (245, 1)]
discards: [  0  39  45 164 169 273 274]
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 575.0718 - loglik: -5.7363e+02 - logprior: -1.4444e+00
Epoch 2/2
39/39 - 25s - loss: 569.9662 - loglik: -5.6953e+02 - logprior: -4.3570e-01
Fitted a model with MAP estimate = -567.1197
expansions: [(274, 1)]
discards: [0 1]
Fitting a model of length 307 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 575.7095 - loglik: -5.7366e+02 - logprior: -2.0469e+00
Epoch 2/10
39/39 - 25s - loss: 569.3160 - loglik: -5.6904e+02 - logprior: -2.7632e-01
Epoch 3/10
39/39 - 25s - loss: 566.5627 - loglik: -5.6662e+02 - logprior: 0.0537
Epoch 4/10
39/39 - 25s - loss: 564.1782 - loglik: -5.6434e+02 - logprior: 0.1590
Epoch 5/10
39/39 - 25s - loss: 561.9827 - loglik: -5.6221e+02 - logprior: 0.2321
Epoch 6/10
39/39 - 25s - loss: 561.4487 - loglik: -5.6179e+02 - logprior: 0.3383
Epoch 7/10
39/39 - 25s - loss: 561.2174 - loglik: -5.6167e+02 - logprior: 0.4574
Epoch 8/10
39/39 - 25s - loss: 560.9268 - loglik: -5.6151e+02 - logprior: 0.5783
Epoch 9/10
39/39 - 25s - loss: 560.2041 - loglik: -5.6090e+02 - logprior: 0.6952
Epoch 10/10
39/39 - 25s - loss: 561.2772 - loglik: -5.6211e+02 - logprior: 0.8299
Fitted a model with MAP estimate = -560.1728
Time for alignment: 652.1806
Computed alignments with likelihoods: ['-559.3694', '-565.5086', '-560.1728']
Best model has likelihood: -559.3694
SP score = 0.4490
Training of 3 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f487d5de3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f484a536280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 973.3690 - loglik: -9.7202e+02 - logprior: -1.3477e+00
Epoch 2/10
39/39 - 33s - loss: 812.1052 - loglik: -8.1096e+02 - logprior: -1.1482e+00
Epoch 3/10
39/39 - 33s - loss: 799.5261 - loglik: -7.9829e+02 - logprior: -1.2352e+00
Epoch 4/10
39/39 - 33s - loss: 796.7278 - loglik: -7.9547e+02 - logprior: -1.2607e+00
Epoch 5/10
39/39 - 33s - loss: 795.3260 - loglik: -7.9408e+02 - logprior: -1.2422e+00
Epoch 6/10
39/39 - 33s - loss: 795.0909 - loglik: -7.9385e+02 - logprior: -1.2447e+00
Epoch 7/10
39/39 - 33s - loss: 794.9351 - loglik: -7.9366e+02 - logprior: -1.2706e+00
Epoch 8/10
39/39 - 33s - loss: 794.7253 - loglik: -7.9345e+02 - logprior: -1.2741e+00
Epoch 9/10
39/39 - 33s - loss: 794.6433 - loglik: -7.9334e+02 - logprior: -1.3059e+00
Epoch 10/10
39/39 - 33s - loss: 794.7316 - loglik: -7.9346e+02 - logprior: -1.2672e+00
Fitted a model with MAP estimate = -786.3236
expansions: [(0, 3), (16, 1), (48, 1), (51, 1), (53, 1), (57, 1), (61, 1), (69, 1), (70, 1), (73, 2), (76, 1), (120, 2), (121, 2), (122, 2), (145, 3), (146, 1), (147, 1), (163, 1), (169, 1), (172, 1), (174, 1), (175, 1), (192, 2), (194, 1), (195, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 2), (222, 1), (250, 1), (251, 1), (253, 1), (254, 1), (256, 1), (258, 1), (259, 1), (261, 1), (262, 1), (263, 1), (265, 2), (284, 1), (285, 2), (286, 8), (295, 1), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 778.0102 - loglik: -7.7572e+02 - logprior: -2.2858e+00
Epoch 2/2
39/39 - 45s - loss: 761.3264 - loglik: -7.6063e+02 - logprior: -6.9514e-01
Fitted a model with MAP estimate = -751.0695
expansions: [(0, 2)]
discards: [  1   2 135 169 316 384]
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 764.5875 - loglik: -7.6268e+02 - logprior: -1.9081e+00
Epoch 2/2
39/39 - 45s - loss: 759.9759 - loglik: -7.5998e+02 - logprior: 0.0051
Fitted a model with MAP estimate = -750.1867
expansions: [(0, 2), (339, 2), (360, 1)]
discards: [1 2 4 5]
Fitting a model of length 388 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 756.5718 - loglik: -7.5466e+02 - logprior: -1.9089e+00
Epoch 2/10
39/39 - 45s - loss: 751.6599 - loglik: -7.5194e+02 - logprior: 0.2759
Epoch 3/10
39/39 - 45s - loss: 750.1829 - loglik: -7.5063e+02 - logprior: 0.4444
Epoch 4/10
39/39 - 45s - loss: 748.5724 - loglik: -7.4910e+02 - logprior: 0.5296
Epoch 5/10
39/39 - 45s - loss: 746.6917 - loglik: -7.4741e+02 - logprior: 0.7136
Epoch 6/10
39/39 - 46s - loss: 746.6242 - loglik: -7.4748e+02 - logprior: 0.8605
Epoch 7/10
39/39 - 45s - loss: 747.4048 - loglik: -7.4841e+02 - logprior: 1.0010
Fitted a model with MAP estimate = -746.0825
Time for alignment: 1085.6956
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 971.7162 - loglik: -9.7038e+02 - logprior: -1.3321e+00
Epoch 2/10
39/39 - 33s - loss: 810.3615 - loglik: -8.0924e+02 - logprior: -1.1166e+00
Epoch 3/10
39/39 - 33s - loss: 798.8466 - loglik: -7.9769e+02 - logprior: -1.1614e+00
Epoch 4/10
39/39 - 33s - loss: 795.7543 - loglik: -7.9460e+02 - logprior: -1.1509e+00
Epoch 5/10
39/39 - 33s - loss: 794.8835 - loglik: -7.9371e+02 - logprior: -1.1688e+00
Epoch 6/10
39/39 - 33s - loss: 794.2605 - loglik: -7.9308e+02 - logprior: -1.1824e+00
Epoch 7/10
39/39 - 33s - loss: 793.6204 - loglik: -7.9242e+02 - logprior: -1.1999e+00
Epoch 8/10
39/39 - 33s - loss: 793.6434 - loglik: -7.9240e+02 - logprior: -1.2474e+00
Fitted a model with MAP estimate = -785.4087
expansions: [(0, 3), (34, 1), (52, 1), (53, 1), (57, 1), (62, 1), (70, 1), (71, 1), (75, 2), (121, 2), (122, 2), (123, 2), (128, 1), (146, 3), (147, 1), (148, 1), (163, 1), (170, 1), (173, 1), (175, 1), (176, 1), (193, 2), (195, 1), (196, 1), (205, 1), (219, 1), (220, 1), (221, 1), (222, 2), (223, 3), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (258, 1), (259, 1), (261, 1), (262, 1), (264, 1), (284, 1), (285, 4), (286, 3), (287, 1), (288, 1), (295, 3), (296, 1), (309, 1), (313, 1), (314, 3)]
discards: []
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 777.2023 - loglik: -7.7512e+02 - logprior: -2.0860e+00
Epoch 2/2
39/39 - 46s - loss: 760.7823 - loglik: -7.6028e+02 - logprior: -5.0346e-01
Fitted a model with MAP estimate = -750.5958
expansions: [(0, 2), (338, 1)]
discards: [ 85 134 139 169 263 334 335 339]
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 765.8271 - loglik: -7.6394e+02 - logprior: -1.8833e+00
Epoch 2/2
39/39 - 45s - loss: 760.5715 - loglik: -7.6039e+02 - logprior: -1.8261e-01
Fitted a model with MAP estimate = -750.8953
expansions: [(260, 1), (331, 1), (334, 2)]
discards: [0 1]
Fitting a model of length 388 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 755.0628 - loglik: -7.5387e+02 - logprior: -1.1963e+00
Epoch 2/10
39/39 - 45s - loss: 751.1092 - loglik: -7.5132e+02 - logprior: 0.2151
Epoch 3/10
39/39 - 46s - loss: 749.4380 - loglik: -7.4985e+02 - logprior: 0.4081
Epoch 4/10
39/39 - 45s - loss: 747.4968 - loglik: -7.4801e+02 - logprior: 0.5092
Epoch 5/10
39/39 - 46s - loss: 746.2015 - loglik: -7.4682e+02 - logprior: 0.6206
Epoch 6/10
39/39 - 45s - loss: 746.9301 - loglik: -7.4770e+02 - logprior: 0.7744
Fitted a model with MAP estimate = -745.8708
Time for alignment: 973.3720
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 973.8636 - loglik: -9.7252e+02 - logprior: -1.3432e+00
Epoch 2/10
39/39 - 33s - loss: 810.1234 - loglik: -8.0902e+02 - logprior: -1.1081e+00
Epoch 3/10
39/39 - 33s - loss: 798.1567 - loglik: -7.9701e+02 - logprior: -1.1478e+00
Epoch 4/10
39/39 - 33s - loss: 795.2954 - loglik: -7.9416e+02 - logprior: -1.1315e+00
Epoch 5/10
39/39 - 33s - loss: 794.1806 - loglik: -7.9304e+02 - logprior: -1.1401e+00
Epoch 6/10
39/39 - 33s - loss: 793.9527 - loglik: -7.9278e+02 - logprior: -1.1747e+00
Epoch 7/10
39/39 - 33s - loss: 793.7457 - loglik: -7.9260e+02 - logprior: -1.1419e+00
Epoch 8/10
39/39 - 33s - loss: 793.6420 - loglik: -7.9248e+02 - logprior: -1.1647e+00
Epoch 9/10
39/39 - 33s - loss: 793.5546 - loglik: -7.9241e+02 - logprior: -1.1495e+00
Epoch 10/10
39/39 - 33s - loss: 792.9289 - loglik: -7.9176e+02 - logprior: -1.1695e+00
Fitted a model with MAP estimate = -784.9493
expansions: [(0, 3), (5, 2), (28, 1), (47, 1), (51, 1), (52, 1), (56, 1), (67, 1), (69, 1), (70, 1), (74, 2), (98, 1), (119, 2), (120, 1), (121, 1), (122, 1), (145, 3), (147, 1), (166, 1), (169, 1), (172, 1), (174, 1), (175, 1), (192, 2), (195, 1), (196, 3), (219, 1), (220, 1), (221, 1), (222, 2), (223, 3), (246, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (259, 1), (261, 3), (262, 1), (285, 6), (286, 4), (287, 1), (288, 1), (296, 2), (297, 1), (298, 3), (309, 1), (313, 2), (314, 3)]
discards: []
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 777.4824 - loglik: -7.7538e+02 - logprior: -2.1017e+00
Epoch 2/2
39/39 - 47s - loss: 759.4934 - loglik: -7.5900e+02 - logprior: -4.9515e-01
Fitted a model with MAP estimate = -749.1916
expansions: [(0, 2), (347, 1)]
discards: [  1   2   8  87 135 222 230 264 366 392]
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 763.7916 - loglik: -7.6164e+02 - logprior: -2.1547e+00
Epoch 2/2
39/39 - 46s - loss: 758.3547 - loglik: -7.5826e+02 - logprior: -9.7026e-02
Fitted a model with MAP estimate = -748.6465
expansions: [(0, 2)]
discards: [  1   2   5 334]
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 756.2116 - loglik: -7.5414e+02 - logprior: -2.0741e+00
Epoch 2/10
39/39 - 46s - loss: 750.3672 - loglik: -7.5058e+02 - logprior: 0.2165
Epoch 3/10
39/39 - 46s - loss: 749.9351 - loglik: -7.5040e+02 - logprior: 0.4616
Epoch 4/10
39/39 - 46s - loss: 746.2062 - loglik: -7.4680e+02 - logprior: 0.5905
Epoch 5/10
39/39 - 46s - loss: 746.8689 - loglik: -7.4758e+02 - logprior: 0.7073
Fitted a model with MAP estimate = -745.7927
Time for alignment: 1005.0359
Computed alignments with likelihoods: ['-746.0825', '-745.8708', '-745.7927']
Best model has likelihood: -745.7927
SP score = 0.4341
Training of 3 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f487da26fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f481ff8a370>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 133.6358 - loglik: -1.3041e+02 - logprior: -3.2240e+00
Epoch 2/10
19/19 - 1s - loss: 108.9484 - loglik: -1.0754e+02 - logprior: -1.4044e+00
Epoch 3/10
19/19 - 1s - loss: 101.3079 - loglik: -9.9752e+01 - logprior: -1.5562e+00
Epoch 4/10
19/19 - 1s - loss: 99.0793 - loglik: -9.7646e+01 - logprior: -1.4330e+00
Epoch 5/10
19/19 - 1s - loss: 98.4672 - loglik: -9.7050e+01 - logprior: -1.4169e+00
Epoch 6/10
19/19 - 1s - loss: 98.2783 - loglik: -9.6879e+01 - logprior: -1.3989e+00
Epoch 7/10
19/19 - 1s - loss: 98.2165 - loglik: -9.6834e+01 - logprior: -1.3829e+00
Epoch 8/10
19/19 - 1s - loss: 97.9377 - loglik: -9.6562e+01 - logprior: -1.3753e+00
Epoch 9/10
19/19 - 1s - loss: 98.2443 - loglik: -9.6877e+01 - logprior: -1.3678e+00
Fitted a model with MAP estimate = -97.9637
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.3346 - loglik: -9.8158e+01 - logprior: -4.1765e+00
Epoch 2/2
19/19 - 1s - loss: 95.3796 - loglik: -9.3264e+01 - logprior: -2.1158e+00
Fitted a model with MAP estimate = -94.0853
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 95.5543 - loglik: -9.2482e+01 - logprior: -3.0722e+00
Epoch 2/2
19/19 - 1s - loss: 92.8075 - loglik: -9.1543e+01 - logprior: -1.2650e+00
Fitted a model with MAP estimate = -92.3389
expansions: []
discards: [0]
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.9393 - loglik: -9.3361e+01 - logprior: -3.5786e+00
Epoch 2/10
19/19 - 1s - loss: 93.5147 - loglik: -9.2063e+01 - logprior: -1.4512e+00
Epoch 3/10
19/19 - 1s - loss: 92.9756 - loglik: -9.1614e+01 - logprior: -1.3614e+00
Epoch 4/10
19/19 - 1s - loss: 92.7924 - loglik: -9.1482e+01 - logprior: -1.3104e+00
Epoch 5/10
19/19 - 1s - loss: 92.5140 - loglik: -9.1228e+01 - logprior: -1.2863e+00
Epoch 6/10
19/19 - 1s - loss: 92.6406 - loglik: -9.1369e+01 - logprior: -1.2721e+00
Fitted a model with MAP estimate = -92.4018
Time for alignment: 35.1374
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.6988 - loglik: -1.3048e+02 - logprior: -3.2227e+00
Epoch 2/10
19/19 - 1s - loss: 108.3200 - loglik: -1.0692e+02 - logprior: -1.3995e+00
Epoch 3/10
19/19 - 1s - loss: 100.7116 - loglik: -9.9155e+01 - logprior: -1.5570e+00
Epoch 4/10
19/19 - 1s - loss: 98.8757 - loglik: -9.7443e+01 - logprior: -1.4325e+00
Epoch 5/10
19/19 - 1s - loss: 98.3535 - loglik: -9.6944e+01 - logprior: -1.4099e+00
Epoch 6/10
19/19 - 1s - loss: 98.2658 - loglik: -9.6870e+01 - logprior: -1.3961e+00
Epoch 7/10
19/19 - 1s - loss: 98.1292 - loglik: -9.6747e+01 - logprior: -1.3827e+00
Epoch 8/10
19/19 - 1s - loss: 98.2514 - loglik: -9.6878e+01 - logprior: -1.3739e+00
Fitted a model with MAP estimate = -97.9860
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 102.2379 - loglik: -9.8063e+01 - logprior: -4.1746e+00
Epoch 2/2
19/19 - 1s - loss: 95.3268 - loglik: -9.3224e+01 - logprior: -2.1030e+00
Fitted a model with MAP estimate = -93.9981
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.5557 - loglik: -9.2482e+01 - logprior: -3.0733e+00
Epoch 2/2
19/19 - 1s - loss: 92.7900 - loglik: -9.1526e+01 - logprior: -1.2643e+00
Fitted a model with MAP estimate = -92.3557
expansions: []
discards: [0]
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.9165 - loglik: -9.3327e+01 - logprior: -3.5890e+00
Epoch 2/10
19/19 - 1s - loss: 93.4684 - loglik: -9.2018e+01 - logprior: -1.4506e+00
Epoch 3/10
19/19 - 1s - loss: 93.0110 - loglik: -9.1648e+01 - logprior: -1.3629e+00
Epoch 4/10
19/19 - 1s - loss: 92.7172 - loglik: -9.1404e+01 - logprior: -1.3129e+00
Epoch 5/10
19/19 - 1s - loss: 92.5917 - loglik: -9.1308e+01 - logprior: -1.2838e+00
Epoch 6/10
19/19 - 1s - loss: 92.6044 - loglik: -9.1331e+01 - logprior: -1.2733e+00
Fitted a model with MAP estimate = -92.4169
Time for alignment: 33.6516
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 133.7086 - loglik: -1.3049e+02 - logprior: -3.2224e+00
Epoch 2/10
19/19 - 1s - loss: 108.6955 - loglik: -1.0730e+02 - logprior: -1.3991e+00
Epoch 3/10
19/19 - 1s - loss: 100.5579 - loglik: -9.8994e+01 - logprior: -1.5634e+00
Epoch 4/10
19/19 - 1s - loss: 98.8453 - loglik: -9.7408e+01 - logprior: -1.4369e+00
Epoch 5/10
19/19 - 1s - loss: 98.3749 - loglik: -9.6962e+01 - logprior: -1.4125e+00
Epoch 6/10
19/19 - 1s - loss: 98.2593 - loglik: -9.6860e+01 - logprior: -1.3991e+00
Epoch 7/10
19/19 - 1s - loss: 98.1666 - loglik: -9.6786e+01 - logprior: -1.3810e+00
Epoch 8/10
19/19 - 1s - loss: 98.1370 - loglik: -9.6762e+01 - logprior: -1.3754e+00
Epoch 9/10
19/19 - 1s - loss: 98.1906 - loglik: -9.6823e+01 - logprior: -1.3674e+00
Fitted a model with MAP estimate = -97.9697
expansions: [(7, 1), (8, 3), (9, 2), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.2550 - loglik: -9.8081e+01 - logprior: -4.1735e+00
Epoch 2/2
19/19 - 1s - loss: 95.5073 - loglik: -9.3391e+01 - logprior: -2.1165e+00
Fitted a model with MAP estimate = -94.0442
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 95.5416 - loglik: -9.2469e+01 - logprior: -3.0721e+00
Epoch 2/2
19/19 - 1s - loss: 92.8104 - loglik: -9.1547e+01 - logprior: -1.2631e+00
Fitted a model with MAP estimate = -92.3656
expansions: []
discards: [0]
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 96.8874 - loglik: -9.3327e+01 - logprior: -3.5604e+00
Epoch 2/10
19/19 - 1s - loss: 93.5354 - loglik: -9.2088e+01 - logprior: -1.4472e+00
Epoch 3/10
19/19 - 1s - loss: 92.9484 - loglik: -9.1588e+01 - logprior: -1.3600e+00
Epoch 4/10
19/19 - 1s - loss: 92.7374 - loglik: -9.1422e+01 - logprior: -1.3150e+00
Epoch 5/10
19/19 - 1s - loss: 92.6495 - loglik: -9.1367e+01 - logprior: -1.2820e+00
Epoch 6/10
19/19 - 1s - loss: 92.5068 - loglik: -9.1235e+01 - logprior: -1.2715e+00
Epoch 7/10
19/19 - 1s - loss: 92.3979 - loglik: -9.1143e+01 - logprior: -1.2551e+00
Epoch 8/10
19/19 - 1s - loss: 92.4288 - loglik: -9.1184e+01 - logprior: -1.2448e+00
Fitted a model with MAP estimate = -92.3374
Time for alignment: 36.7775
Computed alignments with likelihoods: ['-92.3389', '-92.3557', '-92.3374']
Best model has likelihood: -92.3374
SP score = 0.8435
Training of 3 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48532c6cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48532be4c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 524.7377 - loglik: -5.2199e+02 - logprior: -2.7431e+00
Epoch 2/10
19/19 - 6s - loss: 423.5764 - loglik: -4.2264e+02 - logprior: -9.3256e-01
Epoch 3/10
19/19 - 6s - loss: 378.2927 - loglik: -3.7671e+02 - logprior: -1.5861e+00
Epoch 4/10
19/19 - 6s - loss: 367.0801 - loglik: -3.6520e+02 - logprior: -1.8758e+00
Epoch 5/10
19/19 - 6s - loss: 364.6224 - loglik: -3.6282e+02 - logprior: -1.7985e+00
Epoch 6/10
19/19 - 6s - loss: 362.5806 - loglik: -3.6084e+02 - logprior: -1.7364e+00
Epoch 7/10
19/19 - 6s - loss: 362.3813 - loglik: -3.6068e+02 - logprior: -1.6981e+00
Epoch 8/10
19/19 - 6s - loss: 360.6573 - loglik: -3.5898e+02 - logprior: -1.6741e+00
Epoch 9/10
19/19 - 6s - loss: 360.9817 - loglik: -3.5931e+02 - logprior: -1.6691e+00
Fitted a model with MAP estimate = -360.3496
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (37, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (60, 1), (63, 1), (66, 1), (74, 2), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 2), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 353.1635 - loglik: -3.5050e+02 - logprior: -2.6627e+00
Epoch 2/2
19/19 - 9s - loss: 332.5950 - loglik: -3.3170e+02 - logprior: -8.9675e-01
Fitted a model with MAP estimate = -329.7722
expansions: []
discards: [118 169]
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 336.0522 - loglik: -3.3348e+02 - logprior: -2.5714e+00
Epoch 2/2
19/19 - 9s - loss: 330.5296 - loglik: -3.2979e+02 - logprior: -7.4048e-01
Fitted a model with MAP estimate = -329.0800
expansions: []
discards: []
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 335.0134 - loglik: -3.3251e+02 - logprior: -2.5033e+00
Epoch 2/10
19/19 - 9s - loss: 330.5994 - loglik: -3.2994e+02 - logprior: -6.5458e-01
Epoch 3/10
19/19 - 9s - loss: 327.7950 - loglik: -3.2725e+02 - logprior: -5.4402e-01
Epoch 4/10
19/19 - 9s - loss: 327.8354 - loglik: -3.2735e+02 - logprior: -4.8881e-01
Fitted a model with MAP estimate = -326.3730
Time for alignment: 187.6738
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 525.1357 - loglik: -5.2240e+02 - logprior: -2.7385e+00
Epoch 2/10
19/19 - 6s - loss: 422.7591 - loglik: -4.2180e+02 - logprior: -9.5766e-01
Epoch 3/10
19/19 - 6s - loss: 379.5900 - loglik: -3.7797e+02 - logprior: -1.6220e+00
Epoch 4/10
19/19 - 6s - loss: 367.9965 - loglik: -3.6608e+02 - logprior: -1.9134e+00
Epoch 5/10
19/19 - 6s - loss: 364.7032 - loglik: -3.6287e+02 - logprior: -1.8312e+00
Epoch 6/10
19/19 - 6s - loss: 362.2862 - loglik: -3.6050e+02 - logprior: -1.7876e+00
Epoch 7/10
19/19 - 6s - loss: 360.3964 - loglik: -3.5864e+02 - logprior: -1.7519e+00
Epoch 8/10
19/19 - 6s - loss: 362.1031 - loglik: -3.6037e+02 - logprior: -1.7291e+00
Fitted a model with MAP estimate = -359.8905
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (22, 1), (27, 1), (34, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (74, 1), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Fitting a model of length 194 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 354.4135 - loglik: -3.5174e+02 - logprior: -2.6772e+00
Epoch 2/2
19/19 - 9s - loss: 332.3589 - loglik: -3.3145e+02 - logprior: -9.0875e-01
Fitted a model with MAP estimate = -329.7626
expansions: []
discards: [118 169]
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 335.5678 - loglik: -3.3301e+02 - logprior: -2.5599e+00
Epoch 2/2
19/19 - 9s - loss: 330.2683 - loglik: -3.2955e+02 - logprior: -7.2043e-01
Fitted a model with MAP estimate = -329.0803
expansions: []
discards: []
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 334.1929 - loglik: -3.3170e+02 - logprior: -2.4917e+00
Epoch 2/10
19/19 - 9s - loss: 331.1502 - loglik: -3.3052e+02 - logprior: -6.2781e-01
Epoch 3/10
19/19 - 9s - loss: 328.1028 - loglik: -3.2758e+02 - logprior: -5.2124e-01
Epoch 4/10
19/19 - 9s - loss: 327.8867 - loglik: -3.2744e+02 - logprior: -4.4697e-01
Epoch 5/10
19/19 - 8s - loss: 324.7281 - loglik: -3.2433e+02 - logprior: -4.0012e-01
Epoch 6/10
19/19 - 9s - loss: 325.2036 - loglik: -3.2485e+02 - logprior: -3.5601e-01
Fitted a model with MAP estimate = -324.1253
Time for alignment: 198.3808
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 525.2922 - loglik: -5.2255e+02 - logprior: -2.7455e+00
Epoch 2/10
19/19 - 6s - loss: 422.8467 - loglik: -4.2191e+02 - logprior: -9.3974e-01
Epoch 3/10
19/19 - 6s - loss: 379.0412 - loglik: -3.7744e+02 - logprior: -1.6049e+00
Epoch 4/10
19/19 - 6s - loss: 368.0766 - loglik: -3.6621e+02 - logprior: -1.8707e+00
Epoch 5/10
19/19 - 6s - loss: 363.4031 - loglik: -3.6161e+02 - logprior: -1.7902e+00
Epoch 6/10
19/19 - 6s - loss: 363.2859 - loglik: -3.6155e+02 - logprior: -1.7408e+00
Epoch 7/10
19/19 - 6s - loss: 360.4550 - loglik: -3.5875e+02 - logprior: -1.7031e+00
Epoch 8/10
19/19 - 6s - loss: 360.9176 - loglik: -3.5923e+02 - logprior: -1.6876e+00
Fitted a model with MAP estimate = -360.0024
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (37, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 2), (60, 1), (63, 1), (66, 1), (74, 1), (93, 1), (97, 2), (99, 1), (102, 1), (103, 1), (108, 1), (110, 1), (113, 1), (116, 2), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Fitting a model of length 195 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 353.3552 - loglik: -3.5068e+02 - logprior: -2.6740e+00
Epoch 2/2
19/19 - 9s - loss: 332.3137 - loglik: -3.3135e+02 - logprior: -9.6083e-01
Fitted a model with MAP estimate = -329.2367
expansions: []
discards: [ 74 119 170]
Fitting a model of length 192 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 334.9770 - loglik: -3.3235e+02 - logprior: -2.6271e+00
Epoch 2/2
19/19 - 9s - loss: 330.3541 - loglik: -3.2956e+02 - logprior: -7.9619e-01
Fitted a model with MAP estimate = -328.8298
expansions: []
discards: []
Fitting a model of length 192 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 333.7995 - loglik: -3.3126e+02 - logprior: -2.5430e+00
Epoch 2/10
19/19 - 9s - loss: 330.1615 - loglik: -3.2947e+02 - logprior: -6.9436e-01
Epoch 3/10
19/19 - 9s - loss: 327.8372 - loglik: -3.2729e+02 - logprior: -5.4425e-01
Epoch 4/10
19/19 - 9s - loss: 327.8905 - loglik: -3.2738e+02 - logprior: -5.0618e-01
Fitted a model with MAP estimate = -326.1625
Time for alignment: 181.0742
Computed alignments with likelihoods: ['-326.3730', '-324.1253', '-326.1625']
Best model has likelihood: -324.1253
SP score = 0.7490
Training of 3 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4806c32f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f486c638f40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 258.0999 - loglik: -2.5499e+02 - logprior: -3.1137e+00
Epoch 2/10
19/19 - 2s - loss: 226.6832 - loglik: -2.2539e+02 - logprior: -1.2882e+00
Epoch 3/10
19/19 - 2s - loss: 212.9410 - loglik: -2.1158e+02 - logprior: -1.3571e+00
Epoch 4/10
19/19 - 2s - loss: 208.2867 - loglik: -2.0692e+02 - logprior: -1.3621e+00
Epoch 5/10
19/19 - 2s - loss: 205.9363 - loglik: -2.0447e+02 - logprior: -1.4667e+00
Epoch 6/10
19/19 - 2s - loss: 205.1230 - loglik: -2.0367e+02 - logprior: -1.4501e+00
Epoch 7/10
19/19 - 2s - loss: 204.9865 - loglik: -2.0354e+02 - logprior: -1.4420e+00
Epoch 8/10
19/19 - 2s - loss: 204.7796 - loglik: -2.0335e+02 - logprior: -1.4275e+00
Epoch 9/10
19/19 - 2s - loss: 204.6842 - loglik: -2.0326e+02 - logprior: -1.4258e+00
Epoch 10/10
19/19 - 2s - loss: 204.8938 - loglik: -2.0347e+02 - logprior: -1.4214e+00
Fitted a model with MAP estimate = -204.3075
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (26, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 2), (68, 1), (69, 3)]
discards: [0]
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.6484 - loglik: -2.0367e+02 - logprior: -3.9738e+00
Epoch 2/2
19/19 - 2s - loss: 197.3349 - loglik: -1.9539e+02 - logprior: -1.9450e+00
Fitted a model with MAP estimate = -195.4014
expansions: [(0, 2)]
discards: [ 0  7 46 79]
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 197.1592 - loglik: -1.9433e+02 - logprior: -2.8275e+00
Epoch 2/2
19/19 - 2s - loss: 193.7354 - loglik: -1.9272e+02 - logprior: -1.0129e+00
Fitted a model with MAP estimate = -192.9573
expansions: []
discards: [0]
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 198.4719 - loglik: -1.9505e+02 - logprior: -3.4186e+00
Epoch 2/10
19/19 - 2s - loss: 194.5573 - loglik: -1.9338e+02 - logprior: -1.1730e+00
Epoch 3/10
19/19 - 2s - loss: 193.3345 - loglik: -1.9231e+02 - logprior: -1.0265e+00
Epoch 4/10
19/19 - 2s - loss: 193.0147 - loglik: -1.9203e+02 - logprior: -9.8965e-01
Epoch 5/10
19/19 - 2s - loss: 192.2430 - loglik: -1.9129e+02 - logprior: -9.5615e-01
Epoch 6/10
19/19 - 2s - loss: 192.0795 - loglik: -1.9114e+02 - logprior: -9.4334e-01
Epoch 7/10
19/19 - 2s - loss: 192.1508 - loglik: -1.9123e+02 - logprior: -9.2260e-01
Fitted a model with MAP estimate = -191.6601
Time for alignment: 60.8900
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 258.1698 - loglik: -2.5506e+02 - logprior: -3.1096e+00
Epoch 2/10
19/19 - 2s - loss: 226.2376 - loglik: -2.2494e+02 - logprior: -1.2988e+00
Epoch 3/10
19/19 - 2s - loss: 211.9982 - loglik: -2.1062e+02 - logprior: -1.3770e+00
Epoch 4/10
19/19 - 2s - loss: 206.0624 - loglik: -2.0471e+02 - logprior: -1.3496e+00
Epoch 5/10
19/19 - 2s - loss: 203.9631 - loglik: -2.0268e+02 - logprior: -1.2872e+00
Epoch 6/10
19/19 - 2s - loss: 203.3317 - loglik: -2.0209e+02 - logprior: -1.2438e+00
Epoch 7/10
19/19 - 2s - loss: 203.1714 - loglik: -2.0194e+02 - logprior: -1.2271e+00
Epoch 8/10
19/19 - 2s - loss: 203.1276 - loglik: -2.0191e+02 - logprior: -1.2180e+00
Epoch 9/10
19/19 - 2s - loss: 203.0195 - loglik: -2.0181e+02 - logprior: -1.2126e+00
Epoch 10/10
19/19 - 2s - loss: 202.9624 - loglik: -2.0175e+02 - logprior: -1.2142e+00
Fitted a model with MAP estimate = -202.5426
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (27, 1), (29, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (63, 1), (72, 1), (73, 3)]
discards: [0]
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 206.3603 - loglik: -2.0240e+02 - logprior: -3.9607e+00
Epoch 2/2
19/19 - 2s - loss: 197.3300 - loglik: -1.9540e+02 - logprior: -1.9293e+00
Fitted a model with MAP estimate = -195.2899
expansions: [(0, 2)]
discards: [ 0  7 46]
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 196.9430 - loglik: -1.9411e+02 - logprior: -2.8281e+00
Epoch 2/2
19/19 - 2s - loss: 193.9720 - loglik: -1.9296e+02 - logprior: -1.0079e+00
Fitted a model with MAP estimate = -192.9576
expansions: []
discards: [0]
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 198.4837 - loglik: -1.9508e+02 - logprior: -3.4084e+00
Epoch 2/10
19/19 - 2s - loss: 194.2832 - loglik: -1.9312e+02 - logprior: -1.1655e+00
Epoch 3/10
19/19 - 2s - loss: 193.2255 - loglik: -1.9220e+02 - logprior: -1.0302e+00
Epoch 4/10
19/19 - 2s - loss: 192.9595 - loglik: -1.9197e+02 - logprior: -9.8749e-01
Epoch 5/10
19/19 - 2s - loss: 192.5805 - loglik: -1.9163e+02 - logprior: -9.5487e-01
Epoch 6/10
19/19 - 2s - loss: 192.0581 - loglik: -1.9112e+02 - logprior: -9.3365e-01
Epoch 7/10
19/19 - 2s - loss: 191.6535 - loglik: -1.9073e+02 - logprior: -9.2016e-01
Epoch 8/10
19/19 - 2s - loss: 191.9102 - loglik: -1.9101e+02 - logprior: -9.0172e-01
Fitted a model with MAP estimate = -191.5837
Time for alignment: 62.5570
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 258.2344 - loglik: -2.5513e+02 - logprior: -3.1085e+00
Epoch 2/10
19/19 - 2s - loss: 226.2501 - loglik: -2.2494e+02 - logprior: -1.3077e+00
Epoch 3/10
19/19 - 2s - loss: 211.3540 - loglik: -2.0995e+02 - logprior: -1.4004e+00
Epoch 4/10
19/19 - 2s - loss: 207.6358 - loglik: -2.0633e+02 - logprior: -1.3024e+00
Epoch 5/10
19/19 - 2s - loss: 206.4230 - loglik: -2.0513e+02 - logprior: -1.2938e+00
Epoch 6/10
19/19 - 2s - loss: 204.9353 - loglik: -2.0368e+02 - logprior: -1.2563e+00
Epoch 7/10
19/19 - 2s - loss: 203.8331 - loglik: -2.0262e+02 - logprior: -1.2151e+00
Epoch 8/10
19/19 - 2s - loss: 203.9648 - loglik: -2.0277e+02 - logprior: -1.1998e+00
Fitted a model with MAP estimate = -203.4663
expansions: [(7, 2), (8, 2), (9, 4), (12, 1), (15, 1), (22, 1), (29, 1), (30, 2), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (63, 1), (72, 1), (73, 3)]
discards: [0]
Fitting a model of length 97 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 207.3672 - loglik: -2.0341e+02 - logprior: -3.9544e+00
Epoch 2/2
19/19 - 2s - loss: 197.2119 - loglik: -1.9527e+02 - logprior: -1.9462e+00
Fitted a model with MAP estimate = -194.9911
expansions: [(0, 2)]
discards: [ 0  7  9 10 13 41 51]
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 197.1330 - loglik: -1.9430e+02 - logprior: -2.8346e+00
Epoch 2/2
19/19 - 2s - loss: 193.8900 - loglik: -1.9287e+02 - logprior: -1.0189e+00
Fitted a model with MAP estimate = -192.9274
expansions: []
discards: [0]
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 198.3799 - loglik: -1.9495e+02 - logprior: -3.4279e+00
Epoch 2/10
19/19 - 2s - loss: 194.4101 - loglik: -1.9323e+02 - logprior: -1.1785e+00
Epoch 3/10
19/19 - 2s - loss: 193.5493 - loglik: -1.9251e+02 - logprior: -1.0344e+00
Epoch 4/10
19/19 - 2s - loss: 192.7129 - loglik: -1.9171e+02 - logprior: -1.0023e+00
Epoch 5/10
19/19 - 2s - loss: 192.3890 - loglik: -1.9143e+02 - logprior: -9.6309e-01
Epoch 6/10
19/19 - 2s - loss: 192.0344 - loglik: -1.9108e+02 - logprior: -9.5419e-01
Epoch 7/10
19/19 - 2s - loss: 192.0664 - loglik: -1.9114e+02 - logprior: -9.2895e-01
Fitted a model with MAP estimate = -191.6662
Time for alignment: 57.1240
Computed alignments with likelihoods: ['-191.6601', '-191.5837', '-191.6662']
Best model has likelihood: -191.5837
SP score = 0.9749
Training of 3 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4806998b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48641feb20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 604.0807 - loglik: -6.0143e+02 - logprior: -2.6468e+00
Epoch 2/10
19/19 - 7s - loss: 529.8223 - loglik: -5.2894e+02 - logprior: -8.8137e-01
Epoch 3/10
19/19 - 7s - loss: 491.0173 - loglik: -4.8963e+02 - logprior: -1.3877e+00
Epoch 4/10
19/19 - 7s - loss: 482.8943 - loglik: -4.8141e+02 - logprior: -1.4797e+00
Epoch 5/10
19/19 - 7s - loss: 479.8407 - loglik: -4.7832e+02 - logprior: -1.5179e+00
Epoch 6/10
19/19 - 7s - loss: 478.8746 - loglik: -4.7734e+02 - logprior: -1.5357e+00
Epoch 7/10
19/19 - 7s - loss: 478.4818 - loglik: -4.7691e+02 - logprior: -1.5673e+00
Epoch 8/10
19/19 - 7s - loss: 477.8435 - loglik: -4.7627e+02 - logprior: -1.5730e+00
Epoch 9/10
19/19 - 7s - loss: 477.9087 - loglik: -4.7632e+02 - logprior: -1.5921e+00
Fitted a model with MAP estimate = -475.1212
expansions: [(5, 1), (6, 2), (7, 2), (8, 1), (20, 2), (21, 1), (26, 1), (35, 2), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (112, 1), (125, 3), (126, 1), (141, 5), (149, 2), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: [0]
Fitting a model of length 223 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 479.7179 - loglik: -4.7606e+02 - logprior: -3.6595e+00
Epoch 2/2
19/19 - 10s - loss: 462.2377 - loglik: -4.6041e+02 - logprior: -1.8283e+00
Fitted a model with MAP estimate = -457.6216
expansions: [(0, 2)]
discards: [  0   8  26  45  57  95 114 180 181 193]
Fitting a model of length 215 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 462.9473 - loglik: -4.6035e+02 - logprior: -2.5964e+00
Epoch 2/2
19/19 - 9s - loss: 458.5211 - loglik: -4.5770e+02 - logprior: -8.2303e-01
Fitted a model with MAP estimate = -455.0770
expansions: [(181, 1)]
discards: [0]
Fitting a model of length 215 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 461.2076 - loglik: -4.5775e+02 - logprior: -3.4577e+00
Epoch 2/10
19/19 - 9s - loss: 457.0472 - loglik: -4.5610e+02 - logprior: -9.5193e-01
Epoch 3/10
19/19 - 9s - loss: 454.7053 - loglik: -4.5413e+02 - logprior: -5.7503e-01
Epoch 4/10
19/19 - 9s - loss: 454.1273 - loglik: -4.5358e+02 - logprior: -5.4272e-01
Epoch 5/10
19/19 - 9s - loss: 452.4664 - loglik: -4.5196e+02 - logprior: -5.0813e-01
Epoch 6/10
19/19 - 9s - loss: 451.8675 - loglik: -4.5135e+02 - logprior: -5.1674e-01
Epoch 7/10
19/19 - 9s - loss: 451.4991 - loglik: -4.5101e+02 - logprior: -4.8867e-01
Epoch 8/10
19/19 - 9s - loss: 450.7328 - loglik: -4.5024e+02 - logprior: -4.9496e-01
Epoch 9/10
19/19 - 9s - loss: 451.5422 - loglik: -4.5110e+02 - logprior: -4.4527e-01
Fitted a model with MAP estimate = -450.6554
Time for alignment: 256.3260
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 603.9142 - loglik: -6.0125e+02 - logprior: -2.6670e+00
Epoch 2/10
19/19 - 7s - loss: 531.9837 - loglik: -5.3108e+02 - logprior: -9.0335e-01
Epoch 3/10
19/19 - 7s - loss: 494.8110 - loglik: -4.9352e+02 - logprior: -1.2937e+00
Epoch 4/10
19/19 - 7s - loss: 484.6732 - loglik: -4.8334e+02 - logprior: -1.3374e+00
Epoch 5/10
19/19 - 7s - loss: 480.5042 - loglik: -4.7914e+02 - logprior: -1.3627e+00
Epoch 6/10
19/19 - 7s - loss: 479.1064 - loglik: -4.7776e+02 - logprior: -1.3434e+00
Epoch 7/10
19/19 - 7s - loss: 478.4966 - loglik: -4.7716e+02 - logprior: -1.3397e+00
Epoch 8/10
19/19 - 7s - loss: 478.1718 - loglik: -4.7685e+02 - logprior: -1.3262e+00
Epoch 9/10
19/19 - 7s - loss: 477.6278 - loglik: -4.7631e+02 - logprior: -1.3208e+00
Epoch 10/10
19/19 - 7s - loss: 477.4381 - loglik: -4.7611e+02 - logprior: -1.3288e+00
Fitted a model with MAP estimate = -475.1786
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (26, 1), (35, 2), (45, 2), (46, 1), (51, 1), (52, 1), (69, 1), (71, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (108, 1), (113, 1), (116, 1), (125, 2), (127, 1), (132, 1), (135, 1), (137, 1), (147, 1), (149, 3), (150, 2), (151, 2), (153, 1), (159, 1), (161, 1)]
discards: []
Fitting a model of length 222 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 479.9827 - loglik: -4.7607e+02 - logprior: -3.9098e+00
Epoch 2/2
19/19 - 10s - loss: 462.6972 - loglik: -4.6146e+02 - logprior: -1.2325e+00
Fitted a model with MAP estimate = -457.6887
expansions: []
discards: [ 26  45  57  95 114 158 192]
Fitting a model of length 215 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 464.1653 - loglik: -4.6139e+02 - logprior: -2.7796e+00
Epoch 2/2
19/19 - 9s - loss: 459.8406 - loglik: -4.5896e+02 - logprior: -8.7652e-01
Fitted a model with MAP estimate = -456.2924
expansions: []
discards: []
Fitting a model of length 215 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 460.5111 - loglik: -4.5782e+02 - logprior: -2.6898e+00
Epoch 2/10
19/19 - 9s - loss: 457.0854 - loglik: -4.5629e+02 - logprior: -8.0034e-01
Epoch 3/10
19/19 - 9s - loss: 456.1615 - loglik: -4.5555e+02 - logprior: -6.1170e-01
Epoch 4/10
19/19 - 9s - loss: 454.7490 - loglik: -4.5420e+02 - logprior: -5.5372e-01
Epoch 5/10
19/19 - 9s - loss: 453.7873 - loglik: -4.5325e+02 - logprior: -5.3267e-01
Epoch 6/10
19/19 - 9s - loss: 452.8077 - loglik: -4.5231e+02 - logprior: -5.0098e-01
Epoch 7/10
19/19 - 9s - loss: 452.2947 - loglik: -4.5178e+02 - logprior: -5.1224e-01
Epoch 8/10
19/19 - 9s - loss: 452.1767 - loglik: -4.5170e+02 - logprior: -4.7245e-01
Epoch 9/10
19/19 - 9s - loss: 452.1057 - loglik: -4.5166e+02 - logprior: -4.4429e-01
Epoch 10/10
19/19 - 9s - loss: 451.3672 - loglik: -4.5094e+02 - logprior: -4.2356e-01
Fitted a model with MAP estimate = -451.5642
Time for alignment: 271.9718
Fitting a model of length 173 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 604.1393 - loglik: -6.0149e+02 - logprior: -2.6524e+00
Epoch 2/10
19/19 - 7s - loss: 530.6677 - loglik: -5.2977e+02 - logprior: -8.9465e-01
Epoch 3/10
19/19 - 7s - loss: 492.9320 - loglik: -4.9159e+02 - logprior: -1.3379e+00
Epoch 4/10
19/19 - 7s - loss: 483.4702 - loglik: -4.8210e+02 - logprior: -1.3696e+00
Epoch 5/10
19/19 - 7s - loss: 479.5111 - loglik: -4.7818e+02 - logprior: -1.3264e+00
Epoch 6/10
19/19 - 7s - loss: 478.4742 - loglik: -4.7716e+02 - logprior: -1.3145e+00
Epoch 7/10
19/19 - 7s - loss: 477.5482 - loglik: -4.7625e+02 - logprior: -1.3002e+00
Epoch 8/10
19/19 - 7s - loss: 476.7812 - loglik: -4.7549e+02 - logprior: -1.2913e+00
Epoch 9/10
19/19 - 7s - loss: 477.6280 - loglik: -4.7634e+02 - logprior: -1.2910e+00
Fitted a model with MAP estimate = -474.6793
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (34, 1), (36, 1), (45, 2), (46, 1), (51, 1), (52, 1), (69, 1), (71, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (112, 1), (116, 1), (124, 3), (131, 1), (140, 3), (149, 2), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Fitting a model of length 221 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 478.2283 - loglik: -4.7437e+02 - logprior: -3.8611e+00
Epoch 2/2
19/19 - 10s - loss: 461.1742 - loglik: -4.5999e+02 - logprior: -1.1883e+00
Fitted a model with MAP estimate = -456.3326
expansions: [(186, 1)]
discards: [ 26  56  94 113 178 191]
Fitting a model of length 216 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 462.6295 - loglik: -4.5987e+02 - logprior: -2.7610e+00
Epoch 2/2
19/19 - 9s - loss: 458.5439 - loglik: -4.5769e+02 - logprior: -8.5393e-01
Fitted a model with MAP estimate = -454.9227
expansions: []
discards: []
Fitting a model of length 216 on 10089 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 12s - loss: 459.4645 - loglik: -4.5678e+02 - logprior: -2.6818e+00
Epoch 2/10
19/19 - 9s - loss: 455.6140 - loglik: -4.5483e+02 - logprior: -7.8085e-01
Epoch 3/10
19/19 - 9s - loss: 454.7346 - loglik: -4.5412e+02 - logprior: -6.1724e-01
Epoch 4/10
19/19 - 9s - loss: 453.7738 - loglik: -4.5323e+02 - logprior: -5.4472e-01
Epoch 5/10
19/19 - 9s - loss: 452.4897 - loglik: -4.5197e+02 - logprior: -5.2236e-01
Epoch 6/10
19/19 - 9s - loss: 450.9601 - loglik: -4.5044e+02 - logprior: -5.1523e-01
Epoch 7/10
19/19 - 9s - loss: 451.3571 - loglik: -4.5086e+02 - logprior: -4.9260e-01
Fitted a model with MAP estimate = -450.7455
Time for alignment: 238.0883
Computed alignments with likelihoods: ['-450.6554', '-451.5642', '-450.7455']
Best model has likelihood: -450.6554
SP score = 0.7313
Training of 3 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f487d25ff70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f480f536310>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.7833 - loglik: -1.6159e+02 - logprior: -3.1918e+00
Epoch 2/10
19/19 - 1s - loss: 136.4637 - loglik: -1.3507e+02 - logprior: -1.3939e+00
Epoch 3/10
19/19 - 1s - loss: 128.6266 - loglik: -1.2715e+02 - logprior: -1.4809e+00
Epoch 4/10
19/19 - 1s - loss: 127.4500 - loglik: -1.2612e+02 - logprior: -1.3297e+00
Epoch 5/10
19/19 - 1s - loss: 127.1186 - loglik: -1.2579e+02 - logprior: -1.3277e+00
Epoch 6/10
19/19 - 1s - loss: 126.8559 - loglik: -1.2556e+02 - logprior: -1.2991e+00
Epoch 7/10
19/19 - 1s - loss: 126.8675 - loglik: -1.2558e+02 - logprior: -1.2899e+00
Fitted a model with MAP estimate = -126.7147
expansions: [(11, 2), (12, 3), (13, 4), (14, 2), (18, 1), (30, 2), (34, 3), (36, 2)]
discards: [0]
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.8588 - loglik: -1.2977e+02 - logprior: -4.0853e+00
Epoch 2/2
19/19 - 1s - loss: 127.0532 - loglik: -1.2487e+02 - logprior: -2.1872e+00
Fitted a model with MAP estimate = -124.5696
expansions: []
discards: [13 14 42 48 52]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.4493 - loglik: -1.2311e+02 - logprior: -3.3370e+00
Epoch 2/2
19/19 - 1s - loss: 123.3733 - loglik: -1.2195e+02 - logprior: -1.4243e+00
Fitted a model with MAP estimate = -122.8778
expansions: []
discards: [11]
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.6993 - loglik: -1.2250e+02 - logprior: -3.2034e+00
Epoch 2/10
19/19 - 1s - loss: 123.2483 - loglik: -1.2186e+02 - logprior: -1.3850e+00
Epoch 3/10
19/19 - 1s - loss: 122.6900 - loglik: -1.2139e+02 - logprior: -1.3013e+00
Epoch 4/10
19/19 - 1s - loss: 122.5001 - loglik: -1.2126e+02 - logprior: -1.2426e+00
Epoch 5/10
19/19 - 1s - loss: 122.3635 - loglik: -1.2115e+02 - logprior: -1.2110e+00
Epoch 6/10
19/19 - 1s - loss: 122.2828 - loglik: -1.2109e+02 - logprior: -1.1879e+00
Epoch 7/10
19/19 - 1s - loss: 122.2450 - loglik: -1.2107e+02 - logprior: -1.1785e+00
Epoch 8/10
19/19 - 1s - loss: 122.4488 - loglik: -1.2129e+02 - logprior: -1.1539e+00
Fitted a model with MAP estimate = -122.0939
Time for alignment: 40.6044
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.7246 - loglik: -1.6153e+02 - logprior: -3.1925e+00
Epoch 2/10
19/19 - 1s - loss: 135.8198 - loglik: -1.3443e+02 - logprior: -1.3856e+00
Epoch 3/10
19/19 - 1s - loss: 128.6685 - loglik: -1.2721e+02 - logprior: -1.4584e+00
Epoch 4/10
19/19 - 1s - loss: 127.1034 - loglik: -1.2580e+02 - logprior: -1.3037e+00
Epoch 5/10
19/19 - 1s - loss: 126.7105 - loglik: -1.2541e+02 - logprior: -1.3005e+00
Epoch 6/10
19/19 - 1s - loss: 126.5184 - loglik: -1.2524e+02 - logprior: -1.2793e+00
Epoch 7/10
19/19 - 1s - loss: 126.4329 - loglik: -1.2517e+02 - logprior: -1.2673e+00
Epoch 8/10
19/19 - 1s - loss: 126.5085 - loglik: -1.2525e+02 - logprior: -1.2591e+00
Fitted a model with MAP estimate = -126.3555
expansions: [(11, 5), (12, 3), (13, 1), (28, 2), (30, 2), (34, 3), (36, 2)]
discards: [0]
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.7654 - loglik: -1.2968e+02 - logprior: -4.0868e+00
Epoch 2/2
19/19 - 1s - loss: 126.8606 - loglik: -1.2464e+02 - logprior: -2.2189e+00
Fitted a model with MAP estimate = -124.4683
expansions: []
discards: [12 13 16 36 41 47 51]
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.7776 - loglik: -1.2346e+02 - logprior: -3.3131e+00
Epoch 2/2
19/19 - 1s - loss: 123.4408 - loglik: -1.2204e+02 - logprior: -1.4001e+00
Fitted a model with MAP estimate = -123.0724
expansions: []
discards: []
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.8135 - loglik: -1.2262e+02 - logprior: -3.1956e+00
Epoch 2/10
19/19 - 1s - loss: 123.4030 - loglik: -1.2202e+02 - logprior: -1.3810e+00
Epoch 3/10
19/19 - 1s - loss: 123.0211 - loglik: -1.2173e+02 - logprior: -1.2919e+00
Epoch 4/10
19/19 - 1s - loss: 122.7162 - loglik: -1.2148e+02 - logprior: -1.2390e+00
Epoch 5/10
19/19 - 1s - loss: 122.5876 - loglik: -1.2138e+02 - logprior: -1.2076e+00
Epoch 6/10
19/19 - 1s - loss: 122.5224 - loglik: -1.2134e+02 - logprior: -1.1850e+00
Epoch 7/10
19/19 - 1s - loss: 122.2684 - loglik: -1.2109e+02 - logprior: -1.1754e+00
Epoch 8/10
19/19 - 1s - loss: 122.5940 - loglik: -1.2143e+02 - logprior: -1.1600e+00
Fitted a model with MAP estimate = -122.3294
Time for alignment: 41.5811
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 165.0427 - loglik: -1.6185e+02 - logprior: -3.1927e+00
Epoch 2/10
19/19 - 1s - loss: 136.9610 - loglik: -1.3556e+02 - logprior: -1.4000e+00
Epoch 3/10
19/19 - 1s - loss: 128.5196 - loglik: -1.2702e+02 - logprior: -1.5013e+00
Epoch 4/10
19/19 - 1s - loss: 127.0666 - loglik: -1.2573e+02 - logprior: -1.3354e+00
Epoch 5/10
19/19 - 1s - loss: 126.6077 - loglik: -1.2528e+02 - logprior: -1.3282e+00
Epoch 6/10
19/19 - 1s - loss: 126.1380 - loglik: -1.2483e+02 - logprior: -1.3039e+00
Epoch 7/10
19/19 - 1s - loss: 126.2365 - loglik: -1.2495e+02 - logprior: -1.2905e+00
Fitted a model with MAP estimate = -126.1292
expansions: [(11, 2), (12, 3), (13, 4), (14, 2), (26, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Fitting a model of length 63 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.2334 - loglik: -1.3015e+02 - logprior: -4.0872e+00
Epoch 2/2
19/19 - 1s - loss: 127.0987 - loglik: -1.2488e+02 - logprior: -2.2163e+00
Fitted a model with MAP estimate = -124.6037
expansions: []
discards: [11 13 14 36 43 44 53]
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.5843 - loglik: -1.2325e+02 - logprior: -3.3376e+00
Epoch 2/2
19/19 - 1s - loss: 123.2841 - loglik: -1.2187e+02 - logprior: -1.4107e+00
Fitted a model with MAP estimate = -122.8651
expansions: []
discards: []
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.5867 - loglik: -1.2238e+02 - logprior: -3.2019e+00
Epoch 2/10
19/19 - 1s - loss: 123.1715 - loglik: -1.2178e+02 - logprior: -1.3896e+00
Epoch 3/10
19/19 - 1s - loss: 122.5302 - loglik: -1.2123e+02 - logprior: -1.2992e+00
Epoch 4/10
19/19 - 1s - loss: 122.3657 - loglik: -1.2112e+02 - logprior: -1.2429e+00
Epoch 5/10
19/19 - 1s - loss: 122.4570 - loglik: -1.2125e+02 - logprior: -1.2082e+00
Fitted a model with MAP estimate = -122.0975
Time for alignment: 36.9246
Computed alignments with likelihoods: ['-122.0939', '-122.3294', '-122.0975']
Best model has likelihood: -122.0939
SP score = 0.8855
Training of 3 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4853173bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4885ed8d00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 900.3259 - loglik: -8.9889e+02 - logprior: -1.4409e+00
Epoch 2/10
39/39 - 42s - loss: 720.0950 - loglik: -7.1904e+02 - logprior: -1.0518e+00
Epoch 3/10
39/39 - 42s - loss: 707.8793 - loglik: -7.0685e+02 - logprior: -1.0267e+00
Epoch 4/10
39/39 - 42s - loss: 706.5142 - loglik: -7.0557e+02 - logprior: -9.4079e-01
Epoch 5/10
39/39 - 42s - loss: 705.1439 - loglik: -7.0423e+02 - logprior: -9.0948e-01
Epoch 6/10
39/39 - 42s - loss: 704.0868 - loglik: -7.0317e+02 - logprior: -9.1457e-01
Epoch 7/10
39/39 - 42s - loss: 704.1078 - loglik: -7.0319e+02 - logprior: -9.2248e-01
Fitted a model with MAP estimate = -703.3419
expansions: [(0, 3), (38, 1), (145, 1), (165, 1), (166, 1), (178, 1), (179, 8), (180, 1), (190, 1), (192, 2), (193, 6), (194, 1), (197, 1), (198, 1), (199, 1), (200, 2), (201, 1), (202, 2), (203, 2), (204, 1), (209, 1), (212, 2), (215, 1), (219, 1), (223, 2), (224, 5), (225, 3), (227, 1), (228, 2), (229, 4), (240, 1), (241, 1), (243, 1), (244, 1), (245, 3), (246, 2), (248, 2), (249, 5), (254, 1), (255, 1), (268, 1), (272, 1), (287, 1), (288, 2), (289, 4), (291, 1), (302, 1), (304, 4), (312, 1), (326, 1), (339, 1), (341, 1), (355, 6)]
discards: [1]
Fitting a model of length 457 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 676.8257 - loglik: -6.7469e+02 - logprior: -2.1334e+00
Epoch 2/2
39/39 - 64s - loss: 658.5750 - loglik: -6.5790e+02 - logprior: -6.7333e-01
Fitted a model with MAP estimate = -656.1775
expansions: [(283, 1), (285, 1), (392, 1)]
discards: [  2 187 188 189 190 237 272 274 313 314 315 316 455]
Fitting a model of length 447 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 663.2934 - loglik: -6.6200e+02 - logprior: -1.2895e+00
Epoch 2/2
39/39 - 61s - loss: 658.6851 - loglik: -6.5844e+02 - logprior: -2.4890e-01
Fitted a model with MAP estimate = -656.9269
expansions: [(304, 2), (305, 1)]
discards: []
Fitting a model of length 450 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 661.0578 - loglik: -6.5995e+02 - logprior: -1.1041e+00
Epoch 2/10
39/39 - 62s - loss: 657.8383 - loglik: -6.5788e+02 - logprior: 0.0414
Epoch 3/10
39/39 - 62s - loss: 656.6136 - loglik: -6.5675e+02 - logprior: 0.1369
Epoch 4/10
39/39 - 62s - loss: 655.1469 - loglik: -6.5538e+02 - logprior: 0.2361
Epoch 5/10
39/39 - 62s - loss: 654.3681 - loglik: -6.5488e+02 - logprior: 0.5164
Epoch 6/10
39/39 - 62s - loss: 653.9011 - loglik: -6.5458e+02 - logprior: 0.6804
Epoch 7/10
39/39 - 62s - loss: 654.3951 - loglik: -6.5509e+02 - logprior: 0.6903
Fitted a model with MAP estimate = -653.0235
Time for alignment: 1280.4300
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 902.5884 - loglik: -9.0107e+02 - logprior: -1.5196e+00
Epoch 2/10
39/39 - 42s - loss: 721.5108 - loglik: -7.1996e+02 - logprior: -1.5534e+00
Epoch 3/10
39/39 - 42s - loss: 708.8340 - loglik: -7.0722e+02 - logprior: -1.6098e+00
Epoch 4/10
39/39 - 42s - loss: 705.8154 - loglik: -7.0424e+02 - logprior: -1.5750e+00
Epoch 5/10
39/39 - 42s - loss: 704.9054 - loglik: -7.0312e+02 - logprior: -1.7879e+00
Epoch 6/10
39/39 - 42s - loss: 704.1859 - loglik: -7.0238e+02 - logprior: -1.8084e+00
Epoch 7/10
39/39 - 42s - loss: 704.3980 - loglik: -7.0284e+02 - logprior: -1.5534e+00
Fitted a model with MAP estimate = -703.0919
expansions: [(0, 3), (37, 1), (38, 1), (49, 1), (134, 1), (136, 1), (147, 1), (163, 1), (164, 1), (175, 8), (176, 1), (177, 1), (178, 1), (186, 1), (188, 2), (189, 5), (190, 1), (191, 1), (194, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 2), (200, 2), (201, 1), (204, 1), (206, 1), (208, 1), (212, 1), (215, 1), (220, 2), (221, 7), (224, 1), (225, 2), (226, 3), (227, 2), (239, 1), (243, 1), (244, 3), (245, 2), (247, 2), (248, 3), (249, 3), (251, 1), (253, 1), (254, 1), (268, 1), (269, 1), (284, 1), (285, 1), (287, 2), (288, 2), (290, 1), (291, 1), (303, 2), (304, 2), (312, 1), (325, 2), (328, 1), (345, 1), (347, 1), (349, 1), (350, 1), (352, 1)]
discards: [  1   2   3 127]
Fitting a model of length 457 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 677.0660 - loglik: -6.7510e+02 - logprior: -1.9644e+00
Epoch 2/2
39/39 - 64s - loss: 658.5380 - loglik: -6.5787e+02 - logprior: -6.6819e-01
Fitted a model with MAP estimate = -655.6023
expansions: [(215, 1), (280, 1)]
discards: [185 186 211 212 236 271 312 313 314 315 370]
Fitting a model of length 448 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 663.6265 - loglik: -6.6228e+02 - logprior: -1.3447e+00
Epoch 2/2
39/39 - 61s - loss: 659.4276 - loglik: -6.5917e+02 - logprior: -2.5339e-01
Fitted a model with MAP estimate = -657.5258
expansions: [(305, 2), (306, 1)]
discards: [2]
Fitting a model of length 450 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 662.1321 - loglik: -6.6096e+02 - logprior: -1.1748e+00
Epoch 2/10
39/39 - 62s - loss: 658.4974 - loglik: -6.5849e+02 - logprior: -6.5587e-03
Epoch 3/10
39/39 - 62s - loss: 657.2713 - loglik: -6.5742e+02 - logprior: 0.1476
Epoch 4/10
39/39 - 62s - loss: 655.6815 - loglik: -6.5600e+02 - logprior: 0.3178
Epoch 5/10
39/39 - 62s - loss: 654.7708 - loglik: -6.5514e+02 - logprior: 0.3678
Epoch 6/10
39/39 - 62s - loss: 653.6944 - loglik: -6.5428e+02 - logprior: 0.5873
Epoch 7/10
39/39 - 62s - loss: 654.8837 - loglik: -6.5557e+02 - logprior: 0.6824
Fitted a model with MAP estimate = -653.3708
Time for alignment: 1283.0249
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 899.0495 - loglik: -8.9758e+02 - logprior: -1.4729e+00
Epoch 2/10
39/39 - 42s - loss: 721.1456 - loglik: -7.1990e+02 - logprior: -1.2482e+00
Epoch 3/10
39/39 - 42s - loss: 708.1427 - loglik: -7.0665e+02 - logprior: -1.4931e+00
Epoch 4/10
39/39 - 42s - loss: 706.3366 - loglik: -7.0477e+02 - logprior: -1.5713e+00
Epoch 5/10
39/39 - 42s - loss: 704.7752 - loglik: -7.0316e+02 - logprior: -1.6201e+00
Epoch 6/10
39/39 - 42s - loss: 703.9091 - loglik: -7.0227e+02 - logprior: -1.6374e+00
Epoch 7/10
39/39 - 42s - loss: 704.1880 - loglik: -7.0265e+02 - logprior: -1.5347e+00
Fitted a model with MAP estimate = -702.9427
expansions: [(37, 1), (43, 1), (134, 1), (164, 1), (166, 1), (170, 1), (177, 3), (178, 7), (179, 1), (183, 1), (191, 2), (192, 6), (193, 1), (196, 1), (197, 1), (198, 1), (199, 2), (200, 1), (201, 2), (202, 2), (203, 1), (206, 1), (208, 1), (210, 2), (217, 1), (221, 2), (222, 7), (225, 1), (226, 2), (227, 3), (228, 2), (241, 1), (245, 2), (246, 2), (247, 3), (249, 2), (250, 5), (252, 1), (254, 1), (255, 1), (268, 1), (274, 1), (287, 1), (289, 1), (290, 4), (291, 1), (292, 1), (301, 1), (303, 2), (304, 2), (306, 2), (325, 1), (329, 1), (338, 1), (348, 1), (351, 1), (352, 2)]
discards: [2]
Fitting a model of length 456 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 66s - loss: 677.1749 - loglik: -6.7532e+02 - logprior: -1.8545e+00
Epoch 2/2
39/39 - 64s - loss: 659.5874 - loglik: -6.5893e+02 - logprior: -6.5898e-01
Fitted a model with MAP estimate = -656.5040
expansions: [(249, 1), (279, 1), (453, 1)]
discards: [185 186 187 188 189 236 270 285 312 313 314 315 316 399]
Fitting a model of length 445 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 663.9001 - loglik: -6.6249e+02 - logprior: -1.4073e+00
Epoch 2/2
39/39 - 61s - loss: 659.4573 - loglik: -6.5930e+02 - logprior: -1.6008e-01
Fitted a model with MAP estimate = -657.5804
expansions: [(210, 2), (303, 1)]
discards: []
Fitting a model of length 448 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 661.1222 - loglik: -6.5992e+02 - logprior: -1.2007e+00
Epoch 2/10
39/39 - 62s - loss: 657.1141 - loglik: -6.5728e+02 - logprior: 0.1626
Epoch 3/10
39/39 - 62s - loss: 656.1600 - loglik: -6.5639e+02 - logprior: 0.2269
Epoch 4/10
39/39 - 62s - loss: 655.3181 - loglik: -6.5561e+02 - logprior: 0.2893
Epoch 5/10
39/39 - 62s - loss: 653.3920 - loglik: -6.5397e+02 - logprior: 0.5791
Epoch 6/10
39/39 - 62s - loss: 653.1390 - loglik: -6.5391e+02 - logprior: 0.7708
Epoch 7/10
39/39 - 62s - loss: 653.0970 - loglik: -6.5402e+02 - logprior: 0.9222
Epoch 8/10
39/39 - 62s - loss: 653.1418 - loglik: -6.5408e+02 - logprior: 0.9423
Fitted a model with MAP estimate = -652.3683
Time for alignment: 1338.0126
Computed alignments with likelihoods: ['-653.0235', '-653.3708', '-652.3683']
Best model has likelihood: -652.3683
SP score = 0.8704
Training of 3 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f487d15bac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f486410de20>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 409.3115 - loglik: -4.0640e+02 - logprior: -2.9105e+00
Epoch 2/10
19/19 - 3s - loss: 326.5154 - loglik: -3.2554e+02 - logprior: -9.7257e-01
Epoch 3/10
19/19 - 3s - loss: 296.9076 - loglik: -2.9578e+02 - logprior: -1.1305e+00
Epoch 4/10
19/19 - 3s - loss: 292.5118 - loglik: -2.9144e+02 - logprior: -1.0674e+00
Epoch 5/10
19/19 - 3s - loss: 291.1124 - loglik: -2.9009e+02 - logprior: -1.0249e+00
Epoch 6/10
19/19 - 3s - loss: 290.4977 - loglik: -2.8950e+02 - logprior: -1.0013e+00
Epoch 7/10
19/19 - 3s - loss: 289.8280 - loglik: -2.8885e+02 - logprior: -9.7740e-01
Epoch 8/10
19/19 - 3s - loss: 289.9210 - loglik: -2.8896e+02 - logprior: -9.6582e-01
Fitted a model with MAP estimate = -289.3675
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 1), (60, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (97, 1), (98, 1), (99, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 290.2681 - loglik: -2.8641e+02 - logprior: -3.8623e+00
Epoch 2/2
19/19 - 4s - loss: 278.0162 - loglik: -2.7694e+02 - logprior: -1.0744e+00
Fitted a model with MAP estimate = -276.1997
expansions: [(0, 4)]
discards: [  1   2   3   4   5  22  74 100 101 147]
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 283.5238 - loglik: -2.7972e+02 - logprior: -3.8063e+00
Epoch 2/2
19/19 - 4s - loss: 278.1980 - loglik: -2.7709e+02 - logprior: -1.1074e+00
Fitted a model with MAP estimate = -276.7486
expansions: [(0, 5)]
discards: [0 1 2 4]
Fitting a model of length 153 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 280.9968 - loglik: -2.7799e+02 - logprior: -3.0094e+00
Epoch 2/10
19/19 - 4s - loss: 277.0286 - loglik: -2.7613e+02 - logprior: -8.9739e-01
Epoch 3/10
19/19 - 4s - loss: 276.0414 - loglik: -2.7538e+02 - logprior: -6.5822e-01
Epoch 4/10
19/19 - 4s - loss: 275.1172 - loglik: -2.7452e+02 - logprior: -5.9794e-01
Epoch 5/10
19/19 - 4s - loss: 274.8129 - loglik: -2.7430e+02 - logprior: -5.1171e-01
Epoch 6/10
19/19 - 4s - loss: 274.1423 - loglik: -2.7366e+02 - logprior: -4.7751e-01
Epoch 7/10
19/19 - 4s - loss: 274.1501 - loglik: -2.7371e+02 - logprior: -4.4283e-01
Fitted a model with MAP estimate = -273.5277
Time for alignment: 112.9213
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 408.8493 - loglik: -4.0594e+02 - logprior: -2.9142e+00
Epoch 2/10
19/19 - 3s - loss: 326.8993 - loglik: -3.2591e+02 - logprior: -9.8813e-01
Epoch 3/10
19/19 - 3s - loss: 297.3173 - loglik: -2.9621e+02 - logprior: -1.1122e+00
Epoch 4/10
19/19 - 3s - loss: 293.2357 - loglik: -2.9220e+02 - logprior: -1.0348e+00
Epoch 5/10
19/19 - 3s - loss: 291.9047 - loglik: -2.9091e+02 - logprior: -9.9589e-01
Epoch 6/10
19/19 - 3s - loss: 290.8043 - loglik: -2.8984e+02 - logprior: -9.6630e-01
Epoch 7/10
19/19 - 3s - loss: 290.2226 - loglik: -2.8928e+02 - logprior: -9.4018e-01
Epoch 8/10
19/19 - 3s - loss: 290.3476 - loglik: -2.8941e+02 - logprior: -9.3829e-01
Fitted a model with MAP estimate = -289.8441
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 2), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Fitting a model of length 160 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 290.5916 - loglik: -2.8673e+02 - logprior: -3.8597e+00
Epoch 2/2
19/19 - 4s - loss: 278.3304 - loglik: -2.7722e+02 - logprior: -1.1065e+00
Fitted a model with MAP estimate = -276.3088
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  75  76  80 102 103 149]
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 283.6470 - loglik: -2.7986e+02 - logprior: -3.7906e+00
Epoch 2/2
19/19 - 4s - loss: 278.4602 - loglik: -2.7740e+02 - logprior: -1.0571e+00
Fitted a model with MAP estimate = -276.8886
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 281.1661 - loglik: -2.7818e+02 - logprior: -2.9855e+00
Epoch 2/10
19/19 - 4s - loss: 277.1746 - loglik: -2.7633e+02 - logprior: -8.4065e-01
Epoch 3/10
19/19 - 4s - loss: 276.3233 - loglik: -2.7566e+02 - logprior: -6.5948e-01
Epoch 4/10
19/19 - 4s - loss: 275.7836 - loglik: -2.7523e+02 - logprior: -5.4981e-01
Epoch 5/10
19/19 - 4s - loss: 275.1012 - loglik: -2.7463e+02 - logprior: -4.6845e-01
Epoch 6/10
19/19 - 4s - loss: 274.0757 - loglik: -2.7365e+02 - logprior: -4.2927e-01
Epoch 7/10
19/19 - 4s - loss: 274.4658 - loglik: -2.7407e+02 - logprior: -3.9904e-01
Fitted a model with MAP estimate = -273.8672
Time for alignment: 111.2276
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 408.7855 - loglik: -4.0587e+02 - logprior: -2.9135e+00
Epoch 2/10
19/19 - 3s - loss: 328.9620 - loglik: -3.2797e+02 - logprior: -9.8935e-01
Epoch 3/10
19/19 - 3s - loss: 299.5351 - loglik: -2.9841e+02 - logprior: -1.1228e+00
Epoch 4/10
19/19 - 3s - loss: 292.9695 - loglik: -2.9191e+02 - logprior: -1.0582e+00
Epoch 5/10
19/19 - 3s - loss: 291.5008 - loglik: -2.9047e+02 - logprior: -1.0298e+00
Epoch 6/10
19/19 - 3s - loss: 290.6808 - loglik: -2.8968e+02 - logprior: -1.0017e+00
Epoch 7/10
19/19 - 3s - loss: 290.2265 - loglik: -2.8925e+02 - logprior: -9.8131e-01
Epoch 8/10
19/19 - 3s - loss: 290.0149 - loglik: -2.8904e+02 - logprior: -9.7186e-01
Epoch 9/10
19/19 - 3s - loss: 289.4517 - loglik: -2.8849e+02 - logprior: -9.6252e-01
Epoch 10/10
19/19 - 3s - loss: 290.1287 - loglik: -2.8916e+02 - logprior: -9.6644e-01
Fitted a model with MAP estimate = -289.2399
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 2), (35, 1), (37, 1), (49, 1), (52, 1), (54, 2), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (99, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Fitting a model of length 159 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 290.1449 - loglik: -2.8619e+02 - logprior: -3.9581e+00
Epoch 2/2
19/19 - 4s - loss: 277.8062 - loglik: -2.7669e+02 - logprior: -1.1113e+00
Fitted a model with MAP estimate = -275.9564
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  49  74 101 102 148]
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 283.6830 - loglik: -2.7986e+02 - logprior: -3.8269e+00
Epoch 2/2
19/19 - 4s - loss: 278.7667 - loglik: -2.7767e+02 - logprior: -1.1010e+00
Fitted a model with MAP estimate = -277.0127
expansions: [(0, 5)]
discards: [0 1 2 4]
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 281.4199 - loglik: -2.7838e+02 - logprior: -3.0415e+00
Epoch 2/10
19/19 - 4s - loss: 277.2992 - loglik: -2.7638e+02 - logprior: -9.2137e-01
Epoch 3/10
19/19 - 4s - loss: 276.3545 - loglik: -2.7559e+02 - logprior: -7.6921e-01
Epoch 4/10
19/19 - 4s - loss: 275.5818 - loglik: -2.7493e+02 - logprior: -6.5340e-01
Epoch 5/10
19/19 - 4s - loss: 274.9714 - loglik: -2.7440e+02 - logprior: -5.7348e-01
Epoch 6/10
19/19 - 4s - loss: 274.2368 - loglik: -2.7372e+02 - logprior: -5.1384e-01
Epoch 7/10
19/19 - 4s - loss: 273.6500 - loglik: -2.7318e+02 - logprior: -4.7484e-01
Epoch 8/10
19/19 - 4s - loss: 273.6720 - loglik: -2.7322e+02 - logprior: -4.5114e-01
Fitted a model with MAP estimate = -273.4914
Time for alignment: 122.3981
Computed alignments with likelihoods: ['-273.5277', '-273.8672', '-273.4914']
Best model has likelihood: -273.4914
SP score = 0.6772
Training of 3 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47fdd70cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f487d0e47f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.8869 - loglik: -6.5530e+01 - logprior: -3.3565e+00
Epoch 2/10
19/19 - 0s - loss: 52.1778 - loglik: -5.0731e+01 - logprior: -1.4467e+00
Epoch 3/10
19/19 - 0s - loss: 46.9847 - loglik: -4.5504e+01 - logprior: -1.4802e+00
Epoch 4/10
19/19 - 0s - loss: 44.9082 - loglik: -4.3394e+01 - logprior: -1.5146e+00
Epoch 5/10
19/19 - 0s - loss: 44.4727 - loglik: -4.2976e+01 - logprior: -1.4963e+00
Epoch 6/10
19/19 - 0s - loss: 44.2582 - loglik: -4.2773e+01 - logprior: -1.4854e+00
Epoch 7/10
19/19 - 0s - loss: 44.2821 - loglik: -4.2806e+01 - logprior: -1.4757e+00
Fitted a model with MAP estimate = -44.1953
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 46.9929 - loglik: -4.2460e+01 - logprior: -4.5327e+00
Epoch 2/2
19/19 - 0s - loss: 42.0668 - loglik: -4.0729e+01 - logprior: -1.3379e+00
Fitted a model with MAP estimate = -41.3703
expansions: [(2, 1)]
discards: [0]
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.2429 - loglik: -4.0926e+01 - logprior: -3.3166e+00
Epoch 2/2
19/19 - 0s - loss: 41.8579 - loglik: -4.0339e+01 - logprior: -1.5186e+00
Fitted a model with MAP estimate = -41.5007
expansions: []
discards: []
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.9179 - loglik: -4.0660e+01 - logprior: -3.2580e+00
Epoch 2/10
19/19 - 0s - loss: 41.7472 - loglik: -4.0251e+01 - logprior: -1.4960e+00
Epoch 3/10
19/19 - 0s - loss: 41.3087 - loglik: -3.9901e+01 - logprior: -1.4081e+00
Epoch 4/10
19/19 - 0s - loss: 41.2084 - loglik: -3.9854e+01 - logprior: -1.3543e+00
Epoch 5/10
19/19 - 0s - loss: 41.1050 - loglik: -3.9775e+01 - logprior: -1.3303e+00
Epoch 6/10
19/19 - 1s - loss: 41.0196 - loglik: -3.9706e+01 - logprior: -1.3138e+00
Epoch 7/10
19/19 - 0s - loss: 40.9322 - loglik: -3.9631e+01 - logprior: -1.3016e+00
Epoch 8/10
19/19 - 0s - loss: 41.0630 - loglik: -3.9772e+01 - logprior: -1.2908e+00
Fitted a model with MAP estimate = -40.9383
Time for alignment: 26.1296
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.8867 - loglik: -6.5534e+01 - logprior: -3.3531e+00
Epoch 2/10
19/19 - 0s - loss: 52.2481 - loglik: -5.0797e+01 - logprior: -1.4506e+00
Epoch 3/10
19/19 - 0s - loss: 46.4983 - loglik: -4.4990e+01 - logprior: -1.5079e+00
Epoch 4/10
19/19 - 0s - loss: 44.8100 - loglik: -4.3283e+01 - logprior: -1.5272e+00
Epoch 5/10
19/19 - 0s - loss: 44.4501 - loglik: -4.2949e+01 - logprior: -1.5012e+00
Epoch 6/10
19/19 - 0s - loss: 44.3117 - loglik: -4.2823e+01 - logprior: -1.4884e+00
Epoch 7/10
19/19 - 0s - loss: 44.2372 - loglik: -4.2762e+01 - logprior: -1.4755e+00
Epoch 8/10
19/19 - 0s - loss: 44.1951 - loglik: -4.2728e+01 - logprior: -1.4669e+00
Epoch 9/10
19/19 - 0s - loss: 44.1442 - loglik: -4.2686e+01 - logprior: -1.4584e+00
Epoch 10/10
19/19 - 0s - loss: 44.1748 - loglik: -4.2717e+01 - logprior: -1.4578e+00
Fitted a model with MAP estimate = -44.1133
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.1348 - loglik: -4.2440e+01 - logprior: -4.6949e+00
Epoch 2/2
19/19 - 0s - loss: 42.1721 - loglik: -4.0825e+01 - logprior: -1.3475e+00
Fitted a model with MAP estimate = -41.3938
expansions: [(2, 1)]
discards: [0]
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.2605 - loglik: -4.0953e+01 - logprior: -3.3074e+00
Epoch 2/2
19/19 - 0s - loss: 41.7903 - loglik: -4.0277e+01 - logprior: -1.5130e+00
Fitted a model with MAP estimate = -41.5018
expansions: []
discards: []
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.9220 - loglik: -4.0673e+01 - logprior: -3.2491e+00
Epoch 2/10
19/19 - 0s - loss: 41.7436 - loglik: -4.0250e+01 - logprior: -1.4935e+00
Epoch 3/10
19/19 - 0s - loss: 41.3893 - loglik: -3.9981e+01 - logprior: -1.4078e+00
Epoch 4/10
19/19 - 0s - loss: 41.1836 - loglik: -3.9829e+01 - logprior: -1.3546e+00
Epoch 5/10
19/19 - 0s - loss: 41.1321 - loglik: -3.9802e+01 - logprior: -1.3300e+00
Epoch 6/10
19/19 - 0s - loss: 40.9710 - loglik: -3.9660e+01 - logprior: -1.3106e+00
Epoch 7/10
19/19 - 0s - loss: 40.9379 - loglik: -3.9637e+01 - logprior: -1.3005e+00
Epoch 8/10
19/19 - 0s - loss: 41.0553 - loglik: -3.9768e+01 - logprior: -1.2876e+00
Fitted a model with MAP estimate = -40.9376
Time for alignment: 27.0590
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 68.9603 - loglik: -6.5602e+01 - logprior: -3.3581e+00
Epoch 2/10
19/19 - 0s - loss: 52.3631 - loglik: -5.0906e+01 - logprior: -1.4572e+00
Epoch 3/10
19/19 - 0s - loss: 46.2928 - loglik: -4.4767e+01 - logprior: -1.5256e+00
Epoch 4/10
19/19 - 0s - loss: 44.7610 - loglik: -4.3233e+01 - logprior: -1.5278e+00
Epoch 5/10
19/19 - 0s - loss: 44.4183 - loglik: -4.2920e+01 - logprior: -1.4980e+00
Epoch 6/10
19/19 - 0s - loss: 44.3452 - loglik: -4.2857e+01 - logprior: -1.4877e+00
Epoch 7/10
19/19 - 0s - loss: 44.1776 - loglik: -4.2701e+01 - logprior: -1.4764e+00
Epoch 8/10
19/19 - 0s - loss: 44.1793 - loglik: -4.2715e+01 - logprior: -1.4648e+00
Fitted a model with MAP estimate = -44.1553
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.0095 - loglik: -4.2431e+01 - logprior: -4.5788e+00
Epoch 2/2
19/19 - 0s - loss: 42.1144 - loglik: -4.0781e+01 - logprior: -1.3338e+00
Fitted a model with MAP estimate = -41.3689
expansions: [(2, 1)]
discards: [0]
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.2305 - loglik: -4.0923e+01 - logprior: -3.3071e+00
Epoch 2/2
19/19 - 1s - loss: 41.8262 - loglik: -4.0304e+01 - logprior: -1.5226e+00
Fitted a model with MAP estimate = -41.4998
expansions: []
discards: []
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.9357 - loglik: -4.0668e+01 - logprior: -3.2673e+00
Epoch 2/10
19/19 - 0s - loss: 41.7129 - loglik: -4.0217e+01 - logprior: -1.4957e+00
Epoch 3/10
19/19 - 0s - loss: 41.3350 - loglik: -3.9930e+01 - logprior: -1.4050e+00
Epoch 4/10
19/19 - 0s - loss: 41.2462 - loglik: -3.9889e+01 - logprior: -1.3569e+00
Epoch 5/10
19/19 - 0s - loss: 41.0910 - loglik: -3.9764e+01 - logprior: -1.3274e+00
Epoch 6/10
19/19 - 0s - loss: 40.9641 - loglik: -3.9647e+01 - logprior: -1.3166e+00
Epoch 7/10
19/19 - 0s - loss: 41.0703 - loglik: -3.9769e+01 - logprior: -1.3008e+00
Fitted a model with MAP estimate = -40.9568
Time for alignment: 25.4345
Computed alignments with likelihoods: ['-40.9383', '-40.9376', '-40.9568']
Best model has likelihood: -40.9376
SP score = 0.9109
Training of 3 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f486ca4a850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f486374dc40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 734.9134 - loglik: -7.3338e+02 - logprior: -1.5318e+00
Epoch 2/10
39/39 - 21s - loss: 529.9096 - loglik: -5.2834e+02 - logprior: -1.5674e+00
Epoch 3/10
39/39 - 21s - loss: 516.2490 - loglik: -5.1466e+02 - logprior: -1.5857e+00
Epoch 4/10
39/39 - 21s - loss: 513.7884 - loglik: -5.1229e+02 - logprior: -1.4946e+00
Epoch 5/10
39/39 - 21s - loss: 512.6500 - loglik: -5.1117e+02 - logprior: -1.4807e+00
Epoch 6/10
39/39 - 21s - loss: 512.1016 - loglik: -5.1062e+02 - logprior: -1.4862e+00
Epoch 7/10
39/39 - 21s - loss: 512.0740 - loglik: -5.1058e+02 - logprior: -1.4937e+00
Epoch 8/10
39/39 - 21s - loss: 511.7465 - loglik: -5.1025e+02 - logprior: -1.4976e+00
Epoch 9/10
39/39 - 21s - loss: 511.6959 - loglik: -5.1019e+02 - logprior: -1.5087e+00
Epoch 10/10
39/39 - 21s - loss: 511.6622 - loglik: -5.1016e+02 - logprior: -1.5035e+00
Fitted a model with MAP estimate = -510.8997
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (63, 1), (67, 2), (68, 1), (70, 1), (77, 1), (79, 2), (82, 1), (83, 1), (84, 1), (85, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (133, 1), (134, 1), (137, 1), (145, 2), (147, 1), (148, 1), (159, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (180, 1), (182, 1), (188, 1), (189, 1), (209, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (227, 1), (230, 2), (231, 1), (244, 1), (259, 1), (265, 1), (267, 1), (270, 2), (272, 3), (273, 5)]
discards: []
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 483.6227 - loglik: -4.8166e+02 - logprior: -1.9619e+00
Epoch 2/2
39/39 - 30s - loss: 466.1610 - loglik: -4.6563e+02 - logprior: -5.2858e-01
Fitted a model with MAP estimate = -463.8338
expansions: [(0, 2)]
discards: [  0   1  82  98 179 214]
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 470.6171 - loglik: -4.6925e+02 - logprior: -1.3722e+00
Epoch 2/2
39/39 - 30s - loss: 465.7300 - loglik: -4.6551e+02 - logprior: -2.2086e-01
Fitted a model with MAP estimate = -463.7095
expansions: []
discards: [  0 342]
Fitting a model of length 341 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 471.9580 - loglik: -4.6998e+02 - logprior: -1.9769e+00
Epoch 2/10
39/39 - 29s - loss: 466.4362 - loglik: -4.6655e+02 - logprior: 0.1156
Epoch 3/10
39/39 - 30s - loss: 464.1456 - loglik: -4.6469e+02 - logprior: 0.5474
Epoch 4/10
39/39 - 30s - loss: 462.2306 - loglik: -4.6287e+02 - logprior: 0.6366
Epoch 5/10
39/39 - 30s - loss: 461.2408 - loglik: -4.6208e+02 - logprior: 0.8404
Epoch 6/10
39/39 - 30s - loss: 461.3891 - loglik: -4.6231e+02 - logprior: 0.9186
Fitted a model with MAP estimate = -460.3541
Time for alignment: 686.0918
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 735.4352 - loglik: -7.3390e+02 - logprior: -1.5317e+00
Epoch 2/10
39/39 - 21s - loss: 533.1523 - loglik: -5.3173e+02 - logprior: -1.4234e+00
Epoch 3/10
39/39 - 21s - loss: 521.9065 - loglik: -5.2054e+02 - logprior: -1.3628e+00
Epoch 4/10
39/39 - 21s - loss: 518.9247 - loglik: -5.1764e+02 - logprior: -1.2861e+00
Epoch 5/10
39/39 - 21s - loss: 517.9940 - loglik: -5.1671e+02 - logprior: -1.2837e+00
Epoch 6/10
39/39 - 21s - loss: 517.3322 - loglik: -5.1604e+02 - logprior: -1.2949e+00
Epoch 7/10
39/39 - 21s - loss: 516.5830 - loglik: -5.1528e+02 - logprior: -1.3061e+00
Epoch 8/10
39/39 - 21s - loss: 516.9470 - loglik: -5.1564e+02 - logprior: -1.3056e+00
Fitted a model with MAP estimate = -515.7549
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 2), (37, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (132, 3), (133, 1), (145, 2), (148, 1), (149, 1), (159, 1), (163, 1), (167, 1), (170, 1), (172, 2), (173, 1), (174, 1), (182, 3), (188, 1), (205, 1), (209, 1), (215, 2), (217, 2), (228, 1), (230, 3), (231, 1), (244, 1), (259, 1), (265, 1), (267, 2), (269, 1), (272, 3), (273, 5)]
discards: []
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 483.9683 - loglik: -4.8201e+02 - logprior: -1.9556e+00
Epoch 2/2
39/39 - 31s - loss: 466.1177 - loglik: -4.6544e+02 - logprior: -6.7498e-01
Fitted a model with MAP estimate = -463.8982
expansions: []
discards: [  0   1  97 163 179 214 229]
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 471.6965 - loglik: -4.7018e+02 - logprior: -1.5122e+00
Epoch 2/2
39/39 - 29s - loss: 466.9151 - loglik: -4.6695e+02 - logprior: 0.0302
Fitted a model with MAP estimate = -464.6209
expansions: [(0, 2)]
discards: []
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 469.4044 - loglik: -4.6803e+02 - logprior: -1.3703e+00
Epoch 2/10
39/39 - 30s - loss: 464.7942 - loglik: -4.6480e+02 - logprior: 0.0109
Epoch 3/10
39/39 - 30s - loss: 462.7595 - loglik: -4.6310e+02 - logprior: 0.3399
Epoch 4/10
39/39 - 30s - loss: 461.8936 - loglik: -4.6218e+02 - logprior: 0.2868
Epoch 5/10
39/39 - 30s - loss: 461.0324 - loglik: -4.6152e+02 - logprior: 0.4894
Epoch 6/10
39/39 - 30s - loss: 459.8570 - loglik: -4.6056e+02 - logprior: 0.7059
Epoch 7/10
39/39 - 30s - loss: 459.9964 - loglik: -4.6067e+02 - logprior: 0.6686
Fitted a model with MAP estimate = -459.5767
Time for alignment: 676.1694
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 734.6575 - loglik: -7.3310e+02 - logprior: -1.5528e+00
Epoch 2/10
39/39 - 21s - loss: 531.3068 - loglik: -5.2971e+02 - logprior: -1.6006e+00
Epoch 3/10
39/39 - 21s - loss: 518.3266 - loglik: -5.1677e+02 - logprior: -1.5565e+00
Epoch 4/10
39/39 - 21s - loss: 515.9265 - loglik: -5.1448e+02 - logprior: -1.4436e+00
Epoch 5/10
39/39 - 21s - loss: 515.1060 - loglik: -5.1369e+02 - logprior: -1.4143e+00
Epoch 6/10
39/39 - 21s - loss: 514.6499 - loglik: -5.1323e+02 - logprior: -1.4200e+00
Epoch 7/10
39/39 - 21s - loss: 514.0977 - loglik: -5.1267e+02 - logprior: -1.4295e+00
Epoch 8/10
39/39 - 21s - loss: 513.7481 - loglik: -5.1231e+02 - logprior: -1.4363e+00
Epoch 9/10
39/39 - 21s - loss: 514.0884 - loglik: -5.1264e+02 - logprior: -1.4442e+00
Fitted a model with MAP estimate = -513.1248
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (43, 1), (44, 1), (45, 2), (63, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (85, 1), (90, 1), (92, 1), (108, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (142, 1), (147, 1), (148, 1), (149, 1), (163, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (183, 1), (189, 1), (206, 1), (208, 1), (214, 2), (216, 3), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 2), (271, 1), (272, 2), (273, 4)]
discards: []
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 483.0447 - loglik: -4.8114e+02 - logprior: -1.9017e+00
Epoch 2/2
39/39 - 30s - loss: 466.2048 - loglik: -4.6573e+02 - logprior: -4.7082e-01
Fitted a model with MAP estimate = -463.5806
expansions: [(0, 2)]
discards: [  0   1  57  98 213]
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 469.7629 - loglik: -4.6837e+02 - logprior: -1.3909e+00
Epoch 2/2
39/39 - 30s - loss: 465.4704 - loglik: -4.6537e+02 - logprior: -9.5465e-02
Fitted a model with MAP estimate = -463.3638
expansions: []
discards: [0]
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 471.1542 - loglik: -4.6928e+02 - logprior: -1.8774e+00
Epoch 2/10
39/39 - 30s - loss: 466.0345 - loglik: -4.6601e+02 - logprior: -2.8969e-02
Epoch 3/10
39/39 - 30s - loss: 463.8465 - loglik: -4.6403e+02 - logprior: 0.1849
Epoch 4/10
39/39 - 30s - loss: 462.2297 - loglik: -4.6275e+02 - logprior: 0.5236
Epoch 5/10
39/39 - 30s - loss: 461.1447 - loglik: -4.6145e+02 - logprior: 0.3059
Epoch 6/10
39/39 - 30s - loss: 460.5284 - loglik: -4.6135e+02 - logprior: 0.8193
Epoch 7/10
39/39 - 30s - loss: 461.0058 - loglik: -4.6168e+02 - logprior: 0.6730
Fitted a model with MAP estimate = -460.0137
Time for alignment: 696.4038
Computed alignments with likelihoods: ['-460.3541', '-459.5767', '-460.0137']
Best model has likelihood: -459.5767
SP score = 0.9111
Training of 3 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f480f075a00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f48526f29d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 397.8373 - loglik: -3.9482e+02 - logprior: -3.0218e+00
Epoch 2/10
19/19 - 4s - loss: 327.7114 - loglik: -3.2648e+02 - logprior: -1.2336e+00
Epoch 3/10
19/19 - 4s - loss: 305.5714 - loglik: -3.0394e+02 - logprior: -1.6332e+00
Epoch 4/10
19/19 - 4s - loss: 300.5670 - loglik: -2.9910e+02 - logprior: -1.4648e+00
Epoch 5/10
19/19 - 4s - loss: 298.5232 - loglik: -2.9712e+02 - logprior: -1.4074e+00
Epoch 6/10
19/19 - 4s - loss: 297.9868 - loglik: -2.9662e+02 - logprior: -1.3668e+00
Epoch 7/10
19/19 - 4s - loss: 297.1702 - loglik: -2.9582e+02 - logprior: -1.3490e+00
Epoch 8/10
19/19 - 4s - loss: 297.3643 - loglik: -2.9604e+02 - logprior: -1.3262e+00
Fitted a model with MAP estimate = -294.9286
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (29, 2), (35, 1), (37, 2), (46, 3), (51, 1), (56, 1), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 306.0284 - loglik: -3.0197e+02 - logprior: -4.0543e+00
Epoch 2/2
19/19 - 5s - loss: 296.8246 - loglik: -2.9449e+02 - logprior: -2.3299e+00
Fitted a model with MAP estimate = -293.3461
expansions: [(0, 2)]
discards: [  0  23  30  36  48 136]
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 295.4151 - loglik: -2.9231e+02 - logprior: -3.1095e+00
Epoch 2/2
19/19 - 5s - loss: 291.7535 - loglik: -2.9047e+02 - logprior: -1.2846e+00
Fitted a model with MAP estimate = -289.0867
expansions: []
discards: [ 0 56]
Fitting a model of length 138 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 297.4101 - loglik: -2.9344e+02 - logprior: -3.9703e+00
Epoch 2/10
19/19 - 5s - loss: 291.6257 - loglik: -2.9004e+02 - logprior: -1.5876e+00
Epoch 3/10
19/19 - 5s - loss: 289.9133 - loglik: -2.8863e+02 - logprior: -1.2801e+00
Epoch 4/10
19/19 - 5s - loss: 288.3514 - loglik: -2.8713e+02 - logprior: -1.2191e+00
Epoch 5/10
19/19 - 5s - loss: 287.8858 - loglik: -2.8676e+02 - logprior: -1.1241e+00
Epoch 6/10
19/19 - 5s - loss: 284.4911 - loglik: -2.8345e+02 - logprior: -1.0446e+00
Epoch 7/10
19/19 - 5s - loss: 286.0202 - loglik: -2.8505e+02 - logprior: -9.6932e-01
Fitted a model with MAP estimate = -284.2226
Time for alignment: 126.4824
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.5050 - loglik: -3.9449e+02 - logprior: -3.0151e+00
Epoch 2/10
19/19 - 4s - loss: 327.6784 - loglik: -3.2643e+02 - logprior: -1.2464e+00
Epoch 3/10
19/19 - 4s - loss: 304.2781 - loglik: -3.0266e+02 - logprior: -1.6212e+00
Epoch 4/10
19/19 - 4s - loss: 301.5922 - loglik: -3.0014e+02 - logprior: -1.4472e+00
Epoch 5/10
19/19 - 4s - loss: 298.8309 - loglik: -2.9744e+02 - logprior: -1.3943e+00
Epoch 6/10
19/19 - 4s - loss: 297.9941 - loglik: -2.9664e+02 - logprior: -1.3556e+00
Epoch 7/10
19/19 - 4s - loss: 297.2380 - loglik: -2.9590e+02 - logprior: -1.3332e+00
Epoch 8/10
19/19 - 4s - loss: 297.4213 - loglik: -2.9610e+02 - logprior: -1.3192e+00
Fitted a model with MAP estimate = -295.0647
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (29, 2), (35, 1), (37, 2), (45, 1), (46, 3), (49, 1), (50, 2), (55, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Fitting a model of length 146 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 307.3215 - loglik: -3.0326e+02 - logprior: -4.0589e+00
Epoch 2/2
19/19 - 5s - loss: 295.3635 - loglik: -2.9300e+02 - logprior: -2.3659e+00
Fitted a model with MAP estimate = -293.3415
expansions: [(0, 2)]
discards: [  0  23  30  36  48  60  67 138]
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 295.6216 - loglik: -2.9253e+02 - logprior: -3.0917e+00
Epoch 2/2
19/19 - 5s - loss: 291.5609 - loglik: -2.9030e+02 - logprior: -1.2593e+00
Fitted a model with MAP estimate = -289.0681
expansions: []
discards: [0]
Fitting a model of length 139 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 297.1555 - loglik: -2.9320e+02 - logprior: -3.9580e+00
Epoch 2/10
19/19 - 5s - loss: 291.6559 - loglik: -2.9012e+02 - logprior: -1.5408e+00
Epoch 3/10
19/19 - 5s - loss: 289.6630 - loglik: -2.8840e+02 - logprior: -1.2597e+00
Epoch 4/10
19/19 - 5s - loss: 288.6798 - loglik: -2.8752e+02 - logprior: -1.1641e+00
Epoch 5/10
19/19 - 5s - loss: 286.6871 - loglik: -2.8560e+02 - logprior: -1.0842e+00
Epoch 6/10
19/19 - 5s - loss: 285.9233 - loglik: -2.8492e+02 - logprior: -9.9895e-01
Epoch 7/10
19/19 - 5s - loss: 284.4244 - loglik: -2.8347e+02 - logprior: -9.5532e-01
Epoch 8/10
19/19 - 5s - loss: 283.7717 - loglik: -2.8287e+02 - logprior: -8.9886e-01
Epoch 9/10
19/19 - 5s - loss: 284.0206 - loglik: -2.8316e+02 - logprior: -8.5613e-01
Fitted a model with MAP estimate = -283.5032
Time for alignment: 136.2739
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 397.7963 - loglik: -3.9477e+02 - logprior: -3.0224e+00
Epoch 2/10
19/19 - 4s - loss: 327.5727 - loglik: -3.2633e+02 - logprior: -1.2453e+00
Epoch 3/10
19/19 - 4s - loss: 305.4094 - loglik: -3.0375e+02 - logprior: -1.6641e+00
Epoch 4/10
19/19 - 4s - loss: 300.4781 - loglik: -2.9899e+02 - logprior: -1.4892e+00
Epoch 5/10
19/19 - 4s - loss: 297.6699 - loglik: -2.9623e+02 - logprior: -1.4370e+00
Epoch 6/10
19/19 - 4s - loss: 298.3387 - loglik: -2.9694e+02 - logprior: -1.4007e+00
Fitted a model with MAP estimate = -295.7328
expansions: [(7, 2), (22, 2), (23, 1), (24, 1), (25, 2), (28, 1), (29, 2), (36, 1), (37, 2), (46, 3), (49, 1), (50, 1), (55, 1), (56, 1), (71, 2), (84, 5), (85, 3), (86, 1), (106, 3)]
discards: [0]
Fitting a model of length 146 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 305.9913 - loglik: -3.0196e+02 - logprior: -4.0329e+00
Epoch 2/2
19/19 - 5s - loss: 295.8738 - loglik: -2.9361e+02 - logprior: -2.2650e+00
Fitted a model with MAP estimate = -293.2684
expansions: [(0, 2)]
discards: [  0  23  30  37  48  91 110 138]
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 295.6613 - loglik: -2.9265e+02 - logprior: -3.0160e+00
Epoch 2/2
19/19 - 5s - loss: 291.4562 - loglik: -2.9026e+02 - logprior: -1.1943e+00
Fitted a model with MAP estimate = -289.0981
expansions: []
discards: [ 0 56 58]
Fitting a model of length 137 on 10065 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 297.6068 - loglik: -2.9372e+02 - logprior: -3.8844e+00
Epoch 2/10
19/19 - 5s - loss: 291.7028 - loglik: -2.9021e+02 - logprior: -1.4945e+00
Epoch 3/10
19/19 - 5s - loss: 290.0540 - loglik: -2.8885e+02 - logprior: -1.2004e+00
Epoch 4/10
19/19 - 5s - loss: 288.5111 - loglik: -2.8738e+02 - logprior: -1.1279e+00
Epoch 5/10
19/19 - 5s - loss: 287.2545 - loglik: -2.8619e+02 - logprior: -1.0680e+00
Epoch 6/10
19/19 - 5s - loss: 286.0106 - loglik: -2.8501e+02 - logprior: -1.0002e+00
Epoch 7/10
19/19 - 5s - loss: 284.8666 - loglik: -2.8391e+02 - logprior: -9.6100e-01
Epoch 8/10
19/19 - 5s - loss: 284.3074 - loglik: -2.8342e+02 - logprior: -8.8675e-01
Epoch 9/10
19/19 - 5s - loss: 284.5309 - loglik: -2.8367e+02 - logprior: -8.6227e-01
Fitted a model with MAP estimate = -283.6957
Time for alignment: 127.1621
Computed alignments with likelihoods: ['-284.2226', '-283.5032', '-283.6957']
Best model has likelihood: -283.5032
SP score = 0.2671
Training of 3 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4849de1bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f487d15e9d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 488.5858 - loglik: -4.8574e+02 - logprior: -2.8424e+00
Epoch 2/10
19/19 - 5s - loss: 450.7988 - loglik: -4.4968e+02 - logprior: -1.1161e+00
Epoch 3/10
19/19 - 5s - loss: 432.2519 - loglik: -4.3095e+02 - logprior: -1.3012e+00
Epoch 4/10
19/19 - 5s - loss: 427.1926 - loglik: -4.2589e+02 - logprior: -1.3017e+00
Epoch 5/10
19/19 - 5s - loss: 424.6616 - loglik: -4.2332e+02 - logprior: -1.3406e+00
Epoch 6/10
19/19 - 5s - loss: 424.0318 - loglik: -4.2270e+02 - logprior: -1.3338e+00
Epoch 7/10
19/19 - 5s - loss: 424.5999 - loglik: -4.2326e+02 - logprior: -1.3368e+00
Fitted a model with MAP estimate = -422.4051
expansions: [(19, 2), (20, 1), (32, 15), (33, 1), (34, 1), (47, 1), (49, 1), (50, 1), (74, 1), (76, 2), (77, 3), (95, 1), (98, 1), (99, 1), (107, 1), (108, 1), (110, 2), (112, 1)]
discards: [0 1]
Fitting a model of length 163 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 429.2053 - loglik: -4.2554e+02 - logprior: -3.6664e+00
Epoch 2/2
19/19 - 7s - loss: 419.2171 - loglik: -4.1748e+02 - logprior: -1.7322e+00
Fitted a model with MAP estimate = -415.7084
expansions: [(0, 6)]
discards: [ 0 35 36 37 38 39 40 41 42 43 44 45 46]
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 422.8225 - loglik: -4.1994e+02 - logprior: -2.8785e+00
Epoch 2/2
19/19 - 7s - loss: 416.2577 - loglik: -4.1516e+02 - logprior: -1.0978e+00
Fitted a model with MAP estimate = -415.4782
expansions: [(25, 1)]
discards: [1 2 3 4]
Fitting a model of length 153 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 419.2872 - loglik: -4.1651e+02 - logprior: -2.7760e+00
Epoch 2/10
19/19 - 6s - loss: 416.0361 - loglik: -4.1517e+02 - logprior: -8.6292e-01
Epoch 3/10
19/19 - 6s - loss: 414.9324 - loglik: -4.1426e+02 - logprior: -6.7179e-01
Epoch 4/10
19/19 - 6s - loss: 414.5855 - loglik: -4.1396e+02 - logprior: -6.2425e-01
Epoch 5/10
19/19 - 6s - loss: 413.2935 - loglik: -4.1267e+02 - logprior: -6.2613e-01
Epoch 6/10
19/19 - 6s - loss: 411.2730 - loglik: -4.1066e+02 - logprior: -6.1074e-01
Epoch 7/10
19/19 - 6s - loss: 410.6541 - loglik: -4.1005e+02 - logprior: -5.9995e-01
Epoch 8/10
19/19 - 6s - loss: 411.5804 - loglik: -4.1099e+02 - logprior: -5.8811e-01
Fitted a model with MAP estimate = -410.4748
Time for alignment: 164.7452
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 489.1316 - loglik: -4.8629e+02 - logprior: -2.8384e+00
Epoch 2/10
19/19 - 5s - loss: 448.5613 - loglik: -4.4749e+02 - logprior: -1.0754e+00
Epoch 3/10
19/19 - 5s - loss: 431.7935 - loglik: -4.3056e+02 - logprior: -1.2312e+00
Epoch 4/10
19/19 - 5s - loss: 425.0361 - loglik: -4.2386e+02 - logprior: -1.1810e+00
Epoch 5/10
19/19 - 5s - loss: 425.6448 - loglik: -4.2443e+02 - logprior: -1.2179e+00
Fitted a model with MAP estimate = -423.7297
expansions: [(19, 2), (20, 1), (21, 1), (32, 3), (33, 3), (46, 1), (47, 2), (48, 1), (49, 2), (55, 4), (75, 1), (77, 3), (93, 1), (107, 4), (108, 1)]
discards: [0]
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 425.7753 - loglik: -4.2218e+02 - logprior: -3.5927e+00
Epoch 2/2
19/19 - 7s - loss: 416.6133 - loglik: -4.1495e+02 - logprior: -1.6589e+00
Fitted a model with MAP estimate = -415.5302
expansions: [(0, 5), (100, 1)]
discards: [ 0 20 60]
Fitting a model of length 160 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 418.6004 - loglik: -4.1575e+02 - logprior: -2.8469e+00
Epoch 2/2
19/19 - 7s - loss: 415.3469 - loglik: -4.1434e+02 - logprior: -1.0065e+00
Fitted a model with MAP estimate = -413.3733
expansions: []
discards: [1 2 3 4]
Fitting a model of length 156 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 417.2612 - loglik: -4.1453e+02 - logprior: -2.7334e+00
Epoch 2/10
19/19 - 7s - loss: 414.8134 - loglik: -4.1399e+02 - logprior: -8.2630e-01
Epoch 3/10
19/19 - 7s - loss: 412.7612 - loglik: -4.1213e+02 - logprior: -6.3107e-01
Epoch 4/10
19/19 - 7s - loss: 413.2025 - loglik: -4.1261e+02 - logprior: -5.8870e-01
Fitted a model with MAP estimate = -410.9630
Time for alignment: 127.6734
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 488.3935 - loglik: -4.8556e+02 - logprior: -2.8348e+00
Epoch 2/10
19/19 - 5s - loss: 451.0778 - loglik: -4.4999e+02 - logprior: -1.0873e+00
Epoch 3/10
19/19 - 5s - loss: 430.9903 - loglik: -4.2973e+02 - logprior: -1.2622e+00
Epoch 4/10
19/19 - 5s - loss: 423.9398 - loglik: -4.2268e+02 - logprior: -1.2566e+00
Epoch 5/10
19/19 - 5s - loss: 421.4405 - loglik: -4.2015e+02 - logprior: -1.2947e+00
Epoch 6/10
19/19 - 5s - loss: 422.9857 - loglik: -4.2171e+02 - logprior: -1.2720e+00
Fitted a model with MAP estimate = -420.8095
expansions: [(20, 1), (31, 1), (32, 3), (33, 3), (48, 3), (50, 1), (59, 7), (73, 1), (76, 2), (77, 4), (78, 1), (92, 1), (93, 1), (104, 1), (106, 2), (107, 2), (108, 1)]
discards: [0 1]
Fitting a model of length 161 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 424.1083 - loglik: -4.2051e+02 - logprior: -3.5985e+00
Epoch 2/2
19/19 - 7s - loss: 415.9044 - loglik: -4.1424e+02 - logprior: -1.6672e+00
Fitted a model with MAP estimate = -413.7166
expansions: [(0, 5)]
discards: [  0  55  71  73  98 135]
Fitting a model of length 160 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 416.2647 - loglik: -4.1348e+02 - logprior: -2.7830e+00
Epoch 2/2
19/19 - 7s - loss: 412.9241 - loglik: -4.1192e+02 - logprior: -1.0068e+00
Fitted a model with MAP estimate = -411.3166
expansions: []
discards: [1 2 3]
Fitting a model of length 157 on 10006 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 415.4255 - loglik: -4.1270e+02 - logprior: -2.7228e+00
Epoch 2/10
19/19 - 7s - loss: 412.9598 - loglik: -4.1216e+02 - logprior: -8.0368e-01
Epoch 3/10
19/19 - 7s - loss: 411.7509 - loglik: -4.1112e+02 - logprior: -6.2833e-01
Epoch 4/10
19/19 - 7s - loss: 409.8653 - loglik: -4.0928e+02 - logprior: -5.8924e-01
Epoch 5/10
19/19 - 7s - loss: 410.5909 - loglik: -4.1002e+02 - logprior: -5.7541e-01
Fitted a model with MAP estimate = -408.2327
Time for alignment: 140.4148
Computed alignments with likelihoods: ['-410.4748', '-410.9630', '-408.2327']
Best model has likelihood: -408.2327
SP score = 0.8760
Training of 3 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4806a06400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f484a178250>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 791.4528 - loglik: -7.9002e+02 - logprior: -1.4278e+00
Epoch 2/10
39/39 - 18s - loss: 722.8453 - loglik: -7.2210e+02 - logprior: -7.4044e-01
Epoch 3/10
39/39 - 18s - loss: 715.2599 - loglik: -7.1443e+02 - logprior: -8.2957e-01
Epoch 4/10
39/39 - 18s - loss: 712.4020 - loglik: -7.1158e+02 - logprior: -8.2184e-01
Epoch 5/10
39/39 - 18s - loss: 710.4625 - loglik: -7.0963e+02 - logprior: -8.3155e-01
Epoch 6/10
39/39 - 18s - loss: 710.0662 - loglik: -7.0921e+02 - logprior: -8.5596e-01
Epoch 7/10
39/39 - 18s - loss: 709.4372 - loglik: -7.0859e+02 - logprior: -8.4921e-01
Epoch 8/10
39/39 - 18s - loss: 709.6650 - loglik: -7.0880e+02 - logprior: -8.6345e-01
Fitted a model with MAP estimate = -708.5927
expansions: [(0, 3), (9, 1), (10, 1), (24, 1), (40, 1), (43, 1), (51, 2), (53, 1), (61, 1), (80, 1), (84, 4), (85, 5), (86, 1), (87, 4), (115, 2), (118, 2), (119, 1), (121, 1), (153, 2), (155, 3), (179, 5), (180, 1), (184, 1), (206, 2), (207, 5), (209, 1), (214, 1)]
discards: [158]
Fitting a model of length 282 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 713.1508 - loglik: -7.1098e+02 - logprior: -2.1746e+00
Epoch 2/2
39/39 - 24s - loss: 703.3705 - loglik: -7.0276e+02 - logprior: -6.0864e-01
Fitted a model with MAP estimate = -701.0080
expansions: [(110, 2), (217, 1)]
discards: [  1   2   3   4  59  92  93  94  99 148 195 210 251 254 255]
Fitting a model of length 270 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 707.2941 - loglik: -7.0586e+02 - logprior: -1.4350e+00
Epoch 2/2
39/39 - 22s - loss: 703.4693 - loglik: -7.0316e+02 - logprior: -3.0994e-01
Fitted a model with MAP estimate = -701.1428
expansions: [(0, 4), (242, 1)]
discards: [ 96 201 202 203]
Fitting a model of length 271 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 706.9680 - loglik: -7.0489e+02 - logprior: -2.0799e+00
Epoch 2/10
39/39 - 23s - loss: 703.2629 - loglik: -7.0303e+02 - logprior: -2.3314e-01
Epoch 3/10
39/39 - 23s - loss: 701.2161 - loglik: -7.0103e+02 - logprior: -1.8646e-01
Epoch 4/10
39/39 - 23s - loss: 699.4260 - loglik: -6.9932e+02 - logprior: -1.0894e-01
Epoch 5/10
39/39 - 23s - loss: 698.1723 - loglik: -6.9814e+02 - logprior: -2.8194e-02
Epoch 6/10
39/39 - 23s - loss: 697.5341 - loglik: -6.9755e+02 - logprior: 0.0189
Epoch 7/10
39/39 - 23s - loss: 697.4551 - loglik: -6.9754e+02 - logprior: 0.0885
Epoch 8/10
39/39 - 23s - loss: 697.3385 - loglik: -6.9750e+02 - logprior: 0.1621
Epoch 9/10
39/39 - 23s - loss: 696.7827 - loglik: -6.9701e+02 - logprior: 0.2224
Epoch 10/10
39/39 - 22s - loss: 696.9400 - loglik: -6.9724e+02 - logprior: 0.2957
Fitted a model with MAP estimate = -696.2368
Time for alignment: 592.8348
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 792.8572 - loglik: -7.9143e+02 - logprior: -1.4272e+00
Epoch 2/10
39/39 - 18s - loss: 724.3298 - loglik: -7.2358e+02 - logprior: -7.4757e-01
Epoch 3/10
39/39 - 18s - loss: 714.8705 - loglik: -7.1400e+02 - logprior: -8.6591e-01
Epoch 4/10
39/39 - 18s - loss: 711.5398 - loglik: -7.1065e+02 - logprior: -8.8620e-01
Epoch 5/10
39/39 - 18s - loss: 710.3657 - loglik: -7.0947e+02 - logprior: -8.9505e-01
Epoch 6/10
39/39 - 18s - loss: 710.1988 - loglik: -7.0929e+02 - logprior: -9.0544e-01
Epoch 7/10
39/39 - 18s - loss: 709.3478 - loglik: -7.0843e+02 - logprior: -9.1592e-01
Epoch 8/10
39/39 - 18s - loss: 709.5295 - loglik: -7.0862e+02 - logprior: -9.1182e-01
Fitted a model with MAP estimate = -708.6071
expansions: [(0, 3), (9, 1), (10, 1), (18, 1), (40, 1), (43, 2), (51, 2), (53, 1), (61, 1), (89, 6), (96, 1), (117, 4), (118, 2), (119, 1), (120, 1), (122, 1), (124, 1), (153, 2), (155, 3), (156, 1), (179, 7), (183, 1), (184, 1), (206, 7), (207, 1), (214, 1)]
discards: []
Fitting a model of length 283 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 712.0331 - loglik: -7.0990e+02 - logprior: -2.1291e+00
Epoch 2/2
39/39 - 24s - loss: 703.2969 - loglik: -7.0277e+02 - logprior: -5.2285e-01
Fitted a model with MAP estimate = -700.8724
expansions: [(98, 3)]
discards: [  1   2   3   4  16  51  60 104 139 140 141 189 193 217 252 254 255 256]
Fitting a model of length 268 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 707.4827 - loglik: -7.0605e+02 - logprior: -1.4282e+00
Epoch 2/2
39/39 - 22s - loss: 703.4881 - loglik: -7.0318e+02 - logprior: -3.0806e-01
Fitted a model with MAP estimate = -701.4923
expansions: [(0, 4), (12, 1), (92, 1), (241, 1), (242, 2)]
discards: [183]
Fitting a model of length 276 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 706.0543 - loglik: -7.0401e+02 - logprior: -2.0396e+00
Epoch 2/10
39/39 - 23s - loss: 701.7816 - loglik: -7.0157e+02 - logprior: -2.1291e-01
Epoch 3/10
39/39 - 23s - loss: 699.8325 - loglik: -6.9966e+02 - logprior: -1.7626e-01
Epoch 4/10
39/39 - 23s - loss: 697.7413 - loglik: -6.9762e+02 - logprior: -1.1844e-01
Epoch 5/10
39/39 - 23s - loss: 696.8090 - loglik: -6.9674e+02 - logprior: -6.6916e-02
Epoch 6/10
39/39 - 23s - loss: 695.9822 - loglik: -6.9598e+02 - logprior: -1.9874e-04
Epoch 7/10
39/39 - 24s - loss: 695.8404 - loglik: -6.9591e+02 - logprior: 0.0714
Epoch 8/10
39/39 - 23s - loss: 696.0026 - loglik: -6.9614e+02 - logprior: 0.1404
Fitted a model with MAP estimate = -695.2951
Time for alignment: 550.2367
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 792.7531 - loglik: -7.9131e+02 - logprior: -1.4384e+00
Epoch 2/10
39/39 - 18s - loss: 724.6590 - loglik: -7.2392e+02 - logprior: -7.3472e-01
Epoch 3/10
39/39 - 18s - loss: 716.0475 - loglik: -7.1521e+02 - logprior: -8.3590e-01
Epoch 4/10
39/39 - 18s - loss: 712.9473 - loglik: -7.1214e+02 - logprior: -8.0756e-01
Epoch 5/10
39/39 - 18s - loss: 711.0361 - loglik: -7.1023e+02 - logprior: -8.0187e-01
Epoch 6/10
39/39 - 18s - loss: 710.9637 - loglik: -7.1015e+02 - logprior: -8.1803e-01
Epoch 7/10
39/39 - 18s - loss: 710.4583 - loglik: -7.0962e+02 - logprior: -8.3438e-01
Epoch 8/10
39/39 - 18s - loss: 709.7428 - loglik: -7.0889e+02 - logprior: -8.4894e-01
Epoch 9/10
39/39 - 18s - loss: 709.4230 - loglik: -7.0858e+02 - logprior: -8.4760e-01
Epoch 10/10
39/39 - 18s - loss: 710.1067 - loglik: -7.0926e+02 - logprior: -8.4797e-01
Fitted a model with MAP estimate = -708.4754
expansions: [(0, 4), (40, 1), (42, 1), (49, 1), (50, 1), (52, 1), (80, 1), (85, 1), (90, 14), (96, 1), (105, 1), (116, 3), (119, 2), (120, 1), (122, 1), (126, 1), (127, 1), (152, 2), (158, 2), (178, 8), (182, 1), (183, 1), (205, 6), (209, 1), (214, 1)]
discards: []
Fitting a model of length 287 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 712.8683 - loglik: -7.1073e+02 - logprior: -2.1370e+00
Epoch 2/2
39/39 - 25s - loss: 702.1848 - loglik: -7.0170e+02 - logprior: -4.8536e-01
Fitted a model with MAP estimate = -699.5717
expansions: [(0, 4), (192, 2)]
discards: [  1   2 105 106 107 108 109 149 196 219 220 258 259]
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 706.2154 - loglik: -7.0386e+02 - logprior: -2.3562e+00
Epoch 2/2
39/39 - 24s - loss: 701.9332 - loglik: -7.0154e+02 - logprior: -3.9199e-01
Fitted a model with MAP estimate = -699.7329
expansions: [(0, 3)]
discards: [  1   2   3   4 190 191 192 193]
Fitting a model of length 275 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 705.9805 - loglik: -7.0339e+02 - logprior: -2.5950e+00
Epoch 2/10
39/39 - 23s - loss: 702.0649 - loglik: -7.0169e+02 - logprior: -3.7762e-01
Epoch 3/10
39/39 - 23s - loss: 700.2755 - loglik: -7.0016e+02 - logprior: -1.1638e-01
Epoch 4/10
39/39 - 23s - loss: 698.1823 - loglik: -6.9814e+02 - logprior: -4.4893e-02
Epoch 5/10
39/39 - 23s - loss: 697.3489 - loglik: -6.9737e+02 - logprior: 0.0247
Epoch 6/10
39/39 - 23s - loss: 696.1742 - loglik: -6.9627e+02 - logprior: 0.0952
Epoch 7/10
39/39 - 23s - loss: 696.5204 - loglik: -6.9669e+02 - logprior: 0.1725
Fitted a model with MAP estimate = -695.7919
Time for alignment: 570.5791
Computed alignments with likelihoods: ['-696.2368', '-695.2951', '-695.7919']
Best model has likelihood: -695.2951
SP score = 0.6022
Training of 3 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47e3fa0e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47e3fbd070>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 623.3083 - loglik: -6.2069e+02 - logprior: -2.6214e+00
Epoch 2/10
19/19 - 8s - loss: 527.2006 - loglik: -5.2612e+02 - logprior: -1.0819e+00
Epoch 3/10
19/19 - 8s - loss: 484.0387 - loglik: -4.8242e+02 - logprior: -1.6155e+00
Epoch 4/10
19/19 - 8s - loss: 474.6402 - loglik: -4.7291e+02 - logprior: -1.7314e+00
Epoch 5/10
19/19 - 8s - loss: 471.2981 - loglik: -4.6950e+02 - logprior: -1.7957e+00
Epoch 6/10
19/19 - 8s - loss: 470.5784 - loglik: -4.6882e+02 - logprior: -1.7600e+00
Epoch 7/10
19/19 - 8s - loss: 469.3159 - loglik: -4.6759e+02 - logprior: -1.7227e+00
Epoch 8/10
19/19 - 8s - loss: 470.3531 - loglik: -4.6864e+02 - logprior: -1.7134e+00
Fitted a model with MAP estimate = -466.7479
expansions: [(10, 1), (12, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (48, 1), (50, 1), (66, 1), (67, 2), (71, 1), (77, 1), (83, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 2), (155, 2), (156, 2), (157, 2), (163, 1), (168, 1), (176, 1), (182, 1), (184, 1), (188, 1)]
discards: [0]
Fitting a model of length 245 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 15s - loss: 471.6350 - loglik: -4.6816e+02 - logprior: -3.4703e+00
Epoch 2/2
19/19 - 11s - loss: 456.1542 - loglik: -4.5438e+02 - logprior: -1.7722e+00
Fitted a model with MAP estimate = -451.1131
expansions: [(3, 1)]
discards: [  0  26  85 139 142 189 190 191 201 203]
Fitting a model of length 236 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 459.5836 - loglik: -4.5623e+02 - logprior: -3.3581e+00
Epoch 2/2
19/19 - 11s - loss: 454.4002 - loglik: -4.5338e+02 - logprior: -1.0173e+00
Fitted a model with MAP estimate = -449.9522
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 236 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 456.3445 - loglik: -4.5296e+02 - logprior: -3.3865e+00
Epoch 2/10
19/19 - 11s - loss: 451.2967 - loglik: -4.4978e+02 - logprior: -1.5125e+00
Epoch 3/10
19/19 - 11s - loss: 450.1544 - loglik: -4.4891e+02 - logprior: -1.2426e+00
Epoch 4/10
19/19 - 11s - loss: 448.6240 - loglik: -4.4808e+02 - logprior: -5.3952e-01
Epoch 5/10
19/19 - 11s - loss: 447.9407 - loglik: -4.4755e+02 - logprior: -3.9186e-01
Epoch 6/10
19/19 - 11s - loss: 446.6645 - loglik: -4.4628e+02 - logprior: -3.8817e-01
Epoch 7/10
19/19 - 11s - loss: 447.3236 - loglik: -4.4695e+02 - logprior: -3.6920e-01
Fitted a model with MAP estimate = -446.5272
Time for alignment: 269.0063
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 623.5893 - loglik: -6.2097e+02 - logprior: -2.6200e+00
Epoch 2/10
19/19 - 8s - loss: 529.3372 - loglik: -5.2827e+02 - logprior: -1.0718e+00
Epoch 3/10
19/19 - 8s - loss: 483.7772 - loglik: -4.8214e+02 - logprior: -1.6374e+00
Epoch 4/10
19/19 - 8s - loss: 475.5153 - loglik: -4.7380e+02 - logprior: -1.7193e+00
Epoch 5/10
19/19 - 8s - loss: 473.4236 - loglik: -4.7173e+02 - logprior: -1.6968e+00
Epoch 6/10
19/19 - 8s - loss: 472.4081 - loglik: -4.7076e+02 - logprior: -1.6474e+00
Epoch 7/10
19/19 - 8s - loss: 472.0870 - loglik: -4.7047e+02 - logprior: -1.6162e+00
Epoch 8/10
19/19 - 8s - loss: 471.1564 - loglik: -4.6956e+02 - logprior: -1.5990e+00
Epoch 9/10
19/19 - 8s - loss: 472.3492 - loglik: -4.7075e+02 - logprior: -1.5990e+00
Fitted a model with MAP estimate = -468.6141
expansions: [(12, 1), (15, 1), (18, 1), (23, 2), (24, 1), (31, 1), (33, 3), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (49, 1), (66, 2), (67, 2), (71, 1), (77, 1), (83, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (129, 1), (130, 1), (145, 2), (150, 1), (151, 3), (153, 2), (154, 1), (155, 3), (156, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Fitting a model of length 244 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 471.8886 - loglik: -4.6836e+02 - logprior: -3.5319e+00
Epoch 2/2
19/19 - 11s - loss: 455.9367 - loglik: -4.5412e+02 - logprior: -1.8140e+00
Fitted a model with MAP estimate = -451.2110
expansions: [(3, 1)]
discards: [  0  26  84 139 142 180 196 203]
Fitting a model of length 237 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 459.2552 - loglik: -4.5589e+02 - logprior: -3.3662e+00
Epoch 2/2
19/19 - 11s - loss: 453.8654 - loglik: -4.5279e+02 - logprior: -1.0718e+00
Fitted a model with MAP estimate = -449.7012
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 237 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 456.0985 - loglik: -4.5270e+02 - logprior: -3.3952e+00
Epoch 2/10
19/19 - 11s - loss: 451.7086 - loglik: -4.5019e+02 - logprior: -1.5215e+00
Epoch 3/10
19/19 - 11s - loss: 450.1197 - loglik: -4.4883e+02 - logprior: -1.2947e+00
Epoch 4/10
19/19 - 11s - loss: 447.8983 - loglik: -4.4731e+02 - logprior: -5.8944e-01
Epoch 5/10
19/19 - 11s - loss: 447.7679 - loglik: -4.4734e+02 - logprior: -4.2310e-01
Epoch 6/10
19/19 - 11s - loss: 446.4060 - loglik: -4.4601e+02 - logprior: -4.0071e-01
Epoch 7/10
19/19 - 11s - loss: 447.2218 - loglik: -4.4684e+02 - logprior: -3.8390e-01
Fitted a model with MAP estimate = -446.3895
Time for alignment: 277.1105
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 623.4010 - loglik: -6.2078e+02 - logprior: -2.6203e+00
Epoch 2/10
19/19 - 8s - loss: 527.5919 - loglik: -5.2653e+02 - logprior: -1.0660e+00
Epoch 3/10
19/19 - 8s - loss: 485.2116 - loglik: -4.8361e+02 - logprior: -1.6022e+00
Epoch 4/10
19/19 - 8s - loss: 476.5007 - loglik: -4.7480e+02 - logprior: -1.6986e+00
Epoch 5/10
19/19 - 8s - loss: 472.6853 - loglik: -4.7094e+02 - logprior: -1.7432e+00
Epoch 6/10
19/19 - 8s - loss: 472.4724 - loglik: -4.7075e+02 - logprior: -1.7206e+00
Epoch 7/10
19/19 - 8s - loss: 470.8636 - loglik: -4.6917e+02 - logprior: -1.6905e+00
Epoch 8/10
19/19 - 8s - loss: 471.3983 - loglik: -4.6971e+02 - logprior: -1.6839e+00
Fitted a model with MAP estimate = -468.1866
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (47, 1), (48, 1), (66, 2), (67, 1), (69, 1), (70, 1), (83, 1), (86, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (111, 2), (112, 1), (122, 1), (128, 1), (149, 2), (150, 3), (151, 2), (153, 3), (154, 1), (155, 1), (156, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Fitting a model of length 244 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 470.9112 - loglik: -4.6738e+02 - logprior: -3.5318e+00
Epoch 2/2
19/19 - 11s - loss: 455.6033 - loglik: -4.5377e+02 - logprior: -1.8316e+00
Fitted a model with MAP estimate = -450.9969
expansions: [(3, 1)]
discards: [  0  26  84 139 141 190 191 195 203]
Fitting a model of length 236 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 459.1837 - loglik: -4.5583e+02 - logprior: -3.3495e+00
Epoch 2/2
19/19 - 11s - loss: 454.2546 - loglik: -4.5330e+02 - logprior: -9.5870e-01
Fitted a model with MAP estimate = -449.8195
expansions: [(4, 1)]
discards: [0]
Fitting a model of length 236 on 10077 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 14s - loss: 456.1234 - loglik: -4.5274e+02 - logprior: -3.3820e+00
Epoch 2/10
19/19 - 11s - loss: 452.2397 - loglik: -4.5071e+02 - logprior: -1.5249e+00
Epoch 3/10
19/19 - 11s - loss: 449.5989 - loglik: -4.4831e+02 - logprior: -1.2906e+00
Epoch 4/10
19/19 - 11s - loss: 448.6098 - loglik: -4.4804e+02 - logprior: -5.7249e-01
Epoch 5/10
19/19 - 11s - loss: 447.1655 - loglik: -4.4676e+02 - logprior: -4.0852e-01
Epoch 6/10
19/19 - 11s - loss: 447.8010 - loglik: -4.4740e+02 - logprior: -4.0122e-01
Fitted a model with MAP estimate = -446.6248
Time for alignment: 257.7584
Computed alignments with likelihoods: ['-446.5272', '-446.3895', '-446.6248']
Best model has likelihood: -446.3895
SP score = 0.7218
Training of 3 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f46400f99a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f464857c3a0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f464857cc70>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6670>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6910>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6c70>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f46400e6af0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6040>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e68e0>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e67c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6760>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6550>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e63a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6400>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e66a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6a00>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e69d0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6520>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f46400e6340> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f46400e6220>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f46400e6d00> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4640046790> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f47ed1ffcd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f482024bd30>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 320.9685 - loglik: -3.1789e+02 - logprior: -3.0827e+00
Epoch 2/10
19/19 - 2s - loss: 276.1674 - loglik: -2.7479e+02 - logprior: -1.3737e+00
Epoch 3/10
19/19 - 2s - loss: 253.8868 - loglik: -2.5221e+02 - logprior: -1.6740e+00
Epoch 4/10
19/19 - 2s - loss: 247.3852 - loglik: -2.4575e+02 - logprior: -1.6360e+00
Epoch 5/10
19/19 - 2s - loss: 244.4149 - loglik: -2.4277e+02 - logprior: -1.6481e+00
Epoch 6/10
19/19 - 2s - loss: 243.9992 - loglik: -2.4239e+02 - logprior: -1.6130e+00
Epoch 7/10
19/19 - 2s - loss: 243.2663 - loglik: -2.4167e+02 - logprior: -1.5951e+00
Epoch 8/10
19/19 - 2s - loss: 243.8365 - loglik: -2.4226e+02 - logprior: -1.5798e+00
Fitted a model with MAP estimate = -243.1510
expansions: [(17, 1), (18, 2), (19, 2), (20, 2), (22, 1), (23, 2), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (48, 1), (49, 1), (56, 1), (62, 1), (66, 2), (68, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 249.0918 - loglik: -2.4513e+02 - logprior: -3.9612e+00
Epoch 2/2
19/19 - 3s - loss: 238.1794 - loglik: -2.3616e+02 - logprior: -2.0242e+00
Fitted a model with MAP estimate = -236.1409
expansions: [(0, 2)]
discards: [ 0 15 16 20 30 38 44]
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 238.2732 - loglik: -2.3539e+02 - logprior: -2.8853e+00
Epoch 2/2
19/19 - 3s - loss: 234.4718 - loglik: -2.3343e+02 - logprior: -1.0412e+00
Fitted a model with MAP estimate = -233.4420
expansions: [(21, 1)]
discards: [0]
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 240.0782 - loglik: -2.3635e+02 - logprior: -3.7268e+00
Epoch 2/10
19/19 - 3s - loss: 235.4874 - loglik: -2.3423e+02 - logprior: -1.2527e+00
Epoch 3/10
19/19 - 3s - loss: 233.6169 - loglik: -2.3257e+02 - logprior: -1.0517e+00
Epoch 4/10
19/19 - 3s - loss: 232.7494 - loglik: -2.3174e+02 - logprior: -1.0097e+00
Epoch 5/10
19/19 - 3s - loss: 231.8011 - loglik: -2.3082e+02 - logprior: -9.8422e-01
Epoch 6/10
19/19 - 3s - loss: 231.6522 - loglik: -2.3070e+02 - logprior: -9.5133e-01
Epoch 7/10
19/19 - 3s - loss: 231.1470 - loglik: -2.3021e+02 - logprior: -9.3443e-01
Epoch 8/10
19/19 - 3s - loss: 231.2771 - loglik: -2.3037e+02 - logprior: -9.1058e-01
Fitted a model with MAP estimate = -230.9229
Time for alignment: 83.8106
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 320.9902 - loglik: -3.1791e+02 - logprior: -3.0839e+00
Epoch 2/10
19/19 - 2s - loss: 277.8792 - loglik: -2.7652e+02 - logprior: -1.3637e+00
Epoch 3/10
19/19 - 2s - loss: 254.3206 - loglik: -2.5260e+02 - logprior: -1.7164e+00
Epoch 4/10
19/19 - 2s - loss: 247.6431 - loglik: -2.4599e+02 - logprior: -1.6580e+00
Epoch 5/10
19/19 - 2s - loss: 245.0914 - loglik: -2.4348e+02 - logprior: -1.6138e+00
Epoch 6/10
19/19 - 2s - loss: 244.5565 - loglik: -2.4299e+02 - logprior: -1.5635e+00
Epoch 7/10
19/19 - 2s - loss: 243.8965 - loglik: -2.4236e+02 - logprior: -1.5411e+00
Epoch 8/10
19/19 - 2s - loss: 244.0255 - loglik: -2.4250e+02 - logprior: -1.5297e+00
Fitted a model with MAP estimate = -243.5778
expansions: [(17, 1), (18, 3), (19, 4), (22, 1), (24, 1), (28, 2), (32, 1), (34, 1), (40, 1), (44, 1), (50, 1), (56, 2), (62, 1), (63, 1), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 247.2777 - loglik: -2.4336e+02 - logprior: -3.9191e+00
Epoch 2/2
19/19 - 3s - loss: 237.8862 - loglik: -2.3593e+02 - logprior: -1.9557e+00
Fitted a model with MAP estimate = -235.8992
expansions: [(0, 2)]
discards: [ 0 15 22 23 37]
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 238.1363 - loglik: -2.3528e+02 - logprior: -2.8595e+00
Epoch 2/2
19/19 - 3s - loss: 234.0905 - loglik: -2.3306e+02 - logprior: -1.0272e+00
Fitted a model with MAP estimate = -233.3418
expansions: []
discards: [0]
Fitting a model of length 117 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 240.1278 - loglik: -2.3643e+02 - logprior: -3.6989e+00
Epoch 2/10
19/19 - 3s - loss: 235.7934 - loglik: -2.3454e+02 - logprior: -1.2535e+00
Epoch 3/10
19/19 - 3s - loss: 233.4209 - loglik: -2.3237e+02 - logprior: -1.0466e+00
Epoch 4/10
19/19 - 3s - loss: 232.9982 - loglik: -2.3198e+02 - logprior: -1.0145e+00
Epoch 5/10
19/19 - 3s - loss: 232.4592 - loglik: -2.3148e+02 - logprior: -9.8264e-01
Epoch 6/10
19/19 - 3s - loss: 231.7340 - loglik: -2.3078e+02 - logprior: -9.5704e-01
Epoch 7/10
19/19 - 3s - loss: 230.9898 - loglik: -2.3005e+02 - logprior: -9.4262e-01
Epoch 8/10
19/19 - 3s - loss: 231.6746 - loglik: -2.3076e+02 - logprior: -9.1622e-01
Fitted a model with MAP estimate = -231.1883
Time for alignment: 82.0715
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.9481 - loglik: -3.1787e+02 - logprior: -3.0830e+00
Epoch 2/10
19/19 - 2s - loss: 275.4122 - loglik: -2.7404e+02 - logprior: -1.3725e+00
Epoch 3/10
19/19 - 2s - loss: 251.3002 - loglik: -2.4960e+02 - logprior: -1.7040e+00
Epoch 4/10
19/19 - 2s - loss: 245.6445 - loglik: -2.4399e+02 - logprior: -1.6515e+00
Epoch 5/10
19/19 - 2s - loss: 244.0495 - loglik: -2.4238e+02 - logprior: -1.6704e+00
Epoch 6/10
19/19 - 2s - loss: 243.1420 - loglik: -2.4149e+02 - logprior: -1.6532e+00
Epoch 7/10
19/19 - 2s - loss: 243.0111 - loglik: -2.4138e+02 - logprior: -1.6288e+00
Epoch 8/10
19/19 - 2s - loss: 242.6122 - loglik: -2.4100e+02 - logprior: -1.6134e+00
Epoch 9/10
19/19 - 2s - loss: 242.4054 - loglik: -2.4079e+02 - logprior: -1.6145e+00
Epoch 10/10
19/19 - 2s - loss: 242.4694 - loglik: -2.4087e+02 - logprior: -1.6025e+00
Fitted a model with MAP estimate = -242.2440
expansions: [(17, 1), (18, 2), (19, 3), (20, 1), (22, 2), (23, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (43, 1), (48, 1), (49, 1), (56, 1), (62, 1), (63, 1), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (81, 1), (84, 1)]
discards: [0]
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 248.4595 - loglik: -2.4451e+02 - logprior: -3.9511e+00
Epoch 2/2
19/19 - 3s - loss: 238.1444 - loglik: -2.3610e+02 - logprior: -2.0480e+00
Fitted a model with MAP estimate = -236.3968
expansions: [(0, 2)]
discards: [ 0 15 16 29 38 44]
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 238.3914 - loglik: -2.3552e+02 - logprior: -2.8719e+00
Epoch 2/2
19/19 - 3s - loss: 234.8416 - loglik: -2.3382e+02 - logprior: -1.0225e+00
Fitted a model with MAP estimate = -233.7925
expansions: []
discards: [0]
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 240.4772 - loglik: -2.3680e+02 - logprior: -3.6765e+00
Epoch 2/10
19/19 - 3s - loss: 235.8397 - loglik: -2.3463e+02 - logprior: -1.2143e+00
Epoch 3/10
19/19 - 3s - loss: 233.6263 - loglik: -2.3262e+02 - logprior: -1.0106e+00
Epoch 4/10
19/19 - 3s - loss: 232.6607 - loglik: -2.3169e+02 - logprior: -9.7130e-01
Epoch 5/10
19/19 - 3s - loss: 232.3742 - loglik: -2.3142e+02 - logprior: -9.5077e-01
Epoch 6/10
19/19 - 3s - loss: 231.3075 - loglik: -2.3039e+02 - logprior: -9.2142e-01
Epoch 7/10
19/19 - 3s - loss: 231.2259 - loglik: -2.3031e+02 - logprior: -9.1092e-01
Epoch 8/10
19/19 - 3s - loss: 231.1148 - loglik: -2.3023e+02 - logprior: -8.8932e-01
Epoch 9/10
19/19 - 3s - loss: 231.3314 - loglik: -2.3047e+02 - logprior: -8.6259e-01
Fitted a model with MAP estimate = -230.9412
Time for alignment: 89.7663
Computed alignments with likelihoods: ['-230.9229', '-231.1883', '-230.9412']
Best model has likelihood: -230.9229
SP score = 0.8530
