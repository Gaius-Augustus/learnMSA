Training of 5 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4be9516700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d855fcbb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4be0255e80>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4db7a751f0>
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 784.4694 - loglik: -7.8295e+02 - logprior: -1.5192e+00
Epoch 2/10
39/39 - 32s - loss: 656.8842 - loglik: -6.5511e+02 - logprior: -1.7740e+00
Epoch 3/10
39/39 - 32s - loss: 644.7524 - loglik: -6.4309e+02 - logprior: -1.6647e+00
Epoch 4/10
39/39 - 32s - loss: 641.6198 - loglik: -6.4002e+02 - logprior: -1.5949e+00
Epoch 5/10
39/39 - 34s - loss: 641.1808 - loglik: -6.3959e+02 - logprior: -1.5915e+00
Epoch 6/10
39/39 - 34s - loss: 640.0428 - loglik: -6.3845e+02 - logprior: -1.5949e+00
Epoch 7/10
39/39 - 36s - loss: 638.5444 - loglik: -6.3696e+02 - logprior: -1.5816e+00
Epoch 8/10
39/39 - 37s - loss: 638.4304 - loglik: -6.3685e+02 - logprior: -1.5813e+00
Epoch 9/10
39/39 - 38s - loss: 638.9502 - loglik: -6.3738e+02 - logprior: -1.5727e+00
Fitted a model with MAP estimate = -637.6058
expansions: [(0, 3), (11, 1), (13, 1), (14, 1), (45, 1), (51, 3), (63, 1), (64, 2), (65, 1), (66, 1), (67, 1), (68, 1), (77, 1), (78, 1), (79, 1), (83, 1), (84, 1), (91, 1), (92, 1), (95, 1), (102, 1), (107, 1), (114, 1), (117, 2), (118, 1), (119, 1), (120, 1), (121, 1), (141, 1), (143, 1), (144, 1), (147, 1), (149, 1), (160, 1), (161, 1), (163, 1), (166, 1), (173, 1), (175, 2), (188, 1), (195, 1), (196, 1), (200, 3), (201, 1), (215, 1), (216, 1), (218, 1), (219, 1), (221, 1), (225, 1), (228, 1), (229, 1), (232, 1), (234, 1), (235, 2), (237, 1), (262, 1), (263, 2), (265, 3), (267, 1), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 627.5012 - loglik: -6.2576e+02 - logprior: -1.7368e+00
Epoch 2/2
39/39 - 63s - loss: 613.9739 - loglik: -6.1359e+02 - logprior: -3.8640e-01
Fitted a model with MAP estimate = -611.3395
expansions: [(206, 1), (207, 1)]
discards: [  1   2 147]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 69s - loss: 616.4215 - loglik: -6.1521e+02 - logprior: -1.2069e+00
Epoch 2/2
39/39 - 68s - loss: 611.8648 - loglik: -6.1169e+02 - logprior: -1.7187e-01
Fitted a model with MAP estimate = -609.8604
expansions: []
discards: [145 288]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 615.3027 - loglik: -6.1427e+02 - logprior: -1.0352e+00
Epoch 2/10
39/39 - 64s - loss: 611.6903 - loglik: -6.1177e+02 - logprior: 0.0832
Epoch 3/10
39/39 - 63s - loss: 609.8367 - loglik: -6.0996e+02 - logprior: 0.1271
Epoch 4/10
39/39 - 63s - loss: 608.2222 - loglik: -6.0850e+02 - logprior: 0.2765
Epoch 5/10
39/39 - 62s - loss: 607.1674 - loglik: -6.0753e+02 - logprior: 0.3663
Epoch 6/10
39/39 - 61s - loss: 607.7238 - loglik: -6.0811e+02 - logprior: 0.3885
Fitted a model with MAP estimate = -606.3571
Time for alignment: 1209.1793
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 782.7471 - loglik: -7.8122e+02 - logprior: -1.5294e+00
Epoch 2/10
39/39 - 40s - loss: 655.3544 - loglik: -6.5365e+02 - logprior: -1.7005e+00
Epoch 3/10
39/39 - 39s - loss: 643.8117 - loglik: -6.4216e+02 - logprior: -1.6536e+00
Epoch 4/10
39/39 - 41s - loss: 641.3317 - loglik: -6.3976e+02 - logprior: -1.5749e+00
Epoch 5/10
39/39 - 45s - loss: 639.5349 - loglik: -6.3796e+02 - logprior: -1.5714e+00
Epoch 6/10
39/39 - 45s - loss: 639.1306 - loglik: -6.3757e+02 - logprior: -1.5609e+00
Epoch 7/10
39/39 - 47s - loss: 638.5530 - loglik: -6.3700e+02 - logprior: -1.5510e+00
Epoch 8/10
39/39 - 48s - loss: 638.1959 - loglik: -6.3665e+02 - logprior: -1.5498e+00
Epoch 9/10
39/39 - 50s - loss: 637.9470 - loglik: -6.3640e+02 - logprior: -1.5460e+00
Epoch 10/10
39/39 - 51s - loss: 637.1819 - loglik: -6.3564e+02 - logprior: -1.5386e+00
Fitted a model with MAP estimate = -636.8763
expansions: [(0, 3), (13, 1), (14, 1), (34, 1), (52, 5), (55, 1), (62, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (77, 1), (78, 1), (79, 1), (80, 1), (84, 1), (90, 1), (91, 1), (92, 1), (98, 1), (101, 1), (114, 1), (116, 1), (118, 2), (119, 1), (121, 1), (135, 1), (140, 1), (143, 1), (146, 1), (149, 1), (158, 1), (159, 1), (160, 1), (162, 1), (164, 1), (165, 1), (172, 1), (174, 1), (184, 1), (192, 1), (194, 1), (195, 1), (196, 2), (197, 1), (200, 1), (201, 1), (213, 1), (214, 1), (216, 1), (217, 1), (219, 1), (226, 1), (227, 2), (233, 1), (234, 2), (236, 1), (261, 1), (262, 2), (264, 3), (266, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 369 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 82s - loss: 627.4325 - loglik: -6.2573e+02 - logprior: -1.7016e+00
Epoch 2/2
39/39 - 81s - loss: 613.7974 - loglik: -6.1342e+02 - logprior: -3.7502e-01
Fitted a model with MAP estimate = -611.2910
expansions: [(206, 1)]
discards: [  1   2 288]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 616.3901 - loglik: -6.1518e+02 - logprior: -1.2077e+00
Epoch 2/2
39/39 - 79s - loss: 611.8308 - loglik: -6.1171e+02 - logprior: -1.2432e-01
Fitted a model with MAP estimate = -609.9363
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 79s - loss: 614.4307 - loglik: -6.1336e+02 - logprior: -1.0669e+00
Epoch 2/10
39/39 - 63s - loss: 611.3065 - loglik: -6.1134e+02 - logprior: 0.0369
Epoch 3/10
39/39 - 57s - loss: 609.4431 - loglik: -6.0961e+02 - logprior: 0.1646
Epoch 4/10
39/39 - 55s - loss: 607.7306 - loglik: -6.0803e+02 - logprior: 0.2983
Epoch 5/10
39/39 - 58s - loss: 607.1715 - loglik: -6.0756e+02 - logprior: 0.3853
Epoch 6/10
39/39 - 66s - loss: 606.4095 - loglik: -6.0688e+02 - logprior: 0.4694
Epoch 7/10
39/39 - 72s - loss: 605.6628 - loglik: -6.0623e+02 - logprior: 0.5637
Epoch 8/10
39/39 - 75s - loss: 606.2893 - loglik: -6.0697e+02 - logprior: 0.6825
Fitted a model with MAP estimate = -605.2300
Time for alignment: 1595.8796
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 784.8930 - loglik: -7.8338e+02 - logprior: -1.5155e+00
Epoch 2/10
39/39 - 50s - loss: 657.3289 - loglik: -6.5560e+02 - logprior: -1.7321e+00
Epoch 3/10
39/39 - 51s - loss: 645.6619 - loglik: -6.4404e+02 - logprior: -1.6225e+00
Epoch 4/10
39/39 - 52s - loss: 642.8331 - loglik: -6.4130e+02 - logprior: -1.5365e+00
Epoch 5/10
39/39 - 54s - loss: 641.2875 - loglik: -6.3974e+02 - logprior: -1.5500e+00
Epoch 6/10
39/39 - 52s - loss: 640.8673 - loglik: -6.3934e+02 - logprior: -1.5282e+00
Epoch 7/10
39/39 - 54s - loss: 639.9510 - loglik: -6.3841e+02 - logprior: -1.5374e+00
Epoch 8/10
39/39 - 52s - loss: 639.4098 - loglik: -6.3788e+02 - logprior: -1.5301e+00
Epoch 9/10
39/39 - 51s - loss: 638.8210 - loglik: -6.3730e+02 - logprior: -1.5200e+00
Epoch 10/10
39/39 - 47s - loss: 639.3255 - loglik: -6.3782e+02 - logprior: -1.5047e+00
Fitted a model with MAP estimate = -638.1566
expansions: [(0, 3), (13, 2), (14, 1), (45, 1), (46, 1), (50, 5), (54, 1), (61, 1), (62, 2), (63, 1), (66, 1), (67, 1), (76, 1), (77, 1), (78, 1), (82, 1), (83, 1), (90, 1), (91, 1), (94, 1), (101, 1), (114, 1), (116, 1), (117, 1), (118, 2), (119, 2), (134, 1), (140, 1), (143, 1), (146, 1), (159, 1), (160, 1), (161, 1), (163, 1), (165, 1), (166, 1), (173, 1), (174, 1), (185, 1), (191, 1), (192, 1), (194, 1), (195, 1), (196, 2), (199, 2), (207, 1), (214, 1), (215, 1), (217, 1), (218, 1), (228, 1), (229, 2), (232, 1), (234, 1), (235, 2), (237, 1), (239, 1), (261, 1), (262, 1), (263, 1), (264, 3), (266, 1), (277, 1), (279, 1), (280, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 372 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 629.0482 - loglik: -6.2725e+02 - logprior: -1.7941e+00
Epoch 2/2
39/39 - 57s - loss: 613.3889 - loglik: -6.1289e+02 - logprior: -5.0140e-01
Fitted a model with MAP estimate = -610.9794
expansions: [(208, 1)]
discards: [  1   2  58  59  60 149 254]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 616.8843 - loglik: -6.1570e+02 - logprior: -1.1847e+00
Epoch 2/2
39/39 - 57s - loss: 612.0897 - loglik: -6.1195e+02 - logprior: -1.4291e-01
Fitted a model with MAP estimate = -610.1921
expansions: [(56, 2)]
discards: [287]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 614.9893 - loglik: -6.1378e+02 - logprior: -1.2109e+00
Epoch 2/10
39/39 - 56s - loss: 611.5637 - loglik: -6.1164e+02 - logprior: 0.0764
Epoch 3/10
39/39 - 55s - loss: 609.1729 - loglik: -6.0940e+02 - logprior: 0.2292
Epoch 4/10
39/39 - 55s - loss: 608.1851 - loglik: -6.0845e+02 - logprior: 0.2613
Epoch 5/10
39/39 - 54s - loss: 607.3315 - loglik: -6.0771e+02 - logprior: 0.3749
Epoch 6/10
39/39 - 55s - loss: 606.5437 - loglik: -6.0701e+02 - logprior: 0.4620
Epoch 7/10
39/39 - 56s - loss: 606.6249 - loglik: -6.0724e+02 - logprior: 0.6116
Fitted a model with MAP estimate = -605.7281
Time for alignment: 1357.9837
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 783.4873 - loglik: -7.8198e+02 - logprior: -1.5106e+00
Epoch 2/10
39/39 - 37s - loss: 655.7116 - loglik: -6.5394e+02 - logprior: -1.7760e+00
Epoch 3/10
39/39 - 40s - loss: 642.8971 - loglik: -6.4117e+02 - logprior: -1.7315e+00
Epoch 4/10
39/39 - 42s - loss: 640.5552 - loglik: -6.3889e+02 - logprior: -1.6646e+00
Epoch 5/10
39/39 - 44s - loss: 638.5470 - loglik: -6.3690e+02 - logprior: -1.6425e+00
Epoch 6/10
39/39 - 45s - loss: 638.4030 - loglik: -6.3676e+02 - logprior: -1.6444e+00
Epoch 7/10
39/39 - 45s - loss: 637.3696 - loglik: -6.3574e+02 - logprior: -1.6256e+00
Epoch 8/10
39/39 - 46s - loss: 637.1669 - loglik: -6.3553e+02 - logprior: -1.6322e+00
Epoch 9/10
39/39 - 47s - loss: 636.7499 - loglik: -6.3513e+02 - logprior: -1.6240e+00
Epoch 10/10
39/39 - 46s - loss: 636.9827 - loglik: -6.3537e+02 - logprior: -1.6153e+00
Fitted a model with MAP estimate = -635.6116
expansions: [(0, 3), (11, 1), (12, 1), (13, 1), (45, 1), (51, 5), (55, 1), (61, 1), (62, 1), (63, 1), (65, 1), (67, 1), (68, 1), (75, 1), (76, 1), (77, 1), (78, 1), (83, 1), (91, 1), (92, 1), (95, 1), (98, 1), (101, 1), (114, 1), (117, 2), (119, 1), (120, 1), (121, 1), (141, 1), (144, 1), (147, 1), (150, 1), (159, 1), (160, 1), (161, 1), (163, 1), (165, 1), (166, 1), (176, 2), (185, 1), (193, 1), (195, 1), (196, 1), (197, 1), (199, 1), (200, 2), (204, 1), (207, 1), (214, 1), (215, 1), (217, 1), (218, 1), (220, 1), (228, 1), (229, 1), (233, 1), (234, 1), (235, 2), (237, 1), (239, 1), (261, 1), (262, 2), (264, 3), (266, 1), (267, 1), (278, 2), (279, 1), (280, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 371 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 627.3676 - loglik: -6.2559e+02 - logprior: -1.7781e+00
Epoch 2/2
39/39 - 73s - loss: 613.3085 - loglik: -6.1282e+02 - logprior: -4.8727e-01
Fitted a model with MAP estimate = -610.6608
expansions: [(207, 1)]
discards: [  1   2  58  60 253]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 615.7064 - loglik: -6.1448e+02 - logprior: -1.2257e+00
Epoch 2/2
39/39 - 62s - loss: 611.5831 - loglik: -6.1148e+02 - logprior: -1.0128e-01
Fitted a model with MAP estimate = -609.5293
expansions: []
discards: [ 56 145 288]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 62s - loss: 615.5305 - loglik: -6.1447e+02 - logprior: -1.0594e+00
Epoch 2/10
39/39 - 58s - loss: 611.8796 - loglik: -6.1198e+02 - logprior: 0.0999
Epoch 3/10
39/39 - 61s - loss: 609.8275 - loglik: -6.1002e+02 - logprior: 0.1911
Epoch 4/10
39/39 - 59s - loss: 608.6382 - loglik: -6.0882e+02 - logprior: 0.1834
Epoch 5/10
39/39 - 59s - loss: 607.4219 - loglik: -6.0775e+02 - logprior: 0.3312
Epoch 6/10
39/39 - 59s - loss: 607.4525 - loglik: -6.0789e+02 - logprior: 0.4414
Fitted a model with MAP estimate = -606.6368
Time for alignment: 1329.0474
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 784.8508 - loglik: -7.8332e+02 - logprior: -1.5263e+00
Epoch 2/10
39/39 - 41s - loss: 655.3807 - loglik: -6.5363e+02 - logprior: -1.7522e+00
Epoch 3/10
39/39 - 41s - loss: 644.2573 - loglik: -6.4259e+02 - logprior: -1.6627e+00
Epoch 4/10
39/39 - 41s - loss: 641.6236 - loglik: -6.4003e+02 - logprior: -1.5887e+00
Epoch 5/10
39/39 - 42s - loss: 640.9133 - loglik: -6.3934e+02 - logprior: -1.5727e+00
Epoch 6/10
39/39 - 42s - loss: 639.4380 - loglik: -6.3787e+02 - logprior: -1.5674e+00
Epoch 7/10
39/39 - 45s - loss: 639.0327 - loglik: -6.3746e+02 - logprior: -1.5684e+00
Epoch 8/10
39/39 - 47s - loss: 638.4783 - loglik: -6.3692e+02 - logprior: -1.5548e+00
Epoch 9/10
39/39 - 46s - loss: 638.2817 - loglik: -6.3672e+02 - logprior: -1.5582e+00
Epoch 10/10
39/39 - 47s - loss: 637.8397 - loglik: -6.3630e+02 - logprior: -1.5443e+00
Fitted a model with MAP estimate = -637.2417
expansions: [(0, 3), (13, 1), (14, 1), (34, 1), (45, 1), (51, 5), (55, 1), (62, 1), (63, 2), (64, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (79, 1), (80, 1), (82, 1), (83, 1), (90, 1), (91, 1), (94, 1), (97, 1), (100, 1), (113, 1), (114, 1), (117, 1), (118, 1), (119, 1), (120, 1), (140, 1), (142, 1), (143, 1), (146, 1), (158, 1), (161, 1), (163, 1), (167, 1), (174, 2), (175, 3), (193, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 2), (200, 1), (214, 1), (215, 1), (219, 1), (225, 1), (226, 1), (227, 1), (228, 2), (234, 1), (235, 2), (237, 1), (239, 1), (261, 1), (262, 2), (264, 3), (266, 1), (267, 1), (278, 2), (279, 1), (280, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 373 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 628.7082 - loglik: -6.2686e+02 - logprior: -1.8503e+00
Epoch 2/2
39/39 - 75s - loss: 613.9418 - loglik: -6.1337e+02 - logprior: -5.7643e-01
Fitted a model with MAP estimate = -610.9492
expansions: [(213, 1)]
discards: [  1   2  58  59  60  80 220 255]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 81s - loss: 617.0833 - loglik: -6.1588e+02 - logprior: -1.2012e+00
Epoch 2/2
39/39 - 76s - loss: 612.3334 - loglik: -6.1221e+02 - logprior: -1.2455e-01
Fitted a model with MAP estimate = -610.0315
expansions: [(56, 2)]
discards: [287]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 367 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 82s - loss: 614.7681 - loglik: -6.1370e+02 - logprior: -1.0640e+00
Epoch 2/10
39/39 - 80s - loss: 611.4752 - loglik: -6.1156e+02 - logprior: 0.0821
Epoch 3/10
39/39 - 83s - loss: 609.4421 - loglik: -6.0958e+02 - logprior: 0.1347
Epoch 4/10
39/39 - 80s - loss: 607.6871 - loglik: -6.0799e+02 - logprior: 0.3019
Epoch 5/10
39/39 - 80s - loss: 607.2334 - loglik: -6.0756e+02 - logprior: 0.3309
Epoch 6/10
39/39 - 80s - loss: 606.6275 - loglik: -6.0712e+02 - logprior: 0.4971
Epoch 7/10
39/39 - 80s - loss: 606.0923 - loglik: -6.0669e+02 - logprior: 0.5988
Epoch 8/10
39/39 - 83s - loss: 606.4388 - loglik: -6.0714e+02 - logprior: 0.7002
Fitted a model with MAP estimate = -605.2672
Time for alignment: 1695.2753
Computed alignments with likelihoods: ['-606.3571', '-605.2300', '-605.7281', '-606.6368', '-605.2672']
Best model has likelihood: -605.2300  (prior= 0.7978 )
time for generating output: 0.3227
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.9106753812636166
Training of 5 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4daf298eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d855fcc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4be0255eb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4db7a751f0>
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 151.0618 - loglik: -1.4789e+02 - logprior: -3.1752e+00
Epoch 2/10
19/19 - 1s - loss: 131.2857 - loglik: -1.2992e+02 - logprior: -1.3671e+00
Epoch 3/10
19/19 - 1s - loss: 122.2828 - loglik: -1.2073e+02 - logprior: -1.5566e+00
Epoch 4/10
19/19 - 1s - loss: 119.4610 - loglik: -1.1798e+02 - logprior: -1.4860e+00
Epoch 5/10
19/19 - 1s - loss: 118.7473 - loglik: -1.1727e+02 - logprior: -1.4812e+00
Epoch 6/10
19/19 - 1s - loss: 118.5438 - loglik: -1.1710e+02 - logprior: -1.4483e+00
Epoch 7/10
19/19 - 1s - loss: 118.2488 - loglik: -1.1681e+02 - logprior: -1.4373e+00
Epoch 8/10
19/19 - 1s - loss: 118.2648 - loglik: -1.1684e+02 - logprior: -1.4262e+00
Fitted a model with MAP estimate = -117.9539
expansions: [(13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (27, 2), (28, 2), (32, 1), (33, 2), (35, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 122.1480 - loglik: -1.1806e+02 - logprior: -4.0880e+00
Epoch 2/2
19/19 - 1s - loss: 114.3097 - loglik: -1.1231e+02 - logprior: -1.9950e+00
Fitted a model with MAP estimate = -112.9918
expansions: [(0, 2)]
discards: [ 0 20 34 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.2780 - loglik: -1.1125e+02 - logprior: -3.0276e+00
Epoch 2/2
19/19 - 1s - loss: 111.3832 - loglik: -1.1016e+02 - logprior: -1.2250e+00
Fitted a model with MAP estimate = -110.8551
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.9303 - loglik: -1.1219e+02 - logprior: -3.7448e+00
Epoch 2/10
19/19 - 1s - loss: 111.7773 - loglik: -1.1045e+02 - logprior: -1.3226e+00
Epoch 3/10
19/19 - 1s - loss: 111.0688 - loglik: -1.0991e+02 - logprior: -1.1567e+00
Epoch 4/10
19/19 - 1s - loss: 110.9176 - loglik: -1.0980e+02 - logprior: -1.1151e+00
Epoch 5/10
19/19 - 1s - loss: 110.5876 - loglik: -1.0949e+02 - logprior: -1.0997e+00
Epoch 6/10
19/19 - 1s - loss: 110.5693 - loglik: -1.0948e+02 - logprior: -1.0848e+00
Epoch 7/10
19/19 - 1s - loss: 110.4507 - loglik: -1.0938e+02 - logprior: -1.0658e+00
Epoch 8/10
19/19 - 1s - loss: 110.3902 - loglik: -1.0934e+02 - logprior: -1.0507e+00
Epoch 9/10
19/19 - 1s - loss: 110.1171 - loglik: -1.0908e+02 - logprior: -1.0396e+00
Epoch 10/10
19/19 - 1s - loss: 110.0698 - loglik: -1.0904e+02 - logprior: -1.0315e+00
Fitted a model with MAP estimate = -110.0207
Time for alignment: 41.6561
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 151.1202 - loglik: -1.4795e+02 - logprior: -3.1751e+00
Epoch 2/10
19/19 - 1s - loss: 131.1782 - loglik: -1.2981e+02 - logprior: -1.3700e+00
Epoch 3/10
19/19 - 1s - loss: 121.9237 - loglik: -1.2034e+02 - logprior: -1.5819e+00
Epoch 4/10
19/19 - 1s - loss: 119.6502 - loglik: -1.1816e+02 - logprior: -1.4951e+00
Epoch 5/10
19/19 - 1s - loss: 118.9334 - loglik: -1.1744e+02 - logprior: -1.4900e+00
Epoch 6/10
19/19 - 1s - loss: 118.6203 - loglik: -1.1717e+02 - logprior: -1.4525e+00
Epoch 7/10
19/19 - 1s - loss: 118.4875 - loglik: -1.1704e+02 - logprior: -1.4446e+00
Epoch 8/10
19/19 - 1s - loss: 118.2888 - loglik: -1.1686e+02 - logprior: -1.4298e+00
Epoch 9/10
19/19 - 1s - loss: 118.4662 - loglik: -1.1704e+02 - logprior: -1.4247e+00
Fitted a model with MAP estimate = -118.1057
expansions: [(13, 1), (14, 1), (15, 1), (16, 1), (18, 2), (19, 2), (27, 2), (28, 2), (32, 1), (33, 2), (35, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.8206 - loglik: -1.1871e+02 - logprior: -4.1124e+00
Epoch 2/2
19/19 - 1s - loss: 114.4464 - loglik: -1.1241e+02 - logprior: -2.0396e+00
Fitted a model with MAP estimate = -113.0728
expansions: [(0, 2)]
discards: [ 0 22 24 35 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.3801 - loglik: -1.1135e+02 - logprior: -3.0324e+00
Epoch 2/2
19/19 - 1s - loss: 111.3217 - loglik: -1.1010e+02 - logprior: -1.2258e+00
Fitted a model with MAP estimate = -110.8310
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 115.9454 - loglik: -1.1220e+02 - logprior: -3.7450e+00
Epoch 2/10
19/19 - 1s - loss: 111.7208 - loglik: -1.1039e+02 - logprior: -1.3321e+00
Epoch 3/10
19/19 - 1s - loss: 111.0960 - loglik: -1.0993e+02 - logprior: -1.1638e+00
Epoch 4/10
19/19 - 1s - loss: 110.8603 - loglik: -1.0974e+02 - logprior: -1.1197e+00
Epoch 5/10
19/19 - 1s - loss: 110.6200 - loglik: -1.0952e+02 - logprior: -1.0990e+00
Epoch 6/10
19/19 - 1s - loss: 110.5581 - loglik: -1.0947e+02 - logprior: -1.0889e+00
Epoch 7/10
19/19 - 1s - loss: 110.5831 - loglik: -1.0952e+02 - logprior: -1.0661e+00
Fitted a model with MAP estimate = -110.2806
Time for alignment: 39.0176
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 151.0437 - loglik: -1.4787e+02 - logprior: -3.1736e+00
Epoch 2/10
19/19 - 1s - loss: 131.0087 - loglik: -1.2964e+02 - logprior: -1.3706e+00
Epoch 3/10
19/19 - 1s - loss: 121.8841 - loglik: -1.2032e+02 - logprior: -1.5687e+00
Epoch 4/10
19/19 - 1s - loss: 119.5876 - loglik: -1.1810e+02 - logprior: -1.4858e+00
Epoch 5/10
19/19 - 1s - loss: 118.9553 - loglik: -1.1747e+02 - logprior: -1.4857e+00
Epoch 6/10
19/19 - 1s - loss: 118.6978 - loglik: -1.1725e+02 - logprior: -1.4513e+00
Epoch 7/10
19/19 - 1s - loss: 118.5331 - loglik: -1.1709e+02 - logprior: -1.4406e+00
Epoch 8/10
19/19 - 1s - loss: 118.4177 - loglik: -1.1699e+02 - logprior: -1.4312e+00
Epoch 9/10
19/19 - 1s - loss: 118.2682 - loglik: -1.1684e+02 - logprior: -1.4241e+00
Epoch 10/10
19/19 - 1s - loss: 118.2113 - loglik: -1.1679e+02 - logprior: -1.4215e+00
Fitted a model with MAP estimate = -118.0448
expansions: [(13, 1), (14, 1), (15, 1), (16, 1), (18, 2), (19, 1), (27, 2), (28, 2), (32, 1), (33, 2), (35, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.4079 - loglik: -1.1831e+02 - logprior: -4.1004e+00
Epoch 2/2
19/19 - 1s - loss: 114.3630 - loglik: -1.1233e+02 - logprior: -2.0291e+00
Fitted a model with MAP estimate = -113.0695
expansions: [(0, 2)]
discards: [ 0 22 34 36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.2601 - loglik: -1.1123e+02 - logprior: -3.0309e+00
Epoch 2/2
19/19 - 1s - loss: 111.4538 - loglik: -1.1023e+02 - logprior: -1.2277e+00
Fitted a model with MAP estimate = -110.8247
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.8844 - loglik: -1.1216e+02 - logprior: -3.7249e+00
Epoch 2/10
19/19 - 1s - loss: 111.8138 - loglik: -1.1049e+02 - logprior: -1.3262e+00
Epoch 3/10
19/19 - 1s - loss: 110.9570 - loglik: -1.0980e+02 - logprior: -1.1592e+00
Epoch 4/10
19/19 - 1s - loss: 110.9170 - loglik: -1.0979e+02 - logprior: -1.1221e+00
Epoch 5/10
19/19 - 1s - loss: 110.5308 - loglik: -1.0943e+02 - logprior: -1.1003e+00
Epoch 6/10
19/19 - 1s - loss: 110.6529 - loglik: -1.0957e+02 - logprior: -1.0822e+00
Fitted a model with MAP estimate = -110.3641
Time for alignment: 39.3258
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.9422 - loglik: -1.4777e+02 - logprior: -3.1754e+00
Epoch 2/10
19/19 - 1s - loss: 130.8938 - loglik: -1.2952e+02 - logprior: -1.3713e+00
Epoch 3/10
19/19 - 1s - loss: 121.6745 - loglik: -1.2008e+02 - logprior: -1.5933e+00
Epoch 4/10
19/19 - 1s - loss: 119.3866 - loglik: -1.1789e+02 - logprior: -1.4973e+00
Epoch 5/10
19/19 - 1s - loss: 118.6485 - loglik: -1.1716e+02 - logprior: -1.4854e+00
Epoch 6/10
19/19 - 1s - loss: 118.5249 - loglik: -1.1708e+02 - logprior: -1.4480e+00
Epoch 7/10
19/19 - 1s - loss: 118.3027 - loglik: -1.1686e+02 - logprior: -1.4400e+00
Epoch 8/10
19/19 - 1s - loss: 118.2110 - loglik: -1.1678e+02 - logprior: -1.4281e+00
Epoch 9/10
19/19 - 1s - loss: 118.0786 - loglik: -1.1666e+02 - logprior: -1.4198e+00
Epoch 10/10
19/19 - 1s - loss: 118.1598 - loglik: -1.1674e+02 - logprior: -1.4178e+00
Fitted a model with MAP estimate = -117.8234
expansions: [(13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (27, 1), (28, 2), (32, 2), (33, 2), (35, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 121.9133 - loglik: -1.1781e+02 - logprior: -4.1046e+00
Epoch 2/2
19/19 - 1s - loss: 114.3709 - loglik: -1.1236e+02 - logprior: -2.0064e+00
Fitted a model with MAP estimate = -113.0777
expansions: [(0, 2)]
discards: [ 0 34 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.3058 - loglik: -1.1127e+02 - logprior: -3.0323e+00
Epoch 2/2
19/19 - 1s - loss: 111.3507 - loglik: -1.1013e+02 - logprior: -1.2218e+00
Fitted a model with MAP estimate = -110.8457
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.9957 - loglik: -1.1223e+02 - logprior: -3.7628e+00
Epoch 2/10
19/19 - 1s - loss: 111.7198 - loglik: -1.1039e+02 - logprior: -1.3277e+00
Epoch 3/10
19/19 - 1s - loss: 111.0105 - loglik: -1.0986e+02 - logprior: -1.1539e+00
Epoch 4/10
19/19 - 1s - loss: 110.9654 - loglik: -1.0985e+02 - logprior: -1.1156e+00
Epoch 5/10
19/19 - 1s - loss: 110.6997 - loglik: -1.0961e+02 - logprior: -1.0931e+00
Epoch 6/10
19/19 - 1s - loss: 110.4058 - loglik: -1.0932e+02 - logprior: -1.0845e+00
Epoch 7/10
19/19 - 1s - loss: 110.3815 - loglik: -1.0932e+02 - logprior: -1.0585e+00
Epoch 8/10
19/19 - 1s - loss: 110.2179 - loglik: -1.0916e+02 - logprior: -1.0550e+00
Epoch 9/10
19/19 - 1s - loss: 110.2943 - loglik: -1.0926e+02 - logprior: -1.0362e+00
Fitted a model with MAP estimate = -110.0816
Time for alignment: 41.7749
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 151.0127 - loglik: -1.4784e+02 - logprior: -3.1732e+00
Epoch 2/10
19/19 - 1s - loss: 131.2736 - loglik: -1.2993e+02 - logprior: -1.3412e+00
Epoch 3/10
19/19 - 1s - loss: 121.8110 - loglik: -1.2028e+02 - logprior: -1.5315e+00
Epoch 4/10
19/19 - 1s - loss: 119.6114 - loglik: -1.1818e+02 - logprior: -1.4349e+00
Epoch 5/10
19/19 - 1s - loss: 119.1912 - loglik: -1.1776e+02 - logprior: -1.4325e+00
Epoch 6/10
19/19 - 1s - loss: 118.8569 - loglik: -1.1745e+02 - logprior: -1.4029e+00
Epoch 7/10
19/19 - 1s - loss: 118.6395 - loglik: -1.1725e+02 - logprior: -1.3878e+00
Epoch 8/10
19/19 - 1s - loss: 118.5720 - loglik: -1.1719e+02 - logprior: -1.3792e+00
Epoch 9/10
19/19 - 1s - loss: 118.4323 - loglik: -1.1706e+02 - logprior: -1.3681e+00
Epoch 10/10
19/19 - 1s - loss: 118.4461 - loglik: -1.1708e+02 - logprior: -1.3675e+00
Fitted a model with MAP estimate = -118.1883
expansions: [(13, 1), (16, 4), (19, 2), (20, 1), (27, 2), (28, 2), (32, 1), (33, 2), (35, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.9754 - loglik: -1.1886e+02 - logprior: -4.1179e+00
Epoch 2/2
19/19 - 1s - loss: 114.7438 - loglik: -1.1268e+02 - logprior: -2.0664e+00
Fitted a model with MAP estimate = -113.3043
expansions: [(0, 2)]
discards: [ 0 17 24 35 37]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 114.6263 - loglik: -1.1159e+02 - logprior: -3.0353e+00
Epoch 2/2
19/19 - 1s - loss: 111.4201 - loglik: -1.1020e+02 - logprior: -1.2201e+00
Fitted a model with MAP estimate = -110.8776
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 116.0097 - loglik: -1.1225e+02 - logprior: -3.7604e+00
Epoch 2/10
19/19 - 1s - loss: 111.8369 - loglik: -1.1051e+02 - logprior: -1.3260e+00
Epoch 3/10
19/19 - 1s - loss: 110.9994 - loglik: -1.0984e+02 - logprior: -1.1547e+00
Epoch 4/10
19/19 - 1s - loss: 110.8083 - loglik: -1.0970e+02 - logprior: -1.1096e+00
Epoch 5/10
19/19 - 1s - loss: 110.5728 - loglik: -1.0948e+02 - logprior: -1.0948e+00
Epoch 6/10
19/19 - 1s - loss: 110.5699 - loglik: -1.0949e+02 - logprior: -1.0759e+00
Epoch 7/10
19/19 - 1s - loss: 110.1716 - loglik: -1.0910e+02 - logprior: -1.0705e+00
Epoch 8/10
19/19 - 1s - loss: 110.3356 - loglik: -1.0929e+02 - logprior: -1.0455e+00
Fitted a model with MAP estimate = -110.1242
Time for alignment: 39.8547
Computed alignments with likelihoods: ['-110.0207', '-110.2806', '-110.3641', '-110.0816', '-110.1242']
Best model has likelihood: -110.0207  (prior= -1.0116 )
time for generating output: 0.1087
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.6143869616366818
Training of 5 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d7ce43970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4dd414ad60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d1bc1c190>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4da6d29790>
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 364.0167 - loglik: -3.6102e+02 - logprior: -2.9981e+00
Epoch 2/10
19/19 - 3s - loss: 309.4382 - loglik: -3.0828e+02 - logprior: -1.1557e+00
Epoch 3/10
19/19 - 3s - loss: 282.7131 - loglik: -2.8143e+02 - logprior: -1.2798e+00
Epoch 4/10
19/19 - 3s - loss: 276.0916 - loglik: -2.7487e+02 - logprior: -1.2200e+00
Epoch 5/10
19/19 - 3s - loss: 272.3152 - loglik: -2.7112e+02 - logprior: -1.1914e+00
Epoch 6/10
19/19 - 3s - loss: 270.0923 - loglik: -2.6894e+02 - logprior: -1.1532e+00
Epoch 7/10
19/19 - 3s - loss: 269.4559 - loglik: -2.6831e+02 - logprior: -1.1432e+00
Epoch 8/10
19/19 - 3s - loss: 269.3658 - loglik: -2.6825e+02 - logprior: -1.1170e+00
Epoch 9/10
19/19 - 4s - loss: 269.0023 - loglik: -2.6789e+02 - logprior: -1.1126e+00
Epoch 10/10
19/19 - 4s - loss: 269.2060 - loglik: -2.6810e+02 - logprior: -1.1062e+00
Fitted a model with MAP estimate = -268.6668
expansions: [(12, 2), (14, 2), (15, 1), (17, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (41, 1), (43, 1), (44, 1), (45, 1), (56, 2), (57, 2), (58, 1), (70, 1), (71, 1), (94, 1), (99, 2), (102, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 272.1453 - loglik: -2.6823e+02 - logprior: -3.9124e+00
Epoch 2/2
19/19 - 5s - loss: 261.4442 - loglik: -2.5956e+02 - logprior: -1.8822e+00
Fitted a model with MAP estimate = -259.8514
expansions: [(0, 2)]
discards: [  0  12  16  38  73  76 124 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 263.2963 - loglik: -2.6054e+02 - logprior: -2.7551e+00
Epoch 2/2
19/19 - 5s - loss: 259.4070 - loglik: -2.5847e+02 - logprior: -9.3376e-01
Fitted a model with MAP estimate = -258.7234
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 263.7684 - loglik: -2.6006e+02 - logprior: -3.7069e+00
Epoch 2/10
19/19 - 4s - loss: 260.1053 - loglik: -2.5890e+02 - logprior: -1.2004e+00
Epoch 3/10
19/19 - 6s - loss: 258.7382 - loglik: -2.5817e+02 - logprior: -5.7088e-01
Epoch 4/10
19/19 - 6s - loss: 258.0906 - loglik: -2.5753e+02 - logprior: -5.6304e-01
Epoch 5/10
19/19 - 5s - loss: 256.9897 - loglik: -2.5649e+02 - logprior: -5.0370e-01
Epoch 6/10
19/19 - 6s - loss: 257.3743 - loglik: -2.5693e+02 - logprior: -4.4204e-01
Fitted a model with MAP estimate = -256.7674
Time for alignment: 127.4841
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 363.8495 - loglik: -3.6085e+02 - logprior: -3.0004e+00
Epoch 2/10
19/19 - 4s - loss: 306.5983 - loglik: -3.0540e+02 - logprior: -1.1992e+00
Epoch 3/10
19/19 - 4s - loss: 280.5251 - loglik: -2.7926e+02 - logprior: -1.2679e+00
Epoch 4/10
19/19 - 4s - loss: 274.7543 - loglik: -2.7356e+02 - logprior: -1.1914e+00
Epoch 5/10
19/19 - 4s - loss: 272.7122 - loglik: -2.7157e+02 - logprior: -1.1471e+00
Epoch 6/10
19/19 - 3s - loss: 272.4009 - loglik: -2.7130e+02 - logprior: -1.1014e+00
Epoch 7/10
19/19 - 4s - loss: 271.2800 - loglik: -2.7021e+02 - logprior: -1.0688e+00
Epoch 8/10
19/19 - 4s - loss: 271.5616 - loglik: -2.7051e+02 - logprior: -1.0550e+00
Fitted a model with MAP estimate = -270.9248
expansions: [(14, 2), (15, 1), (17, 1), (18, 1), (19, 1), (26, 1), (27, 1), (29, 2), (30, 1), (41, 1), (43, 1), (44, 1), (45, 1), (55, 1), (57, 2), (58, 2), (70, 1), (71, 1), (94, 1), (99, 2), (102, 1), (103, 1), (105, 1), (108, 1), (109, 2), (110, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 270.8927 - loglik: -2.6771e+02 - logprior: -3.1810e+00
Epoch 2/2
19/19 - 5s - loss: 260.7322 - loglik: -2.5957e+02 - logprior: -1.1615e+00
Fitted a model with MAP estimate = -259.2580
expansions: [(144, 1)]
discards: [ 15  38  74  76 122 141 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 263.6285 - loglik: -2.6055e+02 - logprior: -3.0754e+00
Epoch 2/2
19/19 - 5s - loss: 259.9216 - loglik: -2.5888e+02 - logprior: -1.0403e+00
Fitted a model with MAP estimate = -258.8397
expansions: [(138, 1)]
discards: [ 36 135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 262.5807 - loglik: -2.5961e+02 - logprior: -2.9730e+00
Epoch 2/10
19/19 - 5s - loss: 259.4136 - loglik: -2.5843e+02 - logprior: -9.8377e-01
Epoch 3/10
19/19 - 4s - loss: 258.5909 - loglik: -2.5785e+02 - logprior: -7.4193e-01
Epoch 4/10
19/19 - 4s - loss: 258.0573 - loglik: -2.5743e+02 - logprior: -6.2264e-01
Epoch 5/10
19/19 - 5s - loss: 257.6791 - loglik: -2.5713e+02 - logprior: -5.5237e-01
Epoch 6/10
19/19 - 5s - loss: 257.4906 - loglik: -2.5698e+02 - logprior: -5.0619e-01
Epoch 7/10
19/19 - 5s - loss: 257.3121 - loglik: -2.5685e+02 - logprior: -4.6199e-01
Epoch 8/10
19/19 - 5s - loss: 256.9559 - loglik: -2.5651e+02 - logprior: -4.4537e-01
Epoch 9/10
19/19 - 5s - loss: 256.9057 - loglik: -2.5649e+02 - logprior: -4.1357e-01
Epoch 10/10
19/19 - 5s - loss: 256.4996 - loglik: -2.5611e+02 - logprior: -3.8705e-01
Fitted a model with MAP estimate = -256.4108
Time for alignment: 134.8514
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 363.8951 - loglik: -3.6090e+02 - logprior: -2.9977e+00
Epoch 2/10
19/19 - 4s - loss: 304.3738 - loglik: -3.0318e+02 - logprior: -1.1979e+00
Epoch 3/10
19/19 - 4s - loss: 278.3378 - loglik: -2.7708e+02 - logprior: -1.2611e+00
Epoch 4/10
19/19 - 4s - loss: 273.4061 - loglik: -2.7224e+02 - logprior: -1.1624e+00
Epoch 5/10
19/19 - 4s - loss: 271.0754 - loglik: -2.6998e+02 - logprior: -1.0913e+00
Epoch 6/10
19/19 - 4s - loss: 269.7269 - loglik: -2.6865e+02 - logprior: -1.0784e+00
Epoch 7/10
19/19 - 4s - loss: 268.9311 - loglik: -2.6786e+02 - logprior: -1.0700e+00
Epoch 8/10
19/19 - 4s - loss: 269.5797 - loglik: -2.6852e+02 - logprior: -1.0591e+00
Fitted a model with MAP estimate = -268.7139
expansions: [(14, 2), (15, 1), (17, 1), (19, 1), (26, 1), (28, 1), (29, 3), (30, 2), (41, 1), (43, 1), (44, 1), (45, 1), (56, 2), (57, 2), (58, 1), (70, 1), (71, 1), (98, 1), (99, 2), (102, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 269.5331 - loglik: -2.6634e+02 - logprior: -3.1908e+00
Epoch 2/2
19/19 - 6s - loss: 259.8180 - loglik: -2.5860e+02 - logprior: -1.2186e+00
Fitted a model with MAP estimate = -258.1833
expansions: [(145, 1)]
discards: [ 15  37  38  73  76 123 142 143]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 262.9395 - loglik: -2.5979e+02 - logprior: -3.1490e+00
Epoch 2/2
19/19 - 5s - loss: 259.1309 - loglik: -2.5805e+02 - logprior: -1.0787e+00
Fitted a model with MAP estimate = -258.0536
expansions: [(138, 1)]
discards: [135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 261.4747 - loglik: -2.5843e+02 - logprior: -3.0469e+00
Epoch 2/10
19/19 - 5s - loss: 258.9562 - loglik: -2.5792e+02 - logprior: -1.0365e+00
Epoch 3/10
19/19 - 5s - loss: 257.9113 - loglik: -2.5710e+02 - logprior: -8.0748e-01
Epoch 4/10
19/19 - 5s - loss: 257.3821 - loglik: -2.5669e+02 - logprior: -6.9251e-01
Epoch 5/10
19/19 - 5s - loss: 257.2216 - loglik: -2.5663e+02 - logprior: -5.9492e-01
Epoch 6/10
19/19 - 5s - loss: 256.9246 - loglik: -2.5637e+02 - logprior: -5.5131e-01
Epoch 7/10
19/19 - 6s - loss: 256.4342 - loglik: -2.5592e+02 - logprior: -5.1892e-01
Epoch 8/10
19/19 - 5s - loss: 257.0533 - loglik: -2.5657e+02 - logprior: -4.7940e-01
Fitted a model with MAP estimate = -256.3029
Time for alignment: 131.7020
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 364.2013 - loglik: -3.6121e+02 - logprior: -2.9960e+00
Epoch 2/10
19/19 - 4s - loss: 309.3454 - loglik: -3.0821e+02 - logprior: -1.1398e+00
Epoch 3/10
19/19 - 4s - loss: 282.7555 - loglik: -2.8149e+02 - logprior: -1.2634e+00
Epoch 4/10
19/19 - 4s - loss: 275.7739 - loglik: -2.7453e+02 - logprior: -1.2428e+00
Epoch 5/10
19/19 - 4s - loss: 272.8346 - loglik: -2.7164e+02 - logprior: -1.1906e+00
Epoch 6/10
19/19 - 4s - loss: 271.6974 - loglik: -2.7049e+02 - logprior: -1.2070e+00
Epoch 7/10
19/19 - 4s - loss: 271.1537 - loglik: -2.6997e+02 - logprior: -1.1825e+00
Epoch 8/10
19/19 - 4s - loss: 270.5493 - loglik: -2.6937e+02 - logprior: -1.1760e+00
Epoch 9/10
19/19 - 4s - loss: 270.3561 - loglik: -2.6920e+02 - logprior: -1.1610e+00
Epoch 10/10
19/19 - 4s - loss: 270.4190 - loglik: -2.6926e+02 - logprior: -1.1632e+00
Fitted a model with MAP estimate = -269.9872
expansions: [(12, 2), (14, 2), (15, 1), (17, 1), (23, 1), (26, 1), (27, 1), (29, 2), (30, 2), (41, 1), (43, 1), (44, 1), (45, 1), (56, 2), (57, 2), (58, 2), (70, 1), (71, 1), (94, 1), (99, 2), (102, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 146 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 273.4614 - loglik: -2.6953e+02 - logprior: -3.9311e+00
Epoch 2/2
19/19 - 5s - loss: 262.0545 - loglik: -2.6012e+02 - logprior: -1.9360e+00
Fitted a model with MAP estimate = -260.4451
expansions: [(0, 2)]
discards: [  0  12  16  38  73  76  78 125 141 142 143]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 264.1223 - loglik: -2.6123e+02 - logprior: -2.8935e+00
Epoch 2/2
19/19 - 4s - loss: 260.2135 - loglik: -2.5918e+02 - logprior: -1.0341e+00
Fitted a model with MAP estimate = -259.2490
expansions: [(135, 1), (137, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 264.2757 - loglik: -2.6053e+02 - logprior: -3.7444e+00
Epoch 2/10
19/19 - 5s - loss: 260.6627 - loglik: -2.5935e+02 - logprior: -1.3121e+00
Epoch 3/10
19/19 - 5s - loss: 259.2164 - loglik: -2.5857e+02 - logprior: -6.4148e-01
Epoch 4/10
19/19 - 5s - loss: 257.9875 - loglik: -2.5740e+02 - logprior: -5.8743e-01
Epoch 5/10
19/19 - 5s - loss: 258.2368 - loglik: -2.5769e+02 - logprior: -5.4378e-01
Fitted a model with MAP estimate = -257.5414
Time for alignment: 119.3925
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 363.7434 - loglik: -3.6075e+02 - logprior: -2.9969e+00
Epoch 2/10
19/19 - 4s - loss: 306.3905 - loglik: -3.0520e+02 - logprior: -1.1955e+00
Epoch 3/10
19/19 - 3s - loss: 278.2020 - loglik: -2.7692e+02 - logprior: -1.2814e+00
Epoch 4/10
19/19 - 4s - loss: 272.1049 - loglik: -2.7091e+02 - logprior: -1.1933e+00
Epoch 5/10
19/19 - 4s - loss: 270.5464 - loglik: -2.6943e+02 - logprior: -1.1135e+00
Epoch 6/10
19/19 - 3s - loss: 269.6500 - loglik: -2.6860e+02 - logprior: -1.0525e+00
Epoch 7/10
19/19 - 3s - loss: 268.8897 - loglik: -2.6787e+02 - logprior: -1.0210e+00
Epoch 8/10
19/19 - 4s - loss: 268.1458 - loglik: -2.6714e+02 - logprior: -1.0019e+00
Epoch 9/10
19/19 - 3s - loss: 268.1037 - loglik: -2.6711e+02 - logprior: -9.9401e-01
Epoch 10/10
19/19 - 4s - loss: 268.3453 - loglik: -2.6736e+02 - logprior: -9.8874e-01
Fitted a model with MAP estimate = -267.7807
expansions: [(14, 2), (15, 1), (17, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (41, 1), (43, 1), (44, 1), (45, 1), (56, 2), (57, 2), (58, 1), (70, 1), (71, 1), (94, 1), (99, 2), (102, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 268.8333 - loglik: -2.6565e+02 - logprior: -3.1796e+00
Epoch 2/2
19/19 - 5s - loss: 259.0969 - loglik: -2.5795e+02 - logprior: -1.1471e+00
Fitted a model with MAP estimate = -257.8528
expansions: [(144, 1)]
discards: [ 15  37  72  75 123 141 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 262.8090 - loglik: -2.5973e+02 - logprior: -3.0797e+00
Epoch 2/2
19/19 - 5s - loss: 259.0581 - loglik: -2.5806e+02 - logprior: -1.0009e+00
Fitted a model with MAP estimate = -257.9729
expansions: [(138, 1)]
discards: [135]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 261.7473 - loglik: -2.5877e+02 - logprior: -2.9774e+00
Epoch 2/10
19/19 - 5s - loss: 258.6820 - loglik: -2.5769e+02 - logprior: -9.8779e-01
Epoch 3/10
19/19 - 5s - loss: 257.9685 - loglik: -2.5723e+02 - logprior: -7.3499e-01
Epoch 4/10
19/19 - 5s - loss: 256.9941 - loglik: -2.5638e+02 - logprior: -6.1751e-01
Epoch 5/10
19/19 - 5s - loss: 256.9622 - loglik: -2.5641e+02 - logprior: -5.4812e-01
Epoch 6/10
19/19 - 5s - loss: 256.6919 - loglik: -2.5621e+02 - logprior: -4.8079e-01
Epoch 7/10
19/19 - 5s - loss: 255.7507 - loglik: -2.5530e+02 - logprior: -4.4833e-01
Epoch 8/10
19/19 - 5s - loss: 256.6143 - loglik: -2.5620e+02 - logprior: -4.1593e-01
Fitted a model with MAP estimate = -255.8914
Time for alignment: 135.0159
Computed alignments with likelihoods: ['-256.7674', '-256.4108', '-256.3029', '-257.5414', '-255.8914']
Best model has likelihood: -255.8914  (prior= -0.3831 )
time for generating output: 0.1604
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.7841059602649006
Training of 5 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c0aac2fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d963112b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d1bdee100>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4da6d29790>
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 276.1423 - loglik: -2.7307e+02 - logprior: -3.0752e+00
Epoch 2/10
19/19 - 2s - loss: 239.7152 - loglik: -2.3847e+02 - logprior: -1.2454e+00
Epoch 3/10
19/19 - 2s - loss: 222.5586 - loglik: -2.2121e+02 - logprior: -1.3437e+00
Epoch 4/10
19/19 - 2s - loss: 218.6179 - loglik: -2.1729e+02 - logprior: -1.3237e+00
Epoch 5/10
19/19 - 2s - loss: 217.6061 - loglik: -2.1634e+02 - logprior: -1.2709e+00
Epoch 6/10
19/19 - 2s - loss: 217.6066 - loglik: -2.1639e+02 - logprior: -1.2120e+00
Fitted a model with MAP estimate = -216.6339
expansions: [(0, 2), (6, 1), (7, 1), (11, 1), (16, 1), (18, 2), (19, 1), (20, 1), (29, 1), (30, 1), (34, 1), (35, 1), (48, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 218.9008 - loglik: -2.1512e+02 - logprior: -3.7851e+00
Epoch 2/2
19/19 - 3s - loss: 209.2469 - loglik: -2.0796e+02 - logprior: -1.2840e+00
Fitted a model with MAP estimate = -207.5497
expansions: []
discards: [ 0 27 74 79 82 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 213.1766 - loglik: -2.0916e+02 - logprior: -4.0175e+00
Epoch 2/2
19/19 - 3s - loss: 208.9767 - loglik: -2.0744e+02 - logprior: -1.5329e+00
Fitted a model with MAP estimate = -207.4774
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 209.9484 - loglik: -2.0706e+02 - logprior: -2.8926e+00
Epoch 2/10
19/19 - 3s - loss: 207.4212 - loglik: -2.0632e+02 - logprior: -1.1001e+00
Epoch 3/10
19/19 - 3s - loss: 206.9771 - loglik: -2.0593e+02 - logprior: -1.0439e+00
Epoch 4/10
19/19 - 3s - loss: 206.8215 - loglik: -2.0583e+02 - logprior: -9.8838e-01
Epoch 5/10
19/19 - 3s - loss: 206.3090 - loglik: -2.0535e+02 - logprior: -9.5984e-01
Epoch 6/10
19/19 - 3s - loss: 205.7214 - loglik: -2.0477e+02 - logprior: -9.4847e-01
Epoch 7/10
19/19 - 3s - loss: 205.8348 - loglik: -2.0491e+02 - logprior: -9.2825e-01
Fitted a model with MAP estimate = -205.4406
Time for alignment: 79.3215
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 276.4677 - loglik: -2.7339e+02 - logprior: -3.0740e+00
Epoch 2/10
19/19 - 2s - loss: 239.1391 - loglik: -2.3789e+02 - logprior: -1.2541e+00
Epoch 3/10
19/19 - 2s - loss: 222.0024 - loglik: -2.2063e+02 - logprior: -1.3769e+00
Epoch 4/10
19/19 - 3s - loss: 218.6289 - loglik: -2.1727e+02 - logprior: -1.3632e+00
Epoch 5/10
19/19 - 2s - loss: 217.6829 - loglik: -2.1640e+02 - logprior: -1.2839e+00
Epoch 6/10
19/19 - 2s - loss: 216.6364 - loglik: -2.1538e+02 - logprior: -1.2549e+00
Epoch 7/10
19/19 - 2s - loss: 216.7332 - loglik: -2.1551e+02 - logprior: -1.2248e+00
Fitted a model with MAP estimate = -215.9532
expansions: [(0, 2), (6, 1), (7, 1), (11, 1), (15, 1), (18, 1), (19, 1), (20, 1), (29, 1), (30, 1), (34, 1), (35, 1), (48, 1), (51, 1), (58, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 218.2689 - loglik: -2.1447e+02 - logprior: -3.7967e+00
Epoch 2/2
19/19 - 3s - loss: 208.8380 - loglik: -2.0758e+02 - logprior: -1.2569e+00
Fitted a model with MAP estimate = -207.4483
expansions: []
discards: [ 0 73 80 93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.7691 - loglik: -2.0879e+02 - logprior: -3.9813e+00
Epoch 2/2
19/19 - 3s - loss: 208.7068 - loglik: -2.0723e+02 - logprior: -1.4754e+00
Fitted a model with MAP estimate = -207.2098
expansions: [(0, 2), (22, 1)]
discards: [ 0 23]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 210.0538 - loglik: -2.0718e+02 - logprior: -2.8753e+00
Epoch 2/10
19/19 - 3s - loss: 207.3200 - loglik: -2.0623e+02 - logprior: -1.0861e+00
Epoch 3/10
19/19 - 3s - loss: 207.0920 - loglik: -2.0607e+02 - logprior: -1.0266e+00
Epoch 4/10
19/19 - 3s - loss: 206.2871 - loglik: -2.0531e+02 - logprior: -9.7257e-01
Epoch 5/10
19/19 - 3s - loss: 206.0518 - loglik: -2.0510e+02 - logprior: -9.4954e-01
Epoch 6/10
19/19 - 3s - loss: 205.2342 - loglik: -2.0430e+02 - logprior: -9.3064e-01
Epoch 7/10
19/19 - 3s - loss: 205.2920 - loglik: -2.0438e+02 - logprior: -9.0951e-01
Fitted a model with MAP estimate = -205.0807
Time for alignment: 81.0572
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 276.0365 - loglik: -2.7297e+02 - logprior: -3.0654e+00
Epoch 2/10
19/19 - 2s - loss: 239.3652 - loglik: -2.3813e+02 - logprior: -1.2401e+00
Epoch 3/10
19/19 - 2s - loss: 223.4956 - loglik: -2.2218e+02 - logprior: -1.3178e+00
Epoch 4/10
19/19 - 2s - loss: 219.8269 - loglik: -2.1851e+02 - logprior: -1.3200e+00
Epoch 5/10
19/19 - 2s - loss: 218.6073 - loglik: -2.1736e+02 - logprior: -1.2458e+00
Epoch 6/10
19/19 - 2s - loss: 218.2745 - loglik: -2.1707e+02 - logprior: -1.2055e+00
Epoch 7/10
19/19 - 3s - loss: 217.9378 - loglik: -2.1675e+02 - logprior: -1.1895e+00
Epoch 8/10
19/19 - 2s - loss: 217.4839 - loglik: -2.1632e+02 - logprior: -1.1644e+00
Epoch 9/10
19/19 - 2s - loss: 217.0801 - loglik: -2.1593e+02 - logprior: -1.1519e+00
Epoch 10/10
19/19 - 2s - loss: 217.2673 - loglik: -2.1611e+02 - logprior: -1.1602e+00
Fitted a model with MAP estimate = -216.5995
expansions: [(0, 2), (6, 1), (7, 1), (11, 1), (15, 1), (17, 1), (18, 1), (19, 1), (20, 1), (30, 2), (35, 1), (47, 1), (48, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 217.9032 - loglik: -2.1409e+02 - logprior: -3.8102e+00
Epoch 2/2
19/19 - 3s - loss: 208.4110 - loglik: -2.0718e+02 - logprior: -1.2267e+00
Fitted a model with MAP estimate = -206.8453
expansions: []
discards: [ 0 74 79]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.3619 - loglik: -2.0840e+02 - logprior: -3.9666e+00
Epoch 2/2
19/19 - 3s - loss: 207.8844 - loglik: -2.0647e+02 - logprior: -1.4123e+00
Fitted a model with MAP estimate = -206.4459
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 210.3747 - loglik: -2.0729e+02 - logprior: -3.0845e+00
Epoch 2/10
19/19 - 3s - loss: 207.3175 - loglik: -2.0638e+02 - logprior: -9.4246e-01
Epoch 3/10
19/19 - 3s - loss: 206.5706 - loglik: -2.0574e+02 - logprior: -8.2629e-01
Epoch 4/10
19/19 - 3s - loss: 205.7985 - loglik: -2.0499e+02 - logprior: -8.1082e-01
Epoch 5/10
19/19 - 3s - loss: 205.4930 - loglik: -2.0473e+02 - logprior: -7.6535e-01
Epoch 6/10
19/19 - 3s - loss: 205.1683 - loglik: -2.0442e+02 - logprior: -7.5181e-01
Epoch 7/10
19/19 - 3s - loss: 205.0299 - loglik: -2.0431e+02 - logprior: -7.2320e-01
Epoch 8/10
19/19 - 3s - loss: 205.0897 - loglik: -2.0438e+02 - logprior: -7.0693e-01
Fitted a model with MAP estimate = -204.8413
Time for alignment: 87.2687
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 276.0886 - loglik: -2.7302e+02 - logprior: -3.0717e+00
Epoch 2/10
19/19 - 2s - loss: 239.6073 - loglik: -2.3836e+02 - logprior: -1.2427e+00
Epoch 3/10
19/19 - 2s - loss: 224.0421 - loglik: -2.2271e+02 - logprior: -1.3316e+00
Epoch 4/10
19/19 - 2s - loss: 218.8865 - loglik: -2.1755e+02 - logprior: -1.3381e+00
Epoch 5/10
19/19 - 2s - loss: 217.7399 - loglik: -2.1646e+02 - logprior: -1.2831e+00
Epoch 6/10
19/19 - 2s - loss: 217.1330 - loglik: -2.1589e+02 - logprior: -1.2416e+00
Epoch 7/10
19/19 - 2s - loss: 216.9241 - loglik: -2.1572e+02 - logprior: -1.2084e+00
Epoch 8/10
19/19 - 2s - loss: 216.4428 - loglik: -2.1524e+02 - logprior: -1.1998e+00
Epoch 9/10
19/19 - 2s - loss: 216.0118 - loglik: -2.1482e+02 - logprior: -1.1891e+00
Epoch 10/10
19/19 - 2s - loss: 216.4191 - loglik: -2.1525e+02 - logprior: -1.1729e+00
Fitted a model with MAP estimate = -215.5982
expansions: [(0, 2), (6, 1), (7, 1), (11, 1), (15, 1), (17, 1), (18, 1), (19, 1), (20, 1), (29, 1), (30, 1), (34, 1), (47, 1), (48, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 218.3248 - loglik: -2.1449e+02 - logprior: -3.8309e+00
Epoch 2/2
19/19 - 3s - loss: 208.5918 - loglik: -2.0734e+02 - logprior: -1.2497e+00
Fitted a model with MAP estimate = -206.8372
expansions: []
discards: [ 0 74 78 82 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.4585 - loglik: -2.0848e+02 - logprior: -3.9737e+00
Epoch 2/2
19/19 - 3s - loss: 208.0870 - loglik: -2.0668e+02 - logprior: -1.4056e+00
Fitted a model with MAP estimate = -206.5675
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 210.4417 - loglik: -2.0737e+02 - logprior: -3.0691e+00
Epoch 2/10
19/19 - 3s - loss: 207.2715 - loglik: -2.0634e+02 - logprior: -9.3058e-01
Epoch 3/10
19/19 - 3s - loss: 206.5758 - loglik: -2.0577e+02 - logprior: -8.0941e-01
Epoch 4/10
19/19 - 3s - loss: 206.0630 - loglik: -2.0526e+02 - logprior: -8.0722e-01
Epoch 5/10
19/19 - 3s - loss: 205.4677 - loglik: -2.0472e+02 - logprior: -7.5094e-01
Epoch 6/10
19/19 - 3s - loss: 205.1754 - loglik: -2.0444e+02 - logprior: -7.3800e-01
Epoch 7/10
19/19 - 3s - loss: 205.3858 - loglik: -2.0466e+02 - logprior: -7.2350e-01
Fitted a model with MAP estimate = -204.9128
Time for alignment: 83.7699
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 275.9793 - loglik: -2.7291e+02 - logprior: -3.0721e+00
Epoch 2/10
19/19 - 2s - loss: 238.7712 - loglik: -2.3752e+02 - logprior: -1.2559e+00
Epoch 3/10
19/19 - 2s - loss: 222.5372 - loglik: -2.2119e+02 - logprior: -1.3440e+00
Epoch 4/10
19/19 - 2s - loss: 218.7938 - loglik: -2.1743e+02 - logprior: -1.3604e+00
Epoch 5/10
19/19 - 2s - loss: 217.7509 - loglik: -2.1647e+02 - logprior: -1.2769e+00
Epoch 6/10
19/19 - 2s - loss: 217.2126 - loglik: -2.1596e+02 - logprior: -1.2545e+00
Epoch 7/10
19/19 - 2s - loss: 216.6873 - loglik: -2.1545e+02 - logprior: -1.2379e+00
Epoch 8/10
19/19 - 2s - loss: 216.4331 - loglik: -2.1521e+02 - logprior: -1.2246e+00
Epoch 9/10
19/19 - 2s - loss: 216.4137 - loglik: -2.1521e+02 - logprior: -1.2046e+00
Epoch 10/10
19/19 - 2s - loss: 216.0166 - loglik: -2.1483e+02 - logprior: -1.1915e+00
Fitted a model with MAP estimate = -215.6135
expansions: [(0, 2), (6, 1), (7, 1), (11, 1), (15, 1), (17, 1), (18, 1), (19, 1), (20, 1), (29, 1), (30, 1), (34, 1), (47, 1), (48, 1), (51, 1), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 217.7791 - loglik: -2.1396e+02 - logprior: -3.8164e+00
Epoch 2/2
19/19 - 3s - loss: 208.3263 - loglik: -2.0711e+02 - logprior: -1.2155e+00
Fitted a model with MAP estimate = -206.6730
expansions: []
discards: [ 0 78 81 95]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 212.5150 - loglik: -2.0857e+02 - logprior: -3.9431e+00
Epoch 2/2
19/19 - 3s - loss: 208.0066 - loglik: -2.0660e+02 - logprior: -1.4033e+00
Fitted a model with MAP estimate = -206.5491
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 210.2583 - loglik: -2.0720e+02 - logprior: -3.0599e+00
Epoch 2/10
19/19 - 3s - loss: 207.2541 - loglik: -2.0633e+02 - logprior: -9.2264e-01
Epoch 3/10
19/19 - 3s - loss: 206.6292 - loglik: -2.0583e+02 - logprior: -8.0288e-01
Epoch 4/10
19/19 - 3s - loss: 206.0195 - loglik: -2.0525e+02 - logprior: -7.7432e-01
Epoch 5/10
19/19 - 3s - loss: 205.5308 - loglik: -2.0479e+02 - logprior: -7.4433e-01
Epoch 6/10
19/19 - 3s - loss: 205.4621 - loglik: -2.0473e+02 - logprior: -7.2924e-01
Epoch 7/10
19/19 - 3s - loss: 205.1431 - loglik: -2.0445e+02 - logprior: -6.9708e-01
Epoch 8/10
19/19 - 3s - loss: 204.5987 - loglik: -2.0391e+02 - logprior: -6.9323e-01
Epoch 9/10
19/19 - 3s - loss: 205.1459 - loglik: -2.0448e+02 - logprior: -6.6909e-01
Fitted a model with MAP estimate = -204.7296
Time for alignment: 89.5706
Computed alignments with likelihoods: ['-205.4406', '-205.0807', '-204.8413', '-204.9128', '-204.7296']
Best model has likelihood: -204.7296  (prior= -0.7223 )
time for generating output: 0.1710
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.6454252053805182
Training of 5 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c0b28aca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c0b2956d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c0b2c72e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d7cc6a1f0>
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 178.5541 - loglik: -1.7537e+02 - logprior: -3.1820e+00
Epoch 2/10
19/19 - 1s - loss: 134.8370 - loglik: -1.3351e+02 - logprior: -1.3228e+00
Epoch 3/10
19/19 - 1s - loss: 118.5486 - loglik: -1.1725e+02 - logprior: -1.3008e+00
Epoch 4/10
19/19 - 1s - loss: 116.3218 - loglik: -1.1497e+02 - logprior: -1.3540e+00
Epoch 5/10
19/19 - 1s - loss: 115.4282 - loglik: -1.1412e+02 - logprior: -1.3054e+00
Epoch 6/10
19/19 - 1s - loss: 114.9257 - loglik: -1.1365e+02 - logprior: -1.2806e+00
Epoch 7/10
19/19 - 1s - loss: 114.5391 - loglik: -1.1327e+02 - logprior: -1.2703e+00
Epoch 8/10
19/19 - 1s - loss: 113.8795 - loglik: -1.1262e+02 - logprior: -1.2566e+00
Epoch 9/10
19/19 - 1s - loss: 113.8293 - loglik: -1.1258e+02 - logprior: -1.2541e+00
Epoch 10/10
19/19 - 1s - loss: 113.5361 - loglik: -1.1229e+02 - logprior: -1.2492e+00
Fitted a model with MAP estimate = -113.3188
expansions: [(0, 2), (8, 1), (15, 1), (19, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 115.2394 - loglik: -1.1101e+02 - logprior: -4.2264e+00
Epoch 2/2
19/19 - 1s - loss: 106.2586 - loglik: -1.0502e+02 - logprior: -1.2359e+00
Fitted a model with MAP estimate = -104.8386
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 110.4665 - loglik: -1.0641e+02 - logprior: -4.0581e+00
Epoch 2/2
19/19 - 1s - loss: 106.0585 - loglik: -1.0462e+02 - logprior: -1.4350e+00
Fitted a model with MAP estimate = -105.1722
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 108.4218 - loglik: -1.0525e+02 - logprior: -3.1738e+00
Epoch 2/10
19/19 - 1s - loss: 105.5752 - loglik: -1.0425e+02 - logprior: -1.3240e+00
Epoch 3/10
19/19 - 1s - loss: 104.6639 - loglik: -1.0345e+02 - logprior: -1.2174e+00
Epoch 4/10
19/19 - 1s - loss: 103.8985 - loglik: -1.0272e+02 - logprior: -1.1778e+00
Epoch 5/10
19/19 - 1s - loss: 103.4005 - loglik: -1.0225e+02 - logprior: -1.1534e+00
Epoch 6/10
19/19 - 1s - loss: 102.7635 - loglik: -1.0162e+02 - logprior: -1.1409e+00
Epoch 7/10
19/19 - 1s - loss: 102.4858 - loglik: -1.0136e+02 - logprior: -1.1250e+00
Epoch 8/10
19/19 - 1s - loss: 102.0865 - loglik: -1.0097e+02 - logprior: -1.1191e+00
Epoch 9/10
19/19 - 1s - loss: 102.2287 - loglik: -1.0112e+02 - logprior: -1.1038e+00
Fitted a model with MAP estimate = -101.9373
Time for alignment: 47.3006
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 178.4931 - loglik: -1.7531e+02 - logprior: -3.1851e+00
Epoch 2/10
19/19 - 1s - loss: 134.2669 - loglik: -1.3293e+02 - logprior: -1.3333e+00
Epoch 3/10
19/19 - 1s - loss: 118.4529 - loglik: -1.1714e+02 - logprior: -1.3147e+00
Epoch 4/10
19/19 - 1s - loss: 116.1057 - loglik: -1.1476e+02 - logprior: -1.3480e+00
Epoch 5/10
19/19 - 1s - loss: 115.3378 - loglik: -1.1404e+02 - logprior: -1.2985e+00
Epoch 6/10
19/19 - 1s - loss: 114.5074 - loglik: -1.1324e+02 - logprior: -1.2702e+00
Epoch 7/10
19/19 - 1s - loss: 114.4950 - loglik: -1.1323e+02 - logprior: -1.2680e+00
Epoch 8/10
19/19 - 1s - loss: 114.0423 - loglik: -1.1278e+02 - logprior: -1.2597e+00
Epoch 9/10
19/19 - 1s - loss: 113.6600 - loglik: -1.1241e+02 - logprior: -1.2545e+00
Epoch 10/10
19/19 - 1s - loss: 113.2490 - loglik: -1.1200e+02 - logprior: -1.2524e+00
Fitted a model with MAP estimate = -113.1772
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 115.3104 - loglik: -1.1107e+02 - logprior: -4.2413e+00
Epoch 2/2
19/19 - 1s - loss: 106.2009 - loglik: -1.0497e+02 - logprior: -1.2303e+00
Fitted a model with MAP estimate = -104.9297
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 110.4481 - loglik: -1.0639e+02 - logprior: -4.0610e+00
Epoch 2/2
19/19 - 1s - loss: 106.1624 - loglik: -1.0472e+02 - logprior: -1.4392e+00
Fitted a model with MAP estimate = -105.1843
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 108.3656 - loglik: -1.0519e+02 - logprior: -3.1734e+00
Epoch 2/10
19/19 - 1s - loss: 105.4851 - loglik: -1.0415e+02 - logprior: -1.3317e+00
Epoch 3/10
19/19 - 1s - loss: 104.8848 - loglik: -1.0367e+02 - logprior: -1.2177e+00
Epoch 4/10
19/19 - 1s - loss: 103.8756 - loglik: -1.0270e+02 - logprior: -1.1754e+00
Epoch 5/10
19/19 - 1s - loss: 103.3656 - loglik: -1.0221e+02 - logprior: -1.1560e+00
Epoch 6/10
19/19 - 1s - loss: 102.8606 - loglik: -1.0173e+02 - logprior: -1.1320e+00
Epoch 7/10
19/19 - 1s - loss: 102.5806 - loglik: -1.0145e+02 - logprior: -1.1309e+00
Epoch 8/10
19/19 - 1s - loss: 102.0987 - loglik: -1.0099e+02 - logprior: -1.1112e+00
Epoch 9/10
19/19 - 1s - loss: 102.3046 - loglik: -1.0120e+02 - logprior: -1.1095e+00
Fitted a model with MAP estimate = -101.9902
Time for alignment: 47.2543
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 178.3307 - loglik: -1.7515e+02 - logprior: -3.1852e+00
Epoch 2/10
19/19 - 1s - loss: 134.4014 - loglik: -1.3309e+02 - logprior: -1.3150e+00
Epoch 3/10
19/19 - 1s - loss: 118.7035 - loglik: -1.1741e+02 - logprior: -1.2937e+00
Epoch 4/10
19/19 - 1s - loss: 116.2403 - loglik: -1.1489e+02 - logprior: -1.3517e+00
Epoch 5/10
19/19 - 1s - loss: 115.5821 - loglik: -1.1428e+02 - logprior: -1.3020e+00
Epoch 6/10
19/19 - 1s - loss: 114.6920 - loglik: -1.1342e+02 - logprior: -1.2744e+00
Epoch 7/10
19/19 - 1s - loss: 114.2638 - loglik: -1.1299e+02 - logprior: -1.2696e+00
Epoch 8/10
19/19 - 1s - loss: 113.7466 - loglik: -1.1249e+02 - logprior: -1.2560e+00
Epoch 9/10
19/19 - 1s - loss: 113.7101 - loglik: -1.1245e+02 - logprior: -1.2569e+00
Epoch 10/10
19/19 - 1s - loss: 113.4382 - loglik: -1.1219e+02 - logprior: -1.2510e+00
Fitted a model with MAP estimate = -113.2053
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 115.3013 - loglik: -1.1106e+02 - logprior: -4.2442e+00
Epoch 2/2
19/19 - 1s - loss: 106.1080 - loglik: -1.0488e+02 - logprior: -1.2289e+00
Fitted a model with MAP estimate = -104.9065
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 110.4928 - loglik: -1.0643e+02 - logprior: -4.0626e+00
Epoch 2/2
19/19 - 1s - loss: 106.1734 - loglik: -1.0473e+02 - logprior: -1.4424e+00
Fitted a model with MAP estimate = -105.1811
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 108.4281 - loglik: -1.0526e+02 - logprior: -3.1712e+00
Epoch 2/10
19/19 - 1s - loss: 105.4708 - loglik: -1.0414e+02 - logprior: -1.3282e+00
Epoch 3/10
19/19 - 1s - loss: 104.6142 - loglik: -1.0340e+02 - logprior: -1.2139e+00
Epoch 4/10
19/19 - 1s - loss: 104.0483 - loglik: -1.0287e+02 - logprior: -1.1751e+00
Epoch 5/10
19/19 - 1s - loss: 103.2600 - loglik: -1.0211e+02 - logprior: -1.1466e+00
Epoch 6/10
19/19 - 1s - loss: 102.7072 - loglik: -1.0156e+02 - logprior: -1.1429e+00
Epoch 7/10
19/19 - 1s - loss: 102.6723 - loglik: -1.0155e+02 - logprior: -1.1241e+00
Epoch 8/10
19/19 - 1s - loss: 102.1808 - loglik: -1.0107e+02 - logprior: -1.1140e+00
Epoch 9/10
19/19 - 1s - loss: 102.3291 - loglik: -1.0122e+02 - logprior: -1.1079e+00
Fitted a model with MAP estimate = -101.9683
Time for alignment: 46.6575
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 178.2765 - loglik: -1.7509e+02 - logprior: -3.1852e+00
Epoch 2/10
19/19 - 1s - loss: 133.8643 - loglik: -1.3257e+02 - logprior: -1.2977e+00
Epoch 3/10
19/19 - 1s - loss: 119.0475 - loglik: -1.1778e+02 - logprior: -1.2672e+00
Epoch 4/10
19/19 - 1s - loss: 116.4719 - loglik: -1.1514e+02 - logprior: -1.3293e+00
Epoch 5/10
19/19 - 1s - loss: 115.7075 - loglik: -1.1442e+02 - logprior: -1.2848e+00
Epoch 6/10
19/19 - 1s - loss: 115.2394 - loglik: -1.1398e+02 - logprior: -1.2592e+00
Epoch 7/10
19/19 - 1s - loss: 115.0299 - loglik: -1.1378e+02 - logprior: -1.2519e+00
Epoch 8/10
19/19 - 1s - loss: 114.3410 - loglik: -1.1310e+02 - logprior: -1.2414e+00
Epoch 9/10
19/19 - 1s - loss: 114.1873 - loglik: -1.1295e+02 - logprior: -1.2385e+00
Epoch 10/10
19/19 - 1s - loss: 113.9293 - loglik: -1.1269e+02 - logprior: -1.2379e+00
Fitted a model with MAP estimate = -113.7185
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (40, 3), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 115.5395 - loglik: -1.1127e+02 - logprior: -4.2655e+00
Epoch 2/2
19/19 - 1s - loss: 106.3508 - loglik: -1.0510e+02 - logprior: -1.2557e+00
Fitted a model with MAP estimate = -104.9976
expansions: []
discards: [ 0 38 54]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 110.4753 - loglik: -1.0640e+02 - logprior: -4.0767e+00
Epoch 2/2
19/19 - 1s - loss: 106.0766 - loglik: -1.0464e+02 - logprior: -1.4343e+00
Fitted a model with MAP estimate = -105.1796
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 108.3236 - loglik: -1.0524e+02 - logprior: -3.0867e+00
Epoch 2/10
19/19 - 1s - loss: 105.7942 - loglik: -1.0447e+02 - logprior: -1.3256e+00
Epoch 3/10
19/19 - 1s - loss: 104.6045 - loglik: -1.0338e+02 - logprior: -1.2281e+00
Epoch 4/10
19/19 - 1s - loss: 104.0921 - loglik: -1.0291e+02 - logprior: -1.1842e+00
Epoch 5/10
19/19 - 1s - loss: 103.5835 - loglik: -1.0243e+02 - logprior: -1.1551e+00
Epoch 6/10
19/19 - 1s - loss: 102.7151 - loglik: -1.0157e+02 - logprior: -1.1430e+00
Epoch 7/10
19/19 - 1s - loss: 102.5195 - loglik: -1.0139e+02 - logprior: -1.1287e+00
Epoch 8/10
19/19 - 1s - loss: 102.3789 - loglik: -1.0126e+02 - logprior: -1.1184e+00
Epoch 9/10
19/19 - 1s - loss: 101.7435 - loglik: -1.0064e+02 - logprior: -1.1084e+00
Epoch 10/10
19/19 - 1s - loss: 102.2750 - loglik: -1.0117e+02 - logprior: -1.1009e+00
Fitted a model with MAP estimate = -101.9048
Time for alignment: 47.9552
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 178.5939 - loglik: -1.7541e+02 - logprior: -3.1829e+00
Epoch 2/10
19/19 - 1s - loss: 135.2052 - loglik: -1.3387e+02 - logprior: -1.3402e+00
Epoch 3/10
19/19 - 1s - loss: 118.6547 - loglik: -1.1734e+02 - logprior: -1.3175e+00
Epoch 4/10
19/19 - 1s - loss: 116.2512 - loglik: -1.1489e+02 - logprior: -1.3583e+00
Epoch 5/10
19/19 - 1s - loss: 115.5325 - loglik: -1.1423e+02 - logprior: -1.2983e+00
Epoch 6/10
19/19 - 1s - loss: 114.3986 - loglik: -1.1312e+02 - logprior: -1.2773e+00
Epoch 7/10
19/19 - 1s - loss: 114.3882 - loglik: -1.1312e+02 - logprior: -1.2724e+00
Epoch 8/10
19/19 - 1s - loss: 113.8374 - loglik: -1.1258e+02 - logprior: -1.2600e+00
Epoch 9/10
19/19 - 1s - loss: 113.5376 - loglik: -1.1228e+02 - logprior: -1.2592e+00
Epoch 10/10
19/19 - 1s - loss: 113.1989 - loglik: -1.1195e+02 - logprior: -1.2505e+00
Fitted a model with MAP estimate = -113.1453
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 115.4104 - loglik: -1.1117e+02 - logprior: -4.2359e+00
Epoch 2/2
19/19 - 1s - loss: 106.2444 - loglik: -1.0501e+02 - logprior: -1.2301e+00
Fitted a model with MAP estimate = -104.9114
expansions: []
discards: [ 0 38]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 110.5023 - loglik: -1.0643e+02 - logprior: -4.0725e+00
Epoch 2/2
19/19 - 1s - loss: 106.1186 - loglik: -1.0468e+02 - logprior: -1.4351e+00
Fitted a model with MAP estimate = -105.1769
expansions: [(0, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 108.4963 - loglik: -1.0542e+02 - logprior: -3.0763e+00
Epoch 2/10
19/19 - 1s - loss: 105.7293 - loglik: -1.0441e+02 - logprior: -1.3224e+00
Epoch 3/10
19/19 - 1s - loss: 104.6083 - loglik: -1.0337e+02 - logprior: -1.2346e+00
Epoch 4/10
19/19 - 1s - loss: 104.1541 - loglik: -1.0297e+02 - logprior: -1.1819e+00
Epoch 5/10
19/19 - 1s - loss: 103.3108 - loglik: -1.0215e+02 - logprior: -1.1581e+00
Epoch 6/10
19/19 - 1s - loss: 102.5416 - loglik: -1.0140e+02 - logprior: -1.1425e+00
Epoch 7/10
19/19 - 1s - loss: 102.6020 - loglik: -1.0147e+02 - logprior: -1.1325e+00
Fitted a model with MAP estimate = -102.2736
Time for alignment: 43.6695
Computed alignments with likelihoods: ['-101.9373', '-101.9902', '-101.9683', '-101.9048', '-102.2736']
Best model has likelihood: -101.9048  (prior= -1.1073 )
time for generating output: 0.1044
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8018575851393189
Training of 5 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4aac144fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4daf105b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4da6f2c850>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4be858f310>
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 571.2649 - loglik: -5.6945e+02 - logprior: -1.8121e+00
Epoch 2/10
39/39 - 16s - loss: 490.2275 - loglik: -4.8927e+02 - logprior: -9.5339e-01
Epoch 3/10
39/39 - 16s - loss: 485.6332 - loglik: -4.8479e+02 - logprior: -8.4653e-01
Epoch 4/10
39/39 - 16s - loss: 484.2562 - loglik: -4.8342e+02 - logprior: -8.3288e-01
Epoch 5/10
39/39 - 17s - loss: 482.2311 - loglik: -4.8136e+02 - logprior: -8.7525e-01
Epoch 6/10
39/39 - 17s - loss: 481.3106 - loglik: -4.8045e+02 - logprior: -8.6434e-01
Epoch 7/10
39/39 - 16s - loss: 481.1023 - loglik: -4.8023e+02 - logprior: -8.6800e-01
Epoch 8/10
39/39 - 16s - loss: 479.8523 - loglik: -4.7898e+02 - logprior: -8.6823e-01
Epoch 9/10
39/39 - 17s - loss: 479.8557 - loglik: -4.7899e+02 - logprior: -8.7021e-01
Fitted a model with MAP estimate = -477.9129
expansions: [(20, 1), (31, 2), (60, 1), (61, 8), (79, 4), (138, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 483.2053 - loglik: -4.8038e+02 - logprior: -2.8286e+00
Epoch 2/2
39/39 - 20s - loss: 474.0728 - loglik: -4.7298e+02 - logprior: -1.0973e+00
Fitted a model with MAP estimate = -470.6111
expansions: [(0, 2), (93, 2)]
discards: [ 0 70 71 72 73 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 477.2563 - loglik: -4.7539e+02 - logprior: -1.8691e+00
Epoch 2/2
39/39 - 20s - loss: 472.1144 - loglik: -4.7130e+02 - logprior: -8.1287e-01
Fitted a model with MAP estimate = -466.9620
expansions: [(36, 1), (91, 1)]
discards: [ 0 69 70]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 469.7392 - loglik: -4.6728e+02 - logprior: -2.4593e+00
Epoch 2/10
39/39 - 19s - loss: 464.7950 - loglik: -4.6425e+02 - logprior: -5.4727e-01
Epoch 3/10
39/39 - 20s - loss: 462.9794 - loglik: -4.6251e+02 - logprior: -4.6700e-01
Epoch 4/10
39/39 - 19s - loss: 462.0710 - loglik: -4.6165e+02 - logprior: -4.2382e-01
Epoch 5/10
39/39 - 19s - loss: 461.1276 - loglik: -4.6077e+02 - logprior: -3.5277e-01
Epoch 6/10
39/39 - 19s - loss: 460.5788 - loglik: -4.6029e+02 - logprior: -2.8676e-01
Epoch 7/10
39/39 - 19s - loss: 460.5125 - loglik: -4.6030e+02 - logprior: -2.1626e-01
Epoch 8/10
39/39 - 19s - loss: 459.5540 - loglik: -4.5941e+02 - logprior: -1.4134e-01
Epoch 9/10
39/39 - 18s - loss: 459.9209 - loglik: -4.5985e+02 - logprior: -7.4646e-02
Fitted a model with MAP estimate = -459.4233
Time for alignment: 491.6601
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 573.2453 - loglik: -5.7140e+02 - logprior: -1.8492e+00
Epoch 2/10
39/39 - 17s - loss: 486.8509 - loglik: -4.8572e+02 - logprior: -1.1340e+00
Epoch 3/10
39/39 - 17s - loss: 480.8574 - loglik: -4.7980e+02 - logprior: -1.0575e+00
Epoch 4/10
39/39 - 18s - loss: 479.5793 - loglik: -4.7858e+02 - logprior: -9.9768e-01
Epoch 5/10
39/39 - 18s - loss: 478.8076 - loglik: -4.7782e+02 - logprior: -9.8932e-01
Epoch 6/10
39/39 - 19s - loss: 477.8743 - loglik: -4.7690e+02 - logprior: -9.7778e-01
Epoch 7/10
39/39 - 19s - loss: 478.1982 - loglik: -4.7722e+02 - logprior: -9.7765e-01
Fitted a model with MAP estimate = -475.9229
expansions: [(20, 1), (31, 2), (78, 3), (79, 3), (80, 4), (105, 1), (106, 1), (137, 1)]
discards: [  0 120]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 476.9551 - loglik: -4.7416e+02 - logprior: -2.7989e+00
Epoch 2/2
39/39 - 21s - loss: 468.7267 - loglik: -4.6776e+02 - logprior: -9.6282e-01
Fitted a model with MAP estimate = -465.7919
expansions: [(0, 2), (35, 1)]
discards: [ 0 31 32 81 85]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 471.2297 - loglik: -4.6941e+02 - logprior: -1.8159e+00
Epoch 2/2
39/39 - 20s - loss: 468.2712 - loglik: -4.6754e+02 - logprior: -7.2655e-01
Fitted a model with MAP estimate = -465.6798
expansions: [(32, 2)]
discards: [ 0 34]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 471.0617 - loglik: -4.6857e+02 - logprior: -2.4929e+00
Epoch 2/10
39/39 - 20s - loss: 467.2155 - loglik: -4.6660e+02 - logprior: -6.1755e-01
Epoch 3/10
39/39 - 20s - loss: 464.5488 - loglik: -4.6400e+02 - logprior: -5.4802e-01
Epoch 4/10
39/39 - 20s - loss: 465.4158 - loglik: -4.6492e+02 - logprior: -5.0061e-01
Fitted a model with MAP estimate = -463.7391
Time for alignment: 378.2883
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 572.0524 - loglik: -5.7019e+02 - logprior: -1.8660e+00
Epoch 2/10
39/39 - 17s - loss: 486.9158 - loglik: -4.8572e+02 - logprior: -1.1979e+00
Epoch 3/10
39/39 - 16s - loss: 480.6609 - loglik: -4.7959e+02 - logprior: -1.0741e+00
Epoch 4/10
39/39 - 16s - loss: 478.4644 - loglik: -4.7743e+02 - logprior: -1.0369e+00
Epoch 5/10
39/39 - 17s - loss: 477.5705 - loglik: -4.7655e+02 - logprior: -1.0226e+00
Epoch 6/10
39/39 - 17s - loss: 477.1341 - loglik: -4.7612e+02 - logprior: -1.0146e+00
Epoch 7/10
39/39 - 17s - loss: 476.6784 - loglik: -4.7566e+02 - logprior: -1.0219e+00
Epoch 8/10
39/39 - 17s - loss: 475.6476 - loglik: -4.7462e+02 - logprior: -1.0320e+00
Epoch 9/10
39/39 - 16s - loss: 476.0573 - loglik: -4.7503e+02 - logprior: -1.0251e+00
Fitted a model with MAP estimate = -473.7833
expansions: [(26, 1), (31, 2), (57, 1), (77, 3), (78, 2), (79, 1), (80, 2), (81, 1), (105, 1), (106, 1)]
discards: [  0 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 478.2916 - loglik: -4.7547e+02 - logprior: -2.8246e+00
Epoch 2/2
39/39 - 18s - loss: 470.7006 - loglik: -4.6968e+02 - logprior: -1.0168e+00
Fitted a model with MAP estimate = -467.5165
expansions: [(0, 2)]
discards: [ 0 32 84 89]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 472.6731 - loglik: -4.7084e+02 - logprior: -1.8285e+00
Epoch 2/2
39/39 - 18s - loss: 469.7250 - loglik: -4.6899e+02 - logprior: -7.3107e-01
Fitted a model with MAP estimate = -467.2893
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 181 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 472.6308 - loglik: -4.7029e+02 - logprior: -2.3442e+00
Epoch 2/10
39/39 - 18s - loss: 468.5390 - loglik: -4.6793e+02 - logprior: -6.0807e-01
Epoch 3/10
39/39 - 19s - loss: 467.9660 - loglik: -4.6740e+02 - logprior: -5.6642e-01
Epoch 4/10
39/39 - 20s - loss: 466.2943 - loglik: -4.6578e+02 - logprior: -5.1760e-01
Epoch 5/10
39/39 - 20s - loss: 465.7346 - loglik: -4.6527e+02 - logprior: -4.6814e-01
Epoch 6/10
39/39 - 20s - loss: 464.6148 - loglik: -4.6422e+02 - logprior: -3.9110e-01
Epoch 7/10
39/39 - 20s - loss: 465.3942 - loglik: -4.6505e+02 - logprior: -3.4448e-01
Fitted a model with MAP estimate = -464.1860
Time for alignment: 443.4235
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 571.8351 - loglik: -5.6996e+02 - logprior: -1.8790e+00
Epoch 2/10
39/39 - 17s - loss: 482.5109 - loglik: -4.8132e+02 - logprior: -1.1875e+00
Epoch 3/10
39/39 - 17s - loss: 477.0926 - loglik: -4.7605e+02 - logprior: -1.0431e+00
Epoch 4/10
39/39 - 18s - loss: 475.5153 - loglik: -4.7452e+02 - logprior: -9.9965e-01
Epoch 5/10
39/39 - 18s - loss: 475.1247 - loglik: -4.7414e+02 - logprior: -9.8130e-01
Epoch 6/10
39/39 - 18s - loss: 473.5833 - loglik: -4.7261e+02 - logprior: -9.7469e-01
Epoch 7/10
39/39 - 18s - loss: 474.1365 - loglik: -4.7316e+02 - logprior: -9.7454e-01
Fitted a model with MAP estimate = -471.7228
expansions: [(26, 1), (30, 1), (31, 1), (34, 1), (73, 1), (76, 2), (77, 1), (79, 1), (80, 2), (106, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 475.9669 - loglik: -4.7322e+02 - logprior: -2.7508e+00
Epoch 2/2
39/39 - 20s - loss: 468.7472 - loglik: -4.6781e+02 - logprior: -9.3845e-01
Fitted a model with MAP estimate = -465.6496
expansions: [(0, 2), (39, 3)]
discards: [ 0 80 88]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 469.9673 - loglik: -4.6819e+02 - logprior: -1.7777e+00
Epoch 2/2
39/39 - 19s - loss: 466.4466 - loglik: -4.6576e+02 - logprior: -6.8976e-01
Fitted a model with MAP estimate = -464.0132
expansions: []
discards: [ 0 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 184 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 469.6953 - loglik: -4.6742e+02 - logprior: -2.2786e+00
Epoch 2/10
39/39 - 18s - loss: 465.6398 - loglik: -4.6508e+02 - logprior: -5.5630e-01
Epoch 3/10
39/39 - 18s - loss: 464.6021 - loglik: -4.6408e+02 - logprior: -5.2338e-01
Epoch 4/10
39/39 - 18s - loss: 463.8250 - loglik: -4.6336e+02 - logprior: -4.6003e-01
Epoch 5/10
39/39 - 18s - loss: 463.1678 - loglik: -4.6277e+02 - logprior: -3.9764e-01
Epoch 6/10
39/39 - 18s - loss: 461.7391 - loglik: -4.6140e+02 - logprior: -3.3495e-01
Epoch 7/10
39/39 - 18s - loss: 461.5247 - loglik: -4.6125e+02 - logprior: -2.7232e-01
Epoch 8/10
39/39 - 18s - loss: 461.3910 - loglik: -4.6119e+02 - logprior: -1.9893e-01
Epoch 9/10
39/39 - 18s - loss: 460.8211 - loglik: -4.6068e+02 - logprior: -1.3865e-01
Epoch 10/10
39/39 - 18s - loss: 461.7910 - loglik: -4.6172e+02 - logprior: -7.1474e-02
Fitted a model with MAP estimate = -460.7246
Time for alignment: 475.2524
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 573.0964 - loglik: -5.7125e+02 - logprior: -1.8443e+00
Epoch 2/10
39/39 - 17s - loss: 491.8982 - loglik: -4.9078e+02 - logprior: -1.1136e+00
Epoch 3/10
39/39 - 17s - loss: 486.4047 - loglik: -4.8540e+02 - logprior: -1.0061e+00
Epoch 4/10
39/39 - 18s - loss: 484.0287 - loglik: -4.8309e+02 - logprior: -9.3581e-01
Epoch 5/10
39/39 - 18s - loss: 481.5577 - loglik: -4.8064e+02 - logprior: -9.2048e-01
Epoch 6/10
39/39 - 18s - loss: 479.5644 - loglik: -4.7865e+02 - logprior: -9.1004e-01
Epoch 7/10
39/39 - 18s - loss: 479.3491 - loglik: -4.7844e+02 - logprior: -9.0523e-01
Epoch 8/10
39/39 - 18s - loss: 478.2280 - loglik: -4.7732e+02 - logprior: -9.1032e-01
Epoch 9/10
39/39 - 17s - loss: 478.1788 - loglik: -4.7728e+02 - logprior: -8.9868e-01
Epoch 10/10
39/39 - 18s - loss: 478.2934 - loglik: -4.7740e+02 - logprior: -8.9212e-01
Fitted a model with MAP estimate = -475.9594
expansions: [(21, 1), (30, 1), (60, 1), (80, 6), (106, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 481.8947 - loglik: -4.7906e+02 - logprior: -2.8362e+00
Epoch 2/2
39/39 - 20s - loss: 473.7854 - loglik: -4.7271e+02 - logprior: -1.0718e+00
Fitted a model with MAP estimate = -470.4929
expansions: [(0, 2), (85, 1), (86, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 474.0891 - loglik: -4.7229e+02 - logprior: -1.8008e+00
Epoch 2/2
39/39 - 18s - loss: 470.4756 - loglik: -4.6973e+02 - logprior: -7.4130e-01
Fitted a model with MAP estimate = -468.2644
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 182 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 473.6143 - loglik: -4.7132e+02 - logprior: -2.2982e+00
Epoch 2/10
39/39 - 18s - loss: 469.7955 - loglik: -4.6916e+02 - logprior: -6.3214e-01
Epoch 3/10
39/39 - 18s - loss: 468.5390 - loglik: -4.6797e+02 - logprior: -5.7382e-01
Epoch 4/10
39/39 - 18s - loss: 467.9141 - loglik: -4.6742e+02 - logprior: -4.9513e-01
Epoch 5/10
39/39 - 18s - loss: 466.3869 - loglik: -4.6595e+02 - logprior: -4.3562e-01
Epoch 6/10
39/39 - 18s - loss: 465.3843 - loglik: -4.6501e+02 - logprior: -3.7194e-01
Epoch 7/10
39/39 - 18s - loss: 465.8293 - loglik: -4.6553e+02 - logprior: -2.9991e-01
Fitted a model with MAP estimate = -465.0967
Time for alignment: 466.9390
Computed alignments with likelihoods: ['-459.4233', '-463.7391', '-464.1860', '-460.7246', '-465.0967']
Best model has likelihood: -459.4233  (prior= -0.1103 )
time for generating output: 0.2215
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7407237874598311
Training of 5 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4a8416b430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bd5114d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c0b036c10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d8529d310>
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 144.0555 - loglik: -1.4086e+02 - logprior: -3.1987e+00
Epoch 2/10
19/19 - 1s - loss: 115.1495 - loglik: -1.1372e+02 - logprior: -1.4326e+00
Epoch 3/10
19/19 - 1s - loss: 106.5957 - loglik: -1.0504e+02 - logprior: -1.5515e+00
Epoch 4/10
19/19 - 1s - loss: 104.6948 - loglik: -1.0329e+02 - logprior: -1.4028e+00
Epoch 5/10
19/19 - 1s - loss: 103.8038 - loglik: -1.0241e+02 - logprior: -1.3944e+00
Epoch 6/10
19/19 - 1s - loss: 103.5788 - loglik: -1.0221e+02 - logprior: -1.3732e+00
Epoch 7/10
19/19 - 1s - loss: 103.3865 - loglik: -1.0203e+02 - logprior: -1.3600e+00
Epoch 8/10
19/19 - 1s - loss: 103.1685 - loglik: -1.0182e+02 - logprior: -1.3488e+00
Epoch 9/10
19/19 - 1s - loss: 103.2332 - loglik: -1.0189e+02 - logprior: -1.3437e+00
Fitted a model with MAP estimate = -103.0615
expansions: [(10, 1), (13, 2), (14, 3), (18, 2), (19, 2), (20, 2), (27, 2), (28, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.1365 - loglik: -1.0498e+02 - logprior: -4.1571e+00
Epoch 2/2
19/19 - 1s - loss: 100.6104 - loglik: -9.8520e+01 - logprior: -2.0904e+00
Fitted a model with MAP estimate = -98.6844
expansions: [(0, 1)]
discards: [ 0 13 17 24 27 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.1946 - loglik: -9.7908e+01 - logprior: -3.2864e+00
Epoch 2/2
19/19 - 1s - loss: 97.2455 - loglik: -9.5809e+01 - logprior: -1.4362e+00
Fitted a model with MAP estimate = -96.6682
expansions: []
discards: [36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.9817 - loglik: -9.6713e+01 - logprior: -3.2685e+00
Epoch 2/10
19/19 - 1s - loss: 97.2940 - loglik: -9.5896e+01 - logprior: -1.3976e+00
Epoch 3/10
19/19 - 1s - loss: 96.9027 - loglik: -9.5603e+01 - logprior: -1.2993e+00
Epoch 4/10
19/19 - 1s - loss: 96.5926 - loglik: -9.5340e+01 - logprior: -1.2524e+00
Epoch 5/10
19/19 - 1s - loss: 96.5223 - loglik: -9.5303e+01 - logprior: -1.2196e+00
Epoch 6/10
19/19 - 1s - loss: 96.3711 - loglik: -9.5165e+01 - logprior: -1.2062e+00
Epoch 7/10
19/19 - 1s - loss: 96.3523 - loglik: -9.5162e+01 - logprior: -1.1902e+00
Epoch 8/10
19/19 - 1s - loss: 96.2555 - loglik: -9.5077e+01 - logprior: -1.1784e+00
Epoch 9/10
19/19 - 1s - loss: 95.9823 - loglik: -9.4811e+01 - logprior: -1.1709e+00
Epoch 10/10
19/19 - 1s - loss: 96.1927 - loglik: -9.5029e+01 - logprior: -1.1636e+00
Fitted a model with MAP estimate = -95.9933
Time for alignment: 42.9717
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 144.0863 - loglik: -1.4089e+02 - logprior: -3.1948e+00
Epoch 2/10
19/19 - 1s - loss: 115.4211 - loglik: -1.1400e+02 - logprior: -1.4216e+00
Epoch 3/10
19/19 - 1s - loss: 107.1884 - loglik: -1.0565e+02 - logprior: -1.5417e+00
Epoch 4/10
19/19 - 1s - loss: 104.8164 - loglik: -1.0342e+02 - logprior: -1.3936e+00
Epoch 5/10
19/19 - 1s - loss: 103.9949 - loglik: -1.0261e+02 - logprior: -1.3871e+00
Epoch 6/10
19/19 - 1s - loss: 103.6528 - loglik: -1.0228e+02 - logprior: -1.3715e+00
Epoch 7/10
19/19 - 1s - loss: 103.3282 - loglik: -1.0197e+02 - logprior: -1.3580e+00
Epoch 8/10
19/19 - 1s - loss: 103.2116 - loglik: -1.0186e+02 - logprior: -1.3480e+00
Epoch 9/10
19/19 - 1s - loss: 103.1807 - loglik: -1.0184e+02 - logprior: -1.3414e+00
Epoch 10/10
19/19 - 1s - loss: 103.0154 - loglik: -1.0168e+02 - logprior: -1.3384e+00
Fitted a model with MAP estimate = -103.0024
expansions: [(10, 1), (13, 2), (14, 3), (18, 2), (19, 3), (20, 2), (27, 2), (28, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.3421 - loglik: -1.0518e+02 - logprior: -4.1585e+00
Epoch 2/2
19/19 - 1s - loss: 100.6116 - loglik: -9.8500e+01 - logprior: -2.1118e+00
Fitted a model with MAP estimate = -98.7627
expansions: [(0, 1)]
discards: [ 0 14 17 24 27 28 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.3795 - loglik: -9.8101e+01 - logprior: -3.2789e+00
Epoch 2/2
19/19 - 1s - loss: 97.2027 - loglik: -9.5771e+01 - logprior: -1.4319e+00
Fitted a model with MAP estimate = -96.6513
expansions: []
discards: [36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 100.0339 - loglik: -9.6769e+01 - logprior: -3.2653e+00
Epoch 2/10
19/19 - 1s - loss: 97.2910 - loglik: -9.5897e+01 - logprior: -1.3941e+00
Epoch 3/10
19/19 - 1s - loss: 96.8347 - loglik: -9.5534e+01 - logprior: -1.3003e+00
Epoch 4/10
19/19 - 1s - loss: 96.6413 - loglik: -9.5387e+01 - logprior: -1.2545e+00
Epoch 5/10
19/19 - 1s - loss: 96.4128 - loglik: -9.5196e+01 - logprior: -1.2168e+00
Epoch 6/10
19/19 - 1s - loss: 96.5502 - loglik: -9.5344e+01 - logprior: -1.2065e+00
Fitted a model with MAP estimate = -96.2665
Time for alignment: 39.2961
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 144.0199 - loglik: -1.4082e+02 - logprior: -3.1972e+00
Epoch 2/10
19/19 - 1s - loss: 115.3814 - loglik: -1.1395e+02 - logprior: -1.4311e+00
Epoch 3/10
19/19 - 1s - loss: 107.2011 - loglik: -1.0565e+02 - logprior: -1.5497e+00
Epoch 4/10
19/19 - 1s - loss: 104.5937 - loglik: -1.0319e+02 - logprior: -1.4029e+00
Epoch 5/10
19/19 - 1s - loss: 104.0265 - loglik: -1.0263e+02 - logprior: -1.3961e+00
Epoch 6/10
19/19 - 1s - loss: 103.4754 - loglik: -1.0210e+02 - logprior: -1.3790e+00
Epoch 7/10
19/19 - 1s - loss: 103.2660 - loglik: -1.0190e+02 - logprior: -1.3640e+00
Epoch 8/10
19/19 - 1s - loss: 103.4280 - loglik: -1.0207e+02 - logprior: -1.3531e+00
Fitted a model with MAP estimate = -103.1276
expansions: [(10, 1), (13, 2), (14, 3), (18, 2), (19, 2), (20, 2), (27, 2), (28, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.1946 - loglik: -1.0504e+02 - logprior: -4.1521e+00
Epoch 2/2
19/19 - 1s - loss: 100.5300 - loglik: -9.8451e+01 - logprior: -2.0785e+00
Fitted a model with MAP estimate = -98.6555
expansions: [(0, 1)]
discards: [ 0 13 17 24 27 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.1934 - loglik: -9.7909e+01 - logprior: -3.2840e+00
Epoch 2/2
19/19 - 1s - loss: 97.2736 - loglik: -9.5835e+01 - logprior: -1.4381e+00
Fitted a model with MAP estimate = -96.6822
expansions: []
discards: [36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 100.0567 - loglik: -9.6790e+01 - logprior: -3.2668e+00
Epoch 2/10
19/19 - 1s - loss: 97.2625 - loglik: -9.5865e+01 - logprior: -1.3971e+00
Epoch 3/10
19/19 - 1s - loss: 96.7893 - loglik: -9.5486e+01 - logprior: -1.3038e+00
Epoch 4/10
19/19 - 1s - loss: 96.6415 - loglik: -9.5386e+01 - logprior: -1.2556e+00
Epoch 5/10
19/19 - 1s - loss: 96.5887 - loglik: -9.5369e+01 - logprior: -1.2195e+00
Epoch 6/10
19/19 - 1s - loss: 96.3334 - loglik: -9.5126e+01 - logprior: -1.2070e+00
Epoch 7/10
19/19 - 1s - loss: 96.3136 - loglik: -9.5120e+01 - logprior: -1.1940e+00
Epoch 8/10
19/19 - 1s - loss: 96.2496 - loglik: -9.5072e+01 - logprior: -1.1772e+00
Epoch 9/10
19/19 - 1s - loss: 96.1935 - loglik: -9.5021e+01 - logprior: -1.1723e+00
Epoch 10/10
19/19 - 1s - loss: 96.1062 - loglik: -9.4944e+01 - logprior: -1.1618e+00
Fitted a model with MAP estimate = -96.0161
Time for alignment: 42.1558
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 143.9464 - loglik: -1.4075e+02 - logprior: -3.1973e+00
Epoch 2/10
19/19 - 1s - loss: 115.2621 - loglik: -1.1383e+02 - logprior: -1.4293e+00
Epoch 3/10
19/19 - 1s - loss: 106.8856 - loglik: -1.0534e+02 - logprior: -1.5413e+00
Epoch 4/10
19/19 - 1s - loss: 104.6854 - loglik: -1.0329e+02 - logprior: -1.3966e+00
Epoch 5/10
19/19 - 1s - loss: 103.9559 - loglik: -1.0257e+02 - logprior: -1.3894e+00
Epoch 6/10
19/19 - 1s - loss: 103.6576 - loglik: -1.0229e+02 - logprior: -1.3721e+00
Epoch 7/10
19/19 - 1s - loss: 103.2834 - loglik: -1.0193e+02 - logprior: -1.3572e+00
Epoch 8/10
19/19 - 1s - loss: 103.2316 - loglik: -1.0188e+02 - logprior: -1.3496e+00
Epoch 9/10
19/19 - 1s - loss: 103.2463 - loglik: -1.0190e+02 - logprior: -1.3421e+00
Fitted a model with MAP estimate = -103.0696
expansions: [(10, 1), (13, 2), (14, 3), (18, 2), (19, 2), (20, 2), (27, 2), (28, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.1017 - loglik: -1.0495e+02 - logprior: -4.1562e+00
Epoch 2/2
19/19 - 1s - loss: 100.5163 - loglik: -9.8428e+01 - logprior: -2.0885e+00
Fitted a model with MAP estimate = -98.6974
expansions: [(0, 1)]
discards: [ 0 14 17 24 27 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.3774 - loglik: -9.8098e+01 - logprior: -3.2798e+00
Epoch 2/2
19/19 - 1s - loss: 97.1952 - loglik: -9.5761e+01 - logprior: -1.4340e+00
Fitted a model with MAP estimate = -96.6695
expansions: []
discards: [36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.9418 - loglik: -9.6674e+01 - logprior: -3.2676e+00
Epoch 2/10
19/19 - 1s - loss: 97.3257 - loglik: -9.5929e+01 - logprior: -1.3965e+00
Epoch 3/10
19/19 - 1s - loss: 96.8334 - loglik: -9.5531e+01 - logprior: -1.3028e+00
Epoch 4/10
19/19 - 1s - loss: 96.6715 - loglik: -9.5421e+01 - logprior: -1.2501e+00
Epoch 5/10
19/19 - 1s - loss: 96.5768 - loglik: -9.5355e+01 - logprior: -1.2220e+00
Epoch 6/10
19/19 - 1s - loss: 96.3384 - loglik: -9.5135e+01 - logprior: -1.2033e+00
Epoch 7/10
19/19 - 1s - loss: 96.3441 - loglik: -9.5153e+01 - logprior: -1.1916e+00
Fitted a model with MAP estimate = -96.2007
Time for alignment: 38.4175
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 144.0001 - loglik: -1.4080e+02 - logprior: -3.1965e+00
Epoch 2/10
19/19 - 1s - loss: 115.3341 - loglik: -1.1390e+02 - logprior: -1.4309e+00
Epoch 3/10
19/19 - 1s - loss: 106.9599 - loglik: -1.0539e+02 - logprior: -1.5673e+00
Epoch 4/10
19/19 - 1s - loss: 104.6459 - loglik: -1.0322e+02 - logprior: -1.4234e+00
Epoch 5/10
19/19 - 1s - loss: 103.8900 - loglik: -1.0247e+02 - logprior: -1.4199e+00
Epoch 6/10
19/19 - 1s - loss: 103.3613 - loglik: -1.0196e+02 - logprior: -1.4024e+00
Epoch 7/10
19/19 - 1s - loss: 103.3597 - loglik: -1.0197e+02 - logprior: -1.3888e+00
Epoch 8/10
19/19 - 1s - loss: 103.0659 - loglik: -1.0168e+02 - logprior: -1.3829e+00
Epoch 9/10
19/19 - 1s - loss: 103.0976 - loglik: -1.0172e+02 - logprior: -1.3768e+00
Fitted a model with MAP estimate = -102.9522
expansions: [(10, 1), (13, 2), (14, 3), (18, 2), (19, 2), (20, 1), (26, 1), (27, 2), (28, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.2576 - loglik: -1.0511e+02 - logprior: -4.1520e+00
Epoch 2/2
19/19 - 1s - loss: 100.3421 - loglik: -9.8248e+01 - logprior: -2.0941e+00
Fitted a model with MAP estimate = -98.5977
expansions: [(0, 1)]
discards: [ 0 13 17 24 27 39]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.0548 - loglik: -9.7773e+01 - logprior: -3.2818e+00
Epoch 2/2
19/19 - 1s - loss: 97.0828 - loglik: -9.5646e+01 - logprior: -1.4370e+00
Fitted a model with MAP estimate = -96.5921
expansions: []
discards: [36]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 99.9080 - loglik: -9.6641e+01 - logprior: -3.2674e+00
Epoch 2/10
19/19 - 1s - loss: 97.2537 - loglik: -9.5856e+01 - logprior: -1.3977e+00
Epoch 3/10
19/19 - 1s - loss: 96.8097 - loglik: -9.5504e+01 - logprior: -1.3053e+00
Epoch 4/10
19/19 - 1s - loss: 96.6737 - loglik: -9.5423e+01 - logprior: -1.2508e+00
Epoch 5/10
19/19 - 1s - loss: 96.4437 - loglik: -9.5219e+01 - logprior: -1.2245e+00
Epoch 6/10
19/19 - 1s - loss: 96.3309 - loglik: -9.5121e+01 - logprior: -1.2094e+00
Epoch 7/10
19/19 - 1s - loss: 96.2236 - loglik: -9.5028e+01 - logprior: -1.1961e+00
Epoch 8/10
19/19 - 1s - loss: 96.1428 - loglik: -9.4963e+01 - logprior: -1.1795e+00
Epoch 9/10
19/19 - 1s - loss: 96.3169 - loglik: -9.5142e+01 - logprior: -1.1751e+00
Fitted a model with MAP estimate = -96.0539
Time for alignment: 41.9801
Computed alignments with likelihoods: ['-95.9933', '-96.2665', '-96.0161', '-96.2007', '-96.0539']
Best model has likelihood: -95.9933  (prior= -1.1493 )
time for generating output: 0.0959
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.7714915503306392
Training of 5 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bd5061160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d1b9a03a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d6bf06490>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4da6f0b1f0>
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 462.0250 - loglik: -4.5918e+02 - logprior: -2.8472e+00
Epoch 2/10
19/19 - 6s - loss: 386.2896 - loglik: -3.8498e+02 - logprior: -1.3060e+00
Epoch 3/10
19/19 - 6s - loss: 351.1796 - loglik: -3.4944e+02 - logprior: -1.7384e+00
Epoch 4/10
19/19 - 7s - loss: 344.6573 - loglik: -3.4281e+02 - logprior: -1.8449e+00
Epoch 5/10
19/19 - 7s - loss: 342.7556 - loglik: -3.4103e+02 - logprior: -1.7251e+00
Epoch 6/10
19/19 - 7s - loss: 341.3611 - loglik: -3.3966e+02 - logprior: -1.7032e+00
Epoch 7/10
19/19 - 7s - loss: 340.3972 - loglik: -3.3874e+02 - logprior: -1.6591e+00
Epoch 8/10
19/19 - 7s - loss: 340.3424 - loglik: -3.3870e+02 - logprior: -1.6471e+00
Epoch 9/10
19/19 - 7s - loss: 341.3951 - loglik: -3.3975e+02 - logprior: -1.6459e+00
Fitted a model with MAP estimate = -336.5489
expansions: [(0, 2), (11, 1), (16, 1), (17, 1), (18, 2), (19, 2), (21, 1), (22, 1), (36, 1), (43, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (66, 2), (67, 1), (69, 1), (70, 1), (74, 1), (80, 1), (87, 1), (89, 1), (100, 1), (101, 1), (103, 1), (105, 1), (113, 1), (114, 2), (116, 1), (118, 1), (124, 1), (134, 2), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 328.7549 - loglik: -3.2634e+02 - logprior: -2.4121e+00
Epoch 2/2
39/39 - 11s - loss: 317.6698 - loglik: -3.1664e+02 - logprior: -1.0327e+00
Fitted a model with MAP estimate = -313.7956
expansions: []
discards: [  0  25  56  86 172]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 323.0096 - loglik: -3.2016e+02 - logprior: -2.8545e+00
Epoch 2/2
39/39 - 11s - loss: 318.3314 - loglik: -3.1743e+02 - logprior: -9.0622e-01
Fitted a model with MAP estimate = -314.1657
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 316.9709 - loglik: -3.1518e+02 - logprior: -1.7952e+00
Epoch 2/10
39/39 - 10s - loss: 314.3608 - loglik: -3.1372e+02 - logprior: -6.4099e-01
Epoch 3/10
39/39 - 10s - loss: 313.7834 - loglik: -3.1321e+02 - logprior: -5.7262e-01
Epoch 4/10
39/39 - 10s - loss: 312.4493 - loglik: -3.1193e+02 - logprior: -5.1453e-01
Epoch 5/10
39/39 - 11s - loss: 311.9273 - loglik: -3.1146e+02 - logprior: -4.6689e-01
Epoch 6/10
39/39 - 11s - loss: 310.7128 - loglik: -3.1030e+02 - logprior: -4.0881e-01
Epoch 7/10
39/39 - 10s - loss: 310.6283 - loglik: -3.1027e+02 - logprior: -3.5654e-01
Epoch 8/10
39/39 - 11s - loss: 310.9917 - loglik: -3.1069e+02 - logprior: -3.0141e-01
Fitted a model with MAP estimate = -310.2512
Time for alignment: 251.4953
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 462.5994 - loglik: -4.5976e+02 - logprior: -2.8365e+00
Epoch 2/10
19/19 - 7s - loss: 388.5311 - loglik: -3.8726e+02 - logprior: -1.2666e+00
Epoch 3/10
19/19 - 7s - loss: 355.8929 - loglik: -3.5420e+02 - logprior: -1.6933e+00
Epoch 4/10
19/19 - 7s - loss: 345.2678 - loglik: -3.4342e+02 - logprior: -1.8458e+00
Epoch 5/10
19/19 - 7s - loss: 340.3770 - loglik: -3.3862e+02 - logprior: -1.7550e+00
Epoch 6/10
19/19 - 7s - loss: 339.5768 - loglik: -3.3786e+02 - logprior: -1.7187e+00
Epoch 7/10
19/19 - 7s - loss: 338.5989 - loglik: -3.3690e+02 - logprior: -1.7012e+00
Epoch 8/10
19/19 - 7s - loss: 338.1512 - loglik: -3.3646e+02 - logprior: -1.6871e+00
Epoch 9/10
19/19 - 6s - loss: 338.5492 - loglik: -3.3688e+02 - logprior: -1.6659e+00
Fitted a model with MAP estimate = -334.6428
expansions: [(0, 2), (11, 1), (16, 1), (17, 1), (18, 2), (19, 2), (21, 1), (22, 1), (30, 1), (43, 2), (44, 1), (49, 1), (50, 1), (52, 1), (53, 1), (66, 2), (67, 1), (69, 1), (70, 1), (80, 1), (85, 1), (89, 1), (90, 2), (96, 1), (99, 1), (100, 1), (105, 1), (113, 1), (114, 2), (116, 1), (118, 1), (124, 1), (134, 2), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 326.4724 - loglik: -3.2407e+02 - logprior: -2.4040e+00
Epoch 2/2
39/39 - 11s - loss: 315.2557 - loglik: -3.1424e+02 - logprior: -1.0189e+00
Fitted a model with MAP estimate = -311.1666
expansions: []
discards: [  0  26  56  86 117 173]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 320.7164 - loglik: -3.1792e+02 - logprior: -2.7984e+00
Epoch 2/2
39/39 - 10s - loss: 316.1883 - loglik: -3.1532e+02 - logprior: -8.6756e-01
Fitted a model with MAP estimate = -312.2453
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 314.8266 - loglik: -3.1306e+02 - logprior: -1.7697e+00
Epoch 2/10
39/39 - 10s - loss: 312.7354 - loglik: -3.1212e+02 - logprior: -6.1333e-01
Epoch 3/10
39/39 - 10s - loss: 311.5841 - loglik: -3.1104e+02 - logprior: -5.4713e-01
Epoch 4/10
39/39 - 10s - loss: 311.0021 - loglik: -3.1051e+02 - logprior: -4.9439e-01
Epoch 5/10
39/39 - 10s - loss: 309.5309 - loglik: -3.0909e+02 - logprior: -4.3764e-01
Epoch 6/10
39/39 - 10s - loss: 308.8548 - loglik: -3.0847e+02 - logprior: -3.8435e-01
Epoch 7/10
39/39 - 10s - loss: 309.1780 - loglik: -3.0886e+02 - logprior: -3.2092e-01
Fitted a model with MAP estimate = -308.4251
Time for alignment: 235.5294
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 462.0968 - loglik: -4.5925e+02 - logprior: -2.8451e+00
Epoch 2/10
19/19 - 7s - loss: 386.3006 - loglik: -3.8499e+02 - logprior: -1.3075e+00
Epoch 3/10
19/19 - 7s - loss: 350.1556 - loglik: -3.4839e+02 - logprior: -1.7633e+00
Epoch 4/10
19/19 - 7s - loss: 342.9719 - loglik: -3.4116e+02 - logprior: -1.8088e+00
Epoch 5/10
19/19 - 8s - loss: 339.4846 - loglik: -3.3774e+02 - logprior: -1.7425e+00
Epoch 6/10
19/19 - 8s - loss: 338.7782 - loglik: -3.3705e+02 - logprior: -1.7253e+00
Epoch 7/10
19/19 - 8s - loss: 337.5837 - loglik: -3.3589e+02 - logprior: -1.6968e+00
Epoch 8/10
19/19 - 8s - loss: 337.9647 - loglik: -3.3628e+02 - logprior: -1.6810e+00
Fitted a model with MAP estimate = -333.7904
expansions: [(0, 2), (11, 1), (16, 1), (17, 1), (18, 2), (19, 2), (21, 1), (22, 1), (23, 1), (38, 1), (44, 1), (50, 1), (51, 1), (52, 1), (53, 1), (58, 1), (66, 1), (69, 1), (70, 1), (71, 1), (80, 2), (87, 1), (90, 2), (100, 1), (101, 1), (103, 1), (105, 1), (113, 1), (114, 2), (116, 1), (118, 1), (124, 1), (134, 2), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 325.8254 - loglik: -3.2342e+02 - logprior: -2.4013e+00
Epoch 2/2
39/39 - 12s - loss: 315.0000 - loglik: -3.1401e+02 - logprior: -9.8735e-01
Fitted a model with MAP estimate = -310.7163
expansions: []
discards: [  0  25 103 116 172]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 319.8853 - loglik: -3.1710e+02 - logprior: -2.7901e+00
Epoch 2/2
39/39 - 10s - loss: 315.3435 - loglik: -3.1448e+02 - logprior: -8.6242e-01
Fitted a model with MAP estimate = -311.4220
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 314.1398 - loglik: -3.1238e+02 - logprior: -1.7640e+00
Epoch 2/10
39/39 - 11s - loss: 311.7292 - loglik: -3.1110e+02 - logprior: -6.2482e-01
Epoch 3/10
39/39 - 11s - loss: 311.2037 - loglik: -3.1066e+02 - logprior: -5.4750e-01
Epoch 4/10
39/39 - 10s - loss: 309.6788 - loglik: -3.0917e+02 - logprior: -5.0759e-01
Epoch 5/10
39/39 - 12s - loss: 308.8221 - loglik: -3.0837e+02 - logprior: -4.5648e-01
Epoch 6/10
39/39 - 11s - loss: 308.1394 - loglik: -3.0774e+02 - logprior: -3.9862e-01
Epoch 7/10
39/39 - 11s - loss: 308.3569 - loglik: -3.0800e+02 - logprior: -3.5583e-01
Fitted a model with MAP estimate = -307.7231
Time for alignment: 245.9532
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 462.5079 - loglik: -4.5966e+02 - logprior: -2.8452e+00
Epoch 2/10
19/19 - 7s - loss: 385.5261 - loglik: -3.8422e+02 - logprior: -1.3040e+00
Epoch 3/10
19/19 - 7s - loss: 350.0273 - loglik: -3.4831e+02 - logprior: -1.7194e+00
Epoch 4/10
19/19 - 7s - loss: 342.8700 - loglik: -3.4105e+02 - logprior: -1.8156e+00
Epoch 5/10
19/19 - 8s - loss: 340.5075 - loglik: -3.3879e+02 - logprior: -1.7127e+00
Epoch 6/10
19/19 - 7s - loss: 339.5150 - loglik: -3.3784e+02 - logprior: -1.6799e+00
Epoch 7/10
19/19 - 8s - loss: 338.7272 - loglik: -3.3706e+02 - logprior: -1.6696e+00
Epoch 8/10
19/19 - 8s - loss: 338.2686 - loglik: -3.3661e+02 - logprior: -1.6571e+00
Epoch 9/10
19/19 - 7s - loss: 338.3788 - loglik: -3.3673e+02 - logprior: -1.6531e+00
Fitted a model with MAP estimate = -334.7287
expansions: [(0, 2), (11, 1), (16, 1), (17, 1), (18, 2), (19, 2), (21, 1), (22, 1), (36, 1), (43, 2), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (66, 2), (67, 1), (69, 1), (70, 1), (80, 1), (88, 1), (89, 1), (90, 2), (100, 1), (101, 1), (103, 1), (106, 1), (113, 1), (114, 2), (116, 1), (118, 1), (124, 1), (134, 2), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 326.4616 - loglik: -3.2407e+02 - logprior: -2.3880e+00
Epoch 2/2
39/39 - 11s - loss: 315.3621 - loglik: -3.1435e+02 - logprior: -1.0076e+00
Fitted a model with MAP estimate = -311.1444
expansions: []
discards: [  0  26  55  86 117 173]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 320.6372 - loglik: -3.1787e+02 - logprior: -2.7681e+00
Epoch 2/2
39/39 - 10s - loss: 316.6660 - loglik: -3.1579e+02 - logprior: -8.7110e-01
Fitted a model with MAP estimate = -312.4692
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 315.3176 - loglik: -3.1357e+02 - logprior: -1.7483e+00
Epoch 2/10
39/39 - 10s - loss: 312.9826 - loglik: -3.1237e+02 - logprior: -6.1415e-01
Epoch 3/10
39/39 - 10s - loss: 311.8770 - loglik: -3.1134e+02 - logprior: -5.3817e-01
Epoch 4/10
39/39 - 10s - loss: 310.5989 - loglik: -3.1012e+02 - logprior: -4.8186e-01
Epoch 5/10
39/39 - 10s - loss: 309.1477 - loglik: -3.0872e+02 - logprior: -4.2813e-01
Epoch 6/10
39/39 - 10s - loss: 309.9102 - loglik: -3.0953e+02 - logprior: -3.8073e-01
Fitted a model with MAP estimate = -308.8882
Time for alignment: 231.2084
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 461.9433 - loglik: -4.5910e+02 - logprior: -2.8394e+00
Epoch 2/10
19/19 - 7s - loss: 386.5362 - loglik: -3.8523e+02 - logprior: -1.3084e+00
Epoch 3/10
19/19 - 6s - loss: 350.5392 - loglik: -3.4873e+02 - logprior: -1.8073e+00
Epoch 4/10
19/19 - 7s - loss: 341.2589 - loglik: -3.3930e+02 - logprior: -1.9572e+00
Epoch 5/10
19/19 - 7s - loss: 339.2898 - loglik: -3.3742e+02 - logprior: -1.8675e+00
Epoch 6/10
19/19 - 7s - loss: 338.3836 - loglik: -3.3656e+02 - logprior: -1.8255e+00
Epoch 7/10
19/19 - 7s - loss: 337.6128 - loglik: -3.3581e+02 - logprior: -1.7988e+00
Epoch 8/10
19/19 - 7s - loss: 337.2998 - loglik: -3.3552e+02 - logprior: -1.7828e+00
Epoch 9/10
19/19 - 7s - loss: 336.5034 - loglik: -3.3473e+02 - logprior: -1.7771e+00
Epoch 10/10
19/19 - 7s - loss: 337.2244 - loglik: -3.3545e+02 - logprior: -1.7721e+00
Fitted a model with MAP estimate = -333.4158
expansions: [(0, 2), (11, 1), (16, 1), (17, 1), (18, 2), (19, 2), (21, 1), (22, 2), (38, 1), (44, 1), (49, 1), (50, 1), (52, 1), (53, 1), (58, 1), (66, 1), (69, 1), (70, 1), (71, 1), (80, 2), (87, 1), (89, 1), (100, 1), (101, 1), (103, 1), (106, 1), (113, 1), (114, 2), (116, 1), (118, 1), (124, 1), (134, 2), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 326.3990 - loglik: -3.2387e+02 - logprior: -2.5259e+00
Epoch 2/2
39/39 - 10s - loss: 315.3527 - loglik: -3.1427e+02 - logprior: -1.0861e+00
Fitted a model with MAP estimate = -311.0085
expansions: []
discards: [  0  26 103 171]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 320.2989 - loglik: -3.1732e+02 - logprior: -2.9766e+00
Epoch 2/2
39/39 - 10s - loss: 316.0714 - loglik: -3.1507e+02 - logprior: -1.0017e+00
Fitted a model with MAP estimate = -311.9688
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 315.0019 - loglik: -3.1310e+02 - logprior: -1.8993e+00
Epoch 2/10
39/39 - 10s - loss: 312.7558 - loglik: -3.1200e+02 - logprior: -7.5289e-01
Epoch 3/10
39/39 - 10s - loss: 311.3189 - loglik: -3.1064e+02 - logprior: -6.7569e-01
Epoch 4/10
39/39 - 11s - loss: 310.6105 - loglik: -3.1000e+02 - logprior: -6.0918e-01
Epoch 5/10
39/39 - 11s - loss: 309.5993 - loglik: -3.0902e+02 - logprior: -5.8398e-01
Epoch 6/10
39/39 - 11s - loss: 309.1655 - loglik: -3.0866e+02 - logprior: -5.0685e-01
Epoch 7/10
39/39 - 11s - loss: 308.9179 - loglik: -3.0844e+02 - logprior: -4.7415e-01
Epoch 8/10
39/39 - 11s - loss: 308.2003 - loglik: -3.0780e+02 - logprior: -3.9876e-01
Epoch 9/10
39/39 - 12s - loss: 308.8635 - loglik: -3.0850e+02 - logprior: -3.6288e-01
Fitted a model with MAP estimate = -308.0371
Time for alignment: 267.0407
Computed alignments with likelihoods: ['-310.2512', '-308.4251', '-307.7231', '-308.8882', '-308.0371']
Best model has likelihood: -307.7231  (prior= -0.2670 )
time for generating output: 0.3972
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.19666854163033412
Training of 5 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d1b7fac40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4daf73f7c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bcc180f70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d8539dd30>
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 923.6782 - loglik: -9.2232e+02 - logprior: -1.3601e+00
Epoch 2/10
39/39 - 55s - loss: 734.0738 - loglik: -7.3230e+02 - logprior: -1.7721e+00
Epoch 3/10
39/39 - 46s - loss: 717.4551 - loglik: -7.1576e+02 - logprior: -1.6914e+00
Epoch 4/10
39/39 - 43s - loss: 713.4371 - loglik: -7.1185e+02 - logprior: -1.5889e+00
Epoch 5/10
39/39 - 41s - loss: 712.2762 - loglik: -7.1070e+02 - logprior: -1.5727e+00
Epoch 6/10
39/39 - 40s - loss: 710.4001 - loglik: -7.0883e+02 - logprior: -1.5657e+00
Epoch 7/10
39/39 - 40s - loss: 709.6856 - loglik: -7.0805e+02 - logprior: -1.6361e+00
Epoch 8/10
39/39 - 40s - loss: 709.2437 - loglik: -7.0763e+02 - logprior: -1.6128e+00
Epoch 9/10
39/39 - 40s - loss: 707.4006 - loglik: -7.0580e+02 - logprior: -1.6046e+00
Epoch 10/10
39/39 - 40s - loss: 708.5037 - loglik: -7.0689e+02 - logprior: -1.6126e+00
Fitted a model with MAP estimate = -706.7039
expansions: [(0, 23), (16, 1), (20, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (80, 1), (81, 1), (82, 3), (83, 1), (84, 1), (90, 1), (114, 1), (115, 1), (120, 1), (122, 1), (123, 2), (126, 2), (128, 1), (145, 4), (146, 2), (148, 1), (150, 1), (158, 1), (160, 2), (161, 2), (162, 1), (166, 1), (186, 1), (187, 1), (188, 2), (189, 1), (190, 1), (195, 1), (205, 1), (209, 1), (210, 1), (211, 1), (214, 1), (219, 1), (227, 3), (231, 3), (257, 1), (258, 2), (259, 6), (260, 2), (280, 1), (281, 1), (282, 1), (283, 1), (284, 1), (286, 1), (287, 4), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 407 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 692.2773 - loglik: -6.8968e+02 - logprior: -2.5941e+00
Epoch 2/2
39/39 - 68s - loss: 666.7625 - loglik: -6.6531e+02 - logprior: -1.4552e+00
Fitted a model with MAP estimate = -662.0583
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19 166 171 194 195 220 253 303 345 386 387]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 669.7680 - loglik: -6.6812e+02 - logprior: -1.6510e+00
Epoch 2/2
39/39 - 61s - loss: 664.1947 - loglik: -6.6413e+02 - logprior: -6.4041e-02
Fitted a model with MAP estimate = -661.2924
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 89s - loss: 666.2803 - loglik: -6.6505e+02 - logprior: -1.2329e+00
Epoch 2/10
39/39 - 94s - loss: 662.1942 - loglik: -6.6215e+02 - logprior: -3.9499e-02
Epoch 3/10
39/39 - 85s - loss: 660.8825 - loglik: -6.6099e+02 - logprior: 0.1092
Epoch 4/10
39/39 - 90s - loss: 658.0035 - loglik: -6.5821e+02 - logprior: 0.2031
Epoch 5/10
39/39 - 78s - loss: 656.0599 - loglik: -6.5637e+02 - logprior: 0.3093
Epoch 6/10
39/39 - 86s - loss: 653.9703 - loglik: -6.5412e+02 - logprior: 0.1461
Epoch 7/10
39/39 - 80s - loss: 653.6895 - loglik: -6.5398e+02 - logprior: 0.2888
Epoch 8/10
39/39 - 87s - loss: 652.6457 - loglik: -6.5305e+02 - logprior: 0.4085
Epoch 9/10
39/39 - 97s - loss: 652.0908 - loglik: -6.5268e+02 - logprior: 0.5872
Epoch 10/10
39/39 - 94s - loss: 652.1685 - loglik: -6.5293e+02 - logprior: 0.7587
Fitted a model with MAP estimate = -651.2632
Time for alignment: 1907.0628
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 922.8132 - loglik: -9.2145e+02 - logprior: -1.3665e+00
Epoch 2/10
39/39 - 65s - loss: 735.5161 - loglik: -7.3389e+02 - logprior: -1.6286e+00
Epoch 3/10
39/39 - 59s - loss: 719.6877 - loglik: -7.1813e+02 - logprior: -1.5606e+00
Epoch 4/10
39/39 - 60s - loss: 716.7275 - loglik: -7.1529e+02 - logprior: -1.4392e+00
Epoch 5/10
39/39 - 62s - loss: 714.8153 - loglik: -7.1333e+02 - logprior: -1.4880e+00
Epoch 6/10
39/39 - 53s - loss: 713.3602 - loglik: -7.1185e+02 - logprior: -1.5146e+00
Epoch 7/10
39/39 - 57s - loss: 712.2983 - loglik: -7.1081e+02 - logprior: -1.4918e+00
Epoch 8/10
39/39 - 61s - loss: 711.7666 - loglik: -7.1027e+02 - logprior: -1.4965e+00
Epoch 9/10
39/39 - 52s - loss: 710.2819 - loglik: -7.0873e+02 - logprior: -1.5511e+00
Epoch 10/10
39/39 - 56s - loss: 709.8036 - loglik: -7.0817e+02 - logprior: -1.6335e+00
Fitted a model with MAP estimate = -709.0603
expansions: [(0, 20), (16, 1), (45, 1), (61, 1), (62, 2), (63, 1), (65, 1), (66, 1), (74, 1), (82, 1), (83, 3), (84, 1), (85, 1), (96, 1), (115, 2), (120, 2), (123, 2), (126, 2), (142, 1), (145, 4), (146, 2), (149, 1), (155, 1), (158, 1), (160, 2), (161, 2), (162, 1), (166, 1), (182, 2), (186, 1), (188, 2), (189, 1), (190, 1), (195, 1), (204, 1), (209, 1), (210, 3), (213, 1), (227, 3), (231, 1), (232, 1), (233, 1), (257, 1), (258, 1), (259, 1), (260, 2), (261, 5), (277, 1), (278, 1), (279, 1), (281, 1), (283, 1), (284, 1), (286, 1), (287, 4), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 405 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 117s - loss: 690.1685 - loglik: -6.8759e+02 - logprior: -2.5791e+00
Epoch 2/2
39/39 - 105s - loss: 664.9224 - loglik: -6.6359e+02 - logprior: -1.3335e+00
Fitted a model with MAP estimate = -661.1340
expansions: [(153, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  86
 163 168 191 192 217 242 251 302 343 384 385]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 91s - loss: 668.5886 - loglik: -6.6706e+02 - logprior: -1.5278e+00
Epoch 2/2
39/39 - 80s - loss: 663.3052 - loglik: -6.6333e+02 - logprior: 0.0206
Fitted a model with MAP estimate = -660.8389
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 86s - loss: 666.1230 - loglik: -6.6502e+02 - logprior: -1.1070e+00
Epoch 2/10
39/39 - 96s - loss: 662.2212 - loglik: -6.6238e+02 - logprior: 0.1564
Epoch 3/10
39/39 - 100s - loss: 659.1631 - loglik: -6.5927e+02 - logprior: 0.1019
Epoch 4/10
39/39 - 97s - loss: 657.0197 - loglik: -6.5714e+02 - logprior: 0.1154
Epoch 5/10
39/39 - 94s - loss: 655.2459 - loglik: -6.5539e+02 - logprior: 0.1427
Epoch 6/10
39/39 - 95s - loss: 653.2531 - loglik: -6.5373e+02 - logprior: 0.4735
Epoch 7/10
39/39 - 97s - loss: 652.1710 - loglik: -6.5276e+02 - logprior: 0.5929
Epoch 8/10
39/39 - 87s - loss: 651.7393 - loglik: -6.5246e+02 - logprior: 0.7191
Epoch 9/10
39/39 - 90s - loss: 651.8395 - loglik: -6.5267e+02 - logprior: 0.8281
Fitted a model with MAP estimate = -651.0853
Time for alignment: 2277.0910
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 922.3347 - loglik: -9.2098e+02 - logprior: -1.3551e+00
Epoch 2/10
39/39 - 61s - loss: 731.6780 - loglik: -7.2990e+02 - logprior: -1.7749e+00
Epoch 3/10
39/39 - 56s - loss: 716.4510 - loglik: -7.1469e+02 - logprior: -1.7636e+00
Epoch 4/10
39/39 - 53s - loss: 712.6732 - loglik: -7.1101e+02 - logprior: -1.6656e+00
Epoch 5/10
39/39 - 61s - loss: 710.9998 - loglik: -7.0934e+02 - logprior: -1.6613e+00
Epoch 6/10
39/39 - 66s - loss: 709.6992 - loglik: -7.0802e+02 - logprior: -1.6762e+00
Epoch 7/10
39/39 - 66s - loss: 708.7255 - loglik: -7.0702e+02 - logprior: -1.7037e+00
Epoch 8/10
39/39 - 63s - loss: 707.5576 - loglik: -7.0582e+02 - logprior: -1.7402e+00
Epoch 9/10
39/39 - 65s - loss: 707.3939 - loglik: -7.0563e+02 - logprior: -1.7640e+00
Epoch 10/10
39/39 - 66s - loss: 706.6566 - loglik: -7.0487e+02 - logprior: -1.7884e+00
Fitted a model with MAP estimate = -705.8467
expansions: [(0, 24), (12, 1), (35, 1), (57, 1), (62, 2), (67, 1), (73, 1), (81, 1), (82, 1), (83, 3), (84, 1), (85, 1), (91, 1), (101, 1), (114, 1), (115, 1), (120, 1), (122, 1), (123, 2), (126, 2), (128, 1), (145, 4), (146, 2), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 1), (187, 2), (188, 2), (189, 1), (190, 1), (194, 1), (197, 1), (206, 1), (209, 1), (210, 1), (211, 1), (218, 1), (219, 1), (227, 2), (231, 1), (232, 1), (233, 1), (257, 1), (258, 2), (259, 6), (260, 2), (280, 1), (281, 1), (282, 1), (283, 1), (284, 1), (286, 1), (287, 4), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 406 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 101s - loss: 690.2510 - loglik: -6.8762e+02 - logprior: -2.6315e+00
Epoch 2/2
39/39 - 91s - loss: 664.9155 - loglik: -6.6353e+02 - logprior: -1.3846e+00
Fitted a model with MAP estimate = -660.8433
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20 167 172 195 196 252 344 385 386]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 95s - loss: 668.4868 - loglik: -6.6686e+02 - logprior: -1.6274e+00
Epoch 2/2
39/39 - 88s - loss: 663.4761 - loglik: -6.6330e+02 - logprior: -1.7955e-01
Fitted a model with MAP estimate = -660.9195
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 74s - loss: 665.4216 - loglik: -6.6407e+02 - logprior: -1.3540e+00
Epoch 2/10
39/39 - 73s - loss: 661.9963 - loglik: -6.6188e+02 - logprior: -1.1342e-01
Epoch 3/10
39/39 - 72s - loss: 660.3325 - loglik: -6.6037e+02 - logprior: 0.0388
Epoch 4/10
39/39 - 70s - loss: 656.1605 - loglik: -6.5629e+02 - logprior: 0.1275
Epoch 5/10
39/39 - 71s - loss: 654.9050 - loglik: -6.5516e+02 - logprior: 0.2514
Epoch 6/10
39/39 - 72s - loss: 653.7883 - loglik: -6.5414e+02 - logprior: 0.3562
Epoch 7/10
39/39 - 70s - loss: 652.8490 - loglik: -6.5316e+02 - logprior: 0.3146
Epoch 8/10
39/39 - 74s - loss: 653.1210 - loglik: -6.5352e+02 - logprior: 0.3976
Fitted a model with MAP estimate = -651.7664
Time for alignment: 1957.2319
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 923.4639 - loglik: -9.2211e+02 - logprior: -1.3543e+00
Epoch 2/10
39/39 - 48s - loss: 732.8023 - loglik: -7.3108e+02 - logprior: -1.7200e+00
Epoch 3/10
39/39 - 47s - loss: 716.5431 - loglik: -7.1482e+02 - logprior: -1.7245e+00
Epoch 4/10
39/39 - 48s - loss: 712.5772 - loglik: -7.1090e+02 - logprior: -1.6763e+00
Epoch 5/10
39/39 - 50s - loss: 710.5693 - loglik: -7.0891e+02 - logprior: -1.6634e+00
Epoch 6/10
39/39 - 49s - loss: 710.0952 - loglik: -7.0842e+02 - logprior: -1.6738e+00
Epoch 7/10
39/39 - 48s - loss: 708.5500 - loglik: -7.0685e+02 - logprior: -1.6966e+00
Epoch 8/10
39/39 - 47s - loss: 707.6266 - loglik: -7.0592e+02 - logprior: -1.7082e+00
Epoch 9/10
39/39 - 44s - loss: 707.1198 - loglik: -7.0541e+02 - logprior: -1.7063e+00
Epoch 10/10
39/39 - 40s - loss: 706.3158 - loglik: -7.0460e+02 - logprior: -1.7125e+00
Fitted a model with MAP estimate = -705.4361
expansions: [(0, 23), (16, 1), (35, 1), (61, 1), (62, 3), (66, 1), (72, 1), (80, 1), (81, 1), (82, 3), (83, 1), (84, 1), (101, 1), (114, 1), (115, 1), (120, 1), (122, 1), (123, 2), (126, 2), (128, 1), (145, 4), (146, 2), (148, 1), (155, 1), (158, 1), (160, 2), (161, 2), (162, 1), (181, 1), (182, 1), (186, 1), (188, 2), (189, 1), (194, 1), (197, 1), (205, 1), (209, 1), (210, 1), (211, 1), (218, 1), (219, 1), (227, 2), (231, 1), (232, 1), (233, 1), (257, 1), (258, 2), (259, 6), (260, 2), (280, 2), (281, 1), (282, 1), (283, 1), (284, 1), (286, 1), (287, 5)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 406 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 693.1907 - loglik: -6.9074e+02 - logprior: -2.4538e+00
Epoch 2/2
39/39 - 70s - loss: 665.6591 - loglik: -6.6434e+02 - logprior: -1.3145e+00
Fitted a model with MAP estimate = -661.4759
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19 166 171 194 195 220 253 344 373 386]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 668.9904 - loglik: -6.6739e+02 - logprior: -1.6053e+00
Epoch 2/2
39/39 - 64s - loss: 664.4106 - loglik: -6.6434e+02 - logprior: -7.3189e-02
Fitted a model with MAP estimate = -661.8079
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 666.6597 - loglik: -6.6553e+02 - logprior: -1.1312e+00
Epoch 2/10
39/39 - 67s - loss: 663.3196 - loglik: -6.6349e+02 - logprior: 0.1674
Epoch 3/10
39/39 - 66s - loss: 660.5659 - loglik: -6.6087e+02 - logprior: 0.3050
Epoch 4/10
39/39 - 70s - loss: 658.3896 - loglik: -6.5870e+02 - logprior: 0.3072
Epoch 5/10
39/39 - 72s - loss: 656.0118 - loglik: -6.5647e+02 - logprior: 0.4535
Epoch 6/10
39/39 - 77s - loss: 654.5742 - loglik: -6.5523e+02 - logprior: 0.6558
Epoch 7/10
39/39 - 75s - loss: 653.6093 - loglik: -6.5439e+02 - logprior: 0.7812
Epoch 8/10
39/39 - 76s - loss: 653.5428 - loglik: -6.5445e+02 - logprior: 0.9050
Epoch 9/10
39/39 - 76s - loss: 653.0813 - loglik: -6.5410e+02 - logprior: 1.0221
Epoch 10/10
39/39 - 81s - loss: 651.8338 - loglik: -6.5299e+02 - logprior: 1.1541
Fitted a model with MAP estimate = -652.0217
Time for alignment: 1776.1961
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 921.6987 - loglik: -9.2035e+02 - logprior: -1.3447e+00
Epoch 2/10
39/39 - 54s - loss: 731.5525 - loglik: -7.2981e+02 - logprior: -1.7423e+00
Epoch 3/10
39/39 - 53s - loss: 716.2653 - loglik: -7.1459e+02 - logprior: -1.6777e+00
Epoch 4/10
39/39 - 53s - loss: 713.7650 - loglik: -7.1219e+02 - logprior: -1.5713e+00
Epoch 5/10
39/39 - 51s - loss: 711.1465 - loglik: -7.0959e+02 - logprior: -1.5557e+00
Epoch 6/10
39/39 - 44s - loss: 710.0305 - loglik: -7.0848e+02 - logprior: -1.5520e+00
Epoch 7/10
39/39 - 41s - loss: 709.1449 - loglik: -7.0751e+02 - logprior: -1.6376e+00
Epoch 8/10
39/39 - 39s - loss: 708.2355 - loglik: -7.0661e+02 - logprior: -1.6233e+00
Epoch 9/10
39/39 - 39s - loss: 708.2328 - loglik: -7.0651e+02 - logprior: -1.7178e+00
Epoch 10/10
39/39 - 39s - loss: 706.8160 - loglik: -7.0520e+02 - logprior: -1.6130e+00
Fitted a model with MAP estimate = -705.9947
expansions: [(0, 22), (12, 1), (20, 1), (44, 1), (60, 1), (61, 2), (66, 1), (72, 1), (73, 1), (81, 1), (82, 3), (83, 1), (84, 1), (92, 1), (114, 2), (119, 1), (121, 1), (122, 2), (125, 2), (145, 3), (146, 2), (148, 1), (155, 1), (158, 1), (160, 2), (161, 2), (162, 1), (166, 1), (182, 2), (186, 1), (188, 1), (189, 1), (194, 1), (195, 1), (206, 1), (209, 1), (210, 1), (211, 1), (220, 1), (228, 4), (231, 1), (232, 1), (233, 1), (257, 1), (258, 2), (259, 5), (260, 1), (261, 1), (277, 1), (278, 1), (279, 1), (281, 1), (283, 1), (284, 1), (286, 1), (287, 4), (296, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 404 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 691.7744 - loglik: -6.8919e+02 - logprior: -2.5839e+00
Epoch 2/2
39/39 - 72s - loss: 666.6595 - loglik: -6.6519e+02 - logprior: -1.4680e+00
Fitted a model with MAP estimate = -662.3165
expansions: [(154, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18 164 169 192 217 242 301 342 383 384]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 669.0024 - loglik: -6.6737e+02 - logprior: -1.6347e+00
Epoch 2/2
39/39 - 65s - loss: 664.1348 - loglik: -6.6412e+02 - logprior: -1.2673e-02
Fitted a model with MAP estimate = -661.2743
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 377 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 79s - loss: 666.3740 - loglik: -6.6528e+02 - logprior: -1.0910e+00
Epoch 2/10
39/39 - 80s - loss: 662.5557 - loglik: -6.6262e+02 - logprior: 0.0688
Epoch 3/10
39/39 - 79s - loss: 659.8844 - loglik: -6.5997e+02 - logprior: 0.0855
Epoch 4/10
39/39 - 80s - loss: 657.5107 - loglik: -6.5760e+02 - logprior: 0.0884
Epoch 5/10
39/39 - 84s - loss: 655.3383 - loglik: -6.5566e+02 - logprior: 0.3253
Epoch 6/10
39/39 - 92s - loss: 653.2515 - loglik: -6.5367e+02 - logprior: 0.4176
Epoch 7/10
39/39 - 84s - loss: 652.5854 - loglik: -6.5313e+02 - logprior: 0.5480
Epoch 8/10
39/39 - 81s - loss: 652.7549 - loglik: -6.5320e+02 - logprior: 0.4421
Fitted a model with MAP estimate = -651.5928
Time for alignment: 1712.5023
Computed alignments with likelihoods: ['-651.2632', '-651.0853', '-651.7664', '-652.0217', '-651.5928']
Best model has likelihood: -651.0853  (prior= 0.9162 )
time for generating output: 0.3581
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.9432465172622653
Training of 5 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d963180d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4be8b781c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bcc08d9a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d1bd1ba60>
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 658.3257 - loglik: -6.5651e+02 - logprior: -1.8205e+00
Epoch 2/10
39/39 - 21s - loss: 560.7415 - loglik: -5.5925e+02 - logprior: -1.4870e+00
Epoch 3/10
39/39 - 22s - loss: 550.3083 - loglik: -5.4880e+02 - logprior: -1.5053e+00
Epoch 4/10
39/39 - 24s - loss: 548.6185 - loglik: -5.4715e+02 - logprior: -1.4716e+00
Epoch 5/10
39/39 - 24s - loss: 547.4981 - loglik: -5.4603e+02 - logprior: -1.4669e+00
Epoch 6/10
39/39 - 23s - loss: 547.4210 - loglik: -5.4595e+02 - logprior: -1.4663e+00
Epoch 7/10
39/39 - 23s - loss: 546.9827 - loglik: -5.4552e+02 - logprior: -1.4590e+00
Epoch 8/10
39/39 - 23s - loss: 546.8199 - loglik: -5.4536e+02 - logprior: -1.4624e+00
Epoch 9/10
39/39 - 23s - loss: 546.5651 - loglik: -5.4511e+02 - logprior: -1.4578e+00
Epoch 10/10
39/39 - 23s - loss: 546.4622 - loglik: -5.4500e+02 - logprior: -1.4599e+00
Fitted a model with MAP estimate = -545.6956
expansions: [(4, 1), (6, 1), (31, 1), (40, 1), (49, 1), (86, 1), (90, 1), (115, 1), (116, 2), (117, 8), (118, 2), (119, 1), (120, 1), (126, 1), (127, 1), (130, 3), (131, 3), (141, 1), (142, 8), (143, 2), (144, 1), (157, 1), (159, 1), (160, 1), (162, 3), (163, 1), (174, 6), (187, 1), (212, 4)]
discards: [  0 164 165 166 167 168 169 170 171 172 189 190 191 192 193 194 195 196
 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 556.7113 - loglik: -5.5418e+02 - logprior: -2.5357e+00
Epoch 2/2
39/39 - 26s - loss: 545.1986 - loglik: -5.4422e+02 - logprior: -9.7840e-01
Fitted a model with MAP estimate = -541.9706
expansions: [(220, 8), (230, 7), (239, 11)]
discards: [  0 127 128 129 130 154 156 179 180 212 213 214 215 216 217 233 234 235]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 247 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 547.9778 - loglik: -5.4550e+02 - logprior: -2.4741e+00
Epoch 2/2
39/39 - 27s - loss: 538.8651 - loglik: -5.3818e+02 - logprior: -6.8164e-01
Fitted a model with MAP estimate = -536.6290
expansions: [(0, 2), (209, 3), (225, 1), (247, 5)]
discards: [  0 123 172 203 204 205 206 226 227 228 229 230 231 232 233 234 235 236
 237 238 239 240 241 242 243 244 245 246]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 230 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 549.9044 - loglik: -5.4839e+02 - logprior: -1.5188e+00
Epoch 2/10
39/39 - 25s - loss: 544.1996 - loglik: -5.4391e+02 - logprior: -2.9374e-01
Epoch 3/10
39/39 - 26s - loss: 542.6135 - loglik: -5.4244e+02 - logprior: -1.7213e-01
Epoch 4/10
39/39 - 27s - loss: 542.1723 - loglik: -5.4207e+02 - logprior: -1.0030e-01
Epoch 5/10
39/39 - 26s - loss: 541.0266 - loglik: -5.4098e+02 - logprior: -4.5128e-02
Epoch 6/10
39/39 - 26s - loss: 541.1000 - loglik: -5.4111e+02 - logprior: 0.0131
Fitted a model with MAP estimate = -540.3657
Time for alignment: 613.5401
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 659.1973 - loglik: -6.5736e+02 - logprior: -1.8391e+00
Epoch 2/10
39/39 - 23s - loss: 560.6962 - loglik: -5.5916e+02 - logprior: -1.5355e+00
Epoch 3/10
39/39 - 21s - loss: 551.1916 - loglik: -5.4966e+02 - logprior: -1.5271e+00
Epoch 4/10
39/39 - 21s - loss: 549.5911 - loglik: -5.4809e+02 - logprior: -1.5026e+00
Epoch 5/10
39/39 - 22s - loss: 548.6212 - loglik: -5.4712e+02 - logprior: -1.4978e+00
Epoch 6/10
39/39 - 21s - loss: 548.4793 - loglik: -5.4698e+02 - logprior: -1.4975e+00
Epoch 7/10
39/39 - 21s - loss: 547.7888 - loglik: -5.4628e+02 - logprior: -1.5079e+00
Epoch 8/10
39/39 - 21s - loss: 547.3594 - loglik: -5.4585e+02 - logprior: -1.5110e+00
Epoch 9/10
39/39 - 21s - loss: 547.9235 - loglik: -5.4642e+02 - logprior: -1.5081e+00
Fitted a model with MAP estimate = -546.8992
expansions: [(7, 1), (27, 1), (31, 1), (32, 1), (44, 1), (76, 2), (82, 3), (87, 1), (88, 3), (90, 2), (112, 1), (113, 2), (114, 6), (115, 1), (116, 2), (117, 1), (118, 2), (123, 1), (125, 2), (127, 7), (139, 9), (140, 2), (145, 1), (157, 1), (158, 1), (161, 2), (162, 1), (174, 3)]
discards: [  0   1 150 154 163 164 165 166 167 168 169 170 171 178 179 180 181 182
 183 184 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 563.4324 - loglik: -5.6082e+02 - logprior: -2.6125e+00
Epoch 2/2
39/39 - 23s - loss: 550.2488 - loglik: -5.4930e+02 - logprior: -9.4769e-01
Fitted a model with MAP estimate = -547.3120
expansions: [(0, 2), (194, 1), (213, 1), (216, 1), (217, 2), (223, 11), (226, 1), (231, 7)]
discards: [  0  80  99 102 129 131 132 133 134 161 162 184 185 201 202 203]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 547.8132 - loglik: -5.4608e+02 - logprior: -1.7304e+00
Epoch 2/2
39/39 - 29s - loss: 539.2806 - loglik: -5.3882e+02 - logprior: -4.5921e-01
Fitted a model with MAP estimate = -537.4843
expansions: [(0, 2), (191, 2), (195, 4), (241, 4)]
discards: [  0   1   2 174 215 216 217 221 222 223 224 233 234 235 236 237 238 239
 240]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 234 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 546.8998 - loglik: -5.4540e+02 - logprior: -1.4959e+00
Epoch 2/10
39/39 - 24s - loss: 541.5222 - loglik: -5.4135e+02 - logprior: -1.7069e-01
Epoch 3/10
39/39 - 25s - loss: 539.5845 - loglik: -5.3955e+02 - logprior: -3.4394e-02
Epoch 4/10
39/39 - 24s - loss: 538.8837 - loglik: -5.3892e+02 - logprior: 0.0379
Epoch 5/10
39/39 - 24s - loss: 538.4719 - loglik: -5.3857e+02 - logprior: 0.0932
Epoch 6/10
39/39 - 24s - loss: 537.6980 - loglik: -5.3784e+02 - logprior: 0.1468
Epoch 7/10
39/39 - 24s - loss: 537.4692 - loglik: -5.3767e+02 - logprior: 0.2035
Epoch 8/10
39/39 - 24s - loss: 537.3459 - loglik: -5.3760e+02 - logprior: 0.2551
Epoch 9/10
39/39 - 25s - loss: 537.3146 - loglik: -5.3763e+02 - logprior: 0.3174
Epoch 10/10
39/39 - 25s - loss: 537.3190 - loglik: -5.3770e+02 - logprior: 0.3767
Fitted a model with MAP estimate = -536.8280
Time for alignment: 664.2362
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 660.1288 - loglik: -6.5835e+02 - logprior: -1.7765e+00
Epoch 2/10
39/39 - 19s - loss: 561.5627 - loglik: -5.6008e+02 - logprior: -1.4860e+00
Epoch 3/10
39/39 - 18s - loss: 551.8707 - loglik: -5.5039e+02 - logprior: -1.4782e+00
Epoch 4/10
39/39 - 17s - loss: 549.2759 - loglik: -5.4781e+02 - logprior: -1.4695e+00
Epoch 5/10
39/39 - 16s - loss: 547.7329 - loglik: -5.4625e+02 - logprior: -1.4782e+00
Epoch 6/10
39/39 - 16s - loss: 547.3944 - loglik: -5.4592e+02 - logprior: -1.4698e+00
Epoch 7/10
39/39 - 16s - loss: 546.8078 - loglik: -5.4535e+02 - logprior: -1.4576e+00
Epoch 8/10
39/39 - 16s - loss: 546.4877 - loglik: -5.4503e+02 - logprior: -1.4593e+00
Epoch 9/10
39/39 - 16s - loss: 546.3876 - loglik: -5.4493e+02 - logprior: -1.4558e+00
Epoch 10/10
39/39 - 16s - loss: 546.6019 - loglik: -5.4515e+02 - logprior: -1.4524e+00
Fitted a model with MAP estimate = -545.7102
expansions: [(4, 1), (6, 1), (31, 1), (32, 1), (35, 1), (43, 1), (79, 7), (84, 1), (88, 1), (90, 1), (112, 1), (113, 2), (114, 7), (115, 3), (116, 1), (117, 2), (122, 1), (125, 1), (127, 2), (128, 4), (140, 9), (141, 2), (159, 1), (162, 2), (163, 1), (175, 5), (212, 5)]
discards: [  0 164 165 166 167 168 169 170 171 172 173 180 189 190 191 192 193 194
 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 558.5746 - loglik: -5.5596e+02 - logprior: -2.6113e+00
Epoch 2/2
39/39 - 19s - loss: 546.4965 - loglik: -5.4535e+02 - logprior: -1.1512e+00
Fitted a model with MAP estimate = -543.0865
expansions: [(189, 1), (214, 1), (218, 2), (242, 22)]
discards: [  0  86  87 130 133 134 135 136 141 162 163 219 220 221 222 223 224 225
 226 227 228 240 241]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 245 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 548.8788 - loglik: -5.4634e+02 - logprior: -2.5353e+00
Epoch 2/2
39/39 - 20s - loss: 539.1644 - loglik: -5.3835e+02 - logprior: -8.0951e-01
Fitted a model with MAP estimate = -536.7966
expansions: [(0, 2), (202, 8), (208, 6), (209, 1), (222, 1), (245, 4)]
discards: [  0 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239
 240 241 242 243 244]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 244 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 545.3186 - loglik: -5.4373e+02 - logprior: -1.5873e+00
Epoch 2/10
39/39 - 20s - loss: 539.5543 - loglik: -5.3918e+02 - logprior: -3.7846e-01
Epoch 3/10
39/39 - 20s - loss: 538.2251 - loglik: -5.3796e+02 - logprior: -2.6107e-01
Epoch 4/10
39/39 - 20s - loss: 537.4498 - loglik: -5.3725e+02 - logprior: -2.0289e-01
Epoch 5/10
39/39 - 20s - loss: 536.7677 - loglik: -5.3661e+02 - logprior: -1.5412e-01
Epoch 6/10
39/39 - 20s - loss: 536.1161 - loglik: -5.3601e+02 - logprior: -1.0482e-01
Epoch 7/10
39/39 - 20s - loss: 536.3434 - loglik: -5.3630e+02 - logprior: -3.9426e-02
Fitted a model with MAP estimate = -535.6234
Time for alignment: 492.5375
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 658.3994 - loglik: -6.5660e+02 - logprior: -1.8005e+00
Epoch 2/10
39/39 - 17s - loss: 559.6467 - loglik: -5.5818e+02 - logprior: -1.4678e+00
Epoch 3/10
39/39 - 17s - loss: 550.4467 - loglik: -5.4900e+02 - logprior: -1.4428e+00
Epoch 4/10
39/39 - 17s - loss: 548.3110 - loglik: -5.4690e+02 - logprior: -1.4111e+00
Epoch 5/10
39/39 - 17s - loss: 547.8364 - loglik: -5.4642e+02 - logprior: -1.4155e+00
Epoch 6/10
39/39 - 17s - loss: 546.8893 - loglik: -5.4547e+02 - logprior: -1.4157e+00
Epoch 7/10
39/39 - 17s - loss: 546.5679 - loglik: -5.4515e+02 - logprior: -1.4151e+00
Epoch 8/10
39/39 - 17s - loss: 546.7029 - loglik: -5.4529e+02 - logprior: -1.4144e+00
Fitted a model with MAP estimate = -545.7203
expansions: [(4, 1), (6, 1), (26, 1), (32, 1), (35, 1), (48, 1), (81, 7), (86, 1), (88, 1), (90, 2), (114, 1), (115, 6), (116, 3), (117, 1), (118, 2), (127, 1), (129, 7), (139, 1), (140, 9), (141, 2), (145, 1), (146, 1), (158, 1), (159, 1), (162, 2), (163, 1), (175, 4), (188, 1), (212, 6)]
discards: [  0 164 165 166 167 168 169 170 171 172 180 190 191 192 193 194 195 196
 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 247 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 556.2836 - loglik: -5.5369e+02 - logprior: -2.5948e+00
Epoch 2/2
39/39 - 20s - loss: 544.5010 - loglik: -5.4347e+02 - logprior: -1.0339e+00
Fitted a model with MAP estimate = -541.3626
expansions: [(227, 8), (228, 1), (247, 11)]
discards: [  0  87 133 134 135 140 161 162 184 185 215 222 223 224 240 241 242 243
 244 245 246]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 547.8476 - loglik: -5.4551e+02 - logprior: -2.3350e+00
Epoch 2/2
39/39 - 19s - loss: 540.7432 - loglik: -5.4017e+02 - logprior: -5.7252e-01
Fitted a model with MAP estimate = -538.5673
expansions: [(0, 2), (212, 1), (217, 2), (246, 8)]
discards: [  0  89 175 222 235 236 237 238 239 240 241 242 243 244 245]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 244 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 544.9263 - loglik: -5.4333e+02 - logprior: -1.6011e+00
Epoch 2/10
39/39 - 20s - loss: 539.3749 - loglik: -5.3902e+02 - logprior: -3.5844e-01
Epoch 3/10
39/39 - 20s - loss: 538.5262 - loglik: -5.3830e+02 - logprior: -2.2484e-01
Epoch 4/10
39/39 - 20s - loss: 537.1730 - loglik: -5.3701e+02 - logprior: -1.6658e-01
Epoch 5/10
39/39 - 21s - loss: 537.2233 - loglik: -5.3712e+02 - logprior: -1.0648e-01
Fitted a model with MAP estimate = -536.6226
Time for alignment: 414.2057
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 658.8344 - loglik: -6.5702e+02 - logprior: -1.8161e+00
Epoch 2/10
39/39 - 17s - loss: 559.6807 - loglik: -5.5810e+02 - logprior: -1.5839e+00
Epoch 3/10
39/39 - 17s - loss: 549.5611 - loglik: -5.4794e+02 - logprior: -1.6165e+00
Epoch 4/10
39/39 - 18s - loss: 547.7440 - loglik: -5.4616e+02 - logprior: -1.5863e+00
Epoch 5/10
39/39 - 18s - loss: 546.2506 - loglik: -5.4465e+02 - logprior: -1.5984e+00
Epoch 6/10
39/39 - 18s - loss: 546.0726 - loglik: -5.4448e+02 - logprior: -1.5906e+00
Epoch 7/10
39/39 - 18s - loss: 545.7369 - loglik: -5.4415e+02 - logprior: -1.5892e+00
Epoch 8/10
39/39 - 18s - loss: 545.0509 - loglik: -5.4346e+02 - logprior: -1.5889e+00
Epoch 9/10
39/39 - 18s - loss: 545.3158 - loglik: -5.4373e+02 - logprior: -1.5832e+00
Fitted a model with MAP estimate = -544.2742
expansions: [(4, 1), (6, 1), (33, 1), (36, 1), (44, 1), (80, 3), (87, 1), (89, 1), (90, 1), (91, 1), (113, 1), (114, 2), (115, 7), (116, 3), (117, 1), (118, 3), (123, 1), (125, 2), (127, 2), (128, 3), (131, 1), (139, 9), (140, 2), (145, 1), (155, 1), (157, 1), (161, 2), (162, 1), (173, 5), (212, 6)]
discards: [  0 163 164 165 166 167 168 169 170 171 178 188 189 190 191 192 193 194
 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 557.5807 - loglik: -5.5494e+02 - logprior: -2.6370e+00
Epoch 2/2
39/39 - 23s - loss: 544.7551 - loglik: -5.4362e+02 - logprior: -1.1316e+00
Fitted a model with MAP estimate = -541.5114
expansions: [(217, 3), (236, 1), (243, 7)]
discards: [  0   1  87 127 130 131 132 138 144 160 182 183 213 218 219 220 221 222
 223 224 225 226 227 239 240 241]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 228 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 554.1901 - loglik: -5.5181e+02 - logprior: -2.3839e+00
Epoch 2/2
39/39 - 21s - loss: 545.2066 - loglik: -5.4467e+02 - logprior: -5.3657e-01
Fitted a model with MAP estimate = -543.0252
expansions: [(0, 3), (188, 1), (201, 2), (204, 3), (205, 1), (228, 7)]
discards: [171 189 220 221 222 223 224 225 226 227]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 235 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 545.2404 - loglik: -5.4359e+02 - logprior: -1.6539e+00
Epoch 2/10
39/39 - 21s - loss: 538.4610 - loglik: -5.3813e+02 - logprior: -3.3411e-01
Epoch 3/10
39/39 - 21s - loss: 536.8227 - loglik: -5.3659e+02 - logprior: -2.3620e-01
Epoch 4/10
39/39 - 21s - loss: 536.1914 - loglik: -5.3604e+02 - logprior: -1.5157e-01
Epoch 5/10
39/39 - 20s - loss: 535.4648 - loglik: -5.3538e+02 - logprior: -8.2273e-02
Epoch 6/10
39/39 - 20s - loss: 535.3992 - loglik: -5.3537e+02 - logprior: -3.1226e-02
Epoch 7/10
39/39 - 20s - loss: 534.5793 - loglik: -5.3461e+02 - logprior: 0.0262
Epoch 8/10
39/39 - 20s - loss: 534.9285 - loglik: -5.3503e+02 - logprior: 0.1010
Fitted a model with MAP estimate = -534.4936
Time for alignment: 516.8356
Computed alignments with likelihoods: ['-536.6290', '-536.8280', '-535.6234', '-536.6226', '-534.4936']
Best model has likelihood: -534.4936  (prior= 0.1499 )
time for generating output: 0.2759
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.8834657586076429
Training of 5 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bd458cc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d95869af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d95869850>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d1be1ddc0>
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 275.9422 - loglik: -2.7284e+02 - logprior: -3.1058e+00
Epoch 2/10
19/19 - 2s - loss: 207.4303 - loglik: -2.0606e+02 - logprior: -1.3698e+00
Epoch 3/10
19/19 - 2s - loss: 182.9404 - loglik: -1.8134e+02 - logprior: -1.6024e+00
Epoch 4/10
19/19 - 2s - loss: 178.3133 - loglik: -1.7682e+02 - logprior: -1.4913e+00
Epoch 5/10
19/19 - 2s - loss: 177.1198 - loglik: -1.7566e+02 - logprior: -1.4590e+00
Epoch 6/10
19/19 - 2s - loss: 176.4834 - loglik: -1.7506e+02 - logprior: -1.4254e+00
Epoch 7/10
19/19 - 2s - loss: 176.2368 - loglik: -1.7483e+02 - logprior: -1.4019e+00
Epoch 8/10
19/19 - 2s - loss: 176.1289 - loglik: -1.7473e+02 - logprior: -1.4009e+00
Epoch 9/10
19/19 - 2s - loss: 175.3063 - loglik: -1.7391e+02 - logprior: -1.4008e+00
Epoch 10/10
19/19 - 2s - loss: 175.3415 - loglik: -1.7393e+02 - logprior: -1.4160e+00
Fitted a model with MAP estimate = -174.7275
expansions: [(6, 1), (7, 1), (8, 3), (19, 1), (24, 1), (33, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 4), (72, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 170.6473 - loglik: -1.6671e+02 - logprior: -3.9383e+00
Epoch 2/2
19/19 - 3s - loss: 158.2567 - loglik: -1.5624e+02 - logprior: -2.0182e+00
Fitted a model with MAP estimate = -156.3509
expansions: [(0, 2)]
discards: [ 0 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 158.1039 - loglik: -1.5518e+02 - logprior: -2.9256e+00
Epoch 2/2
19/19 - 3s - loss: 154.4261 - loglik: -1.5333e+02 - logprior: -1.0995e+00
Fitted a model with MAP estimate = -153.5390
expansions: []
discards: [ 0 43]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 160.0214 - loglik: -1.5613e+02 - logprior: -3.8921e+00
Epoch 2/10
19/19 - 3s - loss: 156.0229 - loglik: -1.5455e+02 - logprior: -1.4734e+00
Epoch 3/10
19/19 - 3s - loss: 154.3692 - loglik: -1.5330e+02 - logprior: -1.0675e+00
Epoch 4/10
19/19 - 3s - loss: 154.5310 - loglik: -1.5351e+02 - logprior: -1.0213e+00
Fitted a model with MAP estimate = -153.9146
Time for alignment: 71.8485
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 275.7195 - loglik: -2.7262e+02 - logprior: -3.1038e+00
Epoch 2/10
19/19 - 2s - loss: 207.7188 - loglik: -2.0635e+02 - logprior: -1.3675e+00
Epoch 3/10
19/19 - 2s - loss: 182.2688 - loglik: -1.8066e+02 - logprior: -1.6134e+00
Epoch 4/10
19/19 - 2s - loss: 177.4971 - loglik: -1.7598e+02 - logprior: -1.5210e+00
Epoch 5/10
19/19 - 2s - loss: 176.4649 - loglik: -1.7496e+02 - logprior: -1.5038e+00
Epoch 6/10
19/19 - 2s - loss: 175.5498 - loglik: -1.7408e+02 - logprior: -1.4695e+00
Epoch 7/10
19/19 - 2s - loss: 175.2933 - loglik: -1.7385e+02 - logprior: -1.4429e+00
Epoch 8/10
19/19 - 2s - loss: 175.2464 - loglik: -1.7381e+02 - logprior: -1.4371e+00
Epoch 9/10
19/19 - 2s - loss: 174.7114 - loglik: -1.7328e+02 - logprior: -1.4303e+00
Epoch 10/10
19/19 - 2s - loss: 174.5299 - loglik: -1.7310e+02 - logprior: -1.4295e+00
Fitted a model with MAP estimate = -174.3923
expansions: [(6, 3), (7, 1), (8, 1), (17, 1), (22, 1), (33, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 4), (72, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 170.4033 - loglik: -1.6643e+02 - logprior: -3.9740e+00
Epoch 2/2
19/19 - 3s - loss: 157.8469 - loglik: -1.5573e+02 - logprior: -2.1184e+00
Fitted a model with MAP estimate = -155.6053
expansions: [(2, 1)]
discards: [ 0 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 159.7758 - loglik: -1.5606e+02 - logprior: -3.7120e+00
Epoch 2/2
19/19 - 3s - loss: 154.5471 - loglik: -1.5333e+02 - logprior: -1.2203e+00
Fitted a model with MAP estimate = -153.9876
expansions: []
discards: [42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 157.2551 - loglik: -1.5428e+02 - logprior: -2.9784e+00
Epoch 2/10
19/19 - 3s - loss: 154.2129 - loglik: -1.5309e+02 - logprior: -1.1246e+00
Epoch 3/10
19/19 - 3s - loss: 153.6457 - loglik: -1.5263e+02 - logprior: -1.0137e+00
Epoch 4/10
19/19 - 3s - loss: 153.1954 - loglik: -1.5224e+02 - logprior: -9.5533e-01
Epoch 5/10
19/19 - 3s - loss: 152.8855 - loglik: -1.5196e+02 - logprior: -9.2435e-01
Epoch 6/10
19/19 - 3s - loss: 152.7144 - loglik: -1.5180e+02 - logprior: -9.1064e-01
Epoch 7/10
19/19 - 3s - loss: 152.4725 - loglik: -1.5160e+02 - logprior: -8.7745e-01
Epoch 8/10
19/19 - 3s - loss: 151.9512 - loglik: -1.5108e+02 - logprior: -8.7335e-01
Epoch 9/10
19/19 - 3s - loss: 151.4187 - loglik: -1.5056e+02 - logprior: -8.5839e-01
Epoch 10/10
19/19 - 3s - loss: 151.6273 - loglik: -1.5078e+02 - logprior: -8.4830e-01
Fitted a model with MAP estimate = -150.9636
Time for alignment: 87.5903
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 275.8204 - loglik: -2.7272e+02 - logprior: -3.1045e+00
Epoch 2/10
19/19 - 2s - loss: 208.1522 - loglik: -2.0678e+02 - logprior: -1.3697e+00
Epoch 3/10
19/19 - 2s - loss: 183.2191 - loglik: -1.8160e+02 - logprior: -1.6155e+00
Epoch 4/10
19/19 - 2s - loss: 177.4995 - loglik: -1.7597e+02 - logprior: -1.5302e+00
Epoch 5/10
19/19 - 2s - loss: 176.0027 - loglik: -1.7449e+02 - logprior: -1.5151e+00
Epoch 6/10
19/19 - 2s - loss: 175.8655 - loglik: -1.7439e+02 - logprior: -1.4789e+00
Epoch 7/10
19/19 - 2s - loss: 175.2377 - loglik: -1.7378e+02 - logprior: -1.4613e+00
Epoch 8/10
19/19 - 2s - loss: 174.4912 - loglik: -1.7304e+02 - logprior: -1.4493e+00
Epoch 9/10
19/19 - 2s - loss: 174.8796 - loglik: -1.7344e+02 - logprior: -1.4445e+00
Fitted a model with MAP estimate = -174.3312
expansions: [(6, 1), (7, 1), (8, 3), (19, 1), (22, 1), (33, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 3), (48, 2), (69, 1), (70, 4), (72, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 170.2133 - loglik: -1.6624e+02 - logprior: -3.9696e+00
Epoch 2/2
19/19 - 3s - loss: 158.1625 - loglik: -1.5616e+02 - logprior: -2.0069e+00
Fitted a model with MAP estimate = -156.4659
expansions: [(0, 2)]
discards: [ 0 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 157.5347 - loglik: -1.5464e+02 - logprior: -2.8957e+00
Epoch 2/2
19/19 - 3s - loss: 153.8977 - loglik: -1.5282e+02 - logprior: -1.0806e+00
Fitted a model with MAP estimate = -153.1929
expansions: []
discards: [ 0 43]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 160.0016 - loglik: -1.5616e+02 - logprior: -3.8445e+00
Epoch 2/10
19/19 - 3s - loss: 155.5111 - loglik: -1.5404e+02 - logprior: -1.4745e+00
Epoch 3/10
19/19 - 3s - loss: 153.9112 - loglik: -1.5295e+02 - logprior: -9.6452e-01
Epoch 4/10
19/19 - 3s - loss: 153.3804 - loglik: -1.5246e+02 - logprior: -9.1860e-01
Epoch 5/10
19/19 - 3s - loss: 153.2816 - loglik: -1.5237e+02 - logprior: -9.0854e-01
Epoch 6/10
19/19 - 3s - loss: 152.7220 - loglik: -1.5183e+02 - logprior: -8.9188e-01
Epoch 7/10
19/19 - 3s - loss: 152.1225 - loglik: -1.5125e+02 - logprior: -8.6939e-01
Epoch 8/10
19/19 - 3s - loss: 151.8089 - loglik: -1.5094e+02 - logprior: -8.7177e-01
Epoch 9/10
19/19 - 3s - loss: 151.2405 - loglik: -1.5039e+02 - logprior: -8.4740e-01
Epoch 10/10
19/19 - 3s - loss: 151.6940 - loglik: -1.5084e+02 - logprior: -8.5607e-01
Fitted a model with MAP estimate = -150.8131
Time for alignment: 85.2963
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 275.5929 - loglik: -2.7249e+02 - logprior: -3.1014e+00
Epoch 2/10
19/19 - 2s - loss: 208.6686 - loglik: -2.0730e+02 - logprior: -1.3684e+00
Epoch 3/10
19/19 - 2s - loss: 183.4135 - loglik: -1.8181e+02 - logprior: -1.6043e+00
Epoch 4/10
19/19 - 2s - loss: 177.3250 - loglik: -1.7579e+02 - logprior: -1.5323e+00
Epoch 5/10
19/19 - 2s - loss: 176.6046 - loglik: -1.7509e+02 - logprior: -1.5189e+00
Epoch 6/10
19/19 - 2s - loss: 175.0937 - loglik: -1.7361e+02 - logprior: -1.4851e+00
Epoch 7/10
19/19 - 2s - loss: 175.1620 - loglik: -1.7370e+02 - logprior: -1.4655e+00
Fitted a model with MAP estimate = -174.7834
expansions: [(6, 1), (7, 1), (8, 3), (19, 1), (22, 1), (33, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 3), (48, 2), (69, 1), (70, 4), (72, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 172.3674 - loglik: -1.6838e+02 - logprior: -3.9848e+00
Epoch 2/2
19/19 - 3s - loss: 159.2444 - loglik: -1.5723e+02 - logprior: -2.0145e+00
Fitted a model with MAP estimate = -157.2049
expansions: [(0, 2)]
discards: [ 0 42 60]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 159.3104 - loglik: -1.5638e+02 - logprior: -2.9297e+00
Epoch 2/2
19/19 - 3s - loss: 154.7921 - loglik: -1.5367e+02 - logprior: -1.1209e+00
Fitted a model with MAP estimate = -153.9760
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 160.6416 - loglik: -1.5678e+02 - logprior: -3.8618e+00
Epoch 2/10
19/19 - 3s - loss: 155.8423 - loglik: -1.5441e+02 - logprior: -1.4339e+00
Epoch 3/10
19/19 - 3s - loss: 154.0845 - loglik: -1.5311e+02 - logprior: -9.7737e-01
Epoch 4/10
19/19 - 3s - loss: 153.7632 - loglik: -1.5281e+02 - logprior: -9.5573e-01
Epoch 5/10
19/19 - 3s - loss: 152.4934 - loglik: -1.5157e+02 - logprior: -9.2827e-01
Epoch 6/10
19/19 - 3s - loss: 152.9391 - loglik: -1.5203e+02 - logprior: -9.0825e-01
Fitted a model with MAP estimate = -152.2266
Time for alignment: 69.0143
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 275.6173 - loglik: -2.7251e+02 - logprior: -3.1027e+00
Epoch 2/10
19/19 - 2s - loss: 207.9534 - loglik: -2.0659e+02 - logprior: -1.3650e+00
Epoch 3/10
19/19 - 2s - loss: 183.0098 - loglik: -1.8140e+02 - logprior: -1.6102e+00
Epoch 4/10
19/19 - 2s - loss: 177.6135 - loglik: -1.7609e+02 - logprior: -1.5258e+00
Epoch 5/10
19/19 - 2s - loss: 176.1954 - loglik: -1.7469e+02 - logprior: -1.5091e+00
Epoch 6/10
19/19 - 2s - loss: 175.3030 - loglik: -1.7382e+02 - logprior: -1.4795e+00
Epoch 7/10
19/19 - 2s - loss: 175.1488 - loglik: -1.7369e+02 - logprior: -1.4607e+00
Epoch 8/10
19/19 - 2s - loss: 174.9520 - loglik: -1.7350e+02 - logprior: -1.4521e+00
Epoch 9/10
19/19 - 2s - loss: 174.2640 - loglik: -1.7281e+02 - logprior: -1.4541e+00
Epoch 10/10
19/19 - 2s - loss: 174.7671 - loglik: -1.7332e+02 - logprior: -1.4486e+00
Fitted a model with MAP estimate = -174.1639
expansions: [(6, 1), (7, 1), (8, 3), (19, 1), (22, 1), (33, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 4), (72, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 172.6973 - loglik: -1.6872e+02 - logprior: -3.9794e+00
Epoch 2/2
19/19 - 3s - loss: 159.4672 - loglik: -1.5743e+02 - logprior: -2.0381e+00
Fitted a model with MAP estimate = -157.3930
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 159.4262 - loglik: -1.5650e+02 - logprior: -2.9233e+00
Epoch 2/2
19/19 - 3s - loss: 155.0085 - loglik: -1.5390e+02 - logprior: -1.1086e+00
Fitted a model with MAP estimate = -154.0742
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 161.2065 - loglik: -1.5735e+02 - logprior: -3.8549e+00
Epoch 2/10
19/19 - 3s - loss: 156.0812 - loglik: -1.5466e+02 - logprior: -1.4231e+00
Epoch 3/10
19/19 - 3s - loss: 154.3754 - loglik: -1.5339e+02 - logprior: -9.8597e-01
Epoch 4/10
19/19 - 3s - loss: 153.5676 - loglik: -1.5263e+02 - logprior: -9.3750e-01
Epoch 5/10
19/19 - 3s - loss: 153.1474 - loglik: -1.5223e+02 - logprior: -9.1240e-01
Epoch 6/10
19/19 - 3s - loss: 152.8249 - loglik: -1.5193e+02 - logprior: -8.9200e-01
Epoch 7/10
19/19 - 3s - loss: 152.1329 - loglik: -1.5125e+02 - logprior: -8.7941e-01
Epoch 8/10
19/19 - 3s - loss: 151.7509 - loglik: -1.5087e+02 - logprior: -8.7598e-01
Epoch 9/10
19/19 - 3s - loss: 151.2852 - loglik: -1.5044e+02 - logprior: -8.4992e-01
Epoch 10/10
19/19 - 3s - loss: 151.6375 - loglik: -1.5079e+02 - logprior: -8.5103e-01
Fitted a model with MAP estimate = -150.9553
Time for alignment: 87.9372
Computed alignments with likelihoods: ['-153.5390', '-150.9636', '-150.8131', '-152.2266', '-150.9553']
Best model has likelihood: -150.8131  (prior= -0.8367 )
time for generating output: 0.1277
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9106382978723404
Training of 5 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bd4676e20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4daf6b5940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d95a16730>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4c0a4b69d0>
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 235.4595 - loglik: -2.3236e+02 - logprior: -3.1035e+00
Epoch 2/10
19/19 - 1s - loss: 199.6900 - loglik: -1.9841e+02 - logprior: -1.2769e+00
Epoch 3/10
19/19 - 1s - loss: 185.3547 - loglik: -1.8407e+02 - logprior: -1.2862e+00
Epoch 4/10
19/19 - 1s - loss: 181.8166 - loglik: -1.8048e+02 - logprior: -1.3404e+00
Epoch 5/10
19/19 - 1s - loss: 181.0132 - loglik: -1.7971e+02 - logprior: -1.3070e+00
Epoch 6/10
19/19 - 1s - loss: 179.5607 - loglik: -1.7825e+02 - logprior: -1.3133e+00
Epoch 7/10
19/19 - 1s - loss: 179.5023 - loglik: -1.7819e+02 - logprior: -1.3133e+00
Epoch 8/10
19/19 - 1s - loss: 178.9394 - loglik: -1.7763e+02 - logprior: -1.3123e+00
Epoch 9/10
19/19 - 1s - loss: 178.8061 - loglik: -1.7749e+02 - logprior: -1.3136e+00
Epoch 10/10
19/19 - 1s - loss: 178.9387 - loglik: -1.7763e+02 - logprior: -1.3086e+00
Fitted a model with MAP estimate = -178.5499
expansions: [(7, 4), (9, 1), (22, 1), (23, 1), (24, 1), (31, 2), (36, 1), (38, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (57, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 185.4267 - loglik: -1.8148e+02 - logprior: -3.9424e+00
Epoch 2/2
19/19 - 1s - loss: 177.0878 - loglik: -1.7507e+02 - logprior: -2.0185e+00
Fitted a model with MAP estimate = -175.3257
expansions: [(0, 2), (6, 1)]
discards: [ 0 39 67]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 178.1578 - loglik: -1.7527e+02 - logprior: -2.8843e+00
Epoch 2/2
19/19 - 1s - loss: 174.3930 - loglik: -1.7325e+02 - logprior: -1.1400e+00
Fitted a model with MAP estimate = -173.1814
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 178.7421 - loglik: -1.7513e+02 - logprior: -3.6082e+00
Epoch 2/10
19/19 - 1s - loss: 174.3424 - loglik: -1.7320e+02 - logprior: -1.1418e+00
Epoch 3/10
19/19 - 1s - loss: 173.0387 - loglik: -1.7207e+02 - logprior: -9.6645e-01
Epoch 4/10
19/19 - 1s - loss: 171.7991 - loglik: -1.7087e+02 - logprior: -9.3208e-01
Epoch 5/10
19/19 - 1s - loss: 171.6568 - loglik: -1.7075e+02 - logprior: -9.0360e-01
Epoch 6/10
19/19 - 1s - loss: 170.7890 - loglik: -1.6989e+02 - logprior: -8.9692e-01
Epoch 7/10
19/19 - 2s - loss: 170.6671 - loglik: -1.6979e+02 - logprior: -8.7305e-01
Epoch 8/10
19/19 - 1s - loss: 170.2433 - loglik: -1.6938e+02 - logprior: -8.6779e-01
Epoch 9/10
19/19 - 1s - loss: 170.7381 - loglik: -1.6988e+02 - logprior: -8.5922e-01
Fitted a model with MAP estimate = -170.2391
Time for alignment: 55.6660
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 235.6057 - loglik: -2.3250e+02 - logprior: -3.1034e+00
Epoch 2/10
19/19 - 1s - loss: 200.4553 - loglik: -1.9919e+02 - logprior: -1.2692e+00
Epoch 3/10
19/19 - 1s - loss: 185.5408 - loglik: -1.8423e+02 - logprior: -1.3157e+00
Epoch 4/10
19/19 - 1s - loss: 181.6721 - loglik: -1.8036e+02 - logprior: -1.3144e+00
Epoch 5/10
19/19 - 1s - loss: 180.0550 - loglik: -1.7876e+02 - logprior: -1.2911e+00
Epoch 6/10
19/19 - 1s - loss: 179.6532 - loglik: -1.7835e+02 - logprior: -1.3035e+00
Epoch 7/10
19/19 - 1s - loss: 179.0089 - loglik: -1.7772e+02 - logprior: -1.2906e+00
Epoch 8/10
19/19 - 1s - loss: 178.5842 - loglik: -1.7729e+02 - logprior: -1.2962e+00
Epoch 9/10
19/19 - 1s - loss: 178.7266 - loglik: -1.7743e+02 - logprior: -1.2952e+00
Fitted a model with MAP estimate = -178.3558
expansions: [(7, 2), (8, 2), (9, 2), (22, 1), (23, 1), (24, 1), (31, 1), (36, 1), (37, 1), (38, 1), (49, 1), (50, 1), (51, 1), (52, 2), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 185.1050 - loglik: -1.8117e+02 - logprior: -3.9385e+00
Epoch 2/2
19/19 - 1s - loss: 176.6803 - loglik: -1.7466e+02 - logprior: -2.0239e+00
Fitted a model with MAP estimate = -174.9094
expansions: [(0, 2), (9, 1), (10, 1)]
discards: [ 0  6 67 72 73]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 178.5521 - loglik: -1.7568e+02 - logprior: -2.8692e+00
Epoch 2/2
19/19 - 2s - loss: 174.0932 - loglik: -1.7296e+02 - logprior: -1.1293e+00
Fitted a model with MAP estimate = -173.0271
expansions: []
discards: [ 0 15]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 178.8872 - loglik: -1.7527e+02 - logprior: -3.6197e+00
Epoch 2/10
19/19 - 1s - loss: 174.5197 - loglik: -1.7338e+02 - logprior: -1.1416e+00
Epoch 3/10
19/19 - 1s - loss: 172.8884 - loglik: -1.7193e+02 - logprior: -9.5684e-01
Epoch 4/10
19/19 - 1s - loss: 172.0319 - loglik: -1.7110e+02 - logprior: -9.2891e-01
Epoch 5/10
19/19 - 1s - loss: 171.5652 - loglik: -1.7066e+02 - logprior: -9.0822e-01
Epoch 6/10
19/19 - 1s - loss: 170.9879 - loglik: -1.7009e+02 - logprior: -8.9383e-01
Epoch 7/10
19/19 - 1s - loss: 170.3090 - loglik: -1.6943e+02 - logprior: -8.8296e-01
Epoch 8/10
19/19 - 1s - loss: 170.7705 - loglik: -1.6989e+02 - logprior: -8.7694e-01
Fitted a model with MAP estimate = -170.3316
Time for alignment: 53.0051
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 235.6969 - loglik: -2.3259e+02 - logprior: -3.1033e+00
Epoch 2/10
19/19 - 1s - loss: 200.9009 - loglik: -1.9963e+02 - logprior: -1.2678e+00
Epoch 3/10
19/19 - 1s - loss: 186.6124 - loglik: -1.8529e+02 - logprior: -1.3242e+00
Epoch 4/10
19/19 - 1s - loss: 182.1241 - loglik: -1.8079e+02 - logprior: -1.3295e+00
Epoch 5/10
19/19 - 1s - loss: 180.4036 - loglik: -1.7909e+02 - logprior: -1.3106e+00
Epoch 6/10
19/19 - 1s - loss: 179.4482 - loglik: -1.7813e+02 - logprior: -1.3140e+00
Epoch 7/10
19/19 - 1s - loss: 179.0993 - loglik: -1.7779e+02 - logprior: -1.3076e+00
Epoch 8/10
19/19 - 1s - loss: 178.8863 - loglik: -1.7757e+02 - logprior: -1.3116e+00
Epoch 9/10
19/19 - 1s - loss: 178.6316 - loglik: -1.7732e+02 - logprior: -1.3078e+00
Epoch 10/10
19/19 - 1s - loss: 178.2833 - loglik: -1.7698e+02 - logprior: -1.3044e+00
Fitted a model with MAP estimate = -178.2437
expansions: [(7, 2), (8, 2), (9, 2), (22, 1), (23, 1), (24, 1), (31, 2), (36, 1), (38, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 185.5152 - loglik: -1.8156e+02 - logprior: -3.9521e+00
Epoch 2/2
19/19 - 1s - loss: 176.6842 - loglik: -1.7463e+02 - logprior: -2.0539e+00
Fitted a model with MAP estimate = -174.9665
expansions: [(0, 2), (10, 1)]
discards: [ 0  6 40 68 73 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 178.6860 - loglik: -1.7581e+02 - logprior: -2.8758e+00
Epoch 2/2
19/19 - 1s - loss: 174.4968 - loglik: -1.7336e+02 - logprior: -1.1403e+00
Fitted a model with MAP estimate = -173.3302
expansions: [(9, 1)]
discards: [ 0 14]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 179.2328 - loglik: -1.7551e+02 - logprior: -3.7228e+00
Epoch 2/10
19/19 - 1s - loss: 174.6042 - loglik: -1.7340e+02 - logprior: -1.2038e+00
Epoch 3/10
19/19 - 1s - loss: 172.9069 - loglik: -1.7194e+02 - logprior: -9.6884e-01
Epoch 4/10
19/19 - 1s - loss: 172.1980 - loglik: -1.7126e+02 - logprior: -9.3527e-01
Epoch 5/10
19/19 - 1s - loss: 171.4126 - loglik: -1.7050e+02 - logprior: -9.0877e-01
Epoch 6/10
19/19 - 2s - loss: 170.9506 - loglik: -1.7005e+02 - logprior: -8.9745e-01
Epoch 7/10
19/19 - 2s - loss: 170.3389 - loglik: -1.6945e+02 - logprior: -8.8532e-01
Epoch 8/10
19/19 - 2s - loss: 170.9081 - loglik: -1.7004e+02 - logprior: -8.6750e-01
Fitted a model with MAP estimate = -170.4226
Time for alignment: 54.8325
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 235.5160 - loglik: -2.3241e+02 - logprior: -3.1025e+00
Epoch 2/10
19/19 - 1s - loss: 199.6073 - loglik: -1.9833e+02 - logprior: -1.2749e+00
Epoch 3/10
19/19 - 1s - loss: 184.9197 - loglik: -1.8360e+02 - logprior: -1.3164e+00
Epoch 4/10
19/19 - 1s - loss: 181.8731 - loglik: -1.8055e+02 - logprior: -1.3276e+00
Epoch 5/10
19/19 - 1s - loss: 180.3510 - loglik: -1.7904e+02 - logprior: -1.3101e+00
Epoch 6/10
19/19 - 1s - loss: 180.1049 - loglik: -1.7880e+02 - logprior: -1.3062e+00
Epoch 7/10
19/19 - 1s - loss: 179.5885 - loglik: -1.7829e+02 - logprior: -1.3001e+00
Epoch 8/10
19/19 - 1s - loss: 179.2393 - loglik: -1.7794e+02 - logprior: -1.2967e+00
Epoch 9/10
19/19 - 1s - loss: 178.7523 - loglik: -1.7747e+02 - logprior: -1.2871e+00
Epoch 10/10
19/19 - 1s - loss: 178.8877 - loglik: -1.7759e+02 - logprior: -1.2936e+00
Fitted a model with MAP estimate = -178.7112
expansions: [(7, 2), (8, 2), (9, 2), (22, 1), (23, 1), (24, 1), (31, 2), (36, 1), (37, 1), (38, 1), (49, 1), (50, 1), (53, 1), (55, 2), (57, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 185.8888 - loglik: -1.8193e+02 - logprior: -3.9551e+00
Epoch 2/2
19/19 - 2s - loss: 177.5961 - loglik: -1.7556e+02 - logprior: -2.0366e+00
Fitted a model with MAP estimate = -175.8375
expansions: [(0, 2), (9, 1), (10, 1)]
discards: [ 0  6 40 72]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 179.1733 - loglik: -1.7629e+02 - logprior: -2.8815e+00
Epoch 2/2
19/19 - 1s - loss: 174.6574 - loglik: -1.7351e+02 - logprior: -1.1450e+00
Fitted a model with MAP estimate = -173.6414
expansions: []
discards: [ 0 15]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 179.5823 - loglik: -1.7593e+02 - logprior: -3.6566e+00
Epoch 2/10
19/19 - 2s - loss: 174.8981 - loglik: -1.7373e+02 - logprior: -1.1639e+00
Epoch 3/10
19/19 - 2s - loss: 173.8844 - loglik: -1.7292e+02 - logprior: -9.6656e-01
Epoch 4/10
19/19 - 1s - loss: 172.6042 - loglik: -1.7167e+02 - logprior: -9.3858e-01
Epoch 5/10
19/19 - 1s - loss: 172.0262 - loglik: -1.7110e+02 - logprior: -9.2595e-01
Epoch 6/10
19/19 - 1s - loss: 171.3367 - loglik: -1.7042e+02 - logprior: -9.1177e-01
Epoch 7/10
19/19 - 1s - loss: 171.4581 - loglik: -1.7056e+02 - logprior: -8.9657e-01
Fitted a model with MAP estimate = -171.0846
Time for alignment: 51.8335
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 235.4984 - loglik: -2.3239e+02 - logprior: -3.1050e+00
Epoch 2/10
19/19 - 1s - loss: 199.7020 - loglik: -1.9843e+02 - logprior: -1.2681e+00
Epoch 3/10
19/19 - 1s - loss: 185.3138 - loglik: -1.8400e+02 - logprior: -1.3138e+00
Epoch 4/10
19/19 - 1s - loss: 182.0376 - loglik: -1.8071e+02 - logprior: -1.3239e+00
Epoch 5/10
19/19 - 1s - loss: 180.3779 - loglik: -1.7909e+02 - logprior: -1.2928e+00
Epoch 6/10
19/19 - 1s - loss: 179.5782 - loglik: -1.7828e+02 - logprior: -1.2984e+00
Epoch 7/10
19/19 - 1s - loss: 179.0749 - loglik: -1.7779e+02 - logprior: -1.2898e+00
Epoch 8/10
19/19 - 1s - loss: 178.9025 - loglik: -1.7761e+02 - logprior: -1.2950e+00
Epoch 9/10
19/19 - 1s - loss: 178.8899 - loglik: -1.7760e+02 - logprior: -1.2901e+00
Epoch 10/10
19/19 - 1s - loss: 178.2642 - loglik: -1.7698e+02 - logprior: -1.2865e+00
Fitted a model with MAP estimate = -178.3355
expansions: [(7, 2), (8, 3), (9, 2), (23, 1), (24, 1), (31, 2), (36, 1), (38, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 2), (55, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 185.4922 - loglik: -1.8153e+02 - logprior: -3.9598e+00
Epoch 2/2
19/19 - 2s - loss: 176.6079 - loglik: -1.7452e+02 - logprior: -2.0871e+00
Fitted a model with MAP estimate = -174.7903
expansions: [(0, 2)]
discards: [ 0 40 68 73 74]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 178.5044 - loglik: -1.7556e+02 - logprior: -2.9419e+00
Epoch 2/2
19/19 - 1s - loss: 174.7088 - loglik: -1.7355e+02 - logprior: -1.1610e+00
Fitted a model with MAP estimate = -173.5424
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 82 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 178.6688 - loglik: -1.7500e+02 - logprior: -3.6691e+00
Epoch 2/10
19/19 - 1s - loss: 174.6978 - loglik: -1.7349e+02 - logprior: -1.2076e+00
Epoch 3/10
19/19 - 1s - loss: 173.2638 - loglik: -1.7225e+02 - logprior: -1.0115e+00
Epoch 4/10
19/19 - 1s - loss: 172.2872 - loglik: -1.7130e+02 - logprior: -9.9136e-01
Epoch 5/10
19/19 - 1s - loss: 171.9097 - loglik: -1.7095e+02 - logprior: -9.6153e-01
Epoch 6/10
19/19 - 1s - loss: 170.7557 - loglik: -1.6979e+02 - logprior: -9.6188e-01
Epoch 7/10
19/19 - 1s - loss: 171.0334 - loglik: -1.7008e+02 - logprior: -9.5010e-01
Fitted a model with MAP estimate = -170.6418
Time for alignment: 52.6737
Computed alignments with likelihoods: ['-170.2391', '-170.3316', '-170.4226', '-171.0846', '-170.6418']
Best model has likelihood: -170.2391  (prior= -0.8635 )
time for generating output: 0.1085
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.8346938775510204
Training of 5 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bde39d6a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4daf2b8d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bdea29700>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d1bd100d0>
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 650.3542 - loglik: -6.4863e+02 - logprior: -1.7255e+00
Epoch 2/10
39/39 - 16s - loss: 599.3698 - loglik: -5.9825e+02 - logprior: -1.1218e+00
Epoch 3/10
39/39 - 16s - loss: 593.7610 - loglik: -5.9262e+02 - logprior: -1.1407e+00
Epoch 4/10
39/39 - 16s - loss: 592.7068 - loglik: -5.9162e+02 - logprior: -1.0913e+00
Epoch 5/10
39/39 - 16s - loss: 592.1988 - loglik: -5.9112e+02 - logprior: -1.0757e+00
Epoch 6/10
39/39 - 16s - loss: 592.2126 - loglik: -5.9116e+02 - logprior: -1.0571e+00
Fitted a model with MAP estimate = -591.6164
expansions: [(11, 1), (22, 1), (23, 5), (27, 1), (46, 4), (47, 1), (48, 1), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (83, 1), (84, 1), (105, 2), (106, 7), (107, 2), (120, 1), (122, 1), (123, 1), (125, 1), (132, 1), (138, 1), (146, 1), (149, 3), (154, 2), (166, 1), (167, 1), (169, 1), (192, 14)]
discards: [  0   1 187 188 189 190 191]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 248 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 594.1910 - loglik: -5.9173e+02 - logprior: -2.4640e+00
Epoch 2/2
39/39 - 22s - loss: 586.6616 - loglik: -5.8598e+02 - logprior: -6.8434e-01
Fitted a model with MAP estimate = -584.7880
expansions: [(0, 3), (30, 3), (223, 2), (224, 2), (248, 6)]
discards: [ 25  26  27  53  54  55 128 129 132 133 237 238 239 240 241 242 243 244
 245 246 247]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 588.7960 - loglik: -5.8707e+02 - logprior: -1.7217e+00
Epoch 2/2
39/39 - 21s - loss: 584.9687 - loglik: -5.8450e+02 - logprior: -4.6397e-01
Fitted a model with MAP estimate = -583.9113
expansions: []
discards: [  0   1   2  55 225 236 237 238 239 240 241 242]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 591.1010 - loglik: -5.8947e+02 - logprior: -1.6327e+00
Epoch 2/10
39/39 - 21s - loss: 587.2993 - loglik: -5.8732e+02 - logprior: 0.0223
Epoch 3/10
39/39 - 22s - loss: 586.4373 - loglik: -5.8654e+02 - logprior: 0.0982
Epoch 4/10
39/39 - 23s - loss: 586.1812 - loglik: -5.8635e+02 - logprior: 0.1644
Epoch 5/10
39/39 - 24s - loss: 585.7190 - loglik: -5.8587e+02 - logprior: 0.1498
Epoch 6/10
39/39 - 25s - loss: 584.7719 - loglik: -5.8493e+02 - logprior: 0.1570
Epoch 7/10
39/39 - 26s - loss: 584.4921 - loglik: -5.8470e+02 - logprior: 0.2039
Epoch 8/10
39/39 - 25s - loss: 584.6083 - loglik: -5.8486e+02 - logprior: 0.2503
Fitted a model with MAP estimate = -584.2177
Time for alignment: 470.5999
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 649.2935 - loglik: -6.4755e+02 - logprior: -1.7419e+00
Epoch 2/10
39/39 - 18s - loss: 599.5650 - loglik: -5.9842e+02 - logprior: -1.1433e+00
Epoch 3/10
39/39 - 18s - loss: 593.5116 - loglik: -5.9234e+02 - logprior: -1.1713e+00
Epoch 4/10
39/39 - 18s - loss: 592.4218 - loglik: -5.9131e+02 - logprior: -1.1126e+00
Epoch 5/10
39/39 - 19s - loss: 592.2468 - loglik: -5.9116e+02 - logprior: -1.0917e+00
Epoch 6/10
39/39 - 20s - loss: 591.7970 - loglik: -5.9072e+02 - logprior: -1.0774e+00
Epoch 7/10
39/39 - 20s - loss: 591.5457 - loglik: -5.9046e+02 - logprior: -1.0853e+00
Epoch 8/10
39/39 - 19s - loss: 591.8002 - loglik: -5.9073e+02 - logprior: -1.0725e+00
Fitted a model with MAP estimate = -591.0828
expansions: [(17, 1), (22, 1), (23, 5), (27, 1), (46, 3), (47, 1), (49, 1), (63, 2), (64, 2), (65, 2), (81, 1), (82, 1), (84, 1), (85, 1), (105, 5), (106, 2), (108, 1), (109, 1), (121, 1), (123, 1), (126, 1), (133, 1), (134, 1), (150, 2), (154, 1), (155, 2), (164, 1), (166, 1), (167, 1), (170, 1), (192, 13)]
discards: [  1 189 190 191]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 247 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 591.8298 - loglik: -5.9010e+02 - logprior: -1.7276e+00
Epoch 2/2
39/39 - 26s - loss: 585.3797 - loglik: -5.8471e+02 - logprior: -6.7133e-01
Fitted a model with MAP estimate = -584.2171
expansions: [(31, 2)]
discards: [ 26  27  28  53  54 130 132 234 235 236 237 238 239 240 241 242 243 244
 245 246]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 590.6224 - loglik: -5.8902e+02 - logprior: -1.6053e+00
Epoch 2/2
39/39 - 26s - loss: 587.7652 - loglik: -5.8747e+02 - logprior: -2.9452e-01
Fitted a model with MAP estimate = -586.6443
expansions: [(26, 3), (184, 5), (217, 3), (229, 12)]
discards: [53]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 251 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 588.2610 - loglik: -5.8664e+02 - logprior: -1.6171e+00
Epoch 2/10
39/39 - 26s - loss: 584.8958 - loglik: -5.8446e+02 - logprior: -4.4014e-01
Epoch 3/10
39/39 - 27s - loss: 583.3956 - loglik: -5.8303e+02 - logprior: -3.6279e-01
Epoch 4/10
39/39 - 29s - loss: 583.5894 - loglik: -5.8328e+02 - logprior: -3.1269e-01
Fitted a model with MAP estimate = -582.8749
Time for alignment: 472.2949
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 649.7540 - loglik: -6.4802e+02 - logprior: -1.7346e+00
Epoch 2/10
39/39 - 20s - loss: 600.2748 - loglik: -5.9921e+02 - logprior: -1.0653e+00
Epoch 3/10
39/39 - 21s - loss: 594.3185 - loglik: -5.9319e+02 - logprior: -1.1269e+00
Epoch 4/10
39/39 - 22s - loss: 592.9589 - loglik: -5.9186e+02 - logprior: -1.0951e+00
Epoch 5/10
39/39 - 22s - loss: 592.6761 - loglik: -5.9160e+02 - logprior: -1.0778e+00
Epoch 6/10
39/39 - 22s - loss: 592.3658 - loglik: -5.9131e+02 - logprior: -1.0590e+00
Epoch 7/10
39/39 - 22s - loss: 592.3774 - loglik: -5.9133e+02 - logprior: -1.0521e+00
Fitted a model with MAP estimate = -591.6759
expansions: [(21, 1), (22, 1), (23, 2), (24, 2), (28, 1), (47, 4), (48, 1), (63, 2), (64, 2), (65, 2), (81, 1), (82, 2), (83, 2), (84, 2), (104, 4), (105, 3), (107, 1), (108, 1), (119, 1), (121, 1), (122, 1), (124, 1), (131, 1), (149, 3), (154, 2), (161, 1), (165, 1), (166, 1), (176, 1), (177, 3), (192, 12)]
discards: [  0   1 190 191]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 251 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 593.9803 - loglik: -5.9140e+02 - logprior: -2.5755e+00
Epoch 2/2
39/39 - 33s - loss: 586.6013 - loglik: -5.8576e+02 - logprior: -8.3811e-01
Fitted a model with MAP estimate = -584.6965
expansions: [(0, 2), (30, 1), (59, 1), (220, 2)]
discards: [ 25  26  27  52  53  54  55  99 105 131 132 221 223 224 239 240 241 242
 243 244 245 246 247 248 249 250]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 231 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 589.1930 - loglik: -5.8751e+02 - logprior: -1.6780e+00
Epoch 2/2
39/39 - 28s - loss: 585.8375 - loglik: -5.8554e+02 - logprior: -3.0070e-01
Fitted a model with MAP estimate = -584.7011
expansions: [(52, 3), (53, 1), (183, 4), (184, 1), (231, 11)]
discards: [  0   1 216 230]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 247 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 589.9095 - loglik: -5.8822e+02 - logprior: -1.6928e+00
Epoch 2/10
39/39 - 31s - loss: 585.3178 - loglik: -5.8509e+02 - logprior: -2.3075e-01
Epoch 3/10
39/39 - 32s - loss: 584.4540 - loglik: -5.8429e+02 - logprior: -1.6691e-01
Epoch 4/10
39/39 - 32s - loss: 584.4630 - loglik: -5.8436e+02 - logprior: -1.0499e-01
Fitted a model with MAP estimate = -583.8973
Time for alignment: 524.3794
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 649.5897 - loglik: -6.4787e+02 - logprior: -1.7234e+00
Epoch 2/10
39/39 - 22s - loss: 600.2192 - loglik: -5.9913e+02 - logprior: -1.0858e+00
Epoch 3/10
39/39 - 20s - loss: 594.6113 - loglik: -5.9351e+02 - logprior: -1.1033e+00
Epoch 4/10
39/39 - 19s - loss: 593.5782 - loglik: -5.9251e+02 - logprior: -1.0678e+00
Epoch 5/10
39/39 - 19s - loss: 593.1287 - loglik: -5.9207e+02 - logprior: -1.0544e+00
Epoch 6/10
39/39 - 18s - loss: 592.8597 - loglik: -5.9181e+02 - logprior: -1.0458e+00
Epoch 7/10
39/39 - 19s - loss: 592.9940 - loglik: -5.9195e+02 - logprior: -1.0428e+00
Fitted a model with MAP estimate = -592.2900
expansions: [(17, 1), (22, 1), (23, 5), (27, 1), (46, 4), (47, 1), (48, 1), (63, 3), (64, 1), (65, 1), (81, 1), (82, 2), (83, 3), (84, 2), (105, 3), (106, 3), (107, 2), (108, 1), (122, 1), (123, 1), (124, 1), (125, 1), (131, 1), (149, 2), (154, 2), (161, 1), (167, 1), (168, 1), (169, 1), (175, 1), (192, 12)]
discards: [  0   1 189 190 191]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 249 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 33s - loss: 594.5715 - loglik: -5.9203e+02 - logprior: -2.5376e+00
Epoch 2/2
39/39 - 29s - loss: 586.6676 - loglik: -5.8588e+02 - logprior: -7.9211e-01
Fitted a model with MAP estimate = -584.8161
expansions: [(0, 3), (30, 3), (190, 1), (193, 1)]
discards: [ 25  26  27  53  54  55  77  78 101 105 107 133 135 238 239 240 241 242
 243 244 245 246 247 248]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 233 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 589.4031 - loglik: -5.8771e+02 - logprior: -1.6959e+00
Epoch 2/2
39/39 - 23s - loss: 585.7346 - loglik: -5.8540e+02 - logprior: -3.3275e-01
Fitted a model with MAP estimate = -584.5948
expansions: [(233, 11)]
discards: [  1   2  55 218 232]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 239 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 30s - loss: 588.9950 - loglik: -5.8748e+02 - logprior: -1.5181e+00
Epoch 2/10
39/39 - 28s - loss: 585.4575 - loglik: -5.8500e+02 - logprior: -4.5461e-01
Epoch 3/10
39/39 - 27s - loss: 584.5306 - loglik: -5.8416e+02 - logprior: -3.6716e-01
Epoch 4/10
39/39 - 25s - loss: 584.1418 - loglik: -5.8381e+02 - logprior: -3.3123e-01
Epoch 5/10
39/39 - 25s - loss: 584.0964 - loglik: -5.8384e+02 - logprior: -2.5974e-01
Epoch 6/10
39/39 - 26s - loss: 584.0604 - loglik: -5.8385e+02 - logprior: -2.1145e-01
Epoch 7/10
39/39 - 27s - loss: 583.4509 - loglik: -5.8329e+02 - logprior: -1.6013e-01
Epoch 8/10
39/39 - 29s - loss: 584.0923 - loglik: -5.8399e+02 - logprior: -1.0377e-01
Fitted a model with MAP estimate = -583.3903
Time for alignment: 573.3615
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 649.3026 - loglik: -6.4757e+02 - logprior: -1.7356e+00
Epoch 2/10
39/39 - 21s - loss: 599.2393 - loglik: -5.9808e+02 - logprior: -1.1631e+00
Epoch 3/10
39/39 - 21s - loss: 593.5677 - loglik: -5.9237e+02 - logprior: -1.1983e+00
Epoch 4/10
39/39 - 22s - loss: 592.0986 - loglik: -5.9095e+02 - logprior: -1.1509e+00
Epoch 5/10
39/39 - 22s - loss: 592.3529 - loglik: -5.9121e+02 - logprior: -1.1382e+00
Fitted a model with MAP estimate = -591.4525
expansions: [(21, 1), (22, 1), (23, 5), (27, 1), (47, 4), (48, 1), (49, 1), (50, 1), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (83, 1), (84, 1), (97, 1), (104, 1), (105, 3), (106, 2), (108, 1), (109, 1), (123, 1), (124, 1), (126, 1), (133, 1), (134, 1), (150, 2), (155, 2), (162, 1), (166, 1), (167, 1), (170, 1), (192, 13)]
discards: [  0 189 190 191]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 247 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 593.2468 - loglik: -5.9059e+02 - logprior: -2.6539e+00
Epoch 2/2
39/39 - 35s - loss: 585.9516 - loglik: -5.8504e+02 - logprior: -9.1377e-01
Fitted a model with MAP estimate = -583.9182
expansions: [(31, 3), (181, 1), (189, 1), (190, 3), (221, 2), (222, 2), (247, 2)]
discards: [ 26  27  28  53  54  55  57 129 133 235 236 237 238 239 240 241 242 243
 244 245 246]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 588.5241 - loglik: -5.8692e+02 - logprior: -1.6030e+00
Epoch 2/2
39/39 - 30s - loss: 585.2908 - loglik: -5.8494e+02 - logprior: -3.5089e-01
Fitted a model with MAP estimate = -584.2351
expansions: [(53, 1), (240, 2)]
discards: [226 238 239]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 587.5156 - loglik: -5.8599e+02 - logprior: -1.5208e+00
Epoch 2/10
39/39 - 30s - loss: 584.5994 - loglik: -5.8433e+02 - logprior: -2.7274e-01
Epoch 3/10
39/39 - 31s - loss: 583.6823 - loglik: -5.8350e+02 - logprior: -1.8517e-01
Epoch 4/10
39/39 - 31s - loss: 583.2649 - loglik: -5.8314e+02 - logprior: -1.2108e-01
Epoch 5/10
39/39 - 32s - loss: 583.4622 - loglik: -5.8341e+02 - logprior: -5.1417e-02
Fitted a model with MAP estimate = -582.7947
Time for alignment: 521.6543
Computed alignments with likelihoods: ['-583.9113', '-582.8749', '-583.8973', '-583.3903', '-582.7947']
Best model has likelihood: -582.7947  (prior= 0.0278 )
time for generating output: 0.2502
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.45802583025830257
Training of 5 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c0acd85b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4dd42224c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c0ab78fd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4daf50c9d0>
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 156.6535 - loglik: -1.5344e+02 - logprior: -3.2088e+00
Epoch 2/10
19/19 - 1s - loss: 127.7832 - loglik: -1.2627e+02 - logprior: -1.5102e+00
Epoch 3/10
19/19 - 1s - loss: 111.6513 - loglik: -1.1000e+02 - logprior: -1.6549e+00
Epoch 4/10
19/19 - 1s - loss: 108.4160 - loglik: -1.0678e+02 - logprior: -1.6324e+00
Epoch 5/10
19/19 - 1s - loss: 107.6796 - loglik: -1.0609e+02 - logprior: -1.5863e+00
Epoch 6/10
19/19 - 1s - loss: 106.9376 - loglik: -1.0539e+02 - logprior: -1.5508e+00
Epoch 7/10
19/19 - 1s - loss: 106.6708 - loglik: -1.0513e+02 - logprior: -1.5419e+00
Epoch 8/10
19/19 - 1s - loss: 106.6424 - loglik: -1.0512e+02 - logprior: -1.5267e+00
Epoch 9/10
19/19 - 1s - loss: 106.2549 - loglik: -1.0473e+02 - logprior: -1.5244e+00
Epoch 10/10
19/19 - 1s - loss: 106.7306 - loglik: -1.0521e+02 - logprior: -1.5171e+00
Fitted a model with MAP estimate = -106.2032
expansions: [(5, 1), (7, 1), (10, 1), (13, 1), (16, 1), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.4315 - loglik: -1.0614e+02 - logprior: -3.2956e+00
Epoch 2/2
19/19 - 1s - loss: 101.3264 - loglik: -9.9939e+01 - logprior: -1.3877e+00
Fitted a model with MAP estimate = -100.3016
expansions: []
discards: [37 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 103.5560 - loglik: -1.0037e+02 - logprior: -3.1878e+00
Epoch 2/2
19/19 - 1s - loss: 100.4396 - loglik: -9.9149e+01 - logprior: -1.2909e+00
Fitted a model with MAP estimate = -99.8744
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 103.0734 - loglik: -9.9924e+01 - logprior: -3.1489e+00
Epoch 2/10
19/19 - 1s - loss: 100.3782 - loglik: -9.9103e+01 - logprior: -1.2747e+00
Epoch 3/10
19/19 - 1s - loss: 99.6853 - loglik: -9.8539e+01 - logprior: -1.1462e+00
Epoch 4/10
19/19 - 1s - loss: 99.3487 - loglik: -9.8244e+01 - logprior: -1.1052e+00
Epoch 5/10
19/19 - 1s - loss: 98.8801 - loglik: -9.7799e+01 - logprior: -1.0807e+00
Epoch 6/10
19/19 - 1s - loss: 98.7076 - loglik: -9.7642e+01 - logprior: -1.0658e+00
Epoch 7/10
19/19 - 1s - loss: 98.2025 - loglik: -9.7151e+01 - logprior: -1.0512e+00
Epoch 8/10
19/19 - 1s - loss: 98.2789 - loglik: -9.7234e+01 - logprior: -1.0452e+00
Fitted a model with MAP estimate = -98.0855
Time for alignment: 43.9770
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 156.4646 - loglik: -1.5326e+02 - logprior: -3.2093e+00
Epoch 2/10
19/19 - 1s - loss: 128.6711 - loglik: -1.2720e+02 - logprior: -1.4757e+00
Epoch 3/10
19/19 - 1s - loss: 112.3994 - loglik: -1.1080e+02 - logprior: -1.6032e+00
Epoch 4/10
19/19 - 1s - loss: 108.6835 - loglik: -1.0710e+02 - logprior: -1.5828e+00
Epoch 5/10
19/19 - 1s - loss: 107.6242 - loglik: -1.0609e+02 - logprior: -1.5338e+00
Epoch 6/10
19/19 - 1s - loss: 107.3218 - loglik: -1.0581e+02 - logprior: -1.5154e+00
Epoch 7/10
19/19 - 1s - loss: 107.0982 - loglik: -1.0561e+02 - logprior: -1.4876e+00
Epoch 8/10
19/19 - 1s - loss: 106.7457 - loglik: -1.0527e+02 - logprior: -1.4783e+00
Epoch 9/10
19/19 - 1s - loss: 106.5273 - loglik: -1.0505e+02 - logprior: -1.4747e+00
Epoch 10/10
19/19 - 1s - loss: 106.6573 - loglik: -1.0519e+02 - logprior: -1.4649e+00
Fitted a model with MAP estimate = -106.4059
expansions: [(5, 1), (7, 1), (10, 1), (14, 1), (16, 1), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.2240 - loglik: -1.0594e+02 - logprior: -3.2877e+00
Epoch 2/2
19/19 - 1s - loss: 101.2715 - loglik: -9.9881e+01 - logprior: -1.3909e+00
Fitted a model with MAP estimate = -100.3327
expansions: []
discards: [37 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 103.5963 - loglik: -1.0041e+02 - logprior: -3.1833e+00
Epoch 2/2
19/19 - 1s - loss: 100.4670 - loglik: -9.9183e+01 - logprior: -1.2843e+00
Fitted a model with MAP estimate = -99.8853
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 102.9845 - loglik: -9.9832e+01 - logprior: -3.1520e+00
Epoch 2/10
19/19 - 1s - loss: 100.5078 - loglik: -9.9243e+01 - logprior: -1.2646e+00
Epoch 3/10
19/19 - 1s - loss: 99.7136 - loglik: -9.8564e+01 - logprior: -1.1498e+00
Epoch 4/10
19/19 - 1s - loss: 99.2374 - loglik: -9.8133e+01 - logprior: -1.1040e+00
Epoch 5/10
19/19 - 1s - loss: 98.9798 - loglik: -9.7909e+01 - logprior: -1.0707e+00
Epoch 6/10
19/19 - 1s - loss: 98.6927 - loglik: -9.7632e+01 - logprior: -1.0611e+00
Epoch 7/10
19/19 - 1s - loss: 98.4150 - loglik: -9.7361e+01 - logprior: -1.0538e+00
Epoch 8/10
19/19 - 1s - loss: 98.1524 - loglik: -9.7118e+01 - logprior: -1.0345e+00
Epoch 9/10
19/19 - 1s - loss: 97.8624 - loglik: -9.6832e+01 - logprior: -1.0306e+00
Epoch 10/10
19/19 - 1s - loss: 98.2788 - loglik: -9.7258e+01 - logprior: -1.0206e+00
Fitted a model with MAP estimate = -97.8713
Time for alignment: 46.2307
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 156.5828 - loglik: -1.5337e+02 - logprior: -3.2080e+00
Epoch 2/10
19/19 - 1s - loss: 127.4506 - loglik: -1.2593e+02 - logprior: -1.5178e+00
Epoch 3/10
19/19 - 1s - loss: 111.5762 - loglik: -1.0993e+02 - logprior: -1.6434e+00
Epoch 4/10
19/19 - 1s - loss: 108.4445 - loglik: -1.0681e+02 - logprior: -1.6317e+00
Epoch 5/10
19/19 - 1s - loss: 107.6659 - loglik: -1.0609e+02 - logprior: -1.5776e+00
Epoch 6/10
19/19 - 1s - loss: 107.0640 - loglik: -1.0550e+02 - logprior: -1.5593e+00
Epoch 7/10
19/19 - 1s - loss: 106.8077 - loglik: -1.0527e+02 - logprior: -1.5344e+00
Epoch 8/10
19/19 - 1s - loss: 106.6841 - loglik: -1.0515e+02 - logprior: -1.5306e+00
Epoch 9/10
19/19 - 1s - loss: 106.5067 - loglik: -1.0499e+02 - logprior: -1.5215e+00
Epoch 10/10
19/19 - 1s - loss: 106.3357 - loglik: -1.0481e+02 - logprior: -1.5237e+00
Fitted a model with MAP estimate = -106.2092
expansions: [(5, 1), (7, 1), (10, 1), (13, 1), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 110.0099 - loglik: -1.0669e+02 - logprior: -3.3194e+00
Epoch 2/2
19/19 - 1s - loss: 101.3588 - loglik: -9.9954e+01 - logprior: -1.4047e+00
Fitted a model with MAP estimate = -100.3512
expansions: []
discards: [21 38 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.8235 - loglik: -1.0064e+02 - logprior: -3.1839e+00
Epoch 2/2
19/19 - 1s - loss: 100.5453 - loglik: -9.9265e+01 - logprior: -1.2801e+00
Fitted a model with MAP estimate = -99.9052
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 103.0268 - loglik: -9.9879e+01 - logprior: -3.1482e+00
Epoch 2/10
19/19 - 1s - loss: 100.4793 - loglik: -9.9208e+01 - logprior: -1.2709e+00
Epoch 3/10
19/19 - 1s - loss: 99.7293 - loglik: -9.8587e+01 - logprior: -1.1424e+00
Epoch 4/10
19/19 - 1s - loss: 99.2881 - loglik: -9.8182e+01 - logprior: -1.1056e+00
Epoch 5/10
19/19 - 1s - loss: 98.9450 - loglik: -9.7867e+01 - logprior: -1.0776e+00
Epoch 6/10
19/19 - 1s - loss: 98.6059 - loglik: -9.7536e+01 - logprior: -1.0697e+00
Epoch 7/10
19/19 - 1s - loss: 98.2885 - loglik: -9.7241e+01 - logprior: -1.0477e+00
Epoch 8/10
19/19 - 1s - loss: 98.5048 - loglik: -9.7464e+01 - logprior: -1.0404e+00
Fitted a model with MAP estimate = -98.0878
Time for alignment: 44.0191
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 156.5930 - loglik: -1.5338e+02 - logprior: -3.2091e+00
Epoch 2/10
19/19 - 1s - loss: 128.2834 - loglik: -1.2678e+02 - logprior: -1.5044e+00
Epoch 3/10
19/19 - 1s - loss: 113.1664 - loglik: -1.1157e+02 - logprior: -1.5989e+00
Epoch 4/10
19/19 - 1s - loss: 109.6941 - loglik: -1.0810e+02 - logprior: -1.5891e+00
Epoch 5/10
19/19 - 1s - loss: 108.9115 - loglik: -1.0737e+02 - logprior: -1.5437e+00
Epoch 6/10
19/19 - 1s - loss: 108.5284 - loglik: -1.0701e+02 - logprior: -1.5187e+00
Epoch 7/10
19/19 - 1s - loss: 108.0322 - loglik: -1.0652e+02 - logprior: -1.5086e+00
Epoch 8/10
19/19 - 1s - loss: 107.9656 - loglik: -1.0647e+02 - logprior: -1.4914e+00
Epoch 9/10
19/19 - 1s - loss: 107.8140 - loglik: -1.0633e+02 - logprior: -1.4877e+00
Epoch 10/10
19/19 - 1s - loss: 107.4446 - loglik: -1.0596e+02 - logprior: -1.4866e+00
Fitted a model with MAP estimate = -107.5305
expansions: [(5, 1), (7, 1), (14, 2), (16, 1), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.4020 - loglik: -1.0610e+02 - logprior: -3.2972e+00
Epoch 2/2
19/19 - 1s - loss: 101.2880 - loglik: -9.9902e+01 - logprior: -1.3862e+00
Fitted a model with MAP estimate = -100.2964
expansions: []
discards: [37 40]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.5335 - loglik: -1.0035e+02 - logprior: -3.1807e+00
Epoch 2/2
19/19 - 1s - loss: 100.4524 - loglik: -9.9170e+01 - logprior: -1.2820e+00
Fitted a model with MAP estimate = -99.8835
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 103.0855 - loglik: -9.9934e+01 - logprior: -3.1515e+00
Epoch 2/10
19/19 - 1s - loss: 100.3707 - loglik: -9.9100e+01 - logprior: -1.2706e+00
Epoch 3/10
19/19 - 1s - loss: 99.6849 - loglik: -9.8537e+01 - logprior: -1.1478e+00
Epoch 4/10
19/19 - 1s - loss: 99.2328 - loglik: -9.8129e+01 - logprior: -1.1039e+00
Epoch 5/10
19/19 - 1s - loss: 98.9429 - loglik: -9.7865e+01 - logprior: -1.0775e+00
Epoch 6/10
19/19 - 1s - loss: 98.6590 - loglik: -9.7595e+01 - logprior: -1.0635e+00
Epoch 7/10
19/19 - 1s - loss: 98.3218 - loglik: -9.7270e+01 - logprior: -1.0517e+00
Epoch 8/10
19/19 - 1s - loss: 98.2517 - loglik: -9.7215e+01 - logprior: -1.0370e+00
Epoch 9/10
19/19 - 1s - loss: 98.2948 - loglik: -9.7267e+01 - logprior: -1.0273e+00
Fitted a model with MAP estimate = -97.9588
Time for alignment: 43.7728
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 156.5862 - loglik: -1.5338e+02 - logprior: -3.2068e+00
Epoch 2/10
19/19 - 1s - loss: 128.6148 - loglik: -1.2715e+02 - logprior: -1.4619e+00
Epoch 3/10
19/19 - 1s - loss: 113.1478 - loglik: -1.1154e+02 - logprior: -1.6063e+00
Epoch 4/10
19/19 - 1s - loss: 109.6893 - loglik: -1.0810e+02 - logprior: -1.5858e+00
Epoch 5/10
19/19 - 1s - loss: 108.6499 - loglik: -1.0711e+02 - logprior: -1.5433e+00
Epoch 6/10
19/19 - 1s - loss: 108.3608 - loglik: -1.0683e+02 - logprior: -1.5266e+00
Epoch 7/10
19/19 - 1s - loss: 107.9278 - loglik: -1.0642e+02 - logprior: -1.5069e+00
Epoch 8/10
19/19 - 1s - loss: 107.6437 - loglik: -1.0614e+02 - logprior: -1.4994e+00
Epoch 9/10
19/19 - 1s - loss: 107.8464 - loglik: -1.0636e+02 - logprior: -1.4900e+00
Fitted a model with MAP estimate = -107.4895
expansions: [(5, 1), (7, 1), (10, 2), (16, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.8885 - loglik: -1.0657e+02 - logprior: -3.3159e+00
Epoch 2/2
19/19 - 1s - loss: 101.5400 - loglik: -1.0013e+02 - logprior: -1.4120e+00
Fitted a model with MAP estimate = -100.3493
expansions: []
discards: [20 38 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.5891 - loglik: -1.0041e+02 - logprior: -3.1833e+00
Epoch 2/2
19/19 - 1s - loss: 100.3915 - loglik: -9.9103e+01 - logprior: -1.2886e+00
Fitted a model with MAP estimate = -99.8915
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 103.0705 - loglik: -9.9918e+01 - logprior: -3.1521e+00
Epoch 2/10
19/19 - 1s - loss: 100.3571 - loglik: -9.9085e+01 - logprior: -1.2722e+00
Epoch 3/10
19/19 - 1s - loss: 99.8950 - loglik: -9.8748e+01 - logprior: -1.1465e+00
Epoch 4/10
19/19 - 1s - loss: 99.1712 - loglik: -9.8068e+01 - logprior: -1.1032e+00
Epoch 5/10
19/19 - 1s - loss: 98.9610 - loglik: -9.7879e+01 - logprior: -1.0823e+00
Epoch 6/10
19/19 - 1s - loss: 98.5804 - loglik: -9.7514e+01 - logprior: -1.0669e+00
Epoch 7/10
19/19 - 1s - loss: 98.4733 - loglik: -9.7417e+01 - logprior: -1.0559e+00
Epoch 8/10
19/19 - 1s - loss: 98.2085 - loglik: -9.7172e+01 - logprior: -1.0362e+00
Epoch 9/10
19/19 - 1s - loss: 98.1180 - loglik: -9.7081e+01 - logprior: -1.0373e+00
Epoch 10/10
19/19 - 1s - loss: 97.9425 - loglik: -9.6924e+01 - logprior: -1.0183e+00
Fitted a model with MAP estimate = -97.8840
Time for alignment: 44.7676
Computed alignments with likelihoods: ['-98.0855', '-97.8713', '-98.0878', '-97.9588', '-97.8840']
Best model has likelihood: -97.8713  (prior= -1.0050 )
time for generating output: 0.0933
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9826388888888888
Training of 5 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d852e4af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4a4c517130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c0b2c1f70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4db7bb1b80>
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 367.1933 - loglik: -3.6421e+02 - logprior: -2.9793e+00
Epoch 2/10
19/19 - 3s - loss: 339.8850 - loglik: -3.3890e+02 - logprior: -9.8764e-01
Epoch 3/10
19/19 - 3s - loss: 327.3839 - loglik: -3.2614e+02 - logprior: -1.2483e+00
Epoch 4/10
19/19 - 3s - loss: 323.1857 - loglik: -3.2203e+02 - logprior: -1.1534e+00
Epoch 5/10
19/19 - 3s - loss: 320.9463 - loglik: -3.1980e+02 - logprior: -1.1487e+00
Epoch 6/10
19/19 - 3s - loss: 319.4668 - loglik: -3.1830e+02 - logprior: -1.1642e+00
Epoch 7/10
19/19 - 3s - loss: 318.4300 - loglik: -3.1728e+02 - logprior: -1.1492e+00
Epoch 8/10
19/19 - 3s - loss: 317.9708 - loglik: -3.1683e+02 - logprior: -1.1401e+00
Epoch 9/10
19/19 - 3s - loss: 317.9917 - loglik: -3.1685e+02 - logprior: -1.1410e+00
Fitted a model with MAP estimate = -317.4366
expansions: [(18, 1), (20, 1), (21, 5), (24, 3), (26, 1), (33, 1), (52, 2), (55, 4), (75, 4), (76, 1), (81, 2), (82, 1), (83, 1), (86, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 325.6361 - loglik: -3.2188e+02 - logprior: -3.7584e+00
Epoch 2/2
19/19 - 4s - loss: 317.5716 - loglik: -3.1573e+02 - logprior: -1.8387e+00
Fitted a model with MAP estimate = -315.5509
expansions: [(0, 2), (32, 1), (34, 1), (96, 2)]
discards: [  0  64 104]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 318.9175 - loglik: -3.1610e+02 - logprior: -2.8164e+00
Epoch 2/2
19/19 - 4s - loss: 314.5158 - loglik: -3.1351e+02 - logprior: -1.0055e+00
Fitted a model with MAP estimate = -313.0684
expansions: []
discards: [ 0 26 27 95 96 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 321.1899 - loglik: -3.1764e+02 - logprior: -3.5523e+00
Epoch 2/10
19/19 - 4s - loss: 315.9161 - loglik: -3.1493e+02 - logprior: -9.8272e-01
Epoch 3/10
19/19 - 4s - loss: 314.4015 - loglik: -3.1369e+02 - logprior: -7.0942e-01
Epoch 4/10
19/19 - 4s - loss: 313.1015 - loglik: -3.1240e+02 - logprior: -6.9884e-01
Epoch 5/10
19/19 - 4s - loss: 312.0758 - loglik: -3.1138e+02 - logprior: -6.9928e-01
Epoch 6/10
19/19 - 5s - loss: 312.4508 - loglik: -3.1178e+02 - logprior: -6.7028e-01
Fitted a model with MAP estimate = -311.8465
Time for alignment: 107.1164
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 367.4164 - loglik: -3.6444e+02 - logprior: -2.9808e+00
Epoch 2/10
19/19 - 3s - loss: 340.7289 - loglik: -3.3975e+02 - logprior: -9.8128e-01
Epoch 3/10
19/19 - 3s - loss: 326.9473 - loglik: -3.2572e+02 - logprior: -1.2234e+00
Epoch 4/10
19/19 - 4s - loss: 322.3309 - loglik: -3.2118e+02 - logprior: -1.1522e+00
Epoch 5/10
19/19 - 4s - loss: 320.4491 - loglik: -3.1929e+02 - logprior: -1.1577e+00
Epoch 6/10
19/19 - 4s - loss: 319.1538 - loglik: -3.1801e+02 - logprior: -1.1398e+00
Epoch 7/10
19/19 - 4s - loss: 319.0592 - loglik: -3.1794e+02 - logprior: -1.1171e+00
Epoch 8/10
19/19 - 3s - loss: 318.3228 - loglik: -3.1722e+02 - logprior: -1.1060e+00
Epoch 9/10
19/19 - 4s - loss: 318.3367 - loglik: -3.1724e+02 - logprior: -1.0993e+00
Fitted a model with MAP estimate = -318.0169
expansions: [(19, 1), (20, 6), (23, 1), (26, 1), (34, 1), (52, 2), (53, 1), (55, 3), (75, 4), (80, 1), (81, 2), (82, 4), (83, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 325.9806 - loglik: -3.2223e+02 - logprior: -3.7531e+00
Epoch 2/2
19/19 - 4s - loss: 317.4889 - loglik: -3.1564e+02 - logprior: -1.8483e+00
Fitted a model with MAP estimate = -315.5644
expansions: [(0, 2), (24, 1), (25, 2), (68, 1)]
discards: [  0  63  64  91  92 104 105 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 128 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 320.2783 - loglik: -3.1749e+02 - logprior: -2.7883e+00
Epoch 2/2
19/19 - 4s - loss: 316.0082 - loglik: -3.1502e+02 - logprior: -9.8643e-01
Fitted a model with MAP estimate = -314.3291
expansions: [(95, 3)]
discards: [ 0 28]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 320.7867 - loglik: -3.1718e+02 - logprior: -3.6058e+00
Epoch 2/10
19/19 - 4s - loss: 315.6376 - loglik: -3.1458e+02 - logprior: -1.0568e+00
Epoch 3/10
19/19 - 4s - loss: 313.5978 - loglik: -3.1287e+02 - logprior: -7.3245e-01
Epoch 4/10
19/19 - 4s - loss: 312.8061 - loglik: -3.1213e+02 - logprior: -6.8025e-01
Epoch 5/10
19/19 - 4s - loss: 311.8513 - loglik: -3.1119e+02 - logprior: -6.5664e-01
Epoch 6/10
19/19 - 4s - loss: 311.5091 - loglik: -3.1088e+02 - logprior: -6.2704e-01
Epoch 7/10
19/19 - 5s - loss: 311.4324 - loglik: -3.1083e+02 - logprior: -6.0091e-01
Epoch 8/10
19/19 - 4s - loss: 311.5577 - loglik: -3.1098e+02 - logprior: -5.7916e-01
Fitted a model with MAP estimate = -311.1603
Time for alignment: 122.4759
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 367.3994 - loglik: -3.6442e+02 - logprior: -2.9819e+00
Epoch 2/10
19/19 - 3s - loss: 339.6855 - loglik: -3.3870e+02 - logprior: -9.8065e-01
Epoch 3/10
19/19 - 3s - loss: 326.6472 - loglik: -3.2540e+02 - logprior: -1.2431e+00
Epoch 4/10
19/19 - 3s - loss: 322.4889 - loglik: -3.2133e+02 - logprior: -1.1569e+00
Epoch 5/10
19/19 - 3s - loss: 321.1057 - loglik: -3.1995e+02 - logprior: -1.1540e+00
Epoch 6/10
19/19 - 3s - loss: 319.7651 - loglik: -3.1861e+02 - logprior: -1.1516e+00
Epoch 7/10
19/19 - 3s - loss: 318.7805 - loglik: -3.1764e+02 - logprior: -1.1376e+00
Epoch 8/10
19/19 - 4s - loss: 318.2448 - loglik: -3.1712e+02 - logprior: -1.1293e+00
Epoch 9/10
19/19 - 4s - loss: 318.5416 - loglik: -3.1742e+02 - logprior: -1.1219e+00
Fitted a model with MAP estimate = -317.8937
expansions: [(19, 1), (20, 2), (21, 5), (24, 3), (25, 1), (26, 1), (33, 1), (52, 2), (55, 4), (76, 3), (80, 2), (82, 1), (83, 1), (84, 2), (85, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 326.0904 - loglik: -3.2234e+02 - logprior: -3.7518e+00
Epoch 2/2
19/19 - 5s - loss: 317.8403 - loglik: -3.1601e+02 - logprior: -1.8304e+00
Fitted a model with MAP estimate = -315.6901
expansions: [(0, 2), (32, 1), (98, 2)]
discards: [  0  66 102 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 319.1852 - loglik: -3.1638e+02 - logprior: -2.8079e+00
Epoch 2/2
19/19 - 5s - loss: 314.7113 - loglik: -3.1372e+02 - logprior: -9.9423e-01
Fitted a model with MAP estimate = -313.3103
expansions: []
discards: [  0  98  99 100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 320.6338 - loglik: -3.1707e+02 - logprior: -3.5618e+00
Epoch 2/10
19/19 - 4s - loss: 315.5805 - loglik: -3.1460e+02 - logprior: -9.8069e-01
Epoch 3/10
19/19 - 4s - loss: 314.0103 - loglik: -3.1330e+02 - logprior: -7.0719e-01
Epoch 4/10
19/19 - 5s - loss: 312.1429 - loglik: -3.1145e+02 - logprior: -6.9132e-01
Epoch 5/10
19/19 - 5s - loss: 312.2121 - loglik: -3.1154e+02 - logprior: -6.6971e-01
Fitted a model with MAP estimate = -311.7848
Time for alignment: 110.2025
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 367.4327 - loglik: -3.6445e+02 - logprior: -2.9821e+00
Epoch 2/10
19/19 - 4s - loss: 340.5927 - loglik: -3.3962e+02 - logprior: -9.7309e-01
Epoch 3/10
19/19 - 4s - loss: 328.2596 - loglik: -3.2705e+02 - logprior: -1.2098e+00
Epoch 4/10
19/19 - 4s - loss: 322.9820 - loglik: -3.2185e+02 - logprior: -1.1363e+00
Epoch 5/10
19/19 - 4s - loss: 320.7552 - loglik: -3.1962e+02 - logprior: -1.1399e+00
Epoch 6/10
19/19 - 4s - loss: 319.4841 - loglik: -3.1835e+02 - logprior: -1.1371e+00
Epoch 7/10
19/19 - 4s - loss: 318.6474 - loglik: -3.1753e+02 - logprior: -1.1156e+00
Epoch 8/10
19/19 - 4s - loss: 317.9538 - loglik: -3.1684e+02 - logprior: -1.1090e+00
Epoch 9/10
19/19 - 4s - loss: 317.6086 - loglik: -3.1650e+02 - logprior: -1.1086e+00
Epoch 10/10
19/19 - 4s - loss: 317.9924 - loglik: -3.1689e+02 - logprior: -1.1073e+00
Fitted a model with MAP estimate = -317.4454
expansions: [(19, 1), (20, 2), (21, 4), (24, 4), (26, 1), (33, 1), (36, 2), (55, 4), (76, 3), (81, 2), (82, 4), (83, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 326.4318 - loglik: -3.2266e+02 - logprior: -3.7710e+00
Epoch 2/2
19/19 - 5s - loss: 317.7962 - loglik: -3.1593e+02 - logprior: -1.8687e+00
Fitted a model with MAP estimate = -315.8861
expansions: [(0, 2), (35, 1), (93, 1)]
discards: [  0  48 105 106 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 319.5954 - loglik: -3.1678e+02 - logprior: -2.8147e+00
Epoch 2/2
19/19 - 5s - loss: 315.0783 - loglik: -3.1408e+02 - logprior: -1.0015e+00
Fitted a model with MAP estimate = -313.6968
expansions: []
discards: [ 0 26 27]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 127 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 321.0100 - loglik: -3.1746e+02 - logprior: -3.5528e+00
Epoch 2/10
19/19 - 5s - loss: 316.3049 - loglik: -3.1533e+02 - logprior: -9.7535e-01
Epoch 3/10
19/19 - 5s - loss: 314.4135 - loglik: -3.1370e+02 - logprior: -7.1595e-01
Epoch 4/10
19/19 - 5s - loss: 312.6863 - loglik: -3.1199e+02 - logprior: -6.9977e-01
Epoch 5/10
19/19 - 6s - loss: 312.9044 - loglik: -3.1224e+02 - logprior: -6.6560e-01
Fitted a model with MAP estimate = -312.1629
Time for alignment: 123.3775
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 367.4400 - loglik: -3.6446e+02 - logprior: -2.9840e+00
Epoch 2/10
19/19 - 4s - loss: 339.3598 - loglik: -3.3839e+02 - logprior: -9.7427e-01
Epoch 3/10
19/19 - 4s - loss: 326.6098 - loglik: -3.2540e+02 - logprior: -1.2054e+00
Epoch 4/10
19/19 - 4s - loss: 322.9685 - loglik: -3.2185e+02 - logprior: -1.1233e+00
Epoch 5/10
19/19 - 4s - loss: 320.6510 - loglik: -3.1953e+02 - logprior: -1.1226e+00
Epoch 6/10
19/19 - 4s - loss: 319.2581 - loglik: -3.1813e+02 - logprior: -1.1293e+00
Epoch 7/10
19/19 - 4s - loss: 319.2798 - loglik: -3.1816e+02 - logprior: -1.1153e+00
Fitted a model with MAP estimate = -318.2791
expansions: [(19, 1), (20, 2), (21, 4), (24, 3), (25, 1), (26, 1), (33, 1), (52, 1), (53, 1), (55, 3), (76, 3), (80, 1), (81, 2), (82, 4), (83, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 326.0654 - loglik: -3.2229e+02 - logprior: -3.7791e+00
Epoch 2/2
19/19 - 6s - loss: 317.8547 - loglik: -3.1601e+02 - logprior: -1.8419e+00
Fitted a model with MAP estimate = -315.7812
expansions: [(0, 2), (27, 1), (70, 1)]
discards: [  0  65 105 106 107]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 319.7820 - loglik: -3.1694e+02 - logprior: -2.8383e+00
Epoch 2/2
19/19 - 5s - loss: 315.1628 - loglik: -3.1414e+02 - logprior: -1.0197e+00
Fitted a model with MAP estimate = -314.0234
expansions: [(33, 1), (101, 2)]
discards: [ 0 26 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 321.3129 - loglik: -3.1768e+02 - logprior: -3.6316e+00
Epoch 2/10
19/19 - 5s - loss: 316.0352 - loglik: -3.1494e+02 - logprior: -1.0954e+00
Epoch 3/10
19/19 - 5s - loss: 314.0204 - loglik: -3.1329e+02 - logprior: -7.3519e-01
Epoch 4/10
19/19 - 5s - loss: 313.0626 - loglik: -3.1236e+02 - logprior: -7.0668e-01
Epoch 5/10
19/19 - 5s - loss: 312.4850 - loglik: -3.1182e+02 - logprior: -6.6797e-01
Epoch 6/10
19/19 - 6s - loss: 312.1473 - loglik: -3.1151e+02 - logprior: -6.3889e-01
Epoch 7/10
19/19 - 5s - loss: 311.6694 - loglik: -3.1107e+02 - logprior: -6.0422e-01
Epoch 8/10
19/19 - 6s - loss: 312.1627 - loglik: -3.1158e+02 - logprior: -5.8335e-01
Fitted a model with MAP estimate = -311.6269
Time for alignment: 127.4327
Computed alignments with likelihoods: ['-311.8465', '-311.1603', '-311.7848', '-312.1629', '-311.6269']
Best model has likelihood: -311.1603  (prior= -0.5609 )
time for generating output: 0.1295
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.7864838393731636
Training of 5 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bde901310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4be02370a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bddd9ddc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d1b96d0d0>
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 928.5278 - loglik: -9.2711e+02 - logprior: -1.4135e+00
Epoch 2/10
39/39 - 44s - loss: 834.6988 - loglik: -8.3340e+02 - logprior: -1.2945e+00
Epoch 3/10
39/39 - 45s - loss: 822.9824 - loglik: -8.2169e+02 - logprior: -1.2922e+00
Epoch 4/10
39/39 - 47s - loss: 819.9261 - loglik: -8.1867e+02 - logprior: -1.2544e+00
Epoch 5/10
39/39 - 48s - loss: 818.2528 - loglik: -8.1700e+02 - logprior: -1.2566e+00
Epoch 6/10
39/39 - 47s - loss: 816.8315 - loglik: -8.1554e+02 - logprior: -1.2942e+00
Epoch 7/10
39/39 - 43s - loss: 816.3241 - loglik: -8.1501e+02 - logprior: -1.3152e+00
Epoch 8/10
39/39 - 41s - loss: 815.7398 - loglik: -8.1455e+02 - logprior: -1.1871e+00
Epoch 9/10
39/39 - 44s - loss: 815.4530 - loglik: -8.1428e+02 - logprior: -1.1770e+00
Epoch 10/10
39/39 - 44s - loss: 815.3094 - loglik: -8.1414e+02 - logprior: -1.1682e+00
Fitted a model with MAP estimate = -806.7847
expansions: [(0, 2), (16, 1), (17, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 2), (29, 1), (39, 1), (41, 3), (42, 2), (43, 1), (57, 1), (64, 1), (79, 1), (80, 1), (81, 2), (84, 1), (85, 1), (94, 1), (96, 1), (102, 1), (121, 1), (129, 3), (130, 1), (133, 1), (144, 1), (147, 1), (153, 2), (154, 1), (155, 3), (170, 1), (181, 1), (183, 3), (184, 1), (185, 2), (188, 1), (206, 3), (208, 1), (209, 1), (219, 1), (224, 4), (228, 2), (241, 3), (243, 1), (256, 1), (262, 2), (263, 1), (271, 2), (272, 1), (274, 1), (275, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 59s - loss: 808.9772 - loglik: -8.0699e+02 - logprior: -1.9907e+00
Epoch 2/2
39/39 - 64s - loss: 795.0232 - loglik: -7.9444e+02 - logprior: -5.8175e-01
Fitted a model with MAP estimate = -784.3681
expansions: [(55, 1), (116, 2)]
discards: [  0   1 103 192 195 229 235 259 285 290 344]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 799.5411 - loglik: -7.9798e+02 - logprior: -1.5570e+00
Epoch 2/2
39/39 - 70s - loss: 793.6899 - loglik: -7.9351e+02 - logprior: -1.8269e-01
Fitted a model with MAP estimate = -783.3185
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 789.2250 - loglik: -7.8761e+02 - logprior: -1.6188e+00
Epoch 2/10
39/39 - 70s - loss: 785.0230 - loglik: -7.8481e+02 - logprior: -2.1029e-01
Epoch 3/10
39/39 - 70s - loss: 782.7993 - loglik: -7.8273e+02 - logprior: -6.6715e-02
Epoch 4/10
39/39 - 60s - loss: 781.1910 - loglik: -7.8120e+02 - logprior: 0.0059
Epoch 5/10
39/39 - 64s - loss: 780.0997 - loglik: -7.8019e+02 - logprior: 0.0938
Epoch 6/10
39/39 - 62s - loss: 779.4616 - loglik: -7.7963e+02 - logprior: 0.1686
Epoch 7/10
39/39 - 55s - loss: 780.3058 - loglik: -7.8057e+02 - logprior: 0.2649
Fitted a model with MAP estimate = -778.9053
Time for alignment: 1472.4659
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 928.1186 - loglik: -9.2669e+02 - logprior: -1.4250e+00
Epoch 2/10
39/39 - 43s - loss: 834.0599 - loglik: -8.3272e+02 - logprior: -1.3392e+00
Epoch 3/10
39/39 - 41s - loss: 820.4430 - loglik: -8.1896e+02 - logprior: -1.4868e+00
Epoch 4/10
39/39 - 38s - loss: 817.4092 - loglik: -8.1599e+02 - logprior: -1.4228e+00
Epoch 5/10
39/39 - 40s - loss: 815.0319 - loglik: -8.1364e+02 - logprior: -1.3896e+00
Epoch 6/10
39/39 - 43s - loss: 813.9263 - loglik: -8.1254e+02 - logprior: -1.3912e+00
Epoch 7/10
39/39 - 46s - loss: 813.1138 - loglik: -8.1176e+02 - logprior: -1.3510e+00
Epoch 8/10
39/39 - 48s - loss: 813.1006 - loglik: -8.1179e+02 - logprior: -1.3084e+00
Epoch 9/10
39/39 - 50s - loss: 812.1751 - loglik: -8.1081e+02 - logprior: -1.3686e+00
Epoch 10/10
39/39 - 49s - loss: 812.4738 - loglik: -8.1113e+02 - logprior: -1.3488e+00
Fitted a model with MAP estimate = -803.8129
expansions: [(0, 2), (16, 1), (17, 1), (20, 1), (21, 1), (22, 3), (23, 2), (33, 1), (39, 1), (40, 1), (41, 2), (43, 3), (54, 1), (61, 1), (80, 1), (81, 1), (82, 1), (86, 1), (91, 2), (95, 1), (99, 2), (101, 1), (102, 1), (130, 3), (131, 1), (134, 1), (145, 1), (150, 1), (153, 2), (154, 1), (155, 3), (157, 2), (181, 1), (184, 1), (185, 2), (186, 2), (189, 1), (202, 2), (207, 2), (209, 1), (210, 1), (211, 1), (219, 1), (220, 1), (221, 1), (227, 2), (228, 1), (238, 1), (240, 3), (242, 1), (262, 2), (263, 1), (265, 1), (272, 1), (273, 1), (274, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 359 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 805.9378 - loglik: -8.0396e+02 - logprior: -1.9741e+00
Epoch 2/2
39/39 - 46s - loss: 793.0007 - loglik: -7.9242e+02 - logprior: -5.7745e-01
Fitted a model with MAP estimate = -782.5676
expansions: [(186, 1)]
discards: [  0  30  31 127 193 197 203 235 237 256 264 291]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 799.5233 - loglik: -7.9711e+02 - logprior: -2.4120e+00
Epoch 2/2
39/39 - 48s - loss: 793.7740 - loglik: -7.9339e+02 - logprior: -3.8019e-01
Fitted a model with MAP estimate = -783.3736
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 788.7106 - loglik: -7.8751e+02 - logprior: -1.1989e+00
Epoch 2/10
39/39 - 53s - loss: 785.0178 - loglik: -7.8492e+02 - logprior: -9.4564e-02
Epoch 3/10
39/39 - 54s - loss: 782.6890 - loglik: -7.8270e+02 - logprior: 0.0104
Epoch 4/10
39/39 - 52s - loss: 781.5696 - loglik: -7.8162e+02 - logprior: 0.0491
Epoch 5/10
39/39 - 49s - loss: 780.1212 - loglik: -7.8027e+02 - logprior: 0.1446
Epoch 6/10
39/39 - 48s - loss: 779.5306 - loglik: -7.7976e+02 - logprior: 0.2336
Epoch 7/10
39/39 - 47s - loss: 779.8177 - loglik: -7.8014e+02 - logprior: 0.3199
Fitted a model with MAP estimate = -779.1525
Time for alignment: 1228.1890
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 927.4997 - loglik: -9.2607e+02 - logprior: -1.4276e+00
Epoch 2/10
39/39 - 32s - loss: 832.8448 - loglik: -8.3148e+02 - logprior: -1.3622e+00
Epoch 3/10
39/39 - 32s - loss: 821.5193 - loglik: -8.2012e+02 - logprior: -1.3953e+00
Epoch 4/10
39/39 - 31s - loss: 818.6589 - loglik: -8.1729e+02 - logprior: -1.3703e+00
Epoch 5/10
39/39 - 32s - loss: 816.7019 - loglik: -8.1540e+02 - logprior: -1.3060e+00
Epoch 6/10
39/39 - 31s - loss: 815.5327 - loglik: -8.1424e+02 - logprior: -1.2944e+00
Epoch 7/10
39/39 - 32s - loss: 814.8970 - loglik: -8.1355e+02 - logprior: -1.3475e+00
Epoch 8/10
39/39 - 32s - loss: 814.1764 - loglik: -8.1287e+02 - logprior: -1.3098e+00
Epoch 9/10
39/39 - 32s - loss: 814.3063 - loglik: -8.1304e+02 - logprior: -1.2712e+00
Fitted a model with MAP estimate = -806.0418
expansions: [(0, 2), (16, 1), (17, 1), (20, 1), (21, 1), (22, 3), (23, 2), (33, 1), (39, 1), (40, 1), (41, 2), (43, 2), (44, 1), (58, 1), (61, 1), (80, 1), (81, 1), (82, 1), (83, 2), (91, 2), (95, 1), (97, 1), (98, 1), (102, 2), (123, 1), (129, 3), (130, 1), (133, 1), (144, 1), (147, 1), (152, 1), (153, 3), (154, 2), (181, 1), (183, 2), (184, 1), (189, 1), (190, 1), (207, 2), (209, 1), (210, 1), (213, 1), (221, 4), (227, 2), (229, 1), (238, 1), (240, 2), (241, 1), (243, 1), (256, 1), (262, 2), (263, 1), (271, 2), (272, 1), (274, 1), (275, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 359 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 55s - loss: 806.8433 - loglik: -8.0481e+02 - logprior: -2.0298e+00
Epoch 2/2
39/39 - 53s - loss: 793.0053 - loglik: -7.9241e+02 - logprior: -5.9186e-01
Fitted a model with MAP estimate = -782.4430
expansions: []
discards: [  0  30  31 107 134 261 281 290 306 346]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 799.1583 - loglik: -7.9686e+02 - logprior: -2.3015e+00
Epoch 2/2
39/39 - 53s - loss: 793.5876 - loglik: -7.9317e+02 - logprior: -4.1589e-01
Fitted a model with MAP estimate = -783.4740
expansions: [(0, 2)]
discards: [  0 191 255]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 348 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 61s - loss: 789.3375 - loglik: -7.8814e+02 - logprior: -1.1992e+00
Epoch 2/10
39/39 - 58s - loss: 785.0460 - loglik: -7.8487e+02 - logprior: -1.7288e-01
Epoch 3/10
39/39 - 58s - loss: 782.5229 - loglik: -7.8248e+02 - logprior: -4.6256e-02
Epoch 4/10
39/39 - 59s - loss: 781.4675 - loglik: -7.8150e+02 - logprior: 0.0309
Epoch 5/10
39/39 - 63s - loss: 779.7853 - loglik: -7.7994e+02 - logprior: 0.1504
Epoch 6/10
39/39 - 62s - loss: 780.5245 - loglik: -7.8070e+02 - logprior: 0.1723
Fitted a model with MAP estimate = -779.6625
Time for alignment: 1126.5866
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 927.1253 - loglik: -9.2569e+02 - logprior: -1.4334e+00
Epoch 2/10
39/39 - 42s - loss: 833.4498 - loglik: -8.3215e+02 - logprior: -1.3015e+00
Epoch 3/10
39/39 - 42s - loss: 821.4992 - loglik: -8.2012e+02 - logprior: -1.3823e+00
Epoch 4/10
39/39 - 40s - loss: 818.2076 - loglik: -8.1684e+02 - logprior: -1.3708e+00
Epoch 5/10
39/39 - 41s - loss: 816.1545 - loglik: -8.1475e+02 - logprior: -1.4007e+00
Epoch 6/10
39/39 - 40s - loss: 815.8277 - loglik: -8.1444e+02 - logprior: -1.3913e+00
Epoch 7/10
39/39 - 41s - loss: 814.6160 - loglik: -8.1327e+02 - logprior: -1.3469e+00
Epoch 8/10
39/39 - 41s - loss: 814.3274 - loglik: -8.1300e+02 - logprior: -1.3311e+00
Epoch 9/10
39/39 - 42s - loss: 813.9254 - loglik: -8.1256e+02 - logprior: -1.3608e+00
Epoch 10/10
39/39 - 44s - loss: 813.6810 - loglik: -8.1232e+02 - logprior: -1.3591e+00
Fitted a model with MAP estimate = -805.4126
expansions: [(0, 2), (16, 1), (17, 1), (20, 1), (21, 1), (22, 3), (23, 2), (31, 1), (35, 1), (37, 1), (38, 1), (39, 1), (40, 1), (42, 2), (54, 1), (57, 1), (60, 1), (79, 1), (80, 1), (81, 2), (85, 1), (90, 2), (94, 1), (96, 1), (97, 1), (101, 1), (126, 1), (127, 1), (128, 1), (130, 1), (132, 2), (144, 1), (149, 1), (152, 2), (153, 1), (154, 3), (171, 1), (182, 1), (183, 1), (184, 2), (189, 1), (190, 1), (206, 3), (209, 1), (210, 1), (212, 1), (224, 4), (228, 2), (241, 3), (243, 1), (246, 1), (262, 2), (263, 1), (271, 2), (272, 1), (274, 1), (275, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 359 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 809.4679 - loglik: -8.0742e+02 - logprior: -2.0472e+00
Epoch 2/2
39/39 - 64s - loss: 793.7531 - loglik: -7.9313e+02 - logprior: -6.2048e-01
Fitted a model with MAP estimate = -782.9343
expansions: [(187, 1)]
discards: [  0  30  31 105 169 195 196 234 261 287 292 346]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 799.5854 - loglik: -7.9729e+02 - logprior: -2.2980e+00
Epoch 2/2
39/39 - 64s - loss: 793.7490 - loglik: -7.9340e+02 - logprior: -3.4672e-01
Fitted a model with MAP estimate = -783.5461
expansions: [(0, 2), (191, 1)]
discards: [  0 117]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 62s - loss: 788.8493 - loglik: -7.8767e+02 - logprior: -1.1804e+00
Epoch 2/10
39/39 - 60s - loss: 786.0171 - loglik: -7.8585e+02 - logprior: -1.6235e-01
Epoch 3/10
39/39 - 62s - loss: 782.4752 - loglik: -7.8241e+02 - logprior: -6.2869e-02
Epoch 4/10
39/39 - 64s - loss: 780.7308 - loglik: -7.8074e+02 - logprior: 0.0064
Epoch 5/10
39/39 - 64s - loss: 780.7493 - loglik: -7.8083e+02 - logprior: 0.0766
Fitted a model with MAP estimate = -779.7268
Time for alignment: 1293.8004
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 927.6293 - loglik: -9.2620e+02 - logprior: -1.4288e+00
Epoch 2/10
39/39 - 41s - loss: 833.6190 - loglik: -8.3230e+02 - logprior: -1.3238e+00
Epoch 3/10
39/39 - 42s - loss: 820.4800 - loglik: -8.1912e+02 - logprior: -1.3634e+00
Epoch 4/10
39/39 - 41s - loss: 817.2190 - loglik: -8.1588e+02 - logprior: -1.3340e+00
Epoch 5/10
39/39 - 43s - loss: 815.2973 - loglik: -8.1395e+02 - logprior: -1.3457e+00
Epoch 6/10
39/39 - 45s - loss: 814.2903 - loglik: -8.1298e+02 - logprior: -1.3082e+00
Epoch 7/10
39/39 - 43s - loss: 813.2477 - loglik: -8.1196e+02 - logprior: -1.2868e+00
Epoch 8/10
39/39 - 38s - loss: 812.8986 - loglik: -8.1162e+02 - logprior: -1.2822e+00
Epoch 9/10
39/39 - 36s - loss: 812.7995 - loglik: -8.1153e+02 - logprior: -1.2744e+00
Epoch 10/10
39/39 - 35s - loss: 812.3677 - loglik: -8.1113e+02 - logprior: -1.2367e+00
Fitted a model with MAP estimate = -804.1942
expansions: [(0, 2), (16, 1), (17, 1), (20, 1), (21, 1), (22, 3), (23, 2), (33, 1), (39, 1), (41, 1), (42, 2), (43, 2), (44, 1), (58, 1), (61, 1), (80, 1), (81, 1), (82, 1), (86, 1), (88, 1), (95, 1), (101, 2), (102, 3), (123, 1), (130, 2), (131, 1), (133, 2), (145, 1), (150, 1), (153, 2), (154, 1), (155, 3), (157, 2), (181, 1), (184, 1), (185, 2), (186, 2), (189, 1), (207, 3), (209, 1), (210, 1), (213, 1), (221, 4), (227, 2), (228, 1), (237, 1), (240, 3), (242, 1), (258, 1), (261, 2), (262, 1), (270, 2), (271, 1), (273, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 361 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 54s - loss: 806.8104 - loglik: -8.0481e+02 - logprior: -1.9999e+00
Epoch 2/2
39/39 - 50s - loss: 792.3136 - loglik: -7.9173e+02 - logprior: -5.8831e-01
Fitted a model with MAP estimate = -781.6576
expansions: [(117, 2)]
discards: [  0  30  31 132 133 169 195 198 199 236 238 263 283 293 348]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 798.6760 - loglik: -7.9636e+02 - logprior: -2.3140e+00
Epoch 2/2
39/39 - 43s - loss: 792.8325 - loglik: -7.9242e+02 - logprior: -4.1592e-01
Fitted a model with MAP estimate = -782.5218
expansions: [(0, 2), (193, 1)]
discards: [  0 197]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 788.0328 - loglik: -7.8681e+02 - logprior: -1.2215e+00
Epoch 2/10
39/39 - 47s - loss: 784.5858 - loglik: -7.8446e+02 - logprior: -1.2532e-01
Epoch 3/10
39/39 - 48s - loss: 781.3153 - loglik: -7.8131e+02 - logprior: -4.3890e-03
Epoch 4/10
39/39 - 50s - loss: 780.6793 - loglik: -7.8074e+02 - logprior: 0.0646
Epoch 5/10
39/39 - 52s - loss: 780.1929 - loglik: -7.8033e+02 - logprior: 0.1364
Epoch 6/10
39/39 - 52s - loss: 779.5286 - loglik: -7.7974e+02 - logprior: 0.2129
Epoch 7/10
39/39 - 52s - loss: 778.4293 - loglik: -7.7869e+02 - logprior: 0.2653
Epoch 8/10
39/39 - 52s - loss: 779.1104 - loglik: -7.7945e+02 - logprior: 0.3440
Fitted a model with MAP estimate = -778.2859
Time for alignment: 1221.2427
Computed alignments with likelihoods: ['-778.9053', '-779.1525', '-779.6625', '-779.7268', '-778.2859']
Best model has likelihood: -778.2859  (prior= 0.4536 )
time for generating output: 1.1203
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.42046197590638074
Training of 5 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49f0672580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d1bba70a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bdebbbbe0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4be876cd30>
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 920.3196 - loglik: -9.1885e+02 - logprior: -1.4743e+00
Epoch 2/10
39/39 - 56s - loss: 767.6661 - loglik: -7.6605e+02 - logprior: -1.6160e+00
Epoch 3/10
39/39 - 57s - loss: 750.0958 - loglik: -7.4829e+02 - logprior: -1.8051e+00
Epoch 4/10
39/39 - 52s - loss: 746.2893 - loglik: -7.4451e+02 - logprior: -1.7809e+00
Epoch 5/10
39/39 - 48s - loss: 745.4181 - loglik: -7.4362e+02 - logprior: -1.7995e+00
Epoch 6/10
39/39 - 47s - loss: 743.1540 - loglik: -7.4134e+02 - logprior: -1.8095e+00
Epoch 7/10
39/39 - 46s - loss: 743.6279 - loglik: -7.4184e+02 - logprior: -1.7899e+00
Fitted a model with MAP estimate = -741.9560
expansions: [(20, 2), (21, 2), (62, 1), (63, 2), (65, 2), (66, 3), (72, 1), (73, 1), (78, 1), (94, 2), (97, 1), (99, 1), (105, 1), (109, 1), (114, 1), (133, 1), (135, 1), (136, 1), (138, 3), (139, 2), (154, 1), (156, 1), (157, 1), (158, 1), (173, 1), (174, 1), (175, 3), (178, 3), (179, 2), (181, 1), (188, 1), (189, 1), (190, 1), (191, 4), (219, 1), (220, 1), (224, 1), (233, 1), (234, 3), (243, 2), (250, 1), (257, 1), (260, 1), (273, 1), (277, 1), (279, 1), (281, 1), (282, 1), (283, 1), (284, 1), (290, 1), (296, 1), (297, 1), (298, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 193 194 195 196 197 198 199 200 201 202 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 741.0205 - loglik: -7.3856e+02 - logprior: -2.4566e+00
Epoch 2/2
39/39 - 62s - loss: 725.8368 - loglik: -7.2462e+02 - logprior: -1.2146e+00
Fitted a model with MAP estimate = -722.1658
expansions: [(4, 2), (237, 5), (244, 16), (284, 1), (286, 2)]
discards: [  0   1  21  73 209 210 217 218 272 273]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 403 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 727.4680 - loglik: -7.2539e+02 - logprior: -2.0785e+00
Epoch 2/2
39/39 - 71s - loss: 719.9552 - loglik: -7.1926e+02 - logprior: -6.9900e-01
Fitted a model with MAP estimate = -716.2255
expansions: [(5, 1), (213, 2), (287, 2)]
discards: [  2 233 234 250 251 252 253 254 255 256 257 258 259 260 301]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 393 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 722.8355 - loglik: -7.2170e+02 - logprior: -1.1324e+00
Epoch 2/10
39/39 - 70s - loss: 717.8472 - loglik: -7.1799e+02 - logprior: 0.1385
Epoch 3/10
39/39 - 71s - loss: 716.3020 - loglik: -7.1656e+02 - logprior: 0.2546
Epoch 4/10
39/39 - 70s - loss: 714.8701 - loglik: -7.1521e+02 - logprior: 0.3395
Epoch 5/10
39/39 - 70s - loss: 713.4529 - loglik: -7.1376e+02 - logprior: 0.3102
Epoch 6/10
39/39 - 72s - loss: 713.0537 - loglik: -7.1357e+02 - logprior: 0.5121
Epoch 7/10
39/39 - 72s - loss: 713.1180 - loglik: -7.1372e+02 - logprior: 0.6010
Fitted a model with MAP estimate = -711.9243
Time for alignment: 1414.9086
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 922.0128 - loglik: -9.2056e+02 - logprior: -1.4578e+00
Epoch 2/10
39/39 - 52s - loss: 771.8753 - loglik: -7.7022e+02 - logprior: -1.6527e+00
Epoch 3/10
39/39 - 52s - loss: 753.3285 - loglik: -7.5158e+02 - logprior: -1.7453e+00
Epoch 4/10
39/39 - 52s - loss: 750.1584 - loglik: -7.4840e+02 - logprior: -1.7545e+00
Epoch 5/10
39/39 - 54s - loss: 747.7733 - loglik: -7.4605e+02 - logprior: -1.7261e+00
Epoch 6/10
39/39 - 61s - loss: 747.6185 - loglik: -7.4588e+02 - logprior: -1.7350e+00
Epoch 7/10
39/39 - 65s - loss: 746.5759 - loglik: -7.4483e+02 - logprior: -1.7438e+00
Epoch 8/10
39/39 - 65s - loss: 747.0472 - loglik: -7.4531e+02 - logprior: -1.7367e+00
Fitted a model with MAP estimate = -745.0276
expansions: [(20, 2), (57, 1), (62, 1), (63, 3), (64, 3), (65, 1), (71, 1), (72, 2), (90, 1), (93, 2), (96, 1), (98, 1), (104, 1), (108, 1), (112, 1), (113, 1), (131, 1), (133, 1), (134, 1), (136, 3), (137, 2), (151, 1), (152, 1), (153, 2), (154, 1), (155, 1), (169, 1), (170, 2), (171, 1), (176, 4), (185, 1), (186, 1), (187, 1), (189, 2), (200, 3), (201, 13), (217, 2), (218, 2), (219, 1), (223, 1), (232, 1), (233, 3), (239, 1), (241, 2), (250, 1), (252, 1), (256, 1), (272, 1), (276, 1), (277, 2), (281, 3), (282, 2), (298, 1), (299, 1), (300, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 192 193 194 195 196 197 198 203 204 205 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 406 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 96s - loss: 742.6829 - loglik: -7.4013e+02 - logprior: -2.5507e+00
Epoch 2/2
39/39 - 106s - loss: 723.8326 - loglik: -7.2245e+02 - logprior: -1.3800e+00
Fitted a model with MAP estimate = -719.5585
expansions: [(4, 2), (233, 3), (249, 8), (345, 1), (348, 1)]
discards: [  0   1  66 207 216 217 235 236 237 238 242 243 266 268 286 349 350 351
 352]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 94s - loss: 729.1240 - loglik: -7.2697e+02 - logprior: -2.1577e+00
Epoch 2/2
39/39 - 94s - loss: 721.7698 - loglik: -7.2110e+02 - logprior: -6.6723e-01
Fitted a model with MAP estimate = -717.8289
expansions: [(5, 1), (259, 2), (260, 1)]
discards: [  2 207 208 209 210 212 213 235 253 254 255 256 257 287]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 392 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 99s - loss: 727.2552 - loglik: -7.2605e+02 - logprior: -1.2071e+00
Epoch 2/10
39/39 - 94s - loss: 721.4926 - loglik: -7.2155e+02 - logprior: 0.0527
Epoch 3/10
39/39 - 91s - loss: 720.1644 - loglik: -7.2028e+02 - logprior: 0.1198
Epoch 4/10
39/39 - 90s - loss: 718.2412 - loglik: -7.1846e+02 - logprior: 0.2180
Epoch 5/10
39/39 - 94s - loss: 717.0175 - loglik: -7.1734e+02 - logprior: 0.3204
Epoch 6/10
39/39 - 102s - loss: 718.4120 - loglik: -7.1883e+02 - logprior: 0.4229
Fitted a model with MAP estimate = -716.2247
Time for alignment: 1850.2881
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 73s - loss: 920.2411 - loglik: -9.1879e+02 - logprior: -1.4477e+00
Epoch 2/10
39/39 - 72s - loss: 768.7624 - loglik: -7.6719e+02 - logprior: -1.5721e+00
Epoch 3/10
39/39 - 67s - loss: 751.4198 - loglik: -7.4974e+02 - logprior: -1.6774e+00
Epoch 4/10
39/39 - 65s - loss: 747.4199 - loglik: -7.4575e+02 - logprior: -1.6705e+00
Epoch 5/10
39/39 - 66s - loss: 746.3856 - loglik: -7.4470e+02 - logprior: -1.6863e+00
Epoch 6/10
39/39 - 67s - loss: 744.1733 - loglik: -7.4248e+02 - logprior: -1.6893e+00
Epoch 7/10
39/39 - 73s - loss: 743.5482 - loglik: -7.4188e+02 - logprior: -1.6647e+00
Epoch 8/10
39/39 - 75s - loss: 744.1212 - loglik: -7.4242e+02 - logprior: -1.7051e+00
Fitted a model with MAP estimate = -742.3665
expansions: [(19, 2), (62, 1), (63, 2), (66, 3), (67, 1), (74, 3), (96, 2), (99, 1), (112, 1), (117, 1), (118, 1), (136, 1), (138, 1), (139, 1), (141, 4), (156, 1), (158, 2), (159, 2), (163, 1), (173, 1), (174, 1), (175, 3), (178, 3), (179, 5), (181, 2), (187, 1), (188, 1), (189, 1), (190, 1), (203, 3), (204, 1), (217, 2), (218, 1), (219, 1), (223, 1), (232, 1), (233, 3), (242, 2), (243, 2), (252, 1), (253, 1), (256, 1), (269, 1), (279, 1), (280, 1), (281, 1), (282, 6), (296, 1), (297, 1), (298, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 193 194 195 196 197 198 199 200 201 206 207 208 209 210 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 105s - loss: 739.4482 - loglik: -7.3697e+02 - logprior: -2.4734e+00
Epoch 2/2
39/39 - 93s - loss: 722.4753 - loglik: -7.2125e+02 - logprior: -1.2227e+00
Fitted a model with MAP estimate = -718.9696
expansions: [(4, 2), (5, 2), (160, 2), (245, 3)]
discards: [  0   1  65  70 206 207 213 214 215 216 218 254 276 277 291 292 340 341]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 96s - loss: 728.4282 - loglik: -7.2629e+02 - logprior: -2.1375e+00
Epoch 2/2
39/39 - 69s - loss: 721.2817 - loglik: -7.2084e+02 - logprior: -4.4052e-01
Fitted a model with MAP estimate = -717.8307
expansions: [(20, 1), (217, 1), (273, 2)]
discards: [  0   1   2 161]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 387 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 725.0308 - loglik: -7.2317e+02 - logprior: -1.8631e+00
Epoch 2/10
39/39 - 73s - loss: 719.9098 - loglik: -7.1993e+02 - logprior: 0.0162
Epoch 3/10
39/39 - 75s - loss: 717.0826 - loglik: -7.1728e+02 - logprior: 0.1986
Epoch 4/10
39/39 - 78s - loss: 715.4536 - loglik: -7.1569e+02 - logprior: 0.2380
Epoch 5/10
39/39 - 78s - loss: 715.4834 - loglik: -7.1589e+02 - logprior: 0.4099
Fitted a model with MAP estimate = -713.8333
Time for alignment: 1651.5398
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 921.5102 - loglik: -9.2003e+02 - logprior: -1.4810e+00
Epoch 2/10
39/39 - 57s - loss: 769.4559 - loglik: -7.6792e+02 - logprior: -1.5408e+00
Epoch 3/10
39/39 - 57s - loss: 749.9966 - loglik: -7.4825e+02 - logprior: -1.7482e+00
Epoch 4/10
39/39 - 57s - loss: 746.5284 - loglik: -7.4482e+02 - logprior: -1.7034e+00
Epoch 5/10
39/39 - 57s - loss: 744.6949 - loglik: -7.4298e+02 - logprior: -1.7178e+00
Epoch 6/10
39/39 - 53s - loss: 743.7296 - loglik: -7.4200e+02 - logprior: -1.7275e+00
Epoch 7/10
39/39 - 49s - loss: 743.2706 - loglik: -7.4155e+02 - logprior: -1.7252e+00
Epoch 8/10
39/39 - 48s - loss: 743.6019 - loglik: -7.4191e+02 - logprior: -1.6920e+00
Fitted a model with MAP estimate = -741.5062
expansions: [(16, 1), (19, 2), (31, 1), (55, 1), (60, 1), (61, 2), (64, 2), (65, 3), (72, 1), (73, 1), (90, 1), (93, 2), (96, 1), (102, 1), (109, 1), (113, 1), (114, 1), (132, 1), (134, 1), (135, 1), (136, 3), (137, 2), (153, 1), (155, 3), (156, 1), (168, 1), (170, 1), (171, 1), (172, 3), (177, 2), (178, 1), (180, 1), (185, 1), (187, 1), (189, 1), (198, 1), (201, 2), (217, 2), (218, 2), (219, 1), (233, 3), (240, 1), (242, 2), (250, 1), (251, 1), (257, 1), (258, 1), (277, 1), (278, 1), (281, 1), (282, 1), (283, 5), (289, 1), (296, 1), (297, 1), (298, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 191 192 193 194 195 196 203 204 205 211]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 398 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 73s - loss: 738.9470 - loglik: -7.3638e+02 - logprior: -2.5671e+00
Epoch 2/2
39/39 - 75s - loss: 723.1749 - loglik: -7.2186e+02 - logprior: -1.3154e+00
Fitted a model with MAP estimate = -719.5214
expansions: [(4, 2), (233, 4), (243, 2), (244, 1)]
discards: [  0   1  66  72 162 209 210 258 260 341 342 343]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 395 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 728.4258 - loglik: -7.2630e+02 - logprior: -2.1218e+00
Epoch 2/2
39/39 - 72s - loss: 720.9858 - loglik: -7.2040e+02 - logprior: -5.8708e-01
Fitted a model with MAP estimate = -718.1181
expansions: [(5, 1), (229, 1), (230, 1)]
discards: [  2 216 231 232 233 234 235 242 278]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 389 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 725.3544 - loglik: -7.2419e+02 - logprior: -1.1616e+00
Epoch 2/10
39/39 - 65s - loss: 720.9213 - loglik: -7.2105e+02 - logprior: 0.1336
Epoch 3/10
39/39 - 73s - loss: 719.0808 - loglik: -7.1934e+02 - logprior: 0.2546
Epoch 4/10
39/39 - 79s - loss: 717.8677 - loglik: -7.1813e+02 - logprior: 0.2633
Epoch 5/10
39/39 - 83s - loss: 717.2175 - loglik: -7.1766e+02 - logprior: 0.4416
Epoch 6/10
39/39 - 88s - loss: 716.1019 - loglik: -7.1658e+02 - logprior: 0.4776
Epoch 7/10
39/39 - 89s - loss: 713.9019 - loglik: -7.1455e+02 - logprior: 0.6452
Epoch 8/10
39/39 - 85s - loss: 716.1695 - loglik: -7.1684e+02 - logprior: 0.6661
Fitted a model with MAP estimate = -714.3676
Time for alignment: 1665.2141
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 921.0524 - loglik: -9.1957e+02 - logprior: -1.4848e+00
Epoch 2/10
39/39 - 68s - loss: 768.2518 - loglik: -7.6657e+02 - logprior: -1.6806e+00
Epoch 3/10
39/39 - 66s - loss: 750.8508 - loglik: -7.4896e+02 - logprior: -1.8877e+00
Epoch 4/10
39/39 - 64s - loss: 746.8008 - loglik: -7.4500e+02 - logprior: -1.8024e+00
Epoch 5/10
39/39 - 67s - loss: 746.0802 - loglik: -7.4431e+02 - logprior: -1.7680e+00
Epoch 6/10
39/39 - 68s - loss: 744.2016 - loglik: -7.4237e+02 - logprior: -1.8334e+00
Epoch 7/10
39/39 - 67s - loss: 744.2686 - loglik: -7.4245e+02 - logprior: -1.8136e+00
Fitted a model with MAP estimate = -742.5645
expansions: [(20, 2), (21, 1), (56, 1), (61, 1), (62, 2), (64, 2), (65, 3), (71, 3), (89, 1), (92, 2), (95, 1), (97, 1), (100, 1), (106, 1), (111, 1), (112, 1), (130, 1), (132, 1), (133, 1), (135, 3), (136, 2), (151, 1), (153, 2), (154, 1), (155, 2), (166, 1), (168, 1), (169, 1), (176, 1), (184, 1), (185, 1), (186, 2), (187, 1), (198, 1), (199, 2), (200, 8), (216, 2), (217, 2), (218, 1), (222, 1), (231, 1), (232, 2), (238, 1), (240, 2), (249, 1), (251, 1), (268, 1), (272, 1), (277, 1), (278, 1), (280, 1), (281, 2), (282, 2), (288, 1), (297, 1), (298, 1), (305, 1), (317, 1), (319, 1), (321, 1)]
discards: [  0   1 170 171 172 173 174 202 203 204 205 206]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 102s - loss: 738.6664 - loglik: -7.3605e+02 - logprior: -2.6177e+00
Epoch 2/2
39/39 - 99s - loss: 723.3670 - loglik: -7.2199e+02 - logprior: -1.3803e+00
Fitted a model with MAP estimate = -719.2343
expansions: [(4, 2), (5, 1), (211, 7), (241, 1), (245, 1), (255, 1)]
discards: [  0   1  71 189 248 249 250 251 252 262 265 347 348]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 728.5353 - loglik: -7.2627e+02 - logprior: -2.2690e+00
Epoch 2/2
39/39 - 66s - loss: 721.8478 - loglik: -7.2107e+02 - logprior: -7.7522e-01
Fitted a model with MAP estimate = -718.7372
expansions: [(4, 1), (5, 1), (247, 7)]
discards: [  0   1 210 211 212 213 252 253 254]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 402 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 76s - loss: 725.6384 - loglik: -7.2372e+02 - logprior: -1.9171e+00
Epoch 2/10
39/39 - 74s - loss: 719.0963 - loglik: -7.1896e+02 - logprior: -1.4094e-01
Epoch 3/10
39/39 - 75s - loss: 717.0964 - loglik: -7.1716e+02 - logprior: 0.0620
Epoch 4/10
39/39 - 87s - loss: 715.5786 - loglik: -7.1571e+02 - logprior: 0.1317
Epoch 5/10
39/39 - 92s - loss: 714.8190 - loglik: -7.1513e+02 - logprior: 0.3101
Epoch 6/10
39/39 - 91s - loss: 713.3243 - loglik: -7.1374e+02 - logprior: 0.4178
Epoch 7/10
39/39 - 89s - loss: 712.7908 - loglik: -7.1330e+02 - logprior: 0.5092
Epoch 8/10
39/39 - 92s - loss: 714.2946 - loglik: -7.1492e+02 - logprior: 0.6235
Fitted a model with MAP estimate = -712.4164
Time for alignment: 1847.9828
Computed alignments with likelihoods: ['-711.9243', '-716.2247', '-713.8333', '-714.3676', '-712.4164']
Best model has likelihood: -711.9243  (prior= 0.7355 )
time for generating output: 0.4098
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.8011579434923576
Training of 5 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49f0741c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bd482ef40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bd4826fd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d6bac2e50>
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 254.5397 - loglik: -2.5148e+02 - logprior: -3.0576e+00
Epoch 2/10
19/19 - 2s - loss: 223.2832 - loglik: -2.2200e+02 - logprior: -1.2853e+00
Epoch 3/10
19/19 - 2s - loss: 209.0837 - loglik: -2.0753e+02 - logprior: -1.5576e+00
Epoch 4/10
19/19 - 2s - loss: 205.8557 - loglik: -2.0436e+02 - logprior: -1.4977e+00
Epoch 5/10
19/19 - 2s - loss: 205.0212 - loglik: -2.0352e+02 - logprior: -1.5054e+00
Epoch 6/10
19/19 - 2s - loss: 204.4587 - loglik: -2.0300e+02 - logprior: -1.4607e+00
Epoch 7/10
19/19 - 2s - loss: 204.2771 - loglik: -2.0283e+02 - logprior: -1.4436e+00
Epoch 8/10
19/19 - 2s - loss: 204.2912 - loglik: -2.0286e+02 - logprior: -1.4362e+00
Fitted a model with MAP estimate = -203.9290
expansions: [(8, 1), (10, 1), (11, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (32, 1), (40, 2), (42, 2), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.4065 - loglik: -2.0449e+02 - logprior: -3.9125e+00
Epoch 2/2
19/19 - 2s - loss: 200.8398 - loglik: -1.9884e+02 - logprior: -2.0029e+00
Fitted a model with MAP estimate = -199.4658
expansions: [(0, 2)]
discards: [ 0 51 65 84]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 201.0412 - loglik: -1.9815e+02 - logprior: -2.8926e+00
Epoch 2/2
19/19 - 2s - loss: 197.8552 - loglik: -1.9673e+02 - logprior: -1.1224e+00
Fitted a model with MAP estimate = -197.2804
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 202.1860 - loglik: -1.9852e+02 - logprior: -3.6703e+00
Epoch 2/10
19/19 - 2s - loss: 198.0715 - loglik: -1.9691e+02 - logprior: -1.1655e+00
Epoch 3/10
19/19 - 2s - loss: 197.2052 - loglik: -1.9626e+02 - logprior: -9.4893e-01
Epoch 4/10
19/19 - 2s - loss: 197.0532 - loglik: -1.9615e+02 - logprior: -9.0530e-01
Epoch 5/10
19/19 - 2s - loss: 196.8608 - loglik: -1.9598e+02 - logprior: -8.8158e-01
Epoch 6/10
19/19 - 2s - loss: 196.7322 - loglik: -1.9587e+02 - logprior: -8.6019e-01
Epoch 7/10
19/19 - 2s - loss: 196.5006 - loglik: -1.9566e+02 - logprior: -8.4310e-01
Epoch 8/10
19/19 - 2s - loss: 196.5069 - loglik: -1.9567e+02 - logprior: -8.3238e-01
Fitted a model with MAP estimate = -196.3547
Time for alignment: 64.5985
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 254.5395 - loglik: -2.5148e+02 - logprior: -3.0603e+00
Epoch 2/10
19/19 - 2s - loss: 224.5852 - loglik: -2.2331e+02 - logprior: -1.2766e+00
Epoch 3/10
19/19 - 2s - loss: 208.7382 - loglik: -2.0725e+02 - logprior: -1.4911e+00
Epoch 4/10
19/19 - 2s - loss: 205.1023 - loglik: -2.0367e+02 - logprior: -1.4331e+00
Epoch 5/10
19/19 - 2s - loss: 204.1594 - loglik: -2.0272e+02 - logprior: -1.4429e+00
Epoch 6/10
19/19 - 2s - loss: 203.7884 - loglik: -2.0238e+02 - logprior: -1.4070e+00
Epoch 7/10
19/19 - 2s - loss: 203.7962 - loglik: -2.0240e+02 - logprior: -1.4003e+00
Fitted a model with MAP estimate = -203.4043
expansions: [(8, 1), (10, 2), (11, 3), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (33, 1), (38, 1), (39, 1), (42, 2), (43, 1), (49, 2), (50, 1), (59, 1), (60, 1), (62, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.0976 - loglik: -2.0421e+02 - logprior: -3.8883e+00
Epoch 2/2
19/19 - 2s - loss: 200.6193 - loglik: -1.9861e+02 - logprior: -2.0143e+00
Fitted a model with MAP estimate = -199.4100
expansions: [(0, 2)]
discards: [ 0 11 13 66]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.9629 - loglik: -1.9807e+02 - logprior: -2.8977e+00
Epoch 2/2
19/19 - 2s - loss: 197.9471 - loglik: -1.9683e+02 - logprior: -1.1152e+00
Fitted a model with MAP estimate = -197.2802
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 202.2383 - loglik: -1.9855e+02 - logprior: -3.6877e+00
Epoch 2/10
19/19 - 2s - loss: 198.1064 - loglik: -1.9693e+02 - logprior: -1.1724e+00
Epoch 3/10
19/19 - 2s - loss: 197.3680 - loglik: -1.9643e+02 - logprior: -9.4170e-01
Epoch 4/10
19/19 - 2s - loss: 196.8572 - loglik: -1.9595e+02 - logprior: -9.0796e-01
Epoch 5/10
19/19 - 2s - loss: 196.7248 - loglik: -1.9584e+02 - logprior: -8.8377e-01
Epoch 6/10
19/19 - 2s - loss: 196.7497 - loglik: -1.9589e+02 - logprior: -8.6141e-01
Fitted a model with MAP estimate = -196.5076
Time for alignment: 56.8086
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 254.5810 - loglik: -2.5152e+02 - logprior: -3.0576e+00
Epoch 2/10
19/19 - 2s - loss: 224.2092 - loglik: -2.2292e+02 - logprior: -1.2883e+00
Epoch 3/10
19/19 - 2s - loss: 208.1133 - loglik: -2.0657e+02 - logprior: -1.5419e+00
Epoch 4/10
19/19 - 2s - loss: 205.3635 - loglik: -2.0389e+02 - logprior: -1.4688e+00
Epoch 5/10
19/19 - 2s - loss: 204.5998 - loglik: -2.0313e+02 - logprior: -1.4655e+00
Epoch 6/10
19/19 - 2s - loss: 204.3010 - loglik: -2.0287e+02 - logprior: -1.4268e+00
Epoch 7/10
19/19 - 2s - loss: 203.8015 - loglik: -2.0238e+02 - logprior: -1.4190e+00
Epoch 8/10
19/19 - 2s - loss: 204.1362 - loglik: -2.0273e+02 - logprior: -1.4069e+00
Fitted a model with MAP estimate = -203.7723
expansions: [(8, 1), (9, 1), (11, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (38, 1), (39, 1), (42, 2), (43, 1), (49, 2), (50, 1), (59, 1), (60, 1), (62, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.6963 - loglik: -2.0382e+02 - logprior: -3.8726e+00
Epoch 2/2
19/19 - 2s - loss: 200.6823 - loglik: -1.9870e+02 - logprior: -1.9849e+00
Fitted a model with MAP estimate = -199.4198
expansions: [(0, 2)]
discards: [ 0 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.9320 - loglik: -1.9803e+02 - logprior: -2.8983e+00
Epoch 2/2
19/19 - 2s - loss: 197.8556 - loglik: -1.9674e+02 - logprior: -1.1153e+00
Fitted a model with MAP estimate = -197.2591
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 202.2093 - loglik: -1.9853e+02 - logprior: -3.6766e+00
Epoch 2/10
19/19 - 2s - loss: 198.1095 - loglik: -1.9693e+02 - logprior: -1.1822e+00
Epoch 3/10
19/19 - 2s - loss: 197.2436 - loglik: -1.9630e+02 - logprior: -9.4709e-01
Epoch 4/10
19/19 - 2s - loss: 196.9975 - loglik: -1.9609e+02 - logprior: -9.0512e-01
Epoch 5/10
19/19 - 2s - loss: 196.8419 - loglik: -1.9595e+02 - logprior: -8.8714e-01
Epoch 6/10
19/19 - 2s - loss: 196.5401 - loglik: -1.9568e+02 - logprior: -8.6467e-01
Epoch 7/10
19/19 - 2s - loss: 196.7045 - loglik: -1.9586e+02 - logprior: -8.4460e-01
Fitted a model with MAP estimate = -196.4104
Time for alignment: 59.8036
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 254.7713 - loglik: -2.5171e+02 - logprior: -3.0569e+00
Epoch 2/10
19/19 - 2s - loss: 223.3354 - loglik: -2.2204e+02 - logprior: -1.2957e+00
Epoch 3/10
19/19 - 2s - loss: 209.0262 - loglik: -2.0749e+02 - logprior: -1.5321e+00
Epoch 4/10
19/19 - 2s - loss: 205.9586 - loglik: -2.0449e+02 - logprior: -1.4688e+00
Epoch 5/10
19/19 - 2s - loss: 204.6346 - loglik: -2.0316e+02 - logprior: -1.4782e+00
Epoch 6/10
19/19 - 2s - loss: 204.8246 - loglik: -2.0339e+02 - logprior: -1.4328e+00
Fitted a model with MAP estimate = -204.2646
expansions: [(8, 1), (9, 1), (11, 1), (13, 2), (14, 1), (15, 1), (16, 1), (19, 1), (33, 1), (38, 1), (39, 1), (42, 2), (43, 1), (50, 2), (58, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.1661 - loglik: -2.0426e+02 - logprior: -3.9075e+00
Epoch 2/2
19/19 - 2s - loss: 200.6814 - loglik: -1.9872e+02 - logprior: -1.9619e+00
Fitted a model with MAP estimate = -199.2406
expansions: [(0, 2)]
discards: [ 0 65 83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 201.0153 - loglik: -1.9812e+02 - logprior: -2.8928e+00
Epoch 2/2
19/19 - 2s - loss: 197.9929 - loglik: -1.9688e+02 - logprior: -1.1141e+00
Fitted a model with MAP estimate = -197.2825
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 202.2886 - loglik: -1.9860e+02 - logprior: -3.6920e+00
Epoch 2/10
19/19 - 2s - loss: 198.1442 - loglik: -1.9695e+02 - logprior: -1.1917e+00
Epoch 3/10
19/19 - 2s - loss: 197.1485 - loglik: -1.9620e+02 - logprior: -9.4468e-01
Epoch 4/10
19/19 - 2s - loss: 196.9971 - loglik: -1.9609e+02 - logprior: -9.0984e-01
Epoch 5/10
19/19 - 2s - loss: 196.8016 - loglik: -1.9591e+02 - logprior: -8.9137e-01
Epoch 6/10
19/19 - 2s - loss: 196.8111 - loglik: -1.9594e+02 - logprior: -8.6824e-01
Fitted a model with MAP estimate = -196.5073
Time for alignment: 56.0313
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 254.5962 - loglik: -2.5154e+02 - logprior: -3.0553e+00
Epoch 2/10
19/19 - 2s - loss: 226.4246 - loglik: -2.2513e+02 - logprior: -1.2940e+00
Epoch 3/10
19/19 - 2s - loss: 208.8665 - loglik: -2.0730e+02 - logprior: -1.5633e+00
Epoch 4/10
19/19 - 2s - loss: 205.4550 - loglik: -2.0396e+02 - logprior: -1.4996e+00
Epoch 5/10
19/19 - 2s - loss: 204.7856 - loglik: -2.0331e+02 - logprior: -1.4761e+00
Epoch 6/10
19/19 - 2s - loss: 204.3190 - loglik: -2.0288e+02 - logprior: -1.4395e+00
Epoch 7/10
19/19 - 2s - loss: 204.1055 - loglik: -2.0268e+02 - logprior: -1.4243e+00
Epoch 8/10
19/19 - 2s - loss: 204.0575 - loglik: -2.0265e+02 - logprior: -1.4121e+00
Epoch 9/10
19/19 - 2s - loss: 203.7014 - loglik: -2.0229e+02 - logprior: -1.4120e+00
Epoch 10/10
19/19 - 2s - loss: 204.0854 - loglik: -2.0269e+02 - logprior: -1.4002e+00
Fitted a model with MAP estimate = -203.6559
expansions: [(8, 1), (10, 1), (11, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (24, 1), (38, 1), (39, 1), (42, 2), (43, 1), (49, 2), (50, 1), (59, 1), (60, 1), (62, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 207.6586 - loglik: -2.0378e+02 - logprior: -3.8805e+00
Epoch 2/2
19/19 - 2s - loss: 200.7419 - loglik: -1.9876e+02 - logprior: -1.9799e+00
Fitted a model with MAP estimate = -199.4898
expansions: [(0, 2)]
discards: [ 0 64]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.8160 - loglik: -1.9793e+02 - logprior: -2.8909e+00
Epoch 2/2
19/19 - 2s - loss: 197.8800 - loglik: -1.9676e+02 - logprior: -1.1203e+00
Fitted a model with MAP estimate = -197.2530
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 202.1666 - loglik: -1.9850e+02 - logprior: -3.6655e+00
Epoch 2/10
19/19 - 2s - loss: 198.0564 - loglik: -1.9690e+02 - logprior: -1.1563e+00
Epoch 3/10
19/19 - 2s - loss: 197.2343 - loglik: -1.9630e+02 - logprior: -9.3538e-01
Epoch 4/10
19/19 - 2s - loss: 196.9483 - loglik: -1.9605e+02 - logprior: -9.0108e-01
Epoch 5/10
19/19 - 2s - loss: 196.7346 - loglik: -1.9585e+02 - logprior: -8.8213e-01
Epoch 6/10
19/19 - 2s - loss: 196.5616 - loglik: -1.9571e+02 - logprior: -8.4932e-01
Epoch 7/10
19/19 - 2s - loss: 196.6036 - loglik: -1.9576e+02 - logprior: -8.4676e-01
Fitted a model with MAP estimate = -196.4117
Time for alignment: 61.9989
Computed alignments with likelihoods: ['-196.3547', '-196.5076', '-196.4104', '-196.5073', '-196.4117']
Best model has likelihood: -196.3547  (prior= -0.7706 )
time for generating output: 0.1355
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.7929125138427464
Training of 5 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c962c7eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bdde1ea30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d6bdb82e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4dd46aa310>
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 304.6897 - loglik: -3.0168e+02 - logprior: -3.0106e+00
Epoch 2/10
19/19 - 2s - loss: 275.5105 - loglik: -2.7431e+02 - logprior: -1.2037e+00
Epoch 3/10
19/19 - 2s - loss: 261.8540 - loglik: -2.6072e+02 - logprior: -1.1342e+00
Epoch 4/10
19/19 - 2s - loss: 258.3813 - loglik: -2.5732e+02 - logprior: -1.0564e+00
Epoch 5/10
19/19 - 2s - loss: 257.4480 - loglik: -2.5644e+02 - logprior: -1.0128e+00
Epoch 6/10
19/19 - 2s - loss: 256.9680 - loglik: -2.5599e+02 - logprior: -9.8155e-01
Epoch 7/10
19/19 - 2s - loss: 256.4380 - loglik: -2.5547e+02 - logprior: -9.7014e-01
Epoch 8/10
19/19 - 2s - loss: 255.9935 - loglik: -2.5503e+02 - logprior: -9.6797e-01
Epoch 9/10
19/19 - 2s - loss: 256.3523 - loglik: -2.5540e+02 - logprior: -9.5655e-01
Fitted a model with MAP estimate = -255.4040
expansions: [(0, 2), (7, 1), (20, 3), (21, 2), (22, 1), (34, 2), (38, 2), (41, 2), (42, 2), (43, 1), (50, 2), (51, 2), (52, 1), (74, 2), (75, 2), (76, 3), (77, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 261.0954 - loglik: -2.5690e+02 - logprior: -4.1943e+00
Epoch 2/2
19/19 - 3s - loss: 254.6370 - loglik: -2.5336e+02 - logprior: -1.2768e+00
Fitted a model with MAP estimate = -252.7745
expansions: []
discards: [  1  27  43  57  72 101]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 256.7833 - loglik: -2.5382e+02 - logprior: -2.9584e+00
Epoch 2/2
19/19 - 3s - loss: 253.5043 - loglik: -2.5242e+02 - logprior: -1.0796e+00
Fitted a model with MAP estimate = -252.1072
expansions: []
discards: [93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 255.5824 - loglik: -2.5262e+02 - logprior: -2.9607e+00
Epoch 2/10
19/19 - 2s - loss: 252.6937 - loglik: -2.5167e+02 - logprior: -1.0274e+00
Epoch 3/10
19/19 - 3s - loss: 252.0007 - loglik: -2.5116e+02 - logprior: -8.3745e-01
Epoch 4/10
19/19 - 2s - loss: 251.1087 - loglik: -2.5033e+02 - logprior: -7.8268e-01
Epoch 5/10
19/19 - 3s - loss: 251.2621 - loglik: -2.5051e+02 - logprior: -7.4838e-01
Fitted a model with MAP estimate = -250.7571
Time for alignment: 72.6497
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 304.8354 - loglik: -3.0183e+02 - logprior: -3.0061e+00
Epoch 2/10
19/19 - 2s - loss: 276.3145 - loglik: -2.7513e+02 - logprior: -1.1842e+00
Epoch 3/10
19/19 - 2s - loss: 262.5691 - loglik: -2.6144e+02 - logprior: -1.1281e+00
Epoch 4/10
19/19 - 2s - loss: 259.1744 - loglik: -2.5811e+02 - logprior: -1.0603e+00
Epoch 5/10
19/19 - 2s - loss: 257.5150 - loglik: -2.5648e+02 - logprior: -1.0329e+00
Epoch 6/10
19/19 - 2s - loss: 257.0626 - loglik: -2.5603e+02 - logprior: -1.0289e+00
Epoch 7/10
19/19 - 2s - loss: 256.8585 - loglik: -2.5586e+02 - logprior: -1.0027e+00
Epoch 8/10
19/19 - 2s - loss: 256.2792 - loglik: -2.5529e+02 - logprior: -9.8829e-01
Epoch 9/10
19/19 - 2s - loss: 256.5046 - loglik: -2.5552e+02 - logprior: -9.8618e-01
Fitted a model with MAP estimate = -255.5449
expansions: [(0, 2), (7, 1), (20, 3), (21, 2), (22, 1), (34, 2), (38, 1), (41, 2), (43, 1), (44, 1), (51, 2), (52, 2), (53, 1), (73, 1), (74, 2), (75, 2), (76, 3), (77, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 260.9116 - loglik: -2.5672e+02 - logprior: -4.1914e+00
Epoch 2/2
19/19 - 3s - loss: 254.5960 - loglik: -2.5333e+02 - logprior: -1.2618e+00
Fitted a model with MAP estimate = -252.6687
expansions: []
discards: [ 1 27 43 71]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 256.6775 - loglik: -2.5371e+02 - logprior: -2.9695e+00
Epoch 2/2
19/19 - 3s - loss: 253.4448 - loglik: -2.5238e+02 - logprior: -1.0692e+00
Fitted a model with MAP estimate = -252.0425
expansions: []
discards: [96 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 255.8884 - loglik: -2.5292e+02 - logprior: -2.9682e+00
Epoch 2/10
19/19 - 3s - loss: 252.6644 - loglik: -2.5164e+02 - logprior: -1.0213e+00
Epoch 3/10
19/19 - 3s - loss: 251.8855 - loglik: -2.5105e+02 - logprior: -8.3532e-01
Epoch 4/10
19/19 - 3s - loss: 251.8577 - loglik: -2.5108e+02 - logprior: -7.7637e-01
Epoch 5/10
19/19 - 3s - loss: 251.0134 - loglik: -2.5027e+02 - logprior: -7.4049e-01
Epoch 6/10
19/19 - 3s - loss: 250.5863 - loglik: -2.4986e+02 - logprior: -7.3040e-01
Epoch 7/10
19/19 - 3s - loss: 250.7523 - loglik: -2.5003e+02 - logprior: -7.2199e-01
Fitted a model with MAP estimate = -250.4224
Time for alignment: 76.1396
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 304.7361 - loglik: -3.0173e+02 - logprior: -3.0068e+00
Epoch 2/10
19/19 - 2s - loss: 277.1788 - loglik: -2.7598e+02 - logprior: -1.1970e+00
Epoch 3/10
19/19 - 2s - loss: 262.5755 - loglik: -2.6146e+02 - logprior: -1.1192e+00
Epoch 4/10
19/19 - 2s - loss: 258.8852 - loglik: -2.5784e+02 - logprior: -1.0497e+00
Epoch 5/10
19/19 - 2s - loss: 258.1847 - loglik: -2.5720e+02 - logprior: -9.8194e-01
Epoch 6/10
19/19 - 2s - loss: 257.1287 - loglik: -2.5617e+02 - logprior: -9.5906e-01
Epoch 7/10
19/19 - 2s - loss: 257.1103 - loglik: -2.5615e+02 - logprior: -9.5879e-01
Epoch 8/10
19/19 - 2s - loss: 256.9512 - loglik: -2.5601e+02 - logprior: -9.3803e-01
Epoch 9/10
19/19 - 2s - loss: 256.1738 - loglik: -2.5524e+02 - logprior: -9.3743e-01
Epoch 10/10
19/19 - 2s - loss: 256.4684 - loglik: -2.5554e+02 - logprior: -9.2796e-01
Fitted a model with MAP estimate = -255.5510
expansions: [(0, 2), (7, 1), (20, 3), (21, 2), (22, 1), (34, 2), (41, 3), (42, 1), (44, 2), (50, 2), (51, 2), (52, 1), (74, 2), (75, 2), (76, 3), (77, 2), (84, 2)]
discards: [83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 261.5852 - loglik: -2.5736e+02 - logprior: -4.2212e+00
Epoch 2/2
19/19 - 3s - loss: 254.9213 - loglik: -2.5363e+02 - logprior: -1.2958e+00
Fitted a model with MAP estimate = -252.9169
expansions: [(53, 1)]
discards: [  1  27  44  59  71 100 114]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 256.8956 - loglik: -2.5397e+02 - logprior: -2.9220e+00
Epoch 2/2
19/19 - 3s - loss: 253.5770 - loglik: -2.5251e+02 - logprior: -1.0653e+00
Fitted a model with MAP estimate = -252.1461
expansions: []
discards: [93]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 255.6094 - loglik: -2.5265e+02 - logprior: -2.9549e+00
Epoch 2/10
19/19 - 3s - loss: 252.7012 - loglik: -2.5168e+02 - logprior: -1.0211e+00
Epoch 3/10
19/19 - 3s - loss: 251.9818 - loglik: -2.5114e+02 - logprior: -8.4114e-01
Epoch 4/10
19/19 - 3s - loss: 251.2039 - loglik: -2.5041e+02 - logprior: -7.8923e-01
Epoch 5/10
19/19 - 3s - loss: 251.1922 - loglik: -2.5044e+02 - logprior: -7.5644e-01
Epoch 6/10
19/19 - 3s - loss: 250.5931 - loglik: -2.4987e+02 - logprior: -7.2683e-01
Epoch 7/10
19/19 - 3s - loss: 250.9247 - loglik: -2.5020e+02 - logprior: -7.2129e-01
Fitted a model with MAP estimate = -250.4291
Time for alignment: 80.3484
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 304.5842 - loglik: -3.0157e+02 - logprior: -3.0111e+00
Epoch 2/10
19/19 - 2s - loss: 276.2925 - loglik: -2.7508e+02 - logprior: -1.2113e+00
Epoch 3/10
19/19 - 2s - loss: 262.9356 - loglik: -2.6179e+02 - logprior: -1.1473e+00
Epoch 4/10
19/19 - 2s - loss: 258.9210 - loglik: -2.5783e+02 - logprior: -1.0861e+00
Epoch 5/10
19/19 - 2s - loss: 258.2068 - loglik: -2.5717e+02 - logprior: -1.0330e+00
Epoch 6/10
19/19 - 2s - loss: 257.2962 - loglik: -2.5628e+02 - logprior: -1.0188e+00
Epoch 7/10
19/19 - 2s - loss: 256.9672 - loglik: -2.5597e+02 - logprior: -9.9671e-01
Epoch 8/10
19/19 - 2s - loss: 256.9047 - loglik: -2.5592e+02 - logprior: -9.8841e-01
Epoch 9/10
19/19 - 2s - loss: 256.8072 - loglik: -2.5581e+02 - logprior: -9.9767e-01
Epoch 10/10
19/19 - 2s - loss: 256.2305 - loglik: -2.5528e+02 - logprior: -9.4987e-01
Fitted a model with MAP estimate = -255.7201
expansions: [(0, 2), (7, 1), (20, 2), (21, 2), (22, 1), (23, 1), (24, 1), (34, 2), (42, 2), (43, 1), (44, 1), (49, 1), (50, 2), (51, 2), (52, 1), (74, 2), (75, 2), (76, 3), (77, 2), (84, 2)]
discards: [83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 261.7679 - loglik: -2.5753e+02 - logprior: -4.2376e+00
Epoch 2/2
19/19 - 3s - loss: 254.7228 - loglik: -2.5342e+02 - logprior: -1.3050e+00
Fitted a model with MAP estimate = -253.0090
expansions: [(54, 1)]
discards: [  1  24  44  71 114]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 256.7440 - loglik: -2.5382e+02 - logprior: -2.9278e+00
Epoch 2/2
19/19 - 3s - loss: 253.5675 - loglik: -2.5249e+02 - logprior: -1.0788e+00
Fitted a model with MAP estimate = -252.2536
expansions: []
discards: [25 94 97 98]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 108 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 255.9707 - loglik: -2.5307e+02 - logprior: -2.9004e+00
Epoch 2/10
19/19 - 3s - loss: 252.7954 - loglik: -2.5176e+02 - logprior: -1.0355e+00
Epoch 3/10
19/19 - 3s - loss: 252.3559 - loglik: -2.5152e+02 - logprior: -8.4071e-01
Epoch 4/10
19/19 - 3s - loss: 251.3514 - loglik: -2.5057e+02 - logprior: -7.8158e-01
Epoch 5/10
19/19 - 3s - loss: 251.1088 - loglik: -2.5036e+02 - logprior: -7.4614e-01
Epoch 6/10
19/19 - 3s - loss: 250.9811 - loglik: -2.5026e+02 - logprior: -7.2081e-01
Epoch 7/10
19/19 - 3s - loss: 250.9534 - loglik: -2.5023e+02 - logprior: -7.2151e-01
Epoch 8/10
19/19 - 3s - loss: 250.3836 - loglik: -2.4970e+02 - logprior: -6.8429e-01
Epoch 9/10
19/19 - 3s - loss: 250.7973 - loglik: -2.5017e+02 - logprior: -6.3017e-01
Fitted a model with MAP estimate = -250.1703
Time for alignment: 90.3054
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 304.5388 - loglik: -3.0153e+02 - logprior: -3.0109e+00
Epoch 2/10
19/19 - 2s - loss: 275.5319 - loglik: -2.7433e+02 - logprior: -1.1995e+00
Epoch 3/10
19/19 - 2s - loss: 262.5924 - loglik: -2.6147e+02 - logprior: -1.1247e+00
Epoch 4/10
19/19 - 2s - loss: 258.7780 - loglik: -2.5774e+02 - logprior: -1.0417e+00
Epoch 5/10
19/19 - 2s - loss: 258.0283 - loglik: -2.5704e+02 - logprior: -9.8836e-01
Epoch 6/10
19/19 - 2s - loss: 256.7757 - loglik: -2.5580e+02 - logprior: -9.7629e-01
Epoch 7/10
19/19 - 2s - loss: 256.6931 - loglik: -2.5574e+02 - logprior: -9.4831e-01
Epoch 8/10
19/19 - 2s - loss: 256.4460 - loglik: -2.5550e+02 - logprior: -9.4475e-01
Epoch 9/10
19/19 - 2s - loss: 256.4984 - loglik: -2.5556e+02 - logprior: -9.3857e-01
Fitted a model with MAP estimate = -255.5508
expansions: [(0, 2), (7, 1), (20, 3), (21, 2), (22, 1), (34, 2), (38, 2), (41, 3), (43, 2), (50, 1), (51, 1), (52, 1), (74, 2), (75, 2), (76, 3), (77, 2), (84, 2)]
discards: [83]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 115 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 261.4893 - loglik: -2.5730e+02 - logprior: -4.1868e+00
Epoch 2/2
19/19 - 3s - loss: 255.0770 - loglik: -2.5377e+02 - logprior: -1.3066e+00
Fitted a model with MAP estimate = -252.9946
expansions: []
discards: [  1  27  43 113]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 256.9712 - loglik: -2.5399e+02 - logprior: -2.9858e+00
Epoch 2/2
19/19 - 3s - loss: 253.3373 - loglik: -2.5226e+02 - logprior: -1.0765e+00
Fitted a model with MAP estimate = -252.1527
expansions: []
discards: [96]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 255.7597 - loglik: -2.5278e+02 - logprior: -2.9836e+00
Epoch 2/10
19/19 - 3s - loss: 252.7671 - loglik: -2.5173e+02 - logprior: -1.0392e+00
Epoch 3/10
19/19 - 3s - loss: 251.9365 - loglik: -2.5108e+02 - logprior: -8.5226e-01
Epoch 4/10
19/19 - 3s - loss: 251.6763 - loglik: -2.5087e+02 - logprior: -8.0452e-01
Epoch 5/10
19/19 - 3s - loss: 250.8888 - loglik: -2.5012e+02 - logprior: -7.7137e-01
Epoch 6/10
19/19 - 3s - loss: 251.0217 - loglik: -2.5028e+02 - logprior: -7.4466e-01
Fitted a model with MAP estimate = -250.6866
Time for alignment: 79.6109
Computed alignments with likelihoods: ['-250.7571', '-250.4224', '-250.4291', '-250.1703', '-250.6866']
Best model has likelihood: -250.1703  (prior= -0.5574 )
time for generating output: 0.1798
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.8677184676733244
Training of 5 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bcbe0e760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49caea8160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4daf617eb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4dd46aa310>
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 188.4951 - loglik: -1.8533e+02 - logprior: -3.1672e+00
Epoch 2/10
19/19 - 1s - loss: 155.8047 - loglik: -1.5434e+02 - logprior: -1.4631e+00
Epoch 3/10
19/19 - 1s - loss: 142.6176 - loglik: -1.4092e+02 - logprior: -1.7020e+00
Epoch 4/10
19/19 - 1s - loss: 138.7650 - loglik: -1.3715e+02 - logprior: -1.6150e+00
Epoch 5/10
19/19 - 1s - loss: 137.4387 - loglik: -1.3580e+02 - logprior: -1.6422e+00
Epoch 6/10
19/19 - 1s - loss: 137.0115 - loglik: -1.3541e+02 - logprior: -1.6056e+00
Epoch 7/10
19/19 - 1s - loss: 136.6238 - loglik: -1.3504e+02 - logprior: -1.5873e+00
Epoch 8/10
19/19 - 1s - loss: 136.5345 - loglik: -1.3496e+02 - logprior: -1.5792e+00
Epoch 9/10
19/19 - 1s - loss: 136.0586 - loglik: -1.3448e+02 - logprior: -1.5737e+00
Epoch 10/10
19/19 - 1s - loss: 135.8836 - loglik: -1.3431e+02 - logprior: -1.5698e+00
Fitted a model with MAP estimate = -135.9090
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (22, 2), (23, 1), (24, 1), (26, 1), (29, 1), (32, 1), (34, 1), (40, 1), (41, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.3799 - loglik: -1.3730e+02 - logprior: -4.0810e+00
Epoch 2/2
19/19 - 1s - loss: 131.7688 - loglik: -1.2979e+02 - logprior: -1.9801e+00
Fitted a model with MAP estimate = -130.0596
expansions: [(0, 2)]
discards: [ 0 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 131.6193 - loglik: -1.2860e+02 - logprior: -3.0162e+00
Epoch 2/2
19/19 - 1s - loss: 128.2575 - loglik: -1.2706e+02 - logprior: -1.1975e+00
Fitted a model with MAP estimate = -127.4191
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.7010 - loglik: -1.2993e+02 - logprior: -3.7682e+00
Epoch 2/10
19/19 - 1s - loss: 128.7964 - loglik: -1.2751e+02 - logprior: -1.2832e+00
Epoch 3/10
19/19 - 1s - loss: 127.6883 - loglik: -1.2659e+02 - logprior: -1.0934e+00
Epoch 4/10
19/19 - 1s - loss: 126.8457 - loglik: -1.2577e+02 - logprior: -1.0724e+00
Epoch 5/10
19/19 - 1s - loss: 126.2571 - loglik: -1.2519e+02 - logprior: -1.0677e+00
Epoch 6/10
19/19 - 1s - loss: 125.8613 - loglik: -1.2480e+02 - logprior: -1.0590e+00
Epoch 7/10
19/19 - 1s - loss: 125.6410 - loglik: -1.2459e+02 - logprior: -1.0483e+00
Epoch 8/10
19/19 - 1s - loss: 125.4874 - loglik: -1.2445e+02 - logprior: -1.0376e+00
Epoch 9/10
19/19 - 1s - loss: 125.4531 - loglik: -1.2443e+02 - logprior: -1.0265e+00
Epoch 10/10
19/19 - 1s - loss: 125.0807 - loglik: -1.2406e+02 - logprior: -1.0238e+00
Fitted a model with MAP estimate = -125.1504
Time for alignment: 50.5910
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 188.4474 - loglik: -1.8528e+02 - logprior: -3.1664e+00
Epoch 2/10
19/19 - 1s - loss: 155.8362 - loglik: -1.5438e+02 - logprior: -1.4533e+00
Epoch 3/10
19/19 - 1s - loss: 144.5445 - loglik: -1.4289e+02 - logprior: -1.6502e+00
Epoch 4/10
19/19 - 1s - loss: 140.2753 - loglik: -1.3871e+02 - logprior: -1.5629e+00
Epoch 5/10
19/19 - 1s - loss: 139.2437 - loglik: -1.3765e+02 - logprior: -1.5957e+00
Epoch 6/10
19/19 - 1s - loss: 138.5645 - loglik: -1.3701e+02 - logprior: -1.5521e+00
Epoch 7/10
19/19 - 1s - loss: 138.3122 - loglik: -1.3678e+02 - logprior: -1.5365e+00
Epoch 8/10
19/19 - 1s - loss: 137.9237 - loglik: -1.3640e+02 - logprior: -1.5280e+00
Epoch 9/10
19/19 - 1s - loss: 137.6187 - loglik: -1.3609e+02 - logprior: -1.5240e+00
Epoch 10/10
19/19 - 1s - loss: 137.7949 - loglik: -1.3627e+02 - logprior: -1.5213e+00
Fitted a model with MAP estimate = -137.4813
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (22, 2), (23, 1), (24, 1), (25, 1), (32, 2), (34, 1), (40, 1), (41, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 141.5089 - loglik: -1.3743e+02 - logprior: -4.0806e+00
Epoch 2/2
19/19 - 1s - loss: 131.6855 - loglik: -1.2971e+02 - logprior: -1.9794e+00
Fitted a model with MAP estimate = -130.1435
expansions: [(0, 2)]
discards: [ 0 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 131.6989 - loglik: -1.2868e+02 - logprior: -3.0139e+00
Epoch 2/2
19/19 - 1s - loss: 128.0823 - loglik: -1.2689e+02 - logprior: -1.1958e+00
Fitted a model with MAP estimate = -127.4238
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.6373 - loglik: -1.2986e+02 - logprior: -3.7726e+00
Epoch 2/10
19/19 - 1s - loss: 128.9044 - loglik: -1.2762e+02 - logprior: -1.2801e+00
Epoch 3/10
19/19 - 1s - loss: 127.7016 - loglik: -1.2661e+02 - logprior: -1.0909e+00
Epoch 4/10
19/19 - 1s - loss: 126.7837 - loglik: -1.2571e+02 - logprior: -1.0753e+00
Epoch 5/10
19/19 - 1s - loss: 126.4416 - loglik: -1.2538e+02 - logprior: -1.0660e+00
Epoch 6/10
19/19 - 1s - loss: 125.8659 - loglik: -1.2481e+02 - logprior: -1.0546e+00
Epoch 7/10
19/19 - 1s - loss: 125.4898 - loglik: -1.2444e+02 - logprior: -1.0500e+00
Epoch 8/10
19/19 - 1s - loss: 125.3374 - loglik: -1.2430e+02 - logprior: -1.0423e+00
Epoch 9/10
19/19 - 1s - loss: 125.5285 - loglik: -1.2451e+02 - logprior: -1.0235e+00
Fitted a model with MAP estimate = -125.2219
Time for alignment: 48.7926
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 188.7487 - loglik: -1.8558e+02 - logprior: -3.1653e+00
Epoch 2/10
19/19 - 1s - loss: 157.0327 - loglik: -1.5560e+02 - logprior: -1.4313e+00
Epoch 3/10
19/19 - 1s - loss: 144.3683 - loglik: -1.4270e+02 - logprior: -1.6698e+00
Epoch 4/10
19/19 - 1s - loss: 140.0639 - loglik: -1.3848e+02 - logprior: -1.5810e+00
Epoch 5/10
19/19 - 1s - loss: 138.8092 - loglik: -1.3720e+02 - logprior: -1.6077e+00
Epoch 6/10
19/19 - 1s - loss: 138.3656 - loglik: -1.3680e+02 - logprior: -1.5650e+00
Epoch 7/10
19/19 - 1s - loss: 137.9277 - loglik: -1.3638e+02 - logprior: -1.5438e+00
Epoch 8/10
19/19 - 1s - loss: 137.6840 - loglik: -1.3615e+02 - logprior: -1.5335e+00
Epoch 9/10
19/19 - 1s - loss: 137.8094 - loglik: -1.3628e+02 - logprior: -1.5290e+00
Fitted a model with MAP estimate = -137.3478
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (22, 2), (23, 1), (24, 1), (27, 1), (32, 2), (34, 1), (40, 1), (41, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 141.4092 - loglik: -1.3733e+02 - logprior: -4.0825e+00
Epoch 2/2
19/19 - 1s - loss: 131.6738 - loglik: -1.2970e+02 - logprior: -1.9697e+00
Fitted a model with MAP estimate = -130.0958
expansions: [(0, 2)]
discards: [ 0 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 131.7029 - loglik: -1.2869e+02 - logprior: -3.0120e+00
Epoch 2/2
19/19 - 1s - loss: 128.2284 - loglik: -1.2704e+02 - logprior: -1.1905e+00
Fitted a model with MAP estimate = -127.4663
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.6248 - loglik: -1.2985e+02 - logprior: -3.7730e+00
Epoch 2/10
19/19 - 1s - loss: 128.7962 - loglik: -1.2750e+02 - logprior: -1.2913e+00
Epoch 3/10
19/19 - 1s - loss: 127.5911 - loglik: -1.2650e+02 - logprior: -1.0861e+00
Epoch 4/10
19/19 - 1s - loss: 127.0033 - loglik: -1.2593e+02 - logprior: -1.0780e+00
Epoch 5/10
19/19 - 1s - loss: 126.4131 - loglik: -1.2535e+02 - logprior: -1.0659e+00
Epoch 6/10
19/19 - 1s - loss: 125.7639 - loglik: -1.2470e+02 - logprior: -1.0608e+00
Epoch 7/10
19/19 - 1s - loss: 125.7843 - loglik: -1.2473e+02 - logprior: -1.0508e+00
Fitted a model with MAP estimate = -125.4569
Time for alignment: 53.2817
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 188.5746 - loglik: -1.8541e+02 - logprior: -3.1656e+00
Epoch 2/10
19/19 - 1s - loss: 156.6498 - loglik: -1.5520e+02 - logprior: -1.4496e+00
Epoch 3/10
19/19 - 1s - loss: 143.4585 - loglik: -1.4176e+02 - logprior: -1.6952e+00
Epoch 4/10
19/19 - 1s - loss: 139.0868 - loglik: -1.3747e+02 - logprior: -1.6187e+00
Epoch 5/10
19/19 - 1s - loss: 138.0628 - loglik: -1.3641e+02 - logprior: -1.6503e+00
Epoch 6/10
19/19 - 1s - loss: 137.1238 - loglik: -1.3552e+02 - logprior: -1.6087e+00
Epoch 7/10
19/19 - 1s - loss: 137.1040 - loglik: -1.3551e+02 - logprior: -1.5903e+00
Epoch 8/10
19/19 - 1s - loss: 136.5734 - loglik: -1.3499e+02 - logprior: -1.5805e+00
Epoch 9/10
19/19 - 1s - loss: 136.6229 - loglik: -1.3505e+02 - logprior: -1.5756e+00
Fitted a model with MAP estimate = -136.3246
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (22, 2), (23, 1), (24, 1), (26, 1), (29, 1), (32, 1), (33, 1), (40, 1), (41, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 141.5041 - loglik: -1.3743e+02 - logprior: -4.0787e+00
Epoch 2/2
19/19 - 1s - loss: 131.7455 - loglik: -1.2977e+02 - logprior: -1.9718e+00
Fitted a model with MAP estimate = -130.0742
expansions: [(0, 2)]
discards: [ 0 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 131.5829 - loglik: -1.2856e+02 - logprior: -3.0196e+00
Epoch 2/2
19/19 - 1s - loss: 128.2329 - loglik: -1.2704e+02 - logprior: -1.1963e+00
Fitted a model with MAP estimate = -127.4344
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.6272 - loglik: -1.2986e+02 - logprior: -3.7664e+00
Epoch 2/10
19/19 - 1s - loss: 128.6452 - loglik: -1.2736e+02 - logprior: -1.2881e+00
Epoch 3/10
19/19 - 1s - loss: 127.7543 - loglik: -1.2666e+02 - logprior: -1.0947e+00
Epoch 4/10
19/19 - 1s - loss: 126.8939 - loglik: -1.2582e+02 - logprior: -1.0749e+00
Epoch 5/10
19/19 - 1s - loss: 126.2172 - loglik: -1.2514e+02 - logprior: -1.0728e+00
Epoch 6/10
19/19 - 1s - loss: 126.0877 - loglik: -1.2503e+02 - logprior: -1.0574e+00
Epoch 7/10
19/19 - 1s - loss: 125.4948 - loglik: -1.2444e+02 - logprior: -1.0555e+00
Epoch 8/10
19/19 - 1s - loss: 125.2896 - loglik: -1.2425e+02 - logprior: -1.0388e+00
Epoch 9/10
19/19 - 1s - loss: 125.4039 - loglik: -1.2437e+02 - logprior: -1.0296e+00
Fitted a model with MAP estimate = -125.1898
Time for alignment: 46.3769
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 188.6092 - loglik: -1.8544e+02 - logprior: -3.1652e+00
Epoch 2/10
19/19 - 1s - loss: 156.2166 - loglik: -1.5476e+02 - logprior: -1.4600e+00
Epoch 3/10
19/19 - 1s - loss: 142.7203 - loglik: -1.4102e+02 - logprior: -1.7051e+00
Epoch 4/10
19/19 - 1s - loss: 139.1493 - loglik: -1.3752e+02 - logprior: -1.6266e+00
Epoch 5/10
19/19 - 1s - loss: 137.7306 - loglik: -1.3608e+02 - logprior: -1.6506e+00
Epoch 6/10
19/19 - 1s - loss: 137.0459 - loglik: -1.3544e+02 - logprior: -1.6108e+00
Epoch 7/10
19/19 - 1s - loss: 137.0861 - loglik: -1.3550e+02 - logprior: -1.5901e+00
Fitted a model with MAP estimate = -136.6501
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (22, 2), (23, 1), (24, 1), (26, 1), (29, 1), (32, 1), (33, 1), (40, 1), (41, 1), (44, 1), (45, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.6417 - loglik: -1.3757e+02 - logprior: -4.0761e+00
Epoch 2/2
19/19 - 1s - loss: 131.9719 - loglik: -1.3003e+02 - logprior: -1.9397e+00
Fitted a model with MAP estimate = -130.0265
expansions: [(0, 2)]
discards: [ 0 25]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 131.8101 - loglik: -1.2879e+02 - logprior: -3.0173e+00
Epoch 2/2
19/19 - 1s - loss: 128.1236 - loglik: -1.2692e+02 - logprior: -1.2000e+00
Fitted a model with MAP estimate = -127.4797
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.5922 - loglik: -1.2982e+02 - logprior: -3.7746e+00
Epoch 2/10
19/19 - 1s - loss: 128.8882 - loglik: -1.2760e+02 - logprior: -1.2890e+00
Epoch 3/10
19/19 - 1s - loss: 127.6633 - loglik: -1.2657e+02 - logprior: -1.0947e+00
Epoch 4/10
19/19 - 1s - loss: 126.9952 - loglik: -1.2592e+02 - logprior: -1.0759e+00
Epoch 5/10
19/19 - 1s - loss: 126.2642 - loglik: -1.2519e+02 - logprior: -1.0706e+00
Epoch 6/10
19/19 - 1s - loss: 125.9678 - loglik: -1.2490e+02 - logprior: -1.0629e+00
Epoch 7/10
19/19 - 1s - loss: 125.4462 - loglik: -1.2440e+02 - logprior: -1.0479e+00
Epoch 8/10
19/19 - 1s - loss: 125.6985 - loglik: -1.2466e+02 - logprior: -1.0365e+00
Fitted a model with MAP estimate = -125.3197
Time for alignment: 43.8382
Computed alignments with likelihoods: ['-125.1504', '-125.2219', '-125.4569', '-125.1898', '-125.3197']
Best model has likelihood: -125.1504  (prior= -0.9738 )
time for generating output: 0.0962
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.9752413963852439
Training of 5 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c0b1536a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bd50d2fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d85303970>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d966aab80>
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 735.5483 - loglik: -7.3388e+02 - logprior: -1.6701e+00
Epoch 2/10
39/39 - 26s - loss: 618.2335 - loglik: -6.1653e+02 - logprior: -1.7056e+00
Epoch 3/10
39/39 - 27s - loss: 606.8987 - loglik: -6.0515e+02 - logprior: -1.7524e+00
Epoch 4/10
39/39 - 28s - loss: 603.9152 - loglik: -6.0221e+02 - logprior: -1.7005e+00
Epoch 5/10
39/39 - 29s - loss: 602.4479 - loglik: -6.0074e+02 - logprior: -1.7081e+00
Epoch 6/10
39/39 - 29s - loss: 601.3480 - loglik: -5.9964e+02 - logprior: -1.7110e+00
Epoch 7/10
39/39 - 29s - loss: 600.9071 - loglik: -5.9919e+02 - logprior: -1.7163e+00
Epoch 8/10
39/39 - 30s - loss: 600.7311 - loglik: -5.9901e+02 - logprior: -1.7193e+00
Epoch 9/10
39/39 - 31s - loss: 600.6528 - loglik: -5.9893e+02 - logprior: -1.7182e+00
Epoch 10/10
39/39 - 32s - loss: 600.0972 - loglik: -5.9838e+02 - logprior: -1.7152e+00
Fitted a model with MAP estimate = -598.9947
expansions: [(11, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (20, 1), (33, 1), (34, 1), (35, 1), (37, 1), (39, 1), (47, 1), (52, 1), (71, 3), (72, 1), (73, 1), (79, 1), (103, 2), (116, 1), (117, 1), (118, 3), (133, 1), (137, 1), (138, 1), (148, 1), (150, 1), (154, 2), (165, 1), (166, 3), (167, 2), (175, 1), (178, 11), (187, 3), (193, 3), (194, 1), (197, 1), (212, 1), (213, 3), (214, 1), (227, 1), (228, 1), (230, 1), (231, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 593.7308 - loglik: -5.9116e+02 - logprior: -2.5709e+00
Epoch 2/2
39/39 - 44s - loss: 578.2624 - loglik: -5.7682e+02 - logprior: -1.4471e+00
Fitted a model with MAP estimate = -573.3463
expansions: [(0, 3), (52, 2)]
discards: [  0  49  50 123 142 188 219 220]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 580.3958 - loglik: -5.7884e+02 - logprior: -1.5607e+00
Epoch 2/2
39/39 - 41s - loss: 575.1489 - loglik: -5.7467e+02 - logprior: -4.8082e-01
Fitted a model with MAP estimate = -571.9782
expansions: []
discards: [  0   2 217]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 306 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 580.5908 - loglik: -5.7844e+02 - logprior: -2.1550e+00
Epoch 2/10
39/39 - 47s - loss: 574.7069 - loglik: -5.7447e+02 - logprior: -2.4171e-01
Epoch 3/10
39/39 - 46s - loss: 571.5754 - loglik: -5.7148e+02 - logprior: -9.3055e-02
Epoch 4/10
39/39 - 46s - loss: 569.7271 - loglik: -5.6974e+02 - logprior: 0.0152
Epoch 5/10
39/39 - 46s - loss: 569.5229 - loglik: -5.6959e+02 - logprior: 0.0635
Epoch 6/10
39/39 - 43s - loss: 568.2047 - loglik: -5.6841e+02 - logprior: 0.2036
Epoch 7/10
39/39 - 42s - loss: 567.9012 - loglik: -5.6828e+02 - logprior: 0.3767
Epoch 8/10
39/39 - 41s - loss: 567.5822 - loglik: -5.6804e+02 - logprior: 0.4538
Epoch 9/10
39/39 - 40s - loss: 567.6607 - loglik: -5.6824e+02 - logprior: 0.5829
Fitted a model with MAP estimate = -567.1532
Time for alignment: 1061.1384
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 738.0466 - loglik: -7.3639e+02 - logprior: -1.6562e+00
Epoch 2/10
39/39 - 27s - loss: 618.9177 - loglik: -6.1723e+02 - logprior: -1.6832e+00
Epoch 3/10
39/39 - 27s - loss: 606.5854 - loglik: -6.0486e+02 - logprior: -1.7209e+00
Epoch 4/10
39/39 - 29s - loss: 603.5325 - loglik: -6.0185e+02 - logprior: -1.6805e+00
Epoch 5/10
39/39 - 30s - loss: 601.5714 - loglik: -5.9990e+02 - logprior: -1.6692e+00
Epoch 6/10
39/39 - 31s - loss: 600.4321 - loglik: -5.9875e+02 - logprior: -1.6821e+00
Epoch 7/10
39/39 - 32s - loss: 599.9619 - loglik: -5.9828e+02 - logprior: -1.6834e+00
Epoch 8/10
39/39 - 31s - loss: 599.6924 - loglik: -5.9801e+02 - logprior: -1.6815e+00
Epoch 9/10
39/39 - 30s - loss: 599.8257 - loglik: -5.9815e+02 - logprior: -1.6801e+00
Fitted a model with MAP estimate = -598.1523
expansions: [(11, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (19, 1), (24, 1), (32, 1), (35, 1), (37, 1), (39, 1), (47, 1), (52, 1), (71, 3), (72, 1), (73, 1), (79, 1), (87, 1), (102, 1), (115, 1), (116, 1), (117, 1), (118, 1), (120, 6), (130, 3), (136, 1), (137, 1), (147, 1), (148, 1), (149, 1), (154, 1), (163, 1), (164, 10), (173, 2), (186, 5), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 1), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [  0 174]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 589.8798 - loglik: -5.8734e+02 - logprior: -2.5360e+00
Epoch 2/2
39/39 - 38s - loss: 571.9373 - loglik: -5.7059e+02 - logprior: -1.3424e+00
Fitted a model with MAP estimate = -566.1043
expansions: [(163, 1), (210, 1), (211, 1), (227, 1), (241, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 571.5004 - loglik: -5.6998e+02 - logprior: -1.5232e+00
Epoch 2/2
39/39 - 40s - loss: 566.3632 - loglik: -5.6605e+02 - logprior: -3.1466e-01
Fitted a model with MAP estimate = -563.4035
expansions: []
discards: [229 230]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 317 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 569.7343 - loglik: -5.6842e+02 - logprior: -1.3184e+00
Epoch 2/10
39/39 - 39s - loss: 566.3842 - loglik: -5.6628e+02 - logprior: -1.0922e-01
Epoch 3/10
39/39 - 38s - loss: 563.8688 - loglik: -5.6387e+02 - logprior: 0.0017
Epoch 4/10
39/39 - 38s - loss: 560.9022 - loglik: -5.6098e+02 - logprior: 0.0731
Epoch 5/10
39/39 - 40s - loss: 561.1506 - loglik: -5.6136e+02 - logprior: 0.2082
Fitted a model with MAP estimate = -559.9462
Time for alignment: 808.9628
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 736.9890 - loglik: -7.3532e+02 - logprior: -1.6657e+00
Epoch 2/10
39/39 - 26s - loss: 617.9292 - loglik: -6.1620e+02 - logprior: -1.7302e+00
Epoch 3/10
39/39 - 27s - loss: 604.8585 - loglik: -6.0307e+02 - logprior: -1.7889e+00
Epoch 4/10
39/39 - 27s - loss: 601.8181 - loglik: -6.0008e+02 - logprior: -1.7379e+00
Epoch 5/10
39/39 - 26s - loss: 600.1741 - loglik: -5.9845e+02 - logprior: -1.7215e+00
Epoch 6/10
39/39 - 26s - loss: 599.1061 - loglik: -5.9739e+02 - logprior: -1.7130e+00
Epoch 7/10
39/39 - 25s - loss: 598.9998 - loglik: -5.9730e+02 - logprior: -1.7042e+00
Epoch 8/10
39/39 - 25s - loss: 598.6954 - loglik: -5.9700e+02 - logprior: -1.6979e+00
Epoch 9/10
39/39 - 24s - loss: 598.4265 - loglik: -5.9674e+02 - logprior: -1.6861e+00
Epoch 10/10
39/39 - 24s - loss: 597.8499 - loglik: -5.9616e+02 - logprior: -1.6850e+00
Fitted a model with MAP estimate = -597.0379
expansions: [(11, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (20, 1), (26, 1), (32, 1), (35, 1), (37, 1), (39, 1), (47, 1), (52, 1), (71, 3), (72, 1), (73, 1), (79, 1), (94, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 5), (130, 3), (136, 1), (137, 1), (138, 1), (147, 1), (154, 2), (163, 1), (164, 10), (173, 2), (177, 1), (187, 1), (189, 1), (190, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 1), (226, 1), (227, 2), (231, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 591.0140 - loglik: -5.8850e+02 - logprior: -2.5177e+00
Epoch 2/2
39/39 - 40s - loss: 574.0422 - loglik: -5.7267e+02 - logprior: -1.3694e+00
Fitted a model with MAP estimate = -568.5674
expansions: [(0, 3), (146, 1), (147, 1), (161, 1), (208, 1), (209, 1)]
discards: [  0 193 294]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 317 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 573.7272 - loglik: -5.7224e+02 - logprior: -1.4895e+00
Epoch 2/2
39/39 - 38s - loss: 568.0173 - loglik: -5.6755e+02 - logprior: -4.7177e-01
Fitted a model with MAP estimate = -565.0959
expansions: []
discards: [  0   2 229]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 314 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 573.6484 - loglik: -5.7159e+02 - logprior: -2.0604e+00
Epoch 2/10
39/39 - 40s - loss: 567.4807 - loglik: -5.6735e+02 - logprior: -1.3074e-01
Epoch 3/10
39/39 - 41s - loss: 565.1123 - loglik: -5.6511e+02 - logprior: -5.6081e-03
Epoch 4/10
39/39 - 40s - loss: 562.1865 - loglik: -5.6230e+02 - logprior: 0.1157
Epoch 5/10
39/39 - 39s - loss: 562.0280 - loglik: -5.6220e+02 - logprior: 0.1747
Epoch 6/10
39/39 - 38s - loss: 561.3540 - loglik: -5.6158e+02 - logprior: 0.2301
Epoch 7/10
39/39 - 37s - loss: 560.4854 - loglik: -5.6084e+02 - logprior: 0.3558
Epoch 8/10
39/39 - 38s - loss: 560.3449 - loglik: -5.6082e+02 - logprior: 0.4778
Epoch 9/10
39/39 - 38s - loss: 560.4905 - loglik: -5.6106e+02 - logprior: 0.5679
Fitted a model with MAP estimate = -560.0789
Time for alignment: 944.0385
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 738.3556 - loglik: -7.3670e+02 - logprior: -1.6540e+00
Epoch 2/10
39/39 - 29s - loss: 618.9377 - loglik: -6.1723e+02 - logprior: -1.7115e+00
Epoch 3/10
39/39 - 30s - loss: 607.5530 - loglik: -6.0582e+02 - logprior: -1.7326e+00
Epoch 4/10
39/39 - 32s - loss: 604.5133 - loglik: -6.0285e+02 - logprior: -1.6594e+00
Epoch 5/10
39/39 - 33s - loss: 602.8597 - loglik: -6.0123e+02 - logprior: -1.6307e+00
Epoch 6/10
39/39 - 34s - loss: 601.9011 - loglik: -6.0027e+02 - logprior: -1.6345e+00
Epoch 7/10
39/39 - 34s - loss: 601.5290 - loglik: -5.9989e+02 - logprior: -1.6367e+00
Epoch 8/10
39/39 - 34s - loss: 601.0161 - loglik: -5.9938e+02 - logprior: -1.6347e+00
Epoch 9/10
39/39 - 34s - loss: 600.3620 - loglik: -5.9873e+02 - logprior: -1.6290e+00
Epoch 10/10
39/39 - 31s - loss: 600.5573 - loglik: -5.9893e+02 - logprior: -1.6317e+00
Fitted a model with MAP estimate = -599.4268
expansions: [(11, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (19, 1), (24, 1), (34, 1), (35, 1), (37, 1), (42, 2), (53, 1), (54, 1), (71, 3), (72, 1), (78, 1), (80, 1), (103, 1), (116, 1), (117, 1), (118, 1), (122, 5), (131, 2), (133, 1), (137, 1), (138, 2), (139, 1), (152, 1), (154, 2), (165, 1), (166, 4), (175, 1), (178, 6), (187, 3), (193, 3), (194, 1), (197, 1), (212, 1), (213, 3), (214, 1), (227, 1), (228, 1), (230, 1), (231, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 312 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 591.0018 - loglik: -5.8854e+02 - logprior: -2.4618e+00
Epoch 2/2
39/39 - 44s - loss: 573.9561 - loglik: -5.7262e+02 - logprior: -1.3346e+00
Fitted a model with MAP estimate = -568.9644
expansions: [(0, 3), (147, 1), (148, 1), (162, 1), (209, 1), (225, 1), (227, 1)]
discards: [  0  52 194]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 318 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 573.6927 - loglik: -5.7226e+02 - logprior: -1.4284e+00
Epoch 2/2
39/39 - 41s - loss: 568.0720 - loglik: -5.6770e+02 - logprior: -3.7522e-01
Fitted a model with MAP estimate = -565.1078
expansions: []
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 316 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 573.9288 - loglik: -5.7177e+02 - logprior: -2.1546e+00
Epoch 2/10
39/39 - 46s - loss: 568.2348 - loglik: -5.6798e+02 - logprior: -2.5694e-01
Epoch 3/10
39/39 - 48s - loss: 564.6863 - loglik: -5.6475e+02 - logprior: 0.0683
Epoch 4/10
39/39 - 42s - loss: 562.8595 - loglik: -5.6303e+02 - logprior: 0.1703
Epoch 5/10
39/39 - 38s - loss: 561.5490 - loglik: -5.6181e+02 - logprior: 0.2611
Epoch 6/10
39/39 - 38s - loss: 560.9481 - loglik: -5.6129e+02 - logprior: 0.3414
Epoch 7/10
39/39 - 39s - loss: 560.6506 - loglik: -5.6111e+02 - logprior: 0.4609
Epoch 8/10
39/39 - 43s - loss: 560.0388 - loglik: -5.6058e+02 - logprior: 0.5392
Epoch 9/10
39/39 - 49s - loss: 560.0057 - loglik: -5.6068e+02 - logprior: 0.6701
Epoch 10/10
39/39 - 52s - loss: 560.1119 - loglik: -5.6087e+02 - logprior: 0.7541
Fitted a model with MAP estimate = -559.2517
Time for alignment: 1135.8284
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 738.7790 - loglik: -7.3712e+02 - logprior: -1.6585e+00
Epoch 2/10
39/39 - 30s - loss: 620.5109 - loglik: -6.1884e+02 - logprior: -1.6731e+00
Epoch 3/10
39/39 - 31s - loss: 606.9736 - loglik: -6.0523e+02 - logprior: -1.7470e+00
Epoch 4/10
39/39 - 31s - loss: 604.0317 - loglik: -6.0233e+02 - logprior: -1.7014e+00
Epoch 5/10
39/39 - 33s - loss: 602.0895 - loglik: -6.0041e+02 - logprior: -1.6833e+00
Epoch 6/10
39/39 - 34s - loss: 601.7280 - loglik: -6.0005e+02 - logprior: -1.6786e+00
Epoch 7/10
39/39 - 34s - loss: 601.0995 - loglik: -5.9943e+02 - logprior: -1.6648e+00
Epoch 8/10
39/39 - 34s - loss: 600.6162 - loglik: -5.9895e+02 - logprior: -1.6640e+00
Epoch 9/10
39/39 - 35s - loss: 600.3360 - loglik: -5.9868e+02 - logprior: -1.6586e+00
Epoch 10/10
39/39 - 34s - loss: 600.2210 - loglik: -5.9856e+02 - logprior: -1.6601e+00
Fitted a model with MAP estimate = -598.9798
expansions: [(11, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (19, 1), (33, 1), (34, 1), (35, 1), (37, 1), (39, 1), (47, 1), (52, 1), (53, 1), (70, 3), (71, 1), (79, 1), (103, 2), (116, 1), (117, 1), (118, 1), (119, 1), (121, 5), (131, 2), (133, 1), (137, 1), (139, 1), (148, 1), (152, 1), (154, 2), (165, 10), (174, 1), (175, 1), (186, 1), (187, 1), (193, 3), (194, 1), (197, 1), (211, 1), (212, 1), (214, 3), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 592.9485 - loglik: -5.9039e+02 - logprior: -2.5593e+00
Epoch 2/2
39/39 - 41s - loss: 575.8685 - loglik: -5.7446e+02 - logprior: -1.4106e+00
Fitted a model with MAP estimate = -571.0390
expansions: [(0, 3), (52, 2), (149, 1), (163, 1), (209, 1), (210, 1), (276, 1)]
discards: [  0  50 123 194]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 316 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 53s - loss: 575.4181 - loglik: -5.7387e+02 - logprior: -1.5507e+00
Epoch 2/2
39/39 - 43s - loss: 568.7621 - loglik: -5.6832e+02 - logprior: -4.4484e-01
Fitted a model with MAP estimate = -565.5029
expansions: [(233, 1)]
discards: [0 1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 315 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 574.0807 - loglik: -5.7193e+02 - logprior: -2.1473e+00
Epoch 2/10
39/39 - 42s - loss: 567.6944 - loglik: -5.6740e+02 - logprior: -2.9287e-01
Epoch 3/10
39/39 - 47s - loss: 564.6725 - loglik: -5.6467e+02 - logprior: 0.0017
Epoch 4/10
39/39 - 49s - loss: 562.4194 - loglik: -5.6251e+02 - logprior: 0.0888
Epoch 5/10
39/39 - 44s - loss: 561.9624 - loglik: -5.6215e+02 - logprior: 0.1846
Epoch 6/10
39/39 - 40s - loss: 561.3339 - loglik: -5.6160e+02 - logprior: 0.2694
Epoch 7/10
39/39 - 39s - loss: 560.3525 - loglik: -5.6073e+02 - logprior: 0.3778
Epoch 8/10
39/39 - 41s - loss: 560.6011 - loglik: -5.6108e+02 - logprior: 0.4788
Fitted a model with MAP estimate = -559.7311
Time for alignment: 1051.6372
Computed alignments with likelihoods: ['-567.1532', '-559.9462', '-560.0789', '-559.2517', '-559.7311']
Best model has likelihood: -559.2517  (prior= 0.8216 )
time for generating output: 0.3507
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.07684112398455187
Training of 5 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4a4c144d90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4c0a7beaf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d6bfafbb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4a8430f430>
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 62s - loss: 990.1102 - loglik: -9.8878e+02 - logprior: -1.3262e+00
Epoch 2/10
39/39 - 67s - loss: 821.6544 - loglik: -8.2045e+02 - logprior: -1.2076e+00
Epoch 3/10
39/39 - 69s - loss: 804.5958 - loglik: -8.0324e+02 - logprior: -1.3518e+00
Epoch 4/10
39/39 - 67s - loss: 800.5719 - loglik: -7.9926e+02 - logprior: -1.3121e+00
Epoch 5/10
39/39 - 61s - loss: 799.4056 - loglik: -7.9815e+02 - logprior: -1.2601e+00
Epoch 6/10
39/39 - 66s - loss: 798.6025 - loglik: -7.9737e+02 - logprior: -1.2331e+00
Epoch 7/10
39/39 - 68s - loss: 797.5541 - loglik: -7.9634e+02 - logprior: -1.2130e+00
Epoch 8/10
39/39 - 70s - loss: 796.8687 - loglik: -7.9565e+02 - logprior: -1.2205e+00
Epoch 9/10
39/39 - 70s - loss: 796.7259 - loglik: -7.9550e+02 - logprior: -1.2258e+00
Epoch 10/10
39/39 - 69s - loss: 796.0875 - loglik: -7.9489e+02 - logprior: -1.1983e+00
Fitted a model with MAP estimate = -787.8055
expansions: [(0, 2), (6, 2), (16, 1), (44, 1), (51, 1), (58, 1), (63, 1), (64, 1), (70, 1), (71, 1), (73, 2), (75, 1), (122, 2), (124, 1), (133, 1), (144, 1), (146, 3), (147, 1), (148, 1), (164, 1), (170, 1), (171, 1), (172, 1), (178, 1), (193, 2), (196, 1), (197, 1), (200, 1), (205, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (247, 1), (248, 1), (251, 1), (256, 1), (257, 1), (258, 1), (260, 1), (261, 1), (262, 3), (263, 2), (266, 2), (281, 1), (284, 2), (285, 2), (286, 8), (295, 1), (296, 1), (297, 2), (309, 1), (313, 1), (314, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 94s - loss: 780.5075 - loglik: -7.7846e+02 - logprior: -2.0477e+00
Epoch 2/2
39/39 - 91s - loss: 762.8210 - loglik: -7.6236e+02 - logprior: -4.5657e-01
Fitted a model with MAP estimate = -752.8410
expansions: [(0, 2), (348, 1)]
discards: [  1   2   8 138 170 223 315 322 342 343 344 370]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 87s - loss: 768.0053 - loglik: -7.6608e+02 - logprior: -1.9267e+00
Epoch 2/2
39/39 - 77s - loss: 763.2426 - loglik: -7.6324e+02 - logprior: -7.5208e-03
Fitted a model with MAP estimate = -753.9802
expansions: [(0, 2)]
discards: [  1   2   4   5 333 334 335]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 385 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 92s - loss: 761.7494 - loglik: -7.5971e+02 - logprior: -2.0386e+00
Epoch 2/10
39/39 - 95s - loss: 756.7301 - loglik: -7.5687e+02 - logprior: 0.1412
Epoch 3/10
39/39 - 97s - loss: 755.9151 - loglik: -7.5626e+02 - logprior: 0.3412
Epoch 4/10
39/39 - 91s - loss: 752.7399 - loglik: -7.5319e+02 - logprior: 0.4547
Epoch 5/10
39/39 - 90s - loss: 752.8726 - loglik: -7.5336e+02 - logprior: 0.4913
Fitted a model with MAP estimate = -752.2820
Time for alignment: 1865.8188
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 988.8713 - loglik: -9.8754e+02 - logprior: -1.3292e+00
Epoch 2/10
39/39 - 68s - loss: 819.9880 - loglik: -8.1890e+02 - logprior: -1.0920e+00
Epoch 3/10
39/39 - 69s - loss: 804.5519 - loglik: -8.0333e+02 - logprior: -1.2182e+00
Epoch 4/10
39/39 - 62s - loss: 801.3828 - loglik: -8.0022e+02 - logprior: -1.1659e+00
Epoch 5/10
39/39 - 54s - loss: 800.2207 - loglik: -7.9909e+02 - logprior: -1.1311e+00
Epoch 6/10
39/39 - 56s - loss: 799.3770 - loglik: -7.9824e+02 - logprior: -1.1408e+00
Epoch 7/10
39/39 - 65s - loss: 798.2717 - loglik: -7.9709e+02 - logprior: -1.1827e+00
Epoch 8/10
39/39 - 62s - loss: 797.8882 - loglik: -7.9674e+02 - logprior: -1.1508e+00
Epoch 9/10
39/39 - 51s - loss: 796.8159 - loglik: -7.9561e+02 - logprior: -1.2078e+00
Epoch 10/10
39/39 - 49s - loss: 796.0786 - loglik: -7.9486e+02 - logprior: -1.2229e+00
Fitted a model with MAP estimate = -787.0680
expansions: [(0, 2), (6, 2), (20, 1), (42, 1), (56, 2), (58, 1), (63, 1), (64, 1), (69, 1), (70, 2), (72, 2), (98, 1), (121, 1), (133, 1), (144, 1), (146, 3), (147, 1), (167, 1), (170, 1), (171, 1), (172, 1), (176, 1), (193, 2), (194, 1), (195, 1), (197, 1), (199, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (248, 1), (249, 1), (250, 1), (256, 1), (257, 1), (258, 2), (259, 1), (260, 1), (262, 1), (263, 1), (267, 2), (282, 1), (284, 2), (285, 2), (286, 8), (295, 1), (296, 1), (297, 2), (309, 1), (313, 1), (314, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 395 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 93s - loss: 778.2762 - loglik: -7.7672e+02 - logprior: -1.5550e+00
Epoch 2/2
39/39 - 77s - loss: 762.7681 - loglik: -7.6249e+02 - logprior: -2.7545e-01
Fitted a model with MAP estimate = -752.9828
expansions: [(0, 2), (346, 1), (347, 2)]
discards: [  1   7  62 224 319 336 339 340 341 342 366]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 97s - loss: 767.7448 - loglik: -7.6628e+02 - logprior: -1.4647e+00
Epoch 2/2
39/39 - 96s - loss: 763.7350 - loglik: -7.6371e+02 - logprior: -2.6112e-02
Fitted a model with MAP estimate = -754.2764
expansions: [(0, 2)]
discards: [  1   2 340]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 388 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 97s - loss: 759.2121 - loglik: -7.5759e+02 - logprior: -1.6237e+00
Epoch 2/10
39/39 - 98s - loss: 755.5853 - loglik: -7.5571e+02 - logprior: 0.1205
Epoch 3/10
39/39 - 82s - loss: 754.0982 - loglik: -7.5439e+02 - logprior: 0.2932
Epoch 4/10
39/39 - 79s - loss: 752.1182 - loglik: -7.5252e+02 - logprior: 0.4017
Epoch 5/10
39/39 - 92s - loss: 751.2629 - loglik: -7.5175e+02 - logprior: 0.4891
Epoch 6/10
39/39 - 76s - loss: 751.2213 - loglik: -7.5180e+02 - logprior: 0.5766
Epoch 7/10
39/39 - 69s - loss: 750.5741 - loglik: -7.5123e+02 - logprior: 0.6577
Epoch 8/10
39/39 - 74s - loss: 751.4488 - loglik: -7.5224e+02 - logprior: 0.7887
Fitted a model with MAP estimate = -750.1397
Time for alignment: 2061.3915
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 989.0077 - loglik: -9.8766e+02 - logprior: -1.3504e+00
Epoch 2/10
39/39 - 56s - loss: 819.6942 - loglik: -8.1847e+02 - logprior: -1.2246e+00
Epoch 3/10
39/39 - 51s - loss: 805.1776 - loglik: -8.0391e+02 - logprior: -1.2720e+00
Epoch 4/10
39/39 - 50s - loss: 802.0993 - loglik: -8.0083e+02 - logprior: -1.2654e+00
Epoch 5/10
39/39 - 56s - loss: 799.9922 - loglik: -7.9875e+02 - logprior: -1.2424e+00
Epoch 6/10
39/39 - 62s - loss: 799.5060 - loglik: -7.9828e+02 - logprior: -1.2287e+00
Epoch 7/10
39/39 - 66s - loss: 798.7219 - loglik: -7.9744e+02 - logprior: -1.2821e+00
Epoch 8/10
39/39 - 68s - loss: 797.5782 - loglik: -7.9635e+02 - logprior: -1.2250e+00
Epoch 9/10
39/39 - 63s - loss: 797.8468 - loglik: -7.9664e+02 - logprior: -1.2107e+00
Fitted a model with MAP estimate = -788.6527
expansions: [(0, 2), (6, 2), (17, 1), (44, 1), (51, 1), (58, 1), (63, 1), (64, 1), (70, 1), (71, 1), (73, 2), (74, 1), (98, 1), (103, 1), (128, 1), (144, 1), (146, 3), (147, 1), (148, 1), (164, 1), (170, 1), (171, 1), (172, 1), (176, 1), (193, 2), (194, 1), (195, 1), (197, 1), (199, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (225, 4), (250, 1), (255, 1), (256, 1), (257, 1), (260, 1), (262, 3), (263, 2), (266, 2), (267, 1), (284, 2), (285, 2), (286, 8), (288, 1), (296, 1), (297, 2), (309, 1), (313, 1), (314, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 79s - loss: 780.5989 - loglik: -7.7912e+02 - logprior: -1.4745e+00
Epoch 2/2
39/39 - 77s - loss: 763.2955 - loglik: -7.6297e+02 - logprior: -3.2362e-01
Fitted a model with MAP estimate = -753.1845
expansions: [(0, 2), (346, 1)]
discards: [  1   7 168 224 266 313 320 337 338 339 340 341 342 368]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 768.6862 - loglik: -7.6721e+02 - logprior: -1.4730e+00
Epoch 2/2
39/39 - 72s - loss: 764.5434 - loglik: -7.6452e+02 - logprior: -2.1635e-02
Fitted a model with MAP estimate = -755.1902
expansions: [(0, 2), (332, 2), (334, 2)]
discards: [1 2]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 78s - loss: 759.5455 - loglik: -7.5769e+02 - logprior: -1.8536e+00
Epoch 2/10
39/39 - 73s - loss: 754.5464 - loglik: -7.5468e+02 - logprior: 0.1305
Epoch 3/10
39/39 - 74s - loss: 753.8286 - loglik: -7.5411e+02 - logprior: 0.2833
Epoch 4/10
39/39 - 74s - loss: 751.4008 - loglik: -7.5182e+02 - logprior: 0.4176
Epoch 5/10
39/39 - 73s - loss: 751.0739 - loglik: -7.5159e+02 - logprior: 0.5166
Epoch 6/10
39/39 - 73s - loss: 752.0409 - loglik: -7.5265e+02 - logprior: 0.6076
Fitted a model with MAP estimate = -749.9904
Time for alignment: 1687.7705
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 989.6016 - loglik: -9.8827e+02 - logprior: -1.3334e+00
Epoch 2/10
39/39 - 52s - loss: 818.7526 - loglik: -8.1755e+02 - logprior: -1.2032e+00
Epoch 3/10
39/39 - 54s - loss: 802.9356 - loglik: -8.0168e+02 - logprior: -1.2583e+00
Epoch 4/10
39/39 - 53s - loss: 800.3094 - loglik: -7.9911e+02 - logprior: -1.1997e+00
Epoch 5/10
39/39 - 53s - loss: 799.1767 - loglik: -7.9797e+02 - logprior: -1.2045e+00
Epoch 6/10
39/39 - 55s - loss: 798.1336 - loglik: -7.9691e+02 - logprior: -1.2188e+00
Epoch 7/10
39/39 - 52s - loss: 797.6561 - loglik: -7.9642e+02 - logprior: -1.2382e+00
Epoch 8/10
39/39 - 52s - loss: 797.2970 - loglik: -7.9598e+02 - logprior: -1.3190e+00
Epoch 9/10
39/39 - 54s - loss: 796.0066 - loglik: -7.9482e+02 - logprior: -1.1895e+00
Epoch 10/10
39/39 - 53s - loss: 795.9283 - loglik: -7.9470e+02 - logprior: -1.2301e+00
Fitted a model with MAP estimate = -787.4259
expansions: [(0, 3), (6, 2), (15, 1), (35, 1), (51, 1), (52, 1), (57, 1), (62, 1), (63, 1), (69, 1), (70, 1), (72, 2), (74, 1), (121, 2), (123, 1), (132, 1), (143, 1), (145, 3), (146, 1), (147, 1), (166, 1), (170, 1), (172, 1), (176, 1), (177, 1), (192, 2), (195, 1), (197, 1), (199, 1), (218, 1), (219, 2), (220, 1), (221, 3), (222, 1), (225, 3), (252, 1), (255, 1), (256, 1), (257, 1), (259, 1), (260, 1), (262, 1), (263, 1), (264, 1), (266, 2), (281, 1), (284, 2), (285, 2), (286, 8), (288, 1), (296, 1), (297, 2), (309, 1), (313, 2), (314, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 400 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 779.9164 - loglik: -7.7825e+02 - logprior: -1.6629e+00
Epoch 2/2
39/39 - 80s - loss: 762.9636 - loglik: -7.6256e+02 - logprior: -4.0494e-01
Fitted a model with MAP estimate = -753.0180
expansions: [(0, 2), (351, 2)]
discards: [  1   2   3 170 223 261 269 322 339 340 341 342 343 344 370 393]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 388 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 769.3228 - loglik: -7.6763e+02 - logprior: -1.6959e+00
Epoch 2/2
39/39 - 65s - loss: 764.2365 - loglik: -7.6408e+02 - logprior: -1.5906e-01
Fitted a model with MAP estimate = -754.9747
expansions: [(0, 2), (333, 2), (336, 1)]
discards: [  1   2 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 759.6292 - loglik: -7.5779e+02 - logprior: -1.8430e+00
Epoch 2/10
39/39 - 64s - loss: 754.1497 - loglik: -7.5425e+02 - logprior: 0.1007
Epoch 3/10
39/39 - 68s - loss: 753.2672 - loglik: -7.5356e+02 - logprior: 0.2968
Epoch 4/10
39/39 - 72s - loss: 752.2356 - loglik: -7.5264e+02 - logprior: 0.4054
Epoch 5/10
39/39 - 75s - loss: 750.9259 - loglik: -7.5144e+02 - logprior: 0.5151
Epoch 6/10
39/39 - 77s - loss: 751.0391 - loglik: -7.5167e+02 - logprior: 0.6278
Fitted a model with MAP estimate = -750.0251
Time for alignment: 1617.6788
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 988.9888 - loglik: -9.8766e+02 - logprior: -1.3328e+00
Epoch 2/10
39/39 - 57s - loss: 819.8657 - loglik: -8.1871e+02 - logprior: -1.1529e+00
Epoch 3/10
39/39 - 55s - loss: 805.9127 - loglik: -8.0471e+02 - logprior: -1.2019e+00
Epoch 4/10
39/39 - 54s - loss: 802.8545 - loglik: -8.0169e+02 - logprior: -1.1611e+00
Epoch 5/10
39/39 - 56s - loss: 801.2941 - loglik: -8.0015e+02 - logprior: -1.1483e+00
Epoch 6/10
39/39 - 55s - loss: 800.7968 - loglik: -7.9966e+02 - logprior: -1.1325e+00
Epoch 7/10
39/39 - 56s - loss: 799.5936 - loglik: -7.9844e+02 - logprior: -1.1553e+00
Epoch 8/10
39/39 - 58s - loss: 799.1848 - loglik: -7.9800e+02 - logprior: -1.1814e+00
Epoch 9/10
39/39 - 57s - loss: 798.8099 - loglik: -7.9762e+02 - logprior: -1.1860e+00
Epoch 10/10
39/39 - 55s - loss: 798.4331 - loglik: -7.9732e+02 - logprior: -1.1140e+00
Fitted a model with MAP estimate = -789.4011
expansions: [(0, 2), (6, 2), (20, 1), (44, 1), (56, 2), (58, 1), (63, 1), (64, 1), (70, 1), (71, 1), (73, 2), (74, 1), (98, 1), (121, 1), (128, 1), (144, 2), (145, 3), (163, 1), (170, 3), (176, 1), (191, 1), (192, 2), (195, 1), (197, 1), (199, 1), (219, 1), (220, 1), (221, 1), (222, 2), (223, 3), (246, 1), (247, 1), (252, 1), (255, 1), (256, 1), (257, 1), (260, 1), (263, 1), (264, 3), (266, 2), (267, 1), (284, 1), (285, 2), (286, 8), (295, 1), (296, 1), (297, 2), (309, 1), (313, 1), (314, 3)]
discards: [  0 280]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 61s - loss: 779.6157 - loglik: -7.7805e+02 - logprior: -1.5626e+00
Epoch 2/2
39/39 - 58s - loss: 763.3450 - loglik: -7.6295e+02 - logprior: -3.9583e-01
Fitted a model with MAP estimate = -753.3035
expansions: [(0, 2), (345, 1), (346, 2)]
discards: [  1   7  62 221 262 319 336 337 338 339 340 341 365]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 768.4675 - loglik: -7.6685e+02 - logprior: -1.6135e+00
Epoch 2/2
39/39 - 68s - loss: 763.8672 - loglik: -7.6386e+02 - logprior: -7.2065e-03
Fitted a model with MAP estimate = -754.3177
expansions: [(0, 2), (336, 5), (337, 1)]
discards: [  1   2 332]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 391 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 759.0916 - loglik: -7.5717e+02 - logprior: -1.9193e+00
Epoch 2/10
39/39 - 71s - loss: 754.4759 - loglik: -7.5455e+02 - logprior: 0.0694
Epoch 3/10
39/39 - 71s - loss: 752.7359 - loglik: -7.5302e+02 - logprior: 0.2827
Epoch 4/10
39/39 - 69s - loss: 751.4040 - loglik: -7.5180e+02 - logprior: 0.3955
Epoch 5/10
39/39 - 71s - loss: 750.4625 - loglik: -7.5096e+02 - logprior: 0.5011
Epoch 6/10
39/39 - 71s - loss: 749.1359 - loglik: -7.4974e+02 - logprior: 0.6057
Epoch 7/10
39/39 - 70s - loss: 750.7915 - loglik: -7.5148e+02 - logprior: 0.6884
Fitted a model with MAP estimate = -749.1039
Time for alignment: 1644.5788
Computed alignments with likelihoods: ['-752.2820', '-750.1397', '-749.9904', '-750.0251', '-749.1039']
Best model has likelihood: -749.1039  (prior= 0.8113 )
time for generating output: 1.3767
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.28134637630384474
Training of 5 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bdd679cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4a146312b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bde8f1cd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4a546e4d30>
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 134.9923 - loglik: -1.3178e+02 - logprior: -3.2113e+00
Epoch 2/10
19/19 - 1s - loss: 110.1903 - loglik: -1.0876e+02 - logprior: -1.4323e+00
Epoch 3/10
19/19 - 1s - loss: 102.2601 - loglik: -1.0070e+02 - logprior: -1.5627e+00
Epoch 4/10
19/19 - 1s - loss: 100.2599 - loglik: -9.8832e+01 - logprior: -1.4280e+00
Epoch 5/10
19/19 - 1s - loss: 99.6765 - loglik: -9.8253e+01 - logprior: -1.4231e+00
Epoch 6/10
19/19 - 1s - loss: 99.2210 - loglik: -9.7815e+01 - logprior: -1.4060e+00
Epoch 7/10
19/19 - 1s - loss: 99.2251 - loglik: -9.7830e+01 - logprior: -1.3953e+00
Fitted a model with MAP estimate = -98.8812
expansions: [(9, 3), (10, 2), (12, 1), (13, 1), (18, 1), (20, 1), (21, 2), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 102.8466 - loglik: -9.8689e+01 - logprior: -4.1573e+00
Epoch 2/2
19/19 - 1s - loss: 95.5167 - loglik: -9.3514e+01 - logprior: -2.0030e+00
Fitted a model with MAP estimate = -94.1032
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.0117 - loglik: -9.2928e+01 - logprior: -3.0838e+00
Epoch 2/2
19/19 - 1s - loss: 93.1093 - loglik: -9.1836e+01 - logprior: -1.2735e+00
Fitted a model with MAP estimate = -92.6888
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 97.0884 - loglik: -9.3508e+01 - logprior: -3.5805e+00
Epoch 2/10
19/19 - 1s - loss: 93.5750 - loglik: -9.2179e+01 - logprior: -1.3961e+00
Epoch 3/10
19/19 - 1s - loss: 92.9214 - loglik: -9.1616e+01 - logprior: -1.3055e+00
Epoch 4/10
19/19 - 1s - loss: 92.8979 - loglik: -9.1639e+01 - logprior: -1.2593e+00
Epoch 5/10
19/19 - 1s - loss: 92.5930 - loglik: -9.1365e+01 - logprior: -1.2276e+00
Epoch 6/10
19/19 - 1s - loss: 92.3961 - loglik: -9.1179e+01 - logprior: -1.2176e+00
Epoch 7/10
19/19 - 1s - loss: 92.4177 - loglik: -9.1216e+01 - logprior: -1.2020e+00
Fitted a model with MAP estimate = -92.2746
Time for alignment: 37.1167
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.8928 - loglik: -1.3168e+02 - logprior: -3.2129e+00
Epoch 2/10
19/19 - 1s - loss: 110.0626 - loglik: -1.0862e+02 - logprior: -1.4380e+00
Epoch 3/10
19/19 - 1s - loss: 102.4165 - loglik: -1.0086e+02 - logprior: -1.5523e+00
Epoch 4/10
19/19 - 1s - loss: 100.5002 - loglik: -9.9071e+01 - logprior: -1.4294e+00
Epoch 5/10
19/19 - 1s - loss: 99.8037 - loglik: -9.8381e+01 - logprior: -1.4223e+00
Epoch 6/10
19/19 - 1s - loss: 99.6772 - loglik: -9.8277e+01 - logprior: -1.4005e+00
Epoch 7/10
19/19 - 1s - loss: 99.1887 - loglik: -9.7799e+01 - logprior: -1.3898e+00
Epoch 8/10
19/19 - 1s - loss: 99.1627 - loglik: -9.7784e+01 - logprior: -1.3789e+00
Epoch 9/10
19/19 - 1s - loss: 99.0010 - loglik: -9.7627e+01 - logprior: -1.3736e+00
Epoch 10/10
19/19 - 1s - loss: 98.9017 - loglik: -9.7536e+01 - logprior: -1.3661e+00
Fitted a model with MAP estimate = -98.7982
expansions: [(9, 3), (10, 2), (12, 2), (14, 1), (18, 1), (20, 1), (21, 2), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.6702 - loglik: -9.9496e+01 - logprior: -4.1743e+00
Epoch 2/2
19/19 - 1s - loss: 95.6500 - loglik: -9.3559e+01 - logprior: -2.0915e+00
Fitted a model with MAP estimate = -94.2714
expansions: [(0, 2)]
discards: [ 0  9 12 16 30 40 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.0684 - loglik: -9.2984e+01 - logprior: -3.0840e+00
Epoch 2/2
19/19 - 1s - loss: 93.0652 - loglik: -9.1791e+01 - logprior: -1.2742e+00
Fitted a model with MAP estimate = -92.6760
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 97.1500 - loglik: -9.3578e+01 - logprior: -3.5717e+00
Epoch 2/10
19/19 - 1s - loss: 93.5670 - loglik: -9.2177e+01 - logprior: -1.3896e+00
Epoch 3/10
19/19 - 1s - loss: 92.9646 - loglik: -9.1663e+01 - logprior: -1.3018e+00
Epoch 4/10
19/19 - 1s - loss: 92.7621 - loglik: -9.1506e+01 - logprior: -1.2563e+00
Epoch 5/10
19/19 - 1s - loss: 92.6523 - loglik: -9.1426e+01 - logprior: -1.2259e+00
Epoch 6/10
19/19 - 1s - loss: 92.3743 - loglik: -9.1161e+01 - logprior: -1.2134e+00
Epoch 7/10
19/19 - 1s - loss: 92.3529 - loglik: -9.1155e+01 - logprior: -1.1980e+00
Epoch 8/10
19/19 - 1s - loss: 92.1948 - loglik: -9.1007e+01 - logprior: -1.1875e+00
Epoch 9/10
19/19 - 1s - loss: 92.4021 - loglik: -9.1223e+01 - logprior: -1.1793e+00
Fitted a model with MAP estimate = -92.1313
Time for alignment: 39.5533
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 135.0245 - loglik: -1.3181e+02 - logprior: -3.2096e+00
Epoch 2/10
19/19 - 1s - loss: 109.9548 - loglik: -1.0852e+02 - logprior: -1.4364e+00
Epoch 3/10
19/19 - 1s - loss: 102.1690 - loglik: -1.0061e+02 - logprior: -1.5631e+00
Epoch 4/10
19/19 - 1s - loss: 100.1077 - loglik: -9.8677e+01 - logprior: -1.4311e+00
Epoch 5/10
19/19 - 1s - loss: 99.7317 - loglik: -9.8310e+01 - logprior: -1.4219e+00
Epoch 6/10
19/19 - 1s - loss: 99.2493 - loglik: -9.7844e+01 - logprior: -1.4053e+00
Epoch 7/10
19/19 - 1s - loss: 99.1643 - loglik: -9.7772e+01 - logprior: -1.3918e+00
Epoch 8/10
19/19 - 1s - loss: 98.8703 - loglik: -9.7487e+01 - logprior: -1.3829e+00
Epoch 9/10
19/19 - 1s - loss: 98.8042 - loglik: -9.7431e+01 - logprior: -1.3736e+00
Epoch 10/10
19/19 - 1s - loss: 98.8733 - loglik: -9.7506e+01 - logprior: -1.3670e+00
Fitted a model with MAP estimate = -98.6070
expansions: [(9, 3), (10, 2), (12, 1), (13, 1), (18, 1), (20, 1), (21, 2), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.8221 - loglik: -9.8656e+01 - logprior: -4.1664e+00
Epoch 2/2
19/19 - 1s - loss: 95.4981 - loglik: -9.3448e+01 - logprior: -2.0500e+00
Fitted a model with MAP estimate = -94.1707
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.9525 - loglik: -9.2871e+01 - logprior: -3.0820e+00
Epoch 2/2
19/19 - 1s - loss: 93.1096 - loglik: -9.1837e+01 - logprior: -1.2727e+00
Fitted a model with MAP estimate = -92.6738
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 97.0611 - loglik: -9.3502e+01 - logprior: -3.5595e+00
Epoch 2/10
19/19 - 1s - loss: 93.5221 - loglik: -9.2128e+01 - logprior: -1.3939e+00
Epoch 3/10
19/19 - 1s - loss: 93.1109 - loglik: -9.1808e+01 - logprior: -1.3029e+00
Epoch 4/10
19/19 - 1s - loss: 92.7693 - loglik: -9.1512e+01 - logprior: -1.2574e+00
Epoch 5/10
19/19 - 1s - loss: 92.4864 - loglik: -9.1259e+01 - logprior: -1.2274e+00
Epoch 6/10
19/19 - 1s - loss: 92.4758 - loglik: -9.1261e+01 - logprior: -1.2151e+00
Epoch 7/10
19/19 - 1s - loss: 92.2777 - loglik: -9.1076e+01 - logprior: -1.2018e+00
Epoch 8/10
19/19 - 1s - loss: 92.4442 - loglik: -9.1255e+01 - logprior: -1.1888e+00
Fitted a model with MAP estimate = -92.1735
Time for alignment: 39.8279
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 135.0320 - loglik: -1.3182e+02 - logprior: -3.2115e+00
Epoch 2/10
19/19 - 1s - loss: 109.8686 - loglik: -1.0844e+02 - logprior: -1.4304e+00
Epoch 3/10
19/19 - 1s - loss: 102.2308 - loglik: -1.0067e+02 - logprior: -1.5654e+00
Epoch 4/10
19/19 - 1s - loss: 100.1346 - loglik: -9.8708e+01 - logprior: -1.4262e+00
Epoch 5/10
19/19 - 1s - loss: 99.5695 - loglik: -9.8148e+01 - logprior: -1.4212e+00
Epoch 6/10
19/19 - 1s - loss: 99.3710 - loglik: -9.7969e+01 - logprior: -1.4015e+00
Epoch 7/10
19/19 - 1s - loss: 99.2460 - loglik: -9.7856e+01 - logprior: -1.3901e+00
Epoch 8/10
19/19 - 1s - loss: 98.9156 - loglik: -9.7534e+01 - logprior: -1.3817e+00
Epoch 9/10
19/19 - 1s - loss: 98.7857 - loglik: -9.7413e+01 - logprior: -1.3724e+00
Epoch 10/10
19/19 - 1s - loss: 98.6492 - loglik: -9.7280e+01 - logprior: -1.3689e+00
Fitted a model with MAP estimate = -98.6081
expansions: [(9, 3), (10, 2), (12, 1), (13, 1), (18, 1), (20, 1), (21, 2), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 102.8622 - loglik: -9.8700e+01 - logprior: -4.1622e+00
Epoch 2/2
19/19 - 1s - loss: 95.5187 - loglik: -9.3472e+01 - logprior: -2.0462e+00
Fitted a model with MAP estimate = -94.1947
expansions: [(0, 2)]
discards: [ 0  9 12 29 39 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 95.8910 - loglik: -9.2809e+01 - logprior: -3.0819e+00
Epoch 2/2
19/19 - 1s - loss: 93.1153 - loglik: -9.1844e+01 - logprior: -1.2714e+00
Fitted a model with MAP estimate = -92.6793
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 97.1086 - loglik: -9.3533e+01 - logprior: -3.5759e+00
Epoch 2/10
19/19 - 1s - loss: 93.6017 - loglik: -9.2208e+01 - logprior: -1.3940e+00
Epoch 3/10
19/19 - 1s - loss: 92.9779 - loglik: -9.1676e+01 - logprior: -1.3015e+00
Epoch 4/10
19/19 - 1s - loss: 92.7643 - loglik: -9.1507e+01 - logprior: -1.2572e+00
Epoch 5/10
19/19 - 1s - loss: 92.5198 - loglik: -9.1295e+01 - logprior: -1.2251e+00
Epoch 6/10
19/19 - 1s - loss: 92.3803 - loglik: -9.1169e+01 - logprior: -1.2116e+00
Epoch 7/10
19/19 - 1s - loss: 92.4172 - loglik: -9.1218e+01 - logprior: -1.1996e+00
Fitted a model with MAP estimate = -92.2488
Time for alignment: 37.4083
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 134.9936 - loglik: -1.3178e+02 - logprior: -3.2104e+00
Epoch 2/10
19/19 - 1s - loss: 109.8757 - loglik: -1.0845e+02 - logprior: -1.4290e+00
Epoch 3/10
19/19 - 1s - loss: 102.2759 - loglik: -1.0071e+02 - logprior: -1.5687e+00
Epoch 4/10
19/19 - 1s - loss: 100.2310 - loglik: -9.8802e+01 - logprior: -1.4290e+00
Epoch 5/10
19/19 - 1s - loss: 99.6397 - loglik: -9.8220e+01 - logprior: -1.4194e+00
Epoch 6/10
19/19 - 1s - loss: 99.2688 - loglik: -9.7864e+01 - logprior: -1.4045e+00
Epoch 7/10
19/19 - 1s - loss: 99.0113 - loglik: -9.7618e+01 - logprior: -1.3937e+00
Epoch 8/10
19/19 - 1s - loss: 98.9715 - loglik: -9.7589e+01 - logprior: -1.3822e+00
Epoch 9/10
19/19 - 1s - loss: 98.9005 - loglik: -9.7526e+01 - logprior: -1.3749e+00
Epoch 10/10
19/19 - 1s - loss: 98.6592 - loglik: -9.7291e+01 - logprior: -1.3678e+00
Fitted a model with MAP estimate = -98.6054
expansions: [(9, 3), (10, 2), (12, 2), (13, 1), (18, 1), (20, 1), (21, 2), (28, 2), (29, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 103.4550 - loglik: -9.9282e+01 - logprior: -4.1727e+00
Epoch 2/2
19/19 - 1s - loss: 95.5540 - loglik: -9.3468e+01 - logprior: -2.0856e+00
Fitted a model with MAP estimate = -94.2481
expansions: [(0, 2)]
discards: [ 0  9 12 16 30 40 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.0115 - loglik: -9.2928e+01 - logprior: -3.0830e+00
Epoch 2/2
19/19 - 1s - loss: 93.0994 - loglik: -9.1827e+01 - logprior: -1.2722e+00
Fitted a model with MAP estimate = -92.6847
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 97.0934 - loglik: -9.3529e+01 - logprior: -3.5640e+00
Epoch 2/10
19/19 - 1s - loss: 93.6203 - loglik: -9.2230e+01 - logprior: -1.3906e+00
Epoch 3/10
19/19 - 1s - loss: 92.9597 - loglik: -9.1658e+01 - logprior: -1.3022e+00
Epoch 4/10
19/19 - 1s - loss: 92.8227 - loglik: -9.1566e+01 - logprior: -1.2566e+00
Epoch 5/10
19/19 - 1s - loss: 92.5707 - loglik: -9.1344e+01 - logprior: -1.2266e+00
Epoch 6/10
19/19 - 1s - loss: 92.4604 - loglik: -9.1249e+01 - logprior: -1.2119e+00
Epoch 7/10
19/19 - 1s - loss: 92.3111 - loglik: -9.1110e+01 - logprior: -1.2015e+00
Epoch 8/10
19/19 - 1s - loss: 92.4084 - loglik: -9.1217e+01 - logprior: -1.1910e+00
Fitted a model with MAP estimate = -92.1896
Time for alignment: 38.5244
Computed alignments with likelihoods: ['-92.2746', '-92.1313', '-92.1735', '-92.2488', '-92.1896']
Best model has likelihood: -92.1313  (prior= -1.1759 )
time for generating output: 0.0943
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.8292499624229671
Training of 5 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d95bfba30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bd4cf6c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bdded6040>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4be86280d0>
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 479.9059 - loglik: -4.7809e+02 - logprior: -1.8155e+00
Epoch 2/10
39/39 - 9s - loss: 370.8949 - loglik: -3.6911e+02 - logprior: -1.7892e+00
Epoch 3/10
39/39 - 9s - loss: 363.3640 - loglik: -3.6162e+02 - logprior: -1.7473e+00
Epoch 4/10
39/39 - 9s - loss: 361.6061 - loglik: -3.5994e+02 - logprior: -1.6656e+00
Epoch 5/10
39/39 - 9s - loss: 360.6721 - loglik: -3.5901e+02 - logprior: -1.6587e+00
Epoch 6/10
39/39 - 9s - loss: 359.7941 - loglik: -3.5813e+02 - logprior: -1.6666e+00
Epoch 7/10
39/39 - 9s - loss: 359.9837 - loglik: -3.5832e+02 - logprior: -1.6595e+00
Fitted a model with MAP estimate = -359.0350
expansions: [(4, 2), (5, 1), (13, 1), (14, 1), (21, 1), (27, 1), (28, 1), (36, 1), (40, 1), (41, 1), (42, 1), (43, 1), (55, 1), (58, 1), (59, 1), (64, 1), (67, 1), (75, 2), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (107, 1), (110, 1), (112, 1), (117, 1), (120, 1), (127, 1), (130, 1), (132, 1), (133, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 342.1324 - loglik: -3.4037e+02 - logprior: -1.7671e+00
Epoch 2/2
39/39 - 13s - loss: 329.1257 - loglik: -3.2840e+02 - logprior: -7.2708e-01
Fitted a model with MAP estimate = -327.5498
expansions: []
discards: [118]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 332.6191 - loglik: -3.3101e+02 - logprior: -1.6114e+00
Epoch 2/2
39/39 - 15s - loss: 329.1928 - loglik: -3.2864e+02 - logprior: -5.4954e-01
Fitted a model with MAP estimate = -327.8979
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 331.9868 - loglik: -3.3052e+02 - logprior: -1.4688e+00
Epoch 2/10
39/39 - 15s - loss: 328.6846 - loglik: -3.2830e+02 - logprior: -3.8430e-01
Epoch 3/10
39/39 - 14s - loss: 327.4702 - loglik: -3.2719e+02 - logprior: -2.8032e-01
Epoch 4/10
39/39 - 14s - loss: 325.6312 - loglik: -3.2541e+02 - logprior: -2.1656e-01
Epoch 5/10
39/39 - 14s - loss: 324.2969 - loglik: -3.2415e+02 - logprior: -1.4604e-01
Epoch 6/10
39/39 - 14s - loss: 322.5117 - loglik: -3.2245e+02 - logprior: -6.5076e-02
Epoch 7/10
39/39 - 14s - loss: 323.0353 - loglik: -3.2305e+02 - logprior: 0.0163
Fitted a model with MAP estimate = -322.0708
Time for alignment: 295.5891
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 479.9735 - loglik: -4.7815e+02 - logprior: -1.8279e+00
Epoch 2/10
39/39 - 10s - loss: 373.6565 - loglik: -3.7190e+02 - logprior: -1.7540e+00
Epoch 3/10
39/39 - 10s - loss: 365.4612 - loglik: -3.6374e+02 - logprior: -1.7214e+00
Epoch 4/10
39/39 - 10s - loss: 363.8155 - loglik: -3.6217e+02 - logprior: -1.6429e+00
Epoch 5/10
39/39 - 10s - loss: 362.6760 - loglik: -3.6105e+02 - logprior: -1.6296e+00
Epoch 6/10
39/39 - 10s - loss: 362.1676 - loglik: -3.6052e+02 - logprior: -1.6439e+00
Epoch 7/10
39/39 - 10s - loss: 361.8268 - loglik: -3.6018e+02 - logprior: -1.6432e+00
Epoch 8/10
39/39 - 10s - loss: 360.5034 - loglik: -3.5885e+02 - logprior: -1.6522e+00
Epoch 9/10
39/39 - 10s - loss: 360.0996 - loglik: -3.5845e+02 - logprior: -1.6513e+00
Epoch 10/10
39/39 - 10s - loss: 360.1499 - loglik: -3.5849e+02 - logprior: -1.6566e+00
Fitted a model with MAP estimate = -359.2712
expansions: [(4, 2), (5, 1), (13, 1), (14, 1), (21, 1), (27, 1), (28, 1), (36, 1), (40, 1), (41, 1), (42, 1), (43, 1), (52, 1), (58, 1), (59, 1), (64, 1), (67, 1), (75, 2), (98, 1), (99, 1), (100, 1), (101, 2), (108, 1), (110, 1), (112, 1), (121, 1), (124, 1), (127, 1), (130, 1), (131, 1), (133, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 343.0855 - loglik: -3.4138e+02 - logprior: -1.7034e+00
Epoch 2/2
39/39 - 14s - loss: 330.4919 - loglik: -3.2988e+02 - logprior: -6.1391e-01
Fitted a model with MAP estimate = -328.9072
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 332.7139 - loglik: -3.3119e+02 - logprior: -1.5282e+00
Epoch 2/10
39/39 - 15s - loss: 329.2562 - loglik: -3.2877e+02 - logprior: -4.8217e-01
Epoch 3/10
39/39 - 15s - loss: 328.3060 - loglik: -3.2791e+02 - logprior: -3.9535e-01
Epoch 4/10
39/39 - 16s - loss: 326.2928 - loglik: -3.2596e+02 - logprior: -3.3313e-01
Epoch 5/10
39/39 - 16s - loss: 324.6595 - loglik: -3.2439e+02 - logprior: -2.6930e-01
Epoch 6/10
39/39 - 16s - loss: 323.4527 - loglik: -3.2325e+02 - logprior: -2.0013e-01
Epoch 7/10
39/39 - 15s - loss: 322.8459 - loglik: -3.2272e+02 - logprior: -1.3085e-01
Epoch 8/10
39/39 - 15s - loss: 323.3185 - loglik: -3.2325e+02 - logprior: -6.4218e-02
Fitted a model with MAP estimate = -322.2827
Time for alignment: 304.6447
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 480.1241 - loglik: -4.7829e+02 - logprior: -1.8310e+00
Epoch 2/10
39/39 - 11s - loss: 371.2707 - loglik: -3.6946e+02 - logprior: -1.8155e+00
Epoch 3/10
39/39 - 11s - loss: 363.1465 - loglik: -3.6139e+02 - logprior: -1.7547e+00
Epoch 4/10
39/39 - 11s - loss: 361.7564 - loglik: -3.6008e+02 - logprior: -1.6762e+00
Epoch 5/10
39/39 - 11s - loss: 360.3084 - loglik: -3.5864e+02 - logprior: -1.6691e+00
Epoch 6/10
39/39 - 11s - loss: 360.2657 - loglik: -3.5859e+02 - logprior: -1.6720e+00
Epoch 7/10
39/39 - 11s - loss: 358.9937 - loglik: -3.5731e+02 - logprior: -1.6792e+00
Epoch 8/10
39/39 - 11s - loss: 358.7202 - loglik: -3.5704e+02 - logprior: -1.6799e+00
Epoch 9/10
39/39 - 11s - loss: 358.2570 - loglik: -3.5657e+02 - logprior: -1.6893e+00
Epoch 10/10
39/39 - 11s - loss: 357.5824 - loglik: -3.5589e+02 - logprior: -1.6889e+00
Fitted a model with MAP estimate = -356.8523
expansions: [(4, 2), (5, 1), (13, 1), (14, 1), (21, 1), (27, 1), (28, 1), (36, 1), (40, 1), (41, 1), (42, 1), (43, 1), (52, 1), (58, 1), (59, 1), (64, 1), (67, 1), (75, 2), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (108, 1), (110, 1), (112, 1), (121, 1), (124, 1), (127, 1), (130, 1), (131, 1), (133, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 342.8969 - loglik: -3.4116e+02 - logprior: -1.7356e+00
Epoch 2/2
39/39 - 15s - loss: 330.2399 - loglik: -3.2956e+02 - logprior: -6.7602e-01
Fitted a model with MAP estimate = -328.5463
expansions: []
discards: [118]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 333.2968 - loglik: -3.3175e+02 - logprior: -1.5492e+00
Epoch 2/2
39/39 - 15s - loss: 330.0427 - loglik: -3.2955e+02 - logprior: -4.9155e-01
Fitted a model with MAP estimate = -328.9193
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 332.6588 - loglik: -3.3123e+02 - logprior: -1.4242e+00
Epoch 2/10
39/39 - 14s - loss: 329.8486 - loglik: -3.2951e+02 - logprior: -3.3501e-01
Epoch 3/10
39/39 - 14s - loss: 328.5331 - loglik: -3.2830e+02 - logprior: -2.3372e-01
Epoch 4/10
39/39 - 15s - loss: 326.8678 - loglik: -3.2671e+02 - logprior: -1.6055e-01
Epoch 5/10
39/39 - 14s - loss: 325.5192 - loglik: -3.2542e+02 - logprior: -9.6754e-02
Epoch 6/10
39/39 - 14s - loss: 323.6785 - loglik: -3.2365e+02 - logprior: -2.5852e-02
Epoch 7/10
39/39 - 14s - loss: 323.4413 - loglik: -3.2350e+02 - logprior: 0.0587
Epoch 8/10
39/39 - 15s - loss: 323.3342 - loglik: -3.2347e+02 - logprior: 0.1356
Epoch 9/10
39/39 - 14s - loss: 322.2817 - loglik: -3.2250e+02 - logprior: 0.2195
Epoch 10/10
39/39 - 15s - loss: 322.4146 - loglik: -3.2271e+02 - logprior: 0.3000
Fitted a model with MAP estimate = -322.1143
Time for alignment: 388.2746
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 479.4595 - loglik: -4.7765e+02 - logprior: -1.8045e+00
Epoch 2/10
39/39 - 11s - loss: 371.2569 - loglik: -3.6950e+02 - logprior: -1.7520e+00
Epoch 3/10
39/39 - 11s - loss: 363.4467 - loglik: -3.6170e+02 - logprior: -1.7493e+00
Epoch 4/10
39/39 - 11s - loss: 361.5965 - loglik: -3.5991e+02 - logprior: -1.6843e+00
Epoch 5/10
39/39 - 11s - loss: 360.4873 - loglik: -3.5882e+02 - logprior: -1.6660e+00
Epoch 6/10
39/39 - 11s - loss: 360.2407 - loglik: -3.5858e+02 - logprior: -1.6631e+00
Epoch 7/10
39/39 - 11s - loss: 359.4341 - loglik: -3.5777e+02 - logprior: -1.6606e+00
Epoch 8/10
39/39 - 11s - loss: 359.3320 - loglik: -3.5767e+02 - logprior: -1.6661e+00
Epoch 9/10
39/39 - 12s - loss: 358.3211 - loglik: -3.5666e+02 - logprior: -1.6593e+00
Epoch 10/10
39/39 - 11s - loss: 357.6518 - loglik: -3.5599e+02 - logprior: -1.6602e+00
Fitted a model with MAP estimate = -357.1142
expansions: [(4, 2), (5, 1), (13, 1), (14, 1), (21, 1), (27, 1), (28, 1), (36, 1), (40, 1), (41, 1), (42, 1), (43, 1), (52, 1), (58, 1), (59, 1), (64, 1), (67, 1), (75, 2), (93, 1), (97, 2), (99, 1), (102, 1), (103, 1), (107, 1), (110, 1), (112, 1), (121, 1), (124, 1), (127, 1), (130, 1), (131, 1), (132, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 342.8210 - loglik: -3.4104e+02 - logprior: -1.7857e+00
Epoch 2/2
39/39 - 15s - loss: 329.7805 - loglik: -3.2903e+02 - logprior: -7.4670e-01
Fitted a model with MAP estimate = -327.9455
expansions: []
discards: [118 169]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 332.8338 - loglik: -3.3124e+02 - logprior: -1.5957e+00
Epoch 2/2
39/39 - 16s - loss: 329.1889 - loglik: -3.2865e+02 - logprior: -5.3878e-01
Fitted a model with MAP estimate = -327.5792
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 331.7169 - loglik: -3.3026e+02 - logprior: -1.4616e+00
Epoch 2/10
39/39 - 16s - loss: 328.6878 - loglik: -3.2831e+02 - logprior: -3.7995e-01
Epoch 3/10
39/39 - 16s - loss: 327.0844 - loglik: -3.2682e+02 - logprior: -2.6786e-01
Epoch 4/10
39/39 - 16s - loss: 325.4497 - loglik: -3.2525e+02 - logprior: -1.9874e-01
Epoch 5/10
39/39 - 15s - loss: 323.9559 - loglik: -3.2383e+02 - logprior: -1.2884e-01
Epoch 6/10
39/39 - 15s - loss: 323.4378 - loglik: -3.2339e+02 - logprior: -4.8293e-02
Epoch 7/10
39/39 - 15s - loss: 322.2641 - loglik: -3.2230e+02 - logprior: 0.0349
Epoch 8/10
39/39 - 15s - loss: 321.7441 - loglik: -3.2186e+02 - logprior: 0.1177
Epoch 9/10
39/39 - 16s - loss: 321.1236 - loglik: -3.2134e+02 - logprior: 0.2154
Epoch 10/10
39/39 - 15s - loss: 321.9942 - loglik: -3.2230e+02 - logprior: 0.3015
Fitted a model with MAP estimate = -321.2805
Time for alignment: 408.5929
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 480.1913 - loglik: -4.7836e+02 - logprior: -1.8268e+00
Epoch 2/10
39/39 - 10s - loss: 371.0064 - loglik: -3.6922e+02 - logprior: -1.7867e+00
Epoch 3/10
39/39 - 10s - loss: 363.0594 - loglik: -3.6131e+02 - logprior: -1.7540e+00
Epoch 4/10
39/39 - 10s - loss: 361.2333 - loglik: -3.5956e+02 - logprior: -1.6780e+00
Epoch 5/10
39/39 - 10s - loss: 360.4771 - loglik: -3.5881e+02 - logprior: -1.6679e+00
Epoch 6/10
39/39 - 10s - loss: 359.7881 - loglik: -3.5812e+02 - logprior: -1.6639e+00
Epoch 7/10
39/39 - 11s - loss: 359.0109 - loglik: -3.5734e+02 - logprior: -1.6688e+00
Epoch 8/10
39/39 - 10s - loss: 358.9766 - loglik: -3.5731e+02 - logprior: -1.6685e+00
Epoch 9/10
39/39 - 10s - loss: 358.2410 - loglik: -3.5657e+02 - logprior: -1.6704e+00
Epoch 10/10
39/39 - 10s - loss: 357.1692 - loglik: -3.5550e+02 - logprior: -1.6708e+00
Fitted a model with MAP estimate = -356.8479
expansions: [(4, 2), (5, 1), (13, 1), (14, 1), (21, 1), (27, 1), (28, 1), (36, 1), (40, 1), (41, 1), (42, 1), (43, 1), (52, 1), (58, 1), (59, 1), (64, 1), (67, 1), (75, 2), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (107, 1), (110, 1), (112, 1), (117, 1), (120, 1), (127, 1), (130, 1), (132, 1), (133, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 342.3971 - loglik: -3.4062e+02 - logprior: -1.7776e+00
Epoch 2/2
39/39 - 14s - loss: 330.0285 - loglik: -3.2930e+02 - logprior: -7.2476e-01
Fitted a model with MAP estimate = -328.1992
expansions: []
discards: [118]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 333.1044 - loglik: -3.3153e+02 - logprior: -1.5721e+00
Epoch 2/2
39/39 - 14s - loss: 329.7225 - loglik: -3.2920e+02 - logprior: -5.1782e-01
Fitted a model with MAP estimate = -328.5474
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 332.4810 - loglik: -3.3105e+02 - logprior: -1.4338e+00
Epoch 2/10
39/39 - 15s - loss: 329.3695 - loglik: -3.2900e+02 - logprior: -3.6628e-01
Epoch 3/10
39/39 - 15s - loss: 327.9230 - loglik: -3.2766e+02 - logprior: -2.6572e-01
Epoch 4/10
39/39 - 16s - loss: 326.7215 - loglik: -3.2652e+02 - logprior: -1.9653e-01
Epoch 5/10
39/39 - 16s - loss: 324.3817 - loglik: -3.2426e+02 - logprior: -1.2643e-01
Epoch 6/10
39/39 - 16s - loss: 324.1658 - loglik: -3.2412e+02 - logprior: -4.2380e-02
Epoch 7/10
39/39 - 16s - loss: 323.1158 - loglik: -3.2317e+02 - logprior: 0.0515
Epoch 8/10
39/39 - 16s - loss: 322.5821 - loglik: -3.2273e+02 - logprior: 0.1477
Epoch 9/10
39/39 - 16s - loss: 322.7966 - loglik: -3.2302e+02 - logprior: 0.2267
Fitted a model with MAP estimate = -322.1092
Time for alignment: 377.6657
Computed alignments with likelihoods: ['-322.0708', '-322.2827', '-322.1143', '-321.2805', '-322.1092']
Best model has likelihood: -321.2805  (prior= 0.3320 )
time for generating output: 0.1771
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.8392173543173118
Training of 5 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4a1407cf10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bde072d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4a4c375040>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d7cb6a8b0>
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 259.1279 - loglik: -2.5603e+02 - logprior: -3.0980e+00
Epoch 2/10
19/19 - 2s - loss: 228.9369 - loglik: -2.2773e+02 - logprior: -1.2062e+00
Epoch 3/10
19/19 - 2s - loss: 216.3558 - loglik: -2.1503e+02 - logprior: -1.3287e+00
Epoch 4/10
19/19 - 2s - loss: 212.4922 - loglik: -2.1117e+02 - logprior: -1.3247e+00
Epoch 5/10
19/19 - 2s - loss: 210.8424 - loglik: -2.0963e+02 - logprior: -1.2110e+00
Epoch 6/10
19/19 - 2s - loss: 208.9718 - loglik: -2.0786e+02 - logprior: -1.1099e+00
Epoch 7/10
19/19 - 2s - loss: 207.5288 - loglik: -2.0648e+02 - logprior: -1.0447e+00
Epoch 8/10
19/19 - 2s - loss: 206.9788 - loglik: -2.0595e+02 - logprior: -1.0271e+00
Epoch 9/10
19/19 - 2s - loss: 206.8392 - loglik: -2.0582e+02 - logprior: -1.0152e+00
Epoch 10/10
19/19 - 2s - loss: 206.7562 - loglik: -2.0575e+02 - logprior: -1.0065e+00
Fitted a model with MAP estimate = -206.1731
expansions: [(7, 2), (9, 1), (10, 1), (13, 1), (14, 1), (15, 1), (22, 1), (29, 1), (30, 1), (36, 1), (37, 1), (42, 1), (49, 2), (65, 1), (66, 1), (73, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 209.5977 - loglik: -2.0564e+02 - logprior: -3.9619e+00
Epoch 2/2
19/19 - 2s - loss: 199.5849 - loglik: -1.9771e+02 - logprior: -1.8699e+00
Fitted a model with MAP estimate = -197.5819
expansions: [(0, 2)]
discards: [ 0  7 61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 199.7930 - loglik: -1.9693e+02 - logprior: -2.8619e+00
Epoch 2/2
19/19 - 2s - loss: 196.3671 - loglik: -1.9532e+02 - logprior: -1.0488e+00
Fitted a model with MAP estimate = -195.4350
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 200.7835 - loglik: -1.9731e+02 - logprior: -3.4745e+00
Epoch 2/10
19/19 - 2s - loss: 196.4118 - loglik: -1.9529e+02 - logprior: -1.1207e+00
Epoch 3/10
19/19 - 2s - loss: 195.5925 - loglik: -1.9462e+02 - logprior: -9.7004e-01
Epoch 4/10
19/19 - 2s - loss: 195.4543 - loglik: -1.9453e+02 - logprior: -9.2381e-01
Epoch 5/10
19/19 - 2s - loss: 194.9720 - loglik: -1.9408e+02 - logprior: -8.9237e-01
Epoch 6/10
19/19 - 2s - loss: 195.1683 - loglik: -1.9430e+02 - logprior: -8.7171e-01
Fitted a model with MAP estimate = -194.6521
Time for alignment: 66.6759
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 259.0335 - loglik: -2.5594e+02 - logprior: -3.0929e+00
Epoch 2/10
19/19 - 2s - loss: 229.2607 - loglik: -2.2805e+02 - logprior: -1.2088e+00
Epoch 3/10
19/19 - 2s - loss: 213.6210 - loglik: -2.1229e+02 - logprior: -1.3310e+00
Epoch 4/10
19/19 - 2s - loss: 209.9786 - loglik: -2.0863e+02 - logprior: -1.3515e+00
Epoch 5/10
19/19 - 2s - loss: 208.3300 - loglik: -2.0711e+02 - logprior: -1.2185e+00
Epoch 6/10
19/19 - 2s - loss: 207.2346 - loglik: -2.0613e+02 - logprior: -1.1083e+00
Epoch 7/10
19/19 - 2s - loss: 206.7983 - loglik: -2.0575e+02 - logprior: -1.0469e+00
Epoch 8/10
19/19 - 2s - loss: 206.4391 - loglik: -2.0541e+02 - logprior: -1.0282e+00
Epoch 9/10
19/19 - 2s - loss: 206.2747 - loglik: -2.0525e+02 - logprior: -1.0223e+00
Epoch 10/10
19/19 - 2s - loss: 205.9751 - loglik: -2.0496e+02 - logprior: -1.0194e+00
Fitted a model with MAP estimate = -205.5774
expansions: [(7, 2), (9, 2), (10, 1), (13, 3), (14, 1), (22, 1), (23, 1), (30, 1), (37, 2), (38, 1), (50, 1), (53, 1), (65, 1), (66, 1), (73, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 208.8674 - loglik: -2.0489e+02 - logprior: -3.9755e+00
Epoch 2/2
19/19 - 2s - loss: 198.8007 - loglik: -1.9688e+02 - logprior: -1.9184e+00
Fitted a model with MAP estimate = -196.8614
expansions: [(0, 2)]
discards: [ 0  7 11 18 49]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 198.9799 - loglik: -1.9613e+02 - logprior: -2.8472e+00
Epoch 2/2
19/19 - 2s - loss: 195.3402 - loglik: -1.9431e+02 - logprior: -1.0277e+00
Fitted a model with MAP estimate = -194.4682
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 199.6295 - loglik: -1.9617e+02 - logprior: -3.4572e+00
Epoch 2/10
19/19 - 2s - loss: 195.5348 - loglik: -1.9443e+02 - logprior: -1.1056e+00
Epoch 3/10
19/19 - 2s - loss: 194.8788 - loglik: -1.9393e+02 - logprior: -9.4985e-01
Epoch 4/10
19/19 - 2s - loss: 194.2041 - loglik: -1.9330e+02 - logprior: -9.0573e-01
Epoch 5/10
19/19 - 2s - loss: 194.2875 - loglik: -1.9341e+02 - logprior: -8.7620e-01
Fitted a model with MAP estimate = -193.8268
Time for alignment: 63.3775
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 259.2081 - loglik: -2.5611e+02 - logprior: -3.0988e+00
Epoch 2/10
19/19 - 2s - loss: 229.4700 - loglik: -2.2828e+02 - logprior: -1.1922e+00
Epoch 3/10
19/19 - 2s - loss: 215.5532 - loglik: -2.1431e+02 - logprior: -1.2461e+00
Epoch 4/10
19/19 - 2s - loss: 210.5662 - loglik: -2.0925e+02 - logprior: -1.3126e+00
Epoch 5/10
19/19 - 2s - loss: 208.5454 - loglik: -2.0740e+02 - logprior: -1.1452e+00
Epoch 6/10
19/19 - 2s - loss: 207.7165 - loglik: -2.0662e+02 - logprior: -1.0989e+00
Epoch 7/10
19/19 - 2s - loss: 207.1838 - loglik: -2.0612e+02 - logprior: -1.0590e+00
Epoch 8/10
19/19 - 2s - loss: 206.9105 - loglik: -2.0586e+02 - logprior: -1.0476e+00
Epoch 9/10
19/19 - 2s - loss: 206.7952 - loglik: -2.0575e+02 - logprior: -1.0415e+00
Epoch 10/10
19/19 - 2s - loss: 206.6081 - loglik: -2.0555e+02 - logprior: -1.0560e+00
Fitted a model with MAP estimate = -205.5396
expansions: [(7, 2), (9, 1), (10, 1), (13, 1), (14, 1), (15, 1), (22, 1), (23, 1), (30, 1), (36, 1), (38, 1), (42, 1), (49, 2), (63, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 209.6982 - loglik: -2.0574e+02 - logprior: -3.9594e+00
Epoch 2/2
19/19 - 2s - loss: 199.6582 - loglik: -1.9780e+02 - logprior: -1.8611e+00
Fitted a model with MAP estimate = -197.6593
expansions: [(0, 2)]
discards: [ 0  7 61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 199.8392 - loglik: -1.9699e+02 - logprior: -2.8525e+00
Epoch 2/2
19/19 - 2s - loss: 196.4122 - loglik: -1.9537e+02 - logprior: -1.0463e+00
Fitted a model with MAP estimate = -195.4577
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 200.7185 - loglik: -1.9724e+02 - logprior: -3.4746e+00
Epoch 2/10
19/19 - 2s - loss: 196.4585 - loglik: -1.9534e+02 - logprior: -1.1219e+00
Epoch 3/10
19/19 - 2s - loss: 195.6206 - loglik: -1.9465e+02 - logprior: -9.7394e-01
Epoch 4/10
19/19 - 2s - loss: 195.4032 - loglik: -1.9448e+02 - logprior: -9.2252e-01
Epoch 5/10
19/19 - 2s - loss: 195.2039 - loglik: -1.9431e+02 - logprior: -8.9190e-01
Epoch 6/10
19/19 - 2s - loss: 194.7904 - loglik: -1.9392e+02 - logprior: -8.7292e-01
Epoch 7/10
19/19 - 2s - loss: 194.8226 - loglik: -1.9397e+02 - logprior: -8.5481e-01
Fitted a model with MAP estimate = -194.4837
Time for alignment: 65.5982
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 259.0591 - loglik: -2.5597e+02 - logprior: -3.0928e+00
Epoch 2/10
19/19 - 2s - loss: 227.9835 - loglik: -2.2678e+02 - logprior: -1.2057e+00
Epoch 3/10
19/19 - 2s - loss: 214.4638 - loglik: -2.1315e+02 - logprior: -1.3141e+00
Epoch 4/10
19/19 - 2s - loss: 211.1033 - loglik: -2.0985e+02 - logprior: -1.2557e+00
Epoch 5/10
19/19 - 2s - loss: 208.3778 - loglik: -2.0725e+02 - logprior: -1.1233e+00
Epoch 6/10
19/19 - 2s - loss: 207.2617 - loglik: -2.0620e+02 - logprior: -1.0655e+00
Epoch 7/10
19/19 - 2s - loss: 206.7067 - loglik: -2.0568e+02 - logprior: -1.0311e+00
Epoch 8/10
19/19 - 2s - loss: 206.5664 - loglik: -2.0555e+02 - logprior: -1.0153e+00
Epoch 9/10
19/19 - 2s - loss: 206.1845 - loglik: -2.0518e+02 - logprior: -1.0083e+00
Epoch 10/10
19/19 - 2s - loss: 206.0451 - loglik: -2.0503e+02 - logprior: -1.0161e+00
Fitted a model with MAP estimate = -205.4453
expansions: [(7, 2), (9, 1), (10, 1), (13, 1), (14, 1), (15, 1), (22, 1), (23, 1), (28, 1), (36, 1), (37, 1), (42, 1), (49, 2), (63, 1), (64, 1), (72, 1), (73, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 210.3645 - loglik: -2.0636e+02 - logprior: -4.0046e+00
Epoch 2/2
19/19 - 2s - loss: 199.9367 - loglik: -1.9807e+02 - logprior: -1.8639e+00
Fitted a model with MAP estimate = -197.7479
expansions: [(0, 2)]
discards: [ 0  7 61 91]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.0637 - loglik: -1.9720e+02 - logprior: -2.8670e+00
Epoch 2/2
19/19 - 2s - loss: 196.5087 - loglik: -1.9545e+02 - logprior: -1.0627e+00
Fitted a model with MAP estimate = -195.4626
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 200.7413 - loglik: -1.9727e+02 - logprior: -3.4697e+00
Epoch 2/10
19/19 - 2s - loss: 196.5350 - loglik: -1.9541e+02 - logprior: -1.1222e+00
Epoch 3/10
19/19 - 2s - loss: 195.7311 - loglik: -1.9475e+02 - logprior: -9.7907e-01
Epoch 4/10
19/19 - 2s - loss: 195.4546 - loglik: -1.9454e+02 - logprior: -9.1956e-01
Epoch 5/10
19/19 - 2s - loss: 194.9649 - loglik: -1.9407e+02 - logprior: -8.9692e-01
Epoch 6/10
19/19 - 2s - loss: 194.6090 - loglik: -1.9373e+02 - logprior: -8.7931e-01
Epoch 7/10
19/19 - 2s - loss: 194.7237 - loglik: -1.9387e+02 - logprior: -8.5135e-01
Fitted a model with MAP estimate = -194.3979
Time for alignment: 66.0829
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 258.9391 - loglik: -2.5584e+02 - logprior: -3.0943e+00
Epoch 2/10
19/19 - 2s - loss: 226.6563 - loglik: -2.2546e+02 - logprior: -1.1919e+00
Epoch 3/10
19/19 - 2s - loss: 212.5556 - loglik: -2.1132e+02 - logprior: -1.2364e+00
Epoch 4/10
19/19 - 2s - loss: 208.7459 - loglik: -2.0738e+02 - logprior: -1.3625e+00
Epoch 5/10
19/19 - 2s - loss: 207.4828 - loglik: -2.0628e+02 - logprior: -1.2000e+00
Epoch 6/10
19/19 - 2s - loss: 206.5612 - loglik: -2.0546e+02 - logprior: -1.1043e+00
Epoch 7/10
19/19 - 2s - loss: 206.1266 - loglik: -2.0508e+02 - logprior: -1.0454e+00
Epoch 8/10
19/19 - 2s - loss: 206.0594 - loglik: -2.0503e+02 - logprior: -1.0311e+00
Epoch 9/10
19/19 - 2s - loss: 205.7549 - loglik: -2.0473e+02 - logprior: -1.0204e+00
Epoch 10/10
19/19 - 2s - loss: 205.7846 - loglik: -2.0477e+02 - logprior: -1.0170e+00
Fitted a model with MAP estimate = -205.1876
expansions: [(7, 2), (9, 1), (10, 1), (13, 1), (14, 1), (15, 1), (22, 1), (23, 1), (28, 1), (29, 1), (36, 2), (37, 1), (49, 1), (64, 1), (65, 2), (73, 4)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.5935 - loglik: -2.0463e+02 - logprior: -3.9681e+00
Epoch 2/2
19/19 - 2s - loss: 198.8673 - loglik: -1.9697e+02 - logprior: -1.8925e+00
Fitted a model with MAP estimate = -196.8211
expansions: [(0, 2)]
discards: [ 0  7 47 80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 198.7470 - loglik: -1.9590e+02 - logprior: -2.8476e+00
Epoch 2/2
19/19 - 2s - loss: 195.3418 - loglik: -1.9431e+02 - logprior: -1.0354e+00
Fitted a model with MAP estimate = -194.3830
expansions: []
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 199.6293 - loglik: -1.9617e+02 - logprior: -3.4607e+00
Epoch 2/10
19/19 - 2s - loss: 195.5576 - loglik: -1.9444e+02 - logprior: -1.1184e+00
Epoch 3/10
19/19 - 2s - loss: 194.3972 - loglik: -1.9344e+02 - logprior: -9.5960e-01
Epoch 4/10
19/19 - 2s - loss: 194.6179 - loglik: -1.9371e+02 - logprior: -9.1214e-01
Fitted a model with MAP estimate = -194.1035
Time for alignment: 61.5594
Computed alignments with likelihoods: ['-194.6521', '-193.8268', '-194.4837', '-194.3979', '-194.1035']
Best model has likelihood: -193.8268  (prior= -0.8543 )
time for generating output: 0.1348
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.9711038985950939
Training of 5 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bd5471bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4a54211df0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49cb5cac70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4c0a188ca0>
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 571.7761 - loglik: -5.7007e+02 - logprior: -1.7066e+00
Epoch 2/10
39/39 - 12s - loss: 492.9619 - loglik: -4.9182e+02 - logprior: -1.1460e+00
Epoch 3/10
39/39 - 12s - loss: 484.0685 - loglik: -4.8296e+02 - logprior: -1.1084e+00
Epoch 4/10
39/39 - 12s - loss: 481.7538 - loglik: -4.8062e+02 - logprior: -1.1319e+00
Epoch 5/10
39/39 - 12s - loss: 480.6883 - loglik: -4.7956e+02 - logprior: -1.1330e+00
Epoch 6/10
39/39 - 12s - loss: 480.1155 - loglik: -4.7899e+02 - logprior: -1.1220e+00
Epoch 7/10
39/39 - 13s - loss: 479.7613 - loglik: -4.7863e+02 - logprior: -1.1268e+00
Epoch 8/10
39/39 - 13s - loss: 479.4868 - loglik: -4.7837e+02 - logprior: -1.1215e+00
Epoch 9/10
39/39 - 13s - loss: 479.4831 - loglik: -4.7837e+02 - logprior: -1.1131e+00
Epoch 10/10
39/39 - 13s - loss: 479.4324 - loglik: -4.7833e+02 - logprior: -1.1053e+00
Fitted a model with MAP estimate = -476.7295
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (23, 1), (36, 1), (46, 1), (50, 1), (51, 1), (57, 1), (69, 1), (70, 1), (72, 1), (86, 1), (88, 1), (89, 1), (91, 2), (94, 1), (96, 1), (102, 1), (108, 1), (109, 1), (113, 1), (116, 1), (124, 3), (141, 2), (142, 1), (146, 2), (149, 2), (152, 2), (153, 1), (154, 1), (155, 1), (159, 1), (162, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 219 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 471.4131 - loglik: -4.6894e+02 - logprior: -2.4749e+00
Epoch 2/2
39/39 - 18s - loss: 461.0811 - loglik: -4.6047e+02 - logprior: -6.1345e-01
Fitted a model with MAP estimate = -457.2637
expansions: [(155, 1)]
discards: [ 23  25 111 175 181 194]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 463.4949 - loglik: -4.6177e+02 - logprior: -1.7240e+00
Epoch 2/2
39/39 - 16s - loss: 460.2093 - loglik: -4.5978e+02 - logprior: -4.2844e-01
Fitted a model with MAP estimate = -456.9311
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 214 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 460.3191 - loglik: -4.5870e+02 - logprior: -1.6236e+00
Epoch 2/10
39/39 - 16s - loss: 457.5150 - loglik: -4.5719e+02 - logprior: -3.2357e-01
Epoch 3/10
39/39 - 16s - loss: 456.5633 - loglik: -4.5632e+02 - logprior: -2.3865e-01
Epoch 4/10
39/39 - 17s - loss: 455.9640 - loglik: -4.5579e+02 - logprior: -1.7778e-01
Epoch 5/10
39/39 - 17s - loss: 455.2112 - loglik: -4.5509e+02 - logprior: -1.1614e-01
Epoch 6/10
39/39 - 17s - loss: 454.8624 - loglik: -4.5481e+02 - logprior: -5.2787e-02
Epoch 7/10
39/39 - 16s - loss: 455.0472 - loglik: -4.5506e+02 - logprior: 0.0128
Fitted a model with MAP estimate = -454.3846
Time for alignment: 400.2684
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 572.4121 - loglik: -5.7070e+02 - logprior: -1.7164e+00
Epoch 2/10
39/39 - 12s - loss: 494.0333 - loglik: -4.9276e+02 - logprior: -1.2755e+00
Epoch 3/10
39/39 - 12s - loss: 484.6009 - loglik: -4.8339e+02 - logprior: -1.2120e+00
Epoch 4/10
39/39 - 12s - loss: 482.6715 - loglik: -4.8149e+02 - logprior: -1.1813e+00
Epoch 5/10
39/39 - 12s - loss: 481.7396 - loglik: -4.8058e+02 - logprior: -1.1602e+00
Epoch 6/10
39/39 - 12s - loss: 481.0949 - loglik: -4.7993e+02 - logprior: -1.1610e+00
Epoch 7/10
39/39 - 13s - loss: 480.9397 - loglik: -4.7978e+02 - logprior: -1.1575e+00
Epoch 8/10
39/39 - 13s - loss: 480.5844 - loglik: -4.7942e+02 - logprior: -1.1613e+00
Epoch 9/10
39/39 - 13s - loss: 480.5667 - loglik: -4.7941e+02 - logprior: -1.1580e+00
Epoch 10/10
39/39 - 13s - loss: 480.4100 - loglik: -4.7926e+02 - logprior: -1.1527e+00
Fitted a model with MAP estimate = -477.7025
expansions: [(0, 3), (19, 2), (20, 1), (27, 1), (32, 1), (35, 1), (46, 1), (49, 1), (51, 1), (55, 2), (65, 2), (69, 1), (72, 1), (86, 1), (88, 2), (89, 2), (90, 1), (91, 1), (93, 1), (102, 1), (108, 1), (109, 1), (113, 1), (115, 1), (116, 1), (124, 2), (135, 1), (141, 1), (144, 4), (145, 2), (152, 2), (154, 1), (155, 1), (159, 1), (162, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 221 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 472.7947 - loglik: -4.7029e+02 - logprior: -2.5041e+00
Epoch 2/2
39/39 - 19s - loss: 461.7809 - loglik: -4.6109e+02 - logprior: -6.9234e-01
Fitted a model with MAP estimate = -457.9700
expansions: []
discards: [ 80 108 110 180 185]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 464.2106 - loglik: -4.6246e+02 - logprior: -1.7526e+00
Epoch 2/2
39/39 - 14s - loss: 461.2975 - loglik: -4.6085e+02 - logprior: -4.4893e-01
Fitted a model with MAP estimate = -457.9395
expansions: []
discards: [68]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 461.4080 - loglik: -4.5980e+02 - logprior: -1.6089e+00
Epoch 2/10
39/39 - 13s - loss: 458.8600 - loglik: -4.5853e+02 - logprior: -3.3072e-01
Epoch 3/10
39/39 - 13s - loss: 457.9131 - loglik: -4.5767e+02 - logprior: -2.4172e-01
Epoch 4/10
39/39 - 13s - loss: 456.9038 - loglik: -4.5673e+02 - logprior: -1.7429e-01
Epoch 5/10
39/39 - 13s - loss: 456.6122 - loglik: -4.5650e+02 - logprior: -1.0923e-01
Epoch 6/10
39/39 - 13s - loss: 455.9146 - loglik: -4.5585e+02 - logprior: -5.9602e-02
Epoch 7/10
39/39 - 13s - loss: 456.0785 - loglik: -4.5609e+02 - logprior: 0.0112
Fitted a model with MAP estimate = -455.5391
Time for alignment: 374.0973
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 571.2546 - loglik: -5.6954e+02 - logprior: -1.7149e+00
Epoch 2/10
39/39 - 10s - loss: 493.0259 - loglik: -4.9184e+02 - logprior: -1.1861e+00
Epoch 3/10
39/39 - 10s - loss: 483.3512 - loglik: -4.8217e+02 - logprior: -1.1790e+00
Epoch 4/10
39/39 - 10s - loss: 481.1302 - loglik: -4.7999e+02 - logprior: -1.1407e+00
Epoch 5/10
39/39 - 10s - loss: 480.2094 - loglik: -4.7909e+02 - logprior: -1.1237e+00
Epoch 6/10
39/39 - 10s - loss: 479.6839 - loglik: -4.7857e+02 - logprior: -1.1160e+00
Epoch 7/10
39/39 - 11s - loss: 479.4501 - loglik: -4.7834e+02 - logprior: -1.1107e+00
Epoch 8/10
39/39 - 11s - loss: 479.1528 - loglik: -4.7804e+02 - logprior: -1.1160e+00
Epoch 9/10
39/39 - 11s - loss: 479.1410 - loglik: -4.7803e+02 - logprior: -1.1121e+00
Epoch 10/10
39/39 - 11s - loss: 479.0430 - loglik: -4.7794e+02 - logprior: -1.1048e+00
Fitted a model with MAP estimate = -476.3275
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (31, 1), (35, 2), (46, 1), (50, 1), (51, 1), (52, 1), (69, 1), (73, 1), (76, 1), (86, 1), (88, 1), (89, 1), (91, 2), (94, 1), (96, 1), (102, 1), (108, 1), (109, 1), (113, 1), (116, 1), (124, 3), (141, 1), (142, 1), (150, 2), (151, 1), (152, 2), (154, 1), (155, 1), (159, 1), (162, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 471.4871 - loglik: -4.6903e+02 - logprior: -2.4613e+00
Epoch 2/2
39/39 - 16s - loss: 461.3342 - loglik: -4.6071e+02 - logprior: -6.2530e-01
Fitted a model with MAP estimate = -457.6993
expansions: [(156, 1), (184, 1)]
discards: [ 23  25  45 112]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 463.3939 - loglik: -4.6168e+02 - logprior: -1.7183e+00
Epoch 2/2
39/39 - 16s - loss: 460.0994 - loglik: -4.5967e+02 - logprior: -4.2926e-01
Fitted a model with MAP estimate = -456.7484
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 460.0943 - loglik: -4.5847e+02 - logprior: -1.6232e+00
Epoch 2/10
39/39 - 16s - loss: 457.4725 - loglik: -4.5714e+02 - logprior: -3.3623e-01
Epoch 3/10
39/39 - 17s - loss: 456.6905 - loglik: -4.5644e+02 - logprior: -2.4680e-01
Epoch 4/10
39/39 - 17s - loss: 455.6878 - loglik: -4.5550e+02 - logprior: -1.8569e-01
Epoch 5/10
39/39 - 16s - loss: 454.9273 - loglik: -4.5481e+02 - logprior: -1.2139e-01
Epoch 6/10
39/39 - 16s - loss: 454.7292 - loglik: -4.5466e+02 - logprior: -6.6216e-02
Epoch 7/10
39/39 - 16s - loss: 455.0666 - loglik: -4.5507e+02 - logprior: -9.9729e-04
Fitted a model with MAP estimate = -454.4650
Time for alignment: 368.6868
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 571.4440 - loglik: -5.6973e+02 - logprior: -1.7114e+00
Epoch 2/10
39/39 - 12s - loss: 493.7674 - loglik: -4.9253e+02 - logprior: -1.2337e+00
Epoch 3/10
39/39 - 12s - loss: 483.5599 - loglik: -4.8233e+02 - logprior: -1.2341e+00
Epoch 4/10
39/39 - 13s - loss: 481.4469 - loglik: -4.8027e+02 - logprior: -1.1770e+00
Epoch 5/10
39/39 - 13s - loss: 480.5721 - loglik: -4.7941e+02 - logprior: -1.1651e+00
Epoch 6/10
39/39 - 13s - loss: 480.0690 - loglik: -4.7891e+02 - logprior: -1.1614e+00
Epoch 7/10
39/39 - 14s - loss: 479.5151 - loglik: -4.7836e+02 - logprior: -1.1572e+00
Epoch 8/10
39/39 - 13s - loss: 479.2307 - loglik: -4.7807e+02 - logprior: -1.1603e+00
Epoch 9/10
39/39 - 14s - loss: 478.9726 - loglik: -4.7782e+02 - logprior: -1.1506e+00
Epoch 10/10
39/39 - 13s - loss: 478.7177 - loglik: -4.7757e+02 - logprior: -1.1506e+00
Fitted a model with MAP estimate = -476.0957
expansions: [(0, 3), (8, 2), (18, 1), (20, 2), (21, 1), (24, 1), (35, 2), (46, 1), (50, 1), (51, 1), (52, 1), (56, 1), (68, 1), (72, 1), (75, 1), (86, 1), (88, 1), (90, 1), (91, 1), (93, 1), (95, 1), (102, 1), (108, 1), (109, 1), (112, 1), (115, 1), (124, 3), (128, 1), (140, 2), (141, 1), (148, 2), (152, 2), (154, 1), (155, 1), (159, 1), (162, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 219 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 471.0363 - loglik: -4.6857e+02 - logprior: -2.4642e+00
Epoch 2/2
39/39 - 19s - loss: 460.5846 - loglik: -4.5996e+02 - logprior: -6.2663e-01
Fitted a model with MAP estimate = -456.7851
expansions: [(187, 1)]
discards: [ 12  24  27  46 177]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 463.0527 - loglik: -4.6132e+02 - logprior: -1.7327e+00
Epoch 2/2
39/39 - 19s - loss: 459.9582 - loglik: -4.5951e+02 - logprior: -4.4463e-01
Fitted a model with MAP estimate = -456.6960
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 459.8870 - loglik: -4.5825e+02 - logprior: -1.6353e+00
Epoch 2/10
39/39 - 17s - loss: 457.4850 - loglik: -4.5714e+02 - logprior: -3.4029e-01
Epoch 3/10
39/39 - 17s - loss: 456.2093 - loglik: -4.5596e+02 - logprior: -2.5296e-01
Epoch 4/10
39/39 - 18s - loss: 455.5494 - loglik: -4.5536e+02 - logprior: -1.8460e-01
Epoch 5/10
39/39 - 17s - loss: 454.8939 - loglik: -4.5476e+02 - logprior: -1.3290e-01
Epoch 6/10
39/39 - 17s - loss: 454.7228 - loglik: -4.5465e+02 - logprior: -6.7830e-02
Epoch 7/10
39/39 - 17s - loss: 454.7281 - loglik: -4.5471e+02 - logprior: -1.6849e-02
Fitted a model with MAP estimate = -454.1174
Time for alignment: 420.0600
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 571.0410 - loglik: -5.6934e+02 - logprior: -1.7020e+00
Epoch 2/10
39/39 - 12s - loss: 493.4767 - loglik: -4.9232e+02 - logprior: -1.1526e+00
Epoch 3/10
39/39 - 12s - loss: 483.8455 - loglik: -4.8271e+02 - logprior: -1.1336e+00
Epoch 4/10
39/39 - 12s - loss: 482.0417 - loglik: -4.8096e+02 - logprior: -1.0821e+00
Epoch 5/10
39/39 - 12s - loss: 481.0215 - loglik: -4.7996e+02 - logprior: -1.0653e+00
Epoch 6/10
39/39 - 12s - loss: 480.6618 - loglik: -4.7959e+02 - logprior: -1.0673e+00
Epoch 7/10
39/39 - 12s - loss: 480.3415 - loglik: -4.7928e+02 - logprior: -1.0657e+00
Epoch 8/10
39/39 - 12s - loss: 479.9613 - loglik: -4.7892e+02 - logprior: -1.0460e+00
Epoch 9/10
39/39 - 12s - loss: 480.0557 - loglik: -4.7901e+02 - logprior: -1.0420e+00
Fitted a model with MAP estimate = -477.2854
expansions: [(0, 3), (19, 2), (20, 4), (35, 2), (46, 1), (47, 1), (49, 1), (52, 1), (58, 1), (68, 1), (72, 1), (86, 1), (88, 2), (89, 1), (90, 1), (91, 1), (93, 1), (102, 1), (108, 1), (109, 1), (113, 1), (116, 1), (135, 1), (141, 1), (149, 3), (150, 1), (152, 3), (154, 1), (155, 2), (159, 2)]
discards: [144]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 216 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 474.7945 - loglik: -4.7226e+02 - logprior: -2.5345e+00
Epoch 2/2
39/39 - 19s - loss: 464.2932 - loglik: -4.6360e+02 - logprior: -6.9781e-01
Fitted a model with MAP estimate = -460.4488
expansions: [(155, 3), (156, 1), (181, 1), (183, 1)]
discards: [ 25  44 108 188 194]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 463.7422 - loglik: -4.6200e+02 - logprior: -1.7450e+00
Epoch 2/2
39/39 - 18s - loss: 459.9750 - loglik: -4.5953e+02 - logprior: -4.4982e-01
Fitted a model with MAP estimate = -456.5772
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 217 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 460.0577 - loglik: -4.5842e+02 - logprior: -1.6413e+00
Epoch 2/10
39/39 - 17s - loss: 457.6374 - loglik: -4.5729e+02 - logprior: -3.5009e-01
Epoch 3/10
39/39 - 18s - loss: 456.2629 - loglik: -4.5601e+02 - logprior: -2.4811e-01
Epoch 4/10
39/39 - 18s - loss: 455.7209 - loglik: -4.5554e+02 - logprior: -1.8529e-01
Epoch 5/10
39/39 - 17s - loss: 455.1844 - loglik: -4.5507e+02 - logprior: -1.1803e-01
Epoch 6/10
39/39 - 17s - loss: 454.6380 - loglik: -4.5459e+02 - logprior: -5.2815e-02
Epoch 7/10
39/39 - 17s - loss: 454.5648 - loglik: -4.5457e+02 - logprior: 0.0030
Epoch 8/10
39/39 - 18s - loss: 454.6101 - loglik: -4.5467e+02 - logprior: 0.0569
Fitted a model with MAP estimate = -454.2720
Time for alignment: 413.8518
Computed alignments with likelihoods: ['-454.3846', '-455.5391', '-454.4650', '-454.1174', '-454.2720']
Best model has likelihood: -454.1174  (prior= 0.0013 )
time for generating output: 0.3116
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.6931541941937285
Training of 5 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d855de1c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bcc1ea0d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bdf5cc880>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d7cd0be50>
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 168.2159 - loglik: -1.6506e+02 - logprior: -3.1511e+00
Epoch 2/10
19/19 - 1s - loss: 139.2457 - loglik: -1.3793e+02 - logprior: -1.3190e+00
Epoch 3/10
19/19 - 1s - loss: 128.8968 - loglik: -1.2738e+02 - logprior: -1.5134e+00
Epoch 4/10
19/19 - 1s - loss: 127.1463 - loglik: -1.2579e+02 - logprior: -1.3522e+00
Epoch 5/10
19/19 - 1s - loss: 126.4807 - loglik: -1.2514e+02 - logprior: -1.3455e+00
Epoch 6/10
19/19 - 1s - loss: 126.2871 - loglik: -1.2495e+02 - logprior: -1.3394e+00
Epoch 7/10
19/19 - 1s - loss: 126.2314 - loglik: -1.2491e+02 - logprior: -1.3254e+00
Epoch 8/10
19/19 - 1s - loss: 125.8921 - loglik: -1.2457e+02 - logprior: -1.3217e+00
Epoch 9/10
19/19 - 1s - loss: 126.1411 - loglik: -1.2483e+02 - logprior: -1.3157e+00
Fitted a model with MAP estimate = -125.8896
expansions: [(11, 1), (12, 6), (13, 1), (28, 2), (29, 2), (30, 2), (31, 1), (33, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 134.2287 - loglik: -1.3013e+02 - logprior: -4.0993e+00
Epoch 2/2
19/19 - 1s - loss: 127.0250 - loglik: -1.2483e+02 - logprior: -2.1926e+00
Fitted a model with MAP estimate = -124.2935
expansions: []
discards: [35 40 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.4987 - loglik: -1.2312e+02 - logprior: -3.3828e+00
Epoch 2/2
19/19 - 1s - loss: 123.0378 - loglik: -1.2165e+02 - logprior: -1.3880e+00
Fitted a model with MAP estimate = -122.5632
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 125.6250 - loglik: -1.2239e+02 - logprior: -3.2337e+00
Epoch 2/10
19/19 - 1s - loss: 122.9400 - loglik: -1.2157e+02 - logprior: -1.3734e+00
Epoch 3/10
19/19 - 1s - loss: 122.3361 - loglik: -1.2106e+02 - logprior: -1.2758e+00
Epoch 4/10
19/19 - 1s - loss: 122.2488 - loglik: -1.2102e+02 - logprior: -1.2247e+00
Epoch 5/10
19/19 - 1s - loss: 122.0854 - loglik: -1.2090e+02 - logprior: -1.1852e+00
Epoch 6/10
19/19 - 1s - loss: 121.9150 - loglik: -1.2074e+02 - logprior: -1.1772e+00
Epoch 7/10
19/19 - 1s - loss: 121.9162 - loglik: -1.2076e+02 - logprior: -1.1576e+00
Fitted a model with MAP estimate = -121.7168
Time for alignment: 43.3313
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 168.2695 - loglik: -1.6512e+02 - logprior: -3.1508e+00
Epoch 2/10
19/19 - 1s - loss: 139.1248 - loglik: -1.3780e+02 - logprior: -1.3289e+00
Epoch 3/10
19/19 - 1s - loss: 127.9175 - loglik: -1.2636e+02 - logprior: -1.5580e+00
Epoch 4/10
19/19 - 1s - loss: 126.2281 - loglik: -1.2482e+02 - logprior: -1.4035e+00
Epoch 5/10
19/19 - 1s - loss: 125.7578 - loglik: -1.2437e+02 - logprior: -1.3881e+00
Epoch 6/10
19/19 - 1s - loss: 125.5574 - loglik: -1.2418e+02 - logprior: -1.3779e+00
Epoch 7/10
19/19 - 1s - loss: 125.4654 - loglik: -1.2410e+02 - logprior: -1.3658e+00
Epoch 8/10
19/19 - 1s - loss: 125.3019 - loglik: -1.2394e+02 - logprior: -1.3619e+00
Epoch 9/10
19/19 - 1s - loss: 125.0002 - loglik: -1.2364e+02 - logprior: -1.3563e+00
Epoch 10/10
19/19 - 1s - loss: 125.4091 - loglik: -1.2406e+02 - logprior: -1.3538e+00
Fitted a model with MAP estimate = -125.1121
expansions: [(11, 1), (12, 3), (13, 3), (14, 1), (27, 2), (28, 2), (29, 2), (30, 2), (31, 1), (33, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.9384 - loglik: -1.3082e+02 - logprior: -4.1176e+00
Epoch 2/2
19/19 - 1s - loss: 127.4217 - loglik: -1.2516e+02 - logprior: -2.2630e+00
Fitted a model with MAP estimate = -124.6026
expansions: []
discards: [12 13 16 35 37 42 43]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 127.0438 - loglik: -1.2366e+02 - logprior: -3.3864e+00
Epoch 2/2
19/19 - 1s - loss: 123.3431 - loglik: -1.2196e+02 - logprior: -1.3809e+00
Fitted a model with MAP estimate = -122.7220
expansions: [(16, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.8845 - loglik: -1.2263e+02 - logprior: -3.2536e+00
Epoch 2/10
19/19 - 1s - loss: 122.8655 - loglik: -1.2150e+02 - logprior: -1.3683e+00
Epoch 3/10
19/19 - 1s - loss: 122.3267 - loglik: -1.2105e+02 - logprior: -1.2758e+00
Epoch 4/10
19/19 - 1s - loss: 122.1569 - loglik: -1.2093e+02 - logprior: -1.2279e+00
Epoch 5/10
19/19 - 1s - loss: 121.7966 - loglik: -1.2061e+02 - logprior: -1.1876e+00
Epoch 6/10
19/19 - 1s - loss: 122.0994 - loglik: -1.2093e+02 - logprior: -1.1689e+00
Fitted a model with MAP estimate = -121.7217
Time for alignment: 41.2970
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 168.3556 - loglik: -1.6520e+02 - logprior: -3.1508e+00
Epoch 2/10
19/19 - 1s - loss: 139.4737 - loglik: -1.3815e+02 - logprior: -1.3248e+00
Epoch 3/10
19/19 - 1s - loss: 128.8782 - loglik: -1.2735e+02 - logprior: -1.5286e+00
Epoch 4/10
19/19 - 1s - loss: 127.0350 - loglik: -1.2567e+02 - logprior: -1.3645e+00
Epoch 5/10
19/19 - 1s - loss: 126.5694 - loglik: -1.2522e+02 - logprior: -1.3496e+00
Epoch 6/10
19/19 - 1s - loss: 126.4070 - loglik: -1.2507e+02 - logprior: -1.3393e+00
Epoch 7/10
19/19 - 1s - loss: 126.3359 - loglik: -1.2500e+02 - logprior: -1.3309e+00
Epoch 8/10
19/19 - 1s - loss: 125.9737 - loglik: -1.2465e+02 - logprior: -1.3217e+00
Epoch 9/10
19/19 - 1s - loss: 126.0737 - loglik: -1.2475e+02 - logprior: -1.3187e+00
Fitted a model with MAP estimate = -125.9854
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (28, 2), (29, 2), (30, 2), (31, 1), (33, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 134.5496 - loglik: -1.3044e+02 - logprior: -4.1069e+00
Epoch 2/2
19/19 - 1s - loss: 127.1587 - loglik: -1.2493e+02 - logprior: -2.2239e+00
Fitted a model with MAP estimate = -124.3924
expansions: []
discards: [16 17 37 42 43]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.5145 - loglik: -1.2312e+02 - logprior: -3.3959e+00
Epoch 2/2
19/19 - 1s - loss: 122.9009 - loglik: -1.2151e+02 - logprior: -1.3937e+00
Fitted a model with MAP estimate = -122.4563
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.5711 - loglik: -1.2233e+02 - logprior: -3.2418e+00
Epoch 2/10
19/19 - 1s - loss: 122.8143 - loglik: -1.2144e+02 - logprior: -1.3695e+00
Epoch 3/10
19/19 - 1s - loss: 122.3516 - loglik: -1.2107e+02 - logprior: -1.2784e+00
Epoch 4/10
19/19 - 1s - loss: 122.1555 - loglik: -1.2094e+02 - logprior: -1.2192e+00
Epoch 5/10
19/19 - 1s - loss: 121.9314 - loglik: -1.2074e+02 - logprior: -1.1919e+00
Epoch 6/10
19/19 - 1s - loss: 121.7276 - loglik: -1.2056e+02 - logprior: -1.1715e+00
Epoch 7/10
19/19 - 1s - loss: 121.7943 - loglik: -1.2064e+02 - logprior: -1.1590e+00
Fitted a model with MAP estimate = -121.6292
Time for alignment: 41.1629
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 168.3700 - loglik: -1.6522e+02 - logprior: -3.1505e+00
Epoch 2/10
19/19 - 1s - loss: 139.4001 - loglik: -1.3808e+02 - logprior: -1.3198e+00
Epoch 3/10
19/19 - 1s - loss: 129.0200 - loglik: -1.2750e+02 - logprior: -1.5179e+00
Epoch 4/10
19/19 - 1s - loss: 127.2146 - loglik: -1.2586e+02 - logprior: -1.3567e+00
Epoch 5/10
19/19 - 1s - loss: 126.5645 - loglik: -1.2522e+02 - logprior: -1.3441e+00
Epoch 6/10
19/19 - 1s - loss: 126.4519 - loglik: -1.2512e+02 - logprior: -1.3335e+00
Epoch 7/10
19/19 - 1s - loss: 126.2514 - loglik: -1.2492e+02 - logprior: -1.3279e+00
Epoch 8/10
19/19 - 1s - loss: 125.9289 - loglik: -1.2461e+02 - logprior: -1.3185e+00
Epoch 9/10
19/19 - 1s - loss: 126.0510 - loglik: -1.2473e+02 - logprior: -1.3162e+00
Fitted a model with MAP estimate = -125.9025
expansions: [(11, 1), (12, 6), (13, 1), (28, 2), (29, 2), (30, 2), (31, 1), (33, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.1738 - loglik: -1.3008e+02 - logprior: -4.0957e+00
Epoch 2/2
19/19 - 1s - loss: 127.0959 - loglik: -1.2489e+02 - logprior: -2.2016e+00
Fitted a model with MAP estimate = -124.3054
expansions: []
discards: [35 40 41]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.5109 - loglik: -1.2313e+02 - logprior: -3.3825e+00
Epoch 2/2
19/19 - 1s - loss: 122.9968 - loglik: -1.2161e+02 - logprior: -1.3869e+00
Fitted a model with MAP estimate = -122.5511
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.6345 - loglik: -1.2240e+02 - logprior: -3.2355e+00
Epoch 2/10
19/19 - 1s - loss: 122.8921 - loglik: -1.2152e+02 - logprior: -1.3701e+00
Epoch 3/10
19/19 - 1s - loss: 122.4820 - loglik: -1.2121e+02 - logprior: -1.2757e+00
Epoch 4/10
19/19 - 1s - loss: 122.1152 - loglik: -1.2089e+02 - logprior: -1.2207e+00
Epoch 5/10
19/19 - 1s - loss: 121.9831 - loglik: -1.2080e+02 - logprior: -1.1873e+00
Epoch 6/10
19/19 - 1s - loss: 121.9867 - loglik: -1.2081e+02 - logprior: -1.1757e+00
Fitted a model with MAP estimate = -121.7175
Time for alignment: 40.7921
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 168.3153 - loglik: -1.6516e+02 - logprior: -3.1527e+00
Epoch 2/10
19/19 - 1s - loss: 139.5891 - loglik: -1.3827e+02 - logprior: -1.3231e+00
Epoch 3/10
19/19 - 1s - loss: 128.9185 - loglik: -1.2739e+02 - logprior: -1.5268e+00
Epoch 4/10
19/19 - 1s - loss: 127.2230 - loglik: -1.2586e+02 - logprior: -1.3650e+00
Epoch 5/10
19/19 - 1s - loss: 126.5033 - loglik: -1.2515e+02 - logprior: -1.3521e+00
Epoch 6/10
19/19 - 1s - loss: 126.2979 - loglik: -1.2496e+02 - logprior: -1.3387e+00
Epoch 7/10
19/19 - 1s - loss: 126.4253 - loglik: -1.2509e+02 - logprior: -1.3308e+00
Fitted a model with MAP estimate = -126.1154
expansions: [(11, 1), (12, 3), (13, 3), (14, 2), (28, 2), (29, 2), (30, 2), (31, 1), (33, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.3381 - loglik: -1.3021e+02 - logprior: -4.1290e+00
Epoch 2/2
19/19 - 1s - loss: 126.8617 - loglik: -1.2466e+02 - logprior: -2.1990e+00
Fitted a model with MAP estimate = -124.0024
expansions: []
discards: [12 13 16 36 41 42]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 126.8677 - loglik: -1.2353e+02 - logprior: -3.3385e+00
Epoch 2/2
19/19 - 1s - loss: 123.3340 - loglik: -1.2195e+02 - logprior: -1.3848e+00
Fitted a model with MAP estimate = -122.7889
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.8574 - loglik: -1.2263e+02 - logprior: -3.2305e+00
Epoch 2/10
19/19 - 1s - loss: 123.0273 - loglik: -1.2166e+02 - logprior: -1.3673e+00
Epoch 3/10
19/19 - 1s - loss: 122.4877 - loglik: -1.2122e+02 - logprior: -1.2707e+00
Epoch 4/10
19/19 - 1s - loss: 122.5584 - loglik: -1.2135e+02 - logprior: -1.2134e+00
Fitted a model with MAP estimate = -122.1870
Time for alignment: 35.4585
Computed alignments with likelihoods: ['-121.7168', '-121.7217', '-121.6292', '-121.7175', '-122.1870']
Best model has likelihood: -121.6292  (prior= -1.1655 )
time for generating output: 0.0922
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.8855421686746988
Training of 5 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d6b8b6970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d1bd3bcd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bcc0bd1c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f49c8441b80>
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 76s - loss: 937.4471 - loglik: -9.3599e+02 - logprior: -1.4578e+00
Epoch 2/10
39/39 - 80s - loss: 738.6221 - loglik: -7.3698e+02 - logprior: -1.6372e+00
Epoch 3/10
39/39 - 74s - loss: 719.8059 - loglik: -7.1812e+02 - logprior: -1.6814e+00
Epoch 4/10
39/39 - 74s - loss: 716.3710 - loglik: -7.1484e+02 - logprior: -1.5314e+00
Epoch 5/10
39/39 - 81s - loss: 713.6880 - loglik: -7.1226e+02 - logprior: -1.4231e+00
Epoch 6/10
39/39 - 85s - loss: 713.3928 - loglik: -7.1198e+02 - logprior: -1.4138e+00
Epoch 7/10
39/39 - 85s - loss: 712.0071 - loglik: -7.1056e+02 - logprior: -1.4433e+00
Epoch 8/10
39/39 - 77s - loss: 711.6017 - loglik: -7.1009e+02 - logprior: -1.5092e+00
Epoch 9/10
39/39 - 73s - loss: 711.1396 - loglik: -7.0960e+02 - logprior: -1.5352e+00
Epoch 10/10
39/39 - 76s - loss: 710.3305 - loglik: -7.0878e+02 - logprior: -1.5547e+00
Fitted a model with MAP estimate = -709.3924
expansions: [(0, 3), (41, 3), (63, 1), (85, 1), (88, 1), (90, 1), (97, 1), (99, 2), (101, 1), (122, 1), (123, 4), (129, 2), (131, 1), (137, 1), (147, 1), (148, 1), (159, 1), (160, 6), (161, 2), (162, 1), (163, 1), (176, 1), (177, 1), (179, 3), (180, 1), (181, 1), (184, 1), (185, 1), (186, 1), (187, 2), (191, 1), (194, 1), (199, 1), (207, 2), (208, 1), (209, 2), (213, 1), (215, 5), (216, 1), (219, 1), (220, 3), (221, 1), (222, 1), (223, 1), (243, 2), (244, 2), (247, 1), (249, 4), (250, 2), (256, 1), (272, 1), (273, 1), (274, 1), (287, 1), (289, 1), (290, 4), (291, 1), (292, 1), (301, 1), (303, 1), (304, 1), (305, 1), (317, 1), (327, 1), (330, 1), (342, 1), (346, 1), (352, 1), (354, 2), (355, 3)]
discards: [  2 118]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 98s - loss: 681.3614 - loglik: -6.7904e+02 - logprior: -2.3251e+00
Epoch 2/2
39/39 - 92s - loss: 661.3019 - loglik: -6.6043e+02 - logprior: -8.6704e-01
Fitted a model with MAP estimate = -658.7824
expansions: [(141, 1), (215, 1), (325, 1), (378, 1)]
discards: [  2 186 187 188 189 190 191 261 272 316 317 318 319 320 321 459]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 449 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 95s - loss: 666.9423 - loglik: -6.6552e+02 - logprior: -1.4262e+00
Epoch 2/2
39/39 - 92s - loss: 662.7652 - loglik: -6.6240e+02 - logprior: -3.6544e-01
Fitted a model with MAP estimate = -661.2272
expansions: [(306, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 83s - loss: 664.5742 - loglik: -6.6318e+02 - logprior: -1.3928e+00
Epoch 2/10
39/39 - 78s - loss: 661.4678 - loglik: -6.6125e+02 - logprior: -2.1493e-01
Epoch 3/10
39/39 - 75s - loss: 660.3937 - loglik: -6.6041e+02 - logprior: 0.0173
Epoch 4/10
39/39 - 73s - loss: 659.3632 - loglik: -6.5946e+02 - logprior: 0.0999
Epoch 5/10
39/39 - 72s - loss: 658.3211 - loglik: -6.5836e+02 - logprior: 0.0423
Epoch 6/10
39/39 - 72s - loss: 657.0757 - loglik: -6.5732e+02 - logprior: 0.2424
Epoch 7/10
39/39 - 73s - loss: 655.7468 - loglik: -6.5605e+02 - logprior: 0.2984
Epoch 8/10
39/39 - 74s - loss: 655.2843 - loglik: -6.5583e+02 - logprior: 0.5451
Epoch 9/10
39/39 - 76s - loss: 655.6844 - loglik: -6.5624e+02 - logprior: 0.5518
Fitted a model with MAP estimate = -654.0076
Time for alignment: 2273.7090
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 936.4998 - loglik: -9.3500e+02 - logprior: -1.5030e+00
Epoch 2/10
39/39 - 52s - loss: 737.6137 - loglik: -7.3602e+02 - logprior: -1.5983e+00
Epoch 3/10
39/39 - 53s - loss: 720.0091 - loglik: -7.1835e+02 - logprior: -1.6596e+00
Epoch 4/10
39/39 - 55s - loss: 716.3159 - loglik: -7.1466e+02 - logprior: -1.6552e+00
Epoch 5/10
39/39 - 57s - loss: 715.6918 - loglik: -7.1395e+02 - logprior: -1.7465e+00
Epoch 6/10
39/39 - 58s - loss: 715.2214 - loglik: -7.1338e+02 - logprior: -1.8373e+00
Epoch 7/10
39/39 - 59s - loss: 713.9290 - loglik: -7.1198e+02 - logprior: -1.9497e+00
Epoch 8/10
39/39 - 59s - loss: 714.0161 - loglik: -7.1210e+02 - logprior: -1.9169e+00
Fitted a model with MAP estimate = -712.7919
expansions: [(0, 3), (41, 3), (64, 1), (85, 1), (99, 1), (101, 2), (125, 1), (126, 4), (137, 1), (138, 1), (140, 1), (151, 1), (152, 1), (163, 1), (165, 9), (167, 1), (179, 1), (180, 2), (181, 3), (182, 2), (183, 1), (185, 1), (186, 1), (187, 1), (188, 1), (190, 1), (196, 1), (201, 1), (204, 1), (205, 1), (207, 1), (209, 1), (210, 1), (215, 1), (217, 7), (221, 2), (222, 1), (223, 1), (225, 1), (226, 1), (241, 1), (242, 1), (243, 2), (246, 1), (248, 4), (249, 2), (252, 1), (254, 2), (269, 1), (270, 1), (271, 1), (286, 2), (287, 2), (288, 1), (291, 2), (300, 1), (302, 1), (303, 1), (304, 1), (326, 2), (329, 1), (341, 1), (348, 1), (351, 1), (352, 2), (355, 2)]
discards: [  2 121 218 219]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 456 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 128s - loss: 684.7897 - loglik: -6.8261e+02 - logprior: -2.1754e+00
Epoch 2/2
39/39 - 121s - loss: 665.4995 - loglik: -6.6474e+02 - logprior: -7.5603e-01
Fitted a model with MAP estimate = -662.6224
expansions: [(141, 1), (273, 1), (279, 1), (319, 1)]
discards: [  2 186 187 188 189 190 191 311 312 313 314 315 329 369 373 454]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 444 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 101s - loss: 671.0453 - loglik: -6.6943e+02 - logprior: -1.6175e+00
Epoch 2/2
39/39 - 122s - loss: 666.6324 - loglik: -6.6636e+02 - logprior: -2.7018e-01
Fitted a model with MAP estimate = -664.6318
expansions: [(304, 3), (365, 1)]
discards: [306]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 447 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 116s - loss: 668.6664 - loglik: -6.6736e+02 - logprior: -1.3037e+00
Epoch 2/10
39/39 - 105s - loss: 664.9445 - loglik: -6.6469e+02 - logprior: -2.5160e-01
Epoch 3/10
39/39 - 107s - loss: 664.2113 - loglik: -6.6409e+02 - logprior: -1.2241e-01
Epoch 4/10
39/39 - 123s - loss: 662.8157 - loglik: -6.6292e+02 - logprior: 0.1070
Epoch 5/10
39/39 - 127s - loss: 662.1600 - loglik: -6.6205e+02 - logprior: -1.1385e-01
Epoch 6/10
39/39 - 133s - loss: 660.1803 - loglik: -6.6041e+02 - logprior: 0.2322
Epoch 7/10
39/39 - 120s - loss: 660.4209 - loglik: -6.6081e+02 - logprior: 0.3877
Fitted a model with MAP estimate = -658.6714
Time for alignment: 2293.7056
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 73s - loss: 939.1155 - loglik: -9.3759e+02 - logprior: -1.5285e+00
Epoch 2/10
39/39 - 73s - loss: 734.5421 - loglik: -7.3278e+02 - logprior: -1.7624e+00
Epoch 3/10
39/39 - 70s - loss: 715.5984 - loglik: -7.1376e+02 - logprior: -1.8405e+00
Epoch 4/10
39/39 - 75s - loss: 712.7568 - loglik: -7.1091e+02 - logprior: -1.8481e+00
Epoch 5/10
39/39 - 85s - loss: 712.0161 - loglik: -7.1000e+02 - logprior: -2.0151e+00
Epoch 6/10
39/39 - 84s - loss: 710.8195 - loglik: -7.0890e+02 - logprior: -1.9243e+00
Epoch 7/10
39/39 - 89s - loss: 708.9775 - loglik: -7.0706e+02 - logprior: -1.9210e+00
Epoch 8/10
39/39 - 78s - loss: 709.9463 - loglik: -7.0785e+02 - logprior: -2.1003e+00
Fitted a model with MAP estimate = -707.8884
expansions: [(0, 3), (41, 3), (64, 1), (85, 1), (95, 1), (101, 2), (124, 1), (125, 2), (126, 1), (127, 1), (132, 1), (135, 1), (136, 1), (140, 1), (152, 1), (156, 1), (162, 1), (164, 9), (166, 1), (179, 1), (180, 1), (182, 4), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (191, 1), (193, 1), (196, 1), (201, 1), (204, 1), (208, 1), (210, 1), (211, 1), (215, 2), (217, 5), (218, 2), (220, 1), (221, 2), (222, 1), (223, 1), (224, 2), (242, 1), (243, 1), (244, 1), (245, 2), (248, 2), (249, 6), (250, 1), (253, 1), (254, 1), (267, 1), (269, 1), (270, 1), (287, 2), (288, 4), (290, 1), (301, 1), (303, 1), (304, 1), (305, 1), (312, 1), (326, 2), (329, 1), (338, 1), (345, 1), (351, 1), (352, 1), (355, 3)]
discards: [1]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 465 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 121s - loss: 680.1633 - loglik: -6.7772e+02 - logprior: -2.4407e+00
Epoch 2/2
39/39 - 121s - loss: 660.5477 - loglik: -6.5974e+02 - logprior: -8.0696e-01
Fitted a model with MAP estimate = -657.7426
expansions: [(285, 1), (328, 2), (460, 1)]
discards: [  2   3 187 188 189 190 191 192 219 220 274 275 291 318 319 320 321 322
 323 324 331 463 464]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 446 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 115s - loss: 668.5263 - loglik: -6.6718e+02 - logprior: -1.3425e+00
Epoch 2/2
39/39 - 106s - loss: 663.2663 - loglik: -6.6314e+02 - logprior: -1.2932e-01
Fitted a model with MAP estimate = -661.3068
expansions: [(303, 2), (304, 2), (309, 1)]
discards: [311 312]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 124s - loss: 665.4440 - loglik: -6.6419e+02 - logprior: -1.2588e+00
Epoch 2/10
39/39 - 95s - loss: 662.3802 - loglik: -6.6232e+02 - logprior: -5.7301e-02
Epoch 3/10
39/39 - 91s - loss: 661.0501 - loglik: -6.6125e+02 - logprior: 0.1950
Epoch 4/10
39/39 - 90s - loss: 659.7369 - loglik: -6.6010e+02 - logprior: 0.3673
Epoch 5/10
39/39 - 90s - loss: 658.6006 - loglik: -6.5890e+02 - logprior: 0.2950
Epoch 6/10
39/39 - 89s - loss: 657.0962 - loglik: -6.5761e+02 - logprior: 0.5096
Epoch 7/10
39/39 - 90s - loss: 657.7457 - loglik: -6.5829e+02 - logprior: 0.5458
Fitted a model with MAP estimate = -656.2236
Time for alignment: 2342.4580
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 58s - loss: 936.9234 - loglik: -9.3543e+02 - logprior: -1.4937e+00
Epoch 2/10
39/39 - 56s - loss: 735.5837 - loglik: -7.3383e+02 - logprior: -1.7538e+00
Epoch 3/10
39/39 - 56s - loss: 717.9530 - loglik: -7.1613e+02 - logprior: -1.8220e+00
Epoch 4/10
39/39 - 57s - loss: 714.2038 - loglik: -7.1250e+02 - logprior: -1.7078e+00
Epoch 5/10
39/39 - 57s - loss: 713.8160 - loglik: -7.1211e+02 - logprior: -1.7046e+00
Epoch 6/10
39/39 - 58s - loss: 712.9003 - loglik: -7.1103e+02 - logprior: -1.8720e+00
Epoch 7/10
39/39 - 59s - loss: 711.8835 - loglik: -7.1010e+02 - logprior: -1.7826e+00
Epoch 8/10
39/39 - 59s - loss: 711.0168 - loglik: -7.0908e+02 - logprior: -1.9411e+00
Epoch 9/10
39/39 - 60s - loss: 711.1360 - loglik: -7.0920e+02 - logprior: -1.9403e+00
Fitted a model with MAP estimate = -709.8983
expansions: [(0, 3), (41, 3), (84, 2), (85, 1), (88, 1), (98, 1), (100, 2), (115, 1), (123, 1), (124, 4), (126, 1), (129, 2), (132, 1), (136, 1), (147, 1), (148, 1), (159, 1), (160, 6), (161, 1), (162, 1), (164, 1), (176, 1), (177, 1), (178, 5), (179, 1), (180, 2), (182, 1), (183, 1), (184, 1), (185, 1), (187, 1), (191, 1), (193, 1), (197, 1), (200, 1), (204, 1), (206, 1), (207, 1), (212, 1), (214, 5), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (241, 2), (242, 2), (245, 1), (247, 4), (248, 1), (251, 1), (253, 2), (268, 1), (269, 1), (286, 1), (288, 2), (289, 2), (291, 1), (302, 1), (304, 4), (312, 1), (325, 1), (329, 1), (341, 1), (345, 1), (351, 1), (352, 2), (355, 2)]
discards: [  2 119]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 89s - loss: 682.8057 - loglik: -6.8056e+02 - logprior: -2.2423e+00
Epoch 2/2
39/39 - 78s - loss: 662.3201 - loglik: -6.6148e+02 - logprior: -8.4381e-01
Fitted a model with MAP estimate = -659.5486
expansions: [(142, 1), (216, 1), (397, 1), (453, 1)]
discards: [  2  90 187 188 189 190 191 192 218 219 314 315 316 318 319 332 372 457
 458]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 444 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 95s - loss: 669.4349 - loglik: -6.6792e+02 - logprior: -1.5153e+00
Epoch 2/2
39/39 - 103s - loss: 664.7934 - loglik: -6.6437e+02 - logprior: -4.2011e-01
Fitted a model with MAP estimate = -662.7202
expansions: [(302, 2), (303, 2), (305, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 107s - loss: 666.2382 - loglik: -6.6500e+02 - logprior: -1.2352e+00
Epoch 2/10
39/39 - 105s - loss: 663.0913 - loglik: -6.6286e+02 - logprior: -2.3360e-01
Epoch 3/10
39/39 - 113s - loss: 662.1224 - loglik: -6.6207e+02 - logprior: -4.9426e-02
Epoch 4/10
39/39 - 106s - loss: 661.4371 - loglik: -6.6150e+02 - logprior: 0.0618
Epoch 5/10
39/39 - 106s - loss: 659.5679 - loglik: -6.5979e+02 - logprior: 0.2219
Epoch 6/10
39/39 - 105s - loss: 658.4117 - loglik: -6.5873e+02 - logprior: 0.3166
Epoch 7/10
39/39 - 99s - loss: 657.6107 - loglik: -6.5787e+02 - logprior: 0.2593
Epoch 8/10
39/39 - 91s - loss: 657.4777 - loglik: -6.5784e+02 - logprior: 0.3625
Epoch 9/10
39/39 - 87s - loss: 657.2474 - loglik: -6.5788e+02 - logprior: 0.6366
Epoch 10/10
39/39 - 79s - loss: 656.0230 - loglik: -6.5662e+02 - logprior: 0.5994
Fitted a model with MAP estimate = -655.8209
Time for alignment: 2303.7467
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 937.9194 - loglik: -9.3644e+02 - logprior: -1.4746e+00
Epoch 2/10
39/39 - 50s - loss: 735.5972 - loglik: -7.3394e+02 - logprior: -1.6571e+00
Epoch 3/10
39/39 - 50s - loss: 718.3626 - loglik: -7.1669e+02 - logprior: -1.6739e+00
Epoch 4/10
39/39 - 51s - loss: 715.2853 - loglik: -7.1370e+02 - logprior: -1.5823e+00
Epoch 5/10
39/39 - 51s - loss: 714.3559 - loglik: -7.1277e+02 - logprior: -1.5879e+00
Epoch 6/10
39/39 - 51s - loss: 713.2672 - loglik: -7.1128e+02 - logprior: -1.9878e+00
Epoch 7/10
39/39 - 52s - loss: 712.9183 - loglik: -7.1117e+02 - logprior: -1.7487e+00
Epoch 8/10
39/39 - 52s - loss: 712.0538 - loglik: -7.1030e+02 - logprior: -1.7524e+00
Epoch 9/10
39/39 - 54s - loss: 711.0739 - loglik: -7.0929e+02 - logprior: -1.7845e+00
Epoch 10/10
39/39 - 58s - loss: 711.5386 - loglik: -7.0977e+02 - logprior: -1.7650e+00
Fitted a model with MAP estimate = -710.1549
expansions: [(0, 3), (41, 2), (67, 1), (85, 1), (102, 2), (126, 1), (127, 2), (128, 1), (129, 1), (134, 1), (137, 2), (138, 1), (141, 1), (153, 1), (157, 1), (163, 1), (165, 7), (166, 1), (167, 1), (180, 1), (181, 1), (182, 5), (183, 1), (184, 1), (185, 1), (186, 1), (187, 2), (188, 1), (190, 1), (196, 1), (199, 1), (201, 1), (204, 1), (207, 1), (209, 1), (210, 1), (215, 1), (217, 6), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 2), (241, 1), (242, 1), (243, 2), (244, 2), (247, 2), (248, 4), (249, 2), (251, 1), (253, 1), (266, 1), (268, 1), (269, 1), (270, 1), (285, 1), (287, 2), (288, 2), (290, 1), (301, 1), (303, 2), (304, 2), (326, 2), (329, 1), (341, 1), (348, 1), (351, 1), (352, 2), (355, 2)]
discards: [  2 122]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 103s - loss: 681.4213 - loglik: -6.7937e+02 - logprior: -2.0523e+00
Epoch 2/2
39/39 - 111s - loss: 661.5818 - loglik: -6.6101e+02 - logprior: -5.7411e-01
Fitted a model with MAP estimate = -658.7764
expansions: [(214, 1), (228, 1), (455, 1)]
discards: [  2 186 187 188 189 190 218 219 220 309 315 316 317 318 319 320 373 457
 458 459 460]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 443 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 103s - loss: 670.8488 - loglik: -6.6953e+02 - logprior: -1.3223e+00
Epoch 2/2
39/39 - 100s - loss: 664.9948 - loglik: -6.6503e+02 - logprior: 0.0338
Fitted a model with MAP estimate = -663.0818
expansions: [(303, 1), (304, 2), (310, 2), (443, 4)]
discards: [3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 451 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 84s - loss: 667.1996 - loglik: -6.6585e+02 - logprior: -1.3474e+00
Epoch 2/10
39/39 - 78s - loss: 662.7980 - loglik: -6.6291e+02 - logprior: 0.1134
Epoch 3/10
39/39 - 76s - loss: 661.8145 - loglik: -6.6206e+02 - logprior: 0.2475
Epoch 4/10
39/39 - 76s - loss: 660.9554 - loglik: -6.6124e+02 - logprior: 0.2799
Epoch 5/10
39/39 - 84s - loss: 659.8731 - loglik: -6.5984e+02 - logprior: -2.9598e-02
Epoch 6/10
39/39 - 91s - loss: 658.8163 - loglik: -6.5918e+02 - logprior: 0.3626
Epoch 7/10
39/39 - 95s - loss: 657.5953 - loglik: -6.5807e+02 - logprior: 0.4713
Epoch 8/10
39/39 - 104s - loss: 657.0283 - loglik: -6.5751e+02 - logprior: 0.4845
Epoch 9/10
39/39 - 102s - loss: 656.7423 - loglik: -6.5733e+02 - logprior: 0.5854
Epoch 10/10
39/39 - 96s - loss: 656.2873 - loglik: -6.5720e+02 - logprior: 0.9157
Fitted a model with MAP estimate = -656.0813
Time for alignment: 2273.3046
Computed alignments with likelihoods: ['-654.0076', '-658.6714', '-656.2236', '-655.8209', '-656.0813']
Best model has likelihood: -654.0076  (prior= 0.9009 )
time for generating output: 0.4375
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.8210549744046294
Training of 5 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49d12e9220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d8526c9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4db781fb20>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4db7a75160>
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 416.3395 - loglik: -4.1342e+02 - logprior: -2.9166e+00
Epoch 2/10
19/19 - 4s - loss: 336.0973 - loglik: -3.3516e+02 - logprior: -9.3488e-01
Epoch 3/10
19/19 - 4s - loss: 302.7301 - loglik: -3.0162e+02 - logprior: -1.1089e+00
Epoch 4/10
19/19 - 4s - loss: 296.5816 - loglik: -2.9554e+02 - logprior: -1.0389e+00
Epoch 5/10
19/19 - 4s - loss: 294.2061 - loglik: -2.9321e+02 - logprior: -9.9179e-01
Epoch 6/10
19/19 - 4s - loss: 293.4629 - loglik: -2.9250e+02 - logprior: -9.6232e-01
Epoch 7/10
19/19 - 5s - loss: 292.2712 - loglik: -2.9133e+02 - logprior: -9.4064e-01
Epoch 8/10
19/19 - 4s - loss: 292.3138 - loglik: -2.9138e+02 - logprior: -9.3252e-01
Fitted a model with MAP estimate = -291.5541
expansions: [(0, 7), (5, 1), (6, 1), (7, 1), (8, 4), (9, 1), (36, 1), (37, 1), (49, 1), (55, 2), (56, 1), (58, 1), (74, 1), (75, 1), (77, 2), (79, 1), (83, 1), (85, 1), (97, 1), (100, 2), (101, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 293.1192 - loglik: -2.8932e+02 - logprior: -3.7966e+00
Epoch 2/2
19/19 - 6s - loss: 281.3689 - loglik: -2.8036e+02 - logprior: -1.0107e+00
Fitted a model with MAP estimate = -279.4016
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  18  19  74 101 131 147]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 286.9898 - loglik: -2.8320e+02 - logprior: -3.7880e+00
Epoch 2/2
19/19 - 6s - loss: 281.5663 - loglik: -2.8053e+02 - logprior: -1.0412e+00
Fitted a model with MAP estimate = -280.3931
expansions: [(0, 6)]
discards: [0 1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 284.1698 - loglik: -2.8107e+02 - logprior: -3.0974e+00
Epoch 2/10
19/19 - 6s - loss: 280.4322 - loglik: -2.7969e+02 - logprior: -7.4627e-01
Epoch 3/10
19/19 - 6s - loss: 279.3351 - loglik: -2.7879e+02 - logprior: -5.4211e-01
Epoch 4/10
19/19 - 6s - loss: 279.0850 - loglik: -2.7862e+02 - logprior: -4.6504e-01
Epoch 5/10
19/19 - 6s - loss: 278.4104 - loglik: -2.7800e+02 - logprior: -4.1265e-01
Epoch 6/10
19/19 - 6s - loss: 277.7334 - loglik: -2.7735e+02 - logprior: -3.8617e-01
Epoch 7/10
19/19 - 6s - loss: 277.0323 - loglik: -2.7668e+02 - logprior: -3.5686e-01
Epoch 8/10
19/19 - 6s - loss: 277.4746 - loglik: -2.7713e+02 - logprior: -3.4526e-01
Fitted a model with MAP estimate = -276.8352
Time for alignment: 148.9004
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 415.8958 - loglik: -4.1298e+02 - logprior: -2.9152e+00
Epoch 2/10
19/19 - 5s - loss: 338.1865 - loglik: -3.3723e+02 - logprior: -9.5183e-01
Epoch 3/10
19/19 - 4s - loss: 304.5988 - loglik: -3.0345e+02 - logprior: -1.1438e+00
Epoch 4/10
19/19 - 4s - loss: 297.4398 - loglik: -2.9634e+02 - logprior: -1.0964e+00
Epoch 5/10
19/19 - 4s - loss: 296.2752 - loglik: -2.9523e+02 - logprior: -1.0435e+00
Epoch 6/10
19/19 - 4s - loss: 294.0969 - loglik: -2.9308e+02 - logprior: -1.0142e+00
Epoch 7/10
19/19 - 4s - loss: 293.9896 - loglik: -2.9301e+02 - logprior: -9.7927e-01
Epoch 8/10
19/19 - 4s - loss: 292.4667 - loglik: -2.9149e+02 - logprior: -9.7558e-01
Epoch 9/10
19/19 - 4s - loss: 292.0375 - loglik: -2.9106e+02 - logprior: -9.8168e-01
Epoch 10/10
19/19 - 4s - loss: 291.7449 - loglik: -2.9077e+02 - logprior: -9.7171e-01
Fitted a model with MAP estimate = -291.2152
expansions: [(0, 7), (5, 1), (6, 1), (7, 1), (8, 4), (28, 1), (36, 2), (37, 1), (49, 1), (55, 2), (56, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 2), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 160 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 293.9944 - loglik: -2.9011e+02 - logprior: -3.8803e+00
Epoch 2/2
19/19 - 6s - loss: 282.4857 - loglik: -2.8145e+02 - logprior: -1.0347e+00
Fitted a model with MAP estimate = -280.0293
expansions: [(0, 4), (19, 1), (20, 1)]
discards: [  1   2   3   4   5   6  75 101 102 133]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 285.9086 - loglik: -2.8204e+02 - logprior: -3.8668e+00
Epoch 2/2
19/19 - 6s - loss: 280.3519 - loglik: -2.7914e+02 - logprior: -1.2167e+00
Fitted a model with MAP estimate = -278.9547
expansions: []
discards: [  0   1   2   3   4   6   7  52 145]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 284.7057 - loglik: -2.8181e+02 - logprior: -2.8915e+00
Epoch 2/10
19/19 - 5s - loss: 280.8610 - loglik: -2.8021e+02 - logprior: -6.5400e-01
Epoch 3/10
19/19 - 5s - loss: 280.4429 - loglik: -2.7996e+02 - logprior: -4.8150e-01
Epoch 4/10
19/19 - 5s - loss: 279.5869 - loglik: -2.7918e+02 - logprior: -4.0555e-01
Epoch 5/10
19/19 - 5s - loss: 278.7320 - loglik: -2.7837e+02 - logprior: -3.6467e-01
Epoch 6/10
19/19 - 5s - loss: 278.1534 - loglik: -2.7783e+02 - logprior: -3.2718e-01
Epoch 7/10
19/19 - 6s - loss: 278.1907 - loglik: -2.7789e+02 - logprior: -3.0437e-01
Fitted a model with MAP estimate = -277.5012
Time for alignment: 147.8177
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 415.5410 - loglik: -4.1262e+02 - logprior: -2.9176e+00
Epoch 2/10
19/19 - 4s - loss: 334.1818 - loglik: -3.3322e+02 - logprior: -9.6081e-01
Epoch 3/10
19/19 - 4s - loss: 302.1270 - loglik: -3.0103e+02 - logprior: -1.0995e+00
Epoch 4/10
19/19 - 5s - loss: 296.6613 - loglik: -2.9566e+02 - logprior: -1.0025e+00
Epoch 5/10
19/19 - 4s - loss: 294.7971 - loglik: -2.9384e+02 - logprior: -9.5529e-01
Epoch 6/10
19/19 - 4s - loss: 293.4425 - loglik: -2.9251e+02 - logprior: -9.3501e-01
Epoch 7/10
19/19 - 4s - loss: 292.8860 - loglik: -2.9197e+02 - logprior: -9.1668e-01
Epoch 8/10
19/19 - 4s - loss: 291.8928 - loglik: -2.9099e+02 - logprior: -9.0358e-01
Epoch 9/10
19/19 - 5s - loss: 292.2204 - loglik: -2.9132e+02 - logprior: -8.9722e-01
Fitted a model with MAP estimate = -291.4936
expansions: [(0, 7), (5, 2), (6, 3), (7, 2), (9, 2), (36, 1), (37, 1), (49, 1), (52, 1), (55, 1), (58, 1), (74, 1), (75, 1), (77, 1), (80, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 2), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 293.0467 - loglik: -2.8926e+02 - logprior: -3.7833e+00
Epoch 2/2
19/19 - 6s - loss: 281.3509 - loglik: -2.8040e+02 - logprior: -9.5486e-01
Fitted a model with MAP estimate = -279.5905
expansions: [(0, 5)]
discards: [  1   2   3   4   5   6  19  23 130]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 285.7467 - loglik: -2.8199e+02 - logprior: -3.7525e+00
Epoch 2/2
19/19 - 6s - loss: 281.4146 - loglik: -2.8035e+02 - logprior: -1.0631e+00
Fitted a model with MAP estimate = -279.7698
expansions: [(0, 5)]
discards: [  0   1   2   3   4   5  15 142]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 285.1715 - loglik: -2.8211e+02 - logprior: -3.0653e+00
Epoch 2/10
19/19 - 6s - loss: 280.6432 - loglik: -2.7989e+02 - logprior: -7.5172e-01
Epoch 3/10
19/19 - 6s - loss: 279.7202 - loglik: -2.7915e+02 - logprior: -5.7396e-01
Epoch 4/10
19/19 - 6s - loss: 279.5443 - loglik: -2.7906e+02 - logprior: -4.8785e-01
Epoch 5/10
19/19 - 6s - loss: 278.4441 - loglik: -2.7801e+02 - logprior: -4.3711e-01
Epoch 6/10
19/19 - 6s - loss: 278.1442 - loglik: -2.7773e+02 - logprior: -4.1699e-01
Epoch 7/10
19/19 - 6s - loss: 278.0932 - loglik: -2.7770e+02 - logprior: -3.9558e-01
Epoch 8/10
19/19 - 6s - loss: 277.3209 - loglik: -2.7695e+02 - logprior: -3.6622e-01
Epoch 9/10
19/19 - 6s - loss: 277.1717 - loglik: -2.7682e+02 - logprior: -3.5139e-01
Epoch 10/10
19/19 - 6s - loss: 277.0197 - loglik: -2.7669e+02 - logprior: -3.2526e-01
Fitted a model with MAP estimate = -276.6874
Time for alignment: 167.1312
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 416.1594 - loglik: -4.1324e+02 - logprior: -2.9166e+00
Epoch 2/10
19/19 - 4s - loss: 335.7668 - loglik: -3.3481e+02 - logprior: -9.5946e-01
Epoch 3/10
19/19 - 4s - loss: 303.1026 - loglik: -3.0197e+02 - logprior: -1.1342e+00
Epoch 4/10
19/19 - 4s - loss: 297.3426 - loglik: -2.9627e+02 - logprior: -1.0693e+00
Epoch 5/10
19/19 - 4s - loss: 295.2041 - loglik: -2.9417e+02 - logprior: -1.0373e+00
Epoch 6/10
19/19 - 4s - loss: 293.2386 - loglik: -2.9224e+02 - logprior: -1.0035e+00
Epoch 7/10
19/19 - 4s - loss: 292.2348 - loglik: -2.9125e+02 - logprior: -9.8314e-01
Epoch 8/10
19/19 - 4s - loss: 291.9753 - loglik: -2.9099e+02 - logprior: -9.8429e-01
Epoch 9/10
19/19 - 4s - loss: 292.1936 - loglik: -2.9122e+02 - logprior: -9.7392e-01
Fitted a model with MAP estimate = -291.1089
expansions: [(0, 7), (5, 1), (6, 1), (7, 1), (8, 4), (9, 1), (36, 1), (37, 1), (49, 1), (55, 2), (56, 1), (58, 1), (74, 1), (77, 2), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 2), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 293.8617 - loglik: -2.8999e+02 - logprior: -3.8762e+00
Epoch 2/2
19/19 - 6s - loss: 282.2915 - loglik: -2.8127e+02 - logprior: -1.0239e+00
Fitted a model with MAP estimate = -280.5148
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  18  19  74 130 146]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 287.5623 - loglik: -2.8375e+02 - logprior: -3.8079e+00
Epoch 2/2
19/19 - 5s - loss: 282.3587 - loglik: -2.8129e+02 - logprior: -1.0726e+00
Fitted a model with MAP estimate = -281.0322
expansions: [(0, 6)]
discards: [0 1 2 3 4]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 285.1667 - loglik: -2.8207e+02 - logprior: -3.0957e+00
Epoch 2/10
19/19 - 5s - loss: 281.0528 - loglik: -2.8029e+02 - logprior: -7.6050e-01
Epoch 3/10
19/19 - 5s - loss: 280.2254 - loglik: -2.7966e+02 - logprior: -5.6985e-01
Epoch 4/10
19/19 - 5s - loss: 279.8049 - loglik: -2.7932e+02 - logprior: -4.8296e-01
Epoch 5/10
19/19 - 6s - loss: 279.4571 - loglik: -2.7902e+02 - logprior: -4.3825e-01
Epoch 6/10
19/19 - 5s - loss: 278.3742 - loglik: -2.7797e+02 - logprior: -4.0663e-01
Epoch 7/10
19/19 - 6s - loss: 278.5046 - loglik: -2.7812e+02 - logprior: -3.8303e-01
Fitted a model with MAP estimate = -277.9772
Time for alignment: 142.4175
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 415.7797 - loglik: -4.1287e+02 - logprior: -2.9110e+00
Epoch 2/10
19/19 - 4s - loss: 334.5844 - loglik: -3.3364e+02 - logprior: -9.4532e-01
Epoch 3/10
19/19 - 4s - loss: 304.1806 - loglik: -3.0306e+02 - logprior: -1.1249e+00
Epoch 4/10
19/19 - 4s - loss: 297.2436 - loglik: -2.9617e+02 - logprior: -1.0742e+00
Epoch 5/10
19/19 - 4s - loss: 295.2569 - loglik: -2.9420e+02 - logprior: -1.0618e+00
Epoch 6/10
19/19 - 4s - loss: 293.4155 - loglik: -2.9238e+02 - logprior: -1.0390e+00
Epoch 7/10
19/19 - 4s - loss: 293.2004 - loglik: -2.9217e+02 - logprior: -1.0338e+00
Epoch 8/10
19/19 - 4s - loss: 291.7153 - loglik: -2.9069e+02 - logprior: -1.0245e+00
Epoch 9/10
19/19 - 4s - loss: 292.1561 - loglik: -2.9114e+02 - logprior: -1.0160e+00
Fitted a model with MAP estimate = -291.4041
expansions: [(0, 7), (3, 1), (6, 2), (7, 2), (8, 2), (9, 2), (36, 1), (37, 1), (49, 1), (52, 1), (55, 1), (58, 1), (74, 1), (78, 1), (80, 1), (83, 1), (84, 1), (85, 1), (97, 1), (98, 1), (100, 2), (101, 1), (104, 1), (111, 1), (112, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 295.1824 - loglik: -2.9128e+02 - logprior: -3.9022e+00
Epoch 2/2
19/19 - 6s - loss: 281.7603 - loglik: -2.8075e+02 - logprior: -1.0076e+00
Fitted a model with MAP estimate = -279.4946
expansions: []
discards: [  1   2   3   4   5   6  18  23 131]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 285.5939 - loglik: -2.8296e+02 - logprior: -2.6347e+00
Epoch 2/2
19/19 - 5s - loss: 282.0999 - loglik: -2.8148e+02 - logprior: -6.2482e-01
Fitted a model with MAP estimate = -280.6687
expansions: [(0, 6)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 153 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 285.4377 - loglik: -2.8103e+02 - logprior: -4.4094e+00
Epoch 2/10
19/19 - 6s - loss: 281.1430 - loglik: -2.8004e+02 - logprior: -1.1059e+00
Epoch 3/10
19/19 - 6s - loss: 278.9413 - loglik: -2.7815e+02 - logprior: -7.8932e-01
Epoch 4/10
19/19 - 6s - loss: 278.2959 - loglik: -2.7770e+02 - logprior: -5.9394e-01
Epoch 5/10
19/19 - 6s - loss: 277.7894 - loglik: -2.7727e+02 - logprior: -5.2370e-01
Epoch 6/10
19/19 - 6s - loss: 277.3113 - loglik: -2.7683e+02 - logprior: -4.8439e-01
Epoch 7/10
19/19 - 6s - loss: 277.5382 - loglik: -2.7708e+02 - logprior: -4.5894e-01
Fitted a model with MAP estimate = -276.9921
Time for alignment: 144.9254
Computed alignments with likelihoods: ['-276.8352', '-277.5012', '-276.6874', '-277.9772', '-276.9921']
Best model has likelihood: -276.6874  (prior= -0.3300 )
time for generating output: 0.1659
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.5236355992327298
Training of 5 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4be9c12be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bdf6badf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bde232ac0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d963f4940>
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 70.4705 - loglik: -6.7147e+01 - logprior: -3.3239e+00
Epoch 2/10
19/19 - 0s - loss: 56.0460 - loglik: -5.4734e+01 - logprior: -1.3125e+00
Epoch 3/10
19/19 - 0s - loss: 47.0981 - loglik: -4.5749e+01 - logprior: -1.3491e+00
Epoch 4/10
19/19 - 0s - loss: 44.5099 - loglik: -4.3077e+01 - logprior: -1.4331e+00
Epoch 5/10
19/19 - 0s - loss: 44.1284 - loglik: -4.2694e+01 - logprior: -1.4348e+00
Epoch 6/10
19/19 - 0s - loss: 43.8821 - loglik: -4.2449e+01 - logprior: -1.4329e+00
Epoch 7/10
19/19 - 0s - loss: 43.7587 - loglik: -4.2331e+01 - logprior: -1.4277e+00
Epoch 8/10
19/19 - 0s - loss: 43.6105 - loglik: -4.2190e+01 - logprior: -1.4203e+00
Epoch 9/10
19/19 - 0s - loss: 43.6129 - loglik: -4.2204e+01 - logprior: -1.4086e+00
Fitted a model with MAP estimate = -43.5381
expansions: [(0, 1), (5, 1), (6, 2), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.8228 - loglik: -4.3048e+01 - logprior: -4.7748e+00
Epoch 2/2
19/19 - 0s - loss: 42.3761 - loglik: -4.0922e+01 - logprior: -1.4546e+00
Fitted a model with MAP estimate = -41.5190
expansions: [(2, 1)]
discards: [0 8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.0929 - loglik: -4.0730e+01 - logprior: -3.3632e+00
Epoch 2/2
19/19 - 0s - loss: 41.4380 - loglik: -3.9964e+01 - logprior: -1.4741e+00
Fitted a model with MAP estimate = -41.1147
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 43.7261 - loglik: -4.0416e+01 - logprior: -3.3100e+00
Epoch 2/10
19/19 - 0s - loss: 41.3435 - loglik: -3.9883e+01 - logprior: -1.4602e+00
Epoch 3/10
19/19 - 1s - loss: 40.9557 - loglik: -3.9610e+01 - logprior: -1.3461e+00
Epoch 4/10
19/19 - 0s - loss: 40.6931 - loglik: -3.9395e+01 - logprior: -1.2983e+00
Epoch 5/10
19/19 - 0s - loss: 40.5836 - loglik: -3.9311e+01 - logprior: -1.2730e+00
Epoch 6/10
19/19 - 0s - loss: 40.3809 - loglik: -3.9123e+01 - logprior: -1.2580e+00
Epoch 7/10
19/19 - 0s - loss: 40.3362 - loglik: -3.9090e+01 - logprior: -1.2460e+00
Epoch 8/10
19/19 - 0s - loss: 40.0751 - loglik: -3.8832e+01 - logprior: -1.2432e+00
Epoch 9/10
19/19 - 0s - loss: 40.1543 - loglik: -3.8926e+01 - logprior: -1.2287e+00
Fitted a model with MAP estimate = -40.0458
Time for alignment: 28.3454
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 70.4267 - loglik: -6.7106e+01 - logprior: -3.3208e+00
Epoch 2/10
19/19 - 0s - loss: 55.5744 - loglik: -5.4243e+01 - logprior: -1.3313e+00
Epoch 3/10
19/19 - 0s - loss: 46.6665 - loglik: -4.5252e+01 - logprior: -1.4148e+00
Epoch 4/10
19/19 - 0s - loss: 44.4640 - loglik: -4.3003e+01 - logprior: -1.4608e+00
Epoch 5/10
19/19 - 0s - loss: 44.0429 - loglik: -4.2607e+01 - logprior: -1.4356e+00
Epoch 6/10
19/19 - 0s - loss: 43.8510 - loglik: -4.2418e+01 - logprior: -1.4333e+00
Epoch 7/10
19/19 - 0s - loss: 43.7350 - loglik: -4.2312e+01 - logprior: -1.4231e+00
Epoch 8/10
19/19 - 0s - loss: 43.6193 - loglik: -4.2199e+01 - logprior: -1.4204e+00
Epoch 9/10
19/19 - 0s - loss: 43.5702 - loglik: -4.2157e+01 - logprior: -1.4133e+00
Epoch 10/10
19/19 - 0s - loss: 43.4926 - loglik: -4.2089e+01 - logprior: -1.4032e+00
Fitted a model with MAP estimate = -43.4548
expansions: [(0, 1), (5, 1), (6, 2), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.8351 - loglik: -4.3027e+01 - logprior: -4.8084e+00
Epoch 2/2
19/19 - 0s - loss: 42.3966 - loglik: -4.0926e+01 - logprior: -1.4702e+00
Fitted a model with MAP estimate = -41.5339
expansions: [(2, 1)]
discards: [0 8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.1122 - loglik: -4.0746e+01 - logprior: -3.3665e+00
Epoch 2/2
19/19 - 0s - loss: 41.4308 - loglik: -3.9962e+01 - logprior: -1.4687e+00
Fitted a model with MAP estimate = -41.0959
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.7600 - loglik: -4.0449e+01 - logprior: -3.3111e+00
Epoch 2/10
19/19 - 0s - loss: 41.3014 - loglik: -3.9846e+01 - logprior: -1.4556e+00
Epoch 3/10
19/19 - 0s - loss: 40.9454 - loglik: -3.9595e+01 - logprior: -1.3506e+00
Epoch 4/10
19/19 - 0s - loss: 40.7485 - loglik: -3.9451e+01 - logprior: -1.2980e+00
Epoch 5/10
19/19 - 0s - loss: 40.5109 - loglik: -3.9236e+01 - logprior: -1.2745e+00
Epoch 6/10
19/19 - 0s - loss: 40.4629 - loglik: -3.9206e+01 - logprior: -1.2568e+00
Epoch 7/10
19/19 - 0s - loss: 40.3065 - loglik: -3.9058e+01 - logprior: -1.2483e+00
Epoch 8/10
19/19 - 0s - loss: 40.2067 - loglik: -3.8967e+01 - logprior: -1.2400e+00
Epoch 9/10
19/19 - 0s - loss: 40.0508 - loglik: -3.8820e+01 - logprior: -1.2312e+00
Epoch 10/10
19/19 - 0s - loss: 40.0286 - loglik: -3.8804e+01 - logprior: -1.2243e+00
Fitted a model with MAP estimate = -40.0080
Time for alignment: 27.8388
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 70.5052 - loglik: -6.7179e+01 - logprior: -3.3263e+00
Epoch 2/10
19/19 - 0s - loss: 55.6954 - loglik: -5.4351e+01 - logprior: -1.3447e+00
Epoch 3/10
19/19 - 0s - loss: 46.5122 - loglik: -4.5113e+01 - logprior: -1.3996e+00
Epoch 4/10
19/19 - 0s - loss: 44.4481 - loglik: -4.2988e+01 - logprior: -1.4601e+00
Epoch 5/10
19/19 - 0s - loss: 43.9784 - loglik: -4.2540e+01 - logprior: -1.4383e+00
Epoch 6/10
19/19 - 1s - loss: 43.8240 - loglik: -4.2392e+01 - logprior: -1.4322e+00
Epoch 7/10
19/19 - 0s - loss: 43.7181 - loglik: -4.2292e+01 - logprior: -1.4257e+00
Epoch 8/10
19/19 - 0s - loss: 43.6650 - loglik: -4.2245e+01 - logprior: -1.4203e+00
Epoch 9/10
19/19 - 0s - loss: 43.5059 - loglik: -4.2096e+01 - logprior: -1.4101e+00
Epoch 10/10
19/19 - 0s - loss: 43.5518 - loglik: -4.2141e+01 - logprior: -1.4110e+00
Fitted a model with MAP estimate = -43.4511
expansions: [(0, 1), (5, 1), (6, 2), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.8351 - loglik: -4.3016e+01 - logprior: -4.8193e+00
Epoch 2/2
19/19 - 0s - loss: 42.4011 - loglik: -4.0922e+01 - logprior: -1.4791e+00
Fitted a model with MAP estimate = -41.5243
expansions: [(2, 1)]
discards: [0 8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 44.1209 - loglik: -4.0758e+01 - logprior: -3.3633e+00
Epoch 2/2
19/19 - 1s - loss: 41.3829 - loglik: -3.9910e+01 - logprior: -1.4731e+00
Fitted a model with MAP estimate = -41.0825
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.7203 - loglik: -4.0412e+01 - logprior: -3.3088e+00
Epoch 2/10
19/19 - 0s - loss: 41.3141 - loglik: -3.9857e+01 - logprior: -1.4571e+00
Epoch 3/10
19/19 - 0s - loss: 40.9327 - loglik: -3.9585e+01 - logprior: -1.3482e+00
Epoch 4/10
19/19 - 0s - loss: 40.6085 - loglik: -3.9306e+01 - logprior: -1.3024e+00
Epoch 5/10
19/19 - 0s - loss: 40.5666 - loglik: -3.9294e+01 - logprior: -1.2723e+00
Epoch 6/10
19/19 - 0s - loss: 40.3384 - loglik: -3.9079e+01 - logprior: -1.2589e+00
Epoch 7/10
19/19 - 0s - loss: 40.3190 - loglik: -3.9068e+01 - logprior: -1.2513e+00
Epoch 8/10
19/19 - 0s - loss: 40.0667 - loglik: -3.8829e+01 - logprior: -1.2373e+00
Epoch 9/10
19/19 - 0s - loss: 40.0640 - loglik: -3.8830e+01 - logprior: -1.2342e+00
Epoch 10/10
19/19 - 0s - loss: 40.0588 - loglik: -3.8836e+01 - logprior: -1.2231e+00
Fitted a model with MAP estimate = -39.9547
Time for alignment: 30.2435
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 70.5158 - loglik: -6.7185e+01 - logprior: -3.3306e+00
Epoch 2/10
19/19 - 0s - loss: 56.3969 - loglik: -5.5056e+01 - logprior: -1.3412e+00
Epoch 3/10
19/19 - 0s - loss: 47.3797 - loglik: -4.5991e+01 - logprior: -1.3887e+00
Epoch 4/10
19/19 - 0s - loss: 44.5909 - loglik: -4.3140e+01 - logprior: -1.4513e+00
Epoch 5/10
19/19 - 0s - loss: 44.0537 - loglik: -4.2618e+01 - logprior: -1.4360e+00
Epoch 6/10
19/19 - 0s - loss: 43.8743 - loglik: -4.2444e+01 - logprior: -1.4303e+00
Epoch 7/10
19/19 - 0s - loss: 43.7535 - loglik: -4.2323e+01 - logprior: -1.4310e+00
Epoch 8/10
19/19 - 0s - loss: 43.6788 - loglik: -4.2258e+01 - logprior: -1.4203e+00
Epoch 9/10
19/19 - 1s - loss: 43.5840 - loglik: -4.2176e+01 - logprior: -1.4076e+00
Epoch 10/10
19/19 - 0s - loss: 43.5391 - loglik: -4.2131e+01 - logprior: -1.4077e+00
Fitted a model with MAP estimate = -43.4777
expansions: [(0, 1), (5, 1), (6, 2), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.8579 - loglik: -4.3047e+01 - logprior: -4.8107e+00
Epoch 2/2
19/19 - 0s - loss: 42.3951 - loglik: -4.0928e+01 - logprior: -1.4670e+00
Fitted a model with MAP estimate = -41.5355
expansions: [(2, 1)]
discards: [0 8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.0768 - loglik: -4.0713e+01 - logprior: -3.3636e+00
Epoch 2/2
19/19 - 0s - loss: 41.4390 - loglik: -3.9971e+01 - logprior: -1.4682e+00
Fitted a model with MAP estimate = -41.0939
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.7198 - loglik: -4.0407e+01 - logprior: -3.3125e+00
Epoch 2/10
19/19 - 0s - loss: 41.3378 - loglik: -3.9885e+01 - logprior: -1.4524e+00
Epoch 3/10
19/19 - 0s - loss: 40.9108 - loglik: -3.9562e+01 - logprior: -1.3492e+00
Epoch 4/10
19/19 - 0s - loss: 40.7610 - loglik: -3.9465e+01 - logprior: -1.2965e+00
Epoch 5/10
19/19 - 0s - loss: 40.6442 - loglik: -3.9372e+01 - logprior: -1.2720e+00
Epoch 6/10
19/19 - 0s - loss: 40.3806 - loglik: -3.9123e+01 - logprior: -1.2577e+00
Epoch 7/10
19/19 - 0s - loss: 40.2614 - loglik: -3.9015e+01 - logprior: -1.2461e+00
Epoch 8/10
19/19 - 0s - loss: 40.1211 - loglik: -3.8881e+01 - logprior: -1.2397e+00
Epoch 9/10
19/19 - 0s - loss: 40.0863 - loglik: -3.8853e+01 - logprior: -1.2334e+00
Epoch 10/10
19/19 - 0s - loss: 40.0741 - loglik: -3.8853e+01 - logprior: -1.2210e+00
Fitted a model with MAP estimate = -39.9602
Time for alignment: 28.1628
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 70.5217 - loglik: -6.7192e+01 - logprior: -3.3294e+00
Epoch 2/10
19/19 - 0s - loss: 55.9925 - loglik: -5.4644e+01 - logprior: -1.3489e+00
Epoch 3/10
19/19 - 0s - loss: 46.8466 - loglik: -4.5473e+01 - logprior: -1.3733e+00
Epoch 4/10
19/19 - 0s - loss: 44.4940 - loglik: -4.3043e+01 - logprior: -1.4507e+00
Epoch 5/10
19/19 - 0s - loss: 44.0275 - loglik: -4.2596e+01 - logprior: -1.4312e+00
Epoch 6/10
19/19 - 0s - loss: 43.8904 - loglik: -4.2459e+01 - logprior: -1.4316e+00
Epoch 7/10
19/19 - 0s - loss: 43.7286 - loglik: -4.2303e+01 - logprior: -1.4252e+00
Epoch 8/10
19/19 - 0s - loss: 43.7087 - loglik: -4.2290e+01 - logprior: -1.4186e+00
Epoch 9/10
19/19 - 0s - loss: 43.5841 - loglik: -4.2176e+01 - logprior: -1.4083e+00
Epoch 10/10
19/19 - 0s - loss: 43.5911 - loglik: -4.2186e+01 - logprior: -1.4049e+00
Fitted a model with MAP estimate = -43.4962
expansions: [(0, 1), (5, 1), (6, 2), (11, 1), (12, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 25 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 47.8760 - loglik: -4.3059e+01 - logprior: -4.8170e+00
Epoch 2/2
19/19 - 0s - loss: 42.3960 - loglik: -4.0929e+01 - logprior: -1.4675e+00
Fitted a model with MAP estimate = -41.5507
expansions: [(2, 1)]
discards: [0 8]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.1017 - loglik: -4.0737e+01 - logprior: -3.3647e+00
Epoch 2/2
19/19 - 0s - loss: 41.4513 - loglik: -3.9982e+01 - logprior: -1.4689e+00
Fitted a model with MAP estimate = -41.0975
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 43.7266 - loglik: -4.0418e+01 - logprior: -3.3083e+00
Epoch 2/10
19/19 - 0s - loss: 41.3443 - loglik: -3.9891e+01 - logprior: -1.4532e+00
Epoch 3/10
19/19 - 0s - loss: 40.9345 - loglik: -3.9585e+01 - logprior: -1.3494e+00
Epoch 4/10
19/19 - 0s - loss: 40.6838 - loglik: -3.9386e+01 - logprior: -1.2977e+00
Epoch 5/10
19/19 - 0s - loss: 40.5896 - loglik: -3.9318e+01 - logprior: -1.2720e+00
Epoch 6/10
19/19 - 0s - loss: 40.3611 - loglik: -3.9104e+01 - logprior: -1.2576e+00
Epoch 7/10
19/19 - 0s - loss: 40.2461 - loglik: -3.8998e+01 - logprior: -1.2478e+00
Epoch 8/10
19/19 - 0s - loss: 40.2333 - loglik: -3.8995e+01 - logprior: -1.2379e+00
Epoch 9/10
19/19 - 0s - loss: 40.0238 - loglik: -3.8795e+01 - logprior: -1.2289e+00
Epoch 10/10
19/19 - 0s - loss: 40.0740 - loglik: -3.8849e+01 - logprior: -1.2248e+00
Fitted a model with MAP estimate = -39.9629
Time for alignment: 27.5698
Computed alignments with likelihoods: ['-40.0458', '-40.0080', '-39.9547', '-39.9602', '-39.9629']
Best model has likelihood: -39.9547  (prior= -1.2197 )
time for generating output: 0.0721
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.869983948635634
Training of 5 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d7cbe2d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d1b8af9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bdf6deac0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f49ca1ba160>
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 753.8908 - loglik: -7.5241e+02 - logprior: -1.4849e+00
Epoch 2/10
39/39 - 27s - loss: 542.1533 - loglik: -5.4065e+02 - logprior: -1.5028e+00
Epoch 3/10
39/39 - 30s - loss: 526.2279 - loglik: -5.2491e+02 - logprior: -1.3175e+00
Epoch 4/10
39/39 - 31s - loss: 523.8438 - loglik: -5.2265e+02 - logprior: -1.1909e+00
Epoch 5/10
39/39 - 31s - loss: 522.1965 - loglik: -5.2104e+02 - logprior: -1.1545e+00
Epoch 6/10
39/39 - 30s - loss: 520.8811 - loglik: -5.1972e+02 - logprior: -1.1591e+00
Epoch 7/10
39/39 - 31s - loss: 520.1641 - loglik: -5.1897e+02 - logprior: -1.1925e+00
Epoch 8/10
39/39 - 32s - loss: 519.3755 - loglik: -5.1816e+02 - logprior: -1.2120e+00
Epoch 9/10
39/39 - 31s - loss: 518.1332 - loglik: -5.1689e+02 - logprior: -1.2416e+00
Epoch 10/10
39/39 - 31s - loss: 518.0385 - loglik: -5.1679e+02 - logprior: -1.2513e+00
Fitted a model with MAP estimate = -517.0268
expansions: [(0, 2), (1, 1), (14, 1), (15, 1), (16, 1), (18, 1), (38, 1), (39, 1), (44, 1), (45, 1), (46, 2), (47, 1), (48, 1), (64, 1), (66, 2), (67, 1), (69, 1), (76, 1), (78, 2), (83, 1), (84, 1), (88, 1), (90, 1), (92, 1), (108, 1), (109, 2), (110, 1), (111, 1), (112, 1), (119, 1), (131, 1), (141, 1), (144, 2), (146, 1), (147, 1), (148, 1), (166, 1), (167, 1), (169, 1), (170, 3), (171, 1), (182, 1), (183, 1), (187, 1), (204, 1), (205, 1), (207, 2), (212, 1), (215, 1), (216, 1), (226, 1), (228, 3), (242, 2), (257, 1), (259, 1), (265, 1), (268, 2), (271, 1), (272, 1), (273, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 52s - loss: 489.8426 - loglik: -4.8787e+02 - logprior: -1.9708e+00
Epoch 2/2
39/39 - 46s - loss: 470.8241 - loglik: -4.7041e+02 - logprior: -4.1077e-01
Fitted a model with MAP estimate = -467.8946
expansions: []
discards: [  0   1  57  99 143 181 216]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 476.1641 - loglik: -4.7475e+02 - logprior: -1.4180e+00
Epoch 2/2
39/39 - 45s - loss: 471.1560 - loglik: -4.7131e+02 - logprior: 0.1540
Fitted a model with MAP estimate = -468.8780
expansions: [(0, 2)]
discards: [80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 473.9703 - loglik: -4.7276e+02 - logprior: -1.2090e+00
Epoch 2/10
39/39 - 39s - loss: 469.6970 - loglik: -4.6956e+02 - logprior: -1.3866e-01
Epoch 3/10
39/39 - 35s - loss: 467.3501 - loglik: -4.6755e+02 - logprior: 0.2004
Epoch 4/10
39/39 - 34s - loss: 466.4471 - loglik: -4.6656e+02 - logprior: 0.1111
Epoch 5/10
39/39 - 33s - loss: 464.8521 - loglik: -4.6501e+02 - logprior: 0.1536
Epoch 6/10
39/39 - 32s - loss: 463.0851 - loglik: -4.6366e+02 - logprior: 0.5755
Epoch 7/10
39/39 - 32s - loss: 462.8230 - loglik: -4.6304e+02 - logprior: 0.2201
Epoch 8/10
39/39 - 32s - loss: 461.8230 - loglik: -4.6247e+02 - logprior: 0.6471
Epoch 9/10
39/39 - 32s - loss: 462.5241 - loglik: -4.6327e+02 - logprior: 0.7423
Fitted a model with MAP estimate = -462.1261
Time for alignment: 1036.7862
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 750.9193 - loglik: -7.4941e+02 - logprior: -1.5045e+00
Epoch 2/10
39/39 - 22s - loss: 541.6552 - loglik: -5.4008e+02 - logprior: -1.5703e+00
Epoch 3/10
39/39 - 22s - loss: 528.0919 - loglik: -5.2665e+02 - logprior: -1.4402e+00
Epoch 4/10
39/39 - 22s - loss: 524.4794 - loglik: -5.2315e+02 - logprior: -1.3334e+00
Epoch 5/10
39/39 - 22s - loss: 523.3402 - loglik: -5.2204e+02 - logprior: -1.2986e+00
Epoch 6/10
39/39 - 22s - loss: 522.1957 - loglik: -5.2088e+02 - logprior: -1.3185e+00
Epoch 7/10
39/39 - 22s - loss: 521.4580 - loglik: -5.2015e+02 - logprior: -1.3031e+00
Epoch 8/10
39/39 - 22s - loss: 520.5756 - loglik: -5.1926e+02 - logprior: -1.3186e+00
Epoch 9/10
39/39 - 22s - loss: 519.9341 - loglik: -5.1860e+02 - logprior: -1.3389e+00
Epoch 10/10
39/39 - 22s - loss: 519.5134 - loglik: -5.1817e+02 - logprior: -1.3405e+00
Fitted a model with MAP estimate = -518.6761
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (40, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (67, 2), (68, 1), (69, 1), (72, 1), (76, 1), (78, 2), (83, 1), (84, 1), (88, 1), (90, 1), (92, 1), (95, 1), (107, 1), (109, 2), (111, 2), (112, 1), (119, 1), (142, 1), (145, 2), (147, 3), (158, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (183, 1), (187, 1), (204, 1), (208, 2), (210, 1), (212, 1), (215, 1), (216, 1), (226, 1), (228, 3), (243, 1), (258, 1), (264, 1), (266, 2), (268, 2), (271, 1), (272, 1), (273, 4)]
discards: [52]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 494.6487 - loglik: -4.9264e+02 - logprior: -2.0096e+00
Epoch 2/2
39/39 - 32s - loss: 476.1902 - loglik: -4.7567e+02 - logprior: -5.2157e-01
Fitted a model with MAP estimate = -473.7166
expansions: []
discards: [  0  57  82  98 139 142 181 216]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 482.5157 - loglik: -4.8027e+02 - logprior: -2.2460e+00
Epoch 2/2
39/39 - 31s - loss: 476.8271 - loglik: -4.7666e+02 - logprior: -1.6373e-01
Fitted a model with MAP estimate = -474.1069
expansions: [(0, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 478.7496 - loglik: -4.7771e+02 - logprior: -1.0353e+00
Epoch 2/10
39/39 - 31s - loss: 474.7746 - loglik: -4.7466e+02 - logprior: -1.0994e-01
Epoch 3/10
39/39 - 31s - loss: 473.1164 - loglik: -4.7336e+02 - logprior: 0.2451
Epoch 4/10
39/39 - 31s - loss: 471.5224 - loglik: -4.7155e+02 - logprior: 0.0305
Epoch 5/10
39/39 - 31s - loss: 469.8689 - loglik: -4.7023e+02 - logprior: 0.3601
Epoch 6/10
39/39 - 31s - loss: 468.9785 - loglik: -4.6930e+02 - logprior: 0.3182
Epoch 7/10
39/39 - 31s - loss: 468.0888 - loglik: -4.6865e+02 - logprior: 0.5576
Epoch 8/10
39/39 - 31s - loss: 468.1334 - loglik: -4.6857e+02 - logprior: 0.4346
Fitted a model with MAP estimate = -467.4934
Time for alignment: 770.9974
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 752.2361 - loglik: -7.5074e+02 - logprior: -1.4964e+00
Epoch 2/10
39/39 - 22s - loss: 544.7090 - loglik: -5.4311e+02 - logprior: -1.6003e+00
Epoch 3/10
39/39 - 22s - loss: 530.8180 - loglik: -5.2935e+02 - logprior: -1.4645e+00
Epoch 4/10
39/39 - 22s - loss: 527.6619 - loglik: -5.2633e+02 - logprior: -1.3352e+00
Epoch 5/10
39/39 - 22s - loss: 526.3005 - loglik: -5.2498e+02 - logprior: -1.3202e+00
Epoch 6/10
39/39 - 22s - loss: 525.4491 - loglik: -5.2413e+02 - logprior: -1.3215e+00
Epoch 7/10
39/39 - 22s - loss: 524.5517 - loglik: -5.2320e+02 - logprior: -1.3479e+00
Epoch 8/10
39/39 - 22s - loss: 523.6954 - loglik: -5.2235e+02 - logprior: -1.3412e+00
Epoch 9/10
39/39 - 22s - loss: 522.9366 - loglik: -5.2159e+02 - logprior: -1.3447e+00
Epoch 10/10
39/39 - 22s - loss: 523.3353 - loglik: -5.2199e+02 - logprior: -1.3485e+00
Fitted a model with MAP estimate = -521.6447
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 1), (38, 1), (40, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (67, 2), (68, 1), (69, 1), (72, 1), (78, 2), (79, 1), (83, 1), (89, 1), (90, 2), (92, 1), (108, 1), (110, 2), (111, 2), (112, 1), (131, 3), (142, 1), (144, 2), (146, 1), (147, 1), (148, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (183, 1), (187, 1), (204, 1), (206, 1), (207, 1), (210, 1), (212, 1), (215, 1), (216, 1), (226, 1), (228, 3), (242, 2), (257, 1), (263, 1), (266, 1), (267, 1), (268, 1), (271, 1), (272, 1), (273, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 495.2145 - loglik: -4.9319e+02 - logprior: -2.0204e+00
Epoch 2/2
39/39 - 32s - loss: 475.4152 - loglik: -4.7483e+02 - logprior: -5.8157e-01
Fitted a model with MAP estimate = -473.0820
expansions: []
discards: [  0  57  83  99 139 166 182 217]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 482.3440 - loglik: -4.8015e+02 - logprior: -2.1980e+00
Epoch 2/2
39/39 - 31s - loss: 476.1620 - loglik: -4.7588e+02 - logprior: -2.7849e-01
Fitted a model with MAP estimate = -473.6770
expansions: [(0, 2)]
discards: [  0 341]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 479.0849 - loglik: -4.7801e+02 - logprior: -1.0725e+00
Epoch 2/10
39/39 - 31s - loss: 474.8777 - loglik: -4.7506e+02 - logprior: 0.1847
Epoch 3/10
39/39 - 31s - loss: 472.8766 - loglik: -4.7337e+02 - logprior: 0.4886
Epoch 4/10
39/39 - 31s - loss: 471.3661 - loglik: -4.7192e+02 - logprior: 0.5568
Epoch 5/10
39/39 - 31s - loss: 469.9237 - loglik: -4.7055e+02 - logprior: 0.6272
Epoch 6/10
39/39 - 31s - loss: 467.7802 - loglik: -4.6841e+02 - logprior: 0.6325
Epoch 7/10
39/39 - 31s - loss: 467.9115 - loglik: -4.6871e+02 - logprior: 0.7962
Fitted a model with MAP estimate = -467.2414
Time for alignment: 737.5410
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 750.5055 - loglik: -7.4901e+02 - logprior: -1.4978e+00
Epoch 2/10
39/39 - 22s - loss: 536.5135 - loglik: -5.3488e+02 - logprior: -1.6348e+00
Epoch 3/10
39/39 - 22s - loss: 523.7749 - loglik: -5.2228e+02 - logprior: -1.4933e+00
Epoch 4/10
39/39 - 22s - loss: 521.0502 - loglik: -5.1968e+02 - logprior: -1.3675e+00
Epoch 5/10
39/39 - 22s - loss: 520.2666 - loglik: -5.1893e+02 - logprior: -1.3355e+00
Epoch 6/10
39/39 - 22s - loss: 519.0014 - loglik: -5.1767e+02 - logprior: -1.3269e+00
Epoch 7/10
39/39 - 22s - loss: 518.2233 - loglik: -5.1689e+02 - logprior: -1.3377e+00
Epoch 8/10
39/39 - 22s - loss: 517.3630 - loglik: -5.1602e+02 - logprior: -1.3441e+00
Epoch 9/10
39/39 - 22s - loss: 516.9349 - loglik: -5.1556e+02 - logprior: -1.3734e+00
Epoch 10/10
39/39 - 22s - loss: 516.4257 - loglik: -5.1506e+02 - logprior: -1.3658e+00
Fitted a model with MAP estimate = -515.3957
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 1), (38, 1), (40, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (67, 2), (68, 1), (69, 1), (72, 1), (78, 2), (79, 1), (83, 1), (84, 1), (88, 1), (90, 1), (92, 1), (104, 1), (108, 1), (109, 2), (111, 2), (112, 1), (132, 1), (142, 1), (145, 2), (147, 1), (148, 1), (149, 1), (159, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (183, 1), (187, 1), (204, 1), (208, 2), (210, 1), (212, 1), (215, 1), (216, 1), (226, 1), (228, 3), (242, 2), (257, 1), (263, 1), (265, 1), (268, 2), (271, 1), (272, 1), (273, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 350 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 489.4628 - loglik: -4.8752e+02 - logprior: -1.9404e+00
Epoch 2/2
39/39 - 32s - loss: 470.3753 - loglik: -4.6980e+02 - logprior: -5.7588e-01
Fitted a model with MAP estimate = -467.7629
expansions: []
discards: [  0   2  57  83  99 140 143 182 217]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 478.6578 - loglik: -4.7672e+02 - logprior: -1.9364e+00
Epoch 2/2
39/39 - 31s - loss: 472.5656 - loglik: -4.7263e+02 - logprior: 0.0610
Fitted a model with MAP estimate = -469.6951
expansions: [(0, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 473.5310 - loglik: -4.7231e+02 - logprior: -1.2219e+00
Epoch 2/10
39/39 - 31s - loss: 469.3848 - loglik: -4.6942e+02 - logprior: 0.0363
Epoch 3/10
39/39 - 31s - loss: 467.5413 - loglik: -4.6775e+02 - logprior: 0.2080
Epoch 4/10
39/39 - 31s - loss: 465.9278 - loglik: -4.6608e+02 - logprior: 0.1552
Epoch 5/10
39/39 - 31s - loss: 464.1640 - loglik: -4.6462e+02 - logprior: 0.4545
Epoch 6/10
39/39 - 31s - loss: 463.5344 - loglik: -4.6375e+02 - logprior: 0.2182
Epoch 7/10
39/39 - 31s - loss: 462.4237 - loglik: -4.6306e+02 - logprior: 0.6337
Epoch 8/10
39/39 - 31s - loss: 462.5532 - loglik: -4.6303e+02 - logprior: 0.4810
Fitted a model with MAP estimate = -461.8289
Time for alignment: 764.7668
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 749.0347 - loglik: -7.4753e+02 - logprior: -1.5014e+00
Epoch 2/10
39/39 - 22s - loss: 537.8046 - loglik: -5.3618e+02 - logprior: -1.6217e+00
Epoch 3/10
39/39 - 22s - loss: 524.3456 - loglik: -5.2286e+02 - logprior: -1.4852e+00
Epoch 4/10
39/39 - 22s - loss: 521.5613 - loglik: -5.2019e+02 - logprior: -1.3686e+00
Epoch 5/10
39/39 - 22s - loss: 520.1900 - loglik: -5.1884e+02 - logprior: -1.3475e+00
Epoch 6/10
39/39 - 22s - loss: 519.5764 - loglik: -5.1824e+02 - logprior: -1.3398e+00
Epoch 7/10
39/39 - 22s - loss: 518.6533 - loglik: -5.1731e+02 - logprior: -1.3471e+00
Epoch 8/10
39/39 - 22s - loss: 518.1313 - loglik: -5.1677e+02 - logprior: -1.3608e+00
Epoch 9/10
39/39 - 22s - loss: 517.3140 - loglik: -5.1595e+02 - logprior: -1.3615e+00
Epoch 10/10
39/39 - 22s - loss: 516.8131 - loglik: -5.1544e+02 - logprior: -1.3686e+00
Fitted a model with MAP estimate = -515.9268
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 1), (38, 1), (39, 1), (44, 1), (45, 1), (46, 2), (48, 1), (63, 1), (64, 1), (66, 2), (67, 1), (69, 1), (76, 1), (78, 2), (83, 1), (89, 1), (90, 2), (92, 1), (108, 1), (109, 2), (110, 1), (111, 1), (112, 1), (119, 1), (131, 1), (141, 1), (144, 2), (146, 1), (147, 1), (148, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (174, 1), (182, 1), (183, 1), (187, 1), (204, 1), (208, 2), (210, 1), (212, 1), (215, 1), (216, 1), (226, 1), (228, 3), (242, 1), (243, 1), (257, 1), (263, 1), (265, 1), (266, 1), (267, 1), (271, 1), (272, 1), (273, 4)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 35s - loss: 489.9716 - loglik: -4.8808e+02 - logprior: -1.8896e+00
Epoch 2/2
39/39 - 32s - loss: 470.5912 - loglik: -4.7012e+02 - logprior: -4.6652e-01
Fitted a model with MAP estimate = -467.7468
expansions: []
discards: [  0   2  57  99 181 216]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 478.2630 - loglik: -4.7633e+02 - logprior: -1.9338e+00
Epoch 2/2
39/39 - 31s - loss: 472.2819 - loglik: -4.7230e+02 - logprior: 0.0180
Fitted a model with MAP estimate = -469.1338
expansions: [(0, 2)]
discards: [ 80 139]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 474.1372 - loglik: -4.7291e+02 - logprior: -1.2236e+00
Epoch 2/10
39/39 - 31s - loss: 469.3194 - loglik: -4.6938e+02 - logprior: 0.0562
Epoch 3/10
39/39 - 31s - loss: 467.6079 - loglik: -4.6769e+02 - logprior: 0.0862
Epoch 4/10
39/39 - 31s - loss: 466.1272 - loglik: -4.6646e+02 - logprior: 0.3290
Epoch 5/10
39/39 - 31s - loss: 464.6542 - loglik: -4.6479e+02 - logprior: 0.1313
Epoch 6/10
39/39 - 31s - loss: 462.8737 - loglik: -4.6330e+02 - logprior: 0.4291
Epoch 7/10
39/39 - 31s - loss: 462.7484 - loglik: -4.6327e+02 - logprior: 0.5185
Epoch 8/10
39/39 - 31s - loss: 462.2682 - loglik: -4.6293e+02 - logprior: 0.6637
Epoch 9/10
39/39 - 31s - loss: 462.0829 - loglik: -4.6274e+02 - logprior: 0.6601
Epoch 10/10
39/39 - 31s - loss: 461.8137 - loglik: -4.6268e+02 - logprior: 0.8648
Fitted a model with MAP estimate = -461.2479
Time for alignment: 827.0097
Computed alignments with likelihoods: ['-462.1261', '-467.4934', '-467.2414', '-461.8289', '-461.2479']
Best model has likelihood: -461.2479  (prior= 1.0417 )
time for generating output: 0.2278
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9190609204575685
Training of 5 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4a846b1760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bd4957d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bcbeb13a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f49ca1ba160>
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 402.3324 - loglik: -3.9933e+02 - logprior: -3.0052e+00
Epoch 2/10
19/19 - 4s - loss: 332.7872 - loglik: -3.3158e+02 - logprior: -1.2033e+00
Epoch 3/10
19/19 - 4s - loss: 309.4161 - loglik: -3.0780e+02 - logprior: -1.6131e+00
Epoch 4/10
19/19 - 4s - loss: 304.7998 - loglik: -3.0329e+02 - logprior: -1.5055e+00
Epoch 5/10
19/19 - 4s - loss: 301.9930 - loglik: -3.0052e+02 - logprior: -1.4738e+00
Epoch 6/10
19/19 - 4s - loss: 301.4702 - loglik: -2.9999e+02 - logprior: -1.4797e+00
Epoch 7/10
19/19 - 4s - loss: 300.5244 - loglik: -2.9906e+02 - logprior: -1.4622e+00
Epoch 8/10
19/19 - 4s - loss: 300.1395 - loglik: -2.9869e+02 - logprior: -1.4479e+00
Epoch 9/10
19/19 - 4s - loss: 298.7860 - loglik: -2.9735e+02 - logprior: -1.4359e+00
Epoch 10/10
19/19 - 4s - loss: 298.9846 - loglik: -2.9757e+02 - logprior: -1.4167e+00
Fitted a model with MAP estimate = -297.6945
expansions: [(8, 2), (23, 4), (24, 1), (25, 2), (28, 1), (29, 1), (31, 1), (36, 1), (46, 3), (51, 1), (55, 1), (56, 1), (57, 1), (85, 6), (86, 4), (104, 1), (105, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 303.8738 - loglik: -3.0075e+02 - logprior: -3.1217e+00
Epoch 2/2
39/39 - 6s - loss: 294.3390 - loglik: -2.9284e+02 - logprior: -1.4982e+00
Fitted a model with MAP estimate = -290.5245
expansions: []
discards: [  7  25  31  60 107 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 295.9843 - loglik: -2.9360e+02 - logprior: -2.3841e+00
Epoch 2/2
39/39 - 6s - loss: 292.7741 - loglik: -2.9144e+02 - logprior: -1.3296e+00
Fitted a model with MAP estimate = -290.3115
expansions: [(106, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 294.1281 - loglik: -2.9175e+02 - logprior: -2.3778e+00
Epoch 2/10
39/39 - 6s - loss: 291.5759 - loglik: -2.9024e+02 - logprior: -1.3365e+00
Epoch 3/10
39/39 - 6s - loss: 289.5154 - loglik: -2.8827e+02 - logprior: -1.2422e+00
Epoch 4/10
39/39 - 6s - loss: 288.9377 - loglik: -2.8778e+02 - logprior: -1.1595e+00
Epoch 5/10
39/39 - 6s - loss: 287.3272 - loglik: -2.8628e+02 - logprior: -1.0438e+00
Epoch 6/10
39/39 - 6s - loss: 287.4444 - loglik: -2.8646e+02 - logprior: -9.8160e-01
Fitted a model with MAP estimate = -286.8038
Time for alignment: 143.4728
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 401.8871 - loglik: -3.9888e+02 - logprior: -3.0053e+00
Epoch 2/10
19/19 - 4s - loss: 333.6446 - loglik: -3.3243e+02 - logprior: -1.2182e+00
Epoch 3/10
19/19 - 4s - loss: 308.1476 - loglik: -3.0650e+02 - logprior: -1.6469e+00
Epoch 4/10
19/19 - 4s - loss: 303.8784 - loglik: -3.0230e+02 - logprior: -1.5775e+00
Epoch 5/10
19/19 - 4s - loss: 301.3732 - loglik: -2.9983e+02 - logprior: -1.5413e+00
Epoch 6/10
19/19 - 4s - loss: 300.7633 - loglik: -2.9926e+02 - logprior: -1.5010e+00
Epoch 7/10
19/19 - 4s - loss: 300.2357 - loglik: -2.9876e+02 - logprior: -1.4776e+00
Epoch 8/10
19/19 - 4s - loss: 300.0267 - loglik: -2.9857e+02 - logprior: -1.4573e+00
Epoch 9/10
19/19 - 4s - loss: 298.8976 - loglik: -2.9746e+02 - logprior: -1.4328e+00
Epoch 10/10
19/19 - 4s - loss: 298.4724 - loglik: -2.9706e+02 - logprior: -1.4138e+00
Fitted a model with MAP estimate = -297.4282
expansions: [(8, 1), (23, 4), (25, 3), (28, 1), (29, 1), (34, 1), (36, 1), (50, 1), (51, 1), (56, 1), (57, 1), (71, 1), (85, 4), (86, 6), (104, 1), (105, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 141 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 303.4754 - loglik: -3.0035e+02 - logprior: -3.1299e+00
Epoch 2/2
39/39 - 6s - loss: 294.4593 - loglik: -2.9296e+02 - logprior: -1.5010e+00
Fitted a model with MAP estimate = -290.7899
expansions: []
discards: [ 25  30 133]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 295.9386 - loglik: -2.9351e+02 - logprior: -2.4286e+00
Epoch 2/2
39/39 - 6s - loss: 292.7068 - loglik: -2.9135e+02 - logprior: -1.3564e+00
Fitted a model with MAP estimate = -290.3771
expansions: []
discards: [100]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 137 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 294.8237 - loglik: -2.9246e+02 - logprior: -2.3668e+00
Epoch 2/10
39/39 - 6s - loss: 291.6802 - loglik: -2.9038e+02 - logprior: -1.2970e+00
Epoch 3/10
39/39 - 6s - loss: 290.1989 - loglik: -2.8899e+02 - logprior: -1.2077e+00
Epoch 4/10
39/39 - 6s - loss: 289.0341 - loglik: -2.8790e+02 - logprior: -1.1354e+00
Epoch 5/10
39/39 - 6s - loss: 288.4193 - loglik: -2.8737e+02 - logprior: -1.0522e+00
Epoch 6/10
39/39 - 6s - loss: 287.4342 - loglik: -2.8647e+02 - logprior: -9.6409e-01
Epoch 7/10
39/39 - 6s - loss: 287.8970 - loglik: -2.8701e+02 - logprior: -8.8495e-01
Fitted a model with MAP estimate = -287.0092
Time for alignment: 148.7096
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 401.8710 - loglik: -3.9887e+02 - logprior: -3.0040e+00
Epoch 2/10
19/19 - 4s - loss: 333.6304 - loglik: -3.3242e+02 - logprior: -1.2054e+00
Epoch 3/10
19/19 - 4s - loss: 308.8819 - loglik: -3.0724e+02 - logprior: -1.6447e+00
Epoch 4/10
19/19 - 4s - loss: 303.4846 - loglik: -3.0191e+02 - logprior: -1.5777e+00
Epoch 5/10
19/19 - 4s - loss: 301.6405 - loglik: -3.0010e+02 - logprior: -1.5437e+00
Epoch 6/10
19/19 - 4s - loss: 300.2203 - loglik: -2.9870e+02 - logprior: -1.5195e+00
Epoch 7/10
19/19 - 4s - loss: 299.1877 - loglik: -2.9769e+02 - logprior: -1.4957e+00
Epoch 8/10
19/19 - 4s - loss: 301.6894 - loglik: -3.0022e+02 - logprior: -1.4732e+00
Fitted a model with MAP estimate = -297.6751
expansions: [(8, 1), (23, 4), (24, 1), (25, 2), (28, 1), (29, 1), (31, 1), (36, 1), (46, 3), (51, 1), (56, 1), (57, 1), (60, 1), (85, 7), (86, 5), (104, 1), (105, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 145 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 303.3553 - loglik: -3.0023e+02 - logprior: -3.1271e+00
Epoch 2/2
39/39 - 6s - loss: 293.9975 - loglik: -2.9249e+02 - logprior: -1.5105e+00
Fitted a model with MAP estimate = -290.5389
expansions: [(111, 1)]
discards: [ 24  30  58 106 107 137]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 296.0088 - loglik: -2.9359e+02 - logprior: -2.4152e+00
Epoch 2/2
39/39 - 6s - loss: 292.5987 - loglik: -2.9121e+02 - logprior: -1.3921e+00
Fitted a model with MAP estimate = -290.3056
expansions: []
discards: [101 102]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 138 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 294.8128 - loglik: -2.9240e+02 - logprior: -2.4108e+00
Epoch 2/10
39/39 - 6s - loss: 291.8881 - loglik: -2.9055e+02 - logprior: -1.3344e+00
Epoch 3/10
39/39 - 6s - loss: 290.1181 - loglik: -2.8889e+02 - logprior: -1.2306e+00
Epoch 4/10
39/39 - 6s - loss: 289.0061 - loglik: -2.8784e+02 - logprior: -1.1633e+00
Epoch 5/10
39/39 - 6s - loss: 288.1571 - loglik: -2.8712e+02 - logprior: -1.0391e+00
Epoch 6/10
39/39 - 6s - loss: 287.8613 - loglik: -2.8690e+02 - logprior: -9.6489e-01
Epoch 7/10
39/39 - 6s - loss: 287.2882 - loglik: -2.8639e+02 - logprior: -8.9672e-01
Epoch 8/10
39/39 - 6s - loss: 286.7783 - loglik: -2.8595e+02 - logprior: -8.2692e-01
Epoch 9/10
39/39 - 6s - loss: 286.7762 - loglik: -2.8603e+02 - logprior: -7.4936e-01
Epoch 10/10
39/39 - 6s - loss: 287.2101 - loglik: -2.8658e+02 - logprior: -6.2551e-01
Fitted a model with MAP estimate = -286.3657
Time for alignment: 157.9277
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 401.8578 - loglik: -3.9885e+02 - logprior: -3.0055e+00
Epoch 2/10
19/19 - 4s - loss: 334.3103 - loglik: -3.3309e+02 - logprior: -1.2161e+00
Epoch 3/10
19/19 - 4s - loss: 310.1459 - loglik: -3.0852e+02 - logprior: -1.6300e+00
Epoch 4/10
19/19 - 4s - loss: 304.4143 - loglik: -3.0288e+02 - logprior: -1.5383e+00
Epoch 5/10
19/19 - 4s - loss: 304.1319 - loglik: -3.0263e+02 - logprior: -1.4983e+00
Epoch 6/10
19/19 - 4s - loss: 300.8102 - loglik: -2.9935e+02 - logprior: -1.4652e+00
Epoch 7/10
19/19 - 4s - loss: 301.0073 - loglik: -2.9956e+02 - logprior: -1.4440e+00
Fitted a model with MAP estimate = -299.5685
expansions: [(8, 1), (23, 4), (25, 3), (28, 1), (29, 1), (31, 1), (36, 1), (46, 2), (51, 1), (55, 1), (56, 1), (57, 1), (85, 7), (86, 5), (104, 1), (105, 2)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 303.3215 - loglik: -3.0021e+02 - logprior: -3.1164e+00
Epoch 2/2
39/39 - 6s - loss: 294.1948 - loglik: -2.9272e+02 - logprior: -1.4752e+00
Fitted a model with MAP estimate = -290.6771
expansions: [(110, 1)]
discards: [ 25  30 105 106 112 136]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 295.9115 - loglik: -2.9352e+02 - logprior: -2.3923e+00
Epoch 2/2
39/39 - 6s - loss: 292.7837 - loglik: -2.9144e+02 - logprior: -1.3468e+00
Fitted a model with MAP estimate = -290.4912
expansions: []
discards: [100 101 102]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 136 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 8s - loss: 294.9883 - loglik: -2.9264e+02 - logprior: -2.3522e+00
Epoch 2/10
39/39 - 6s - loss: 292.1257 - loglik: -2.9083e+02 - logprior: -1.2977e+00
Epoch 3/10
39/39 - 6s - loss: 290.2326 - loglik: -2.8901e+02 - logprior: -1.2212e+00
Epoch 4/10
39/39 - 6s - loss: 288.5071 - loglik: -2.8734e+02 - logprior: -1.1639e+00
Epoch 5/10
39/39 - 6s - loss: 288.7538 - loglik: -2.8766e+02 - logprior: -1.0943e+00
Fitted a model with MAP estimate = -287.8415
Time for alignment: 123.3903
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 402.1012 - loglik: -3.9910e+02 - logprior: -3.0012e+00
Epoch 2/10
19/19 - 4s - loss: 333.5516 - loglik: -3.3235e+02 - logprior: -1.1999e+00
Epoch 3/10
19/19 - 4s - loss: 309.6418 - loglik: -3.0801e+02 - logprior: -1.6358e+00
Epoch 4/10
19/19 - 4s - loss: 303.8441 - loglik: -3.0231e+02 - logprior: -1.5309e+00
Epoch 5/10
19/19 - 4s - loss: 301.5122 - loglik: -3.0001e+02 - logprior: -1.4992e+00
Epoch 6/10
19/19 - 4s - loss: 301.9273 - loglik: -3.0044e+02 - logprior: -1.4837e+00
Fitted a model with MAP estimate = -299.4554
expansions: [(8, 1), (23, 4), (24, 1), (25, 2), (28, 1), (29, 1), (30, 1), (36, 1), (46, 2), (51, 1), (55, 1), (56, 1), (57, 1), (84, 5), (85, 2), (86, 6), (104, 1), (105, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 144 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 302.7206 - loglik: -2.9962e+02 - logprior: -3.0982e+00
Epoch 2/2
39/39 - 6s - loss: 293.8553 - loglik: -2.9240e+02 - logprior: -1.4512e+00
Fitted a model with MAP estimate = -290.5956
expansions: []
discards: [ 24  30 111 112]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 295.7719 - loglik: -2.9337e+02 - logprior: -2.4047e+00
Epoch 2/2
39/39 - 6s - loss: 292.6304 - loglik: -2.9129e+02 - logprior: -1.3417e+00
Fitted a model with MAP estimate = -290.4162
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 140 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 294.3541 - loglik: -2.9196e+02 - logprior: -2.3961e+00
Epoch 2/10
39/39 - 6s - loss: 291.6441 - loglik: -2.9032e+02 - logprior: -1.3250e+00
Epoch 3/10
39/39 - 6s - loss: 289.5178 - loglik: -2.8826e+02 - logprior: -1.2600e+00
Epoch 4/10
39/39 - 6s - loss: 289.1654 - loglik: -2.8799e+02 - logprior: -1.1741e+00
Epoch 5/10
39/39 - 6s - loss: 287.6196 - loglik: -2.8655e+02 - logprior: -1.0717e+00
Epoch 6/10
39/39 - 6s - loss: 287.4744 - loglik: -2.8647e+02 - logprior: -1.0030e+00
Epoch 7/10
39/39 - 6s - loss: 286.4799 - loglik: -2.8553e+02 - logprior: -9.4726e-01
Epoch 8/10
39/39 - 6s - loss: 287.3488 - loglik: -2.8647e+02 - logprior: -8.7640e-01
Fitted a model with MAP estimate = -286.3918
Time for alignment: 138.8505
Computed alignments with likelihoods: ['-286.8038', '-287.0092', '-286.3657', '-287.8415', '-286.3918']
Best model has likelihood: -286.3657  (prior= -0.5912 )
time for generating output: 0.2171
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.3015173892399071
Training of 5 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49c8378100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49c834d940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4daf107fd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4daf1154c0>
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 474.3472 - loglik: -4.7242e+02 - logprior: -1.9295e+00
Epoch 2/10
39/39 - 7s - loss: 430.3634 - loglik: -4.2908e+02 - logprior: -1.2823e+00
Epoch 3/10
39/39 - 7s - loss: 424.9242 - loglik: -4.2361e+02 - logprior: -1.3104e+00
Epoch 4/10
39/39 - 7s - loss: 423.6810 - loglik: -4.2241e+02 - logprior: -1.2688e+00
Epoch 5/10
39/39 - 7s - loss: 423.6028 - loglik: -4.2235e+02 - logprior: -1.2565e+00
Epoch 6/10
39/39 - 7s - loss: 422.8956 - loglik: -4.2164e+02 - logprior: -1.2523e+00
Epoch 7/10
39/39 - 7s - loss: 422.5956 - loglik: -4.2135e+02 - logprior: -1.2451e+00
Epoch 8/10
39/39 - 7s - loss: 422.4536 - loglik: -4.2121e+02 - logprior: -1.2453e+00
Epoch 9/10
39/39 - 7s - loss: 422.0950 - loglik: -4.2086e+02 - logprior: -1.2345e+00
Epoch 10/10
39/39 - 7s - loss: 422.1965 - loglik: -4.2096e+02 - logprior: -1.2389e+00
Fitted a model with MAP estimate = -421.6463
expansions: [(19, 1), (31, 19), (45, 3), (46, 1), (77, 2), (78, 8), (79, 2), (98, 1), (101, 2), (102, 1), (103, 1), (104, 1), (105, 1)]
discards: [ 1 56 57 58 59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 166 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 422.1792 - loglik: -4.2023e+02 - logprior: -1.9468e+00
Epoch 2/2
39/39 - 9s - loss: 415.4776 - loglik: -4.1450e+02 - logprior: -9.7271e-01
Fitted a model with MAP estimate = -414.1509
expansions: []
discards: [ 36  37  38  39  40  41  42  43  44  45  65  66 103 104 105 106 107 108
 109]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 421.4436 - loglik: -4.1956e+02 - logprior: -1.8805e+00
Epoch 2/2
39/39 - 8s - loss: 418.3674 - loglik: -4.1759e+02 - logprior: -7.7566e-01
Fitted a model with MAP estimate = -417.4298
expansions: [(91, 3)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 150 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 419.9176 - loglik: -4.1812e+02 - logprior: -1.8001e+00
Epoch 2/10
39/39 - 8s - loss: 417.6302 - loglik: -4.1693e+02 - logprior: -7.0357e-01
Epoch 3/10
39/39 - 8s - loss: 416.6068 - loglik: -4.1595e+02 - logprior: -6.6031e-01
Epoch 4/10
39/39 - 8s - loss: 416.3743 - loglik: -4.1576e+02 - logprior: -6.1190e-01
Epoch 5/10
39/39 - 8s - loss: 415.9919 - loglik: -4.1541e+02 - logprior: -5.8149e-01
Epoch 6/10
39/39 - 8s - loss: 415.8545 - loglik: -4.1531e+02 - logprior: -5.4021e-01
Epoch 7/10
39/39 - 8s - loss: 415.6345 - loglik: -4.1513e+02 - logprior: -5.0280e-01
Epoch 8/10
39/39 - 8s - loss: 416.0206 - loglik: -4.1555e+02 - logprior: -4.6737e-01
Fitted a model with MAP estimate = -415.3847
Time for alignment: 217.3816
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 474.6472 - loglik: -4.7272e+02 - logprior: -1.9286e+00
Epoch 2/10
39/39 - 7s - loss: 432.9062 - loglik: -4.3160e+02 - logprior: -1.3025e+00
Epoch 3/10
39/39 - 7s - loss: 428.4706 - loglik: -4.2721e+02 - logprior: -1.2605e+00
Epoch 4/10
39/39 - 7s - loss: 427.5819 - loglik: -4.2636e+02 - logprior: -1.2260e+00
Epoch 5/10
39/39 - 7s - loss: 426.9486 - loglik: -4.2574e+02 - logprior: -1.2107e+00
Epoch 6/10
39/39 - 7s - loss: 426.7498 - loglik: -4.2555e+02 - logprior: -1.1988e+00
Epoch 7/10
39/39 - 7s - loss: 426.3944 - loglik: -4.2520e+02 - logprior: -1.1991e+00
Epoch 8/10
39/39 - 7s - loss: 426.3421 - loglik: -4.2514e+02 - logprior: -1.2050e+00
Epoch 9/10
39/39 - 7s - loss: 426.0093 - loglik: -4.2481e+02 - logprior: -1.1975e+00
Epoch 10/10
39/39 - 7s - loss: 426.0362 - loglik: -4.2484e+02 - logprior: -1.1913e+00
Fitted a model with MAP estimate = -425.4483
expansions: [(32, 1), (33, 3), (34, 2), (35, 1), (36, 1), (45, 1), (46, 2), (47, 1), (79, 1), (80, 8), (81, 3), (103, 5), (104, 1), (105, 2)]
discards: [ 1 57 58 59 60 61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 425.6440 - loglik: -4.2372e+02 - logprior: -1.9288e+00
Epoch 2/2
39/39 - 8s - loss: 419.0423 - loglik: -4.1811e+02 - logprior: -9.2862e-01
Fitted a model with MAP estimate = -417.6532
expansions: [(36, 1), (37, 1), (56, 1), (65, 19)]
discards: [41 42 43 92 93 94 97]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 420.1726 - loglik: -4.1828e+02 - logprior: -1.8954e+00
Epoch 2/2
39/39 - 9s - loss: 413.7152 - loglik: -4.1285e+02 - logprior: -8.6479e-01
Fitted a model with MAP estimate = -412.0012
expansions: []
discards: [68 69 70 71 72 73 74 75 76 77 78 79 80]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 156 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 418.6349 - loglik: -4.1684e+02 - logprior: -1.7945e+00
Epoch 2/10
39/39 - 8s - loss: 415.9519 - loglik: -4.1527e+02 - logprior: -6.8104e-01
Epoch 3/10
39/39 - 8s - loss: 414.9196 - loglik: -4.1429e+02 - logprior: -6.3388e-01
Epoch 4/10
39/39 - 8s - loss: 414.6224 - loglik: -4.1404e+02 - logprior: -5.8521e-01
Epoch 5/10
39/39 - 8s - loss: 413.9690 - loglik: -4.1343e+02 - logprior: -5.3925e-01
Epoch 6/10
39/39 - 8s - loss: 413.5475 - loglik: -4.1306e+02 - logprior: -4.8884e-01
Epoch 7/10
39/39 - 8s - loss: 413.8953 - loglik: -4.1344e+02 - logprior: -4.5649e-01
Fitted a model with MAP estimate = -413.3758
Time for alignment: 212.5708
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 474.4457 - loglik: -4.7253e+02 - logprior: -1.9169e+00
Epoch 2/10
39/39 - 7s - loss: 432.7597 - loglik: -4.3152e+02 - logprior: -1.2391e+00
Epoch 3/10
39/39 - 7s - loss: 428.1716 - loglik: -4.2696e+02 - logprior: -1.2161e+00
Epoch 4/10
39/39 - 7s - loss: 427.2271 - loglik: -4.2604e+02 - logprior: -1.1822e+00
Epoch 5/10
39/39 - 7s - loss: 426.9923 - loglik: -4.2583e+02 - logprior: -1.1605e+00
Epoch 6/10
39/39 - 7s - loss: 426.2257 - loglik: -4.2506e+02 - logprior: -1.1613e+00
Epoch 7/10
39/39 - 7s - loss: 425.8116 - loglik: -4.2464e+02 - logprior: -1.1679e+00
Epoch 8/10
39/39 - 7s - loss: 426.1313 - loglik: -4.2496e+02 - logprior: -1.1670e+00
Fitted a model with MAP estimate = -425.1949
expansions: [(31, 2), (32, 3), (33, 1), (48, 16), (78, 1), (79, 8), (80, 2), (103, 5), (104, 1), (105, 2)]
discards: [ 1 36 37 57 58 59 60 61]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 161 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 425.5484 - loglik: -4.2357e+02 - logprior: -1.9759e+00
Epoch 2/2
39/39 - 8s - loss: 416.9398 - loglik: -4.1597e+02 - logprior: -9.7292e-01
Fitted a model with MAP estimate = -415.4927
expansions: [(35, 1), (36, 1)]
discards: [ 58  59  60  61  62  63  64  99 100 101 104]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 152 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 421.0959 - loglik: -4.1922e+02 - logprior: -1.8789e+00
Epoch 2/2
39/39 - 8s - loss: 417.5604 - loglik: -4.1678e+02 - logprior: -7.8363e-01
Fitted a model with MAP estimate = -416.4773
expansions: []
discards: [59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 420.8007 - loglik: -4.1899e+02 - logprior: -1.8057e+00
Epoch 2/10
39/39 - 8s - loss: 417.8262 - loglik: -4.1711e+02 - logprior: -7.1658e-01
Epoch 3/10
39/39 - 8s - loss: 416.9046 - loglik: -4.1624e+02 - logprior: -6.6594e-01
Epoch 4/10
39/39 - 8s - loss: 416.8506 - loglik: -4.1623e+02 - logprior: -6.1917e-01
Epoch 5/10
39/39 - 8s - loss: 416.2912 - loglik: -4.1572e+02 - logprior: -5.7434e-01
Epoch 6/10
39/39 - 8s - loss: 416.6453 - loglik: -4.1612e+02 - logprior: -5.2969e-01
Fitted a model with MAP estimate = -416.0262
Time for alignment: 186.6410
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 474.0694 - loglik: -4.7211e+02 - logprior: -1.9553e+00
Epoch 2/10
39/39 - 7s - loss: 431.9470 - loglik: -4.3063e+02 - logprior: -1.3131e+00
Epoch 3/10
39/39 - 7s - loss: 427.2792 - loglik: -4.2597e+02 - logprior: -1.3098e+00
Epoch 4/10
39/39 - 7s - loss: 426.3104 - loglik: -4.2500e+02 - logprior: -1.3102e+00
Epoch 5/10
39/39 - 7s - loss: 425.0478 - loglik: -4.2375e+02 - logprior: -1.2957e+00
Epoch 6/10
39/39 - 7s - loss: 425.3234 - loglik: -4.2403e+02 - logprior: -1.2916e+00
Fitted a model with MAP estimate = -424.7256
expansions: [(12, 1), (21, 1), (31, 6), (32, 1), (46, 3), (47, 3), (77, 1), (78, 1), (79, 1), (80, 6), (81, 1), (105, 1), (106, 3), (107, 4), (108, 1)]
discards: [ 1  5 57 58 59 60 61 62]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 154 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 426.3547 - loglik: -4.2438e+02 - logprior: -1.9722e+00
Epoch 2/2
39/39 - 8s - loss: 419.3440 - loglik: -4.1838e+02 - logprior: -9.6424e-01
Fitted a model with MAP estimate = -417.4614
expansions: [(34, 1), (57, 1), (126, 1)]
discards: [ 54  92  93  94  95 129]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 420.3393 - loglik: -4.1845e+02 - logprior: -1.8932e+00
Epoch 2/2
39/39 - 8s - loss: 416.9433 - loglik: -4.1604e+02 - logprior: -9.0298e-01
Fitted a model with MAP estimate = -415.7177
expansions: []
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 151 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 418.3536 - loglik: -4.1643e+02 - logprior: -1.9265e+00
Epoch 2/10
39/39 - 8s - loss: 416.2654 - loglik: -4.1542e+02 - logprior: -8.4967e-01
Epoch 3/10
39/39 - 8s - loss: 415.7395 - loglik: -4.1495e+02 - logprior: -7.8925e-01
Epoch 4/10
39/39 - 8s - loss: 414.7811 - loglik: -4.1403e+02 - logprior: -7.5231e-01
Epoch 5/10
39/39 - 8s - loss: 414.7038 - loglik: -4.1400e+02 - logprior: -7.0273e-01
Epoch 6/10
39/39 - 8s - loss: 414.4549 - loglik: -4.1379e+02 - logprior: -6.6111e-01
Epoch 7/10
39/39 - 8s - loss: 414.5960 - loglik: -4.1397e+02 - logprior: -6.2789e-01
Fitted a model with MAP estimate = -414.1311
Time for alignment: 177.3798
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 474.5806 - loglik: -4.7264e+02 - logprior: -1.9386e+00
Epoch 2/10
39/39 - 7s - loss: 432.9395 - loglik: -4.3166e+02 - logprior: -1.2766e+00
Epoch 3/10
39/39 - 7s - loss: 428.2587 - loglik: -4.2699e+02 - logprior: -1.2707e+00
Epoch 4/10
39/39 - 7s - loss: 427.0750 - loglik: -4.2582e+02 - logprior: -1.2543e+00
Epoch 5/10
39/39 - 7s - loss: 426.7779 - loglik: -4.2552e+02 - logprior: -1.2539e+00
Epoch 6/10
39/39 - 7s - loss: 426.1956 - loglik: -4.2494e+02 - logprior: -1.2521e+00
Epoch 7/10
39/39 - 7s - loss: 426.3831 - loglik: -4.2514e+02 - logprior: -1.2415e+00
Fitted a model with MAP estimate = -425.7090
expansions: [(38, 17), (44, 1), (45, 3), (46, 1), (77, 1), (78, 1), (79, 6), (80, 2), (95, 1), (102, 2), (103, 3), (104, 1), (105, 1)]
discards: [ 1 56 57 58 59]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 163 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 422.3273 - loglik: -4.2037e+02 - logprior: -1.9620e+00
Epoch 2/2
39/39 - 8s - loss: 413.6696 - loglik: -4.1270e+02 - logprior: -9.7411e-01
Fitted a model with MAP estimate = -412.3068
expansions: []
discards: [ 41  42  43  44  45  46  47  48  49  63 101 102 103 104 105 106]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 420.4748 - loglik: -4.1856e+02 - logprior: -1.9139e+00
Epoch 2/2
39/39 - 8s - loss: 417.2105 - loglik: -4.1639e+02 - logprior: -8.2292e-01
Fitted a model with MAP estimate = -416.2806
expansions: [(91, 2)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 149 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 418.4376 - loglik: -4.1661e+02 - logprior: -1.8250e+00
Epoch 2/10
39/39 - 8s - loss: 416.2555 - loglik: -4.1551e+02 - logprior: -7.4546e-01
Epoch 3/10
39/39 - 8s - loss: 415.5741 - loglik: -4.1489e+02 - logprior: -6.8836e-01
Epoch 4/10
39/39 - 8s - loss: 415.2710 - loglik: -4.1462e+02 - logprior: -6.5180e-01
Epoch 5/10
39/39 - 8s - loss: 414.7015 - loglik: -4.1409e+02 - logprior: -6.1019e-01
Epoch 6/10
39/39 - 8s - loss: 414.9519 - loglik: -4.1437e+02 - logprior: -5.7812e-01
Fitted a model with MAP estimate = -414.3509
Time for alignment: 179.0328
Computed alignments with likelihoods: ['-414.1509', '-412.0012', '-415.4927', '-414.1311', '-412.3068']
Best model has likelihood: -412.0012  (prior= -0.8555 )
time for generating output: 0.1868
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.8190514149063371
Training of 5 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4daf4afa30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49f007f370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4a4c62afa0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d957fed30>
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 803.3395 - loglik: -8.0198e+02 - logprior: -1.3561e+00
Epoch 2/10
39/39 - 18s - loss: 731.0106 - loglik: -7.3048e+02 - logprior: -5.2795e-01
Epoch 3/10
39/39 - 18s - loss: 720.2388 - loglik: -7.1961e+02 - logprior: -6.2732e-01
Epoch 4/10
39/39 - 18s - loss: 717.7370 - loglik: -7.1714e+02 - logprior: -5.9624e-01
Epoch 5/10
39/39 - 18s - loss: 716.2969 - loglik: -7.1569e+02 - logprior: -6.0572e-01
Epoch 6/10
39/39 - 18s - loss: 715.3714 - loglik: -7.1476e+02 - logprior: -6.0644e-01
Epoch 7/10
39/39 - 18s - loss: 714.9081 - loglik: -7.1430e+02 - logprior: -6.1151e-01
Epoch 8/10
39/39 - 18s - loss: 713.9547 - loglik: -7.1334e+02 - logprior: -6.1934e-01
Epoch 9/10
39/39 - 18s - loss: 713.6552 - loglik: -7.1304e+02 - logprior: -6.1490e-01
Epoch 10/10
39/39 - 18s - loss: 713.5423 - loglik: -7.1294e+02 - logprior: -5.9921e-01
Fitted a model with MAP estimate = -712.5362
expansions: [(0, 6), (6, 2), (7, 2), (40, 1), (41, 2), (43, 1), (46, 1), (48, 3), (49, 2), (83, 4), (84, 4), (86, 4), (104, 1), (117, 1), (120, 1), (121, 1), (126, 1), (153, 3), (157, 2), (209, 1), (210, 1), (212, 1), (214, 1)]
discards: [1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 272 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 713.7593 - loglik: -7.1160e+02 - logprior: -2.1557e+00
Epoch 2/2
39/39 - 23s - loss: 705.1475 - loglik: -7.0453e+02 - logprior: -6.1884e-01
Fitted a model with MAP estimate = -703.0081
expansions: [(103, 2), (104, 1), (147, 1), (195, 1), (196, 2), (240, 3)]
discards: [  1   2  10  64 105 106 107 190 191 192]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 272 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 707.3841 - loglik: -7.0591e+02 - logprior: -1.4788e+00
Epoch 2/2
39/39 - 23s - loss: 703.5836 - loglik: -7.0324e+02 - logprior: -3.4641e-01
Fitted a model with MAP estimate = -701.6433
expansions: [(0, 3), (192, 1)]
discards: [ 48 187 188 189 190]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 271 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 706.8864 - loglik: -7.0470e+02 - logprior: -2.1847e+00
Epoch 2/10
39/39 - 23s - loss: 702.9656 - loglik: -7.0271e+02 - logprior: -2.5868e-01
Epoch 3/10
39/39 - 23s - loss: 701.6946 - loglik: -7.0153e+02 - logprior: -1.6738e-01
Epoch 4/10
39/39 - 23s - loss: 701.3278 - loglik: -7.0125e+02 - logprior: -7.6072e-02
Epoch 5/10
39/39 - 23s - loss: 701.4728 - loglik: -7.0147e+02 - logprior: -2.8235e-03
Fitted a model with MAP estimate = -700.4080
Time for alignment: 517.1504
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 803.6407 - loglik: -8.0229e+02 - logprior: -1.3533e+00
Epoch 2/10
39/39 - 18s - loss: 731.8739 - loglik: -7.3135e+02 - logprior: -5.2158e-01
Epoch 3/10
39/39 - 18s - loss: 720.0917 - loglik: -7.1945e+02 - logprior: -6.3792e-01
Epoch 4/10
39/39 - 18s - loss: 717.3846 - loglik: -7.1676e+02 - logprior: -6.2569e-01
Epoch 5/10
39/39 - 18s - loss: 715.8782 - loglik: -7.1527e+02 - logprior: -6.0809e-01
Epoch 6/10
39/39 - 18s - loss: 715.8906 - loglik: -7.1530e+02 - logprior: -5.9228e-01
Fitted a model with MAP estimate = -714.6057
expansions: [(0, 5), (9, 2), (41, 1), (44, 2), (47, 2), (48, 2), (49, 2), (50, 1), (91, 10), (103, 1), (119, 1), (121, 1), (127, 1), (152, 1), (154, 4), (206, 5), (207, 2), (208, 1), (209, 1), (218, 1)]
discards: [0 1 2 3]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 271 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 714.5593 - loglik: -7.1294e+02 - logprior: -1.6176e+00
Epoch 2/2
39/39 - 23s - loss: 706.6620 - loglik: -7.0612e+02 - logprior: -5.3812e-01
Fitted a model with MAP estimate = -704.4500
expansions: [(0, 5), (107, 2), (108, 1), (140, 1), (180, 1), (183, 1)]
discards: [  0   2  10  58 184 185 187 188 240 241 242 243 244 245]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 268 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 709.8103 - loglik: -7.0835e+02 - logprior: -1.4581e+00
Epoch 2/2
39/39 - 23s - loss: 705.8527 - loglik: -7.0561e+02 - logprior: -2.4466e-01
Fitted a model with MAP estimate = -703.6865
expansions: [(0, 5), (144, 1), (240, 3)]
discards: [ 1  2  3  4  5  6  7  8 50]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 268 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 708.5321 - loglik: -7.0697e+02 - logprior: -1.5591e+00
Epoch 2/10
39/39 - 23s - loss: 704.6377 - loglik: -7.0445e+02 - logprior: -1.8658e-01
Epoch 3/10
39/39 - 23s - loss: 703.2714 - loglik: -7.0321e+02 - logprior: -5.7679e-02
Epoch 4/10
39/39 - 23s - loss: 703.0200 - loglik: -7.0305e+02 - logprior: 0.0312
Epoch 5/10
39/39 - 23s - loss: 702.4077 - loglik: -7.0252e+02 - logprior: 0.1110
Epoch 6/10
39/39 - 23s - loss: 702.0292 - loglik: -7.0220e+02 - logprior: 0.1756
Epoch 7/10
39/39 - 23s - loss: 702.1849 - loglik: -7.0243e+02 - logprior: 0.2479
Fitted a model with MAP estimate = -701.6479
Time for alignment: 486.5550
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 802.9816 - loglik: -8.0163e+02 - logprior: -1.3535e+00
Epoch 2/10
39/39 - 18s - loss: 731.4006 - loglik: -7.3078e+02 - logprior: -6.2151e-01
Epoch 3/10
39/39 - 18s - loss: 720.7017 - loglik: -7.1995e+02 - logprior: -7.5160e-01
Epoch 4/10
39/39 - 18s - loss: 718.1471 - loglik: -7.1743e+02 - logprior: -7.1383e-01
Epoch 5/10
39/39 - 18s - loss: 716.3311 - loglik: -7.1563e+02 - logprior: -7.0580e-01
Epoch 6/10
39/39 - 18s - loss: 715.6966 - loglik: -7.1500e+02 - logprior: -6.9350e-01
Epoch 7/10
39/39 - 18s - loss: 715.2783 - loglik: -7.1458e+02 - logprior: -6.9636e-01
Epoch 8/10
39/39 - 18s - loss: 714.8508 - loglik: -7.1416e+02 - logprior: -6.8639e-01
Epoch 9/10
39/39 - 18s - loss: 714.5001 - loglik: -7.1383e+02 - logprior: -6.7142e-01
Epoch 10/10
39/39 - 18s - loss: 714.1999 - loglik: -7.1354e+02 - logprior: -6.5853e-01
Fitted a model with MAP estimate = -713.3413
expansions: [(0, 4), (23, 1), (43, 2), (44, 6), (46, 1), (47, 2), (48, 2), (56, 1), (86, 1), (90, 1), (91, 15), (103, 1), (116, 1), (119, 1), (120, 1), (125, 1), (152, 4), (156, 3), (178, 4), (184, 1), (206, 6), (208, 1), (209, 1), (216, 1)]
discards: [ 76 228]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 713.8572 - loglik: -7.1164e+02 - logprior: -2.2200e+00
Epoch 2/2
39/39 - 25s - loss: 703.5291 - loglik: -7.0320e+02 - logprior: -3.2478e-01
Fitted a model with MAP estimate = -701.1088
expansions: [(0, 4), (152, 1), (289, 3)]
discards: [  1   2   3  51  52  53  54  55  56  57  62  95  96  97  98 114 115 116
 117 118 195 196 197 198 200 261 262 288]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 269 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 710.7094 - loglik: -7.0838e+02 - logprior: -2.3266e+00
Epoch 2/2
39/39 - 23s - loss: 705.5004 - loglik: -7.0520e+02 - logprior: -3.0284e-01
Fitted a model with MAP estimate = -703.6611
expansions: [(0, 3), (91, 2)]
discards: [  0   1   2   3 265 266 267]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 267 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 707.6049 - loglik: -7.0607e+02 - logprior: -1.5306e+00
Epoch 2/10
39/39 - 23s - loss: 703.8981 - loglik: -7.0390e+02 - logprior: 0.0036
Epoch 3/10
39/39 - 23s - loss: 702.8415 - loglik: -7.0306e+02 - logprior: 0.2170
Epoch 4/10
39/39 - 23s - loss: 702.1284 - loglik: -7.0245e+02 - logprior: 0.3199
Epoch 5/10
39/39 - 23s - loss: 701.5125 - loglik: -7.0192e+02 - logprior: 0.4088
Epoch 6/10
39/39 - 23s - loss: 701.5480 - loglik: -7.0202e+02 - logprior: 0.4711
Fitted a model with MAP estimate = -700.9855
Time for alignment: 544.8787
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 803.9117 - loglik: -8.0256e+02 - logprior: -1.3511e+00
Epoch 2/10
39/39 - 18s - loss: 732.1710 - loglik: -7.3165e+02 - logprior: -5.2163e-01
Epoch 3/10
39/39 - 18s - loss: 721.1486 - loglik: -7.2048e+02 - logprior: -6.6745e-01
Epoch 4/10
39/39 - 18s - loss: 718.4500 - loglik: -7.1780e+02 - logprior: -6.4538e-01
Epoch 5/10
39/39 - 18s - loss: 717.1819 - loglik: -7.1655e+02 - logprior: -6.3664e-01
Epoch 6/10
39/39 - 18s - loss: 716.3707 - loglik: -7.1573e+02 - logprior: -6.3845e-01
Epoch 7/10
39/39 - 18s - loss: 716.2055 - loglik: -7.1558e+02 - logprior: -6.2656e-01
Epoch 8/10
39/39 - 18s - loss: 715.3832 - loglik: -7.1476e+02 - logprior: -6.2175e-01
Epoch 9/10
39/39 - 18s - loss: 715.8866 - loglik: -7.1527e+02 - logprior: -6.1390e-01
Fitted a model with MAP estimate = -714.7824
expansions: [(0, 6), (23, 1), (43, 2), (46, 1), (47, 2), (48, 3), (79, 1), (91, 9), (92, 1), (101, 1), (115, 2), (120, 1), (127, 1), (154, 2), (179, 5), (180, 1), (206, 7), (207, 1), (214, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 277 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 712.8214 - loglik: -7.1061e+02 - logprior: -2.2149e+00
Epoch 2/2
39/39 - 24s - loss: 704.4243 - loglik: -7.0384e+02 - logprior: -5.8878e-01
Fitted a model with MAP estimate = -702.3301
expansions: [(106, 1), (110, 1), (186, 2)]
discards: [  0   2  91  92 248 249]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 275 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 709.2587 - loglik: -7.0687e+02 - logprior: -2.3897e+00
Epoch 2/2
39/39 - 24s - loss: 703.4185 - loglik: -7.0304e+02 - logprior: -3.7696e-01
Fitted a model with MAP estimate = -701.3950
expansions: [(0, 4), (95, 1)]
discards: [  0  58  89  90  91 184 203 204 205 206]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 270 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 707.9250 - loglik: -7.0657e+02 - logprior: -1.3571e+00
Epoch 2/10
39/39 - 23s - loss: 703.8500 - loglik: -7.0361e+02 - logprior: -2.3885e-01
Epoch 3/10
39/39 - 23s - loss: 702.3813 - loglik: -7.0222e+02 - logprior: -1.6443e-01
Epoch 4/10
39/39 - 23s - loss: 701.7895 - loglik: -7.0170e+02 - logprior: -9.2292e-02
Epoch 5/10
39/39 - 23s - loss: 701.5295 - loglik: -7.0152e+02 - logprior: -5.7139e-03
Epoch 6/10
39/39 - 23s - loss: 701.0244 - loglik: -7.0108e+02 - logprior: 0.0530
Epoch 7/10
39/39 - 23s - loss: 700.9159 - loglik: -7.0105e+02 - logprior: 0.1340
Epoch 8/10
39/39 - 23s - loss: 700.9612 - loglik: -7.0118e+02 - logprior: 0.2142
Fitted a model with MAP estimate = -700.2923
Time for alignment: 571.2205
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 802.6675 - loglik: -8.0131e+02 - logprior: -1.3588e+00
Epoch 2/10
39/39 - 18s - loss: 730.8110 - loglik: -7.3032e+02 - logprior: -4.8862e-01
Epoch 3/10
39/39 - 18s - loss: 719.8428 - loglik: -7.1928e+02 - logprior: -5.6338e-01
Epoch 4/10
39/39 - 18s - loss: 717.3029 - loglik: -7.1676e+02 - logprior: -5.4013e-01
Epoch 5/10
39/39 - 18s - loss: 716.5668 - loglik: -7.1603e+02 - logprior: -5.3597e-01
Epoch 6/10
39/39 - 18s - loss: 715.8751 - loglik: -7.1535e+02 - logprior: -5.2876e-01
Epoch 7/10
39/39 - 18s - loss: 715.6927 - loglik: -7.1517e+02 - logprior: -5.2264e-01
Epoch 8/10
39/39 - 18s - loss: 714.8813 - loglik: -7.1434e+02 - logprior: -5.4063e-01
Epoch 9/10
39/39 - 18s - loss: 714.2065 - loglik: -7.1366e+02 - logprior: -5.5056e-01
Epoch 10/10
39/39 - 18s - loss: 714.4308 - loglik: -7.1389e+02 - logprior: -5.4028e-01
Fitted a model with MAP estimate = -713.0976
expansions: [(0, 5), (40, 1), (43, 3), (44, 5), (46, 1), (71, 1), (82, 2), (83, 2), (84, 3), (86, 2), (104, 1), (120, 2), (121, 3), (127, 1), (152, 1), (154, 3), (157, 2), (206, 5), (207, 1), (214, 1)]
discards: []
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 274 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 713.8966 - loglik: -7.1172e+02 - logprior: -2.1791e+00
Epoch 2/2
39/39 - 23s - loss: 705.7297 - loglik: -7.0525e+02 - logprior: -4.8425e-01
Fitted a model with MAP estimate = -703.6784
expansions: [(0, 4), (12, 3), (101, 1), (109, 2), (143, 1)]
discards: [  1   2   3   4   5   6   7   8  54  55 106 146 150 190 191 192 193 194
 245 246 247]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 264 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 710.8353 - loglik: -7.0860e+02 - logprior: -2.2381e+00
Epoch 2/2
39/39 - 22s - loss: 705.3015 - loglik: -7.0507e+02 - logprior: -2.3123e-01
Fitted a model with MAP estimate = -703.3780
expansions: [(0, 5), (48, 1), (142, 1), (184, 2)]
discards: [ 93 106 187]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 270 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 708.8478 - loglik: -7.0664e+02 - logprior: -2.2120e+00
Epoch 2/10
39/39 - 23s - loss: 704.0841 - loglik: -7.0384e+02 - logprior: -2.4870e-01
Epoch 3/10
39/39 - 23s - loss: 702.7985 - loglik: -7.0283e+02 - logprior: 0.0331
Epoch 4/10
39/39 - 23s - loss: 702.1068 - loglik: -7.0223e+02 - logprior: 0.1209
Epoch 5/10
39/39 - 23s - loss: 702.0993 - loglik: -7.0230e+02 - logprior: 0.2036
Epoch 6/10
39/39 - 23s - loss: 701.7034 - loglik: -7.0198e+02 - logprior: 0.2757
Epoch 7/10
39/39 - 23s - loss: 701.3273 - loglik: -7.0168e+02 - logprior: 0.3491
Epoch 8/10
39/39 - 23s - loss: 701.0917 - loglik: -7.0151e+02 - logprior: 0.4190
Epoch 9/10
39/39 - 23s - loss: 700.9883 - loglik: -7.0147e+02 - logprior: 0.4821
Epoch 10/10
39/39 - 23s - loss: 701.4360 - loglik: -7.0199e+02 - logprior: 0.5569
Fitted a model with MAP estimate = -700.5487
Time for alignment: 631.4847
Computed alignments with likelihoods: ['-700.4080', '-701.6479', '-700.9855', '-700.2923', '-700.5487']
Best model has likelihood: -700.2923  (prior= 0.2307 )
time for generating output: 0.3413
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.5280446927374302
Training of 5 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f440c5f0580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4da6f3f460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4bddf7ad30>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4da680b0d0>
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 583.3591 - loglik: -5.8155e+02 - logprior: -1.8069e+00
Epoch 2/10
39/39 - 9s - loss: 486.5338 - loglik: -4.8488e+02 - logprior: -1.6581e+00
Epoch 3/10
39/39 - 9s - loss: 478.2646 - loglik: -4.7661e+02 - logprior: -1.6569e+00
Epoch 4/10
39/39 - 9s - loss: 476.3758 - loglik: -4.7476e+02 - logprior: -1.6187e+00
Epoch 5/10
39/39 - 9s - loss: 475.6485 - loglik: -4.7405e+02 - logprior: -1.5970e+00
Epoch 6/10
39/39 - 9s - loss: 475.4031 - loglik: -4.7381e+02 - logprior: -1.5939e+00
Epoch 7/10
39/39 - 9s - loss: 474.6947 - loglik: -4.7312e+02 - logprior: -1.5724e+00
Epoch 8/10
39/39 - 9s - loss: 474.7481 - loglik: -4.7318e+02 - logprior: -1.5721e+00
Fitted a model with MAP estimate = -471.6329
expansions: [(12, 1), (13, 1), (14, 1), (17, 1), (26, 1), (33, 3), (35, 1), (36, 2), (37, 2), (46, 1), (47, 1), (48, 1), (50, 1), (66, 2), (67, 2), (69, 1), (84, 1), (85, 1), (88, 1), (89, 1), (90, 1), (91, 1), (93, 1), (108, 1), (112, 2), (122, 1), (129, 1), (130, 1), (131, 1), (149, 5), (150, 2), (153, 1), (154, 1), (155, 1), (156, 1), (169, 1), (177, 1), (178, 1), (179, 1), (182, 1), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 241 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 467.8912 - loglik: -4.6530e+02 - logprior: -2.5907e+00
Epoch 2/2
39/39 - 13s - loss: 457.1476 - loglik: -4.5597e+02 - logprior: -1.1729e+00
Fitted a model with MAP estimate = -452.5502
expansions: [(3, 1), (40, 1)]
discards: [  0  47  83  85 142 189 190]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 460.9766 - loglik: -4.5855e+02 - logprior: -2.4289e+00
Epoch 2/2
39/39 - 13s - loss: 456.2995 - loglik: -4.5554e+02 - logprior: -7.6028e-01
Fitted a model with MAP estimate = -452.3928
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 457.9041 - loglik: -4.5551e+02 - logprior: -2.3948e+00
Epoch 2/10
39/39 - 13s - loss: 454.0757 - loglik: -4.5294e+02 - logprior: -1.1339e+00
Epoch 3/10
39/39 - 13s - loss: 452.3083 - loglik: -4.5200e+02 - logprior: -3.0650e-01
Epoch 4/10
39/39 - 13s - loss: 451.5471 - loglik: -4.5129e+02 - logprior: -2.5675e-01
Epoch 5/10
39/39 - 13s - loss: 450.7924 - loglik: -4.5061e+02 - logprior: -1.7792e-01
Epoch 6/10
39/39 - 13s - loss: 451.1284 - loglik: -4.5100e+02 - logprior: -1.2478e-01
Fitted a model with MAP estimate = -450.4363
Time for alignment: 288.2413
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 584.3353 - loglik: -5.8253e+02 - logprior: -1.8079e+00
Epoch 2/10
39/39 - 9s - loss: 486.1648 - loglik: -4.8449e+02 - logprior: -1.6776e+00
Epoch 3/10
39/39 - 10s - loss: 478.5266 - loglik: -4.7687e+02 - logprior: -1.6573e+00
Epoch 4/10
39/39 - 9s - loss: 476.5088 - loglik: -4.7489e+02 - logprior: -1.6230e+00
Epoch 5/10
39/39 - 10s - loss: 476.0105 - loglik: -4.7441e+02 - logprior: -1.6003e+00
Epoch 6/10
39/39 - 10s - loss: 475.3458 - loglik: -4.7376e+02 - logprior: -1.5898e+00
Epoch 7/10
39/39 - 10s - loss: 475.0999 - loglik: -4.7352e+02 - logprior: -1.5786e+00
Epoch 8/10
39/39 - 10s - loss: 474.8930 - loglik: -4.7332e+02 - logprior: -1.5734e+00
Epoch 9/10
39/39 - 10s - loss: 474.6785 - loglik: -4.7312e+02 - logprior: -1.5581e+00
Epoch 10/10
39/39 - 9s - loss: 474.5818 - loglik: -4.7302e+02 - logprior: -1.5615e+00
Fitted a model with MAP estimate = -471.5805
expansions: [(12, 1), (13, 1), (14, 1), (17, 1), (29, 1), (33, 3), (35, 1), (36, 2), (37, 2), (46, 1), (47, 1), (48, 1), (50, 1), (66, 1), (67, 2), (69, 1), (84, 1), (85, 1), (88, 1), (89, 1), (90, 1), (91, 1), (93, 1), (108, 1), (112, 2), (122, 1), (129, 1), (130, 1), (131, 1), (149, 5), (150, 2), (153, 1), (154, 1), (155, 1), (156, 1), (169, 1), (177, 1), (178, 1), (179, 1), (182, 1), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 468.0381 - loglik: -4.6544e+02 - logprior: -2.6011e+00
Epoch 2/2
39/39 - 13s - loss: 457.0323 - loglik: -4.5586e+02 - logprior: -1.1688e+00
Fitted a model with MAP estimate = -452.4858
expansions: [(3, 1), (40, 1)]
discards: [  0  47  84 141 188 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 460.8058 - loglik: -4.5838e+02 - logprior: -2.4217e+00
Epoch 2/2
39/39 - 13s - loss: 456.3885 - loglik: -4.5561e+02 - logprior: -7.7520e-01
Fitted a model with MAP estimate = -452.2563
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 457.8011 - loglik: -4.5543e+02 - logprior: -2.3727e+00
Epoch 2/10
39/39 - 13s - loss: 453.8752 - loglik: -4.5274e+02 - logprior: -1.1386e+00
Epoch 3/10
39/39 - 13s - loss: 452.4297 - loglik: -4.5215e+02 - logprior: -2.7605e-01
Epoch 4/10
39/39 - 13s - loss: 451.3081 - loglik: -4.5108e+02 - logprior: -2.2513e-01
Epoch 5/10
39/39 - 13s - loss: 450.9971 - loglik: -4.5084e+02 - logprior: -1.5821e-01
Epoch 6/10
39/39 - 13s - loss: 451.0841 - loglik: -4.5099e+02 - logprior: -9.7703e-02
Fitted a model with MAP estimate = -450.3146
Time for alignment: 306.8678
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 584.6976 - loglik: -5.8288e+02 - logprior: -1.8143e+00
Epoch 2/10
39/39 - 9s - loss: 487.0835 - loglik: -4.8540e+02 - logprior: -1.6871e+00
Epoch 3/10
39/39 - 9s - loss: 477.5779 - loglik: -4.7588e+02 - logprior: -1.6998e+00
Epoch 4/10
39/39 - 10s - loss: 475.8074 - loglik: -4.7417e+02 - logprior: -1.6340e+00
Epoch 5/10
39/39 - 10s - loss: 474.9160 - loglik: -4.7332e+02 - logprior: -1.6008e+00
Epoch 6/10
39/39 - 10s - loss: 474.3817 - loglik: -4.7280e+02 - logprior: -1.5850e+00
Epoch 7/10
39/39 - 10s - loss: 474.1126 - loglik: -4.7253e+02 - logprior: -1.5795e+00
Epoch 8/10
39/39 - 9s - loss: 473.7697 - loglik: -4.7220e+02 - logprior: -1.5662e+00
Epoch 9/10
39/39 - 9s - loss: 473.7936 - loglik: -4.7223e+02 - logprior: -1.5600e+00
Fitted a model with MAP estimate = -470.6501
expansions: [(8, 1), (12, 1), (14, 1), (23, 2), (29, 1), (33, 3), (35, 1), (36, 2), (37, 2), (46, 1), (47, 1), (48, 1), (50, 1), (66, 1), (67, 2), (69, 1), (70, 1), (83, 1), (84, 1), (89, 1), (90, 1), (91, 1), (93, 1), (110, 2), (112, 2), (122, 1), (129, 1), (130, 1), (131, 1), (149, 5), (150, 2), (153, 1), (154, 1), (155, 1), (156, 1), (169, 1), (177, 1), (178, 1), (179, 1), (182, 1), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 467.9013 - loglik: -4.6525e+02 - logprior: -2.6499e+00
Epoch 2/2
39/39 - 13s - loss: 456.9889 - loglik: -4.5577e+02 - logprior: -1.2145e+00
Fitted a model with MAP estimate = -452.3720
expansions: [(3, 1), (41, 1)]
discards: [  0  26  48  85 139 142 190 191]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 460.8555 - loglik: -4.5843e+02 - logprior: -2.4271e+00
Epoch 2/2
39/39 - 13s - loss: 456.3745 - loglik: -4.5560e+02 - logprior: -7.7024e-01
Fitted a model with MAP estimate = -452.3816
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 457.9184 - loglik: -4.5554e+02 - logprior: -2.3808e+00
Epoch 2/10
39/39 - 13s - loss: 453.9493 - loglik: -4.5280e+02 - logprior: -1.1530e+00
Epoch 3/10
39/39 - 13s - loss: 452.0137 - loglik: -4.5171e+02 - logprior: -3.0815e-01
Epoch 4/10
39/39 - 13s - loss: 451.6127 - loglik: -4.5137e+02 - logprior: -2.4183e-01
Epoch 5/10
39/39 - 13s - loss: 451.3053 - loglik: -4.5113e+02 - logprior: -1.7417e-01
Epoch 6/10
39/39 - 13s - loss: 450.6971 - loglik: -4.5058e+02 - logprior: -1.1345e-01
Epoch 7/10
39/39 - 13s - loss: 450.9070 - loglik: -4.5085e+02 - logprior: -5.5344e-02
Fitted a model with MAP estimate = -450.3505
Time for alignment: 312.1382
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 583.8651 - loglik: -5.8206e+02 - logprior: -1.8085e+00
Epoch 2/10
39/39 - 9s - loss: 485.6454 - loglik: -4.8395e+02 - logprior: -1.6917e+00
Epoch 3/10
39/39 - 9s - loss: 478.2715 - loglik: -4.7663e+02 - logprior: -1.6389e+00
Epoch 4/10
39/39 - 10s - loss: 476.5589 - loglik: -4.7497e+02 - logprior: -1.5908e+00
Epoch 5/10
39/39 - 9s - loss: 475.7319 - loglik: -4.7416e+02 - logprior: -1.5704e+00
Epoch 6/10
39/39 - 9s - loss: 475.2649 - loglik: -4.7372e+02 - logprior: -1.5481e+00
Epoch 7/10
39/39 - 10s - loss: 475.2552 - loglik: -4.7371e+02 - logprior: -1.5453e+00
Epoch 8/10
39/39 - 9s - loss: 474.7046 - loglik: -4.7317e+02 - logprior: -1.5338e+00
Epoch 9/10
39/39 - 9s - loss: 474.5422 - loglik: -4.7302e+02 - logprior: -1.5265e+00
Epoch 10/10
39/39 - 9s - loss: 474.7534 - loglik: -4.7323e+02 - logprior: -1.5241e+00
Fitted a model with MAP estimate = -471.4817
expansions: [(12, 2), (13, 1), (14, 1), (26, 1), (33, 3), (35, 1), (36, 2), (37, 2), (46, 1), (47, 1), (48, 1), (50, 1), (66, 1), (67, 2), (69, 1), (77, 1), (83, 1), (84, 1), (89, 1), (90, 1), (91, 1), (93, 1), (108, 1), (112, 2), (122, 1), (129, 1), (130, 1), (131, 1), (149, 5), (150, 2), (153, 1), (154, 1), (155, 1), (156, 1), (163, 1), (169, 1), (176, 1), (179, 1), (182, 1), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 467.8347 - loglik: -4.6526e+02 - logprior: -2.5761e+00
Epoch 2/2
39/39 - 13s - loss: 457.0557 - loglik: -4.5587e+02 - logprior: -1.1870e+00
Fitted a model with MAP estimate = -452.4715
expansions: [(3, 1), (40, 1)]
discards: [  0  47  84 140 188 189]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 460.7859 - loglik: -4.5837e+02 - logprior: -2.4184e+00
Epoch 2/2
39/39 - 13s - loss: 456.2498 - loglik: -4.5548e+02 - logprior: -7.6543e-01
Fitted a model with MAP estimate = -452.2536
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 457.9470 - loglik: -4.5558e+02 - logprior: -2.3671e+00
Epoch 2/10
39/39 - 13s - loss: 453.9530 - loglik: -4.5282e+02 - logprior: -1.1326e+00
Epoch 3/10
39/39 - 13s - loss: 451.9730 - loglik: -4.5169e+02 - logprior: -2.8332e-01
Epoch 4/10
39/39 - 13s - loss: 451.3387 - loglik: -4.5112e+02 - logprior: -2.1669e-01
Epoch 5/10
39/39 - 13s - loss: 451.1510 - loglik: -4.5100e+02 - logprior: -1.5597e-01
Epoch 6/10
39/39 - 13s - loss: 450.6055 - loglik: -4.5051e+02 - logprior: -9.2803e-02
Epoch 7/10
39/39 - 13s - loss: 450.4465 - loglik: -4.5041e+02 - logprior: -3.4514e-02
Epoch 8/10
39/39 - 13s - loss: 450.1795 - loglik: -4.5020e+02 - logprior: 0.0221
Epoch 9/10
39/39 - 13s - loss: 450.7594 - loglik: -4.5084e+02 - logprior: 0.0808
Fitted a model with MAP estimate = -449.9865
Time for alignment: 345.4781
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 585.1071 - loglik: -5.8330e+02 - logprior: -1.8051e+00
Epoch 2/10
39/39 - 9s - loss: 487.0286 - loglik: -4.8533e+02 - logprior: -1.6982e+00
Epoch 3/10
39/39 - 9s - loss: 478.3607 - loglik: -4.7672e+02 - logprior: -1.6400e+00
Epoch 4/10
39/39 - 9s - loss: 476.4946 - loglik: -4.7493e+02 - logprior: -1.5606e+00
Epoch 5/10
39/39 - 9s - loss: 475.8795 - loglik: -4.7435e+02 - logprior: -1.5285e+00
Epoch 6/10
39/39 - 9s - loss: 475.4091 - loglik: -4.7389e+02 - logprior: -1.5151e+00
Epoch 7/10
39/39 - 9s - loss: 474.8844 - loglik: -4.7338e+02 - logprior: -1.5041e+00
Epoch 8/10
39/39 - 9s - loss: 474.8637 - loglik: -4.7337e+02 - logprior: -1.4963e+00
Epoch 9/10
39/39 - 9s - loss: 474.5573 - loglik: -4.7306e+02 - logprior: -1.4944e+00
Epoch 10/10
39/39 - 10s - loss: 474.4674 - loglik: -4.7298e+02 - logprior: -1.4885e+00
Fitted a model with MAP estimate = -471.4580
expansions: [(12, 1), (13, 1), (14, 1), (23, 2), (26, 1), (33, 3), (35, 1), (36, 2), (37, 2), (46, 1), (47, 1), (48, 1), (50, 1), (66, 2), (67, 2), (69, 1), (77, 1), (85, 1), (87, 1), (89, 1), (90, 1), (93, 1), (96, 1), (110, 2), (112, 2), (122, 1), (129, 1), (130, 1), (132, 1), (149, 5), (150, 2), (153, 1), (154, 1), (155, 1), (156, 1), (169, 1), (177, 1), (178, 1), (179, 1), (182, 1), (183, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 468.7168 - loglik: -4.6613e+02 - logprior: -2.5874e+00
Epoch 2/2
39/39 - 13s - loss: 457.0675 - loglik: -4.5585e+02 - logprior: -1.2196e+00
Fitted a model with MAP estimate = -452.4536
expansions: [(3, 1), (41, 1)]
discards: [  0  26  48  84  86 140 143 191 192]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 460.9446 - loglik: -4.5853e+02 - logprior: -2.4152e+00
Epoch 2/2
39/39 - 13s - loss: 456.4030 - loglik: -4.5562e+02 - logprior: -7.8327e-01
Fitted a model with MAP estimate = -452.2640
expansions: [(3, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 457.9002 - loglik: -4.5553e+02 - logprior: -2.3677e+00
Epoch 2/10
39/39 - 13s - loss: 453.9956 - loglik: -4.5287e+02 - logprior: -1.1298e+00
Epoch 3/10
39/39 - 13s - loss: 452.1917 - loglik: -4.5191e+02 - logprior: -2.7819e-01
Epoch 4/10
39/39 - 13s - loss: 451.1449 - loglik: -4.5092e+02 - logprior: -2.2223e-01
Epoch 5/10
39/39 - 13s - loss: 451.2632 - loglik: -4.5111e+02 - logprior: -1.4973e-01
Fitted a model with MAP estimate = -450.4999
Time for alignment: 294.8807
Computed alignments with likelihoods: ['-450.4363', '-450.3146', '-450.3505', '-449.9865', '-450.4999']
Best model has likelihood: -449.9865  (prior= 0.1068 )
time for generating output: 0.3524
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.6008563444560944
Training of 5 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f4b783f2c10>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f4b783f2880>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2e20>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2b50>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2d00>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f21c0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f4b783f2310>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25e0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2640>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2100>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2340>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f20a0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f26a0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f28e0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2bb0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2850>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2520>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f25b0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f4b783f2760> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f4b783f2b20>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4b783f2b80> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f4b78412e50> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f49f0684e20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d6b93c160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f4d95a63400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f4c99ca5550>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f4b82e94af0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f4b783f2160> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : False , transposed : True
 , encoder_weight_extractor : <function get_encoder_weights.<locals>._extract_encoder_weights_callback at 0x7f4d7cb09f70>
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 324.1642 - loglik: -3.2110e+02 - logprior: -3.0632e+00
Epoch 2/10
19/19 - 2s - loss: 279.8047 - loglik: -2.7843e+02 - logprior: -1.3782e+00
Epoch 3/10
19/19 - 2s - loss: 253.6424 - loglik: -2.5183e+02 - logprior: -1.8092e+00
Epoch 4/10
19/19 - 2s - loss: 248.4048 - loglik: -2.4667e+02 - logprior: -1.7359e+00
Epoch 5/10
19/19 - 2s - loss: 246.2395 - loglik: -2.4449e+02 - logprior: -1.7497e+00
Epoch 6/10
19/19 - 2s - loss: 245.3896 - loglik: -2.4370e+02 - logprior: -1.6914e+00
Epoch 7/10
19/19 - 2s - loss: 244.7006 - loglik: -2.4306e+02 - logprior: -1.6423e+00
Epoch 8/10
19/19 - 2s - loss: 244.7436 - loglik: -2.4312e+02 - logprior: -1.6212e+00
Fitted a model with MAP estimate = -244.1199
expansions: [(9, 1), (16, 1), (18, 1), (19, 4), (21, 2), (22, 2), (32, 2), (34, 1), (41, 1), (44, 1), (48, 1), (50, 1), (57, 1), (65, 1), (70, 1), (71, 2), (72, 1), (75, 1), (77, 1), (79, 1), (82, 1), (85, 1), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 252.2377 - loglik: -2.4827e+02 - logprior: -3.9701e+00
Epoch 2/2
19/19 - 3s - loss: 240.4257 - loglik: -2.3846e+02 - logprior: -1.9625e+00
Fitted a model with MAP estimate = -238.3065
expansions: [(0, 2), (20, 1)]
discards: [ 0 15 16 28 30 43 89]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 240.4194 - loglik: -2.3751e+02 - logprior: -2.9128e+00
Epoch 2/2
19/19 - 3s - loss: 236.8967 - loglik: -2.3583e+02 - logprior: -1.0632e+00
Fitted a model with MAP estimate = -235.7346
expansions: [(16, 2)]
discards: [ 0 21 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 242.0396 - loglik: -2.3831e+02 - logprior: -3.7317e+00
Epoch 2/10
19/19 - 3s - loss: 237.4263 - loglik: -2.3623e+02 - logprior: -1.1955e+00
Epoch 3/10
19/19 - 3s - loss: 235.4770 - loglik: -2.3450e+02 - logprior: -9.7911e-01
Epoch 4/10
19/19 - 3s - loss: 234.4128 - loglik: -2.3346e+02 - logprior: -9.5237e-01
Epoch 5/10
19/19 - 3s - loss: 233.7156 - loglik: -2.3280e+02 - logprior: -9.1969e-01
Epoch 6/10
19/19 - 3s - loss: 233.4017 - loglik: -2.3251e+02 - logprior: -8.9230e-01
Epoch 7/10
19/19 - 3s - loss: 233.0121 - loglik: -2.3214e+02 - logprior: -8.7286e-01
Epoch 8/10
19/19 - 3s - loss: 232.7221 - loglik: -2.3188e+02 - logprior: -8.4412e-01
Epoch 9/10
19/19 - 3s - loss: 232.8796 - loglik: -2.3205e+02 - logprior: -8.2592e-01
Fitted a model with MAP estimate = -232.3887
Time for alignment: 86.7210
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 324.1548 - loglik: -3.2109e+02 - logprior: -3.0645e+00
Epoch 2/10
19/19 - 2s - loss: 280.0440 - loglik: -2.7867e+02 - logprior: -1.3729e+00
Epoch 3/10
19/19 - 2s - loss: 254.2802 - loglik: -2.5248e+02 - logprior: -1.8001e+00
Epoch 4/10
19/19 - 2s - loss: 247.8861 - loglik: -2.4614e+02 - logprior: -1.7491e+00
Epoch 5/10
19/19 - 2s - loss: 246.0350 - loglik: -2.4429e+02 - logprior: -1.7427e+00
Epoch 6/10
19/19 - 2s - loss: 245.6286 - loglik: -2.4393e+02 - logprior: -1.6984e+00
Epoch 7/10
19/19 - 2s - loss: 245.0450 - loglik: -2.4339e+02 - logprior: -1.6517e+00
Epoch 8/10
19/19 - 2s - loss: 244.6783 - loglik: -2.4304e+02 - logprior: -1.6334e+00
Epoch 9/10
19/19 - 2s - loss: 244.3387 - loglik: -2.4272e+02 - logprior: -1.6171e+00
Epoch 10/10
19/19 - 2s - loss: 243.7718 - loglik: -2.4216e+02 - logprior: -1.6136e+00
Fitted a model with MAP estimate = -243.8050
expansions: [(9, 1), (16, 1), (18, 1), (19, 4), (21, 1), (29, 2), (33, 1), (34, 1), (41, 1), (44, 1), (48, 1), (50, 1), (57, 1), (65, 1), (70, 1), (71, 2), (72, 1), (75, 1), (77, 1), (79, 1), (82, 1), (85, 1), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 252.0327 - loglik: -2.4808e+02 - logprior: -3.9536e+00
Epoch 2/2
19/19 - 3s - loss: 240.6778 - loglik: -2.3873e+02 - logprior: -1.9479e+00
Fitted a model with MAP estimate = -238.6494
expansions: [(0, 2), (20, 1)]
discards: [ 0 15 16 36 87]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 241.1527 - loglik: -2.3823e+02 - logprior: -2.9245e+00
Epoch 2/2
19/19 - 3s - loss: 237.2557 - loglik: -2.3617e+02 - logprior: -1.0897e+00
Fitted a model with MAP estimate = -236.0634
expansions: [(16, 2)]
discards: [ 0 21 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 242.9464 - loglik: -2.3920e+02 - logprior: -3.7495e+00
Epoch 2/10
19/19 - 3s - loss: 237.9568 - loglik: -2.3676e+02 - logprior: -1.1992e+00
Epoch 3/10
19/19 - 3s - loss: 235.5453 - loglik: -2.3454e+02 - logprior: -1.0006e+00
Epoch 4/10
19/19 - 3s - loss: 234.6317 - loglik: -2.3367e+02 - logprior: -9.6048e-01
Epoch 5/10
19/19 - 3s - loss: 234.2502 - loglik: -2.3332e+02 - logprior: -9.2830e-01
Epoch 6/10
19/19 - 3s - loss: 233.4718 - loglik: -2.3258e+02 - logprior: -8.9531e-01
Epoch 7/10
19/19 - 3s - loss: 233.3974 - loglik: -2.3252e+02 - logprior: -8.7587e-01
Epoch 8/10
19/19 - 3s - loss: 233.0509 - loglik: -2.3220e+02 - logprior: -8.4805e-01
Epoch 9/10
19/19 - 3s - loss: 232.8471 - loglik: -2.3202e+02 - logprior: -8.3160e-01
Epoch 10/10
19/19 - 3s - loss: 232.6032 - loglik: -2.3180e+02 - logprior: -8.0235e-01
Fitted a model with MAP estimate = -232.4472
Time for alignment: 93.7317
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 324.1539 - loglik: -3.2109e+02 - logprior: -3.0666e+00
Epoch 2/10
19/19 - 2s - loss: 280.1511 - loglik: -2.7877e+02 - logprior: -1.3776e+00
Epoch 3/10
19/19 - 2s - loss: 255.7220 - loglik: -2.5394e+02 - logprior: -1.7834e+00
Epoch 4/10
19/19 - 2s - loss: 248.5449 - loglik: -2.4682e+02 - logprior: -1.7254e+00
Epoch 5/10
19/19 - 2s - loss: 246.7805 - loglik: -2.4504e+02 - logprior: -1.7424e+00
Epoch 6/10
19/19 - 2s - loss: 245.6308 - loglik: -2.4393e+02 - logprior: -1.7025e+00
Epoch 7/10
19/19 - 2s - loss: 245.3915 - loglik: -2.4374e+02 - logprior: -1.6559e+00
Epoch 8/10
19/19 - 2s - loss: 245.0063 - loglik: -2.4337e+02 - logprior: -1.6400e+00
Epoch 9/10
19/19 - 2s - loss: 244.5417 - loglik: -2.4292e+02 - logprior: -1.6208e+00
Epoch 10/10
19/19 - 2s - loss: 244.7092 - loglik: -2.4309e+02 - logprior: -1.6180e+00
Fitted a model with MAP estimate = -244.1672
expansions: [(9, 1), (16, 1), (18, 1), (19, 4), (21, 1), (29, 1), (32, 2), (34, 1), (40, 1), (43, 1), (44, 1), (50, 1), (57, 1), (65, 1), (70, 1), (71, 2), (72, 1), (75, 1), (77, 1), (79, 1), (82, 1), (85, 1), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 252.1296 - loglik: -2.4817e+02 - logprior: -3.9571e+00
Epoch 2/2
19/19 - 3s - loss: 240.7593 - loglik: -2.3881e+02 - logprior: -1.9477e+00
Fitted a model with MAP estimate = -238.5112
expansions: [(0, 2), (20, 1)]
discards: [ 0 15 16 41 87]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 240.6943 - loglik: -2.3778e+02 - logprior: -2.9165e+00
Epoch 2/2
19/19 - 3s - loss: 236.7013 - loglik: -2.3563e+02 - logprior: -1.0698e+00
Fitted a model with MAP estimate = -235.7368
expansions: [(16, 2)]
discards: [ 0 21 22]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 242.1916 - loglik: -2.3846e+02 - logprior: -3.7296e+00
Epoch 2/10
19/19 - 3s - loss: 237.2328 - loglik: -2.3605e+02 - logprior: -1.1834e+00
Epoch 3/10
19/19 - 3s - loss: 235.4104 - loglik: -2.3442e+02 - logprior: -9.9087e-01
Epoch 4/10
19/19 - 3s - loss: 234.8562 - loglik: -2.3391e+02 - logprior: -9.4478e-01
Epoch 5/10
19/19 - 3s - loss: 233.5296 - loglik: -2.3261e+02 - logprior: -9.2029e-01
Epoch 6/10
19/19 - 3s - loss: 233.1769 - loglik: -2.3228e+02 - logprior: -8.9838e-01
Epoch 7/10
19/19 - 3s - loss: 232.8928 - loglik: -2.3203e+02 - logprior: -8.6684e-01
Epoch 8/10
19/19 - 3s - loss: 232.9494 - loglik: -2.3210e+02 - logprior: -8.4614e-01
Fitted a model with MAP estimate = -232.5340
Time for alignment: 86.1001
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 324.0953 - loglik: -3.2103e+02 - logprior: -3.0667e+00
Epoch 2/10
19/19 - 2s - loss: 280.0377 - loglik: -2.7866e+02 - logprior: -1.3788e+00
Epoch 3/10
19/19 - 2s - loss: 254.1152 - loglik: -2.5230e+02 - logprior: -1.8165e+00
Epoch 4/10
19/19 - 2s - loss: 248.3317 - loglik: -2.4659e+02 - logprior: -1.7406e+00
Epoch 5/10
19/19 - 2s - loss: 246.4162 - loglik: -2.4468e+02 - logprior: -1.7369e+00
Epoch 6/10
19/19 - 2s - loss: 245.7895 - loglik: -2.4410e+02 - logprior: -1.6933e+00
Epoch 7/10
19/19 - 2s - loss: 245.4569 - loglik: -2.4381e+02 - logprior: -1.6480e+00
Epoch 8/10
19/19 - 2s - loss: 245.2121 - loglik: -2.4358e+02 - logprior: -1.6315e+00
Epoch 9/10
19/19 - 2s - loss: 244.7904 - loglik: -2.4317e+02 - logprior: -1.6206e+00
Epoch 10/10
19/19 - 2s - loss: 244.6602 - loglik: -2.4305e+02 - logprior: -1.6105e+00
Fitted a model with MAP estimate = -244.1697
expansions: [(9, 1), (16, 1), (19, 4), (21, 2), (29, 1), (32, 2), (34, 1), (37, 1), (44, 1), (48, 1), (50, 1), (57, 1), (65, 1), (70, 1), (71, 2), (72, 1), (75, 1), (77, 1), (79, 1), (82, 1), (85, 1), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 252.5333 - loglik: -2.4858e+02 - logprior: -3.9522e+00
Epoch 2/2
19/19 - 3s - loss: 240.5411 - loglik: -2.3859e+02 - logprior: -1.9536e+00
Fitted a model with MAP estimate = -238.5234
expansions: [(0, 2)]
discards: [ 0 26 41 87]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 239.5985 - loglik: -2.3667e+02 - logprior: -2.9251e+00
Epoch 2/2
19/19 - 3s - loss: 236.2239 - loglik: -2.3514e+02 - logprior: -1.0810e+00
Fitted a model with MAP estimate = -235.0765
expansions: []
discards: [ 0 16 17]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 242.4320 - loglik: -2.3876e+02 - logprior: -3.6699e+00
Epoch 2/10
19/19 - 3s - loss: 238.0109 - loglik: -2.3689e+02 - logprior: -1.1180e+00
Epoch 3/10
19/19 - 3s - loss: 235.9568 - loglik: -2.3500e+02 - logprior: -9.5417e-01
Epoch 4/10
19/19 - 3s - loss: 235.2070 - loglik: -2.3429e+02 - logprior: -9.1583e-01
Epoch 5/10
19/19 - 3s - loss: 234.2642 - loglik: -2.3336e+02 - logprior: -9.0704e-01
Epoch 6/10
19/19 - 3s - loss: 233.4140 - loglik: -2.3252e+02 - logprior: -8.9267e-01
Epoch 7/10
19/19 - 3s - loss: 233.5592 - loglik: -2.3269e+02 - logprior: -8.6771e-01
Fitted a model with MAP estimate = -233.0873
Time for alignment: 82.6173
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 324.0802 - loglik: -3.2102e+02 - logprior: -3.0630e+00
Epoch 2/10
19/19 - 2s - loss: 280.7702 - loglik: -2.7941e+02 - logprior: -1.3622e+00
Epoch 3/10
19/19 - 2s - loss: 256.0416 - loglik: -2.5425e+02 - logprior: -1.7955e+00
Epoch 4/10
19/19 - 2s - loss: 248.7520 - loglik: -2.4702e+02 - logprior: -1.7308e+00
Epoch 5/10
19/19 - 2s - loss: 246.4631 - loglik: -2.4473e+02 - logprior: -1.7287e+00
Epoch 6/10
19/19 - 2s - loss: 245.4775 - loglik: -2.4380e+02 - logprior: -1.6799e+00
Epoch 7/10
19/19 - 2s - loss: 245.0413 - loglik: -2.4340e+02 - logprior: -1.6435e+00
Epoch 8/10
19/19 - 2s - loss: 244.7384 - loglik: -2.4312e+02 - logprior: -1.6189e+00
Epoch 9/10
19/19 - 2s - loss: 244.4905 - loglik: -2.4288e+02 - logprior: -1.6137e+00
Epoch 10/10
19/19 - 2s - loss: 244.0789 - loglik: -2.4248e+02 - logprior: -1.6025e+00
Fitted a model with MAP estimate = -243.7761
expansions: [(9, 1), (16, 1), (18, 2), (19, 4), (21, 1), (29, 2), (33, 1), (34, 1), (41, 1), (43, 1), (44, 1), (50, 1), (56, 1), (68, 2), (70, 1), (71, 2), (72, 1), (75, 1), (77, 1), (79, 1), (82, 1), (85, 1), (89, 1)]
discards: [0]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 251.9763 - loglik: -2.4801e+02 - logprior: -3.9661e+00
Epoch 2/2
19/19 - 3s - loss: 240.0287 - loglik: -2.3805e+02 - logprior: -1.9737e+00
Fitted a model with MAP estimate = -238.0352
expansions: [(0, 2)]
discards: [ 0 15 16 22 23 37 90]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 240.6368 - loglik: -2.3772e+02 - logprior: -2.9193e+00
Epoch 2/2
19/19 - 3s - loss: 236.7154 - loglik: -2.3564e+02 - logprior: -1.0765e+00
Fitted a model with MAP estimate = -235.8498
expansions: [(16, 2)]
discards: [ 0 81]
Used the encoder_weight_extractor callback to pass the encoder parameters to the next iteration.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 242.0712 - loglik: -2.3833e+02 - logprior: -3.7460e+00
Epoch 2/10
19/19 - 3s - loss: 237.3825 - loglik: -2.3618e+02 - logprior: -1.2031e+00
Epoch 3/10
19/19 - 3s - loss: 235.4548 - loglik: -2.3447e+02 - logprior: -9.8172e-01
Epoch 4/10
19/19 - 3s - loss: 234.3725 - loglik: -2.3343e+02 - logprior: -9.4589e-01
Epoch 5/10
19/19 - 3s - loss: 233.9537 - loglik: -2.3303e+02 - logprior: -9.2076e-01
Epoch 6/10
19/19 - 3s - loss: 233.2388 - loglik: -2.3235e+02 - logprior: -8.9122e-01
Epoch 7/10
19/19 - 3s - loss: 232.9969 - loglik: -2.3212e+02 - logprior: -8.7366e-01
Epoch 8/10
19/19 - 3s - loss: 232.2764 - loglik: -2.3143e+02 - logprior: -8.4359e-01
Epoch 9/10
19/19 - 3s - loss: 233.0662 - loglik: -2.3224e+02 - logprior: -8.2331e-01
Fitted a model with MAP estimate = -232.3738
Time for alignment: 90.1660
Computed alignments with likelihoods: ['-232.3887', '-232.4472', '-232.5340', '-233.0873', '-232.3738']
Best model has likelihood: -232.3738  (prior= -0.8261 )
time for generating output: 0.1541
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8516942474389283
