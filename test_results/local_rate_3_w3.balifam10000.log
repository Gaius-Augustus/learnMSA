Training of 3 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff7caf51b80>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff834c445e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff834c442e0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff7caeeaca0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff86f82bdc0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff87477eb20>, <__main__.SimpleDirichletPrior object at 0x7ff83d25efd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 771.0801 - loglik: -7.6922e+02 - logprior: -1.5794e+00
Epoch 2/10
39/39 - 35s - loss: 641.0095 - loglik: -6.3779e+02 - logprior: -2.1117e+00
Epoch 3/10
39/39 - 36s - loss: 629.8723 - loglik: -6.2646e+02 - logprior: -2.0884e+00
Epoch 4/10
39/39 - 37s - loss: 628.3129 - loglik: -6.2499e+02 - logprior: -2.0099e+00
Epoch 5/10
39/39 - 38s - loss: 626.8487 - loglik: -6.2360e+02 - logprior: -2.0096e+00
Epoch 6/10
39/39 - 38s - loss: 626.0483 - loglik: -6.2288e+02 - logprior: -2.0167e+00
Epoch 7/10
39/39 - 38s - loss: 625.4597 - loglik: -6.2234e+02 - logprior: -2.0345e+00
Epoch 8/10
39/39 - 40s - loss: 625.4277 - loglik: -6.2234e+02 - logprior: -2.0475e+00
Epoch 9/10
39/39 - 40s - loss: 624.4200 - loglik: -6.2140e+02 - logprior: -2.0505e+00
Epoch 10/10
39/39 - 41s - loss: 624.2524 - loglik: -6.2105e+02 - logprior: -2.2794e+00
Fitted a model with MAP estimate = -622.4854
expansions: [(9, 1), (12, 1), (14, 1), (15, 1), (46, 1), (49, 1), (51, 4), (55, 1), (61, 1), (62, 1), (63, 1), (65, 1), (66, 1), (68, 1), (77, 1), (78, 1), (79, 1), (83, 1), (84, 1), (85, 1), (90, 1), (91, 1), (94, 1), (97, 1), (99, 1), (102, 1), (119, 1), (120, 1), (121, 1), (135, 1), (140, 1), (143, 1), (146, 1), (159, 1), (160, 1), (161, 1), (163, 2), (164, 3), (165, 1), (171, 1), (184, 2), (187, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (203, 1), (214, 1), (217, 1), (220, 1), (228, 1), (229, 1), (233, 1), (234, 1), (235, 2), (246, 1), (262, 1), (263, 1), (264, 1), (266, 2), (267, 2), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 64s - loss: 629.5836 - loglik: -6.2767e+02 - logprior: -1.6597e+00
Epoch 2/2
39/39 - 61s - loss: 602.0676 - loglik: -6.0093e+02 - logprior: -5.9370e-01
Fitted a model with MAP estimate = -596.8659
expansions: [(146, 1)]
discards: [ 58  59 250 289]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 66s - loss: 611.7652 - loglik: -6.1037e+02 - logprior: -1.2477e+00
Epoch 2/2
39/39 - 61s - loss: 601.7996 - loglik: -6.0120e+02 - logprior: -1.4652e-01
Fitted a model with MAP estimate = -597.0447
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 64s - loss: 610.4648 - loglik: -6.0928e+02 - logprior: -1.0685e+00
Epoch 2/10
39/39 - 60s - loss: 600.8845 - loglik: -6.0054e+02 - logprior: 0.1116
Epoch 3/10
39/39 - 60s - loss: 596.8643 - loglik: -5.9630e+02 - logprior: 0.2249
Epoch 4/10
39/39 - 60s - loss: 594.1298 - loglik: -5.9345e+02 - logprior: 0.3620
Epoch 5/10
39/39 - 64s - loss: 593.2759 - loglik: -5.9256e+02 - logprior: 0.4686
Epoch 6/10
39/39 - 65s - loss: 592.2961 - loglik: -5.9164e+02 - logprior: 0.5756
Epoch 7/10
39/39 - 68s - loss: 591.7460 - loglik: -5.9122e+02 - logprior: 0.7239
Epoch 8/10
39/39 - 72s - loss: 590.9741 - loglik: -5.9054e+02 - logprior: 0.7856
Epoch 9/10
39/39 - 69s - loss: 590.7540 - loglik: -5.9049e+02 - logprior: 0.9106
Epoch 10/10
39/39 - 69s - loss: 588.9122 - loglik: -5.8893e+02 - logprior: 1.1296
Fitted a model with MAP estimate = -587.7308
Time for alignment: 1511.2882
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 767.3679 - loglik: -7.6535e+02 - logprior: -1.6029e+00
Epoch 2/10
39/39 - 49s - loss: 639.6102 - loglik: -6.3616e+02 - logprior: -2.1703e+00
Epoch 3/10
39/39 - 49s - loss: 629.5172 - loglik: -6.2599e+02 - logprior: -2.1130e+00
Epoch 4/10
39/39 - 48s - loss: 627.5441 - loglik: -6.2415e+02 - logprior: -2.0445e+00
Epoch 5/10
39/39 - 48s - loss: 625.9883 - loglik: -6.2267e+02 - logprior: -2.0495e+00
Epoch 6/10
39/39 - 48s - loss: 624.4904 - loglik: -6.2120e+02 - logprior: -2.0830e+00
Epoch 7/10
39/39 - 50s - loss: 624.5640 - loglik: -6.2131e+02 - logprior: -2.1176e+00
Fitted a model with MAP estimate = -622.2007
expansions: [(13, 1), (14, 2), (15, 1), (50, 1), (52, 4), (56, 1), (62, 1), (63, 1), (64, 1), (67, 1), (68, 1), (69, 1), (78, 1), (79, 1), (80, 1), (84, 1), (85, 1), (86, 1), (88, 1), (91, 1), (95, 1), (98, 1), (99, 1), (100, 1), (106, 1), (112, 1), (117, 1), (119, 1), (121, 1), (135, 1), (140, 1), (143, 1), (146, 1), (148, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (172, 1), (183, 1), (184, 1), (188, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (213, 1), (214, 1), (217, 1), (220, 1), (229, 1), (234, 1), (236, 2), (241, 1), (242, 1), (262, 1), (263, 1), (264, 1), (266, 2), (267, 3), (268, 1), (277, 1), (278, 2), (279, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 369 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 629.9717 - loglik: -6.2798e+02 - logprior: -1.6452e+00
Epoch 2/2
39/39 - 59s - loss: 601.9052 - loglik: -6.0026e+02 - logprior: -5.7348e-01
Fitted a model with MAP estimate = -595.3877
expansions: []
discards: [ 58 251]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 611.1644 - loglik: -6.0973e+02 - logprior: -1.2827e+00
Epoch 2/2
39/39 - 68s - loss: 601.4396 - loglik: -6.0077e+02 - logprior: -1.8915e-01
Fitted a model with MAP estimate = -596.7682
expansions: [(143, 1)]
discards: [58]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 606.8148 - loglik: -6.0544e+02 - logprior: -1.0297e+00
Epoch 2/10
39/39 - 70s - loss: 599.3157 - loglik: -5.9854e+02 - logprior: 0.0486
Epoch 3/10
39/39 - 69s - loss: 595.3112 - loglik: -5.9431e+02 - logprior: 0.2235
Epoch 4/10
39/39 - 68s - loss: 593.7897 - loglik: -5.9270e+02 - logprior: 0.3411
Epoch 5/10
39/39 - 62s - loss: 592.4810 - loglik: -5.9147e+02 - logprior: 0.4372
Epoch 6/10
39/39 - 60s - loss: 592.2538 - loglik: -5.9142e+02 - logprior: 0.5732
Epoch 7/10
39/39 - 60s - loss: 591.9423 - loglik: -5.9128e+02 - logprior: 0.6800
Epoch 8/10
39/39 - 59s - loss: 590.5604 - loglik: -5.9015e+02 - logprior: 0.8523
Epoch 9/10
39/39 - 59s - loss: 590.0634 - loglik: -5.8985e+02 - logprior: 0.9725
Epoch 10/10
39/39 - 59s - loss: 589.5538 - loglik: -5.8954e+02 - logprior: 1.1103
Fitted a model with MAP estimate = -587.5749
Time for alignment: 1490.3140
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 770.1005 - loglik: -7.6805e+02 - logprior: -1.5904e+00
Epoch 2/10
39/39 - 41s - loss: 642.5881 - loglik: -6.3916e+02 - logprior: -2.0978e+00
Epoch 3/10
39/39 - 41s - loss: 631.1718 - loglik: -6.2761e+02 - logprior: -2.0817e+00
Epoch 4/10
39/39 - 41s - loss: 628.4749 - loglik: -6.2505e+02 - logprior: -1.9983e+00
Epoch 5/10
39/39 - 41s - loss: 627.5392 - loglik: -6.2417e+02 - logprior: -2.0146e+00
Epoch 6/10
39/39 - 42s - loss: 626.5629 - loglik: -6.2326e+02 - logprior: -2.0437e+00
Epoch 7/10
39/39 - 41s - loss: 625.6240 - loglik: -6.2238e+02 - logprior: -2.0558e+00
Epoch 8/10
39/39 - 41s - loss: 625.4171 - loglik: -6.2226e+02 - logprior: -2.0697e+00
Epoch 9/10
39/39 - 43s - loss: 624.9435 - loglik: -6.2185e+02 - logprior: -2.0735e+00
Epoch 10/10
39/39 - 43s - loss: 624.5029 - loglik: -6.2147e+02 - logprior: -2.0825e+00
Fitted a model with MAP estimate = -622.8162
expansions: [(13, 1), (14, 1), (15, 1), (50, 1), (52, 4), (56, 1), (62, 1), (63, 1), (64, 1), (67, 1), (68, 1), (69, 1), (78, 1), (79, 1), (82, 1), (84, 1), (85, 1), (86, 1), (92, 1), (96, 1), (99, 1), (102, 1), (104, 1), (107, 1), (109, 1), (118, 1), (120, 1), (122, 1), (135, 1), (141, 1), (143, 1), (144, 1), (157, 1), (158, 1), (161, 1), (163, 1), (164, 1), (166, 1), (167, 1), (173, 1), (184, 1), (185, 1), (189, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 2), (200, 1), (214, 1), (215, 1), (218, 1), (220, 1), (230, 1), (235, 2), (236, 3), (238, 1), (263, 1), (264, 1), (265, 1), (267, 2), (268, 2), (269, 1), (278, 1), (279, 2), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 629.2318 - loglik: -6.2693e+02 - logprior: -1.6962e+00
Epoch 2/2
39/39 - 63s - loss: 601.8387 - loglik: -5.9996e+02 - logprior: -6.2987e-01
Fitted a model with MAP estimate = -596.1925
expansions: [(16, 1)]
discards: [ 57 249 294]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 615.2726 - loglik: -6.1374e+02 - logprior: -1.2470e+00
Epoch 2/2
39/39 - 63s - loss: 601.6270 - loglik: -6.0054e+02 - logprior: -1.3090e-01
Fitted a model with MAP estimate = -596.0078
expansions: [(143, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 64s - loss: 609.1061 - loglik: -6.0791e+02 - logprior: -9.6334e-01
Epoch 2/10
39/39 - 60s - loss: 600.4456 - loglik: -6.0001e+02 - logprior: 0.1468
Epoch 3/10
39/39 - 61s - loss: 596.4628 - loglik: -5.9578e+02 - logprior: 0.2422
Epoch 4/10
39/39 - 60s - loss: 594.0184 - loglik: -5.9324e+02 - logprior: 0.3829
Epoch 5/10
39/39 - 60s - loss: 593.5950 - loglik: -5.9279e+02 - logprior: 0.4818
Epoch 6/10
39/39 - 60s - loss: 591.7370 - loglik: -5.9106e+02 - logprior: 0.6434
Epoch 7/10
39/39 - 60s - loss: 592.0411 - loglik: -5.9149e+02 - logprior: 0.7554
Fitted a model with MAP estimate = -589.1486
Time for alignment: 1344.5265
Computed alignments with likelihoods: ['-587.7308', '-587.5749', '-589.1486']
Best model has likelihood: -587.5749
time for generating output: 0.3492
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.9357298474945533
Training of 3 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff834d1b190>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff834c445e0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67f7d93a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff687fe6d30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff7caed5b20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff84647d250>, <__main__.SimpleDirichletPrior object at 0x7ff66d2627c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 150.0494 - loglik: -1.4682e+02 - logprior: -3.1999e+00
Epoch 2/10
19/19 - 1s - loss: 127.8022 - loglik: -1.2619e+02 - logprior: -1.3995e+00
Epoch 3/10
19/19 - 1s - loss: 118.4586 - loglik: -1.1639e+02 - logprior: -1.6384e+00
Epoch 4/10
19/19 - 1s - loss: 116.9287 - loglik: -1.1513e+02 - logprior: -1.4988e+00
Epoch 5/10
19/19 - 1s - loss: 116.4263 - loglik: -1.1470e+02 - logprior: -1.4754e+00
Epoch 6/10
19/19 - 1s - loss: 116.2982 - loglik: -1.1462e+02 - logprior: -1.4476e+00
Epoch 7/10
19/19 - 1s - loss: 116.0345 - loglik: -1.1439e+02 - logprior: -1.4350e+00
Epoch 8/10
19/19 - 1s - loss: 116.1341 - loglik: -1.1450e+02 - logprior: -1.4243e+00
Fitted a model with MAP estimate = -115.5399
expansions: [(6, 1), (7, 1), (8, 1), (15, 1), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 122.6465 - loglik: -1.1847e+02 - logprior: -4.1205e+00
Epoch 2/2
19/19 - 1s - loss: 113.4869 - loglik: -1.1123e+02 - logprior: -2.0640e+00
Fitted a model with MAP estimate = -111.5334
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 113.7016 - loglik: -1.1067e+02 - logprior: -3.0132e+00
Epoch 2/2
19/19 - 1s - loss: 110.0126 - loglik: -1.0867e+02 - logprior: -1.2080e+00
Fitted a model with MAP estimate = -109.0289
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 115.2750 - loglik: -1.1141e+02 - logprior: -3.7496e+00
Epoch 2/10
19/19 - 1s - loss: 110.6214 - loglik: -1.0906e+02 - logprior: -1.4172e+00
Epoch 3/10
19/19 - 1s - loss: 109.4813 - loglik: -1.0795e+02 - logprior: -1.2436e+00
Epoch 4/10
19/19 - 1s - loss: 109.0903 - loglik: -1.0750e+02 - logprior: -1.2164e+00
Epoch 5/10
19/19 - 1s - loss: 108.7498 - loglik: -1.0718e+02 - logprior: -1.2013e+00
Epoch 6/10
19/19 - 1s - loss: 108.4290 - loglik: -1.0689e+02 - logprior: -1.1926e+00
Epoch 7/10
19/19 - 1s - loss: 108.1848 - loglik: -1.0668e+02 - logprior: -1.1795e+00
Epoch 8/10
19/19 - 1s - loss: 107.9798 - loglik: -1.0649e+02 - logprior: -1.1660e+00
Epoch 9/10
19/19 - 1s - loss: 107.9493 - loglik: -1.0649e+02 - logprior: -1.1559e+00
Epoch 10/10
19/19 - 1s - loss: 107.6465 - loglik: -1.0619e+02 - logprior: -1.1470e+00
Fitted a model with MAP estimate = -107.3279
Time for alignment: 51.3743
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 149.9370 - loglik: -1.4671e+02 - logprior: -3.1963e+00
Epoch 2/10
19/19 - 1s - loss: 128.1197 - loglik: -1.2648e+02 - logprior: -1.3944e+00
Epoch 3/10
19/19 - 1s - loss: 118.7252 - loglik: -1.1662e+02 - logprior: -1.6166e+00
Epoch 4/10
19/19 - 1s - loss: 116.9306 - loglik: -1.1510e+02 - logprior: -1.4921e+00
Epoch 5/10
19/19 - 1s - loss: 116.4551 - loglik: -1.1472e+02 - logprior: -1.4745e+00
Epoch 6/10
19/19 - 1s - loss: 116.2831 - loglik: -1.1461e+02 - logprior: -1.4483e+00
Epoch 7/10
19/19 - 1s - loss: 115.9338 - loglik: -1.1429e+02 - logprior: -1.4371e+00
Epoch 8/10
19/19 - 1s - loss: 115.9291 - loglik: -1.1430e+02 - logprior: -1.4292e+00
Epoch 9/10
19/19 - 1s - loss: 115.8520 - loglik: -1.1423e+02 - logprior: -1.4236e+00
Epoch 10/10
19/19 - 1s - loss: 115.6683 - loglik: -1.1405e+02 - logprior: -1.4241e+00
Fitted a model with MAP estimate = -115.3346
expansions: [(6, 1), (7, 1), (8, 2), (15, 1), (19, 2), (23, 2), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.8448 - loglik: -1.1846e+02 - logprior: -4.1480e+00
Epoch 2/2
19/19 - 1s - loss: 113.2124 - loglik: -1.1060e+02 - logprior: -2.1654e+00
Fitted a model with MAP estimate = -111.1179
expansions: [(0, 2)]
discards: [ 0  9 23 29 36 43]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.8499 - loglik: -1.1174e+02 - logprior: -3.0173e+00
Epoch 2/2
19/19 - 1s - loss: 110.3966 - loglik: -1.0915e+02 - logprior: -1.2029e+00
Fitted a model with MAP estimate = -109.3990
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 114.7687 - loglik: -1.1077e+02 - logprior: -3.7542e+00
Epoch 2/10
19/19 - 1s - loss: 110.2434 - loglik: -1.0840e+02 - logprior: -1.4032e+00
Epoch 3/10
19/19 - 1s - loss: 109.4767 - loglik: -1.0775e+02 - logprior: -1.2520e+00
Epoch 4/10
19/19 - 1s - loss: 108.8858 - loglik: -1.0723e+02 - logprior: -1.2122e+00
Epoch 5/10
19/19 - 1s - loss: 108.5698 - loglik: -1.0696e+02 - logprior: -1.2003e+00
Epoch 6/10
19/19 - 1s - loss: 108.3088 - loglik: -1.0676e+02 - logprior: -1.1892e+00
Epoch 7/10
19/19 - 1s - loss: 108.0589 - loglik: -1.0655e+02 - logprior: -1.1743e+00
Epoch 8/10
19/19 - 1s - loss: 107.9272 - loglik: -1.0645e+02 - logprior: -1.1640e+00
Epoch 9/10
19/19 - 1s - loss: 107.8257 - loglik: -1.0637e+02 - logprior: -1.1509e+00
Epoch 10/10
19/19 - 1s - loss: 107.6456 - loglik: -1.0620e+02 - logprior: -1.1416e+00
Fitted a model with MAP estimate = -107.2516
Time for alignment: 53.3292
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 150.1646 - loglik: -1.4692e+02 - logprior: -3.2018e+00
Epoch 2/10
19/19 - 1s - loss: 128.0507 - loglik: -1.2645e+02 - logprior: -1.3943e+00
Epoch 3/10
19/19 - 1s - loss: 118.3487 - loglik: -1.1631e+02 - logprior: -1.6388e+00
Epoch 4/10
19/19 - 1s - loss: 116.9370 - loglik: -1.1514e+02 - logprior: -1.5042e+00
Epoch 5/10
19/19 - 1s - loss: 116.5467 - loglik: -1.1482e+02 - logprior: -1.4756e+00
Epoch 6/10
19/19 - 1s - loss: 116.1253 - loglik: -1.1445e+02 - logprior: -1.4509e+00
Epoch 7/10
19/19 - 1s - loss: 116.1237 - loglik: -1.1446e+02 - logprior: -1.4373e+00
Epoch 8/10
19/19 - 1s - loss: 116.0321 - loglik: -1.1440e+02 - logprior: -1.4260e+00
Epoch 9/10
19/19 - 1s - loss: 115.8851 - loglik: -1.1426e+02 - logprior: -1.4231e+00
Epoch 10/10
19/19 - 1s - loss: 115.7811 - loglik: -1.1416e+02 - logprior: -1.4201e+00
Fitted a model with MAP estimate = -115.4402
expansions: [(6, 1), (7, 1), (8, 1), (10, 1), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 122.9368 - loglik: -1.1877e+02 - logprior: -4.1260e+00
Epoch 2/2
19/19 - 1s - loss: 113.5662 - loglik: -1.1137e+02 - logprior: -2.0666e+00
Fitted a model with MAP estimate = -111.5383
expansions: [(0, 2)]
discards: [ 0 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 113.2681 - loglik: -1.1016e+02 - logprior: -3.0099e+00
Epoch 2/2
19/19 - 1s - loss: 109.7589 - loglik: -1.0826e+02 - logprior: -1.2117e+00
Fitted a model with MAP estimate = -108.6638
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 115.1785 - loglik: -1.1140e+02 - logprior: -3.7354e+00
Epoch 2/10
19/19 - 1s - loss: 110.5321 - loglik: -1.0896e+02 - logprior: -1.4023e+00
Epoch 3/10
19/19 - 1s - loss: 109.4630 - loglik: -1.0792e+02 - logprior: -1.2464e+00
Epoch 4/10
19/19 - 1s - loss: 109.0365 - loglik: -1.0745e+02 - logprior: -1.2110e+00
Epoch 5/10
19/19 - 1s - loss: 108.6879 - loglik: -1.0711e+02 - logprior: -1.1974e+00
Epoch 6/10
19/19 - 1s - loss: 108.3596 - loglik: -1.0682e+02 - logprior: -1.1844e+00
Epoch 7/10
19/19 - 1s - loss: 108.1458 - loglik: -1.0664e+02 - logprior: -1.1722e+00
Epoch 8/10
19/19 - 1s - loss: 107.9489 - loglik: -1.0647e+02 - logprior: -1.1600e+00
Epoch 9/10
19/19 - 1s - loss: 107.7966 - loglik: -1.0634e+02 - logprior: -1.1466e+00
Epoch 10/10
19/19 - 1s - loss: 107.8349 - loglik: -1.0639e+02 - logprior: -1.1373e+00
Fitted a model with MAP estimate = -107.2876
Time for alignment: 53.8533
Computed alignments with likelihoods: ['-107.3279', '-107.2516', '-107.2876']
Best model has likelihood: -107.2516
time for generating output: 0.1199
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.6675848612665347
Training of 3 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff6985b0a30>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff7ce53a3a0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff82c359b20>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff6762abfa0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff6762abe20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff67f7d93a0>, <__main__.SimpleDirichletPrior object at 0x7ff675d8ef70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 360.3507 - loglik: -3.5722e+02 - logprior: -3.0153e+00
Epoch 2/10
19/19 - 3s - loss: 305.0498 - loglik: -3.0344e+02 - logprior: -1.2475e+00
Epoch 3/10
19/19 - 4s - loss: 276.4232 - loglik: -2.7406e+02 - logprior: -1.5317e+00
Epoch 4/10
19/19 - 4s - loss: 269.3952 - loglik: -2.6708e+02 - logprior: -1.5192e+00
Epoch 5/10
19/19 - 4s - loss: 267.1934 - loglik: -2.6510e+02 - logprior: -1.4431e+00
Epoch 6/10
19/19 - 4s - loss: 266.0955 - loglik: -2.6412e+02 - logprior: -1.3887e+00
Epoch 7/10
19/19 - 4s - loss: 265.8632 - loglik: -2.6397e+02 - logprior: -1.3594e+00
Epoch 8/10
19/19 - 4s - loss: 265.4036 - loglik: -2.6356e+02 - logprior: -1.3514e+00
Epoch 9/10
19/19 - 4s - loss: 265.2288 - loglik: -2.6340e+02 - logprior: -1.3469e+00
Epoch 10/10
19/19 - 4s - loss: 264.5193 - loglik: -2.6270e+02 - logprior: -1.3420e+00
Fitted a model with MAP estimate = -264.1859
expansions: [(12, 1), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (60, 1), (70, 1), (71, 1), (93, 1), (96, 1), (101, 1), (105, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 274.1082 - loglik: -2.7013e+02 - logprior: -3.9085e+00
Epoch 2/2
19/19 - 5s - loss: 259.7052 - loglik: -2.5744e+02 - logprior: -1.9080e+00
Fitted a model with MAP estimate = -256.9167
expansions: [(0, 2)]
discards: [  0  16  37 136]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 260.7706 - loglik: -2.5795e+02 - logprior: -2.7388e+00
Epoch 2/2
19/19 - 5s - loss: 256.3850 - loglik: -2.5501e+02 - logprior: -9.8234e-01
Fitted a model with MAP estimate = -254.6995
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 262.2299 - loglik: -2.5847e+02 - logprior: -3.7067e+00
Epoch 2/10
19/19 - 5s - loss: 257.0545 - loglik: -2.5550e+02 - logprior: -1.3113e+00
Epoch 3/10
19/19 - 5s - loss: 255.1510 - loglik: -2.5394e+02 - logprior: -7.5448e-01
Epoch 4/10
19/19 - 5s - loss: 254.2586 - loglik: -2.5296e+02 - logprior: -7.4265e-01
Epoch 5/10
19/19 - 5s - loss: 253.1408 - loglik: -2.5187e+02 - logprior: -7.0073e-01
Epoch 6/10
19/19 - 5s - loss: 252.9823 - loglik: -2.5176e+02 - logprior: -6.5279e-01
Epoch 7/10
19/19 - 5s - loss: 252.3116 - loglik: -2.5116e+02 - logprior: -6.0167e-01
Epoch 8/10
19/19 - 5s - loss: 252.8369 - loglik: -2.5175e+02 - logprior: -5.6185e-01
Fitted a model with MAP estimate = -251.6609
Time for alignment: 131.4904
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 359.9530 - loglik: -3.5684e+02 - logprior: -3.0210e+00
Epoch 2/10
19/19 - 4s - loss: 300.8929 - loglik: -2.9926e+02 - logprior: -1.2298e+00
Epoch 3/10
19/19 - 4s - loss: 276.2378 - loglik: -2.7396e+02 - logprior: -1.3786e+00
Epoch 4/10
19/19 - 4s - loss: 270.7222 - loglik: -2.6859e+02 - logprior: -1.3698e+00
Epoch 5/10
19/19 - 4s - loss: 269.1496 - loglik: -2.6718e+02 - logprior: -1.2960e+00
Epoch 6/10
19/19 - 4s - loss: 268.1316 - loglik: -2.6631e+02 - logprior: -1.2554e+00
Epoch 7/10
19/19 - 4s - loss: 267.7460 - loglik: -2.6599e+02 - logprior: -1.2324e+00
Epoch 8/10
19/19 - 4s - loss: 267.0289 - loglik: -2.6531e+02 - logprior: -1.2228e+00
Epoch 9/10
19/19 - 4s - loss: 267.5013 - loglik: -2.6581e+02 - logprior: -1.2193e+00
Fitted a model with MAP estimate = -266.5787
expansions: [(12, 1), (14, 4), (17, 1), (26, 2), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (51, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (94, 1), (97, 1), (101, 1), (107, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 272.8541 - loglik: -2.6885e+02 - logprior: -3.9131e+00
Epoch 2/2
19/19 - 5s - loss: 258.9885 - loglik: -2.5671e+02 - logprior: -1.9465e+00
Fitted a model with MAP estimate = -256.0166
expansions: [(0, 3)]
discards: [  0  14  15  38  76 138]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 263.2885 - loglik: -2.6049e+02 - logprior: -2.7622e+00
Epoch 2/2
19/19 - 5s - loss: 257.5536 - loglik: -2.5641e+02 - logprior: -9.8732e-01
Fitted a model with MAP estimate = -255.9486
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 262.8351 - loglik: -2.5902e+02 - logprior: -3.7485e+00
Epoch 2/10
19/19 - 5s - loss: 258.3561 - loglik: -2.5649e+02 - logprior: -1.5395e+00
Epoch 3/10
19/19 - 5s - loss: 255.6244 - loglik: -2.5431e+02 - logprior: -7.5902e-01
Epoch 4/10
19/19 - 5s - loss: 255.0565 - loglik: -2.5379e+02 - logprior: -6.6586e-01
Epoch 5/10
19/19 - 5s - loss: 254.0038 - loglik: -2.5276e+02 - logprior: -6.7688e-01
Epoch 6/10
19/19 - 5s - loss: 253.4723 - loglik: -2.5234e+02 - logprior: -6.0990e-01
Epoch 7/10
19/19 - 5s - loss: 253.4177 - loglik: -2.5236e+02 - logprior: -5.6343e-01
Epoch 8/10
19/19 - 5s - loss: 253.2433 - loglik: -2.5224e+02 - logprior: -5.2379e-01
Epoch 9/10
19/19 - 5s - loss: 252.8060 - loglik: -2.5184e+02 - logprior: -4.9601e-01
Epoch 10/10
19/19 - 5s - loss: 253.0435 - loglik: -2.5213e+02 - logprior: -4.5124e-01
Fitted a model with MAP estimate = -252.1699
Time for alignment: 140.1998
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 360.2153 - loglik: -3.5711e+02 - logprior: -3.0197e+00
Epoch 2/10
19/19 - 4s - loss: 303.8549 - loglik: -3.0223e+02 - logprior: -1.2582e+00
Epoch 3/10
19/19 - 4s - loss: 275.7907 - loglik: -2.7338e+02 - logprior: -1.5179e+00
Epoch 4/10
19/19 - 4s - loss: 269.2099 - loglik: -2.6687e+02 - logprior: -1.5023e+00
Epoch 5/10
19/19 - 4s - loss: 267.0135 - loglik: -2.6489e+02 - logprior: -1.4377e+00
Epoch 6/10
19/19 - 4s - loss: 266.5203 - loglik: -2.6454e+02 - logprior: -1.3711e+00
Epoch 7/10
19/19 - 4s - loss: 265.6075 - loglik: -2.6369e+02 - logprior: -1.3677e+00
Epoch 8/10
19/19 - 4s - loss: 265.1940 - loglik: -2.6333e+02 - logprior: -1.3533e+00
Epoch 9/10
19/19 - 4s - loss: 265.1063 - loglik: -2.6327e+02 - logprior: -1.3481e+00
Epoch 10/10
19/19 - 4s - loss: 264.5875 - loglik: -2.6276e+02 - logprior: -1.3487e+00
Fitted a model with MAP estimate = -264.1399
expansions: [(12, 1), (14, 3), (16, 1), (17, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (56, 1), (60, 1), (70, 1), (71, 1), (93, 1), (96, 1), (101, 1), (103, 1), (108, 1), (109, 1), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 273.7582 - loglik: -2.6978e+02 - logprior: -3.9088e+00
Epoch 2/2
19/19 - 5s - loss: 259.5655 - loglik: -2.5729e+02 - logprior: -1.9100e+00
Fitted a model with MAP estimate = -256.9653
expansions: [(0, 2)]
discards: [  0  16  37 136]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 262.0382 - loglik: -2.5913e+02 - logprior: -2.7298e+00
Epoch 2/2
19/19 - 5s - loss: 256.4433 - loglik: -2.5531e+02 - logprior: -9.5711e-01
Fitted a model with MAP estimate = -254.7148
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 260.4304 - loglik: -2.5653e+02 - logprior: -3.6943e+00
Epoch 2/10
19/19 - 5s - loss: 256.2121 - loglik: -2.5437e+02 - logprior: -1.3176e+00
Epoch 3/10
19/19 - 5s - loss: 254.4684 - loglik: -2.5306e+02 - logprior: -7.5260e-01
Epoch 4/10
19/19 - 5s - loss: 253.1910 - loglik: -2.5177e+02 - logprior: -7.6175e-01
Epoch 5/10
19/19 - 5s - loss: 252.9140 - loglik: -2.5161e+02 - logprior: -7.0574e-01
Epoch 6/10
19/19 - 5s - loss: 252.5518 - loglik: -2.5134e+02 - logprior: -6.4349e-01
Epoch 7/10
19/19 - 5s - loss: 251.8173 - loglik: -2.5068e+02 - logprior: -6.0909e-01
Epoch 8/10
19/19 - 5s - loss: 252.2212 - loglik: -2.5113e+02 - logprior: -5.7582e-01
Fitted a model with MAP estimate = -251.1414
Time for alignment: 135.2403
Computed alignments with likelihoods: ['-251.6609', '-252.1699', '-251.1414']
Best model has likelihood: -251.1414
time for generating output: 0.1827
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.8105960264900662
Training of 3 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff86fbc0d30>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff676517670>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff86fda1c40>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff874bc5580>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff834f8ecd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6765ba6a0>, <__main__.SimpleDirichletPrior object at 0x7ff698bcc7c0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 274.5071 - loglik: -2.7135e+02 - logprior: -3.0792e+00
Epoch 2/10
19/19 - 3s - loss: 235.4627 - loglik: -2.3381e+02 - logprior: -1.2801e+00
Epoch 3/10
19/19 - 3s - loss: 221.0592 - loglik: -2.1893e+02 - logprior: -1.5090e+00
Epoch 4/10
19/19 - 3s - loss: 217.9731 - loglik: -2.1589e+02 - logprior: -1.5801e+00
Epoch 5/10
19/19 - 3s - loss: 216.7928 - loglik: -2.1485e+02 - logprior: -1.5031e+00
Epoch 6/10
19/19 - 3s - loss: 215.6938 - loglik: -2.1381e+02 - logprior: -1.4864e+00
Epoch 7/10
19/19 - 3s - loss: 215.6424 - loglik: -2.1382e+02 - logprior: -1.4684e+00
Epoch 8/10
19/19 - 3s - loss: 215.2724 - loglik: -2.1347e+02 - logprior: -1.4630e+00
Epoch 9/10
19/19 - 3s - loss: 214.9300 - loglik: -2.1315e+02 - logprior: -1.4514e+00
Epoch 10/10
19/19 - 3s - loss: 214.8795 - loglik: -2.1310e+02 - logprior: -1.4524e+00
Fitted a model with MAP estimate = -214.1361
expansions: [(5, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (21, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (70, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 217.8798 - loglik: -2.1478e+02 - logprior: -3.0004e+00
Epoch 2/2
19/19 - 3s - loss: 206.7651 - loglik: -2.0518e+02 - logprior: -1.2547e+00
Fitted a model with MAP estimate = -204.5785
expansions: []
discards: [ 1 45 74 78 94]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 210.2632 - loglik: -2.0726e+02 - logprior: -2.7749e+00
Epoch 2/2
19/19 - 3s - loss: 205.9311 - loglik: -2.0447e+02 - logprior: -1.1596e+00
Fitted a model with MAP estimate = -204.3633
expansions: [(24, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 208.7498 - loglik: -2.0587e+02 - logprior: -2.8378e+00
Epoch 2/10
19/19 - 3s - loss: 205.6163 - loglik: -2.0422e+02 - logprior: -1.0793e+00
Epoch 3/10
19/19 - 3s - loss: 204.3197 - loglik: -2.0276e+02 - logprior: -1.0509e+00
Epoch 4/10
19/19 - 3s - loss: 203.5674 - loglik: -2.0206e+02 - logprior: -1.0062e+00
Epoch 5/10
19/19 - 3s - loss: 203.2928 - loglik: -2.0188e+02 - logprior: -9.6579e-01
Epoch 6/10
19/19 - 3s - loss: 202.9678 - loglik: -2.0163e+02 - logprior: -9.4775e-01
Epoch 7/10
19/19 - 3s - loss: 202.8493 - loglik: -2.0156e+02 - logprior: -9.2855e-01
Epoch 8/10
19/19 - 3s - loss: 202.6755 - loglik: -2.0143e+02 - logprior: -9.0903e-01
Epoch 9/10
19/19 - 4s - loss: 202.5726 - loglik: -2.0135e+02 - logprior: -8.8795e-01
Epoch 10/10
19/19 - 3s - loss: 202.6145 - loglik: -2.0141e+02 - logprior: -8.7892e-01
Fitted a model with MAP estimate = -201.9787
Time for alignment: 108.8776
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.5593 - loglik: -2.7140e+02 - logprior: -3.0757e+00
Epoch 2/10
19/19 - 3s - loss: 236.2032 - loglik: -2.3454e+02 - logprior: -1.2638e+00
Epoch 3/10
19/19 - 3s - loss: 220.8889 - loglik: -2.1870e+02 - logprior: -1.5649e+00
Epoch 4/10
19/19 - 3s - loss: 217.7194 - loglik: -2.1559e+02 - logprior: -1.6308e+00
Epoch 5/10
19/19 - 3s - loss: 217.0958 - loglik: -2.1512e+02 - logprior: -1.5145e+00
Epoch 6/10
19/19 - 3s - loss: 215.8933 - loglik: -2.1398e+02 - logprior: -1.4998e+00
Epoch 7/10
19/19 - 3s - loss: 216.0843 - loglik: -2.1423e+02 - logprior: -1.4842e+00
Fitted a model with MAP estimate = -215.0388
expansions: [(5, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (21, 1), (28, 1), (29, 1), (33, 1), (46, 1), (59, 2), (61, 2), (63, 3), (64, 1), (71, 2), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 221.2671 - loglik: -2.1727e+02 - logprior: -3.9543e+00
Epoch 2/2
19/19 - 3s - loss: 208.8940 - loglik: -2.0671e+02 - logprior: -2.1039e+00
Fitted a model with MAP estimate = -206.6665
expansions: [(0, 2)]
discards: [ 0 72 76 92]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 209.9999 - loglik: -2.0702e+02 - logprior: -2.9181e+00
Epoch 2/2
19/19 - 3s - loss: 206.1148 - loglik: -2.0493e+02 - logprior: -1.1257e+00
Fitted a model with MAP estimate = -204.7484
expansions: [(25, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 210.1066 - loglik: -2.0719e+02 - logprior: -2.7160e+00
Epoch 2/10
19/19 - 3s - loss: 206.4673 - loglik: -2.0526e+02 - logprior: -1.1115e+00
Epoch 3/10
19/19 - 3s - loss: 204.6395 - loglik: -2.0329e+02 - logprior: -1.0785e+00
Epoch 4/10
19/19 - 3s - loss: 204.0192 - loglik: -2.0264e+02 - logprior: -1.0168e+00
Epoch 5/10
19/19 - 3s - loss: 203.7693 - loglik: -2.0242e+02 - logprior: -9.8881e-01
Epoch 6/10
19/19 - 3s - loss: 203.2684 - loglik: -2.0196e+02 - logprior: -9.7119e-01
Epoch 7/10
19/19 - 3s - loss: 203.3439 - loglik: -2.0207e+02 - logprior: -9.5469e-01
Fitted a model with MAP estimate = -202.6170
Time for alignment: 89.8435
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 274.6516 - loglik: -2.7148e+02 - logprior: -3.0807e+00
Epoch 2/10
19/19 - 3s - loss: 236.5809 - loglik: -2.3498e+02 - logprior: -1.2659e+00
Epoch 3/10
19/19 - 3s - loss: 221.7846 - loglik: -2.1974e+02 - logprior: -1.4724e+00
Epoch 4/10
19/19 - 3s - loss: 216.8064 - loglik: -2.1472e+02 - logprior: -1.5533e+00
Epoch 5/10
19/19 - 3s - loss: 215.5443 - loglik: -2.1371e+02 - logprior: -1.3909e+00
Epoch 6/10
19/19 - 3s - loss: 214.7294 - loglik: -2.1297e+02 - logprior: -1.3705e+00
Epoch 7/10
19/19 - 3s - loss: 214.4374 - loglik: -2.1273e+02 - logprior: -1.3442e+00
Epoch 8/10
19/19 - 3s - loss: 214.3185 - loglik: -2.1264e+02 - logprior: -1.3314e+00
Epoch 9/10
19/19 - 3s - loss: 214.2294 - loglik: -2.1258e+02 - logprior: -1.3230e+00
Epoch 10/10
19/19 - 3s - loss: 213.8335 - loglik: -2.1219e+02 - logprior: -1.3222e+00
Fitted a model with MAP estimate = -213.1725
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (19, 1), (30, 1), (33, 2), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (68, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 219.4467 - loglik: -2.1528e+02 - logprior: -3.9248e+00
Epoch 2/2
19/19 - 4s - loss: 207.4692 - loglik: -2.0576e+02 - logprior: -1.3245e+00
Fitted a model with MAP estimate = -204.9558
expansions: []
discards: [ 0 45 74 78 94]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 211.5070 - loglik: -2.0734e+02 - logprior: -3.9902e+00
Epoch 2/2
19/19 - 3s - loss: 206.8591 - loglik: -2.0505e+02 - logprior: -1.4437e+00
Fitted a model with MAP estimate = -204.9058
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 210.9238 - loglik: -2.0779e+02 - logprior: -3.1008e+00
Epoch 2/10
19/19 - 3s - loss: 206.7397 - loglik: -2.0562e+02 - logprior: -1.0122e+00
Epoch 3/10
19/19 - 3s - loss: 205.5785 - loglik: -2.0444e+02 - logprior: -9.2124e-01
Epoch 4/10
19/19 - 3s - loss: 204.7292 - loglik: -2.0350e+02 - logprior: -8.9450e-01
Epoch 5/10
19/19 - 3s - loss: 204.6175 - loglik: -2.0339e+02 - logprior: -8.4298e-01
Epoch 6/10
19/19 - 3s - loss: 203.9923 - loglik: -2.0278e+02 - logprior: -8.0913e-01
Epoch 7/10
19/19 - 3s - loss: 203.9638 - loglik: -2.0278e+02 - logprior: -7.9638e-01
Epoch 8/10
19/19 - 3s - loss: 203.7940 - loglik: -2.0265e+02 - logprior: -7.7622e-01
Epoch 9/10
19/19 - 3s - loss: 203.2441 - loglik: -2.0214e+02 - logprior: -7.5237e-01
Epoch 10/10
19/19 - 3s - loss: 203.7257 - loglik: -2.0262e+02 - logprior: -7.5722e-01
Fitted a model with MAP estimate = -203.0157
Time for alignment: 107.7899
Computed alignments with likelihoods: ['-201.9787', '-202.6170', '-203.0157']
Best model has likelihood: -201.9787
time for generating output: 0.2099
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.6600952423941501
Training of 3 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff69852c8e0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff698bde640>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff698c7b5e0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff5007678b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66d35bfd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff7caa7e610>, <__main__.SimpleDirichletPrior object at 0x7ff66daa7cd0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.5569 - loglik: -1.7331e+02 - logprior: -3.2200e+00
Epoch 2/10
19/19 - 2s - loss: 133.6719 - loglik: -1.3217e+02 - logprior: -1.4695e+00
Epoch 3/10
19/19 - 1s - loss: 116.8879 - loglik: -1.1524e+02 - logprior: -1.4638e+00
Epoch 4/10
19/19 - 1s - loss: 114.2163 - loglik: -1.1250e+02 - logprior: -1.4710e+00
Epoch 5/10
19/19 - 2s - loss: 113.3402 - loglik: -1.1174e+02 - logprior: -1.4094e+00
Epoch 6/10
19/19 - 2s - loss: 113.0700 - loglik: -1.1152e+02 - logprior: -1.3809e+00
Epoch 7/10
19/19 - 2s - loss: 112.7893 - loglik: -1.1124e+02 - logprior: -1.3700e+00
Epoch 8/10
19/19 - 2s - loss: 112.5208 - loglik: -1.1097e+02 - logprior: -1.3589e+00
Epoch 9/10
19/19 - 2s - loss: 112.6699 - loglik: -1.1112e+02 - logprior: -1.3552e+00
Fitted a model with MAP estimate = -112.2328
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.9138 - loglik: -1.1072e+02 - logprior: -4.1412e+00
Epoch 2/2
19/19 - 2s - loss: 104.9057 - loglik: -1.0355e+02 - logprior: -1.2968e+00
Fitted a model with MAP estimate = -103.4507
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 109.2671 - loglik: -1.0515e+02 - logprior: -4.0453e+00
Epoch 2/2
19/19 - 2s - loss: 104.9426 - loglik: -1.0326e+02 - logprior: -1.5403e+00
Fitted a model with MAP estimate = -103.7224
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 107.6302 - loglik: -1.0469e+02 - logprior: -2.8845e+00
Epoch 2/10
19/19 - 2s - loss: 104.2108 - loglik: -1.0309e+02 - logprior: -1.0337e+00
Epoch 3/10
19/19 - 2s - loss: 103.2511 - loglik: -1.0184e+02 - logprior: -1.2519e+00
Epoch 4/10
19/19 - 2s - loss: 102.3741 - loglik: -1.0101e+02 - logprior: -1.1228e+00
Epoch 5/10
19/19 - 2s - loss: 101.5809 - loglik: -1.0024e+02 - logprior: -1.0881e+00
Epoch 6/10
19/19 - 2s - loss: 101.5345 - loglik: -1.0022e+02 - logprior: -1.0799e+00
Epoch 7/10
19/19 - 2s - loss: 101.5132 - loglik: -1.0022e+02 - logprior: -1.0604e+00
Epoch 8/10
19/19 - 2s - loss: 101.0940 - loglik: -9.9816e+01 - logprior: -1.0434e+00
Epoch 9/10
19/19 - 2s - loss: 101.2107 - loglik: -9.9937e+01 - logprior: -1.0274e+00
Fitted a model with MAP estimate = -100.7072
Time for alignment: 58.8865
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.3572 - loglik: -1.7312e+02 - logprior: -3.2201e+00
Epoch 2/10
19/19 - 2s - loss: 133.4911 - loglik: -1.3200e+02 - logprior: -1.4648e+00
Epoch 3/10
19/19 - 2s - loss: 117.2153 - loglik: -1.1553e+02 - logprior: -1.4605e+00
Epoch 4/10
19/19 - 2s - loss: 114.3767 - loglik: -1.1266e+02 - logprior: -1.4742e+00
Epoch 5/10
19/19 - 2s - loss: 113.6444 - loglik: -1.1206e+02 - logprior: -1.4081e+00
Epoch 6/10
19/19 - 2s - loss: 113.1400 - loglik: -1.1159e+02 - logprior: -1.3816e+00
Epoch 7/10
19/19 - 1s - loss: 113.1753 - loglik: -1.1164e+02 - logprior: -1.3685e+00
Fitted a model with MAP estimate = -112.7150
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.1774 - loglik: -1.1006e+02 - logprior: -4.0815e+00
Epoch 2/2
19/19 - 2s - loss: 104.6372 - loglik: -1.0322e+02 - logprior: -1.2949e+00
Fitted a model with MAP estimate = -103.1152
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 109.2318 - loglik: -1.0514e+02 - logprior: -4.0516e+00
Epoch 2/2
19/19 - 2s - loss: 104.8589 - loglik: -1.0325e+02 - logprior: -1.5254e+00
Fitted a model with MAP estimate = -103.6807
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.3396 - loglik: -1.0406e+02 - logprior: -3.1881e+00
Epoch 2/10
19/19 - 2s - loss: 104.4344 - loglik: -1.0288e+02 - logprior: -1.3954e+00
Epoch 3/10
19/19 - 2s - loss: 103.0743 - loglik: -1.0154e+02 - logprior: -1.2953e+00
Epoch 4/10
19/19 - 2s - loss: 102.4830 - loglik: -1.0095e+02 - logprior: -1.2587e+00
Epoch 5/10
19/19 - 2s - loss: 102.1924 - loglik: -1.0070e+02 - logprior: -1.2234e+00
Epoch 6/10
19/19 - 2s - loss: 101.5678 - loglik: -1.0012e+02 - logprior: -1.2041e+00
Epoch 7/10
19/19 - 2s - loss: 101.8014 - loglik: -1.0038e+02 - logprior: -1.1858e+00
Fitted a model with MAP estimate = -101.3132
Time for alignment: 52.4719
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 176.4039 - loglik: -1.7315e+02 - logprior: -3.2225e+00
Epoch 2/10
19/19 - 1s - loss: 133.6734 - loglik: -1.3219e+02 - logprior: -1.4591e+00
Epoch 3/10
19/19 - 2s - loss: 116.9352 - loglik: -1.1526e+02 - logprior: -1.4652e+00
Epoch 4/10
19/19 - 2s - loss: 114.1869 - loglik: -1.1243e+02 - logprior: -1.4659e+00
Epoch 5/10
19/19 - 2s - loss: 113.6954 - loglik: -1.1210e+02 - logprior: -1.4038e+00
Epoch 6/10
19/19 - 2s - loss: 113.1842 - loglik: -1.1162e+02 - logprior: -1.3772e+00
Epoch 7/10
19/19 - 2s - loss: 112.6783 - loglik: -1.1112e+02 - logprior: -1.3695e+00
Epoch 8/10
19/19 - 2s - loss: 112.4902 - loglik: -1.1095e+02 - logprior: -1.3614e+00
Epoch 9/10
19/19 - 2s - loss: 112.6883 - loglik: -1.1114e+02 - logprior: -1.3558e+00
Fitted a model with MAP estimate = -112.2546
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 114.1907 - loglik: -1.0987e+02 - logprior: -4.1497e+00
Epoch 2/2
19/19 - 2s - loss: 104.7305 - loglik: -1.0326e+02 - logprior: -1.2962e+00
Fitted a model with MAP estimate = -103.2096
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 109.6441 - loglik: -1.0557e+02 - logprior: -4.0213e+00
Epoch 2/2
19/19 - 2s - loss: 105.1349 - loglik: -1.0355e+02 - logprior: -1.5108e+00
Fitted a model with MAP estimate = -103.9764
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 107.6468 - loglik: -1.0438e+02 - logprior: -3.1650e+00
Epoch 2/10
19/19 - 2s - loss: 104.5569 - loglik: -1.0301e+02 - logprior: -1.3881e+00
Epoch 3/10
19/19 - 2s - loss: 103.3923 - loglik: -1.0186e+02 - logprior: -1.2937e+00
Epoch 4/10
19/19 - 2s - loss: 102.7489 - loglik: -1.0123e+02 - logprior: -1.2524e+00
Epoch 5/10
19/19 - 2s - loss: 102.3643 - loglik: -1.0088e+02 - logprior: -1.2160e+00
Epoch 6/10
19/19 - 2s - loss: 101.7099 - loglik: -1.0026e+02 - logprior: -1.2024e+00
Epoch 7/10
19/19 - 2s - loss: 101.6590 - loglik: -1.0023e+02 - logprior: -1.1837e+00
Epoch 8/10
19/19 - 2s - loss: 101.5583 - loglik: -1.0016e+02 - logprior: -1.1631e+00
Epoch 9/10
19/19 - 2s - loss: 101.5043 - loglik: -1.0011e+02 - logprior: -1.1420e+00
Epoch 10/10
19/19 - 2s - loss: 101.3410 - loglik: -9.9944e+01 - logprior: -1.1331e+00
Fitted a model with MAP estimate = -100.9094
Time for alignment: 60.0563
Computed alignments with likelihoods: ['-100.7072', '-101.3132', '-100.9094']
Best model has likelihood: -100.7072
time for generating output: 0.1374
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8080495356037152
Training of 3 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff85758ca30>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff83dac64f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff8574461f0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff83d91f160>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff50079c2b0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff50069a4c0>, <__main__.SimpleDirichletPrior object at 0x7ff66b558550>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 566.2578 - loglik: -5.6386e+02 - logprior: -1.8828e+00
Epoch 2/10
39/39 - 18s - loss: 479.8579 - loglik: -4.7758e+02 - logprior: -1.2730e+00
Epoch 3/10
39/39 - 18s - loss: 470.9712 - loglik: -4.6879e+02 - logprior: -1.1540e+00
Epoch 4/10
39/39 - 18s - loss: 468.7451 - loglik: -4.6667e+02 - logprior: -1.1058e+00
Epoch 5/10
39/39 - 18s - loss: 467.4340 - loglik: -4.6545e+02 - logprior: -1.0991e+00
Epoch 6/10
39/39 - 19s - loss: 466.9518 - loglik: -4.6507e+02 - logprior: -1.0983e+00
Epoch 7/10
39/39 - 18s - loss: 466.3269 - loglik: -4.6448e+02 - logprior: -1.0959e+00
Epoch 8/10
39/39 - 18s - loss: 465.7961 - loglik: -4.6394e+02 - logprior: -1.1063e+00
Epoch 9/10
39/39 - 19s - loss: 466.3637 - loglik: -4.6452e+02 - logprior: -1.1044e+00
Fitted a model with MAP estimate = -463.2387
expansions: [(25, 1), (54, 1), (56, 1), (76, 1), (82, 1), (83, 1), (101, 1), (102, 1), (106, 1), (138, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 473.5658 - loglik: -4.7119e+02 - logprior: -1.9646e+00
Epoch 2/2
39/39 - 19s - loss: 463.7559 - loglik: -4.6192e+02 - logprior: -9.3369e-01
Fitted a model with MAP estimate = -459.1713
expansions: []
discards: [149]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 469.7426 - loglik: -4.6778e+02 - logprior: -1.8154e+00
Epoch 2/2
39/39 - 19s - loss: 463.8248 - loglik: -4.6260e+02 - logprior: -7.3723e-01
Fitted a model with MAP estimate = -459.8073
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 471.6799 - loglik: -4.6891e+02 - logprior: -2.5830e+00
Epoch 2/10
39/39 - 20s - loss: 464.5458 - loglik: -4.6262e+02 - logprior: -1.2640e+00
Epoch 3/10
39/39 - 20s - loss: 460.9245 - loglik: -4.5945e+02 - logprior: -4.8250e-01
Epoch 4/10
39/39 - 20s - loss: 459.2076 - loglik: -4.5785e+02 - logprior: -4.1442e-01
Epoch 5/10
39/39 - 20s - loss: 458.5867 - loglik: -4.5743e+02 - logprior: -3.1871e-01
Epoch 6/10
39/39 - 20s - loss: 457.9811 - loglik: -4.5695e+02 - logprior: -2.4329e-01
Epoch 7/10
39/39 - 20s - loss: 457.7790 - loglik: -4.5687e+02 - logprior: -1.5995e-01
Epoch 8/10
39/39 - 20s - loss: 456.5736 - loglik: -4.5576e+02 - logprior: -8.7857e-02
Epoch 9/10
39/39 - 20s - loss: 456.5913 - loglik: -4.5589e+02 - logprior: 0.0066
Fitted a model with MAP estimate = -455.8943
Time for alignment: 515.1789
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 567.8317 - loglik: -5.6557e+02 - logprior: -1.8786e+00
Epoch 2/10
39/39 - 19s - loss: 482.3430 - loglik: -4.8011e+02 - logprior: -1.2953e+00
Epoch 3/10
39/39 - 19s - loss: 472.9779 - loglik: -4.7075e+02 - logprior: -1.2609e+00
Epoch 4/10
39/39 - 19s - loss: 470.9412 - loglik: -4.6882e+02 - logprior: -1.2870e+00
Epoch 5/10
39/39 - 19s - loss: 469.5688 - loglik: -4.6744e+02 - logprior: -1.3395e+00
Epoch 6/10
39/39 - 19s - loss: 469.0066 - loglik: -4.6688e+02 - logprior: -1.3675e+00
Epoch 7/10
39/39 - 19s - loss: 467.8117 - loglik: -4.6567e+02 - logprior: -1.4031e+00
Epoch 8/10
39/39 - 19s - loss: 467.4438 - loglik: -4.6533e+02 - logprior: -1.4050e+00
Epoch 9/10
39/39 - 18s - loss: 467.2442 - loglik: -4.6517e+02 - logprior: -1.3981e+00
Epoch 10/10
39/39 - 19s - loss: 466.9802 - loglik: -4.6494e+02 - logprior: -1.3935e+00
Fitted a model with MAP estimate = -464.4231
expansions: [(20, 1), (52, 1), (56, 1), (57, 1), (82, 1), (102, 1), (104, 1), (107, 1), (138, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 479.1037 - loglik: -4.7582e+02 - logprior: -3.0528e+00
Epoch 2/2
39/39 - 20s - loss: 467.2201 - loglik: -4.6523e+02 - logprior: -1.4543e+00
Fitted a model with MAP estimate = -462.0077
expansions: [(0, 1), (146, 1)]
discards: [  0 136]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 469.3453 - loglik: -4.6713e+02 - logprior: -1.7781e+00
Epoch 2/2
39/39 - 20s - loss: 464.4454 - loglik: -4.6269e+02 - logprior: -9.0380e-01
Fitted a model with MAP estimate = -460.3306
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 467.8645 - loglik: -4.6593e+02 - logprior: -1.7271e+00
Epoch 2/10
39/39 - 20s - loss: 463.0415 - loglik: -4.6171e+02 - logprior: -7.2910e-01
Epoch 3/10
39/39 - 20s - loss: 461.0963 - loglik: -4.5954e+02 - logprior: -7.0046e-01
Epoch 4/10
39/39 - 20s - loss: 459.4163 - loglik: -4.5783e+02 - logprior: -6.5207e-01
Epoch 5/10
39/39 - 20s - loss: 459.2813 - loglik: -4.5778e+02 - logprior: -6.0283e-01
Epoch 6/10
39/39 - 20s - loss: 459.3733 - loglik: -4.5802e+02 - logprior: -5.3687e-01
Fitted a model with MAP estimate = -457.7576
Time for alignment: 486.4561
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 567.6826 - loglik: -5.6531e+02 - logprior: -1.8621e+00
Epoch 2/10
39/39 - 19s - loss: 482.4187 - loglik: -4.8012e+02 - logprior: -1.2601e+00
Epoch 3/10
39/39 - 19s - loss: 475.0622 - loglik: -4.7303e+02 - logprior: -1.1239e+00
Epoch 4/10
39/39 - 19s - loss: 473.0226 - loglik: -4.7105e+02 - logprior: -1.0811e+00
Epoch 5/10
39/39 - 19s - loss: 471.8944 - loglik: -4.7002e+02 - logprior: -1.0678e+00
Epoch 6/10
39/39 - 19s - loss: 471.4224 - loglik: -4.6961e+02 - logprior: -1.0684e+00
Epoch 7/10
39/39 - 19s - loss: 471.7832 - loglik: -4.7000e+02 - logprior: -1.0666e+00
Fitted a model with MAP estimate = -468.6157
expansions: [(29, 1), (30, 3), (36, 1), (78, 3), (80, 1), (81, 1), (82, 2), (101, 1), (102, 1), (105, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 476.6456 - loglik: -4.7443e+02 - logprior: -2.0192e+00
Epoch 2/2
39/39 - 20s - loss: 464.0172 - loglik: -4.6247e+02 - logprior: -1.0442e+00
Fitted a model with MAP estimate = -459.5813
expansions: [(92, 1)]
discards: [ 0 84 85]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 472.5349 - loglik: -4.6956e+02 - logprior: -2.8024e+00
Epoch 2/2
39/39 - 20s - loss: 464.9030 - loglik: -4.6320e+02 - logprior: -1.2915e+00
Fitted a model with MAP estimate = -459.6202
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 466.4688 - loglik: -4.6455e+02 - logprior: -1.7364e+00
Epoch 2/10
39/39 - 21s - loss: 461.0685 - loglik: -4.5976e+02 - logprior: -6.6581e-01
Epoch 3/10
39/39 - 21s - loss: 457.8864 - loglik: -4.5643e+02 - logprior: -5.0695e-01
Epoch 4/10
39/39 - 21s - loss: 457.3356 - loglik: -4.5604e+02 - logprior: -4.0774e-01
Epoch 5/10
39/39 - 21s - loss: 456.3148 - loglik: -4.5522e+02 - logprior: -3.2628e-01
Epoch 6/10
39/39 - 21s - loss: 455.9272 - loglik: -4.5496e+02 - logprior: -2.4083e-01
Epoch 7/10
39/39 - 21s - loss: 455.8140 - loglik: -4.5498e+02 - logprior: -1.6564e-01
Epoch 8/10
39/39 - 21s - loss: 454.9756 - loglik: -4.5424e+02 - logprior: -8.6648e-02
Epoch 9/10
39/39 - 21s - loss: 454.8357 - loglik: -4.5419e+02 - logprior: -6.8348e-03
Epoch 10/10
39/39 - 21s - loss: 455.1979 - loglik: -4.5467e+02 - logprior: 0.0732
Fitted a model with MAP estimate = -454.1197
Time for alignment: 522.8229
Computed alignments with likelihoods: ['-455.8943', '-457.7576', '-454.1197']
Best model has likelihood: -454.1197
time for generating output: 0.2724
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7793139526193857
Training of 3 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff687e7a460>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff834b8f700>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff7cafa1250>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff846034df0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff86fb41340>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff834c69940>, <__main__.SimpleDirichletPrior object at 0x7ff67fc11e50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.7485 - loglik: -1.3951e+02 - logprior: -3.2187e+00
Epoch 2/10
19/19 - 1s - loss: 113.3946 - loglik: -1.1191e+02 - logprior: -1.4365e+00
Epoch 3/10
19/19 - 1s - loss: 103.3563 - loglik: -1.0146e+02 - logprior: -1.6186e+00
Epoch 4/10
19/19 - 1s - loss: 101.5859 - loglik: -9.9870e+01 - logprior: -1.4723e+00
Epoch 5/10
19/19 - 1s - loss: 101.0234 - loglik: -9.9368e+01 - logprior: -1.4537e+00
Epoch 6/10
19/19 - 1s - loss: 100.6843 - loglik: -9.9062e+01 - logprior: -1.4392e+00
Epoch 7/10
19/19 - 1s - loss: 100.4072 - loglik: -9.8809e+01 - logprior: -1.4271e+00
Epoch 8/10
19/19 - 1s - loss: 100.2904 - loglik: -9.8704e+01 - logprior: -1.4207e+00
Epoch 9/10
19/19 - 1s - loss: 100.1904 - loglik: -9.8612e+01 - logprior: -1.4132e+00
Epoch 10/10
19/19 - 1s - loss: 100.3272 - loglik: -9.8745e+01 - logprior: -1.4078e+00
Fitted a model with MAP estimate = -99.9576
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 108.0527 - loglik: -1.0378e+02 - logprior: -4.1717e+00
Epoch 2/2
19/19 - 1s - loss: 99.7911 - loglik: -9.7339e+01 - logprior: -2.1866e+00
Fitted a model with MAP estimate = -97.7684
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 44]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.0198 - loglik: -9.7741e+01 - logprior: -3.2141e+00
Epoch 2/2
19/19 - 1s - loss: 96.6441 - loglik: -9.5047e+01 - logprior: -1.4693e+00
Fitted a model with MAP estimate = -95.9705
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 99.6840 - loglik: -9.6307e+01 - logprior: -3.2470e+00
Epoch 2/10
19/19 - 1s - loss: 96.8900 - loglik: -9.5279e+01 - logprior: -1.4445e+00
Epoch 3/10
19/19 - 1s - loss: 96.3610 - loglik: -9.4783e+01 - logprior: -1.3517e+00
Epoch 4/10
19/19 - 1s - loss: 96.0496 - loglik: -9.4515e+01 - logprior: -1.3051e+00
Epoch 5/10
19/19 - 1s - loss: 95.6863 - loglik: -9.4208e+01 - logprior: -1.2679e+00
Epoch 6/10
19/19 - 1s - loss: 95.8226 - loglik: -9.4357e+01 - logprior: -1.2521e+00
Fitted a model with MAP estimate = -95.3890
Time for alignment: 50.6193
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.9342 - loglik: -1.3968e+02 - logprior: -3.2182e+00
Epoch 2/10
19/19 - 1s - loss: 114.3466 - loglik: -1.1284e+02 - logprior: -1.4343e+00
Epoch 3/10
19/19 - 1s - loss: 104.8735 - loglik: -1.0302e+02 - logprior: -1.5998e+00
Epoch 4/10
19/19 - 1s - loss: 102.4820 - loglik: -1.0082e+02 - logprior: -1.4500e+00
Epoch 5/10
19/19 - 1s - loss: 102.0008 - loglik: -1.0038e+02 - logprior: -1.4274e+00
Epoch 6/10
19/19 - 1s - loss: 101.5282 - loglik: -9.9937e+01 - logprior: -1.4066e+00
Epoch 7/10
19/19 - 1s - loss: 101.5783 - loglik: -1.0001e+02 - logprior: -1.3916e+00
Fitted a model with MAP estimate = -101.0704
expansions: [(6, 1), (7, 2), (12, 2), (13, 1), (20, 2), (21, 3), (28, 3), (29, 1), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 108.8275 - loglik: -1.0458e+02 - logprior: -4.1749e+00
Epoch 2/2
19/19 - 1s - loss: 99.6480 - loglik: -9.7443e+01 - logprior: -2.1429e+00
Fitted a model with MAP estimate = -97.7414
expansions: [(0, 1)]
discards: [ 0  8 14 26 29 40 41 45]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.3536 - loglik: -9.8084e+01 - logprior: -3.2013e+00
Epoch 2/2
19/19 - 1s - loss: 97.3114 - loglik: -9.5807e+01 - logprior: -1.4552e+00
Fitted a model with MAP estimate = -96.6930
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 99.5469 - loglik: -9.6249e+01 - logprior: -3.2504e+00
Epoch 2/10
19/19 - 1s - loss: 96.9827 - loglik: -9.5391e+01 - logprior: -1.4506e+00
Epoch 3/10
19/19 - 1s - loss: 96.4020 - loglik: -9.4828e+01 - logprior: -1.3613e+00
Epoch 4/10
19/19 - 1s - loss: 95.9312 - loglik: -9.4403e+01 - logprior: -1.3129e+00
Epoch 5/10
19/19 - 1s - loss: 95.8619 - loglik: -9.4371e+01 - logprior: -1.2841e+00
Epoch 6/10
19/19 - 1s - loss: 95.5654 - loglik: -9.4092e+01 - logprior: -1.2671e+00
Epoch 7/10
19/19 - 1s - loss: 95.7394 - loglik: -9.4292e+01 - logprior: -1.2489e+00
Fitted a model with MAP estimate = -95.2563
Time for alignment: 46.6026
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 142.7784 - loglik: -1.3954e+02 - logprior: -3.2172e+00
Epoch 2/10
19/19 - 1s - loss: 113.9884 - loglik: -1.1246e+02 - logprior: -1.4374e+00
Epoch 3/10
19/19 - 1s - loss: 103.6345 - loglik: -1.0171e+02 - logprior: -1.6092e+00
Epoch 4/10
19/19 - 1s - loss: 101.5873 - loglik: -9.9886e+01 - logprior: -1.4712e+00
Epoch 5/10
19/19 - 1s - loss: 100.9763 - loglik: -9.9319e+01 - logprior: -1.4578e+00
Epoch 6/10
19/19 - 1s - loss: 100.6623 - loglik: -9.9044e+01 - logprior: -1.4424e+00
Epoch 7/10
19/19 - 1s - loss: 100.4355 - loglik: -9.8832e+01 - logprior: -1.4284e+00
Epoch 8/10
19/19 - 1s - loss: 100.3796 - loglik: -9.8793e+01 - logprior: -1.4215e+00
Epoch 9/10
19/19 - 1s - loss: 100.3063 - loglik: -9.8723e+01 - logprior: -1.4124e+00
Epoch 10/10
19/19 - 1s - loss: 100.1018 - loglik: -9.8519e+01 - logprior: -1.4086e+00
Fitted a model with MAP estimate = -99.9384
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 108.3663 - loglik: -1.0401e+02 - logprior: -4.1724e+00
Epoch 2/2
19/19 - 1s - loss: 99.8173 - loglik: -9.7387e+01 - logprior: -2.1825e+00
Fitted a model with MAP estimate = -97.8402
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 44]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.5693 - loglik: -9.7131e+01 - logprior: -3.2137e+00
Epoch 2/2
19/19 - 1s - loss: 96.7154 - loglik: -9.4998e+01 - logprior: -1.4745e+00
Fitted a model with MAP estimate = -95.8131
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 100.3867 - loglik: -9.7103e+01 - logprior: -3.2498e+00
Epoch 2/10
19/19 - 1s - loss: 97.1242 - loglik: -9.5644e+01 - logprior: -1.4340e+00
Epoch 3/10
19/19 - 1s - loss: 96.4236 - loglik: -9.4955e+01 - logprior: -1.3459e+00
Epoch 4/10
19/19 - 1s - loss: 96.1245 - loglik: -9.4645e+01 - logprior: -1.2987e+00
Epoch 5/10
19/19 - 1s - loss: 95.9170 - loglik: -9.4451e+01 - logprior: -1.2651e+00
Epoch 6/10
19/19 - 1s - loss: 95.7005 - loglik: -9.4256e+01 - logprior: -1.2501e+00
Epoch 7/10
19/19 - 1s - loss: 95.5854 - loglik: -9.4153e+01 - logprior: -1.2346e+00
Epoch 8/10
19/19 - 1s - loss: 95.5248 - loglik: -9.4105e+01 - logprior: -1.2177e+00
Epoch 9/10
19/19 - 1s - loss: 95.4329 - loglik: -9.4019e+01 - logprior: -1.2050e+00
Epoch 10/10
19/19 - 1s - loss: 95.2938 - loglik: -9.3885e+01 - logprior: -1.1933e+00
Fitted a model with MAP estimate = -95.1100
Time for alignment: 54.8784
Computed alignments with likelihoods: ['-95.3890', '-95.2563', '-95.1100']
Best model has likelihood: -95.1100
time for generating output: 0.1207
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.778104335047759
Training of 3 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff66cc6f4c0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff82c378c10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff5000e78e0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66b91a3a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66b910eb0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff66cdbe5e0>, <__main__.SimpleDirichletPrior object at 0x7ff82c21ae80>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 458.1251 - loglik: -4.5513e+02 - logprior: -2.8679e+00
Epoch 2/10
19/19 - 6s - loss: 379.6769 - loglik: -3.7793e+02 - logprior: -1.3916e+00
Epoch 3/10
19/19 - 6s - loss: 348.7760 - loglik: -3.4641e+02 - logprior: -1.7484e+00
Epoch 4/10
19/19 - 7s - loss: 341.9351 - loglik: -3.3948e+02 - logprior: -1.9164e+00
Epoch 5/10
19/19 - 7s - loss: 339.1138 - loglik: -3.3680e+02 - logprior: -1.8686e+00
Epoch 6/10
19/19 - 7s - loss: 337.7555 - loglik: -3.3551e+02 - logprior: -1.8234e+00
Epoch 7/10
19/19 - 7s - loss: 337.0594 - loglik: -3.3488e+02 - logprior: -1.7842e+00
Epoch 8/10
19/19 - 7s - loss: 336.0914 - loglik: -3.3392e+02 - logprior: -1.7666e+00
Epoch 9/10
19/19 - 7s - loss: 336.5971 - loglik: -3.3444e+02 - logprior: -1.7637e+00
Fitted a model with MAP estimate = -332.1480
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (65, 1), (66, 1), (68, 1), (71, 2), (80, 1), (87, 2), (88, 1), (96, 1), (99, 1), (100, 1), (111, 1), (113, 2), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (136, 1), (137, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 326.7165 - loglik: -3.2417e+02 - logprior: -2.4039e+00
Epoch 2/2
39/39 - 12s - loss: 312.0719 - loglik: -3.1075e+02 - logprior: -9.7045e-01
Fitted a model with MAP estimate = -306.8131
expansions: []
discards: [  0  54  93 114 145]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 318.7672 - loglik: -3.1579e+02 - logprior: -2.7840e+00
Epoch 2/2
39/39 - 11s - loss: 312.5146 - loglik: -3.1122e+02 - logprior: -9.0219e-01
Fitted a model with MAP estimate = -307.2008
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 313.7506 - loglik: -3.1194e+02 - logprior: -1.7060e+00
Epoch 2/10
39/39 - 11s - loss: 309.5313 - loglik: -3.0863e+02 - logprior: -6.1905e-01
Epoch 3/10
39/39 - 11s - loss: 307.0953 - loglik: -3.0611e+02 - logprior: -5.4216e-01
Epoch 4/10
39/39 - 11s - loss: 305.7527 - loglik: -3.0475e+02 - logprior: -4.9365e-01
Epoch 5/10
39/39 - 11s - loss: 305.3479 - loglik: -3.0442e+02 - logprior: -4.1191e-01
Epoch 6/10
39/39 - 11s - loss: 304.7042 - loglik: -3.0388e+02 - logprior: -3.4085e-01
Epoch 7/10
39/39 - 11s - loss: 304.8395 - loglik: -3.0411e+02 - logprior: -2.5986e-01
Fitted a model with MAP estimate = -303.5836
Time for alignment: 253.9599
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 457.9880 - loglik: -4.5507e+02 - logprior: -2.8635e+00
Epoch 2/10
19/19 - 7s - loss: 378.3336 - loglik: -3.7669e+02 - logprior: -1.3948e+00
Epoch 3/10
19/19 - 7s - loss: 346.5551 - loglik: -3.4426e+02 - logprior: -1.8015e+00
Epoch 4/10
19/19 - 7s - loss: 340.5628 - loglik: -3.3829e+02 - logprior: -1.8429e+00
Epoch 5/10
19/19 - 7s - loss: 337.9938 - loglik: -3.3584e+02 - logprior: -1.7404e+00
Epoch 6/10
19/19 - 7s - loss: 335.9987 - loglik: -3.3390e+02 - logprior: -1.7037e+00
Epoch 7/10
19/19 - 7s - loss: 336.0778 - loglik: -3.3403e+02 - logprior: -1.6710e+00
Fitted a model with MAP estimate = -331.7173
expansions: [(0, 2), (14, 1), (16, 2), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (30, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (66, 1), (69, 1), (70, 1), (71, 1), (80, 1), (87, 1), (88, 1), (89, 2), (99, 1), (100, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 325.8686 - loglik: -3.2336e+02 - logprior: -2.3712e+00
Epoch 2/2
39/39 - 12s - loss: 312.3603 - loglik: -3.1102e+02 - logprior: -9.9919e-01
Fitted a model with MAP estimate = -307.0835
expansions: []
discards: [  0  21 116]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 318.4911 - loglik: -3.1564e+02 - logprior: -2.7544e+00
Epoch 2/2
39/39 - 11s - loss: 312.5420 - loglik: -3.1142e+02 - logprior: -8.1466e-01
Fitted a model with MAP estimate = -307.4199
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 314.4035 - loglik: -3.1247e+02 - logprior: -1.8278e+00
Epoch 2/10
39/39 - 11s - loss: 309.7926 - loglik: -3.0896e+02 - logprior: -5.1967e-01
Epoch 3/10
39/39 - 11s - loss: 307.2298 - loglik: -3.0625e+02 - logprior: -4.7011e-01
Epoch 4/10
39/39 - 11s - loss: 306.3342 - loglik: -3.0536e+02 - logprior: -4.1099e-01
Epoch 5/10
39/39 - 11s - loss: 304.8430 - loglik: -3.0397e+02 - logprior: -3.3114e-01
Epoch 6/10
39/39 - 11s - loss: 305.5601 - loglik: -3.0479e+02 - logprior: -2.6381e-01
Fitted a model with MAP estimate = -303.9774
Time for alignment: 228.4392
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 458.2667 - loglik: -4.5528e+02 - logprior: -2.8728e+00
Epoch 2/10
19/19 - 7s - loss: 381.0510 - loglik: -3.7930e+02 - logprior: -1.3969e+00
Epoch 3/10
19/19 - 7s - loss: 346.4771 - loglik: -3.4403e+02 - logprior: -1.8418e+00
Epoch 4/10
19/19 - 7s - loss: 337.5089 - loglik: -3.3500e+02 - logprior: -1.9898e+00
Epoch 5/10
19/19 - 7s - loss: 335.6856 - loglik: -3.3332e+02 - logprior: -1.8927e+00
Epoch 6/10
19/19 - 7s - loss: 334.1577 - loglik: -3.3187e+02 - logprior: -1.8616e+00
Epoch 7/10
19/19 - 7s - loss: 334.7980 - loglik: -3.3255e+02 - logprior: -1.8363e+00
Fitted a model with MAP estimate = -330.2319
expansions: [(0, 3), (11, 1), (14, 1), (15, 1), (16, 1), (18, 1), (19, 1), (21, 1), (22, 1), (23, 1), (38, 1), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (58, 1), (66, 1), (68, 1), (70, 1), (71, 1), (80, 2), (87, 1), (88, 1), (89, 2), (99, 1), (100, 1), (111, 1), (113, 1), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (136, 1), (137, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 323.9749 - loglik: -3.2152e+02 - logprior: -2.2909e+00
Epoch 2/2
39/39 - 11s - loss: 310.0056 - loglik: -3.0877e+02 - logprior: -9.2676e-01
Fitted a model with MAP estimate = -304.7373
expansions: []
discards: [  1   2 103 116]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 313.9311 - loglik: -3.1207e+02 - logprior: -1.6485e+00
Epoch 2/2
39/39 - 11s - loss: 309.7709 - loglik: -3.0855e+02 - logprior: -7.6071e-01
Fitted a model with MAP estimate = -304.5577
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 311.9490 - loglik: -3.1022e+02 - logprior: -1.6159e+00
Epoch 2/10
39/39 - 11s - loss: 307.1125 - loglik: -3.0620e+02 - logprior: -5.9315e-01
Epoch 3/10
39/39 - 11s - loss: 304.6632 - loglik: -3.0361e+02 - logprior: -5.4080e-01
Epoch 4/10
39/39 - 11s - loss: 303.6920 - loglik: -3.0267e+02 - logprior: -4.6810e-01
Epoch 5/10
39/39 - 11s - loss: 302.7061 - loglik: -3.0178e+02 - logprior: -4.0206e-01
Epoch 6/10
39/39 - 11s - loss: 302.3023 - loglik: -3.0147e+02 - logprior: -3.4379e-01
Epoch 7/10
39/39 - 11s - loss: 302.2535 - loglik: -3.0151e+02 - logprior: -2.7887e-01
Epoch 8/10
39/39 - 11s - loss: 301.4027 - loglik: -3.0073e+02 - logprior: -2.1162e-01
Epoch 9/10
39/39 - 11s - loss: 301.6470 - loglik: -3.0106e+02 - logprior: -1.4577e-01
Fitted a model with MAP estimate = -300.6137
Time for alignment: 258.4909
Computed alignments with likelihoods: ['-303.5836', '-303.9774', '-300.6137']
Best model has likelihood: -300.6137
time for generating output: 0.3795
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.3528986610794387
Training of 3 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff7ca924880>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff4e85ce400>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67f9f13d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff67f9f1430>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff676116b20>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff66dcaf5b0>, <__main__.SimpleDirichletPrior object at 0x7ff67f584f70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 56s - loss: 903.1028 - loglik: -9.0134e+02 - logprior: -1.4111e+00
Epoch 2/10
39/39 - 55s - loss: 720.2598 - loglik: -7.1697e+02 - logprior: -1.8578e+00
Epoch 3/10
39/39 - 55s - loss: 702.8113 - loglik: -6.9940e+02 - logprior: -1.9274e+00
Epoch 4/10
39/39 - 57s - loss: 698.4749 - loglik: -6.9520e+02 - logprior: -1.8794e+00
Epoch 5/10
39/39 - 58s - loss: 695.8018 - loglik: -6.9265e+02 - logprior: -1.9052e+00
Epoch 6/10
39/39 - 54s - loss: 694.3309 - loglik: -6.9124e+02 - logprior: -1.9549e+00
Epoch 7/10
39/39 - 52s - loss: 693.6118 - loglik: -6.9052e+02 - logprior: -2.0579e+00
Epoch 8/10
39/39 - 53s - loss: 691.9074 - loglik: -6.8904e+02 - logprior: -1.9177e+00
Epoch 9/10
39/39 - 51s - loss: 692.9999 - loglik: -6.9019e+02 - logprior: -1.9137e+00
Fitted a model with MAP estimate = -690.3246
expansions: [(0, 4), (46, 2), (64, 2), (68, 1), (74, 1), (75, 1), (81, 1), (82, 1), (85, 1), (86, 3), (92, 1), (93, 1), (94, 1), (114, 3), (119, 1), (123, 2), (126, 2), (142, 1), (143, 1), (145, 1), (147, 1), (150, 1), (156, 1), (159, 1), (161, 1), (163, 1), (164, 1), (165, 1), (166, 1), (169, 1), (187, 1), (189, 2), (190, 1), (191, 1), (195, 1), (206, 1), (208, 2), (209, 2), (210, 3), (228, 1), (229, 1), (230, 1), (231, 3), (237, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 2), (261, 1), (277, 1), (279, 1), (280, 1), (282, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 384 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 82s - loss: 686.4214 - loglik: -6.8413e+02 - logprior: -2.1515e+00
Epoch 2/2
39/39 - 85s - loss: 654.2510 - loglik: -6.5261e+02 - logprior: -8.1725e-01
Fitted a model with MAP estimate = -646.4218
expansions: []
discards: [  0   2 100 147 152 231 259 260 325]
Re-initialized the encoder parameters.
Fitting a model of length 375 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 82s - loss: 665.6279 - loglik: -6.6348e+02 - logprior: -1.9795e+00
Epoch 2/2
39/39 - 75s - loss: 654.4658 - loglik: -6.5372e+02 - logprior: -2.0638e-01
Fitted a model with MAP estimate = -648.3584
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 79s - loss: 661.8589 - loglik: -6.6048e+02 - logprior: -1.2082e+00
Epoch 2/10
39/39 - 75s - loss: 652.3832 - loglik: -6.5177e+02 - logprior: -4.9883e-02
Epoch 3/10
39/39 - 81s - loss: 646.3144 - loglik: -6.4548e+02 - logprior: 0.1209
Epoch 4/10
39/39 - 77s - loss: 642.8802 - loglik: -6.4195e+02 - logprior: 0.3300
Epoch 5/10
39/39 - 64s - loss: 641.2015 - loglik: -6.4035e+02 - logprior: 0.4907
Epoch 6/10
39/39 - 62s - loss: 640.6302 - loglik: -6.4001e+02 - logprior: 0.6714
Epoch 7/10
39/39 - 61s - loss: 639.6483 - loglik: -6.3930e+02 - logprior: 0.8520
Epoch 8/10
39/39 - 61s - loss: 638.2971 - loglik: -6.3819e+02 - logprior: 1.0101
Epoch 9/10
39/39 - 61s - loss: 638.8311 - loglik: -6.3896e+02 - logprior: 1.1658
Fitted a model with MAP estimate = -636.5893
Time for alignment: 1802.5709
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 904.2881 - loglik: -9.0248e+02 - logprior: -1.4134e+00
Epoch 2/10
39/39 - 43s - loss: 718.6197 - loglik: -7.1509e+02 - logprior: -2.0111e+00
Epoch 3/10
39/39 - 44s - loss: 700.5880 - loglik: -6.9706e+02 - logprior: -2.0572e+00
Epoch 4/10
39/39 - 43s - loss: 696.4478 - loglik: -6.9307e+02 - logprior: -1.9917e+00
Epoch 5/10
39/39 - 43s - loss: 693.9526 - loglik: -6.9073e+02 - logprior: -1.9584e+00
Epoch 6/10
39/39 - 44s - loss: 692.8684 - loglik: -6.8977e+02 - logprior: -1.9506e+00
Epoch 7/10
39/39 - 46s - loss: 691.8311 - loglik: -6.8884e+02 - logprior: -1.9585e+00
Epoch 8/10
39/39 - 49s - loss: 692.0594 - loglik: -6.8903e+02 - logprior: -2.0741e+00
Fitted a model with MAP estimate = -689.4865
expansions: [(0, 5), (25, 1), (45, 2), (60, 1), (62, 1), (66, 1), (67, 1), (73, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (103, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (141, 1), (145, 1), (147, 1), (149, 1), (151, 1), (155, 1), (161, 2), (162, 2), (163, 1), (165, 1), (166, 1), (173, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (229, 1), (230, 1), (231, 3), (232, 1), (256, 4), (258, 2), (259, 2), (260, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 386 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 87s - loss: 686.3519 - loglik: -6.8390e+02 - logprior: -2.3282e+00
Epoch 2/2
39/39 - 83s - loss: 653.3643 - loglik: -6.5153e+02 - logprior: -1.0291e+00
Fitted a model with MAP estimate = -645.1677
expansions: []
discards: [  1   2   3 148 153 200 233 324 365 366]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 666.3647 - loglik: -6.6482e+02 - logprior: -1.4505e+00
Epoch 2/2
39/39 - 75s - loss: 652.6855 - loglik: -6.5156e+02 - logprior: -3.4097e-01
Fitted a model with MAP estimate = -645.4017
expansions: []
discards: [99]
Re-initialized the encoder parameters.
Fitting a model of length 375 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 78s - loss: 661.0381 - loglik: -6.5947e+02 - logprior: -1.3336e+00
Epoch 2/10
39/39 - 84s - loss: 652.5418 - loglik: -6.5195e+02 - logprior: -6.0886e-02
Epoch 3/10
39/39 - 88s - loss: 645.9135 - loglik: -6.4508e+02 - logprior: 0.0831
Epoch 4/10
39/39 - 88s - loss: 642.7313 - loglik: -6.4177e+02 - logprior: 0.2346
Epoch 5/10
39/39 - 87s - loss: 641.0726 - loglik: -6.4016e+02 - logprior: 0.3745
Epoch 6/10
39/39 - 86s - loss: 640.5094 - loglik: -6.3957e+02 - logprior: 0.2960
Epoch 7/10
39/39 - 82s - loss: 638.5157 - loglik: -6.3789e+02 - logprior: 0.5186
Epoch 8/10
39/39 - 82s - loss: 638.5171 - loglik: -6.3814e+02 - logprior: 0.6958
Fitted a model with MAP estimate = -636.7845
Time for alignment: 1723.8449
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 903.8727 - loglik: -9.0204e+02 - logprior: -1.4224e+00
Epoch 2/10
39/39 - 58s - loss: 716.5527 - loglik: -7.1305e+02 - logprior: -2.0230e+00
Epoch 3/10
39/39 - 53s - loss: 700.3596 - loglik: -6.9688e+02 - logprior: -2.0570e+00
Epoch 4/10
39/39 - 53s - loss: 695.8760 - loglik: -6.9260e+02 - logprior: -1.9696e+00
Epoch 5/10
39/39 - 57s - loss: 693.4799 - loglik: -6.9022e+02 - logprior: -2.0862e+00
Epoch 6/10
39/39 - 56s - loss: 692.5002 - loglik: -6.8935e+02 - logprior: -2.0687e+00
Epoch 7/10
39/39 - 49s - loss: 691.3221 - loglik: -6.8837e+02 - logprior: -1.9761e+00
Epoch 8/10
39/39 - 52s - loss: 690.8069 - loglik: -6.8781e+02 - logprior: -2.0730e+00
Epoch 9/10
39/39 - 58s - loss: 691.0643 - loglik: -6.8818e+02 - logprior: -2.0183e+00
Fitted a model with MAP estimate = -688.7674
expansions: [(0, 4), (24, 1), (46, 1), (60, 1), (62, 2), (66, 1), (67, 1), (72, 1), (73, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (102, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (141, 1), (144, 1), (146, 1), (149, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 3), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (228, 1), (230, 1), (231, 3), (232, 1), (256, 4), (258, 2), (259, 2), (260, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 384 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 681.4056 - loglik: -6.7895e+02 - logprior: -2.2177e+00
Epoch 2/2
39/39 - 90s - loss: 652.6153 - loglik: -6.5101e+02 - logprior: -8.0630e-01
Fitted a model with MAP estimate = -645.0507
expansions: []
discards: [  0 101 147 152 231 322 363 364]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 90s - loss: 665.0718 - loglik: -6.6259e+02 - logprior: -2.2439e+00
Epoch 2/2
39/39 - 87s - loss: 652.6094 - loglik: -6.5170e+02 - logprior: -1.9062e-01
Fitted a model with MAP estimate = -646.9024
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 87s - loss: 661.9295 - loglik: -6.6057e+02 - logprior: -1.1826e+00
Epoch 2/10
39/39 - 86s - loss: 652.2172 - loglik: -6.5168e+02 - logprior: 0.0209
Epoch 3/10
39/39 - 83s - loss: 646.1946 - loglik: -6.4542e+02 - logprior: 0.1365
Epoch 4/10
39/39 - 86s - loss: 643.1133 - loglik: -6.4207e+02 - logprior: 0.1529
Epoch 5/10
39/39 - 85s - loss: 641.3115 - loglik: -6.4034e+02 - logprior: 0.3526
Epoch 6/10
39/39 - 77s - loss: 639.1737 - loglik: -6.3833e+02 - logprior: 0.4530
Epoch 7/10
39/39 - 82s - loss: 639.3549 - loglik: -6.3875e+02 - logprior: 0.6132
Fitted a model with MAP estimate = -636.8699
Time for alignment: 1831.0348
Computed alignments with likelihoods: ['-636.5893', '-636.7845', '-636.8699']
Best model has likelihood: -636.5893
time for generating output: 0.4423
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.9537855844942459
Training of 3 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4e878e730>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff67fc18b50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66c1ed910>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff676cab160>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff7ca924880>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff82c22a280>, <__main__.SimpleDirichletPrior object at 0x7ff66af74f10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 643.4876 - loglik: -6.4101e+02 - logprior: -1.8459e+00
Epoch 2/10
39/39 - 20s - loss: 551.9802 - loglik: -5.4903e+02 - logprior: -1.5617e+00
Epoch 3/10
39/39 - 20s - loss: 542.8207 - loglik: -5.3964e+02 - logprior: -1.6750e+00
Epoch 4/10
39/39 - 20s - loss: 539.8177 - loglik: -5.3660e+02 - logprior: -1.6916e+00
Epoch 5/10
39/39 - 20s - loss: 538.4271 - loglik: -5.3526e+02 - logprior: -1.7115e+00
Epoch 6/10
39/39 - 21s - loss: 537.0515 - loglik: -5.3392e+02 - logprior: -1.7380e+00
Epoch 7/10
39/39 - 22s - loss: 536.2415 - loglik: -5.3316e+02 - logprior: -1.7501e+00
Epoch 8/10
39/39 - 22s - loss: 535.9907 - loglik: -5.3294e+02 - logprior: -1.7775e+00
Epoch 9/10
39/39 - 23s - loss: 535.0560 - loglik: -5.3204e+02 - logprior: -1.7863e+00
Epoch 10/10
39/39 - 22s - loss: 534.3298 - loglik: -5.3136e+02 - logprior: -1.7879e+00
Fitted a model with MAP estimate = -532.6058
expansions: [(4, 1), (6, 1), (31, 1), (32, 1), (78, 1), (81, 3), (82, 6), (86, 2), (87, 3), (89, 2), (113, 1), (114, 8), (115, 2), (116, 1), (117, 1), (124, 1), (127, 5), (129, 2), (130, 2), (135, 1), (137, 1), (140, 1), (141, 9), (144, 2), (145, 2), (149, 1), (154, 1), (155, 1), (157, 2), (158, 4), (159, 1), (186, 1)]
discards: [  0 160 161 162 163 164 165 166 167 168 169 175 176 177 178 179 180 181
 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204
 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 569.0952 - loglik: -5.6565e+02 - logprior: -2.8036e+00
Epoch 2/2
39/39 - 24s - loss: 544.1931 - loglik: -5.4180e+02 - logprior: -1.2886e+00
Fitted a model with MAP estimate = -537.8605
expansions: [(80, 1), (230, 1), (232, 1), (240, 17)]
discards: [  0  81 102 136 137 138 139 158 159 160 164 165 170 195 196 201 206 207
 208 209 210 211 228 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 549.8807 - loglik: -5.4654e+02 - logprior: -2.6714e+00
Epoch 2/2
39/39 - 26s - loss: 536.9747 - loglik: -5.3471e+02 - logprior: -1.0752e+00
Fitted a model with MAP estimate = -531.8563
expansions: [(0, 2), (183, 1), (235, 3)]
discards: [  0  80 191 215 227 228 229 230 231 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 547.7720 - loglik: -5.4583e+02 - logprior: -1.6563e+00
Epoch 2/10
39/39 - 26s - loss: 539.3690 - loglik: -5.3826e+02 - logprior: -4.0440e-01
Epoch 3/10
39/39 - 27s - loss: 536.5582 - loglik: -5.3521e+02 - logprior: -2.8204e-01
Epoch 4/10
39/39 - 26s - loss: 534.2349 - loglik: -5.3265e+02 - logprior: -2.1199e-01
Epoch 5/10
39/39 - 27s - loss: 532.8483 - loglik: -5.3116e+02 - logprior: -1.4492e-01
Epoch 6/10
39/39 - 26s - loss: 531.3680 - loglik: -5.2970e+02 - logprior: -7.2047e-02
Epoch 7/10
39/39 - 27s - loss: 530.8403 - loglik: -5.2926e+02 - logprior: 0.0022
Epoch 8/10
39/39 - 27s - loss: 529.1455 - loglik: -5.2769e+02 - logprior: 0.0760
Epoch 9/10
39/39 - 26s - loss: 528.8534 - loglik: -5.2755e+02 - logprior: 0.1524
Epoch 10/10
39/39 - 25s - loss: 527.9564 - loglik: -5.2682e+02 - logprior: 0.2454
Fitted a model with MAP estimate = -526.1194
Time for alignment: 698.6338
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 644.9730 - loglik: -6.4284e+02 - logprior: -1.8786e+00
Epoch 2/10
39/39 - 20s - loss: 550.8931 - loglik: -5.4820e+02 - logprior: -1.6594e+00
Epoch 3/10
39/39 - 20s - loss: 541.8894 - loglik: -5.3888e+02 - logprior: -1.7700e+00
Epoch 4/10
39/39 - 19s - loss: 539.7514 - loglik: -5.3668e+02 - logprior: -1.7743e+00
Epoch 5/10
39/39 - 19s - loss: 537.9817 - loglik: -5.3485e+02 - logprior: -1.7894e+00
Epoch 6/10
39/39 - 19s - loss: 536.7726 - loglik: -5.3361e+02 - logprior: -1.8231e+00
Epoch 7/10
39/39 - 19s - loss: 536.2270 - loglik: -5.3310e+02 - logprior: -1.8325e+00
Epoch 8/10
39/39 - 19s - loss: 535.4177 - loglik: -5.3236e+02 - logprior: -1.8419e+00
Epoch 9/10
39/39 - 19s - loss: 534.7327 - loglik: -5.3173e+02 - logprior: -1.8589e+00
Epoch 10/10
39/39 - 19s - loss: 534.3784 - loglik: -5.3142e+02 - logprior: -1.8681e+00
Fitted a model with MAP estimate = -532.1300
expansions: [(4, 1), (6, 1), (31, 1), (34, 1), (78, 1), (80, 10), (83, 1), (86, 1), (87, 1), (88, 1), (91, 1), (111, 1), (112, 8), (113, 2), (114, 1), (115, 2), (127, 4), (128, 2), (131, 1), (132, 1), (135, 1), (137, 1), (139, 9), (145, 1), (152, 1), (154, 1), (155, 1), (158, 2), (159, 1), (160, 2), (171, 6), (186, 1)]
discards: [  0 161 162 163 164 165 166 167 168 169 175 176 177 178 179 180 181 187
 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 570.4897 - loglik: -5.6742e+02 - logprior: -2.7657e+00
Epoch 2/2
39/39 - 23s - loss: 543.2953 - loglik: -5.4117e+02 - logprior: -1.3884e+00
Fitted a model with MAP estimate = -537.9338
expansions: [(4, 1), (80, 1), (88, 1), (239, 28)]
discards: [  0   1  81  82  83  84 133 134 135 136 163 179 180 181 204 205 206 207
 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225
 226 227 228 229 230 231 232 233 234 235 236 237 238]
Re-initialized the encoder parameters.
Fitting a model of length 221 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 561.4093 - loglik: -5.5873e+02 - logprior: -2.4636e+00
Epoch 2/2
39/39 - 20s - loss: 545.8913 - loglik: -5.4435e+02 - logprior: -1.0097e+00
Fitted a model with MAP estimate = -540.3786
expansions: [(3, 1), (82, 1), (83, 2), (203, 5), (204, 3), (207, 1), (208, 1), (221, 9)]
discards: [  0 130 193 209 210 211 212 213 214 215 216 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 550.9548 - loglik: -5.4823e+02 - logprior: -2.5379e+00
Epoch 2/10
39/39 - 22s - loss: 538.0994 - loglik: -5.3633e+02 - logprior: -7.5801e-01
Epoch 3/10
39/39 - 21s - loss: 534.2645 - loglik: -5.3221e+02 - logprior: -4.2853e-01
Epoch 4/10
39/39 - 21s - loss: 531.9011 - loglik: -5.2989e+02 - logprior: -3.8843e-01
Epoch 5/10
39/39 - 21s - loss: 530.8531 - loglik: -5.2889e+02 - logprior: -3.4911e-01
Epoch 6/10
39/39 - 21s - loss: 529.3843 - loglik: -5.2753e+02 - logprior: -2.9418e-01
Epoch 7/10
39/39 - 21s - loss: 527.9951 - loglik: -5.2619e+02 - logprior: -2.5574e-01
Epoch 8/10
39/39 - 21s - loss: 527.0072 - loglik: -5.2534e+02 - logprior: -2.0410e-01
Epoch 9/10
39/39 - 21s - loss: 526.7983 - loglik: -5.2526e+02 - logprior: -1.5805e-01
Epoch 10/10
39/39 - 21s - loss: 525.5199 - loglik: -5.2413e+02 - logprior: -1.0137e-01
Fitted a model with MAP estimate = -523.6235
Time for alignment: 605.9468
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 645.7404 - loglik: -6.4354e+02 - logprior: -1.8734e+00
Epoch 2/10
39/39 - 19s - loss: 552.0389 - loglik: -5.4927e+02 - logprior: -1.6339e+00
Epoch 3/10
39/39 - 19s - loss: 541.3184 - loglik: -5.3822e+02 - logprior: -1.7299e+00
Epoch 4/10
39/39 - 19s - loss: 539.4883 - loglik: -5.3630e+02 - logprior: -1.7590e+00
Epoch 5/10
39/39 - 19s - loss: 537.3853 - loglik: -5.3422e+02 - logprior: -1.7741e+00
Epoch 6/10
39/39 - 19s - loss: 536.2508 - loglik: -5.3306e+02 - logprior: -1.7964e+00
Epoch 7/10
39/39 - 19s - loss: 535.1565 - loglik: -5.3202e+02 - logprior: -1.8152e+00
Epoch 8/10
39/39 - 19s - loss: 534.7307 - loglik: -5.3168e+02 - logprior: -1.8238e+00
Epoch 9/10
39/39 - 19s - loss: 533.7737 - loglik: -5.3078e+02 - logprior: -1.8451e+00
Epoch 10/10
39/39 - 19s - loss: 533.8556 - loglik: -5.3092e+02 - logprior: -1.8481e+00
Fitted a model with MAP estimate = -531.8211
expansions: [(4, 1), (6, 1), (31, 1), (32, 1), (51, 1), (81, 8), (91, 1), (118, 1), (119, 8), (120, 2), (121, 1), (122, 1), (129, 1), (131, 2), (132, 7), (133, 2), (136, 1), (137, 1), (139, 1), (140, 1), (141, 2), (142, 10), (145, 2), (146, 2), (153, 1), (156, 1), (159, 5), (160, 1), (161, 1), (173, 1)]
discards: [  0 150 162 163 164 165 166 167 168 169 170 175 176 177 178 179 180 181
 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199
 200 201 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 573.4541 - loglik: -5.7029e+02 - logprior: -2.8220e+00
Epoch 2/2
39/39 - 21s - loss: 549.1837 - loglik: -5.4710e+02 - logprior: -1.2917e+00
Fitted a model with MAP estimate = -543.2787
expansions: [(229, 1), (233, 19)]
discards: [  0  79  80  81  82  83  84  85 134 135 136 137 156 157 159 164 192 195
 196 201 202 221 227]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 555.1373 - loglik: -5.5216e+02 - logprior: -2.6347e+00
Epoch 2/2
39/39 - 21s - loss: 542.9278 - loglik: -5.4108e+02 - logprior: -1.0788e+00
Fitted a model with MAP estimate = -538.2088
expansions: [(0, 2), (82, 1), (83, 5), (152, 4), (182, 1), (209, 17)]
discards: [  0 126 218 219 220 221 222 223 224 225 226 227 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 543.1277 - loglik: -5.4112e+02 - logprior: -1.5243e+00
Epoch 2/10
39/39 - 24s - loss: 532.2457 - loglik: -5.3097e+02 - logprior: -2.8209e-01
Epoch 3/10
39/39 - 24s - loss: 529.0640 - loglik: -5.2747e+02 - logprior: -1.9507e-01
Epoch 4/10
39/39 - 23s - loss: 526.3829 - loglik: -5.2459e+02 - logprior: -1.3064e-01
Epoch 5/10
39/39 - 23s - loss: 524.7465 - loglik: -5.2289e+02 - logprior: -7.0852e-02
Epoch 6/10
39/39 - 22s - loss: 523.1857 - loglik: -5.2140e+02 - logprior: -2.5431e-02
Epoch 7/10
39/39 - 22s - loss: 522.4795 - loglik: -5.2078e+02 - logprior: 0.0153
Epoch 8/10
39/39 - 22s - loss: 520.8640 - loglik: -5.1932e+02 - logprior: 0.0847
Epoch 9/10
39/39 - 22s - loss: 520.1212 - loglik: -5.1876e+02 - logprior: 0.1307
Epoch 10/10
39/39 - 22s - loss: 519.5814 - loglik: -5.1841e+02 - logprior: 0.2060
Fitted a model with MAP estimate = -517.6225
Time for alignment: 614.7395
Computed alignments with likelihoods: ['-526.1194', '-523.6235', '-517.6225']
Best model has likelihood: -517.6225
time for generating output: 0.3273
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.89910455290705
Training of 3 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff698559b50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff83db511f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff82c10fd30>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66b760b50>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff4e86cba60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fee88eafd60>, <__main__.SimpleDirichletPrior object at 0x7ff698a0b0d0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 270.1318 - loglik: -2.6697e+02 - logprior: -3.1325e+00
Epoch 2/10
19/19 - 3s - loss: 199.8667 - loglik: -1.9849e+02 - logprior: -1.3450e+00
Epoch 3/10
19/19 - 3s - loss: 176.1154 - loglik: -1.7425e+02 - logprior: -1.7296e+00
Epoch 4/10
19/19 - 3s - loss: 172.3997 - loglik: -1.7049e+02 - logprior: -1.6750e+00
Epoch 5/10
19/19 - 3s - loss: 170.7321 - loglik: -1.6892e+02 - logprior: -1.6148e+00
Epoch 6/10
19/19 - 3s - loss: 169.8901 - loglik: -1.6807e+02 - logprior: -1.6062e+00
Epoch 7/10
19/19 - 3s - loss: 169.5978 - loglik: -1.6780e+02 - logprior: -1.5911e+00
Epoch 8/10
19/19 - 3s - loss: 169.5983 - loglik: -1.6782e+02 - logprior: -1.5833e+00
Fitted a model with MAP estimate = -169.0197
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (44, 1), (46, 3), (47, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 169.1925 - loglik: -1.6516e+02 - logprior: -3.9590e+00
Epoch 2/2
19/19 - 3s - loss: 156.4148 - loglik: -1.5429e+02 - logprior: -2.0368e+00
Fitted a model with MAP estimate = -154.1741
expansions: [(0, 2)]
discards: [ 0 42 60]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 155.9996 - loglik: -1.5297e+02 - logprior: -2.9263e+00
Epoch 2/2
19/19 - 3s - loss: 151.8572 - loglik: -1.5053e+02 - logprior: -1.1412e+00
Fitted a model with MAP estimate = -150.6167
expansions: []
discards: [ 0 63]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 158.8426 - loglik: -1.5494e+02 - logprior: -3.8574e+00
Epoch 2/10
19/19 - 3s - loss: 153.6120 - loglik: -1.5196e+02 - logprior: -1.4722e+00
Epoch 3/10
19/19 - 3s - loss: 151.5629 - loglik: -1.5027e+02 - logprior: -1.0448e+00
Epoch 4/10
19/19 - 3s - loss: 150.9013 - loglik: -1.4960e+02 - logprior: -1.0193e+00
Epoch 5/10
19/19 - 3s - loss: 150.1945 - loglik: -1.4888e+02 - logprior: -1.0270e+00
Epoch 6/10
19/19 - 3s - loss: 149.9244 - loglik: -1.4866e+02 - logprior: -1.0038e+00
Epoch 7/10
19/19 - 3s - loss: 149.6712 - loglik: -1.4843e+02 - logprior: -9.8678e-01
Epoch 8/10
19/19 - 3s - loss: 149.3340 - loglik: -1.4813e+02 - logprior: -9.5302e-01
Epoch 9/10
19/19 - 3s - loss: 149.6910 - loglik: -1.4852e+02 - logprior: -9.3048e-01
Fitted a model with MAP estimate = -149.0681
Time for alignment: 99.7318
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 270.0021 - loglik: -2.6680e+02 - logprior: -3.1304e+00
Epoch 2/10
19/19 - 3s - loss: 199.9050 - loglik: -1.9850e+02 - logprior: -1.3472e+00
Epoch 3/10
19/19 - 3s - loss: 176.1543 - loglik: -1.7424e+02 - logprior: -1.7168e+00
Epoch 4/10
19/19 - 3s - loss: 172.6019 - loglik: -1.7071e+02 - logprior: -1.6586e+00
Epoch 5/10
19/19 - 3s - loss: 171.5511 - loglik: -1.6977e+02 - logprior: -1.6017e+00
Epoch 6/10
19/19 - 3s - loss: 170.5269 - loglik: -1.6875e+02 - logprior: -1.6044e+00
Epoch 7/10
19/19 - 3s - loss: 170.1877 - loglik: -1.6841e+02 - logprior: -1.5963e+00
Epoch 8/10
19/19 - 3s - loss: 169.9156 - loglik: -1.6816e+02 - logprior: -1.5825e+00
Epoch 9/10
19/19 - 3s - loss: 170.1332 - loglik: -1.6837e+02 - logprior: -1.5860e+00
Fitted a model with MAP estimate = -169.3840
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 168.9124 - loglik: -1.6491e+02 - logprior: -3.9612e+00
Epoch 2/2
19/19 - 3s - loss: 156.4369 - loglik: -1.5421e+02 - logprior: -2.0634e+00
Fitted a model with MAP estimate = -154.1370
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 156.3881 - loglik: -1.5334e+02 - logprior: -2.9296e+00
Epoch 2/2
19/19 - 3s - loss: 152.1733 - loglik: -1.5082e+02 - logprior: -1.1289e+00
Fitted a model with MAP estimate = -150.9198
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.2254 - loglik: -1.5532e+02 - logprior: -3.8414e+00
Epoch 2/10
19/19 - 3s - loss: 153.9285 - loglik: -1.5235e+02 - logprior: -1.4625e+00
Epoch 3/10
19/19 - 3s - loss: 151.5348 - loglik: -1.5032e+02 - logprior: -1.0394e+00
Epoch 4/10
19/19 - 3s - loss: 151.2887 - loglik: -1.5002e+02 - logprior: -1.0299e+00
Epoch 5/10
19/19 - 3s - loss: 150.4824 - loglik: -1.4921e+02 - logprior: -1.0132e+00
Epoch 6/10
19/19 - 3s - loss: 150.1481 - loglik: -1.4888e+02 - logprior: -1.0125e+00
Epoch 7/10
19/19 - 3s - loss: 149.9222 - loglik: -1.4869e+02 - logprior: -9.7797e-01
Epoch 8/10
19/19 - 3s - loss: 149.3706 - loglik: -1.4817e+02 - logprior: -9.5981e-01
Epoch 9/10
19/19 - 3s - loss: 149.5249 - loglik: -1.4834e+02 - logprior: -9.4372e-01
Fitted a model with MAP estimate = -149.2303
Time for alignment: 102.3928
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 270.1586 - loglik: -2.6698e+02 - logprior: -3.1334e+00
Epoch 2/10
19/19 - 3s - loss: 200.2404 - loglik: -1.9885e+02 - logprior: -1.3465e+00
Epoch 3/10
19/19 - 3s - loss: 175.8100 - loglik: -1.7388e+02 - logprior: -1.7273e+00
Epoch 4/10
19/19 - 3s - loss: 171.6591 - loglik: -1.6971e+02 - logprior: -1.6908e+00
Epoch 5/10
19/19 - 3s - loss: 170.4520 - loglik: -1.6865e+02 - logprior: -1.6139e+00
Epoch 6/10
19/19 - 3s - loss: 169.6897 - loglik: -1.6792e+02 - logprior: -1.6055e+00
Epoch 7/10
19/19 - 3s - loss: 169.6962 - loglik: -1.6794e+02 - logprior: -1.5878e+00
Fitted a model with MAP estimate = -169.0952
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 164.7589 - loglik: -1.6165e+02 - logprior: -2.9847e+00
Epoch 2/2
19/19 - 3s - loss: 153.3571 - loglik: -1.5195e+02 - logprior: -1.2137e+00
Fitted a model with MAP estimate = -151.8187
expansions: []
discards: [42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 155.9977 - loglik: -1.5298e+02 - logprior: -2.9341e+00
Epoch 2/2
19/19 - 3s - loss: 152.5893 - loglik: -1.5130e+02 - logprior: -1.1350e+00
Fitted a model with MAP estimate = -151.5693
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 159.3402 - loglik: -1.5527e+02 - logprior: -3.9220e+00
Epoch 2/10
19/19 - 3s - loss: 154.3728 - loglik: -1.5188e+02 - logprior: -2.2417e+00
Epoch 3/10
19/19 - 3s - loss: 153.0006 - loglik: -1.5065e+02 - logprior: -2.0807e+00
Epoch 4/10
19/19 - 3s - loss: 152.1550 - loglik: -1.5026e+02 - logprior: -1.6047e+00
Epoch 5/10
19/19 - 3s - loss: 151.3994 - loglik: -1.5004e+02 - logprior: -1.0774e+00
Epoch 6/10
19/19 - 3s - loss: 150.7866 - loglik: -1.4945e+02 - logprior: -1.0824e+00
Epoch 7/10
19/19 - 3s - loss: 150.9416 - loglik: -1.4964e+02 - logprior: -1.0431e+00
Fitted a model with MAP estimate = -150.2747
Time for alignment: 89.7379
Computed alignments with likelihoods: ['-149.0681', '-149.2303', '-150.2747']
Best model has likelihood: -149.0681
time for generating output: 0.1449
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9313475177304964
Training of 3 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff66bb10190>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff687d541f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fee90760af0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff698a0bd30>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff52447ab50>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff675dc5430>, <__main__.SimpleDirichletPrior object at 0x7ff66c151280>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 232.0690 - loglik: -2.2889e+02 - logprior: -3.1320e+00
Epoch 2/10
19/19 - 2s - loss: 196.5399 - loglik: -1.9509e+02 - logprior: -1.3211e+00
Epoch 3/10
19/19 - 2s - loss: 183.9140 - loglik: -1.8218e+02 - logprior: -1.4004e+00
Epoch 4/10
19/19 - 2s - loss: 180.7758 - loglik: -1.7909e+02 - logprior: -1.3653e+00
Epoch 5/10
19/19 - 2s - loss: 179.5374 - loglik: -1.7790e+02 - logprior: -1.3553e+00
Epoch 6/10
19/19 - 2s - loss: 179.3037 - loglik: -1.7772e+02 - logprior: -1.3282e+00
Epoch 7/10
19/19 - 2s - loss: 178.9307 - loglik: -1.7737e+02 - logprior: -1.3271e+00
Epoch 8/10
19/19 - 2s - loss: 178.4452 - loglik: -1.7688e+02 - logprior: -1.3294e+00
Epoch 9/10
19/19 - 2s - loss: 178.5474 - loglik: -1.7699e+02 - logprior: -1.3195e+00
Fitted a model with MAP estimate = -178.0281
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (22, 1), (23, 1), (32, 1), (33, 2), (34, 1), (46, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.1113 - loglik: -1.7902e+02 - logprior: -3.9081e+00
Epoch 2/2
19/19 - 2s - loss: 175.4754 - loglik: -1.7303e+02 - logprior: -2.1232e+00
Fitted a model with MAP estimate = -173.2635
expansions: [(0, 2)]
discards: [ 0 44 72]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 176.5869 - loglik: -1.7361e+02 - logprior: -2.8973e+00
Epoch 2/2
19/19 - 2s - loss: 172.6295 - loglik: -1.7130e+02 - logprior: -1.1332e+00
Fitted a model with MAP estimate = -171.4098
expansions: [(8, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.8038 - loglik: -1.7408e+02 - logprior: -3.6990e+00
Epoch 2/10
19/19 - 2s - loss: 173.5517 - loglik: -1.7219e+02 - logprior: -1.2814e+00
Epoch 3/10
19/19 - 2s - loss: 171.3935 - loglik: -1.7014e+02 - logprior: -1.0558e+00
Epoch 4/10
19/19 - 2s - loss: 170.4127 - loglik: -1.6906e+02 - logprior: -1.0305e+00
Epoch 5/10
19/19 - 2s - loss: 170.3865 - loglik: -1.6903e+02 - logprior: -1.0071e+00
Epoch 6/10
19/19 - 2s - loss: 169.6082 - loglik: -1.6828e+02 - logprior: -9.7945e-01
Epoch 7/10
19/19 - 2s - loss: 169.5516 - loglik: -1.6825e+02 - logprior: -9.7324e-01
Epoch 8/10
19/19 - 2s - loss: 168.9019 - loglik: -1.6763e+02 - logprior: -9.4843e-01
Epoch 9/10
19/19 - 2s - loss: 169.4155 - loglik: -1.6817e+02 - logprior: -9.2932e-01
Fitted a model with MAP estimate = -168.7443
Time for alignment: 70.0809
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 231.8941 - loglik: -2.2873e+02 - logprior: -3.1306e+00
Epoch 2/10
19/19 - 2s - loss: 196.1918 - loglik: -1.9474e+02 - logprior: -1.3139e+00
Epoch 3/10
19/19 - 2s - loss: 183.3140 - loglik: -1.8155e+02 - logprior: -1.4144e+00
Epoch 4/10
19/19 - 2s - loss: 180.1984 - loglik: -1.7849e+02 - logprior: -1.3867e+00
Epoch 5/10
19/19 - 2s - loss: 179.6169 - loglik: -1.7797e+02 - logprior: -1.3634e+00
Epoch 6/10
19/19 - 2s - loss: 179.4318 - loglik: -1.7784e+02 - logprior: -1.3403e+00
Epoch 7/10
19/19 - 2s - loss: 178.8161 - loglik: -1.7726e+02 - logprior: -1.3298e+00
Epoch 8/10
19/19 - 2s - loss: 178.8596 - loglik: -1.7732e+02 - logprior: -1.3190e+00
Fitted a model with MAP estimate = -178.4233
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (32, 1), (33, 1), (35, 2), (37, 1), (49, 1), (50, 1), (53, 2), (56, 3), (57, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.5419 - loglik: -1.7957e+02 - logprior: -3.9060e+00
Epoch 2/2
19/19 - 2s - loss: 174.9986 - loglik: -1.7274e+02 - logprior: -2.1082e+00
Fitted a model with MAP estimate = -172.9657
expansions: [(0, 2)]
discards: [ 0 68 72 73]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 177.0568 - loglik: -1.7408e+02 - logprior: -2.8966e+00
Epoch 2/2
19/19 - 2s - loss: 172.9571 - loglik: -1.7173e+02 - logprior: -1.1162e+00
Fitted a model with MAP estimate = -171.6588
expansions: [(8, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 177.3657 - loglik: -1.7349e+02 - logprior: -3.6955e+00
Epoch 2/10
19/19 - 2s - loss: 172.9912 - loglik: -1.7149e+02 - logprior: -1.2810e+00
Epoch 3/10
19/19 - 2s - loss: 171.2744 - loglik: -1.6984e+02 - logprior: -1.0691e+00
Epoch 4/10
19/19 - 2s - loss: 170.6659 - loglik: -1.6923e+02 - logprior: -1.0378e+00
Epoch 5/10
19/19 - 2s - loss: 170.1057 - loglik: -1.6872e+02 - logprior: -1.0164e+00
Epoch 6/10
19/19 - 2s - loss: 169.6048 - loglik: -1.6827e+02 - logprior: -1.0013e+00
Epoch 7/10
19/19 - 2s - loss: 169.4729 - loglik: -1.6817e+02 - logprior: -9.8076e-01
Epoch 8/10
19/19 - 2s - loss: 169.6496 - loglik: -1.6837e+02 - logprior: -9.6867e-01
Fitted a model with MAP estimate = -168.9021
Time for alignment: 65.9420
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 231.8889 - loglik: -2.2870e+02 - logprior: -3.1329e+00
Epoch 2/10
19/19 - 2s - loss: 196.1464 - loglik: -1.9471e+02 - logprior: -1.3278e+00
Epoch 3/10
19/19 - 2s - loss: 184.2567 - loglik: -1.8251e+02 - logprior: -1.3965e+00
Epoch 4/10
19/19 - 2s - loss: 180.6978 - loglik: -1.7898e+02 - logprior: -1.3732e+00
Epoch 5/10
19/19 - 2s - loss: 179.6047 - loglik: -1.7794e+02 - logprior: -1.3617e+00
Epoch 6/10
19/19 - 2s - loss: 179.1207 - loglik: -1.7750e+02 - logprior: -1.3428e+00
Epoch 7/10
19/19 - 2s - loss: 178.6637 - loglik: -1.7708e+02 - logprior: -1.3347e+00
Epoch 8/10
19/19 - 2s - loss: 178.6203 - loglik: -1.7704e+02 - logprior: -1.3321e+00
Epoch 9/10
19/19 - 2s - loss: 178.4926 - loglik: -1.7691e+02 - logprior: -1.3292e+00
Epoch 10/10
19/19 - 2s - loss: 178.4571 - loglik: -1.7687e+02 - logprior: -1.3308e+00
Fitted a model with MAP estimate = -177.9395
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (22, 1), (23, 1), (31, 1), (34, 1), (35, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 183.2188 - loglik: -1.7907e+02 - logprior: -3.8989e+00
Epoch 2/2
19/19 - 2s - loss: 175.3577 - loglik: -1.7290e+02 - logprior: -2.1162e+00
Fitted a model with MAP estimate = -173.1802
expansions: [(0, 2)]
discards: [ 0 71]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 176.4559 - loglik: -1.7344e+02 - logprior: -2.8995e+00
Epoch 2/2
19/19 - 2s - loss: 172.5686 - loglik: -1.7120e+02 - logprior: -1.1336e+00
Fitted a model with MAP estimate = -171.3110
expansions: [(8, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 178.0356 - loglik: -1.7427e+02 - logprior: -3.7050e+00
Epoch 2/10
19/19 - 2s - loss: 173.3278 - loglik: -1.7199e+02 - logprior: -1.2768e+00
Epoch 3/10
19/19 - 2s - loss: 171.6948 - loglik: -1.7044e+02 - logprior: -1.0578e+00
Epoch 4/10
19/19 - 2s - loss: 170.7958 - loglik: -1.6945e+02 - logprior: -1.0262e+00
Epoch 5/10
19/19 - 2s - loss: 169.7277 - loglik: -1.6838e+02 - logprior: -1.0028e+00
Epoch 6/10
19/19 - 2s - loss: 169.8240 - loglik: -1.6850e+02 - logprior: -9.8085e-01
Fitted a model with MAP estimate = -169.1464
Time for alignment: 65.8521
Computed alignments with likelihoods: ['-168.7443', '-168.9021', '-169.1464']
Best model has likelihood: -168.7443
time for generating output: 0.1405
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.8326530612244898
Training of 3 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4983c5640>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66db90fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff86fb76190>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66d1c1b20>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff84eae0490>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff66b549be0>, <__main__.SimpleDirichletPrior object at 0x7ff857599790>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 641.3711 - loglik: -6.3936e+02 - logprior: -1.7682e+00
Epoch 2/10
39/39 - 19s - loss: 583.5446 - loglik: -5.8119e+02 - logprior: -1.2090e+00
Epoch 3/10
39/39 - 19s - loss: 573.4136 - loglik: -5.7024e+02 - logprior: -1.2715e+00
Epoch 4/10
39/39 - 19s - loss: 569.4036 - loglik: -5.6576e+02 - logprior: -1.2829e+00
Epoch 5/10
39/39 - 19s - loss: 565.9834 - loglik: -5.6212e+02 - logprior: -1.3244e+00
Epoch 6/10
39/39 - 20s - loss: 563.5192 - loglik: -5.5985e+02 - logprior: -1.3639e+00
Epoch 7/10
39/39 - 20s - loss: 561.8977 - loglik: -5.5856e+02 - logprior: -1.3781e+00
Epoch 8/10
39/39 - 20s - loss: 561.5602 - loglik: -5.5848e+02 - logprior: -1.3931e+00
Epoch 9/10
39/39 - 20s - loss: 560.4106 - loglik: -5.5756e+02 - logprior: -1.3997e+00
Epoch 10/10
39/39 - 20s - loss: 560.4139 - loglik: -5.5772e+02 - logprior: -1.3986e+00
Fitted a model with MAP estimate = -558.1400
expansions: [(23, 1), (24, 6), (44, 2), (48, 6), (52, 1), (61, 2), (62, 2), (64, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 2), (103, 2), (104, 4), (107, 1), (108, 1), (119, 1), (122, 1), (125, 1), (132, 1), (155, 8), (160, 1), (162, 1), (164, 1), (169, 1), (170, 1), (176, 3), (179, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 247 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 626.6452 - loglik: -6.2382e+02 - logprior: -2.4218e+00
Epoch 2/2
39/39 - 27s - loss: 576.2345 - loglik: -5.7414e+02 - logprior: -1.0135e+00
Fitted a model with MAP estimate = -566.2699
expansions: [(26, 3), (33, 1), (76, 1), (108, 2), (109, 1)]
discards: [ 29  30  31  80  81  82 102 104 106 130 131]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 593.3773 - loglik: -5.9145e+02 - logprior: -1.7036e+00
Epoch 2/2
39/39 - 27s - loss: 571.3340 - loglik: -5.6961e+02 - logprior: -3.9852e-01
Fitted a model with MAP estimate = -561.1746
expansions: []
discards: [ 26  27  28 106 226 227]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 586.7289 - loglik: -5.8503e+02 - logprior: -1.4439e+00
Epoch 2/10
39/39 - 25s - loss: 572.2563 - loglik: -5.7136e+02 - logprior: -2.2210e-01
Epoch 3/10
39/39 - 25s - loss: 563.2203 - loglik: -5.6178e+02 - logprior: -1.8827e-01
Epoch 4/10
39/39 - 26s - loss: 555.0333 - loglik: -5.5303e+02 - logprior: -1.7272e-01
Epoch 5/10
39/39 - 26s - loss: 549.7415 - loglik: -5.4725e+02 - logprior: -2.1755e-01
Epoch 6/10
39/39 - 26s - loss: 546.8943 - loglik: -5.4434e+02 - logprior: -2.0547e-01
Epoch 7/10
39/39 - 26s - loss: 545.3889 - loglik: -5.4297e+02 - logprior: -1.7664e-01
Epoch 8/10
39/39 - 26s - loss: 544.5406 - loglik: -5.4229e+02 - logprior: -1.2120e-01
Epoch 9/10
39/39 - 26s - loss: 542.6432 - loglik: -5.4042e+02 - logprior: -8.7896e-02
Epoch 10/10
39/39 - 25s - loss: 540.4601 - loglik: -5.3796e+02 - logprior: -7.1389e-02
Fitted a model with MAP estimate = -536.3704
Time for alignment: 673.8684
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 639.7869 - loglik: -6.3746e+02 - logprior: -1.7530e+00
Epoch 2/10
39/39 - 20s - loss: 582.7783 - loglik: -5.7996e+02 - logprior: -1.1848e+00
Epoch 3/10
39/39 - 20s - loss: 573.1945 - loglik: -5.6955e+02 - logprior: -1.2673e+00
Epoch 4/10
39/39 - 20s - loss: 568.8807 - loglik: -5.6492e+02 - logprior: -1.2763e+00
Epoch 5/10
39/39 - 20s - loss: 565.9108 - loglik: -5.6194e+02 - logprior: -1.3201e+00
Epoch 6/10
39/39 - 19s - loss: 564.1071 - loglik: -5.6045e+02 - logprior: -1.3522e+00
Epoch 7/10
39/39 - 19s - loss: 562.2953 - loglik: -5.5898e+02 - logprior: -1.3760e+00
Epoch 8/10
39/39 - 19s - loss: 561.6796 - loglik: -5.5862e+02 - logprior: -1.3868e+00
Epoch 9/10
39/39 - 19s - loss: 560.7131 - loglik: -5.5782e+02 - logprior: -1.3928e+00
Epoch 10/10
39/39 - 19s - loss: 559.9302 - loglik: -5.5711e+02 - logprior: -1.3997e+00
Fitted a model with MAP estimate = -557.8932
expansions: [(23, 1), (24, 6), (44, 2), (48, 5), (62, 2), (63, 1), (64, 1), (65, 1), (80, 1), (81, 1), (82, 1), (83, 1), (104, 2), (105, 4), (108, 1), (109, 1), (123, 1), (124, 1), (126, 1), (135, 1), (142, 2), (156, 8), (161, 1), (163, 1), (167, 1), (168, 1), (170, 1), (172, 1), (176, 3), (179, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 246 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 639.5497 - loglik: -6.3612e+02 - logprior: -3.1059e+00
Epoch 2/2
39/39 - 24s - loss: 578.4819 - loglik: -5.7517e+02 - logprior: -1.5204e+00
Fitted a model with MAP estimate = -564.1206
expansions: [(33, 1), (82, 1), (101, 1), (102, 1), (154, 1), (158, 1), (195, 1), (199, 1)]
discards: [ 23  29  60  78  80  99 103 126 127 155 159 178 179 200 201 202 203 206
 207]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 592.8758 - loglik: -5.9095e+02 - logprior: -1.6078e+00
Epoch 2/2
39/39 - 23s - loss: 571.4897 - loglik: -5.6983e+02 - logprior: -2.2843e-01
Fitted a model with MAP estimate = -562.4415
expansions: [(187, 1), (188, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 587.9658 - loglik: -5.8640e+02 - logprior: -1.3498e+00
Epoch 2/10
39/39 - 24s - loss: 572.1914 - loglik: -5.7150e+02 - logprior: -3.5277e-02
Epoch 3/10
39/39 - 24s - loss: 562.7995 - loglik: -5.6161e+02 - logprior: 0.0135
Epoch 4/10
39/39 - 24s - loss: 555.5509 - loglik: -5.5381e+02 - logprior: -2.6972e-02
Epoch 5/10
39/39 - 24s - loss: 549.2797 - loglik: -5.4705e+02 - logprior: -7.1834e-02
Epoch 6/10
39/39 - 24s - loss: 546.7576 - loglik: -5.4440e+02 - logprior: -6.1818e-02
Epoch 7/10
39/39 - 24s - loss: 545.4846 - loglik: -5.4323e+02 - logprior: -2.5608e-02
Epoch 8/10
39/39 - 24s - loss: 543.9410 - loglik: -5.4183e+02 - logprior: 0.0281
Epoch 9/10
39/39 - 24s - loss: 542.4009 - loglik: -5.4026e+02 - logprior: 0.0492
Epoch 10/10
39/39 - 24s - loss: 539.4474 - loglik: -5.3700e+02 - logprior: 0.0457
Fitted a model with MAP estimate = -535.5177
Time for alignment: 639.4596
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 641.2350 - loglik: -6.3926e+02 - logprior: -1.7784e+00
Epoch 2/10
39/39 - 19s - loss: 584.4239 - loglik: -5.8214e+02 - logprior: -1.2110e+00
Epoch 3/10
39/39 - 20s - loss: 573.0462 - loglik: -5.6984e+02 - logprior: -1.3063e+00
Epoch 4/10
39/39 - 20s - loss: 568.5018 - loglik: -5.6477e+02 - logprior: -1.3216e+00
Epoch 5/10
39/39 - 20s - loss: 564.7665 - loglik: -5.6082e+02 - logprior: -1.3680e+00
Epoch 6/10
39/39 - 21s - loss: 563.4026 - loglik: -5.5970e+02 - logprior: -1.3988e+00
Epoch 7/10
39/39 - 21s - loss: 561.7971 - loglik: -5.5847e+02 - logprior: -1.4058e+00
Epoch 8/10
39/39 - 21s - loss: 561.0938 - loglik: -5.5803e+02 - logprior: -1.4291e+00
Epoch 9/10
39/39 - 21s - loss: 560.3085 - loglik: -5.5747e+02 - logprior: -1.4182e+00
Epoch 10/10
39/39 - 21s - loss: 560.1453 - loglik: -5.5742e+02 - logprior: -1.4090e+00
Fitted a model with MAP estimate = -557.6196
expansions: [(22, 1), (24, 4), (27, 1), (45, 2), (48, 6), (55, 1), (58, 1), (60, 2), (61, 2), (63, 1), (66, 1), (78, 1), (79, 1), (80, 1), (81, 1), (102, 2), (103, 4), (106, 1), (107, 1), (122, 1), (125, 1), (132, 1), (154, 8), (159, 1), (161, 1), (167, 1), (168, 1), (169, 1), (175, 2)]
discards: [ 0 29]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 633.0133 - loglik: -6.2918e+02 - logprior: -3.1513e+00
Epoch 2/2
39/39 - 27s - loss: 578.4229 - loglik: -5.7470e+02 - logprior: -1.2240e+00
Fitted a model with MAP estimate = -564.0532
expansions: [(25, 1), (34, 2), (52, 1), (74, 1), (75, 1), (177, 1), (202, 1)]
discards: [ 28  31  59  78  79  80 126 127 196 225]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 597.1473 - loglik: -5.9482e+02 - logprior: -1.6835e+00
Epoch 2/2
39/39 - 27s - loss: 570.9835 - loglik: -5.6910e+02 - logprior: -2.3761e-01
Fitted a model with MAP estimate = -561.5932
expansions: [(31, 1), (34, 1), (61, 1), (223, 2)]
discards: [25 26 55]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 586.1058 - loglik: -5.8449e+02 - logprior: -1.3147e+00
Epoch 2/10
39/39 - 29s - loss: 570.9786 - loglik: -5.7016e+02 - logprior: -7.6180e-03
Epoch 3/10
39/39 - 29s - loss: 561.4463 - loglik: -5.6006e+02 - logprior: -2.6533e-06
Epoch 4/10
39/39 - 29s - loss: 554.1376 - loglik: -5.5215e+02 - logprior: -3.8286e-02
Epoch 5/10
39/39 - 28s - loss: 548.4135 - loglik: -5.4595e+02 - logprior: -7.7951e-02
Epoch 6/10
39/39 - 28s - loss: 545.9343 - loglik: -5.4344e+02 - logprior: -6.6569e-02
Epoch 7/10
39/39 - 28s - loss: 544.3630 - loglik: -5.4204e+02 - logprior: -3.3858e-02
Epoch 8/10
39/39 - 28s - loss: 543.5088 - loglik: -5.4140e+02 - logprior: 0.0338
Epoch 9/10
39/39 - 28s - loss: 542.0497 - loglik: -5.4003e+02 - logprior: 0.0659
Epoch 10/10
39/39 - 28s - loss: 540.3499 - loglik: -5.3822e+02 - logprior: 0.1072
Fitted a model with MAP estimate = -536.3877
Time for alignment: 710.7525
Computed alignments with likelihoods: ['-536.3704', '-535.5177', '-536.3877']
Best model has likelihood: -535.5177
time for generating output: 0.2755
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.6628228782287823
Training of 3 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff8574cbb50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff5243be550>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff7ce15d070>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66becd8b0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff8573c68e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7feebc15ffd0>, <__main__.SimpleDirichletPrior object at 0x7ff4f8125e50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.1278 - loglik: -1.5089e+02 - logprior: -3.2277e+00
Epoch 2/10
19/19 - 1s - loss: 123.6137 - loglik: -1.2209e+02 - logprior: -1.4839e+00
Epoch 3/10
19/19 - 1s - loss: 110.3873 - loglik: -1.0841e+02 - logprior: -1.6101e+00
Epoch 4/10
19/19 - 1s - loss: 107.2811 - loglik: -1.0534e+02 - logprior: -1.6364e+00
Epoch 5/10
19/19 - 1s - loss: 106.1011 - loglik: -1.0432e+02 - logprior: -1.5632e+00
Epoch 6/10
19/19 - 1s - loss: 106.1224 - loglik: -1.0436e+02 - logprior: -1.5655e+00
Fitted a model with MAP estimate = -105.5074
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 107.3828 - loglik: -1.0425e+02 - logprior: -3.0864e+00
Epoch 2/2
19/19 - 1s - loss: 99.6474 - loglik: -9.8186e+01 - logprior: -1.3487e+00
Fitted a model with MAP estimate = -98.4728
expansions: []
discards: [22 37 40]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.4259 - loglik: -9.8290e+01 - logprior: -2.9870e+00
Epoch 2/2
19/19 - 2s - loss: 98.8583 - loglik: -9.7452e+01 - logprior: -1.2151e+00
Fitted a model with MAP estimate = -97.9431
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 105.0049 - loglik: -1.0088e+02 - logprior: -4.0610e+00
Epoch 2/10
19/19 - 1s - loss: 101.6153 - loglik: -9.9231e+01 - logprior: -2.2189e+00
Epoch 3/10
19/19 - 1s - loss: 100.6974 - loglik: -9.8412e+01 - logprior: -2.0707e+00
Epoch 4/10
19/19 - 1s - loss: 98.9177 - loglik: -9.7247e+01 - logprior: -1.4401e+00
Epoch 5/10
19/19 - 1s - loss: 97.8347 - loglik: -9.6400e+01 - logprior: -1.1989e+00
Epoch 6/10
19/19 - 1s - loss: 97.7953 - loglik: -9.6383e+01 - logprior: -1.1901e+00
Epoch 7/10
19/19 - 1s - loss: 97.6049 - loglik: -9.6239e+01 - logprior: -1.1673e+00
Epoch 8/10
19/19 - 1s - loss: 97.1999 - loglik: -9.5858e+01 - logprior: -1.1517e+00
Epoch 9/10
19/19 - 1s - loss: 97.5969 - loglik: -9.6279e+01 - logprior: -1.1289e+00
Fitted a model with MAP estimate = -97.1049
Time for alignment: 52.2144
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 154.0372 - loglik: -1.5079e+02 - logprior: -3.2325e+00
Epoch 2/10
19/19 - 1s - loss: 124.3262 - loglik: -1.2278e+02 - logprior: -1.4996e+00
Epoch 3/10
19/19 - 1s - loss: 110.2396 - loglik: -1.0831e+02 - logprior: -1.5789e+00
Epoch 4/10
19/19 - 1s - loss: 106.8299 - loglik: -1.0493e+02 - logprior: -1.6439e+00
Epoch 5/10
19/19 - 1s - loss: 105.7400 - loglik: -1.0397e+02 - logprior: -1.5687e+00
Epoch 6/10
19/19 - 1s - loss: 105.3689 - loglik: -1.0363e+02 - logprior: -1.5677e+00
Epoch 7/10
19/19 - 1s - loss: 105.3356 - loglik: -1.0364e+02 - logprior: -1.5360e+00
Epoch 8/10
19/19 - 1s - loss: 104.9861 - loglik: -1.0330e+02 - logprior: -1.5305e+00
Epoch 9/10
19/19 - 1s - loss: 104.7782 - loglik: -1.0310e+02 - logprior: -1.5177e+00
Epoch 10/10
19/19 - 1s - loss: 105.1243 - loglik: -1.0345e+02 - logprior: -1.5153e+00
Fitted a model with MAP estimate = -104.5976
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (14, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.5492 - loglik: -1.0333e+02 - logprior: -3.0661e+00
Epoch 2/2
19/19 - 2s - loss: 99.4359 - loglik: -9.7917e+01 - logprior: -1.3161e+00
Fitted a model with MAP estimate = -98.3214
expansions: []
discards: [36 39]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 101.9197 - loglik: -9.8912e+01 - logprior: -2.9686e+00
Epoch 2/2
19/19 - 1s - loss: 98.9973 - loglik: -9.7729e+01 - logprior: -1.2065e+00
Fitted a model with MAP estimate = -98.2602
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 101.2936 - loglik: -9.8319e+01 - logprior: -2.9337e+00
Epoch 2/10
19/19 - 2s - loss: 98.8811 - loglik: -9.7615e+01 - logprior: -1.1776e+00
Epoch 3/10
19/19 - 1s - loss: 98.1434 - loglik: -9.6827e+01 - logprior: -1.1577e+00
Epoch 4/10
19/19 - 1s - loss: 97.5357 - loglik: -9.6204e+01 - logprior: -1.1082e+00
Epoch 5/10
19/19 - 1s - loss: 97.1936 - loglik: -9.5895e+01 - logprior: -1.0834e+00
Epoch 6/10
19/19 - 1s - loss: 96.8836 - loglik: -9.5612e+01 - logprior: -1.0699e+00
Epoch 7/10
19/19 - 1s - loss: 96.9583 - loglik: -9.5710e+01 - logprior: -1.0518e+00
Fitted a model with MAP estimate = -96.4835
Time for alignment: 55.2406
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 154.0358 - loglik: -1.5079e+02 - logprior: -3.2277e+00
Epoch 2/10
19/19 - 1s - loss: 123.4756 - loglik: -1.2192e+02 - logprior: -1.4974e+00
Epoch 3/10
19/19 - 1s - loss: 110.5407 - loglik: -1.0859e+02 - logprior: -1.5959e+00
Epoch 4/10
19/19 - 1s - loss: 107.2920 - loglik: -1.0540e+02 - logprior: -1.6281e+00
Epoch 5/10
19/19 - 1s - loss: 106.1281 - loglik: -1.0435e+02 - logprior: -1.5638e+00
Epoch 6/10
19/19 - 1s - loss: 105.9377 - loglik: -1.0419e+02 - logprior: -1.5599e+00
Epoch 7/10
19/19 - 1s - loss: 105.7991 - loglik: -1.0410e+02 - logprior: -1.5275e+00
Epoch 8/10
19/19 - 1s - loss: 105.4240 - loglik: -1.0373e+02 - logprior: -1.5197e+00
Epoch 9/10
19/19 - 1s - loss: 105.6401 - loglik: -1.0395e+02 - logprior: -1.5137e+00
Fitted a model with MAP estimate = -105.1663
expansions: [(3, 1), (5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 107.4003 - loglik: -1.0424e+02 - logprior: -3.0830e+00
Epoch 2/2
19/19 - 1s - loss: 99.5461 - loglik: -9.8010e+01 - logprior: -1.3516e+00
Fitted a model with MAP estimate = -98.3674
expansions: []
discards: [22 37 40]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 101.5233 - loglik: -9.8462e+01 - logprior: -2.9756e+00
Epoch 2/2
19/19 - 1s - loss: 98.7682 - loglik: -9.7403e+01 - logprior: -1.2144e+00
Fitted a model with MAP estimate = -98.0327
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 101.4356 - loglik: -9.8467e+01 - logprior: -2.9343e+00
Epoch 2/10
19/19 - 1s - loss: 98.7811 - loglik: -9.7520e+01 - logprior: -1.1856e+00
Epoch 3/10
19/19 - 1s - loss: 98.1050 - loglik: -9.6787e+01 - logprior: -1.1579e+00
Epoch 4/10
19/19 - 1s - loss: 97.6253 - loglik: -9.6297e+01 - logprior: -1.1086e+00
Epoch 5/10
19/19 - 1s - loss: 97.0638 - loglik: -9.5750e+01 - logprior: -1.0883e+00
Epoch 6/10
19/19 - 1s - loss: 96.9058 - loglik: -9.5618e+01 - logprior: -1.0672e+00
Epoch 7/10
19/19 - 1s - loss: 96.6805 - loglik: -9.5421e+01 - logprior: -1.0561e+00
Epoch 8/10
19/19 - 1s - loss: 96.7411 - loglik: -9.5510e+01 - logprior: -1.0378e+00
Fitted a model with MAP estimate = -96.3938
Time for alignment: 54.8193
Computed alignments with likelihoods: ['-97.1049', '-96.4835', '-96.3938']
Best model has likelihood: -96.3938
time for generating output: 0.1195
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9629629629629629
Training of 3 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7feebc247ac0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff4f812ccd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff478179a30>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee906d61c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff7ce15d070>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff66d033100>, <__main__.SimpleDirichletPrior object at 0x7ff687d1b130>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 365.7176 - loglik: -3.6265e+02 - logprior: -3.0050e+00
Epoch 2/10
19/19 - 4s - loss: 333.7261 - loglik: -3.3242e+02 - logprior: -1.0500e+00
Epoch 3/10
19/19 - 4s - loss: 319.2409 - loglik: -3.1720e+02 - logprior: -1.3035e+00
Epoch 4/10
19/19 - 4s - loss: 313.1931 - loglik: -3.1076e+02 - logprior: -1.2221e+00
Epoch 5/10
19/19 - 4s - loss: 310.8384 - loglik: -3.0828e+02 - logprior: -1.2436e+00
Epoch 6/10
19/19 - 4s - loss: 309.6812 - loglik: -3.0719e+02 - logprior: -1.2542e+00
Epoch 7/10
19/19 - 4s - loss: 309.0071 - loglik: -3.0664e+02 - logprior: -1.2680e+00
Epoch 8/10
19/19 - 4s - loss: 307.9207 - loglik: -3.0568e+02 - logprior: -1.2710e+00
Epoch 9/10
19/19 - 4s - loss: 307.8762 - loglik: -3.0574e+02 - logprior: -1.2750e+00
Epoch 10/10
19/19 - 4s - loss: 306.8123 - loglik: -3.0475e+02 - logprior: -1.2694e+00
Fitted a model with MAP estimate = -306.3206
expansions: [(15, 1), (17, 1), (19, 1), (21, 4), (23, 4), (25, 1), (28, 1), (55, 4), (73, 1), (74, 1), (78, 1), (82, 1), (84, 3), (86, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 354.0236 - loglik: -3.5098e+02 - logprior: -2.9812e+00
Epoch 2/2
19/19 - 5s - loss: 321.1085 - loglik: -3.1975e+02 - logprior: -1.1189e+00
Fitted a model with MAP estimate = -313.9742
expansions: [(22, 1), (26, 1), (95, 1)]
discards: [ 27  33 104]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 322.3799 - loglik: -3.1952e+02 - logprior: -2.8210e+00
Epoch 2/2
19/19 - 5s - loss: 313.7835 - loglik: -3.1261e+02 - logprior: -1.0066e+00
Fitted a model with MAP estimate = -310.4372
expansions: [(2, 1), (31, 2)]
discards: [ 0 34 97]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 321.0257 - loglik: -3.1729e+02 - logprior: -3.7109e+00
Epoch 2/10
19/19 - 5s - loss: 314.2944 - loglik: -3.1224e+02 - logprior: -1.8940e+00
Epoch 3/10
19/19 - 5s - loss: 309.5326 - loglik: -3.0736e+02 - logprior: -1.7141e+00
Epoch 4/10
19/19 - 5s - loss: 305.2353 - loglik: -3.0329e+02 - logprior: -1.1439e+00
Epoch 5/10
19/19 - 5s - loss: 301.4196 - loglik: -2.9957e+02 - logprior: -7.9003e-01
Epoch 6/10
19/19 - 5s - loss: 300.1282 - loglik: -2.9819e+02 - logprior: -7.8327e-01
Epoch 7/10
19/19 - 5s - loss: 298.8786 - loglik: -2.9692e+02 - logprior: -7.8306e-01
Epoch 8/10
19/19 - 5s - loss: 298.3194 - loglik: -2.9638e+02 - logprior: -7.7618e-01
Epoch 9/10
19/19 - 5s - loss: 297.1539 - loglik: -2.9524e+02 - logprior: -7.6923e-01
Epoch 10/10
19/19 - 5s - loss: 296.9185 - loglik: -2.9505e+02 - logprior: -7.6458e-01
Fitted a model with MAP estimate = -295.5025
Time for alignment: 152.1125
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 365.1083 - loglik: -3.6201e+02 - logprior: -3.0013e+00
Epoch 2/10
19/19 - 4s - loss: 334.0626 - loglik: -3.3255e+02 - logprior: -1.0591e+00
Epoch 3/10
19/19 - 4s - loss: 318.3766 - loglik: -3.1599e+02 - logprior: -1.3265e+00
Epoch 4/10
19/19 - 4s - loss: 313.4135 - loglik: -3.1067e+02 - logprior: -1.2528e+00
Epoch 5/10
19/19 - 4s - loss: 310.1893 - loglik: -3.0737e+02 - logprior: -1.2923e+00
Epoch 6/10
19/19 - 4s - loss: 309.1966 - loglik: -3.0656e+02 - logprior: -1.2899e+00
Epoch 7/10
19/19 - 4s - loss: 308.2917 - loglik: -3.0584e+02 - logprior: -1.2880e+00
Epoch 8/10
19/19 - 4s - loss: 308.0141 - loglik: -3.0572e+02 - logprior: -1.2913e+00
Epoch 9/10
19/19 - 4s - loss: 308.0134 - loglik: -3.0584e+02 - logprior: -1.2815e+00
Epoch 10/10
19/19 - 4s - loss: 307.0917 - loglik: -3.0499e+02 - logprior: -1.2860e+00
Fitted a model with MAP estimate = -306.2718
expansions: [(18, 1), (20, 2), (22, 4), (24, 2), (27, 1), (28, 1), (29, 2), (31, 1), (51, 2), (54, 4), (74, 1), (75, 4), (76, 1), (83, 1), (84, 3), (87, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 354.7797 - loglik: -3.5172e+02 - logprior: -2.9996e+00
Epoch 2/2
19/19 - 5s - loss: 320.5163 - loglik: -3.1911e+02 - logprior: -1.1328e+00
Fitted a model with MAP estimate = -312.9679
expansions: []
discards: [ 32 100 101 102 103 110]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 320.6863 - loglik: -3.1778e+02 - logprior: -2.8373e+00
Epoch 2/2
19/19 - 5s - loss: 312.6040 - loglik: -3.1134e+02 - logprior: -1.0232e+00
Fitted a model with MAP estimate = -309.4722
expansions: [(2, 1), (27, 2), (94, 3)]
discards: [ 0 37 65]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 324.7797 - loglik: -3.2086e+02 - logprior: -3.6833e+00
Epoch 2/10
19/19 - 5s - loss: 316.1838 - loglik: -3.1418e+02 - logprior: -1.8852e+00
Epoch 3/10
19/19 - 5s - loss: 310.0809 - loglik: -3.0779e+02 - logprior: -1.7128e+00
Epoch 4/10
19/19 - 5s - loss: 304.8963 - loglik: -3.0248e+02 - logprior: -1.1500e+00
Epoch 5/10
19/19 - 5s - loss: 302.0248 - loglik: -2.9981e+02 - logprior: -7.9586e-01
Epoch 6/10
19/19 - 5s - loss: 300.1362 - loglik: -2.9798e+02 - logprior: -7.8995e-01
Epoch 7/10
19/19 - 5s - loss: 298.6345 - loglik: -2.9658e+02 - logprior: -8.0772e-01
Epoch 8/10
19/19 - 5s - loss: 297.7519 - loglik: -2.9584e+02 - logprior: -7.8783e-01
Epoch 9/10
19/19 - 5s - loss: 297.1402 - loglik: -2.9536e+02 - logprior: -7.9905e-01
Epoch 10/10
19/19 - 5s - loss: 296.9207 - loglik: -2.9525e+02 - logprior: -7.7352e-01
Fitted a model with MAP estimate = -295.5573
Time for alignment: 152.7687
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 365.0749 - loglik: -3.6203e+02 - logprior: -3.0018e+00
Epoch 2/10
19/19 - 4s - loss: 332.8224 - loglik: -3.3147e+02 - logprior: -1.0680e+00
Epoch 3/10
19/19 - 4s - loss: 318.1938 - loglik: -3.1604e+02 - logprior: -1.3356e+00
Epoch 4/10
19/19 - 4s - loss: 313.0886 - loglik: -3.1056e+02 - logprior: -1.2450e+00
Epoch 5/10
19/19 - 4s - loss: 310.3461 - loglik: -3.0770e+02 - logprior: -1.2699e+00
Epoch 6/10
19/19 - 4s - loss: 309.1166 - loglik: -3.0658e+02 - logprior: -1.2705e+00
Epoch 7/10
19/19 - 4s - loss: 308.5025 - loglik: -3.0611e+02 - logprior: -1.2649e+00
Epoch 8/10
19/19 - 4s - loss: 308.0752 - loglik: -3.0580e+02 - logprior: -1.2746e+00
Epoch 9/10
19/19 - 4s - loss: 307.4638 - loglik: -3.0530e+02 - logprior: -1.2773e+00
Epoch 10/10
19/19 - 4s - loss: 307.0942 - loglik: -3.0500e+02 - logprior: -1.2818e+00
Fitted a model with MAP estimate = -306.0840
expansions: [(18, 1), (20, 2), (22, 4), (24, 4), (27, 1), (29, 1), (32, 1), (52, 2), (53, 2), (73, 1), (74, 1), (75, 4), (82, 1), (83, 1), (84, 3), (86, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 357.2555 - loglik: -3.5419e+02 - logprior: -3.0336e+00
Epoch 2/2
19/19 - 5s - loss: 322.3154 - loglik: -3.2093e+02 - logprior: -1.1764e+00
Fitted a model with MAP estimate = -314.4799
expansions: []
discards: [ 33  34  68  97  98  99 100 109]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 322.5189 - loglik: -3.1963e+02 - logprior: -2.8558e+00
Epoch 2/2
19/19 - 5s - loss: 314.1984 - loglik: -3.1304e+02 - logprior: -9.9326e-01
Fitted a model with MAP estimate = -310.8305
expansions: [(2, 1), (27, 2), (95, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 322.9065 - loglik: -3.1914e+02 - logprior: -3.7138e+00
Epoch 2/10
19/19 - 5s - loss: 315.7000 - loglik: -3.1364e+02 - logprior: -1.9482e+00
Epoch 3/10
19/19 - 5s - loss: 310.3644 - loglik: -3.0796e+02 - logprior: -1.7687e+00
Epoch 4/10
19/19 - 6s - loss: 305.2200 - loglik: -3.0267e+02 - logprior: -1.2081e+00
Epoch 5/10
19/19 - 5s - loss: 302.2969 - loglik: -2.9991e+02 - logprior: -8.6671e-01
Epoch 6/10
19/19 - 5s - loss: 301.1958 - loglik: -2.9889e+02 - logprior: -8.6451e-01
Epoch 7/10
19/19 - 5s - loss: 299.2336 - loglik: -2.9709e+02 - logprior: -8.7403e-01
Epoch 8/10
19/19 - 5s - loss: 298.9894 - loglik: -2.9700e+02 - logprior: -8.7303e-01
Epoch 9/10
19/19 - 6s - loss: 298.3147 - loglik: -2.9646e+02 - logprior: -8.6725e-01
Epoch 10/10
19/19 - 5s - loss: 297.4486 - loglik: -2.9571e+02 - logprior: -8.4275e-01
Fitted a model with MAP estimate = -296.5801
Time for alignment: 151.8040
Computed alignments with likelihoods: ['-295.5025', '-295.5573', '-296.5801']
Best model has likelihood: -295.5025
time for generating output: 0.1719
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.8276199804113614
Training of 3 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff4f84dfa60>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff67fcdb070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66af46d30>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66b570460>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff4f8564370>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff67609f250>, <__main__.SimpleDirichletPrior object at 0x7ff82c0818e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 915.4789 - loglik: -9.1340e+02 - logprior: -1.4699e+00
Epoch 2/10
39/39 - 39s - loss: 811.8048 - loglik: -8.0865e+02 - logprior: -1.6293e+00
Epoch 3/10
39/39 - 36s - loss: 800.2659 - loglik: -7.9659e+02 - logprior: -1.7085e+00
Epoch 4/10
39/39 - 35s - loss: 796.7836 - loglik: -7.9310e+02 - logprior: -1.6726e+00
Epoch 5/10
39/39 - 34s - loss: 795.3515 - loglik: -7.9175e+02 - logprior: -1.6912e+00
Epoch 6/10
39/39 - 35s - loss: 794.1358 - loglik: -7.9069e+02 - logprior: -1.6871e+00
Epoch 7/10
39/39 - 35s - loss: 793.0292 - loglik: -7.8961e+02 - logprior: -1.7536e+00
Epoch 8/10
39/39 - 35s - loss: 792.5770 - loglik: -7.8930e+02 - logprior: -1.7326e+00
Epoch 9/10
39/39 - 35s - loss: 792.1848 - loglik: -7.8895e+02 - logprior: -1.7775e+00
Epoch 10/10
39/39 - 35s - loss: 791.4014 - loglik: -7.8826e+02 - logprior: -1.8200e+00
Fitted a model with MAP estimate = -781.7780
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 3), (24, 1), (25, 1), (30, 1), (36, 1), (40, 1), (41, 1), (42, 2), (44, 1), (45, 1), (55, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (86, 1), (87, 1), (94, 1), (100, 1), (103, 1), (119, 1), (124, 2), (130, 1), (145, 1), (148, 1), (149, 1), (154, 1), (155, 2), (169, 1), (181, 1), (184, 1), (185, 1), (186, 1), (187, 2), (188, 1), (205, 4), (208, 1), (209, 1), (210, 1), (219, 1), (220, 1), (226, 1), (228, 1), (240, 3), (242, 1), (245, 1), (250, 1), (261, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 53s - loss: 830.5643 - loglik: -8.2787e+02 - logprior: -2.2546e+00
Epoch 2/2
39/39 - 49s - loss: 778.1845 - loglik: -7.7620e+02 - logprior: -6.9723e-01
Fitted a model with MAP estimate = -762.9247
expansions: [(157, 1), (329, 1)]
discards: [  0  30 314 338]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 51s - loss: 798.4197 - loglik: -7.9563e+02 - logprior: -2.2511e+00
Epoch 2/2
39/39 - 48s - loss: 775.8496 - loglik: -7.7376e+02 - logprior: -4.4635e-01
Fitted a model with MAP estimate = -760.9737
expansions: [(0, 2), (117, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 780.9822 - loglik: -7.7947e+02 - logprior: -1.0754e+00
Epoch 2/10
39/39 - 52s - loss: 767.5755 - loglik: -7.6665e+02 - logprior: -4.7457e-02
Epoch 3/10
39/39 - 52s - loss: 761.8635 - loglik: -7.6063e+02 - logprior: 0.0717
Epoch 4/10
39/39 - 52s - loss: 758.3136 - loglik: -7.5683e+02 - logprior: 0.1204
Epoch 5/10
39/39 - 52s - loss: 755.6138 - loglik: -7.5405e+02 - logprior: 0.2361
Epoch 6/10
39/39 - 51s - loss: 755.6893 - loglik: -7.5416e+02 - logprior: 0.3513
Fitted a model with MAP estimate = -751.7349
Time for alignment: 1100.8240
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 915.9594 - loglik: -9.1410e+02 - logprior: -1.4897e+00
Epoch 2/10
39/39 - 35s - loss: 813.5416 - loglik: -8.1070e+02 - logprior: -1.6089e+00
Epoch 3/10
39/39 - 35s - loss: 800.2503 - loglik: -7.9686e+02 - logprior: -1.7063e+00
Epoch 4/10
39/39 - 35s - loss: 796.7803 - loglik: -7.9324e+02 - logprior: -1.7189e+00
Epoch 5/10
39/39 - 34s - loss: 795.1561 - loglik: -7.9160e+02 - logprior: -1.7393e+00
Epoch 6/10
39/39 - 35s - loss: 793.9858 - loglik: -7.9052e+02 - logprior: -1.7424e+00
Epoch 7/10
39/39 - 35s - loss: 792.9985 - loglik: -7.8961e+02 - logprior: -1.7397e+00
Epoch 8/10
39/39 - 35s - loss: 792.2375 - loglik: -7.8891e+02 - logprior: -1.7714e+00
Epoch 9/10
39/39 - 35s - loss: 791.6929 - loglik: -7.8843e+02 - logprior: -1.7761e+00
Epoch 10/10
39/39 - 35s - loss: 790.5507 - loglik: -7.8740e+02 - logprior: -1.7751e+00
Fitted a model with MAP estimate = -781.3559
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2), (26, 1), (32, 1), (40, 2), (41, 1), (42, 1), (45, 3), (55, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (86, 1), (87, 1), (94, 2), (103, 1), (119, 1), (120, 1), (123, 1), (124, 1), (130, 1), (131, 2), (144, 1), (147, 1), (148, 1), (152, 2), (153, 1), (154, 3), (156, 2), (183, 1), (185, 1), (186, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (210, 1), (219, 1), (222, 1), (226, 2), (227, 2), (239, 2), (240, 1), (242, 1), (245, 1), (250, 1), (261, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 359 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 55s - loss: 838.5947 - loglik: -8.3568e+02 - logprior: -2.2440e+00
Epoch 2/2
39/39 - 52s - loss: 778.4815 - loglik: -7.7627e+02 - logprior: -8.3282e-01
Fitted a model with MAP estimate = -761.9758
expansions: [(337, 1)]
discards: [  0  30 167 193 203 291 307 322 346]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 53s - loss: 795.0396 - loglik: -7.9246e+02 - logprior: -2.2953e+00
Epoch 2/2
39/39 - 49s - loss: 777.5991 - loglik: -7.7647e+02 - logprior: -4.2284e-01
Fitted a model with MAP estimate = -763.8186
expansions: [(0, 2)]
discards: [  0 194]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 54s - loss: 786.1589 - loglik: -7.8501e+02 - logprior: -1.0181e+00
Epoch 2/10
39/39 - 54s - loss: 771.0177 - loglik: -7.7048e+02 - logprior: -4.7402e-02
Epoch 3/10
39/39 - 58s - loss: 762.9960 - loglik: -7.6221e+02 - logprior: 0.0831
Epoch 4/10
39/39 - 61s - loss: 758.5088 - loglik: -7.5748e+02 - logprior: 0.1651
Epoch 5/10
39/39 - 63s - loss: 756.5020 - loglik: -7.5536e+02 - logprior: 0.2720
Epoch 6/10
39/39 - 60s - loss: 755.3989 - loglik: -7.5416e+02 - logprior: 0.3278
Epoch 7/10
39/39 - 58s - loss: 754.4229 - loglik: -7.5320e+02 - logprior: 0.4328
Epoch 8/10
39/39 - 63s - loss: 751.4222 - loglik: -7.5029e+02 - logprior: 0.5552
Epoch 9/10
39/39 - 57s - loss: 753.1126 - loglik: -7.5207e+02 - logprior: 0.6073
Fitted a model with MAP estimate = -748.6011
Time for alignment: 1317.0219
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 916.1857 - loglik: -9.1405e+02 - logprior: -1.4656e+00
Epoch 2/10
39/39 - 40s - loss: 815.1966 - loglik: -8.1188e+02 - logprior: -1.5562e+00
Epoch 3/10
39/39 - 43s - loss: 803.5524 - loglik: -7.9981e+02 - logprior: -1.6293e+00
Epoch 4/10
39/39 - 40s - loss: 800.4146 - loglik: -7.9672e+02 - logprior: -1.6112e+00
Epoch 5/10
39/39 - 38s - loss: 798.7855 - loglik: -7.9520e+02 - logprior: -1.6683e+00
Epoch 6/10
39/39 - 39s - loss: 797.5940 - loglik: -7.9420e+02 - logprior: -1.6832e+00
Epoch 7/10
39/39 - 41s - loss: 797.0759 - loglik: -7.9383e+02 - logprior: -1.6763e+00
Epoch 8/10
39/39 - 44s - loss: 796.4370 - loglik: -7.9330e+02 - logprior: -1.7110e+00
Epoch 9/10
39/39 - 46s - loss: 795.7970 - loglik: -7.9276e+02 - logprior: -1.7135e+00
Epoch 10/10
39/39 - 47s - loss: 795.4026 - loglik: -7.9246e+02 - logprior: -1.7155e+00
Fitted a model with MAP estimate = -785.8723
expansions: [(0, 2), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (25, 1), (30, 1), (36, 1), (39, 1), (41, 1), (42, 1), (44, 2), (56, 1), (57, 1), (59, 1), (61, 1), (80, 1), (81, 1), (82, 1), (86, 1), (88, 1), (93, 2), (94, 1), (101, 1), (103, 1), (121, 1), (129, 3), (132, 2), (145, 1), (150, 1), (153, 2), (156, 2), (169, 2), (184, 1), (186, 2), (187, 1), (188, 2), (189, 1), (206, 4), (207, 1), (208, 1), (209, 1), (210, 1), (219, 1), (222, 1), (226, 2), (227, 2), (240, 3), (242, 1), (245, 1), (250, 1), (261, 1), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 358 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 836.8400 - loglik: -8.3436e+02 - logprior: -2.2021e+00
Epoch 2/2
39/39 - 72s - loss: 778.3598 - loglik: -7.7663e+02 - logprior: -7.1142e-01
Fitted a model with MAP estimate = -762.7347
expansions: []
discards: [  0 166 232 288 304 345]
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 799.2557 - loglik: -7.9682e+02 - logprior: -2.3176e+00
Epoch 2/2
39/39 - 64s - loss: 779.4828 - loglik: -7.7868e+02 - logprior: -4.2208e-01
Fitted a model with MAP estimate = -765.1904
expansions: [(0, 2), (182, 1)]
discards: [  0 190 255 314]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 70s - loss: 780.1943 - loglik: -7.7874e+02 - logprior: -1.1457e+00
Epoch 2/10
39/39 - 65s - loss: 768.1319 - loglik: -7.6714e+02 - logprior: -1.0683e-01
Epoch 3/10
39/39 - 59s - loss: 761.6482 - loglik: -7.6021e+02 - logprior: -5.6432e-03
Epoch 4/10
39/39 - 62s - loss: 758.2306 - loglik: -7.5652e+02 - logprior: 0.0675
Epoch 5/10
39/39 - 62s - loss: 756.6008 - loglik: -7.5481e+02 - logprior: 0.1664
Epoch 6/10
39/39 - 56s - loss: 755.1304 - loglik: -7.5338e+02 - logprior: 0.2638
Epoch 7/10
39/39 - 58s - loss: 754.4131 - loglik: -7.5274e+02 - logprior: 0.3419
Epoch 8/10
39/39 - 62s - loss: 752.8553 - loglik: -7.5144e+02 - logprior: 0.4807
Epoch 9/10
39/39 - 58s - loss: 751.5934 - loglik: -7.5037e+02 - logprior: 0.5380
Epoch 10/10
39/39 - 57s - loss: 751.4760 - loglik: -7.5056e+02 - logprior: 0.6958
Fitted a model with MAP estimate = -748.1598
Time for alignment: 1656.5389
Computed alignments with likelihoods: ['-751.7349', '-748.6011', '-748.1598']
Best model has likelihood: -748.1598
time for generating output: 1.3427
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.4507697005050884
Training of 3 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff524134ca0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff83d024c40>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fee9c2dac70>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff7caa7ed90>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff83d4f3a60>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff66d474f10>, <__main__.SimpleDirichletPrior object at 0x7ff66cc4d0a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 892.3055 - loglik: -8.9047e+02 - logprior: -1.5251e+00
Epoch 2/10
39/39 - 74s - loss: 751.2953 - loglik: -7.4867e+02 - logprior: -1.6320e+00
Epoch 3/10
39/39 - 74s - loss: 738.9330 - loglik: -7.3601e+02 - logprior: -1.7489e+00
Epoch 4/10
39/39 - 76s - loss: 735.4676 - loglik: -7.3250e+02 - logprior: -1.7972e+00
Epoch 5/10
39/39 - 74s - loss: 733.8488 - loglik: -7.3092e+02 - logprior: -1.7801e+00
Epoch 6/10
39/39 - 75s - loss: 732.1569 - loglik: -7.2924e+02 - logprior: -1.8011e+00
Epoch 7/10
39/39 - 75s - loss: 732.4376 - loglik: -7.2950e+02 - logprior: -1.8464e+00
Fitted a model with MAP estimate = -729.5183
expansions: [(6, 1), (20, 2), (21, 1), (31, 1), (67, 1), (69, 1), (101, 2), (104, 1), (106, 1), (116, 1), (122, 1), (141, 1), (143, 1), (144, 1), (146, 5), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (169, 1), (177, 2), (178, 1), (179, 2), (180, 1), (181, 1), (182, 6), (183, 1), (185, 1), (187, 1), (188, 1), (189, 1), (190, 2), (203, 2), (204, 1), (210, 1), (211, 5), (218, 2), (219, 1), (220, 1), (227, 1), (233, 1), (234, 3), (240, 1), (242, 1), (244, 2), (247, 1), (250, 1), (251, 1), (256, 1), (258, 1), (277, 1), (279, 2), (280, 1), (281, 1), (282, 5), (288, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 191 192 193 194 195]
Re-initialized the encoder parameters.
Fitting a model of length 411 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 105s - loss: 729.2061 - loglik: -7.2693e+02 - logprior: -1.7134e+00
Epoch 2/2
39/39 - 108s - loss: 710.3735 - loglik: -7.0863e+02 - logprior: -5.5761e-01
Fitted a model with MAP estimate = -704.6389
expansions: [(234, 4)]
discards: [  5 208 217 218 235 236 237 250 251 258 259 260 263 268 290 306 355 356
 357]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 95s - loss: 726.6148 - loglik: -7.2520e+02 - logprior: -1.3520e+00
Epoch 2/2
39/39 - 85s - loss: 713.4303 - loglik: -7.1272e+02 - logprior: -1.5355e-01
Fitted a model with MAP estimate = -708.3358
expansions: [(5, 1), (230, 2), (231, 4)]
discards: [238 239 240 241 242 243 244 245 246]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 85s - loss: 718.2882 - loglik: -7.1690e+02 - logprior: -1.1144e+00
Epoch 2/10
39/39 - 91s - loss: 710.6828 - loglik: -7.0995e+02 - logprior: 0.1762
Epoch 3/10
39/39 - 101s - loss: 707.4220 - loglik: -7.0629e+02 - logprior: 0.2389
Epoch 4/10
39/39 - 104s - loss: 705.1124 - loglik: -7.0401e+02 - logprior: 0.2837
Epoch 5/10
39/39 - 103s - loss: 705.0968 - loglik: -7.0438e+02 - logprior: 0.5694
Epoch 6/10
39/39 - 103s - loss: 703.6394 - loglik: -7.0310e+02 - logprior: 0.6879
Epoch 7/10
39/39 - 102s - loss: 702.7066 - loglik: -7.0235e+02 - logprior: 0.8290
Epoch 8/10
39/39 - 81s - loss: 702.2889 - loglik: -7.0207e+02 - logprior: 0.9430
Epoch 9/10
39/39 - 70s - loss: 702.0577 - loglik: -7.0192e+02 - logprior: 1.0272
Epoch 10/10
39/39 - 70s - loss: 700.0992 - loglik: -7.0007e+02 - logprior: 1.1394
Fitted a model with MAP estimate = -699.0515
Time for alignment: 2241.6910
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 894.5763 - loglik: -8.9279e+02 - logprior: -1.5366e+00
Epoch 2/10
39/39 - 56s - loss: 754.0196 - loglik: -7.5141e+02 - logprior: -1.6861e+00
Epoch 3/10
39/39 - 57s - loss: 740.9229 - loglik: -7.3795e+02 - logprior: -1.8989e+00
Epoch 4/10
39/39 - 55s - loss: 738.2429 - loglik: -7.3517e+02 - logprior: -1.9207e+00
Epoch 5/10
39/39 - 53s - loss: 734.7982 - loglik: -7.3164e+02 - logprior: -1.9837e+00
Epoch 6/10
39/39 - 53s - loss: 735.1958 - loglik: -7.3203e+02 - logprior: -2.0071e+00
Fitted a model with MAP estimate = -731.8944
expansions: [(9, 1), (19, 2), (20, 1), (66, 1), (68, 1), (100, 2), (103, 1), (105, 1), (106, 1), (110, 1), (114, 1), (119, 1), (137, 1), (138, 1), (139, 1), (140, 1), (142, 3), (143, 1), (144, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (166, 1), (173, 1), (175, 1), (176, 5), (179, 1), (180, 8), (184, 1), (185, 1), (186, 1), (188, 1), (189, 2), (190, 1), (210, 2), (218, 2), (219, 1), (220, 1), (227, 1), (233, 3), (236, 1), (239, 1), (241, 1), (243, 2), (246, 1), (249, 1), (250, 1), (256, 1), (277, 1), (278, 2), (279, 1), (280, 1), (281, 1), (282, 1), (283, 3), (288, 1), (297, 1), (311, 1), (313, 1), (316, 1), (317, 1), (318, 1)]
discards: [  0   1 192 193 194 195 196 197 198 199 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 733.0112 - loglik: -7.3004e+02 - logprior: -2.5980e+00
Epoch 2/2
39/39 - 72s - loss: 711.1956 - loglik: -7.0906e+02 - logprior: -1.3965e+00
Fitted a model with MAP estimate = -705.8610
expansions: [(4, 2), (339, 1), (350, 1)]
discards: [  0   1 161 207 208 217 218 219 220 251 260 297 344 345 346 347 348]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 721.3826 - loglik: -7.1900e+02 - logprior: -2.0449e+00
Epoch 2/2
39/39 - 71s - loss: 712.5233 - loglik: -7.1129e+02 - logprior: -4.7622e-01
Fitted a model with MAP estimate = -708.0054
expansions: [(4, 1), (5, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 83s - loss: 718.6112 - loglik: -7.1649e+02 - logprior: -1.8165e+00
Epoch 2/10
39/39 - 80s - loss: 710.6416 - loglik: -7.0999e+02 - logprior: 0.0503
Epoch 3/10
39/39 - 81s - loss: 707.5693 - loglik: -7.0683e+02 - logprior: 0.2878
Epoch 4/10
39/39 - 84s - loss: 705.8825 - loglik: -7.0508e+02 - logprior: 0.4320
Epoch 5/10
39/39 - 84s - loss: 704.2510 - loglik: -7.0345e+02 - logprior: 0.5032
Epoch 6/10
39/39 - 84s - loss: 704.4730 - loglik: -7.0389e+02 - logprior: 0.7038
Fitted a model with MAP estimate = -701.5676
Time for alignment: 1430.3646
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 63s - loss: 891.0161 - loglik: -8.8933e+02 - logprior: -1.5213e+00
Epoch 2/10
39/39 - 61s - loss: 752.2397 - loglik: -7.4984e+02 - logprior: -1.5402e+00
Epoch 3/10
39/39 - 62s - loss: 739.6909 - loglik: -7.3685e+02 - logprior: -1.7095e+00
Epoch 4/10
39/39 - 63s - loss: 736.6897 - loglik: -7.3376e+02 - logprior: -1.6875e+00
Epoch 5/10
39/39 - 64s - loss: 734.9301 - loglik: -7.3196e+02 - logprior: -1.7521e+00
Epoch 6/10
39/39 - 63s - loss: 733.6528 - loglik: -7.3075e+02 - logprior: -1.7578e+00
Epoch 7/10
39/39 - 62s - loss: 733.7612 - loglik: -7.3090e+02 - logprior: -1.7793e+00
Fitted a model with MAP estimate = -730.6054
expansions: [(9, 1), (19, 1), (31, 1), (66, 1), (95, 1), (100, 2), (102, 2), (104, 1), (105, 1), (113, 1), (119, 1), (138, 2), (139, 1), (140, 1), (142, 2), (143, 3), (158, 1), (160, 1), (161, 1), (162, 1), (167, 1), (176, 1), (177, 4), (180, 1), (181, 5), (183, 1), (185, 1), (187, 1), (189, 2), (190, 2), (191, 1), (202, 1), (203, 2), (211, 3), (218, 1), (219, 2), (220, 1), (228, 1), (233, 1), (237, 1), (240, 1), (242, 1), (244, 2), (246, 1), (252, 1), (253, 2), (256, 1), (257, 1), (275, 1), (277, 1), (278, 1), (282, 1), (283, 1), (288, 1), (289, 2), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1 193 194 195 196 197]
Re-initialized the encoder parameters.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 89s - loss: 734.4272 - loglik: -7.3138e+02 - logprior: -2.6718e+00
Epoch 2/2
39/39 - 90s - loss: 715.2602 - loglik: -7.1311e+02 - logprior: -1.3745e+00
Fitted a model with MAP estimate = -709.3072
expansions: [(4, 2), (231, 1), (232, 2)]
discards: [  0   1 157 205 214 233 235 255 265 299 357]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 722.5344 - loglik: -7.2004e+02 - logprior: -2.1814e+00
Epoch 2/2
39/39 - 70s - loss: 713.7795 - loglik: -7.1251e+02 - logprior: -5.6539e-01
Fitted a model with MAP estimate = -708.6125
expansions: [(4, 1), (230, 1), (231, 2), (241, 1)]
discards: [209 210 212 233 234 235 295]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 72s - loss: 722.8874 - loglik: -7.2153e+02 - logprior: -1.1432e+00
Epoch 2/10
39/39 - 70s - loss: 712.0497 - loglik: -7.1130e+02 - logprior: 0.1184
Epoch 3/10
39/39 - 71s - loss: 707.8504 - loglik: -7.0669e+02 - logprior: 0.2231
Epoch 4/10
39/39 - 73s - loss: 706.7354 - loglik: -7.0570e+02 - logprior: 0.3461
Epoch 5/10
39/39 - 76s - loss: 705.2170 - loglik: -7.0425e+02 - logprior: 0.3364
Epoch 6/10
39/39 - 74s - loss: 704.1396 - loglik: -7.0344e+02 - logprior: 0.5531
Epoch 7/10
39/39 - 71s - loss: 703.1440 - loglik: -7.0267e+02 - logprior: 0.7461
Epoch 8/10
39/39 - 69s - loss: 703.2100 - loglik: -7.0288e+02 - logprior: 0.8503
Fitted a model with MAP estimate = -700.5183
Time for alignment: 1653.8380
Computed alignments with likelihoods: ['-699.0515', '-701.5676', '-700.5183']
Best model has likelihood: -699.0515
time for generating output: 0.4785
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.7909680407596109
Training of 3 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7feea549d490>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff834c694c0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff506189370>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee89874190>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee88a60280>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff698b87310>, <__main__.SimpleDirichletPrior object at 0x7ff4f8261730>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 252.6514 - loglik: -2.4940e+02 - logprior: -3.0739e+00
Epoch 2/10
19/19 - 2s - loss: 221.4866 - loglik: -2.1979e+02 - logprior: -1.2895e+00
Epoch 3/10
19/19 - 2s - loss: 206.7889 - loglik: -2.0467e+02 - logprior: -1.5818e+00
Epoch 4/10
19/19 - 2s - loss: 203.4208 - loglik: -2.0144e+02 - logprior: -1.5055e+00
Epoch 5/10
19/19 - 2s - loss: 202.7883 - loglik: -2.0089e+02 - logprior: -1.4996e+00
Epoch 6/10
19/19 - 2s - loss: 201.8043 - loglik: -1.9995e+02 - logprior: -1.4646e+00
Epoch 7/10
19/19 - 2s - loss: 201.8992 - loglik: -2.0009e+02 - logprior: -1.4417e+00
Fitted a model with MAP estimate = -201.1924
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (15, 1), (28, 1), (37, 2), (39, 2), (41, 2), (43, 2), (48, 1), (49, 2), (50, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 208.0649 - loglik: -2.0381e+02 - logprior: -3.9612e+00
Epoch 2/2
19/19 - 2s - loss: 199.0194 - loglik: -1.9636e+02 - logprior: -2.1431e+00
Fitted a model with MAP estimate = -196.6686
expansions: [(0, 2)]
discards: [ 0 10 50 67 86]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 200.8674 - loglik: -1.9792e+02 - logprior: -2.8852e+00
Epoch 2/2
19/19 - 2s - loss: 196.3226 - loglik: -1.9502e+02 - logprior: -1.0980e+00
Fitted a model with MAP estimate = -195.1086
expansions: []
discards: [ 0 46 52]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 202.1111 - loglik: -1.9838e+02 - logprior: -3.6934e+00
Epoch 2/10
19/19 - 2s - loss: 196.9371 - loglik: -1.9555e+02 - logprior: -1.2263e+00
Epoch 3/10
19/19 - 2s - loss: 195.5945 - loglik: -1.9428e+02 - logprior: -1.0090e+00
Epoch 4/10
19/19 - 2s - loss: 194.8331 - loglik: -1.9343e+02 - logprior: -9.8759e-01
Epoch 5/10
19/19 - 2s - loss: 194.2665 - loglik: -1.9285e+02 - logprior: -9.5078e-01
Epoch 6/10
19/19 - 2s - loss: 193.8251 - loglik: -1.9239e+02 - logprior: -9.4241e-01
Epoch 7/10
19/19 - 2s - loss: 193.5526 - loglik: -1.9212e+02 - logprior: -9.2638e-01
Epoch 8/10
19/19 - 2s - loss: 193.1176 - loglik: -1.9169e+02 - logprior: -9.0962e-01
Epoch 9/10
19/19 - 2s - loss: 192.9565 - loglik: -1.9154e+02 - logprior: -8.9694e-01
Epoch 10/10
19/19 - 2s - loss: 192.4748 - loglik: -1.9103e+02 - logprior: -8.9117e-01
Fitted a model with MAP estimate = -191.5470
Time for alignment: 77.6373
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 252.8434 - loglik: -2.4961e+02 - logprior: -3.0705e+00
Epoch 2/10
19/19 - 2s - loss: 221.1843 - loglik: -2.1951e+02 - logprior: -1.2746e+00
Epoch 3/10
19/19 - 2s - loss: 206.1352 - loglik: -2.0401e+02 - logprior: -1.5422e+00
Epoch 4/10
19/19 - 2s - loss: 203.1432 - loglik: -2.0118e+02 - logprior: -1.4672e+00
Epoch 5/10
19/19 - 2s - loss: 201.9904 - loglik: -2.0011e+02 - logprior: -1.4601e+00
Epoch 6/10
19/19 - 2s - loss: 201.8785 - loglik: -2.0007e+02 - logprior: -1.4279e+00
Epoch 7/10
19/19 - 2s - loss: 201.3467 - loglik: -1.9954e+02 - logprior: -1.4144e+00
Epoch 8/10
19/19 - 2s - loss: 201.1982 - loglik: -1.9941e+02 - logprior: -1.4083e+00
Epoch 9/10
19/19 - 2s - loss: 201.0961 - loglik: -1.9931e+02 - logprior: -1.4076e+00
Epoch 10/10
19/19 - 2s - loss: 200.8212 - loglik: -1.9904e+02 - logprior: -1.4022e+00
Fitted a model with MAP estimate = -200.2188
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (18, 1), (19, 1), (26, 1), (37, 2), (39, 1), (41, 2), (43, 2), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 209.8517 - loglik: -2.0554e+02 - logprior: -3.9265e+00
Epoch 2/2
19/19 - 2s - loss: 199.6512 - loglik: -1.9693e+02 - logprior: -2.1411e+00
Fitted a model with MAP estimate = -197.3872
expansions: [(0, 2)]
discards: [ 0 10 45 53 66 85]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 201.3090 - loglik: -1.9843e+02 - logprior: -2.8552e+00
Epoch 2/2
19/19 - 2s - loss: 196.7951 - loglik: -1.9554e+02 - logprior: -1.0729e+00
Fitted a model with MAP estimate = -195.3666
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 201.7861 - loglik: -1.9793e+02 - logprior: -3.7157e+00
Epoch 2/10
19/19 - 2s - loss: 196.7591 - loglik: -1.9527e+02 - logprior: -1.2558e+00
Epoch 3/10
19/19 - 2s - loss: 195.4164 - loglik: -1.9402e+02 - logprior: -1.0094e+00
Epoch 4/10
19/19 - 2s - loss: 194.4650 - loglik: -1.9300e+02 - logprior: -9.7711e-01
Epoch 5/10
19/19 - 2s - loss: 193.9460 - loglik: -1.9248e+02 - logprior: -9.5319e-01
Epoch 6/10
19/19 - 2s - loss: 193.5474 - loglik: -1.9211e+02 - logprior: -9.3485e-01
Epoch 7/10
19/19 - 2s - loss: 193.3287 - loglik: -1.9191e+02 - logprior: -9.2631e-01
Epoch 8/10
19/19 - 2s - loss: 192.8227 - loglik: -1.9141e+02 - logprior: -9.1111e-01
Epoch 9/10
19/19 - 2s - loss: 192.5286 - loglik: -1.9111e+02 - logprior: -8.9600e-01
Epoch 10/10
19/19 - 2s - loss: 192.2858 - loglik: -1.9083e+02 - logprior: -8.9147e-01
Fitted a model with MAP estimate = -191.3918
Time for alignment: 82.5956
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 252.6288 - loglik: -2.4948e+02 - logprior: -3.0720e+00
Epoch 2/10
19/19 - 2s - loss: 220.1039 - loglik: -2.1848e+02 - logprior: -1.2809e+00
Epoch 3/10
19/19 - 2s - loss: 206.0750 - loglik: -2.0394e+02 - logprior: -1.5418e+00
Epoch 4/10
19/19 - 2s - loss: 203.0092 - loglik: -2.0108e+02 - logprior: -1.4527e+00
Epoch 5/10
19/19 - 2s - loss: 202.2604 - loglik: -2.0037e+02 - logprior: -1.4515e+00
Epoch 6/10
19/19 - 2s - loss: 201.8022 - loglik: -1.9997e+02 - logprior: -1.4280e+00
Epoch 7/10
19/19 - 2s - loss: 201.5308 - loglik: -1.9973e+02 - logprior: -1.4190e+00
Epoch 8/10
19/19 - 2s - loss: 201.3462 - loglik: -1.9956e+02 - logprior: -1.4124e+00
Epoch 9/10
19/19 - 2s - loss: 201.3434 - loglik: -1.9955e+02 - logprior: -1.4099e+00
Epoch 10/10
19/19 - 2s - loss: 200.7639 - loglik: -1.9896e+02 - logprior: -1.4083e+00
Fitted a model with MAP estimate = -200.3415
expansions: [(9, 1), (10, 1), (11, 1), (13, 1), (15, 1), (16, 1), (19, 1), (20, 1), (40, 2), (41, 2), (43, 4), (47, 1), (48, 2), (49, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 212.7343 - loglik: -2.0885e+02 - logprior: -3.8237e+00
Epoch 2/2
19/19 - 2s - loss: 200.1013 - loglik: -1.9783e+02 - logprior: -2.1044e+00
Fitted a model with MAP estimate = -197.9875
expansions: [(0, 2)]
discards: [ 0 48 65]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 200.6353 - loglik: -1.9775e+02 - logprior: -2.8628e+00
Epoch 2/2
19/19 - 2s - loss: 196.5924 - loglik: -1.9540e+02 - logprior: -1.0845e+00
Fitted a model with MAP estimate = -195.3061
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 201.1725 - loglik: -1.9739e+02 - logprior: -3.7031e+00
Epoch 2/10
19/19 - 2s - loss: 196.4875 - loglik: -1.9498e+02 - logprior: -1.2493e+00
Epoch 3/10
19/19 - 2s - loss: 195.2529 - loglik: -1.9380e+02 - logprior: -1.0326e+00
Epoch 4/10
19/19 - 2s - loss: 194.4374 - loglik: -1.9293e+02 - logprior: -9.9928e-01
Epoch 5/10
19/19 - 2s - loss: 194.1067 - loglik: -1.9261e+02 - logprior: -9.7547e-01
Epoch 6/10
19/19 - 2s - loss: 193.8054 - loglik: -1.9233e+02 - logprior: -9.5496e-01
Epoch 7/10
19/19 - 2s - loss: 192.9902 - loglik: -1.9153e+02 - logprior: -9.4782e-01
Epoch 8/10
19/19 - 2s - loss: 193.0980 - loglik: -1.9166e+02 - logprior: -9.3263e-01
Fitted a model with MAP estimate = -192.1328
Time for alignment: 77.4462
Computed alignments with likelihoods: ['-191.5470', '-191.3918', '-192.1328']
Best model has likelihood: -191.3918
time for generating output: 0.1449
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.7810077519379846
Training of 3 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fee9c2ce6d0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66c20dfa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff84eaa56a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff67f97d160>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee89874190>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7feea549d490>, <__main__.SimpleDirichletPrior object at 0x7ff874b99520>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.4561 - loglik: -2.9830e+02 - logprior: -3.0451e+00
Epoch 2/10
19/19 - 3s - loss: 270.6137 - loglik: -2.6894e+02 - logprior: -1.2611e+00
Epoch 3/10
19/19 - 3s - loss: 257.4898 - loglik: -2.5551e+02 - logprior: -1.2572e+00
Epoch 4/10
19/19 - 2s - loss: 254.5322 - loglik: -2.5267e+02 - logprior: -1.1422e+00
Epoch 5/10
19/19 - 3s - loss: 252.5724 - loglik: -2.5073e+02 - logprior: -1.1135e+00
Epoch 6/10
19/19 - 3s - loss: 252.4040 - loglik: -2.5059e+02 - logprior: -1.1123e+00
Epoch 7/10
19/19 - 2s - loss: 251.1732 - loglik: -2.4935e+02 - logprior: -1.0988e+00
Epoch 8/10
19/19 - 3s - loss: 251.0663 - loglik: -2.4924e+02 - logprior: -1.1032e+00
Epoch 9/10
19/19 - 2s - loss: 250.5106 - loglik: -2.4870e+02 - logprior: -1.1045e+00
Epoch 10/10
19/19 - 3s - loss: 250.2946 - loglik: -2.4853e+02 - logprior: -1.0965e+00
Fitted a model with MAP estimate = -248.7083
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (39, 2), (41, 3), (42, 1), (44, 2), (51, 1), (52, 1), (53, 1), (75, 2), (76, 2), (77, 4), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 269.3675 - loglik: -2.6517e+02 - logprior: -4.1511e+00
Epoch 2/2
19/19 - 3s - loss: 255.0426 - loglik: -2.5335e+02 - logprior: -1.3995e+00
Fitted a model with MAP estimate = -251.4305
expansions: []
discards: [ 1 28 55 59 96]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 255.3578 - loglik: -2.5221e+02 - logprior: -2.9081e+00
Epoch 2/2
19/19 - 3s - loss: 251.0896 - loglik: -2.4952e+02 - logprior: -1.1284e+00
Fitted a model with MAP estimate = -248.5597
expansions: [(51, 1), (52, 1), (101, 1)]
discards: [94]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 255.7958 - loglik: -2.5286e+02 - logprior: -2.8328e+00
Epoch 2/10
19/19 - 3s - loss: 251.2678 - loglik: -2.5011e+02 - logprior: -1.0438e+00
Epoch 3/10
19/19 - 3s - loss: 248.6832 - loglik: -2.4726e+02 - logprior: -8.9439e-01
Epoch 4/10
19/19 - 3s - loss: 247.4838 - loglik: -2.4576e+02 - logprior: -8.4959e-01
Epoch 5/10
19/19 - 3s - loss: 245.9014 - loglik: -2.4414e+02 - logprior: -8.3349e-01
Epoch 6/10
19/19 - 3s - loss: 245.1109 - loglik: -2.4335e+02 - logprior: -8.0880e-01
Epoch 7/10
19/19 - 3s - loss: 244.1887 - loglik: -2.4244e+02 - logprior: -7.8348e-01
Epoch 8/10
19/19 - 3s - loss: 243.3045 - loglik: -2.4158e+02 - logprior: -7.7992e-01
Epoch 9/10
19/19 - 3s - loss: 242.6259 - loglik: -2.4095e+02 - logprior: -7.7071e-01
Epoch 10/10
19/19 - 3s - loss: 242.2419 - loglik: -2.4058e+02 - logprior: -7.5412e-01
Fitted a model with MAP estimate = -240.8136
Time for alignment: 100.6750
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 301.5779 - loglik: -2.9850e+02 - logprior: -3.0399e+00
Epoch 2/10
19/19 - 2s - loss: 269.9232 - loglik: -2.6841e+02 - logprior: -1.2476e+00
Epoch 3/10
19/19 - 2s - loss: 257.5741 - loglik: -2.5570e+02 - logprior: -1.2612e+00
Epoch 4/10
19/19 - 2s - loss: 254.1722 - loglik: -2.5229e+02 - logprior: -1.1494e+00
Epoch 5/10
19/19 - 2s - loss: 253.2817 - loglik: -2.5146e+02 - logprior: -1.1235e+00
Epoch 6/10
19/19 - 3s - loss: 252.2794 - loglik: -2.5044e+02 - logprior: -1.1190e+00
Epoch 7/10
19/19 - 2s - loss: 251.4668 - loglik: -2.4961e+02 - logprior: -1.1136e+00
Epoch 8/10
19/19 - 3s - loss: 250.7717 - loglik: -2.4894e+02 - logprior: -1.1172e+00
Epoch 9/10
19/19 - 2s - loss: 250.4582 - loglik: -2.4865e+02 - logprior: -1.1173e+00
Epoch 10/10
19/19 - 3s - loss: 250.4069 - loglik: -2.4864e+02 - logprior: -1.1127e+00
Fitted a model with MAP estimate = -248.7397
expansions: [(0, 2), (17, 1), (20, 3), (21, 2), (22, 1), (23, 1), (38, 2), (41, 2), (42, 1), (44, 2), (53, 1), (54, 1), (74, 1), (75, 2), (76, 2), (77, 4), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 266.8762 - loglik: -2.6248e+02 - logprior: -4.2147e+00
Epoch 2/2
19/19 - 3s - loss: 253.5110 - loglik: -2.5179e+02 - logprior: -1.3696e+00
Fitted a model with MAP estimate = -250.0048
expansions: [(54, 1)]
discards: [ 1 28 55 59 96]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.4825 - loglik: -2.5254e+02 - logprior: -2.8562e+00
Epoch 2/2
19/19 - 3s - loss: 251.2225 - loglik: -2.4987e+02 - logprior: -1.1124e+00
Fitted a model with MAP estimate = -249.0115
expansions: [(51, 1), (102, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 254.3540 - loglik: -2.5155e+02 - logprior: -2.7618e+00
Epoch 2/10
19/19 - 3s - loss: 250.4240 - loglik: -2.4920e+02 - logprior: -1.0731e+00
Epoch 3/10
19/19 - 3s - loss: 248.8784 - loglik: -2.4760e+02 - logprior: -9.3428e-01
Epoch 4/10
19/19 - 3s - loss: 247.1437 - loglik: -2.4568e+02 - logprior: -9.1595e-01
Epoch 5/10
19/19 - 3s - loss: 246.3537 - loglik: -2.4478e+02 - logprior: -8.8645e-01
Epoch 6/10
19/19 - 3s - loss: 245.4263 - loglik: -2.4376e+02 - logprior: -8.5892e-01
Epoch 7/10
19/19 - 3s - loss: 244.5517 - loglik: -2.4282e+02 - logprior: -8.3699e-01
Epoch 8/10
19/19 - 3s - loss: 243.2498 - loglik: -2.4149e+02 - logprior: -8.2269e-01
Epoch 9/10
19/19 - 3s - loss: 243.1824 - loglik: -2.4142e+02 - logprior: -8.0660e-01
Epoch 10/10
19/19 - 3s - loss: 241.8279 - loglik: -2.4008e+02 - logprior: -7.9957e-01
Fitted a model with MAP estimate = -240.9048
Time for alignment: 101.5785
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 301.6906 - loglik: -2.9855e+02 - logprior: -3.0428e+00
Epoch 2/10
19/19 - 2s - loss: 269.7511 - loglik: -2.6812e+02 - logprior: -1.2494e+00
Epoch 3/10
19/19 - 2s - loss: 257.2150 - loglik: -2.5526e+02 - logprior: -1.2426e+00
Epoch 4/10
19/19 - 2s - loss: 254.6591 - loglik: -2.5281e+02 - logprior: -1.0978e+00
Epoch 5/10
19/19 - 2s - loss: 253.2971 - loglik: -2.5148e+02 - logprior: -1.0808e+00
Epoch 6/10
19/19 - 3s - loss: 252.3630 - loglik: -2.5056e+02 - logprior: -1.0875e+00
Epoch 7/10
19/19 - 2s - loss: 252.1348 - loglik: -2.5033e+02 - logprior: -1.0788e+00
Epoch 8/10
19/19 - 3s - loss: 251.2178 - loglik: -2.4944e+02 - logprior: -1.0645e+00
Epoch 9/10
19/19 - 3s - loss: 251.0080 - loglik: -2.4927e+02 - logprior: -1.0661e+00
Epoch 10/10
19/19 - 3s - loss: 250.6422 - loglik: -2.4891e+02 - logprior: -1.0605e+00
Fitted a model with MAP estimate = -249.2955
expansions: [(0, 2), (20, 4), (21, 2), (22, 1), (32, 1), (39, 2), (42, 4), (44, 1), (51, 1), (52, 1), (53, 1), (75, 2), (76, 1), (77, 4), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 270.9273 - loglik: -2.6667e+02 - logprior: -4.1426e+00
Epoch 2/2
19/19 - 3s - loss: 255.2884 - loglik: -2.5360e+02 - logprior: -1.3982e+00
Fitted a model with MAP estimate = -251.5421
expansions: []
discards: [ 1 28 49 97]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 255.4906 - loglik: -2.5229e+02 - logprior: -2.8511e+00
Epoch 2/2
19/19 - 3s - loss: 251.3729 - loglik: -2.4985e+02 - logprior: -1.1163e+00
Fitted a model with MAP estimate = -248.9590
expansions: [(101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 255.0360 - loglik: -2.5148e+02 - logprior: -3.3648e+00
Epoch 2/10
19/19 - 3s - loss: 250.2118 - loglik: -2.4868e+02 - logprior: -1.1139e+00
Epoch 3/10
19/19 - 3s - loss: 248.9329 - loglik: -2.4741e+02 - logprior: -9.0145e-01
Epoch 4/10
19/19 - 3s - loss: 247.7126 - loglik: -2.4608e+02 - logprior: -8.4563e-01
Epoch 5/10
19/19 - 3s - loss: 246.3157 - loglik: -2.4460e+02 - logprior: -8.1895e-01
Epoch 6/10
19/19 - 3s - loss: 245.8264 - loglik: -2.4409e+02 - logprior: -7.8470e-01
Epoch 7/10
19/19 - 3s - loss: 244.7507 - loglik: -2.4301e+02 - logprior: -7.7209e-01
Epoch 8/10
19/19 - 3s - loss: 243.8999 - loglik: -2.4218e+02 - logprior: -7.5302e-01
Epoch 9/10
19/19 - 3s - loss: 243.5764 - loglik: -2.4189e+02 - logprior: -7.3382e-01
Epoch 10/10
19/19 - 3s - loss: 242.8383 - loglik: -2.4119e+02 - logprior: -7.1780e-01
Fitted a model with MAP estimate = -241.5294
Time for alignment: 99.6854
Computed alignments with likelihoods: ['-240.8136', '-240.9048', '-241.5294']
Best model has likelihood: -240.8136
time for generating output: 0.1678
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.8832176776650481
Training of 3 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff676891280>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee8102aeb0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff83d579ca0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee9c1b2970>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66b701d00>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fee9052abb0>, <__main__.SimpleDirichletPrior object at 0x7ff675e425e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.4978 - loglik: -1.8326e+02 - logprior: -3.1978e+00
Epoch 2/10
19/19 - 2s - loss: 154.4903 - loglik: -1.5293e+02 - logprior: -1.4511e+00
Epoch 3/10
19/19 - 2s - loss: 141.1026 - loglik: -1.3914e+02 - logprior: -1.6572e+00
Epoch 4/10
19/19 - 2s - loss: 137.4186 - loglik: -1.3558e+02 - logprior: -1.5789e+00
Epoch 5/10
19/19 - 2s - loss: 136.0883 - loglik: -1.3427e+02 - logprior: -1.6005e+00
Epoch 6/10
19/19 - 2s - loss: 135.6449 - loglik: -1.3388e+02 - logprior: -1.5814e+00
Epoch 7/10
19/19 - 2s - loss: 135.2219 - loglik: -1.3348e+02 - logprior: -1.5705e+00
Epoch 8/10
19/19 - 2s - loss: 135.4253 - loglik: -1.3371e+02 - logprior: -1.5526e+00
Fitted a model with MAP estimate = -134.8391
expansions: [(12, 1), (14, 1), (16, 2), (18, 1), (22, 2), (23, 2), (30, 1), (33, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 141.1333 - loglik: -1.3701e+02 - logprior: -4.0998e+00
Epoch 2/2
19/19 - 2s - loss: 130.9304 - loglik: -1.2878e+02 - logprior: -2.0518e+00
Fitted a model with MAP estimate = -128.8529
expansions: [(0, 2)]
discards: [ 0 18 29]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 131.2214 - loglik: -1.2810e+02 - logprior: -3.0036e+00
Epoch 2/2
19/19 - 2s - loss: 127.2417 - loglik: -1.2585e+02 - logprior: -1.1900e+00
Fitted a model with MAP estimate = -125.9157
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.4912 - loglik: -1.2966e+02 - logprior: -3.8071e+00
Epoch 2/10
19/19 - 2s - loss: 128.3181 - loglik: -1.2679e+02 - logprior: -1.3832e+00
Epoch 3/10
19/19 - 2s - loss: 126.5198 - loglik: -1.2501e+02 - logprior: -1.1837e+00
Epoch 4/10
19/19 - 2s - loss: 125.7112 - loglik: -1.2420e+02 - logprior: -1.1661e+00
Epoch 5/10
19/19 - 1s - loss: 125.0457 - loglik: -1.2361e+02 - logprior: -1.1417e+00
Epoch 6/10
19/19 - 2s - loss: 124.8383 - loglik: -1.2345e+02 - logprior: -1.1375e+00
Epoch 7/10
19/19 - 2s - loss: 124.6752 - loglik: -1.2333e+02 - logprior: -1.1146e+00
Epoch 8/10
19/19 - 2s - loss: 124.5944 - loglik: -1.2328e+02 - logprior: -1.1057e+00
Epoch 9/10
19/19 - 2s - loss: 124.2207 - loglik: -1.2293e+02 - logprior: -1.0883e+00
Epoch 10/10
19/19 - 2s - loss: 124.4107 - loglik: -1.2314e+02 - logprior: -1.0743e+00
Fitted a model with MAP estimate = -123.9957
Time for alignment: 60.3368
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 186.5023 - loglik: -1.8327e+02 - logprior: -3.2040e+00
Epoch 2/10
19/19 - 2s - loss: 155.1221 - loglik: -1.5346e+02 - logprior: -1.5571e+00
Epoch 3/10
19/19 - 1s - loss: 140.9863 - loglik: -1.3896e+02 - logprior: -1.6830e+00
Epoch 4/10
19/19 - 1s - loss: 137.9642 - loglik: -1.3602e+02 - logprior: -1.6719e+00
Epoch 5/10
19/19 - 2s - loss: 136.8799 - loglik: -1.3503e+02 - logprior: -1.6348e+00
Epoch 6/10
19/19 - 2s - loss: 136.3387 - loglik: -1.3454e+02 - logprior: -1.6185e+00
Epoch 7/10
19/19 - 2s - loss: 136.3023 - loglik: -1.3451e+02 - logprior: -1.6124e+00
Epoch 8/10
19/19 - 2s - loss: 136.1218 - loglik: -1.3435e+02 - logprior: -1.6017e+00
Epoch 9/10
19/19 - 2s - loss: 135.5799 - loglik: -1.3377e+02 - logprior: -1.6176e+00
Epoch 10/10
19/19 - 1s - loss: 135.3919 - loglik: -1.3359e+02 - logprior: -1.6158e+00
Fitted a model with MAP estimate = -135.0223
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (20, 2), (21, 1), (22, 1), (23, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 137.6158 - loglik: -1.3427e+02 - logprior: -3.3183e+00
Epoch 2/2
19/19 - 2s - loss: 128.1583 - loglik: -1.2667e+02 - logprior: -1.3901e+00
Fitted a model with MAP estimate = -126.6868
expansions: []
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 130.8704 - loglik: -1.2764e+02 - logprior: -3.1940e+00
Epoch 2/2
19/19 - 2s - loss: 127.3897 - loglik: -1.2593e+02 - logprior: -1.3270e+00
Fitted a model with MAP estimate = -126.1783
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 130.5137 - loglik: -1.2724e+02 - logprior: -3.1763e+00
Epoch 2/10
19/19 - 1s - loss: 126.9647 - loglik: -1.2544e+02 - logprior: -1.3085e+00
Epoch 3/10
19/19 - 2s - loss: 126.0251 - loglik: -1.2451e+02 - logprior: -1.1854e+00
Epoch 4/10
19/19 - 2s - loss: 125.0925 - loglik: -1.2358e+02 - logprior: -1.1572e+00
Epoch 5/10
19/19 - 2s - loss: 124.6847 - loglik: -1.2324e+02 - logprior: -1.1352e+00
Epoch 6/10
19/19 - 2s - loss: 124.5669 - loglik: -1.2318e+02 - logprior: -1.1156e+00
Epoch 7/10
19/19 - 2s - loss: 124.1149 - loglik: -1.2277e+02 - logprior: -1.1004e+00
Epoch 8/10
19/19 - 2s - loss: 124.2120 - loglik: -1.2290e+02 - logprior: -1.0837e+00
Fitted a model with MAP estimate = -123.7674
Time for alignment: 60.4459
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 186.4377 - loglik: -1.8315e+02 - logprior: -3.1994e+00
Epoch 2/10
19/19 - 2s - loss: 156.4106 - loglik: -1.5486e+02 - logprior: -1.4793e+00
Epoch 3/10
19/19 - 2s - loss: 143.2222 - loglik: -1.4131e+02 - logprior: -1.5863e+00
Epoch 4/10
19/19 - 2s - loss: 137.5510 - loglik: -1.3562e+02 - logprior: -1.6615e+00
Epoch 5/10
19/19 - 2s - loss: 136.3895 - loglik: -1.3461e+02 - logprior: -1.5793e+00
Epoch 6/10
19/19 - 1s - loss: 135.8983 - loglik: -1.3412e+02 - logprior: -1.5922e+00
Epoch 7/10
19/19 - 2s - loss: 135.4963 - loglik: -1.3374e+02 - logprior: -1.5780e+00
Epoch 8/10
19/19 - 2s - loss: 135.5674 - loglik: -1.3384e+02 - logprior: -1.5696e+00
Fitted a model with MAP estimate = -135.0975
expansions: [(12, 1), (14, 1), (16, 1), (17, 1), (18, 1), (21, 1), (22, 2), (23, 1), (33, 1), (34, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 140.3258 - loglik: -1.3605e+02 - logprior: -4.0963e+00
Epoch 2/2
19/19 - 2s - loss: 130.5003 - loglik: -1.2832e+02 - logprior: -2.0319e+00
Fitted a model with MAP estimate = -128.4868
expansions: [(0, 2)]
discards: [ 0 28]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 131.2304 - loglik: -1.2818e+02 - logprior: -3.0002e+00
Epoch 2/2
19/19 - 2s - loss: 127.2970 - loglik: -1.2607e+02 - logprior: -1.1699e+00
Fitted a model with MAP estimate = -126.1205
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 133.0006 - loglik: -1.2914e+02 - logprior: -3.7952e+00
Epoch 2/10
19/19 - 2s - loss: 127.8255 - loglik: -1.2630e+02 - logprior: -1.3723e+00
Epoch 3/10
19/19 - 2s - loss: 126.5091 - loglik: -1.2502e+02 - logprior: -1.1906e+00
Epoch 4/10
19/19 - 2s - loss: 125.2872 - loglik: -1.2378e+02 - logprior: -1.1645e+00
Epoch 5/10
19/19 - 2s - loss: 125.2009 - loglik: -1.2375e+02 - logprior: -1.1453e+00
Epoch 6/10
19/19 - 2s - loss: 124.5558 - loglik: -1.2316e+02 - logprior: -1.1353e+00
Epoch 7/10
19/19 - 2s - loss: 124.6465 - loglik: -1.2329e+02 - logprior: -1.1154e+00
Fitted a model with MAP estimate = -124.1419
Time for alignment: 56.5588
Computed alignments with likelihoods: ['-123.9957', '-123.7674', '-124.1419']
Best model has likelihood: -123.7674
time for generating output: 0.1226
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.9320376330774944
Training of 3 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff676bc8490>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee9c780490>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff8572e8df0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff478b41310>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff5001ece80>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fee89e0a670>, <__main__.SimpleDirichletPrior object at 0x7ff66d1cf460>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 727.7848 - loglik: -7.2574e+02 - logprior: -1.6876e+00
Epoch 2/10
39/39 - 24s - loss: 611.2906 - loglik: -6.0848e+02 - logprior: -1.7066e+00
Epoch 3/10
39/39 - 25s - loss: 599.1868 - loglik: -5.9625e+02 - logprior: -1.7582e+00
Epoch 4/10
39/39 - 25s - loss: 596.0660 - loglik: -5.9322e+02 - logprior: -1.7680e+00
Epoch 5/10
39/39 - 25s - loss: 594.3625 - loglik: -5.9161e+02 - logprior: -1.7962e+00
Epoch 6/10
39/39 - 25s - loss: 594.2728 - loglik: -5.9159e+02 - logprior: -1.8039e+00
Epoch 7/10
39/39 - 25s - loss: 592.6915 - loglik: -5.9006e+02 - logprior: -1.8082e+00
Epoch 8/10
39/39 - 25s - loss: 592.7361 - loglik: -5.9016e+02 - logprior: -1.8073e+00
Fitted a model with MAP estimate = -590.6905
expansions: [(10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (33, 1), (42, 1), (44, 1), (45, 1), (46, 1), (51, 1), (67, 1), (68, 1), (69, 2), (71, 1), (72, 1), (78, 1), (86, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (123, 1), (132, 1), (134, 1), (136, 2), (137, 1), (147, 1), (154, 3), (155, 5), (163, 3), (164, 2), (174, 1), (175, 1), (177, 1), (180, 1), (183, 1), (185, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 591.5651 - loglik: -5.8841e+02 - logprior: -2.6353e+00
Epoch 2/2
39/39 - 40s - loss: 565.4574 - loglik: -5.6260e+02 - logprior: -1.5839e+00
Fitted a model with MAP estimate = -557.5695
expansions: [(0, 3), (149, 1)]
discards: [  0  86 249]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 575.8846 - loglik: -5.7411e+02 - logprior: -1.5536e+00
Epoch 2/2
39/39 - 45s - loss: 561.2132 - loglik: -5.5991e+02 - logprior: -4.9781e-01
Fitted a model with MAP estimate = -555.3385
expansions: [(46, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 314 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 573.2117 - loglik: -5.7080e+02 - logprior: -2.1066e+00
Epoch 2/10
39/39 - 43s - loss: 560.3643 - loglik: -5.5867e+02 - logprior: -4.3561e-01
Epoch 3/10
39/39 - 45s - loss: 555.0685 - loglik: -5.5341e+02 - logprior: -1.1415e-01
Epoch 4/10
39/39 - 48s - loss: 553.1454 - loglik: -5.5176e+02 - logprior: -6.7186e-03
Epoch 5/10
39/39 - 50s - loss: 552.0435 - loglik: -5.5096e+02 - logprior: 0.1259
Epoch 6/10
39/39 - 46s - loss: 551.4304 - loglik: -5.5061e+02 - logprior: 0.2612
Epoch 7/10
39/39 - 48s - loss: 550.7100 - loglik: -5.5008e+02 - logprior: 0.3556
Epoch 8/10
39/39 - 47s - loss: 550.0793 - loglik: -5.4966e+02 - logprior: 0.4955
Epoch 9/10
39/39 - 48s - loss: 549.9644 - loglik: -5.4972e+02 - logprior: 0.6429
Epoch 10/10
39/39 - 47s - loss: 549.4529 - loglik: -5.4939e+02 - logprior: 0.7646
Fitted a model with MAP estimate = -548.0668
Time for alignment: 1024.1752
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 729.6078 - loglik: -7.2756e+02 - logprior: -1.7010e+00
Epoch 2/10
39/39 - 30s - loss: 614.7291 - loglik: -6.1195e+02 - logprior: -1.7139e+00
Epoch 3/10
39/39 - 28s - loss: 599.6923 - loglik: -5.9670e+02 - logprior: -1.8445e+00
Epoch 4/10
39/39 - 29s - loss: 596.8327 - loglik: -5.9394e+02 - logprior: -1.8250e+00
Epoch 5/10
39/39 - 30s - loss: 595.3936 - loglik: -5.9259e+02 - logprior: -1.8264e+00
Epoch 6/10
39/39 - 29s - loss: 594.2558 - loglik: -5.9155e+02 - logprior: -1.8322e+00
Epoch 7/10
39/39 - 29s - loss: 593.5781 - loglik: -5.9093e+02 - logprior: -1.8398e+00
Epoch 8/10
39/39 - 30s - loss: 593.5267 - loglik: -5.9092e+02 - logprior: -1.8477e+00
Epoch 9/10
39/39 - 32s - loss: 592.9798 - loglik: -5.9042e+02 - logprior: -1.8468e+00
Epoch 10/10
39/39 - 33s - loss: 593.0738 - loglik: -5.9055e+02 - logprior: -1.8518e+00
Fitted a model with MAP estimate = -591.0036
expansions: [(10, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (35, 2), (42, 2), (46, 1), (47, 1), (52, 1), (68, 1), (69, 1), (70, 2), (71, 1), (77, 1), (78, 1), (102, 1), (115, 1), (116, 1), (117, 1), (121, 4), (122, 1), (133, 1), (135, 1), (137, 2), (138, 1), (148, 1), (150, 1), (154, 3), (163, 1), (164, 1), (165, 1), (166, 1), (175, 1), (180, 1), (184, 1), (185, 1), (186, 1), (193, 1), (194, 1), (197, 1), (211, 1), (212, 3), (213, 2), (214, 2), (215, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 595.8304 - loglik: -5.9292e+02 - logprior: -2.6295e+00
Epoch 2/2
39/39 - 46s - loss: 571.8094 - loglik: -5.6936e+02 - logprior: -1.5443e+00
Fitted a model with MAP estimate = -564.0824
expansions: [(0, 3), (148, 1), (241, 1)]
discards: [  0 191 264 265]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 46s - loss: 576.3354 - loglik: -5.7461e+02 - logprior: -1.5198e+00
Epoch 2/2
39/39 - 44s - loss: 566.4310 - loglik: -5.6536e+02 - logprior: -4.8649e-01
Fitted a model with MAP estimate = -561.1790
expansions: [(219, 1)]
discards: [ 0  1 52]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 575.0222 - loglik: -5.7239e+02 - logprior: -2.1384e+00
Epoch 2/10
39/39 - 42s - loss: 565.1746 - loglik: -5.6374e+02 - logprior: -4.2703e-01
Epoch 3/10
39/39 - 44s - loss: 559.8662 - loglik: -5.5848e+02 - logprior: -5.1165e-02
Epoch 4/10
39/39 - 46s - loss: 558.6231 - loglik: -5.5733e+02 - logprior: 0.0639
Epoch 5/10
39/39 - 47s - loss: 557.5305 - loglik: -5.5652e+02 - logprior: 0.2096
Epoch 6/10
39/39 - 47s - loss: 556.1523 - loglik: -5.5532e+02 - logprior: 0.2880
Epoch 7/10
39/39 - 47s - loss: 556.0886 - loglik: -5.5556e+02 - logprior: 0.4965
Epoch 8/10
39/39 - 38s - loss: 556.2205 - loglik: -5.5589e+02 - logprior: 0.6273
Fitted a model with MAP estimate = -553.9482
Time for alignment: 1053.4173
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 729.3405 - loglik: -7.2723e+02 - logprior: -1.6921e+00
Epoch 2/10
39/39 - 25s - loss: 613.9197 - loglik: -6.1096e+02 - logprior: -1.7170e+00
Epoch 3/10
39/39 - 25s - loss: 600.6243 - loglik: -5.9764e+02 - logprior: -1.7686e+00
Epoch 4/10
39/39 - 25s - loss: 597.7698 - loglik: -5.9497e+02 - logprior: -1.7483e+00
Epoch 5/10
39/39 - 26s - loss: 595.7762 - loglik: -5.9309e+02 - logprior: -1.7481e+00
Epoch 6/10
39/39 - 26s - loss: 594.9830 - loglik: -5.9238e+02 - logprior: -1.7542e+00
Epoch 7/10
39/39 - 26s - loss: 594.7630 - loglik: -5.9220e+02 - logprior: -1.7554e+00
Epoch 8/10
39/39 - 26s - loss: 593.9777 - loglik: -5.9147e+02 - logprior: -1.7611e+00
Epoch 9/10
39/39 - 26s - loss: 594.0680 - loglik: -5.9160e+02 - logprior: -1.7631e+00
Fitted a model with MAP estimate = -592.6258
expansions: [(10, 1), (14, 1), (15, 1), (16, 1), (17, 2), (19, 1), (33, 1), (35, 1), (42, 2), (46, 1), (47, 1), (48, 1), (51, 1), (67, 1), (68, 1), (69, 1), (70, 2), (72, 1), (76, 1), (86, 1), (115, 1), (116, 1), (117, 1), (121, 6), (132, 2), (134, 1), (137, 1), (147, 1), (154, 2), (163, 1), (164, 1), (165, 1), (166, 1), (180, 1), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 5), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [  0 184]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 594.4975 - loglik: -5.9146e+02 - logprior: -2.6197e+00
Epoch 2/2
39/39 - 35s - loss: 573.5240 - loglik: -5.7111e+02 - logprior: -1.5197e+00
Fitted a model with MAP estimate = -566.6042
expansions: [(0, 3), (149, 1), (170, 2)]
discards: [  0  87 163 235 260]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 578.4581 - loglik: -5.7665e+02 - logprior: -1.5452e+00
Epoch 2/2
39/39 - 36s - loss: 569.0781 - loglik: -5.6799e+02 - logprior: -4.7536e-01
Fitted a model with MAP estimate = -563.7426
expansions: []
discards: [  0   1  52 185]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 581.9626 - loglik: -5.7960e+02 - logprior: -2.1295e+00
Epoch 2/10
39/39 - 35s - loss: 569.7545 - loglik: -5.6890e+02 - logprior: -3.4856e-01
Epoch 3/10
39/39 - 34s - loss: 563.6425 - loglik: -5.6277e+02 - logprior: -8.7705e-02
Epoch 4/10
39/39 - 34s - loss: 561.7354 - loglik: -5.6073e+02 - logprior: -3.1534e-02
Epoch 5/10
39/39 - 33s - loss: 560.4051 - loglik: -5.5955e+02 - logprior: 0.1745
Epoch 6/10
39/39 - 33s - loss: 560.0146 - loglik: -5.5926e+02 - logprior: 0.2675
Epoch 7/10
39/39 - 33s - loss: 558.6392 - loglik: -5.5810e+02 - logprior: 0.4411
Epoch 8/10
39/39 - 33s - loss: 558.6957 - loglik: -5.5831e+02 - logprior: 0.5408
Fitted a model with MAP estimate = -557.3834
Time for alignment: 811.6290
Computed alignments with likelihoods: ['-548.0668', '-553.9482', '-557.3834']
Best model has likelihood: -548.0668
time for generating output: 0.3382
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.34195631908376617
Training of 3 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff66daf80a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee907d5070>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff67fc18940>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff6881fff10>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff834ddf5e0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff67f9ec7c0>, <__main__.SimpleDirichletPrior object at 0x7fee91817640>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 969.1647 - loglik: -9.6718e+02 - logprior: -1.4069e+00
Epoch 2/10
39/39 - 47s - loss: 802.0737 - loglik: -7.9934e+02 - logprior: -1.3225e+00
Epoch 3/10
39/39 - 48s - loss: 789.4896 - loglik: -7.8638e+02 - logprior: -1.3638e+00
Epoch 4/10
39/39 - 48s - loss: 786.4601 - loglik: -7.8336e+02 - logprior: -1.3419e+00
Epoch 5/10
39/39 - 48s - loss: 784.5400 - loglik: -7.8153e+02 - logprior: -1.3691e+00
Epoch 6/10
39/39 - 47s - loss: 783.8431 - loglik: -7.8088e+02 - logprior: -1.4529e+00
Epoch 7/10
39/39 - 46s - loss: 782.9292 - loglik: -7.8012e+02 - logprior: -1.4211e+00
Epoch 8/10
39/39 - 46s - loss: 782.2172 - loglik: -7.7951e+02 - logprior: -1.4203e+00
Epoch 9/10
39/39 - 46s - loss: 781.7086 - loglik: -7.7905e+02 - logprior: -1.4570e+00
Epoch 10/10
39/39 - 46s - loss: 781.4896 - loglik: -7.7891e+02 - logprior: -1.4494e+00
Fitted a model with MAP estimate = -771.6831
expansions: [(0, 3), (5, 1), (34, 1), (50, 1), (51, 1), (52, 1), (56, 2), (69, 1), (70, 1), (73, 1), (74, 1), (121, 2), (122, 2), (123, 2), (131, 1), (146, 3), (147, 2), (166, 1), (169, 1), (175, 1), (176, 1), (188, 1), (192, 2), (195, 1), (196, 1), (205, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (260, 1), (264, 1), (285, 7), (287, 1), (288, 1), (289, 1), (292, 1), (296, 3), (297, 2), (298, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 798.9525 - loglik: -7.9641e+02 - logprior: -2.1914e+00
Epoch 2/2
39/39 - 77s - loss: 753.0749 - loglik: -7.5151e+02 - logprior: -6.7699e-01
Fitted a model with MAP estimate = -739.4791
expansions: [(347, 4)]
discards: [ 65 139 170 224 390]
Re-initialized the encoder parameters.
Fitting a model of length 395 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 87s - loss: 761.8058 - loglik: -7.6014e+02 - logprior: -1.2527e+00
Epoch 2/2
39/39 - 83s - loss: 749.5540 - loglik: -7.4805e+02 - logprior: -1.5063e-01
Fitted a model with MAP estimate = -736.3739
expansions: []
discards: [  1 332 335 336]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 88s - loss: 756.5353 - loglik: -7.5532e+02 - logprior: -1.1004e+00
Epoch 2/10
39/39 - 84s - loss: 744.0490 - loglik: -7.4370e+02 - logprior: 0.1041
Epoch 3/10
39/39 - 82s - loss: 739.1367 - loglik: -7.3868e+02 - logprior: 0.3159
Epoch 4/10
39/39 - 81s - loss: 737.3392 - loglik: -7.3681e+02 - logprior: 0.4638
Epoch 5/10
39/39 - 80s - loss: 735.9514 - loglik: -7.3540e+02 - logprior: 0.5983
Epoch 6/10
39/39 - 85s - loss: 735.0823 - loglik: -7.3461e+02 - logprior: 0.7494
Epoch 7/10
39/39 - 65s - loss: 734.6939 - loglik: -7.3433e+02 - logprior: 0.8771
Epoch 8/10
39/39 - 62s - loss: 733.6875 - loglik: -7.3347e+02 - logprior: 1.0148
Epoch 9/10
39/39 - 62s - loss: 733.3380 - loglik: -7.3325e+02 - logprior: 1.1459
Epoch 10/10
39/39 - 63s - loss: 732.3865 - loglik: -7.3246e+02 - logprior: 1.2673
Fitted a model with MAP estimate = -730.1669
Time for alignment: 1955.2642
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 973.2673 - loglik: -9.7138e+02 - logprior: -1.4044e+00
Epoch 2/10
39/39 - 48s - loss: 804.9429 - loglik: -8.0221e+02 - logprior: -1.3767e+00
Epoch 3/10
39/39 - 51s - loss: 792.0767 - loglik: -7.8891e+02 - logprior: -1.4428e+00
Epoch 4/10
39/39 - 54s - loss: 789.1353 - loglik: -7.8602e+02 - logprior: -1.3850e+00
Epoch 5/10
39/39 - 56s - loss: 787.4427 - loglik: -7.8439e+02 - logprior: -1.4049e+00
Epoch 6/10
39/39 - 53s - loss: 785.6840 - loglik: -7.8276e+02 - logprior: -1.4000e+00
Epoch 7/10
39/39 - 55s - loss: 785.5794 - loglik: -7.8258e+02 - logprior: -1.5493e+00
Epoch 8/10
39/39 - 53s - loss: 784.4849 - loglik: -7.8162e+02 - logprior: -1.5353e+00
Epoch 9/10
39/39 - 54s - loss: 784.0277 - loglik: -7.8123e+02 - logprior: -1.5459e+00
Epoch 10/10
39/39 - 53s - loss: 783.5303 - loglik: -7.8082e+02 - logprior: -1.5614e+00
Fitted a model with MAP estimate = -773.9036
expansions: [(0, 3), (15, 1), (44, 1), (52, 1), (53, 1), (59, 1), (62, 1), (70, 1), (74, 1), (75, 2), (121, 2), (122, 2), (123, 1), (147, 3), (149, 1), (164, 1), (171, 1), (174, 1), (176, 1), (177, 1), (186, 1), (193, 2), (196, 1), (197, 3), (218, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (246, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 2), (257, 1), (258, 1), (263, 1), (264, 1), (265, 2), (284, 7), (285, 3), (286, 1), (287, 1), (295, 1), (296, 1), (297, 2), (298, 1), (313, 2), (314, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 793.1768 - loglik: -7.9079e+02 - logprior: -2.1368e+00
Epoch 2/2
39/39 - 82s - loss: 752.9299 - loglik: -7.5148e+02 - logprior: -5.7489e-01
Fitted a model with MAP estimate = -739.7295
expansions: [(348, 2)]
discards: [139 222 228 319]
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 80s - loss: 762.9357 - loglik: -7.6141e+02 - logprior: -1.3507e+00
Epoch 2/2
39/39 - 78s - loss: 751.1686 - loglik: -7.5045e+02 - logprior: -2.0057e-01
Fitted a model with MAP estimate = -739.3876
expansions: []
discards: [  1 335 337 339 366]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 86s - loss: 755.3430 - loglik: -7.5431e+02 - logprior: -8.8280e-01
Epoch 2/10
39/39 - 81s - loss: 745.0488 - loglik: -7.4476e+02 - logprior: 0.1655
Epoch 3/10
39/39 - 81s - loss: 740.6850 - loglik: -7.4023e+02 - logprior: 0.3046
Epoch 4/10
39/39 - 76s - loss: 737.6599 - loglik: -7.3711e+02 - logprior: 0.4694
Epoch 5/10
39/39 - 78s - loss: 737.6324 - loglik: -7.3702e+02 - logprior: 0.5731
Epoch 6/10
39/39 - 79s - loss: 735.8384 - loglik: -7.3530e+02 - logprior: 0.7095
Epoch 7/10
39/39 - 79s - loss: 735.3210 - loglik: -7.3492e+02 - logprior: 0.8857
Epoch 8/10
39/39 - 87s - loss: 735.0194 - loglik: -7.3474e+02 - logprior: 0.9955
Epoch 9/10
39/39 - 90s - loss: 733.6031 - loglik: -7.3347e+02 - logprior: 1.1192
Epoch 10/10
39/39 - 82s - loss: 733.5873 - loglik: -7.3368e+02 - logprior: 1.2979
Fitted a model with MAP estimate = -730.9788
Time for alignment: 2097.8693
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 969.3585 - loglik: -9.6762e+02 - logprior: -1.3932e+00
Epoch 2/10
39/39 - 51s - loss: 800.3828 - loglik: -7.9793e+02 - logprior: -1.3026e+00
Epoch 3/10
39/39 - 49s - loss: 787.9555 - loglik: -7.8504e+02 - logprior: -1.3765e+00
Epoch 4/10
39/39 - 48s - loss: 784.8313 - loglik: -7.8185e+02 - logprior: -1.3699e+00
Epoch 5/10
39/39 - 47s - loss: 783.0145 - loglik: -7.8001e+02 - logprior: -1.3953e+00
Epoch 6/10
39/39 - 48s - loss: 782.1536 - loglik: -7.7923e+02 - logprior: -1.4163e+00
Epoch 7/10
39/39 - 48s - loss: 781.0512 - loglik: -7.7822e+02 - logprior: -1.4229e+00
Epoch 8/10
39/39 - 50s - loss: 780.5738 - loglik: -7.7786e+02 - logprior: -1.4314e+00
Epoch 9/10
39/39 - 50s - loss: 780.4250 - loglik: -7.7776e+02 - logprior: -1.4582e+00
Epoch 10/10
39/39 - 49s - loss: 779.7276 - loglik: -7.7713e+02 - logprior: -1.4838e+00
Fitted a model with MAP estimate = -770.1846
expansions: [(0, 3), (19, 1), (47, 1), (51, 1), (52, 1), (56, 1), (60, 1), (68, 1), (69, 1), (72, 1), (73, 2), (103, 1), (118, 1), (120, 1), (121, 1), (145, 3), (147, 1), (166, 1), (169, 1), (175, 1), (176, 1), (188, 1), (192, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (260, 1), (264, 1), (266, 2), (285, 7), (286, 3), (287, 1), (288, 1), (296, 3), (297, 2), (298, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 395 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 791.2110 - loglik: -7.8842e+02 - logprior: -2.3689e+00
Epoch 2/2
39/39 - 67s - loss: 753.1773 - loglik: -7.5114e+02 - logprior: -9.2310e-01
Fitted a model with MAP estimate = -739.1915
expansions: [(345, 1)]
discards: [220 316 389]
Re-initialized the encoder parameters.
Fitting a model of length 393 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 764.6362 - loglik: -7.6317e+02 - logprior: -1.2021e+00
Epoch 2/2
39/39 - 65s - loss: 749.8295 - loglik: -7.4865e+02 - logprior: -1.0052e-01
Fitted a model with MAP estimate = -736.9172
expansions: []
discards: [334]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 752.7842 - loglik: -7.5145e+02 - logprior: -9.2560e-01
Epoch 2/10
39/39 - 75s - loss: 741.7409 - loglik: -7.4070e+02 - logprior: 0.2075
Epoch 3/10
39/39 - 85s - loss: 737.5525 - loglik: -7.3613e+02 - logprior: 0.3386
Epoch 4/10
39/39 - 90s - loss: 735.6763 - loglik: -7.3430e+02 - logprior: 0.4885
Epoch 5/10
39/39 - 91s - loss: 736.0819 - loglik: -7.3494e+02 - logprior: 0.6193
Fitted a model with MAP estimate = -732.4343
Time for alignment: 1516.8577
Computed alignments with likelihoods: ['-730.1669', '-730.9788', '-732.4343']
Best model has likelihood: -730.1669
time for generating output: 1.7503
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.34594383307967824
Training of 3 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff8466da700>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66af74e80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6984fdeb0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff84609a940>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7feea4c8dc40>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff84ea37fa0>, <__main__.SimpleDirichletPrior object at 0x7ff676426370>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.8294 - loglik: -1.3051e+02 - logprior: -3.2207e+00
Epoch 2/10
19/19 - 1s - loss: 109.3099 - loglik: -1.0782e+02 - logprior: -1.4344e+00
Epoch 3/10
19/19 - 1s - loss: 100.4094 - loglik: -9.8688e+01 - logprior: -1.4995e+00
Epoch 4/10
19/19 - 1s - loss: 98.6113 - loglik: -9.7035e+01 - logprior: -1.3990e+00
Epoch 5/10
19/19 - 1s - loss: 98.0033 - loglik: -9.6508e+01 - logprior: -1.3654e+00
Epoch 6/10
19/19 - 1s - loss: 98.0258 - loglik: -9.6561e+01 - logprior: -1.3481e+00
Fitted a model with MAP estimate = -97.5978
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 100.5979 - loglik: -9.6253e+01 - logprior: -4.1548e+00
Epoch 2/2
19/19 - 1s - loss: 92.9737 - loglik: -9.1374e+01 - logprior: -1.4324e+00
Fitted a model with MAP estimate = -91.8607
expansions: []
discards: [ 0 10 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.8109 - loglik: -9.3629e+01 - logprior: -4.0916e+00
Epoch 2/2
19/19 - 1s - loss: 93.3745 - loglik: -9.1683e+01 - logprior: -1.5856e+00
Fitted a model with MAP estimate = -92.6514
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 96.0381 - loglik: -9.2737e+01 - logprior: -3.2789e+00
Epoch 2/10
19/19 - 1s - loss: 93.2467 - loglik: -9.1722e+01 - logprior: -1.4669e+00
Epoch 3/10
19/19 - 1s - loss: 92.5950 - loglik: -9.1099e+01 - logprior: -1.3718e+00
Epoch 4/10
19/19 - 1s - loss: 92.3350 - loglik: -9.0860e+01 - logprior: -1.3217e+00
Epoch 5/10
19/19 - 1s - loss: 92.1377 - loglik: -9.0692e+01 - logprior: -1.2935e+00
Epoch 6/10
19/19 - 1s - loss: 92.1365 - loglik: -9.0711e+01 - logprior: -1.2771e+00
Epoch 7/10
19/19 - 1s - loss: 91.8273 - loglik: -9.0419e+01 - logprior: -1.2615e+00
Epoch 8/10
19/19 - 1s - loss: 91.8909 - loglik: -9.0508e+01 - logprior: -1.2453e+00
Fitted a model with MAP estimate = -91.7035
Time for alignment: 44.5364
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.7644 - loglik: -1.3053e+02 - logprior: -3.2282e+00
Epoch 2/10
19/19 - 1s - loss: 109.1145 - loglik: -1.0761e+02 - logprior: -1.4204e+00
Epoch 3/10
19/19 - 1s - loss: 101.3771 - loglik: -9.9628e+01 - logprior: -1.5141e+00
Epoch 4/10
19/19 - 1s - loss: 98.6713 - loglik: -9.7101e+01 - logprior: -1.3889e+00
Epoch 5/10
19/19 - 1s - loss: 98.1353 - loglik: -9.6632e+01 - logprior: -1.3655e+00
Epoch 6/10
19/19 - 1s - loss: 97.8327 - loglik: -9.6368e+01 - logprior: -1.3481e+00
Epoch 7/10
19/19 - 1s - loss: 97.8416 - loglik: -9.6401e+01 - logprior: -1.3335e+00
Fitted a model with MAP estimate = -97.5448
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 100.5369 - loglik: -9.6146e+01 - logprior: -4.2078e+00
Epoch 2/2
19/19 - 1s - loss: 92.9779 - loglik: -9.1373e+01 - logprior: -1.4286e+00
Fitted a model with MAP estimate = -91.8184
expansions: []
discards: [ 0 11 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 98.4909 - loglik: -9.4341e+01 - logprior: -4.0776e+00
Epoch 2/2
19/19 - 1s - loss: 93.6554 - loglik: -9.2043e+01 - logprior: -1.5827e+00
Fitted a model with MAP estimate = -92.9032
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 95.7879 - loglik: -9.2475e+01 - logprior: -3.2806e+00
Epoch 2/10
19/19 - 1s - loss: 93.0913 - loglik: -9.1508e+01 - logprior: -1.4683e+00
Epoch 3/10
19/19 - 1s - loss: 92.6799 - loglik: -9.1142e+01 - logprior: -1.3762e+00
Epoch 4/10
19/19 - 1s - loss: 92.3294 - loglik: -9.0834e+01 - logprior: -1.3303e+00
Epoch 5/10
19/19 - 1s - loss: 92.0612 - loglik: -9.0610e+01 - logprior: -1.2952e+00
Epoch 6/10
19/19 - 1s - loss: 91.9797 - loglik: -9.0548e+01 - logprior: -1.2842e+00
Epoch 7/10
19/19 - 1s - loss: 92.0544 - loglik: -9.0647e+01 - logprior: -1.2632e+00
Fitted a model with MAP estimate = -91.7564
Time for alignment: 43.4756
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 133.7152 - loglik: -1.3043e+02 - logprior: -3.2269e+00
Epoch 2/10
19/19 - 1s - loss: 109.0610 - loglik: -1.0754e+02 - logprior: -1.4090e+00
Epoch 3/10
19/19 - 1s - loss: 100.7571 - loglik: -9.8951e+01 - logprior: -1.5818e+00
Epoch 4/10
19/19 - 1s - loss: 98.4556 - loglik: -9.6832e+01 - logprior: -1.4533e+00
Epoch 5/10
19/19 - 1s - loss: 97.9623 - loglik: -9.6379e+01 - logprior: -1.4388e+00
Epoch 6/10
19/19 - 1s - loss: 97.7830 - loglik: -9.6235e+01 - logprior: -1.4223e+00
Epoch 7/10
19/19 - 1s - loss: 97.5942 - loglik: -9.6066e+01 - logprior: -1.4068e+00
Epoch 8/10
19/19 - 1s - loss: 97.5910 - loglik: -9.6076e+01 - logprior: -1.3980e+00
Epoch 9/10
19/19 - 1s - loss: 97.4409 - loglik: -9.5933e+01 - logprior: -1.3915e+00
Epoch 10/10
19/19 - 1s - loss: 97.5390 - loglik: -9.6036e+01 - logprior: -1.3877e+00
Fitted a model with MAP estimate = -97.2317
expansions: [(7, 1), (8, 3), (9, 2), (13, 2), (18, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 103.2218 - loglik: -9.8988e+01 - logprior: -4.2024e+00
Epoch 2/2
19/19 - 1s - loss: 95.3653 - loglik: -9.3073e+01 - logprior: -2.1611e+00
Fitted a model with MAP estimate = -93.6765
expansions: [(0, 2)]
discards: [ 0  9 12 19 30 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 95.9261 - loglik: -9.2731e+01 - logprior: -3.0775e+00
Epoch 2/2
19/19 - 1s - loss: 92.6653 - loglik: -9.1317e+01 - logprior: -1.2676e+00
Fitted a model with MAP estimate = -91.9813
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 96.6946 - loglik: -9.3017e+01 - logprior: -3.5571e+00
Epoch 2/10
19/19 - 1s - loss: 93.2146 - loglik: -9.1601e+01 - logprior: -1.4540e+00
Epoch 3/10
19/19 - 1s - loss: 92.6205 - loglik: -9.1071e+01 - logprior: -1.3711e+00
Epoch 4/10
19/19 - 1s - loss: 92.2734 - loglik: -9.0773e+01 - logprior: -1.3226e+00
Epoch 5/10
19/19 - 1s - loss: 92.2242 - loglik: -9.0767e+01 - logprior: -1.2964e+00
Epoch 6/10
19/19 - 1s - loss: 91.9484 - loglik: -9.0516e+01 - logprior: -1.2784e+00
Epoch 7/10
19/19 - 1s - loss: 92.0130 - loglik: -9.0598e+01 - logprior: -1.2645e+00
Fitted a model with MAP estimate = -91.7393
Time for alignment: 47.0913
Computed alignments with likelihoods: ['-91.7035', '-91.7564', '-91.7393']
Best model has likelihood: -91.7035
time for generating output: 0.1120
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.8421764617465805
Training of 3 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fee88b85af0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff7ce132e50>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff8462a7c40>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff874bc1220>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff5000ff430>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff66c2e49d0>, <__main__.SimpleDirichletPrior object at 0x7fee9d6c8b50>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 472.5082 - loglik: -4.7061e+02 - logprior: -1.8389e+00
Epoch 2/10
39/39 - 12s - loss: 371.1358 - loglik: -3.6894e+02 - logprior: -1.8362e+00
Epoch 3/10
39/39 - 12s - loss: 363.0275 - loglik: -3.6084e+02 - logprior: -1.8228e+00
Epoch 4/10
39/39 - 12s - loss: 360.5150 - loglik: -3.5842e+02 - logprior: -1.7584e+00
Epoch 5/10
39/39 - 12s - loss: 359.6010 - loglik: -3.5758e+02 - logprior: -1.7152e+00
Epoch 6/10
39/39 - 12s - loss: 358.9949 - loglik: -3.5699e+02 - logprior: -1.7089e+00
Epoch 7/10
39/39 - 12s - loss: 358.1150 - loglik: -3.5613e+02 - logprior: -1.7038e+00
Epoch 8/10
39/39 - 11s - loss: 357.9836 - loglik: -3.5602e+02 - logprior: -1.7012e+00
Epoch 9/10
39/39 - 11s - loss: 357.7542 - loglik: -3.5581e+02 - logprior: -1.6981e+00
Epoch 10/10
39/39 - 12s - loss: 357.3771 - loglik: -3.5545e+02 - logprior: -1.6687e+00
Fitted a model with MAP estimate = -356.0346
expansions: [(4, 1), (5, 2), (8, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 2), (41, 1), (42, 2), (58, 1), (59, 1), (61, 2), (63, 1), (70, 1), (74, 1), (93, 1), (97, 2), (99, 1), (102, 1), (103, 1), (108, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 343.2714 - loglik: -3.4128e+02 - logprior: -1.8790e+00
Epoch 2/2
39/39 - 16s - loss: 328.4669 - loglik: -3.2738e+02 - logprior: -8.5853e-01
Fitted a model with MAP estimate = -325.4254
expansions: []
discards: [ 51  56  79 121]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 332.5890 - loglik: -3.3075e+02 - logprior: -1.6685e+00
Epoch 2/2
39/39 - 16s - loss: 327.6327 - loglik: -3.2669e+02 - logprior: -5.9914e-01
Fitted a model with MAP estimate = -325.3103
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 332.3148 - loglik: -3.3066e+02 - logprior: -1.4995e+00
Epoch 2/10
39/39 - 16s - loss: 327.2420 - loglik: -3.2646e+02 - logprior: -4.3315e-01
Epoch 3/10
39/39 - 16s - loss: 324.8994 - loglik: -3.2419e+02 - logprior: -3.1206e-01
Epoch 4/10
39/39 - 16s - loss: 322.6345 - loglik: -3.2208e+02 - logprior: -1.6775e-01
Epoch 5/10
39/39 - 15s - loss: 321.3975 - loglik: -3.2097e+02 - logprior: -5.1760e-02
Epoch 6/10
39/39 - 15s - loss: 320.1075 - loglik: -3.1983e+02 - logprior: 0.0643
Epoch 7/10
39/39 - 15s - loss: 319.3606 - loglik: -3.1918e+02 - logprior: 0.1698
Epoch 8/10
39/39 - 15s - loss: 318.1004 - loglik: -3.1797e+02 - logprior: 0.2701
Epoch 9/10
39/39 - 15s - loss: 317.0535 - loglik: -3.1700e+02 - logprior: 0.3746
Epoch 10/10
39/39 - 15s - loss: 316.4485 - loglik: -3.1650e+02 - logprior: 0.4713
Fitted a model with MAP estimate = -315.7842
Time for alignment: 418.6666
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 472.2442 - loglik: -4.7035e+02 - logprior: -1.8484e+00
Epoch 2/10
39/39 - 12s - loss: 370.1411 - loglik: -3.6794e+02 - logprior: -1.8461e+00
Epoch 3/10
39/39 - 12s - loss: 360.9213 - loglik: -3.5869e+02 - logprior: -1.8791e+00
Epoch 4/10
39/39 - 12s - loss: 358.5513 - loglik: -3.5642e+02 - logprior: -1.8092e+00
Epoch 5/10
39/39 - 12s - loss: 357.0197 - loglik: -3.5494e+02 - logprior: -1.7832e+00
Epoch 6/10
39/39 - 12s - loss: 356.5312 - loglik: -3.5446e+02 - logprior: -1.7708e+00
Epoch 7/10
39/39 - 12s - loss: 355.9814 - loglik: -3.5393e+02 - logprior: -1.7691e+00
Epoch 8/10
39/39 - 12s - loss: 355.5481 - loglik: -3.5352e+02 - logprior: -1.7616e+00
Epoch 9/10
39/39 - 12s - loss: 355.6884 - loglik: -3.5367e+02 - logprior: -1.7618e+00
Fitted a model with MAP estimate = -354.8800
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (34, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (74, 1), (93, 1), (97, 2), (100, 1), (102, 1), (103, 1), (108, 1), (110, 1), (112, 1), (113, 1), (116, 1), (123, 1), (126, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 342.8077 - loglik: -3.4085e+02 - logprior: -1.8434e+00
Epoch 2/2
39/39 - 16s - loss: 328.9113 - loglik: -3.2789e+02 - logprior: -8.0958e-01
Fitted a model with MAP estimate = -325.6452
expansions: []
discards: [118 169]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 332.6863 - loglik: -3.3093e+02 - logprior: -1.6482e+00
Epoch 2/2
39/39 - 16s - loss: 328.1602 - loglik: -3.2728e+02 - logprior: -5.9542e-01
Fitted a model with MAP estimate = -325.6272
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 331.9289 - loglik: -3.3028e+02 - logprior: -1.4889e+00
Epoch 2/10
39/39 - 17s - loss: 327.6088 - loglik: -3.2687e+02 - logprior: -4.3352e-01
Epoch 3/10
39/39 - 17s - loss: 325.3795 - loglik: -3.2465e+02 - logprior: -3.0643e-01
Epoch 4/10
39/39 - 17s - loss: 322.5142 - loglik: -3.2189e+02 - logprior: -1.8149e-01
Epoch 5/10
39/39 - 17s - loss: 321.3779 - loglik: -3.2095e+02 - logprior: -5.0153e-02
Epoch 6/10
39/39 - 17s - loss: 320.7909 - loglik: -3.2049e+02 - logprior: 0.0530
Epoch 7/10
39/39 - 17s - loss: 319.2537 - loglik: -3.1902e+02 - logprior: 0.1663
Epoch 8/10
39/39 - 18s - loss: 317.7673 - loglik: -3.1759e+02 - logprior: 0.2674
Epoch 9/10
39/39 - 17s - loss: 317.0799 - loglik: -3.1701e+02 - logprior: 0.3599
Epoch 10/10
39/39 - 17s - loss: 316.5438 - loglik: -3.1658e+02 - logprior: 0.4618
Fitted a model with MAP estimate = -316.0436
Time for alignment: 435.2139
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 470.4116 - loglik: -4.6850e+02 - logprior: -1.8505e+00
Epoch 2/10
39/39 - 12s - loss: 369.7404 - loglik: -3.6752e+02 - logprior: -1.8462e+00
Epoch 3/10
39/39 - 12s - loss: 361.7777 - loglik: -3.5956e+02 - logprior: -1.8586e+00
Epoch 4/10
39/39 - 12s - loss: 359.7494 - loglik: -3.5762e+02 - logprior: -1.8012e+00
Epoch 5/10
39/39 - 12s - loss: 357.8913 - loglik: -3.5582e+02 - logprior: -1.7722e+00
Epoch 6/10
39/39 - 12s - loss: 357.4955 - loglik: -3.5543e+02 - logprior: -1.7704e+00
Epoch 7/10
39/39 - 12s - loss: 356.7294 - loglik: -3.5470e+02 - logprior: -1.7628e+00
Epoch 8/10
39/39 - 12s - loss: 356.8803 - loglik: -3.5488e+02 - logprior: -1.7504e+00
Fitted a model with MAP estimate = -355.7160
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (34, 1), (35, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (60, 1), (63, 1), (66, 1), (75, 1), (93, 1), (97, 2), (99, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (127, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 341.9003 - loglik: -3.3998e+02 - logprior: -1.8380e+00
Epoch 2/2
39/39 - 16s - loss: 328.2719 - loglik: -3.2723e+02 - logprior: -8.0897e-01
Fitted a model with MAP estimate = -325.2709
expansions: []
discards: [118]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 331.9750 - loglik: -3.3012e+02 - logprior: -1.6738e+00
Epoch 2/2
39/39 - 17s - loss: 327.6440 - loglik: -3.2673e+02 - logprior: -6.1083e-01
Fitted a model with MAP estimate = -325.3064
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 331.8039 - loglik: -3.3016e+02 - logprior: -1.4912e+00
Epoch 2/10
39/39 - 17s - loss: 327.3085 - loglik: -3.2656e+02 - logprior: -4.2467e-01
Epoch 3/10
39/39 - 17s - loss: 325.0023 - loglik: -3.2432e+02 - logprior: -3.0939e-01
Epoch 4/10
39/39 - 17s - loss: 322.8203 - loglik: -3.2223e+02 - logprior: -1.9419e-01
Epoch 5/10
39/39 - 17s - loss: 321.2884 - loglik: -3.2085e+02 - logprior: -6.8764e-02
Epoch 6/10
39/39 - 17s - loss: 319.8961 - loglik: -3.1959e+02 - logprior: 0.0309
Epoch 7/10
39/39 - 17s - loss: 318.6416 - loglik: -3.1840e+02 - logprior: 0.1447
Epoch 8/10
39/39 - 17s - loss: 317.2396 - loglik: -3.1704e+02 - logprior: 0.2518
Epoch 9/10
39/39 - 17s - loss: 317.0373 - loglik: -3.1695e+02 - logprior: 0.3428
Epoch 10/10
39/39 - 17s - loss: 317.2016 - loglik: -3.1722e+02 - logprior: 0.4359
Fitted a model with MAP estimate = -315.9849
Time for alignment: 423.2286
Computed alignments with likelihoods: ['-315.7842', '-316.0436', '-315.9849']
Best model has likelihood: -315.7842
time for generating output: 0.2039
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.7592513823904722
Training of 3 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fee914eca00>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee88f7fd90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff676097430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff67609f1f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff478700940>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4789a9a60>, <__main__.SimpleDirichletPrior object at 0x7ff66ae43c40>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 257.8368 - loglik: -2.5466e+02 - logprior: -3.1160e+00
Epoch 2/10
19/19 - 2s - loss: 224.0115 - loglik: -2.2237e+02 - logprior: -1.3080e+00
Epoch 3/10
19/19 - 2s - loss: 209.7431 - loglik: -2.0772e+02 - logprior: -1.4210e+00
Epoch 4/10
19/19 - 2s - loss: 206.1358 - loglik: -2.0436e+02 - logprior: -1.3566e+00
Epoch 5/10
19/19 - 2s - loss: 204.4656 - loglik: -2.0277e+02 - logprior: -1.3439e+00
Epoch 6/10
19/19 - 2s - loss: 203.4855 - loglik: -2.0194e+02 - logprior: -1.2719e+00
Epoch 7/10
19/19 - 2s - loss: 203.2542 - loglik: -2.0179e+02 - logprior: -1.2427e+00
Epoch 8/10
19/19 - 2s - loss: 202.6477 - loglik: -2.0121e+02 - logprior: -1.2387e+00
Epoch 9/10
19/19 - 2s - loss: 202.6177 - loglik: -2.0120e+02 - logprior: -1.2322e+00
Epoch 10/10
19/19 - 2s - loss: 202.8461 - loglik: -2.0144e+02 - logprior: -1.2303e+00
Fitted a model with MAP estimate = -202.0165
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (29, 1), (30, 2), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 207.3420 - loglik: -2.0325e+02 - logprior: -3.9848e+00
Epoch 2/2
19/19 - 3s - loss: 196.1680 - loglik: -1.9390e+02 - logprior: -1.9709e+00
Fitted a model with MAP estimate = -193.6635
expansions: [(0, 2)]
discards: [ 0  7 37 47]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 197.6776 - loglik: -1.9479e+02 - logprior: -2.8441e+00
Epoch 2/2
19/19 - 3s - loss: 193.2850 - loglik: -1.9215e+02 - logprior: -1.0195e+00
Fitted a model with MAP estimate = -191.8219
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 198.1098 - loglik: -1.9462e+02 - logprior: -3.4257e+00
Epoch 2/10
19/19 - 3s - loss: 193.4893 - loglik: -1.9211e+02 - logprior: -1.1863e+00
Epoch 3/10
19/19 - 3s - loss: 191.9101 - loglik: -1.9050e+02 - logprior: -1.0665e+00
Epoch 4/10
19/19 - 3s - loss: 191.6873 - loglik: -1.9025e+02 - logprior: -1.0270e+00
Epoch 5/10
19/19 - 3s - loss: 191.1202 - loglik: -1.8974e+02 - logprior: -9.9288e-01
Epoch 6/10
19/19 - 3s - loss: 190.7950 - loglik: -1.8948e+02 - logprior: -9.6657e-01
Epoch 7/10
19/19 - 3s - loss: 190.7203 - loglik: -1.8945e+02 - logprior: -9.4813e-01
Epoch 8/10
19/19 - 3s - loss: 190.7061 - loglik: -1.8948e+02 - logprior: -9.2904e-01
Epoch 9/10
19/19 - 3s - loss: 190.4439 - loglik: -1.8926e+02 - logprior: -9.0352e-01
Epoch 10/10
19/19 - 3s - loss: 190.3773 - loglik: -1.8921e+02 - logprior: -8.9436e-01
Fitted a model with MAP estimate = -189.9229
Time for alignment: 93.1459
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 257.6768 - loglik: -2.5450e+02 - logprior: -3.1133e+00
Epoch 2/10
19/19 - 2s - loss: 225.9084 - loglik: -2.2428e+02 - logprior: -1.3156e+00
Epoch 3/10
19/19 - 2s - loss: 213.1154 - loglik: -2.1114e+02 - logprior: -1.4049e+00
Epoch 4/10
19/19 - 2s - loss: 209.5640 - loglik: -2.0779e+02 - logprior: -1.3220e+00
Epoch 5/10
19/19 - 2s - loss: 208.0546 - loglik: -2.0637e+02 - logprior: -1.3260e+00
Epoch 6/10
19/19 - 2s - loss: 207.8300 - loglik: -2.0628e+02 - logprior: -1.2692e+00
Epoch 7/10
19/19 - 2s - loss: 207.0273 - loglik: -2.0555e+02 - logprior: -1.2307e+00
Epoch 8/10
19/19 - 2s - loss: 206.4469 - loglik: -2.0500e+02 - logprior: -1.2093e+00
Epoch 9/10
19/19 - 2s - loss: 205.8707 - loglik: -2.0443e+02 - logprior: -1.2209e+00
Epoch 10/10
19/19 - 2s - loss: 204.9061 - loglik: -2.0349e+02 - logprior: -1.2280e+00
Fitted a model with MAP estimate = -203.4693
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (22, 1), (29, 1), (37, 2), (38, 1), (43, 1), (50, 2), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 212.3055 - loglik: -2.0821e+02 - logprior: -4.0069e+00
Epoch 2/2
19/19 - 3s - loss: 198.2812 - loglik: -1.9606e+02 - logprior: -1.9741e+00
Fitted a model with MAP estimate = -195.2117
expansions: [(0, 2)]
discards: [ 0  7 45 62]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 198.3881 - loglik: -1.9546e+02 - logprior: -2.8612e+00
Epoch 2/2
19/19 - 3s - loss: 194.0750 - loglik: -1.9289e+02 - logprior: -1.0454e+00
Fitted a model with MAP estimate = -192.6615
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 199.1911 - loglik: -1.9571e+02 - logprior: -3.4405e+00
Epoch 2/10
19/19 - 3s - loss: 194.3972 - loglik: -1.9301e+02 - logprior: -1.2173e+00
Epoch 3/10
19/19 - 3s - loss: 193.0831 - loglik: -1.9169e+02 - logprior: -1.0811e+00
Epoch 4/10
19/19 - 3s - loss: 192.5309 - loglik: -1.9110e+02 - logprior: -1.0455e+00
Epoch 5/10
19/19 - 2s - loss: 191.9899 - loglik: -1.9059e+02 - logprior: -1.0132e+00
Epoch 6/10
19/19 - 3s - loss: 191.8717 - loglik: -1.9053e+02 - logprior: -9.8966e-01
Epoch 7/10
19/19 - 3s - loss: 191.9232 - loglik: -1.9064e+02 - logprior: -9.6466e-01
Fitted a model with MAP estimate = -191.2101
Time for alignment: 82.0971
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 258.0718 - loglik: -2.5485e+02 - logprior: -3.1120e+00
Epoch 2/10
19/19 - 2s - loss: 226.0193 - loglik: -2.2441e+02 - logprior: -1.3089e+00
Epoch 3/10
19/19 - 2s - loss: 211.8451 - loglik: -2.0988e+02 - logprior: -1.4229e+00
Epoch 4/10
19/19 - 2s - loss: 206.6662 - loglik: -2.0485e+02 - logprior: -1.3774e+00
Epoch 5/10
19/19 - 2s - loss: 205.7770 - loglik: -2.0405e+02 - logprior: -1.3719e+00
Epoch 6/10
19/19 - 2s - loss: 204.9158 - loglik: -2.0331e+02 - logprior: -1.3269e+00
Epoch 7/10
19/19 - 2s - loss: 204.5893 - loglik: -2.0306e+02 - logprior: -1.2774e+00
Epoch 8/10
19/19 - 2s - loss: 204.2922 - loglik: -2.0279e+02 - logprior: -1.2446e+00
Epoch 9/10
19/19 - 2s - loss: 203.7183 - loglik: -2.0224e+02 - logprior: -1.2326e+00
Epoch 10/10
19/19 - 2s - loss: 203.1380 - loglik: -2.0166e+02 - logprior: -1.2370e+00
Fitted a model with MAP estimate = -202.1110
expansions: [(8, 3), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (25, 1), (30, 2), (32, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 209.2228 - loglik: -2.0516e+02 - logprior: -3.9822e+00
Epoch 2/2
19/19 - 3s - loss: 196.8436 - loglik: -1.9459e+02 - logprior: -1.9622e+00
Fitted a model with MAP estimate = -193.8834
expansions: [(0, 2)]
discards: [ 0  7  8 38 49]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 197.6865 - loglik: -1.9482e+02 - logprior: -2.8385e+00
Epoch 2/2
19/19 - 2s - loss: 193.3026 - loglik: -1.9211e+02 - logprior: -1.0290e+00
Fitted a model with MAP estimate = -191.7037
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 199.1343 - loglik: -1.9565e+02 - logprior: -3.4009e+00
Epoch 2/10
19/19 - 2s - loss: 193.9128 - loglik: -1.9260e+02 - logprior: -1.1832e+00
Epoch 3/10
19/19 - 2s - loss: 192.4264 - loglik: -1.9104e+02 - logprior: -1.0553e+00
Epoch 4/10
19/19 - 2s - loss: 191.5776 - loglik: -1.9018e+02 - logprior: -1.0225e+00
Epoch 5/10
19/19 - 2s - loss: 191.4261 - loglik: -1.9009e+02 - logprior: -9.9167e-01
Epoch 6/10
19/19 - 2s - loss: 190.5950 - loglik: -1.8933e+02 - logprior: -9.6614e-01
Epoch 7/10
19/19 - 2s - loss: 191.1825 - loglik: -1.8996e+02 - logprior: -9.4917e-01
Fitted a model with MAP estimate = -190.3818
Time for alignment: 82.2280
Computed alignments with likelihoods: ['-189.9229', '-191.2101', '-190.3818']
Best model has likelihood: -189.9229
time for generating output: 0.1449
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.9757076322230662
Training of 3 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff66cd701c0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee91981970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff687faf880>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff7cafa14c0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee9c1a4f10>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff8573ca820>, <__main__.SimpleDirichletPrior object at 0x7fee80316910>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 564.6111 - loglik: -5.6267e+02 - logprior: -1.7834e+00
Epoch 2/10
39/39 - 12s - loss: 484.7771 - loglik: -4.8268e+02 - logprior: -1.3323e+00
Epoch 3/10
39/39 - 13s - loss: 476.9874 - loglik: -4.7462e+02 - logprior: -1.3185e+00
Epoch 4/10
39/39 - 13s - loss: 475.1216 - loglik: -4.7279e+02 - logprior: -1.3200e+00
Epoch 5/10
39/39 - 13s - loss: 474.1993 - loglik: -4.7194e+02 - logprior: -1.3141e+00
Epoch 6/10
39/39 - 13s - loss: 473.7517 - loglik: -4.7156e+02 - logprior: -1.3138e+00
Epoch 7/10
39/39 - 13s - loss: 473.2635 - loglik: -4.7109e+02 - logprior: -1.3283e+00
Epoch 8/10
39/39 - 13s - loss: 472.7312 - loglik: -4.7054e+02 - logprior: -1.3402e+00
Epoch 9/10
39/39 - 13s - loss: 472.3401 - loglik: -4.7011e+02 - logprior: -1.3478e+00
Epoch 10/10
39/39 - 13s - loss: 471.9723 - loglik: -4.6971e+02 - logprior: -1.3548e+00
Fitted a model with MAP estimate = -468.3835
expansions: [(0, 3), (19, 2), (20, 1), (21, 1), (34, 1), (36, 1), (45, 2), (46, 1), (51, 1), (52, 1), (65, 2), (69, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (107, 1), (108, 1), (109, 1), (124, 4), (141, 3), (142, 2), (147, 1), (149, 2), (150, 2), (151, 3), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 475.5080 - loglik: -4.7255e+02 - logprior: -2.6828e+00
Epoch 2/2
39/39 - 17s - loss: 453.6321 - loglik: -4.5197e+02 - logprior: -9.6710e-01
Fitted a model with MAP estimate = -447.6939
expansions: []
discards: [ 55  80  93 113 180 193 194]
Re-initialized the encoder parameters.
Fitting a model of length 217 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 466.2011 - loglik: -4.6434e+02 - logprior: -1.7394e+00
Epoch 2/2
39/39 - 17s - loss: 453.5722 - loglik: -4.5222e+02 - logprior: -6.4561e-01
Fitted a model with MAP estimate = -447.3285
expansions: []
discards: [175]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 460.6636 - loglik: -4.5891e+02 - logprior: -1.5830e+00
Epoch 2/10
39/39 - 17s - loss: 450.9067 - loglik: -4.4954e+02 - logprior: -4.9600e-01
Epoch 3/10
39/39 - 17s - loss: 448.3537 - loglik: -4.4678e+02 - logprior: -4.2556e-01
Epoch 4/10
39/39 - 17s - loss: 446.9799 - loglik: -4.4555e+02 - logprior: -3.6737e-01
Epoch 5/10
39/39 - 17s - loss: 446.6683 - loglik: -4.4540e+02 - logprior: -3.0587e-01
Epoch 6/10
39/39 - 17s - loss: 445.9666 - loglik: -4.4483e+02 - logprior: -2.2818e-01
Epoch 7/10
39/39 - 18s - loss: 445.6457 - loglik: -4.4461e+02 - logprior: -1.6244e-01
Epoch 8/10
39/39 - 18s - loss: 445.0926 - loglik: -4.4417e+02 - logprior: -8.9104e-02
Epoch 9/10
39/39 - 18s - loss: 444.8492 - loglik: -4.4397e+02 - logprior: -3.6493e-02
Epoch 10/10
39/39 - 18s - loss: 443.9597 - loglik: -4.4316e+02 - logprior: 0.0409
Fitted a model with MAP estimate = -442.8923
Time for alignment: 467.8206
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 566.9786 - loglik: -5.6450e+02 - logprior: -1.7824e+00
Epoch 2/10
39/39 - 14s - loss: 485.7492 - loglik: -4.8295e+02 - logprior: -1.4545e+00
Epoch 3/10
39/39 - 14s - loss: 476.5419 - loglik: -4.7389e+02 - logprior: -1.4272e+00
Epoch 4/10
39/39 - 14s - loss: 474.1149 - loglik: -4.7164e+02 - logprior: -1.4065e+00
Epoch 5/10
39/39 - 14s - loss: 473.2039 - loglik: -4.7086e+02 - logprior: -1.3986e+00
Epoch 6/10
39/39 - 14s - loss: 472.6528 - loglik: -4.7040e+02 - logprior: -1.4025e+00
Epoch 7/10
39/39 - 14s - loss: 472.2694 - loglik: -4.7007e+02 - logprior: -1.4105e+00
Epoch 8/10
39/39 - 14s - loss: 471.7737 - loglik: -4.6959e+02 - logprior: -1.4156e+00
Epoch 9/10
39/39 - 14s - loss: 471.6871 - loglik: -4.6953e+02 - logprior: -1.4173e+00
Epoch 10/10
39/39 - 14s - loss: 471.3156 - loglik: -4.6917e+02 - logprior: -1.4098e+00
Fitted a model with MAP estimate = -467.9087
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (31, 1), (36, 1), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (124, 1), (125, 1), (132, 1), (142, 4), (147, 2), (149, 2), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 484.5595 - loglik: -4.8162e+02 - logprior: -2.6796e+00
Epoch 2/2
39/39 - 19s - loss: 455.5987 - loglik: -4.5391e+02 - logprior: -9.9539e-01
Fitted a model with MAP estimate = -449.0505
expansions: []
discards: [ 26  56  93 113 177 178 192]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 461.5894 - loglik: -4.5964e+02 - logprior: -1.7578e+00
Epoch 2/2
39/39 - 19s - loss: 454.2709 - loglik: -4.5312e+02 - logprior: -6.2622e-01
Fitted a model with MAP estimate = -448.9611
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 458.3506 - loglik: -4.5657e+02 - logprior: -1.6112e+00
Epoch 2/10
39/39 - 18s - loss: 451.5824 - loglik: -4.5062e+02 - logprior: -4.7416e-01
Epoch 3/10
39/39 - 18s - loss: 448.8736 - loglik: -4.4772e+02 - logprior: -4.0508e-01
Epoch 4/10
39/39 - 18s - loss: 447.5155 - loglik: -4.4628e+02 - logprior: -3.4566e-01
Epoch 5/10
39/39 - 18s - loss: 447.2698 - loglik: -4.4607e+02 - logprior: -2.6822e-01
Epoch 6/10
39/39 - 18s - loss: 446.7798 - loglik: -4.4565e+02 - logprior: -2.0734e-01
Epoch 7/10
39/39 - 19s - loss: 446.1704 - loglik: -4.4513e+02 - logprior: -1.2954e-01
Epoch 8/10
39/39 - 19s - loss: 445.5838 - loglik: -4.4464e+02 - logprior: -5.4965e-02
Epoch 9/10
39/39 - 19s - loss: 445.5562 - loglik: -4.4470e+02 - logprior: 0.0198
Epoch 10/10
39/39 - 19s - loss: 444.6638 - loglik: -4.4390e+02 - logprior: 0.1056
Fitted a model with MAP estimate = -443.3365
Time for alignment: 502.8654
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 566.2571 - loglik: -5.6426e+02 - logprior: -1.7696e+00
Epoch 2/10
39/39 - 14s - loss: 487.0651 - loglik: -4.8476e+02 - logprior: -1.3990e+00
Epoch 3/10
39/39 - 14s - loss: 478.3215 - loglik: -4.7587e+02 - logprior: -1.3980e+00
Epoch 4/10
39/39 - 14s - loss: 476.0011 - loglik: -4.7360e+02 - logprior: -1.3777e+00
Epoch 5/10
39/39 - 14s - loss: 474.7613 - loglik: -4.7243e+02 - logprior: -1.3799e+00
Epoch 6/10
39/39 - 14s - loss: 474.3830 - loglik: -4.7211e+02 - logprior: -1.3860e+00
Epoch 7/10
39/39 - 14s - loss: 473.7661 - loglik: -4.7150e+02 - logprior: -1.3918e+00
Epoch 8/10
39/39 - 14s - loss: 473.1781 - loglik: -4.7091e+02 - logprior: -1.4008e+00
Epoch 9/10
39/39 - 14s - loss: 472.7222 - loglik: -4.7041e+02 - logprior: -1.4105e+00
Epoch 10/10
39/39 - 14s - loss: 472.3647 - loglik: -4.7002e+02 - logprior: -1.4334e+00
Fitted a model with MAP estimate = -468.6682
expansions: [(0, 3), (11, 1), (20, 2), (21, 1), (26, 1), (35, 2), (45, 2), (46, 1), (51, 1), (52, 1), (65, 1), (69, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (102, 1), (108, 1), (113, 1), (116, 1), (125, 1), (126, 1), (127, 1), (132, 1), (140, 1), (141, 3), (149, 3), (150, 2), (153, 1), (159, 1), (161, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 220 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 474.0907 - loglik: -4.7087e+02 - logprior: -2.6654e+00
Epoch 2/2
39/39 - 19s - loss: 454.5070 - loglik: -4.5247e+02 - logprior: -9.3759e-01
Fitted a model with MAP estimate = -448.3760
expansions: []
discards: [ 25  44  56  94 113]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 462.6205 - loglik: -4.6050e+02 - logprior: -1.7675e+00
Epoch 2/2
39/39 - 19s - loss: 454.1529 - loglik: -4.5250e+02 - logprior: -6.5262e-01
Fitted a model with MAP estimate = -448.2063
expansions: [(22, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 457.5605 - loglik: -4.5566e+02 - logprior: -1.6154e+00
Epoch 2/10
39/39 - 18s - loss: 451.0503 - loglik: -4.4989e+02 - logprior: -5.0091e-01
Epoch 3/10
39/39 - 17s - loss: 448.5413 - loglik: -4.4722e+02 - logprior: -4.1925e-01
Epoch 4/10
39/39 - 17s - loss: 447.4481 - loglik: -4.4609e+02 - logprior: -3.4524e-01
Epoch 5/10
39/39 - 17s - loss: 446.8988 - loglik: -4.4562e+02 - logprior: -2.7790e-01
Epoch 6/10
39/39 - 18s - loss: 446.3035 - loglik: -4.4515e+02 - logprior: -1.9232e-01
Epoch 7/10
39/39 - 18s - loss: 446.1336 - loglik: -4.4510e+02 - logprior: -1.1281e-01
Epoch 8/10
39/39 - 18s - loss: 445.6521 - loglik: -4.4472e+02 - logprior: -3.9253e-02
Epoch 9/10
39/39 - 18s - loss: 445.0385 - loglik: -4.4421e+02 - logprior: 0.0468
Epoch 10/10
39/39 - 19s - loss: 444.8465 - loglik: -4.4410e+02 - logprior: 0.1186
Fitted a model with MAP estimate = -443.4418
Time for alignment: 499.9068
Computed alignments with likelihoods: ['-442.8923', '-443.3365', '-443.4418']
Best model has likelihood: -442.8923
time for generating output: 0.3548
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.7086163580534103
Training of 3 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff66cf76e50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff4f81523d0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66d8c26a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66d161490>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff86faa1970>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff4786e9790>, <__main__.SimpleDirichletPrior object at 0x7ff66d433910>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.8496 - loglik: -1.6160e+02 - logprior: -3.1936e+00
Epoch 2/10
19/19 - 1s - loss: 136.1615 - loglik: -1.3468e+02 - logprior: -1.3953e+00
Epoch 3/10
19/19 - 1s - loss: 128.3942 - loglik: -1.2658e+02 - logprior: -1.4911e+00
Epoch 4/10
19/19 - 1s - loss: 127.0730 - loglik: -1.2542e+02 - logprior: -1.3446e+00
Epoch 5/10
19/19 - 1s - loss: 126.5749 - loglik: -1.2496e+02 - logprior: -1.3422e+00
Epoch 6/10
19/19 - 1s - loss: 126.3005 - loglik: -1.2472e+02 - logprior: -1.3154e+00
Epoch 7/10
19/19 - 1s - loss: 126.0024 - loglik: -1.2443e+02 - logprior: -1.3042e+00
Epoch 8/10
19/19 - 1s - loss: 126.2292 - loglik: -1.2467e+02 - logprior: -1.2952e+00
Fitted a model with MAP estimate = -125.6596
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (16, 1), (30, 2), (34, 3), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 134.5272 - loglik: -1.3039e+02 - logprior: -4.0906e+00
Epoch 2/2
19/19 - 1s - loss: 126.7499 - loglik: -1.2441e+02 - logprior: -2.2033e+00
Fitted a model with MAP estimate = -123.8632
expansions: []
discards: [13 14 41 47 51]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 126.2780 - loglik: -1.2275e+02 - logprior: -3.3446e+00
Epoch 2/2
19/19 - 1s - loss: 122.8111 - loglik: -1.2110e+02 - logprior: -1.4242e+00
Fitted a model with MAP estimate = -121.9264
expansions: []
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 125.9561 - loglik: -1.2272e+02 - logprior: -3.2059e+00
Epoch 2/10
19/19 - 2s - loss: 123.0432 - loglik: -1.2151e+02 - logprior: -1.4105e+00
Epoch 3/10
19/19 - 2s - loss: 122.2843 - loglik: -1.2076e+02 - logprior: -1.3158e+00
Epoch 4/10
19/19 - 1s - loss: 121.8216 - loglik: -1.2027e+02 - logprior: -1.2588e+00
Epoch 5/10
19/19 - 1s - loss: 121.6043 - loglik: -1.2006e+02 - logprior: -1.2240e+00
Epoch 6/10
19/19 - 1s - loss: 121.4942 - loglik: -1.1995e+02 - logprior: -1.2099e+00
Epoch 7/10
19/19 - 2s - loss: 121.2457 - loglik: -1.1972e+02 - logprior: -1.1898e+00
Epoch 8/10
19/19 - 1s - loss: 121.0457 - loglik: -1.1952e+02 - logprior: -1.1789e+00
Epoch 9/10
19/19 - 1s - loss: 121.1007 - loglik: -1.1959e+02 - logprior: -1.1653e+00
Fitted a model with MAP estimate = -120.5439
Time for alignment: 56.5729
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 164.9590 - loglik: -1.6170e+02 - logprior: -3.1953e+00
Epoch 2/10
19/19 - 1s - loss: 136.1809 - loglik: -1.3470e+02 - logprior: -1.3960e+00
Epoch 3/10
19/19 - 1s - loss: 128.3332 - loglik: -1.2650e+02 - logprior: -1.4981e+00
Epoch 4/10
19/19 - 1s - loss: 126.7560 - loglik: -1.2510e+02 - logprior: -1.3451e+00
Epoch 5/10
19/19 - 1s - loss: 126.4797 - loglik: -1.2487e+02 - logprior: -1.3413e+00
Epoch 6/10
19/19 - 1s - loss: 126.1066 - loglik: -1.2452e+02 - logprior: -1.3167e+00
Epoch 7/10
19/19 - 1s - loss: 126.1935 - loglik: -1.2462e+02 - logprior: -1.3061e+00
Fitted a model with MAP estimate = -125.6465
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (18, 1), (30, 2), (34, 3), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.9682 - loglik: -1.2985e+02 - logprior: -4.0898e+00
Epoch 2/2
19/19 - 1s - loss: 126.3807 - loglik: -1.2404e+02 - logprior: -2.1938e+00
Fitted a model with MAP estimate = -123.5322
expansions: []
discards: [13 14 41 47 51]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 126.3677 - loglik: -1.2299e+02 - logprior: -3.3297e+00
Epoch 2/2
19/19 - 1s - loss: 123.1121 - loglik: -1.2158e+02 - logprior: -1.4220e+00
Fitted a model with MAP estimate = -122.3542
expansions: []
discards: [14 15]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 125.8412 - loglik: -1.2260e+02 - logprior: -3.2035e+00
Epoch 2/10
19/19 - 1s - loss: 123.1305 - loglik: -1.2155e+02 - logprior: -1.3993e+00
Epoch 3/10
19/19 - 1s - loss: 122.3954 - loglik: -1.2083e+02 - logprior: -1.3069e+00
Epoch 4/10
19/19 - 2s - loss: 122.0195 - loglik: -1.2046e+02 - logprior: -1.2543e+00
Epoch 5/10
19/19 - 1s - loss: 121.7173 - loglik: -1.2019e+02 - logprior: -1.2226e+00
Epoch 6/10
19/19 - 1s - loss: 121.7174 - loglik: -1.2019e+02 - logprior: -1.2053e+00
Fitted a model with MAP estimate = -121.0558
Time for alignment: 48.4801
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 164.8370 - loglik: -1.6162e+02 - logprior: -3.1943e+00
Epoch 2/10
19/19 - 1s - loss: 135.7096 - loglik: -1.3422e+02 - logprior: -1.3986e+00
Epoch 3/10
19/19 - 1s - loss: 127.5690 - loglik: -1.2576e+02 - logprior: -1.4944e+00
Epoch 4/10
19/19 - 1s - loss: 126.4123 - loglik: -1.2476e+02 - logprior: -1.3437e+00
Epoch 5/10
19/19 - 1s - loss: 125.8272 - loglik: -1.2420e+02 - logprior: -1.3444e+00
Epoch 6/10
19/19 - 1s - loss: 125.4386 - loglik: -1.2386e+02 - logprior: -1.3181e+00
Epoch 7/10
19/19 - 1s - loss: 125.3746 - loglik: -1.2379e+02 - logprior: -1.3081e+00
Epoch 8/10
19/19 - 1s - loss: 125.4152 - loglik: -1.2382e+02 - logprior: -1.3008e+00
Fitted a model with MAP estimate = -124.8857
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (26, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.8357 - loglik: -1.2971e+02 - logprior: -4.0760e+00
Epoch 2/2
19/19 - 1s - loss: 126.7782 - loglik: -1.2437e+02 - logprior: -2.2056e+00
Fitted a model with MAP estimate = -123.8748
expansions: []
discards: [13 14 16 17 41 42 51]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 126.8335 - loglik: -1.2330e+02 - logprior: -3.3315e+00
Epoch 2/2
19/19 - 1s - loss: 123.1146 - loglik: -1.2143e+02 - logprior: -1.4162e+00
Fitted a model with MAP estimate = -122.1739
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 126.3994 - loglik: -1.2306e+02 - logprior: -3.1979e+00
Epoch 2/10
19/19 - 1s - loss: 123.1720 - loglik: -1.2171e+02 - logprior: -1.3892e+00
Epoch 3/10
19/19 - 1s - loss: 122.5074 - loglik: -1.2104e+02 - logprior: -1.2952e+00
Epoch 4/10
19/19 - 1s - loss: 122.2358 - loglik: -1.2074e+02 - logprior: -1.2476e+00
Epoch 5/10
19/19 - 1s - loss: 121.5846 - loglik: -1.2008e+02 - logprior: -1.2162e+00
Epoch 6/10
19/19 - 1s - loss: 121.7916 - loglik: -1.2029e+02 - logprior: -1.1998e+00
Fitted a model with MAP estimate = -121.0910
Time for alignment: 50.3170
Computed alignments with likelihoods: ['-120.5439', '-121.0558', '-121.0910']
Best model has likelihood: -120.5439
time for generating output: 0.1174
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.8885542168674698
Training of 3 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff687de0b50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee89c9ba90>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feea4784670>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff498683610>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7feea513b550>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff857497670>, <__main__.SimpleDirichletPrior object at 0x7fee90f01be0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 62s - loss: 906.0630 - loglik: -9.0431e+02 - logprior: -1.5307e+00
Epoch 2/10
39/39 - 66s - loss: 720.3468 - loglik: -7.1787e+02 - logprior: -1.5493e+00
Epoch 3/10
39/39 - 77s - loss: 704.9948 - loglik: -7.0217e+02 - logprior: -1.7587e+00
Epoch 4/10
39/39 - 72s - loss: 701.7917 - loglik: -6.9902e+02 - logprior: -1.6898e+00
Epoch 5/10
39/39 - 64s - loss: 700.5889 - loglik: -6.9741e+02 - logprior: -2.1082e+00
Epoch 6/10
39/39 - 64s - loss: 700.4517 - loglik: -6.9745e+02 - logprior: -1.9453e+00
Epoch 7/10
39/39 - 71s - loss: 697.7928 - loglik: -6.9484e+02 - logprior: -1.9188e+00
Epoch 8/10
39/39 - 78s - loss: 698.1517 - loglik: -6.9501e+02 - logprior: -2.1270e+00
Fitted a model with MAP estimate = -695.8057
expansions: [(0, 3), (38, 1), (133, 1), (134, 1), (135, 1), (161, 1), (162, 1), (163, 1), (175, 9), (176, 1), (189, 1), (190, 1), (191, 6), (192, 1), (195, 1), (196, 1), (197, 1), (198, 1), (200, 1), (201, 2), (202, 2), (203, 1), (206, 1), (208, 1), (210, 1), (211, 1), (213, 1), (217, 1), (221, 3), (222, 4), (223, 2), (225, 1), (226, 2), (227, 3), (228, 2), (229, 1), (239, 1), (243, 2), (244, 3), (245, 2), (246, 2), (247, 6), (250, 1), (252, 1), (266, 1), (268, 1), (284, 1), (286, 1), (287, 2), (288, 1), (291, 1), (292, 1), (301, 1), (303, 2), (304, 1), (307, 1), (312, 1), (325, 2), (328, 1), (345, 1), (350, 2), (351, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 457 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 130s - loss: 675.2922 - loglik: -6.7282e+02 - logprior: -2.1074e+00
Epoch 2/2
39/39 - 128s - loss: 653.0160 - loglik: -6.5160e+02 - logprior: -6.1907e-01
Fitted a model with MAP estimate = -648.1804
expansions: [(215, 1), (281, 1), (322, 1), (323, 2), (454, 1)]
discards: [  2   3 186 187 237 268 307 313 314 325 326 371 375]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 119s - loss: 662.1670 - loglik: -6.6041e+02 - logprior: -1.5101e+00
Epoch 2/2
39/39 - 102s - loss: 653.4910 - loglik: -6.5242e+02 - logprior: -2.4411e-01
Fitted a model with MAP estimate = -649.0350
expansions: [(304, 1), (306, 1), (368, 1)]
discards: [211 317]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 103s - loss: 661.6098 - loglik: -6.6019e+02 - logprior: -1.2075e+00
Epoch 2/10
39/39 - 104s - loss: 653.1397 - loglik: -6.5267e+02 - logprior: 0.1485
Epoch 3/10
39/39 - 102s - loss: 649.3817 - loglik: -6.4873e+02 - logprior: 0.3037
Epoch 4/10
39/39 - 96s - loss: 648.1570 - loglik: -6.4730e+02 - logprior: 0.3210
Epoch 5/10
39/39 - 93s - loss: 647.3200 - loglik: -6.4658e+02 - logprior: 0.4705
Epoch 6/10
39/39 - 94s - loss: 645.4899 - loglik: -6.4498e+02 - logprior: 0.6795
Epoch 7/10
39/39 - 94s - loss: 645.3779 - loglik: -6.4504e+02 - logprior: 0.8420
Epoch 8/10
39/39 - 96s - loss: 644.7792 - loglik: -6.4457e+02 - logprior: 0.9499
Epoch 9/10
39/39 - 97s - loss: 644.0024 - loglik: -6.4406e+02 - logprior: 1.1642
Epoch 10/10
39/39 - 96s - loss: 643.9583 - loglik: -6.4427e+02 - logprior: 1.4095
Fitted a model with MAP estimate = -641.4534
Time for alignment: 2552.4909
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 899.2999 - loglik: -8.9744e+02 - logprior: -1.4944e+00
Epoch 2/10
39/39 - 64s - loss: 717.1622 - loglik: -7.1481e+02 - logprior: -1.2852e+00
Epoch 3/10
39/39 - 64s - loss: 702.6475 - loglik: -7.0024e+02 - logprior: -1.2909e+00
Epoch 4/10
39/39 - 63s - loss: 699.7344 - loglik: -6.9737e+02 - logprior: -1.2471e+00
Epoch 5/10
39/39 - 64s - loss: 697.8688 - loglik: -6.9547e+02 - logprior: -1.2783e+00
Epoch 6/10
39/39 - 66s - loss: 696.6719 - loglik: -6.9430e+02 - logprior: -1.2843e+00
Epoch 7/10
39/39 - 60s - loss: 696.5975 - loglik: -6.9429e+02 - logprior: -1.2763e+00
Epoch 8/10
39/39 - 59s - loss: 696.4827 - loglik: -6.9423e+02 - logprior: -1.2821e+00
Epoch 9/10
39/39 - 58s - loss: 695.1934 - loglik: -6.9297e+02 - logprior: -1.2894e+00
Epoch 10/10
39/39 - 58s - loss: 695.2130 - loglik: -6.9302e+02 - logprior: -1.2897e+00
Fitted a model with MAP estimate = -693.4271
expansions: [(0, 3), (37, 1), (43, 1), (132, 1), (135, 1), (164, 1), (166, 1), (172, 1), (177, 8), (178, 1), (179, 1), (192, 2), (193, 7), (194, 2), (196, 1), (197, 1), (198, 1), (199, 2), (200, 1), (201, 1), (203, 1), (204, 1), (207, 1), (209, 1), (211, 1), (212, 1), (214, 1), (216, 1), (217, 1), (222, 1), (223, 8), (225, 1), (226, 2), (227, 3), (228, 2), (239, 1), (240, 1), (244, 2), (245, 3), (246, 2), (247, 2), (248, 7), (250, 1), (253, 1), (273, 1), (286, 1), (287, 1), (289, 3), (291, 1), (292, 1), (301, 1), (303, 1), (304, 1), (305, 1), (307, 1), (327, 1), (329, 1), (339, 1), (341, 1), (355, 6)]
discards: [  2   3 128]
Re-initialized the encoder parameters.
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 98s - loss: 678.9520 - loglik: -6.7617e+02 - logprior: -2.3520e+00
Epoch 2/2
39/39 - 100s - loss: 654.2634 - loglik: -6.5241e+02 - logprior: -9.7271e-01
Fitted a model with MAP estimate = -648.9224
expansions: [(281, 1), (323, 2)]
discards: [  2 187 188 189 190 217 273 274 275 276 277 278 287 314 315 316 457]
Re-initialized the encoder parameters.
Fitting a model of length 445 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 101s - loss: 671.0350 - loglik: -6.6934e+02 - logprior: -1.5289e+00
Epoch 2/2
39/39 - 99s - loss: 658.9020 - loglik: -6.5777e+02 - logprior: -3.3653e-01
Fitted a model with MAP estimate = -654.9890
expansions: [(267, 6), (299, 1), (300, 1), (362, 1)]
discards: [  2 312 313 314 315]
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 87s - loss: 661.5665 - loglik: -6.6007e+02 - logprior: -1.1505e+00
Epoch 2/10
39/39 - 86s - loss: 654.0181 - loglik: -6.5355e+02 - logprior: 0.2169
Epoch 3/10
39/39 - 87s - loss: 650.7926 - loglik: -6.5017e+02 - logprior: 0.3648
Epoch 4/10
39/39 - 89s - loss: 650.2441 - loglik: -6.4936e+02 - logprior: 0.2879
Epoch 5/10
39/39 - 88s - loss: 648.6612 - loglik: -6.4798e+02 - logprior: 0.5599
Epoch 6/10
39/39 - 86s - loss: 648.5543 - loglik: -6.4816e+02 - logprior: 0.8356
Epoch 7/10
39/39 - 88s - loss: 647.4308 - loglik: -6.4722e+02 - logprior: 1.0127
Epoch 8/10
39/39 - 93s - loss: 645.9354 - loglik: -6.4568e+02 - logprior: 0.9504
Epoch 9/10
39/39 - 88s - loss: 646.2772 - loglik: -6.4637e+02 - logprior: 1.2928
Fitted a model with MAP estimate = -643.7709
Time for alignment: 2248.2290
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 61s - loss: 899.1130 - loglik: -8.9721e+02 - logprior: -1.5307e+00
Epoch 2/10
39/39 - 59s - loss: 718.9990 - loglik: -7.1657e+02 - logprior: -1.3596e+00
Epoch 3/10
39/39 - 61s - loss: 706.8951 - loglik: -7.0437e+02 - logprior: -1.2934e+00
Epoch 4/10
39/39 - 64s - loss: 703.0483 - loglik: -7.0070e+02 - logprior: -1.1924e+00
Epoch 5/10
39/39 - 70s - loss: 701.8954 - loglik: -6.9957e+02 - logprior: -1.1626e+00
Epoch 6/10
39/39 - 68s - loss: 701.0479 - loglik: -6.9875e+02 - logprior: -1.1925e+00
Epoch 7/10
39/39 - 71s - loss: 699.4667 - loglik: -6.9720e+02 - logprior: -1.2027e+00
Epoch 8/10
39/39 - 69s - loss: 698.9576 - loglik: -6.9676e+02 - logprior: -1.2084e+00
Epoch 9/10
39/39 - 70s - loss: 699.7779 - loglik: -6.9761e+02 - logprior: -1.1999e+00
Fitted a model with MAP estimate = -697.0939
expansions: [(0, 3), (37, 1), (43, 1), (140, 1), (167, 1), (173, 1), (178, 1), (179, 8), (180, 1), (193, 2), (194, 8), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (201, 1), (202, 2), (203, 2), (204, 1), (209, 1), (210, 1), (211, 1), (215, 2), (216, 1), (217, 2), (222, 1), (223, 7), (225, 4), (226, 2), (227, 2), (228, 1), (239, 1), (243, 2), (244, 2), (245, 2), (247, 2), (248, 6), (250, 1), (252, 1), (253, 1), (255, 1), (268, 1), (269, 1), (270, 1), (285, 1), (287, 1), (288, 2), (290, 1), (291, 1), (303, 1), (304, 1), (305, 1), (307, 1), (327, 1), (329, 2), (349, 1), (355, 6)]
discards: [2 3]
Re-initialized the encoder parameters.
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 123s - loss: 683.4537 - loglik: -6.8083e+02 - logprior: -2.1512e+00
Epoch 2/2
39/39 - 117s - loss: 655.8822 - loglik: -6.5406e+02 - logprior: -7.2208e-01
Fitted a model with MAP estimate = -650.5078
expansions: [(218, 1), (231, 2), (281, 1)]
discards: [  2   3 187 188 189 190 237 260 313 314 315 316 317 318 319 457]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 108s - loss: 663.6242 - loglik: -6.6203e+02 - logprior: -1.3736e+00
Epoch 2/2
39/39 - 115s - loss: 653.9484 - loglik: -6.5300e+02 - logprior: -4.2583e-01
Fitted a model with MAP estimate = -650.5823
expansions: [(306, 1)]
discards: [213 214 215]
Re-initialized the encoder parameters.
Fitting a model of length 445 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 107s - loss: 666.7444 - loglik: -6.6539e+02 - logprior: -1.1738e+00
Epoch 2/10
39/39 - 104s - loss: 655.5313 - loglik: -6.5491e+02 - logprior: 0.0076
Epoch 3/10
39/39 - 106s - loss: 651.7131 - loglik: -6.5077e+02 - logprior: 0.0878
Epoch 4/10
39/39 - 117s - loss: 650.5387 - loglik: -6.4971e+02 - logprior: 0.3223
Epoch 5/10
39/39 - 96s - loss: 649.6532 - loglik: -6.4886e+02 - logprior: 0.3891
Epoch 6/10
39/39 - 84s - loss: 649.1520 - loglik: -6.4869e+02 - logprior: 0.6717
Epoch 7/10
39/39 - 92s - loss: 647.8895 - loglik: -6.4741e+02 - logprior: 0.6497
Epoch 8/10
39/39 - 100s - loss: 646.8857 - loglik: -6.4676e+02 - logprior: 0.9719
Epoch 9/10
39/39 - 100s - loss: 646.6298 - loglik: -6.4668e+02 - logprior: 1.1473
Epoch 10/10
39/39 - 110s - loss: 646.9206 - loglik: -6.4704e+02 - logprior: 1.1748
Fitted a model with MAP estimate = -643.9229
Time for alignment: 2592.0530
Computed alignments with likelihoods: ['-641.4534', '-643.7709', '-643.9229']
Best model has likelihood: -641.4534
time for generating output: 0.4326
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.8566659247718673
Training of 3 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff67699ae20>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee9c3bf970>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff83d941490>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff86f838070>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee911c2cd0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fee889957c0>, <__main__.SimpleDirichletPrior object at 0x7fee882bd0a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 409.3498 - loglik: -4.0632e+02 - logprior: -2.9359e+00
Epoch 2/10
19/19 - 5s - loss: 327.9571 - loglik: -3.2662e+02 - logprior: -1.0088e+00
Epoch 3/10
19/19 - 5s - loss: 300.2103 - loglik: -2.9830e+02 - logprior: -1.2173e+00
Epoch 4/10
19/19 - 5s - loss: 293.4759 - loglik: -2.9161e+02 - logprior: -1.1619e+00
Epoch 5/10
19/19 - 5s - loss: 290.1588 - loglik: -2.8835e+02 - logprior: -1.1277e+00
Epoch 6/10
19/19 - 5s - loss: 289.6589 - loglik: -2.8796e+02 - logprior: -1.0925e+00
Epoch 7/10
19/19 - 5s - loss: 288.0081 - loglik: -2.8638e+02 - logprior: -1.0641e+00
Epoch 8/10
19/19 - 5s - loss: 288.6392 - loglik: -2.8704e+02 - logprior: -1.0598e+00
Fitted a model with MAP estimate = -287.0785
expansions: [(0, 6), (7, 2), (8, 2), (9, 2), (10, 2), (34, 1), (35, 1), (37, 1), (53, 1), (55, 2), (59, 1), (73, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (99, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 290.7184 - loglik: -2.8660e+02 - logprior: -3.9143e+00
Epoch 2/2
19/19 - 7s - loss: 277.2440 - loglik: -2.7559e+02 - logprior: -1.1351e+00
Fitted a model with MAP estimate = -273.8113
expansions: [(0, 4)]
discards: [  1   2   3   4   5  22  73 100 101 147]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 284.5139 - loglik: -2.8056e+02 - logprior: -3.8588e+00
Epoch 2/2
19/19 - 7s - loss: 277.4249 - loglik: -2.7613e+02 - logprior: -1.0859e+00
Fitted a model with MAP estimate = -275.3146
expansions: [(0, 5)]
discards: [ 0  1  2  4 17]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 280.3643 - loglik: -2.7712e+02 - logprior: -3.0321e+00
Epoch 2/10
19/19 - 7s - loss: 275.5307 - loglik: -2.7405e+02 - logprior: -9.3670e-01
Epoch 3/10
19/19 - 7s - loss: 273.6987 - loglik: -2.7216e+02 - logprior: -7.6310e-01
Epoch 4/10
19/19 - 8s - loss: 272.4425 - loglik: -2.7095e+02 - logprior: -6.6859e-01
Epoch 5/10
19/19 - 7s - loss: 272.2194 - loglik: -2.7081e+02 - logprior: -6.0524e-01
Epoch 6/10
19/19 - 7s - loss: 271.0204 - loglik: -2.6973e+02 - logprior: -5.4789e-01
Epoch 7/10
19/19 - 7s - loss: 271.4154 - loglik: -2.7020e+02 - logprior: -5.0873e-01
Fitted a model with MAP estimate = -269.9879
Time for alignment: 172.6795
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 408.6971 - loglik: -4.0571e+02 - logprior: -2.9277e+00
Epoch 2/10
19/19 - 5s - loss: 325.6264 - loglik: -3.2425e+02 - logprior: -1.0083e+00
Epoch 3/10
19/19 - 5s - loss: 295.4229 - loglik: -2.9342e+02 - logprior: -1.2297e+00
Epoch 4/10
19/19 - 5s - loss: 290.5240 - loglik: -2.8864e+02 - logprior: -1.1516e+00
Epoch 5/10
19/19 - 5s - loss: 288.3752 - loglik: -2.8664e+02 - logprior: -1.1114e+00
Epoch 6/10
19/19 - 5s - loss: 287.8775 - loglik: -2.8624e+02 - logprior: -1.0787e+00
Epoch 7/10
19/19 - 5s - loss: 286.7325 - loglik: -2.8515e+02 - logprior: -1.0510e+00
Epoch 8/10
19/19 - 5s - loss: 287.3387 - loglik: -2.8579e+02 - logprior: -1.0395e+00
Fitted a model with MAP estimate = -285.7519
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (27, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (97, 1), (98, 1), (99, 1), (105, 1), (112, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 291.2232 - loglik: -2.8726e+02 - logprior: -3.8737e+00
Epoch 2/2
19/19 - 7s - loss: 277.6944 - loglik: -2.7632e+02 - logprior: -1.0761e+00
Fitted a model with MAP estimate = -274.6767
expansions: [(0, 4)]
discards: [  1   2   3   4   5  21  74  75 100 101]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 283.4590 - loglik: -2.7958e+02 - logprior: -3.8171e+00
Epoch 2/2
19/19 - 6s - loss: 277.0435 - loglik: -2.7578e+02 - logprior: -1.0734e+00
Fitted a model with MAP estimate = -274.8704
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 280.0702 - loglik: -2.7683e+02 - logprior: -3.0087e+00
Epoch 2/10
19/19 - 6s - loss: 275.0659 - loglik: -2.7361e+02 - logprior: -8.8933e-01
Epoch 3/10
19/19 - 7s - loss: 273.8531 - loglik: -2.7240e+02 - logprior: -6.6879e-01
Epoch 4/10
19/19 - 6s - loss: 272.8319 - loglik: -2.7143e+02 - logprior: -6.1425e-01
Epoch 5/10
19/19 - 6s - loss: 271.7393 - loglik: -2.7042e+02 - logprior: -5.4779e-01
Epoch 6/10
19/19 - 7s - loss: 270.8906 - loglik: -2.6966e+02 - logprior: -5.1081e-01
Epoch 7/10
19/19 - 7s - loss: 271.3613 - loglik: -2.7018e+02 - logprior: -4.7784e-01
Fitted a model with MAP estimate = -270.0265
Time for alignment: 161.7299
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 408.3689 - loglik: -4.0529e+02 - logprior: -2.9245e+00
Epoch 2/10
19/19 - 5s - loss: 331.0690 - loglik: -3.2971e+02 - logprior: -1.0007e+00
Epoch 3/10
19/19 - 5s - loss: 300.3222 - loglik: -2.9832e+02 - logprior: -1.2003e+00
Epoch 4/10
19/19 - 5s - loss: 293.3846 - loglik: -2.9154e+02 - logprior: -1.1399e+00
Epoch 5/10
19/19 - 5s - loss: 290.3688 - loglik: -2.8866e+02 - logprior: -1.1024e+00
Epoch 6/10
19/19 - 5s - loss: 289.1436 - loglik: -2.8751e+02 - logprior: -1.0785e+00
Epoch 7/10
19/19 - 5s - loss: 288.0679 - loglik: -2.8651e+02 - logprior: -1.0528e+00
Epoch 8/10
19/19 - 5s - loss: 288.3926 - loglik: -2.8685e+02 - logprior: -1.0399e+00
Fitted a model with MAP estimate = -287.0447
expansions: [(0, 6), (7, 3), (8, 2), (10, 2), (27, 1), (33, 1), (34, 1), (36, 2), (52, 1), (54, 2), (58, 1), (59, 1), (73, 1), (75, 3), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (99, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 290.6839 - loglik: -2.8654e+02 - logprior: -3.9085e+00
Epoch 2/2
19/19 - 7s - loss: 276.5116 - loglik: -2.7495e+02 - logprior: -1.1455e+00
Fitted a model with MAP estimate = -273.5130
expansions: [(0, 4)]
discards: [  1   2   3   4   5  21  52  73 100 101 147]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 282.6736 - loglik: -2.7860e+02 - logprior: -3.8759e+00
Epoch 2/2
19/19 - 7s - loss: 276.9080 - loglik: -2.7531e+02 - logprior: -1.1222e+00
Fitted a model with MAP estimate = -274.3832
expansions: [(0, 5)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 281.2660 - loglik: -2.7801e+02 - logprior: -3.1779e+00
Epoch 2/10
19/19 - 7s - loss: 275.2829 - loglik: -2.7404e+02 - logprior: -1.0574e+00
Epoch 3/10
19/19 - 7s - loss: 273.8033 - loglik: -2.7255e+02 - logprior: -8.5307e-01
Epoch 4/10
19/19 - 7s - loss: 272.3316 - loglik: -2.7106e+02 - logprior: -7.0975e-01
Epoch 5/10
19/19 - 7s - loss: 272.0781 - loglik: -2.7080e+02 - logprior: -6.3333e-01
Epoch 6/10
19/19 - 7s - loss: 270.3766 - loglik: -2.6910e+02 - logprior: -5.9062e-01
Epoch 7/10
19/19 - 7s - loss: 270.9184 - loglik: -2.6968e+02 - logprior: -5.5642e-01
Fitted a model with MAP estimate = -269.6712
Time for alignment: 164.1701
Computed alignments with likelihoods: ['-269.9879', '-270.0265', '-269.6712']
Best model has likelihood: -269.6712
time for generating output: 0.1858
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.658748719026723
Training of 3 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff83dfd8a60>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee89c54fd0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff66b535880>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7feea515f610>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff4781523a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fee81600d90>, <__main__.SimpleDirichletPrior object at 0x7fee898683a0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 69.0358 - loglik: -6.5650e+01 - logprior: -3.3523e+00
Epoch 2/10
19/19 - 1s - loss: 52.3857 - loglik: -5.0932e+01 - logprior: -1.4439e+00
Epoch 3/10
19/19 - 1s - loss: 46.4288 - loglik: -4.4871e+01 - logprior: -1.5016e+00
Epoch 4/10
19/19 - 1s - loss: 44.5779 - loglik: -4.2905e+01 - logprior: -1.5298e+00
Epoch 5/10
19/19 - 1s - loss: 44.1713 - loglik: -4.2539e+01 - logprior: -1.5114e+00
Epoch 6/10
19/19 - 1s - loss: 43.9774 - loglik: -4.2330e+01 - logprior: -1.5026e+00
Epoch 7/10
19/19 - 1s - loss: 43.9176 - loglik: -4.2265e+01 - logprior: -1.4938e+00
Epoch 8/10
19/19 - 1s - loss: 43.8244 - loglik: -4.2174e+01 - logprior: -1.4816e+00
Epoch 9/10
19/19 - 1s - loss: 43.7799 - loglik: -4.2116e+01 - logprior: -1.4838e+00
Epoch 10/10
19/19 - 1s - loss: 43.7693 - loglik: -4.2108e+01 - logprior: -1.4778e+00
Fitted a model with MAP estimate = -43.5237
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 47.3500 - loglik: -4.2575e+01 - logprior: -4.7252e+00
Epoch 2/2
19/19 - 1s - loss: 42.0644 - loglik: -4.0634e+01 - logprior: -1.3419e+00
Fitted a model with MAP estimate = -41.0872
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 44.1836 - loglik: -4.0778e+01 - logprior: -3.3172e+00
Epoch 2/2
19/19 - 1s - loss: 41.6612 - loglik: -4.0026e+01 - logprior: -1.5245e+00
Fitted a model with MAP estimate = -41.1159
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.9979 - loglik: -4.0669e+01 - logprior: -3.2698e+00
Epoch 2/10
19/19 - 1s - loss: 41.5643 - loglik: -4.0006e+01 - logprior: -1.5051e+00
Epoch 3/10
19/19 - 1s - loss: 41.0956 - loglik: -3.9563e+01 - logprior: -1.4140e+00
Epoch 4/10
19/19 - 1s - loss: 40.8292 - loglik: -3.9278e+01 - logprior: -1.3713e+00
Epoch 5/10
19/19 - 1s - loss: 40.6241 - loglik: -3.9082e+01 - logprior: -1.3447e+00
Epoch 6/10
19/19 - 1s - loss: 40.4062 - loglik: -3.8868e+01 - logprior: -1.3326e+00
Epoch 7/10
19/19 - 1s - loss: 40.3147 - loglik: -3.8782e+01 - logprior: -1.3213e+00
Epoch 8/10
19/19 - 1s - loss: 40.2257 - loglik: -3.8699e+01 - logprior: -1.3108e+00
Epoch 9/10
19/19 - 1s - loss: 40.2696 - loglik: -3.8740e+01 - logprior: -1.3049e+00
Fitted a model with MAP estimate = -39.9294
Time for alignment: 39.7815
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 68.9049 - loglik: -6.5513e+01 - logprior: -3.3542e+00
Epoch 2/10
19/19 - 1s - loss: 51.6133 - loglik: -5.0171e+01 - logprior: -1.4266e+00
Epoch 3/10
19/19 - 1s - loss: 46.0689 - loglik: -4.4605e+01 - logprior: -1.3391e+00
Epoch 4/10
19/19 - 1s - loss: 44.6264 - loglik: -4.3224e+01 - logprior: -1.2495e+00
Epoch 5/10
19/19 - 1s - loss: 44.2003 - loglik: -4.2856e+01 - logprior: -1.2107e+00
Epoch 6/10
19/19 - 1s - loss: 44.0861 - loglik: -4.2746e+01 - logprior: -1.1951e+00
Epoch 7/10
19/19 - 1s - loss: 44.0095 - loglik: -4.2676e+01 - logprior: -1.1813e+00
Epoch 8/10
19/19 - 1s - loss: 43.9318 - loglik: -4.2597e+01 - logprior: -1.1704e+00
Epoch 9/10
19/19 - 1s - loss: 43.9074 - loglik: -4.2570e+01 - logprior: -1.1651e+00
Epoch 10/10
19/19 - 1s - loss: 43.8973 - loglik: -4.2557e+01 - logprior: -1.1614e+00
Fitted a model with MAP estimate = -43.6644
expansions: [(0, 2), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 47.4557 - loglik: -4.2676e+01 - logprior: -4.6684e+00
Epoch 2/2
19/19 - 1s - loss: 42.2287 - loglik: -4.0493e+01 - logprior: -1.6619e+00
Fitted a model with MAP estimate = -41.4304
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.8850 - loglik: -4.0617e+01 - logprior: -3.1367e+00
Epoch 2/10
19/19 - 1s - loss: 41.8102 - loglik: -4.0330e+01 - logprior: -1.4048e+00
Epoch 3/10
19/19 - 1s - loss: 41.2635 - loglik: -3.9708e+01 - logprior: -1.3989e+00
Epoch 4/10
19/19 - 1s - loss: 40.9422 - loglik: -3.9342e+01 - logprior: -1.4036e+00
Epoch 5/10
19/19 - 1s - loss: 40.5661 - loglik: -3.8992e+01 - logprior: -1.3639e+00
Epoch 6/10
19/19 - 1s - loss: 40.4837 - loglik: -3.8926e+01 - logprior: -1.3444e+00
Epoch 7/10
19/19 - 1s - loss: 40.3158 - loglik: -3.8767e+01 - logprior: -1.3354e+00
Epoch 8/10
19/19 - 1s - loss: 40.3580 - loglik: -3.8821e+01 - logprior: -1.3223e+00
Fitted a model with MAP estimate = -40.0268
Time for alignment: 30.4509
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 69.0366 - loglik: -6.5632e+01 - logprior: -3.3602e+00
Epoch 2/10
19/19 - 1s - loss: 52.6816 - loglik: -5.1198e+01 - logprior: -1.4756e+00
Epoch 3/10
19/19 - 1s - loss: 45.8011 - loglik: -4.4196e+01 - logprior: -1.5707e+00
Epoch 4/10
19/19 - 1s - loss: 44.0578 - loglik: -4.2393e+01 - logprior: -1.5490e+00
Epoch 5/10
19/19 - 1s - loss: 43.7595 - loglik: -4.2119e+01 - logprior: -1.5169e+00
Epoch 6/10
19/19 - 1s - loss: 43.4408 - loglik: -4.1803e+01 - logprior: -1.5045e+00
Epoch 7/10
19/19 - 1s - loss: 43.4364 - loglik: -4.1803e+01 - logprior: -1.4914e+00
Epoch 8/10
19/19 - 1s - loss: 43.3468 - loglik: -4.1719e+01 - logprior: -1.4835e+00
Epoch 9/10
19/19 - 1s - loss: 43.3140 - loglik: -4.1680e+01 - logprior: -1.4795e+00
Epoch 10/10
19/19 - 1s - loss: 43.3183 - loglik: -4.1693e+01 - logprior: -1.4687e+00
Fitted a model with MAP estimate = -43.0947
expansions: [(0, 1), (3, 1), (4, 1), (9, 1), (11, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 47.2407 - loglik: -4.2395e+01 - logprior: -4.7146e+00
Epoch 2/2
19/19 - 1s - loss: 42.0098 - loglik: -4.0576e+01 - logprior: -1.3492e+00
Fitted a model with MAP estimate = -41.0335
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 44.2521 - loglik: -4.0891e+01 - logprior: -3.3158e+00
Epoch 2/2
19/19 - 1s - loss: 41.6740 - loglik: -4.0079e+01 - logprior: -1.5246e+00
Fitted a model with MAP estimate = -41.1611
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 43.9982 - loglik: -4.0638e+01 - logprior: -3.2706e+00
Epoch 2/10
19/19 - 1s - loss: 41.6062 - loglik: -4.0018e+01 - logprior: -1.5045e+00
Epoch 3/10
19/19 - 1s - loss: 41.0361 - loglik: -3.9491e+01 - logprior: -1.4120e+00
Epoch 4/10
19/19 - 1s - loss: 40.7779 - loglik: -3.9227e+01 - logprior: -1.3663e+00
Epoch 5/10
19/19 - 1s - loss: 40.5642 - loglik: -3.9025e+01 - logprior: -1.3392e+00
Epoch 6/10
19/19 - 1s - loss: 40.4417 - loglik: -3.8909e+01 - logprior: -1.3289e+00
Epoch 7/10
19/19 - 1s - loss: 40.4209 - loglik: -3.8893e+01 - logprior: -1.3192e+00
Epoch 8/10
19/19 - 1s - loss: 40.3294 - loglik: -3.8808e+01 - logprior: -1.3070e+00
Epoch 9/10
19/19 - 1s - loss: 40.2079 - loglik: -3.8681e+01 - logprior: -1.2956e+00
Epoch 10/10
19/19 - 1s - loss: 40.1183 - loglik: -3.8582e+01 - logprior: -1.2936e+00
Fitted a model with MAP estimate = -39.8892
Time for alignment: 37.7371
Computed alignments with likelihoods: ['-39.9294', '-40.0268', '-39.8892']
Best model has likelihood: -39.8892
time for generating output: 0.1003
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.8828250401284109
Training of 3 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fee912e3c40>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee7996dd00>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fee8048ddc0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff500606370>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66b89c730>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff47814cee0>, <__main__.SimpleDirichletPrior object at 0x7fee91326f10>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 34s - loss: 737.1948 - loglik: -7.3547e+02 - logprior: -1.5619e+00
Epoch 2/10
39/39 - 32s - loss: 528.4818 - loglik: -5.2608e+02 - logprior: -1.6534e+00
Epoch 3/10
39/39 - 34s - loss: 516.1447 - loglik: -5.1383e+02 - logprior: -1.6197e+00
Epoch 4/10
39/39 - 33s - loss: 513.5014 - loglik: -5.1135e+02 - logprior: -1.5287e+00
Epoch 5/10
39/39 - 34s - loss: 512.3796 - loglik: -5.1029e+02 - logprior: -1.5221e+00
Epoch 6/10
39/39 - 33s - loss: 511.7122 - loglik: -5.0966e+02 - logprior: -1.5269e+00
Epoch 7/10
39/39 - 33s - loss: 511.3591 - loglik: -5.0933e+02 - logprior: -1.5385e+00
Epoch 8/10
39/39 - 34s - loss: 511.0075 - loglik: -5.0899e+02 - logprior: -1.5348e+00
Epoch 9/10
39/39 - 36s - loss: 510.7689 - loglik: -5.0874e+02 - logprior: -1.5492e+00
Epoch 10/10
39/39 - 38s - loss: 510.2520 - loglik: -5.0822e+02 - logprior: -1.5513e+00
Fitted a model with MAP estimate = -509.3063
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (71, 1), (73, 1), (79, 2), (80, 1), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 2), (112, 2), (113, 1), (132, 3), (133, 1), (145, 2), (147, 1), (148, 1), (159, 1), (163, 1), (169, 1), (170, 1), (172, 2), (173, 1), (180, 1), (183, 1), (184, 1), (190, 1), (207, 1), (210, 1), (215, 2), (217, 2), (218, 1), (230, 3), (231, 1), (244, 1), (259, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 483.6248 - loglik: -4.8133e+02 - logprior: -2.1448e+00
Epoch 2/2
39/39 - 47s - loss: 463.1636 - loglik: -4.6194e+02 - logprior: -7.1485e-01
Fitted a model with MAP estimate = -459.6138
expansions: []
discards: [  0   1  97 137 141 165 181 216]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 472.3179 - loglik: -4.7076e+02 - logprior: -1.4988e+00
Epoch 2/2
39/39 - 41s - loss: 464.0129 - loglik: -4.6370e+02 - logprior: 0.0279
Fitted a model with MAP estimate = -460.3757
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 467.3485 - loglik: -4.6572e+02 - logprior: -1.4080e+00
Epoch 2/10
39/39 - 42s - loss: 461.4564 - loglik: -4.6086e+02 - logprior: -1.8849e-02
Epoch 3/10
39/39 - 42s - loss: 459.1721 - loglik: -4.5874e+02 - logprior: 0.2833
Epoch 4/10
39/39 - 41s - loss: 457.5230 - loglik: -4.5708e+02 - logprior: 0.2734
Epoch 5/10
39/39 - 41s - loss: 456.9686 - loglik: -4.5680e+02 - logprior: 0.4871
Epoch 6/10
39/39 - 40s - loss: 456.2296 - loglik: -4.5619e+02 - logprior: 0.5811
Epoch 7/10
39/39 - 40s - loss: 456.8071 - loglik: -4.5683e+02 - logprior: 0.6133
Fitted a model with MAP estimate = -454.9207
Time for alignment: 1048.8615
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 737.6917 - loglik: -7.3595e+02 - logprior: -1.5739e+00
Epoch 2/10
39/39 - 28s - loss: 528.5259 - loglik: -5.2608e+02 - logprior: -1.7046e+00
Epoch 3/10
39/39 - 28s - loss: 516.3500 - loglik: -5.1399e+02 - logprior: -1.6564e+00
Epoch 4/10
39/39 - 28s - loss: 513.6586 - loglik: -5.1147e+02 - logprior: -1.5780e+00
Epoch 5/10
39/39 - 28s - loss: 512.6741 - loglik: -5.1055e+02 - logprior: -1.5743e+00
Epoch 6/10
39/39 - 27s - loss: 511.9285 - loglik: -5.0984e+02 - logprior: -1.5894e+00
Epoch 7/10
39/39 - 27s - loss: 511.4916 - loglik: -5.0942e+02 - logprior: -1.5905e+00
Epoch 8/10
39/39 - 27s - loss: 511.2578 - loglik: -5.0919e+02 - logprior: -1.6005e+00
Epoch 9/10
39/39 - 27s - loss: 511.0354 - loglik: -5.0896e+02 - logprior: -1.6041e+00
Epoch 10/10
39/39 - 27s - loss: 510.9103 - loglik: -5.0883e+02 - logprior: -1.6071e+00
Fitted a model with MAP estimate = -509.5629
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 2), (80, 1), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (123, 1), (132, 1), (133, 1), (142, 1), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (188, 1), (189, 1), (209, 1), (212, 1), (214, 1), (215, 1), (216, 1), (217, 1), (227, 1), (229, 3), (230, 1), (243, 1), (265, 1), (267, 2), (269, 2), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 345 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 484.7989 - loglik: -4.8275e+02 - logprior: -1.8988e+00
Epoch 2/2
39/39 - 39s - loss: 463.5005 - loglik: -4.6237e+02 - logprior: -6.2042e-01
Fitted a model with MAP estimate = -460.1003
expansions: []
discards: [  0  97 212]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 470.0448 - loglik: -4.6727e+02 - logprior: -2.4017e+00
Epoch 2/2
39/39 - 40s - loss: 463.4377 - loglik: -4.6237e+02 - logprior: -3.4745e-01
Fitted a model with MAP estimate = -460.0027
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 45s - loss: 470.6927 - loglik: -4.6947e+02 - logprior: -1.1679e+00
Epoch 2/10
39/39 - 41s - loss: 462.1365 - loglik: -4.6194e+02 - logprior: 0.1095
Epoch 3/10
39/39 - 42s - loss: 459.7383 - loglik: -4.5938e+02 - logprior: 0.1539
Epoch 4/10
39/39 - 42s - loss: 457.6147 - loglik: -4.5740e+02 - logprior: 0.3749
Epoch 5/10
39/39 - 42s - loss: 457.7245 - loglik: -4.5745e+02 - logprior: 0.3163
Fitted a model with MAP estimate = -456.1243
Time for alignment: 840.0285
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 736.5007 - loglik: -7.3474e+02 - logprior: -1.5593e+00
Epoch 2/10
39/39 - 33s - loss: 531.2194 - loglik: -5.2888e+02 - logprior: -1.6010e+00
Epoch 3/10
39/39 - 35s - loss: 519.2440 - loglik: -5.1702e+02 - logprior: -1.5282e+00
Epoch 4/10
39/39 - 36s - loss: 516.4491 - loglik: -5.1431e+02 - logprior: -1.5240e+00
Epoch 5/10
39/39 - 37s - loss: 515.0434 - loglik: -5.1299e+02 - logprior: -1.5118e+00
Epoch 6/10
39/39 - 37s - loss: 514.6650 - loglik: -5.1264e+02 - logprior: -1.5319e+00
Epoch 7/10
39/39 - 37s - loss: 514.1388 - loglik: -5.1213e+02 - logprior: -1.5521e+00
Epoch 8/10
39/39 - 38s - loss: 513.6271 - loglik: -5.1164e+02 - logprior: -1.5466e+00
Epoch 9/10
39/39 - 34s - loss: 513.8406 - loglik: -5.1183e+02 - logprior: -1.5672e+00
Fitted a model with MAP estimate = -512.2845
expansions: [(0, 2), (1, 1), (14, 1), (15, 1), (17, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (72, 1), (79, 2), (80, 1), (82, 3), (83, 1), (89, 1), (92, 1), (108, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (137, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (181, 1), (182, 1), (188, 1), (189, 1), (209, 1), (213, 1), (214, 1), (215, 1), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 1), (270, 1), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 56s - loss: 484.2691 - loglik: -4.8213e+02 - logprior: -1.9374e+00
Epoch 2/2
39/39 - 46s - loss: 464.1311 - loglik: -4.6303e+02 - logprior: -6.8854e-01
Fitted a model with MAP estimate = -460.2176
expansions: []
discards: [  0   2  97 178 213]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 474.4565 - loglik: -4.7224e+02 - logprior: -2.1378e+00
Epoch 2/2
39/39 - 47s - loss: 465.0424 - loglik: -4.6461e+02 - logprior: -1.1329e-01
Fitted a model with MAP estimate = -461.1281
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 468.0185 - loglik: -4.6649e+02 - logprior: -1.3609e+00
Epoch 2/10
39/39 - 57s - loss: 461.8428 - loglik: -4.6139e+02 - logprior: 0.0549
Epoch 3/10
39/39 - 58s - loss: 459.6357 - loglik: -4.5893e+02 - logprior: -3.0426e-02
Epoch 4/10
39/39 - 54s - loss: 458.5000 - loglik: -4.5803e+02 - logprior: 0.2376
Epoch 5/10
39/39 - 56s - loss: 457.6926 - loglik: -4.5746e+02 - logprior: 0.4060
Epoch 6/10
39/39 - 57s - loss: 456.8952 - loglik: -4.5684e+02 - logprior: 0.5506
Epoch 7/10
39/39 - 55s - loss: 456.5262 - loglik: -4.5656e+02 - logprior: 0.6056
Epoch 8/10
39/39 - 50s - loss: 456.3290 - loglik: -4.5655e+02 - logprior: 0.7726
Epoch 9/10
39/39 - 54s - loss: 456.2762 - loglik: -4.5666e+02 - logprior: 0.9280
Epoch 10/10
39/39 - 49s - loss: 455.7570 - loglik: -4.5621e+02 - logprior: 0.9923
Fitted a model with MAP estimate = -454.7083
Time for alignment: 1324.0761
Computed alignments with likelihoods: ['-454.9207', '-456.1243', '-454.7083']
Best model has likelihood: -454.7083
time for generating output: 0.2834
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9197259909550413
Training of 3 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7feea5164a60>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7feea42e4fa0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fee9da5ae50>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee80ea0460>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee81a674c0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fee8048ddc0>, <__main__.SimpleDirichletPrior object at 0x7ff7cafa1100>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 397.5921 - loglik: -3.9452e+02 - logprior: -3.0303e+00
Epoch 2/10
19/19 - 6s - loss: 326.5904 - loglik: -3.2508e+02 - logprior: -1.2695e+00
Epoch 3/10
19/19 - 6s - loss: 300.6961 - loglik: -2.9834e+02 - logprior: -1.6185e+00
Epoch 4/10
19/19 - 6s - loss: 296.8458 - loglik: -2.9443e+02 - logprior: -1.4559e+00
Epoch 5/10
19/19 - 6s - loss: 294.2173 - loglik: -2.9190e+02 - logprior: -1.4128e+00
Epoch 6/10
19/19 - 6s - loss: 293.5020 - loglik: -2.9132e+02 - logprior: -1.3746e+00
Epoch 7/10
19/19 - 6s - loss: 292.9800 - loglik: -2.9090e+02 - logprior: -1.3596e+00
Epoch 8/10
19/19 - 6s - loss: 291.8192 - loglik: -2.8980e+02 - logprior: -1.3538e+00
Epoch 9/10
19/19 - 6s - loss: 292.6378 - loglik: -2.9068e+02 - logprior: -1.3473e+00
Fitted a model with MAP estimate = -290.2607
expansions: [(7, 2), (22, 1), (23, 1), (24, 1), (25, 2), (28, 1), (29, 2), (37, 2), (45, 2), (46, 3), (49, 2), (50, 2), (57, 2), (71, 1), (84, 2), (85, 3), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 302.3287 - loglik: -2.9885e+02 - logprior: -3.2336e+00
Epoch 2/2
39/39 - 10s - loss: 289.6869 - loglik: -2.8746e+02 - logprior: -1.6176e+00
Fitted a model with MAP estimate = -284.2730
expansions: []
discards: [ 29  36  47  58  60  66  68  77 108 141]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 294.4468 - loglik: -2.9199e+02 - logprior: -2.3550e+00
Epoch 2/2
39/39 - 9s - loss: 288.7798 - loglik: -2.8719e+02 - logprior: -1.2488e+00
Fitted a model with MAP estimate = -284.6531
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 291.8713 - loglik: -2.8945e+02 - logprior: -2.2212e+00
Epoch 2/10
39/39 - 9s - loss: 287.6090 - loglik: -2.8599e+02 - logprior: -1.1383e+00
Epoch 3/10
39/39 - 10s - loss: 283.9421 - loglik: -2.8212e+02 - logprior: -9.5449e-01
Epoch 4/10
39/39 - 9s - loss: 282.2963 - loglik: -2.8046e+02 - logprior: -8.2007e-01
Epoch 5/10
39/39 - 9s - loss: 280.2472 - loglik: -2.7861e+02 - logprior: -7.3059e-01
Epoch 6/10
39/39 - 9s - loss: 280.3825 - loglik: -2.7895e+02 - logprior: -6.6224e-01
Fitted a model with MAP estimate = -278.9462
Time for alignment: 210.7287
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 397.8009 - loglik: -3.9474e+02 - logprior: -3.0269e+00
Epoch 2/10
19/19 - 6s - loss: 326.5224 - loglik: -3.2504e+02 - logprior: -1.2484e+00
Epoch 3/10
19/19 - 6s - loss: 301.6486 - loglik: -2.9929e+02 - logprior: -1.5859e+00
Epoch 4/10
19/19 - 6s - loss: 296.8525 - loglik: -2.9443e+02 - logprior: -1.4268e+00
Epoch 5/10
19/19 - 6s - loss: 295.3076 - loglik: -2.9299e+02 - logprior: -1.3853e+00
Epoch 6/10
19/19 - 6s - loss: 294.4336 - loglik: -2.9224e+02 - logprior: -1.3543e+00
Epoch 7/10
19/19 - 6s - loss: 293.6853 - loglik: -2.9161e+02 - logprior: -1.3422e+00
Epoch 8/10
19/19 - 6s - loss: 292.9923 - loglik: -2.9100e+02 - logprior: -1.3308e+00
Epoch 9/10
19/19 - 7s - loss: 293.4953 - loglik: -2.9155e+02 - logprior: -1.3275e+00
Fitted a model with MAP estimate = -291.0827
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (29, 2), (35, 1), (37, 1), (45, 2), (46, 3), (49, 2), (50, 2), (57, 2), (71, 1), (84, 2), (85, 3), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 302.8255 - loglik: -2.9944e+02 - logprior: -3.2461e+00
Epoch 2/2
39/39 - 10s - loss: 289.8015 - loglik: -2.8770e+02 - logprior: -1.6372e+00
Fitted a model with MAP estimate = -284.4358
expansions: []
discards: [ 23  30  36  58  60  66  68  77 108 141]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 293.4141 - loglik: -2.9085e+02 - logprior: -2.3426e+00
Epoch 2/2
39/39 - 10s - loss: 288.3807 - loglik: -2.8657e+02 - logprior: -1.2358e+00
Fitted a model with MAP estimate = -284.1717
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 293.1185 - loglik: -2.9076e+02 - logprior: -2.2419e+00
Epoch 2/10
39/39 - 10s - loss: 287.3998 - loglik: -2.8592e+02 - logprior: -1.1311e+00
Epoch 3/10
39/39 - 10s - loss: 284.7361 - loglik: -2.8307e+02 - logprior: -9.6659e-01
Epoch 4/10
39/39 - 10s - loss: 281.6444 - loglik: -2.7993e+02 - logprior: -8.0433e-01
Epoch 5/10
39/39 - 10s - loss: 280.1889 - loglik: -2.7860e+02 - logprior: -7.2734e-01
Epoch 6/10
39/39 - 10s - loss: 280.2641 - loglik: -2.7884e+02 - logprior: -6.6350e-01
Fitted a model with MAP estimate = -278.9704
Time for alignment: 214.9396
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 397.8481 - loglik: -3.9479e+02 - logprior: -3.0268e+00
Epoch 2/10
19/19 - 6s - loss: 325.5930 - loglik: -3.2407e+02 - logprior: -1.2704e+00
Epoch 3/10
19/19 - 7s - loss: 301.7402 - loglik: -2.9932e+02 - logprior: -1.6259e+00
Epoch 4/10
19/19 - 6s - loss: 296.7805 - loglik: -2.9431e+02 - logprior: -1.4624e+00
Epoch 5/10
19/19 - 6s - loss: 294.8925 - loglik: -2.9255e+02 - logprior: -1.4085e+00
Epoch 6/10
19/19 - 6s - loss: 294.5237 - loglik: -2.9232e+02 - logprior: -1.3601e+00
Epoch 7/10
19/19 - 6s - loss: 293.9079 - loglik: -2.9182e+02 - logprior: -1.3445e+00
Epoch 8/10
19/19 - 6s - loss: 291.8763 - loglik: -2.8988e+02 - logprior: -1.3305e+00
Epoch 9/10
19/19 - 6s - loss: 292.5915 - loglik: -2.9065e+02 - logprior: -1.3217e+00
Fitted a model with MAP estimate = -290.9978
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (28, 1), (29, 2), (36, 1), (37, 2), (46, 3), (49, 2), (50, 2), (55, 1), (71, 1), (84, 1), (85, 3), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 302.0489 - loglik: -2.9854e+02 - logprior: -3.2117e+00
Epoch 2/2
39/39 - 10s - loss: 289.3975 - loglik: -2.8717e+02 - logprior: -1.5600e+00
Fitted a model with MAP estimate = -284.0542
expansions: []
discards: [ 23  30  37  48  60  66  68 139]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 293.3737 - loglik: -2.9082e+02 - logprior: -2.3112e+00
Epoch 2/2
39/39 - 10s - loss: 288.3051 - loglik: -2.8648e+02 - logprior: -1.2288e+00
Fitted a model with MAP estimate = -284.0726
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 292.8446 - loglik: -2.9051e+02 - logprior: -2.2223e+00
Epoch 2/10
39/39 - 10s - loss: 287.7101 - loglik: -2.8618e+02 - logprior: -1.1296e+00
Epoch 3/10
39/39 - 10s - loss: 283.9285 - loglik: -2.8224e+02 - logprior: -9.4983e-01
Epoch 4/10
39/39 - 10s - loss: 282.0002 - loglik: -2.8025e+02 - logprior: -8.0317e-01
Epoch 5/10
39/39 - 10s - loss: 280.4524 - loglik: -2.7885e+02 - logprior: -7.1433e-01
Epoch 6/10
39/39 - 10s - loss: 280.1365 - loglik: -2.7870e+02 - logprior: -6.4977e-01
Epoch 7/10
39/39 - 10s - loss: 279.7856 - loglik: -2.7849e+02 - logprior: -5.9306e-01
Epoch 8/10
39/39 - 10s - loss: 279.0558 - loglik: -2.7784e+02 - logprior: -5.3886e-01
Epoch 9/10
39/39 - 10s - loss: 279.2014 - loglik: -2.7806e+02 - logprior: -4.7567e-01
Fitted a model with MAP estimate = -278.1426
Time for alignment: 244.0097
Computed alignments with likelihoods: ['-278.9462', '-278.9704', '-278.1426']
Best model has likelihood: -278.1426
time for generating output: 0.2606
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.42405334312106197
Training of 3 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff66b979bb0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66b519460>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feebc54d430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee88a483d0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7feea5b70490>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fee888deb20>, <__main__.SimpleDirichletPrior object at 0x7fee883ce2e0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 467.5980 - loglik: -4.6538e+02 - logprior: -1.9747e+00
Epoch 2/10
39/39 - 11s - loss: 422.1674 - loglik: -4.1970e+02 - logprior: -1.3989e+00
Epoch 3/10
39/39 - 11s - loss: 415.5023 - loglik: -4.1258e+02 - logprior: -1.4420e+00
Epoch 4/10
39/39 - 11s - loss: 413.6152 - loglik: -4.1081e+02 - logprior: -1.4284e+00
Epoch 5/10
39/39 - 11s - loss: 412.6205 - loglik: -4.0999e+02 - logprior: -1.4342e+00
Epoch 6/10
39/39 - 11s - loss: 411.8596 - loglik: -4.0939e+02 - logprior: -1.4366e+00
Epoch 7/10
39/39 - 11s - loss: 410.9642 - loglik: -4.0861e+02 - logprior: -1.4322e+00
Epoch 8/10
39/39 - 11s - loss: 410.8813 - loglik: -4.0859e+02 - logprior: -1.4387e+00
Epoch 9/10
39/39 - 11s - loss: 410.4789 - loglik: -4.0823e+02 - logprior: -1.4409e+00
Epoch 10/10
39/39 - 11s - loss: 409.5588 - loglik: -4.0736e+02 - logprior: -1.4435e+00
Fitted a model with MAP estimate = -408.9357
expansions: [(20, 1), (22, 1), (31, 1), (32, 17), (33, 2), (49, 1), (50, 2), (56, 1), (58, 2), (59, 2), (60, 1), (75, 1), (77, 1), (79, 1), (80, 9), (98, 1), (102, 1), (107, 1), (108, 1), (117, 1), (118, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 429.4072 - loglik: -4.2651e+02 - logprior: -2.2755e+00
Epoch 2/2
39/39 - 15s - loss: 403.8739 - loglik: -4.0131e+02 - logprior: -1.3105e+00
Fitted a model with MAP estimate = -397.3504
expansions: []
discards: [ 23  24  25  26  27  36  37  38  39  40  41  42  43  44  45  46  47  83
  87 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 417.1375 - loglik: -4.1508e+02 - logprior: -1.9135e+00
Epoch 2/2
39/39 - 12s - loss: 408.6073 - loglik: -4.0725e+02 - logprior: -8.3643e-01
Fitted a model with MAP estimate = -405.0993
expansions: [(22, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 414.7835 - loglik: -4.1296e+02 - logprior: -1.7164e+00
Epoch 2/10
39/39 - 13s - loss: 407.9526 - loglik: -4.0691e+02 - logprior: -5.7048e-01
Epoch 3/10
39/39 - 13s - loss: 403.4367 - loglik: -4.0197e+02 - logprior: -5.0568e-01
Epoch 4/10
39/39 - 13s - loss: 400.6842 - loglik: -3.9890e+02 - logprior: -4.7560e-01
Epoch 5/10
39/39 - 13s - loss: 399.1276 - loglik: -3.9733e+02 - logprior: -4.3319e-01
Epoch 6/10
39/39 - 13s - loss: 398.6425 - loglik: -3.9699e+02 - logprior: -3.9001e-01
Epoch 7/10
39/39 - 13s - loss: 398.0362 - loglik: -3.9653e+02 - logprior: -3.5000e-01
Epoch 8/10
39/39 - 13s - loss: 397.5049 - loglik: -3.9616e+02 - logprior: -2.9380e-01
Epoch 9/10
39/39 - 12s - loss: 396.4539 - loglik: -3.9524e+02 - logprior: -2.4768e-01
Epoch 10/10
39/39 - 12s - loss: 396.8101 - loglik: -3.9571e+02 - logprior: -2.0828e-01
Fitted a model with MAP estimate = -395.3318
Time for alignment: 364.6191
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 467.5599 - loglik: -4.6500e+02 - logprior: -1.9453e+00
Epoch 2/10
39/39 - 11s - loss: 421.8365 - loglik: -4.1892e+02 - logprior: -1.4053e+00
Epoch 3/10
39/39 - 11s - loss: 416.7053 - loglik: -4.1371e+02 - logprior: -1.4084e+00
Epoch 4/10
39/39 - 11s - loss: 413.8255 - loglik: -4.1097e+02 - logprior: -1.4079e+00
Epoch 5/10
39/39 - 11s - loss: 412.6028 - loglik: -4.0995e+02 - logprior: -1.4051e+00
Epoch 6/10
39/39 - 11s - loss: 412.1843 - loglik: -4.0971e+02 - logprior: -1.4105e+00
Epoch 7/10
39/39 - 11s - loss: 411.6176 - loglik: -4.0929e+02 - logprior: -1.4056e+00
Epoch 8/10
39/39 - 11s - loss: 411.4923 - loglik: -4.0926e+02 - logprior: -1.4096e+00
Epoch 9/10
39/39 - 11s - loss: 410.3199 - loglik: -4.0815e+02 - logprior: -1.4072e+00
Epoch 10/10
39/39 - 11s - loss: 410.8739 - loglik: -4.0870e+02 - logprior: -1.4095e+00
Fitted a model with MAP estimate = -409.2951
expansions: [(19, 2), (20, 1), (31, 17), (32, 2), (48, 1), (49, 2), (58, 1), (59, 6), (73, 1), (74, 1), (75, 1), (78, 1), (79, 9), (93, 1), (107, 1), (108, 1), (117, 1), (118, 1), (119, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 434.4893 - loglik: -4.3200e+02 - logprior: -2.3640e+00
Epoch 2/2
39/39 - 15s - loss: 405.7382 - loglik: -4.0374e+02 - logprior: -1.3830e+00
Fitted a model with MAP estimate = -398.8604
expansions: [(21, 1), (68, 1)]
discards: [ 22  23  24  25  26  27  28  29  30  36  37  38  39  40  41  42  43  44
  45  46  47  86  87  88 116 117 118 119 120 121 122 123]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 421.6192 - loglik: -4.1957e+02 - logprior: -1.9130e+00
Epoch 2/2
39/39 - 13s - loss: 410.3424 - loglik: -4.0857e+02 - logprior: -8.2675e-01
Fitted a model with MAP estimate = -405.3532
expansions: [(93, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 417.4861 - loglik: -4.1572e+02 - logprior: -1.7031e+00
Epoch 2/10
39/39 - 13s - loss: 409.7469 - loglik: -4.0885e+02 - logprior: -5.5375e-01
Epoch 3/10
39/39 - 13s - loss: 405.1742 - loglik: -4.0385e+02 - logprior: -5.2089e-01
Epoch 4/10
39/39 - 12s - loss: 402.8260 - loglik: -4.0118e+02 - logprior: -4.8834e-01
Epoch 5/10
39/39 - 12s - loss: 401.0336 - loglik: -3.9934e+02 - logprior: -4.5344e-01
Epoch 6/10
39/39 - 12s - loss: 400.0647 - loglik: -3.9847e+02 - logprior: -4.2233e-01
Epoch 7/10
39/39 - 12s - loss: 399.8266 - loglik: -3.9836e+02 - logprior: -3.8904e-01
Epoch 8/10
39/39 - 12s - loss: 398.9025 - loglik: -3.9759e+02 - logprior: -3.4958e-01
Epoch 9/10
39/39 - 12s - loss: 398.6579 - loglik: -3.9747e+02 - logprior: -3.1336e-01
Epoch 10/10
39/39 - 12s - loss: 398.8370 - loglik: -3.9776e+02 - logprior: -2.7650e-01
Fitted a model with MAP estimate = -397.3168
Time for alignment: 365.6392
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 467.9395 - loglik: -4.6536e+02 - logprior: -1.9382e+00
Epoch 2/10
39/39 - 10s - loss: 424.1582 - loglik: -4.2127e+02 - logprior: -1.3026e+00
Epoch 3/10
39/39 - 11s - loss: 418.1359 - loglik: -4.1493e+02 - logprior: -1.3504e+00
Epoch 4/10
39/39 - 10s - loss: 416.0895 - loglik: -4.1314e+02 - logprior: -1.3491e+00
Epoch 5/10
39/39 - 10s - loss: 414.8231 - loglik: -4.1217e+02 - logprior: -1.3467e+00
Epoch 6/10
39/39 - 11s - loss: 414.0002 - loglik: -4.1153e+02 - logprior: -1.3475e+00
Epoch 7/10
39/39 - 10s - loss: 413.7610 - loglik: -4.1144e+02 - logprior: -1.3510e+00
Epoch 8/10
39/39 - 11s - loss: 413.2225 - loglik: -4.1098e+02 - logprior: -1.3550e+00
Epoch 9/10
39/39 - 10s - loss: 412.7599 - loglik: -4.1058e+02 - logprior: -1.3550e+00
Epoch 10/10
39/39 - 10s - loss: 411.9810 - loglik: -4.0983e+02 - logprior: -1.3645e+00
Fitted a model with MAP estimate = -411.0757
expansions: [(20, 1), (33, 1), (34, 2), (47, 1), (48, 3), (49, 2), (58, 1), (59, 2), (60, 2), (74, 1), (75, 1), (76, 1), (78, 1), (79, 9), (93, 1), (107, 4), (108, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 161 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 428.2901 - loglik: -4.2545e+02 - logprior: -2.1836e+00
Epoch 2/2
39/39 - 12s - loss: 407.5576 - loglik: -4.0494e+02 - logprior: -1.0804e+00
Fitted a model with MAP estimate = -401.2257
expansions: [(35, 1)]
discards: [ 70  71  72 101 102 103 104 105 106]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 419.3034 - loglik: -4.1732e+02 - logprior: -1.8823e+00
Epoch 2/2
39/39 - 12s - loss: 408.8942 - loglik: -4.0769e+02 - logprior: -7.8586e-01
Fitted a model with MAP estimate = -405.1588
expansions: [(71, 1), (99, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 412.9137 - loglik: -4.1108e+02 - logprior: -1.6633e+00
Epoch 2/10
39/39 - 12s - loss: 405.8937 - loglik: -4.0472e+02 - logprior: -5.2478e-01
Epoch 3/10
39/39 - 12s - loss: 401.9628 - loglik: -4.0037e+02 - logprior: -4.4355e-01
Epoch 4/10
39/39 - 12s - loss: 399.4403 - loglik: -3.9754e+02 - logprior: -4.1565e-01
Epoch 5/10
39/39 - 12s - loss: 397.4595 - loglik: -3.9561e+02 - logprior: -3.7012e-01
Epoch 6/10
39/39 - 12s - loss: 397.2505 - loglik: -3.9558e+02 - logprior: -3.3546e-01
Epoch 7/10
39/39 - 12s - loss: 396.6486 - loglik: -3.9518e+02 - logprior: -2.9665e-01
Epoch 8/10
39/39 - 12s - loss: 395.6673 - loglik: -3.9437e+02 - logprior: -2.5215e-01
Epoch 9/10
39/39 - 12s - loss: 395.0649 - loglik: -3.9392e+02 - logprior: -2.0878e-01
Epoch 10/10
39/39 - 12s - loss: 395.4867 - loglik: -3.9446e+02 - logprior: -1.7086e-01
Fitted a model with MAP estimate = -394.0477
Time for alignment: 341.8878
Computed alignments with likelihoods: ['-395.3318', '-397.3168', '-394.0477']
Best model has likelihood: -394.0477
time for generating output: 0.2013
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.859306496612196
Training of 3 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7ff83d08bd30>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee9caa11f0>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feea5fb1df0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff8465301f0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66b869310>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7feea57a7340>, <__main__.SimpleDirichletPrior object at 0x7ff524309ca0>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 788.3403 - loglik: -7.8638e+02 - logprior: -1.4270e+00
Epoch 2/10
39/39 - 24s - loss: 716.5901 - loglik: -7.1441e+02 - logprior: -6.9439e-01
Epoch 3/10
39/39 - 25s - loss: 706.0397 - loglik: -7.0300e+02 - logprior: -7.8030e-01
Epoch 4/10
39/39 - 25s - loss: 700.4979 - loglik: -6.9698e+02 - logprior: -7.8894e-01
Epoch 5/10
39/39 - 25s - loss: 696.4080 - loglik: -6.9273e+02 - logprior: -8.2272e-01
Epoch 6/10
39/39 - 25s - loss: 693.8745 - loglik: -6.9045e+02 - logprior: -8.7097e-01
Epoch 7/10
39/39 - 25s - loss: 691.6668 - loglik: -6.8858e+02 - logprior: -9.2012e-01
Epoch 8/10
39/39 - 25s - loss: 690.5665 - loglik: -6.8773e+02 - logprior: -9.5827e-01
Epoch 9/10
39/39 - 25s - loss: 689.4911 - loglik: -6.8687e+02 - logprior: -9.7438e-01
Epoch 10/10
39/39 - 25s - loss: 688.5600 - loglik: -6.8611e+02 - logprior: -9.8775e-01
Fitted a model with MAP estimate = -686.3347
expansions: [(0, 4), (40, 1), (41, 1), (42, 2), (53, 1), (81, 1), (86, 2), (87, 18), (101, 2), (102, 2), (113, 3), (120, 1), (123, 1), (175, 2), (176, 4), (179, 1), (202, 1), (206, 7), (214, 1)]
discards: [ 77  78 157]
Re-initialized the encoder parameters.
Fitting a model of length 281 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 756.5455 - loglik: -7.5314e+02 - logprior: -2.7157e+00
Epoch 2/2
39/39 - 34s - loss: 702.8876 - loglik: -7.0027e+02 - logprior: -9.8750e-01
Fitted a model with MAP estimate = -692.6391
expansions: [(0, 3), (49, 1), (146, 2), (147, 1), (215, 1), (253, 1)]
discards: [  0   1   2  46  50  84  85 130 132 148 149 150 204 245 246 247 248 249]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 718.9186 - loglik: -7.1702e+02 - logprior: -1.7312e+00
Epoch 2/2
39/39 - 32s - loss: 698.8115 - loglik: -6.9756e+02 - logprior: -4.2314e-01
Fitted a model with MAP estimate = -692.7375
expansions: [(100, 3), (186, 2), (239, 3)]
discards: [  1   2  92 104 199]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 36s - loss: 710.9539 - loglik: -7.0954e+02 - logprior: -1.3260e+00
Epoch 2/10
39/39 - 32s - loss: 698.0498 - loglik: -6.9748e+02 - logprior: -2.0298e-01
Epoch 3/10
39/39 - 31s - loss: 691.6825 - loglik: -6.9075e+02 - logprior: -1.3460e-01
Epoch 4/10
39/39 - 31s - loss: 686.8590 - loglik: -6.8556e+02 - logprior: -5.7288e-02
Epoch 5/10
39/39 - 31s - loss: 683.5130 - loglik: -6.8186e+02 - logprior: -4.8940e-03
Epoch 6/10
39/39 - 31s - loss: 678.7759 - loglik: -6.7682e+02 - logprior: 0.0216
Epoch 7/10
39/39 - 32s - loss: 676.8939 - loglik: -6.7459e+02 - logprior: -1.7924e-01
Epoch 8/10
39/39 - 32s - loss: 675.2107 - loglik: -6.7316e+02 - logprior: 0.0377
Epoch 9/10
39/39 - 33s - loss: 672.6844 - loglik: -6.7086e+02 - logprior: 0.1893
Epoch 10/10
39/39 - 33s - loss: 671.6030 - loglik: -6.6973e+02 - logprior: 0.1838
Fitted a model with MAP estimate = -668.6780
Time for alignment: 851.5620
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 790.2357 - loglik: -7.8823e+02 - logprior: -1.4239e+00
Epoch 2/10
39/39 - 26s - loss: 717.0486 - loglik: -7.1471e+02 - logprior: -7.5674e-01
Epoch 3/10
39/39 - 26s - loss: 706.3388 - loglik: -7.0296e+02 - logprior: -9.0914e-01
Epoch 4/10
39/39 - 26s - loss: 701.7203 - loglik: -6.9798e+02 - logprior: -9.1780e-01
Epoch 5/10
39/39 - 26s - loss: 697.8888 - loglik: -6.9411e+02 - logprior: -9.3489e-01
Epoch 6/10
39/39 - 26s - loss: 695.2651 - loglik: -6.9166e+02 - logprior: -9.8707e-01
Epoch 7/10
39/39 - 26s - loss: 692.4915 - loglik: -6.8926e+02 - logprior: -1.0170e+00
Epoch 8/10
39/39 - 26s - loss: 691.8043 - loglik: -6.8888e+02 - logprior: -1.0583e+00
Epoch 9/10
39/39 - 26s - loss: 690.6832 - loglik: -6.8799e+02 - logprior: -1.0499e+00
Epoch 10/10
39/39 - 26s - loss: 689.9662 - loglik: -6.8745e+02 - logprior: -1.0483e+00
Fitted a model with MAP estimate = -687.6169
expansions: [(0, 3), (9, 1), (24, 1), (43, 2), (50, 1), (51, 1), (52, 1), (81, 1), (85, 1), (87, 17), (103, 1), (114, 4), (115, 2), (120, 1), (124, 1), (152, 4), (175, 4), (176, 1), (179, 1), (207, 7), (208, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 286 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 39s - loss: 764.4147 - loglik: -7.6120e+02 - logprior: -2.9662e+00
Epoch 2/2
39/39 - 36s - loss: 706.2408 - loglik: -7.0425e+02 - logprior: -9.9673e-01
Fitted a model with MAP estimate = -695.1517
expansions: [(0, 4), (49, 1), (59, 1), (144, 3), (190, 1), (256, 2), (258, 4)]
discards: [  0   1   2   3  50  51  57  88  89  91  92  93 115 116 117 118 150 151
 152 153 154 194 209 249 250 251 252 253 254]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 715.7881 - loglik: -7.1387e+02 - logprior: -1.6353e+00
Epoch 2/2
39/39 - 33s - loss: 698.1895 - loglik: -6.9647e+02 - logprior: -4.4453e-01
Fitted a model with MAP estimate = -691.3084
expansions: [(91, 2), (93, 2), (94, 1), (179, 1), (197, 1)]
discards: [  3  87  88  89 183]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 705.3746 - loglik: -7.0378e+02 - logprior: -1.3082e+00
Epoch 2/10
39/39 - 34s - loss: 695.4567 - loglik: -6.9443e+02 - logprior: -2.6120e-01
Epoch 3/10
39/39 - 34s - loss: 691.0689 - loglik: -6.8966e+02 - logprior: -1.8495e-01
Epoch 4/10
39/39 - 33s - loss: 687.6685 - loglik: -6.8590e+02 - logprior: -1.2179e-01
Epoch 5/10
39/39 - 33s - loss: 683.6904 - loglik: -6.8163e+02 - logprior: -5.0142e-02
Epoch 6/10
39/39 - 33s - loss: 680.5662 - loglik: -6.7832e+02 - logprior: 0.0176
Epoch 7/10
39/39 - 32s - loss: 677.7388 - loglik: -6.7548e+02 - logprior: 0.0868
Epoch 8/10
39/39 - 32s - loss: 675.9089 - loglik: -6.7381e+02 - logprior: 0.1412
Epoch 9/10
39/39 - 32s - loss: 673.8716 - loglik: -6.7201e+02 - logprior: 0.2285
Epoch 10/10
39/39 - 31s - loss: 672.6296 - loglik: -6.7087e+02 - logprior: 0.2773
Fitted a model with MAP estimate = -669.5170
Time for alignment: 892.3758
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 792.1121 - loglik: -7.9033e+02 - logprior: -1.4381e+00
Epoch 2/10
39/39 - 24s - loss: 718.4240 - loglik: -7.1636e+02 - logprior: -7.7929e-01
Epoch 3/10
39/39 - 24s - loss: 706.1623 - loglik: -7.0316e+02 - logprior: -9.0569e-01
Epoch 4/10
39/39 - 24s - loss: 701.4321 - loglik: -6.9790e+02 - logprior: -9.2437e-01
Epoch 5/10
39/39 - 24s - loss: 697.4144 - loglik: -6.9365e+02 - logprior: -9.6706e-01
Epoch 6/10
39/39 - 24s - loss: 695.0851 - loglik: -6.9149e+02 - logprior: -1.0204e+00
Epoch 7/10
39/39 - 24s - loss: 692.5158 - loglik: -6.8924e+02 - logprior: -1.0499e+00
Epoch 8/10
39/39 - 24s - loss: 691.7646 - loglik: -6.8872e+02 - logprior: -1.0726e+00
Epoch 9/10
39/39 - 24s - loss: 690.4106 - loglik: -6.8755e+02 - logprior: -1.0904e+00
Epoch 10/10
39/39 - 24s - loss: 689.3804 - loglik: -6.8664e+02 - logprior: -1.0954e+00
Fitted a model with MAP estimate = -686.8526
expansions: [(0, 3), (9, 1), (10, 1), (20, 1), (42, 2), (43, 1), (51, 2), (53, 1), (74, 1), (92, 15), (97, 2), (115, 5), (116, 2), (117, 1), (118, 1), (120, 1), (157, 2), (174, 6), (175, 1), (178, 1), (206, 7), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 287 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 756.7449 - loglik: -7.5356e+02 - logprior: -2.8114e+00
Epoch 2/2
39/39 - 33s - loss: 700.9645 - loglik: -6.9902e+02 - logprior: -1.0049e+00
Fitted a model with MAP estimate = -692.7733
expansions: [(0, 3), (222, 1), (225, 1)]
discards: [  0   1   2   3   4  15  16  52  61 110 111 126 150 153 154 155 227 259
 260]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 36s - loss: 708.1893 - loglik: -7.0632e+02 - logprior: -1.5645e+00
Epoch 2/2
39/39 - 33s - loss: 696.8807 - loglik: -6.9577e+02 - logprior: -3.6069e-01
Fitted a model with MAP estimate = -691.8144
expansions: [(0, 3), (13, 1), (179, 2), (182, 2)]
discards: [  1 183 184 185]
Re-initialized the encoder parameters.
Fitting a model of length 277 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 37s - loss: 706.3967 - loglik: -7.0446e+02 - logprior: -1.7795e+00
Epoch 2/10
39/39 - 34s - loss: 696.2575 - loglik: -6.9550e+02 - logprior: -2.1765e-01
Epoch 3/10
39/39 - 34s - loss: 691.1960 - loglik: -6.9011e+02 - logprior: -1.2108e-01
Epoch 4/10
39/39 - 34s - loss: 687.7488 - loglik: -6.8633e+02 - logprior: -4.5226e-02
Epoch 5/10
39/39 - 34s - loss: 684.2291 - loglik: -6.8249e+02 - logprior: 0.0019
Epoch 6/10
39/39 - 34s - loss: 680.3522 - loglik: -6.7834e+02 - logprior: 0.0516
Epoch 7/10
39/39 - 34s - loss: 677.0989 - loglik: -6.7492e+02 - logprior: 0.1140
Epoch 8/10
39/39 - 34s - loss: 675.0619 - loglik: -6.7298e+02 - logprior: 0.2012
Epoch 9/10
39/39 - 34s - loss: 672.8263 - loglik: -6.7087e+02 - logprior: 0.2711
Epoch 10/10
39/39 - 35s - loss: 671.3024 - loglik: -6.6917e+02 - logprior: 0.1404
Fitted a model with MAP estimate = -667.9859
Time for alignment: 862.1646
Computed alignments with likelihoods: ['-668.6780', '-669.5170', '-667.9859']
Best model has likelihood: -667.9859
time for generating output: 0.3851
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.580707635009311
Training of 3 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fee88293b20>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff524360e80>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feebc54ddf0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee9025b040>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee9d22a0a0>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7ff6760019a0>, <__main__.SimpleDirichletPrior object at 0x7ff7ce7a0c70>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 571.4071 - loglik: -5.6924e+02 - logprior: -1.8564e+00
Epoch 2/10
39/39 - 14s - loss: 476.7705 - loglik: -4.7400e+02 - logprior: -1.7970e+00
Epoch 3/10
39/39 - 14s - loss: 469.4469 - loglik: -4.6653e+02 - logprior: -1.8531e+00
Epoch 4/10
39/39 - 14s - loss: 467.4434 - loglik: -4.6458e+02 - logprior: -1.8227e+00
Epoch 5/10
39/39 - 14s - loss: 466.2529 - loglik: -4.6342e+02 - logprior: -1.8196e+00
Epoch 6/10
39/39 - 14s - loss: 465.6999 - loglik: -4.6293e+02 - logprior: -1.8222e+00
Epoch 7/10
39/39 - 14s - loss: 465.1252 - loglik: -4.6239e+02 - logprior: -1.8340e+00
Epoch 8/10
39/39 - 14s - loss: 464.6614 - loglik: -4.6197e+02 - logprior: -1.8417e+00
Epoch 9/10
39/39 - 14s - loss: 464.4437 - loglik: -4.6177e+02 - logprior: -1.8496e+00
Epoch 10/10
39/39 - 14s - loss: 464.3816 - loglik: -4.6177e+02 - logprior: -1.8478e+00
Fitted a model with MAP estimate = -460.4184
expansions: [(12, 1), (15, 1), (22, 1), (23, 2), (24, 1), (30, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (47, 1), (48, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (129, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 1), (154, 1), (155, 1), (156, 1), (157, 2), (163, 1), (168, 1), (178, 1), (179, 1), (182, 1), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 471.2226 - loglik: -4.6817e+02 - logprior: -2.7184e+00
Epoch 2/2
39/39 - 17s - loss: 449.7353 - loglik: -4.4746e+02 - logprior: -1.3702e+00
Fitted a model with MAP estimate = -442.5340
expansions: [(3, 1)]
discards: [  0  26  84 139 143 187 200]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 460.4429 - loglik: -4.5783e+02 - logprior: -2.4339e+00
Epoch 2/2
39/39 - 17s - loss: 448.6023 - loglik: -4.4709e+02 - logprior: -7.7846e-01
Fitted a model with MAP estimate = -442.4241
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 455.0920 - loglik: -4.5240e+02 - logprior: -2.3242e+00
Epoch 2/10
39/39 - 17s - loss: 446.1294 - loglik: -4.4383e+02 - logprior: -1.1449e+00
Epoch 3/10
39/39 - 17s - loss: 442.4888 - loglik: -4.4074e+02 - logprior: -2.8020e-01
Epoch 4/10
39/39 - 17s - loss: 441.5976 - loglik: -4.3995e+02 - logprior: -2.0774e-01
Epoch 5/10
39/39 - 17s - loss: 440.1262 - loglik: -4.3861e+02 - logprior: -1.3345e-01
Epoch 6/10
39/39 - 17s - loss: 439.6796 - loglik: -4.3832e+02 - logprior: -4.6610e-02
Epoch 7/10
39/39 - 17s - loss: 438.7149 - loglik: -4.3749e+02 - logprior: 0.0271
Epoch 8/10
39/39 - 17s - loss: 438.0586 - loglik: -4.3698e+02 - logprior: 0.1051
Epoch 9/10
39/39 - 18s - loss: 436.6065 - loglik: -4.3570e+02 - logprior: 0.1877
Epoch 10/10
39/39 - 18s - loss: 437.5146 - loglik: -4.3675e+02 - logprior: 0.2629
Fitted a model with MAP estimate = -435.0760
Time for alignment: 480.9755
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 572.2272 - loglik: -5.7017e+02 - logprior: -1.8530e+00
Epoch 2/10
39/39 - 14s - loss: 476.9130 - loglik: -4.7429e+02 - logprior: -1.7683e+00
Epoch 3/10
39/39 - 14s - loss: 470.5244 - loglik: -4.6780e+02 - logprior: -1.7585e+00
Epoch 4/10
39/39 - 14s - loss: 468.4842 - loglik: -4.6579e+02 - logprior: -1.7259e+00
Epoch 5/10
39/39 - 14s - loss: 467.3189 - loglik: -4.6463e+02 - logprior: -1.7224e+00
Epoch 6/10
39/39 - 14s - loss: 466.7531 - loglik: -4.6412e+02 - logprior: -1.7303e+00
Epoch 7/10
39/39 - 14s - loss: 466.2924 - loglik: -4.6368e+02 - logprior: -1.7379e+00
Epoch 8/10
39/39 - 14s - loss: 465.7132 - loglik: -4.6313e+02 - logprior: -1.7437e+00
Epoch 9/10
39/39 - 14s - loss: 465.6597 - loglik: -4.6310e+02 - logprior: -1.7510e+00
Epoch 10/10
39/39 - 14s - loss: 465.3199 - loglik: -4.6280e+02 - logprior: -1.7522e+00
Fitted a model with MAP estimate = -461.5137
expansions: [(8, 1), (11, 1), (22, 1), (23, 2), (24, 1), (29, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 2), (37, 1), (42, 1), (45, 1), (46, 1), (48, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (92, 1), (94, 1), (108, 1), (110, 2), (111, 2), (112, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 1), (155, 1), (156, 1), (157, 2), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 475.7426 - loglik: -4.7272e+02 - logprior: -2.7680e+00
Epoch 2/2
39/39 - 18s - loss: 450.1368 - loglik: -4.4812e+02 - logprior: -1.4115e+00
Fitted a model with MAP estimate = -443.1790
expansions: [(3, 1), (198, 1)]
discards: [  0  26  84 139 142 186 199]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 457.3352 - loglik: -4.5476e+02 - logprior: -2.4439e+00
Epoch 2/2
39/39 - 18s - loss: 448.6809 - loglik: -4.4746e+02 - logprior: -7.7493e-01
Fitted a model with MAP estimate = -443.1064
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 455.7960 - loglik: -4.5334e+02 - logprior: -2.3018e+00
Epoch 2/10
39/39 - 18s - loss: 446.7210 - loglik: -4.4479e+02 - logprior: -1.1400e+00
Epoch 3/10
39/39 - 18s - loss: 443.0726 - loglik: -4.4141e+02 - logprior: -2.8752e-01
Epoch 4/10
39/39 - 18s - loss: 441.3342 - loglik: -4.3968e+02 - logprior: -2.1406e-01
Epoch 5/10
39/39 - 18s - loss: 440.5588 - loglik: -4.3898e+02 - logprior: -1.3981e-01
Epoch 6/10
39/39 - 18s - loss: 439.2954 - loglik: -4.3787e+02 - logprior: -6.9353e-02
Epoch 7/10
39/39 - 18s - loss: 438.9498 - loglik: -4.3766e+02 - logprior: 0.0021
Epoch 8/10
39/39 - 18s - loss: 438.1343 - loglik: -4.3703e+02 - logprior: 0.0809
Epoch 9/10
39/39 - 18s - loss: 437.1661 - loglik: -4.3622e+02 - logprior: 0.1541
Epoch 10/10
39/39 - 18s - loss: 436.7599 - loglik: -4.3596e+02 - logprior: 0.2337
Fitted a model with MAP estimate = -435.0586
Time for alignment: 495.9121
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 573.2154 - loglik: -5.7074e+02 - logprior: -1.8448e+00
Epoch 2/10
39/39 - 14s - loss: 477.8157 - loglik: -4.7473e+02 - logprior: -1.7551e+00
Epoch 3/10
39/39 - 14s - loss: 470.7018 - loglik: -4.6768e+02 - logprior: -1.7640e+00
Epoch 4/10
39/39 - 14s - loss: 469.0511 - loglik: -4.6622e+02 - logprior: -1.6856e+00
Epoch 5/10
39/39 - 14s - loss: 467.8751 - loglik: -4.6509e+02 - logprior: -1.6827e+00
Epoch 6/10
39/39 - 14s - loss: 467.2355 - loglik: -4.6453e+02 - logprior: -1.6869e+00
Epoch 7/10
39/39 - 14s - loss: 466.7201 - loglik: -4.6406e+02 - logprior: -1.6991e+00
Epoch 8/10
39/39 - 14s - loss: 466.2345 - loglik: -4.6364e+02 - logprior: -1.7031e+00
Epoch 9/10
39/39 - 14s - loss: 466.1198 - loglik: -4.6357e+02 - logprior: -1.7130e+00
Epoch 10/10
39/39 - 14s - loss: 465.6849 - loglik: -4.6321e+02 - logprior: -1.7143e+00
Fitted a model with MAP estimate = -461.9334
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (29, 1), (31, 1), (33, 3), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (47, 1), (48, 1), (66, 1), (67, 2), (69, 1), (72, 1), (83, 1), (86, 1), (87, 1), (88, 1), (93, 1), (96, 1), (108, 1), (109, 2), (111, 2), (126, 1), (128, 1), (149, 2), (150, 1), (151, 4), (153, 1), (155, 1), (156, 1), (157, 2), (163, 1), (168, 1), (180, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 23s - loss: 475.4746 - loglik: -4.7261e+02 - logprior: -2.7023e+00
Epoch 2/2
39/39 - 18s - loss: 450.6772 - loglik: -4.4894e+02 - logprior: -1.3684e+00
Fitted a model with MAP estimate = -443.8956
expansions: [(3, 1), (198, 1)]
discards: [  0  26  84 139 143 186 199]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 456.7510 - loglik: -4.5397e+02 - logprior: -2.4359e+00
Epoch 2/2
39/39 - 18s - loss: 448.4619 - loglik: -4.4662e+02 - logprior: -8.1138e-01
Fitted a model with MAP estimate = -442.0048
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 457.0841 - loglik: -4.5462e+02 - logprior: -2.2949e+00
Epoch 2/10
39/39 - 18s - loss: 446.6169 - loglik: -4.4486e+02 - logprior: -1.1496e+00
Epoch 3/10
39/39 - 18s - loss: 442.9677 - loglik: -4.4170e+02 - logprior: -2.7864e-01
Epoch 4/10
39/39 - 18s - loss: 441.5447 - loglik: -4.4019e+02 - logprior: -1.8769e-01
Epoch 5/10
39/39 - 18s - loss: 440.9057 - loglik: -4.3954e+02 - logprior: -1.2128e-01
Epoch 6/10
39/39 - 18s - loss: 439.5900 - loglik: -4.3827e+02 - logprior: -4.5413e-02
Epoch 7/10
39/39 - 18s - loss: 438.6450 - loglik: -4.3742e+02 - logprior: 0.0355
Epoch 8/10
39/39 - 18s - loss: 437.9765 - loglik: -4.3690e+02 - logprior: 0.1021
Epoch 9/10
39/39 - 18s - loss: 437.6126 - loglik: -4.3669e+02 - logprior: 0.1771
Epoch 10/10
39/39 - 18s - loss: 436.7550 - loglik: -4.3598e+02 - logprior: 0.2606
Fitted a model with MAP estimate = -434.8666
Time for alignment: 496.5402
Computed alignments with likelihoods: ['-435.0760', '-435.0586', '-434.8666']
Best model has likelihood: -434.8666
time for generating output: 0.3576
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.7282078045829474
Training of 3 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7ff6183e4f40>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7ff6183e4580>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4400>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e40a0>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4370>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4550>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7ff6183e4640>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e43d0>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4220>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e47c0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e44f0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183e4df0>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6070>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f60d0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6130>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6190>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f61f0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6250>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7ff6183f62b0> , emission_init : [<learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fee5828a910>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7ff66bde7c10>]
 , insertion_init : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fee91f7b6a0>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee79c466a0>] , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7ff618498ee0> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_exchangeabilities : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f63a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7ff6183f6430>, <tensorflow.python.keras.initializers.initializers_v2.RandomNormal object at 0x7fee681ba820>] , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147]
 , frozen_insertions : [True, False] , surgery_del : 0.5 , surgery_ins : 0.5
 , emission_func : <function reduce_multinomial_emissions at 0x7ff88d64a280> , emission_matrix_generator : [<function make_default_emission_matrix at 0x7ff72d60f430>, <function make_default_emission_matrix at 0x7ff72d60f430>] , emission_prior : [<learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fee8053b460>, <__main__.SimpleDirichletPrior object at 0x7ff67f824730>]
 , kernel_dim : [25, 3] , num_rate_matrices : 3 , per_matrix_rate : True
 , matrix_rate_l2 : 0.1 , shared_rate_matrix : True
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 321.1588 - loglik: -3.1793e+02 - logprior: -3.0918e+00
Epoch 2/10
19/19 - 3s - loss: 276.1308 - loglik: -2.7431e+02 - logprior: -1.3819e+00
Epoch 3/10
19/19 - 3s - loss: 251.4574 - loglik: -2.4881e+02 - logprior: -1.7898e+00
Epoch 4/10
19/19 - 3s - loss: 244.9780 - loglik: -2.4247e+02 - logprior: -1.7351e+00
Epoch 5/10
19/19 - 3s - loss: 243.2515 - loglik: -2.4100e+02 - logprior: -1.7319e+00
Epoch 6/10
19/19 - 3s - loss: 242.4181 - loglik: -2.4030e+02 - logprior: -1.6848e+00
Epoch 7/10
19/19 - 3s - loss: 242.1851 - loglik: -2.4015e+02 - logprior: -1.6557e+00
Epoch 8/10
19/19 - 3s - loss: 242.0776 - loglik: -2.4010e+02 - logprior: -1.6340e+00
Epoch 9/10
19/19 - 3s - loss: 241.6118 - loglik: -2.3964e+02 - logprior: -1.6296e+00
Epoch 10/10
19/19 - 3s - loss: 241.8546 - loglik: -2.3991e+02 - logprior: -1.6225e+00
Fitted a model with MAP estimate = -241.0150
expansions: [(17, 2), (18, 4), (19, 2), (22, 1), (27, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (48, 1), (52, 1), (56, 1), (62, 1), (63, 1), (65, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 250.3523 - loglik: -2.4626e+02 - logprior: -3.9799e+00
Epoch 2/2
19/19 - 4s - loss: 236.6954 - loglik: -2.3430e+02 - logprior: -2.0635e+00
Fitted a model with MAP estimate = -233.8326
expansions: [(0, 2)]
discards: [ 0 16 17 18 38 43 86]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 239.3762 - loglik: -2.3641e+02 - logprior: -2.9015e+00
Epoch 2/2
19/19 - 4s - loss: 233.9508 - loglik: -2.3267e+02 - logprior: -1.0492e+00
Fitted a model with MAP estimate = -232.1990
expansions: [(21, 2), (23, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 241.0313 - loglik: -2.3713e+02 - logprior: -3.7593e+00
Epoch 2/10
19/19 - 4s - loss: 234.7291 - loglik: -2.3315e+02 - logprior: -1.3118e+00
Epoch 3/10
19/19 - 4s - loss: 231.7321 - loglik: -2.3018e+02 - logprior: -1.0727e+00
Epoch 4/10
19/19 - 4s - loss: 230.5310 - loglik: -2.2891e+02 - logprior: -1.0444e+00
Epoch 5/10
19/19 - 4s - loss: 230.2727 - loglik: -2.2872e+02 - logprior: -1.0067e+00
Epoch 6/10
19/19 - 4s - loss: 229.5895 - loglik: -2.2810e+02 - logprior: -9.8069e-01
Epoch 7/10
19/19 - 4s - loss: 229.5132 - loglik: -2.2808e+02 - logprior: -9.5227e-01
Epoch 8/10
19/19 - 4s - loss: 229.0293 - loglik: -2.2765e+02 - logprior: -9.2628e-01
Epoch 9/10
19/19 - 4s - loss: 229.0449 - loglik: -2.2772e+02 - logprior: -9.0395e-01
Fitted a model with MAP estimate = -228.3352
Time for alignment: 123.4786
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 320.8298 - loglik: -3.1763e+02 - logprior: -3.0929e+00
Epoch 2/10
19/19 - 3s - loss: 275.1635 - loglik: -2.7327e+02 - logprior: -1.3932e+00
Epoch 3/10
19/19 - 3s - loss: 253.2913 - loglik: -2.5069e+02 - logprior: -1.7486e+00
Epoch 4/10
19/19 - 3s - loss: 246.2390 - loglik: -2.4379e+02 - logprior: -1.7126e+00
Epoch 5/10
19/19 - 3s - loss: 243.5878 - loglik: -2.4134e+02 - logprior: -1.7109e+00
Epoch 6/10
19/19 - 3s - loss: 242.6873 - loglik: -2.4057e+02 - logprior: -1.6854e+00
Epoch 7/10
19/19 - 3s - loss: 242.4741 - loglik: -2.4044e+02 - logprior: -1.6635e+00
Epoch 8/10
19/19 - 3s - loss: 241.7988 - loglik: -2.3980e+02 - logprior: -1.6522e+00
Epoch 9/10
19/19 - 3s - loss: 241.7709 - loglik: -2.3981e+02 - logprior: -1.6465e+00
Epoch 10/10
19/19 - 3s - loss: 241.6212 - loglik: -2.3967e+02 - logprior: -1.6420e+00
Fitted a model with MAP estimate = -241.1162
expansions: [(17, 1), (19, 2), (20, 3), (22, 2), (25, 1), (27, 1), (28, 2), (32, 2), (34, 1), (43, 1), (44, 1), (48, 1), (49, 1), (59, 1), (62, 1), (63, 1), (69, 2), (71, 1), (74, 1), (76, 1), (79, 1), (81, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 251.6892 - loglik: -2.4763e+02 - logprior: -4.0065e+00
Epoch 2/2
19/19 - 4s - loss: 237.5721 - loglik: -2.3520e+02 - logprior: -2.1064e+00
Fitted a model with MAP estimate = -234.7533
expansions: [(0, 2)]
discards: [ 0 15 16 27 38 43]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 238.5484 - loglik: -2.3559e+02 - logprior: -2.9182e+00
Epoch 2/2
19/19 - 4s - loss: 233.6242 - loglik: -2.3237e+02 - logprior: -1.0759e+00
Fitted a model with MAP estimate = -231.8596
expansions: []
discards: [ 0 21 86]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 240.9614 - loglik: -2.3716e+02 - logprior: -3.7173e+00
Epoch 2/10
19/19 - 4s - loss: 234.8037 - loglik: -2.3334e+02 - logprior: -1.2514e+00
Epoch 3/10
19/19 - 4s - loss: 232.3327 - loglik: -2.3083e+02 - logprior: -1.0737e+00
Epoch 4/10
19/19 - 4s - loss: 231.3110 - loglik: -2.2973e+02 - logprior: -1.0389e+00
Epoch 5/10
19/19 - 4s - loss: 230.3423 - loglik: -2.2877e+02 - logprior: -1.0077e+00
Epoch 6/10
19/19 - 4s - loss: 230.4827 - loglik: -2.2896e+02 - logprior: -9.8068e-01
Fitted a model with MAP estimate = -229.4482
Time for alignment: 112.0517
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 320.8161 - loglik: -3.1765e+02 - logprior: -3.0890e+00
Epoch 2/10
19/19 - 3s - loss: 276.3287 - loglik: -2.7445e+02 - logprior: -1.3783e+00
Epoch 3/10
19/19 - 3s - loss: 252.8920 - loglik: -2.5031e+02 - logprior: -1.7423e+00
Epoch 4/10
19/19 - 3s - loss: 246.6570 - loglik: -2.4422e+02 - logprior: -1.6820e+00
Epoch 5/10
19/19 - 3s - loss: 244.4470 - loglik: -2.4219e+02 - logprior: -1.6830e+00
Epoch 6/10
19/19 - 3s - loss: 243.4502 - loglik: -2.4135e+02 - logprior: -1.6569e+00
Epoch 7/10
19/19 - 3s - loss: 243.0722 - loglik: -2.4104e+02 - logprior: -1.6474e+00
Epoch 8/10
19/19 - 3s - loss: 242.9287 - loglik: -2.4095e+02 - logprior: -1.6362e+00
Epoch 9/10
19/19 - 3s - loss: 242.7212 - loglik: -2.4077e+02 - logprior: -1.6282e+00
Epoch 10/10
19/19 - 3s - loss: 242.2389 - loglik: -2.4031e+02 - logprior: -1.6258e+00
Fitted a model with MAP estimate = -242.0243
expansions: [(17, 1), (19, 2), (20, 3), (22, 2), (23, 1), (27, 1), (28, 2), (32, 2), (40, 1), (43, 1), (44, 1), (49, 1), (56, 1), (59, 1), (62, 1), (63, 1), (64, 2), (69, 1), (70, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 253.1566 - loglik: -2.4913e+02 - logprior: -3.9956e+00
Epoch 2/2
19/19 - 4s - loss: 237.6175 - loglik: -2.3531e+02 - logprior: -2.1026e+00
Fitted a model with MAP estimate = -234.6854
expansions: [(0, 2)]
discards: [ 0 15 16 28 38 43 86]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 238.2604 - loglik: -2.3531e+02 - logprior: -2.8894e+00
Epoch 2/2
19/19 - 4s - loss: 233.5291 - loglik: -2.3228e+02 - logprior: -1.0506e+00
Fitted a model with MAP estimate = -231.8532
expansions: []
discards: [ 0 21]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 242.0249 - loglik: -2.3825e+02 - logprior: -3.6881e+00
Epoch 2/10
19/19 - 4s - loss: 235.8637 - loglik: -2.3450e+02 - logprior: -1.2272e+00
Epoch 3/10
19/19 - 4s - loss: 232.6463 - loglik: -2.3115e+02 - logprior: -1.0620e+00
Epoch 4/10
19/19 - 4s - loss: 231.1828 - loglik: -2.2955e+02 - logprior: -1.0329e+00
Epoch 5/10
19/19 - 4s - loss: 230.9344 - loglik: -2.2939e+02 - logprior: -9.9997e-01
Epoch 6/10
19/19 - 4s - loss: 230.4288 - loglik: -2.2895e+02 - logprior: -9.7841e-01
Epoch 7/10
19/19 - 4s - loss: 230.0295 - loglik: -2.2861e+02 - logprior: -9.5402e-01
Epoch 8/10
19/19 - 4s - loss: 229.4068 - loglik: -2.2805e+02 - logprior: -9.2740e-01
Epoch 9/10
19/19 - 4s - loss: 229.8383 - loglik: -2.2852e+02 - logprior: -9.0789e-01
Fitted a model with MAP estimate = -228.9880
Time for alignment: 122.2986
Computed alignments with likelihoods: ['-228.3352', '-229.4482', '-228.9880']
Best model has likelihood: -228.3352
time for generating output: 0.1791
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8486997635933806
