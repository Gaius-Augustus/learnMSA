Training of 3 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fad40c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fad40ee0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 575.7700 - loglik: -5.6732e+02 - logprior: -8.4528e+00
Epoch 2/10
12/12 - 5s - loss: 505.0580 - loglik: -5.0344e+02 - logprior: -1.6191e+00
Epoch 3/10
12/12 - 5s - loss: 442.8173 - loglik: -4.4129e+02 - logprior: -1.5243e+00
Epoch 4/10
12/12 - 5s - loss: 416.4699 - loglik: -4.1447e+02 - logprior: -1.9959e+00
Epoch 5/10
12/12 - 5s - loss: 408.5486 - loglik: -4.0652e+02 - logprior: -2.0243e+00
Epoch 6/10
12/12 - 5s - loss: 406.1124 - loglik: -4.0421e+02 - logprior: -1.9072e+00
Epoch 7/10
12/12 - 5s - loss: 406.1126 - loglik: -4.0428e+02 - logprior: -1.8298e+00
Fitted a model with MAP estimate = -404.8316
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 2), (35, 2), (36, 1), (47, 2), (48, 1), (58, 1), (60, 1), (62, 2), (74, 1), (80, 2), (81, 1), (83, 1), (86, 1), (87, 1), (90, 1), (96, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (160, 1), (162, 2), (168, 2)]
discards: [0]
Fitting a model of length 219 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 413.5429 - loglik: -4.0369e+02 - logprior: -9.8559e+00
Epoch 2/2
12/12 - 7s - loss: 393.1723 - loglik: -3.8914e+02 - logprior: -4.0286e+00
Fitted a model with MAP estimate = -390.0763
expansions: [(0, 3), (214, 1)]
discards: [  0   9  41  45  82 102 143 177]
Fitting a model of length 215 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 397.1997 - loglik: -3.8959e+02 - logprior: -7.6090e+00
Epoch 2/2
12/12 - 7s - loss: 388.2815 - loglik: -3.8686e+02 - logprior: -1.4259e+00
Fitted a model with MAP estimate = -385.4725
expansions: []
discards: [  0   2  62 201]
Fitting a model of length 211 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 399.5857 - loglik: -3.8994e+02 - logprior: -9.6470e+00
Epoch 2/10
12/12 - 6s - loss: 389.4935 - loglik: -3.8604e+02 - logprior: -3.4490e+00
Epoch 3/10
12/12 - 6s - loss: 387.7407 - loglik: -3.8574e+02 - logprior: -2.0009e+00
Epoch 4/10
12/12 - 6s - loss: 384.0253 - loglik: -3.8369e+02 - logprior: -3.3343e-01
Epoch 5/10
12/12 - 6s - loss: 384.9600 - loglik: -3.8504e+02 - logprior: 0.0770
Fitted a model with MAP estimate = -382.7873
Time for alignment: 132.1007
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 574.3165 - loglik: -5.6584e+02 - logprior: -8.4808e+00
Epoch 2/10
12/12 - 5s - loss: 506.5864 - loglik: -5.0495e+02 - logprior: -1.6365e+00
Epoch 3/10
12/12 - 5s - loss: 447.2113 - loglik: -4.4570e+02 - logprior: -1.5077e+00
Epoch 4/10
12/12 - 5s - loss: 419.2279 - loglik: -4.1727e+02 - logprior: -1.9557e+00
Epoch 5/10
12/12 - 5s - loss: 407.7672 - loglik: -4.0576e+02 - logprior: -2.0066e+00
Epoch 6/10
12/12 - 5s - loss: 408.0106 - loglik: -4.0612e+02 - logprior: -1.8901e+00
Fitted a model with MAP estimate = -405.7109
expansions: [(9, 3), (10, 1), (11, 1), (13, 1), (16, 1), (31, 1), (32, 1), (33, 1), (35, 2), (48, 3), (57, 1), (58, 1), (60, 1), (62, 2), (74, 1), (80, 1), (81, 1), (82, 1), (83, 2), (90, 2), (96, 1), (102, 1), (112, 2), (122, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (168, 2)]
discards: [0]
Fitting a model of length 216 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 412.8723 - loglik: -4.0296e+02 - logprior: -9.9163e+00
Epoch 2/2
12/12 - 7s - loss: 394.3150 - loglik: -3.9028e+02 - logprior: -4.0356e+00
Fitted a model with MAP estimate = -390.4419
expansions: [(0, 3), (211, 1)]
discards: [  0   9  44  61  81 106 175]
Fitting a model of length 213 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 396.7203 - loglik: -3.8906e+02 - logprior: -7.6583e+00
Epoch 2/2
12/12 - 6s - loss: 388.0463 - loglik: -3.8658e+02 - logprior: -1.4668e+00
Fitted a model with MAP estimate = -385.9573
expansions: []
discards: [0 2]
Fitting a model of length 211 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 397.6704 - loglik: -3.8799e+02 - logprior: -9.6852e+00
Epoch 2/10
12/12 - 6s - loss: 389.8339 - loglik: -3.8643e+02 - logprior: -3.4081e+00
Epoch 3/10
12/12 - 6s - loss: 388.1825 - loglik: -3.8627e+02 - logprior: -1.9095e+00
Epoch 4/10
12/12 - 6s - loss: 384.7211 - loglik: -3.8441e+02 - logprior: -3.1191e-01
Epoch 5/10
12/12 - 6s - loss: 383.1900 - loglik: -3.8327e+02 - logprior: 0.0817
Epoch 6/10
12/12 - 6s - loss: 382.8206 - loglik: -3.8295e+02 - logprior: 0.1309
Epoch 7/10
12/12 - 6s - loss: 381.4490 - loglik: -3.8159e+02 - logprior: 0.1459
Epoch 8/10
12/12 - 6s - loss: 383.5486 - loglik: -3.8371e+02 - logprior: 0.1620
Fitted a model with MAP estimate = -381.1510
Time for alignment: 144.8658
Fitting a model of length 172 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 576.1743 - loglik: -5.6775e+02 - logprior: -8.4291e+00
Epoch 2/10
12/12 - 5s - loss: 502.6333 - loglik: -5.0105e+02 - logprior: -1.5863e+00
Epoch 3/10
12/12 - 5s - loss: 443.3670 - loglik: -4.4188e+02 - logprior: -1.4841e+00
Epoch 4/10
12/12 - 5s - loss: 416.0260 - loglik: -4.1414e+02 - logprior: -1.8873e+00
Epoch 5/10
12/12 - 5s - loss: 410.4136 - loglik: -4.0852e+02 - logprior: -1.8914e+00
Epoch 6/10
12/12 - 5s - loss: 408.6370 - loglik: -4.0691e+02 - logprior: -1.7320e+00
Epoch 7/10
12/12 - 5s - loss: 405.1357 - loglik: -4.0344e+02 - logprior: -1.6960e+00
Epoch 8/10
12/12 - 5s - loss: 405.0680 - loglik: -4.0338e+02 - logprior: -1.6859e+00
Epoch 9/10
12/12 - 5s - loss: 405.7959 - loglik: -4.0413e+02 - logprior: -1.6612e+00
Fitted a model with MAP estimate = -404.9639
expansions: [(9, 3), (10, 1), (11, 2), (31, 1), (32, 2), (33, 2), (35, 2), (48, 3), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (81, 1), (83, 1), (84, 2), (91, 2), (93, 1), (96, 1), (99, 1), (102, 1), (112, 2), (122, 2), (123, 2), (126, 2), (137, 1), (140, 2), (160, 1), (162, 2), (168, 2)]
discards: [0]
Fitting a model of length 219 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 411.9099 - loglik: -4.0202e+02 - logprior: -9.8868e+00
Epoch 2/2
12/12 - 7s - loss: 394.0150 - loglik: -3.8995e+02 - logprior: -4.0692e+00
Fitted a model with MAP estimate = -390.1444
expansions: [(0, 3), (214, 1)]
discards: [  0   9  41  45  62 156 158]
Fitting a model of length 216 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 396.2547 - loglik: -3.8863e+02 - logprior: -7.6211e+00
Epoch 2/2
12/12 - 7s - loss: 387.6631 - loglik: -3.8624e+02 - logprior: -1.4269e+00
Fitted a model with MAP estimate = -385.1328
expansions: []
discards: [  0   2  80 106 202]
Fitting a model of length 211 on 4316 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 9s - loss: 396.4571 - loglik: -3.8681e+02 - logprior: -9.6456e+00
Epoch 2/10
12/12 - 6s - loss: 393.4162 - loglik: -3.9004e+02 - logprior: -3.3792e+00
Epoch 3/10
12/12 - 6s - loss: 385.2195 - loglik: -3.8348e+02 - logprior: -1.7350e+00
Epoch 4/10
12/12 - 6s - loss: 384.5967 - loglik: -3.8439e+02 - logprior: -2.0667e-01
Epoch 5/10
12/12 - 6s - loss: 383.5699 - loglik: -3.8367e+02 - logprior: 0.1043
Epoch 6/10
12/12 - 6s - loss: 383.2567 - loglik: -3.8344e+02 - logprior: 0.1825
Epoch 7/10
12/12 - 6s - loss: 381.2190 - loglik: -3.8142e+02 - logprior: 0.2049
Epoch 8/10
12/12 - 6s - loss: 379.6658 - loglik: -3.7988e+02 - logprior: 0.2149
Epoch 9/10
12/12 - 6s - loss: 383.4672 - loglik: -3.8373e+02 - logprior: 0.2645
Fitted a model with MAP estimate = -380.8657
Time for alignment: 167.3916
Computed alignments with likelihoods: ['-382.7873', '-381.1510', '-380.8657']
Best model has likelihood: -380.8657
SP score = 0.9272
Training of 3 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f9bd2b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fa404c10>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 406.3442 - loglik: -3.8625e+02 - logprior: -2.0097e+01
Epoch 2/10
10/10 - 1s - loss: 341.7620 - loglik: -3.3717e+02 - logprior: -4.5874e+00
Epoch 3/10
10/10 - 1s - loss: 290.1143 - loglik: -2.8769e+02 - logprior: -2.4211e+00
Epoch 4/10
10/10 - 1s - loss: 257.3066 - loglik: -2.5517e+02 - logprior: -2.1327e+00
Epoch 5/10
10/10 - 1s - loss: 245.3691 - loglik: -2.4340e+02 - logprior: -1.9711e+00
Epoch 6/10
10/10 - 1s - loss: 241.2101 - loglik: -2.3955e+02 - logprior: -1.6637e+00
Epoch 7/10
10/10 - 1s - loss: 240.2242 - loglik: -2.3878e+02 - logprior: -1.4452e+00
Epoch 8/10
10/10 - 1s - loss: 239.0411 - loglik: -2.3770e+02 - logprior: -1.3431e+00
Epoch 9/10
10/10 - 1s - loss: 238.4349 - loglik: -2.3717e+02 - logprior: -1.2607e+00
Epoch 10/10
10/10 - 1s - loss: 237.9706 - loglik: -2.3678e+02 - logprior: -1.1903e+00
Fitted a model with MAP estimate = -238.1245
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (22, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (105, 1), (106, 5)]
discards: []
Fitting a model of length 145 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 258.2035 - loglik: -2.3171e+02 - logprior: -2.6498e+01
Epoch 2/2
10/10 - 2s - loss: 221.4902 - loglik: -2.1406e+02 - logprior: -7.4331e+00
Fitted a model with MAP estimate = -214.0272
expansions: [(133, 1)]
discards: [59 62]
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 227.8230 - loglik: -2.0933e+02 - logprior: -1.8497e+01
Epoch 2/2
10/10 - 2s - loss: 210.2690 - loglik: -2.0637e+02 - logprior: -3.9039e+00
Fitted a model with MAP estimate = -207.6484
expansions: []
discards: []
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 223.9107 - loglik: -2.0656e+02 - logprior: -1.7354e+01
Epoch 2/10
10/10 - 2s - loss: 209.2407 - loglik: -2.0571e+02 - logprior: -3.5340e+00
Epoch 3/10
10/10 - 2s - loss: 206.4976 - loglik: -2.0560e+02 - logprior: -8.9877e-01
Epoch 4/10
10/10 - 2s - loss: 204.1790 - loglik: -2.0418e+02 - logprior: -1.3293e-03
Epoch 5/10
10/10 - 2s - loss: 204.3546 - loglik: -2.0479e+02 - logprior: 0.4378
Fitted a model with MAP estimate = -203.3948
Time for alignment: 49.0329
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 406.0394 - loglik: -3.8594e+02 - logprior: -2.0100e+01
Epoch 2/10
10/10 - 1s - loss: 341.0667 - loglik: -3.3648e+02 - logprior: -4.5874e+00
Epoch 3/10
10/10 - 1s - loss: 287.0146 - loglik: -2.8463e+02 - logprior: -2.3825e+00
Epoch 4/10
10/10 - 1s - loss: 256.3519 - loglik: -2.5435e+02 - logprior: -2.0067e+00
Epoch 5/10
10/10 - 1s - loss: 246.0319 - loglik: -2.4421e+02 - logprior: -1.8179e+00
Epoch 6/10
10/10 - 1s - loss: 242.8048 - loglik: -2.4127e+02 - logprior: -1.5352e+00
Epoch 7/10
10/10 - 1s - loss: 241.3471 - loglik: -2.3999e+02 - logprior: -1.3535e+00
Epoch 8/10
10/10 - 1s - loss: 240.8266 - loglik: -2.3950e+02 - logprior: -1.3276e+00
Epoch 9/10
10/10 - 1s - loss: 239.2760 - loglik: -2.3795e+02 - logprior: -1.3230e+00
Epoch 10/10
10/10 - 1s - loss: 239.6309 - loglik: -2.3833e+02 - logprior: -1.2992e+00
Fitted a model with MAP estimate = -239.3835
expansions: [(0, 4), (13, 3), (15, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 1), (47, 1), (48, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 2), (104, 3), (105, 2), (106, 1)]
discards: []
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 257.0297 - loglik: -2.3058e+02 - logprior: -2.6450e+01
Epoch 2/2
10/10 - 2s - loss: 219.1562 - loglik: -2.1186e+02 - logprior: -7.2936e+00
Fitted a model with MAP estimate = -212.3725
expansions: []
discards: [ 62 121]
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 227.1034 - loglik: -2.0865e+02 - logprior: -1.8456e+01
Epoch 2/2
10/10 - 2s - loss: 209.6199 - loglik: -2.0571e+02 - logprior: -3.9059e+00
Fitted a model with MAP estimate = -207.5019
expansions: []
discards: []
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 224.9303 - loglik: -2.0757e+02 - logprior: -1.7362e+01
Epoch 2/10
10/10 - 2s - loss: 208.0747 - loglik: -2.0454e+02 - logprior: -3.5319e+00
Epoch 3/10
10/10 - 2s - loss: 206.4693 - loglik: -2.0556e+02 - logprior: -9.0552e-01
Epoch 4/10
10/10 - 2s - loss: 204.1787 - loglik: -2.0418e+02 - logprior: 7.5195e-04
Epoch 5/10
10/10 - 2s - loss: 204.3299 - loglik: -2.0477e+02 - logprior: 0.4365
Fitted a model with MAP estimate = -203.3354
Time for alignment: 49.0583
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 405.9625 - loglik: -3.8586e+02 - logprior: -2.0099e+01
Epoch 2/10
10/10 - 1s - loss: 341.5617 - loglik: -3.3697e+02 - logprior: -4.5875e+00
Epoch 3/10
10/10 - 1s - loss: 288.4617 - loglik: -2.8604e+02 - logprior: -2.4182e+00
Epoch 4/10
10/10 - 1s - loss: 255.2044 - loglik: -2.5302e+02 - logprior: -2.1840e+00
Epoch 5/10
10/10 - 1s - loss: 245.1667 - loglik: -2.4317e+02 - logprior: -1.9942e+00
Epoch 6/10
10/10 - 1s - loss: 241.5483 - loglik: -2.3992e+02 - logprior: -1.6244e+00
Epoch 7/10
10/10 - 1s - loss: 238.9394 - loglik: -2.3756e+02 - logprior: -1.3828e+00
Epoch 8/10
10/10 - 1s - loss: 238.9242 - loglik: -2.3764e+02 - logprior: -1.2878e+00
Epoch 9/10
10/10 - 1s - loss: 238.1846 - loglik: -2.3697e+02 - logprior: -1.2169e+00
Epoch 10/10
10/10 - 1s - loss: 237.7756 - loglik: -2.3659e+02 - logprior: -1.1815e+00
Fitted a model with MAP estimate = -237.7134
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (27, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (105, 5), (106, 2)]
discards: []
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 257.8376 - loglik: -2.3136e+02 - logprior: -2.6475e+01
Epoch 2/2
10/10 - 2s - loss: 221.3750 - loglik: -2.1395e+02 - logprior: -7.4263e+00
Fitted a model with MAP estimate = -214.0928
expansions: []
discards: [ 59  62 135]
Fitting a model of length 143 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 227.8957 - loglik: -2.0933e+02 - logprior: -1.8569e+01
Epoch 2/2
10/10 - 2s - loss: 210.8864 - loglik: -2.0687e+02 - logprior: -4.0199e+00
Fitted a model with MAP estimate = -208.2121
expansions: [(131, 1)]
discards: []
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.3053 - loglik: -2.0691e+02 - logprior: -1.7391e+01
Epoch 2/10
10/10 - 2s - loss: 209.8153 - loglik: -2.0626e+02 - logprior: -3.5551e+00
Epoch 3/10
10/10 - 2s - loss: 205.9854 - loglik: -2.0507e+02 - logprior: -9.1802e-01
Epoch 4/10
10/10 - 2s - loss: 205.3746 - loglik: -2.0536e+02 - logprior: -1.3059e-02
Epoch 5/10
10/10 - 2s - loss: 203.2721 - loglik: -2.0369e+02 - logprior: 0.4158
Epoch 6/10
10/10 - 2s - loss: 203.0976 - loglik: -2.0384e+02 - logprior: 0.7461
Epoch 7/10
10/10 - 2s - loss: 202.7654 - loglik: -2.0379e+02 - logprior: 1.0219
Epoch 8/10
10/10 - 2s - loss: 202.5931 - loglik: -2.0379e+02 - logprior: 1.1977
Epoch 9/10
10/10 - 2s - loss: 202.1729 - loglik: -2.0347e+02 - logprior: 1.2938
Epoch 10/10
10/10 - 2s - loss: 202.1423 - loglik: -2.0352e+02 - logprior: 1.3782
Fitted a model with MAP estimate = -202.0574
Time for alignment: 57.2766
Computed alignments with likelihoods: ['-203.3948', '-203.3354', '-202.0574']
Best model has likelihood: -202.0574
SP score = 0.9119
Training of 3 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2aa215c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2a1990ac0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 316.9191 - loglik: -2.4845e+02 - logprior: -6.8468e+01
Epoch 2/10
10/10 - 2s - loss: 236.4457 - loglik: -2.1924e+02 - logprior: -1.7203e+01
Epoch 3/10
10/10 - 1s - loss: 203.5830 - loglik: -1.9622e+02 - logprior: -7.3597e+00
Epoch 4/10
10/10 - 2s - loss: 188.8508 - loglik: -1.8501e+02 - logprior: -3.8398e+00
Epoch 5/10
10/10 - 1s - loss: 182.1289 - loglik: -1.8027e+02 - logprior: -1.8638e+00
Epoch 6/10
10/10 - 1s - loss: 178.3729 - loglik: -1.7758e+02 - logprior: -7.9376e-01
Epoch 7/10
10/10 - 1s - loss: 176.0067 - loglik: -1.7574e+02 - logprior: -2.6376e-01
Epoch 8/10
10/10 - 2s - loss: 174.6260 - loglik: -1.7474e+02 - logprior: 0.1113
Epoch 9/10
10/10 - 1s - loss: 173.9553 - loglik: -1.7430e+02 - logprior: 0.3471
Epoch 10/10
10/10 - 2s - loss: 173.5371 - loglik: -1.7403e+02 - logprior: 0.4965
Fitted a model with MAP estimate = -173.3258
expansions: [(9, 2), (14, 1), (15, 1), (25, 1), (26, 2), (27, 2), (37, 1), (39, 1), (58, 6)]
discards: [0]
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 250.0068 - loglik: -1.7328e+02 - logprior: -7.6722e+01
Epoch 2/2
10/10 - 2s - loss: 196.3801 - loglik: -1.6568e+02 - logprior: -3.0702e+01
Fitted a model with MAP estimate = -187.4404
expansions: []
discards: []
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 236.5670 - loglik: -1.6357e+02 - logprior: -7.2995e+01
Epoch 2/10
10/10 - 2s - loss: 182.3481 - loglik: -1.6158e+02 - logprior: -2.0769e+01
Epoch 3/10
10/10 - 2s - loss: 167.6474 - loglik: -1.6091e+02 - logprior: -6.7405e+00
Epoch 4/10
10/10 - 2s - loss: 162.8579 - loglik: -1.6099e+02 - logprior: -1.8715e+00
Epoch 5/10
10/10 - 2s - loss: 160.6791 - loglik: -1.6117e+02 - logprior: 0.4866
Epoch 6/10
10/10 - 2s - loss: 159.3763 - loglik: -1.6118e+02 - logprior: 1.8065
Epoch 7/10
10/10 - 2s - loss: 158.6064 - loglik: -1.6117e+02 - logprior: 2.5686
Epoch 8/10
10/10 - 2s - loss: 158.1275 - loglik: -1.6121e+02 - logprior: 3.0821
Epoch 9/10
10/10 - 2s - loss: 157.7713 - loglik: -1.6129e+02 - logprior: 3.5181
Epoch 10/10
10/10 - 2s - loss: 157.4726 - loglik: -1.6139e+02 - logprior: 3.9143
Fitted a model with MAP estimate = -157.2919
Time for alignment: 45.7449
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 316.9185 - loglik: -2.4845e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 2s - loss: 236.1177 - loglik: -2.1891e+02 - logprior: -1.7208e+01
Epoch 3/10
10/10 - 2s - loss: 203.3495 - loglik: -1.9598e+02 - logprior: -7.3663e+00
Epoch 4/10
10/10 - 2s - loss: 188.8361 - loglik: -1.8498e+02 - logprior: -3.8563e+00
Epoch 5/10
10/10 - 2s - loss: 181.5620 - loglik: -1.7954e+02 - logprior: -2.0202e+00
Epoch 6/10
10/10 - 2s - loss: 177.6158 - loglik: -1.7663e+02 - logprior: -9.8812e-01
Epoch 7/10
10/10 - 2s - loss: 175.5914 - loglik: -1.7521e+02 - logprior: -3.8078e-01
Epoch 8/10
10/10 - 2s - loss: 174.6639 - loglik: -1.7464e+02 - logprior: -2.7006e-02
Epoch 9/10
10/10 - 2s - loss: 174.1884 - loglik: -1.7444e+02 - logprior: 0.2490
Epoch 10/10
10/10 - 1s - loss: 173.8444 - loglik: -1.7427e+02 - logprior: 0.4306
Fitted a model with MAP estimate = -173.6898
expansions: [(14, 2), (15, 1), (25, 1), (26, 2), (37, 1), (52, 1), (58, 6)]
discards: [0]
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.4937 - loglik: -1.7458e+02 - logprior: -7.6911e+01
Epoch 2/2
10/10 - 2s - loss: 198.5533 - loglik: -1.6759e+02 - logprior: -3.0968e+01
Fitted a model with MAP estimate = -189.6912
expansions: []
discards: [ 0 13]
Fitting a model of length 82 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 242.9205 - loglik: -1.6750e+02 - logprior: -7.5421e+01
Epoch 2/2
10/10 - 1s - loss: 191.9060 - loglik: -1.6606e+02 - logprior: -2.5843e+01
Fitted a model with MAP estimate = -180.6690
expansions: [(0, 4), (5, 2), (7, 2)]
discards: [0]
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 227.2227 - loglik: -1.6625e+02 - logprior: -6.0977e+01
Epoch 2/10
10/10 - 2s - loss: 177.7401 - loglik: -1.6287e+02 - logprior: -1.4869e+01
Epoch 3/10
10/10 - 2s - loss: 167.3416 - loglik: -1.6209e+02 - logprior: -5.2555e+00
Epoch 4/10
10/10 - 2s - loss: 163.3338 - loglik: -1.6207e+02 - logprior: -1.2601e+00
Epoch 5/10
10/10 - 2s - loss: 161.1232 - loglik: -1.6205e+02 - logprior: 0.9288
Epoch 6/10
10/10 - 2s - loss: 159.9434 - loglik: -1.6220e+02 - logprior: 2.2614
Epoch 7/10
10/10 - 2s - loss: 159.2412 - loglik: -1.6234e+02 - logprior: 3.1023
Epoch 8/10
10/10 - 2s - loss: 158.7798 - loglik: -1.6245e+02 - logprior: 3.6671
Epoch 9/10
10/10 - 2s - loss: 158.4212 - loglik: -1.6250e+02 - logprior: 4.0821
Epoch 10/10
10/10 - 2s - loss: 158.1273 - loglik: -1.6255e+02 - logprior: 4.4246
Fitted a model with MAP estimate = -157.9442
Time for alignment: 52.4872
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.9142 - loglik: -2.4845e+02 - logprior: -6.8469e+01
Epoch 2/10
10/10 - 2s - loss: 236.0104 - loglik: -2.1880e+02 - logprior: -1.7206e+01
Epoch 3/10
10/10 - 2s - loss: 202.6740 - loglik: -1.9532e+02 - logprior: -7.3553e+00
Epoch 4/10
10/10 - 2s - loss: 187.0742 - loglik: -1.8323e+02 - logprior: -3.8425e+00
Epoch 5/10
10/10 - 2s - loss: 179.7375 - loglik: -1.7775e+02 - logprior: -1.9831e+00
Epoch 6/10
10/10 - 2s - loss: 176.5651 - loglik: -1.7571e+02 - logprior: -8.5472e-01
Epoch 7/10
10/10 - 2s - loss: 174.9530 - loglik: -1.7473e+02 - logprior: -2.2549e-01
Epoch 8/10
10/10 - 2s - loss: 174.1645 - loglik: -1.7428e+02 - logprior: 0.1175
Epoch 9/10
10/10 - 2s - loss: 173.7652 - loglik: -1.7416e+02 - logprior: 0.3979
Epoch 10/10
10/10 - 2s - loss: 173.4997 - loglik: -1.7409e+02 - logprior: 0.5891
Fitted a model with MAP estimate = -173.3773
expansions: [(9, 3), (14, 1), (15, 1), (37, 1), (39, 1), (58, 6)]
discards: [0]
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 250.6482 - loglik: -1.7378e+02 - logprior: -7.6871e+01
Epoch 2/2
10/10 - 2s - loss: 198.1498 - loglik: -1.6739e+02 - logprior: -3.0757e+01
Fitted a model with MAP estimate = -189.0120
expansions: []
discards: []
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 237.8862 - loglik: -1.6513e+02 - logprior: -7.2758e+01
Epoch 2/10
10/10 - 2s - loss: 183.7665 - loglik: -1.6336e+02 - logprior: -2.0411e+01
Epoch 3/10
10/10 - 2s - loss: 169.7542 - loglik: -1.6299e+02 - logprior: -6.7635e+00
Epoch 4/10
10/10 - 2s - loss: 165.2146 - loglik: -1.6323e+02 - logprior: -1.9854e+00
Epoch 5/10
10/10 - 2s - loss: 163.1153 - loglik: -1.6347e+02 - logprior: 0.3514
Epoch 6/10
10/10 - 2s - loss: 161.8992 - loglik: -1.6356e+02 - logprior: 1.6647
Epoch 7/10
10/10 - 2s - loss: 161.1163 - loglik: -1.6356e+02 - logprior: 2.4429
Epoch 8/10
10/10 - 2s - loss: 160.6440 - loglik: -1.6359e+02 - logprior: 2.9509
Epoch 9/10
10/10 - 2s - loss: 160.3021 - loglik: -1.6366e+02 - logprior: 3.3620
Epoch 10/10
10/10 - 2s - loss: 160.0122 - loglik: -1.6374e+02 - logprior: 3.7316
Fitted a model with MAP estimate = -159.8446
Time for alignment: 45.4836
Computed alignments with likelihoods: ['-157.2919', '-157.9442', '-159.8446']
Best model has likelihood: -157.2919
SP score = 0.3784
Training of 3 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c39ffa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe288309c40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 777.3029 - loglik: -7.6345e+02 - logprior: -1.3854e+01
Epoch 2/10
10/10 - 8s - loss: 737.2455 - loglik: -7.3533e+02 - logprior: -1.9159e+00
Epoch 3/10
10/10 - 8s - loss: 695.6308 - loglik: -6.9574e+02 - logprior: 0.1091
Epoch 4/10
10/10 - 8s - loss: 671.1052 - loglik: -6.7149e+02 - logprior: 0.3876
Epoch 5/10
10/10 - 8s - loss: 657.2982 - loglik: -6.5759e+02 - logprior: 0.2895
Epoch 6/10
10/10 - 8s - loss: 653.0502 - loglik: -6.5326e+02 - logprior: 0.2101
Epoch 7/10
10/10 - 8s - loss: 649.2388 - loglik: -6.4951e+02 - logprior: 0.2666
Epoch 8/10
10/10 - 8s - loss: 648.0980 - loglik: -6.4841e+02 - logprior: 0.3150
Epoch 9/10
10/10 - 8s - loss: 645.3160 - loglik: -6.4562e+02 - logprior: 0.3087
Epoch 10/10
10/10 - 8s - loss: 645.7242 - loglik: -6.4602e+02 - logprior: 0.2962
Fitted a model with MAP estimate = -645.6379
expansions: [(0, 4), (23, 2), (25, 3), (52, 4), (86, 12), (87, 1), (94, 1), (100, 1), (111, 5), (114, 1), (115, 2), (117, 1), (147, 1), (153, 2), (171, 9), (199, 4), (207, 1), (208, 1)]
discards: [217 218]
Fitting a model of length 278 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 667.4888 - loglik: -6.5640e+02 - logprior: -1.1093e+01
Epoch 2/2
20/20 - 12s - loss: 639.6736 - loglik: -6.3919e+02 - logprior: -4.8663e-01
Fitted a model with MAP estimate = -637.1995
expansions: [(0, 4), (54, 1)]
discards: [  1   2   3   4   5  27  31 139 141 142 143 149 192 211 212 213 214 215]
Fitting a model of length 265 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 654.6814 - loglik: -6.4270e+02 - logprior: -1.1977e+01
Epoch 2/2
20/20 - 11s - loss: 639.4082 - loglik: -6.3899e+02 - logprior: -4.1455e-01
Fitted a model with MAP estimate = -636.2204
expansions: [(0, 5), (237, 1), (260, 1)]
discards: [2 3 5 6]
Fitting a model of length 268 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 652.3667 - loglik: -6.4141e+02 - logprior: -1.0961e+01
Epoch 2/10
20/20 - 12s - loss: 638.3229 - loglik: -6.3764e+02 - logprior: -6.8767e-01
Epoch 3/10
20/20 - 12s - loss: 632.4996 - loglik: -6.3399e+02 - logprior: 1.4866
Epoch 4/10
20/20 - 11s - loss: 634.1952 - loglik: -6.3626e+02 - logprior: 2.0695
Fitted a model with MAP estimate = -630.0305
Time for alignment: 220.7427
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 777.2172 - loglik: -7.6336e+02 - logprior: -1.3855e+01
Epoch 2/10
10/10 - 8s - loss: 735.6252 - loglik: -7.3371e+02 - logprior: -1.9106e+00
Epoch 3/10
10/10 - 8s - loss: 697.1457 - loglik: -6.9727e+02 - logprior: 0.1216
Epoch 4/10
10/10 - 8s - loss: 673.9897 - loglik: -6.7435e+02 - logprior: 0.3603
Epoch 5/10
10/10 - 8s - loss: 659.4092 - loglik: -6.5959e+02 - logprior: 0.1799
Epoch 6/10
10/10 - 8s - loss: 656.8322 - loglik: -6.5689e+02 - logprior: 0.0558
Epoch 7/10
10/10 - 8s - loss: 654.4246 - loglik: -6.5451e+02 - logprior: 0.0838
Epoch 8/10
10/10 - 8s - loss: 649.8942 - loglik: -6.5007e+02 - logprior: 0.1738
Epoch 9/10
10/10 - 8s - loss: 652.3934 - loglik: -6.5259e+02 - logprior: 0.2003
Fitted a model with MAP estimate = -650.2550
expansions: [(0, 4), (18, 1), (52, 1), (54, 1), (76, 1), (83, 2), (85, 6), (92, 2), (93, 1), (110, 5), (118, 1), (121, 1), (149, 1), (171, 10), (176, 1), (198, 4), (207, 1), (208, 1)]
discards: [217 218]
Fitting a model of length 267 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 673.2911 - loglik: -6.6227e+02 - logprior: -1.1018e+01
Epoch 2/2
20/20 - 11s - loss: 645.7128 - loglik: -6.4537e+02 - logprior: -3.3821e-01
Fitted a model with MAP estimate = -641.5195
expansions: []
discards: [  1   2   3   4   5 132 133 198 199 200 201 202 203 236]
Fitting a model of length 253 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 653.3138 - loglik: -6.4610e+02 - logprior: -7.2145e+00
Epoch 2/2
20/20 - 11s - loss: 643.5167 - loglik: -6.4397e+02 - logprior: 0.4503
Fitted a model with MAP estimate = -639.9784
expansions: [(0, 4), (95, 3)]
discards: [1]
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 656.3386 - loglik: -6.4385e+02 - logprior: -1.2489e+01
Epoch 2/10
20/20 - 11s - loss: 641.5189 - loglik: -6.4116e+02 - logprior: -3.5687e-01
Epoch 3/10
20/20 - 11s - loss: 634.3984 - loglik: -6.3572e+02 - logprior: 1.3229
Epoch 4/10
20/20 - 11s - loss: 635.9362 - loglik: -6.3782e+02 - logprior: 1.8813
Fitted a model with MAP estimate = -632.9926
Time for alignment: 204.8597
Fitting a model of length 225 on 2717 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 777.1491 - loglik: -7.6330e+02 - logprior: -1.3850e+01
Epoch 2/10
10/10 - 8s - loss: 734.6091 - loglik: -7.3270e+02 - logprior: -1.9089e+00
Epoch 3/10
10/10 - 8s - loss: 700.6694 - loglik: -7.0078e+02 - logprior: 0.1097
Epoch 4/10
10/10 - 8s - loss: 674.8333 - loglik: -6.7519e+02 - logprior: 0.3521
Epoch 5/10
10/10 - 8s - loss: 655.3730 - loglik: -6.5552e+02 - logprior: 0.1484
Epoch 6/10
10/10 - 8s - loss: 653.1884 - loglik: -6.5321e+02 - logprior: 0.0242
Epoch 7/10
10/10 - 8s - loss: 648.2039 - loglik: -6.4835e+02 - logprior: 0.1510
Epoch 8/10
10/10 - 8s - loss: 645.9325 - loglik: -6.4616e+02 - logprior: 0.2325
Epoch 9/10
10/10 - 8s - loss: 646.4523 - loglik: -6.4667e+02 - logprior: 0.2166
Fitted a model with MAP estimate = -645.5406
expansions: [(0, 4), (18, 2), (45, 3), (52, 1), (54, 2), (56, 1), (86, 12), (92, 1), (94, 1), (100, 1), (112, 2), (118, 1), (121, 1), (150, 1), (153, 1), (171, 10), (199, 3), (208, 1)]
discards: [217 218]
Fitting a model of length 271 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 15s - loss: 667.0126 - loglik: -6.5597e+02 - logprior: -1.1046e+01
Epoch 2/2
20/20 - 12s - loss: 642.2803 - loglik: -6.4193e+02 - logprior: -3.5520e-01
Fitted a model with MAP estimate = -639.0339
expansions: [(255, 1)]
discards: [  1   2   3   4   5 205 206 207 208 209 210]
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 650.1688 - loglik: -6.4299e+02 - logprior: -7.1753e+00
Epoch 2/2
20/20 - 11s - loss: 638.1593 - loglik: -6.3861e+02 - logprior: 0.4485
Fitted a model with MAP estimate = -636.9868
expansions: [(0, 4)]
discards: []
Fitting a model of length 265 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 651.6312 - loglik: -6.3956e+02 - logprior: -1.2072e+01
Epoch 2/10
20/20 - 11s - loss: 641.2568 - loglik: -6.4094e+02 - logprior: -3.1519e-01
Epoch 3/10
20/20 - 11s - loss: 634.3710 - loglik: -6.3578e+02 - logprior: 1.4062
Epoch 4/10
20/20 - 11s - loss: 630.6591 - loglik: -6.3267e+02 - logprior: 2.0078
Epoch 5/10
20/20 - 11s - loss: 633.0734 - loglik: -6.3530e+02 - logprior: 2.2296
Fitted a model with MAP estimate = -629.8344
Time for alignment: 220.8830
Computed alignments with likelihoods: ['-630.0305', '-632.9926', '-629.8344']
Best model has likelihood: -629.8344
SP score = 0.6196
Training of 3 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb00efd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c0d3d60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.8008 - loglik: -1.3156e+02 - logprior: -3.2395e+00
Epoch 2/10
19/19 - 1s - loss: 115.0736 - loglik: -1.1372e+02 - logprior: -1.3515e+00
Epoch 3/10
19/19 - 1s - loss: 107.7901 - loglik: -1.0634e+02 - logprior: -1.4514e+00
Epoch 4/10
19/19 - 1s - loss: 105.7051 - loglik: -1.0429e+02 - logprior: -1.4115e+00
Epoch 5/10
19/19 - 1s - loss: 104.6974 - loglik: -1.0332e+02 - logprior: -1.3805e+00
Epoch 6/10
19/19 - 1s - loss: 104.7609 - loglik: -1.0340e+02 - logprior: -1.3625e+00
Fitted a model with MAP estimate = -103.1151
expansions: [(10, 2), (12, 3), (13, 1), (16, 1), (20, 2), (22, 2), (28, 3), (30, 2)]
discards: [0]
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 107.5268 - loglik: -1.0330e+02 - logprior: -4.2240e+00
Epoch 2/2
19/19 - 1s - loss: 98.0151 - loglik: -9.5882e+01 - logprior: -2.1331e+00
Fitted a model with MAP estimate = -94.4239
expansions: [(0, 1)]
discards: [ 0  9 21 26 31 40]
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 98.4440 - loglik: -9.5448e+01 - logprior: -2.9962e+00
Epoch 2/2
19/19 - 1s - loss: 94.9981 - loglik: -9.3707e+01 - logprior: -1.2906e+00
Fitted a model with MAP estimate = -92.8404
expansions: []
discards: []
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.9764 - loglik: -9.2007e+01 - logprior: -2.9696e+00
Epoch 2/10
19/19 - 1s - loss: 92.8094 - loglik: -9.1531e+01 - logprior: -1.2780e+00
Epoch 3/10
19/19 - 1s - loss: 92.0096 - loglik: -9.0789e+01 - logprior: -1.2207e+00
Epoch 4/10
19/19 - 1s - loss: 91.6567 - loglik: -9.0476e+01 - logprior: -1.1809e+00
Epoch 5/10
19/19 - 1s - loss: 91.6564 - loglik: -9.0506e+01 - logprior: -1.1501e+00
Epoch 6/10
19/19 - 1s - loss: 91.2086 - loglik: -9.0067e+01 - logprior: -1.1413e+00
Epoch 7/10
19/19 - 1s - loss: 91.2237 - loglik: -9.0097e+01 - logprior: -1.1266e+00
Fitted a model with MAP estimate = -91.1362
Time for alignment: 39.7016
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.7451 - loglik: -1.3151e+02 - logprior: -3.2398e+00
Epoch 2/10
19/19 - 1s - loss: 113.4788 - loglik: -1.1210e+02 - logprior: -1.3752e+00
Epoch 3/10
19/19 - 1s - loss: 106.9815 - loglik: -1.0547e+02 - logprior: -1.5072e+00
Epoch 4/10
19/19 - 1s - loss: 104.7980 - loglik: -1.0337e+02 - logprior: -1.4310e+00
Epoch 5/10
19/19 - 1s - loss: 103.8831 - loglik: -1.0246e+02 - logprior: -1.4203e+00
Epoch 6/10
19/19 - 1s - loss: 102.9032 - loglik: -1.0148e+02 - logprior: -1.4239e+00
Epoch 7/10
19/19 - 1s - loss: 103.0516 - loglik: -1.0163e+02 - logprior: -1.4254e+00
Fitted a model with MAP estimate = -101.3523
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (23, 2), (28, 2), (29, 2), (30, 1)]
discards: [0]
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 105.7179 - loglik: -1.0149e+02 - logprior: -4.2263e+00
Epoch 2/2
19/19 - 1s - loss: 96.6781 - loglik: -9.4568e+01 - logprior: -2.1101e+00
Fitted a model with MAP estimate = -93.7560
expansions: [(0, 1)]
discards: [ 0  9 39]
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 96.8047 - loglik: -9.3736e+01 - logprior: -3.0685e+00
Epoch 2/2
19/19 - 1s - loss: 93.6913 - loglik: -9.2251e+01 - logprior: -1.4398e+00
Fitted a model with MAP estimate = -91.8162
expansions: []
discards: [29]
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.6551 - loglik: -9.1562e+01 - logprior: -3.0927e+00
Epoch 2/10
19/19 - 1s - loss: 92.5389 - loglik: -9.1115e+01 - logprior: -1.4241e+00
Epoch 3/10
19/19 - 1s - loss: 91.8204 - loglik: -9.0499e+01 - logprior: -1.3219e+00
Epoch 4/10
19/19 - 1s - loss: 91.2101 - loglik: -8.9930e+01 - logprior: -1.2796e+00
Epoch 5/10
19/19 - 1s - loss: 90.9625 - loglik: -8.9695e+01 - logprior: -1.2674e+00
Epoch 6/10
19/19 - 1s - loss: 90.5512 - loglik: -8.9299e+01 - logprior: -1.2526e+00
Epoch 7/10
19/19 - 1s - loss: 90.6595 - loglik: -8.9419e+01 - logprior: -1.2410e+00
Fitted a model with MAP estimate = -90.5160
Time for alignment: 39.4507
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 134.7363 - loglik: -1.3150e+02 - logprior: -3.2373e+00
Epoch 2/10
19/19 - 1s - loss: 113.8996 - loglik: -1.1254e+02 - logprior: -1.3583e+00
Epoch 3/10
19/19 - 1s - loss: 107.4994 - loglik: -1.0603e+02 - logprior: -1.4662e+00
Epoch 4/10
19/19 - 1s - loss: 105.9247 - loglik: -1.0453e+02 - logprior: -1.3939e+00
Epoch 5/10
19/19 - 1s - loss: 104.7864 - loglik: -1.0341e+02 - logprior: -1.3777e+00
Epoch 6/10
19/19 - 1s - loss: 104.4264 - loglik: -1.0306e+02 - logprior: -1.3700e+00
Epoch 7/10
19/19 - 1s - loss: 104.1565 - loglik: -1.0279e+02 - logprior: -1.3689e+00
Epoch 8/10
19/19 - 1s - loss: 103.8462 - loglik: -1.0248e+02 - logprior: -1.3615e+00
Epoch 9/10
19/19 - 1s - loss: 103.7921 - loglik: -1.0244e+02 - logprior: -1.3541e+00
Epoch 10/10
19/19 - 1s - loss: 103.8264 - loglik: -1.0248e+02 - logprior: -1.3486e+00
Fitted a model with MAP estimate = -102.2648
expansions: [(10, 2), (12, 3), (13, 3), (23, 2), (28, 2), (29, 2), (30, 1)]
discards: [0]
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.5792 - loglik: -1.0233e+02 - logprior: -4.2516e+00
Epoch 2/2
19/19 - 1s - loss: 97.4353 - loglik: -9.5240e+01 - logprior: -2.1955e+00
Fitted a model with MAP estimate = -94.3213
expansions: [(0, 1)]
discards: [ 0  9 15 40]
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 97.3323 - loglik: -9.4277e+01 - logprior: -3.0553e+00
Epoch 2/2
19/19 - 1s - loss: 93.7354 - loglik: -9.2306e+01 - logprior: -1.4294e+00
Fitted a model with MAP estimate = -91.9020
expansions: []
discards: [29]
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 94.8018 - loglik: -9.1729e+01 - logprior: -3.0733e+00
Epoch 2/10
19/19 - 1s - loss: 92.4249 - loglik: -9.1009e+01 - logprior: -1.4157e+00
Epoch 3/10
19/19 - 1s - loss: 91.7961 - loglik: -9.0480e+01 - logprior: -1.3156e+00
Epoch 4/10
19/19 - 1s - loss: 91.2020 - loglik: -8.9919e+01 - logprior: -1.2835e+00
Epoch 5/10
19/19 - 1s - loss: 90.8466 - loglik: -8.9585e+01 - logprior: -1.2618e+00
Epoch 6/10
19/19 - 1s - loss: 90.7945 - loglik: -8.9550e+01 - logprior: -1.2441e+00
Epoch 7/10
19/19 - 1s - loss: 90.4068 - loglik: -8.9164e+01 - logprior: -1.2426e+00
Epoch 8/10
19/19 - 1s - loss: 90.7369 - loglik: -8.9510e+01 - logprior: -1.2273e+00
Fitted a model with MAP estimate = -90.4607
Time for alignment: 44.3182
Computed alignments with likelihoods: ['-91.1362', '-90.5160', '-90.4607']
Best model has likelihood: -90.4607
SP score = 0.9439
Training of 3 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fad87a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fa3397c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 668.5137 - loglik: -6.4353e+02 - logprior: -2.4986e+01
Epoch 2/10
10/10 - 8s - loss: 607.4368 - loglik: -6.0386e+02 - logprior: -3.5811e+00
Epoch 3/10
10/10 - 8s - loss: 556.4166 - loglik: -5.5662e+02 - logprior: 0.2070
Epoch 4/10
10/10 - 8s - loss: 513.8907 - loglik: -5.1420e+02 - logprior: 0.3075
Epoch 5/10
10/10 - 8s - loss: 489.1586 - loglik: -4.8899e+02 - logprior: -1.6605e-01
Epoch 6/10
10/10 - 8s - loss: 477.4771 - loglik: -4.7692e+02 - logprior: -5.5764e-01
Epoch 7/10
10/10 - 8s - loss: 475.0840 - loglik: -4.7419e+02 - logprior: -8.9395e-01
Epoch 8/10
10/10 - 8s - loss: 470.3787 - loglik: -4.6966e+02 - logprior: -7.1933e-01
Epoch 9/10
10/10 - 8s - loss: 468.3685 - loglik: -4.6771e+02 - logprior: -6.5429e-01
Epoch 10/10
10/10 - 8s - loss: 469.9735 - loglik: -4.6947e+02 - logprior: -5.0331e-01
Fitted a model with MAP estimate = -468.0992
expansions: [(19, 1), (25, 4), (38, 1), (39, 1), (41, 1), (48, 1), (50, 2), (51, 1), (53, 2), (54, 2), (58, 1), (68, 4), (69, 2), (70, 1), (71, 2), (72, 1), (73, 1), (75, 1), (76, 1), (94, 1), (100, 4), (102, 2), (104, 2), (131, 2), (132, 1), (133, 1), (134, 2), (135, 3), (137, 1), (139, 1), (156, 1), (157, 3), (158, 2), (159, 2), (174, 8), (198, 1)]
discards: [  1   2   6 200 223]
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 476.9317 - loglik: -4.6023e+02 - logprior: -1.6697e+01
Epoch 2/2
15/15 - 9s - loss: 438.4135 - loglik: -4.3858e+02 - logprior: 0.1669
Fitted a model with MAP estimate = -434.5155
expansions: [(0, 3), (62, 1)]
discards: [ 23  87 130 138 177 209 235 236 237 238 239]
Fitting a model of length 280 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 463.5436 - loglik: -4.3878e+02 - logprior: -2.4768e+01
Epoch 2/2
15/15 - 9s - loss: 435.5816 - loglik: -4.3344e+02 - logprior: -2.1466e+00
Fitted a model with MAP estimate = -431.7506
expansions: [(28, 1), (205, 2)]
discards: [  0   1   2 167]
Fitting a model of length 279 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 14s - loss: 459.6509 - loglik: -4.3574e+02 - logprior: -2.3906e+01
Epoch 2/10
10/10 - 11s - loss: 433.3129 - loglik: -4.3137e+02 - logprior: -1.9474e+00
Epoch 3/10
10/10 - 11s - loss: 430.6134 - loglik: -4.3335e+02 - logprior: 2.7396
Epoch 4/10
10/10 - 11s - loss: 427.2986 - loglik: -4.3177e+02 - logprior: 4.4722
Epoch 5/10
10/10 - 11s - loss: 426.6097 - loglik: -4.3187e+02 - logprior: 5.2571
Epoch 6/10
10/10 - 11s - loss: 425.7350 - loglik: -4.3139e+02 - logprior: 5.6541
Epoch 7/10
10/10 - 11s - loss: 424.3992 - loglik: -4.3029e+02 - logprior: 5.8920
Epoch 8/10
10/10 - 11s - loss: 424.9894 - loglik: -4.3117e+02 - logprior: 6.1816
Fitted a model with MAP estimate = -424.3629
Time for alignment: 228.4894
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 669.2477 - loglik: -6.4425e+02 - logprior: -2.5001e+01
Epoch 2/10
10/10 - 8s - loss: 606.1079 - loglik: -6.0250e+02 - logprior: -3.6103e+00
Epoch 3/10
10/10 - 8s - loss: 557.7811 - loglik: -5.5803e+02 - logprior: 0.2478
Epoch 4/10
10/10 - 8s - loss: 516.2334 - loglik: -5.1659e+02 - logprior: 0.3525
Epoch 5/10
10/10 - 8s - loss: 492.6713 - loglik: -4.9257e+02 - logprior: -1.0222e-01
Epoch 6/10
10/10 - 8s - loss: 480.7086 - loglik: -4.8021e+02 - logprior: -4.9617e-01
Epoch 7/10
10/10 - 8s - loss: 471.8997 - loglik: -4.7116e+02 - logprior: -7.4426e-01
Epoch 8/10
10/10 - 8s - loss: 471.0524 - loglik: -4.7047e+02 - logprior: -5.8059e-01
Epoch 9/10
10/10 - 8s - loss: 467.3363 - loglik: -4.6669e+02 - logprior: -6.5114e-01
Epoch 10/10
10/10 - 8s - loss: 467.7511 - loglik: -4.6733e+02 - logprior: -4.1963e-01
Fitted a model with MAP estimate = -467.4558
expansions: [(25, 4), (39, 1), (42, 1), (52, 1), (54, 1), (56, 2), (58, 2), (61, 1), (69, 1), (70, 5), (71, 3), (72, 2), (73, 1), (74, 1), (76, 1), (78, 1), (86, 1), (101, 1), (102, 5), (109, 1), (131, 2), (132, 1), (133, 1), (134, 2), (135, 3), (137, 1), (157, 5), (158, 3), (166, 1), (172, 8), (182, 1), (183, 2)]
discards: [  1   2 204 205 223]
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 473.4889 - loglik: -4.5714e+02 - logprior: -1.6353e+01
Epoch 2/2
15/15 - 9s - loss: 438.1935 - loglik: -4.3860e+02 - logprior: 0.4061
Fitted a model with MAP estimate = -432.0373
expansions: [(67, 1), (199, 1), (200, 1), (244, 1), (246, 1), (268, 1)]
discards: [ 23  86 175 231 232 233]
Fitting a model of length 286 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 450.2536 - loglik: -4.3507e+02 - logprior: -1.5186e+01
Epoch 2/2
15/15 - 9s - loss: 427.2948 - loglik: -4.2873e+02 - logprior: 1.4371
Fitted a model with MAP estimate = -426.1252
expansions: [(0, 3), (25, 1), (203, 1)]
discards: [164 198]
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 454.1527 - loglik: -4.3062e+02 - logprior: -2.3533e+01
Epoch 2/10
15/15 - 9s - loss: 428.7524 - loglik: -4.2777e+02 - logprior: -9.8523e-01
Epoch 3/10
15/15 - 9s - loss: 424.7549 - loglik: -4.2890e+02 - logprior: 4.1446
Epoch 4/10
15/15 - 9s - loss: 421.2707 - loglik: -4.2706e+02 - logprior: 5.7854
Epoch 5/10
15/15 - 9s - loss: 419.4933 - loglik: -4.2586e+02 - logprior: 6.3683
Epoch 6/10
15/15 - 9s - loss: 421.2157 - loglik: -4.2797e+02 - logprior: 6.7519
Fitted a model with MAP estimate = -419.4549
Time for alignment: 198.0251
Fitting a model of length 225 on 1502 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 666.9801 - loglik: -6.4199e+02 - logprior: -2.4993e+01
Epoch 2/10
10/10 - 8s - loss: 607.6963 - loglik: -6.0409e+02 - logprior: -3.6100e+00
Epoch 3/10
10/10 - 8s - loss: 552.8156 - loglik: -5.5297e+02 - logprior: 0.1559
Epoch 4/10
10/10 - 8s - loss: 509.0748 - loglik: -5.0922e+02 - logprior: 0.1483
Epoch 5/10
10/10 - 8s - loss: 485.6846 - loglik: -4.8543e+02 - logprior: -2.5204e-01
Epoch 6/10
10/10 - 8s - loss: 474.2254 - loglik: -4.7358e+02 - logprior: -6.4579e-01
Epoch 7/10
10/10 - 8s - loss: 471.4364 - loglik: -4.7060e+02 - logprior: -8.3759e-01
Epoch 8/10
10/10 - 8s - loss: 467.4019 - loglik: -4.6672e+02 - logprior: -6.8133e-01
Epoch 9/10
10/10 - 8s - loss: 466.2381 - loglik: -4.6550e+02 - logprior: -7.3631e-01
Epoch 10/10
10/10 - 8s - loss: 466.7817 - loglik: -4.6621e+02 - logprior: -5.7069e-01
Fitted a model with MAP estimate = -465.8704
expansions: [(24, 3), (25, 1), (26, 1), (40, 1), (50, 1), (53, 1), (54, 1), (55, 1), (56, 1), (59, 1), (68, 1), (69, 1), (70, 1), (71, 3), (72, 5), (73, 2), (74, 1), (77, 2), (79, 1), (101, 1), (102, 1), (103, 3), (105, 2), (132, 2), (133, 1), (134, 1), (135, 2), (136, 3), (138, 1), (140, 1), (157, 1), (158, 5), (159, 2), (174, 8)]
discards: [  1   2   6 223]
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 473.6869 - loglik: -4.5715e+02 - logprior: -1.6536e+01
Epoch 2/2
15/15 - 9s - loss: 436.0130 - loglik: -4.3622e+02 - logprior: 0.2117
Fitted a model with MAP estimate = -433.3372
expansions: [(0, 3), (93, 1), (121, 1)]
discards: [ 21  84 136 175 232 233 234 235]
Fitting a model of length 281 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 11s - loss: 462.8642 - loglik: -4.3821e+02 - logprior: -2.4652e+01
Epoch 2/2
15/15 - 9s - loss: 435.2581 - loglik: -4.3326e+02 - logprior: -2.0019e+00
Fitted a model with MAP estimate = -431.5847
expansions: [(205, 1), (239, 3)]
discards: [  0   1   2 167]
Fitting a model of length 281 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 452.1264 - loglik: -4.3493e+02 - logprior: -1.7198e+01
Epoch 2/10
15/15 - 9s - loss: 431.5963 - loglik: -4.3348e+02 - logprior: 1.8842
Epoch 3/10
15/15 - 9s - loss: 427.7022 - loglik: -4.3249e+02 - logprior: 4.7901
Epoch 4/10
15/15 - 9s - loss: 423.6783 - loglik: -4.2937e+02 - logprior: 5.6891
Epoch 5/10
15/15 - 9s - loss: 425.8714 - loglik: -4.3203e+02 - logprior: 6.1566
Fitted a model with MAP estimate = -423.6458
Time for alignment: 187.1617
Computed alignments with likelihoods: ['-424.3629', '-419.4549', '-423.6458']
Best model has likelihood: -419.4549
SP score = 0.9292
Training of 3 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb79aca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2884baeb0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 226.6986 - loglik: -1.8504e+02 - logprior: -4.1662e+01
Epoch 2/10
10/10 - 1s - loss: 180.2261 - loglik: -1.6904e+02 - logprior: -1.1187e+01
Epoch 3/10
10/10 - 1s - loss: 161.1476 - loglik: -1.5565e+02 - logprior: -5.4953e+00
Epoch 4/10
10/10 - 1s - loss: 151.8552 - loglik: -1.4837e+02 - logprior: -3.4875e+00
Epoch 5/10
10/10 - 1s - loss: 148.1012 - loglik: -1.4562e+02 - logprior: -2.4766e+00
Epoch 6/10
10/10 - 1s - loss: 146.1945 - loglik: -1.4423e+02 - logprior: -1.9601e+00
Epoch 7/10
10/10 - 1s - loss: 144.9949 - loglik: -1.4327e+02 - logprior: -1.7266e+00
Epoch 8/10
10/10 - 1s - loss: 144.4372 - loglik: -1.4292e+02 - logprior: -1.5188e+00
Epoch 9/10
10/10 - 1s - loss: 143.9480 - loglik: -1.4262e+02 - logprior: -1.3264e+00
Epoch 10/10
10/10 - 1s - loss: 143.8447 - loglik: -1.4268e+02 - logprior: -1.1662e+00
Fitted a model with MAP estimate = -143.7582
expansions: [(0, 2), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (37, 1), (43, 3)]
discards: []
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 194.7080 - loglik: -1.3981e+02 - logprior: -5.4896e+01
Epoch 2/2
10/10 - 1s - loss: 151.3655 - loglik: -1.3470e+02 - logprior: -1.6664e+01
Fitted a model with MAP estimate = -143.7051
expansions: [(3, 1), (18, 1)]
discards: [ 0 21]
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 181.8600 - loglik: -1.3456e+02 - logprior: -4.7301e+01
Epoch 2/2
10/10 - 1s - loss: 152.4998 - loglik: -1.3423e+02 - logprior: -1.8267e+01
Fitted a model with MAP estimate = -147.4254
expansions: []
discards: [0]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 176.1287 - loglik: -1.3370e+02 - logprior: -4.2431e+01
Epoch 2/10
10/10 - 1s - loss: 144.4946 - loglik: -1.3305e+02 - logprior: -1.1441e+01
Epoch 3/10
10/10 - 1s - loss: 137.4774 - loglik: -1.3309e+02 - logprior: -4.3886e+00
Epoch 4/10
10/10 - 1s - loss: 134.8806 - loglik: -1.3301e+02 - logprior: -1.8693e+00
Epoch 5/10
10/10 - 1s - loss: 133.4643 - loglik: -1.3278e+02 - logprior: -6.8548e-01
Epoch 6/10
10/10 - 1s - loss: 132.7230 - loglik: -1.3258e+02 - logprior: -1.4174e-01
Epoch 7/10
10/10 - 1s - loss: 132.5060 - loglik: -1.3264e+02 - logprior: 0.1343
Epoch 8/10
10/10 - 1s - loss: 132.1148 - loglik: -1.3254e+02 - logprior: 0.4280
Epoch 9/10
10/10 - 1s - loss: 132.0989 - loglik: -1.3283e+02 - logprior: 0.7322
Epoch 10/10
10/10 - 1s - loss: 131.7845 - loglik: -1.3272e+02 - logprior: 0.9359
Fitted a model with MAP estimate = -131.7818
Time for alignment: 28.8387
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.7493 - loglik: -1.8509e+02 - logprior: -4.1664e+01
Epoch 2/10
10/10 - 1s - loss: 180.3606 - loglik: -1.6918e+02 - logprior: -1.1179e+01
Epoch 3/10
10/10 - 1s - loss: 161.8799 - loglik: -1.5641e+02 - logprior: -5.4744e+00
Epoch 4/10
10/10 - 1s - loss: 152.2316 - loglik: -1.4875e+02 - logprior: -3.4770e+00
Epoch 5/10
10/10 - 1s - loss: 148.3295 - loglik: -1.4588e+02 - logprior: -2.4541e+00
Epoch 6/10
10/10 - 1s - loss: 146.4154 - loglik: -1.4445e+02 - logprior: -1.9638e+00
Epoch 7/10
10/10 - 1s - loss: 145.4010 - loglik: -1.4366e+02 - logprior: -1.7460e+00
Epoch 8/10
10/10 - 1s - loss: 144.8242 - loglik: -1.4327e+02 - logprior: -1.5547e+00
Epoch 9/10
10/10 - 1s - loss: 144.1116 - loglik: -1.4277e+02 - logprior: -1.3402e+00
Epoch 10/10
10/10 - 1s - loss: 144.0562 - loglik: -1.4288e+02 - logprior: -1.1743e+00
Fitted a model with MAP estimate = -143.8860
expansions: [(0, 2), (17, 1), (18, 3), (20, 1), (21, 1), (24, 1), (38, 1), (43, 3)]
discards: []
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 194.8837 - loglik: -1.3997e+02 - logprior: -5.4913e+01
Epoch 2/2
10/10 - 1s - loss: 152.3463 - loglik: -1.3569e+02 - logprior: -1.6661e+01
Fitted a model with MAP estimate = -144.6582
expansions: [(3, 1)]
discards: [ 0 23]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 182.7896 - loglik: -1.3538e+02 - logprior: -4.7414e+01
Epoch 2/2
10/10 - 1s - loss: 153.3835 - loglik: -1.3505e+02 - logprior: -1.8337e+01
Fitted a model with MAP estimate = -148.1890
expansions: [(18, 1)]
discards: [0]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 176.8807 - loglik: -1.3443e+02 - logprior: -4.2455e+01
Epoch 2/10
10/10 - 1s - loss: 145.1421 - loglik: -1.3366e+02 - logprior: -1.1487e+01
Epoch 3/10
10/10 - 1s - loss: 138.1207 - loglik: -1.3368e+02 - logprior: -4.4422e+00
Epoch 4/10
10/10 - 1s - loss: 135.4623 - loglik: -1.3353e+02 - logprior: -1.9367e+00
Epoch 5/10
10/10 - 1s - loss: 133.9975 - loglik: -1.3326e+02 - logprior: -7.3386e-01
Epoch 6/10
10/10 - 1s - loss: 133.3054 - loglik: -1.3313e+02 - logprior: -1.7781e-01
Epoch 7/10
10/10 - 1s - loss: 132.7807 - loglik: -1.3288e+02 - logprior: 0.1018
Epoch 8/10
10/10 - 1s - loss: 132.5807 - loglik: -1.3298e+02 - logprior: 0.3955
Epoch 9/10
10/10 - 1s - loss: 132.3053 - loglik: -1.3300e+02 - logprior: 0.6944
Epoch 10/10
10/10 - 1s - loss: 132.3298 - loglik: -1.3323e+02 - logprior: 0.8959
Fitted a model with MAP estimate = -132.0978
Time for alignment: 28.2413
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 226.8085 - loglik: -1.8515e+02 - logprior: -4.1661e+01
Epoch 2/10
10/10 - 1s - loss: 180.0595 - loglik: -1.6888e+02 - logprior: -1.1184e+01
Epoch 3/10
10/10 - 1s - loss: 161.1240 - loglik: -1.5564e+02 - logprior: -5.4862e+00
Epoch 4/10
10/10 - 1s - loss: 151.8429 - loglik: -1.4842e+02 - logprior: -3.4237e+00
Epoch 5/10
10/10 - 1s - loss: 148.1628 - loglik: -1.4579e+02 - logprior: -2.3759e+00
Epoch 6/10
10/10 - 1s - loss: 146.3056 - loglik: -1.4438e+02 - logprior: -1.9256e+00
Epoch 7/10
10/10 - 1s - loss: 145.3083 - loglik: -1.4359e+02 - logprior: -1.7199e+00
Epoch 8/10
10/10 - 1s - loss: 144.5114 - loglik: -1.4299e+02 - logprior: -1.5208e+00
Epoch 9/10
10/10 - 1s - loss: 144.1622 - loglik: -1.4285e+02 - logprior: -1.3116e+00
Epoch 10/10
10/10 - 1s - loss: 143.8409 - loglik: -1.4268e+02 - logprior: -1.1599e+00
Fitted a model with MAP estimate = -143.6887
expansions: [(0, 2), (1, 1), (17, 1), (18, 3), (20, 1), (21, 2), (24, 1), (35, 1), (43, 3)]
discards: []
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 194.7406 - loglik: -1.4001e+02 - logprior: -5.4729e+01
Epoch 2/2
10/10 - 1s - loss: 151.9415 - loglik: -1.3529e+02 - logprior: -1.6648e+01
Fitted a model with MAP estimate = -144.3146
expansions: [(19, 1)]
discards: [ 0 22 24 29]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 183.2259 - loglik: -1.3585e+02 - logprior: -4.7375e+01
Epoch 2/2
10/10 - 1s - loss: 153.5969 - loglik: -1.3524e+02 - logprior: -1.8355e+01
Fitted a model with MAP estimate = -148.6827
expansions: [(23, 1)]
discards: [0]
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 177.1506 - loglik: -1.3462e+02 - logprior: -4.2534e+01
Epoch 2/10
10/10 - 1s - loss: 144.9227 - loglik: -1.3345e+02 - logprior: -1.1471e+01
Epoch 3/10
10/10 - 1s - loss: 137.6535 - loglik: -1.3328e+02 - logprior: -4.3745e+00
Epoch 4/10
10/10 - 1s - loss: 134.8862 - loglik: -1.3302e+02 - logprior: -1.8619e+00
Epoch 5/10
10/10 - 1s - loss: 133.4232 - loglik: -1.3277e+02 - logprior: -6.5672e-01
Epoch 6/10
10/10 - 1s - loss: 132.7181 - loglik: -1.3261e+02 - logprior: -1.1004e-01
Epoch 7/10
10/10 - 1s - loss: 132.1698 - loglik: -1.3232e+02 - logprior: 0.1479
Epoch 8/10
10/10 - 1s - loss: 132.1452 - loglik: -1.3257e+02 - logprior: 0.4208
Epoch 9/10
10/10 - 1s - loss: 131.9962 - loglik: -1.3271e+02 - logprior: 0.7149
Epoch 10/10
10/10 - 1s - loss: 131.8765 - loglik: -1.3281e+02 - logprior: 0.9309
Fitted a model with MAP estimate = -131.6763
Time for alignment: 29.7435
Computed alignments with likelihoods: ['-131.7818', '-132.0978', '-131.6763']
Best model has likelihood: -131.6763
SP score = 0.9039
Training of 3 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe28826c940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2c3e792b0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 251.1161 - loglik: -2.3792e+02 - logprior: -1.3197e+01
Epoch 2/10
11/11 - 1s - loss: 220.6292 - loglik: -2.1720e+02 - logprior: -3.4329e+00
Epoch 3/10
11/11 - 1s - loss: 200.2032 - loglik: -1.9806e+02 - logprior: -2.1450e+00
Epoch 4/10
11/11 - 1s - loss: 190.4613 - loglik: -1.8844e+02 - logprior: -2.0243e+00
Epoch 5/10
11/11 - 1s - loss: 188.0801 - loglik: -1.8607e+02 - logprior: -2.0075e+00
Epoch 6/10
11/11 - 1s - loss: 185.9142 - loglik: -1.8406e+02 - logprior: -1.8580e+00
Epoch 7/10
11/11 - 1s - loss: 186.3205 - loglik: -1.8461e+02 - logprior: -1.7126e+00
Fitted a model with MAP estimate = -185.5374
expansions: [(9, 1), (10, 3), (11, 1), (15, 1), (24, 1), (25, 1), (32, 1), (34, 1), (35, 1), (45, 1), (49, 1), (50, 3)]
discards: [0]
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 199.2450 - loglik: -1.8426e+02 - logprior: -1.4982e+01
Epoch 2/2
11/11 - 1s - loss: 184.8230 - loglik: -1.7856e+02 - logprior: -6.2637e+00
Fitted a model with MAP estimate = -182.1176
expansions: [(0, 2)]
discards: [ 0 14]
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 188.0112 - loglik: -1.7621e+02 - logprior: -1.1800e+01
Epoch 2/2
11/11 - 1s - loss: 178.8986 - loglik: -1.7580e+02 - logprior: -3.0973e+00
Fitted a model with MAP estimate = -177.1480
expansions: []
discards: [0]
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 191.0908 - loglik: -1.7695e+02 - logprior: -1.4137e+01
Epoch 2/10
11/11 - 1s - loss: 180.8912 - loglik: -1.7678e+02 - logprior: -4.1093e+00
Epoch 3/10
11/11 - 1s - loss: 177.2838 - loglik: -1.7515e+02 - logprior: -2.1331e+00
Epoch 4/10
11/11 - 1s - loss: 176.1363 - loglik: -1.7471e+02 - logprior: -1.4253e+00
Epoch 5/10
11/11 - 1s - loss: 176.3620 - loglik: -1.7534e+02 - logprior: -1.0248e+00
Fitted a model with MAP estimate = -175.3545
Time for alignment: 39.0610
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 251.1486 - loglik: -2.3795e+02 - logprior: -1.3196e+01
Epoch 2/10
11/11 - 1s - loss: 220.3907 - loglik: -2.1697e+02 - logprior: -3.4244e+00
Epoch 3/10
11/11 - 1s - loss: 199.2892 - loglik: -1.9716e+02 - logprior: -2.1266e+00
Epoch 4/10
11/11 - 1s - loss: 191.0202 - loglik: -1.8901e+02 - logprior: -2.0144e+00
Epoch 5/10
11/11 - 1s - loss: 186.7622 - loglik: -1.8480e+02 - logprior: -1.9573e+00
Epoch 6/10
11/11 - 1s - loss: 185.4835 - loglik: -1.8369e+02 - logprior: -1.7972e+00
Epoch 7/10
11/11 - 1s - loss: 186.1284 - loglik: -1.8448e+02 - logprior: -1.6444e+00
Fitted a model with MAP estimate = -185.0621
expansions: [(11, 4), (17, 1), (24, 1), (25, 1), (32, 1), (35, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 198.0480 - loglik: -1.8312e+02 - logprior: -1.4925e+01
Epoch 2/2
11/11 - 1s - loss: 183.6688 - loglik: -1.7749e+02 - logprior: -6.1813e+00
Fitted a model with MAP estimate = -181.4790
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 187.2927 - loglik: -1.7551e+02 - logprior: -1.1778e+01
Epoch 2/2
11/11 - 1s - loss: 178.8148 - loglik: -1.7573e+02 - logprior: -3.0890e+00
Fitted a model with MAP estimate = -176.9300
expansions: []
discards: [0]
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 191.0087 - loglik: -1.7680e+02 - logprior: -1.4212e+01
Epoch 2/10
11/11 - 1s - loss: 180.7260 - loglik: -1.7652e+02 - logprior: -4.2043e+00
Epoch 3/10
11/11 - 1s - loss: 178.1294 - loglik: -1.7599e+02 - logprior: -2.1351e+00
Epoch 4/10
11/11 - 1s - loss: 175.9308 - loglik: -1.7450e+02 - logprior: -1.4279e+00
Epoch 5/10
11/11 - 1s - loss: 175.6004 - loglik: -1.7459e+02 - logprior: -1.0095e+00
Epoch 6/10
11/11 - 1s - loss: 175.0047 - loglik: -1.7408e+02 - logprior: -9.2688e-01
Epoch 7/10
11/11 - 1s - loss: 174.8713 - loglik: -1.7405e+02 - logprior: -8.1913e-01
Epoch 8/10
11/11 - 1s - loss: 175.0024 - loglik: -1.7422e+02 - logprior: -7.8213e-01
Fitted a model with MAP estimate = -174.6548
Time for alignment: 42.9052
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 250.9049 - loglik: -2.3771e+02 - logprior: -1.3196e+01
Epoch 2/10
11/11 - 1s - loss: 221.3696 - loglik: -2.1794e+02 - logprior: -3.4342e+00
Epoch 3/10
11/11 - 1s - loss: 199.0728 - loglik: -1.9691e+02 - logprior: -2.1612e+00
Epoch 4/10
11/11 - 1s - loss: 191.0069 - loglik: -1.8893e+02 - logprior: -2.0762e+00
Epoch 5/10
11/11 - 1s - loss: 187.0190 - loglik: -1.8498e+02 - logprior: -2.0346e+00
Epoch 6/10
11/11 - 1s - loss: 185.2281 - loglik: -1.8337e+02 - logprior: -1.8540e+00
Epoch 7/10
11/11 - 1s - loss: 185.6318 - loglik: -1.8393e+02 - logprior: -1.7020e+00
Fitted a model with MAP estimate = -185.0445
expansions: [(8, 2), (10, 2), (17, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 1), (45, 1), (49, 1), (50, 3)]
discards: [0]
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 198.3252 - loglik: -1.8337e+02 - logprior: -1.4951e+01
Epoch 2/2
11/11 - 1s - loss: 183.9790 - loglik: -1.7780e+02 - logprior: -6.1752e+00
Fitted a model with MAP estimate = -181.6099
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 187.9033 - loglik: -1.7612e+02 - logprior: -1.1779e+01
Epoch 2/2
11/11 - 1s - loss: 177.3550 - loglik: -1.7429e+02 - logprior: -3.0671e+00
Fitted a model with MAP estimate = -176.8307
expansions: []
discards: [0]
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 190.7280 - loglik: -1.7656e+02 - logprior: -1.4170e+01
Epoch 2/10
11/11 - 1s - loss: 180.9954 - loglik: -1.7686e+02 - logprior: -4.1373e+00
Epoch 3/10
11/11 - 1s - loss: 177.4829 - loglik: -1.7536e+02 - logprior: -2.1180e+00
Epoch 4/10
11/11 - 1s - loss: 175.6916 - loglik: -1.7428e+02 - logprior: -1.4111e+00
Epoch 5/10
11/11 - 1s - loss: 175.8463 - loglik: -1.7483e+02 - logprior: -1.0121e+00
Fitted a model with MAP estimate = -175.2686
Time for alignment: 38.4956
Computed alignments with likelihoods: ['-175.3545', '-174.6548', '-175.2686']
Best model has likelihood: -174.6548
SP score = 0.8343
Training of 3 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb1f9e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe220c15820>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 264.3522 - loglik: -2.2371e+02 - logprior: -4.0645e+01
Epoch 2/10
10/10 - 1s - loss: 208.5340 - loglik: -1.9778e+02 - logprior: -1.0758e+01
Epoch 3/10
10/10 - 1s - loss: 177.6474 - loglik: -1.7221e+02 - logprior: -5.4418e+00
Epoch 4/10
10/10 - 1s - loss: 156.0094 - loglik: -1.5219e+02 - logprior: -3.8220e+00
Epoch 5/10
10/10 - 1s - loss: 147.6173 - loglik: -1.4449e+02 - logprior: -3.1276e+00
Epoch 6/10
10/10 - 1s - loss: 144.8297 - loglik: -1.4219e+02 - logprior: -2.6421e+00
Epoch 7/10
10/10 - 1s - loss: 143.3636 - loglik: -1.4108e+02 - logprior: -2.2867e+00
Epoch 8/10
10/10 - 1s - loss: 142.7847 - loglik: -1.4070e+02 - logprior: -2.0858e+00
Epoch 9/10
10/10 - 1s - loss: 142.1285 - loglik: -1.4017e+02 - logprior: -1.9571e+00
Epoch 10/10
10/10 - 1s - loss: 142.0041 - loglik: -1.4013e+02 - logprior: -1.8769e+00
Fitted a model with MAP estimate = -141.8246
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 1), (32, 2), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 169.8924 - loglik: -1.3313e+02 - logprior: -3.6760e+01
Epoch 2/2
10/10 - 1s - loss: 136.5407 - loglik: -1.2700e+02 - logprior: -9.5405e+00
Fitted a model with MAP estimate = -132.1531
expansions: []
discards: [ 0 46 59 66]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 176.5892 - loglik: -1.3143e+02 - logprior: -4.5160e+01
Epoch 2/2
10/10 - 1s - loss: 149.1957 - loglik: -1.3076e+02 - logprior: -1.8437e+01
Fitted a model with MAP estimate = -145.1256
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 164.5742 - loglik: -1.2834e+02 - logprior: -3.6239e+01
Epoch 2/10
10/10 - 1s - loss: 136.0956 - loglik: -1.2710e+02 - logprior: -8.9957e+00
Epoch 3/10
10/10 - 1s - loss: 129.8626 - loglik: -1.2638e+02 - logprior: -3.4784e+00
Epoch 4/10
10/10 - 1s - loss: 127.8553 - loglik: -1.2659e+02 - logprior: -1.2681e+00
Epoch 5/10
10/10 - 1s - loss: 127.3125 - loglik: -1.2719e+02 - logprior: -1.2261e-01
Epoch 6/10
10/10 - 1s - loss: 126.0072 - loglik: -1.2650e+02 - logprior: 0.4932
Epoch 7/10
10/10 - 1s - loss: 126.2133 - loglik: -1.2705e+02 - logprior: 0.8415
Fitted a model with MAP estimate = -125.9662
Time for alignment: 29.8307
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 264.3843 - loglik: -2.2374e+02 - logprior: -4.0644e+01
Epoch 2/10
10/10 - 1s - loss: 208.2500 - loglik: -1.9749e+02 - logprior: -1.0757e+01
Epoch 3/10
10/10 - 1s - loss: 177.1482 - loglik: -1.7174e+02 - logprior: -5.4129e+00
Epoch 4/10
10/10 - 1s - loss: 155.6628 - loglik: -1.5196e+02 - logprior: -3.6986e+00
Epoch 5/10
10/10 - 1s - loss: 148.2459 - loglik: -1.4531e+02 - logprior: -2.9358e+00
Epoch 6/10
10/10 - 1s - loss: 145.5228 - loglik: -1.4302e+02 - logprior: -2.5070e+00
Epoch 7/10
10/10 - 1s - loss: 144.3204 - loglik: -1.4213e+02 - logprior: -2.1935e+00
Epoch 8/10
10/10 - 1s - loss: 143.7995 - loglik: -1.4180e+02 - logprior: -2.0010e+00
Epoch 9/10
10/10 - 1s - loss: 143.3011 - loglik: -1.4145e+02 - logprior: -1.8512e+00
Epoch 10/10
10/10 - 1s - loss: 142.9471 - loglik: -1.4116e+02 - logprior: -1.7850e+00
Fitted a model with MAP estimate = -142.7399
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Fitting a model of length 82 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 170.5700 - loglik: -1.3380e+02 - logprior: -3.6771e+01
Epoch 2/2
10/10 - 1s - loss: 136.9565 - loglik: -1.2740e+02 - logprior: -9.5581e+00
Fitted a model with MAP estimate = -132.2364
expansions: []
discards: [ 0 46 59 66]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 176.7537 - loglik: -1.3160e+02 - logprior: -4.5150e+01
Epoch 2/2
10/10 - 1s - loss: 149.0604 - loglik: -1.3062e+02 - logprior: -1.8436e+01
Fitted a model with MAP estimate = -145.0975
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 164.4449 - loglik: -1.2830e+02 - logprior: -3.6147e+01
Epoch 2/10
10/10 - 1s - loss: 135.8617 - loglik: -1.2689e+02 - logprior: -8.9758e+00
Epoch 3/10
10/10 - 1s - loss: 129.9282 - loglik: -1.2647e+02 - logprior: -3.4611e+00
Epoch 4/10
10/10 - 1s - loss: 127.9649 - loglik: -1.2670e+02 - logprior: -1.2662e+00
Epoch 5/10
10/10 - 1s - loss: 126.9583 - loglik: -1.2684e+02 - logprior: -1.2172e-01
Epoch 6/10
10/10 - 1s - loss: 126.3139 - loglik: -1.2680e+02 - logprior: 0.4879
Epoch 7/10
10/10 - 1s - loss: 126.0277 - loglik: -1.2686e+02 - logprior: 0.8310
Epoch 8/10
10/10 - 1s - loss: 126.0152 - loglik: -1.2710e+02 - logprior: 1.0804
Epoch 9/10
10/10 - 1s - loss: 125.8744 - loglik: -1.2716e+02 - logprior: 1.2841
Epoch 10/10
10/10 - 1s - loss: 125.5624 - loglik: -1.2702e+02 - logprior: 1.4587
Fitted a model with MAP estimate = -125.5404
Time for alignment: 31.8925
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 264.4415 - loglik: -2.2380e+02 - logprior: -4.0645e+01
Epoch 2/10
10/10 - 1s - loss: 208.5688 - loglik: -1.9781e+02 - logprior: -1.0763e+01
Epoch 3/10
10/10 - 1s - loss: 178.6615 - loglik: -1.7317e+02 - logprior: -5.4923e+00
Epoch 4/10
10/10 - 1s - loss: 156.5750 - loglik: -1.5261e+02 - logprior: -3.9608e+00
Epoch 5/10
10/10 - 1s - loss: 147.6353 - loglik: -1.4427e+02 - logprior: -3.3604e+00
Epoch 6/10
10/10 - 1s - loss: 144.1734 - loglik: -1.4123e+02 - logprior: -2.9458e+00
Epoch 7/10
10/10 - 1s - loss: 142.5170 - loglik: -1.3991e+02 - logprior: -2.6046e+00
Epoch 8/10
10/10 - 1s - loss: 141.5517 - loglik: -1.3918e+02 - logprior: -2.3688e+00
Epoch 9/10
10/10 - 1s - loss: 141.1605 - loglik: -1.3896e+02 - logprior: -2.1989e+00
Epoch 10/10
10/10 - 1s - loss: 140.7035 - loglik: -1.3862e+02 - logprior: -2.0840e+00
Fitted a model with MAP estimate = -140.7443
expansions: [(2, 1), (5, 1), (10, 1), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 1), (32, 2), (34, 2), (44, 2), (50, 2), (52, 2)]
discards: []
Fitting a model of length 81 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 170.8521 - loglik: -1.3401e+02 - logprior: -3.6838e+01
Epoch 2/2
10/10 - 1s - loss: 137.9873 - loglik: -1.2826e+02 - logprior: -9.7270e+00
Fitted a model with MAP estimate = -133.1484
expansions: []
discards: [45 58 65]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 163.1905 - loglik: -1.2730e+02 - logprior: -3.5892e+01
Epoch 2/2
10/10 - 1s - loss: 135.4235 - loglik: -1.2615e+02 - logprior: -9.2733e+00
Fitted a model with MAP estimate = -131.9035
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 175.8448 - loglik: -1.3090e+02 - logprior: -4.4948e+01
Epoch 2/10
10/10 - 1s - loss: 148.2281 - loglik: -1.2994e+02 - logprior: -1.8291e+01
Epoch 3/10
10/10 - 1s - loss: 142.1495 - loglik: -1.2934e+02 - logprior: -1.2809e+01
Epoch 4/10
10/10 - 1s - loss: 139.3969 - loglik: -1.2893e+02 - logprior: -1.0468e+01
Epoch 5/10
10/10 - 1s - loss: 137.0053 - loglik: -1.2918e+02 - logprior: -7.8275e+00
Epoch 6/10
10/10 - 1s - loss: 131.2490 - loglik: -1.2930e+02 - logprior: -1.9480e+00
Epoch 7/10
10/10 - 1s - loss: 128.9252 - loglik: -1.2945e+02 - logprior: 0.5271
Epoch 8/10
10/10 - 1s - loss: 128.7654 - loglik: -1.2969e+02 - logprior: 0.9290
Epoch 9/10
10/10 - 1s - loss: 128.6732 - loglik: -1.2980e+02 - logprior: 1.1275
Epoch 10/10
10/10 - 1s - loss: 128.2441 - loglik: -1.2953e+02 - logprior: 1.2866
Fitted a model with MAP estimate = -128.2876
Time for alignment: 31.2135
Computed alignments with likelihoods: ['-125.9662', '-125.5404', '-128.2876']
Best model has likelihood: -125.5404
SP score = 0.8565
Training of 3 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25caaef10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe27772aca0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.3285 - loglik: -1.6671e+02 - logprior: -5.6183e+00
Epoch 2/10
15/15 - 1s - loss: 144.8140 - loglik: -1.4305e+02 - logprior: -1.7597e+00
Epoch 3/10
15/15 - 1s - loss: 130.8309 - loglik: -1.2903e+02 - logprior: -1.8023e+00
Epoch 4/10
15/15 - 1s - loss: 127.1641 - loglik: -1.2543e+02 - logprior: -1.7358e+00
Epoch 5/10
15/15 - 1s - loss: 126.5318 - loglik: -1.2490e+02 - logprior: -1.6315e+00
Epoch 6/10
15/15 - 1s - loss: 126.1000 - loglik: -1.2446e+02 - logprior: -1.6449e+00
Epoch 7/10
15/15 - 1s - loss: 126.0768 - loglik: -1.2446e+02 - logprior: -1.6191e+00
Epoch 8/10
15/15 - 1s - loss: 126.0230 - loglik: -1.2442e+02 - logprior: -1.6037e+00
Epoch 9/10
15/15 - 1s - loss: 125.8639 - loglik: -1.2427e+02 - logprior: -1.5948e+00
Epoch 10/10
15/15 - 1s - loss: 125.7803 - loglik: -1.2419e+02 - logprior: -1.5916e+00
Fitted a model with MAP estimate = -125.6381
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (15, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 2)]
discards: [0]
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 136.0252 - loglik: -1.2911e+02 - logprior: -6.9187e+00
Epoch 2/2
15/15 - 1s - loss: 127.4467 - loglik: -1.2400e+02 - logprior: -3.4422e+00
Fitted a model with MAP estimate = -125.5513
expansions: [(0, 2)]
discards: [ 0 12 15 44 49]
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.5903 - loglik: -1.2247e+02 - logprior: -5.1231e+00
Epoch 2/2
15/15 - 1s - loss: 121.5426 - loglik: -1.1987e+02 - logprior: -1.6680e+00
Fitted a model with MAP estimate = -120.7402
expansions: []
discards: [ 0 37]
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 129.5018 - loglik: -1.2301e+02 - logprior: -6.4924e+00
Epoch 2/10
15/15 - 1s - loss: 122.5752 - loglik: -1.2041e+02 - logprior: -2.1672e+00
Epoch 3/10
15/15 - 1s - loss: 121.5337 - loglik: -1.2006e+02 - logprior: -1.4690e+00
Epoch 4/10
15/15 - 1s - loss: 121.0340 - loglik: -1.1973e+02 - logprior: -1.3025e+00
Epoch 5/10
15/15 - 1s - loss: 120.7493 - loglik: -1.1950e+02 - logprior: -1.2537e+00
Epoch 6/10
15/15 - 1s - loss: 120.7808 - loglik: -1.1955e+02 - logprior: -1.2280e+00
Fitted a model with MAP estimate = -120.6282
Time for alignment: 35.1483
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.4046 - loglik: -1.6679e+02 - logprior: -5.6167e+00
Epoch 2/10
15/15 - 1s - loss: 144.4593 - loglik: -1.4270e+02 - logprior: -1.7623e+00
Epoch 3/10
15/15 - 1s - loss: 130.5636 - loglik: -1.2877e+02 - logprior: -1.7936e+00
Epoch 4/10
15/15 - 1s - loss: 127.0160 - loglik: -1.2527e+02 - logprior: -1.7473e+00
Epoch 5/10
15/15 - 1s - loss: 126.0598 - loglik: -1.2439e+02 - logprior: -1.6696e+00
Epoch 6/10
15/15 - 1s - loss: 125.7228 - loglik: -1.2404e+02 - logprior: -1.6799e+00
Epoch 7/10
15/15 - 1s - loss: 125.5287 - loglik: -1.2388e+02 - logprior: -1.6533e+00
Epoch 8/10
15/15 - 1s - loss: 125.5211 - loglik: -1.2389e+02 - logprior: -1.6338e+00
Epoch 9/10
15/15 - 1s - loss: 125.3904 - loglik: -1.2376e+02 - logprior: -1.6266e+00
Epoch 10/10
15/15 - 1s - loss: 125.5125 - loglik: -1.2390e+02 - logprior: -1.6150e+00
Fitted a model with MAP estimate = -125.3400
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (25, 2), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 136.4316 - loglik: -1.2952e+02 - logprior: -6.9108e+00
Epoch 2/2
15/15 - 1s - loss: 127.0730 - loglik: -1.2363e+02 - logprior: -3.4393e+00
Fitted a model with MAP estimate = -125.3683
expansions: [(0, 2)]
discards: [ 0 12 15 31 46]
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 127.5925 - loglik: -1.2248e+02 - logprior: -5.1149e+00
Epoch 2/2
15/15 - 1s - loss: 121.3328 - loglik: -1.1967e+02 - logprior: -1.6637e+00
Fitted a model with MAP estimate = -120.7100
expansions: []
discards: [0]
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 129.2691 - loglik: -1.2273e+02 - logprior: -6.5370e+00
Epoch 2/10
15/15 - 1s - loss: 122.7387 - loglik: -1.2055e+02 - logprior: -2.1883e+00
Epoch 3/10
15/15 - 1s - loss: 121.1146 - loglik: -1.1963e+02 - logprior: -1.4871e+00
Epoch 4/10
15/15 - 1s - loss: 120.8534 - loglik: -1.1954e+02 - logprior: -1.3136e+00
Epoch 5/10
15/15 - 1s - loss: 120.6683 - loglik: -1.1941e+02 - logprior: -1.2587e+00
Epoch 6/10
15/15 - 1s - loss: 120.7156 - loglik: -1.1948e+02 - logprior: -1.2376e+00
Fitted a model with MAP estimate = -120.4895
Time for alignment: 35.4661
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 4s - loss: 172.2536 - loglik: -1.6663e+02 - logprior: -5.6206e+00
Epoch 2/10
15/15 - 1s - loss: 145.5418 - loglik: -1.4377e+02 - logprior: -1.7673e+00
Epoch 3/10
15/15 - 1s - loss: 132.7500 - loglik: -1.3095e+02 - logprior: -1.8040e+00
Epoch 4/10
15/15 - 1s - loss: 127.9019 - loglik: -1.2616e+02 - logprior: -1.7453e+00
Epoch 5/10
15/15 - 1s - loss: 126.3831 - loglik: -1.2471e+02 - logprior: -1.6700e+00
Epoch 6/10
15/15 - 1s - loss: 126.0779 - loglik: -1.2439e+02 - logprior: -1.6867e+00
Epoch 7/10
15/15 - 1s - loss: 125.8819 - loglik: -1.2423e+02 - logprior: -1.6565e+00
Epoch 8/10
15/15 - 1s - loss: 125.8853 - loglik: -1.2425e+02 - logprior: -1.6353e+00
Fitted a model with MAP estimate = -125.7491
expansions: [(9, 2), (11, 2), (12, 2), (13, 1), (19, 1), (27, 1), (29, 2), (30, 2), (31, 1), (32, 2), (34, 1)]
discards: [0]
Fitting a model of length 61 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 3s - loss: 135.7630 - loglik: -1.2886e+02 - logprior: -6.9011e+00
Epoch 2/2
15/15 - 1s - loss: 126.8479 - loglik: -1.2347e+02 - logprior: -3.3825e+00
Fitted a model with MAP estimate = -124.9527
expansions: []
discards: [12 15 45]
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 128.0486 - loglik: -1.2208e+02 - logprior: -5.9696e+00
Epoch 2/2
15/15 - 1s - loss: 122.0613 - loglik: -1.2007e+02 - logprior: -1.9893e+00
Fitted a model with MAP estimate = -121.3933
expansions: []
discards: []
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 125.7080 - loglik: -1.2036e+02 - logprior: -5.3522e+00
Epoch 2/10
15/15 - 1s - loss: 121.7519 - loglik: -1.1984e+02 - logprior: -1.9138e+00
Epoch 3/10
15/15 - 1s - loss: 121.2118 - loglik: -1.1974e+02 - logprior: -1.4736e+00
Epoch 4/10
15/15 - 1s - loss: 120.7388 - loglik: -1.1943e+02 - logprior: -1.3118e+00
Epoch 5/10
15/15 - 1s - loss: 120.7439 - loglik: -1.1948e+02 - logprior: -1.2605e+00
Fitted a model with MAP estimate = -120.5310
Time for alignment: 31.5623
Computed alignments with likelihoods: ['-120.6282', '-120.4895', '-120.5310']
Best model has likelihood: -120.4895
SP score = 0.9790
Training of 3 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2a1e40f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe27700a910>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.5373 - loglik: -1.8321e+02 - logprior: -1.4330e+01
Epoch 2/10
10/10 - 2s - loss: 171.0379 - loglik: -1.6696e+02 - logprior: -4.0803e+00
Epoch 3/10
10/10 - 2s - loss: 153.6702 - loglik: -1.5119e+02 - logprior: -2.4764e+00
Epoch 4/10
10/10 - 1s - loss: 142.7575 - loglik: -1.4051e+02 - logprior: -2.2471e+00
Epoch 5/10
10/10 - 2s - loss: 138.1013 - loglik: -1.3578e+02 - logprior: -2.3231e+00
Epoch 6/10
10/10 - 2s - loss: 135.8570 - loglik: -1.3343e+02 - logprior: -2.4270e+00
Epoch 7/10
10/10 - 1s - loss: 134.7128 - loglik: -1.3233e+02 - logprior: -2.3870e+00
Epoch 8/10
10/10 - 2s - loss: 133.9794 - loglik: -1.3171e+02 - logprior: -2.2717e+00
Epoch 9/10
10/10 - 2s - loss: 134.0369 - loglik: -1.3184e+02 - logprior: -2.2012e+00
Fitted a model with MAP estimate = -133.7525
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (33, 1), (34, 1), (35, 1), (36, 1), (38, 1), (39, 1), (40, 1), (41, 1)]
discards: [0]
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 149.0592 - loglik: -1.3306e+02 - logprior: -1.6003e+01
Epoch 2/2
10/10 - 2s - loss: 129.4492 - loglik: -1.2265e+02 - logprior: -6.8022e+00
Fitted a model with MAP estimate = -126.1428
expansions: [(0, 2)]
discards: [ 0 10 13]
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 131.9022 - loglik: -1.1925e+02 - logprior: -1.2648e+01
Epoch 2/2
10/10 - 2s - loss: 122.1977 - loglik: -1.1874e+02 - logprior: -3.4551e+00
Fitted a model with MAP estimate = -120.3314
expansions: []
discards: [0]
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 135.0562 - loglik: -1.1997e+02 - logprior: -1.5089e+01
Epoch 2/10
10/10 - 2s - loss: 124.5343 - loglik: -1.1978e+02 - logprior: -4.7532e+00
Epoch 3/10
10/10 - 2s - loss: 121.2799 - loglik: -1.1879e+02 - logprior: -2.4870e+00
Epoch 4/10
10/10 - 1s - loss: 119.5994 - loglik: -1.1786e+02 - logprior: -1.7391e+00
Epoch 5/10
10/10 - 1s - loss: 119.5178 - loglik: -1.1828e+02 - logprior: -1.2414e+00
Epoch 6/10
10/10 - 2s - loss: 118.8275 - loglik: -1.1770e+02 - logprior: -1.1259e+00
Epoch 7/10
10/10 - 1s - loss: 118.7607 - loglik: -1.1775e+02 - logprior: -1.0134e+00
Epoch 8/10
10/10 - 1s - loss: 118.1249 - loglik: -1.1717e+02 - logprior: -9.5357e-01
Epoch 9/10
10/10 - 2s - loss: 118.1922 - loglik: -1.1725e+02 - logprior: -9.4118e-01
Fitted a model with MAP estimate = -118.2787
Time for alignment: 52.0798
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 197.4612 - loglik: -1.8313e+02 - logprior: -1.4328e+01
Epoch 2/10
10/10 - 1s - loss: 170.9175 - loglik: -1.6684e+02 - logprior: -4.0782e+00
Epoch 3/10
10/10 - 1s - loss: 153.4362 - loglik: -1.5095e+02 - logprior: -2.4866e+00
Epoch 4/10
10/10 - 2s - loss: 143.3402 - loglik: -1.4105e+02 - logprior: -2.2894e+00
Epoch 5/10
10/10 - 1s - loss: 137.7578 - loglik: -1.3540e+02 - logprior: -2.3574e+00
Epoch 6/10
10/10 - 2s - loss: 135.3117 - loglik: -1.3287e+02 - logprior: -2.4374e+00
Epoch 7/10
10/10 - 1s - loss: 134.4385 - loglik: -1.3206e+02 - logprior: -2.3740e+00
Epoch 8/10
10/10 - 1s - loss: 133.5122 - loglik: -1.3125e+02 - logprior: -2.2591e+00
Epoch 9/10
10/10 - 2s - loss: 133.6714 - loglik: -1.3146e+02 - logprior: -2.2083e+00
Fitted a model with MAP estimate = -133.2026
expansions: [(4, 2), (5, 1), (6, 1), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (36, 1), (38, 1), (39, 1), (40, 1), (41, 1)]
discards: [0]
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 148.9387 - loglik: -1.3294e+02 - logprior: -1.5996e+01
Epoch 2/2
10/10 - 1s - loss: 129.1391 - loglik: -1.2233e+02 - logprior: -6.8059e+00
Fitted a model with MAP estimate = -126.2018
expansions: [(0, 2)]
discards: [ 0 10 13]
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 132.3162 - loglik: -1.1967e+02 - logprior: -1.2651e+01
Epoch 2/2
10/10 - 1s - loss: 121.6938 - loglik: -1.1824e+02 - logprior: -3.4551e+00
Fitted a model with MAP estimate = -120.3516
expansions: []
discards: [0]
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 135.5350 - loglik: -1.2045e+02 - logprior: -1.5086e+01
Epoch 2/10
10/10 - 2s - loss: 124.2639 - loglik: -1.1953e+02 - logprior: -4.7387e+00
Epoch 3/10
10/10 - 2s - loss: 120.7864 - loglik: -1.1830e+02 - logprior: -2.4844e+00
Epoch 4/10
10/10 - 2s - loss: 120.2882 - loglik: -1.1855e+02 - logprior: -1.7426e+00
Epoch 5/10
10/10 - 1s - loss: 119.0551 - loglik: -1.1782e+02 - logprior: -1.2351e+00
Epoch 6/10
10/10 - 1s - loss: 118.9799 - loglik: -1.1785e+02 - logprior: -1.1283e+00
Epoch 7/10
10/10 - 1s - loss: 118.2984 - loglik: -1.1728e+02 - logprior: -1.0160e+00
Epoch 8/10
10/10 - 1s - loss: 118.3852 - loglik: -1.1744e+02 - logprior: -9.4912e-01
Fitted a model with MAP estimate = -118.3472
Time for alignment: 50.2378
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 197.5685 - loglik: -1.8324e+02 - logprior: -1.4329e+01
Epoch 2/10
10/10 - 1s - loss: 170.4892 - loglik: -1.6641e+02 - logprior: -4.0829e+00
Epoch 3/10
10/10 - 2s - loss: 154.2519 - loglik: -1.5178e+02 - logprior: -2.4733e+00
Epoch 4/10
10/10 - 2s - loss: 144.1352 - loglik: -1.4190e+02 - logprior: -2.2323e+00
Epoch 5/10
10/10 - 2s - loss: 138.4163 - loglik: -1.3614e+02 - logprior: -2.2714e+00
Epoch 6/10
10/10 - 2s - loss: 134.2906 - loglik: -1.3195e+02 - logprior: -2.3387e+00
Epoch 7/10
10/10 - 2s - loss: 133.6467 - loglik: -1.3137e+02 - logprior: -2.2752e+00
Epoch 8/10
10/10 - 1s - loss: 132.6412 - loglik: -1.3051e+02 - logprior: -2.1356e+00
Epoch 9/10
10/10 - 2s - loss: 132.8719 - loglik: -1.3082e+02 - logprior: -2.0565e+00
Fitted a model with MAP estimate = -132.3702
expansions: [(4, 2), (5, 1), (7, 2), (9, 2), (13, 1), (24, 1), (33, 3), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: [0]
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 147.1218 - loglik: -1.3112e+02 - logprior: -1.6001e+01
Epoch 2/2
10/10 - 2s - loss: 128.5895 - loglik: -1.2179e+02 - logprior: -6.8005e+00
Fitted a model with MAP estimate = -125.8711
expansions: [(0, 2)]
discards: [ 0  9 13]
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 132.0644 - loglik: -1.1941e+02 - logprior: -1.2654e+01
Epoch 2/2
10/10 - 1s - loss: 122.1980 - loglik: -1.1874e+02 - logprior: -3.4558e+00
Fitted a model with MAP estimate = -120.4139
expansions: []
discards: [0]
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 135.1779 - loglik: -1.2015e+02 - logprior: -1.5030e+01
Epoch 2/10
10/10 - 1s - loss: 124.3558 - loglik: -1.1968e+02 - logprior: -4.6765e+00
Epoch 3/10
10/10 - 2s - loss: 121.3682 - loglik: -1.1890e+02 - logprior: -2.4729e+00
Epoch 4/10
10/10 - 2s - loss: 119.4306 - loglik: -1.1770e+02 - logprior: -1.7283e+00
Epoch 5/10
10/10 - 2s - loss: 119.3735 - loglik: -1.1814e+02 - logprior: -1.2339e+00
Epoch 6/10
10/10 - 1s - loss: 119.0282 - loglik: -1.1791e+02 - logprior: -1.1162e+00
Epoch 7/10
10/10 - 1s - loss: 118.4441 - loglik: -1.1744e+02 - logprior: -1.0047e+00
Epoch 8/10
10/10 - 1s - loss: 118.4783 - loglik: -1.1754e+02 - logprior: -9.4246e-01
Fitted a model with MAP estimate = -118.3341
Time for alignment: 50.8794
Computed alignments with likelihoods: ['-118.2787', '-118.3472', '-118.3341']
Best model has likelihood: -118.2787
SP score = 0.9966
Training of 3 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c914070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2dee3a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 550.7456 - loglik: -5.3973e+02 - logprior: -1.1017e+01
Epoch 2/10
11/11 - 4s - loss: 509.1364 - loglik: -5.0725e+02 - logprior: -1.8897e+00
Epoch 3/10
11/11 - 4s - loss: 467.2990 - loglik: -4.6648e+02 - logprior: -8.1494e-01
Epoch 4/10
11/11 - 4s - loss: 448.4383 - loglik: -4.4764e+02 - logprior: -7.9517e-01
Epoch 5/10
11/11 - 4s - loss: 438.1958 - loglik: -4.3746e+02 - logprior: -7.3872e-01
Epoch 6/10
11/11 - 4s - loss: 433.8129 - loglik: -4.3322e+02 - logprior: -5.8810e-01
Epoch 7/10
11/11 - 4s - loss: 429.9046 - loglik: -4.2941e+02 - logprior: -4.9718e-01
Epoch 8/10
11/11 - 4s - loss: 429.2847 - loglik: -4.2884e+02 - logprior: -4.4838e-01
Epoch 9/10
11/11 - 4s - loss: 428.1080 - loglik: -4.2770e+02 - logprior: -4.1176e-01
Epoch 10/10
11/11 - 4s - loss: 426.6318 - loglik: -4.2621e+02 - logprior: -4.2116e-01
Fitted a model with MAP estimate = -426.9537
expansions: [(0, 8), (8, 5), (34, 1), (45, 1), (52, 1), (56, 1), (57, 2), (58, 2), (69, 1), (70, 2), (71, 1), (75, 2), (82, 2), (107, 2), (111, 2), (112, 1), (114, 1), (120, 1), (127, 1), (131, 1), (149, 1), (150, 2), (159, 5)]
discards: []
Fitting a model of length 205 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 9s - loss: 453.7390 - loglik: -4.4025e+02 - logprior: -1.3493e+01
Epoch 2/2
11/11 - 5s - loss: 430.3606 - loglik: -4.2697e+02 - logprior: -3.3910e+00
Fitted a model with MAP estimate = -425.9099
expansions: [(0, 21)]
discards: [  0   1   2   3   4   5   6   7  18  77  93 110 136 144 200 201 202 203
 204]
Fitting a model of length 207 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 440.9249 - loglik: -4.2981e+02 - logprior: -1.1116e+01
Epoch 2/2
11/11 - 5s - loss: 427.8312 - loglik: -4.2568e+02 - logprior: -2.1463e+00
Fitted a model with MAP estimate = -424.1276
expansions: [(0, 17), (111, 1), (207, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]
Fitting a model of length 210 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 440.6831 - loglik: -4.2786e+02 - logprior: -1.2826e+01
Epoch 2/10
11/11 - 5s - loss: 423.9981 - loglik: -4.2119e+02 - logprior: -2.8053e+00
Epoch 3/10
11/11 - 5s - loss: 419.5917 - loglik: -4.1888e+02 - logprior: -7.1385e-01
Epoch 4/10
11/11 - 5s - loss: 418.4331 - loglik: -4.1850e+02 - logprior: 0.0702
Epoch 5/10
11/11 - 5s - loss: 414.2227 - loglik: -4.1456e+02 - logprior: 0.3365
Epoch 6/10
11/11 - 5s - loss: 411.3905 - loglik: -4.1183e+02 - logprior: 0.4416
Epoch 7/10
11/11 - 5s - loss: 410.5978 - loglik: -4.1115e+02 - logprior: 0.5521
Epoch 8/10
11/11 - 5s - loss: 407.9344 - loglik: -4.0857e+02 - logprior: 0.6391
Epoch 9/10
11/11 - 5s - loss: 408.1772 - loglik: -4.0886e+02 - logprior: 0.6812
Fitted a model with MAP estimate = -407.3820
Time for alignment: 137.7688
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 551.8155 - loglik: -5.4078e+02 - logprior: -1.1031e+01
Epoch 2/10
11/11 - 4s - loss: 508.0975 - loglik: -5.0620e+02 - logprior: -1.8933e+00
Epoch 3/10
11/11 - 4s - loss: 470.3847 - loglik: -4.6961e+02 - logprior: -7.7322e-01
Epoch 4/10
11/11 - 4s - loss: 448.4961 - loglik: -4.4776e+02 - logprior: -7.3854e-01
Epoch 5/10
11/11 - 4s - loss: 438.0807 - loglik: -4.3731e+02 - logprior: -7.7290e-01
Epoch 6/10
11/11 - 4s - loss: 433.3936 - loglik: -4.3270e+02 - logprior: -6.9228e-01
Epoch 7/10
11/11 - 4s - loss: 428.3878 - loglik: -4.2780e+02 - logprior: -5.8602e-01
Epoch 8/10
11/11 - 4s - loss: 428.2996 - loglik: -4.2778e+02 - logprior: -5.1929e-01
Epoch 9/10
11/11 - 4s - loss: 426.4821 - loglik: -4.2600e+02 - logprior: -4.8062e-01
Epoch 10/10
11/11 - 4s - loss: 425.6102 - loglik: -4.2515e+02 - logprior: -4.6236e-01
Fitted a model with MAP estimate = -425.7469
expansions: [(0, 8), (8, 5), (42, 1), (53, 1), (56, 2), (57, 1), (59, 2), (66, 1), (70, 2), (71, 1), (75, 1), (77, 1), (78, 1), (83, 1), (86, 1), (110, 1), (111, 2), (112, 1), (114, 1), (120, 1), (126, 1), (127, 1), (149, 2), (159, 6)]
discards: []
Fitting a model of length 204 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 451.5848 - loglik: -4.3808e+02 - logprior: -1.3505e+01
Epoch 2/2
11/11 - 5s - loss: 429.3849 - loglik: -4.2598e+02 - logprior: -3.4013e+00
Fitted a model with MAP estimate = -424.5820
expansions: [(0, 21)]
discards: [  0   1   2   3   4   5   6   7  77 142 198 199 200 201 202 203]
Fitting a model of length 209 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 438.9222 - loglik: -4.2773e+02 - logprior: -1.1192e+01
Epoch 2/2
11/11 - 5s - loss: 423.7903 - loglik: -4.2160e+02 - logprior: -2.1950e+00
Fitted a model with MAP estimate = -422.1963
expansions: [(0, 18), (199, 1), (209, 8)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  19  20  21  22  23 111]
Fitting a model of length 212 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 9s - loss: 439.3155 - loglik: -4.2651e+02 - logprior: -1.2805e+01
Epoch 2/10
11/11 - 5s - loss: 423.5808 - loglik: -4.2072e+02 - logprior: -2.8579e+00
Epoch 3/10
11/11 - 5s - loss: 419.2851 - loglik: -4.1845e+02 - logprior: -8.3287e-01
Epoch 4/10
11/11 - 5s - loss: 416.0106 - loglik: -4.1596e+02 - logprior: -5.2728e-02
Epoch 5/10
11/11 - 5s - loss: 413.5874 - loglik: -4.1385e+02 - logprior: 0.2670
Epoch 6/10
11/11 - 5s - loss: 411.0588 - loglik: -4.1146e+02 - logprior: 0.4031
Epoch 7/10
11/11 - 5s - loss: 410.1696 - loglik: -4.1069e+02 - logprior: 0.5186
Epoch 8/10
11/11 - 5s - loss: 406.5709 - loglik: -4.0716e+02 - logprior: 0.5938
Epoch 9/10
11/11 - 5s - loss: 405.8104 - loglik: -4.0647e+02 - logprior: 0.6556
Epoch 10/10
11/11 - 5s - loss: 407.2443 - loglik: -4.0795e+02 - logprior: 0.7032
Fitted a model with MAP estimate = -405.8274
Time for alignment: 144.8603
Fitting a model of length 159 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 551.5279 - loglik: -5.4050e+02 - logprior: -1.1028e+01
Epoch 2/10
11/11 - 4s - loss: 506.3612 - loglik: -5.0447e+02 - logprior: -1.8951e+00
Epoch 3/10
11/11 - 4s - loss: 469.1211 - loglik: -4.6832e+02 - logprior: -7.9852e-01
Epoch 4/10
11/11 - 4s - loss: 448.0977 - loglik: -4.4739e+02 - logprior: -7.1220e-01
Epoch 5/10
11/11 - 4s - loss: 439.5678 - loglik: -4.3897e+02 - logprior: -6.0025e-01
Epoch 6/10
11/11 - 4s - loss: 433.2175 - loglik: -4.3274e+02 - logprior: -4.7791e-01
Epoch 7/10
11/11 - 4s - loss: 429.1665 - loglik: -4.2874e+02 - logprior: -4.3073e-01
Epoch 8/10
11/11 - 4s - loss: 428.2118 - loglik: -4.2782e+02 - logprior: -3.9403e-01
Epoch 9/10
11/11 - 4s - loss: 427.6774 - loglik: -4.2732e+02 - logprior: -3.5971e-01
Epoch 10/10
11/11 - 4s - loss: 427.0509 - loglik: -4.2670e+02 - logprior: -3.5484e-01
Fitted a model with MAP estimate = -426.8726
expansions: [(0, 8), (8, 5), (34, 1), (42, 1), (50, 1), (56, 1), (57, 2), (58, 2), (69, 1), (70, 1), (71, 2), (74, 1), (83, 1), (86, 1), (112, 2), (113, 1), (119, 1), (120, 2), (127, 1), (149, 2), (150, 1), (159, 6)]
discards: []
Fitting a model of length 203 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 451.1328 - loglik: -4.3765e+02 - logprior: -1.3483e+01
Epoch 2/2
11/11 - 5s - loss: 429.9528 - loglik: -4.2664e+02 - logprior: -3.3119e+00
Fitted a model with MAP estimate = -424.1554
expansions: [(0, 21), (185, 1)]
discards: [  0   1   2   3   4   5   6   7  77 141 142 197 198 199 200 201 202]
Fitting a model of length 208 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 8s - loss: 438.8658 - loglik: -4.2783e+02 - logprior: -1.1035e+01
Epoch 2/2
11/11 - 5s - loss: 425.2931 - loglik: -4.2318e+02 - logprior: -2.1178e+00
Fitted a model with MAP estimate = -422.2051
expansions: [(0, 18), (153, 1), (208, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Fitting a model of length 213 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 9s - loss: 438.7670 - loglik: -4.2606e+02 - logprior: -1.2709e+01
Epoch 2/10
11/11 - 5s - loss: 424.1126 - loglik: -4.2136e+02 - logprior: -2.7566e+00
Epoch 3/10
11/11 - 5s - loss: 418.9094 - loglik: -4.1809e+02 - logprior: -8.1648e-01
Epoch 4/10
11/11 - 5s - loss: 416.2770 - loglik: -4.1630e+02 - logprior: 0.0224
Epoch 5/10
11/11 - 5s - loss: 414.1027 - loglik: -4.1445e+02 - logprior: 0.3501
Epoch 6/10
11/11 - 5s - loss: 410.3236 - loglik: -4.1080e+02 - logprior: 0.4742
Epoch 7/10
11/11 - 5s - loss: 409.9390 - loglik: -4.1048e+02 - logprior: 0.5409
Epoch 8/10
11/11 - 5s - loss: 407.5201 - loglik: -4.0814e+02 - logprior: 0.6180
Epoch 9/10
11/11 - 5s - loss: 406.2314 - loglik: -4.0690e+02 - logprior: 0.6649
Epoch 10/10
11/11 - 5s - loss: 408.0328 - loglik: -4.0873e+02 - logprior: 0.7015
Fitted a model with MAP estimate = -406.2903
Time for alignment: 143.9807
Computed alignments with likelihoods: ['-407.3820', '-405.8274', '-406.2903']
Best model has likelihood: -405.8274
SP score = 0.6633
Training of 3 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe277674b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fa4e9640>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 250.5745 - loglik: -2.4941e+02 - logprior: -1.1615e+00
Epoch 2/10
29/29 - 3s - loss: 225.2615 - loglik: -2.2442e+02 - logprior: -8.3775e-01
Epoch 3/10
29/29 - 3s - loss: 220.9060 - loglik: -2.2009e+02 - logprior: -8.1391e-01
Epoch 4/10
29/29 - 3s - loss: 219.6365 - loglik: -2.1882e+02 - logprior: -8.1758e-01
Epoch 5/10
29/29 - 3s - loss: 219.0262 - loglik: -2.1821e+02 - logprior: -8.1836e-01
Epoch 6/10
29/29 - 3s - loss: 217.8883 - loglik: -2.1707e+02 - logprior: -8.1721e-01
Epoch 7/10
29/29 - 3s - loss: 217.7955 - loglik: -2.1698e+02 - logprior: -8.1544e-01
Epoch 8/10
29/29 - 3s - loss: 217.3818 - loglik: -2.1657e+02 - logprior: -8.1213e-01
Epoch 9/10
29/29 - 3s - loss: 217.0522 - loglik: -2.1624e+02 - logprior: -8.1476e-01
Epoch 10/10
29/29 - 3s - loss: 216.5272 - loglik: -2.1572e+02 - logprior: -8.1022e-01
Fitted a model with MAP estimate = -203.7363
expansions: [(1, 1), (2, 1), (14, 2), (15, 3), (17, 2), (25, 2), (38, 1), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 87 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 220.5535 - loglik: -2.1942e+02 - logprior: -1.1292e+00
Epoch 2/2
29/29 - 3s - loss: 215.4582 - loglik: -2.1472e+02 - logprior: -7.4029e-01
Fitted a model with MAP estimate = -197.1761
expansions: []
discards: [ 1 35 59 67]
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.1186 - loglik: -2.1610e+02 - logprior: -1.0183e+00
Epoch 2/2
29/29 - 3s - loss: 216.1446 - loglik: -2.1543e+02 - logprior: -7.1384e-01
Fitted a model with MAP estimate = -197.5227
expansions: []
discards: []
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.4429 - loglik: -1.9677e+02 - logprior: -6.6968e-01
Epoch 2/10
42/42 - 4s - loss: 196.0765 - loglik: -1.9559e+02 - logprior: -4.9129e-01
Epoch 3/10
42/42 - 4s - loss: 196.0300 - loglik: -1.9555e+02 - logprior: -4.7939e-01
Epoch 4/10
42/42 - 4s - loss: 195.5586 - loglik: -1.9508e+02 - logprior: -4.7668e-01
Epoch 5/10
42/42 - 4s - loss: 194.7672 - loglik: -1.9429e+02 - logprior: -4.7296e-01
Epoch 6/10
42/42 - 4s - loss: 194.3755 - loglik: -1.9391e+02 - logprior: -4.6998e-01
Epoch 7/10
42/42 - 4s - loss: 194.0796 - loglik: -1.9361e+02 - logprior: -4.6695e-01
Epoch 8/10
42/42 - 4s - loss: 193.4712 - loglik: -1.9301e+02 - logprior: -4.6320e-01
Epoch 9/10
42/42 - 4s - loss: 193.2734 - loglik: -1.9281e+02 - logprior: -4.6174e-01
Epoch 10/10
42/42 - 4s - loss: 193.5016 - loglik: -1.9304e+02 - logprior: -4.5872e-01
Fitted a model with MAP estimate = -193.1958
Time for alignment: 129.9002
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 6s - loss: 250.3313 - loglik: -2.4917e+02 - logprior: -1.1605e+00
Epoch 2/10
29/29 - 3s - loss: 224.6313 - loglik: -2.2379e+02 - logprior: -8.4271e-01
Epoch 3/10
29/29 - 3s - loss: 221.0143 - loglik: -2.2021e+02 - logprior: -8.0832e-01
Epoch 4/10
29/29 - 3s - loss: 220.0240 - loglik: -2.1921e+02 - logprior: -8.1043e-01
Epoch 5/10
29/29 - 3s - loss: 218.8756 - loglik: -2.1808e+02 - logprior: -7.9702e-01
Epoch 6/10
29/29 - 3s - loss: 218.6602 - loglik: -2.1787e+02 - logprior: -7.9029e-01
Epoch 7/10
29/29 - 3s - loss: 217.8162 - loglik: -2.1701e+02 - logprior: -8.0256e-01
Epoch 8/10
29/29 - 3s - loss: 217.3660 - loglik: -2.1657e+02 - logprior: -7.9946e-01
Epoch 9/10
29/29 - 3s - loss: 217.3860 - loglik: -2.1658e+02 - logprior: -8.0354e-01
Fitted a model with MAP estimate = -203.1792
expansions: [(1, 1), (2, 1), (13, 3), (15, 1), (22, 1), (27, 2), (38, 1), (41, 1), (45, 2), (46, 1), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 220.3446 - loglik: -2.1922e+02 - logprior: -1.1236e+00
Epoch 2/2
29/29 - 3s - loss: 215.9618 - loglik: -2.1522e+02 - logprior: -7.4620e-01
Fitted a model with MAP estimate = -197.3278
expansions: [(19, 1), (20, 1)]
discards: [ 1 35 57 65]
Fitting a model of length 83 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.1585 - loglik: -2.1615e+02 - logprior: -1.0099e+00
Epoch 2/2
29/29 - 3s - loss: 215.9108 - loglik: -2.1520e+02 - logprior: -7.1042e-01
Fitted a model with MAP estimate = -197.6767
expansions: []
discards: [16]
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.2799 - loglik: -1.9661e+02 - logprior: -6.7469e-01
Epoch 2/10
42/42 - 4s - loss: 196.2766 - loglik: -1.9579e+02 - logprior: -4.9081e-01
Epoch 3/10
42/42 - 4s - loss: 196.0669 - loglik: -1.9558e+02 - logprior: -4.8505e-01
Epoch 4/10
42/42 - 4s - loss: 195.5937 - loglik: -1.9512e+02 - logprior: -4.7852e-01
Epoch 5/10
42/42 - 4s - loss: 194.8838 - loglik: -1.9441e+02 - logprior: -4.7600e-01
Epoch 6/10
42/42 - 4s - loss: 194.6795 - loglik: -1.9421e+02 - logprior: -4.7316e-01
Epoch 7/10
42/42 - 4s - loss: 194.1281 - loglik: -1.9366e+02 - logprior: -4.6883e-01
Epoch 8/10
42/42 - 4s - loss: 193.0033 - loglik: -1.9254e+02 - logprior: -4.6809e-01
Epoch 9/10
42/42 - 4s - loss: 193.5397 - loglik: -1.9307e+02 - logprior: -4.6610e-01
Fitted a model with MAP estimate = -193.3370
Time for alignment: 120.7331
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 249.4810 - loglik: -2.4832e+02 - logprior: -1.1627e+00
Epoch 2/10
29/29 - 3s - loss: 224.6030 - loglik: -2.2377e+02 - logprior: -8.3114e-01
Epoch 3/10
29/29 - 3s - loss: 220.0933 - loglik: -2.1928e+02 - logprior: -8.0901e-01
Epoch 4/10
29/29 - 3s - loss: 219.0128 - loglik: -2.1820e+02 - logprior: -8.1359e-01
Epoch 5/10
29/29 - 3s - loss: 218.8029 - loglik: -2.1800e+02 - logprior: -8.0518e-01
Epoch 6/10
29/29 - 3s - loss: 217.6641 - loglik: -2.1686e+02 - logprior: -8.0003e-01
Epoch 7/10
29/29 - 3s - loss: 217.0094 - loglik: -2.1621e+02 - logprior: -7.9638e-01
Epoch 8/10
29/29 - 3s - loss: 216.4668 - loglik: -2.1567e+02 - logprior: -7.9698e-01
Epoch 9/10
29/29 - 3s - loss: 216.5918 - loglik: -2.1579e+02 - logprior: -7.9805e-01
Fitted a model with MAP estimate = -202.7138
expansions: [(3, 1), (5, 1), (14, 2), (16, 3), (17, 2), (21, 1), (38, 2), (41, 1), (43, 2), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 88 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 220.2826 - loglik: -2.1915e+02 - logprior: -1.1348e+00
Epoch 2/2
29/29 - 3s - loss: 216.1817 - loglik: -2.1543e+02 - logprior: -7.5391e-01
Fitted a model with MAP estimate = -197.2403
expansions: [(17, 1)]
discards: [49 57 60 68]
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 217.0389 - loglik: -2.1595e+02 - logprior: -1.0869e+00
Epoch 2/2
29/29 - 3s - loss: 215.5769 - loglik: -2.1488e+02 - logprior: -6.9624e-01
Fitted a model with MAP estimate = -197.2998
expansions: []
discards: [1]
Fitting a model of length 84 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 197.3622 - loglik: -1.9674e+02 - logprior: -6.2560e-01
Epoch 2/10
42/42 - 4s - loss: 196.4201 - loglik: -1.9591e+02 - logprior: -5.1263e-01
Epoch 3/10
42/42 - 4s - loss: 195.9264 - loglik: -1.9543e+02 - logprior: -4.9554e-01
Epoch 4/10
42/42 - 4s - loss: 195.5190 - loglik: -1.9503e+02 - logprior: -4.9102e-01
Epoch 5/10
42/42 - 4s - loss: 195.1603 - loglik: -1.9467e+02 - logprior: -4.8551e-01
Epoch 6/10
42/42 - 4s - loss: 194.4187 - loglik: -1.9393e+02 - logprior: -4.8386e-01
Epoch 7/10
42/42 - 4s - loss: 193.8438 - loglik: -1.9336e+02 - logprior: -4.8374e-01
Epoch 8/10
42/42 - 4s - loss: 193.3835 - loglik: -1.9291e+02 - logprior: -4.7689e-01
Epoch 9/10
42/42 - 4s - loss: 193.2869 - loglik: -1.9281e+02 - logprior: -4.7501e-01
Epoch 10/10
42/42 - 4s - loss: 193.4032 - loglik: -1.9293e+02 - logprior: -4.6887e-01
Fitted a model with MAP estimate = -193.1756
Time for alignment: 127.0829
Computed alignments with likelihoods: ['-193.1958', '-193.3370', '-193.1756']
Best model has likelihood: -193.1756
SP score = 0.6164
Training of 3 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe27714f400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2667dca90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.2999 - loglik: -3.3004e+02 - logprior: -4.5258e+01
Epoch 2/10
10/10 - 1s - loss: 299.0260 - loglik: -2.8821e+02 - logprior: -1.0813e+01
Epoch 3/10
10/10 - 1s - loss: 251.4777 - loglik: -2.4659e+02 - logprior: -4.8888e+00
Epoch 4/10
10/10 - 1s - loss: 222.3000 - loglik: -2.1907e+02 - logprior: -3.2321e+00
Epoch 5/10
10/10 - 1s - loss: 210.6313 - loglik: -2.0805e+02 - logprior: -2.5764e+00
Epoch 6/10
10/10 - 1s - loss: 206.5039 - loglik: -2.0450e+02 - logprior: -2.0037e+00
Epoch 7/10
10/10 - 1s - loss: 205.1974 - loglik: -2.0362e+02 - logprior: -1.5807e+00
Epoch 8/10
10/10 - 1s - loss: 204.1471 - loglik: -2.0284e+02 - logprior: -1.3099e+00
Epoch 9/10
10/10 - 1s - loss: 203.7845 - loglik: -2.0260e+02 - logprior: -1.1871e+00
Epoch 10/10
10/10 - 1s - loss: 203.1389 - loglik: -2.0202e+02 - logprior: -1.1186e+00
Fitted a model with MAP estimate = -202.8877
expansions: [(10, 2), (11, 2), (12, 2), (13, 2), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 251.4423 - loglik: -2.0020e+02 - logprior: -5.1240e+01
Epoch 2/2
10/10 - 1s - loss: 207.9964 - loglik: -1.8781e+02 - logprior: -2.0191e+01
Fitted a model with MAP estimate = -200.3903
expansions: [(0, 3)]
discards: [  0   9  18 102 103]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.9659 - loglik: -1.8655e+02 - logprior: -4.0417e+01
Epoch 2/2
10/10 - 1s - loss: 192.1850 - loglik: -1.8285e+02 - logprior: -9.3372e+00
Fitted a model with MAP estimate = -187.0634
expansions: []
discards: [0 2]
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 234.5410 - loglik: -1.8509e+02 - logprior: -4.9449e+01
Epoch 2/10
10/10 - 1s - loss: 198.9555 - loglik: -1.8349e+02 - logprior: -1.5461e+01
Epoch 3/10
10/10 - 1s - loss: 186.8915 - loglik: -1.8244e+02 - logprior: -4.4484e+00
Epoch 4/10
10/10 - 1s - loss: 182.1009 - loglik: -1.8168e+02 - logprior: -4.1797e-01
Epoch 5/10
10/10 - 1s - loss: 180.2362 - loglik: -1.8128e+02 - logprior: 1.0433
Epoch 6/10
10/10 - 1s - loss: 179.4953 - loglik: -1.8131e+02 - logprior: 1.8168
Epoch 7/10
10/10 - 1s - loss: 178.1617 - loglik: -1.8063e+02 - logprior: 2.4727
Epoch 8/10
10/10 - 1s - loss: 177.7543 - loglik: -1.8076e+02 - logprior: 3.0085
Epoch 9/10
10/10 - 1s - loss: 177.4842 - loglik: -1.8085e+02 - logprior: 3.3621
Epoch 10/10
10/10 - 1s - loss: 177.3432 - loglik: -1.8099e+02 - logprior: 3.6432
Fitted a model with MAP estimate = -177.0228
Time for alignment: 43.4474
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.3526 - loglik: -3.3009e+02 - logprior: -4.5258e+01
Epoch 2/10
10/10 - 1s - loss: 298.7637 - loglik: -2.8795e+02 - logprior: -1.0818e+01
Epoch 3/10
10/10 - 1s - loss: 250.2051 - loglik: -2.4529e+02 - logprior: -4.9165e+00
Epoch 4/10
10/10 - 1s - loss: 220.5701 - loglik: -2.1735e+02 - logprior: -3.2197e+00
Epoch 5/10
10/10 - 1s - loss: 210.8250 - loglik: -2.0842e+02 - logprior: -2.4052e+00
Epoch 6/10
10/10 - 1s - loss: 206.9448 - loglik: -2.0513e+02 - logprior: -1.8101e+00
Epoch 7/10
10/10 - 1s - loss: 205.0170 - loglik: -2.0351e+02 - logprior: -1.5032e+00
Epoch 8/10
10/10 - 1s - loss: 204.4012 - loglik: -2.0308e+02 - logprior: -1.3204e+00
Epoch 9/10
10/10 - 1s - loss: 203.3952 - loglik: -2.0226e+02 - logprior: -1.1364e+00
Epoch 10/10
10/10 - 1s - loss: 202.8987 - loglik: -2.0191e+02 - logprior: -9.8758e-01
Fitted a model with MAP estimate = -202.8634
expansions: [(10, 4), (12, 1), (13, 1), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.7863 - loglik: -1.9863e+02 - logprior: -5.1156e+01
Epoch 2/2
10/10 - 1s - loss: 206.6022 - loglik: -1.8662e+02 - logprior: -1.9984e+01
Fitted a model with MAP estimate = -199.2124
expansions: [(0, 3)]
discards: [  0 100 101]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 225.7451 - loglik: -1.8537e+02 - logprior: -4.0377e+01
Epoch 2/2
10/10 - 1s - loss: 192.0695 - loglik: -1.8274e+02 - logprior: -9.3295e+00
Fitted a model with MAP estimate = -186.8845
expansions: []
discards: [0 2]
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 234.2001 - loglik: -1.8482e+02 - logprior: -4.9379e+01
Epoch 2/10
10/10 - 1s - loss: 198.7462 - loglik: -1.8351e+02 - logprior: -1.5234e+01
Epoch 3/10
10/10 - 1s - loss: 186.6687 - loglik: -1.8234e+02 - logprior: -4.3242e+00
Epoch 4/10
10/10 - 1s - loss: 182.1585 - loglik: -1.8176e+02 - logprior: -3.9359e-01
Epoch 5/10
10/10 - 1s - loss: 180.2246 - loglik: -1.8127e+02 - logprior: 1.0483
Epoch 6/10
10/10 - 1s - loss: 179.2383 - loglik: -1.8106e+02 - logprior: 1.8212
Epoch 7/10
10/10 - 1s - loss: 178.2192 - loglik: -1.8070e+02 - logprior: 2.4815
Epoch 8/10
10/10 - 1s - loss: 177.8393 - loglik: -1.8085e+02 - logprior: 3.0151
Epoch 9/10
10/10 - 1s - loss: 177.4036 - loglik: -1.8077e+02 - logprior: 3.3686
Epoch 10/10
10/10 - 1s - loss: 177.1203 - loglik: -1.8077e+02 - logprior: 3.6502
Fitted a model with MAP estimate = -176.9785
Time for alignment: 43.4052
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 375.2608 - loglik: -3.3000e+02 - logprior: -4.5258e+01
Epoch 2/10
10/10 - 1s - loss: 297.9768 - loglik: -2.8716e+02 - logprior: -1.0813e+01
Epoch 3/10
10/10 - 1s - loss: 250.8531 - loglik: -2.4592e+02 - logprior: -4.9366e+00
Epoch 4/10
10/10 - 1s - loss: 223.9176 - loglik: -2.2073e+02 - logprior: -3.1876e+00
Epoch 5/10
10/10 - 1s - loss: 212.0424 - loglik: -2.0967e+02 - logprior: -2.3746e+00
Epoch 6/10
10/10 - 1s - loss: 208.2271 - loglik: -2.0642e+02 - logprior: -1.8118e+00
Epoch 7/10
10/10 - 1s - loss: 205.7246 - loglik: -2.0428e+02 - logprior: -1.4443e+00
Epoch 8/10
10/10 - 1s - loss: 205.0595 - loglik: -2.0385e+02 - logprior: -1.2121e+00
Epoch 9/10
10/10 - 1s - loss: 204.6537 - loglik: -2.0358e+02 - logprior: -1.0779e+00
Epoch 10/10
10/10 - 1s - loss: 203.8776 - loglik: -2.0289e+02 - logprior: -9.8440e-01
Fitted a model with MAP estimate = -203.6730
expansions: [(10, 4), (12, 1), (13, 1), (16, 1), (30, 1), (32, 1), (44, 2), (45, 2), (51, 2), (52, 1), (58, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 250.1987 - loglik: -1.9904e+02 - logprior: -5.1159e+01
Epoch 2/2
10/10 - 1s - loss: 207.5652 - loglik: -1.8756e+02 - logprior: -2.0001e+01
Fitted a model with MAP estimate = -199.6244
expansions: [(0, 3)]
discards: [  0 100 101]
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 226.0240 - loglik: -1.8562e+02 - logprior: -4.0400e+01
Epoch 2/2
10/10 - 1s - loss: 191.8683 - loglik: -1.8253e+02 - logprior: -9.3391e+00
Fitted a model with MAP estimate = -186.8217
expansions: []
discards: [0 2]
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 234.0083 - loglik: -1.8460e+02 - logprior: -4.9413e+01
Epoch 2/10
10/10 - 1s - loss: 199.0158 - loglik: -1.8371e+02 - logprior: -1.5307e+01
Epoch 3/10
10/10 - 1s - loss: 186.8853 - loglik: -1.8252e+02 - logprior: -4.3607e+00
Epoch 4/10
10/10 - 1s - loss: 181.8732 - loglik: -1.8146e+02 - logprior: -4.0843e-01
Epoch 5/10
10/10 - 1s - loss: 179.9165 - loglik: -1.8096e+02 - logprior: 1.0477
Epoch 6/10
10/10 - 1s - loss: 179.0203 - loglik: -1.8084e+02 - logprior: 1.8157
Epoch 7/10
10/10 - 1s - loss: 178.4269 - loglik: -1.8089e+02 - logprior: 2.4637
Epoch 8/10
10/10 - 1s - loss: 177.8843 - loglik: -1.8087e+02 - logprior: 2.9856
Epoch 9/10
10/10 - 1s - loss: 177.4195 - loglik: -1.8075e+02 - logprior: 3.3338
Epoch 10/10
10/10 - 1s - loss: 177.3075 - loglik: -1.8091e+02 - logprior: 3.6073
Fitted a model with MAP estimate = -177.0033
Time for alignment: 42.7664
Computed alignments with likelihoods: ['-177.0228', '-176.9785', '-177.0033']
Best model has likelihood: -176.9785
SP score = 0.9260
Training of 3 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2880a3520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe277257a00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 644.6281 - loglik: -6.1618e+02 - logprior: -2.8447e+01
Epoch 2/10
10/10 - 4s - loss: 574.7189 - loglik: -5.6991e+02 - logprior: -4.8074e+00
Epoch 3/10
10/10 - 4s - loss: 516.9308 - loglik: -5.1581e+02 - logprior: -1.1178e+00
Epoch 4/10
10/10 - 4s - loss: 481.4354 - loglik: -4.8061e+02 - logprior: -8.2200e-01
Epoch 5/10
10/10 - 4s - loss: 466.6103 - loglik: -4.6591e+02 - logprior: -7.0153e-01
Epoch 6/10
10/10 - 4s - loss: 460.6510 - loglik: -4.6010e+02 - logprior: -5.4818e-01
Epoch 7/10
10/10 - 4s - loss: 458.8258 - loglik: -4.5853e+02 - logprior: -2.9597e-01
Epoch 8/10
10/10 - 4s - loss: 457.8816 - loglik: -4.5776e+02 - logprior: -1.2189e-01
Epoch 9/10
10/10 - 4s - loss: 457.4946 - loglik: -4.5743e+02 - logprior: -6.0675e-02
Epoch 10/10
10/10 - 4s - loss: 457.2580 - loglik: -4.5730e+02 - logprior: 0.0394
Fitted a model with MAP estimate = -456.7648
expansions: [(15, 2), (16, 2), (28, 1), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 1), (44, 1), (50, 1), (51, 1), (54, 3), (61, 1), (64, 1), (80, 1), (90, 1), (91, 7), (112, 1), (113, 2), (114, 2), (115, 3), (117, 1), (119, 1), (120, 3), (124, 1), (153, 1), (163, 1), (166, 8), (167, 1)]
discards: [0]
Fitting a model of length 240 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 486.4571 - loglik: -4.5470e+02 - logprior: -3.1752e+01
Epoch 2/2
10/10 - 6s - loss: 453.1846 - loglik: -4.4256e+02 - logprior: -1.0627e+01
Fitted a model with MAP estimate = -445.9240
expansions: [(147, 1)]
discards: [  0  18  19  50  52 152 220]
Fitting a model of length 234 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 471.6465 - loglik: -4.4043e+02 - logprior: -3.1214e+01
Epoch 2/2
10/10 - 6s - loss: 446.8005 - loglik: -4.3664e+02 - logprior: -1.0161e+01
Fitted a model with MAP estimate = -441.5308
expansions: [(0, 4), (12, 1), (67, 2), (116, 1)]
discards: [  0  13 110 111 112]
Fitting a model of length 237 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 461.9753 - loglik: -4.3800e+02 - logprior: -2.3972e+01
Epoch 2/10
10/10 - 6s - loss: 437.1205 - loglik: -4.3418e+02 - logprior: -2.9410e+00
Epoch 3/10
10/10 - 6s - loss: 430.8752 - loglik: -4.3226e+02 - logprior: 1.3891
Epoch 4/10
10/10 - 6s - loss: 427.8459 - loglik: -4.3099e+02 - logprior: 3.1421
Epoch 5/10
10/10 - 6s - loss: 428.7716 - loglik: -4.3288e+02 - logprior: 4.1041
Fitted a model with MAP estimate = -427.0342
Time for alignment: 118.7748
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 644.4850 - loglik: -6.1603e+02 - logprior: -2.8456e+01
Epoch 2/10
10/10 - 4s - loss: 574.0515 - loglik: -5.6924e+02 - logprior: -4.8086e+00
Epoch 3/10
10/10 - 4s - loss: 517.8948 - loglik: -5.1686e+02 - logprior: -1.0382e+00
Epoch 4/10
10/10 - 4s - loss: 479.6471 - loglik: -4.7921e+02 - logprior: -4.3484e-01
Epoch 5/10
10/10 - 4s - loss: 466.4713 - loglik: -4.6651e+02 - logprior: 0.0368
Epoch 6/10
10/10 - 4s - loss: 462.3497 - loglik: -4.6260e+02 - logprior: 0.2525
Epoch 7/10
10/10 - 4s - loss: 460.9533 - loglik: -4.6131e+02 - logprior: 0.3581
Epoch 8/10
10/10 - 4s - loss: 458.4015 - loglik: -4.5888e+02 - logprior: 0.4762
Epoch 9/10
10/10 - 4s - loss: 458.4642 - loglik: -4.5901e+02 - logprior: 0.5485
Fitted a model with MAP estimate = -457.9816
expansions: [(14, 3), (15, 1), (28, 1), (29, 1), (30, 2), (31, 2), (40, 1), (41, 2), (42, 1), (50, 1), (51, 1), (54, 4), (62, 1), (80, 2), (113, 1), (114, 2), (115, 2), (116, 3), (118, 2), (119, 2), (120, 3), (130, 2), (164, 2), (166, 1), (167, 7)]
discards: [0]
Fitting a model of length 233 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 485.6697 - loglik: -4.5379e+02 - logprior: -3.1880e+01
Epoch 2/2
10/10 - 6s - loss: 453.6133 - loglik: -4.4291e+02 - logprior: -1.0700e+01
Fitted a model with MAP estimate = -447.6293
expansions: [(139, 1)]
discards: [  0  14 144 151 154 168]
Fitting a model of length 228 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 472.9569 - loglik: -4.4188e+02 - logprior: -3.1081e+01
Epoch 2/2
10/10 - 5s - loss: 449.2702 - loglik: -4.3966e+02 - logprior: -9.6137e+00
Fitted a model with MAP estimate = -444.3918
expansions: [(0, 4)]
discards: [ 0 47 68]
Fitting a model of length 229 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 464.2524 - loglik: -4.4041e+02 - logprior: -2.3842e+01
Epoch 2/10
10/10 - 6s - loss: 440.0101 - loglik: -4.3731e+02 - logprior: -2.6970e+00
Epoch 3/10
10/10 - 6s - loss: 434.9095 - loglik: -4.3652e+02 - logprior: 1.6104
Epoch 4/10
10/10 - 6s - loss: 433.0750 - loglik: -4.3645e+02 - logprior: 3.3707
Epoch 5/10
10/10 - 6s - loss: 431.7009 - loglik: -4.3602e+02 - logprior: 4.3202
Epoch 6/10
10/10 - 6s - loss: 431.4602 - loglik: -4.3634e+02 - logprior: 4.8769
Epoch 7/10
10/10 - 6s - loss: 430.4579 - loglik: -4.3568e+02 - logprior: 5.2258
Epoch 8/10
10/10 - 6s - loss: 430.3739 - loglik: -4.3588e+02 - logprior: 5.5016
Epoch 9/10
10/10 - 6s - loss: 431.0917 - loglik: -4.3685e+02 - logprior: 5.7539
Fitted a model with MAP estimate = -430.0202
Time for alignment: 133.3228
Fitting a model of length 184 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 645.6842 - loglik: -6.1724e+02 - logprior: -2.8449e+01
Epoch 2/10
10/10 - 4s - loss: 573.1390 - loglik: -5.6833e+02 - logprior: -4.8042e+00
Epoch 3/10
10/10 - 4s - loss: 513.9470 - loglik: -5.1287e+02 - logprior: -1.0802e+00
Epoch 4/10
10/10 - 4s - loss: 479.6257 - loglik: -4.7897e+02 - logprior: -6.5464e-01
Epoch 5/10
10/10 - 4s - loss: 466.1343 - loglik: -4.6567e+02 - logprior: -4.6865e-01
Epoch 6/10
10/10 - 4s - loss: 460.2534 - loglik: -4.5981e+02 - logprior: -4.4059e-01
Epoch 7/10
10/10 - 4s - loss: 458.1982 - loglik: -4.5799e+02 - logprior: -2.1241e-01
Epoch 8/10
10/10 - 4s - loss: 457.3482 - loglik: -4.5727e+02 - logprior: -7.4103e-02
Epoch 9/10
10/10 - 4s - loss: 457.1078 - loglik: -4.5708e+02 - logprior: -2.4151e-02
Epoch 10/10
10/10 - 4s - loss: 455.6976 - loglik: -4.5578e+02 - logprior: 0.0812
Fitted a model with MAP estimate = -455.9403
expansions: [(14, 1), (15, 1), (16, 2), (28, 1), (29, 1), (30, 2), (32, 1), (36, 1), (40, 1), (41, 2), (42, 2), (50, 1), (51, 1), (54, 4), (62, 1), (80, 1), (90, 1), (113, 1), (114, 2), (115, 2), (116, 3), (118, 2), (119, 2), (120, 1), (125, 1), (161, 1), (164, 1), (166, 2), (167, 8)]
discards: [0]
Fitting a model of length 233 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 486.1414 - loglik: -4.5427e+02 - logprior: -3.1869e+01
Epoch 2/2
10/10 - 6s - loss: 452.7774 - loglik: -4.4213e+02 - logprior: -1.0645e+01
Fitted a model with MAP estimate = -447.2891
expansions: [(140, 1)]
discards: [  0  17  49  54 145 152]
Fitting a model of length 228 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 472.2856 - loglik: -4.4127e+02 - logprior: -3.1013e+01
Epoch 2/2
10/10 - 6s - loss: 450.1129 - loglik: -4.4052e+02 - logprior: -9.5977e+00
Fitted a model with MAP estimate = -444.1445
expansions: [(0, 4), (67, 3)]
discards: [0]
Fitting a model of length 234 on 1379 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 461.6959 - loglik: -4.3791e+02 - logprior: -2.3784e+01
Epoch 2/10
10/10 - 6s - loss: 438.7874 - loglik: -4.3615e+02 - logprior: -2.6415e+00
Epoch 3/10
10/10 - 6s - loss: 433.3297 - loglik: -4.3501e+02 - logprior: 1.6808
Epoch 4/10
10/10 - 6s - loss: 430.8644 - loglik: -4.3429e+02 - logprior: 3.4290
Epoch 5/10
10/10 - 6s - loss: 430.7554 - loglik: -4.3515e+02 - logprior: 4.3909
Epoch 6/10
10/10 - 6s - loss: 428.5117 - loglik: -4.3346e+02 - logprior: 4.9442
Epoch 7/10
10/10 - 6s - loss: 428.9052 - loglik: -4.3420e+02 - logprior: 5.2989
Fitted a model with MAP estimate = -428.6436
Time for alignment: 128.1420
Computed alignments with likelihoods: ['-427.0342', '-430.0202', '-428.6436']
Best model has likelihood: -427.0342
SP score = 0.8769
Training of 3 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c8a39a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2a1d22a30>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 736.1089 - loglik: -6.9553e+02 - logprior: -4.0575e+01
Epoch 2/10
11/11 - 8s - loss: 630.7056 - loglik: -6.2791e+02 - logprior: -2.7974e+00
Epoch 3/10
11/11 - 9s - loss: 541.4457 - loglik: -5.4333e+02 - logprior: 1.8874
Epoch 4/10
11/11 - 9s - loss: 494.3925 - loglik: -4.9702e+02 - logprior: 2.6230
Epoch 5/10
11/11 - 8s - loss: 478.6096 - loglik: -4.8137e+02 - logprior: 2.7574
Epoch 6/10
11/11 - 8s - loss: 471.7777 - loglik: -4.7480e+02 - logprior: 3.0240
Epoch 7/10
11/11 - 8s - loss: 467.6099 - loglik: -4.7102e+02 - logprior: 3.4095
Epoch 8/10
11/11 - 8s - loss: 469.3927 - loglik: -4.7306e+02 - logprior: 3.6670
Fitted a model with MAP estimate = -466.8204
expansions: [(22, 3), (25, 1), (36, 1), (48, 1), (53, 2), (54, 1), (64, 4), (65, 1), (78, 6), (79, 1), (80, 1), (102, 1), (106, 1), (129, 1), (136, 1), (161, 1), (163, 5), (168, 2), (180, 1), (181, 1), (182, 3), (198, 7), (199, 2), (200, 4), (222, 1), (224, 1), (225, 1)]
discards: [0]
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 511.9307 - loglik: -4.6467e+02 - logprior: -4.7265e+01
Epoch 2/2
11/11 - 12s - loss: 456.3029 - loglik: -4.4353e+02 - logprior: -1.2769e+01
Fitted a model with MAP estimate = -446.6410
expansions: [(0, 2), (217, 1), (239, 1)]
discards: [  0  59  75  76 192 193]
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 476.2735 - loglik: -4.4142e+02 - logprior: -3.4857e+01
Epoch 2/2
11/11 - 11s - loss: 435.8658 - loglik: -4.3583e+02 - logprior: -3.6178e-02
Fitted a model with MAP estimate = -428.7851
expansions: [(271, 1)]
discards: [  0 292]
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 483.6139 - loglik: -4.3886e+02 - logprior: -4.4754e+01
Epoch 2/10
11/11 - 11s - loss: 445.1955 - loglik: -4.3595e+02 - logprior: -9.2478e+00
Epoch 3/10
11/11 - 11s - loss: 428.0826 - loglik: -4.3136e+02 - logprior: 3.2754
Epoch 4/10
11/11 - 12s - loss: 422.1318 - loglik: -4.3238e+02 - logprior: 10.2515
Epoch 5/10
11/11 - 10s - loss: 418.3247 - loglik: -4.3067e+02 - logprior: 12.3442
Epoch 6/10
11/11 - 11s - loss: 417.5697 - loglik: -4.3092e+02 - logprior: 13.3507
Epoch 7/10
11/11 - 11s - loss: 413.7452 - loglik: -4.2805e+02 - logprior: 14.3040
Epoch 8/10
11/11 - 11s - loss: 417.8430 - loglik: -4.3289e+02 - logprior: 15.0423
Fitted a model with MAP estimate = -414.8329
Time for alignment: 230.7016
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 736.5414 - loglik: -6.9598e+02 - logprior: -4.0562e+01
Epoch 2/10
11/11 - 8s - loss: 632.5734 - loglik: -6.2978e+02 - logprior: -2.7945e+00
Epoch 3/10
11/11 - 8s - loss: 537.1469 - loglik: -5.3874e+02 - logprior: 1.5923
Epoch 4/10
11/11 - 8s - loss: 489.2879 - loglik: -4.9116e+02 - logprior: 1.8766
Epoch 5/10
11/11 - 8s - loss: 470.4375 - loglik: -4.7234e+02 - logprior: 1.9058
Epoch 6/10
11/11 - 8s - loss: 467.2950 - loglik: -4.6947e+02 - logprior: 2.1773
Epoch 7/10
11/11 - 8s - loss: 465.1555 - loglik: -4.6757e+02 - logprior: 2.4175
Epoch 8/10
11/11 - 8s - loss: 461.6536 - loglik: -4.6436e+02 - logprior: 2.7052
Epoch 9/10
11/11 - 8s - loss: 463.4600 - loglik: -4.6647e+02 - logprior: 3.0142
Fitted a model with MAP estimate = -461.8096
expansions: [(22, 3), (27, 1), (37, 1), (51, 1), (53, 2), (62, 1), (64, 4), (65, 1), (78, 1), (79, 1), (80, 1), (91, 1), (102, 1), (103, 4), (104, 1), (105, 1), (106, 1), (129, 1), (134, 1), (137, 1), (161, 2), (182, 1), (183, 1), (184, 2), (198, 1), (199, 7), (200, 2), (201, 5), (219, 1), (222, 1), (224, 1), (225, 1)]
discards: [0]
Fitting a model of length 297 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 508.4232 - loglik: -4.6133e+02 - logprior: -4.7090e+01
Epoch 2/2
11/11 - 11s - loss: 454.9739 - loglik: -4.4249e+02 - logprior: -1.2486e+01
Fitted a model with MAP estimate = -447.0469
expansions: [(0, 2), (242, 1)]
discards: [  0  59  75 123 124 158 274]
Fitting a model of length 293 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 477.4864 - loglik: -4.4267e+02 - logprior: -3.4819e+01
Epoch 2/2
11/11 - 10s - loss: 437.4773 - loglik: -4.3732e+02 - logprior: -1.5912e-01
Fitted a model with MAP estimate = -430.6762
expansions: [(189, 4), (233, 1)]
discards: [  0  75 289 290]
Fitting a model of length 294 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 486.4451 - loglik: -4.4179e+02 - logprior: -4.4657e+01
Epoch 2/10
11/11 - 11s - loss: 443.7477 - loglik: -4.3444e+02 - logprior: -9.3031e+00
Epoch 3/10
11/11 - 11s - loss: 431.9690 - loglik: -4.3524e+02 - logprior: 3.2728
Epoch 4/10
11/11 - 10s - loss: 423.3122 - loglik: -4.3338e+02 - logprior: 10.0727
Epoch 5/10
11/11 - 10s - loss: 419.4794 - loglik: -4.3163e+02 - logprior: 12.1533
Epoch 6/10
11/11 - 10s - loss: 415.7461 - loglik: -4.2885e+02 - logprior: 13.1069
Epoch 7/10
11/11 - 10s - loss: 418.9294 - loglik: -4.3285e+02 - logprior: 13.9229
Fitted a model with MAP estimate = -415.6070
Time for alignment: 218.8458
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 735.5134 - loglik: -6.9495e+02 - logprior: -4.0568e+01
Epoch 2/10
11/11 - 8s - loss: 630.8465 - loglik: -6.2804e+02 - logprior: -2.8097e+00
Epoch 3/10
11/11 - 8s - loss: 536.3088 - loglik: -5.3797e+02 - logprior: 1.6565
Epoch 4/10
11/11 - 8s - loss: 483.8022 - loglik: -4.8562e+02 - logprior: 1.8168
Epoch 5/10
11/11 - 8s - loss: 473.4109 - loglik: -4.7540e+02 - logprior: 1.9928
Epoch 6/10
11/11 - 8s - loss: 468.0763 - loglik: -4.7054e+02 - logprior: 2.4680
Epoch 7/10
11/11 - 9s - loss: 464.8269 - loglik: -4.6767e+02 - logprior: 2.8473
Epoch 8/10
11/11 - 8s - loss: 461.2953 - loglik: -4.6441e+02 - logprior: 3.1137
Epoch 9/10
11/11 - 9s - loss: 462.3195 - loglik: -4.6568e+02 - logprior: 3.3653
Fitted a model with MAP estimate = -461.1268
expansions: [(22, 4), (26, 1), (27, 1), (37, 1), (51, 1), (53, 2), (62, 1), (64, 4), (65, 2), (77, 2), (78, 3), (79, 1), (101, 3), (102, 1), (103, 1), (131, 3), (133, 1), (136, 1), (161, 1), (163, 5), (180, 1), (181, 1), (182, 1), (183, 2), (199, 7), (200, 2), (201, 5), (219, 1), (222, 1), (224, 1), (225, 1)]
discards: [0 1]
Fitting a model of length 304 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 508.2421 - loglik: -4.6129e+02 - logprior: -4.6948e+01
Epoch 2/2
11/11 - 12s - loss: 455.5217 - loglik: -4.4320e+02 - logprior: -1.2325e+01
Fitted a model with MAP estimate = -444.0326
expansions: [(0, 3)]
discards: [  0  20  76 123 124 158 159 160 281 298 301]
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 477.2872 - loglik: -4.4224e+02 - logprior: -3.5044e+01
Epoch 2/2
11/11 - 11s - loss: 437.3414 - loglik: -4.3685e+02 - logprior: -4.9262e-01
Fitted a model with MAP estimate = -429.7938
expansions: [(239, 1), (245, 1), (292, 1)]
discards: [ 0 61 77]
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 483.6886 - loglik: -4.3904e+02 - logprior: -4.4644e+01
Epoch 2/10
11/11 - 12s - loss: 442.9959 - loglik: -4.3387e+02 - logprior: -9.1245e+00
Epoch 3/10
11/11 - 12s - loss: 429.3488 - loglik: -4.3303e+02 - logprior: 3.6807
Epoch 4/10
11/11 - 11s - loss: 422.3204 - loglik: -4.3265e+02 - logprior: 10.3317
Epoch 5/10
11/11 - 11s - loss: 418.0872 - loglik: -4.3056e+02 - logprior: 12.4748
Epoch 6/10
11/11 - 10s - loss: 417.2293 - loglik: -4.3068e+02 - logprior: 13.4493
Epoch 7/10
11/11 - 12s - loss: 413.6707 - loglik: -4.2792e+02 - logprior: 14.2541
Epoch 8/10
11/11 - 11s - loss: 416.6071 - loglik: -4.3163e+02 - logprior: 15.0205
Fitted a model with MAP estimate = -414.2512
Time for alignment: 238.4120
Computed alignments with likelihoods: ['-414.8329', '-415.6070', '-414.2512']
Best model has likelihood: -414.2512
SP score = 0.9249
Training of 3 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe266691940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25daa0a60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 817.4249 - loglik: -8.1149e+02 - logprior: -5.9323e+00
Epoch 2/10
21/21 - 15s - loss: 692.2445 - loglik: -6.9154e+02 - logprior: -7.0659e-01
Epoch 3/10
21/21 - 15s - loss: 646.5394 - loglik: -6.4388e+02 - logprior: -2.6556e+00
Epoch 4/10
21/21 - 15s - loss: 634.5380 - loglik: -6.3192e+02 - logprior: -2.6168e+00
Epoch 5/10
21/21 - 15s - loss: 635.2101 - loglik: -6.3281e+02 - logprior: -2.4045e+00
Fitted a model with MAP estimate = -632.4603
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 3), (54, 2), (55, 2), (60, 2), (61, 1), (62, 1), (64, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 2), (81, 1), (83, 1), (85, 1), (86, 1), (89, 1), (94, 1), (100, 1), (101, 1), (102, 1), (109, 1), (116, 1), (130, 1), (135, 1), (138, 1), (141, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (170, 1), (180, 1), (183, 1), (187, 1), (189, 1), (190, 1), (191, 1), (193, 1), (194, 2), (195, 1), (196, 1), (208, 1), (209, 1), (212, 1), (215, 1), (219, 1), (223, 1), (229, 2), (230, 2), (232, 1), (236, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 3), (261, 1), (272, 2), (273, 1)]
discards: [0]
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 633.0127 - loglik: -6.2474e+02 - logprior: -8.2711e+00
Epoch 2/2
21/21 - 22s - loss: 611.5938 - loglik: -6.0951e+02 - logprior: -2.0861e+00
Fitted a model with MAP estimate = -606.3666
expansions: [(0, 3), (75, 1), (121, 1), (145, 1)]
discards: [  0  58 249 295 336]
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 613.0477 - loglik: -6.0827e+02 - logprior: -4.7785e+00
Epoch 2/2
21/21 - 23s - loss: 605.1216 - loglik: -6.0692e+02 - logprior: 1.7972
Fitted a model with MAP estimate = -601.0129
expansions: []
discards: [  1   2 338]
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 611.5718 - loglik: -6.0731e+02 - logprior: -4.2611e+00
Epoch 2/10
21/21 - 23s - loss: 604.8919 - loglik: -6.0729e+02 - logprior: 2.3952
Epoch 3/10
21/21 - 22s - loss: 597.8342 - loglik: -6.0084e+02 - logprior: 3.0067
Epoch 4/10
21/21 - 23s - loss: 598.8220 - loglik: -6.0225e+02 - logprior: 3.4301
Fitted a model with MAP estimate = -596.5463
Time for alignment: 330.2728
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 815.6335 - loglik: -8.0973e+02 - logprior: -5.9072e+00
Epoch 2/10
21/21 - 15s - loss: 694.5961 - loglik: -6.9394e+02 - logprior: -6.5179e-01
Epoch 3/10
21/21 - 15s - loss: 645.5015 - loglik: -6.4303e+02 - logprior: -2.4687e+00
Epoch 4/10
21/21 - 15s - loss: 636.2697 - loglik: -6.3379e+02 - logprior: -2.4786e+00
Epoch 5/10
21/21 - 15s - loss: 636.7210 - loglik: -6.3442e+02 - logprior: -2.3045e+00
Fitted a model with MAP estimate = -633.8468
expansions: [(13, 1), (14, 2), (15, 1), (50, 1), (52, 2), (56, 1), (61, 4), (62, 2), (65, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (80, 1), (81, 1), (82, 1), (84, 1), (86, 1), (87, 1), (90, 1), (93, 1), (94, 1), (95, 1), (102, 1), (103, 1), (110, 1), (112, 1), (114, 1), (130, 1), (136, 1), (137, 1), (138, 1), (154, 1), (155, 1), (156, 1), (158, 1), (159, 1), (161, 1), (162, 2), (181, 2), (184, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 1), (195, 1), (209, 1), (210, 1), (212, 1), (213, 1), (219, 1), (224, 1), (228, 1), (229, 2), (230, 2), (232, 1), (257, 1), (258, 3), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 627.4182 - loglik: -6.2178e+02 - logprior: -5.6382e+00
Epoch 2/2
21/21 - 23s - loss: 607.9313 - loglik: -6.0894e+02 - logprior: 1.0046
Fitted a model with MAP estimate = -603.4240
expansions: [(66, 1)]
discards: [293 332]
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 612.5419 - loglik: -6.0787e+02 - logprior: -4.6693e+00
Epoch 2/2
21/21 - 22s - loss: 602.4171 - loglik: -6.0425e+02 - logprior: 1.8377
Fitted a model with MAP estimate = -601.6111
expansions: [(56, 2)]
discards: []
Fitting a model of length 365 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 612.1845 - loglik: -6.0799e+02 - logprior: -4.1954e+00
Epoch 2/10
21/21 - 22s - loss: 602.1104 - loglik: -6.0439e+02 - logprior: 2.2762
Epoch 3/10
21/21 - 22s - loss: 602.1153 - loglik: -6.0511e+02 - logprior: 2.9979
Fitted a model with MAP estimate = -598.2084
Time for alignment: 305.4473
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 19s - loss: 814.7238 - loglik: -8.0882e+02 - logprior: -5.9036e+00
Epoch 2/10
21/21 - 15s - loss: 698.6159 - loglik: -6.9781e+02 - logprior: -8.0141e-01
Epoch 3/10
21/21 - 15s - loss: 642.7726 - loglik: -6.4016e+02 - logprior: -2.6115e+00
Epoch 4/10
21/21 - 15s - loss: 637.2179 - loglik: -6.3483e+02 - logprior: -2.3887e+00
Epoch 5/10
21/21 - 15s - loss: 634.8134 - loglik: -6.3253e+02 - logprior: -2.2844e+00
Epoch 6/10
21/21 - 15s - loss: 632.0126 - loglik: -6.2969e+02 - logprior: -2.3270e+00
Epoch 7/10
21/21 - 15s - loss: 633.3810 - loglik: -6.3098e+02 - logprior: -2.3994e+00
Fitted a model with MAP estimate = -632.5852
expansions: [(13, 1), (14, 2), (15, 1), (43, 1), (52, 2), (54, 3), (55, 2), (60, 2), (61, 1), (62, 1), (64, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 2), (82, 1), (84, 1), (85, 2), (86, 1), (89, 1), (92, 1), (93, 1), (94, 1), (101, 1), (110, 1), (112, 1), (114, 1), (116, 1), (134, 1), (135, 1), (138, 1), (141, 1), (152, 1), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 1), (181, 1), (184, 1), (187, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 2), (195, 1), (196, 1), (198, 1), (209, 1), (212, 1), (214, 1), (219, 1), (223, 1), (229, 2), (230, 2), (232, 1), (236, 1), (256, 1), (257, 1), (258, 1), (259, 2), (261, 1), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Fitting a model of length 367 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 628.2510 - loglik: -6.2268e+02 - logprior: -5.5736e+00
Epoch 2/2
21/21 - 23s - loss: 607.1308 - loglik: -6.0827e+02 - logprior: 1.1381
Fitted a model with MAP estimate = -602.5202
expansions: [(75, 1)]
discards: [251 297]
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 613.2858 - loglik: -6.0873e+02 - logprior: -4.5509e+00
Epoch 2/2
21/21 - 23s - loss: 601.7808 - loglik: -6.0380e+02 - logprior: 2.0175
Fitted a model with MAP estimate = -600.9732
expansions: []
discards: [56 57]
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 613.5162 - loglik: -6.0943e+02 - logprior: -4.0883e+00
Epoch 2/10
21/21 - 22s - loss: 602.4885 - loglik: -6.0486e+02 - logprior: 2.3705
Epoch 3/10
21/21 - 23s - loss: 599.1214 - loglik: -6.0218e+02 - logprior: 3.0612
Epoch 4/10
21/21 - 22s - loss: 597.8126 - loglik: -6.0133e+02 - logprior: 3.5224
Epoch 5/10
21/21 - 22s - loss: 593.5848 - loglik: -5.9733e+02 - logprior: 3.7406
Epoch 6/10
21/21 - 23s - loss: 598.3930 - loglik: -6.0240e+02 - logprior: 4.0083
Fitted a model with MAP estimate = -594.8231
Time for alignment: 404.8646
Computed alignments with likelihoods: ['-596.5463', '-598.2084', '-594.8231']
Best model has likelihood: -594.8231
SP score = 0.9655
Training of 3 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2871340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe24ac90370>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 838.9184 - loglik: -8.3720e+02 - logprior: -1.7171e+00
Epoch 2/10
39/39 - 20s - loss: 741.6293 - loglik: -7.4017e+02 - logprior: -1.4618e+00
Epoch 3/10
39/39 - 20s - loss: 729.2969 - loglik: -7.2763e+02 - logprior: -1.6665e+00
Epoch 4/10
39/39 - 20s - loss: 725.7991 - loglik: -7.2410e+02 - logprior: -1.6946e+00
Epoch 5/10
39/39 - 20s - loss: 724.6256 - loglik: -7.2292e+02 - logprior: -1.7097e+00
Epoch 6/10
39/39 - 20s - loss: 723.6730 - loglik: -7.2193e+02 - logprior: -1.7399e+00
Epoch 7/10
39/39 - 20s - loss: 723.8552 - loglik: -7.2212e+02 - logprior: -1.7344e+00
Fitted a model with MAP estimate = -624.0760
expansions: [(14, 1), (19, 1), (41, 1), (44, 1), (76, 1), (80, 3), (81, 2), (82, 2), (94, 1), (99, 3), (102, 1), (109, 1), (118, 1), (120, 3), (121, 3), (122, 1), (145, 1), (150, 1), (151, 1), (162, 1), (169, 3), (170, 5), (171, 2), (172, 3), (174, 1), (175, 1), (176, 1), (179, 2), (180, 3), (181, 2), (182, 1), (186, 2), (187, 1), (188, 1), (189, 1), (192, 4), (193, 1), (207, 3), (208, 1), (215, 1), (225, 1), (244, 3)]
discards: [  0 103 104 154 155 156 158 159 160]
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 720.9514 - loglik: -7.1804e+02 - logprior: -2.9123e+00
Epoch 2/2
39/39 - 28s - loss: 706.0889 - loglik: -7.0439e+02 - logprior: -1.6992e+00
Fitted a model with MAP estimate = -606.1284
expansions: [(0, 2), (119, 4), (125, 3), (191, 1), (218, 1), (220, 2), (308, 2)]
discards: [  0 112 113 120 121 122 123 137 180 221 222 223 231 244 263 271 305 306
 307]
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 708.7769 - loglik: -7.0692e+02 - logprior: -1.8528e+00
Epoch 2/2
39/39 - 28s - loss: 701.8138 - loglik: -7.0113e+02 - logprior: -6.8185e-01
Fitted a model with MAP estimate = -603.5822
expansions: [(121, 2), (269, 4)]
discards: [  0 196 302 303]
Fitting a model of length 306 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 606.0809 - loglik: -6.0400e+02 - logprior: -2.0767e+00
Epoch 2/10
43/43 - 30s - loss: 603.2512 - loglik: -6.0263e+02 - logprior: -6.1909e-01
Epoch 3/10
43/43 - 31s - loss: 596.2484 - loglik: -5.9586e+02 - logprior: -3.8737e-01
Epoch 4/10
43/43 - 31s - loss: 596.6397 - loglik: -5.9631e+02 - logprior: -3.3117e-01
Fitted a model with MAP estimate = -595.3532
Time for alignment: 536.4947
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 841.8889 - loglik: -8.4019e+02 - logprior: -1.6965e+00
Epoch 2/10
39/39 - 20s - loss: 742.4493 - loglik: -7.4098e+02 - logprior: -1.4657e+00
Epoch 3/10
39/39 - 20s - loss: 730.5250 - loglik: -7.2895e+02 - logprior: -1.5745e+00
Epoch 4/10
39/39 - 20s - loss: 725.8500 - loglik: -7.2423e+02 - logprior: -1.6159e+00
Epoch 5/10
39/39 - 20s - loss: 723.7668 - loglik: -7.2213e+02 - logprior: -1.6381e+00
Epoch 6/10
39/39 - 20s - loss: 722.8472 - loglik: -7.2119e+02 - logprior: -1.6617e+00
Epoch 7/10
39/39 - 20s - loss: 722.4919 - loglik: -7.2083e+02 - logprior: -1.6608e+00
Epoch 8/10
39/39 - 20s - loss: 722.6699 - loglik: -7.2098e+02 - logprior: -1.6857e+00
Fitted a model with MAP estimate = -623.8992
expansions: [(14, 1), (19, 1), (40, 1), (42, 1), (52, 1), (54, 1), (74, 1), (78, 4), (79, 2), (80, 2), (90, 3), (92, 3), (100, 1), (103, 3), (107, 1), (116, 1), (118, 3), (119, 3), (145, 1), (147, 7), (158, 1), (159, 1), (160, 1), (161, 1), (163, 1), (171, 1), (172, 1), (177, 1), (178, 1), (185, 1), (186, 2), (187, 4), (188, 2), (207, 1), (208, 1), (215, 1), (219, 1), (244, 3)]
discards: [  0  33 101 165 166 167 193 194]
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 719.6826 - loglik: -7.1667e+02 - logprior: -3.0108e+00
Epoch 2/2
39/39 - 27s - loss: 705.6349 - loglik: -7.0371e+02 - logprior: -1.9294e+00
Fitted a model with MAP estimate = -606.2892
expansions: [(0, 2), (122, 2), (200, 1), (232, 1), (244, 3), (245, 2)]
discards: [  0  91 109 117 118 123 124 125 126 127 128 143 235 236 265 299 300 301]
Fitting a model of length 295 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 709.7767 - loglik: -7.0788e+02 - logprior: -1.8932e+00
Epoch 2/2
39/39 - 27s - loss: 704.3233 - loglik: -7.0365e+02 - logprior: -6.6880e-01
Fitted a model with MAP estimate = -606.0228
expansions: [(114, 1), (115, 2), (122, 4), (262, 3), (295, 3)]
discards: [  0 103 104]
Fitting a model of length 305 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 34s - loss: 608.3453 - loglik: -6.0625e+02 - logprior: -2.0978e+00
Epoch 2/10
43/43 - 30s - loss: 599.1166 - loglik: -5.9842e+02 - logprior: -6.9334e-01
Epoch 3/10
43/43 - 30s - loss: 601.2828 - loglik: -6.0079e+02 - logprior: -4.9109e-01
Fitted a model with MAP estimate = -597.5478
Time for alignment: 515.7483
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 837.7250 - loglik: -8.3603e+02 - logprior: -1.6952e+00
Epoch 2/10
39/39 - 20s - loss: 740.8348 - loglik: -7.3945e+02 - logprior: -1.3817e+00
Epoch 3/10
39/39 - 20s - loss: 729.5475 - loglik: -7.2804e+02 - logprior: -1.5060e+00
Epoch 4/10
39/39 - 20s - loss: 725.8290 - loglik: -7.2425e+02 - logprior: -1.5762e+00
Epoch 5/10
39/39 - 20s - loss: 724.3149 - loglik: -7.2274e+02 - logprior: -1.5716e+00
Epoch 6/10
39/39 - 20s - loss: 723.9818 - loglik: -7.2240e+02 - logprior: -1.5784e+00
Epoch 7/10
39/39 - 20s - loss: 723.3214 - loglik: -7.2175e+02 - logprior: -1.5676e+00
Epoch 8/10
39/39 - 20s - loss: 723.6849 - loglik: -7.2212e+02 - logprior: -1.5691e+00
Fitted a model with MAP estimate = -624.1301
expansions: [(14, 1), (30, 1), (41, 1), (56, 1), (81, 5), (83, 1), (95, 1), (100, 1), (102, 1), (104, 1), (111, 1), (120, 1), (121, 4), (122, 1), (123, 1), (129, 2), (152, 4), (162, 4), (164, 2), (170, 2), (171, 5), (172, 4), (174, 1), (175, 1), (176, 1), (180, 2), (181, 3), (187, 1), (188, 1), (189, 2), (192, 2), (208, 2), (209, 1), (213, 4), (215, 1), (218, 1), (244, 3)]
discards: [  0 107 108 109]
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 32s - loss: 720.4964 - loglik: -7.1745e+02 - logprior: -3.0499e+00
Epoch 2/2
39/39 - 29s - loss: 705.0301 - loglik: -7.0312e+02 - logprior: -1.9080e+00
Fitted a model with MAP estimate = -605.3566
expansions: [(0, 2), (119, 3), (120, 1), (123, 3), (189, 1), (202, 1), (203, 1), (211, 1), (223, 3)]
discards: [  0 117 134 147 176 177 178 179 180 181 182 183 184 191 241 271 272 273
 308 309 310]
Fitting a model of length 306 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 705.6728 - loglik: -7.0379e+02 - logprior: -1.8875e+00
Epoch 2/2
39/39 - 28s - loss: 698.1319 - loglik: -6.9747e+02 - logprior: -6.5798e-01
Fitted a model with MAP estimate = -600.8947
expansions: [(210, 1), (306, 3)]
discards: [  0 118 119 120 121 122 123 124 125]
Fitting a model of length 301 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 609.5143 - loglik: -6.0744e+02 - logprior: -2.0743e+00
Epoch 2/10
43/43 - 30s - loss: 604.2723 - loglik: -6.0346e+02 - logprior: -8.1608e-01
Epoch 3/10
43/43 - 29s - loss: 599.8650 - loglik: -5.9938e+02 - logprior: -4.8508e-01
Epoch 4/10
43/43 - 30s - loss: 598.7101 - loglik: -5.9833e+02 - logprior: -3.7779e-01
Epoch 5/10
43/43 - 30s - loss: 599.7946 - loglik: -5.9947e+02 - logprior: -3.2004e-01
Fitted a model with MAP estimate = -597.8501
Time for alignment: 584.3904
Computed alignments with likelihoods: ['-595.3532', '-597.5478', '-597.8501']
Best model has likelihood: -595.3532
SP score = 0.5117
Training of 3 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe288550a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe277299730>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.9233 - loglik: -2.3679e+02 - logprior: -3.8136e+01
Epoch 2/10
10/10 - 1s - loss: 222.7748 - loglik: -2.1271e+02 - logprior: -1.0060e+01
Epoch 3/10
10/10 - 1s - loss: 196.3479 - loglik: -1.9129e+02 - logprior: -5.0576e+00
Epoch 4/10
10/10 - 1s - loss: 180.3070 - loglik: -1.7691e+02 - logprior: -3.3972e+00
Epoch 5/10
10/10 - 1s - loss: 173.8741 - loglik: -1.7134e+02 - logprior: -2.5351e+00
Epoch 6/10
10/10 - 1s - loss: 171.3289 - loglik: -1.6925e+02 - logprior: -2.0835e+00
Epoch 7/10
10/10 - 1s - loss: 169.8170 - loglik: -1.6788e+02 - logprior: -1.9384e+00
Epoch 8/10
10/10 - 1s - loss: 169.0810 - loglik: -1.6722e+02 - logprior: -1.8648e+00
Epoch 9/10
10/10 - 1s - loss: 168.3365 - loglik: -1.6652e+02 - logprior: -1.8140e+00
Epoch 10/10
10/10 - 1s - loss: 167.8742 - loglik: -1.6614e+02 - logprior: -1.7320e+00
Fitted a model with MAP estimate = -167.8845
expansions: [(7, 2), (12, 1), (15, 2), (18, 1), (20, 2), (24, 3), (30, 1), (32, 1), (43, 1), (55, 2), (58, 4)]
discards: [0]
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 207.4184 - loglik: -1.6466e+02 - logprior: -4.2757e+01
Epoch 2/2
10/10 - 1s - loss: 173.2195 - loglik: -1.5594e+02 - logprior: -1.7278e+01
Fitted a model with MAP estimate = -167.6743
expansions: [(0, 2)]
discards: [ 0 18]
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 186.4168 - loglik: -1.5260e+02 - logprior: -3.3819e+01
Epoch 2/2
10/10 - 1s - loss: 159.0796 - loglik: -1.5052e+02 - logprior: -8.5577e+00
Fitted a model with MAP estimate = -155.4516
expansions: []
discards: [ 0 26]
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.5888 - loglik: -1.5331e+02 - logprior: -3.9279e+01
Epoch 2/10
10/10 - 1s - loss: 162.7139 - loglik: -1.5215e+02 - logprior: -1.0559e+01
Epoch 3/10
10/10 - 1s - loss: 155.9825 - loglik: -1.5210e+02 - logprior: -3.8809e+00
Epoch 4/10
10/10 - 1s - loss: 152.7436 - loglik: -1.5105e+02 - logprior: -1.6925e+00
Epoch 5/10
10/10 - 1s - loss: 151.5951 - loglik: -1.5083e+02 - logprior: -7.6728e-01
Epoch 6/10
10/10 - 1s - loss: 150.6306 - loglik: -1.5045e+02 - logprior: -1.7917e-01
Epoch 7/10
10/10 - 1s - loss: 150.7917 - loglik: -1.5113e+02 - logprior: 0.3363
Fitted a model with MAP estimate = -150.3945
Time for alignment: 41.5710
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 274.6779 - loglik: -2.3654e+02 - logprior: -3.8137e+01
Epoch 2/10
10/10 - 1s - loss: 223.3989 - loglik: -2.1333e+02 - logprior: -1.0065e+01
Epoch 3/10
10/10 - 1s - loss: 196.6344 - loglik: -1.9154e+02 - logprior: -5.0966e+00
Epoch 4/10
10/10 - 1s - loss: 180.8457 - loglik: -1.7744e+02 - logprior: -3.4088e+00
Epoch 5/10
10/10 - 1s - loss: 173.2059 - loglik: -1.7066e+02 - logprior: -2.5478e+00
Epoch 6/10
10/10 - 1s - loss: 170.3329 - loglik: -1.6805e+02 - logprior: -2.2860e+00
Epoch 7/10
10/10 - 1s - loss: 169.3532 - loglik: -1.6723e+02 - logprior: -2.1274e+00
Epoch 8/10
10/10 - 1s - loss: 168.3210 - loglik: -1.6654e+02 - logprior: -1.7779e+00
Epoch 9/10
10/10 - 1s - loss: 167.8956 - loglik: -1.6632e+02 - logprior: -1.5796e+00
Epoch 10/10
10/10 - 1s - loss: 167.3964 - loglik: -1.6586e+02 - logprior: -1.5335e+00
Fitted a model with MAP estimate = -167.2524
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (24, 3), (30, 1), (32, 1), (46, 1), (55, 2), (58, 4)]
discards: []
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.9839 - loglik: -1.6204e+02 - logprior: -4.9942e+01
Epoch 2/2
10/10 - 1s - loss: 168.4275 - loglik: -1.5345e+02 - logprior: -1.4978e+01
Fitted a model with MAP estimate = -160.6915
expansions: []
discards: [ 0 18 26]
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 197.6593 - loglik: -1.5442e+02 - logprior: -4.3236e+01
Epoch 2/2
10/10 - 1s - loss: 170.6067 - loglik: -1.5381e+02 - logprior: -1.6801e+01
Fitted a model with MAP estimate = -165.7535
expansions: []
discards: []
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.2655 - loglik: -1.5272e+02 - logprior: -3.9543e+01
Epoch 2/10
10/10 - 1s - loss: 162.8949 - loglik: -1.5218e+02 - logprior: -1.0710e+01
Epoch 3/10
10/10 - 1s - loss: 155.7724 - loglik: -1.5190e+02 - logprior: -3.8752e+00
Epoch 4/10
10/10 - 1s - loss: 152.4628 - loglik: -1.5081e+02 - logprior: -1.6518e+00
Epoch 5/10
10/10 - 1s - loss: 151.6585 - loglik: -1.5093e+02 - logprior: -7.2679e-01
Epoch 6/10
10/10 - 1s - loss: 151.0929 - loglik: -1.5096e+02 - logprior: -1.3511e-01
Epoch 7/10
10/10 - 1s - loss: 150.5688 - loglik: -1.5096e+02 - logprior: 0.3935
Epoch 8/10
10/10 - 1s - loss: 150.3602 - loglik: -1.5110e+02 - logprior: 0.7404
Epoch 9/10
10/10 - 1s - loss: 149.8604 - loglik: -1.5079e+02 - logprior: 0.9315
Epoch 10/10
10/10 - 1s - loss: 150.0405 - loglik: -1.5111e+02 - logprior: 1.0687
Fitted a model with MAP estimate = -149.9036
Time for alignment: 43.6606
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 274.6505 - loglik: -2.3652e+02 - logprior: -3.8134e+01
Epoch 2/10
10/10 - 1s - loss: 223.3900 - loglik: -2.1333e+02 - logprior: -1.0057e+01
Epoch 3/10
10/10 - 1s - loss: 197.0536 - loglik: -1.9196e+02 - logprior: -5.0890e+00
Epoch 4/10
10/10 - 1s - loss: 181.3369 - loglik: -1.7797e+02 - logprior: -3.3716e+00
Epoch 5/10
10/10 - 1s - loss: 173.6729 - loglik: -1.7116e+02 - logprior: -2.5167e+00
Epoch 6/10
10/10 - 1s - loss: 170.4729 - loglik: -1.6820e+02 - logprior: -2.2697e+00
Epoch 7/10
10/10 - 1s - loss: 169.4341 - loglik: -1.6726e+02 - logprior: -2.1729e+00
Epoch 8/10
10/10 - 1s - loss: 168.4958 - loglik: -1.6663e+02 - logprior: -1.8685e+00
Epoch 9/10
10/10 - 1s - loss: 168.0739 - loglik: -1.6646e+02 - logprior: -1.6137e+00
Epoch 10/10
10/10 - 1s - loss: 167.6483 - loglik: -1.6611e+02 - logprior: -1.5402e+00
Fitted a model with MAP estimate = -167.4450
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (24, 3), (30, 1), (38, 1), (46, 1), (55, 2), (58, 4)]
discards: []
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 212.3294 - loglik: -1.6240e+02 - logprior: -4.9927e+01
Epoch 2/2
10/10 - 1s - loss: 168.8159 - loglik: -1.5385e+02 - logprior: -1.4967e+01
Fitted a model with MAP estimate = -160.7868
expansions: []
discards: [ 0 18 26]
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 197.8078 - loglik: -1.5457e+02 - logprior: -4.3238e+01
Epoch 2/2
10/10 - 1s - loss: 170.3886 - loglik: -1.5359e+02 - logprior: -1.6794e+01
Fitted a model with MAP estimate = -165.7590
expansions: []
discards: []
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 192.6392 - loglik: -1.5309e+02 - logprior: -3.9545e+01
Epoch 2/10
10/10 - 1s - loss: 162.7183 - loglik: -1.5201e+02 - logprior: -1.0713e+01
Epoch 3/10
10/10 - 1s - loss: 155.5094 - loglik: -1.5163e+02 - logprior: -3.8808e+00
Epoch 4/10
10/10 - 1s - loss: 152.5108 - loglik: -1.5085e+02 - logprior: -1.6572e+00
Epoch 5/10
10/10 - 1s - loss: 151.6496 - loglik: -1.5091e+02 - logprior: -7.3654e-01
Epoch 6/10
10/10 - 1s - loss: 150.8720 - loglik: -1.5074e+02 - logprior: -1.3647e-01
Epoch 7/10
10/10 - 1s - loss: 150.6988 - loglik: -1.5109e+02 - logprior: 0.3885
Epoch 8/10
10/10 - 1s - loss: 150.1306 - loglik: -1.5087e+02 - logprior: 0.7387
Epoch 9/10
10/10 - 1s - loss: 150.0544 - loglik: -1.5098e+02 - logprior: 0.9284
Epoch 10/10
10/10 - 1s - loss: 150.1180 - loglik: -1.5118e+02 - logprior: 1.0648
Fitted a model with MAP estimate = -149.9042
Time for alignment: 44.5155
Computed alignments with likelihoods: ['-150.3945', '-149.9036', '-149.9042']
Best model has likelihood: -149.9036
SP score = 0.9223
Training of 3 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2772578e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fa176d00>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 323.1554 - loglik: -3.0633e+02 - logprior: -1.6830e+01
Epoch 2/10
10/10 - 3s - loss: 278.0815 - loglik: -2.7380e+02 - logprior: -4.2835e+00
Epoch 3/10
10/10 - 4s - loss: 246.6759 - loglik: -2.4443e+02 - logprior: -2.2503e+00
Epoch 4/10
10/10 - 3s - loss: 228.1789 - loglik: -2.2628e+02 - logprior: -1.9019e+00
Epoch 5/10
10/10 - 4s - loss: 222.4248 - loglik: -2.2060e+02 - logprior: -1.8247e+00
Epoch 6/10
10/10 - 4s - loss: 218.4432 - loglik: -2.1670e+02 - logprior: -1.7398e+00
Epoch 7/10
10/10 - 3s - loss: 218.4995 - loglik: -2.1686e+02 - logprior: -1.6372e+00
Fitted a model with MAP estimate = -217.4690
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 1), (27, 1), (43, 1), (47, 4), (48, 4), (49, 1), (57, 1), (72, 1), (73, 4), (74, 2)]
discards: [0]
Fitting a model of length 113 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 233.9405 - loglik: -2.1518e+02 - logprior: -1.8760e+01
Epoch 2/2
10/10 - 4s - loss: 212.2858 - loglik: -2.0465e+02 - logprior: -7.6345e+00
Fitted a model with MAP estimate = -208.1565
expansions: [(0, 2)]
discards: [ 0 12 14 58 59 61 93]
Fitting a model of length 108 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 216.2517 - loglik: -2.0149e+02 - logprior: -1.4766e+01
Epoch 2/2
10/10 - 5s - loss: 202.8520 - loglik: -1.9929e+02 - logprior: -3.5660e+00
Fitted a model with MAP estimate = -200.6720
expansions: []
discards: [0]
Fitting a model of length 107 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 218.7139 - loglik: -2.0127e+02 - logprior: -1.7442e+01
Epoch 2/10
10/10 - 5s - loss: 205.4814 - loglik: -2.0077e+02 - logprior: -4.7082e+00
Epoch 3/10
10/10 - 5s - loss: 201.2878 - loglik: -1.9939e+02 - logprior: -1.8990e+00
Epoch 4/10
10/10 - 4s - loss: 199.6384 - loglik: -1.9854e+02 - logprior: -1.1024e+00
Epoch 5/10
10/10 - 5s - loss: 199.3259 - loglik: -1.9886e+02 - logprior: -4.6217e-01
Epoch 6/10
10/10 - 4s - loss: 198.7554 - loglik: -1.9860e+02 - logprior: -1.5297e-01
Epoch 7/10
10/10 - 4s - loss: 198.0099 - loglik: -1.9796e+02 - logprior: -5.4696e-02
Epoch 8/10
10/10 - 5s - loss: 198.8752 - loglik: -1.9895e+02 - logprior: 0.0764
Fitted a model with MAP estimate = -198.3582
Time for alignment: 107.9556
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 323.1090 - loglik: -3.0628e+02 - logprior: -1.6831e+01
Epoch 2/10
10/10 - 4s - loss: 278.5532 - loglik: -2.7427e+02 - logprior: -4.2838e+00
Epoch 3/10
10/10 - 4s - loss: 247.1142 - loglik: -2.4480e+02 - logprior: -2.3140e+00
Epoch 4/10
10/10 - 4s - loss: 229.7700 - loglik: -2.2772e+02 - logprior: -2.0485e+00
Epoch 5/10
10/10 - 5s - loss: 221.1802 - loglik: -2.1920e+02 - logprior: -1.9813e+00
Epoch 6/10
10/10 - 5s - loss: 218.1264 - loglik: -2.1627e+02 - logprior: -1.8526e+00
Epoch 7/10
10/10 - 4s - loss: 217.7997 - loglik: -2.1612e+02 - logprior: -1.6808e+00
Epoch 8/10
10/10 - 3s - loss: 215.4765 - loglik: -2.1393e+02 - logprior: -1.5499e+00
Epoch 9/10
10/10 - 4s - loss: 216.1106 - loglik: -2.1461e+02 - logprior: -1.5005e+00
Fitted a model with MAP estimate = -215.7435
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 1), (29, 1), (43, 1), (47, 4), (48, 4), (49, 1), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Fitting a model of length 112 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 232.1685 - loglik: -2.1341e+02 - logprior: -1.8757e+01
Epoch 2/2
10/10 - 5s - loss: 211.3012 - loglik: -2.0371e+02 - logprior: -7.5927e+00
Fitted a model with MAP estimate = -207.6422
expansions: [(0, 2)]
discards: [ 0 12 14 58 59 61]
Fitting a model of length 108 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 215.7949 - loglik: -2.0105e+02 - logprior: -1.4743e+01
Epoch 2/2
10/10 - 5s - loss: 202.5898 - loglik: -1.9905e+02 - logprior: -3.5405e+00
Fitted a model with MAP estimate = -200.5959
expansions: []
discards: [0]
Fitting a model of length 107 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 219.1439 - loglik: -2.0172e+02 - logprior: -1.7425e+01
Epoch 2/10
10/10 - 5s - loss: 205.0265 - loglik: -2.0033e+02 - logprior: -4.6941e+00
Epoch 3/10
10/10 - 4s - loss: 200.4515 - loglik: -1.9857e+02 - logprior: -1.8797e+00
Epoch 4/10
10/10 - 5s - loss: 200.4617 - loglik: -1.9938e+02 - logprior: -1.0768e+00
Fitted a model with MAP estimate = -199.1823
Time for alignment: 99.1415
Fitting a model of length 87 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 323.0440 - loglik: -3.0621e+02 - logprior: -1.6830e+01
Epoch 2/10
10/10 - 4s - loss: 277.6239 - loglik: -2.7334e+02 - logprior: -4.2793e+00
Epoch 3/10
10/10 - 4s - loss: 246.7789 - loglik: -2.4453e+02 - logprior: -2.2450e+00
Epoch 4/10
10/10 - 5s - loss: 229.8763 - loglik: -2.2798e+02 - logprior: -1.8946e+00
Epoch 5/10
10/10 - 4s - loss: 222.5254 - loglik: -2.2064e+02 - logprior: -1.8879e+00
Epoch 6/10
10/10 - 3s - loss: 218.8861 - loglik: -2.1700e+02 - logprior: -1.8823e+00
Epoch 7/10
10/10 - 4s - loss: 218.2699 - loglik: -2.1649e+02 - logprior: -1.7749e+00
Epoch 8/10
10/10 - 4s - loss: 217.3627 - loglik: -2.1565e+02 - logprior: -1.7089e+00
Epoch 9/10
10/10 - 4s - loss: 215.8428 - loglik: -2.1415e+02 - logprior: -1.6942e+00
Epoch 10/10
10/10 - 3s - loss: 214.6156 - loglik: -2.1290e+02 - logprior: -1.7119e+00
Fitted a model with MAP estimate = -215.2807
expansions: [(10, 3), (11, 2), (12, 1), (15, 1), (26, 1), (27, 1), (44, 1), (47, 4), (48, 4), (49, 1), (50, 2), (60, 1), (71, 1), (72, 1), (73, 1), (74, 2)]
discards: [0]
Fitting a model of length 113 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 233.1384 - loglik: -2.1437e+02 - logprior: -1.8768e+01
Epoch 2/2
10/10 - 5s - loss: 211.5029 - loglik: -2.0393e+02 - logprior: -7.5752e+00
Fitted a model with MAP estimate = -207.7278
expansions: [(0, 2)]
discards: [ 0 12 14 59 61 68]
Fitting a model of length 109 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 215.6758 - loglik: -2.0093e+02 - logprior: -1.4742e+01
Epoch 2/2
10/10 - 5s - loss: 202.0461 - loglik: -1.9851e+02 - logprior: -3.5385e+00
Fitted a model with MAP estimate = -200.2498
expansions: []
discards: [ 0 57]
Fitting a model of length 107 on 2589 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 218.5819 - loglik: -2.0116e+02 - logprior: -1.7424e+01
Epoch 2/10
10/10 - 4s - loss: 205.4246 - loglik: -2.0068e+02 - logprior: -4.7473e+00
Epoch 3/10
10/10 - 5s - loss: 201.0865 - loglik: -1.9908e+02 - logprior: -2.0085e+00
Epoch 4/10
10/10 - 3s - loss: 200.0432 - loglik: -1.9881e+02 - logprior: -1.2319e+00
Epoch 5/10
10/10 - 4s - loss: 199.0046 - loglik: -1.9844e+02 - logprior: -5.6653e-01
Epoch 6/10
10/10 - 4s - loss: 198.4151 - loglik: -1.9813e+02 - logprior: -2.8520e-01
Epoch 7/10
10/10 - 4s - loss: 198.5737 - loglik: -1.9840e+02 - logprior: -1.7628e-01
Fitted a model with MAP estimate = -198.4553
Time for alignment: 112.5459
Computed alignments with likelihoods: ['-198.3582', '-199.1823', '-198.4553']
Best model has likelihood: -198.3582
SP score = 0.2131
Training of 3 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe24ad5b610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25dac5820>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 43s - loss: 1196.9838 - loglik: -1.1935e+03 - logprior: -3.5164e+00
Epoch 2/10
24/24 - 40s - loss: 1001.4387 - loglik: -1.0022e+03 - logprior: 0.7335
Epoch 3/10
24/24 - 40s - loss: 952.0078 - loglik: -9.5198e+02 - logprior: -2.7418e-02
Epoch 4/10
24/24 - 39s - loss: 954.7978 - loglik: -9.5480e+02 - logprior: -1.3912e-03
Fitted a model with MAP estimate = -948.8340
expansions: [(0, 5), (36, 3), (132, 1), (140, 1), (193, 1), (213, 1), (214, 1), (231, 1), (232, 2), (233, 2), (234, 2), (235, 1), (238, 1), (243, 2), (245, 1), (246, 1), (251, 1), (253, 1), (254, 1), (255, 2), (256, 8), (257, 2), (273, 2), (274, 3), (275, 1), (276, 2), (293, 2), (294, 2), (295, 2), (296, 3), (297, 5), (298, 2), (299, 7), (300, 2), (308, 2), (311, 4), (312, 2), (313, 1), (322, 1), (323, 1), (324, 1), (345, 5), (346, 1), (347, 1), (395, 4), (396, 1), (398, 2), (399, 10), (400, 1), (404, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 86]
Fitting a model of length 502 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 60s - loss: 947.6251 - loglik: -9.4204e+02 - logprior: -5.5816e+00
Epoch 2/2
24/24 - 57s - loss: 930.5696 - loglik: -9.3140e+02 - logprior: 0.8309
Fitted a model with MAP estimate = -921.7969
expansions: [(0, 7)]
discards: [  1   2   3   4   5   6  23  24  25 230 337 338 343 344 468 496 497 498
 499 500 501]
Fitting a model of length 488 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 57s - loss: 937.3152 - loglik: -9.3204e+02 - logprior: -5.2750e+00
Epoch 2/2
24/24 - 55s - loss: 924.2996 - loglik: -9.2585e+02 - logprior: 1.5545
Fitted a model with MAP estimate = -922.1485
expansions: [(0, 6), (322, 2), (476, 1)]
discards: [  0   1   2   3   4   5   6 484 485 486 487]
Fitting a model of length 486 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 56s - loss: 937.3093 - loglik: -9.3373e+02 - logprior: -3.5834e+00
Epoch 2/10
24/24 - 54s - loss: 921.6924 - loglik: -9.2421e+02 - logprior: 2.5127
Epoch 3/10
24/24 - 54s - loss: 923.5867 - loglik: -9.2683e+02 - logprior: 3.2413
Fitted a model with MAP estimate = -920.1075
Time for alignment: 728.2676
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 42s - loss: 1193.2004 - loglik: -1.1898e+03 - logprior: -3.4134e+00
Epoch 2/10
24/24 - 40s - loss: 1004.7973 - loglik: -1.0055e+03 - logprior: 0.7079
Epoch 3/10
24/24 - 40s - loss: 957.1323 - loglik: -9.5726e+02 - logprior: 0.1326
Epoch 4/10
24/24 - 40s - loss: 952.1335 - loglik: -9.5235e+02 - logprior: 0.2204
Epoch 5/10
24/24 - 40s - loss: 949.3165 - loglik: -9.4953e+02 - logprior: 0.2098
Epoch 6/10
24/24 - 40s - loss: 948.4470 - loglik: -9.4868e+02 - logprior: 0.2329
Epoch 7/10
24/24 - 40s - loss: 945.8828 - loglik: -9.4608e+02 - logprior: 0.2018
Epoch 8/10
24/24 - 40s - loss: 950.3139 - loglik: -9.5046e+02 - logprior: 0.1418
Fitted a model with MAP estimate = -947.7347
expansions: [(0, 5), (85, 2), (141, 1), (194, 1), (214, 1), (215, 1), (228, 2), (231, 4), (232, 3), (233, 1), (236, 2), (240, 2), (241, 3), (250, 1), (253, 1), (254, 3), (255, 4), (257, 1), (271, 1), (272, 1), (273, 1), (274, 2), (275, 1), (276, 2), (293, 2), (294, 5), (295, 2), (296, 5), (298, 7), (308, 2), (309, 1), (310, 3), (311, 1), (323, 1), (324, 2), (325, 1), (345, 5), (346, 1), (347, 1), (367, 1), (394, 2), (395, 3), (400, 6), (402, 3), (404, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 35 36 37 38]
Fitting a model of length 489 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 57s - loss: 948.1945 - loglik: -9.4254e+02 - logprior: -5.6577e+00
Epoch 2/2
24/24 - 54s - loss: 929.6414 - loglik: -9.3081e+02 - logprior: 1.1660
Fitted a model with MAP estimate = -923.9382
expansions: [(0, 7), (480, 4)]
discards: [  1   2   3   4   5   6 356 460 483 484 485 486 487 488]
Fitting a model of length 486 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 57s - loss: 938.8089 - loglik: -9.3360e+02 - logprior: -5.2061e+00
Epoch 2/2
24/24 - 54s - loss: 922.5984 - loglik: -9.2423e+02 - logprior: 1.6344
Fitted a model with MAP estimate = -921.9590
expansions: [(0, 6), (247, 1), (321, 2), (325, 2), (335, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9 482 483 484 485]
Fitting a model of length 484 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 56s - loss: 934.0108 - loglik: -9.3088e+02 - logprior: -3.1270e+00
Epoch 2/10
24/24 - 54s - loss: 928.6365 - loglik: -9.3142e+02 - logprior: 2.7866
Epoch 3/10
24/24 - 54s - loss: 920.9383 - loglik: -9.2438e+02 - logprior: 3.4444
Epoch 4/10
24/24 - 54s - loss: 922.3623 - loglik: -9.2604e+02 - logprior: 3.6805
Fitted a model with MAP estimate = -919.6266
Time for alignment: 930.8310
Fitting a model of length 404 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 44s - loss: 1191.8173 - loglik: -1.1885e+03 - logprior: -3.3651e+00
Epoch 2/10
24/24 - 40s - loss: 1001.4021 - loglik: -1.0022e+03 - logprior: 0.7944
Epoch 3/10
24/24 - 40s - loss: 956.6819 - loglik: -9.5675e+02 - logprior: 0.0686
Epoch 4/10
24/24 - 40s - loss: 955.2628 - loglik: -9.5547e+02 - logprior: 0.2100
Epoch 5/10
24/24 - 40s - loss: 943.5481 - loglik: -9.4369e+02 - logprior: 0.1468
Epoch 6/10
24/24 - 40s - loss: 952.3604 - loglik: -9.5241e+02 - logprior: 0.0474
Fitted a model with MAP estimate = -948.8322
expansions: [(0, 5), (142, 1), (184, 1), (215, 1), (216, 1), (230, 1), (232, 3), (233, 2), (234, 3), (235, 1), (239, 1), (246, 1), (253, 1), (254, 1), (255, 1), (258, 1), (259, 7), (260, 2), (276, 2), (277, 3), (278, 4), (294, 2), (295, 5), (296, 2), (297, 5), (299, 6), (300, 2), (308, 2), (309, 1), (310, 4), (312, 1), (323, 1), (324, 1), (325, 1), (326, 1), (331, 1), (344, 1), (345, 4), (346, 1), (347, 1), (395, 4), (396, 1), (400, 12), (404, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 34 35]
Fitting a model of length 494 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 58s - loss: 955.1179 - loglik: -9.4926e+02 - logprior: -5.8565e+00
Epoch 2/2
24/24 - 56s - loss: 923.7470 - loglik: -9.2433e+02 - logprior: 0.5851
Fitted a model with MAP estimate = -922.6704
expansions: [(0, 6), (24, 1), (245, 1), (301, 1), (325, 2), (477, 1), (482, 1), (483, 1)]
discards: [  1   2   3   4   5   6   7 227 405 461 488 489 490 491 492 493]
Fitting a model of length 492 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 58s - loss: 938.2974 - loglik: -9.3286e+02 - logprior: -5.4387e+00
Epoch 2/2
24/24 - 55s - loss: 924.2380 - loglik: -9.2580e+02 - logprior: 1.5606
Fitted a model with MAP estimate = -921.4590
expansions: [(0, 6), (7, 2), (336, 1)]
discards: [  0   1   2   3   4   5 487 488 489 490 491]
Fitting a model of length 490 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 58s - loss: 931.8124 - loglik: -9.2808e+02 - logprior: -3.7287e+00
Epoch 2/10
24/24 - 55s - loss: 926.9998 - loglik: -9.2953e+02 - logprior: 2.5292
Epoch 3/10
24/24 - 55s - loss: 923.8742 - loglik: -9.2707e+02 - logprior: 3.1934
Epoch 4/10
24/24 - 55s - loss: 917.2940 - loglik: -9.2063e+02 - logprior: 3.3348
Epoch 5/10
24/24 - 55s - loss: 919.6768 - loglik: -9.2336e+02 - logprior: 3.6782
Fitted a model with MAP estimate = -917.9236
Time for alignment: 918.4496
Computed alignments with likelihoods: ['-920.1075', '-919.6266', '-917.9236']
Best model has likelihood: -917.9236
SP score = 0.8177
Training of 3 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2c3f4cd60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb705430>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 513.4497 - loglik: -5.1060e+02 - logprior: -2.8512e+00
Epoch 2/10
19/19 - 6s - loss: 444.1263 - loglik: -4.4303e+02 - logprior: -1.0970e+00
Epoch 3/10
19/19 - 6s - loss: 414.4387 - loglik: -4.1284e+02 - logprior: -1.5996e+00
Epoch 4/10
19/19 - 6s - loss: 407.5536 - loglik: -4.0595e+02 - logprior: -1.6018e+00
Epoch 5/10
19/19 - 6s - loss: 404.9353 - loglik: -4.0332e+02 - logprior: -1.6118e+00
Epoch 6/10
19/19 - 7s - loss: 404.3031 - loglik: -4.0270e+02 - logprior: -1.6011e+00
Epoch 7/10
19/19 - 6s - loss: 401.4248 - loglik: -3.9979e+02 - logprior: -1.6333e+00
Epoch 8/10
19/19 - 7s - loss: 401.8475 - loglik: -4.0020e+02 - logprior: -1.6511e+00
Fitted a model with MAP estimate = -401.7486
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 1), (23, 1), (26, 2), (40, 4), (42, 1), (44, 1), (45, 1), (55, 1), (57, 1), (58, 2), (59, 1), (69, 1), (71, 2), (89, 2), (92, 1), (93, 2), (95, 1), (99, 1), (102, 1), (103, 1), (104, 2), (106, 1), (109, 1), (118, 1), (119, 1), (122, 1), (126, 1), (128, 1), (133, 1)]
discards: [0]
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 406.1635 - loglik: -4.0228e+02 - logprior: -3.8850e+00
Epoch 2/2
19/19 - 9s - loss: 389.5798 - loglik: -3.8767e+02 - logprior: -1.9097e+00
Fitted a model with MAP estimate = -387.0672
expansions: [(0, 1)]
discards: [  0  13  34  51  52  77  93 113 120 135 157]
Fitting a model of length 169 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 391.6491 - loglik: -3.8880e+02 - logprior: -2.8510e+00
Epoch 2/2
19/19 - 8s - loss: 385.7691 - loglik: -3.8473e+02 - logprior: -1.0411e+00
Fitted a model with MAP estimate = -385.4604
expansions: []
discards: []
Fitting a model of length 169 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 387.1701 - loglik: -3.8457e+02 - logprior: -2.6040e+00
Epoch 2/10
20/20 - 9s - loss: 383.1780 - loglik: -3.8226e+02 - logprior: -9.1399e-01
Epoch 3/10
20/20 - 9s - loss: 382.4501 - loglik: -3.8171e+02 - logprior: -7.4038e-01
Epoch 4/10
20/20 - 9s - loss: 381.5562 - loglik: -3.8086e+02 - logprior: -6.9289e-01
Epoch 5/10
20/20 - 9s - loss: 380.5048 - loglik: -3.7983e+02 - logprior: -6.7811e-01
Epoch 6/10
20/20 - 9s - loss: 380.3139 - loglik: -3.7968e+02 - logprior: -6.3563e-01
Epoch 7/10
20/20 - 9s - loss: 379.7189 - loglik: -3.7910e+02 - logprior: -6.2120e-01
Epoch 8/10
20/20 - 9s - loss: 378.6927 - loglik: -3.7811e+02 - logprior: -5.8456e-01
Epoch 9/10
20/20 - 9s - loss: 378.6861 - loglik: -3.7813e+02 - logprior: -5.5829e-01
Epoch 10/10
20/20 - 9s - loss: 378.6735 - loglik: -3.7815e+02 - logprior: -5.2593e-01
Fitted a model with MAP estimate = -378.8336
Time for alignment: 233.7343
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 513.2537 - loglik: -5.1041e+02 - logprior: -2.8407e+00
Epoch 2/10
19/19 - 6s - loss: 443.8268 - loglik: -4.4276e+02 - logprior: -1.0696e+00
Epoch 3/10
19/19 - 6s - loss: 413.5397 - loglik: -4.1193e+02 - logprior: -1.6111e+00
Epoch 4/10
19/19 - 6s - loss: 406.4538 - loglik: -4.0480e+02 - logprior: -1.6509e+00
Epoch 5/10
19/19 - 6s - loss: 403.1304 - loglik: -4.0144e+02 - logprior: -1.6896e+00
Epoch 6/10
19/19 - 6s - loss: 401.3970 - loglik: -3.9968e+02 - logprior: -1.7169e+00
Epoch 7/10
19/19 - 6s - loss: 402.3784 - loglik: -4.0065e+02 - logprior: -1.7243e+00
Fitted a model with MAP estimate = -400.8664
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (14, 2), (23, 1), (27, 1), (39, 2), (40, 1), (45, 1), (55, 1), (57, 1), (58, 1), (68, 1), (69, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (95, 1), (99, 2), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (122, 1), (126, 1), (127, 1), (133, 1)]
discards: [0]
Fitting a model of length 174 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 403.4979 - loglik: -3.9957e+02 - logprior: -3.9243e+00
Epoch 2/2
19/19 - 9s - loss: 387.9005 - loglik: -3.8597e+02 - logprior: -1.9263e+00
Fitted a model with MAP estimate = -386.0586
expansions: [(0, 1)]
discards: [  0  13  19  89 110 125]
Fitting a model of length 169 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 389.0895 - loglik: -3.8624e+02 - logprior: -2.8468e+00
Epoch 2/2
19/19 - 8s - loss: 384.8645 - loglik: -3.8380e+02 - logprior: -1.0665e+00
Fitted a model with MAP estimate = -384.4404
expansions: []
discards: []
Fitting a model of length 169 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 386.1467 - loglik: -3.8353e+02 - logprior: -2.6169e+00
Epoch 2/10
20/20 - 9s - loss: 383.2314 - loglik: -3.8232e+02 - logprior: -9.1348e-01
Epoch 3/10
20/20 - 9s - loss: 383.2834 - loglik: -3.8254e+02 - logprior: -7.4406e-01
Fitted a model with MAP estimate = -381.4443
Time for alignment: 164.9942
Fitting a model of length 136 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 513.3389 - loglik: -5.1049e+02 - logprior: -2.8481e+00
Epoch 2/10
19/19 - 6s - loss: 445.4685 - loglik: -4.4437e+02 - logprior: -1.0998e+00
Epoch 3/10
19/19 - 6s - loss: 413.3490 - loglik: -4.1176e+02 - logprior: -1.5878e+00
Epoch 4/10
19/19 - 6s - loss: 404.4668 - loglik: -4.0283e+02 - logprior: -1.6349e+00
Epoch 5/10
19/19 - 6s - loss: 402.0135 - loglik: -4.0034e+02 - logprior: -1.6779e+00
Epoch 6/10
19/19 - 6s - loss: 400.2881 - loglik: -3.9859e+02 - logprior: -1.7019e+00
Epoch 7/10
19/19 - 6s - loss: 399.0910 - loglik: -3.9738e+02 - logprior: -1.7062e+00
Epoch 8/10
19/19 - 6s - loss: 399.4359 - loglik: -3.9774e+02 - logprior: -1.6933e+00
Fitted a model with MAP estimate = -399.2160
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (14, 2), (22, 1), (23, 1), (31, 1), (39, 2), (40, 1), (43, 1), (44, 1), (54, 1), (58, 2), (59, 2), (69, 1), (71, 2), (89, 2), (92, 1), (93, 1), (95, 1), (99, 1), (103, 1), (104, 1), (105, 1), (107, 1), (109, 1), (118, 1), (119, 1), (121, 1), (126, 2), (131, 1)]
discards: [0]
Fitting a model of length 176 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 12s - loss: 402.3769 - loglik: -3.9847e+02 - logprior: -3.9046e+00
Epoch 2/2
19/19 - 9s - loss: 385.3020 - loglik: -3.8337e+02 - logprior: -1.9281e+00
Fitted a model with MAP estimate = -383.6337
expansions: [(0, 1)]
discards: [  0  13  19  75  77  92 112 164]
Fitting a model of length 169 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 385.9315 - loglik: -3.8310e+02 - logprior: -2.8354e+00
Epoch 2/2
19/19 - 8s - loss: 381.7907 - loglik: -3.8076e+02 - logprior: -1.0302e+00
Fitted a model with MAP estimate = -380.6984
expansions: []
discards: []
Fitting a model of length 169 on 11293 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 383.4880 - loglik: -3.8091e+02 - logprior: -2.5793e+00
Epoch 2/10
20/20 - 9s - loss: 379.3918 - loglik: -3.7852e+02 - logprior: -8.6933e-01
Epoch 3/10
20/20 - 9s - loss: 378.3011 - loglik: -3.7762e+02 - logprior: -6.7828e-01
Epoch 4/10
20/20 - 9s - loss: 377.5787 - loglik: -3.7693e+02 - logprior: -6.4385e-01
Epoch 5/10
20/20 - 9s - loss: 376.7607 - loglik: -3.7613e+02 - logprior: -6.3265e-01
Epoch 6/10
20/20 - 9s - loss: 376.4719 - loglik: -3.7589e+02 - logprior: -5.8258e-01
Epoch 7/10
20/20 - 9s - loss: 375.6176 - loglik: -3.7505e+02 - logprior: -5.7020e-01
Epoch 8/10
20/20 - 9s - loss: 375.3951 - loglik: -3.7485e+02 - logprior: -5.4839e-01
Epoch 9/10
20/20 - 9s - loss: 375.9001 - loglik: -3.7538e+02 - logprior: -5.1558e-01
Fitted a model with MAP estimate = -375.3906
Time for alignment: 223.1241
Computed alignments with likelihoods: ['-378.8336', '-381.4443', '-375.3906']
Best model has likelihood: -375.3906
SP score = 0.5147
Training of 3 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f8877c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb72cfd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.5594 - loglik: -1.1166e+02 - logprior: -5.8900e+01
Epoch 2/10
10/10 - 0s - loss: 105.4846 - loglik: -8.8914e+01 - logprior: -1.6570e+01
Epoch 3/10
10/10 - 0s - loss: 80.6433 - loglik: -7.2392e+01 - logprior: -8.2514e+00
Epoch 4/10
10/10 - 0s - loss: 69.9099 - loglik: -6.4715e+01 - logprior: -5.1950e+00
Epoch 5/10
10/10 - 0s - loss: 65.7201 - loglik: -6.2080e+01 - logprior: -3.6399e+00
Epoch 6/10
10/10 - 0s - loss: 64.0611 - loglik: -6.1226e+01 - logprior: -2.8346e+00
Epoch 7/10
10/10 - 0s - loss: 63.4544 - loglik: -6.1077e+01 - logprior: -2.3774e+00
Epoch 8/10
10/10 - 0s - loss: 62.9238 - loglik: -6.0856e+01 - logprior: -2.0681e+00
Epoch 9/10
10/10 - 0s - loss: 62.8007 - loglik: -6.0938e+01 - logprior: -1.8629e+00
Epoch 10/10
10/10 - 0s - loss: 62.5771 - loglik: -6.0830e+01 - logprior: -1.7467e+00
Fitted a model with MAP estimate = -62.4309
expansions: [(0, 4), (10, 2), (21, 1), (25, 2)]
discards: []
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.8213 - loglik: -5.8805e+01 - logprior: -7.9016e+01
Epoch 2/2
10/10 - 0s - loss: 79.5232 - loglik: -5.3942e+01 - logprior: -2.5582e+01
Fitted a model with MAP estimate = -68.6487
expansions: [(0, 2)]
discards: [14 32]
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.3484 - loglik: -5.1665e+01 - logprior: -6.5683e+01
Epoch 2/2
10/10 - 0s - loss: 74.0678 - loglik: -5.1018e+01 - logprior: -2.3050e+01
Fitted a model with MAP estimate = -65.0838
expansions: []
discards: [0]
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 117.3706 - loglik: -5.1648e+01 - logprior: -6.5722e+01
Epoch 2/10
10/10 - 0s - loss: 79.3319 - loglik: -5.2075e+01 - logprior: -2.7257e+01
Epoch 3/10
10/10 - 0s - loss: 68.1145 - loglik: -5.2144e+01 - logprior: -1.5970e+01
Epoch 4/10
10/10 - 0s - loss: 58.7693 - loglik: -5.2381e+01 - logprior: -6.3879e+00
Epoch 5/10
10/10 - 0s - loss: 55.5546 - loglik: -5.2835e+01 - logprior: -2.7195e+00
Epoch 6/10
10/10 - 0s - loss: 54.1021 - loglik: -5.2579e+01 - logprior: -1.5233e+00
Epoch 7/10
10/10 - 0s - loss: 53.8723 - loglik: -5.2931e+01 - logprior: -9.4166e-01
Epoch 8/10
10/10 - 0s - loss: 53.3534 - loglik: -5.2755e+01 - logprior: -5.9886e-01
Epoch 9/10
10/10 - 0s - loss: 53.2339 - loglik: -5.2891e+01 - logprior: -3.4316e-01
Epoch 10/10
10/10 - 0s - loss: 53.1023 - loglik: -5.2973e+01 - logprior: -1.2963e-01
Fitted a model with MAP estimate = -53.0277
Time for alignment: 25.3289
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.4679 - loglik: -1.1157e+02 - logprior: -5.8899e+01
Epoch 2/10
10/10 - 0s - loss: 105.5742 - loglik: -8.9005e+01 - logprior: -1.6569e+01
Epoch 3/10
10/10 - 0s - loss: 80.8784 - loglik: -7.2635e+01 - logprior: -8.2430e+00
Epoch 4/10
10/10 - 0s - loss: 70.5094 - loglik: -6.5358e+01 - logprior: -5.1512e+00
Epoch 5/10
10/10 - 0s - loss: 65.5099 - loglik: -6.1973e+01 - logprior: -3.5374e+00
Epoch 6/10
10/10 - 0s - loss: 62.9502 - loglik: -6.0205e+01 - logprior: -2.7449e+00
Epoch 7/10
10/10 - 0s - loss: 61.9677 - loglik: -5.9646e+01 - logprior: -2.3221e+00
Epoch 8/10
10/10 - 0s - loss: 61.5169 - loglik: -5.9477e+01 - logprior: -2.0402e+00
Epoch 9/10
10/10 - 0s - loss: 61.3741 - loglik: -5.9561e+01 - logprior: -1.8127e+00
Epoch 10/10
10/10 - 0s - loss: 61.1344 - loglik: -5.9504e+01 - logprior: -1.6304e+00
Fitted a model with MAP estimate = -61.0986
expansions: [(0, 4), (10, 2), (26, 2)]
discards: []
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 137.0020 - loglik: -5.7799e+01 - logprior: -7.9203e+01
Epoch 2/2
10/10 - 0s - loss: 79.2509 - loglik: -5.3621e+01 - logprior: -2.5630e+01
Fitted a model with MAP estimate = -68.3657
expansions: [(0, 2)]
discards: [14 32]
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.1799 - loglik: -5.1350e+01 - logprior: -6.5830e+01
Epoch 2/2
10/10 - 0s - loss: 73.7868 - loglik: -5.0707e+01 - logprior: -2.3080e+01
Fitted a model with MAP estimate = -64.8621
expansions: []
discards: [0]
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 117.2216 - loglik: -5.1366e+01 - logprior: -6.5855e+01
Epoch 2/10
10/10 - 0s - loss: 79.1210 - loglik: -5.1824e+01 - logprior: -2.7297e+01
Epoch 3/10
10/10 - 0s - loss: 67.9143 - loglik: -5.1997e+01 - logprior: -1.5917e+01
Epoch 4/10
10/10 - 0s - loss: 58.7864 - loglik: -5.2384e+01 - logprior: -6.4026e+00
Epoch 5/10
10/10 - 0s - loss: 55.1693 - loglik: -5.2364e+01 - logprior: -2.8057e+00
Epoch 6/10
10/10 - 0s - loss: 54.0096 - loglik: -5.2380e+01 - logprior: -1.6296e+00
Epoch 7/10
10/10 - 0s - loss: 53.6039 - loglik: -5.2561e+01 - logprior: -1.0427e+00
Epoch 8/10
10/10 - 0s - loss: 53.2847 - loglik: -5.2586e+01 - logprior: -6.9833e-01
Epoch 9/10
10/10 - 0s - loss: 53.3057 - loglik: -5.2864e+01 - logprior: -4.4145e-01
Fitted a model with MAP estimate = -53.0409
Time for alignment: 24.3270
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 170.5498 - loglik: -1.1165e+02 - logprior: -5.8900e+01
Epoch 2/10
10/10 - 0s - loss: 105.5232 - loglik: -8.8954e+01 - logprior: -1.6569e+01
Epoch 3/10
10/10 - 0s - loss: 80.8141 - loglik: -7.2570e+01 - logprior: -8.2444e+00
Epoch 4/10
10/10 - 0s - loss: 70.4805 - loglik: -6.5322e+01 - logprior: -5.1586e+00
Epoch 5/10
10/10 - 0s - loss: 66.3114 - loglik: -6.2731e+01 - logprior: -3.5807e+00
Epoch 6/10
10/10 - 0s - loss: 63.6737 - loglik: -6.0858e+01 - logprior: -2.8155e+00
Epoch 7/10
10/10 - 0s - loss: 62.3163 - loglik: -5.9931e+01 - logprior: -2.3856e+00
Epoch 8/10
10/10 - 0s - loss: 61.7933 - loglik: -5.9684e+01 - logprior: -2.1092e+00
Epoch 9/10
10/10 - 0s - loss: 61.6799 - loglik: -5.9807e+01 - logprior: -1.8732e+00
Epoch 10/10
10/10 - 0s - loss: 61.4744 - loglik: -5.9792e+01 - logprior: -1.6821e+00
Fitted a model with MAP estimate = -61.3737
expansions: [(0, 4), (10, 2), (21, 1)]
discards: []
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 136.8353 - loglik: -5.7568e+01 - logprior: -7.9267e+01
Epoch 2/2
10/10 - 0s - loss: 79.1771 - loglik: -5.3724e+01 - logprior: -2.5453e+01
Fitted a model with MAP estimate = -68.1128
expansions: [(0, 2)]
discards: [14]
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 117.0802 - loglik: -5.1276e+01 - logprior: -6.5804e+01
Epoch 2/2
10/10 - 0s - loss: 73.7085 - loglik: -5.0628e+01 - logprior: -2.3081e+01
Fitted a model with MAP estimate = -64.8115
expansions: []
discards: [0]
Fitting a model of length 39 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 117.2458 - loglik: -5.1404e+01 - logprior: -6.5841e+01
Epoch 2/10
10/10 - 0s - loss: 79.0025 - loglik: -5.1731e+01 - logprior: -2.7271e+01
Epoch 3/10
10/10 - 0s - loss: 67.7492 - loglik: -5.1926e+01 - logprior: -1.5823e+01
Epoch 4/10
10/10 - 0s - loss: 58.7527 - loglik: -5.2421e+01 - logprior: -6.3314e+00
Epoch 5/10
10/10 - 0s - loss: 55.2645 - loglik: -5.2464e+01 - logprior: -2.8004e+00
Epoch 6/10
10/10 - 0s - loss: 54.0596 - loglik: -5.2437e+01 - logprior: -1.6225e+00
Epoch 7/10
10/10 - 0s - loss: 53.5346 - loglik: -5.2498e+01 - logprior: -1.0365e+00
Epoch 8/10
10/10 - 0s - loss: 53.3913 - loglik: -5.2701e+01 - logprior: -6.9067e-01
Epoch 9/10
10/10 - 0s - loss: 53.0577 - loglik: -5.2624e+01 - logprior: -4.3401e-01
Epoch 10/10
10/10 - 0s - loss: 53.0599 - loglik: -5.2843e+01 - logprior: -2.1703e-01
Fitted a model with MAP estimate = -52.9081
Time for alignment: 23.6355
Computed alignments with likelihoods: ['-53.0277', '-53.0409', '-52.9081']
Best model has likelihood: -52.9081
SP score = 0.9589
Training of 3 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2c3f0e910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2c3f039a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 372.8115 - loglik: -3.7000e+02 - logprior: -2.8119e+00
Epoch 2/10
20/20 - 3s - loss: 329.4997 - loglik: -3.2824e+02 - logprior: -1.2564e+00
Epoch 3/10
20/20 - 3s - loss: 310.7115 - loglik: -3.0924e+02 - logprior: -1.4696e+00
Epoch 4/10
20/20 - 3s - loss: 305.0391 - loglik: -3.0361e+02 - logprior: -1.4333e+00
Epoch 5/10
20/20 - 3s - loss: 303.5334 - loglik: -3.0209e+02 - logprior: -1.4480e+00
Epoch 6/10
20/20 - 4s - loss: 302.9357 - loglik: -3.0152e+02 - logprior: -1.4200e+00
Epoch 7/10
20/20 - 3s - loss: 302.3892 - loglik: -3.0097e+02 - logprior: -1.4191e+00
Epoch 8/10
20/20 - 3s - loss: 302.0902 - loglik: -3.0068e+02 - logprior: -1.4142e+00
Epoch 9/10
20/20 - 3s - loss: 302.0508 - loglik: -3.0064e+02 - logprior: -1.4124e+00
Epoch 10/10
20/20 - 3s - loss: 302.2004 - loglik: -3.0078e+02 - logprior: -1.4164e+00
Fitted a model with MAP estimate = -290.4946
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (23, 2), (35, 1), (36, 2), (38, 1), (47, 1), (50, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 2), (62, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (91, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Fitting a model of length 140 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 8s - loss: 308.8898 - loglik: -3.0522e+02 - logprior: -3.6654e+00
Epoch 2/2
20/20 - 4s - loss: 295.1859 - loglik: -2.9343e+02 - logprior: -1.7579e+00
Fitted a model with MAP estimate = -279.6521
expansions: [(0, 1), (134, 1)]
discards: [  0   8  13  31  47  79  83 106 110]
Fitting a model of length 133 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 7s - loss: 295.4675 - loglik: -2.9274e+02 - logprior: -2.7302e+00
Epoch 2/2
20/20 - 4s - loss: 291.1277 - loglik: -2.9016e+02 - logprior: -9.6962e-01
Fitted a model with MAP estimate = -277.9402
expansions: []
discards: [ 97 119]
Fitting a model of length 131 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 8s - loss: 275.6770 - loglik: -2.7428e+02 - logprior: -1.3988e+00
Epoch 2/10
28/28 - 5s - loss: 272.9853 - loglik: -2.7220e+02 - logprior: -7.8630e-01
Epoch 3/10
28/28 - 5s - loss: 272.2019 - loglik: -2.7147e+02 - logprior: -7.3348e-01
Epoch 4/10
28/28 - 5s - loss: 271.2032 - loglik: -2.7047e+02 - logprior: -7.3098e-01
Epoch 5/10
28/28 - 5s - loss: 270.4177 - loglik: -2.6969e+02 - logprior: -7.2312e-01
Epoch 6/10
28/28 - 5s - loss: 270.0191 - loglik: -2.6929e+02 - logprior: -7.2467e-01
Epoch 7/10
28/28 - 5s - loss: 268.7886 - loglik: -2.6806e+02 - logprior: -7.2357e-01
Epoch 8/10
28/28 - 5s - loss: 269.1278 - loglik: -2.6840e+02 - logprior: -7.2393e-01
Fitted a model with MAP estimate = -268.6593
Time for alignment: 138.9879
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 373.0617 - loglik: -3.7025e+02 - logprior: -2.8138e+00
Epoch 2/10
20/20 - 3s - loss: 328.5473 - loglik: -3.2728e+02 - logprior: -1.2655e+00
Epoch 3/10
20/20 - 3s - loss: 310.8437 - loglik: -3.0935e+02 - logprior: -1.4956e+00
Epoch 4/10
20/20 - 3s - loss: 305.8298 - loglik: -3.0440e+02 - logprior: -1.4253e+00
Epoch 5/10
20/20 - 3s - loss: 304.1372 - loglik: -3.0269e+02 - logprior: -1.4493e+00
Epoch 6/10
20/20 - 3s - loss: 303.3002 - loglik: -3.0188e+02 - logprior: -1.4212e+00
Epoch 7/10
20/20 - 3s - loss: 303.1983 - loglik: -3.0179e+02 - logprior: -1.4096e+00
Epoch 8/10
20/20 - 3s - loss: 302.3759 - loglik: -3.0096e+02 - logprior: -1.4125e+00
Epoch 9/10
20/20 - 3s - loss: 302.5585 - loglik: -3.0115e+02 - logprior: -1.4129e+00
Fitted a model with MAP estimate = -290.6618
expansions: [(5, 1), (8, 2), (10, 1), (13, 2), (19, 1), (20, 1), (23, 2), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (62, 1), (76, 1), (78, 2), (79, 2), (81, 2), (87, 1), (88, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Fitting a model of length 139 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 7s - loss: 308.2818 - loglik: -3.0464e+02 - logprior: -3.6466e+00
Epoch 2/2
20/20 - 4s - loss: 295.0315 - loglik: -2.9333e+02 - logprior: -1.7051e+00
Fitted a model with MAP estimate = -280.0133
expansions: [(0, 1)]
discards: [  0   8  30  46  74  82 102 106 109]
Fitting a model of length 131 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 7s - loss: 295.9126 - loglik: -2.9318e+02 - logprior: -2.7300e+00
Epoch 2/2
20/20 - 4s - loss: 291.7172 - loglik: -2.9076e+02 - logprior: -9.5659e-01
Fitted a model with MAP estimate = -278.4770
expansions: [(125, 1)]
discards: [118]
Fitting a model of length 131 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 8s - loss: 275.1051 - loglik: -2.7370e+02 - logprior: -1.4024e+00
Epoch 2/10
28/28 - 6s - loss: 273.7845 - loglik: -2.7303e+02 - logprior: -7.5891e-01
Epoch 3/10
28/28 - 5s - loss: 271.9521 - loglik: -2.7122e+02 - logprior: -7.2813e-01
Epoch 4/10
28/28 - 6s - loss: 271.1916 - loglik: -2.7047e+02 - logprior: -7.1856e-01
Epoch 5/10
28/28 - 5s - loss: 271.1890 - loglik: -2.7048e+02 - logprior: -7.1214e-01
Epoch 6/10
28/28 - 5s - loss: 269.3835 - loglik: -2.6868e+02 - logprior: -7.0493e-01
Epoch 7/10
28/28 - 5s - loss: 269.1270 - loglik: -2.6843e+02 - logprior: -6.9786e-01
Epoch 8/10
28/28 - 5s - loss: 269.3739 - loglik: -2.6868e+02 - logprior: -6.9042e-01
Fitted a model with MAP estimate = -268.8280
Time for alignment: 134.0247
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 372.8331 - loglik: -3.7002e+02 - logprior: -2.8131e+00
Epoch 2/10
20/20 - 3s - loss: 327.7015 - loglik: -3.2644e+02 - logprior: -1.2629e+00
Epoch 3/10
20/20 - 3s - loss: 308.7692 - loglik: -3.0728e+02 - logprior: -1.4868e+00
Epoch 4/10
20/20 - 3s - loss: 304.7403 - loglik: -3.0332e+02 - logprior: -1.4199e+00
Epoch 5/10
20/20 - 3s - loss: 303.5586 - loglik: -3.0212e+02 - logprior: -1.4386e+00
Epoch 6/10
20/20 - 3s - loss: 302.7430 - loglik: -3.0133e+02 - logprior: -1.4124e+00
Epoch 7/10
20/20 - 3s - loss: 302.3373 - loglik: -3.0094e+02 - logprior: -1.3969e+00
Epoch 8/10
20/20 - 3s - loss: 301.8766 - loglik: -3.0048e+02 - logprior: -1.3991e+00
Epoch 9/10
20/20 - 3s - loss: 302.2065 - loglik: -3.0081e+02 - logprior: -1.3958e+00
Fitted a model with MAP estimate = -290.6275
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (31, 1), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (62, 1), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 2), (95, 1), (96, 1)]
discards: [0]
Fitting a model of length 139 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 7s - loss: 308.0634 - loglik: -3.0441e+02 - logprior: -3.6574e+00
Epoch 2/2
20/20 - 4s - loss: 295.2098 - loglik: -2.9349e+02 - logprior: -1.7227e+00
Fitted a model with MAP estimate = -279.6810
expansions: [(0, 1)]
discards: [  0   8  13  46  74  82 103 106 109]
Fitting a model of length 131 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 7s - loss: 296.2284 - loglik: -2.9349e+02 - logprior: -2.7345e+00
Epoch 2/2
20/20 - 4s - loss: 291.5320 - loglik: -2.9056e+02 - logprior: -9.6899e-01
Fitted a model with MAP estimate = -278.3130
expansions: [(125, 1)]
discards: [118]
Fitting a model of length 131 on 21331 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
28/28 - 8s - loss: 275.4802 - loglik: -2.7408e+02 - logprior: -1.3986e+00
Epoch 2/10
28/28 - 5s - loss: 272.8784 - loglik: -2.7209e+02 - logprior: -7.8539e-01
Epoch 3/10
28/28 - 5s - loss: 272.4769 - loglik: -2.7173e+02 - logprior: -7.4279e-01
Epoch 4/10
28/28 - 5s - loss: 271.1761 - loglik: -2.7044e+02 - logprior: -7.3994e-01
Epoch 5/10
28/28 - 5s - loss: 270.6009 - loglik: -2.6987e+02 - logprior: -7.3347e-01
Epoch 6/10
28/28 - 5s - loss: 269.6465 - loglik: -2.6892e+02 - logprior: -7.2838e-01
Epoch 7/10
28/28 - 5s - loss: 269.3092 - loglik: -2.6858e+02 - logprior: -7.2817e-01
Epoch 8/10
28/28 - 5s - loss: 269.1668 - loglik: -2.6845e+02 - logprior: -7.1600e-01
Epoch 9/10
28/28 - 6s - loss: 268.3454 - loglik: -2.6763e+02 - logprior: -7.1693e-01
Epoch 10/10
28/28 - 5s - loss: 268.7554 - loglik: -2.6805e+02 - logprior: -7.0661e-01
Fitted a model with MAP estimate = -268.4890
Time for alignment: 143.2671
Computed alignments with likelihoods: ['-268.6593', '-268.8280', '-268.4890']
Best model has likelihood: -268.4890
SP score = 0.6749
Training of 3 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2a63ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2c3dd9ee0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 705.4810 - loglik: -6.8986e+02 - logprior: -1.5623e+01
Epoch 2/10
10/10 - 6s - loss: 665.2468 - loglik: -6.6263e+02 - logprior: -2.6166e+00
Epoch 3/10
10/10 - 6s - loss: 629.5907 - loglik: -6.2904e+02 - logprior: -5.4874e-01
Epoch 4/10
10/10 - 6s - loss: 603.6784 - loglik: -6.0308e+02 - logprior: -5.9724e-01
Epoch 5/10
10/10 - 6s - loss: 590.3306 - loglik: -5.8960e+02 - logprior: -7.2760e-01
Epoch 6/10
10/10 - 6s - loss: 587.3268 - loglik: -5.8665e+02 - logprior: -6.8039e-01
Epoch 7/10
10/10 - 6s - loss: 581.7841 - loglik: -5.8115e+02 - logprior: -6.3872e-01
Epoch 8/10
10/10 - 6s - loss: 582.7950 - loglik: -5.8228e+02 - logprior: -5.1495e-01
Fitted a model with MAP estimate = -581.3525
expansions: [(18, 1), (25, 1), (27, 1), (30, 2), (35, 1), (44, 2), (64, 4), (89, 4), (102, 1), (117, 1), (119, 1), (131, 1), (134, 1), (148, 3), (152, 1), (163, 1), (166, 4), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [  2 170 171 172 173 174]
Fitting a model of length 227 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 613.4805 - loglik: -5.9902e+02 - logprior: -1.4458e+01
Epoch 2/2
10/10 - 7s - loss: 588.1295 - loglik: -5.8525e+02 - logprior: -2.8777e+00
Fitted a model with MAP estimate = -583.4296
expansions: [(48, 3), (103, 1)]
discards: [ 32  33 177 178 199]
Fitting a model of length 226 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 598.5646 - loglik: -5.8430e+02 - logprior: -1.4266e+01
Epoch 2/2
10/10 - 7s - loss: 583.0516 - loglik: -5.8040e+02 - logprior: -2.6485e+00
Fitted a model with MAP estimate = -579.4822
expansions: [(30, 1), (103, 2), (172, 2), (179, 1), (192, 2), (211, 1)]
discards: []
Fitting a model of length 235 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 592.9015 - loglik: -5.7892e+02 - logprior: -1.3984e+01
Epoch 2/10
10/10 - 7s - loss: 579.1990 - loglik: -5.7680e+02 - logprior: -2.3993e+00
Epoch 3/10
10/10 - 7s - loss: 574.4500 - loglik: -5.7431e+02 - logprior: -1.4025e-01
Epoch 4/10
10/10 - 7s - loss: 573.1392 - loglik: -5.7378e+02 - logprior: 0.6404
Epoch 5/10
10/10 - 7s - loss: 571.7357 - loglik: -5.7268e+02 - logprior: 0.9412
Epoch 6/10
10/10 - 7s - loss: 566.5728 - loglik: -5.6778e+02 - logprior: 1.2055
Epoch 7/10
10/10 - 7s - loss: 566.7040 - loglik: -5.6817e+02 - logprior: 1.4613
Fitted a model with MAP estimate = -566.6649
Time for alignment: 161.8611
Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 704.6881 - loglik: -6.8906e+02 - logprior: -1.5632e+01
Epoch 2/10
10/10 - 6s - loss: 666.3074 - loglik: -6.6369e+02 - logprior: -2.6198e+00
Epoch 3/10
10/10 - 6s - loss: 629.4144 - loglik: -6.2877e+02 - logprior: -6.4474e-01
Epoch 4/10
10/10 - 6s - loss: 601.9981 - loglik: -6.0131e+02 - logprior: -6.9295e-01
Epoch 5/10
10/10 - 6s - loss: 591.8210 - loglik: -5.9118e+02 - logprior: -6.4410e-01
Epoch 6/10
10/10 - 6s - loss: 587.5614 - loglik: -5.8698e+02 - logprior: -5.8376e-01
Epoch 7/10
10/10 - 6s - loss: 588.0529 - loglik: -5.8753e+02 - logprior: -5.1819e-01
Fitted a model with MAP estimate = -585.0234
expansions: [(25, 1), (27, 1), (28, 1), (30, 2), (63, 2), (87, 1), (89, 2), (101, 5), (104, 1), (117, 2), (119, 1), (134, 1), (135, 1), (146, 2), (149, 3), (150, 1), (162, 1), (163, 1), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [170 171 172 173]
Fitting a model of length 228 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 613.3724 - loglik: -5.9886e+02 - logprior: -1.4512e+01
Epoch 2/2
10/10 - 7s - loss: 591.3444 - loglik: -5.8828e+02 - logprior: -3.0651e+00
Fitted a model with MAP estimate = -586.0904
expansions: [(196, 4)]
discards: [ 33  34 113 134 171 172 189 199 200 201 202 203]
Fitting a model of length 220 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 603.9557 - loglik: -5.8961e+02 - logprior: -1.4341e+01
Epoch 2/2
10/10 - 7s - loss: 589.2082 - loglik: -5.8653e+02 - logprior: -2.6741e+00
Fitted a model with MAP estimate = -586.1725
expansions: [(188, 4), (202, 1)]
discards: []
Fitting a model of length 225 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 601.0045 - loglik: -5.8692e+02 - logprior: -1.4085e+01
Epoch 2/10
10/10 - 7s - loss: 585.7775 - loglik: -5.8340e+02 - logprior: -2.3802e+00
Epoch 3/10
10/10 - 7s - loss: 583.7714 - loglik: -5.8366e+02 - logprior: -1.0801e-01
Epoch 4/10
10/10 - 7s - loss: 579.2282 - loglik: -5.7995e+02 - logprior: 0.7189
Epoch 5/10
10/10 - 7s - loss: 578.9514 - loglik: -5.7997e+02 - logprior: 1.0197
Epoch 6/10
10/10 - 7s - loss: 576.8493 - loglik: -5.7812e+02 - logprior: 1.2750
Epoch 7/10
10/10 - 7s - loss: 575.6996 - loglik: -5.7723e+02 - logprior: 1.5277
Epoch 8/10
10/10 - 7s - loss: 574.4622 - loglik: -5.7613e+02 - logprior: 1.6702
Epoch 9/10
10/10 - 7s - loss: 574.3522 - loglik: -5.7610e+02 - logprior: 1.7457
Epoch 10/10
10/10 - 7s - loss: 574.7573 - loglik: -5.7660e+02 - logprior: 1.8422
Fitted a model with MAP estimate = -573.9182
Time for alignment: 170.9637
Fitting a model of length 198 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 704.1604 - loglik: -6.8854e+02 - logprior: -1.5624e+01
Epoch 2/10
10/10 - 6s - loss: 665.9909 - loglik: -6.6337e+02 - logprior: -2.6199e+00
Epoch 3/10
10/10 - 6s - loss: 628.2038 - loglik: -6.2765e+02 - logprior: -5.5038e-01
Epoch 4/10
10/10 - 6s - loss: 601.9680 - loglik: -6.0145e+02 - logprior: -5.2144e-01
Epoch 5/10
10/10 - 6s - loss: 590.4676 - loglik: -5.8997e+02 - logprior: -5.0065e-01
Epoch 6/10
10/10 - 6s - loss: 585.3494 - loglik: -5.8492e+02 - logprior: -4.2779e-01
Epoch 7/10
10/10 - 6s - loss: 582.1696 - loglik: -5.8183e+02 - logprior: -3.3488e-01
Epoch 8/10
10/10 - 6s - loss: 582.6099 - loglik: -5.8238e+02 - logprior: -2.2858e-01
Fitted a model with MAP estimate = -581.0023
expansions: [(18, 1), (25, 1), (28, 2), (43, 2), (44, 4), (64, 4), (104, 1), (120, 1), (136, 1), (152, 3), (153, 1), (162, 1), (163, 2), (166, 5), (182, 1), (183, 1), (184, 2), (185, 1)]
discards: [  2 170]
Fitting a model of length 230 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 11s - loss: 610.9286 - loglik: -5.9649e+02 - logprior: -1.4441e+01
Epoch 2/2
10/10 - 7s - loss: 587.4420 - loglik: -5.8460e+02 - logprior: -2.8448e+00
Fitted a model with MAP estimate = -583.1483
expansions: [(170, 1)]
discards: [174 175 176 177 185 194 195 198 199 200]
Fitting a model of length 221 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 10s - loss: 601.1100 - loglik: -5.8690e+02 - logprior: -1.4209e+01
Epoch 2/2
10/10 - 7s - loss: 585.9094 - loglik: -5.8327e+02 - logprior: -2.6381e+00
Fitted a model with MAP estimate = -583.3423
expansions: [(168, 1), (203, 1)]
discards: [190]
Fitting a model of length 222 on 2489 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 597.5778 - loglik: -5.8352e+02 - logprior: -1.4055e+01
Epoch 2/10
10/10 - 7s - loss: 584.5908 - loglik: -5.8216e+02 - logprior: -2.4298e+00
Epoch 3/10
10/10 - 7s - loss: 579.4793 - loglik: -5.7929e+02 - logprior: -1.9086e-01
Epoch 4/10
10/10 - 7s - loss: 576.0757 - loglik: -5.7670e+02 - logprior: 0.6216
Epoch 5/10
10/10 - 7s - loss: 575.5928 - loglik: -5.7649e+02 - logprior: 0.8943
Epoch 6/10
10/10 - 7s - loss: 573.0328 - loglik: -5.7414e+02 - logprior: 1.1062
Epoch 7/10
10/10 - 7s - loss: 571.1146 - loglik: -5.7246e+02 - logprior: 1.3501
Epoch 8/10
10/10 - 7s - loss: 571.8095 - loglik: -5.7335e+02 - logprior: 1.5420
Fitted a model with MAP estimate = -570.8167
Time for alignment: 163.1375
Computed alignments with likelihoods: ['-566.6649', '-573.9182', '-570.8167']
Best model has likelihood: -566.6649
SP score = 0.7670
Training of 3 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2aa148eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25de5cd60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.2336 - loglik: -1.5100e+02 - logprior: -3.2301e+00
Epoch 2/10
19/19 - 1s - loss: 121.4043 - loglik: -1.1991e+02 - logprior: -1.4935e+00
Epoch 3/10
19/19 - 1s - loss: 107.7974 - loglik: -1.0624e+02 - logprior: -1.5533e+00
Epoch 4/10
19/19 - 1s - loss: 103.7795 - loglik: -1.0214e+02 - logprior: -1.6382e+00
Epoch 5/10
19/19 - 1s - loss: 103.0870 - loglik: -1.0156e+02 - logprior: -1.5265e+00
Epoch 6/10
19/19 - 1s - loss: 102.5379 - loglik: -1.0100e+02 - logprior: -1.5363e+00
Epoch 7/10
19/19 - 1s - loss: 102.1508 - loglik: -1.0063e+02 - logprior: -1.5181e+00
Epoch 8/10
19/19 - 1s - loss: 102.2963 - loglik: -1.0079e+02 - logprior: -1.5100e+00
Fitted a model with MAP estimate = -98.0004
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (13, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 106.0313 - loglik: -1.0189e+02 - logprior: -4.1363e+00
Epoch 2/2
19/19 - 1s - loss: 96.6497 - loglik: -9.4413e+01 - logprior: -2.2362e+00
Fitted a model with MAP estimate = -91.2123
expansions: [(3, 1)]
discards: [ 0 35 38]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.9025 - loglik: -9.5996e+01 - logprior: -3.9066e+00
Epoch 2/2
19/19 - 1s - loss: 95.3568 - loglik: -9.3928e+01 - logprior: -1.4284e+00
Fitted a model with MAP estimate = -90.8744
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 94.6164 - loglik: -9.0975e+01 - logprior: -3.6412e+00
Epoch 2/10
21/21 - 1s - loss: 90.6423 - loglik: -8.8861e+01 - logprior: -1.7812e+00
Epoch 3/10
21/21 - 1s - loss: 89.8269 - loglik: -8.8486e+01 - logprior: -1.3413e+00
Epoch 4/10
21/21 - 1s - loss: 88.9600 - loglik: -8.7714e+01 - logprior: -1.2464e+00
Epoch 5/10
21/21 - 1s - loss: 88.4816 - loglik: -8.7257e+01 - logprior: -1.2244e+00
Epoch 6/10
21/21 - 1s - loss: 88.3814 - loglik: -8.7172e+01 - logprior: -1.2092e+00
Epoch 7/10
21/21 - 1s - loss: 88.5468 - loglik: -8.7347e+01 - logprior: -1.1995e+00
Fitted a model with MAP estimate = -88.1550
Time for alignment: 42.9627
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.2858 - loglik: -1.5106e+02 - logprior: -3.2286e+00
Epoch 2/10
19/19 - 1s - loss: 119.9813 - loglik: -1.1848e+02 - logprior: -1.5008e+00
Epoch 3/10
19/19 - 1s - loss: 106.4043 - loglik: -1.0483e+02 - logprior: -1.5701e+00
Epoch 4/10
19/19 - 1s - loss: 103.3193 - loglik: -1.0167e+02 - logprior: -1.6476e+00
Epoch 5/10
19/19 - 1s - loss: 102.9159 - loglik: -1.0139e+02 - logprior: -1.5250e+00
Epoch 6/10
19/19 - 1s - loss: 102.3514 - loglik: -1.0082e+02 - logprior: -1.5353e+00
Epoch 7/10
19/19 - 1s - loss: 102.1413 - loglik: -1.0063e+02 - logprior: -1.5158e+00
Epoch 8/10
19/19 - 1s - loss: 101.9652 - loglik: -1.0046e+02 - logprior: -1.5086e+00
Epoch 9/10
19/19 - 1s - loss: 102.1706 - loglik: -1.0067e+02 - logprior: -1.4999e+00
Fitted a model with MAP estimate = -97.8806
expansions: [(3, 1), (5, 1), (6, 1), (7, 1), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.4688 - loglik: -1.0232e+02 - logprior: -4.1501e+00
Epoch 2/2
19/19 - 1s - loss: 97.0267 - loglik: -9.4756e+01 - logprior: -2.2712e+00
Fitted a model with MAP estimate = -91.2184
expansions: [(3, 1)]
discards: [ 0 20 36 39]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.9432 - loglik: -9.6014e+01 - logprior: -3.9290e+00
Epoch 2/2
19/19 - 1s - loss: 95.6560 - loglik: -9.4214e+01 - logprior: -1.4418e+00
Fitted a model with MAP estimate = -90.8607
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 94.5879 - loglik: -9.0948e+01 - logprior: -3.6396e+00
Epoch 2/10
21/21 - 1s - loss: 90.8384 - loglik: -8.9035e+01 - logprior: -1.8034e+00
Epoch 3/10
21/21 - 1s - loss: 89.4540 - loglik: -8.8118e+01 - logprior: -1.3361e+00
Epoch 4/10
21/21 - 1s - loss: 89.1304 - loglik: -8.7875e+01 - logprior: -1.2556e+00
Epoch 5/10
21/21 - 1s - loss: 88.9685 - loglik: -8.7736e+01 - logprior: -1.2325e+00
Epoch 6/10
21/21 - 1s - loss: 88.2809 - loglik: -8.7064e+01 - logprior: -1.2166e+00
Epoch 7/10
21/21 - 1s - loss: 88.6448 - loglik: -8.7455e+01 - logprior: -1.1901e+00
Fitted a model with MAP estimate = -88.1532
Time for alignment: 43.5827
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 154.3142 - loglik: -1.5109e+02 - logprior: -3.2286e+00
Epoch 2/10
19/19 - 1s - loss: 120.1537 - loglik: -1.1865e+02 - logprior: -1.4988e+00
Epoch 3/10
19/19 - 1s - loss: 106.7150 - loglik: -1.0516e+02 - logprior: -1.5591e+00
Epoch 4/10
19/19 - 1s - loss: 103.8021 - loglik: -1.0219e+02 - logprior: -1.6149e+00
Epoch 5/10
19/19 - 1s - loss: 102.7820 - loglik: -1.0129e+02 - logprior: -1.4906e+00
Epoch 6/10
19/19 - 1s - loss: 102.5492 - loglik: -1.0105e+02 - logprior: -1.5028e+00
Epoch 7/10
19/19 - 1s - loss: 102.1974 - loglik: -1.0072e+02 - logprior: -1.4793e+00
Epoch 8/10
19/19 - 1s - loss: 102.3427 - loglik: -1.0088e+02 - logprior: -1.4646e+00
Fitted a model with MAP estimate = -97.9673
expansions: [(3, 1), (5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: [0]
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 106.1214 - loglik: -1.0198e+02 - logprior: -4.1399e+00
Epoch 2/2
19/19 - 1s - loss: 96.9408 - loglik: -9.4648e+01 - logprior: -2.2933e+00
Fitted a model with MAP estimate = -91.2869
expansions: [(3, 1)]
discards: [ 0 20 36 39]
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 99.8075 - loglik: -9.5923e+01 - logprior: -3.8847e+00
Epoch 2/2
19/19 - 1s - loss: 95.6095 - loglik: -9.4180e+01 - logprior: -1.4290e+00
Fitted a model with MAP estimate = -90.8882
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 94.6017 - loglik: -9.0971e+01 - logprior: -3.6303e+00
Epoch 2/10
21/21 - 1s - loss: 90.7246 - loglik: -8.8868e+01 - logprior: -1.8569e+00
Epoch 3/10
21/21 - 1s - loss: 89.5734 - loglik: -8.8228e+01 - logprior: -1.3449e+00
Epoch 4/10
21/21 - 1s - loss: 89.1042 - loglik: -8.7853e+01 - logprior: -1.2512e+00
Epoch 5/10
21/21 - 1s - loss: 88.9568 - loglik: -8.7737e+01 - logprior: -1.2198e+00
Epoch 6/10
21/21 - 1s - loss: 88.2451 - loglik: -8.7030e+01 - logprior: -1.2146e+00
Epoch 7/10
21/21 - 1s - loss: 88.2772 - loglik: -8.7072e+01 - logprior: -1.2055e+00
Fitted a model with MAP estimate = -88.1658
Time for alignment: 41.3886
Computed alignments with likelihoods: ['-88.1550', '-88.1532', '-88.1658']
Best model has likelihood: -88.1532
SP score = 0.9478
Training of 3 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25cc34d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb2bc280>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 448.7757 - loglik: -4.4101e+02 - logprior: -7.7623e+00
Epoch 2/10
13/13 - 3s - loss: 390.3214 - loglik: -3.8847e+02 - logprior: -1.8524e+00
Epoch 3/10
13/13 - 3s - loss: 341.8445 - loglik: -3.4007e+02 - logprior: -1.7715e+00
Epoch 4/10
13/13 - 3s - loss: 325.8207 - loglik: -3.2379e+02 - logprior: -2.0318e+00
Epoch 5/10
13/13 - 3s - loss: 320.5879 - loglik: -3.1871e+02 - logprior: -1.8755e+00
Epoch 6/10
13/13 - 3s - loss: 318.1806 - loglik: -3.1632e+02 - logprior: -1.8631e+00
Epoch 7/10
13/13 - 3s - loss: 316.8804 - loglik: -3.1504e+02 - logprior: -1.8409e+00
Epoch 8/10
13/13 - 3s - loss: 316.6957 - loglik: -3.1488e+02 - logprior: -1.8146e+00
Epoch 9/10
13/13 - 3s - loss: 315.3968 - loglik: -3.1355e+02 - logprior: -1.8425e+00
Epoch 10/10
13/13 - 3s - loss: 315.1025 - loglik: -3.1328e+02 - logprior: -1.8258e+00
Fitted a model with MAP estimate = -314.8234
expansions: [(0, 2), (10, 1), (19, 1), (21, 1), (25, 2), (26, 1), (35, 1), (36, 1), (37, 4), (38, 1), (39, 2), (43, 1), (45, 1), (47, 1), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (82, 2), (100, 2), (101, 1), (102, 1), (105, 1), (106, 1), (114, 1), (115, 1), (116, 1)]
discards: []
Fitting a model of length 161 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 320.7575 - loglik: -3.1057e+02 - logprior: -1.0188e+01
Epoch 2/2
13/13 - 4s - loss: 296.4345 - loglik: -2.9387e+02 - logprior: -2.5618e+00
Fitted a model with MAP estimate = -293.1419
expansions: []
discards: [  0  30  48  54  88 111]
Fitting a model of length 155 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 304.9148 - loglik: -2.9581e+02 - logprior: -9.1028e+00
Epoch 2/2
13/13 - 4s - loss: 295.9251 - loglik: -2.9213e+02 - logprior: -3.7932e+00
Fitted a model with MAP estimate = -293.6201
expansions: [(0, 2), (43, 1)]
discards: [0]
Fitting a model of length 157 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 298.4728 - loglik: -2.9142e+02 - logprior: -7.0563e+00
Epoch 2/10
13/13 - 4s - loss: 290.8080 - loglik: -2.8919e+02 - logprior: -1.6136e+00
Epoch 3/10
13/13 - 4s - loss: 289.0728 - loglik: -2.8813e+02 - logprior: -9.3864e-01
Epoch 4/10
13/13 - 4s - loss: 288.2209 - loglik: -2.8756e+02 - logprior: -6.5797e-01
Epoch 5/10
13/13 - 4s - loss: 288.2584 - loglik: -2.8768e+02 - logprior: -5.7382e-01
Fitted a model with MAP estimate = -287.7074
Time for alignment: 95.2840
Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 448.5760 - loglik: -4.4080e+02 - logprior: -7.7748e+00
Epoch 2/10
13/13 - 3s - loss: 390.0256 - loglik: -3.8818e+02 - logprior: -1.8449e+00
Epoch 3/10
13/13 - 3s - loss: 341.6775 - loglik: -3.3995e+02 - logprior: -1.7264e+00
Epoch 4/10
13/13 - 3s - loss: 322.6582 - loglik: -3.2047e+02 - logprior: -2.1841e+00
Epoch 5/10
13/13 - 3s - loss: 316.4097 - loglik: -3.1431e+02 - logprior: -2.1008e+00
Epoch 6/10
13/13 - 3s - loss: 313.7645 - loglik: -3.1172e+02 - logprior: -2.0493e+00
Epoch 7/10
13/13 - 3s - loss: 313.1543 - loglik: -3.1112e+02 - logprior: -2.0315e+00
Epoch 8/10
13/13 - 3s - loss: 312.3786 - loglik: -3.1043e+02 - logprior: -1.9521e+00
Epoch 9/10
13/13 - 3s - loss: 311.5437 - loglik: -3.0961e+02 - logprior: -1.9323e+00
Epoch 10/10
13/13 - 3s - loss: 312.1511 - loglik: -3.1023e+02 - logprior: -1.9168e+00
Fitted a model with MAP estimate = -311.6271
expansions: [(0, 2), (19, 1), (21, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 1), (67, 2), (68, 1), (69, 1), (71, 2), (73, 1), (78, 1), (82, 2), (94, 5), (100, 1), (102, 1), (103, 2), (104, 2), (114, 1), (115, 1), (116, 1)]
discards: []
Fitting a model of length 165 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 8s - loss: 318.7890 - loglik: -3.0856e+02 - logprior: -1.0228e+01
Epoch 2/2
13/13 - 4s - loss: 293.3658 - loglik: -2.9076e+02 - logprior: -2.6045e+00
Fitted a model with MAP estimate = -289.4589
expansions: []
discards: [  0  29  47  87  90 110 125 126 127 140 142]
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 302.7767 - loglik: -2.9374e+02 - logprior: -9.0367e+00
Epoch 2/2
13/13 - 4s - loss: 293.3964 - loglik: -2.8967e+02 - logprior: -3.7276e+00
Fitted a model with MAP estimate = -291.3801
expansions: [(0, 2)]
discards: [ 0 43 44]
Fitting a model of length 153 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 297.3417 - loglik: -2.9034e+02 - logprior: -6.9996e+00
Epoch 2/10
13/13 - 4s - loss: 289.6030 - loglik: -2.8805e+02 - logprior: -1.5521e+00
Epoch 3/10
13/13 - 4s - loss: 287.9779 - loglik: -2.8708e+02 - logprior: -8.9614e-01
Epoch 4/10
13/13 - 4s - loss: 287.6207 - loglik: -2.8703e+02 - logprior: -5.9445e-01
Epoch 5/10
13/13 - 4s - loss: 286.7596 - loglik: -2.8626e+02 - logprior: -5.0077e-01
Epoch 6/10
13/13 - 3s - loss: 286.0640 - loglik: -2.8563e+02 - logprior: -4.3004e-01
Epoch 7/10
13/13 - 4s - loss: 287.0361 - loglik: -2.8664e+02 - logprior: -3.9267e-01
Fitted a model with MAP estimate = -286.2006
Time for alignment: 100.0870
Fitting a model of length 122 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 449.0443 - loglik: -4.4128e+02 - logprior: -7.7691e+00
Epoch 2/10
13/13 - 3s - loss: 387.8555 - loglik: -3.8601e+02 - logprior: -1.8408e+00
Epoch 3/10
13/13 - 3s - loss: 341.9901 - loglik: -3.4019e+02 - logprior: -1.7952e+00
Epoch 4/10
13/13 - 3s - loss: 326.3210 - loglik: -3.2421e+02 - logprior: -2.1081e+00
Epoch 5/10
13/13 - 3s - loss: 321.8799 - loglik: -3.1993e+02 - logprior: -1.9452e+00
Epoch 6/10
13/13 - 3s - loss: 319.6414 - loglik: -3.1773e+02 - logprior: -1.9159e+00
Epoch 7/10
13/13 - 3s - loss: 319.0427 - loglik: -3.1716e+02 - logprior: -1.8859e+00
Epoch 8/10
13/13 - 3s - loss: 317.3419 - loglik: -3.1551e+02 - logprior: -1.8368e+00
Epoch 9/10
13/13 - 3s - loss: 316.2740 - loglik: -3.1442e+02 - logprior: -1.8519e+00
Epoch 10/10
13/13 - 3s - loss: 315.4612 - loglik: -3.1362e+02 - logprior: -1.8381e+00
Fitted a model with MAP estimate = -315.2811
expansions: [(0, 2), (9, 2), (21, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (47, 1), (68, 1), (69, 2), (71, 2), (72, 1), (77, 1), (78, 1), (79, 1), (81, 2), (99, 1), (100, 1), (102, 1), (103, 2), (104, 2), (105, 1), (115, 1), (116, 1)]
discards: []
Fitting a model of length 162 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 321.4497 - loglik: -3.1118e+02 - logprior: -1.0272e+01
Epoch 2/2
13/13 - 4s - loss: 296.3642 - loglik: -2.9369e+02 - logprior: -2.6742e+00
Fitted a model with MAP estimate = -291.5678
expansions: []
discards: [  0  12  30  48  94 137 139]
Fitting a model of length 155 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 303.2691 - loglik: -2.9411e+02 - logprior: -9.1610e+00
Epoch 2/2
13/13 - 4s - loss: 294.3553 - loglik: -2.9052e+02 - logprior: -3.8359e+00
Fitted a model with MAP estimate = -291.7892
expansions: [(0, 2), (40, 1)]
discards: [  0  43  44 105]
Fitting a model of length 154 on 4795 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 298.4452 - loglik: -2.9141e+02 - logprior: -7.0359e+00
Epoch 2/10
13/13 - 4s - loss: 290.7433 - loglik: -2.8910e+02 - logprior: -1.6430e+00
Epoch 3/10
13/13 - 4s - loss: 288.7840 - loglik: -2.8780e+02 - logprior: -9.8334e-01
Epoch 4/10
13/13 - 4s - loss: 288.4660 - loglik: -2.8777e+02 - logprior: -6.9965e-01
Epoch 5/10
13/13 - 4s - loss: 287.3491 - loglik: -2.8675e+02 - logprior: -5.9946e-01
Epoch 6/10
13/13 - 4s - loss: 287.8998 - loglik: -2.8736e+02 - logprior: -5.3559e-01
Fitted a model with MAP estimate = -287.1256
Time for alignment: 97.3029
Computed alignments with likelihoods: ['-287.7074', '-286.2006', '-287.1256']
Best model has likelihood: -286.2006
SP score = 0.5542
Training of 3 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c554160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2a637c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 16s - loss: 818.1588 - loglik: -8.1473e+02 - logprior: -3.4279e+00
Epoch 2/10
17/17 - 13s - loss: 711.6215 - loglik: -7.1082e+02 - logprior: -8.0552e-01
Epoch 3/10
17/17 - 13s - loss: 647.6400 - loglik: -6.4611e+02 - logprior: -1.5280e+00
Epoch 4/10
17/17 - 13s - loss: 628.5901 - loglik: -6.2694e+02 - logprior: -1.6456e+00
Epoch 5/10
17/17 - 13s - loss: 622.3936 - loglik: -6.2072e+02 - logprior: -1.6747e+00
Epoch 6/10
17/17 - 13s - loss: 620.1798 - loglik: -6.1853e+02 - logprior: -1.6513e+00
Epoch 7/10
17/17 - 13s - loss: 618.9069 - loglik: -6.1730e+02 - logprior: -1.6116e+00
Epoch 8/10
17/17 - 13s - loss: 617.8468 - loglik: -6.1624e+02 - logprior: -1.6037e+00
Epoch 9/10
17/17 - 13s - loss: 618.2777 - loglik: -6.1667e+02 - logprior: -1.6124e+00
Fitted a model with MAP estimate = -617.4733
expansions: [(9, 1), (12, 1), (14, 1), (15, 3), (16, 1), (17, 5), (29, 2), (52, 1), (54, 1), (56, 6), (65, 2), (66, 1), (73, 2), (89, 1), (94, 2), (95, 2), (96, 2), (99, 1), (101, 1), (102, 3), (104, 1), (109, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (147, 1), (165, 1), (167, 1), (172, 2), (173, 2), (175, 1), (187, 3), (188, 2), (190, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Fitting a model of length 303 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 603.3914 - loglik: -6.0005e+02 - logprior: -3.3456e+00
Epoch 2/2
34/34 - 20s - loss: 582.7034 - loglik: -5.8134e+02 - logprior: -1.3650e+00
Fitted a model with MAP estimate = -577.5008
expansions: [(85, 1)]
discards: [ 19  72  73  74  86  97 122 123 125 226]
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 589.1532 - loglik: -5.8707e+02 - logprior: -2.0806e+00
Epoch 2/2
34/34 - 20s - loss: 583.0834 - loglik: -5.8272e+02 - logprior: -3.6786e-01
Fitted a model with MAP estimate = -580.8975
expansions: [(61, 1), (73, 1), (181, 1), (233, 3)]
discards: [82]
Fitting a model of length 299 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 584.8546 - loglik: -5.8300e+02 - logprior: -1.8539e+00
Epoch 2/10
34/34 - 19s - loss: 574.1805 - loglik: -5.7406e+02 - logprior: -1.2527e-01
Epoch 3/10
34/34 - 20s - loss: 572.5278 - loglik: -5.7254e+02 - logprior: 0.0129
Epoch 4/10
34/34 - 20s - loss: 571.2008 - loglik: -5.7135e+02 - logprior: 0.1476
Epoch 5/10
34/34 - 20s - loss: 569.9984 - loglik: -5.7031e+02 - logprior: 0.3068
Epoch 6/10
34/34 - 20s - loss: 569.2023 - loglik: -5.6965e+02 - logprior: 0.4487
Epoch 7/10
34/34 - 20s - loss: 569.3549 - loglik: -5.6997e+02 - logprior: 0.6148
Fitted a model with MAP estimate = -569.0780
Time for alignment: 447.9686
Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 16s - loss: 818.8251 - loglik: -8.1539e+02 - logprior: -3.4309e+00
Epoch 2/10
17/17 - 13s - loss: 713.6008 - loglik: -7.1279e+02 - logprior: -8.1288e-01
Epoch 3/10
17/17 - 14s - loss: 646.7924 - loglik: -6.4526e+02 - logprior: -1.5347e+00
Epoch 4/10
17/17 - 13s - loss: 632.9133 - loglik: -6.3125e+02 - logprior: -1.6594e+00
Epoch 5/10
17/17 - 13s - loss: 625.1600 - loglik: -6.2348e+02 - logprior: -1.6753e+00
Epoch 6/10
17/17 - 14s - loss: 623.4040 - loglik: -6.2175e+02 - logprior: -1.6518e+00
Epoch 7/10
17/17 - 13s - loss: 621.3495 - loglik: -6.1972e+02 - logprior: -1.6293e+00
Epoch 8/10
17/17 - 13s - loss: 621.1839 - loglik: -6.1954e+02 - logprior: -1.6398e+00
Epoch 9/10
17/17 - 14s - loss: 620.4089 - loglik: -6.1876e+02 - logprior: -1.6524e+00
Epoch 10/10
17/17 - 13s - loss: 620.8120 - loglik: -6.1913e+02 - logprior: -1.6808e+00
Fitted a model with MAP estimate = -619.1734
expansions: [(13, 1), (14, 1), (16, 3), (17, 6), (29, 1), (30, 2), (51, 3), (55, 4), (57, 2), (65, 1), (66, 1), (89, 1), (95, 2), (96, 2), (99, 1), (100, 2), (102, 1), (104, 1), (110, 1), (111, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (166, 1), (167, 3), (171, 1), (172, 1), (173, 1), (175, 1), (178, 1), (185, 2), (186, 2), (188, 1), (190, 1), (201, 1), (203, 2), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Fitting a model of length 304 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 601.5275 - loglik: -5.9814e+02 - logprior: -3.3922e+00
Epoch 2/2
34/34 - 20s - loss: 577.3751 - loglik: -5.7575e+02 - logprior: -1.6229e+00
Fitted a model with MAP estimate = -572.9062
expansions: [(23, 3), (186, 1), (213, 1), (215, 1), (240, 1), (241, 1)]
discards: [ 17  40  72  73  74  75 223 264]
Fitting a model of length 304 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 583.9796 - loglik: -5.8194e+02 - logprior: -2.0391e+00
Epoch 2/2
34/34 - 20s - loss: 576.5715 - loglik: -5.7621e+02 - logprior: -3.5882e-01
Fitted a model with MAP estimate = -574.5231
expansions: []
discards: [ 23  24 119]
Fitting a model of length 301 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 581.5347 - loglik: -5.7966e+02 - logprior: -1.8731e+00
Epoch 2/10
34/34 - 20s - loss: 577.0522 - loglik: -5.7698e+02 - logprior: -7.3293e-02
Epoch 3/10
34/34 - 20s - loss: 574.9248 - loglik: -5.7499e+02 - logprior: 0.0667
Epoch 4/10
34/34 - 19s - loss: 572.9530 - loglik: -5.7314e+02 - logprior: 0.1911
Epoch 5/10
34/34 - 20s - loss: 571.9133 - loglik: -5.7228e+02 - logprior: 0.3676
Epoch 6/10
34/34 - 20s - loss: 573.0598 - loglik: -5.7358e+02 - logprior: 0.5239
Fitted a model with MAP estimate = -571.8185
Time for alignment: 447.5534
Fitting a model of length 236 on 7632 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 16s - loss: 818.7793 - loglik: -8.1535e+02 - logprior: -3.4333e+00
Epoch 2/10
17/17 - 13s - loss: 712.7592 - loglik: -7.1196e+02 - logprior: -7.9633e-01
Epoch 3/10
17/17 - 13s - loss: 649.2474 - loglik: -6.4776e+02 - logprior: -1.4846e+00
Epoch 4/10
17/17 - 13s - loss: 631.3742 - loglik: -6.2974e+02 - logprior: -1.6324e+00
Epoch 5/10
17/17 - 14s - loss: 624.7501 - loglik: -6.2308e+02 - logprior: -1.6733e+00
Epoch 6/10
17/17 - 13s - loss: 623.9830 - loglik: -6.2234e+02 - logprior: -1.6406e+00
Epoch 7/10
17/17 - 13s - loss: 621.5427 - loglik: -6.1995e+02 - logprior: -1.5972e+00
Epoch 8/10
17/17 - 13s - loss: 620.8793 - loglik: -6.1930e+02 - logprior: -1.5803e+00
Epoch 9/10
17/17 - 13s - loss: 619.5010 - loglik: -6.1792e+02 - logprior: -1.5807e+00
Epoch 10/10
17/17 - 13s - loss: 619.5963 - loglik: -6.1800e+02 - logprior: -1.5926e+00
Fitted a model with MAP estimate = -618.9824
expansions: [(9, 2), (14, 1), (16, 3), (18, 3), (23, 1), (29, 3), (52, 2), (55, 1), (56, 3), (65, 1), (66, 1), (93, 1), (94, 1), (95, 2), (96, 2), (99, 1), (100, 2), (102, 1), (104, 2), (109, 1), (129, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (147, 1), (165, 1), (172, 3), (173, 2), (175, 1), (178, 1), (188, 1), (193, 1), (203, 1), (204, 3), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Fitting a model of length 296 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 605.9657 - loglik: -6.0268e+02 - logprior: -3.2852e+00
Epoch 2/2
34/34 - 19s - loss: 586.0814 - loglik: -5.8479e+02 - logprior: -1.2910e+00
Fitted a model with MAP estimate = -582.4682
expansions: [(38, 1), (61, 1), (183, 1), (259, 1)]
discards: [ 64 118 120]
Fitting a model of length 297 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 587.8231 - loglik: -5.8579e+02 - logprior: -2.0329e+00
Epoch 2/2
34/34 - 20s - loss: 581.2867 - loglik: -5.8102e+02 - logprior: -2.6644e-01
Fitted a model with MAP estimate = -579.4924
expansions: [(41, 1), (73, 1)]
discards: []
Fitting a model of length 299 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 584.8736 - loglik: -5.8310e+02 - logprior: -1.7720e+00
Epoch 2/10
34/34 - 20s - loss: 579.8370 - loglik: -5.7986e+02 - logprior: 0.0249
Epoch 3/10
34/34 - 20s - loss: 578.6926 - loglik: -5.7894e+02 - logprior: 0.2460
Epoch 4/10
34/34 - 20s - loss: 576.8309 - loglik: -5.7720e+02 - logprior: 0.3658
Epoch 5/10
34/34 - 20s - loss: 574.8087 - loglik: -5.7530e+02 - logprior: 0.4907
Epoch 6/10
34/34 - 20s - loss: 576.8171 - loglik: -5.7749e+02 - logprior: 0.6695
Fitted a model with MAP estimate = -575.5250
Time for alignment: 440.1366
Computed alignments with likelihoods: ['-569.0780', '-571.8185', '-575.5250']
Best model has likelihood: -569.0780
SP score = 0.7227
Training of 3 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb600790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fa4c4730>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 271.2259 - loglik: -1.8299e+02 - logprior: -8.8233e+01
Epoch 2/10
10/10 - 1s - loss: 183.5547 - loglik: -1.5984e+02 - logprior: -2.3719e+01
Epoch 3/10
10/10 - 1s - loss: 152.8556 - loglik: -1.4167e+02 - logprior: -1.1190e+01
Epoch 4/10
10/10 - 1s - loss: 138.6717 - loglik: -1.3209e+02 - logprior: -6.5788e+00
Epoch 5/10
10/10 - 1s - loss: 132.2140 - loglik: -1.2806e+02 - logprior: -4.1563e+00
Epoch 6/10
10/10 - 1s - loss: 128.9168 - loglik: -1.2610e+02 - logprior: -2.8120e+00
Epoch 7/10
10/10 - 1s - loss: 127.2693 - loglik: -1.2536e+02 - logprior: -1.9134e+00
Epoch 8/10
10/10 - 1s - loss: 126.2892 - loglik: -1.2495e+02 - logprior: -1.3416e+00
Epoch 9/10
10/10 - 1s - loss: 125.6185 - loglik: -1.2463e+02 - logprior: -9.8880e-01
Epoch 10/10
10/10 - 1s - loss: 125.0766 - loglik: -1.2433e+02 - logprior: -7.4551e-01
Fitted a model with MAP estimate = -124.8122
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.4536 - loglik: -1.2283e+02 - logprior: -9.8619e+01
Epoch 2/2
10/10 - 1s - loss: 156.8662 - loglik: -1.1663e+02 - logprior: -4.0235e+01
Fitted a model with MAP estimate = -145.3498
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.3294 - loglik: -1.1257e+02 - logprior: -7.8760e+01
Epoch 2/2
10/10 - 1s - loss: 130.9729 - loglik: -1.1025e+02 - logprior: -2.0725e+01
Fitted a model with MAP estimate = -122.0349
expansions: []
discards: []
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 186.9506 - loglik: -1.0964e+02 - logprior: -7.7308e+01
Epoch 2/10
10/10 - 1s - loss: 129.5802 - loglik: -1.0951e+02 - logprior: -2.0072e+01
Epoch 3/10
10/10 - 1s - loss: 117.8770 - loglik: -1.0966e+02 - logprior: -8.2148e+00
Epoch 4/10
10/10 - 1s - loss: 113.3455 - loglik: -1.1017e+02 - logprior: -3.1735e+00
Epoch 5/10
10/10 - 1s - loss: 111.0456 - loglik: -1.1069e+02 - logprior: -3.6056e-01
Epoch 6/10
10/10 - 1s - loss: 109.7617 - loglik: -1.1104e+02 - logprior: 1.2809
Epoch 7/10
10/10 - 1s - loss: 108.9827 - loglik: -1.1123e+02 - logprior: 2.2509
Epoch 8/10
10/10 - 1s - loss: 108.4513 - loglik: -1.1131e+02 - logprior: 2.8583
Epoch 9/10
10/10 - 1s - loss: 108.0712 - loglik: -1.1140e+02 - logprior: 3.3322
Epoch 10/10
10/10 - 1s - loss: 107.7598 - loglik: -1.1147e+02 - logprior: 3.7140
Fitted a model with MAP estimate = -107.6060
Time for alignment: 30.2974
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.2259 - loglik: -1.8299e+02 - logprior: -8.8233e+01
Epoch 2/10
10/10 - 1s - loss: 183.5547 - loglik: -1.5984e+02 - logprior: -2.3719e+01
Epoch 3/10
10/10 - 1s - loss: 152.8556 - loglik: -1.4167e+02 - logprior: -1.1190e+01
Epoch 4/10
10/10 - 1s - loss: 138.6717 - loglik: -1.3209e+02 - logprior: -6.5788e+00
Epoch 5/10
10/10 - 1s - loss: 132.2140 - loglik: -1.2806e+02 - logprior: -4.1563e+00
Epoch 6/10
10/10 - 1s - loss: 128.9168 - loglik: -1.2610e+02 - logprior: -2.8120e+00
Epoch 7/10
10/10 - 1s - loss: 127.2693 - loglik: -1.2536e+02 - logprior: -1.9134e+00
Epoch 8/10
10/10 - 1s - loss: 126.2892 - loglik: -1.2495e+02 - logprior: -1.3416e+00
Epoch 9/10
10/10 - 1s - loss: 125.6185 - loglik: -1.2463e+02 - logprior: -9.8880e-01
Epoch 10/10
10/10 - 1s - loss: 125.0766 - loglik: -1.2433e+02 - logprior: -7.4551e-01
Fitted a model with MAP estimate = -124.8122
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 221.4536 - loglik: -1.2283e+02 - logprior: -9.8619e+01
Epoch 2/2
10/10 - 1s - loss: 156.8662 - loglik: -1.1663e+02 - logprior: -4.0235e+01
Fitted a model with MAP estimate = -145.3498
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 191.3294 - loglik: -1.1257e+02 - logprior: -7.8760e+01
Epoch 2/2
10/10 - 1s - loss: 130.9729 - loglik: -1.1025e+02 - logprior: -2.0725e+01
Fitted a model with MAP estimate = -122.0349
expansions: []
discards: []
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 186.9506 - loglik: -1.0964e+02 - logprior: -7.7308e+01
Epoch 2/10
10/10 - 1s - loss: 129.5802 - loglik: -1.0951e+02 - logprior: -2.0072e+01
Epoch 3/10
10/10 - 1s - loss: 117.8770 - loglik: -1.0966e+02 - logprior: -8.2148e+00
Epoch 4/10
10/10 - 1s - loss: 113.3455 - loglik: -1.1017e+02 - logprior: -3.1735e+00
Epoch 5/10
10/10 - 1s - loss: 111.0456 - loglik: -1.1069e+02 - logprior: -3.6056e-01
Epoch 6/10
10/10 - 1s - loss: 109.7617 - loglik: -1.1104e+02 - logprior: 1.2809
Epoch 7/10
10/10 - 1s - loss: 108.9827 - loglik: -1.1123e+02 - logprior: 2.2509
Epoch 8/10
10/10 - 1s - loss: 108.4513 - loglik: -1.1131e+02 - logprior: 2.8583
Epoch 9/10
10/10 - 1s - loss: 108.0712 - loglik: -1.1140e+02 - logprior: 3.3322
Epoch 10/10
10/10 - 1s - loss: 107.7598 - loglik: -1.1147e+02 - logprior: 3.7140
Fitted a model with MAP estimate = -107.6060
Time for alignment: 28.7180
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 271.2259 - loglik: -1.8299e+02 - logprior: -8.8233e+01
Epoch 2/10
10/10 - 1s - loss: 183.5547 - loglik: -1.5984e+02 - logprior: -2.3719e+01
Epoch 3/10
10/10 - 1s - loss: 152.8556 - loglik: -1.4167e+02 - logprior: -1.1190e+01
Epoch 4/10
10/10 - 1s - loss: 138.6717 - loglik: -1.3209e+02 - logprior: -6.5788e+00
Epoch 5/10
10/10 - 1s - loss: 132.2140 - loglik: -1.2806e+02 - logprior: -4.1563e+00
Epoch 6/10
10/10 - 1s - loss: 128.9168 - loglik: -1.2610e+02 - logprior: -2.8120e+00
Epoch 7/10
10/10 - 1s - loss: 127.2693 - loglik: -1.2536e+02 - logprior: -1.9134e+00
Epoch 8/10
10/10 - 1s - loss: 126.2892 - loglik: -1.2495e+02 - logprior: -1.3416e+00
Epoch 9/10
10/10 - 1s - loss: 125.6185 - loglik: -1.2463e+02 - logprior: -9.8880e-01
Epoch 10/10
10/10 - 1s - loss: 125.0766 - loglik: -1.2433e+02 - logprior: -7.4551e-01
Fitted a model with MAP estimate = -124.8122
expansions: [(7, 2), (8, 4), (19, 1), (22, 1), (23, 3), (38, 3), (39, 1), (40, 1)]
discards: [0]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 221.4536 - loglik: -1.2283e+02 - logprior: -9.8619e+01
Epoch 2/2
10/10 - 1s - loss: 156.8662 - loglik: -1.1663e+02 - logprior: -4.0235e+01
Fitted a model with MAP estimate = -145.3498
expansions: [(0, 1), (48, 1)]
discards: [ 0 30]
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 191.3294 - loglik: -1.1257e+02 - logprior: -7.8760e+01
Epoch 2/2
10/10 - 1s - loss: 130.9729 - loglik: -1.1025e+02 - logprior: -2.0725e+01
Fitted a model with MAP estimate = -122.0349
expansions: []
discards: []
Fitting a model of length 64 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 186.9506 - loglik: -1.0964e+02 - logprior: -7.7308e+01
Epoch 2/10
10/10 - 1s - loss: 129.5802 - loglik: -1.0951e+02 - logprior: -2.0072e+01
Epoch 3/10
10/10 - 1s - loss: 117.8770 - loglik: -1.0966e+02 - logprior: -8.2148e+00
Epoch 4/10
10/10 - 1s - loss: 113.3455 - loglik: -1.1017e+02 - logprior: -3.1735e+00
Epoch 5/10
10/10 - 1s - loss: 111.0456 - loglik: -1.1069e+02 - logprior: -3.6056e-01
Epoch 6/10
10/10 - 1s - loss: 109.7617 - loglik: -1.1104e+02 - logprior: 1.2809
Epoch 7/10
10/10 - 1s - loss: 108.9827 - loglik: -1.1123e+02 - logprior: 2.2509
Epoch 8/10
10/10 - 1s - loss: 108.4513 - loglik: -1.1131e+02 - logprior: 2.8583
Epoch 9/10
10/10 - 1s - loss: 108.0712 - loglik: -1.1140e+02 - logprior: 3.3322
Epoch 10/10
10/10 - 1s - loss: 107.7598 - loglik: -1.1147e+02 - logprior: 3.7140
Fitted a model with MAP estimate = -107.6060
Time for alignment: 27.7717
Computed alignments with likelihoods: ['-107.6060', '-107.6060', '-107.6060']
Best model has likelihood: -107.6060
SP score = 0.8518
Training of 3 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c0b0760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25df17ac0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 316.6493 - loglik: -3.0895e+02 - logprior: -7.6964e+00
Epoch 2/10
13/13 - 1s - loss: 285.4771 - loglik: -2.8361e+02 - logprior: -1.8672e+00
Epoch 3/10
13/13 - 1s - loss: 263.4830 - loglik: -2.6183e+02 - logprior: -1.6504e+00
Epoch 4/10
13/13 - 1s - loss: 252.9295 - loglik: -2.5101e+02 - logprior: -1.9227e+00
Epoch 5/10
13/13 - 1s - loss: 250.1589 - loglik: -2.4831e+02 - logprior: -1.8463e+00
Epoch 6/10
13/13 - 1s - loss: 249.2962 - loglik: -2.4755e+02 - logprior: -1.7495e+00
Epoch 7/10
13/13 - 1s - loss: 248.6202 - loglik: -2.4686e+02 - logprior: -1.7645e+00
Epoch 8/10
13/13 - 1s - loss: 248.2822 - loglik: -2.4652e+02 - logprior: -1.7585e+00
Epoch 9/10
13/13 - 1s - loss: 248.2621 - loglik: -2.4653e+02 - logprior: -1.7342e+00
Epoch 10/10
13/13 - 1s - loss: 248.3671 - loglik: -2.4665e+02 - logprior: -1.7204e+00
Fitted a model with MAP estimate = -248.1419
expansions: [(9, 1), (10, 1), (13, 1), (18, 1), (21, 1), (27, 1), (28, 1), (29, 2), (30, 3), (31, 1), (40, 1), (43, 1), (50, 1), (53, 1), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 257.4426 - loglik: -2.4847e+02 - logprior: -8.9709e+00
Epoch 2/2
13/13 - 2s - loss: 247.7614 - loglik: -2.4380e+02 - logprior: -3.9639e+00
Fitted a model with MAP estimate = -246.2334
expansions: [(0, 2)]
discards: [ 0 94]
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 249.4527 - loglik: -2.4266e+02 - logprior: -6.7915e+00
Epoch 2/2
13/13 - 2s - loss: 243.9296 - loglik: -2.4214e+02 - logprior: -1.7870e+00
Fitted a model with MAP estimate = -242.8102
expansions: []
discards: [ 0 38 90]
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 252.5631 - loglik: -2.4401e+02 - logprior: -8.5499e+00
Epoch 2/10
13/13 - 2s - loss: 245.6523 - loglik: -2.4302e+02 - logprior: -2.6332e+00
Epoch 3/10
13/13 - 2s - loss: 243.3416 - loglik: -2.4210e+02 - logprior: -1.2455e+00
Epoch 4/10
13/13 - 2s - loss: 242.5224 - loglik: -2.4155e+02 - logprior: -9.7268e-01
Epoch 5/10
13/13 - 2s - loss: 242.7107 - loglik: -2.4187e+02 - logprior: -8.4327e-01
Fitted a model with MAP estimate = -242.2230
Time for alignment: 48.9934
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 316.3812 - loglik: -3.0869e+02 - logprior: -7.6927e+00
Epoch 2/10
13/13 - 1s - loss: 285.4165 - loglik: -2.8354e+02 - logprior: -1.8755e+00
Epoch 3/10
13/13 - 1s - loss: 261.9204 - loglik: -2.6024e+02 - logprior: -1.6787e+00
Epoch 4/10
13/13 - 1s - loss: 252.1374 - loglik: -2.5022e+02 - logprior: -1.9196e+00
Epoch 5/10
13/13 - 1s - loss: 250.4931 - loglik: -2.4868e+02 - logprior: -1.8170e+00
Epoch 6/10
13/13 - 1s - loss: 249.4483 - loglik: -2.4774e+02 - logprior: -1.7122e+00
Epoch 7/10
13/13 - 1s - loss: 248.3612 - loglik: -2.4661e+02 - logprior: -1.7489e+00
Epoch 8/10
13/13 - 1s - loss: 248.5063 - loglik: -2.4675e+02 - logprior: -1.7567e+00
Fitted a model with MAP estimate = -248.2094
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (19, 1), (27, 1), (28, 1), (29, 2), (30, 3), (31, 1), (40, 1), (43, 1), (50, 1), (53, 1), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 257.8862 - loglik: -2.4890e+02 - logprior: -8.9885e+00
Epoch 2/2
13/13 - 2s - loss: 248.3799 - loglik: -2.4436e+02 - logprior: -4.0149e+00
Fitted a model with MAP estimate = -246.3465
expansions: [(0, 2)]
discards: [ 0 14 95]
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 249.7808 - loglik: -2.4298e+02 - logprior: -6.8005e+00
Epoch 2/2
13/13 - 2s - loss: 243.4940 - loglik: -2.4170e+02 - logprior: -1.7954e+00
Fitted a model with MAP estimate = -242.8218
expansions: []
discards: [ 0 37 38]
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 252.6055 - loglik: -2.4407e+02 - logprior: -8.5402e+00
Epoch 2/10
13/13 - 2s - loss: 245.2194 - loglik: -2.4260e+02 - logprior: -2.6194e+00
Epoch 3/10
13/13 - 2s - loss: 244.0511 - loglik: -2.4280e+02 - logprior: -1.2466e+00
Epoch 4/10
13/13 - 2s - loss: 242.7128 - loglik: -2.4173e+02 - logprior: -9.8349e-01
Epoch 5/10
13/13 - 2s - loss: 242.5352 - loglik: -2.4170e+02 - logprior: -8.3965e-01
Epoch 6/10
13/13 - 2s - loss: 242.3440 - loglik: -2.4152e+02 - logprior: -8.2534e-01
Epoch 7/10
13/13 - 2s - loss: 241.8814 - loglik: -2.4108e+02 - logprior: -7.9703e-01
Epoch 8/10
13/13 - 2s - loss: 242.4942 - loglik: -2.4172e+02 - logprior: -7.7325e-01
Fitted a model with MAP estimate = -241.9589
Time for alignment: 51.2072
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 316.9415 - loglik: -3.0924e+02 - logprior: -7.6973e+00
Epoch 2/10
13/13 - 1s - loss: 284.9215 - loglik: -2.8305e+02 - logprior: -1.8706e+00
Epoch 3/10
13/13 - 1s - loss: 261.3365 - loglik: -2.5970e+02 - logprior: -1.6412e+00
Epoch 4/10
13/13 - 1s - loss: 251.5812 - loglik: -2.4975e+02 - logprior: -1.8314e+00
Epoch 5/10
13/13 - 1s - loss: 250.1142 - loglik: -2.4837e+02 - logprior: -1.7434e+00
Epoch 6/10
13/13 - 1s - loss: 249.2360 - loglik: -2.4758e+02 - logprior: -1.6556e+00
Epoch 7/10
13/13 - 1s - loss: 249.0098 - loglik: -2.4735e+02 - logprior: -1.6605e+00
Epoch 8/10
13/13 - 1s - loss: 248.6961 - loglik: -2.4703e+02 - logprior: -1.6687e+00
Epoch 9/10
13/13 - 1s - loss: 248.7635 - loglik: -2.4712e+02 - logprior: -1.6387e+00
Fitted a model with MAP estimate = -248.5446
expansions: [(9, 1), (10, 1), (13, 1), (15, 1), (28, 4), (29, 3), (30, 1), (31, 1), (39, 1), (40, 1), (50, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 257.9203 - loglik: -2.4896e+02 - logprior: -8.9606e+00
Epoch 2/2
13/13 - 2s - loss: 248.0523 - loglik: -2.4407e+02 - logprior: -3.9858e+00
Fitted a model with MAP estimate = -246.1471
expansions: [(0, 2)]
discards: [ 0 35 66 95]
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 249.7710 - loglik: -2.4298e+02 - logprior: -6.7887e+00
Epoch 2/2
13/13 - 2s - loss: 243.5419 - loglik: -2.4174e+02 - logprior: -1.8043e+00
Fitted a model with MAP estimate = -242.8297
expansions: []
discards: [0]
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 252.4828 - loglik: -2.4390e+02 - logprior: -8.5804e+00
Epoch 2/10
13/13 - 2s - loss: 245.2302 - loglik: -2.4256e+02 - logprior: -2.6714e+00
Epoch 3/10
13/13 - 1s - loss: 243.5719 - loglik: -2.4231e+02 - logprior: -1.2589e+00
Epoch 4/10
13/13 - 2s - loss: 242.6113 - loglik: -2.4164e+02 - logprior: -9.7283e-01
Epoch 5/10
13/13 - 2s - loss: 242.5299 - loglik: -2.4168e+02 - logprior: -8.4819e-01
Epoch 6/10
13/13 - 2s - loss: 242.3436 - loglik: -2.4153e+02 - logprior: -8.1109e-01
Epoch 7/10
13/13 - 2s - loss: 242.0554 - loglik: -2.4126e+02 - logprior: -7.9387e-01
Epoch 8/10
13/13 - 2s - loss: 241.9422 - loglik: -2.4118e+02 - logprior: -7.6697e-01
Epoch 9/10
13/13 - 2s - loss: 241.7868 - loglik: -2.4104e+02 - logprior: -7.4207e-01
Epoch 10/10
13/13 - 2s - loss: 241.9408 - loglik: -2.4123e+02 - logprior: -7.0822e-01
Fitted a model with MAP estimate = -241.7285
Time for alignment: 54.3149
Computed alignments with likelihoods: ['-242.2230', '-241.9589', '-241.7285']
Best model has likelihood: -241.7285
SP score = 0.9389
Training of 3 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb1baa00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c9beb50>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.4084 - loglik: -3.4684e+02 - logprior: -8.5566e+01
Epoch 2/10
10/10 - 1s - loss: 327.9050 - loglik: -3.0808e+02 - logprior: -1.9823e+01
Epoch 3/10
10/10 - 1s - loss: 276.7765 - loglik: -2.6879e+02 - logprior: -7.9904e+00
Epoch 4/10
10/10 - 1s - loss: 245.4774 - loglik: -2.4096e+02 - logprior: -4.5149e+00
Epoch 5/10
10/10 - 1s - loss: 231.7047 - loglik: -2.2928e+02 - logprior: -2.4224e+00
Epoch 6/10
10/10 - 1s - loss: 226.1646 - loglik: -2.2502e+02 - logprior: -1.1488e+00
Epoch 7/10
10/10 - 1s - loss: 223.2769 - loglik: -2.2305e+02 - logprior: -2.2660e-01
Epoch 8/10
10/10 - 1s - loss: 221.5715 - loglik: -2.2196e+02 - logprior: 0.3885
Epoch 9/10
10/10 - 1s - loss: 220.6325 - loglik: -2.2147e+02 - logprior: 0.8343
Epoch 10/10
10/10 - 1s - loss: 220.0365 - loglik: -2.2128e+02 - logprior: 1.2480
Fitted a model with MAP estimate = -219.7990
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 312.3551 - loglik: -2.1681e+02 - logprior: -9.5544e+01
Epoch 2/2
10/10 - 1s - loss: 236.9229 - loglik: -2.0079e+02 - logprior: -3.6131e+01
Fitted a model with MAP estimate = -223.9264
expansions: [(0, 3)]
discards: [  0  35 112 113]
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 272.7753 - loglik: -1.9772e+02 - logprior: -7.5055e+01
Epoch 2/2
10/10 - 1s - loss: 209.7383 - loglik: -1.9372e+02 - logprior: -1.6022e+01
Fitted a model with MAP estimate = -200.0821
expansions: []
discards: [0 1]
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 286.1806 - loglik: -1.9638e+02 - logprior: -8.9797e+01
Epoch 2/10
10/10 - 1s - loss: 217.5431 - loglik: -1.9499e+02 - logprior: -2.2555e+01
Epoch 3/10
10/10 - 1s - loss: 198.5089 - loglik: -1.9425e+02 - logprior: -4.2585e+00
Epoch 4/10
10/10 - 1s - loss: 191.6660 - loglik: -1.9392e+02 - logprior: 2.2560
Epoch 5/10
10/10 - 1s - loss: 188.4358 - loglik: -1.9387e+02 - logprior: 5.4346
Epoch 6/10
10/10 - 1s - loss: 186.6245 - loglik: -1.9391e+02 - logprior: 7.2806
Epoch 7/10
10/10 - 1s - loss: 185.4809 - loglik: -1.9395e+02 - logprior: 8.4731
Epoch 8/10
10/10 - 1s - loss: 184.6744 - loglik: -1.9400e+02 - logprior: 9.3285
Epoch 9/10
10/10 - 1s - loss: 184.0325 - loglik: -1.9408e+02 - logprior: 10.0474
Epoch 10/10
10/10 - 1s - loss: 183.4712 - loglik: -1.9417e+02 - logprior: 10.7024
Fitted a model with MAP estimate = -183.1833
Time for alignment: 45.8070
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.4084 - loglik: -3.4684e+02 - logprior: -8.5566e+01
Epoch 2/10
10/10 - 1s - loss: 327.9050 - loglik: -3.0808e+02 - logprior: -1.9823e+01
Epoch 3/10
10/10 - 1s - loss: 276.7765 - loglik: -2.6879e+02 - logprior: -7.9904e+00
Epoch 4/10
10/10 - 1s - loss: 245.4774 - loglik: -2.4096e+02 - logprior: -4.5149e+00
Epoch 5/10
10/10 - 1s - loss: 231.7047 - loglik: -2.2928e+02 - logprior: -2.4224e+00
Epoch 6/10
10/10 - 1s - loss: 226.1646 - loglik: -2.2502e+02 - logprior: -1.1488e+00
Epoch 7/10
10/10 - 1s - loss: 223.2769 - loglik: -2.2305e+02 - logprior: -2.2660e-01
Epoch 8/10
10/10 - 1s - loss: 221.5715 - loglik: -2.2196e+02 - logprior: 0.3885
Epoch 9/10
10/10 - 1s - loss: 220.6325 - loglik: -2.2147e+02 - logprior: 0.8343
Epoch 10/10
10/10 - 1s - loss: 220.0365 - loglik: -2.2128e+02 - logprior: 1.2480
Fitted a model with MAP estimate = -219.7990
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 312.3551 - loglik: -2.1681e+02 - logprior: -9.5544e+01
Epoch 2/2
10/10 - 1s - loss: 236.9229 - loglik: -2.0079e+02 - logprior: -3.6131e+01
Fitted a model with MAP estimate = -223.9264
expansions: [(0, 3)]
discards: [  0  35 112 113]
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 272.7753 - loglik: -1.9772e+02 - logprior: -7.5055e+01
Epoch 2/2
10/10 - 1s - loss: 209.7383 - loglik: -1.9372e+02 - logprior: -1.6022e+01
Fitted a model with MAP estimate = -200.0821
expansions: []
discards: [0 1]
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 286.1806 - loglik: -1.9638e+02 - logprior: -8.9797e+01
Epoch 2/10
10/10 - 1s - loss: 217.5431 - loglik: -1.9499e+02 - logprior: -2.2555e+01
Epoch 3/10
10/10 - 1s - loss: 198.5089 - loglik: -1.9425e+02 - logprior: -4.2585e+00
Epoch 4/10
10/10 - 1s - loss: 191.6660 - loglik: -1.9392e+02 - logprior: 2.2560
Epoch 5/10
10/10 - 1s - loss: 188.4358 - loglik: -1.9387e+02 - logprior: 5.4346
Epoch 6/10
10/10 - 1s - loss: 186.6245 - loglik: -1.9391e+02 - logprior: 7.2806
Epoch 7/10
10/10 - 1s - loss: 185.4809 - loglik: -1.9395e+02 - logprior: 8.4731
Epoch 8/10
10/10 - 1s - loss: 184.6744 - loglik: -1.9400e+02 - logprior: 9.3285
Epoch 9/10
10/10 - 1s - loss: 184.0325 - loglik: -1.9408e+02 - logprior: 10.0474
Epoch 10/10
10/10 - 1s - loss: 183.4712 - loglik: -1.9417e+02 - logprior: 10.7024
Fitted a model with MAP estimate = -183.1833
Time for alignment: 43.3055
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.4084 - loglik: -3.4684e+02 - logprior: -8.5566e+01
Epoch 2/10
10/10 - 1s - loss: 327.9050 - loglik: -3.0808e+02 - logprior: -1.9823e+01
Epoch 3/10
10/10 - 1s - loss: 276.7765 - loglik: -2.6879e+02 - logprior: -7.9904e+00
Epoch 4/10
10/10 - 1s - loss: 245.4774 - loglik: -2.4096e+02 - logprior: -4.5149e+00
Epoch 5/10
10/10 - 1s - loss: 231.7047 - loglik: -2.2928e+02 - logprior: -2.4224e+00
Epoch 6/10
10/10 - 1s - loss: 226.1646 - loglik: -2.2502e+02 - logprior: -1.1488e+00
Epoch 7/10
10/10 - 1s - loss: 223.2769 - loglik: -2.2305e+02 - logprior: -2.2660e-01
Epoch 8/10
10/10 - 1s - loss: 221.5715 - loglik: -2.2196e+02 - logprior: 0.3885
Epoch 9/10
10/10 - 1s - loss: 220.6325 - loglik: -2.2147e+02 - logprior: 0.8343
Epoch 10/10
10/10 - 1s - loss: 220.0365 - loglik: -2.2128e+02 - logprior: 1.2480
Fitted a model with MAP estimate = -219.7990
expansions: [(13, 3), (14, 2), (24, 1), (29, 2), (30, 1), (40, 1), (41, 1), (49, 4), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 312.3551 - loglik: -2.1681e+02 - logprior: -9.5544e+01
Epoch 2/2
10/10 - 1s - loss: 236.9229 - loglik: -2.0079e+02 - logprior: -3.6131e+01
Fitted a model with MAP estimate = -223.9264
expansions: [(0, 3)]
discards: [  0  35 112 113]
Fitting a model of length 124 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 272.7753 - loglik: -1.9772e+02 - logprior: -7.5055e+01
Epoch 2/2
10/10 - 1s - loss: 209.7383 - loglik: -1.9372e+02 - logprior: -1.6022e+01
Fitted a model with MAP estimate = -200.0821
expansions: []
discards: [0 1]
Fitting a model of length 122 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 286.1806 - loglik: -1.9638e+02 - logprior: -8.9797e+01
Epoch 2/10
10/10 - 1s - loss: 217.5431 - loglik: -1.9499e+02 - logprior: -2.2555e+01
Epoch 3/10
10/10 - 1s - loss: 198.5089 - loglik: -1.9425e+02 - logprior: -4.2585e+00
Epoch 4/10
10/10 - 1s - loss: 191.6660 - loglik: -1.9392e+02 - logprior: 2.2560
Epoch 5/10
10/10 - 1s - loss: 188.4358 - loglik: -1.9387e+02 - logprior: 5.4346
Epoch 6/10
10/10 - 1s - loss: 186.6245 - loglik: -1.9391e+02 - logprior: 7.2806
Epoch 7/10
10/10 - 1s - loss: 185.4809 - loglik: -1.9395e+02 - logprior: 8.4731
Epoch 8/10
10/10 - 1s - loss: 184.6744 - loglik: -1.9400e+02 - logprior: 9.3285
Epoch 9/10
10/10 - 1s - loss: 184.0325 - loglik: -1.9408e+02 - logprior: 10.0474
Epoch 10/10
10/10 - 1s - loss: 183.4712 - loglik: -1.9417e+02 - logprior: 10.7024
Fitted a model with MAP estimate = -183.1833
Time for alignment: 44.1280
Computed alignments with likelihoods: ['-183.1833', '-183.1833', '-183.1833']
Best model has likelihood: -183.1833
SP score = 0.9116
Training of 3 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe26eaf75b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe277127550>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 167.2296 - loglik: -1.3556e+02 - logprior: -3.1673e+01
Epoch 2/10
10/10 - 0s - loss: 124.1306 - loglik: -1.1518e+02 - logprior: -8.9518e+00
Epoch 3/10
10/10 - 0s - loss: 99.1191 - loglik: -9.4255e+01 - logprior: -4.8645e+00
Epoch 4/10
10/10 - 0s - loss: 84.1436 - loglik: -8.0374e+01 - logprior: -3.7694e+00
Epoch 5/10
10/10 - 0s - loss: 78.9106 - loglik: -7.5465e+01 - logprior: -3.4458e+00
Epoch 6/10
10/10 - 0s - loss: 77.2441 - loglik: -7.4064e+01 - logprior: -3.1802e+00
Epoch 7/10
10/10 - 0s - loss: 76.3950 - loglik: -7.3637e+01 - logprior: -2.7583e+00
Epoch 8/10
10/10 - 0s - loss: 75.8133 - loglik: -7.3354e+01 - logprior: -2.4594e+00
Epoch 9/10
10/10 - 0s - loss: 75.5993 - loglik: -7.3256e+01 - logprior: -2.3431e+00
Epoch 10/10
10/10 - 0s - loss: 75.3058 - loglik: -7.3002e+01 - logprior: -2.3034e+00
Fitted a model with MAP estimate = -75.3074
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 114.3822 - loglik: -7.2112e+01 - logprior: -4.2270e+01
Epoch 2/2
10/10 - 0s - loss: 78.1894 - loglik: -6.4422e+01 - logprior: -1.3767e+01
Fitted a model with MAP estimate = -71.0157
expansions: []
discards: [30 33]
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 90.9962 - loglik: -6.1164e+01 - logprior: -2.9832e+01
Epoch 2/2
10/10 - 0s - loss: 69.2457 - loglik: -6.0820e+01 - logprior: -8.4258e+00
Fitted a model with MAP estimate = -66.2734
expansions: []
discards: []
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.6055 - loglik: -6.0398e+01 - logprior: -2.8208e+01
Epoch 2/10
10/10 - 0s - loss: 68.6606 - loglik: -6.0677e+01 - logprior: -7.9839e+00
Epoch 3/10
10/10 - 0s - loss: 65.0232 - loglik: -6.1049e+01 - logprior: -3.9739e+00
Epoch 4/10
10/10 - 0s - loss: 63.7777 - loglik: -6.1316e+01 - logprior: -2.4614e+00
Epoch 5/10
10/10 - 0s - loss: 63.0822 - loglik: -6.1351e+01 - logprior: -1.7312e+00
Epoch 6/10
10/10 - 0s - loss: 62.5001 - loglik: -6.1126e+01 - logprior: -1.3741e+00
Epoch 7/10
10/10 - 0s - loss: 62.6477 - loglik: -6.1514e+01 - logprior: -1.1333e+00
Fitted a model with MAP estimate = -62.4341
Time for alignment: 25.3211
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.2155 - loglik: -1.3554e+02 - logprior: -3.1673e+01
Epoch 2/10
10/10 - 0s - loss: 124.1998 - loglik: -1.1524e+02 - logprior: -8.9567e+00
Epoch 3/10
10/10 - 0s - loss: 99.1935 - loglik: -9.4328e+01 - logprior: -4.8658e+00
Epoch 4/10
10/10 - 0s - loss: 84.2091 - loglik: -8.0454e+01 - logprior: -3.7546e+00
Epoch 5/10
10/10 - 0s - loss: 79.0119 - loglik: -7.5577e+01 - logprior: -3.4350e+00
Epoch 6/10
10/10 - 0s - loss: 77.2522 - loglik: -7.4075e+01 - logprior: -3.1773e+00
Epoch 7/10
10/10 - 0s - loss: 76.3452 - loglik: -7.3585e+01 - logprior: -2.7600e+00
Epoch 8/10
10/10 - 0s - loss: 75.8792 - loglik: -7.3416e+01 - logprior: -2.4632e+00
Epoch 9/10
10/10 - 0s - loss: 75.5143 - loglik: -7.3169e+01 - logprior: -2.3453e+00
Epoch 10/10
10/10 - 0s - loss: 75.3818 - loglik: -7.3078e+01 - logprior: -2.3036e+00
Fitted a model with MAP estimate = -75.3105
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 114.3982 - loglik: -7.2131e+01 - logprior: -4.2267e+01
Epoch 2/2
10/10 - 0s - loss: 78.1051 - loglik: -6.4340e+01 - logprior: -1.3765e+01
Fitted a model with MAP estimate = -71.0102
expansions: []
discards: [30 33]
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 90.9654 - loglik: -6.1131e+01 - logprior: -2.9834e+01
Epoch 2/2
10/10 - 0s - loss: 69.3507 - loglik: -6.0922e+01 - logprior: -8.4282e+00
Fitted a model with MAP estimate = -66.2726
expansions: []
discards: []
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.6611 - loglik: -6.0453e+01 - logprior: -2.8209e+01
Epoch 2/10
10/10 - 0s - loss: 68.6651 - loglik: -6.0688e+01 - logprior: -7.9769e+00
Epoch 3/10
10/10 - 0s - loss: 64.9802 - loglik: -6.1003e+01 - logprior: -3.9770e+00
Epoch 4/10
10/10 - 0s - loss: 63.7306 - loglik: -6.1274e+01 - logprior: -2.4569e+00
Epoch 5/10
10/10 - 0s - loss: 63.0326 - loglik: -6.1298e+01 - logprior: -1.7349e+00
Epoch 6/10
10/10 - 0s - loss: 62.7802 - loglik: -6.1410e+01 - logprior: -1.3703e+00
Epoch 7/10
10/10 - 0s - loss: 62.4342 - loglik: -6.1299e+01 - logprior: -1.1348e+00
Epoch 8/10
10/10 - 0s - loss: 62.3485 - loglik: -6.1420e+01 - logprior: -9.2876e-01
Epoch 9/10
10/10 - 0s - loss: 62.3115 - loglik: -6.1540e+01 - logprior: -7.7110e-01
Epoch 10/10
10/10 - 0s - loss: 62.2568 - loglik: -6.1583e+01 - logprior: -6.7414e-01
Fitted a model with MAP estimate = -62.1813
Time for alignment: 26.1911
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 167.2400 - loglik: -1.3557e+02 - logprior: -3.1673e+01
Epoch 2/10
10/10 - 0s - loss: 124.0354 - loglik: -1.1508e+02 - logprior: -8.9515e+00
Epoch 3/10
10/10 - 0s - loss: 98.7528 - loglik: -9.3881e+01 - logprior: -4.8714e+00
Epoch 4/10
10/10 - 0s - loss: 83.5700 - loglik: -7.9779e+01 - logprior: -3.7907e+00
Epoch 5/10
10/10 - 0s - loss: 78.7087 - loglik: -7.5259e+01 - logprior: -3.4494e+00
Epoch 6/10
10/10 - 0s - loss: 77.0217 - loglik: -7.3850e+01 - logprior: -3.1715e+00
Epoch 7/10
10/10 - 0s - loss: 76.2712 - loglik: -7.3525e+01 - logprior: -2.7462e+00
Epoch 8/10
10/10 - 0s - loss: 75.7901 - loglik: -7.3337e+01 - logprior: -2.4528e+00
Epoch 9/10
10/10 - 0s - loss: 75.5263 - loglik: -7.3180e+01 - logprior: -2.3466e+00
Epoch 10/10
10/10 - 0s - loss: 75.3324 - loglik: -7.3033e+01 - logprior: -2.2991e+00
Fitted a model with MAP estimate = -75.2936
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 1), (24, 2), (26, 1)]
discards: []
Fitting a model of length 48 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 113.5577 - loglik: -7.1274e+01 - logprior: -4.2284e+01
Epoch 2/2
10/10 - 0s - loss: 77.6741 - loglik: -6.4048e+01 - logprior: -1.3626e+01
Fitted a model with MAP estimate = -70.7316
expansions: []
discards: [32]
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 90.9351 - loglik: -6.1121e+01 - logprior: -2.9815e+01
Epoch 2/2
10/10 - 0s - loss: 69.1762 - loglik: -6.0751e+01 - logprior: -8.4247e+00
Fitted a model with MAP estimate = -66.2537
expansions: []
discards: []
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 88.6641 - loglik: -6.0458e+01 - logprior: -2.8206e+01
Epoch 2/10
10/10 - 0s - loss: 68.6732 - loglik: -6.0688e+01 - logprior: -7.9850e+00
Epoch 3/10
10/10 - 0s - loss: 65.0596 - loglik: -6.1089e+01 - logprior: -3.9707e+00
Epoch 4/10
10/10 - 0s - loss: 63.6486 - loglik: -6.1187e+01 - logprior: -2.4614e+00
Epoch 5/10
10/10 - 0s - loss: 63.0327 - loglik: -6.1305e+01 - logprior: -1.7281e+00
Epoch 6/10
10/10 - 0s - loss: 62.6346 - loglik: -6.1259e+01 - logprior: -1.3755e+00
Epoch 7/10
10/10 - 0s - loss: 62.5155 - loglik: -6.1386e+01 - logprior: -1.1299e+00
Epoch 8/10
10/10 - 0s - loss: 62.3561 - loglik: -6.1429e+01 - logprior: -9.2749e-01
Epoch 9/10
10/10 - 0s - loss: 62.3698 - loglik: -6.1604e+01 - logprior: -7.6621e-01
Fitted a model with MAP estimate = -62.2529
Time for alignment: 25.7493
Computed alignments with likelihoods: ['-62.4341', '-62.1813', '-62.2529']
Best model has likelihood: -62.1813
SP score = 0.9918
Training of 3 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f9cb9550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2775d0f70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 445.5614 - loglik: -4.4023e+02 - logprior: -5.3287e+00
Epoch 2/10
15/15 - 3s - loss: 364.8340 - loglik: -3.6355e+02 - logprior: -1.2881e+00
Epoch 3/10
15/15 - 3s - loss: 317.8851 - loglik: -3.1635e+02 - logprior: -1.5399e+00
Epoch 4/10
15/15 - 3s - loss: 306.4973 - loglik: -3.0483e+02 - logprior: -1.6705e+00
Epoch 5/10
15/15 - 3s - loss: 301.8083 - loglik: -3.0027e+02 - logprior: -1.5398e+00
Epoch 6/10
15/15 - 3s - loss: 300.3713 - loglik: -2.9882e+02 - logprior: -1.5497e+00
Epoch 7/10
15/15 - 3s - loss: 298.0489 - loglik: -2.9651e+02 - logprior: -1.5410e+00
Epoch 8/10
15/15 - 3s - loss: 299.1674 - loglik: -2.9765e+02 - logprior: -1.5156e+00
Fitted a model with MAP estimate = -298.4095
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (26, 1), (49, 2), (60, 1), (62, 1), (66, 2), (67, 1), (69, 1), (91, 1), (92, 2), (105, 2), (112, 1), (114, 1), (115, 1), (116, 4), (119, 2)]
discards: [0]
Fitting a model of length 157 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 303.0483 - loglik: -2.9658e+02 - logprior: -6.4680e+00
Epoch 2/2
15/15 - 4s - loss: 287.6635 - loglik: -2.8452e+02 - logprior: -3.1468e+00
Fitted a model with MAP estimate = -284.4738
expansions: [(0, 2)]
discards: [  0   7  57 108 124]
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 287.3573 - loglik: -2.8246e+02 - logprior: -4.9009e+00
Epoch 2/2
15/15 - 4s - loss: 282.3233 - loglik: -2.8095e+02 - logprior: -1.3706e+00
Fitted a model with MAP estimate = -281.1847
expansions: []
discards: [0]
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 290.2098 - loglik: -2.8392e+02 - logprior: -6.2867e+00
Epoch 2/10
15/15 - 4s - loss: 284.2651 - loglik: -2.8220e+02 - logprior: -2.0646e+00
Epoch 3/10
15/15 - 4s - loss: 281.5213 - loglik: -2.8057e+02 - logprior: -9.4679e-01
Epoch 4/10
15/15 - 4s - loss: 280.2495 - loglik: -2.7945e+02 - logprior: -8.0229e-01
Epoch 5/10
15/15 - 4s - loss: 279.9852 - loglik: -2.7925e+02 - logprior: -7.3865e-01
Epoch 6/10
15/15 - 4s - loss: 278.8065 - loglik: -2.7808e+02 - logprior: -7.2481e-01
Epoch 7/10
15/15 - 4s - loss: 279.4025 - loglik: -2.7873e+02 - logprior: -6.6814e-01
Fitted a model with MAP estimate = -278.5986
Time for alignment: 110.1261
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 445.0604 - loglik: -4.3974e+02 - logprior: -5.3217e+00
Epoch 2/10
15/15 - 3s - loss: 365.3040 - loglik: -3.6403e+02 - logprior: -1.2786e+00
Epoch 3/10
15/15 - 3s - loss: 317.7760 - loglik: -3.1621e+02 - logprior: -1.5649e+00
Epoch 4/10
15/15 - 3s - loss: 306.9715 - loglik: -3.0526e+02 - logprior: -1.7123e+00
Epoch 5/10
15/15 - 3s - loss: 301.9377 - loglik: -3.0036e+02 - logprior: -1.5796e+00
Epoch 6/10
15/15 - 3s - loss: 301.0042 - loglik: -2.9941e+02 - logprior: -1.5936e+00
Epoch 7/10
15/15 - 3s - loss: 298.9733 - loglik: -2.9741e+02 - logprior: -1.5654e+00
Epoch 8/10
15/15 - 3s - loss: 299.6958 - loglik: -2.9816e+02 - logprior: -1.5326e+00
Fitted a model with MAP estimate = -299.0672
expansions: [(8, 2), (9, 1), (10, 1), (16, 1), (24, 2), (25, 1), (26, 1), (60, 1), (66, 3), (69, 1), (91, 1), (92, 2), (109, 1), (112, 1), (114, 2), (115, 1), (116, 4), (119, 2)]
discards: [0]
Fitting a model of length 155 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 302.9101 - loglik: -2.9640e+02 - logprior: -6.5128e+00
Epoch 2/2
15/15 - 4s - loss: 289.1596 - loglik: -2.8596e+02 - logprior: -3.2006e+00
Fitted a model with MAP estimate = -286.3116
expansions: [(0, 2)]
discards: [  0   7 106 133 138]
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 288.7969 - loglik: -2.8384e+02 - logprior: -4.9616e+00
Epoch 2/2
15/15 - 4s - loss: 282.7316 - loglik: -2.8132e+02 - logprior: -1.4155e+00
Fitted a model with MAP estimate = -280.9984
expansions: [(75, 1)]
discards: [ 0 65 66 81]
Fitting a model of length 149 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 293.3912 - loglik: -2.8707e+02 - logprior: -6.3231e+00
Epoch 2/10
15/15 - 4s - loss: 287.5109 - loglik: -2.8538e+02 - logprior: -2.1312e+00
Epoch 3/10
15/15 - 4s - loss: 284.0206 - loglik: -2.8302e+02 - logprior: -9.9803e-01
Epoch 4/10
15/15 - 4s - loss: 283.7057 - loglik: -2.8288e+02 - logprior: -8.2200e-01
Epoch 5/10
15/15 - 4s - loss: 282.6185 - loglik: -2.8185e+02 - logprior: -7.6844e-01
Epoch 6/10
15/15 - 4s - loss: 282.2406 - loglik: -2.8151e+02 - logprior: -7.3447e-01
Epoch 7/10
15/15 - 4s - loss: 281.4923 - loglik: -2.8079e+02 - logprior: -7.0233e-01
Epoch 8/10
15/15 - 4s - loss: 281.5292 - loglik: -2.8088e+02 - logprior: -6.4508e-01
Fitted a model with MAP estimate = -281.1959
Time for alignment: 111.9348
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 445.1669 - loglik: -4.3984e+02 - logprior: -5.3245e+00
Epoch 2/10
15/15 - 3s - loss: 365.7766 - loglik: -3.6450e+02 - logprior: -1.2799e+00
Epoch 3/10
15/15 - 3s - loss: 321.6223 - loglik: -3.2011e+02 - logprior: -1.5163e+00
Epoch 4/10
15/15 - 3s - loss: 305.8376 - loglik: -3.0412e+02 - logprior: -1.7141e+00
Epoch 5/10
15/15 - 3s - loss: 302.4677 - loglik: -3.0090e+02 - logprior: -1.5640e+00
Epoch 6/10
15/15 - 3s - loss: 299.6041 - loglik: -2.9805e+02 - logprior: -1.5531e+00
Epoch 7/10
15/15 - 3s - loss: 299.6177 - loglik: -2.9806e+02 - logprior: -1.5585e+00
Fitted a model with MAP estimate = -298.2582
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (26, 1), (49, 2), (60, 1), (65, 1), (66, 2), (67, 1), (69, 1), (91, 1), (92, 1), (108, 1), (112, 1), (114, 2), (115, 1), (116, 4), (119, 2)]
discards: [0]
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 302.2300 - loglik: -2.9572e+02 - logprior: -6.5133e+00
Epoch 2/2
15/15 - 4s - loss: 286.5473 - loglik: -2.8340e+02 - logprior: -3.1511e+00
Fitted a model with MAP estimate = -284.2848
expansions: [(0, 2)]
discards: [  0   7  57  77 134]
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 288.3629 - loglik: -2.8343e+02 - logprior: -4.9307e+00
Epoch 2/2
15/15 - 4s - loss: 281.6981 - loglik: -2.8031e+02 - logprior: -1.3909e+00
Fitted a model with MAP estimate = -281.1583
expansions: [(75, 1)]
discards: [0]
Fitting a model of length 153 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 289.8878 - loglik: -2.8357e+02 - logprior: -6.3146e+00
Epoch 2/10
15/15 - 4s - loss: 284.7325 - loglik: -2.8260e+02 - logprior: -2.1310e+00
Epoch 3/10
15/15 - 4s - loss: 279.7303 - loglik: -2.7873e+02 - logprior: -9.9896e-01
Epoch 4/10
15/15 - 4s - loss: 280.6278 - loglik: -2.7981e+02 - logprior: -8.1633e-01
Fitted a model with MAP estimate = -279.5806
Time for alignment: 92.6871
Computed alignments with likelihoods: ['-278.5986', '-280.9984', '-279.5806']
Best model has likelihood: -278.5986
SP score = 0.8942
Training of 3 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fac48f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25d75eb80>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 344.7514 - loglik: -3.4063e+02 - logprior: -4.1262e+00
Epoch 2/10
17/17 - 3s - loss: 256.8002 - loglik: -2.5526e+02 - logprior: -1.5386e+00
Epoch 3/10
17/17 - 4s - loss: 213.4986 - loglik: -2.1168e+02 - logprior: -1.8180e+00
Epoch 4/10
17/17 - 4s - loss: 205.1704 - loglik: -2.0338e+02 - logprior: -1.7940e+00
Epoch 5/10
17/17 - 3s - loss: 202.6611 - loglik: -2.0086e+02 - logprior: -1.8029e+00
Epoch 6/10
17/17 - 4s - loss: 201.5582 - loglik: -1.9977e+02 - logprior: -1.7869e+00
Epoch 7/10
17/17 - 4s - loss: 201.8240 - loglik: -2.0005e+02 - logprior: -1.7752e+00
Fitted a model with MAP estimate = -201.2042
expansions: [(0, 2), (15, 1), (16, 1), (17, 3), (18, 1), (26, 1), (27, 1), (28, 1), (40, 1), (41, 1), (44, 1), (48, 1), (49, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (78, 1), (82, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Fitting a model of length 150 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 196.0791 - loglik: -1.9136e+02 - logprior: -4.7179e+00
Epoch 2/2
17/17 - 5s - loss: 180.2937 - loglik: -1.7892e+02 - logprior: -1.3727e+00
Fitted a model with MAP estimate = -178.2888
expansions: []
discards: [ 22 139]
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 182.5190 - loglik: -1.7853e+02 - logprior: -3.9932e+00
Epoch 2/2
17/17 - 4s - loss: 178.3039 - loglik: -1.7692e+02 - logprior: -1.3887e+00
Fitted a model with MAP estimate = -177.4202
expansions: []
discards: []
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 181.4242 - loglik: -1.7748e+02 - logprior: -3.9462e+00
Epoch 2/10
17/17 - 5s - loss: 178.5263 - loglik: -1.7721e+02 - logprior: -1.3198e+00
Epoch 3/10
17/17 - 5s - loss: 176.3304 - loglik: -1.7522e+02 - logprior: -1.1081e+00
Epoch 4/10
17/17 - 5s - loss: 176.4469 - loglik: -1.7540e+02 - logprior: -1.0441e+00
Fitted a model with MAP estimate = -176.2497
Time for alignment: 99.6967
Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 345.1042 - loglik: -3.4097e+02 - logprior: -4.1293e+00
Epoch 2/10
17/17 - 4s - loss: 256.4423 - loglik: -2.5491e+02 - logprior: -1.5337e+00
Epoch 3/10
17/17 - 4s - loss: 215.0853 - loglik: -2.1329e+02 - logprior: -1.7946e+00
Epoch 4/10
17/17 - 4s - loss: 206.3335 - loglik: -2.0454e+02 - logprior: -1.7907e+00
Epoch 5/10
17/17 - 4s - loss: 201.5002 - loglik: -1.9970e+02 - logprior: -1.7992e+00
Epoch 6/10
17/17 - 4s - loss: 201.0914 - loglik: -1.9931e+02 - logprior: -1.7778e+00
Epoch 7/10
17/17 - 4s - loss: 200.8220 - loglik: -1.9908e+02 - logprior: -1.7412e+00
Epoch 8/10
17/17 - 4s - loss: 200.2341 - loglik: -1.9843e+02 - logprior: -1.8075e+00
Epoch 9/10
17/17 - 4s - loss: 199.8238 - loglik: -1.9799e+02 - logprior: -1.8388e+00
Epoch 10/10
17/17 - 3s - loss: 200.2704 - loglik: -1.9843e+02 - logprior: -1.8369e+00
Fitted a model with MAP estimate = -199.8886
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (42, 1), (44, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Fitting a model of length 151 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 195.3029 - loglik: -1.9054e+02 - logprior: -4.7625e+00
Epoch 2/2
17/17 - 5s - loss: 179.1339 - loglik: -1.7779e+02 - logprior: -1.3437e+00
Fitted a model with MAP estimate = -177.3672
expansions: []
discards: [ 34 140]
Fitting a model of length 149 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 182.3577 - loglik: -1.7840e+02 - logprior: -3.9575e+00
Epoch 2/2
17/17 - 5s - loss: 177.2732 - loglik: -1.7589e+02 - logprior: -1.3819e+00
Fitted a model with MAP estimate = -176.9966
expansions: []
discards: []
Fitting a model of length 149 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 181.4799 - loglik: -1.7755e+02 - logprior: -3.9302e+00
Epoch 2/10
17/17 - 5s - loss: 177.4747 - loglik: -1.7616e+02 - logprior: -1.3113e+00
Epoch 3/10
17/17 - 4s - loss: 176.7755 - loglik: -1.7568e+02 - logprior: -1.0980e+00
Epoch 4/10
17/17 - 5s - loss: 176.7409 - loglik: -1.7571e+02 - logprior: -1.0308e+00
Epoch 5/10
17/17 - 4s - loss: 175.2633 - loglik: -1.7428e+02 - logprior: -9.7944e-01
Epoch 6/10
17/17 - 5s - loss: 175.6704 - loglik: -1.7473e+02 - logprior: -9.3616e-01
Fitted a model with MAP estimate = -175.4369
Time for alignment: 120.1435
Fitting a model of length 119 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 344.7010 - loglik: -3.4057e+02 - logprior: -4.1296e+00
Epoch 2/10
17/17 - 4s - loss: 255.1633 - loglik: -2.5363e+02 - logprior: -1.5312e+00
Epoch 3/10
17/17 - 4s - loss: 214.6015 - loglik: -2.1283e+02 - logprior: -1.7760e+00
Epoch 4/10
17/17 - 4s - loss: 204.6764 - loglik: -2.0292e+02 - logprior: -1.7608e+00
Epoch 5/10
17/17 - 3s - loss: 201.6153 - loglik: -1.9988e+02 - logprior: -1.7311e+00
Epoch 6/10
17/17 - 4s - loss: 200.5076 - loglik: -1.9876e+02 - logprior: -1.7463e+00
Epoch 7/10
17/17 - 4s - loss: 200.6254 - loglik: -1.9891e+02 - logprior: -1.7121e+00
Fitted a model with MAP estimate = -200.1409
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (42, 1), (44, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Fitting a model of length 150 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 195.3527 - loglik: -1.9066e+02 - logprior: -4.6923e+00
Epoch 2/2
17/17 - 5s - loss: 179.2690 - loglik: -1.7792e+02 - logprior: -1.3457e+00
Fitted a model with MAP estimate = -177.5847
expansions: []
discards: [ 34 139]
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 182.0941 - loglik: -1.7813e+02 - logprior: -3.9683e+00
Epoch 2/2
17/17 - 5s - loss: 178.5272 - loglik: -1.7715e+02 - logprior: -1.3776e+00
Fitted a model with MAP estimate = -177.3616
expansions: []
discards: []
Fitting a model of length 148 on 7691 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 181.5524 - loglik: -1.7761e+02 - logprior: -3.9437e+00
Epoch 2/10
17/17 - 5s - loss: 178.4274 - loglik: -1.7711e+02 - logprior: -1.3199e+00
Epoch 3/10
17/17 - 5s - loss: 177.0702 - loglik: -1.7596e+02 - logprior: -1.1117e+00
Epoch 4/10
17/17 - 4s - loss: 177.0295 - loglik: -1.7599e+02 - logprior: -1.0401e+00
Epoch 5/10
17/17 - 5s - loss: 175.6324 - loglik: -1.7465e+02 - logprior: -9.8666e-01
Epoch 6/10
17/17 - 5s - loss: 175.4854 - loglik: -1.7454e+02 - logprior: -9.4418e-01
Epoch 7/10
17/17 - 5s - loss: 176.2054 - loglik: -1.7530e+02 - logprior: -9.0264e-01
Fitted a model with MAP estimate = -175.6732
Time for alignment: 113.7643
Computed alignments with likelihoods: ['-176.2497', '-175.4369', '-175.6732']
Best model has likelihood: -175.4369
SP score = 0.6275
Training of 3 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c03f0d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2d02100>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 419.5074 - loglik: -3.9880e+02 - logprior: -2.0704e+01
Epoch 2/10
10/10 - 2s - loss: 364.1573 - loglik: -3.5942e+02 - logprior: -4.7388e+00
Epoch 3/10
10/10 - 2s - loss: 318.8514 - loglik: -3.1634e+02 - logprior: -2.5124e+00
Epoch 4/10
10/10 - 2s - loss: 293.4718 - loglik: -2.9146e+02 - logprior: -2.0164e+00
Epoch 5/10
10/10 - 2s - loss: 282.0246 - loglik: -2.8018e+02 - logprior: -1.8412e+00
Epoch 6/10
10/10 - 2s - loss: 276.0068 - loglik: -2.7428e+02 - logprior: -1.7292e+00
Epoch 7/10
10/10 - 2s - loss: 274.4306 - loglik: -2.7279e+02 - logprior: -1.6438e+00
Epoch 8/10
10/10 - 2s - loss: 274.2876 - loglik: -2.7272e+02 - logprior: -1.5653e+00
Epoch 9/10
10/10 - 2s - loss: 273.4837 - loglik: -2.7202e+02 - logprior: -1.4659e+00
Epoch 10/10
10/10 - 2s - loss: 273.4273 - loglik: -2.7201e+02 - logprior: -1.4173e+00
Fitted a model with MAP estimate = -273.1462
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 3), (24, 1), (29, 2), (53, 1), (55, 2), (62, 1), (79, 2), (80, 2), (81, 2), (84, 2), (86, 2), (98, 1), (101, 2), (102, 1), (103, 1), (104, 1)]
discards: []
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 289.8951 - loglik: -2.7104e+02 - logprior: -1.8852e+01
Epoch 2/2
10/10 - 2s - loss: 263.2251 - loglik: -2.5861e+02 - logprior: -4.6158e+00
Fitted a model with MAP estimate = -259.3287
expansions: []
discards: [  0  38  67 105 109]
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 283.3046 - loglik: -2.5992e+02 - logprior: -2.3385e+01
Epoch 2/2
10/10 - 2s - loss: 266.8022 - loglik: -2.5762e+02 - logprior: -9.1869e+00
Fitted a model with MAP estimate = -263.7594
expansions: [(0, 5)]
discards: [0]
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 276.3970 - loglik: -2.5793e+02 - logprior: -1.8469e+01
Epoch 2/10
10/10 - 2s - loss: 259.0663 - loglik: -2.5499e+02 - logprior: -4.0775e+00
Epoch 3/10
10/10 - 2s - loss: 256.3989 - loglik: -2.5518e+02 - logprior: -1.2233e+00
Epoch 4/10
10/10 - 2s - loss: 253.8015 - loglik: -2.5362e+02 - logprior: -1.7783e-01
Epoch 5/10
10/10 - 2s - loss: 253.4088 - loglik: -2.5369e+02 - logprior: 0.2862
Epoch 6/10
10/10 - 2s - loss: 252.0310 - loglik: -2.5255e+02 - logprior: 0.5192
Epoch 7/10
10/10 - 2s - loss: 251.5152 - loglik: -2.5219e+02 - logprior: 0.6789
Epoch 8/10
10/10 - 2s - loss: 252.7233 - loglik: -2.5357e+02 - logprior: 0.8477
Fitted a model with MAP estimate = -251.4966
Time for alignment: 61.3004
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 419.6864 - loglik: -3.9898e+02 - logprior: -2.0702e+01
Epoch 2/10
10/10 - 2s - loss: 363.7131 - loglik: -3.5896e+02 - logprior: -4.7490e+00
Epoch 3/10
10/10 - 2s - loss: 319.1435 - loglik: -3.1656e+02 - logprior: -2.5797e+00
Epoch 4/10
10/10 - 2s - loss: 291.4354 - loglik: -2.8925e+02 - logprior: -2.1895e+00
Epoch 5/10
10/10 - 2s - loss: 279.7155 - loglik: -2.7751e+02 - logprior: -2.2045e+00
Epoch 6/10
10/10 - 2s - loss: 275.3259 - loglik: -2.7315e+02 - logprior: -2.1794e+00
Epoch 7/10
10/10 - 2s - loss: 272.6075 - loglik: -2.7049e+02 - logprior: -2.1153e+00
Epoch 8/10
10/10 - 2s - loss: 272.7939 - loglik: -2.7073e+02 - logprior: -2.0638e+00
Fitted a model with MAP estimate = -271.9371
expansions: [(8, 1), (9, 1), (10, 1), (13, 1), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 1), (62, 1), (79, 3), (80, 2), (82, 1), (83, 1), (84, 1), (86, 1), (98, 1), (101, 2), (102, 1), (103, 1), (104, 1)]
discards: []
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 288.3052 - loglik: -2.6950e+02 - logprior: -1.8804e+01
Epoch 2/2
10/10 - 2s - loss: 263.1306 - loglik: -2.5866e+02 - logprior: -4.4733e+00
Fitted a model with MAP estimate = -259.5860
expansions: []
discards: [ 0 95]
Fitting a model of length 141 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 283.4211 - loglik: -2.6007e+02 - logprior: -2.3355e+01
Epoch 2/2
10/10 - 2s - loss: 266.6887 - loglik: -2.5758e+02 - logprior: -9.1104e+00
Fitted a model with MAP estimate = -264.1072
expansions: [(0, 5)]
discards: [0]
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 276.4527 - loglik: -2.5803e+02 - logprior: -1.8421e+01
Epoch 2/10
10/10 - 2s - loss: 259.9520 - loglik: -2.5590e+02 - logprior: -4.0471e+00
Epoch 3/10
10/10 - 2s - loss: 255.8574 - loglik: -2.5465e+02 - logprior: -1.2071e+00
Epoch 4/10
10/10 - 2s - loss: 254.9233 - loglik: -2.5474e+02 - logprior: -1.8225e-01
Epoch 5/10
10/10 - 2s - loss: 253.3877 - loglik: -2.5368e+02 - logprior: 0.2883
Epoch 6/10
10/10 - 2s - loss: 252.4633 - loglik: -2.5297e+02 - logprior: 0.5090
Epoch 7/10
10/10 - 2s - loss: 252.5136 - loglik: -2.5318e+02 - logprior: 0.6698
Fitted a model with MAP estimate = -251.9952
Time for alignment: 54.4056
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 419.7532 - loglik: -3.9905e+02 - logprior: -2.0707e+01
Epoch 2/10
10/10 - 2s - loss: 363.9100 - loglik: -3.5917e+02 - logprior: -4.7386e+00
Epoch 3/10
10/10 - 2s - loss: 321.6424 - loglik: -3.1912e+02 - logprior: -2.5190e+00
Epoch 4/10
10/10 - 2s - loss: 295.8436 - loglik: -2.9384e+02 - logprior: -2.0062e+00
Epoch 5/10
10/10 - 2s - loss: 285.9378 - loglik: -2.8411e+02 - logprior: -1.8241e+00
Epoch 6/10
10/10 - 2s - loss: 280.0273 - loglik: -2.7837e+02 - logprior: -1.6529e+00
Epoch 7/10
10/10 - 2s - loss: 279.0942 - loglik: -2.7753e+02 - logprior: -1.5617e+00
Epoch 8/10
10/10 - 2s - loss: 276.9748 - loglik: -2.7549e+02 - logprior: -1.4876e+00
Epoch 9/10
10/10 - 2s - loss: 277.0607 - loglik: -2.7565e+02 - logprior: -1.4102e+00
Fitted a model with MAP estimate = -276.3418
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 3), (24, 1), (37, 1), (53, 1), (55, 2), (62, 1), (79, 3), (80, 2), (81, 2), (87, 1), (97, 4), (98, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Fitting a model of length 146 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 290.9559 - loglik: -2.7211e+02 - logprior: -1.8846e+01
Epoch 2/2
10/10 - 2s - loss: 264.7810 - loglik: -2.6018e+02 - logprior: -4.6047e+00
Fitted a model with MAP estimate = -259.6510
expansions: []
discards: [  0  66  97 121 122 123 124]
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 287.0461 - loglik: -2.6369e+02 - logprior: -2.3360e+01
Epoch 2/2
10/10 - 2s - loss: 269.5658 - loglik: -2.6041e+02 - logprior: -9.1508e+00
Fitted a model with MAP estimate = -266.3832
expansions: [(0, 5), (121, 4)]
discards: [0]
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 277.7400 - loglik: -2.5925e+02 - logprior: -1.8494e+01
Epoch 2/10
10/10 - 2s - loss: 259.1523 - loglik: -2.5507e+02 - logprior: -4.0813e+00
Epoch 3/10
10/10 - 2s - loss: 254.2729 - loglik: -2.5295e+02 - logprior: -1.3229e+00
Epoch 4/10
10/10 - 2s - loss: 252.0601 - loglik: -2.5172e+02 - logprior: -3.4074e-01
Epoch 5/10
10/10 - 2s - loss: 251.0461 - loglik: -2.5115e+02 - logprior: 0.1074
Epoch 6/10
10/10 - 2s - loss: 249.7874 - loglik: -2.5014e+02 - logprior: 0.3541
Epoch 7/10
10/10 - 2s - loss: 249.1584 - loglik: -2.4967e+02 - logprior: 0.5109
Epoch 8/10
10/10 - 2s - loss: 249.9567 - loglik: -2.5064e+02 - logprior: 0.6832
Fitted a model with MAP estimate = -249.2427
Time for alignment: 59.1464
Computed alignments with likelihoods: ['-251.4966', '-251.9952', '-249.2427']
Best model has likelihood: -249.2427
SP score = 0.8758
Training of 3 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c2a8fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2661c3040>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 270.1077 - loglik: -2.3280e+02 - logprior: -3.7304e+01
Epoch 2/10
10/10 - 1s - loss: 231.4310 - loglik: -2.2189e+02 - logprior: -9.5362e+00
Epoch 3/10
10/10 - 1s - loss: 214.8812 - loglik: -2.1059e+02 - logprior: -4.2875e+00
Epoch 4/10
10/10 - 1s - loss: 203.3424 - loglik: -2.0088e+02 - logprior: -2.4638e+00
Epoch 5/10
10/10 - 1s - loss: 197.9358 - loglik: -1.9613e+02 - logprior: -1.8081e+00
Epoch 6/10
10/10 - 1s - loss: 194.8530 - loglik: -1.9317e+02 - logprior: -1.6831e+00
Epoch 7/10
10/10 - 1s - loss: 193.4016 - loglik: -1.9203e+02 - logprior: -1.3689e+00
Epoch 8/10
10/10 - 1s - loss: 192.8971 - loglik: -1.9204e+02 - logprior: -8.5224e-01
Epoch 9/10
10/10 - 1s - loss: 192.3799 - loglik: -1.9172e+02 - logprior: -6.5639e-01
Epoch 10/10
10/10 - 1s - loss: 192.0791 - loglik: -1.9149e+02 - logprior: -5.8960e-01
Fitted a model with MAP estimate = -191.9715
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (28, 1), (31, 2), (40, 2), (43, 1), (56, 1), (65, 4)]
discards: []
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 244.3069 - loglik: -1.9560e+02 - logprior: -4.8705e+01
Epoch 2/2
10/10 - 1s - loss: 202.1473 - loglik: -1.8798e+02 - logprior: -1.4169e+01
Fitted a model with MAP estimate = -194.4742
expansions: [(39, 1)]
discards: [0]
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.0166 - loglik: -1.8717e+02 - logprior: -4.1851e+01
Epoch 2/2
10/10 - 1s - loss: 201.9555 - loglik: -1.8602e+02 - logprior: -1.5934e+01
Fitted a model with MAP estimate = -197.5929
expansions: []
discards: [0 1]
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.6591 - loglik: -1.8603e+02 - logprior: -3.7625e+01
Epoch 2/10
10/10 - 1s - loss: 194.8477 - loglik: -1.8530e+02 - logprior: -9.5447e+00
Epoch 3/10
10/10 - 1s - loss: 188.2669 - loglik: -1.8508e+02 - logprior: -3.1833e+00
Epoch 4/10
10/10 - 1s - loss: 185.7843 - loglik: -1.8484e+02 - logprior: -9.4274e-01
Epoch 5/10
10/10 - 1s - loss: 184.4890 - loglik: -1.8459e+02 - logprior: 0.0996
Epoch 6/10
10/10 - 1s - loss: 183.6714 - loglik: -1.8427e+02 - logprior: 0.6025
Epoch 7/10
10/10 - 1s - loss: 183.5081 - loglik: -1.8443e+02 - logprior: 0.9185
Epoch 8/10
10/10 - 1s - loss: 183.1696 - loglik: -1.8437e+02 - logprior: 1.1985
Epoch 9/10
10/10 - 1s - loss: 182.8676 - loglik: -1.8431e+02 - logprior: 1.4390
Epoch 10/10
10/10 - 1s - loss: 183.0261 - loglik: -1.8464e+02 - logprior: 1.6176
Fitted a model with MAP estimate = -182.7427
Time for alignment: 37.9796
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 270.1124 - loglik: -2.3281e+02 - logprior: -3.7305e+01
Epoch 2/10
10/10 - 1s - loss: 231.3900 - loglik: -2.2185e+02 - logprior: -9.5369e+00
Epoch 3/10
10/10 - 1s - loss: 214.3205 - loglik: -2.1003e+02 - logprior: -4.2904e+00
Epoch 4/10
10/10 - 1s - loss: 202.8017 - loglik: -2.0039e+02 - logprior: -2.4165e+00
Epoch 5/10
10/10 - 1s - loss: 197.2470 - loglik: -1.9541e+02 - logprior: -1.8338e+00
Epoch 6/10
10/10 - 1s - loss: 194.7999 - loglik: -1.9309e+02 - logprior: -1.7112e+00
Epoch 7/10
10/10 - 1s - loss: 193.2073 - loglik: -1.9191e+02 - logprior: -1.3006e+00
Epoch 8/10
10/10 - 1s - loss: 192.4750 - loglik: -1.9165e+02 - logprior: -8.2631e-01
Epoch 9/10
10/10 - 1s - loss: 192.0007 - loglik: -1.9133e+02 - logprior: -6.6774e-01
Epoch 10/10
10/10 - 1s - loss: 191.6605 - loglik: -1.9109e+02 - logprior: -5.7223e-01
Fitted a model with MAP estimate = -191.4926
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (31, 2), (40, 2), (43, 1), (54, 1), (65, 4)]
discards: []
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 243.4363 - loglik: -1.9462e+02 - logprior: -4.8813e+01
Epoch 2/2
10/10 - 1s - loss: 202.5118 - loglik: -1.8828e+02 - logprior: -1.4237e+01
Fitted a model with MAP estimate = -194.8819
expansions: [(38, 1)]
discards: [0]
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.2752 - loglik: -1.8730e+02 - logprior: -4.1980e+01
Epoch 2/2
10/10 - 1s - loss: 202.4547 - loglik: -1.8647e+02 - logprior: -1.5987e+01
Fitted a model with MAP estimate = -197.9618
expansions: []
discards: [0 1]
Fitting a model of length 80 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.0858 - loglik: -1.8642e+02 - logprior: -3.7670e+01
Epoch 2/10
10/10 - 1s - loss: 195.2263 - loglik: -1.8564e+02 - logprior: -9.5827e+00
Epoch 3/10
10/10 - 1s - loss: 188.6546 - loglik: -1.8544e+02 - logprior: -3.2174e+00
Epoch 4/10
10/10 - 1s - loss: 186.0769 - loglik: -1.8510e+02 - logprior: -9.7939e-01
Epoch 5/10
10/10 - 1s - loss: 184.8071 - loglik: -1.8487e+02 - logprior: 0.0658
Epoch 6/10
10/10 - 1s - loss: 184.0807 - loglik: -1.8465e+02 - logprior: 0.5693
Epoch 7/10
10/10 - 1s - loss: 183.7157 - loglik: -1.8460e+02 - logprior: 0.8802
Epoch 8/10
10/10 - 1s - loss: 183.5803 - loglik: -1.8474e+02 - logprior: 1.1629
Epoch 9/10
10/10 - 1s - loss: 183.3256 - loglik: -1.8474e+02 - logprior: 1.4104
Epoch 10/10
10/10 - 1s - loss: 183.3094 - loglik: -1.8489e+02 - logprior: 1.5819
Fitted a model with MAP estimate = -183.1124
Time for alignment: 37.6972
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 270.1466 - loglik: -2.3284e+02 - logprior: -3.7305e+01
Epoch 2/10
10/10 - 1s - loss: 231.1333 - loglik: -2.2160e+02 - logprior: -9.5338e+00
Epoch 3/10
10/10 - 1s - loss: 213.8019 - loglik: -2.0951e+02 - logprior: -4.2872e+00
Epoch 4/10
10/10 - 1s - loss: 203.1271 - loglik: -2.0063e+02 - logprior: -2.4951e+00
Epoch 5/10
10/10 - 1s - loss: 198.3665 - loglik: -1.9654e+02 - logprior: -1.8308e+00
Epoch 6/10
10/10 - 1s - loss: 195.1095 - loglik: -1.9345e+02 - logprior: -1.6611e+00
Epoch 7/10
10/10 - 1s - loss: 193.3999 - loglik: -1.9201e+02 - logprior: -1.3869e+00
Epoch 8/10
10/10 - 1s - loss: 192.8276 - loglik: -1.9196e+02 - logprior: -8.7153e-01
Epoch 9/10
10/10 - 1s - loss: 192.0186 - loglik: -1.9138e+02 - logprior: -6.4070e-01
Epoch 10/10
10/10 - 1s - loss: 191.8540 - loglik: -1.9128e+02 - logprior: -5.7636e-01
Fitted a model with MAP estimate = -191.7226
expansions: [(0, 3), (6, 1), (22, 1), (25, 1), (27, 1), (31, 2), (40, 2), (43, 1), (56, 1), (65, 4)]
discards: []
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 243.5716 - loglik: -1.9476e+02 - logprior: -4.8813e+01
Epoch 2/2
10/10 - 1s - loss: 202.5089 - loglik: -1.8829e+02 - logprior: -1.4221e+01
Fitted a model with MAP estimate = -194.7885
expansions: [(38, 1)]
discards: [0]
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 229.3848 - loglik: -1.8741e+02 - logprior: -4.1976e+01
Epoch 2/2
10/10 - 1s - loss: 202.4081 - loglik: -1.8643e+02 - logprior: -1.5975e+01
Fitted a model with MAP estimate = -197.9035
expansions: []
discards: [0 1]
Fitting a model of length 80 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 224.1410 - loglik: -1.8646e+02 - logprior: -3.7677e+01
Epoch 2/10
10/10 - 1s - loss: 195.1358 - loglik: -1.8555e+02 - logprior: -9.5879e+00
Epoch 3/10
10/10 - 1s - loss: 188.6565 - loglik: -1.8542e+02 - logprior: -3.2359e+00
Epoch 4/10
10/10 - 1s - loss: 185.9854 - loglik: -1.8500e+02 - logprior: -9.9022e-01
Epoch 5/10
10/10 - 1s - loss: 184.7041 - loglik: -1.8475e+02 - logprior: 0.0443
Epoch 6/10
10/10 - 1s - loss: 184.2164 - loglik: -1.8477e+02 - logprior: 0.5535
Epoch 7/10
10/10 - 1s - loss: 183.9317 - loglik: -1.8480e+02 - logprior: 0.8689
Epoch 8/10
10/10 - 1s - loss: 183.4757 - loglik: -1.8463e+02 - logprior: 1.1506
Epoch 9/10
10/10 - 1s - loss: 183.2363 - loglik: -1.8463e+02 - logprior: 1.3957
Epoch 10/10
10/10 - 1s - loss: 183.2037 - loglik: -1.8477e+02 - logprior: 1.5666
Fitted a model with MAP estimate = -183.1277
Time for alignment: 38.4359
Computed alignments with likelihoods: ['-182.7427', '-183.1124', '-183.1277']
Best model has likelihood: -182.7427
SP score = 0.8319
Training of 3 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2f55760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c9ceca0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 953.4403 - loglik: -9.5215e+02 - logprior: -1.2869e+00
Epoch 2/10
43/43 - 27s - loss: 838.6827 - loglik: -8.3694e+02 - logprior: -1.7437e+00
Epoch 3/10
43/43 - 27s - loss: 826.0134 - loglik: -8.2421e+02 - logprior: -1.7990e+00
Epoch 4/10
43/43 - 27s - loss: 821.2697 - loglik: -8.1949e+02 - logprior: -1.7797e+00
Epoch 5/10
43/43 - 27s - loss: 820.6988 - loglik: -8.1888e+02 - logprior: -1.8203e+00
Epoch 6/10
43/43 - 27s - loss: 816.8397 - loglik: -8.1505e+02 - logprior: -1.7888e+00
Epoch 7/10
43/43 - 28s - loss: 818.0903 - loglik: -8.1624e+02 - logprior: -1.8471e+00
Fitted a model with MAP estimate = -826.8078
expansions: [(8, 1), (13, 1), (16, 1), (21, 1), (22, 1), (24, 1), (25, 3), (26, 1), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (51, 2), (57, 1), (59, 1), (62, 2), (64, 2), (80, 1), (81, 1), (82, 2), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (96, 1), (101, 1), (104, 2), (119, 1), (122, 1), (123, 1), (124, 1), (130, 1), (132, 2), (144, 1), (147, 1), (149, 1), (154, 1), (155, 3), (157, 2), (167, 1), (180, 1), (183, 1), (184, 1), (185, 1), (187, 1), (188, 2), (204, 1), (205, 1), (206, 3), (207, 1), (208, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (238, 2), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 47s - loss: 817.9875 - loglik: -8.1575e+02 - logprior: -2.2334e+00
Epoch 2/2
43/43 - 43s - loss: 795.6063 - loglik: -7.9450e+02 - logprior: -1.1047e+00
Fitted a model with MAP estimate = -793.5226
expansions: [(0, 2)]
discards: [  0  66  85  86 107 113 174 203 208 247 268 269 295 300 356 358 361]
Fitting a model of length 356 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 798.9035 - loglik: -7.9780e+02 - logprior: -1.1039e+00
Epoch 2/2
43/43 - 40s - loss: 794.7253 - loglik: -7.9435e+02 - logprior: -3.7480e-01
Fitted a model with MAP estimate = -794.4141
expansions: []
discards: [  0  33 135]
Fitting a model of length 353 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 58s - loss: 778.9479 - loglik: -7.7809e+02 - logprior: -8.5990e-01
Epoch 2/10
61/61 - 54s - loss: 773.7560 - loglik: -7.7333e+02 - logprior: -4.2901e-01
Epoch 3/10
61/61 - 55s - loss: 769.6871 - loglik: -7.6929e+02 - logprior: -3.9228e-01
Epoch 4/10
61/61 - 54s - loss: 768.2628 - loglik: -7.6790e+02 - logprior: -3.6429e-01
Epoch 5/10
61/61 - 55s - loss: 765.0796 - loglik: -7.6477e+02 - logprior: -3.0991e-01
Epoch 6/10
61/61 - 55s - loss: 762.1627 - loglik: -7.6188e+02 - logprior: -2.8032e-01
Epoch 7/10
61/61 - 55s - loss: 765.3365 - loglik: -7.6509e+02 - logprior: -2.4171e-01
Fitted a model with MAP estimate = -763.7202
Time for alignment: 1049.0112
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 951.6221 - loglik: -9.5036e+02 - logprior: -1.2591e+00
Epoch 2/10
43/43 - 27s - loss: 839.9483 - loglik: -8.3829e+02 - logprior: -1.6587e+00
Epoch 3/10
43/43 - 27s - loss: 829.9466 - loglik: -8.2826e+02 - logprior: -1.6907e+00
Epoch 4/10
43/43 - 27s - loss: 824.4016 - loglik: -8.2273e+02 - logprior: -1.6719e+00
Epoch 5/10
43/43 - 28s - loss: 821.0621 - loglik: -8.1937e+02 - logprior: -1.6969e+00
Epoch 6/10
43/43 - 27s - loss: 820.0149 - loglik: -8.1833e+02 - logprior: -1.6819e+00
Epoch 7/10
43/43 - 27s - loss: 820.0955 - loglik: -8.1833e+02 - logprior: -1.7637e+00
Fitted a model with MAP estimate = -828.1848
expansions: [(7, 2), (16, 1), (21, 3), (22, 2), (23, 1), (24, 2), (30, 1), (39, 1), (40, 1), (41, 1), (42, 1), (45, 1), (46, 1), (56, 1), (59, 1), (60, 1), (61, 2), (78, 1), (79, 1), (80, 1), (81, 2), (90, 1), (91, 2), (92, 1), (95, 1), (97, 1), (98, 1), (103, 2), (119, 1), (122, 1), (123, 1), (127, 1), (130, 1), (134, 1), (148, 2), (153, 2), (154, 1), (155, 3), (157, 2), (181, 1), (183, 1), (185, 2), (186, 2), (187, 1), (188, 2), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (239, 3), (240, 1), (242, 1), (245, 1), (248, 1), (250, 2), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Fitting a model of length 373 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 46s - loss: 815.4716 - loglik: -8.1336e+02 - logprior: -2.1145e+00
Epoch 2/2
43/43 - 44s - loss: 794.4180 - loglik: -7.9339e+02 - logprior: -1.0242e+00
Fitted a model with MAP estimate = -794.5051
expansions: [(0, 2)]
discards: [  0  25  26  81 106 122 137 198 207 243 246 267 268 269 295 300 316 333
 358 361 363]
Fitting a model of length 354 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 799.4507 - loglik: -7.9832e+02 - logprior: -1.1357e+00
Epoch 2/2
43/43 - 40s - loss: 795.0656 - loglik: -7.9466e+02 - logprior: -4.0228e-01
Fitted a model with MAP estimate = -794.4061
expansions: []
discards: [0 1]
Fitting a model of length 352 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 780.2475 - loglik: -7.7932e+02 - logprior: -9.2789e-01
Epoch 2/10
61/61 - 54s - loss: 774.6174 - loglik: -7.7427e+02 - logprior: -3.4325e-01
Epoch 3/10
61/61 - 55s - loss: 771.1118 - loglik: -7.7081e+02 - logprior: -2.9985e-01
Epoch 4/10
61/61 - 55s - loss: 770.0980 - loglik: -7.6981e+02 - logprior: -2.8482e-01
Epoch 5/10
61/61 - 54s - loss: 764.7422 - loglik: -7.6451e+02 - logprior: -2.2757e-01
Epoch 6/10
61/61 - 55s - loss: 763.3052 - loglik: -7.6309e+02 - logprior: -2.1121e-01
Epoch 7/10
61/61 - 54s - loss: 765.5794 - loglik: -7.6545e+02 - logprior: -1.3116e-01
Fitted a model with MAP estimate = -764.1309
Time for alignment: 1046.6925
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 32s - loss: 953.1805 - loglik: -9.5189e+02 - logprior: -1.2939e+00
Epoch 2/10
43/43 - 27s - loss: 838.7144 - loglik: -8.3701e+02 - logprior: -1.7094e+00
Epoch 3/10
43/43 - 28s - loss: 826.5789 - loglik: -8.2485e+02 - logprior: -1.7263e+00
Epoch 4/10
43/43 - 27s - loss: 822.9349 - loglik: -8.2121e+02 - logprior: -1.7267e+00
Epoch 5/10
43/43 - 27s - loss: 819.3829 - loglik: -8.1763e+02 - logprior: -1.7501e+00
Epoch 6/10
43/43 - 27s - loss: 818.4597 - loglik: -8.1667e+02 - logprior: -1.7941e+00
Epoch 7/10
43/43 - 27s - loss: 818.3021 - loglik: -8.1651e+02 - logprior: -1.7887e+00
Epoch 8/10
43/43 - 28s - loss: 818.7191 - loglik: -8.1696e+02 - logprior: -1.7610e+00
Fitted a model with MAP estimate = -828.2504
expansions: [(8, 1), (13, 1), (16, 1), (20, 2), (21, 2), (22, 1), (23, 1), (24, 2), (30, 1), (40, 1), (42, 1), (43, 1), (46, 1), (47, 1), (51, 2), (57, 1), (60, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 1), (86, 2), (90, 1), (91, 1), (93, 1), (95, 1), (96, 1), (102, 1), (104, 2), (121, 1), (122, 1), (125, 1), (131, 1), (132, 2), (142, 1), (148, 1), (150, 1), (153, 2), (156, 2), (157, 2), (168, 1), (181, 1), (183, 2), (185, 2), (186, 2), (187, 1), (188, 2), (204, 1), (205, 3), (206, 2), (207, 1), (210, 1), (219, 1), (220, 1), (221, 1), (222, 1), (225, 1), (226, 2), (236, 1), (238, 2), (242, 1), (245, 1), (248, 1), (249, 1), (260, 2), (269, 3), (270, 2), (271, 2)]
discards: [0]
Fitting a model of length 371 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 46s - loss: 817.6418 - loglik: -8.1544e+02 - logprior: -2.1989e+00
Epoch 2/2
43/43 - 43s - loss: 796.5508 - loglik: -7.9542e+02 - logprior: -1.1259e+00
Fitted a model with MAP estimate = -794.7877
expansions: [(0, 2)]
discards: [  0  24  25  67 111 137 171 198 205 236 243 246 267 268 269 295 300 356
 358 361]
Fitting a model of length 353 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 799.9299 - loglik: -7.9878e+02 - logprior: -1.1503e+00
Epoch 2/2
43/43 - 40s - loss: 795.6622 - loglik: -7.9532e+02 - logprior: -3.4701e-01
Fitted a model with MAP estimate = -794.0182
expansions: []
discards: [0 1]
Fitting a model of length 351 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 780.9441 - loglik: -7.8003e+02 - logprior: -9.1033e-01
Epoch 2/10
61/61 - 54s - loss: 773.8907 - loglik: -7.7357e+02 - logprior: -3.2349e-01
Epoch 3/10
61/61 - 54s - loss: 772.0016 - loglik: -7.7174e+02 - logprior: -2.6614e-01
Epoch 4/10
61/61 - 54s - loss: 769.0597 - loglik: -7.6880e+02 - logprior: -2.5635e-01
Epoch 5/10
61/61 - 54s - loss: 767.8283 - loglik: -7.6763e+02 - logprior: -1.9639e-01
Epoch 6/10
61/61 - 54s - loss: 763.8130 - loglik: -7.6368e+02 - logprior: -1.3029e-01
Epoch 7/10
61/61 - 54s - loss: 764.8672 - loglik: -7.6476e+02 - logprior: -1.1087e-01
Fitted a model with MAP estimate = -764.8383
Time for alignment: 1069.1763
Computed alignments with likelihoods: ['-763.7202', '-764.1309', '-764.8383']
Best model has likelihood: -763.7202
SP score = 0.8300
Training of 3 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2886bfbb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c2f5fd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.0511 - loglik: -1.9338e+02 - logprior: -5.6667e+01
Epoch 2/10
10/10 - 1s - loss: 187.8153 - loglik: -1.7222e+02 - logprior: -1.5597e+01
Epoch 3/10
10/10 - 1s - loss: 162.4145 - loglik: -1.5483e+02 - logprior: -7.5894e+00
Epoch 4/10
10/10 - 1s - loss: 152.5027 - loglik: -1.4793e+02 - logprior: -4.5688e+00
Epoch 5/10
10/10 - 1s - loss: 149.8634 - loglik: -1.4666e+02 - logprior: -3.2061e+00
Epoch 6/10
10/10 - 1s - loss: 148.7254 - loglik: -1.4637e+02 - logprior: -2.3533e+00
Epoch 7/10
10/10 - 1s - loss: 147.7116 - loglik: -1.4593e+02 - logprior: -1.7818e+00
Epoch 8/10
10/10 - 1s - loss: 146.9402 - loglik: -1.4547e+02 - logprior: -1.4721e+00
Epoch 9/10
10/10 - 1s - loss: 147.1487 - loglik: -1.4586e+02 - logprior: -1.2923e+00
Fitted a model with MAP estimate = -147.0291
expansions: [(11, 1), (12, 2)]
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.3986 - loglik: -1.4433e+02 - logprior: -6.4070e+01
Epoch 2/2
10/10 - 1s - loss: 170.0568 - loglik: -1.4315e+02 - logprior: -2.6903e+01
Fitted a model with MAP estimate = -163.8747
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 194.5371 - loglik: -1.4229e+02 - logprior: -5.2247e+01
Epoch 2/2
10/10 - 1s - loss: 154.7368 - loglik: -1.4033e+02 - logprior: -1.4409e+01
Fitted a model with MAP estimate = -150.0796
expansions: []
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 204.1480 - loglik: -1.4195e+02 - logprior: -6.2200e+01
Epoch 2/10
10/10 - 1s - loss: 162.3529 - loglik: -1.4123e+02 - logprior: -2.1123e+01
Epoch 3/10
10/10 - 1s - loss: 150.3931 - loglik: -1.4210e+02 - logprior: -8.2969e+00
Epoch 4/10
10/10 - 1s - loss: 146.1738 - loglik: -1.4239e+02 - logprior: -3.7854e+00
Epoch 5/10
10/10 - 1s - loss: 145.0810 - loglik: -1.4311e+02 - logprior: -1.9707e+00
Epoch 6/10
10/10 - 1s - loss: 143.7907 - loglik: -1.4269e+02 - logprior: -1.0974e+00
Epoch 7/10
10/10 - 1s - loss: 142.3631 - loglik: -1.4177e+02 - logprior: -5.9392e-01
Epoch 8/10
10/10 - 1s - loss: 143.4819 - loglik: -1.4322e+02 - logprior: -2.5801e-01
Fitted a model with MAP estimate = -142.6787
Time for alignment: 40.7975
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 250.3831 - loglik: -1.9372e+02 - logprior: -5.6665e+01
Epoch 2/10
10/10 - 1s - loss: 188.1153 - loglik: -1.7252e+02 - logprior: -1.5593e+01
Epoch 3/10
10/10 - 1s - loss: 162.8188 - loglik: -1.5521e+02 - logprior: -7.6048e+00
Epoch 4/10
10/10 - 1s - loss: 153.0164 - loglik: -1.4853e+02 - logprior: -4.4890e+00
Epoch 5/10
10/10 - 1s - loss: 150.3044 - loglik: -1.4735e+02 - logprior: -2.9544e+00
Epoch 6/10
10/10 - 1s - loss: 149.3750 - loglik: -1.4736e+02 - logprior: -2.0123e+00
Epoch 7/10
10/10 - 1s - loss: 149.3581 - loglik: -1.4790e+02 - logprior: -1.4589e+00
Epoch 8/10
10/10 - 1s - loss: 148.5780 - loglik: -1.4740e+02 - logprior: -1.1819e+00
Epoch 9/10
10/10 - 1s - loss: 147.5906 - loglik: -1.4656e+02 - logprior: -1.0264e+00
Epoch 10/10
10/10 - 1s - loss: 148.3600 - loglik: -1.4747e+02 - logprior: -8.8552e-01
Fitted a model with MAP estimate = -147.9336
expansions: []
discards: [0]
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.0035 - loglik: -1.4682e+02 - logprior: -6.4183e+01
Epoch 2/2
10/10 - 1s - loss: 173.7591 - loglik: -1.4673e+02 - logprior: -2.7032e+01
Fitted a model with MAP estimate = -168.2529
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 197.8996 - loglik: -1.4553e+02 - logprior: -5.2366e+01
Epoch 2/2
10/10 - 1s - loss: 160.0687 - loglik: -1.4556e+02 - logprior: -1.4512e+01
Fitted a model with MAP estimate = -154.7036
expansions: []
discards: [0]
Fitting a model of length 46 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 208.2501 - loglik: -1.4613e+02 - logprior: -6.2124e+01
Epoch 2/10
10/10 - 1s - loss: 166.9186 - loglik: -1.4612e+02 - logprior: -2.0798e+01
Epoch 3/10
10/10 - 1s - loss: 155.2715 - loglik: -1.4705e+02 - logprior: -8.2185e+00
Epoch 4/10
10/10 - 1s - loss: 150.7281 - loglik: -1.4685e+02 - logprior: -3.8736e+00
Epoch 5/10
10/10 - 1s - loss: 149.7611 - loglik: -1.4768e+02 - logprior: -2.0800e+00
Epoch 6/10
10/10 - 1s - loss: 147.9621 - loglik: -1.4674e+02 - logprior: -1.2232e+00
Epoch 7/10
10/10 - 1s - loss: 147.9074 - loglik: -1.4713e+02 - logprior: -7.7514e-01
Epoch 8/10
10/10 - 1s - loss: 148.4010 - loglik: -1.4795e+02 - logprior: -4.4887e-01
Fitted a model with MAP estimate = -147.5700
Time for alignment: 40.5944
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 249.5218 - loglik: -1.9286e+02 - logprior: -5.6665e+01
Epoch 2/10
10/10 - 1s - loss: 188.9133 - loglik: -1.7331e+02 - logprior: -1.5603e+01
Epoch 3/10
10/10 - 1s - loss: 162.2852 - loglik: -1.5468e+02 - logprior: -7.6036e+00
Epoch 4/10
10/10 - 1s - loss: 153.3787 - loglik: -1.4881e+02 - logprior: -4.5653e+00
Epoch 5/10
10/10 - 1s - loss: 150.1827 - loglik: -1.4700e+02 - logprior: -3.1828e+00
Epoch 6/10
10/10 - 1s - loss: 148.4256 - loglik: -1.4612e+02 - logprior: -2.3060e+00
Epoch 7/10
10/10 - 1s - loss: 148.2436 - loglik: -1.4652e+02 - logprior: -1.7188e+00
Epoch 8/10
10/10 - 1s - loss: 146.7427 - loglik: -1.4535e+02 - logprior: -1.3953e+00
Epoch 9/10
10/10 - 1s - loss: 147.0685 - loglik: -1.4584e+02 - logprior: -1.2307e+00
Fitted a model with MAP estimate = -147.0380
expansions: [(11, 1), (12, 2)]
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 208.4187 - loglik: -1.4442e+02 - logprior: -6.4000e+01
Epoch 2/2
10/10 - 1s - loss: 170.3195 - loglik: -1.4344e+02 - logprior: -2.6876e+01
Fitted a model with MAP estimate = -163.8651
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 193.5832 - loglik: -1.4140e+02 - logprior: -5.2183e+01
Epoch 2/2
10/10 - 1s - loss: 155.5652 - loglik: -1.4121e+02 - logprior: -1.4352e+01
Fitted a model with MAP estimate = -150.0433
expansions: []
discards: [0]
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 203.8075 - loglik: -1.4172e+02 - logprior: -6.2083e+01
Epoch 2/10
10/10 - 1s - loss: 162.7185 - loglik: -1.4181e+02 - logprior: -2.0906e+01
Epoch 3/10
10/10 - 1s - loss: 149.7705 - loglik: -1.4162e+02 - logprior: -8.1549e+00
Epoch 4/10
10/10 - 1s - loss: 146.9816 - loglik: -1.4327e+02 - logprior: -3.7125e+00
Epoch 5/10
10/10 - 1s - loss: 144.2199 - loglik: -1.4231e+02 - logprior: -1.9135e+00
Epoch 6/10
10/10 - 1s - loss: 143.7795 - loglik: -1.4273e+02 - logprior: -1.0448e+00
Epoch 7/10
10/10 - 1s - loss: 142.7236 - loglik: -1.4213e+02 - logprior: -5.8897e-01
Epoch 8/10
10/10 - 1s - loss: 142.8763 - loglik: -1.4262e+02 - logprior: -2.5356e-01
Fitted a model with MAP estimate = -142.6957
Time for alignment: 40.3964
Computed alignments with likelihoods: ['-142.6787', '-147.5700', '-142.6957']
Best model has likelihood: -142.6787
SP score = 0.9473
Training of 3 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2a183ddf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2aa22fbb0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.5635 - loglik: -4.7182e+02 - logprior: -6.6742e+01
Epoch 2/10
10/10 - 2s - loss: 439.1462 - loglik: -4.2589e+02 - logprior: -1.3260e+01
Epoch 3/10
10/10 - 2s - loss: 386.5689 - loglik: -3.8238e+02 - logprior: -4.1856e+00
Epoch 4/10
10/10 - 2s - loss: 357.2015 - loglik: -3.5584e+02 - logprior: -1.3577e+00
Epoch 5/10
10/10 - 2s - loss: 344.3780 - loglik: -3.4434e+02 - logprior: -4.0287e-02
Epoch 6/10
10/10 - 2s - loss: 338.4525 - loglik: -3.3933e+02 - logprior: 0.8767
Epoch 7/10
10/10 - 2s - loss: 333.9798 - loglik: -3.3547e+02 - logprior: 1.4874
Epoch 8/10
10/10 - 2s - loss: 332.4521 - loglik: -3.3432e+02 - logprior: 1.8680
Epoch 9/10
10/10 - 2s - loss: 330.7134 - loglik: -3.3276e+02 - logprior: 2.0478
Epoch 10/10
10/10 - 2s - loss: 329.8477 - loglik: -3.3212e+02 - logprior: 2.2701
Fitted a model with MAP estimate = -329.4416
expansions: [(11, 3), (12, 2), (16, 2), (19, 1), (24, 1), (26, 2), (36, 2), (38, 2), (39, 2), (55, 1), (66, 1), (78, 5), (79, 2), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (127, 1), (128, 2)]
discards: [0]
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 405.5828 - loglik: -3.3034e+02 - logprior: -7.5240e+01
Epoch 2/2
10/10 - 3s - loss: 340.1828 - loglik: -3.1270e+02 - logprior: -2.7487e+01
Fitted a model with MAP estimate = -328.2692
expansions: [(0, 1), (128, 1)]
discards: [ 0 11 46 51 53]
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 367.1405 - loglik: -3.0854e+02 - logprior: -5.8598e+01
Epoch 2/2
10/10 - 3s - loss: 313.9287 - loglik: -3.0330e+02 - logprior: -1.0631e+01
Fitted a model with MAP estimate = -305.4306
expansions: [(20, 1), (82, 1), (123, 1)]
discards: [ 24 138]
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 361.5140 - loglik: -3.0464e+02 - logprior: -5.6870e+01
Epoch 2/10
10/10 - 3s - loss: 310.0561 - loglik: -3.0066e+02 - logprior: -9.3913e+00
Epoch 3/10
10/10 - 3s - loss: 298.2014 - loglik: -2.9862e+02 - logprior: 0.4180
Epoch 4/10
10/10 - 3s - loss: 293.8243 - loglik: -2.9849e+02 - logprior: 4.6634
Epoch 5/10
10/10 - 3s - loss: 291.5036 - loglik: -2.9863e+02 - logprior: 7.1269
Epoch 6/10
10/10 - 3s - loss: 290.1209 - loglik: -2.9873e+02 - logprior: 8.6105
Epoch 7/10
10/10 - 3s - loss: 288.6278 - loglik: -2.9830e+02 - logprior: 9.6707
Epoch 8/10
10/10 - 3s - loss: 288.5483 - loglik: -2.9900e+02 - logprior: 10.4485
Epoch 9/10
10/10 - 3s - loss: 287.4424 - loglik: -2.9848e+02 - logprior: 11.0423
Epoch 10/10
10/10 - 3s - loss: 286.8763 - loglik: -2.9839e+02 - logprior: 11.5181
Fitted a model with MAP estimate = -286.5758
Time for alignment: 74.2912
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 538.5098 - loglik: -4.7176e+02 - logprior: -6.6745e+01
Epoch 2/10
10/10 - 2s - loss: 439.3145 - loglik: -4.2605e+02 - logprior: -1.3269e+01
Epoch 3/10
10/10 - 2s - loss: 386.3240 - loglik: -3.8197e+02 - logprior: -4.3510e+00
Epoch 4/10
10/10 - 2s - loss: 357.6382 - loglik: -3.5616e+02 - logprior: -1.4786e+00
Epoch 5/10
10/10 - 2s - loss: 345.0865 - loglik: -3.4493e+02 - logprior: -1.5154e-01
Epoch 6/10
10/10 - 2s - loss: 337.8661 - loglik: -3.3852e+02 - logprior: 0.6495
Epoch 7/10
10/10 - 2s - loss: 335.0187 - loglik: -3.3633e+02 - logprior: 1.3160
Epoch 8/10
10/10 - 2s - loss: 333.5545 - loglik: -3.3535e+02 - logprior: 1.7939
Epoch 9/10
10/10 - 2s - loss: 332.3937 - loglik: -3.3450e+02 - logprior: 2.1063
Epoch 10/10
10/10 - 2s - loss: 331.5322 - loglik: -3.3392e+02 - logprior: 2.3928
Fitted a model with MAP estimate = -331.1965
expansions: [(11, 3), (13, 1), (19, 2), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (69, 1), (71, 2), (77, 2), (79, 2), (80, 2), (89, 1), (90, 3), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 409.2061 - loglik: -3.3414e+02 - logprior: -7.5065e+01
Epoch 2/2
10/10 - 3s - loss: 345.0002 - loglik: -3.1765e+02 - logprior: -2.7345e+01
Fitted a model with MAP estimate = -333.3789
expansions: [(0, 2), (130, 1)]
discards: [  0  11  23  44  49  87 115 116]
Fitting a model of length 170 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 373.4057 - loglik: -3.1506e+02 - logprior: -5.8349e+01
Epoch 2/2
10/10 - 3s - loss: 320.5164 - loglik: -3.1014e+02 - logprior: -1.0373e+01
Fitted a model with MAP estimate = -311.2075
expansions: []
discards: [  0  48 138 159]
Fitting a model of length 166 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 384.9918 - loglik: -3.1379e+02 - logprior: -7.1207e+01
Epoch 2/10
10/10 - 3s - loss: 327.1431 - loglik: -3.0995e+02 - logprior: -1.7190e+01
Epoch 3/10
10/10 - 3s - loss: 309.5895 - loglik: -3.0812e+02 - logprior: -1.4671e+00
Epoch 4/10
10/10 - 2s - loss: 302.3294 - loglik: -3.0635e+02 - logprior: 4.0248
Epoch 5/10
10/10 - 3s - loss: 298.3818 - loglik: -3.0499e+02 - logprior: 6.6096
Epoch 6/10
10/10 - 3s - loss: 297.2603 - loglik: -3.0536e+02 - logprior: 8.1015
Epoch 7/10
10/10 - 3s - loss: 296.0861 - loglik: -3.0519e+02 - logprior: 9.1012
Epoch 8/10
10/10 - 3s - loss: 295.6890 - loglik: -3.0559e+02 - logprior: 9.9010
Epoch 9/10
10/10 - 3s - loss: 294.5652 - loglik: -3.0518e+02 - logprior: 10.6139
Epoch 10/10
10/10 - 3s - loss: 294.3707 - loglik: -3.0560e+02 - logprior: 11.2308
Fitted a model with MAP estimate = -294.0111
Time for alignment: 71.4444
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 538.6850 - loglik: -4.7194e+02 - logprior: -6.6742e+01
Epoch 2/10
10/10 - 2s - loss: 439.3997 - loglik: -4.2612e+02 - logprior: -1.3275e+01
Epoch 3/10
10/10 - 2s - loss: 386.2082 - loglik: -3.8181e+02 - logprior: -4.3941e+00
Epoch 4/10
10/10 - 2s - loss: 354.4322 - loglik: -3.5281e+02 - logprior: -1.6242e+00
Epoch 5/10
10/10 - 2s - loss: 340.4030 - loglik: -3.3993e+02 - logprior: -4.7480e-01
Epoch 6/10
10/10 - 2s - loss: 334.0500 - loglik: -3.3448e+02 - logprior: 0.4310
Epoch 7/10
10/10 - 2s - loss: 331.5580 - loglik: -3.3276e+02 - logprior: 1.2056
Epoch 8/10
10/10 - 2s - loss: 329.5592 - loglik: -3.3120e+02 - logprior: 1.6418
Epoch 9/10
10/10 - 2s - loss: 328.5711 - loglik: -3.3048e+02 - logprior: 1.9075
Epoch 10/10
10/10 - 2s - loss: 328.3257 - loglik: -3.3051e+02 - logprior: 2.1889
Fitted a model with MAP estimate = -327.9355
expansions: [(11, 3), (12, 1), (19, 2), (26, 1), (27, 1), (28, 1), (36, 2), (38, 2), (39, 2), (51, 1), (55, 1), (66, 1), (70, 1), (77, 2), (78, 2), (80, 2), (89, 1), (90, 2), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 404.9933 - loglik: -3.2976e+02 - logprior: -7.5233e+01
Epoch 2/2
10/10 - 3s - loss: 339.8057 - loglik: -3.1237e+02 - logprior: -2.7435e+01
Fitted a model with MAP estimate = -328.6001
expansions: [(0, 2), (130, 1)]
discards: [  0  11  23  44  49  51 116 143 164]
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 368.3580 - loglik: -3.0999e+02 - logprior: -5.8364e+01
Epoch 2/2
10/10 - 3s - loss: 315.3183 - loglik: -3.0495e+02 - logprior: -1.0369e+01
Fitted a model with MAP estimate = -306.5882
expansions: [(124, 1)]
discards: [0]
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 379.1641 - loglik: -3.0722e+02 - logprior: -7.1940e+01
Epoch 2/10
10/10 - 3s - loss: 324.2039 - loglik: -3.0538e+02 - logprior: -1.8822e+01
Epoch 3/10
10/10 - 3s - loss: 304.5982 - loglik: -3.0257e+02 - logprior: -2.0283e+00
Epoch 4/10
10/10 - 3s - loss: 297.3322 - loglik: -3.0145e+02 - logprior: 4.1199
Epoch 5/10
10/10 - 3s - loss: 295.1340 - loglik: -3.0191e+02 - logprior: 6.7789
Epoch 6/10
10/10 - 3s - loss: 292.5251 - loglik: -3.0076e+02 - logprior: 8.2302
Epoch 7/10
10/10 - 3s - loss: 292.4261 - loglik: -3.0169e+02 - logprior: 9.2609
Epoch 8/10
10/10 - 3s - loss: 290.5390 - loglik: -3.0062e+02 - logprior: 10.0803
Epoch 9/10
10/10 - 3s - loss: 290.5689 - loglik: -3.0137e+02 - logprior: 10.7998
Fitted a model with MAP estimate = -290.0200
Time for alignment: 70.3399
Computed alignments with likelihoods: ['-286.5758', '-294.0111', '-290.0200']
Best model has likelihood: -286.5758
SP score = 0.7793
Training of 3 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe27778b220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2b71790>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.4485 - loglik: -2.8253e+02 - logprior: -9.9163e+00
Epoch 2/10
12/12 - 1s - loss: 254.0140 - loglik: -2.5161e+02 - logprior: -2.4015e+00
Epoch 3/10
12/12 - 1s - loss: 226.7931 - loglik: -2.2509e+02 - logprior: -1.7003e+00
Epoch 4/10
12/12 - 1s - loss: 217.0720 - loglik: -2.1537e+02 - logprior: -1.7023e+00
Epoch 5/10
12/12 - 1s - loss: 213.3932 - loglik: -2.1173e+02 - logprior: -1.6587e+00
Epoch 6/10
12/12 - 1s - loss: 209.8271 - loglik: -2.0826e+02 - logprior: -1.5706e+00
Epoch 7/10
12/12 - 1s - loss: 208.8217 - loglik: -2.0729e+02 - logprior: -1.5343e+00
Epoch 8/10
12/12 - 1s - loss: 208.6726 - loglik: -2.0713e+02 - logprior: -1.5411e+00
Epoch 9/10
12/12 - 1s - loss: 207.4733 - loglik: -2.0593e+02 - logprior: -1.5384e+00
Epoch 10/10
12/12 - 1s - loss: 207.7831 - loglik: -2.0627e+02 - logprior: -1.5168e+00
Fitted a model with MAP estimate = -207.4412
expansions: [(6, 3), (10, 3), (11, 2), (21, 1), (36, 4), (49, 1), (50, 3), (52, 1), (59, 6), (61, 1), (64, 1), (70, 1)]
discards: [0]
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 219.8877 - loglik: -2.0849e+02 - logprior: -1.1398e+01
Epoch 2/2
12/12 - 2s - loss: 201.3076 - loglik: -1.9658e+02 - logprior: -4.7307e+00
Fitted a model with MAP estimate = -197.6095
expansions: [(0, 5), (45, 1), (46, 1)]
discards: [ 0 16 78 79]
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 203.4443 - loglik: -1.9431e+02 - logprior: -9.1390e+00
Epoch 2/2
12/12 - 1s - loss: 192.5842 - loglik: -1.9028e+02 - logprior: -2.3032e+00
Fitted a model with MAP estimate = -190.3075
expansions: []
discards: [1 2 3 4]
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 198.9366 - loglik: -1.9012e+02 - logprior: -8.8150e+00
Epoch 2/10
12/12 - 1s - loss: 192.0819 - loglik: -1.8983e+02 - logprior: -2.2549e+00
Epoch 3/10
12/12 - 1s - loss: 189.3258 - loglik: -1.8784e+02 - logprior: -1.4865e+00
Epoch 4/10
12/12 - 1s - loss: 188.0415 - loglik: -1.8687e+02 - logprior: -1.1704e+00
Epoch 5/10
12/12 - 1s - loss: 185.8987 - loglik: -1.8493e+02 - logprior: -9.6544e-01
Epoch 6/10
12/12 - 1s - loss: 184.5241 - loglik: -1.8362e+02 - logprior: -9.0686e-01
Epoch 7/10
12/12 - 1s - loss: 184.1668 - loglik: -1.8330e+02 - logprior: -8.7002e-01
Epoch 8/10
12/12 - 1s - loss: 184.1890 - loglik: -1.8333e+02 - logprior: -8.5585e-01
Fitted a model with MAP estimate = -183.1266
Time for alignment: 51.0398
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.4732 - loglik: -2.8256e+02 - logprior: -9.9182e+00
Epoch 2/10
12/12 - 1s - loss: 254.7737 - loglik: -2.5238e+02 - logprior: -2.3983e+00
Epoch 3/10
12/12 - 1s - loss: 227.3587 - loglik: -2.2567e+02 - logprior: -1.6857e+00
Epoch 4/10
12/12 - 1s - loss: 217.5793 - loglik: -2.1591e+02 - logprior: -1.6732e+00
Epoch 5/10
12/12 - 1s - loss: 212.5869 - loglik: -2.1094e+02 - logprior: -1.6465e+00
Epoch 6/10
12/12 - 1s - loss: 209.7100 - loglik: -2.0811e+02 - logprior: -1.5972e+00
Epoch 7/10
12/12 - 1s - loss: 208.5484 - loglik: -2.0697e+02 - logprior: -1.5810e+00
Epoch 8/10
12/12 - 1s - loss: 207.1912 - loglik: -2.0557e+02 - logprior: -1.6204e+00
Epoch 9/10
12/12 - 1s - loss: 206.8730 - loglik: -2.0522e+02 - logprior: -1.6494e+00
Epoch 10/10
12/12 - 1s - loss: 207.1035 - loglik: -2.0547e+02 - logprior: -1.6366e+00
Fitted a model with MAP estimate = -206.4555
expansions: [(7, 2), (8, 1), (10, 4), (11, 2), (21, 1), (36, 4), (49, 1), (50, 3), (58, 2), (59, 5), (61, 1), (64, 1), (70, 1)]
discards: [0]
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 219.2940 - loglik: -2.0787e+02 - logprior: -1.1420e+01
Epoch 2/2
12/12 - 1s - loss: 200.3540 - loglik: -1.9559e+02 - logprior: -4.7625e+00
Fitted a model with MAP estimate = -196.8702
expansions: [(0, 5), (47, 1)]
discards: [ 0 14 15 76 79]
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 203.5221 - loglik: -1.9447e+02 - logprior: -9.0490e+00
Epoch 2/2
12/12 - 1s - loss: 192.8662 - loglik: -1.9066e+02 - logprior: -2.2061e+00
Fitted a model with MAP estimate = -191.0653
expansions: []
discards: [1 2 3 4]
Fitting a model of length 104 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 200.6193 - loglik: -1.9188e+02 - logprior: -8.7416e+00
Epoch 2/10
12/12 - 1s - loss: 192.1268 - loglik: -1.8998e+02 - logprior: -2.1504e+00
Epoch 3/10
12/12 - 1s - loss: 190.4789 - loglik: -1.8915e+02 - logprior: -1.3324e+00
Epoch 4/10
12/12 - 1s - loss: 188.9749 - loglik: -1.8789e+02 - logprior: -1.0872e+00
Epoch 5/10
12/12 - 1s - loss: 187.3120 - loglik: -1.8644e+02 - logprior: -8.7008e-01
Epoch 6/10
12/12 - 1s - loss: 185.4494 - loglik: -1.8461e+02 - logprior: -8.4293e-01
Epoch 7/10
12/12 - 1s - loss: 185.4388 - loglik: -1.8465e+02 - logprior: -7.8854e-01
Epoch 8/10
12/12 - 1s - loss: 184.4137 - loglik: -1.8363e+02 - logprior: -7.8388e-01
Epoch 9/10
12/12 - 1s - loss: 183.8421 - loglik: -1.8309e+02 - logprior: -7.4895e-01
Epoch 10/10
12/12 - 1s - loss: 183.4970 - loglik: -1.8278e+02 - logprior: -7.2000e-01
Fitted a model with MAP estimate = -183.7431
Time for alignment: 52.6983
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 292.5053 - loglik: -2.8259e+02 - logprior: -9.9182e+00
Epoch 2/10
12/12 - 1s - loss: 253.8945 - loglik: -2.5149e+02 - logprior: -2.4010e+00
Epoch 3/10
12/12 - 1s - loss: 227.0973 - loglik: -2.2541e+02 - logprior: -1.6866e+00
Epoch 4/10
12/12 - 1s - loss: 216.5281 - loglik: -2.1485e+02 - logprior: -1.6797e+00
Epoch 5/10
12/12 - 1s - loss: 211.5385 - loglik: -2.0984e+02 - logprior: -1.6972e+00
Epoch 6/10
12/12 - 1s - loss: 209.6479 - loglik: -2.0801e+02 - logprior: -1.6331e+00
Epoch 7/10
12/12 - 1s - loss: 208.6828 - loglik: -2.0708e+02 - logprior: -1.6015e+00
Epoch 8/10
12/12 - 1s - loss: 207.2420 - loglik: -2.0561e+02 - logprior: -1.6272e+00
Epoch 9/10
12/12 - 1s - loss: 207.9267 - loglik: -2.0629e+02 - logprior: -1.6377e+00
Fitted a model with MAP estimate = -207.2073
expansions: [(6, 3), (10, 3), (13, 1), (21, 1), (29, 1), (36, 4), (49, 2), (50, 3), (52, 1), (58, 3), (59, 4), (61, 1), (64, 1), (70, 1)]
discards: [0]
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 219.4830 - loglik: -2.0805e+02 - logprior: -1.1431e+01
Epoch 2/2
12/12 - 1s - loss: 201.4278 - loglik: -1.9666e+02 - logprior: -4.7666e+00
Fitted a model with MAP estimate = -197.2921
expansions: [(0, 6), (48, 1)]
discards: [ 0 62 80]
Fitting a model of length 112 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 203.4660 - loglik: -1.9419e+02 - logprior: -9.2725e+00
Epoch 2/2
12/12 - 2s - loss: 192.2445 - loglik: -1.8977e+02 - logprior: -2.4731e+00
Fitted a model with MAP estimate = -190.3249
expansions: []
discards: [1 2 3 4 5]
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 199.3302 - loglik: -1.9041e+02 - logprior: -8.9182e+00
Epoch 2/10
12/12 - 1s - loss: 191.6702 - loglik: -1.8930e+02 - logprior: -2.3741e+00
Epoch 3/10
12/12 - 1s - loss: 189.9140 - loglik: -1.8831e+02 - logprior: -1.6036e+00
Epoch 4/10
12/12 - 1s - loss: 187.0160 - loglik: -1.8575e+02 - logprior: -1.2657e+00
Epoch 5/10
12/12 - 1s - loss: 185.4596 - loglik: -1.8437e+02 - logprior: -1.0874e+00
Epoch 6/10
12/12 - 1s - loss: 184.6276 - loglik: -1.8360e+02 - logprior: -1.0266e+00
Epoch 7/10
12/12 - 1s - loss: 182.8672 - loglik: -1.8186e+02 - logprior: -1.0073e+00
Epoch 8/10
12/12 - 1s - loss: 182.8679 - loglik: -1.8189e+02 - logprior: -9.7962e-01
Fitted a model with MAP estimate = -182.3401
Time for alignment: 47.8891
Computed alignments with likelihoods: ['-183.1266', '-183.7431', '-182.3401']
Best model has likelihood: -182.3401
SP score = 0.8776
Training of 3 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25db0e3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb3f9b80>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 329.0617 - loglik: -2.8883e+02 - logprior: -4.0232e+01
Epoch 2/10
10/10 - 1s - loss: 276.6012 - loglik: -2.6654e+02 - logprior: -1.0059e+01
Epoch 3/10
10/10 - 1s - loss: 250.1200 - loglik: -2.4596e+02 - logprior: -4.1554e+00
Epoch 4/10
10/10 - 1s - loss: 237.5147 - loglik: -2.3551e+02 - logprior: -2.0081e+00
Epoch 5/10
10/10 - 1s - loss: 232.2466 - loglik: -2.3126e+02 - logprior: -9.8632e-01
Epoch 6/10
10/10 - 1s - loss: 228.9661 - loglik: -2.2841e+02 - logprior: -5.5125e-01
Epoch 7/10
10/10 - 1s - loss: 227.2319 - loglik: -2.2686e+02 - logprior: -3.7268e-01
Epoch 8/10
10/10 - 1s - loss: 226.3595 - loglik: -2.2608e+02 - logprior: -2.8178e-01
Epoch 9/10
10/10 - 1s - loss: 225.0534 - loglik: -2.2488e+02 - logprior: -1.7400e-01
Epoch 10/10
10/10 - 1s - loss: 225.0472 - loglik: -2.2502e+02 - logprior: -3.0586e-02
Fitted a model with MAP estimate = -224.5930
expansions: [(0, 3), (5, 1), (8, 1), (36, 1), (43, 10), (53, 3)]
discards: []
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 273.6084 - loglik: -2.2147e+02 - logprior: -5.2135e+01
Epoch 2/2
10/10 - 1s - loss: 232.2700 - loglik: -2.1695e+02 - logprior: -1.5320e+01
Fitted a model with MAP estimate = -224.3292
expansions: [(7, 1)]
discards: [ 0  1 70]
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 261.9502 - loglik: -2.1624e+02 - logprior: -4.5706e+01
Epoch 2/2
10/10 - 1s - loss: 233.5470 - loglik: -2.1600e+02 - logprior: -1.7548e+01
Fitted a model with MAP estimate = -228.5027
expansions: [(0, 3)]
discards: [0]
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 255.6629 - loglik: -2.1495e+02 - logprior: -4.0715e+01
Epoch 2/10
10/10 - 1s - loss: 224.1689 - loglik: -2.1405e+02 - logprior: -1.0114e+01
Epoch 3/10
10/10 - 1s - loss: 217.2893 - loglik: -2.1400e+02 - logprior: -3.2910e+00
Epoch 4/10
10/10 - 1s - loss: 215.2486 - loglik: -2.1447e+02 - logprior: -7.8171e-01
Epoch 5/10
10/10 - 1s - loss: 213.8902 - loglik: -2.1438e+02 - logprior: 0.4870
Epoch 6/10
10/10 - 1s - loss: 213.6234 - loglik: -2.1482e+02 - logprior: 1.2001
Epoch 7/10
10/10 - 1s - loss: 213.1728 - loglik: -2.1479e+02 - logprior: 1.6131
Epoch 8/10
10/10 - 1s - loss: 213.0570 - loglik: -2.1493e+02 - logprior: 1.8712
Epoch 9/10
10/10 - 1s - loss: 212.3498 - loglik: -2.1441e+02 - logprior: 2.0617
Epoch 10/10
10/10 - 1s - loss: 212.8844 - loglik: -2.1511e+02 - logprior: 2.2261
Fitted a model with MAP estimate = -212.4987
Time for alignment: 43.0600
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 329.3702 - loglik: -2.8913e+02 - logprior: -4.0237e+01
Epoch 2/10
10/10 - 1s - loss: 276.5997 - loglik: -2.6652e+02 - logprior: -1.0075e+01
Epoch 3/10
10/10 - 1s - loss: 251.0407 - loglik: -2.4684e+02 - logprior: -4.1977e+00
Epoch 4/10
10/10 - 1s - loss: 237.3093 - loglik: -2.3525e+02 - logprior: -2.0625e+00
Epoch 5/10
10/10 - 1s - loss: 231.5866 - loglik: -2.3052e+02 - logprior: -1.0665e+00
Epoch 6/10
10/10 - 1s - loss: 229.2671 - loglik: -2.2868e+02 - logprior: -5.8576e-01
Epoch 7/10
10/10 - 1s - loss: 227.4300 - loglik: -2.2703e+02 - logprior: -3.9997e-01
Epoch 8/10
10/10 - 1s - loss: 226.7259 - loglik: -2.2641e+02 - logprior: -3.1808e-01
Epoch 9/10
10/10 - 1s - loss: 225.7912 - loglik: -2.2555e+02 - logprior: -2.4063e-01
Epoch 10/10
10/10 - 1s - loss: 224.9310 - loglik: -2.2484e+02 - logprior: -9.3481e-02
Fitted a model with MAP estimate = -225.0154
expansions: [(0, 3), (5, 1), (7, 1), (8, 1), (36, 1), (37, 2), (43, 5), (53, 3)]
discards: []
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 274.1777 - loglik: -2.2201e+02 - logprior: -5.2165e+01
Epoch 2/2
10/10 - 1s - loss: 233.3706 - loglik: -2.1811e+02 - logprior: -1.5258e+01
Fitted a model with MAP estimate = -225.9281
expansions: []
discards: [ 0  1  2 68]
Fitting a model of length 89 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 254.8149 - loglik: -2.1766e+02 - logprior: -3.7158e+01
Epoch 2/2
10/10 - 1s - loss: 225.8039 - loglik: -2.1675e+02 - logprior: -9.0541e+00
Fitted a model with MAP estimate = -221.7317
expansions: [(0, 3)]
discards: [39 52 53]
Fitting a model of length 89 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 268.0211 - loglik: -2.1789e+02 - logprior: -5.0134e+01
Epoch 2/10
10/10 - 1s - loss: 230.6150 - loglik: -2.1674e+02 - logprior: -1.3872e+01
Epoch 3/10
10/10 - 1s - loss: 221.8916 - loglik: -2.1696e+02 - logprior: -4.9347e+00
Epoch 4/10
10/10 - 1s - loss: 218.4176 - loglik: -2.1689e+02 - logprior: -1.5296e+00
Epoch 5/10
10/10 - 1s - loss: 217.2970 - loglik: -2.1737e+02 - logprior: 0.0738
Epoch 6/10
10/10 - 1s - loss: 216.5284 - loglik: -2.1745e+02 - logprior: 0.9204
Epoch 7/10
10/10 - 1s - loss: 216.3013 - loglik: -2.1768e+02 - logprior: 1.3827
Epoch 8/10
10/10 - 1s - loss: 216.0891 - loglik: -2.1775e+02 - logprior: 1.6624
Epoch 9/10
10/10 - 1s - loss: 215.5175 - loglik: -2.1739e+02 - logprior: 1.8680
Epoch 10/10
10/10 - 1s - loss: 215.9420 - loglik: -2.1799e+02 - logprior: 2.0509
Fitted a model with MAP estimate = -215.4897
Time for alignment: 41.2627
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 329.3536 - loglik: -2.8912e+02 - logprior: -4.0235e+01
Epoch 2/10
10/10 - 1s - loss: 276.2982 - loglik: -2.6623e+02 - logprior: -1.0068e+01
Epoch 3/10
10/10 - 1s - loss: 249.6148 - loglik: -2.4546e+02 - logprior: -4.1582e+00
Epoch 4/10
10/10 - 1s - loss: 236.5416 - loglik: -2.3453e+02 - logprior: -2.0104e+00
Epoch 5/10
10/10 - 1s - loss: 232.2976 - loglik: -2.3129e+02 - logprior: -1.0052e+00
Epoch 6/10
10/10 - 1s - loss: 229.1458 - loglik: -2.2858e+02 - logprior: -5.7028e-01
Epoch 7/10
10/10 - 1s - loss: 227.2034 - loglik: -2.2679e+02 - logprior: -4.1072e-01
Epoch 8/10
10/10 - 1s - loss: 226.4379 - loglik: -2.2613e+02 - logprior: -3.0502e-01
Epoch 9/10
10/10 - 1s - loss: 225.8239 - loglik: -2.2560e+02 - logprior: -2.2188e-01
Epoch 10/10
10/10 - 1s - loss: 225.0827 - loglik: -2.2500e+02 - logprior: -8.7461e-02
Fitted a model with MAP estimate = -224.9954
expansions: [(0, 3), (5, 1), (7, 1), (8, 1), (36, 1), (37, 2), (43, 5), (53, 3)]
discards: []
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 274.3665 - loglik: -2.2213e+02 - logprior: -5.2240e+01
Epoch 2/2
10/10 - 1s - loss: 233.2412 - loglik: -2.1792e+02 - logprior: -1.5318e+01
Fitted a model with MAP estimate = -225.6438
expansions: []
discards: [ 0  1 55 68]
Fitting a model of length 89 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 264.3015 - loglik: -2.1833e+02 - logprior: -4.5969e+01
Epoch 2/2
10/10 - 1s - loss: 235.0684 - loglik: -2.1742e+02 - logprior: -1.7644e+01
Fitted a model with MAP estimate = -230.6588
expansions: [(0, 3)]
discards: [ 0 40]
Fitting a model of length 90 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 258.3912 - loglik: -2.1780e+02 - logprior: -4.0590e+01
Epoch 2/10
10/10 - 1s - loss: 226.7376 - loglik: -2.1667e+02 - logprior: -1.0063e+01
Epoch 3/10
10/10 - 1s - loss: 219.7893 - loglik: -2.1644e+02 - logprior: -3.3460e+00
Epoch 4/10
10/10 - 1s - loss: 218.1159 - loglik: -2.1727e+02 - logprior: -8.4113e-01
Epoch 5/10
10/10 - 1s - loss: 216.5097 - loglik: -2.1693e+02 - logprior: 0.4183
Epoch 6/10
10/10 - 1s - loss: 216.3578 - loglik: -2.1748e+02 - logprior: 1.1216
Epoch 7/10
10/10 - 1s - loss: 215.9656 - loglik: -2.1750e+02 - logprior: 1.5332
Epoch 8/10
10/10 - 1s - loss: 215.6466 - loglik: -2.1743e+02 - logprior: 1.7829
Epoch 9/10
10/10 - 1s - loss: 215.8562 - loglik: -2.1782e+02 - logprior: 1.9658
Fitted a model with MAP estimate = -215.4286
Time for alignment: 40.2202
Computed alignments with likelihoods: ['-212.4987', '-215.4897', '-215.4286']
Best model has likelihood: -212.4987
SP score = 0.7292
Training of 3 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25d11f7c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe24b5a5760>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 551.3204 - loglik: -4.7355e+02 - logprior: -7.7772e+01
Epoch 2/10
10/10 - 2s - loss: 419.4681 - loglik: -4.0425e+02 - logprior: -1.5216e+01
Epoch 3/10
10/10 - 2s - loss: 341.0794 - loglik: -3.3575e+02 - logprior: -5.3324e+00
Epoch 4/10
10/10 - 2s - loss: 293.3528 - loglik: -2.9004e+02 - logprior: -3.3121e+00
Epoch 5/10
10/10 - 2s - loss: 277.0032 - loglik: -2.7465e+02 - logprior: -2.3528e+00
Epoch 6/10
10/10 - 2s - loss: 270.0007 - loglik: -2.6895e+02 - logprior: -1.0486e+00
Epoch 7/10
10/10 - 2s - loss: 267.5856 - loglik: -2.6756e+02 - logprior: -2.1033e-02
Epoch 8/10
10/10 - 2s - loss: 265.5412 - loglik: -2.6596e+02 - logprior: 0.4188
Epoch 9/10
10/10 - 2s - loss: 264.7975 - loglik: -2.6554e+02 - logprior: 0.7462
Epoch 10/10
10/10 - 2s - loss: 264.1953 - loglik: -2.6539e+02 - logprior: 1.1995
Fitted a model with MAP estimate = -263.8150
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (43, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Fitting a model of length 175 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 346.6854 - loglik: -2.5802e+02 - logprior: -8.8661e+01
Epoch 2/2
10/10 - 3s - loss: 271.3531 - loglik: -2.3920e+02 - logprior: -3.2150e+01
Fitted a model with MAP estimate = -258.8324
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (43, 1), (83, 1)]
discards: [  0 103 116]
Fitting a model of length 183 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 300.5698 - loglik: -2.3117e+02 - logprior: -6.9396e+01
Epoch 2/2
10/10 - 3s - loss: 234.6210 - loglik: -2.2280e+02 - logprior: -1.1819e+01
Fitted a model with MAP estimate = -223.9908
expansions: [(18, 1)]
discards: [ 0  1 22]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 308.5034 - loglik: -2.2378e+02 - logprior: -8.4726e+01
Epoch 2/10
10/10 - 3s - loss: 246.2260 - loglik: -2.2095e+02 - logprior: -2.5277e+01
Epoch 3/10
10/10 - 3s - loss: 224.2905 - loglik: -2.2005e+02 - logprior: -4.2409e+00
Epoch 4/10
10/10 - 3s - loss: 214.3117 - loglik: -2.2042e+02 - logprior: 6.1104
Epoch 5/10
10/10 - 3s - loss: 210.1941 - loglik: -2.2007e+02 - logprior: 9.8723
Epoch 6/10
10/10 - 3s - loss: 208.4640 - loglik: -2.2034e+02 - logprior: 11.8728
Epoch 7/10
10/10 - 3s - loss: 207.3651 - loglik: -2.2052e+02 - logprior: 13.1528
Epoch 8/10
10/10 - 3s - loss: 206.2092 - loglik: -2.2029e+02 - logprior: 14.0841
Epoch 9/10
10/10 - 3s - loss: 205.7079 - loglik: -2.2057e+02 - logprior: 14.8585
Epoch 10/10
10/10 - 3s - loss: 205.1662 - loglik: -2.2074e+02 - logprior: 15.5718
Fitted a model with MAP estimate = -204.6876
Time for alignment: 88.5607
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 551.1955 - loglik: -4.7342e+02 - logprior: -7.7772e+01
Epoch 2/10
10/10 - 2s - loss: 419.4809 - loglik: -4.0426e+02 - logprior: -1.5217e+01
Epoch 3/10
10/10 - 2s - loss: 340.6073 - loglik: -3.3526e+02 - logprior: -5.3505e+00
Epoch 4/10
10/10 - 2s - loss: 293.5577 - loglik: -2.9017e+02 - logprior: -3.3914e+00
Epoch 5/10
10/10 - 2s - loss: 274.9606 - loglik: -2.7215e+02 - logprior: -2.8082e+00
Epoch 6/10
10/10 - 2s - loss: 267.9186 - loglik: -2.6593e+02 - logprior: -1.9844e+00
Epoch 7/10
10/10 - 2s - loss: 264.8946 - loglik: -2.6388e+02 - logprior: -1.0192e+00
Epoch 8/10
10/10 - 2s - loss: 262.7133 - loglik: -2.6220e+02 - logprior: -5.1713e-01
Epoch 9/10
10/10 - 2s - loss: 262.2864 - loglik: -2.6211e+02 - logprior: -1.7243e-01
Epoch 10/10
10/10 - 2s - loss: 261.4931 - loglik: -2.6176e+02 - logprior: 0.2625
Fitted a model with MAP estimate = -261.2485
expansions: [(18, 1), (20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 2), (58, 1), (77, 1), (78, 2), (79, 1), (90, 1), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Fitting a model of length 177 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 343.5133 - loglik: -2.5512e+02 - logprior: -8.8389e+01
Epoch 2/2
10/10 - 3s - loss: 268.2007 - loglik: -2.3657e+02 - logprior: -3.1630e+01
Fitted a model with MAP estimate = -256.1936
expansions: [(0, 3), (15, 4), (86, 1)]
discards: [  0  43  53 118]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 299.7764 - loglik: -2.3055e+02 - logprior: -6.9226e+01
Epoch 2/2
10/10 - 3s - loss: 235.2892 - loglik: -2.2357e+02 - logprior: -1.1720e+01
Fitted a model with MAP estimate = -224.8517
expansions: [(16, 1), (22, 1)]
discards: [0 1]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 309.1753 - loglik: -2.2445e+02 - logprior: -8.4726e+01
Epoch 2/10
10/10 - 3s - loss: 246.8030 - loglik: -2.2129e+02 - logprior: -2.5516e+01
Epoch 3/10
10/10 - 3s - loss: 225.1721 - loglik: -2.2059e+02 - logprior: -4.5790e+00
Epoch 4/10
10/10 - 3s - loss: 213.8035 - loglik: -2.1986e+02 - logprior: 6.0561
Epoch 5/10
10/10 - 3s - loss: 210.5113 - loglik: -2.2040e+02 - logprior: 9.8848
Epoch 6/10
10/10 - 3s - loss: 208.2646 - loglik: -2.2014e+02 - logprior: 11.8711
Epoch 7/10
10/10 - 3s - loss: 207.4185 - loglik: -2.2057e+02 - logprior: 13.1533
Epoch 8/10
10/10 - 3s - loss: 206.5116 - loglik: -2.2059e+02 - logprior: 14.0768
Epoch 9/10
10/10 - 3s - loss: 205.5436 - loglik: -2.2040e+02 - logprior: 14.8528
Epoch 10/10
10/10 - 3s - loss: 205.0783 - loglik: -2.2065e+02 - logprior: 15.5673
Fitted a model with MAP estimate = -204.6920
Time for alignment: 86.5955
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 551.4064 - loglik: -4.7363e+02 - logprior: -7.7773e+01
Epoch 2/10
10/10 - 2s - loss: 419.1593 - loglik: -4.0394e+02 - logprior: -1.5220e+01
Epoch 3/10
10/10 - 2s - loss: 341.3614 - loglik: -3.3601e+02 - logprior: -5.3508e+00
Epoch 4/10
10/10 - 2s - loss: 294.4082 - loglik: -2.9116e+02 - logprior: -3.2502e+00
Epoch 5/10
10/10 - 2s - loss: 276.4188 - loglik: -2.7398e+02 - logprior: -2.4436e+00
Epoch 6/10
10/10 - 2s - loss: 269.5016 - loglik: -2.6774e+02 - logprior: -1.7600e+00
Epoch 7/10
10/10 - 2s - loss: 266.2127 - loglik: -2.6538e+02 - logprior: -8.2963e-01
Epoch 8/10
10/10 - 2s - loss: 264.1481 - loglik: -2.6389e+02 - logprior: -2.5874e-01
Epoch 9/10
10/10 - 2s - loss: 262.7738 - loglik: -2.6275e+02 - logprior: -2.5311e-02
Epoch 10/10
10/10 - 2s - loss: 262.5714 - loglik: -2.6296e+02 - logprior: 0.3863
Fitted a model with MAP estimate = -262.0707
expansions: [(21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (45, 1), (58, 1), (77, 1), (78, 2), (79, 1), (90, 2), (91, 1), (98, 1), (100, 2), (110, 1), (118, 1), (119, 1), (124, 1), (125, 1), (127, 1), (129, 3), (138, 1), (141, 1), (142, 1)]
discards: [0]
Fitting a model of length 177 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 343.9300 - loglik: -2.5544e+02 - logprior: -8.8486e+01
Epoch 2/2
10/10 - 3s - loss: 268.2874 - loglik: -2.3632e+02 - logprior: -3.1964e+01
Fitted a model with MAP estimate = -255.7943
expansions: [(0, 3), (14, 1), (15, 3), (16, 2), (85, 1)]
discards: [  0  43 105 118]
Fitting a model of length 183 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 299.2233 - loglik: -2.2985e+02 - logprior: -6.9376e+01
Epoch 2/2
10/10 - 3s - loss: 234.2084 - loglik: -2.2238e+02 - logprior: -1.1829e+01
Fitted a model with MAP estimate = -223.7713
expansions: [(18, 1)]
discards: [ 0  1 22]
Fitting a model of length 181 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 308.5384 - loglik: -2.2382e+02 - logprior: -8.4716e+01
Epoch 2/10
10/10 - 3s - loss: 245.9018 - loglik: -2.2063e+02 - logprior: -2.5270e+01
Epoch 3/10
10/10 - 3s - loss: 224.5414 - loglik: -2.2030e+02 - logprior: -4.2405e+00
Epoch 4/10
10/10 - 3s - loss: 214.0537 - loglik: -2.2017e+02 - logprior: 6.1204
Epoch 5/10
10/10 - 3s - loss: 210.1709 - loglik: -2.2007e+02 - logprior: 9.9001
Epoch 6/10
10/10 - 3s - loss: 208.5440 - loglik: -2.2045e+02 - logprior: 11.9036
Epoch 7/10
10/10 - 3s - loss: 207.1192 - loglik: -2.2030e+02 - logprior: 13.1855
Epoch 8/10
10/10 - 3s - loss: 206.3523 - loglik: -2.2047e+02 - logprior: 14.1185
Epoch 9/10
10/10 - 3s - loss: 205.6566 - loglik: -2.2055e+02 - logprior: 14.8973
Epoch 10/10
10/10 - 3s - loss: 205.1258 - loglik: -2.2074e+02 - logprior: 15.6178
Fitted a model with MAP estimate = -204.6638
Time for alignment: 87.9892
Computed alignments with likelihoods: ['-204.6876', '-204.6920', '-204.6638']
Best model has likelihood: -204.6638
SP score = 0.9137
Training of 3 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2a64580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25de24040>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 26s - loss: 815.0059 - loglik: -8.1301e+02 - logprior: -2.0007e+00
Epoch 2/10
33/33 - 21s - loss: 718.1668 - loglik: -7.1750e+02 - logprior: -6.6229e-01
Epoch 3/10
33/33 - 21s - loss: 710.9635 - loglik: -7.1036e+02 - logprior: -6.0332e-01
Epoch 4/10
33/33 - 21s - loss: 712.4332 - loglik: -7.1187e+02 - logprior: -5.6760e-01
Fitted a model with MAP estimate = -707.1885
expansions: [(0, 5), (7, 1), (9, 3), (33, 4), (40, 1), (63, 1), (65, 1), (71, 1), (72, 1), (78, 2), (112, 1), (113, 1), (118, 1), (132, 2), (155, 3), (163, 5), (183, 1), (217, 1), (220, 1), (221, 4), (230, 4)]
discards: [  2   3 224 225 226 227 228]
Fitting a model of length 267 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 707.3215 - loglik: -7.0451e+02 - logprior: -2.8150e+00
Epoch 2/2
33/33 - 26s - loss: 701.0996 - loglik: -7.0061e+02 - logprior: -4.9057e-01
Fitted a model with MAP estimate = -697.8846
expansions: [(267, 5)]
discards: [  1   6   7   8  41 179 190 191 192 193 262 263 265 266]
Fitting a model of length 258 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 705.4403 - loglik: -7.0343e+02 - logprior: -2.0119e+00
Epoch 2/2
33/33 - 24s - loss: 703.0712 - loglik: -7.0296e+02 - logprior: -1.0777e-01
Fitted a model with MAP estimate = -699.3649
expansions: [(0, 5), (1, 1), (71, 1), (250, 1), (258, 4)]
discards: [ 69 251 252 253 255 256 257]
Fitting a model of length 263 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 706.3707 - loglik: -7.0337e+02 - logprior: -3.0029e+00
Epoch 2/10
33/33 - 25s - loss: 699.3073 - loglik: -6.9926e+02 - logprior: -4.6200e-02
Epoch 3/10
33/33 - 25s - loss: 698.6158 - loglik: -6.9874e+02 - logprior: 0.1235
Epoch 4/10
33/33 - 25s - loss: 700.9635 - loglik: -7.0115e+02 - logprior: 0.1913
Fitted a model with MAP estimate = -696.0003
Time for alignment: 394.7232
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 25s - loss: 811.9686 - loglik: -8.0997e+02 - logprior: -2.0008e+00
Epoch 2/10
33/33 - 21s - loss: 720.2416 - loglik: -7.1960e+02 - logprior: -6.3735e-01
Epoch 3/10
33/33 - 21s - loss: 712.0659 - loglik: -7.1155e+02 - logprior: -5.1106e-01
Epoch 4/10
33/33 - 21s - loss: 709.8120 - loglik: -7.0934e+02 - logprior: -4.7180e-01
Epoch 5/10
33/33 - 21s - loss: 711.9639 - loglik: -7.1152e+02 - logprior: -4.4536e-01
Fitted a model with MAP estimate = -707.8884
expansions: [(0, 5), (11, 1), (32, 3), (62, 1), (63, 2), (72, 1), (73, 2), (78, 1), (91, 1), (108, 1), (110, 3), (113, 2), (116, 1), (132, 1), (155, 3), (163, 5), (181, 2), (204, 1), (210, 1), (220, 2), (221, 3), (230, 3)]
discards: [225 226 227 228]
Fitting a model of length 271 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 711.4653 - loglik: -7.0859e+02 - logprior: -2.8742e+00
Epoch 2/2
33/33 - 26s - loss: 698.9020 - loglik: -6.9842e+02 - logprior: -4.8668e-01
Fitted a model with MAP estimate = -698.2902
expansions: [(40, 1), (41, 1), (271, 4)]
discards: [  1   6  16  71 126 127 134 181 192 193 194 195 196 215 258 261 266 267
 268 269]
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 707.6545 - loglik: -7.0566e+02 - logprior: -1.9930e+00
Epoch 2/2
33/33 - 24s - loss: 699.8836 - loglik: -6.9975e+02 - logprior: -1.3775e-01
Fitted a model with MAP estimate = -699.6930
expansions: [(0, 5), (252, 1)]
discards: [  5   6 253 254 255 256]
Fitting a model of length 257 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 708.6214 - loglik: -7.0566e+02 - logprior: -2.9566e+00
Epoch 2/10
33/33 - 24s - loss: 699.3447 - loglik: -6.9933e+02 - logprior: -1.4107e-02
Epoch 3/10
33/33 - 25s - loss: 700.6050 - loglik: -7.0078e+02 - logprior: 0.1724
Fitted a model with MAP estimate = -697.7154
Time for alignment: 389.5299
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 25s - loss: 813.1912 - loglik: -8.1120e+02 - logprior: -1.9920e+00
Epoch 2/10
33/33 - 21s - loss: 721.6208 - loglik: -7.2094e+02 - logprior: -6.8502e-01
Epoch 3/10
33/33 - 21s - loss: 709.5707 - loglik: -7.0896e+02 - logprior: -6.0890e-01
Epoch 4/10
33/33 - 21s - loss: 715.7491 - loglik: -7.1519e+02 - logprior: -5.6097e-01
Fitted a model with MAP estimate = -708.8436
expansions: [(0, 5), (10, 2), (34, 6), (64, 2), (73, 1), (74, 2), (79, 2), (91, 1), (113, 1), (116, 1), (118, 1), (134, 1), (155, 3), (163, 4), (171, 1), (184, 1), (217, 1), (220, 2), (221, 3), (230, 4)]
discards: [  3 225 226 227 228]
Fitting a model of length 269 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 708.4434 - loglik: -7.0559e+02 - logprior: -2.8578e+00
Epoch 2/2
33/33 - 26s - loss: 702.0138 - loglik: -7.0147e+02 - logprior: -5.4756e-01
Fitted a model with MAP estimate = -697.9279
expansions: []
discards: [  1   5   6   7   8  42  43 180 191 192 193 194 255 263 264 265 266 267
 268]
Fitting a model of length 250 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 707.6680 - loglik: -7.0565e+02 - logprior: -2.0152e+00
Epoch 2/2
33/33 - 24s - loss: 701.5990 - loglik: -7.0154e+02 - logprior: -5.5440e-02
Fitted a model with MAP estimate = -700.6364
expansions: [(0, 5), (1, 1), (12, 1), (246, 2), (250, 4)]
discards: [247]
Fitting a model of length 262 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 706.6723 - loglik: -7.0369e+02 - logprior: -2.9806e+00
Epoch 2/10
33/33 - 25s - loss: 701.9136 - loglik: -7.0186e+02 - logprior: -5.4463e-02
Epoch 3/10
33/33 - 25s - loss: 699.6337 - loglik: -6.9975e+02 - logprior: 0.1194
Epoch 4/10
33/33 - 25s - loss: 695.2234 - loglik: -6.9541e+02 - logprior: 0.1825
Epoch 5/10
33/33 - 25s - loss: 697.3936 - loglik: -6.9768e+02 - logprior: 0.2816
Fitted a model with MAP estimate = -695.9272
Time for alignment: 415.8466
Computed alignments with likelihoods: ['-696.0003', '-697.7154', '-695.9272']
Best model has likelihood: -695.9272
SP score = 0.8059
Training of 3 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe28841a730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe24a9ec940>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 410.6352 - loglik: -3.4872e+02 - logprior: -6.1920e+01
Epoch 2/10
10/10 - 1s - loss: 320.9526 - loglik: -3.0666e+02 - logprior: -1.4292e+01
Epoch 3/10
10/10 - 1s - loss: 273.3664 - loglik: -2.6722e+02 - logprior: -6.1417e+00
Epoch 4/10
10/10 - 1s - loss: 247.6904 - loglik: -2.4402e+02 - logprior: -3.6750e+00
Epoch 5/10
10/10 - 1s - loss: 238.4657 - loglik: -2.3605e+02 - logprior: -2.4169e+00
Epoch 6/10
10/10 - 1s - loss: 234.6537 - loglik: -2.3309e+02 - logprior: -1.5642e+00
Epoch 7/10
10/10 - 1s - loss: 232.8586 - loglik: -2.3199e+02 - logprior: -8.7236e-01
Epoch 8/10
10/10 - 1s - loss: 232.2758 - loglik: -2.3174e+02 - logprior: -5.3318e-01
Epoch 9/10
10/10 - 1s - loss: 231.4588 - loglik: -2.3114e+02 - logprior: -3.1915e-01
Epoch 10/10
10/10 - 1s - loss: 230.5837 - loglik: -2.3049e+02 - logprior: -9.3774e-02
Fitted a model with MAP estimate = -230.6671
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (63, 1), (64, 3), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 300.7010 - loglik: -2.3146e+02 - logprior: -6.9244e+01
Epoch 2/2
10/10 - 2s - loss: 240.6449 - loglik: -2.1444e+02 - logprior: -2.6208e+01
Fitted a model with MAP estimate = -229.3238
expansions: [(0, 5)]
discards: [  0  79 109]
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 265.7515 - loglik: -2.1134e+02 - logprior: -5.4407e+01
Epoch 2/2
10/10 - 2s - loss: 217.5082 - loglik: -2.0583e+02 - logprior: -1.1676e+01
Fitted a model with MAP estimate = -209.8454
expansions: []
discards: [ 1  2  3  4 17]
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 261.9167 - loglik: -2.0861e+02 - logprior: -5.3307e+01
Epoch 2/10
10/10 - 1s - loss: 218.0564 - loglik: -2.0703e+02 - logprior: -1.1022e+01
Epoch 3/10
10/10 - 1s - loss: 207.1055 - loglik: -2.0482e+02 - logprior: -2.2861e+00
Epoch 4/10
10/10 - 1s - loss: 202.5680 - loglik: -2.0393e+02 - logprior: 1.3663
Epoch 5/10
10/10 - 1s - loss: 199.7086 - loglik: -2.0310e+02 - logprior: 3.3947
Epoch 6/10
10/10 - 1s - loss: 198.1446 - loglik: -2.0269e+02 - logprior: 4.5428
Epoch 7/10
10/10 - 1s - loss: 197.3188 - loglik: -2.0253e+02 - logprior: 5.2146
Epoch 8/10
10/10 - 1s - loss: 197.1758 - loglik: -2.0289e+02 - logprior: 5.7127
Epoch 9/10
10/10 - 1s - loss: 196.2834 - loglik: -2.0247e+02 - logprior: 6.1896
Epoch 10/10
10/10 - 1s - loss: 196.2534 - loglik: -2.0292e+02 - logprior: 6.6677
Fitted a model with MAP estimate = -195.7132
Time for alignment: 50.7345
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 410.6180 - loglik: -3.4870e+02 - logprior: -6.1918e+01
Epoch 2/10
10/10 - 1s - loss: 321.2397 - loglik: -3.0694e+02 - logprior: -1.4296e+01
Epoch 3/10
10/10 - 1s - loss: 274.1287 - loglik: -2.6795e+02 - logprior: -6.1795e+00
Epoch 4/10
10/10 - 1s - loss: 249.6680 - loglik: -2.4576e+02 - logprior: -3.9045e+00
Epoch 5/10
10/10 - 1s - loss: 239.4297 - loglik: -2.3658e+02 - logprior: -2.8530e+00
Epoch 6/10
10/10 - 1s - loss: 234.4738 - loglik: -2.3244e+02 - logprior: -2.0297e+00
Epoch 7/10
10/10 - 1s - loss: 233.6879 - loglik: -2.3238e+02 - logprior: -1.3110e+00
Epoch 8/10
10/10 - 1s - loss: 232.2242 - loglik: -2.3133e+02 - logprior: -8.9431e-01
Epoch 9/10
10/10 - 1s - loss: 231.7437 - loglik: -2.3104e+02 - logprior: -7.0125e-01
Epoch 10/10
10/10 - 1s - loss: 231.1465 - loglik: -2.3067e+02 - logprior: -4.7926e-01
Fitted a model with MAP estimate = -230.8049
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (60, 2), (62, 1), (63, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 301.4822 - loglik: -2.3217e+02 - logprior: -6.9309e+01
Epoch 2/2
10/10 - 2s - loss: 241.0056 - loglik: -2.1474e+02 - logprior: -2.6261e+01
Fitted a model with MAP estimate = -229.4694
expansions: [(0, 5)]
discards: [  0  73 109]
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 266.1915 - loglik: -2.1170e+02 - logprior: -5.4488e+01
Epoch 2/2
10/10 - 2s - loss: 217.6179 - loglik: -2.0588e+02 - logprior: -1.1741e+01
Fitted a model with MAP estimate = -209.9631
expansions: []
discards: [ 1  2  3  4 17]
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 262.4039 - loglik: -2.0905e+02 - logprior: -5.3354e+01
Epoch 2/10
10/10 - 1s - loss: 217.5409 - loglik: -2.0648e+02 - logprior: -1.1063e+01
Epoch 3/10
10/10 - 1s - loss: 207.0793 - loglik: -2.0477e+02 - logprior: -2.3116e+00
Epoch 4/10
10/10 - 1s - loss: 202.2440 - loglik: -2.0358e+02 - logprior: 1.3353
Epoch 5/10
10/10 - 1s - loss: 200.0431 - loglik: -2.0340e+02 - logprior: 3.3584
Epoch 6/10
10/10 - 1s - loss: 198.1742 - loglik: -2.0267e+02 - logprior: 4.4965
Epoch 7/10
10/10 - 1s - loss: 197.2176 - loglik: -2.0238e+02 - logprior: 5.1659
Epoch 8/10
10/10 - 1s - loss: 196.0439 - loglik: -2.0170e+02 - logprior: 5.6539
Epoch 9/10
10/10 - 1s - loss: 196.8933 - loglik: -2.0302e+02 - logprior: 6.1238
Fitted a model with MAP estimate = -196.0957
Time for alignment: 49.0618
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 410.4930 - loglik: -3.4858e+02 - logprior: -6.1917e+01
Epoch 2/10
10/10 - 1s - loss: 321.2516 - loglik: -3.0696e+02 - logprior: -1.4293e+01
Epoch 3/10
10/10 - 1s - loss: 273.1178 - loglik: -2.6696e+02 - logprior: -6.1618e+00
Epoch 4/10
10/10 - 1s - loss: 248.4086 - loglik: -2.4464e+02 - logprior: -3.7644e+00
Epoch 5/10
10/10 - 1s - loss: 238.8630 - loglik: -2.3643e+02 - logprior: -2.4367e+00
Epoch 6/10
10/10 - 1s - loss: 234.2212 - loglik: -2.3259e+02 - logprior: -1.6358e+00
Epoch 7/10
10/10 - 1s - loss: 233.7643 - loglik: -2.3269e+02 - logprior: -1.0768e+00
Epoch 8/10
10/10 - 1s - loss: 231.6855 - loglik: -2.3100e+02 - logprior: -6.8112e-01
Epoch 9/10
10/10 - 1s - loss: 231.6671 - loglik: -2.3122e+02 - logprior: -4.4559e-01
Epoch 10/10
10/10 - 1s - loss: 231.2009 - loglik: -2.3095e+02 - logprior: -2.4874e-01
Fitted a model with MAP estimate = -230.8417
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (55, 1), (63, 1), (64, 1), (71, 3), (78, 1), (79, 1), (83, 1), (87, 3), (88, 1), (91, 1)]
discards: [0]
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 301.5574 - loglik: -2.3227e+02 - logprior: -6.9286e+01
Epoch 2/2
10/10 - 2s - loss: 240.8621 - loglik: -2.1470e+02 - logprior: -2.6166e+01
Fitted a model with MAP estimate = -229.7055
expansions: [(0, 5)]
discards: [  0  87 109]
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 266.6827 - loglik: -2.1229e+02 - logprior: -5.4393e+01
Epoch 2/2
10/10 - 2s - loss: 218.2709 - loglik: -2.0668e+02 - logprior: -1.1596e+01
Fitted a model with MAP estimate = -210.3005
expansions: []
discards: [ 1  2  3  4 17]
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 263.0377 - loglik: -2.0974e+02 - logprior: -5.3298e+01
Epoch 2/10
10/10 - 1s - loss: 218.3930 - loglik: -2.0737e+02 - logprior: -1.1023e+01
Epoch 3/10
10/10 - 1s - loss: 207.6855 - loglik: -2.0541e+02 - logprior: -2.2738e+00
Epoch 4/10
10/10 - 1s - loss: 202.2451 - loglik: -2.0366e+02 - logprior: 1.4148
Epoch 5/10
10/10 - 1s - loss: 200.2490 - loglik: -2.0374e+02 - logprior: 3.4918
Epoch 6/10
10/10 - 1s - loss: 197.7652 - loglik: -2.0243e+02 - logprior: 4.6617
Epoch 7/10
10/10 - 1s - loss: 197.3538 - loglik: -2.0270e+02 - logprior: 5.3503
Epoch 8/10
10/10 - 1s - loss: 196.5536 - loglik: -2.0235e+02 - logprior: 5.7955
Epoch 9/10
10/10 - 1s - loss: 196.0058 - loglik: -2.0219e+02 - logprior: 6.1874
Epoch 10/10
10/10 - 1s - loss: 195.9221 - loglik: -2.0255e+02 - logprior: 6.6271
Fitted a model with MAP estimate = -195.4963
Time for alignment: 50.5418
Computed alignments with likelihoods: ['-195.7132', '-196.0957', '-195.4963']
Best model has likelihood: -195.4963
SP score = 0.9507
Training of 3 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe27faf5d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb136f70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 195.4390 - loglik: -1.9324e+02 - logprior: -2.1961e+00
Epoch 2/10
22/22 - 1s - loss: 162.7780 - loglik: -1.6145e+02 - logprior: -1.3240e+00
Epoch 3/10
22/22 - 1s - loss: 155.5416 - loglik: -1.5414e+02 - logprior: -1.4057e+00
Epoch 4/10
22/22 - 1s - loss: 153.8402 - loglik: -1.5253e+02 - logprior: -1.3053e+00
Epoch 5/10
22/22 - 1s - loss: 153.5001 - loglik: -1.5219e+02 - logprior: -1.3058e+00
Epoch 6/10
22/22 - 1s - loss: 152.8911 - loglik: -1.5162e+02 - logprior: -1.2752e+00
Epoch 7/10
22/22 - 1s - loss: 153.0226 - loglik: -1.5176e+02 - logprior: -1.2624e+00
Fitted a model with MAP estimate = -153.0376
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (21, 2), (22, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 156.8928 - loglik: -1.5399e+02 - logprior: -2.9033e+00
Epoch 2/2
22/22 - 1s - loss: 148.0185 - loglik: -1.4648e+02 - logprior: -1.5346e+00
Fitted a model with MAP estimate = -145.9906
expansions: [(0, 2)]
discards: [ 0  9 17 26 29 31 68]
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 147.5981 - loglik: -1.4549e+02 - logprior: -2.1058e+00
Epoch 2/2
22/22 - 1s - loss: 144.9719 - loglik: -1.4397e+02 - logprior: -1.0057e+00
Fitted a model with MAP estimate = -144.6035
expansions: []
discards: [0]
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.2777 - loglik: -1.4408e+02 - logprior: -1.2010e+00
Epoch 2/10
32/32 - 2s - loss: 142.8792 - loglik: -1.4200e+02 - logprior: -8.8361e-01
Epoch 3/10
32/32 - 2s - loss: 142.7365 - loglik: -1.4188e+02 - logprior: -8.5447e-01
Epoch 4/10
32/32 - 2s - loss: 141.8676 - loglik: -1.4101e+02 - logprior: -8.5533e-01
Epoch 5/10
32/32 - 2s - loss: 142.2076 - loglik: -1.4136e+02 - logprior: -8.4421e-01
Fitted a model with MAP estimate = -141.6270
Time for alignment: 54.6027
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.1269 - loglik: -1.9293e+02 - logprior: -2.1950e+00
Epoch 2/10
22/22 - 1s - loss: 162.3919 - loglik: -1.6108e+02 - logprior: -1.3140e+00
Epoch 3/10
22/22 - 1s - loss: 155.3674 - loglik: -1.5398e+02 - logprior: -1.3908e+00
Epoch 4/10
22/22 - 1s - loss: 154.4684 - loglik: -1.5318e+02 - logprior: -1.2861e+00
Epoch 5/10
22/22 - 1s - loss: 154.0289 - loglik: -1.5275e+02 - logprior: -1.2829e+00
Epoch 6/10
22/22 - 1s - loss: 153.5298 - loglik: -1.5227e+02 - logprior: -1.2605e+00
Epoch 7/10
22/22 - 1s - loss: 153.4968 - loglik: -1.5225e+02 - logprior: -1.2451e+00
Epoch 8/10
22/22 - 1s - loss: 153.2321 - loglik: -1.5199e+02 - logprior: -1.2408e+00
Epoch 9/10
22/22 - 1s - loss: 153.6670 - loglik: -1.5243e+02 - logprior: -1.2361e+00
Fitted a model with MAP estimate = -153.7288
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (22, 3), (23, 2), (28, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Fitting a model of length 76 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 156.4182 - loglik: -1.5352e+02 - logprior: -2.8988e+00
Epoch 2/2
22/22 - 1s - loss: 147.9958 - loglik: -1.4647e+02 - logprior: -1.5302e+00
Fitted a model with MAP estimate = -145.8926
expansions: [(0, 2)]
discards: [ 0  9 17 28 31 67]
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 147.4373 - loglik: -1.4533e+02 - logprior: -2.1026e+00
Epoch 2/2
22/22 - 1s - loss: 144.9216 - loglik: -1.4392e+02 - logprior: -1.0060e+00
Fitted a model with MAP estimate = -144.6930
expansions: []
discards: [0]
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 145.1613 - loglik: -1.4396e+02 - logprior: -1.2010e+00
Epoch 2/10
32/32 - 2s - loss: 143.0996 - loglik: -1.4222e+02 - logprior: -8.7507e-01
Epoch 3/10
32/32 - 2s - loss: 142.6231 - loglik: -1.4177e+02 - logprior: -8.5033e-01
Epoch 4/10
32/32 - 2s - loss: 142.1002 - loglik: -1.4125e+02 - logprior: -8.5141e-01
Epoch 5/10
32/32 - 2s - loss: 142.0934 - loglik: -1.4125e+02 - logprior: -8.4074e-01
Epoch 6/10
32/32 - 2s - loss: 141.5546 - loglik: -1.4071e+02 - logprior: -8.4446e-01
Epoch 7/10
32/32 - 2s - loss: 141.3672 - loglik: -1.4053e+02 - logprior: -8.4098e-01
Epoch 8/10
32/32 - 2s - loss: 141.4383 - loglik: -1.4060e+02 - logprior: -8.4105e-01
Fitted a model with MAP estimate = -141.2471
Time for alignment: 62.9777
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 195.5705 - loglik: -1.9338e+02 - logprior: -2.1940e+00
Epoch 2/10
22/22 - 1s - loss: 162.8415 - loglik: -1.6152e+02 - logprior: -1.3219e+00
Epoch 3/10
22/22 - 1s - loss: 156.0538 - loglik: -1.5463e+02 - logprior: -1.4226e+00
Epoch 4/10
22/22 - 1s - loss: 154.2346 - loglik: -1.5291e+02 - logprior: -1.3257e+00
Epoch 5/10
22/22 - 1s - loss: 153.7883 - loglik: -1.5246e+02 - logprior: -1.3258e+00
Epoch 6/10
22/22 - 1s - loss: 153.4324 - loglik: -1.5213e+02 - logprior: -1.2997e+00
Epoch 7/10
22/22 - 1s - loss: 153.2246 - loglik: -1.5193e+02 - logprior: -1.2905e+00
Epoch 8/10
22/22 - 1s - loss: 153.1954 - loglik: -1.5192e+02 - logprior: -1.2794e+00
Epoch 9/10
22/22 - 1s - loss: 152.9991 - loglik: -1.5172e+02 - logprior: -1.2764e+00
Epoch 10/10
22/22 - 1s - loss: 153.1731 - loglik: -1.5190e+02 - logprior: -1.2732e+00
Fitted a model with MAP estimate = -153.3272
expansions: [(8, 1), (9, 2), (11, 2), (14, 2), (17, 2), (21, 1), (22, 2), (25, 1), (40, 1), (41, 1), (45, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 157.2481 - loglik: -1.5434e+02 - logprior: -2.9064e+00
Epoch 2/2
22/22 - 1s - loss: 148.1701 - loglik: -1.4662e+02 - logprior: -1.5504e+00
Fitted a model with MAP estimate = -146.0876
expansions: [(0, 2)]
discards: [ 0  9 14 18 23 31 68]
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 147.5491 - loglik: -1.4545e+02 - logprior: -2.1030e+00
Epoch 2/2
22/22 - 1s - loss: 144.6046 - loglik: -1.4360e+02 - logprior: -1.0020e+00
Fitted a model with MAP estimate = -144.7232
expansions: []
discards: [0]
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 145.1406 - loglik: -1.4394e+02 - logprior: -1.2044e+00
Epoch 2/10
32/32 - 2s - loss: 143.2241 - loglik: -1.4235e+02 - logprior: -8.7782e-01
Epoch 3/10
32/32 - 2s - loss: 142.3714 - loglik: -1.4152e+02 - logprior: -8.5501e-01
Epoch 4/10
32/32 - 2s - loss: 142.3397 - loglik: -1.4149e+02 - logprior: -8.4991e-01
Epoch 5/10
32/32 - 2s - loss: 142.0081 - loglik: -1.4117e+02 - logprior: -8.4021e-01
Epoch 6/10
32/32 - 2s - loss: 141.3830 - loglik: -1.4053e+02 - logprior: -8.4924e-01
Epoch 7/10
32/32 - 2s - loss: 141.7631 - loglik: -1.4092e+02 - logprior: -8.3873e-01
Fitted a model with MAP estimate = -141.2830
Time for alignment: 62.0964
Computed alignments with likelihoods: ['-141.6270', '-141.2471', '-141.2830']
Best model has likelihood: -141.2471
SP score = 0.8508
Training of 3 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe290e6afd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2665ccf10>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 500.0279 - loglik: -4.8090e+02 - logprior: -1.9123e+01
Epoch 2/10
10/10 - 4s - loss: 393.9320 - loglik: -3.9047e+02 - logprior: -3.4596e+00
Epoch 3/10
10/10 - 4s - loss: 314.8337 - loglik: -3.1343e+02 - logprior: -1.4052e+00
Epoch 4/10
10/10 - 4s - loss: 275.4967 - loglik: -2.7409e+02 - logprior: -1.4078e+00
Epoch 5/10
10/10 - 4s - loss: 261.4329 - loglik: -2.6018e+02 - logprior: -1.2569e+00
Epoch 6/10
10/10 - 4s - loss: 256.8824 - loglik: -2.5583e+02 - logprior: -1.0495e+00
Epoch 7/10
10/10 - 4s - loss: 252.6419 - loglik: -2.5164e+02 - logprior: -9.9731e-01
Epoch 8/10
10/10 - 4s - loss: 252.9264 - loglik: -2.5190e+02 - logprior: -1.0217e+00
Fitted a model with MAP estimate = -252.3642
expansions: [(29, 1), (49, 1), (64, 1), (87, 1), (92, 1), (136, 1)]
discards: [0]
Fitting a model of length 167 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 286.6173 - loglik: -2.6368e+02 - logprior: -2.2932e+01
Epoch 2/2
10/10 - 4s - loss: 259.8463 - loglik: -2.5093e+02 - logprior: -8.9143e+00
Fitted a model with MAP estimate = -257.2273
expansions: [(0, 16)]
discards: [ 0 43 44]
Fitting a model of length 180 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 274.6493 - loglik: -2.5573e+02 - logprior: -1.8920e+01
Epoch 2/2
10/10 - 5s - loss: 249.0428 - loglik: -2.4448e+02 - logprior: -4.5578e+00
Fitted a model with MAP estimate = -242.1664
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 58 59]
Fitting a model of length 163 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 277.7451 - loglik: -2.5646e+02 - logprior: -2.1281e+01
Epoch 2/10
10/10 - 4s - loss: 255.9663 - loglik: -2.5051e+02 - logprior: -5.4528e+00
Epoch 3/10
10/10 - 4s - loss: 252.2277 - loglik: -2.5082e+02 - logprior: -1.4028e+00
Epoch 4/10
10/10 - 4s - loss: 249.4379 - loglik: -2.4922e+02 - logprior: -2.1794e-01
Epoch 5/10
10/10 - 4s - loss: 248.2770 - loglik: -2.4837e+02 - logprior: 0.0895
Epoch 6/10
10/10 - 4s - loss: 245.5706 - loglik: -2.4590e+02 - logprior: 0.3253
Epoch 7/10
10/10 - 4s - loss: 248.2238 - loglik: -2.4873e+02 - logprior: 0.5079
Fitted a model with MAP estimate = -246.2697
Time for alignment: 108.0269
Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 499.8679 - loglik: -4.8074e+02 - logprior: -1.9125e+01
Epoch 2/10
10/10 - 4s - loss: 392.4439 - loglik: -3.8886e+02 - logprior: -3.5851e+00
Epoch 3/10
10/10 - 4s - loss: 312.1935 - loglik: -3.1056e+02 - logprior: -1.6293e+00
Epoch 4/10
10/10 - 4s - loss: 270.4433 - loglik: -2.6899e+02 - logprior: -1.4554e+00
Epoch 5/10
10/10 - 4s - loss: 256.3505 - loglik: -2.5515e+02 - logprior: -1.2028e+00
Epoch 6/10
10/10 - 4s - loss: 251.4131 - loglik: -2.5032e+02 - logprior: -1.0968e+00
Epoch 7/10
10/10 - 4s - loss: 251.4810 - loglik: -2.5046e+02 - logprior: -1.0190e+00
Fitted a model with MAP estimate = -250.0049
expansions: [(30, 1), (49, 1), (80, 1), (89, 1), (108, 1), (137, 1)]
discards: [0]
Fitting a model of length 167 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 281.5905 - loglik: -2.5861e+02 - logprior: -2.2982e+01
Epoch 2/2
10/10 - 4s - loss: 257.8571 - loglik: -2.4894e+02 - logprior: -8.9132e+00
Fitted a model with MAP estimate = -253.8855
expansions: [(0, 17)]
discards: [0]
Fitting a model of length 183 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 269.8290 - loglik: -2.5072e+02 - logprior: -1.9105e+01
Epoch 2/2
10/10 - 5s - loss: 250.9280 - loglik: -2.4609e+02 - logprior: -4.8386e+00
Fitted a model with MAP estimate = -246.7379
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]
Fitting a model of length 167 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 268.9502 - loglik: -2.4778e+02 - logprior: -2.1166e+01
Epoch 2/10
10/10 - 5s - loss: 250.2741 - loglik: -2.4521e+02 - logprior: -5.0620e+00
Epoch 3/10
10/10 - 4s - loss: 245.1835 - loglik: -2.4383e+02 - logprior: -1.3564e+00
Epoch 4/10
10/10 - 4s - loss: 241.4137 - loglik: -2.4121e+02 - logprior: -2.0557e-01
Epoch 5/10
10/10 - 4s - loss: 238.9860 - loglik: -2.3918e+02 - logprior: 0.1893
Epoch 6/10
10/10 - 4s - loss: 240.7213 - loglik: -2.4102e+02 - logprior: 0.2983
Fitted a model with MAP estimate = -238.8960
Time for alignment: 98.6103
Fitting a model of length 162 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 500.0676 - loglik: -4.8095e+02 - logprior: -1.9121e+01
Epoch 2/10
10/10 - 4s - loss: 396.4445 - loglik: -3.9307e+02 - logprior: -3.3730e+00
Epoch 3/10
10/10 - 4s - loss: 319.9740 - loglik: -3.1888e+02 - logprior: -1.0953e+00
Epoch 4/10
10/10 - 4s - loss: 276.6778 - loglik: -2.7593e+02 - logprior: -7.4804e-01
Epoch 5/10
10/10 - 4s - loss: 261.6077 - loglik: -2.6098e+02 - logprior: -6.3218e-01
Epoch 6/10
10/10 - 4s - loss: 259.3691 - loglik: -2.5894e+02 - logprior: -4.3032e-01
Epoch 7/10
10/10 - 4s - loss: 254.6547 - loglik: -2.5433e+02 - logprior: -3.2116e-01
Epoch 8/10
10/10 - 4s - loss: 255.7446 - loglik: -2.5546e+02 - logprior: -2.8217e-01
Fitted a model with MAP estimate = -254.7123
expansions: [(0, 36), (7, 1), (24, 2), (25, 1), (50, 1), (60, 1), (80, 1), (91, 1), (108, 1), (136, 1)]
discards: []
Fitting a model of length 208 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 278.7267 - loglik: -2.5604e+02 - logprior: -2.2687e+01
Epoch 2/2
10/10 - 6s - loss: 238.0978 - loglik: -2.3246e+02 - logprior: -5.6329e+00
Fitted a model with MAP estimate = -227.8259
expansions: [(0, 17), (37, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32]
Fitting a model of length 194 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 265.8712 - loglik: -2.4584e+02 - logprior: -2.0027e+01
Epoch 2/2
10/10 - 5s - loss: 233.8780 - loglik: -2.2986e+02 - logprior: -4.0149e+00
Fitted a model with MAP estimate = -227.9864
expansions: [(0, 26)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Fitting a model of length 200 on 2070 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 261.5705 - loglik: -2.4348e+02 - logprior: -1.8089e+01
Epoch 2/10
10/10 - 5s - loss: 233.6665 - loglik: -2.3054e+02 - logprior: -3.1216e+00
Epoch 3/10
10/10 - 6s - loss: 221.6613 - loglik: -2.2133e+02 - logprior: -3.3031e-01
Epoch 4/10
10/10 - 6s - loss: 215.3089 - loglik: -2.1590e+02 - logprior: 0.5872
Epoch 5/10
10/10 - 6s - loss: 212.4105 - loglik: -2.1352e+02 - logprior: 1.1094
Epoch 6/10
10/10 - 5s - loss: 210.8382 - loglik: -2.1226e+02 - logprior: 1.4217
Epoch 7/10
10/10 - 5s - loss: 210.2024 - loglik: -2.1179e+02 - logprior: 1.5917
Epoch 8/10
10/10 - 5s - loss: 208.8829 - loglik: -2.1063e+02 - logprior: 1.7488
Epoch 9/10
10/10 - 6s - loss: 210.1606 - loglik: -2.1204e+02 - logprior: 1.8755
Fitted a model with MAP estimate = -209.1928
Time for alignment: 133.1569
Computed alignments with likelihoods: ['-242.1664', '-238.8960', '-209.1928']
Best model has likelihood: -209.1928
SP score = 0.4433
Training of 3 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb4d0a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe27fd0bfd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 245.9917 - loglik: -2.2573e+02 - logprior: -2.0263e+01
Epoch 2/10
10/10 - 1s - loss: 216.6899 - loglik: -2.1127e+02 - logprior: -5.4205e+00
Epoch 3/10
10/10 - 1s - loss: 199.3358 - loglik: -1.9648e+02 - logprior: -2.8575e+00
Epoch 4/10
10/10 - 1s - loss: 191.1841 - loglik: -1.8901e+02 - logprior: -2.1790e+00
Epoch 5/10
10/10 - 1s - loss: 188.0458 - loglik: -1.8607e+02 - logprior: -1.9719e+00
Epoch 6/10
10/10 - 1s - loss: 185.9522 - loglik: -1.8413e+02 - logprior: -1.8227e+00
Epoch 7/10
10/10 - 1s - loss: 184.9637 - loglik: -1.8342e+02 - logprior: -1.5410e+00
Epoch 8/10
10/10 - 1s - loss: 184.5488 - loglik: -1.8316e+02 - logprior: -1.3859e+00
Epoch 9/10
10/10 - 1s - loss: 184.3293 - loglik: -1.8296e+02 - logprior: -1.3695e+00
Epoch 10/10
10/10 - 1s - loss: 184.1538 - loglik: -1.8280e+02 - logprior: -1.3540e+00
Fitted a model with MAP estimate = -183.9559
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (34, 1), (41, 3), (42, 2), (51, 1), (53, 1)]
discards: []
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 210.9311 - loglik: -1.8417e+02 - logprior: -2.6765e+01
Epoch 2/2
10/10 - 1s - loss: 188.8873 - loglik: -1.8075e+02 - logprior: -8.1377e+00
Fitted a model with MAP estimate = -184.8632
expansions: []
discards: [0]
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.8001 - loglik: -1.8050e+02 - logprior: -2.3296e+01
Epoch 2/2
10/10 - 1s - loss: 189.3055 - loglik: -1.8018e+02 - logprior: -9.1269e+00
Fitted a model with MAP estimate = -186.3634
expansions: [(0, 2)]
discards: [ 0 11]
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.6169 - loglik: -1.7940e+02 - logprior: -2.0212e+01
Epoch 2/10
10/10 - 1s - loss: 183.4560 - loglik: -1.7811e+02 - logprior: -5.3508e+00
Epoch 3/10
10/10 - 1s - loss: 180.7711 - loglik: -1.7848e+02 - logprior: -2.2950e+00
Epoch 4/10
10/10 - 1s - loss: 179.2059 - loglik: -1.7791e+02 - logprior: -1.2918e+00
Epoch 5/10
10/10 - 1s - loss: 178.0423 - loglik: -1.7712e+02 - logprior: -9.2559e-01
Epoch 6/10
10/10 - 1s - loss: 177.6817 - loglik: -1.7689e+02 - logprior: -7.8934e-01
Epoch 7/10
10/10 - 1s - loss: 177.4881 - loglik: -1.7690e+02 - logprior: -5.9220e-01
Epoch 8/10
10/10 - 1s - loss: 177.1426 - loglik: -1.7672e+02 - logprior: -4.1972e-01
Epoch 9/10
10/10 - 1s - loss: 177.0522 - loglik: -1.7672e+02 - logprior: -3.3080e-01
Epoch 10/10
10/10 - 1s - loss: 176.8722 - loglik: -1.7658e+02 - logprior: -2.9033e-01
Fitted a model with MAP estimate = -176.8438
Time for alignment: 37.6200
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 245.9846 - loglik: -2.2572e+02 - logprior: -2.0263e+01
Epoch 2/10
10/10 - 1s - loss: 216.0758 - loglik: -2.1065e+02 - logprior: -5.4235e+00
Epoch 3/10
10/10 - 1s - loss: 198.6348 - loglik: -1.9575e+02 - logprior: -2.8891e+00
Epoch 4/10
10/10 - 1s - loss: 190.2027 - loglik: -1.8800e+02 - logprior: -2.2054e+00
Epoch 5/10
10/10 - 1s - loss: 187.0573 - loglik: -1.8505e+02 - logprior: -2.0057e+00
Epoch 6/10
10/10 - 1s - loss: 185.5138 - loglik: -1.8369e+02 - logprior: -1.8265e+00
Epoch 7/10
10/10 - 1s - loss: 184.6836 - loglik: -1.8313e+02 - logprior: -1.5579e+00
Epoch 8/10
10/10 - 1s - loss: 183.8736 - loglik: -1.8244e+02 - logprior: -1.4337e+00
Epoch 9/10
10/10 - 1s - loss: 184.4202 - loglik: -1.8298e+02 - logprior: -1.4444e+00
Fitted a model with MAP estimate = -183.9224
expansions: [(0, 2), (8, 1), (9, 1), (23, 1), (41, 2), (42, 3), (44, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 77 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.6281 - loglik: -1.8312e+02 - logprior: -2.6511e+01
Epoch 2/2
10/10 - 1s - loss: 187.0570 - loglik: -1.7902e+02 - logprior: -8.0390e+00
Fitted a model with MAP estimate = -183.1794
expansions: []
discards: [ 0 54]
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.4229 - loglik: -1.7929e+02 - logprior: -2.3138e+01
Epoch 2/2
10/10 - 1s - loss: 187.8502 - loglik: -1.7883e+02 - logprior: -9.0223e+00
Fitted a model with MAP estimate = -185.2101
expansions: [(0, 2)]
discards: [ 0 48]
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 198.5400 - loglik: -1.7841e+02 - logprior: -2.0129e+01
Epoch 2/10
10/10 - 1s - loss: 183.1680 - loglik: -1.7788e+02 - logprior: -5.2888e+00
Epoch 3/10
10/10 - 1s - loss: 179.9057 - loglik: -1.7768e+02 - logprior: -2.2275e+00
Epoch 4/10
10/10 - 1s - loss: 178.5371 - loglik: -1.7731e+02 - logprior: -1.2319e+00
Epoch 5/10
10/10 - 1s - loss: 177.6796 - loglik: -1.7681e+02 - logprior: -8.6556e-01
Epoch 6/10
10/10 - 1s - loss: 177.0733 - loglik: -1.7633e+02 - logprior: -7.4071e-01
Epoch 7/10
10/10 - 1s - loss: 176.9821 - loglik: -1.7641e+02 - logprior: -5.6723e-01
Epoch 8/10
10/10 - 1s - loss: 176.7298 - loglik: -1.7635e+02 - logprior: -3.7972e-01
Epoch 9/10
10/10 - 1s - loss: 176.5935 - loglik: -1.7631e+02 - logprior: -2.8303e-01
Epoch 10/10
10/10 - 1s - loss: 176.5922 - loglik: -1.7634e+02 - logprior: -2.4769e-01
Fitted a model with MAP estimate = -176.4579
Time for alignment: 35.4658
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 246.0183 - loglik: -2.2576e+02 - logprior: -2.0259e+01
Epoch 2/10
10/10 - 1s - loss: 216.1762 - loglik: -2.1076e+02 - logprior: -5.4145e+00
Epoch 3/10
10/10 - 1s - loss: 198.6119 - loglik: -1.9574e+02 - logprior: -2.8744e+00
Epoch 4/10
10/10 - 1s - loss: 190.3943 - loglik: -1.8817e+02 - logprior: -2.2203e+00
Epoch 5/10
10/10 - 1s - loss: 187.2768 - loglik: -1.8527e+02 - logprior: -2.0101e+00
Epoch 6/10
10/10 - 1s - loss: 185.3526 - loglik: -1.8352e+02 - logprior: -1.8279e+00
Epoch 7/10
10/10 - 1s - loss: 184.8460 - loglik: -1.8326e+02 - logprior: -1.5903e+00
Epoch 8/10
10/10 - 1s - loss: 184.1514 - loglik: -1.8267e+02 - logprior: -1.4838e+00
Epoch 9/10
10/10 - 1s - loss: 184.1130 - loglik: -1.8263e+02 - logprior: -1.4865e+00
Epoch 10/10
10/10 - 1s - loss: 184.1347 - loglik: -1.8266e+02 - logprior: -1.4711e+00
Fitted a model with MAP estimate = -183.8772
expansions: [(0, 2), (8, 1), (9, 1), (23, 1), (41, 2), (42, 3), (45, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 209.9946 - loglik: -1.8334e+02 - logprior: -2.6650e+01
Epoch 2/2
10/10 - 1s - loss: 187.4729 - loglik: -1.7938e+02 - logprior: -8.0946e+00
Fitted a model with MAP estimate = -183.7519
expansions: []
discards: [ 0 49]
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.3400 - loglik: -1.8011e+02 - logprior: -2.3228e+01
Epoch 2/2
10/10 - 1s - loss: 188.7154 - loglik: -1.7963e+02 - logprior: -9.0865e+00
Fitted a model with MAP estimate = -186.1063
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 199.3322 - loglik: -1.7905e+02 - logprior: -2.0279e+01
Epoch 2/10
10/10 - 1s - loss: 183.7542 - loglik: -1.7836e+02 - logprior: -5.3953e+00
Epoch 3/10
10/10 - 1s - loss: 180.6533 - loglik: -1.7834e+02 - logprior: -2.3174e+00
Epoch 4/10
10/10 - 1s - loss: 179.5428 - loglik: -1.7824e+02 - logprior: -1.2993e+00
Epoch 5/10
10/10 - 1s - loss: 178.5698 - loglik: -1.7767e+02 - logprior: -9.0192e-01
Epoch 6/10
10/10 - 1s - loss: 177.8391 - loglik: -1.7708e+02 - logprior: -7.6337e-01
Epoch 7/10
10/10 - 1s - loss: 177.6664 - loglik: -1.7705e+02 - logprior: -6.1161e-01
Epoch 8/10
10/10 - 1s - loss: 177.8198 - loglik: -1.7741e+02 - logprior: -4.1443e-01
Fitted a model with MAP estimate = -177.4256
Time for alignment: 36.0651
Computed alignments with likelihoods: ['-176.8438', '-176.4579', '-177.4256']
Best model has likelihood: -176.4579
SP score = 0.6488
Training of 3 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fa3414f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b290da30>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 199.0719 - loglik: -1.8759e+02 - logprior: -1.1484e+01
Epoch 2/10
11/11 - 1s - loss: 158.9409 - loglik: -1.5573e+02 - logprior: -3.2097e+00
Epoch 3/10
11/11 - 1s - loss: 128.0136 - loglik: -1.2563e+02 - logprior: -2.3831e+00
Epoch 4/10
11/11 - 1s - loss: 112.4229 - loglik: -1.1015e+02 - logprior: -2.2732e+00
Epoch 5/10
11/11 - 1s - loss: 107.3448 - loglik: -1.0542e+02 - logprior: -1.9290e+00
Epoch 6/10
11/11 - 1s - loss: 105.6845 - loglik: -1.0381e+02 - logprior: -1.8751e+00
Epoch 7/10
11/11 - 1s - loss: 105.1007 - loglik: -1.0324e+02 - logprior: -1.8624e+00
Epoch 8/10
11/11 - 1s - loss: 104.4831 - loglik: -1.0268e+02 - logprior: -1.7984e+00
Epoch 9/10
11/11 - 1s - loss: 104.0000 - loglik: -1.0220e+02 - logprior: -1.7975e+00
Epoch 10/10
11/11 - 1s - loss: 104.1865 - loglik: -1.0241e+02 - logprior: -1.7791e+00
Fitted a model with MAP estimate = -103.9792
expansions: [(0, 3), (15, 2), (26, 1), (27, 1), (28, 3), (29, 1), (30, 1), (31, 1), (38, 2), (51, 1)]
discards: []
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 111.7194 - loglik: -9.8029e+01 - logprior: -1.3690e+01
Epoch 2/2
11/11 - 1s - loss: 95.8286 - loglik: -9.1706e+01 - logprior: -4.1226e+00
Fitted a model with MAP estimate = -93.0065
expansions: []
discards: [ 0 35]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.6958 - loglik: -9.2625e+01 - logprior: -1.3070e+01
Epoch 2/2
11/11 - 1s - loss: 96.4514 - loglik: -9.1122e+01 - logprior: -5.3290e+00
Fitted a model with MAP estimate = -94.4656
expansions: []
discards: [0]
Fitting a model of length 69 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 103.7039 - loglik: -9.1866e+01 - logprior: -1.1838e+01
Epoch 2/10
11/11 - 1s - loss: 94.7212 - loglik: -9.1324e+01 - logprior: -3.3970e+00
Epoch 3/10
11/11 - 1s - loss: 92.7862 - loglik: -9.0659e+01 - logprior: -2.1272e+00
Epoch 4/10
11/11 - 1s - loss: 91.5986 - loglik: -9.0081e+01 - logprior: -1.5177e+00
Epoch 5/10
11/11 - 1s - loss: 91.7644 - loglik: -9.0517e+01 - logprior: -1.2476e+00
Fitted a model with MAP estimate = -91.2741
Time for alignment: 30.9991
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 198.9873 - loglik: -1.8750e+02 - logprior: -1.1485e+01
Epoch 2/10
11/11 - 1s - loss: 158.1008 - loglik: -1.5488e+02 - logprior: -3.2165e+00
Epoch 3/10
11/11 - 1s - loss: 125.9500 - loglik: -1.2356e+02 - logprior: -2.3914e+00
Epoch 4/10
11/11 - 1s - loss: 109.1575 - loglik: -1.0688e+02 - logprior: -2.2778e+00
Epoch 5/10
11/11 - 1s - loss: 104.4199 - loglik: -1.0241e+02 - logprior: -2.0122e+00
Epoch 6/10
11/11 - 1s - loss: 102.7697 - loglik: -1.0075e+02 - logprior: -2.0210e+00
Epoch 7/10
11/11 - 1s - loss: 102.0642 - loglik: -1.0003e+02 - logprior: -2.0307e+00
Epoch 8/10
11/11 - 1s - loss: 101.4735 - loglik: -9.9488e+01 - logprior: -1.9855e+00
Epoch 9/10
11/11 - 1s - loss: 101.4214 - loglik: -9.9413e+01 - logprior: -2.0084e+00
Epoch 10/10
11/11 - 1s - loss: 101.2509 - loglik: -9.9251e+01 - logprior: -1.9996e+00
Fitted a model with MAP estimate = -101.1683
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 112.8245 - loglik: -9.9114e+01 - logprior: -1.3711e+01
Epoch 2/2
11/11 - 1s - loss: 95.5982 - loglik: -9.1362e+01 - logprior: -4.2358e+00
Fitted a model with MAP estimate = -93.0951
expansions: []
discards: [ 0 36 39 44]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 105.6238 - loglik: -9.2515e+01 - logprior: -1.3109e+01
Epoch 2/2
11/11 - 1s - loss: 96.6156 - loglik: -9.1233e+01 - logprior: -5.3823e+00
Fitted a model with MAP estimate = -94.5142
expansions: [(0, 1)]
discards: [0]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 101.9044 - loglik: -9.1119e+01 - logprior: -1.0786e+01
Epoch 2/10
11/11 - 1s - loss: 93.4392 - loglik: -9.0511e+01 - logprior: -2.9281e+00
Epoch 3/10
11/11 - 1s - loss: 91.4643 - loglik: -8.9716e+01 - logprior: -1.7480e+00
Epoch 4/10
11/11 - 1s - loss: 91.1787 - loglik: -8.9636e+01 - logprior: -1.5429e+00
Epoch 5/10
11/11 - 1s - loss: 90.5921 - loglik: -8.9127e+01 - logprior: -1.4655e+00
Epoch 6/10
11/11 - 1s - loss: 90.4419 - loglik: -8.9135e+01 - logprior: -1.3071e+00
Epoch 7/10
11/11 - 1s - loss: 90.2043 - loglik: -8.9006e+01 - logprior: -1.1983e+00
Epoch 8/10
11/11 - 1s - loss: 90.1343 - loglik: -8.8949e+01 - logprior: -1.1851e+00
Epoch 9/10
11/11 - 1s - loss: 90.0726 - loglik: -8.8888e+01 - logprior: -1.1848e+00
Epoch 10/10
11/11 - 1s - loss: 89.9189 - loglik: -8.8767e+01 - logprior: -1.1522e+00
Fitted a model with MAP estimate = -89.9265
Time for alignment: 33.9064
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 198.7391 - loglik: -1.8725e+02 - logprior: -1.1485e+01
Epoch 2/10
11/11 - 1s - loss: 158.1099 - loglik: -1.5490e+02 - logprior: -3.2096e+00
Epoch 3/10
11/11 - 1s - loss: 124.2612 - loglik: -1.2188e+02 - logprior: -2.3793e+00
Epoch 4/10
11/11 - 1s - loss: 109.5067 - loglik: -1.0724e+02 - logprior: -2.2645e+00
Epoch 5/10
11/11 - 1s - loss: 105.2299 - loglik: -1.0323e+02 - logprior: -1.9976e+00
Epoch 6/10
11/11 - 1s - loss: 103.7533 - loglik: -1.0176e+02 - logprior: -1.9886e+00
Epoch 7/10
11/11 - 1s - loss: 103.0990 - loglik: -1.0112e+02 - logprior: -1.9774e+00
Epoch 8/10
11/11 - 1s - loss: 102.5943 - loglik: -1.0066e+02 - logprior: -1.9354e+00
Epoch 9/10
11/11 - 1s - loss: 102.5403 - loglik: -1.0059e+02 - logprior: -1.9464e+00
Epoch 10/10
11/11 - 1s - loss: 102.3848 - loglik: -1.0045e+02 - logprior: -1.9320e+00
Fitted a model with MAP estimate = -102.3152
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (38, 2), (43, 1)]
discards: []
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 113.1307 - loglik: -9.9430e+01 - logprior: -1.3701e+01
Epoch 2/2
11/11 - 1s - loss: 96.0894 - loglik: -9.1873e+01 - logprior: -4.2162e+00
Fitted a model with MAP estimate = -93.1964
expansions: []
discards: [ 0 36 39 44]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 105.7819 - loglik: -9.2690e+01 - logprior: -1.3092e+01
Epoch 2/2
11/11 - 1s - loss: 96.4717 - loglik: -9.1101e+01 - logprior: -5.3703e+00
Fitted a model with MAP estimate = -94.5013
expansions: [(0, 1)]
discards: [0]
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 101.7835 - loglik: -9.1002e+01 - logprior: -1.0781e+01
Epoch 2/10
11/11 - 1s - loss: 93.4521 - loglik: -9.0535e+01 - logprior: -2.9174e+00
Epoch 3/10
11/11 - 1s - loss: 91.8358 - loglik: -9.0094e+01 - logprior: -1.7416e+00
Epoch 4/10
11/11 - 1s - loss: 90.9022 - loglik: -8.9362e+01 - logprior: -1.5403e+00
Epoch 5/10
11/11 - 1s - loss: 90.8124 - loglik: -8.9349e+01 - logprior: -1.4633e+00
Epoch 6/10
11/11 - 1s - loss: 90.1132 - loglik: -8.8809e+01 - logprior: -1.3038e+00
Epoch 7/10
11/11 - 1s - loss: 90.5131 - loglik: -8.9314e+01 - logprior: -1.1993e+00
Fitted a model with MAP estimate = -90.1376
Time for alignment: 32.8909
Computed alignments with likelihoods: ['-91.2741', '-89.9265', '-90.1376']
Best model has likelihood: -89.9265
SP score = 0.9957
Training of 3 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe253acc0a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe24a851c10>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 171.8741 - loglik: -1.7115e+02 - logprior: -7.2779e-01
Epoch 2/10
42/42 - 3s - loss: 82.4419 - loglik: -8.1808e+01 - logprior: -6.3354e-01
Epoch 3/10
42/42 - 3s - loss: 79.7977 - loglik: -7.9180e+01 - logprior: -6.1718e-01
Epoch 4/10
42/42 - 3s - loss: 79.2868 - loglik: -7.8688e+01 - logprior: -5.9903e-01
Epoch 5/10
42/42 - 3s - loss: 78.1925 - loglik: -7.7597e+01 - logprior: -5.9533e-01
Epoch 6/10
42/42 - 3s - loss: 78.4647 - loglik: -7.7885e+01 - logprior: -5.7973e-01
Fitted a model with MAP estimate = -77.7139
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 48.4601 - loglik: -4.7646e+01 - logprior: -8.1399e-01
Epoch 2/2
42/42 - 4s - loss: 35.0762 - loglik: -3.4465e+01 - logprior: -6.1082e-01
Fitted a model with MAP estimate = -34.0728
expansions: []
discards: [0]
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 39.0156 - loglik: -3.8072e+01 - logprior: -9.4382e-01
Epoch 2/2
42/42 - 4s - loss: 36.6158 - loglik: -3.6137e+01 - logprior: -4.7853e-01
Fitted a model with MAP estimate = -35.0156
expansions: []
discards: []
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 35.3241 - loglik: -3.4855e+01 - logprior: -4.6936e-01
Epoch 2/10
59/59 - 5s - loss: 34.4285 - loglik: -3.3862e+01 - logprior: -5.6698e-01
Epoch 3/10
59/59 - 5s - loss: 34.0090 - loglik: -3.3462e+01 - logprior: -5.4721e-01
Epoch 4/10
59/59 - 5s - loss: 32.5191 - loglik: -3.1983e+01 - logprior: -5.3640e-01
Epoch 5/10
59/59 - 5s - loss: 32.5569 - loglik: -3.2034e+01 - logprior: -5.2238e-01
Fitted a model with MAP estimate = -31.8674
Time for alignment: 145.7450
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 171.9645 - loglik: -1.7123e+02 - logprior: -7.3263e-01
Epoch 2/10
42/42 - 3s - loss: 81.6858 - loglik: -8.1051e+01 - logprior: -6.3511e-01
Epoch 3/10
42/42 - 3s - loss: 79.3002 - loglik: -7.8684e+01 - logprior: -6.1639e-01
Epoch 4/10
42/42 - 3s - loss: 78.3340 - loglik: -7.7728e+01 - logprior: -6.0615e-01
Epoch 5/10
42/42 - 3s - loss: 78.2324 - loglik: -7.7646e+01 - logprior: -5.8588e-01
Epoch 6/10
42/42 - 3s - loss: 77.6979 - loglik: -7.7115e+01 - logprior: -5.8322e-01
Epoch 7/10
42/42 - 3s - loss: 77.2836 - loglik: -7.6688e+01 - logprior: -5.9565e-01
Epoch 8/10
42/42 - 3s - loss: 77.1934 - loglik: -7.6596e+01 - logprior: -5.9758e-01
Epoch 9/10
42/42 - 3s - loss: 77.2292 - loglik: -7.6620e+01 - logprior: -6.0964e-01
Fitted a model with MAP estimate = -76.6528
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (44, 1), (47, 1), (48, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 48.5665 - loglik: -4.7699e+01 - logprior: -8.6730e-01
Epoch 2/2
42/42 - 4s - loss: 35.3307 - loglik: -3.4720e+01 - logprior: -6.1036e-01
Fitted a model with MAP estimate = -34.0678
expansions: []
discards: [0]
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 37.7767 - loglik: -3.6663e+01 - logprior: -1.1134e+00
Epoch 2/2
42/42 - 4s - loss: 35.7095 - loglik: -3.4890e+01 - logprior: -8.1994e-01
Fitted a model with MAP estimate = -34.3948
expansions: []
discards: []
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 34.8195 - loglik: -3.4162e+01 - logprior: -6.5698e-01
Epoch 2/10
59/59 - 5s - loss: 34.4673 - loglik: -3.3905e+01 - logprior: -5.6268e-01
Epoch 3/10
59/59 - 5s - loss: 33.8685 - loglik: -3.3320e+01 - logprior: -5.4897e-01
Epoch 4/10
59/59 - 5s - loss: 32.7224 - loglik: -3.2186e+01 - logprior: -5.3643e-01
Epoch 5/10
59/59 - 5s - loss: 32.2525 - loglik: -3.1724e+01 - logprior: -5.2818e-01
Epoch 6/10
59/59 - 5s - loss: 32.2934 - loglik: -3.1773e+01 - logprior: -5.2009e-01
Fitted a model with MAP estimate = -31.5833
Time for alignment: 160.2995
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 172.0139 - loglik: -1.7129e+02 - logprior: -7.2486e-01
Epoch 2/10
42/42 - 3s - loss: 82.5004 - loglik: -8.1868e+01 - logprior: -6.3192e-01
Epoch 3/10
42/42 - 3s - loss: 79.7636 - loglik: -7.9147e+01 - logprior: -6.1629e-01
Epoch 4/10
42/42 - 3s - loss: 78.8721 - loglik: -7.8265e+01 - logprior: -6.0732e-01
Epoch 5/10
42/42 - 3s - loss: 78.6811 - loglik: -7.8090e+01 - logprior: -5.9086e-01
Epoch 6/10
42/42 - 3s - loss: 78.4538 - loglik: -7.7875e+01 - logprior: -5.7838e-01
Epoch 7/10
42/42 - 3s - loss: 77.4092 - loglik: -7.6812e+01 - logprior: -5.9691e-01
Epoch 8/10
42/42 - 3s - loss: 77.9449 - loglik: -7.7354e+01 - logprior: -5.9101e-01
Fitted a model with MAP estimate = -77.3705
expansions: [(0, 2), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (34, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Fitting a model of length 96 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 48.0659 - loglik: -4.7246e+01 - logprior: -8.2027e-01
Epoch 2/2
42/42 - 4s - loss: 35.5843 - loglik: -3.4975e+01 - logprior: -6.0938e-01
Fitted a model with MAP estimate = -34.0662
expansions: []
discards: []
Fitting a model of length 96 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 9s - loss: 34.0201 - loglik: -3.3560e+01 - logprior: -4.6040e-01
Epoch 2/10
59/59 - 5s - loss: 33.9701 - loglik: -3.3572e+01 - logprior: -3.9829e-01
Epoch 3/10
59/59 - 5s - loss: 33.5790 - loglik: -3.3194e+01 - logprior: -3.8524e-01
Epoch 4/10
59/59 - 5s - loss: 31.7369 - loglik: -3.1359e+01 - logprior: -3.7743e-01
Epoch 5/10
59/59 - 5s - loss: 32.0602 - loglik: -3.1692e+01 - logprior: -3.6826e-01
Fitted a model with MAP estimate = -31.4484
Time for alignment: 119.9125
Computed alignments with likelihoods: ['-31.8674', '-31.5833', '-31.4484']
Best model has likelihood: -31.4484
SP score = 0.3295
Training of 3 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25de70460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25dfc0880>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 486.7480 - loglik: -4.5756e+02 - logprior: -2.9188e+01
Epoch 2/10
10/10 - 3s - loss: 431.0451 - loglik: -4.2478e+02 - logprior: -6.2697e+00
Epoch 3/10
10/10 - 3s - loss: 396.7495 - loglik: -3.9434e+02 - logprior: -2.4052e+00
Epoch 4/10
10/10 - 3s - loss: 371.4980 - loglik: -3.7008e+02 - logprior: -1.4137e+00
Epoch 5/10
10/10 - 3s - loss: 361.0732 - loglik: -3.5999e+02 - logprior: -1.0861e+00
Epoch 6/10
10/10 - 3s - loss: 359.7065 - loglik: -3.5881e+02 - logprior: -8.9953e-01
Epoch 7/10
10/10 - 3s - loss: 355.9187 - loglik: -3.5520e+02 - logprior: -7.1647e-01
Epoch 8/10
10/10 - 3s - loss: 357.9485 - loglik: -3.5736e+02 - logprior: -5.8904e-01
Fitted a model with MAP estimate = -356.3475
expansions: [(10, 1), (11, 1), (16, 5), (35, 1), (36, 3), (38, 2), (43, 1), (44, 2), (66, 2), (79, 2), (80, 3), (101, 1), (102, 2), (111, 3), (112, 1), (113, 2)]
discards: [ 0 45 46]
Fitting a model of length 153 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 390.2798 - loglik: -3.5723e+02 - logprior: -3.3049e+01
Epoch 2/2
10/10 - 4s - loss: 362.6071 - loglik: -3.4991e+02 - logprior: -1.2701e+01
Fitted a model with MAP estimate = -355.6783
expansions: [(0, 20), (16, 1), (58, 1)]
discards: [ 0 44 48 95 98]
Fitting a model of length 170 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 374.9693 - loglik: -3.4869e+02 - logprior: -2.6278e+01
Epoch 2/2
10/10 - 4s - loss: 348.0742 - loglik: -3.4253e+02 - logprior: -5.5455e+00
Fitted a model with MAP estimate = -345.1459
expansions: [(100, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Fitting a model of length 152 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 373.0573 - loglik: -3.4294e+02 - logprior: -3.0119e+01
Epoch 2/10
10/10 - 4s - loss: 349.0805 - loglik: -3.4238e+02 - logprior: -6.7043e+00
Epoch 3/10
10/10 - 4s - loss: 342.2282 - loglik: -3.4099e+02 - logprior: -1.2405e+00
Epoch 4/10
10/10 - 4s - loss: 341.1281 - loglik: -3.4180e+02 - logprior: 0.6678
Epoch 5/10
10/10 - 4s - loss: 338.7228 - loglik: -3.4022e+02 - logprior: 1.4946
Epoch 6/10
10/10 - 4s - loss: 338.1526 - loglik: -3.4005e+02 - logprior: 1.8972
Epoch 7/10
10/10 - 4s - loss: 338.1028 - loglik: -3.4029e+02 - logprior: 2.1838
Epoch 8/10
10/10 - 4s - loss: 337.4936 - loglik: -3.3997e+02 - logprior: 2.4755
Epoch 9/10
10/10 - 4s - loss: 337.7492 - loglik: -3.4046e+02 - logprior: 2.7108
Fitted a model with MAP estimate = -337.1423
Time for alignment: 94.4943
Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 485.8000 - loglik: -4.5662e+02 - logprior: -2.9183e+01
Epoch 2/10
10/10 - 3s - loss: 431.0799 - loglik: -4.2480e+02 - logprior: -6.2766e+00
Epoch 3/10
10/10 - 3s - loss: 396.7562 - loglik: -3.9429e+02 - logprior: -2.4665e+00
Epoch 4/10
10/10 - 3s - loss: 369.9318 - loglik: -3.6833e+02 - logprior: -1.6051e+00
Epoch 5/10
10/10 - 3s - loss: 361.6471 - loglik: -3.6034e+02 - logprior: -1.3081e+00
Epoch 6/10
10/10 - 3s - loss: 357.6223 - loglik: -3.5645e+02 - logprior: -1.1734e+00
Epoch 7/10
10/10 - 3s - loss: 357.0723 - loglik: -3.5605e+02 - logprior: -1.0200e+00
Epoch 8/10
10/10 - 3s - loss: 356.0567 - loglik: -3.5514e+02 - logprior: -9.1538e-01
Epoch 9/10
10/10 - 3s - loss: 355.1566 - loglik: -3.5432e+02 - logprior: -8.3852e-01
Epoch 10/10
10/10 - 3s - loss: 353.8409 - loglik: -3.5304e+02 - logprior: -8.0088e-01
Fitted a model with MAP estimate = -354.2543
expansions: [(3, 1), (5, 1), (16, 5), (17, 1), (24, 1), (36, 1), (38, 1), (43, 1), (66, 1), (67, 2), (74, 1), (79, 5), (100, 1), (102, 1), (107, 1), (110, 3), (111, 1), (113, 1)]
discards: [ 0 45 46]
Fitting a model of length 150 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 390.6872 - loglik: -3.5760e+02 - logprior: -3.3089e+01
Epoch 2/2
10/10 - 4s - loss: 361.6041 - loglik: -3.4874e+02 - logprior: -1.2862e+01
Fitted a model with MAP estimate = -356.7688
expansions: [(0, 24), (16, 1), (55, 3)]
discards: [ 0 77]
Fitting a model of length 176 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 375.6967 - loglik: -3.4917e+02 - logprior: -2.6526e+01
Epoch 2/2
10/10 - 5s - loss: 349.6536 - loglik: -3.4414e+02 - logprior: -5.5163e+00
Fitted a model with MAP estimate = -345.3123
expansions: [(80, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Fitting a model of length 154 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 373.1344 - loglik: -3.4260e+02 - logprior: -3.0538e+01
Epoch 2/10
10/10 - 4s - loss: 347.8030 - loglik: -3.4075e+02 - logprior: -7.0560e+00
Epoch 3/10
10/10 - 4s - loss: 341.7860 - loglik: -3.4049e+02 - logprior: -1.2931e+00
Epoch 4/10
10/10 - 4s - loss: 340.3289 - loglik: -3.4101e+02 - logprior: 0.6765
Epoch 5/10
10/10 - 4s - loss: 337.3392 - loglik: -3.3889e+02 - logprior: 1.5497
Epoch 6/10
10/10 - 4s - loss: 337.0026 - loglik: -3.3897e+02 - logprior: 1.9649
Epoch 7/10
10/10 - 4s - loss: 337.2505 - loglik: -3.3952e+02 - logprior: 2.2697
Fitted a model with MAP estimate = -336.5167
Time for alignment: 93.4503
Fitting a model of length 124 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 486.0839 - loglik: -4.5690e+02 - logprior: -2.9187e+01
Epoch 2/10
10/10 - 3s - loss: 431.1898 - loglik: -4.2492e+02 - logprior: -6.2688e+00
Epoch 3/10
10/10 - 3s - loss: 393.5914 - loglik: -3.9115e+02 - logprior: -2.4379e+00
Epoch 4/10
10/10 - 3s - loss: 370.6894 - loglik: -3.6918e+02 - logprior: -1.5064e+00
Epoch 5/10
10/10 - 3s - loss: 361.6342 - loglik: -3.6040e+02 - logprior: -1.2304e+00
Epoch 6/10
10/10 - 3s - loss: 357.7647 - loglik: -3.5674e+02 - logprior: -1.0269e+00
Epoch 7/10
10/10 - 3s - loss: 354.8282 - loglik: -3.5395e+02 - logprior: -8.7982e-01
Epoch 8/10
10/10 - 3s - loss: 354.8022 - loglik: -3.5405e+02 - logprior: -7.5327e-01
Epoch 9/10
10/10 - 3s - loss: 355.4443 - loglik: -3.5480e+02 - logprior: -6.4517e-01
Fitted a model with MAP estimate = -354.4151
expansions: [(3, 1), (16, 5), (17, 1), (18, 1), (36, 1), (42, 1), (67, 2), (74, 1), (79, 2), (80, 3), (81, 1), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [0]
Fitting a model of length 151 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 388.9629 - loglik: -3.5567e+02 - logprior: -3.3294e+01
Epoch 2/2
10/10 - 4s - loss: 359.5723 - loglik: -3.4682e+02 - logprior: -1.2751e+01
Fitted a model with MAP estimate = -355.2482
expansions: [(0, 24), (4, 1), (15, 1)]
discards: [ 0 55 95]
Fitting a model of length 174 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 374.5489 - loglik: -3.4828e+02 - logprior: -2.6267e+01
Epoch 2/2
10/10 - 5s - loss: 351.1406 - loglik: -3.4583e+02 - logprior: -5.3110e+00
Fitted a model with MAP estimate = -346.5484
expansions: [(79, 1), (102, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Fitting a model of length 153 on 1427 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 374.2786 - loglik: -3.4396e+02 - logprior: -3.0316e+01
Epoch 2/10
10/10 - 4s - loss: 350.0051 - loglik: -3.4313e+02 - logprior: -6.8746e+00
Epoch 3/10
10/10 - 4s - loss: 343.5441 - loglik: -3.4230e+02 - logprior: -1.2411e+00
Epoch 4/10
10/10 - 4s - loss: 341.4818 - loglik: -3.4218e+02 - logprior: 0.7027
Epoch 5/10
10/10 - 4s - loss: 340.1573 - loglik: -3.4171e+02 - logprior: 1.5521
Epoch 6/10
10/10 - 4s - loss: 339.5744 - loglik: -3.4153e+02 - logprior: 1.9586
Epoch 7/10
10/10 - 4s - loss: 338.0499 - loglik: -3.4030e+02 - logprior: 2.2459
Epoch 8/10
10/10 - 4s - loss: 338.2709 - loglik: -3.4081e+02 - logprior: 2.5384
Fitted a model with MAP estimate = -338.1768
Time for alignment: 95.3465
Computed alignments with likelihoods: ['-337.1423', '-336.5167', '-338.1768']
Best model has likelihood: -336.5167
SP score = 0.9630
Training of 3 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe28841b0d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f8ba9c70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 856.6158 - loglik: -8.5391e+02 - logprior: -2.7101e+00
Epoch 2/10
29/29 - 19s - loss: 668.6423 - loglik: -6.6687e+02 - logprior: -1.7707e+00
Epoch 3/10
29/29 - 19s - loss: 640.7798 - loglik: -6.3867e+02 - logprior: -2.1147e+00
Epoch 4/10
29/29 - 19s - loss: 635.5929 - loglik: -6.3351e+02 - logprior: -2.0874e+00
Epoch 5/10
29/29 - 19s - loss: 631.3562 - loglik: -6.2923e+02 - logprior: -2.1280e+00
Epoch 6/10
29/29 - 19s - loss: 631.5311 - loglik: -6.2939e+02 - logprior: -2.1386e+00
Fitted a model with MAP estimate = -630.5356
expansions: [(16, 1), (22, 1), (24, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (42, 1), (48, 2), (49, 1), (52, 1), (64, 1), (76, 1), (87, 1), (88, 2), (89, 2), (120, 2), (121, 2), (123, 1), (124, 1), (125, 2), (128, 1), (135, 1), (141, 1), (151, 1), (152, 1), (154, 2), (162, 1), (173, 1), (180, 1), (182, 1), (184, 2), (185, 1), (190, 1), (191, 1), (204, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (261, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Fitting a model of length 343 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 614.3053 - loglik: -6.0930e+02 - logprior: -5.0064e+00
Epoch 2/2
29/29 - 26s - loss: 590.8310 - loglik: -5.8871e+02 - logprior: -2.1205e+00
Fitted a model with MAP estimate = -587.6750
expansions: [(0, 2), (37, 1)]
discards: [  0  57 106 143 222 264 303 329 330]
Fitting a model of length 337 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 595.7812 - loglik: -5.9321e+02 - logprior: -2.5704e+00
Epoch 2/2
29/29 - 25s - loss: 587.0385 - loglik: -5.8702e+02 - logprior: -1.6826e-02
Fitted a model with MAP estimate = -586.3997
expansions: [(325, 2)]
discards: [0]
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 595.5364 - loglik: -5.9139e+02 - logprior: -4.1428e+00
Epoch 2/10
29/29 - 25s - loss: 589.0711 - loglik: -5.8848e+02 - logprior: -5.8775e-01
Epoch 3/10
29/29 - 25s - loss: 584.1870 - loglik: -5.8472e+02 - logprior: 0.5319
Epoch 4/10
29/29 - 25s - loss: 583.3429 - loglik: -5.8364e+02 - logprior: 0.2974
Epoch 5/10
29/29 - 25s - loss: 582.6926 - loglik: -5.8357e+02 - logprior: 0.8735
Epoch 6/10
29/29 - 25s - loss: 582.5839 - loglik: -5.8330e+02 - logprior: 0.7188
Epoch 7/10
29/29 - 25s - loss: 581.6356 - loglik: -5.8287e+02 - logprior: 1.2300
Epoch 8/10
29/29 - 25s - loss: 582.6464 - loglik: -5.8380e+02 - logprior: 1.1522
Fitted a model with MAP estimate = -581.4876
Time for alignment: 532.6704
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 856.2573 - loglik: -8.5355e+02 - logprior: -2.7078e+00
Epoch 2/10
29/29 - 19s - loss: 673.7885 - loglik: -6.7197e+02 - logprior: -1.8170e+00
Epoch 3/10
29/29 - 19s - loss: 648.6035 - loglik: -6.4649e+02 - logprior: -2.1145e+00
Epoch 4/10
29/29 - 19s - loss: 639.2220 - loglik: -6.3719e+02 - logprior: -2.0296e+00
Epoch 5/10
29/29 - 19s - loss: 637.8463 - loglik: -6.3583e+02 - logprior: -2.0179e+00
Epoch 6/10
29/29 - 19s - loss: 637.7708 - loglik: -6.3577e+02 - logprior: -1.9962e+00
Epoch 7/10
29/29 - 19s - loss: 636.8868 - loglik: -6.3488e+02 - logprior: -2.0117e+00
Epoch 8/10
29/29 - 19s - loss: 634.4042 - loglik: -6.3238e+02 - logprior: -2.0270e+00
Epoch 9/10
29/29 - 19s - loss: 636.7680 - loglik: -6.3473e+02 - logprior: -2.0367e+00
Fitted a model with MAP estimate = -634.9963
expansions: [(15, 1), (17, 1), (26, 1), (28, 1), (29, 1), (30, 1), (36, 2), (38, 1), (48, 2), (49, 1), (50, 1), (73, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 2), (122, 2), (123, 1), (124, 1), (125, 1), (128, 1), (142, 1), (152, 1), (155, 2), (156, 1), (163, 1), (172, 1), (173, 1), (183, 1), (185, 1), (186, 1), (187, 1), (192, 1), (206, 1), (216, 1), (217, 2), (218, 2), (219, 1), (234, 1), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Fitting a model of length 341 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 618.2192 - loglik: -6.1338e+02 - logprior: -4.8385e+00
Epoch 2/2
29/29 - 26s - loss: 597.6981 - loglik: -5.9574e+02 - logprior: -1.9593e+00
Fitted a model with MAP estimate = -594.1884
expansions: [(0, 2), (36, 1)]
discards: [  0  41  61 107 108 182 260 299 326]
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 600.3351 - loglik: -5.9765e+02 - logprior: -2.6851e+00
Epoch 2/2
29/29 - 25s - loss: 595.3383 - loglik: -5.9531e+02 - logprior: -3.0445e-02
Fitted a model with MAP estimate = -592.7079
expansions: []
discards: [  0 324]
Fitting a model of length 333 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 602.1028 - loglik: -5.9802e+02 - logprior: -4.0804e+00
Epoch 2/10
29/29 - 25s - loss: 595.7063 - loglik: -5.9550e+02 - logprior: -2.0638e-01
Epoch 3/10
29/29 - 25s - loss: 592.5366 - loglik: -5.9316e+02 - logprior: 0.6231
Epoch 4/10
29/29 - 25s - loss: 591.9081 - loglik: -5.9277e+02 - logprior: 0.8611
Epoch 5/10
29/29 - 25s - loss: 590.4741 - loglik: -5.9131e+02 - logprior: 0.8395
Epoch 6/10
29/29 - 25s - loss: 590.4003 - loglik: -5.9166e+02 - logprior: 1.2630
Epoch 7/10
29/29 - 25s - loss: 588.6851 - loglik: -5.8984e+02 - logprior: 1.1581
Epoch 8/10
29/29 - 25s - loss: 590.3555 - loglik: -5.9198e+02 - logprior: 1.6292
Fitted a model with MAP estimate = -588.8339
Time for alignment: 582.9178
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 23s - loss: 856.2141 - loglik: -8.5351e+02 - logprior: -2.7077e+00
Epoch 2/10
29/29 - 19s - loss: 675.2807 - loglik: -6.7351e+02 - logprior: -1.7671e+00
Epoch 3/10
29/29 - 19s - loss: 643.4534 - loglik: -6.4138e+02 - logprior: -2.0738e+00
Epoch 4/10
29/29 - 19s - loss: 637.9819 - loglik: -6.3592e+02 - logprior: -2.0577e+00
Epoch 5/10
29/29 - 19s - loss: 636.5812 - loglik: -6.3456e+02 - logprior: -2.0169e+00
Epoch 6/10
29/29 - 19s - loss: 634.7686 - loglik: -6.3275e+02 - logprior: -2.0161e+00
Epoch 7/10
29/29 - 19s - loss: 633.8887 - loglik: -6.3188e+02 - logprior: -2.0095e+00
Epoch 8/10
29/29 - 19s - loss: 635.3495 - loglik: -6.3334e+02 - logprior: -2.0083e+00
Fitted a model with MAP estimate = -634.2503
expansions: [(16, 1), (17, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 2), (48, 1), (49, 1), (50, 2), (64, 1), (77, 1), (78, 1), (89, 2), (90, 3), (121, 2), (123, 1), (125, 2), (128, 1), (141, 1), (144, 1), (151, 1), (153, 1), (154, 1), (155, 1), (162, 1), (173, 1), (182, 1), (184, 2), (185, 1), (190, 1), (191, 1), (205, 1), (217, 2), (218, 2), (219, 1), (232, 1), (248, 1), (249, 1), (251, 1), (255, 1), (257, 1), (259, 1), (260, 1), (262, 1), (263, 2), (264, 1), (267, 2)]
discards: [0]
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 619.3879 - loglik: -6.1466e+02 - logprior: -4.7314e+00
Epoch 2/2
29/29 - 25s - loss: 594.9705 - loglik: -5.9276e+02 - logprior: -2.2141e+00
Fitted a model with MAP estimate = -591.5055
expansions: [(0, 2)]
discards: [  0  38  61 105 108 148 261]
Fitting a model of length 334 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 597.9647 - loglik: -5.9517e+02 - logprior: -2.7914e+00
Epoch 2/2
29/29 - 25s - loss: 592.4717 - loglik: -5.9239e+02 - logprior: -8.4399e-02
Fitted a model with MAP estimate = -591.1741
expansions: [(323, 1), (324, 2)]
discards: [0]
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 600.6459 - loglik: -5.9639e+02 - logprior: -4.2599e+00
Epoch 2/10
29/29 - 25s - loss: 594.2687 - loglik: -5.9373e+02 - logprior: -5.3690e-01
Epoch 3/10
29/29 - 25s - loss: 590.0007 - loglik: -5.9067e+02 - logprior: 0.6693
Epoch 4/10
29/29 - 25s - loss: 590.3102 - loglik: -5.9053e+02 - logprior: 0.2243
Fitted a model with MAP estimate = -589.8548
Time for alignment: 465.6446
Computed alignments with likelihoods: ['-581.4876', '-588.8339', '-589.8548']
Best model has likelihood: -581.4876
SP score = 0.8559
Training of 3 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe24b140fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f9ca8580>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.0758 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6613 - loglik: -1.0669e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9666 - loglik: -8.6695e+01 - logprior: -6.2271e+01
Epoch 4/10
10/10 - 0s - loss: 108.2721 - loglik: -7.3521e+01 - logprior: -3.4752e+01
Epoch 5/10
10/10 - 0s - loss: 89.8147 - loglik: -6.9750e+01 - logprior: -2.0065e+01
Epoch 6/10
10/10 - 0s - loss: 79.9434 - loglik: -6.9569e+01 - logprior: -1.0374e+01
Epoch 7/10
10/10 - 0s - loss: 74.1510 - loglik: -7.0105e+01 - logprior: -4.0464e+00
Epoch 8/10
10/10 - 0s - loss: 70.6278 - loglik: -7.0406e+01 - logprior: -2.2161e-01
Epoch 9/10
10/10 - 0s - loss: 68.2908 - loglik: -7.0583e+01 - logprior: 2.2924
Epoch 10/10
10/10 - 0s - loss: 66.6239 - loglik: -7.0783e+01 - logprior: 4.1589
Fitted a model with MAP estimate = -65.8751
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 714.2726 - loglik: -6.2257e+01 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 258.0609 - loglik: -5.4751e+01 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -171.5330
expansions: []
discards: [23]
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.4007 - loglik: -5.0028e+01 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 170.8342 - loglik: -4.9724e+01 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -120.9132
expansions: []
discards: []
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 476.7527 - loglik: -4.9038e+01 - logprior: -4.2772e+02
Epoch 2/10
10/10 - 0s - loss: 163.3216 - loglik: -4.9667e+01 - logprior: -1.1365e+02
Epoch 3/10
10/10 - 0s - loss: 97.3912 - loglik: -5.0392e+01 - logprior: -4.6999e+01
Epoch 4/10
10/10 - 0s - loss: 69.4057 - loglik: -5.0967e+01 - logprior: -1.8439e+01
Epoch 5/10
10/10 - 0s - loss: 54.0201 - loglik: -5.1508e+01 - logprior: -2.5119e+00
Epoch 6/10
10/10 - 0s - loss: 45.0232 - loglik: -5.1959e+01 - logprior: 6.9359
Epoch 7/10
10/10 - 0s - loss: 39.4114 - loglik: -5.2301e+01 - logprior: 12.8898
Epoch 8/10
10/10 - 0s - loss: 35.5871 - loglik: -5.2571e+01 - logprior: 16.9842
Epoch 9/10
10/10 - 0s - loss: 32.7354 - loglik: -5.2788e+01 - logprior: 20.0526
Epoch 10/10
10/10 - 0s - loss: 30.4488 - loglik: -5.2969e+01 - logprior: 22.5198
Fitted a model with MAP estimate = -29.3365
Time for alignment: 24.5297
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 617.0758 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6613 - loglik: -1.0669e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9666 - loglik: -8.6695e+01 - logprior: -6.2271e+01
Epoch 4/10
10/10 - 0s - loss: 108.2721 - loglik: -7.3521e+01 - logprior: -3.4752e+01
Epoch 5/10
10/10 - 0s - loss: 89.8147 - loglik: -6.9750e+01 - logprior: -2.0065e+01
Epoch 6/10
10/10 - 0s - loss: 79.9434 - loglik: -6.9569e+01 - logprior: -1.0374e+01
Epoch 7/10
10/10 - 0s - loss: 74.1510 - loglik: -7.0105e+01 - logprior: -4.0464e+00
Epoch 8/10
10/10 - 0s - loss: 70.6278 - loglik: -7.0406e+01 - logprior: -2.2161e-01
Epoch 9/10
10/10 - 0s - loss: 68.2908 - loglik: -7.0583e+01 - logprior: 2.2924
Epoch 10/10
10/10 - 0s - loss: 66.6239 - loglik: -7.0783e+01 - logprior: 4.1589
Fitted a model with MAP estimate = -65.8751
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 714.2726 - loglik: -6.2257e+01 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 258.0609 - loglik: -5.4751e+01 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -171.5330
expansions: []
discards: [23]
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.4007 - loglik: -5.0028e+01 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 170.8342 - loglik: -4.9724e+01 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -120.9132
expansions: []
discards: []
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 476.7527 - loglik: -4.9038e+01 - logprior: -4.2772e+02
Epoch 2/10
10/10 - 0s - loss: 163.3216 - loglik: -4.9667e+01 - logprior: -1.1365e+02
Epoch 3/10
10/10 - 0s - loss: 97.3912 - loglik: -5.0392e+01 - logprior: -4.6999e+01
Epoch 4/10
10/10 - 0s - loss: 69.4057 - loglik: -5.0967e+01 - logprior: -1.8439e+01
Epoch 5/10
10/10 - 0s - loss: 54.0201 - loglik: -5.1508e+01 - logprior: -2.5119e+00
Epoch 6/10
10/10 - 0s - loss: 45.0232 - loglik: -5.1959e+01 - logprior: 6.9359
Epoch 7/10
10/10 - 0s - loss: 39.4114 - loglik: -5.2301e+01 - logprior: 12.8898
Epoch 8/10
10/10 - 0s - loss: 35.5871 - loglik: -5.2571e+01 - logprior: 16.9842
Epoch 9/10
10/10 - 0s - loss: 32.7354 - loglik: -5.2788e+01 - logprior: 20.0526
Epoch 10/10
10/10 - 0s - loss: 30.4488 - loglik: -5.2969e+01 - logprior: 22.5198
Fitted a model with MAP estimate = -29.3365
Time for alignment: 22.9495
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 617.0758 - loglik: -1.2832e+02 - logprior: -4.8876e+02
Epoch 2/10
10/10 - 0s - loss: 241.6613 - loglik: -1.0669e+02 - logprior: -1.3497e+02
Epoch 3/10
10/10 - 0s - loss: 148.9666 - loglik: -8.6695e+01 - logprior: -6.2271e+01
Epoch 4/10
10/10 - 0s - loss: 108.2721 - loglik: -7.3521e+01 - logprior: -3.4752e+01
Epoch 5/10
10/10 - 0s - loss: 89.8147 - loglik: -6.9750e+01 - logprior: -2.0065e+01
Epoch 6/10
10/10 - 0s - loss: 79.9434 - loglik: -6.9569e+01 - logprior: -1.0374e+01
Epoch 7/10
10/10 - 0s - loss: 74.1510 - loglik: -7.0105e+01 - logprior: -4.0464e+00
Epoch 8/10
10/10 - 0s - loss: 70.6278 - loglik: -7.0406e+01 - logprior: -2.2161e-01
Epoch 9/10
10/10 - 0s - loss: 68.2908 - loglik: -7.0583e+01 - logprior: 2.2924
Epoch 10/10
10/10 - 0s - loss: 66.6239 - loglik: -7.0783e+01 - logprior: 4.1589
Fitted a model with MAP estimate = -65.8751
expansions: [(0, 3), (7, 1), (17, 3), (26, 3)]
discards: []
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 714.2726 - loglik: -6.2257e+01 - logprior: -6.5202e+02
Epoch 2/2
10/10 - 0s - loss: 258.0609 - loglik: -5.4751e+01 - logprior: -2.0331e+02
Fitted a model with MAP estimate = -171.5330
expansions: []
discards: [23]
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 505.4007 - loglik: -5.0028e+01 - logprior: -4.5537e+02
Epoch 2/2
10/10 - 0s - loss: 170.8342 - loglik: -4.9724e+01 - logprior: -1.2111e+02
Fitted a model with MAP estimate = -120.9132
expansions: []
discards: []
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 476.7527 - loglik: -4.9038e+01 - logprior: -4.2772e+02
Epoch 2/10
10/10 - 0s - loss: 163.3216 - loglik: -4.9667e+01 - logprior: -1.1365e+02
Epoch 3/10
10/10 - 0s - loss: 97.3912 - loglik: -5.0392e+01 - logprior: -4.6999e+01
Epoch 4/10
10/10 - 0s - loss: 69.4057 - loglik: -5.0967e+01 - logprior: -1.8439e+01
Epoch 5/10
10/10 - 0s - loss: 54.0201 - loglik: -5.1508e+01 - logprior: -2.5119e+00
Epoch 6/10
10/10 - 0s - loss: 45.0232 - loglik: -5.1959e+01 - logprior: 6.9359
Epoch 7/10
10/10 - 0s - loss: 39.4114 - loglik: -5.2301e+01 - logprior: 12.8898
Epoch 8/10
10/10 - 0s - loss: 35.5871 - loglik: -5.2571e+01 - logprior: 16.9842
Epoch 9/10
10/10 - 0s - loss: 32.7354 - loglik: -5.2788e+01 - logprior: 20.0526
Epoch 10/10
10/10 - 0s - loss: 30.4488 - loglik: -5.2969e+01 - logprior: 22.5198
Fitted a model with MAP estimate = -29.3365
Time for alignment: 23.7372
Computed alignments with likelihoods: ['-29.3365', '-29.3365', '-29.3365']
Best model has likelihood: -29.3365
SP score = 0.7083
Training of 3 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2885093a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2c3e72430>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.0876 - loglik: -4.7290e+02 - logprior: -4.1832e+00
Epoch 2/10
16/16 - 4s - loss: 440.8225 - loglik: -4.3982e+02 - logprior: -1.0044e+00
Epoch 3/10
16/16 - 4s - loss: 417.5515 - loglik: -4.1627e+02 - logprior: -1.2804e+00
Epoch 4/10
16/16 - 4s - loss: 409.5743 - loglik: -4.0829e+02 - logprior: -1.2870e+00
Epoch 5/10
16/16 - 4s - loss: 405.8886 - loglik: -4.0465e+02 - logprior: -1.2370e+00
Epoch 6/10
16/16 - 4s - loss: 404.0560 - loglik: -4.0276e+02 - logprior: -1.2920e+00
Epoch 7/10
16/16 - 4s - loss: 402.0154 - loglik: -4.0072e+02 - logprior: -1.2923e+00
Epoch 8/10
16/16 - 4s - loss: 402.8412 - loglik: -4.0152e+02 - logprior: -1.3198e+00
Fitted a model with MAP estimate = -401.8767
expansions: [(13, 1), (14, 1), (23, 2), (28, 3), (29, 1), (42, 1), (44, 1), (49, 1), (50, 2), (51, 1), (53, 1), (56, 2), (72, 2), (73, 2), (74, 2), (75, 2), (94, 3), (95, 3), (99, 1), (105, 1), (116, 1), (117, 1), (119, 1), (122, 1), (125, 1), (129, 1), (139, 4)]
discards: [0]
Fitting a model of length 181 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 414.0353 - loglik: -4.0875e+02 - logprior: -5.2856e+00
Epoch 2/2
16/16 - 6s - loss: 400.4574 - loglik: -3.9803e+02 - logprior: -2.4272e+00
Fitted a model with MAP estimate = -398.3969
expansions: [(0, 1)]
discards: [  0  32  33  61  71  89  91 121 177 178 179 180]
Fitting a model of length 170 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 403.7272 - loglik: -3.9986e+02 - logprior: -3.8704e+00
Epoch 2/2
16/16 - 5s - loss: 397.5338 - loglik: -3.9638e+02 - logprior: -1.1558e+00
Fitted a model with MAP estimate = -396.4186
expansions: [(32, 1), (170, 3)]
discards: [113 114]
Fitting a model of length 172 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 402.1502 - loglik: -3.9822e+02 - logprior: -3.9279e+00
Epoch 2/10
16/16 - 5s - loss: 397.1237 - loglik: -3.9600e+02 - logprior: -1.1245e+00
Epoch 3/10
16/16 - 5s - loss: 395.1532 - loglik: -3.9440e+02 - logprior: -7.4838e-01
Epoch 4/10
16/16 - 5s - loss: 394.2426 - loglik: -3.9363e+02 - logprior: -6.1157e-01
Epoch 5/10
16/16 - 5s - loss: 392.6958 - loglik: -3.9213e+02 - logprior: -5.6527e-01
Epoch 6/10
16/16 - 5s - loss: 390.9002 - loglik: -3.9036e+02 - logprior: -5.4204e-01
Epoch 7/10
16/16 - 5s - loss: 390.2524 - loglik: -3.8972e+02 - logprior: -5.3022e-01
Epoch 8/10
16/16 - 5s - loss: 388.5325 - loglik: -3.8801e+02 - logprior: -5.2203e-01
Epoch 9/10
16/16 - 5s - loss: 389.8209 - loglik: -3.8932e+02 - logprior: -5.0457e-01
Fitted a model with MAP estimate = -388.6603
Time for alignment: 144.2671
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 477.2503 - loglik: -4.7307e+02 - logprior: -4.1844e+00
Epoch 2/10
16/16 - 4s - loss: 441.3373 - loglik: -4.4033e+02 - logprior: -1.0047e+00
Epoch 3/10
16/16 - 4s - loss: 417.4807 - loglik: -4.1617e+02 - logprior: -1.3111e+00
Epoch 4/10
16/16 - 4s - loss: 408.3402 - loglik: -4.0694e+02 - logprior: -1.4038e+00
Epoch 5/10
16/16 - 4s - loss: 404.3703 - loglik: -4.0308e+02 - logprior: -1.2908e+00
Epoch 6/10
16/16 - 4s - loss: 403.3577 - loglik: -4.0203e+02 - logprior: -1.3253e+00
Epoch 7/10
16/16 - 4s - loss: 402.4855 - loglik: -4.0117e+02 - logprior: -1.3150e+00
Epoch 8/10
16/16 - 4s - loss: 402.1826 - loglik: -4.0084e+02 - logprior: -1.3388e+00
Epoch 9/10
16/16 - 4s - loss: 401.9846 - loglik: -4.0066e+02 - logprior: -1.3263e+00
Epoch 10/10
16/16 - 4s - loss: 401.3949 - loglik: -4.0006e+02 - logprior: -1.3396e+00
Fitted a model with MAP estimate = -401.4432
expansions: [(13, 1), (14, 1), (20, 1), (23, 1), (28, 3), (29, 1), (42, 1), (44, 1), (47, 1), (48, 1), (49, 2), (56, 1), (57, 2), (71, 1), (72, 2), (73, 1), (80, 1), (94, 3), (95, 1), (96, 1), (105, 1), (116, 1), (117, 2), (122, 2), (125, 1), (129, 1), (139, 3)]
discards: [0]
Fitting a model of length 176 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 415.9822 - loglik: -4.1066e+02 - logprior: -5.3250e+00
Epoch 2/2
16/16 - 5s - loss: 401.1731 - loglik: -3.9874e+02 - logprior: -2.4337e+00
Fitted a model with MAP estimate = -399.6512
expansions: [(0, 1)]
discards: [  0  32  33  72 145 173 174 175]
Fitting a model of length 169 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 9s - loss: 404.2014 - loglik: -4.0034e+02 - logprior: -3.8657e+00
Epoch 2/2
16/16 - 5s - loss: 399.1077 - loglik: -3.9799e+02 - logprior: -1.1183e+00
Fitted a model with MAP estimate = -397.1586
expansions: [(169, 3)]
discards: [55]
Fitting a model of length 171 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 402.6835 - loglik: -3.9876e+02 - logprior: -3.9194e+00
Epoch 2/10
16/16 - 5s - loss: 398.2564 - loglik: -3.9715e+02 - logprior: -1.1090e+00
Epoch 3/10
16/16 - 5s - loss: 395.8422 - loglik: -3.9511e+02 - logprior: -7.3117e-01
Epoch 4/10
16/16 - 5s - loss: 394.8615 - loglik: -3.9425e+02 - logprior: -6.1346e-01
Epoch 5/10
16/16 - 5s - loss: 392.1033 - loglik: -3.9157e+02 - logprior: -5.3302e-01
Epoch 6/10
16/16 - 5s - loss: 392.4659 - loglik: -3.9194e+02 - logprior: -5.2981e-01
Fitted a model with MAP estimate = -390.7205
Time for alignment: 135.6385
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 477.4233 - loglik: -4.7324e+02 - logprior: -4.1854e+00
Epoch 2/10
16/16 - 4s - loss: 440.6021 - loglik: -4.3957e+02 - logprior: -1.0278e+00
Epoch 3/10
16/16 - 4s - loss: 415.4569 - loglik: -4.1411e+02 - logprior: -1.3473e+00
Epoch 4/10
16/16 - 4s - loss: 408.2929 - loglik: -4.0689e+02 - logprior: -1.3984e+00
Epoch 5/10
16/16 - 4s - loss: 404.0311 - loglik: -4.0270e+02 - logprior: -1.3360e+00
Epoch 6/10
16/16 - 4s - loss: 403.1987 - loglik: -4.0183e+02 - logprior: -1.3726e+00
Epoch 7/10
16/16 - 4s - loss: 402.8004 - loglik: -4.0145e+02 - logprior: -1.3538e+00
Epoch 8/10
16/16 - 4s - loss: 401.5072 - loglik: -4.0014e+02 - logprior: -1.3676e+00
Epoch 9/10
16/16 - 4s - loss: 401.7266 - loglik: -4.0036e+02 - logprior: -1.3685e+00
Fitted a model with MAP estimate = -401.0995
expansions: [(15, 1), (18, 1), (23, 3), (28, 2), (30, 1), (43, 1), (45, 1), (49, 2), (52, 1), (54, 1), (57, 1), (58, 1), (70, 1), (72, 1), (73, 2), (74, 2), (75, 1), (94, 3), (95, 3), (99, 1), (105, 1), (116, 1), (117, 1), (119, 1), (122, 1), (125, 1), (139, 4)]
discards: [0]
Fitting a model of length 178 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 412.9559 - loglik: -4.0764e+02 - logprior: -5.3167e+00
Epoch 2/2
16/16 - 6s - loss: 400.4726 - loglik: -3.9806e+02 - logprior: -2.4175e+00
Fitted a model with MAP estimate = -397.9984
expansions: [(0, 1), (178, 2)]
discards: [  0  25  33  34  73 119 170 174 176 177]
Fitting a model of length 171 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 403.9315 - loglik: -4.0006e+02 - logprior: -3.8670e+00
Epoch 2/2
16/16 - 5s - loss: 398.2983 - loglik: -3.9707e+02 - logprior: -1.2325e+00
Fitted a model with MAP estimate = -396.9114
expansions: [(171, 2)]
discards: [114 168 169]
Fitting a model of length 170 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 402.9836 - loglik: -3.9901e+02 - logprior: -3.9751e+00
Epoch 2/10
16/16 - 5s - loss: 396.9543 - loglik: -3.9582e+02 - logprior: -1.1390e+00
Epoch 3/10
16/16 - 5s - loss: 395.5215 - loglik: -3.9477e+02 - logprior: -7.4750e-01
Epoch 4/10
16/16 - 5s - loss: 396.3434 - loglik: -3.9572e+02 - logprior: -6.1996e-01
Fitted a model with MAP estimate = -393.7111
Time for alignment: 121.8820
Computed alignments with likelihoods: ['-388.6603', '-390.7205', '-393.7111']
Best model has likelihood: -388.6603
SP score = 0.8202
Training of 3 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2884e6760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f9d660a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 290.4117 - loglik: -2.7548e+02 - logprior: -1.4928e+01
Epoch 2/10
10/10 - 1s - loss: 254.3255 - loglik: -2.5043e+02 - logprior: -3.8940e+00
Epoch 3/10
10/10 - 1s - loss: 227.3166 - loglik: -2.2507e+02 - logprior: -2.2504e+00
Epoch 4/10
10/10 - 1s - loss: 210.5328 - loglik: -2.0858e+02 - logprior: -1.9497e+00
Epoch 5/10
10/10 - 1s - loss: 204.3618 - loglik: -2.0254e+02 - logprior: -1.8224e+00
Epoch 6/10
10/10 - 1s - loss: 203.4582 - loglik: -2.0177e+02 - logprior: -1.6847e+00
Epoch 7/10
10/10 - 1s - loss: 201.3958 - loglik: -1.9977e+02 - logprior: -1.6229e+00
Epoch 8/10
10/10 - 1s - loss: 200.8371 - loglik: -1.9921e+02 - logprior: -1.6290e+00
Epoch 9/10
10/10 - 1s - loss: 200.2903 - loglik: -1.9862e+02 - logprior: -1.6675e+00
Epoch 10/10
10/10 - 1s - loss: 201.6845 - loglik: -1.9998e+02 - logprior: -1.6998e+00
Fitted a model with MAP estimate = -199.8669
expansions: [(21, 1), (31, 1), (33, 1), (39, 1), (44, 1), (46, 1), (54, 1), (56, 1)]
discards: [0]
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 223.3884 - loglik: -2.0617e+02 - logprior: -1.7222e+01
Epoch 2/2
10/10 - 1s - loss: 206.4596 - loglik: -1.9912e+02 - logprior: -7.3379e+00
Fitted a model with MAP estimate = -203.2182
expansions: [(0, 11)]
discards: [0]
Fitting a model of length 85 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 211.2306 - loglik: -1.9683e+02 - logprior: -1.4397e+01
Epoch 2/2
10/10 - 1s - loss: 195.0961 - loglik: -1.9071e+02 - logprior: -4.3867e+00
Fitted a model with MAP estimate = -191.0957
expansions: [(59, 1)]
discards: [0 1 2 3 4 5 6 7 8 9]
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 210.5070 - loglik: -1.9474e+02 - logprior: -1.5763e+01
Epoch 2/10
10/10 - 1s - loss: 197.5308 - loglik: -1.9302e+02 - logprior: -4.5156e+00
Epoch 3/10
10/10 - 1s - loss: 196.2393 - loglik: -1.9410e+02 - logprior: -2.1393e+00
Epoch 4/10
10/10 - 1s - loss: 193.2137 - loglik: -1.9179e+02 - logprior: -1.4192e+00
Epoch 5/10
10/10 - 1s - loss: 192.4658 - loglik: -1.9136e+02 - logprior: -1.1037e+00
Epoch 6/10
10/10 - 1s - loss: 190.3385 - loglik: -1.8951e+02 - logprior: -8.2459e-01
Epoch 7/10
10/10 - 1s - loss: 189.7111 - loglik: -1.8903e+02 - logprior: -6.8259e-01
Epoch 8/10
10/10 - 1s - loss: 190.4800 - loglik: -1.8981e+02 - logprior: -6.7413e-01
Fitted a model with MAP estimate = -189.9273
Time for alignment: 44.4527
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 290.9956 - loglik: -2.7607e+02 - logprior: -1.4930e+01
Epoch 2/10
10/10 - 1s - loss: 255.0920 - loglik: -2.5121e+02 - logprior: -3.8816e+00
Epoch 3/10
10/10 - 1s - loss: 227.7616 - loglik: -2.2549e+02 - logprior: -2.2725e+00
Epoch 4/10
10/10 - 1s - loss: 209.9164 - loglik: -2.0785e+02 - logprior: -2.0652e+00
Epoch 5/10
10/10 - 1s - loss: 203.4122 - loglik: -2.0138e+02 - logprior: -2.0320e+00
Epoch 6/10
10/10 - 1s - loss: 200.7755 - loglik: -1.9869e+02 - logprior: -2.0868e+00
Epoch 7/10
10/10 - 1s - loss: 200.5040 - loglik: -1.9855e+02 - logprior: -1.9550e+00
Epoch 8/10
10/10 - 1s - loss: 199.6004 - loglik: -1.9775e+02 - logprior: -1.8466e+00
Epoch 9/10
10/10 - 1s - loss: 199.1436 - loglik: -1.9730e+02 - logprior: -1.8468e+00
Epoch 10/10
10/10 - 1s - loss: 197.7051 - loglik: -1.9584e+02 - logprior: -1.8676e+00
Fitted a model with MAP estimate = -198.5643
expansions: [(10, 2), (20, 1), (26, 1), (31, 1), (36, 1), (46, 2), (47, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Fitting a model of length 79 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.1129 - loglik: -2.0480e+02 - logprior: -1.7316e+01
Epoch 2/2
10/10 - 1s - loss: 200.8184 - loglik: -1.9332e+02 - logprior: -7.4995e+00
Fitted a model with MAP estimate = -198.4065
expansions: [(0, 8), (51, 1)]
discards: [0 9]
Fitting a model of length 86 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 207.2786 - loglik: -1.9302e+02 - logprior: -1.4254e+01
Epoch 2/2
10/10 - 1s - loss: 189.2523 - loglik: -1.8497e+02 - logprior: -4.2869e+00
Fitted a model with MAP estimate = -186.8503
expansions: [(44, 1)]
discards: [0 1 2 3 4 5 6]
Fitting a model of length 80 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 207.8393 - loglik: -1.9185e+02 - logprior: -1.5985e+01
Epoch 2/10
10/10 - 1s - loss: 193.1073 - loglik: -1.8834e+02 - logprior: -4.7707e+00
Epoch 3/10
10/10 - 1s - loss: 189.5549 - loglik: -1.8726e+02 - logprior: -2.2979e+00
Epoch 4/10
10/10 - 1s - loss: 189.5988 - loglik: -1.8802e+02 - logprior: -1.5799e+00
Fitted a model with MAP estimate = -188.0122
Time for alignment: 39.7577
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 291.8435 - loglik: -2.7691e+02 - logprior: -1.4931e+01
Epoch 2/10
10/10 - 1s - loss: 251.7612 - loglik: -2.4786e+02 - logprior: -3.8991e+00
Epoch 3/10
10/10 - 1s - loss: 228.1436 - loglik: -2.2586e+02 - logprior: -2.2795e+00
Epoch 4/10
10/10 - 1s - loss: 213.9059 - loglik: -2.1192e+02 - logprior: -1.9860e+00
Epoch 5/10
10/10 - 1s - loss: 206.7082 - loglik: -2.0485e+02 - logprior: -1.8540e+00
Epoch 6/10
10/10 - 1s - loss: 204.7557 - loglik: -2.0299e+02 - logprior: -1.7637e+00
Epoch 7/10
10/10 - 1s - loss: 204.6279 - loglik: -2.0287e+02 - logprior: -1.7542e+00
Epoch 8/10
10/10 - 1s - loss: 203.4363 - loglik: -2.0170e+02 - logprior: -1.7345e+00
Epoch 9/10
10/10 - 1s - loss: 200.9247 - loglik: -1.9915e+02 - logprior: -1.7766e+00
Epoch 10/10
10/10 - 1s - loss: 201.2527 - loglik: -1.9945e+02 - logprior: -1.8025e+00
Fitted a model with MAP estimate = -201.0398
expansions: [(21, 1), (26, 1), (27, 1), (34, 1), (37, 1), (39, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 223.5765 - loglik: -2.0626e+02 - logprior: -1.7319e+01
Epoch 2/2
10/10 - 1s - loss: 206.4177 - loglik: -1.9904e+02 - logprior: -7.3775e+00
Fitted a model with MAP estimate = -202.1089
expansions: [(0, 20), (3, 1)]
discards: [0]
Fitting a model of length 96 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 210.8185 - loglik: -1.9614e+02 - logprior: -1.4683e+01
Epoch 2/2
10/10 - 1s - loss: 192.0581 - loglik: -1.8767e+02 - logprior: -4.3917e+00
Fitted a model with MAP estimate = -186.4500
expansions: [(29, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Fitting a model of length 78 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 210.5345 - loglik: -1.9486e+02 - logprior: -1.5673e+01
Epoch 2/10
10/10 - 1s - loss: 196.0103 - loglik: -1.9140e+02 - logprior: -4.6152e+00
Epoch 3/10
10/10 - 1s - loss: 192.9343 - loglik: -1.9068e+02 - logprior: -2.2555e+00
Epoch 4/10
10/10 - 1s - loss: 190.9151 - loglik: -1.8937e+02 - logprior: -1.5409e+00
Epoch 5/10
10/10 - 1s - loss: 190.3251 - loglik: -1.8901e+02 - logprior: -1.3139e+00
Epoch 6/10
10/10 - 1s - loss: 191.0024 - loglik: -1.8986e+02 - logprior: -1.1461e+00
Fitted a model with MAP estimate = -189.0203
Time for alignment: 42.0522
Computed alignments with likelihoods: ['-189.9273', '-186.8503', '-186.4500']
Best model has likelihood: -186.4500
SP score = 0.3710
Training of 3 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb766b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f8b4a250>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 613.7137 - loglik: -5.6090e+02 - logprior: -5.2814e+01
Epoch 2/10
10/10 - 3s - loss: 515.6090 - loglik: -5.0676e+02 - logprior: -8.8496e+00
Epoch 3/10
10/10 - 3s - loss: 451.5602 - loglik: -4.4960e+02 - logprior: -1.9570e+00
Epoch 4/10
10/10 - 3s - loss: 419.0632 - loglik: -4.1917e+02 - logprior: 0.1027
Epoch 5/10
10/10 - 3s - loss: 408.3644 - loglik: -4.0967e+02 - logprior: 1.3080
Epoch 6/10
10/10 - 3s - loss: 404.1749 - loglik: -4.0620e+02 - logprior: 2.0258
Epoch 7/10
10/10 - 3s - loss: 402.6283 - loglik: -4.0490e+02 - logprior: 2.2719
Epoch 8/10
10/10 - 3s - loss: 401.3559 - loglik: -4.0378e+02 - logprior: 2.4203
Epoch 9/10
10/10 - 3s - loss: 400.1620 - loglik: -4.0291e+02 - logprior: 2.7438
Epoch 10/10
10/10 - 3s - loss: 400.1712 - loglik: -4.0317e+02 - logprior: 2.9939
Fitted a model with MAP estimate = -399.3545
expansions: [(9, 2), (19, 2), (22, 3), (23, 1), (25, 1), (29, 3), (44, 1), (45, 2), (46, 1), (52, 1), (75, 1), (81, 1), (82, 1), (94, 1), (104, 2), (106, 1), (107, 2), (114, 1), (117, 3), (119, 1), (145, 6)]
discards: [0]
Fitting a model of length 203 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 473.8464 - loglik: -4.1368e+02 - logprior: -6.0168e+01
Epoch 2/2
10/10 - 4s - loss: 411.9797 - loglik: -3.9101e+02 - logprior: -2.0966e+01
Fitted a model with MAP estimate = -400.3297
expansions: [(0, 13)]
discards: [  0  21  26  27  58 124 144]
Fitting a model of length 209 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 440.8622 - loglik: -3.9422e+02 - logprior: -4.6644e+01
Epoch 2/2
10/10 - 5s - loss: 392.9048 - loglik: -3.8536e+02 - logprior: -7.5430e+00
Fitted a model with MAP estimate = -383.6942
expansions: [(37, 2), (184, 3)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  21  46 181]
Fitting a model of length 199 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 437.4850 - loglik: -3.9218e+02 - logprior: -4.5306e+01
Epoch 2/10
10/10 - 4s - loss: 391.0842 - loglik: -3.8447e+02 - logprior: -6.6179e+00
Epoch 3/10
10/10 - 4s - loss: 378.5714 - loglik: -3.7979e+02 - logprior: 1.2187
Epoch 4/10
10/10 - 4s - loss: 372.0717 - loglik: -3.7681e+02 - logprior: 4.7423
Epoch 5/10
10/10 - 4s - loss: 367.9536 - loglik: -3.7466e+02 - logprior: 6.7104
Epoch 6/10
10/10 - 4s - loss: 366.6682 - loglik: -3.7453e+02 - logprior: 7.8589
Epoch 7/10
10/10 - 4s - loss: 364.9652 - loglik: -3.7365e+02 - logprior: 8.6889
Epoch 8/10
10/10 - 4s - loss: 364.0121 - loglik: -3.7336e+02 - logprior: 9.3462
Epoch 9/10
10/10 - 4s - loss: 364.5570 - loglik: -3.7444e+02 - logprior: 9.8875
Fitted a model with MAP estimate = -363.9528
Time for alignment: 111.7031
Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 613.7614 - loglik: -5.6095e+02 - logprior: -5.2809e+01
Epoch 2/10
10/10 - 3s - loss: 516.1520 - loglik: -5.0736e+02 - logprior: -8.7939e+00
Epoch 3/10
10/10 - 3s - loss: 453.9349 - loglik: -4.5216e+02 - logprior: -1.7700e+00
Epoch 4/10
10/10 - 3s - loss: 420.4999 - loglik: -4.2067e+02 - logprior: 0.1739
Epoch 5/10
10/10 - 3s - loss: 409.1270 - loglik: -4.1006e+02 - logprior: 0.9340
Epoch 6/10
10/10 - 3s - loss: 404.7064 - loglik: -4.0602e+02 - logprior: 1.3179
Epoch 7/10
10/10 - 3s - loss: 402.6517 - loglik: -4.0444e+02 - logprior: 1.7858
Epoch 8/10
10/10 - 3s - loss: 401.9075 - loglik: -4.0399e+02 - logprior: 2.0811
Epoch 9/10
10/10 - 3s - loss: 400.7175 - loglik: -4.0297e+02 - logprior: 2.2478
Epoch 10/10
10/10 - 3s - loss: 399.8127 - loglik: -4.0227e+02 - logprior: 2.4612
Fitted a model with MAP estimate = -400.2776
expansions: [(7, 3), (12, 1), (19, 1), (22, 2), (23, 1), (25, 1), (29, 3), (44, 1), (45, 2), (46, 1), (52, 2), (74, 1), (81, 2), (103, 1), (104, 2), (106, 1), (111, 2), (113, 1), (116, 3), (118, 1), (145, 3), (150, 1), (152, 1)]
discards: [0]
Fitting a model of length 203 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 472.8607 - loglik: -4.1248e+02 - logprior: -6.0386e+01
Epoch 2/2
10/10 - 4s - loss: 411.3773 - loglik: -3.9025e+02 - logprior: -2.1132e+01
Fitted a model with MAP estimate = -399.6542
expansions: [(0, 11)]
discards: [  0  14  26  37  58  67  99 126 135 144 177 178]
Fitting a model of length 202 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 443.5443 - loglik: -3.9668e+02 - logprior: -4.6866e+01
Epoch 2/2
10/10 - 4s - loss: 393.3792 - loglik: -3.8551e+02 - logprior: -7.8703e+00
Fitted a model with MAP estimate = -385.3576
expansions: [(24, 1), (98, 3)]
discards: [ 1  2  3  4  5  6  7  8  9 10]
Fitting a model of length 196 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 435.1733 - loglik: -3.8947e+02 - logprior: -4.5702e+01
Epoch 2/10
10/10 - 4s - loss: 391.0306 - loglik: -3.8431e+02 - logprior: -6.7238e+00
Epoch 3/10
10/10 - 4s - loss: 378.4324 - loglik: -3.7980e+02 - logprior: 1.3638
Epoch 4/10
10/10 - 4s - loss: 371.7409 - loglik: -3.7656e+02 - logprior: 4.8167
Epoch 5/10
10/10 - 4s - loss: 368.5356 - loglik: -3.7532e+02 - logprior: 6.7808
Epoch 6/10
10/10 - 4s - loss: 366.0552 - loglik: -3.7405e+02 - logprior: 7.9984
Epoch 7/10
10/10 - 4s - loss: 365.5761 - loglik: -3.7441e+02 - logprior: 8.8300
Epoch 8/10
10/10 - 4s - loss: 365.1463 - loglik: -3.7463e+02 - logprior: 9.4877
Epoch 9/10
10/10 - 4s - loss: 363.9185 - loglik: -3.7396e+02 - logprior: 10.0406
Epoch 10/10
10/10 - 4s - loss: 363.7700 - loglik: -3.7429e+02 - logprior: 10.5164
Fitted a model with MAP estimate = -363.5582
Time for alignment: 115.6862
Fitting a model of length 167 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 613.5824 - loglik: -5.6077e+02 - logprior: -5.2812e+01
Epoch 2/10
10/10 - 3s - loss: 515.4057 - loglik: -5.0662e+02 - logprior: -8.7849e+00
Epoch 3/10
10/10 - 3s - loss: 453.4262 - loglik: -4.5175e+02 - logprior: -1.6736e+00
Epoch 4/10
10/10 - 3s - loss: 421.7496 - loglik: -4.2212e+02 - logprior: 0.3739
Epoch 5/10
10/10 - 3s - loss: 408.4748 - loglik: -4.0986e+02 - logprior: 1.3823
Epoch 6/10
10/10 - 3s - loss: 405.0594 - loglik: -4.0712e+02 - logprior: 2.0632
Epoch 7/10
10/10 - 3s - loss: 402.5928 - loglik: -4.0485e+02 - logprior: 2.2601
Epoch 8/10
10/10 - 3s - loss: 401.8889 - loglik: -4.0429e+02 - logprior: 2.3970
Epoch 9/10
10/10 - 3s - loss: 400.6130 - loglik: -4.0328e+02 - logprior: 2.6649
Epoch 10/10
10/10 - 3s - loss: 400.6742 - loglik: -4.0365e+02 - logprior: 2.9759
Fitted a model with MAP estimate = -400.1864
expansions: [(0, 4), (19, 1), (22, 1), (23, 3), (29, 3), (44, 1), (45, 2), (46, 1), (52, 2), (76, 3), (81, 1), (82, 1), (100, 1), (104, 1), (107, 2), (113, 1), (116, 3), (118, 1), (144, 3), (150, 4)]
discards: []
Fitting a model of length 206 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 480.7657 - loglik: -4.1252e+02 - logprior: -6.8244e+01
Epoch 2/2
10/10 - 5s - loss: 406.8135 - loglik: -3.9098e+02 - logprior: -1.5832e+01
Fitted a model with MAP estimate = -390.8650
expansions: []
discards: [  0   3  29  30  38  58  68  94  95 133 145]
Fitting a model of length 195 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 457.0930 - loglik: -3.9640e+02 - logprior: -6.0694e+01
Epoch 2/2
10/10 - 4s - loss: 409.3849 - loglik: -3.8897e+02 - logprior: -2.0415e+01
Fitted a model with MAP estimate = -400.6592
expansions: [(176, 2), (177, 1)]
discards: [0]
Fitting a model of length 197 on 747 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 450.0768 - loglik: -3.9235e+02 - logprior: -5.7730e+01
Epoch 2/10
10/10 - 4s - loss: 401.8564 - loglik: -3.8753e+02 - logprior: -1.4329e+01
Epoch 3/10
10/10 - 4s - loss: 381.6347 - loglik: -3.8126e+02 - logprior: -3.7629e-01
Epoch 4/10
10/10 - 4s - loss: 374.5104 - loglik: -3.7924e+02 - logprior: 4.7307
Epoch 5/10
10/10 - 4s - loss: 369.6487 - loglik: -3.7647e+02 - logprior: 6.8220
Epoch 6/10
10/10 - 4s - loss: 368.3965 - loglik: -3.7637e+02 - logprior: 7.9685
Epoch 7/10
10/10 - 4s - loss: 366.9131 - loglik: -3.7563e+02 - logprior: 8.7136
Epoch 8/10
10/10 - 4s - loss: 366.2988 - loglik: -3.7560e+02 - logprior: 9.2992
Epoch 9/10
10/10 - 4s - loss: 366.3546 - loglik: -3.7621e+02 - logprior: 9.8532
Fitted a model with MAP estimate = -365.4519
Time for alignment: 111.0267
Computed alignments with likelihoods: ['-363.9528', '-363.5582', '-365.4519']
Best model has likelihood: -363.5582
SP score = 0.7586
Training of 3 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb794a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe24b0003d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.0948 - loglik: -2.3498e+02 - logprior: -3.1196e+00
Epoch 2/10
19/19 - 2s - loss: 205.7071 - loglik: -2.0453e+02 - logprior: -1.1798e+00
Epoch 3/10
19/19 - 2s - loss: 196.7532 - loglik: -1.9549e+02 - logprior: -1.2668e+00
Epoch 4/10
19/19 - 2s - loss: 194.5649 - loglik: -1.9335e+02 - logprior: -1.2133e+00
Epoch 5/10
19/19 - 2s - loss: 193.4897 - loglik: -1.9229e+02 - logprior: -1.2021e+00
Epoch 6/10
19/19 - 2s - loss: 193.0802 - loglik: -1.9189e+02 - logprior: -1.1865e+00
Epoch 7/10
19/19 - 2s - loss: 192.9722 - loglik: -1.9179e+02 - logprior: -1.1776e+00
Epoch 8/10
19/19 - 2s - loss: 192.9971 - loglik: -1.9183e+02 - logprior: -1.1679e+00
Fitted a model with MAP estimate = -185.9886
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (22, 1), (24, 1), (46, 2), (47, 1), (48, 2), (49, 1), (52, 2), (55, 2), (58, 1)]
discards: []
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 195.8952 - loglik: -1.9180e+02 - logprior: -4.0915e+00
Epoch 2/2
19/19 - 2s - loss: 187.5894 - loglik: -1.8613e+02 - logprior: -1.4590e+00
Fitted a model with MAP estimate = -179.6369
expansions: [(31, 1)]
discards: [ 0  1  2 24 25 26 63 75]
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 193.2491 - loglik: -1.8950e+02 - logprior: -3.7468e+00
Epoch 2/2
19/19 - 2s - loss: 188.8329 - loglik: -1.8714e+02 - logprior: -1.6899e+00
Fitted a model with MAP estimate = -180.5375
expansions: [(0, 4)]
discards: [ 0 19]
Fitting a model of length 86 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 181.2447 - loglik: -1.7937e+02 - logprior: -1.8790e+00
Epoch 2/10
23/23 - 2s - loss: 179.0486 - loglik: -1.7806e+02 - logprior: -9.8556e-01
Epoch 3/10
23/23 - 2s - loss: 178.4809 - loglik: -1.7754e+02 - logprior: -9.3941e-01
Epoch 4/10
23/23 - 2s - loss: 177.7205 - loglik: -1.7682e+02 - logprior: -9.0085e-01
Epoch 5/10
23/23 - 2s - loss: 177.4570 - loglik: -1.7657e+02 - logprior: -8.8584e-01
Epoch 6/10
23/23 - 2s - loss: 177.1974 - loglik: -1.7633e+02 - logprior: -8.7018e-01
Epoch 7/10
23/23 - 2s - loss: 176.8247 - loglik: -1.7596e+02 - logprior: -8.6286e-01
Epoch 8/10
23/23 - 2s - loss: 176.9810 - loglik: -1.7614e+02 - logprior: -8.4515e-01
Fitted a model with MAP estimate = -176.7971
Time for alignment: 63.2311
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.7233 - loglik: -2.3560e+02 - logprior: -3.1184e+00
Epoch 2/10
19/19 - 2s - loss: 208.9652 - loglik: -2.0778e+02 - logprior: -1.1830e+00
Epoch 3/10
19/19 - 2s - loss: 197.7091 - loglik: -1.9643e+02 - logprior: -1.2793e+00
Epoch 4/10
19/19 - 2s - loss: 195.0546 - loglik: -1.9382e+02 - logprior: -1.2375e+00
Epoch 5/10
19/19 - 2s - loss: 194.0402 - loglik: -1.9283e+02 - logprior: -1.2107e+00
Epoch 6/10
19/19 - 2s - loss: 193.5762 - loglik: -1.9238e+02 - logprior: -1.1981e+00
Epoch 7/10
19/19 - 2s - loss: 193.3684 - loglik: -1.9218e+02 - logprior: -1.1865e+00
Epoch 8/10
19/19 - 2s - loss: 193.6018 - loglik: -1.9243e+02 - logprior: -1.1737e+00
Fitted a model with MAP estimate = -186.5337
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (16, 5), (17, 1), (18, 2), (23, 1), (36, 1), (46, 2), (47, 1), (48, 2), (49, 2), (52, 2), (55, 2), (58, 1)]
discards: []
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 196.7425 - loglik: -1.9264e+02 - logprior: -4.1030e+00
Epoch 2/2
19/19 - 2s - loss: 187.5965 - loglik: -1.8614e+02 - logprior: -1.4585e+00
Fitted a model with MAP estimate = -179.7118
expansions: [(31, 1)]
discards: [ 1  2 24 25 26 63 70 76]
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 190.5386 - loglik: -1.8782e+02 - logprior: -2.7149e+00
Epoch 2/2
19/19 - 2s - loss: 187.1469 - loglik: -1.8613e+02 - logprior: -1.0175e+00
Fitted a model with MAP estimate = -179.6973
expansions: []
discards: [0]
Fitting a model of length 84 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 182.0963 - loglik: -1.7971e+02 - logprior: -2.3880e+00
Epoch 2/10
23/23 - 2s - loss: 179.9609 - loglik: -1.7858e+02 - logprior: -1.3798e+00
Epoch 3/10
23/23 - 2s - loss: 178.6503 - loglik: -1.7771e+02 - logprior: -9.3689e-01
Epoch 4/10
23/23 - 2s - loss: 178.4916 - loglik: -1.7774e+02 - logprior: -7.4893e-01
Epoch 5/10
23/23 - 2s - loss: 177.5842 - loglik: -1.7684e+02 - logprior: -7.4819e-01
Epoch 6/10
23/23 - 2s - loss: 177.4169 - loglik: -1.7670e+02 - logprior: -7.2167e-01
Epoch 7/10
23/23 - 2s - loss: 177.1358 - loglik: -1.7642e+02 - logprior: -7.1175e-01
Epoch 8/10
23/23 - 2s - loss: 177.0422 - loglik: -1.7634e+02 - logprior: -7.0117e-01
Epoch 9/10
23/23 - 2s - loss: 176.6137 - loglik: -1.7592e+02 - logprior: -6.8923e-01
Epoch 10/10
23/23 - 2s - loss: 176.4825 - loglik: -1.7581e+02 - logprior: -6.7336e-01
Fitted a model with MAP estimate = -176.4089
Time for alignment: 68.5375
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 238.0462 - loglik: -2.3493e+02 - logprior: -3.1143e+00
Epoch 2/10
19/19 - 2s - loss: 207.7622 - loglik: -2.0659e+02 - logprior: -1.1694e+00
Epoch 3/10
19/19 - 2s - loss: 196.5377 - loglik: -1.9526e+02 - logprior: -1.2754e+00
Epoch 4/10
19/19 - 2s - loss: 194.3133 - loglik: -1.9311e+02 - logprior: -1.2046e+00
Epoch 5/10
19/19 - 2s - loss: 193.7858 - loglik: -1.9261e+02 - logprior: -1.1778e+00
Epoch 6/10
19/19 - 2s - loss: 193.3873 - loglik: -1.9222e+02 - logprior: -1.1695e+00
Epoch 7/10
19/19 - 2s - loss: 193.0383 - loglik: -1.9187e+02 - logprior: -1.1668e+00
Epoch 8/10
19/19 - 2s - loss: 193.2827 - loglik: -1.9213e+02 - logprior: -1.1539e+00
Fitted a model with MAP estimate = -186.2992
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (15, 2), (16, 5), (17, 2), (31, 1), (46, 2), (47, 1), (48, 2), (49, 2), (52, 2), (55, 2), (58, 1)]
discards: []
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 196.4878 - loglik: -1.9238e+02 - logprior: -4.1106e+00
Epoch 2/2
19/19 - 2s - loss: 187.6943 - loglik: -1.8619e+02 - logprior: -1.5052e+00
Fitted a model with MAP estimate = -179.5824
expansions: [(22, 1)]
discards: [ 0  1  2 25 26 27 28 63 70 76]
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 193.2887 - loglik: -1.8955e+02 - logprior: -3.7352e+00
Epoch 2/2
19/19 - 2s - loss: 188.7460 - loglik: -1.8709e+02 - logprior: -1.6593e+00
Fitted a model with MAP estimate = -180.4595
expansions: [(0, 4), (23, 3), (26, 2)]
discards: [0]
Fitting a model of length 91 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 180.5625 - loglik: -1.7868e+02 - logprior: -1.8850e+00
Epoch 2/10
23/23 - 2s - loss: 177.9934 - loglik: -1.7700e+02 - logprior: -9.8899e-01
Epoch 3/10
23/23 - 2s - loss: 177.3719 - loglik: -1.7642e+02 - logprior: -9.5549e-01
Epoch 4/10
23/23 - 2s - loss: 176.7268 - loglik: -1.7580e+02 - logprior: -9.2983e-01
Epoch 5/10
23/23 - 2s - loss: 176.1660 - loglik: -1.7525e+02 - logprior: -9.1666e-01
Epoch 6/10
23/23 - 2s - loss: 175.8347 - loglik: -1.7493e+02 - logprior: -9.0330e-01
Epoch 7/10
23/23 - 2s - loss: 175.6474 - loglik: -1.7475e+02 - logprior: -8.9702e-01
Epoch 8/10
23/23 - 2s - loss: 175.5389 - loglik: -1.7467e+02 - logprior: -8.7245e-01
Epoch 9/10
23/23 - 2s - loss: 175.6356 - loglik: -1.7476e+02 - logprior: -8.7292e-01
Fitted a model with MAP estimate = -175.4438
Time for alignment: 64.2452
Computed alignments with likelihoods: ['-176.7971', '-176.4089', '-175.4438']
Best model has likelihood: -175.4438
SP score = 0.8533
Training of 3 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25d862550>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2420f61f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 795.5174 - loglik: -7.8977e+02 - logprior: -5.7430e+00
Epoch 2/10
22/22 - 12s - loss: 686.7722 - loglik: -6.8622e+02 - logprior: -5.5521e-01
Epoch 3/10
22/22 - 12s - loss: 646.9177 - loglik: -6.4543e+02 - logprior: -1.4863e+00
Epoch 4/10
22/22 - 12s - loss: 641.8667 - loglik: -6.4046e+02 - logprior: -1.4075e+00
Epoch 5/10
22/22 - 12s - loss: 639.4397 - loglik: -6.3801e+02 - logprior: -1.4330e+00
Epoch 6/10
22/22 - 12s - loss: 638.8657 - loglik: -6.3740e+02 - logprior: -1.4657e+00
Epoch 7/10
22/22 - 12s - loss: 638.8381 - loglik: -6.3736e+02 - logprior: -1.4807e+00
Epoch 8/10
22/22 - 12s - loss: 637.1277 - loglik: -6.3565e+02 - logprior: -1.4767e+00
Epoch 9/10
22/22 - 12s - loss: 639.3162 - loglik: -6.3785e+02 - logprior: -1.4703e+00
Fitted a model with MAP estimate = -637.7446
expansions: [(12, 1), (13, 1), (32, 2), (33, 1), (35, 2), (36, 1), (46, 2), (49, 1), (50, 1), (51, 1), (66, 1), (70, 1), (71, 1), (76, 1), (77, 2), (80, 1), (82, 1), (84, 1), (98, 1), (99, 1), (104, 1), (105, 2), (107, 1), (109, 1), (118, 1), (120, 2), (143, 2), (144, 1), (149, 1), (150, 1), (152, 1), (156, 1), (157, 1), (159, 1), (179, 2), (180, 5), (183, 1), (184, 1), (185, 2), (193, 2), (205, 3), (206, 1), (209, 1), (214, 2), (215, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0 1]
Fitting a model of length 317 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 644.7985 - loglik: -6.3655e+02 - logprior: -8.2503e+00
Epoch 2/2
22/22 - 17s - loss: 623.1655 - loglik: -6.2062e+02 - logprior: -2.5463e+00
Fitted a model with MAP estimate = -619.4538
expansions: [(0, 3), (2, 1)]
discards: [  0  33 174 222 223 224 233 257 272]
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 627.5678 - loglik: -6.2288e+02 - logprior: -4.6840e+00
Epoch 2/2
22/22 - 17s - loss: 614.7224 - loglik: -6.1592e+02 - logprior: 1.1960
Fitted a model with MAP estimate = -614.6151
expansions: [(223, 1)]
discards: [  0   1   2 100]
Fitting a model of length 309 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 630.4464 - loglik: -6.2330e+02 - logprior: -7.1484e+00
Epoch 2/10
22/22 - 16s - loss: 619.3729 - loglik: -6.1799e+02 - logprior: -1.3817e+00
Epoch 3/10
22/22 - 16s - loss: 617.8510 - loglik: -6.1798e+02 - logprior: 0.1283
Epoch 4/10
22/22 - 16s - loss: 613.0958 - loglik: -6.1575e+02 - logprior: 2.6588
Epoch 5/10
22/22 - 16s - loss: 611.7957 - loglik: -6.1466e+02 - logprior: 2.8656
Epoch 6/10
22/22 - 16s - loss: 610.2810 - loglik: -6.1333e+02 - logprior: 3.0524
Epoch 7/10
22/22 - 16s - loss: 609.2144 - loglik: -6.1247e+02 - logprior: 3.2510
Epoch 8/10
22/22 - 16s - loss: 609.1131 - loglik: -6.1258e+02 - logprior: 3.4653
Epoch 9/10
22/22 - 17s - loss: 611.2697 - loglik: -6.1499e+02 - logprior: 3.7183
Fitted a model with MAP estimate = -608.9386
Time for alignment: 388.9126
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 797.1131 - loglik: -7.9139e+02 - logprior: -5.7252e+00
Epoch 2/10
22/22 - 12s - loss: 687.5335 - loglik: -6.8700e+02 - logprior: -5.3532e-01
Epoch 3/10
22/22 - 12s - loss: 647.1406 - loglik: -6.4551e+02 - logprior: -1.6290e+00
Epoch 4/10
22/22 - 12s - loss: 638.0756 - loglik: -6.3654e+02 - logprior: -1.5348e+00
Epoch 5/10
22/22 - 12s - loss: 637.9589 - loglik: -6.3644e+02 - logprior: -1.5197e+00
Epoch 6/10
22/22 - 12s - loss: 638.5753 - loglik: -6.3707e+02 - logprior: -1.5062e+00
Fitted a model with MAP estimate = -636.7426
expansions: [(14, 1), (15, 1), (32, 2), (33, 1), (35, 2), (36, 1), (46, 2), (49, 1), (50, 1), (51, 1), (71, 1), (72, 2), (73, 1), (77, 2), (80, 1), (82, 1), (84, 1), (98, 1), (99, 1), (104, 1), (105, 2), (107, 1), (108, 1), (120, 2), (138, 1), (143, 1), (144, 2), (148, 1), (152, 1), (155, 2), (156, 2), (158, 1), (178, 2), (179, 1), (180, 1), (184, 1), (185, 1), (194, 2), (195, 1), (206, 1), (207, 1), (208, 1), (209, 1), (214, 2), (215, 1), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Fitting a model of length 314 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 640.4493 - loglik: -6.3226e+02 - logprior: -8.1878e+00
Epoch 2/2
22/22 - 17s - loss: 618.6730 - loglik: -6.1660e+02 - logprior: -2.0774e+00
Fitted a model with MAP estimate = -617.1863
expansions: [(0, 3)]
discards: [  0  34 176 269]
Fitting a model of length 313 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 624.5121 - loglik: -6.1983e+02 - logprior: -4.6858e+00
Epoch 2/2
22/22 - 17s - loss: 614.5718 - loglik: -6.1580e+02 - logprior: 1.2306
Fitted a model with MAP estimate = -613.3647
expansions: []
discards: [0 1 2]
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 628.1559 - loglik: -6.2094e+02 - logprior: -7.2116e+00
Epoch 2/10
22/22 - 16s - loss: 622.0936 - loglik: -6.2075e+02 - logprior: -1.3400e+00
Epoch 3/10
22/22 - 16s - loss: 614.5422 - loglik: -6.1486e+02 - logprior: 0.3137
Epoch 4/10
22/22 - 17s - loss: 612.0393 - loglik: -6.1462e+02 - logprior: 2.5829
Epoch 5/10
22/22 - 16s - loss: 609.6850 - loglik: -6.1247e+02 - logprior: 2.7856
Epoch 6/10
22/22 - 17s - loss: 610.9921 - loglik: -6.1393e+02 - logprior: 2.9428
Fitted a model with MAP estimate = -609.0125
Time for alignment: 302.9096
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 796.8813 - loglik: -7.9114e+02 - logprior: -5.7400e+00
Epoch 2/10
22/22 - 12s - loss: 684.4507 - loglik: -6.8382e+02 - logprior: -6.3252e-01
Epoch 3/10
22/22 - 12s - loss: 647.6835 - loglik: -6.4614e+02 - logprior: -1.5407e+00
Epoch 4/10
22/22 - 12s - loss: 640.2355 - loglik: -6.3881e+02 - logprior: -1.4264e+00
Epoch 5/10
22/22 - 12s - loss: 638.4797 - loglik: -6.3699e+02 - logprior: -1.4881e+00
Epoch 6/10
22/22 - 12s - loss: 637.5559 - loglik: -6.3604e+02 - logprior: -1.5141e+00
Epoch 7/10
22/22 - 12s - loss: 637.1006 - loglik: -6.3558e+02 - logprior: -1.5246e+00
Epoch 8/10
22/22 - 12s - loss: 636.1785 - loglik: -6.3467e+02 - logprior: -1.5120e+00
Epoch 9/10
22/22 - 12s - loss: 636.8423 - loglik: -6.3534e+02 - logprior: -1.4977e+00
Fitted a model with MAP estimate = -636.1342
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 2), (35, 1), (45, 2), (48, 1), (49, 1), (57, 1), (69, 3), (70, 1), (72, 1), (76, 2), (79, 1), (81, 1), (83, 1), (97, 1), (99, 1), (105, 2), (107, 1), (109, 1), (120, 2), (136, 1), (142, 2), (143, 1), (148, 1), (149, 1), (151, 1), (155, 1), (156, 3), (157, 1), (178, 3), (179, 1), (180, 1), (183, 3), (184, 1), (185, 1), (193, 2), (194, 1), (207, 1), (208, 1), (209, 1), (213, 2), (215, 1), (224, 1), (225, 1), (227, 1), (236, 1), (238, 1)]
discards: [0 1]
Fitting a model of length 318 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 21s - loss: 642.9451 - loglik: -6.3481e+02 - logprior: -8.1353e+00
Epoch 2/2
22/22 - 17s - loss: 621.9386 - loglik: -6.1963e+02 - logprior: -2.3113e+00
Fitted a model with MAP estimate = -618.4858
expansions: [(0, 3), (2, 1)]
discards: [  0  33  82 173 194 195 196 221 231 232]
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 624.9978 - loglik: -6.2031e+02 - logprior: -4.6865e+00
Epoch 2/2
22/22 - 17s - loss: 615.5699 - loglik: -6.1691e+02 - logprior: 1.3413
Fitted a model with MAP estimate = -613.9363
expansions: []
discards: [  0   1   2 100 267]
Fitting a model of length 307 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 631.0919 - loglik: -6.2393e+02 - logprior: -7.1635e+00
Epoch 2/10
22/22 - 16s - loss: 620.0698 - loglik: -6.1878e+02 - logprior: -1.2939e+00
Epoch 3/10
22/22 - 16s - loss: 616.3098 - loglik: -6.1662e+02 - logprior: 0.3056
Epoch 4/10
22/22 - 16s - loss: 612.7115 - loglik: -6.1540e+02 - logprior: 2.6855
Epoch 5/10
22/22 - 16s - loss: 608.6887 - loglik: -6.1164e+02 - logprior: 2.9522
Epoch 6/10
22/22 - 16s - loss: 610.7588 - loglik: -6.1384e+02 - logprior: 3.0778
Fitted a model with MAP estimate = -609.5774
Time for alignment: 336.2175
Computed alignments with likelihoods: ['-608.9386', '-609.0125', '-609.5774']
Best model has likelihood: -608.9386
SP score = 0.9108
Training of 3 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe27ffe3c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe290e53850>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 172.2732 - loglik: -7.9883e+01 - logprior: -9.2390e+01
Epoch 2/10
10/10 - 1s - loss: 95.0732 - loglik: -6.8485e+01 - logprior: -2.6588e+01
Epoch 3/10
10/10 - 1s - loss: 73.1524 - loglik: -5.9855e+01 - logprior: -1.3297e+01
Epoch 4/10
10/10 - 1s - loss: 63.9535 - loglik: -5.5772e+01 - logprior: -8.1818e+00
Epoch 5/10
10/10 - 1s - loss: 59.2842 - loglik: -5.3726e+01 - logprior: -5.5577e+00
Epoch 6/10
10/10 - 1s - loss: 57.2416 - loglik: -5.3154e+01 - logprior: -4.0875e+00
Epoch 7/10
10/10 - 1s - loss: 56.1688 - loglik: -5.2942e+01 - logprior: -3.2268e+00
Epoch 8/10
10/10 - 0s - loss: 55.5262 - loglik: -5.2839e+01 - logprior: -2.6874e+00
Epoch 9/10
10/10 - 0s - loss: 55.2001 - loglik: -5.2841e+01 - logprior: -2.3595e+00
Epoch 10/10
10/10 - 1s - loss: 54.9850 - loglik: -5.2855e+01 - logprior: -2.1302e+00
Fitted a model with MAP estimate = -54.8960
expansions: [(0, 3), (13, 1)]
discards: []
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 173.8564 - loglik: -4.9536e+01 - logprior: -1.2432e+02
Epoch 2/2
10/10 - 1s - loss: 87.6163 - loglik: -4.7379e+01 - logprior: -4.0237e+01
Fitted a model with MAP estimate = -70.7830
expansions: [(0, 1)]
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 148.0487 - loglik: -4.5028e+01 - logprior: -1.0302e+02
Epoch 2/2
10/10 - 1s - loss: 81.0053 - loglik: -4.4496e+01 - logprior: -3.6509e+01
Fitted a model with MAP estimate = -66.7057
expansions: []
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 128.9291 - loglik: -4.3131e+01 - logprior: -8.5798e+01
Epoch 2/10
10/10 - 1s - loss: 68.5461 - loglik: -4.3865e+01 - logprior: -2.4681e+01
Epoch 3/10
10/10 - 1s - loss: 56.6010 - loglik: -4.4536e+01 - logprior: -1.2065e+01
Epoch 4/10
10/10 - 1s - loss: 51.9295 - loglik: -4.5009e+01 - logprior: -6.9200e+00
Epoch 5/10
10/10 - 1s - loss: 49.3076 - loglik: -4.5153e+01 - logprior: -4.1543e+00
Epoch 6/10
10/10 - 1s - loss: 47.4754 - loglik: -4.4868e+01 - logprior: -2.6079e+00
Epoch 7/10
10/10 - 1s - loss: 46.4195 - loglik: -4.4741e+01 - logprior: -1.6788e+00
Epoch 8/10
10/10 - 1s - loss: 45.7586 - loglik: -4.4615e+01 - logprior: -1.1441e+00
Epoch 9/10
10/10 - 1s - loss: 45.3431 - loglik: -4.4531e+01 - logprior: -8.1185e-01
Epoch 10/10
10/10 - 1s - loss: 45.0735 - loglik: -4.4551e+01 - logprior: -5.2271e-01
Fitted a model with MAP estimate = -44.9527
Time for alignment: 27.9668
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 172.2732 - loglik: -7.9883e+01 - logprior: -9.2390e+01
Epoch 2/10
10/10 - 1s - loss: 95.0732 - loglik: -6.8485e+01 - logprior: -2.6588e+01
Epoch 3/10
10/10 - 0s - loss: 73.1524 - loglik: -5.9855e+01 - logprior: -1.3297e+01
Epoch 4/10
10/10 - 1s - loss: 63.9535 - loglik: -5.5772e+01 - logprior: -8.1818e+00
Epoch 5/10
10/10 - 1s - loss: 59.2842 - loglik: -5.3726e+01 - logprior: -5.5577e+00
Epoch 6/10
10/10 - 1s - loss: 57.2416 - loglik: -5.3154e+01 - logprior: -4.0875e+00
Epoch 7/10
10/10 - 1s - loss: 56.1688 - loglik: -5.2942e+01 - logprior: -3.2268e+00
Epoch 8/10
10/10 - 1s - loss: 55.5262 - loglik: -5.2839e+01 - logprior: -2.6874e+00
Epoch 9/10
10/10 - 1s - loss: 55.2001 - loglik: -5.2841e+01 - logprior: -2.3595e+00
Epoch 10/10
10/10 - 1s - loss: 54.9850 - loglik: -5.2855e+01 - logprior: -2.1302e+00
Fitted a model with MAP estimate = -54.8960
expansions: [(0, 3), (13, 1)]
discards: []
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 173.8564 - loglik: -4.9536e+01 - logprior: -1.2432e+02
Epoch 2/2
10/10 - 1s - loss: 87.6163 - loglik: -4.7379e+01 - logprior: -4.0237e+01
Fitted a model with MAP estimate = -70.7830
expansions: [(0, 1)]
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 148.0487 - loglik: -4.5028e+01 - logprior: -1.0302e+02
Epoch 2/2
10/10 - 1s - loss: 81.0053 - loglik: -4.4496e+01 - logprior: -3.6509e+01
Fitted a model with MAP estimate = -66.7057
expansions: []
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 128.9291 - loglik: -4.3131e+01 - logprior: -8.5798e+01
Epoch 2/10
10/10 - 1s - loss: 68.5461 - loglik: -4.3865e+01 - logprior: -2.4681e+01
Epoch 3/10
10/10 - 1s - loss: 56.6010 - loglik: -4.4536e+01 - logprior: -1.2065e+01
Epoch 4/10
10/10 - 1s - loss: 51.9295 - loglik: -4.5009e+01 - logprior: -6.9200e+00
Epoch 5/10
10/10 - 0s - loss: 49.3076 - loglik: -4.5153e+01 - logprior: -4.1543e+00
Epoch 6/10
10/10 - 1s - loss: 47.4754 - loglik: -4.4868e+01 - logprior: -2.6079e+00
Epoch 7/10
10/10 - 0s - loss: 46.4195 - loglik: -4.4741e+01 - logprior: -1.6788e+00
Epoch 8/10
10/10 - 0s - loss: 45.7586 - loglik: -4.4615e+01 - logprior: -1.1441e+00
Epoch 9/10
10/10 - 1s - loss: 45.3431 - loglik: -4.4531e+01 - logprior: -8.1185e-01
Epoch 10/10
10/10 - 1s - loss: 45.0735 - loglik: -4.4551e+01 - logprior: -5.2271e-01
Fitted a model with MAP estimate = -44.9527
Time for alignment: 27.1300
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 172.2732 - loglik: -7.9883e+01 - logprior: -9.2390e+01
Epoch 2/10
10/10 - 1s - loss: 95.0732 - loglik: -6.8485e+01 - logprior: -2.6588e+01
Epoch 3/10
10/10 - 1s - loss: 73.1524 - loglik: -5.9855e+01 - logprior: -1.3297e+01
Epoch 4/10
10/10 - 1s - loss: 63.9535 - loglik: -5.5772e+01 - logprior: -8.1818e+00
Epoch 5/10
10/10 - 1s - loss: 59.2842 - loglik: -5.3726e+01 - logprior: -5.5577e+00
Epoch 6/10
10/10 - 1s - loss: 57.2416 - loglik: -5.3154e+01 - logprior: -4.0875e+00
Epoch 7/10
10/10 - 1s - loss: 56.1688 - loglik: -5.2942e+01 - logprior: -3.2268e+00
Epoch 8/10
10/10 - 1s - loss: 55.5262 - loglik: -5.2839e+01 - logprior: -2.6874e+00
Epoch 9/10
10/10 - 0s - loss: 55.2001 - loglik: -5.2841e+01 - logprior: -2.3595e+00
Epoch 10/10
10/10 - 1s - loss: 54.9850 - loglik: -5.2855e+01 - logprior: -2.1302e+00
Fitted a model with MAP estimate = -54.8960
expansions: [(0, 3), (13, 1)]
discards: []
Fitting a model of length 24 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 173.8564 - loglik: -4.9536e+01 - logprior: -1.2432e+02
Epoch 2/2
10/10 - 1s - loss: 87.6163 - loglik: -4.7379e+01 - logprior: -4.0237e+01
Fitted a model with MAP estimate = -70.7830
expansions: [(0, 1)]
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 148.0487 - loglik: -4.5028e+01 - logprior: -1.0302e+02
Epoch 2/2
10/10 - 1s - loss: 81.0053 - loglik: -4.4496e+01 - logprior: -3.6509e+01
Fitted a model with MAP estimate = -66.7057
expansions: []
discards: []
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 128.9291 - loglik: -4.3131e+01 - logprior: -8.5798e+01
Epoch 2/10
10/10 - 1s - loss: 68.5461 - loglik: -4.3865e+01 - logprior: -2.4681e+01
Epoch 3/10
10/10 - 1s - loss: 56.6010 - loglik: -4.4536e+01 - logprior: -1.2065e+01
Epoch 4/10
10/10 - 1s - loss: 51.9295 - loglik: -4.5009e+01 - logprior: -6.9200e+00
Epoch 5/10
10/10 - 1s - loss: 49.3076 - loglik: -4.5153e+01 - logprior: -4.1543e+00
Epoch 6/10
10/10 - 1s - loss: 47.4754 - loglik: -4.4868e+01 - logprior: -2.6079e+00
Epoch 7/10
10/10 - 1s - loss: 46.4195 - loglik: -4.4741e+01 - logprior: -1.6788e+00
Epoch 8/10
10/10 - 1s - loss: 45.7586 - loglik: -4.4615e+01 - logprior: -1.1441e+00
Epoch 9/10
10/10 - 1s - loss: 45.3431 - loglik: -4.4531e+01 - logprior: -8.1185e-01
Epoch 10/10
10/10 - 1s - loss: 45.0735 - loglik: -4.4551e+01 - logprior: -5.2271e-01
Fitted a model with MAP estimate = -44.9527
Time for alignment: 27.5528
Computed alignments with likelihoods: ['-44.9527', '-44.9527', '-44.9527']
Best model has likelihood: -44.9527
SP score = 0.9419
Training of 3 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c645880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f988f9a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.9941 - loglik: -3.5068e+02 - logprior: -1.2311e+01
Epoch 2/10
11/11 - 2s - loss: 308.5701 - loglik: -3.0562e+02 - logprior: -2.9528e+00
Epoch 3/10
11/11 - 2s - loss: 267.3688 - loglik: -2.6540e+02 - logprior: -1.9714e+00
Epoch 4/10
11/11 - 2s - loss: 248.9176 - loglik: -2.4683e+02 - logprior: -2.0887e+00
Epoch 5/10
11/11 - 2s - loss: 243.9099 - loglik: -2.4174e+02 - logprior: -2.1669e+00
Epoch 6/10
11/11 - 2s - loss: 240.6275 - loglik: -2.3846e+02 - logprior: -2.1699e+00
Epoch 7/10
11/11 - 2s - loss: 240.2688 - loglik: -2.3815e+02 - logprior: -2.1230e+00
Epoch 8/10
11/11 - 2s - loss: 238.2815 - loglik: -2.3619e+02 - logprior: -2.0955e+00
Epoch 9/10
11/11 - 2s - loss: 239.1301 - loglik: -2.3702e+02 - logprior: -2.1101e+00
Fitted a model with MAP estimate = -238.3267
expansions: [(8, 2), (9, 3), (10, 2), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 254.2007 - loglik: -2.4000e+02 - logprior: -1.4203e+01
Epoch 2/2
11/11 - 2s - loss: 231.1921 - loglik: -2.2518e+02 - logprior: -6.0121e+00
Fitted a model with MAP estimate = -226.0684
expansions: [(0, 19)]
discards: [ 0  8 14 84 87]
Fitting a model of length 130 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 233.3868 - loglik: -2.2167e+02 - logprior: -1.1715e+01
Epoch 2/2
11/11 - 3s - loss: 219.6422 - loglik: -2.1641e+02 - logprior: -3.2278e+00
Fitted a model with MAP estimate = -215.2851
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 92]
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 233.4884 - loglik: -2.2105e+02 - logprior: -1.2434e+01
Epoch 2/10
11/11 - 2s - loss: 219.5035 - loglik: -2.1662e+02 - logprior: -2.8806e+00
Epoch 3/10
11/11 - 2s - loss: 217.1324 - loglik: -2.1598e+02 - logprior: -1.1524e+00
Epoch 4/10
11/11 - 2s - loss: 215.3296 - loglik: -2.1469e+02 - logprior: -6.3632e-01
Epoch 5/10
11/11 - 2s - loss: 214.9503 - loglik: -2.1447e+02 - logprior: -4.7814e-01
Epoch 6/10
11/11 - 2s - loss: 215.3825 - loglik: -2.1501e+02 - logprior: -3.7151e-01
Fitted a model with MAP estimate = -213.8811
Time for alignment: 64.0910
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 362.6233 - loglik: -3.5031e+02 - logprior: -1.2310e+01
Epoch 2/10
11/11 - 2s - loss: 308.0239 - loglik: -3.0507e+02 - logprior: -2.9545e+00
Epoch 3/10
11/11 - 2s - loss: 268.4731 - loglik: -2.6655e+02 - logprior: -1.9225e+00
Epoch 4/10
11/11 - 2s - loss: 250.8526 - loglik: -2.4888e+02 - logprior: -1.9691e+00
Epoch 5/10
11/11 - 2s - loss: 245.2413 - loglik: -2.4323e+02 - logprior: -2.0086e+00
Epoch 6/10
11/11 - 2s - loss: 242.1566 - loglik: -2.4017e+02 - logprior: -1.9841e+00
Epoch 7/10
11/11 - 2s - loss: 241.7927 - loglik: -2.3992e+02 - logprior: -1.8740e+00
Epoch 8/10
11/11 - 2s - loss: 240.5461 - loglik: -2.3877e+02 - logprior: -1.7783e+00
Epoch 9/10
11/11 - 2s - loss: 240.9009 - loglik: -2.3913e+02 - logprior: -1.7746e+00
Fitted a model with MAP estimate = -240.3956
expansions: [(8, 2), (9, 3), (10, 2), (11, 1), (33, 1), (34, 1), (38, 1), (39, 2), (61, 1), (62, 3), (65, 2), (66, 2), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 254.7712 - loglik: -2.4065e+02 - logprior: -1.4121e+01
Epoch 2/2
11/11 - 2s - loss: 228.6644 - loglik: -2.2270e+02 - logprior: -5.9606e+00
Fitted a model with MAP estimate = -225.2417
expansions: [(0, 20)]
discards: [  0   8  76  77  84  87 110]
Fitting a model of length 130 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 234.8535 - loglik: -2.2313e+02 - logprior: -1.1722e+01
Epoch 2/2
11/11 - 3s - loss: 218.6581 - loglik: -2.1533e+02 - logprior: -3.3304e+00
Fitted a model with MAP estimate = -217.3830
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 232.5707 - loglik: -2.2039e+02 - logprior: -1.2180e+01
Epoch 2/10
11/11 - 2s - loss: 218.4631 - loglik: -2.1560e+02 - logprior: -2.8633e+00
Epoch 3/10
11/11 - 2s - loss: 216.8575 - loglik: -2.1563e+02 - logprior: -1.2315e+00
Epoch 4/10
11/11 - 2s - loss: 214.5953 - loglik: -2.1382e+02 - logprior: -7.7484e-01
Epoch 5/10
11/11 - 2s - loss: 214.6348 - loglik: -2.1406e+02 - logprior: -5.7075e-01
Fitted a model with MAP estimate = -212.7134
Time for alignment: 62.2898
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 362.2931 - loglik: -3.4998e+02 - logprior: -1.2314e+01
Epoch 2/10
11/11 - 2s - loss: 309.5931 - loglik: -3.0663e+02 - logprior: -2.9602e+00
Epoch 3/10
11/11 - 2s - loss: 272.8708 - loglik: -2.7087e+02 - logprior: -1.9971e+00
Epoch 4/10
11/11 - 2s - loss: 250.8609 - loglik: -2.4869e+02 - logprior: -2.1664e+00
Epoch 5/10
11/11 - 2s - loss: 243.4204 - loglik: -2.4118e+02 - logprior: -2.2427e+00
Epoch 6/10
11/11 - 2s - loss: 240.2111 - loglik: -2.3795e+02 - logprior: -2.2563e+00
Epoch 7/10
11/11 - 2s - loss: 239.1077 - loglik: -2.3690e+02 - logprior: -2.2079e+00
Epoch 8/10
11/11 - 2s - loss: 237.7214 - loglik: -2.3560e+02 - logprior: -2.1247e+00
Epoch 9/10
11/11 - 2s - loss: 238.5668 - loglik: -2.3645e+02 - logprior: -2.1141e+00
Fitted a model with MAP estimate = -237.6359
expansions: [(8, 2), (9, 3), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (47, 1), (62, 3), (64, 1), (65, 1), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Fitting a model of length 112 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 251.0593 - loglik: -2.3693e+02 - logprior: -1.4126e+01
Epoch 2/2
11/11 - 2s - loss: 227.4689 - loglik: -2.2171e+02 - logprior: -5.7560e+00
Fitted a model with MAP estimate = -224.0110
expansions: [(0, 21)]
discards: [0 8]
Fitting a model of length 131 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 232.7506 - loglik: -2.2115e+02 - logprior: -1.1598e+01
Epoch 2/2
11/11 - 3s - loss: 217.2103 - loglik: -2.1404e+02 - logprior: -3.1669e+00
Fitted a model with MAP estimate = -214.0586
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 230.3077 - loglik: -2.1793e+02 - logprior: -1.2374e+01
Epoch 2/10
11/11 - 2s - loss: 219.6171 - loglik: -2.1675e+02 - logprior: -2.8663e+00
Epoch 3/10
11/11 - 2s - loss: 216.7532 - loglik: -2.1564e+02 - logprior: -1.1127e+00
Epoch 4/10
11/11 - 2s - loss: 215.4274 - loglik: -2.1482e+02 - logprior: -6.0251e-01
Epoch 5/10
11/11 - 2s - loss: 214.0371 - loglik: -2.1360e+02 - logprior: -4.4067e-01
Epoch 6/10
11/11 - 2s - loss: 214.8373 - loglik: -2.1451e+02 - logprior: -3.2635e-01
Fitted a model with MAP estimate = -213.3592
Time for alignment: 62.3691
Computed alignments with likelihoods: ['-213.8811', '-212.7134', '-213.3592']
Best model has likelihood: -212.7134
SP score = 0.5492
Training of 3 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c3313a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fabd2f40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 811.2870 - loglik: -8.0879e+02 - logprior: -2.4925e+00
Epoch 2/10
19/19 - 15s - loss: 705.7642 - loglik: -7.0482e+02 - logprior: -9.4878e-01
Epoch 3/10
19/19 - 15s - loss: 643.0594 - loglik: -6.4144e+02 - logprior: -1.6162e+00
Epoch 4/10
19/19 - 15s - loss: 628.7946 - loglik: -6.2697e+02 - logprior: -1.8223e+00
Epoch 5/10
19/19 - 15s - loss: 625.3388 - loglik: -6.2346e+02 - logprior: -1.8795e+00
Epoch 6/10
19/19 - 15s - loss: 624.1659 - loglik: -6.2235e+02 - logprior: -1.8207e+00
Epoch 7/10
19/19 - 15s - loss: 623.5512 - loglik: -6.2176e+02 - logprior: -1.7916e+00
Epoch 8/10
19/19 - 15s - loss: 622.3973 - loglik: -6.2062e+02 - logprior: -1.7749e+00
Epoch 9/10
19/19 - 14s - loss: 622.4269 - loglik: -6.2066e+02 - logprior: -1.7664e+00
Fitted a model with MAP estimate = -580.2508
expansions: [(12, 4), (14, 1), (16, 1), (17, 1), (36, 1), (37, 1), (39, 1), (45, 3), (46, 1), (58, 6), (59, 3), (67, 1), (68, 1), (69, 1), (96, 2), (99, 2), (117, 2), (118, 2), (119, 1), (120, 3), (121, 5), (122, 2), (123, 2), (125, 1), (131, 1), (138, 1), (139, 1), (145, 1), (151, 1), (160, 1), (163, 1), (164, 1), (167, 2), (168, 4), (169, 2), (170, 2), (173, 1), (180, 2), (181, 1), (193, 1), (194, 1), (202, 1), (209, 2), (210, 1), (215, 1), (217, 1), (218, 1)]
discards: [0]
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 615.9020 - loglik: -6.1312e+02 - logprior: -2.7828e+00
Epoch 2/2
39/39 - 24s - loss: 597.7299 - loglik: -5.9657e+02 - logprior: -1.1559e+00
Fitted a model with MAP estimate = -553.7858
expansions: [(0, 2)]
discards: [  0  11  12 121 126 148 150 159 160 164 246 282]
Fitting a model of length 292 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 600.2323 - loglik: -5.9858e+02 - logprior: -1.6562e+00
Epoch 2/2
39/39 - 23s - loss: 596.1124 - loglik: -5.9552e+02 - logprior: -5.9189e-01
Fitted a model with MAP estimate = -552.9261
expansions: []
discards: [  0 225]
Fitting a model of length 290 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 555.3787 - loglik: -5.5360e+02 - logprior: -1.7825e+00
Epoch 2/10
45/45 - 26s - loss: 551.0355 - loglik: -5.5044e+02 - logprior: -5.9182e-01
Epoch 3/10
45/45 - 26s - loss: 549.7818 - loglik: -5.4927e+02 - logprior: -5.1639e-01
Epoch 4/10
45/45 - 26s - loss: 545.5803 - loglik: -5.4506e+02 - logprior: -5.1776e-01
Epoch 5/10
45/45 - 26s - loss: 546.3198 - loglik: -5.4590e+02 - logprior: -4.1575e-01
Fitted a model with MAP estimate = -545.4680
Time for alignment: 507.6055
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 811.7278 - loglik: -8.0924e+02 - logprior: -2.4904e+00
Epoch 2/10
19/19 - 15s - loss: 709.2629 - loglik: -7.0836e+02 - logprior: -9.0725e-01
Epoch 3/10
19/19 - 15s - loss: 644.2258 - loglik: -6.4264e+02 - logprior: -1.5878e+00
Epoch 4/10
19/19 - 15s - loss: 632.4454 - loglik: -6.3068e+02 - logprior: -1.7651e+00
Epoch 5/10
19/19 - 15s - loss: 629.4342 - loglik: -6.2764e+02 - logprior: -1.7912e+00
Epoch 6/10
19/19 - 15s - loss: 627.1326 - loglik: -6.2539e+02 - logprior: -1.7400e+00
Epoch 7/10
19/19 - 15s - loss: 626.9406 - loglik: -6.2521e+02 - logprior: -1.7311e+00
Epoch 8/10
19/19 - 15s - loss: 627.6786 - loglik: -6.2595e+02 - logprior: -1.7276e+00
Fitted a model with MAP estimate = -583.2402
expansions: [(12, 3), (14, 1), (16, 1), (17, 1), (32, 1), (35, 1), (36, 1), (45, 3), (46, 1), (58, 2), (59, 2), (62, 4), (63, 1), (66, 1), (67, 1), (68, 1), (70, 1), (97, 2), (119, 1), (124, 2), (125, 3), (126, 2), (129, 1), (136, 3), (137, 3), (138, 6), (139, 2), (140, 2), (142, 2), (143, 1), (157, 2), (158, 2), (163, 1), (164, 1), (167, 2), (168, 4), (169, 2), (170, 2), (181, 2), (182, 1), (191, 1), (193, 1), (203, 1), (210, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [0]
Fitting a model of length 309 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 616.9982 - loglik: -6.1418e+02 - logprior: -2.8159e+00
Epoch 2/2
39/39 - 25s - loss: 597.1552 - loglik: -5.9597e+02 - logprior: -1.1836e+00
Fitted a model with MAP estimate = -554.5833
expansions: [(0, 2), (153, 1)]
discards: [  0  11  77  78 122 159 173 177 187 189 195 215 239 253 292 293]
Fitting a model of length 296 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 598.8830 - loglik: -5.9725e+02 - logprior: -1.6288e+00
Epoch 2/2
39/39 - 24s - loss: 594.2253 - loglik: -5.9367e+02 - logprior: -5.5324e-01
Fitted a model with MAP estimate = -551.8536
expansions: []
discards: [  0 154 155 156 202 203 204 210]
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 556.8318 - loglik: -5.5509e+02 - logprior: -1.7391e+00
Epoch 2/10
45/45 - 26s - loss: 550.9489 - loglik: -5.5035e+02 - logprior: -5.9889e-01
Epoch 3/10
45/45 - 26s - loss: 550.5273 - loglik: -5.4999e+02 - logprior: -5.3857e-01
Epoch 4/10
45/45 - 26s - loss: 547.6550 - loglik: -5.4711e+02 - logprior: -5.4193e-01
Epoch 5/10
45/45 - 26s - loss: 546.4445 - loglik: -5.4601e+02 - logprior: -4.3241e-01
Epoch 6/10
45/45 - 26s - loss: 546.2723 - loglik: -5.4586e+02 - logprior: -4.1015e-01
Epoch 7/10
45/45 - 26s - loss: 545.8618 - loglik: -5.4559e+02 - logprior: -2.7291e-01
Epoch 8/10
45/45 - 25s - loss: 545.3217 - loglik: -5.4508e+02 - logprior: -2.3822e-01
Epoch 9/10
45/45 - 26s - loss: 545.5836 - loglik: -5.4544e+02 - logprior: -1.4619e-01
Fitted a model with MAP estimate = -545.5558
Time for alignment: 598.0083
Fitting a model of length 224 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 19s - loss: 811.5402 - loglik: -8.0905e+02 - logprior: -2.4887e+00
Epoch 2/10
19/19 - 15s - loss: 711.9344 - loglik: -7.1112e+02 - logprior: -8.1613e-01
Epoch 3/10
19/19 - 15s - loss: 648.9739 - loglik: -6.4773e+02 - logprior: -1.2487e+00
Epoch 4/10
19/19 - 15s - loss: 631.6528 - loglik: -6.3034e+02 - logprior: -1.3151e+00
Epoch 5/10
19/19 - 15s - loss: 626.3320 - loglik: -6.2497e+02 - logprior: -1.3597e+00
Epoch 6/10
19/19 - 15s - loss: 624.0421 - loglik: -6.2270e+02 - logprior: -1.3415e+00
Epoch 7/10
19/19 - 15s - loss: 623.2298 - loglik: -6.2189e+02 - logprior: -1.3366e+00
Epoch 8/10
19/19 - 15s - loss: 623.3174 - loglik: -6.2199e+02 - logprior: -1.3317e+00
Fitted a model with MAP estimate = -579.7053
expansions: [(12, 3), (14, 1), (16, 1), (20, 1), (36, 1), (37, 1), (39, 1), (45, 3), (46, 1), (59, 2), (60, 1), (62, 5), (67, 2), (68, 2), (69, 2), (91, 1), (127, 2), (128, 5), (129, 2), (130, 2), (142, 2), (163, 1), (167, 5), (168, 12), (169, 2), (179, 1), (180, 2), (181, 1), (193, 1), (194, 1), (206, 1), (212, 2), (213, 2), (215, 1), (217, 1), (218, 1)]
discards: [  0 164]
Fitting a model of length 297 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 616.2953 - loglik: -6.1356e+02 - logprior: -2.7343e+00
Epoch 2/2
39/39 - 24s - loss: 598.1781 - loglik: -5.9707e+02 - logprior: -1.1079e+00
Fitted a model with MAP estimate = -554.9460
expansions: [(0, 2), (212, 2), (218, 1), (221, 1)]
discards: [  0  12  77  78  88  93 158 159 160 163 207 208 240 279 281]
Fitting a model of length 288 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 601.8334 - loglik: -6.0019e+02 - logprior: -1.6429e+00
Epoch 2/2
39/39 - 23s - loss: 597.2602 - loglik: -5.9664e+02 - logprior: -6.1902e-01
Fitted a model with MAP estimate = -554.2197
expansions: [(200, 1), (209, 1)]
discards: [0]
Fitting a model of length 289 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 30s - loss: 555.4989 - loglik: -5.5382e+02 - logprior: -1.6813e+00
Epoch 2/10
45/45 - 27s - loss: 549.6862 - loglik: -5.4905e+02 - logprior: -6.3677e-01
Epoch 3/10
45/45 - 26s - loss: 550.4791 - loglik: -5.4990e+02 - logprior: -5.7960e-01
Fitted a model with MAP estimate = -547.1473
Time for alignment: 434.8289
Computed alignments with likelihoods: ['-545.4680', '-545.5558', '-547.1473']
Best model has likelihood: -545.4680
SP score = 0.9080
Training of 3 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25d5de850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2537fafd0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 446.0729 - loglik: -4.4502e+02 - logprior: -1.0538e+00
Epoch 2/10
30/30 - 7s - loss: 375.7031 - loglik: -3.7458e+02 - logprior: -1.1220e+00
Epoch 3/10
30/30 - 7s - loss: 363.0042 - loglik: -3.6190e+02 - logprior: -1.1041e+00
Epoch 4/10
30/30 - 7s - loss: 360.4614 - loglik: -3.5935e+02 - logprior: -1.1092e+00
Epoch 5/10
30/30 - 7s - loss: 359.7496 - loglik: -3.5866e+02 - logprior: -1.0865e+00
Epoch 6/10
30/30 - 7s - loss: 358.7092 - loglik: -3.5764e+02 - logprior: -1.0726e+00
Epoch 7/10
30/30 - 7s - loss: 358.5924 - loglik: -3.5753e+02 - logprior: -1.0659e+00
Epoch 8/10
30/30 - 6s - loss: 358.2591 - loglik: -3.5720e+02 - logprior: -1.0593e+00
Epoch 9/10
30/30 - 7s - loss: 358.7774 - loglik: -3.5772e+02 - logprior: -1.0598e+00
Fitted a model with MAP estimate = -348.5217
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (73, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (98, 2), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Fitting a model of length 179 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 14s - loss: 357.6906 - loglik: -3.5622e+02 - logprior: -1.4679e+00
Epoch 2/2
30/30 - 9s - loss: 347.5858 - loglik: -3.4666e+02 - logprior: -9.2533e-01
Fitted a model with MAP estimate = -335.1352
expansions: []
discards: [ 30  51  92  94 102 127 134 148 152 155]
Fitting a model of length 169 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 12s - loss: 348.1378 - loglik: -3.4691e+02 - logprior: -1.2244e+00
Epoch 2/2
30/30 - 9s - loss: 346.1801 - loglik: -3.4529e+02 - logprior: -8.8639e-01
Fitted a model with MAP estimate = -334.8643
expansions: []
discards: [47]
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 15s - loss: 333.5210 - loglik: -3.3270e+02 - logprior: -8.1701e-01
Epoch 2/10
43/43 - 12s - loss: 333.4217 - loglik: -3.3277e+02 - logprior: -6.5218e-01
Epoch 3/10
43/43 - 12s - loss: 332.2483 - loglik: -3.3161e+02 - logprior: -6.3744e-01
Epoch 4/10
43/43 - 12s - loss: 331.5785 - loglik: -3.3096e+02 - logprior: -6.2227e-01
Epoch 5/10
43/43 - 12s - loss: 330.8130 - loglik: -3.3020e+02 - logprior: -6.1746e-01
Epoch 6/10
43/43 - 12s - loss: 329.9919 - loglik: -3.2938e+02 - logprior: -6.1196e-01
Epoch 7/10
43/43 - 12s - loss: 329.8757 - loglik: -3.2927e+02 - logprior: -6.0606e-01
Epoch 8/10
43/43 - 12s - loss: 329.1522 - loglik: -3.2855e+02 - logprior: -6.0308e-01
Epoch 9/10
43/43 - 12s - loss: 329.4509 - loglik: -3.2886e+02 - logprior: -5.9011e-01
Fitted a model with MAP estimate = -329.5478
Time for alignment: 328.3544
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 10s - loss: 445.5273 - loglik: -4.4448e+02 - logprior: -1.0477e+00
Epoch 2/10
30/30 - 7s - loss: 375.3046 - loglik: -3.7420e+02 - logprior: -1.1065e+00
Epoch 3/10
30/30 - 7s - loss: 363.5122 - loglik: -3.6241e+02 - logprior: -1.0992e+00
Epoch 4/10
30/30 - 7s - loss: 361.1684 - loglik: -3.6007e+02 - logprior: -1.0976e+00
Epoch 5/10
30/30 - 7s - loss: 360.4692 - loglik: -3.5939e+02 - logprior: -1.0811e+00
Epoch 6/10
30/30 - 7s - loss: 359.8201 - loglik: -3.5875e+02 - logprior: -1.0703e+00
Epoch 7/10
30/30 - 7s - loss: 359.0695 - loglik: -3.5801e+02 - logprior: -1.0600e+00
Epoch 8/10
30/30 - 7s - loss: 359.3575 - loglik: -3.5830e+02 - logprior: -1.0570e+00
Fitted a model with MAP estimate = -348.9917
expansions: [(14, 1), (15, 1), (16, 1), (23, 2), (26, 2), (29, 1), (34, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 1), (48, 1), (52, 1), (54, 1), (56, 1), (72, 2), (73, 2), (75, 1), (76, 1), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Fitting a model of length 178 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 12s - loss: 357.4801 - loglik: -3.5602e+02 - logprior: -1.4585e+00
Epoch 2/2
30/30 - 10s - loss: 347.3042 - loglik: -3.4637e+02 - logprior: -9.3334e-01
Fitted a model with MAP estimate = -334.9784
expansions: []
discards: [ 25  30  49  52  93  95 133 147 151 155]
Fitting a model of length 168 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 13s - loss: 347.6346 - loglik: -3.4640e+02 - logprior: -1.2393e+00
Epoch 2/2
30/30 - 9s - loss: 346.7443 - loglik: -3.4585e+02 - logprior: -8.9037e-01
Fitted a model with MAP estimate = -335.0120
expansions: []
discards: []
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 15s - loss: 333.5808 - loglik: -3.3276e+02 - logprior: -8.2253e-01
Epoch 2/10
43/43 - 12s - loss: 333.2697 - loglik: -3.3262e+02 - logprior: -6.4946e-01
Epoch 3/10
43/43 - 12s - loss: 332.1184 - loglik: -3.3148e+02 - logprior: -6.3759e-01
Epoch 4/10
43/43 - 12s - loss: 331.7091 - loglik: -3.3108e+02 - logprior: -6.2759e-01
Epoch 5/10
43/43 - 12s - loss: 331.2106 - loglik: -3.3059e+02 - logprior: -6.1866e-01
Epoch 6/10
43/43 - 12s - loss: 329.6903 - loglik: -3.2908e+02 - logprior: -6.1525e-01
Epoch 7/10
43/43 - 12s - loss: 329.9698 - loglik: -3.2936e+02 - logprior: -6.0835e-01
Fitted a model with MAP estimate = -329.3697
Time for alignment: 297.8481
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 446.4586 - loglik: -4.4540e+02 - logprior: -1.0548e+00
Epoch 2/10
30/30 - 6s - loss: 374.9159 - loglik: -3.7378e+02 - logprior: -1.1321e+00
Epoch 3/10
30/30 - 7s - loss: 363.2362 - loglik: -3.6213e+02 - logprior: -1.1060e+00
Epoch 4/10
30/30 - 6s - loss: 361.2363 - loglik: -3.6014e+02 - logprior: -1.0961e+00
Epoch 5/10
30/30 - 7s - loss: 361.2428 - loglik: -3.6017e+02 - logprior: -1.0689e+00
Fitted a model with MAP estimate = -348.6426
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (26, 2), (29, 1), (36, 1), (38, 1), (39, 2), (41, 2), (42, 2), (43, 2), (48, 1), (52, 1), (55, 1), (56, 1), (72, 2), (74, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 1), (125, 1), (127, 1), (128, 1)]
discards: [0]
Fitting a model of length 179 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 13s - loss: 357.8156 - loglik: -3.5635e+02 - logprior: -1.4677e+00
Epoch 2/2
30/30 - 9s - loss: 347.3468 - loglik: -3.4642e+02 - logprior: -9.2930e-01
Fitted a model with MAP estimate = -334.8841
expansions: []
discards: [ 29  48  51  57  93  96 103 134 148 152 155]
Fitting a model of length 168 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
30/30 - 12s - loss: 348.4471 - loglik: -3.4721e+02 - logprior: -1.2357e+00
Epoch 2/2
30/30 - 9s - loss: 345.4981 - loglik: -3.4460e+02 - logprior: -8.9442e-01
Fitted a model with MAP estimate = -335.0421
expansions: []
discards: []
Fitting a model of length 168 on 50157 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 15s - loss: 334.0969 - loglik: -3.3327e+02 - logprior: -8.2718e-01
Epoch 2/10
43/43 - 12s - loss: 332.9674 - loglik: -3.3231e+02 - logprior: -6.5407e-01
Epoch 3/10
43/43 - 12s - loss: 331.5931 - loglik: -3.3095e+02 - logprior: -6.3850e-01
Epoch 4/10
43/43 - 12s - loss: 331.5721 - loglik: -3.3094e+02 - logprior: -6.3457e-01
Epoch 5/10
43/43 - 12s - loss: 331.1237 - loglik: -3.3050e+02 - logprior: -6.2277e-01
Epoch 6/10
43/43 - 12s - loss: 329.7949 - loglik: -3.2917e+02 - logprior: -6.2035e-01
Epoch 7/10
43/43 - 12s - loss: 330.1069 - loglik: -3.2949e+02 - logprior: -6.1303e-01
Fitted a model with MAP estimate = -329.3859
Time for alignment: 277.5000
Computed alignments with likelihoods: ['-329.5478', '-329.3697', '-329.3859']
Best model has likelihood: -329.3697
SP score = 0.6765
Training of 3 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2aa6a1970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25da33b50>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 1236.3948 - loglik: -1.2350e+03 - logprior: -1.4161e+00
Epoch 2/10
40/40 - 39s - loss: 1112.6278 - loglik: -1.1103e+03 - logprior: -2.3123e+00
Epoch 3/10
40/40 - 39s - loss: 1100.9980 - loglik: -1.0986e+03 - logprior: -2.4017e+00
Epoch 4/10
40/40 - 39s - loss: 1093.9321 - loglik: -1.0914e+03 - logprior: -2.5472e+00
Epoch 5/10
40/40 - 40s - loss: 1089.2991 - loglik: -1.0867e+03 - logprior: -2.5804e+00
Epoch 6/10
40/40 - 39s - loss: 1087.6482 - loglik: -1.0851e+03 - logprior: -2.5854e+00
Epoch 7/10
40/40 - 40s - loss: 1087.6625 - loglik: -1.0851e+03 - logprior: -2.6031e+00
Fitted a model with MAP estimate = -822.5460
expansions: [(125, 1), (127, 2), (173, 1), (202, 1), (225, 2), (330, 5)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 209 210
 211 212 226 236 240 241 244 245 247 248 249 250 251 252 253 254 255 256
 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274
 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292
 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310
 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328
 329]
Fitting a model of length 125 on 10507 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 12s - loss: 1284.0665 - loglik: -1.2804e+03 - logprior: -3.6655e+00
Epoch 2/2
20/20 - 9s - loss: 1255.0817 - loglik: -1.2532e+03 - logprior: -1.9224e+00
Fitted a model with MAP estimate = -912.6748
expansions: [(5, 2), (10, 194), (50, 1), (125, 8)]
discards: [ 90  91  92  93  94  95  96 109 110 111 112 113 114 115 116 117 118 119
 120 121 122 123 124]
Fitting a model of length 307 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 38s - loss: 1173.5851 - loglik: -1.1722e+03 - logprior: -1.3709e+00
Epoch 2/2
40/40 - 35s - loss: 1122.0234 - loglik: -1.1217e+03 - logprior: -3.4881e-01
Fitted a model with MAP estimate = -823.6906
expansions: [(10, 127), (292, 2), (295, 1), (297, 2), (301, 1), (303, 1)]
discards: [  1  48 100 105 157 158 194]
Fitting a model of length 434 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 88s - loss: 787.3935 - loglik: -7.8651e+02 - logprior: -8.8198e-01
Epoch 2/10
56/56 - 85s - loss: 771.8809 - loglik: -7.7129e+02 - logprior: -5.9415e-01
Epoch 3/10
56/56 - 85s - loss: 762.4784 - loglik: -7.6187e+02 - logprior: -6.1072e-01
Epoch 4/10
56/56 - 85s - loss: 761.6417 - loglik: -7.6101e+02 - logprior: -6.3632e-01
Epoch 5/10
56/56 - 84s - loss: 759.5043 - loglik: -7.5889e+02 - logprior: -6.0990e-01
Epoch 6/10
56/56 - 86s - loss: 758.3604 - loglik: -7.5771e+02 - logprior: -6.4614e-01
Epoch 7/10
56/56 - 85s - loss: 755.6476 - loglik: -7.5502e+02 - logprior: -6.2776e-01
Epoch 8/10
56/56 - 85s - loss: 760.9454 - loglik: -7.6026e+02 - logprior: -6.8695e-01
Fitted a model with MAP estimate = -756.3249
Time for alignment: 1287.6594
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 43s - loss: 1246.4296 - loglik: -1.2454e+03 - logprior: -1.0549e+00
Epoch 2/10
40/40 - 39s - loss: 1144.1594 - loglik: -1.1442e+03 - logprior: 0.0843
Epoch 3/10
40/40 - 39s - loss: 1133.2991 - loglik: -1.1334e+03 - logprior: 0.1304
Epoch 4/10
40/40 - 39s - loss: 1127.3534 - loglik: -1.1275e+03 - logprior: 0.1576
Epoch 5/10
40/40 - 39s - loss: 1121.3085 - loglik: -1.1214e+03 - logprior: 0.1197
Epoch 6/10
40/40 - 39s - loss: 1118.7380 - loglik: -1.1188e+03 - logprior: 0.0795
Epoch 7/10
40/40 - 39s - loss: 1117.0973 - loglik: -1.1171e+03 - logprior: 0.0483
Epoch 8/10
40/40 - 39s - loss: 1117.3065 - loglik: -1.1173e+03 - logprior: 0.0273
Fitted a model with MAP estimate = -847.7593
expansions: [(118, 2), (123, 1), (124, 1), (138, 1), (293, 1), (296, 1), (298, 1), (304, 2), (323, 66), (326, 1), (329, 8), (330, 41)]
discards: [1]
Fitting a model of length 455 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 69s - loss: 1095.6617 - loglik: -1.0944e+03 - logprior: -1.2956e+00
Epoch 2/2
40/40 - 66s - loss: 1038.0862 - loglik: -1.0377e+03 - logprior: -3.5879e-01
Fitted a model with MAP estimate = -773.0926
expansions: [(413, 1), (442, 1), (449, 1)]
discards: [118 193 194 195 311 336 383 453 454]
Fitting a model of length 449 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 68s - loss: 1040.4242 - loglik: -1.0393e+03 - logprior: -1.1105e+00
Epoch 2/2
40/40 - 65s - loss: 1035.5089 - loglik: -1.0358e+03 - logprior: 0.2426
Fitted a model with MAP estimate = -774.1197
expansions: [(449, 2)]
discards: [431 432]
Fitting a model of length 449 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 93s - loss: 773.5292 - loglik: -7.7290e+02 - logprior: -6.2585e-01
Epoch 2/10
56/56 - 90s - loss: 762.9296 - loglik: -7.6268e+02 - logprior: -2.4730e-01
Epoch 3/10
56/56 - 90s - loss: 760.2657 - loglik: -7.5999e+02 - logprior: -2.7384e-01
Epoch 4/10
56/56 - 90s - loss: 760.0167 - loglik: -7.5975e+02 - logprior: -2.6884e-01
Epoch 5/10
56/56 - 90s - loss: 756.7787 - loglik: -7.5658e+02 - logprior: -2.0188e-01
Epoch 6/10
56/56 - 90s - loss: 753.1856 - loglik: -7.5301e+02 - logprior: -1.7581e-01
Epoch 7/10
56/56 - 90s - loss: 752.9595 - loglik: -7.5294e+02 - logprior: -2.2695e-02
Epoch 8/10
56/56 - 90s - loss: 756.9971 - loglik: -7.5683e+02 - logprior: -1.6574e-01
Fitted a model with MAP estimate = -752.6555
Time for alignment: 1729.2449
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 43s - loss: 1244.5331 - loglik: -1.2435e+03 - logprior: -1.0278e+00
Epoch 2/10
40/40 - 39s - loss: 1143.6262 - loglik: -1.1438e+03 - logprior: 0.1548
Epoch 3/10
40/40 - 39s - loss: 1134.4343 - loglik: -1.1346e+03 - logprior: 0.2010
Epoch 4/10
40/40 - 39s - loss: 1127.9006 - loglik: -1.1281e+03 - logprior: 0.2076
Epoch 5/10
40/40 - 40s - loss: 1123.3743 - loglik: -1.1236e+03 - logprior: 0.1899
Epoch 6/10
40/40 - 39s - loss: 1119.0881 - loglik: -1.1192e+03 - logprior: 0.1492
Epoch 7/10
40/40 - 39s - loss: 1119.2997 - loglik: -1.1194e+03 - logprior: 0.1024
Fitted a model with MAP estimate = -845.9796
expansions: [(124, 1), (125, 1), (156, 1), (162, 1), (202, 1), (297, 8), (298, 5), (317, 2), (318, 2), (319, 10), (327, 24), (328, 1), (330, 73)]
discards: [  1 191]
Fitting a model of length 458 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 70s - loss: 1089.6707 - loglik: -1.0882e+03 - logprior: -1.4690e+00
Epoch 2/2
40/40 - 67s - loss: 1037.8044 - loglik: -1.0373e+03 - logprior: -4.6675e-01
Fitted a model with MAP estimate = -773.1154
expansions: []
discards: [309 310 336 343 372 414 454]
Fitting a model of length 451 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 70s - loss: 1039.7330 - loglik: -1.0386e+03 - logprior: -1.1475e+00
Epoch 2/2
40/40 - 65s - loss: 1034.9364 - loglik: -1.0352e+03 - logprior: 0.2729
Fitted a model with MAP estimate = -773.5957
expansions: [(439, 1), (451, 2)]
discards: [332]
Fitting a model of length 453 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 94s - loss: 768.0483 - loglik: -7.6752e+02 - logprior: -5.3130e-01
Epoch 2/10
56/56 - 91s - loss: 767.9712 - loglik: -7.6757e+02 - logprior: -3.9965e-01
Epoch 3/10
56/56 - 92s - loss: 761.7604 - loglik: -7.6145e+02 - logprior: -3.1485e-01
Epoch 4/10
56/56 - 92s - loss: 757.2840 - loglik: -7.5692e+02 - logprior: -3.6717e-01
Epoch 5/10
56/56 - 91s - loss: 753.9214 - loglik: -7.5369e+02 - logprior: -2.2828e-01
Epoch 6/10
56/56 - 91s - loss: 757.3003 - loglik: -7.5707e+02 - logprior: -2.2774e-01
Fitted a model with MAP estimate = -753.5318
Time for alignment: 1525.0863
Computed alignments with likelihoods: ['-756.3249', '-752.6555', '-753.5318']
Best model has likelihood: -752.6555
SP score = 0.7677
Training of 3 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c7307c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2211b67f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.6069 - loglik: -4.4672e+02 - logprior: -2.8843e+00
Epoch 2/10
19/19 - 4s - loss: 281.8366 - loglik: -2.8042e+02 - logprior: -1.4199e+00
Epoch 3/10
19/19 - 4s - loss: 213.4655 - loglik: -2.1175e+02 - logprior: -1.7163e+00
Epoch 4/10
19/19 - 4s - loss: 203.1533 - loglik: -2.0130e+02 - logprior: -1.8533e+00
Epoch 5/10
19/19 - 4s - loss: 198.1121 - loglik: -1.9632e+02 - logprior: -1.7906e+00
Epoch 6/10
19/19 - 4s - loss: 197.6831 - loglik: -1.9593e+02 - logprior: -1.7581e+00
Epoch 7/10
19/19 - 4s - loss: 196.3472 - loglik: -1.9448e+02 - logprior: -1.8681e+00
Epoch 8/10
19/19 - 4s - loss: 193.3927 - loglik: -1.9145e+02 - logprior: -1.9406e+00
Epoch 9/10
19/19 - 4s - loss: 195.2082 - loglik: -1.9324e+02 - logprior: -1.9730e+00
Fitted a model with MAP estimate = -187.9584
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 2), (59, 3), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 2), (102, 2), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Fitting a model of length 189 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 185.6290 - loglik: -1.8166e+02 - logprior: -3.9725e+00
Epoch 2/2
19/19 - 5s - loss: 144.2423 - loglik: -1.4301e+02 - logprior: -1.2281e+00
Fitted a model with MAP estimate = -142.4442
expansions: []
discards: [  0  51  71  75  79 129 132 143 157 159 164]
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 150.6238 - loglik: -1.4689e+02 - logprior: -3.7297e+00
Epoch 2/2
19/19 - 5s - loss: 142.4821 - loglik: -1.4133e+02 - logprior: -1.1565e+00
Fitted a model with MAP estimate = -142.1132
expansions: []
discards: []
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 144.0941 - loglik: -1.4221e+02 - logprior: -1.8868e+00
Epoch 2/10
22/22 - 6s - loss: 141.5424 - loglik: -1.4082e+02 - logprior: -7.2102e-01
Epoch 3/10
22/22 - 6s - loss: 137.7407 - loglik: -1.3697e+02 - logprior: -7.7170e-01
Epoch 4/10
22/22 - 6s - loss: 135.9873 - loglik: -1.3517e+02 - logprior: -8.1630e-01
Epoch 5/10
22/22 - 6s - loss: 135.0513 - loglik: -1.3409e+02 - logprior: -9.6474e-01
Epoch 6/10
22/22 - 6s - loss: 132.8201 - loglik: -1.3189e+02 - logprior: -9.3203e-01
Epoch 7/10
22/22 - 6s - loss: 130.9927 - loglik: -1.3009e+02 - logprior: -8.9844e-01
Epoch 8/10
22/22 - 6s - loss: 130.8742 - loglik: -1.3001e+02 - logprior: -8.6007e-01
Epoch 9/10
22/22 - 6s - loss: 129.2946 - loglik: -1.2846e+02 - logprior: -8.3173e-01
Epoch 10/10
22/22 - 6s - loss: 131.3726 - loglik: -1.3058e+02 - logprior: -7.9489e-01
Fitted a model with MAP estimate = -129.9291
Time for alignment: 170.1079
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 449.4920 - loglik: -4.4661e+02 - logprior: -2.8840e+00
Epoch 2/10
19/19 - 4s - loss: 279.7177 - loglik: -2.7827e+02 - logprior: -1.4520e+00
Epoch 3/10
19/19 - 4s - loss: 211.6617 - loglik: -2.0988e+02 - logprior: -1.7836e+00
Epoch 4/10
19/19 - 4s - loss: 201.9248 - loglik: -2.0000e+02 - logprior: -1.9290e+00
Epoch 5/10
19/19 - 4s - loss: 199.2533 - loglik: -1.9739e+02 - logprior: -1.8650e+00
Epoch 6/10
19/19 - 4s - loss: 196.0352 - loglik: -1.9412e+02 - logprior: -1.9126e+00
Epoch 7/10
19/19 - 4s - loss: 196.1532 - loglik: -1.9410e+02 - logprior: -2.0561e+00
Fitted a model with MAP estimate = -188.1290
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Fitting a model of length 185 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 182.2417 - loglik: -1.7949e+02 - logprior: -2.7561e+00
Epoch 2/2
19/19 - 5s - loss: 145.1775 - loglik: -1.4416e+02 - logprior: -1.0165e+00
Fitted a model with MAP estimate = -143.5231
expansions: []
discards: [ 50  70  77 139 153 155 160]
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 147.0952 - loglik: -1.4446e+02 - logprior: -2.6378e+00
Epoch 2/2
19/19 - 5s - loss: 141.7931 - loglik: -1.4105e+02 - logprior: -7.4195e-01
Fitted a model with MAP estimate = -142.7883
expansions: []
discards: []
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 10s - loss: 144.8608 - loglik: -1.4303e+02 - logprior: -1.8285e+00
Epoch 2/10
22/22 - 6s - loss: 140.8172 - loglik: -1.4010e+02 - logprior: -7.1961e-01
Epoch 3/10
22/22 - 6s - loss: 138.6796 - loglik: -1.3794e+02 - logprior: -7.3645e-01
Epoch 4/10
22/22 - 6s - loss: 135.5322 - loglik: -1.3460e+02 - logprior: -9.3171e-01
Epoch 5/10
22/22 - 6s - loss: 133.3737 - loglik: -1.3241e+02 - logprior: -9.6304e-01
Epoch 6/10
22/22 - 6s - loss: 134.3314 - loglik: -1.3340e+02 - logprior: -9.2846e-01
Fitted a model with MAP estimate = -131.7373
Time for alignment: 138.7734
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 449.4283 - loglik: -4.4653e+02 - logprior: -2.8957e+00
Epoch 2/10
19/19 - 4s - loss: 279.8877 - loglik: -2.7842e+02 - logprior: -1.4665e+00
Epoch 3/10
19/19 - 4s - loss: 211.1915 - loglik: -2.0943e+02 - logprior: -1.7633e+00
Epoch 4/10
19/19 - 4s - loss: 201.5357 - loglik: -1.9963e+02 - logprior: -1.9098e+00
Epoch 5/10
19/19 - 4s - loss: 198.7464 - loglik: -1.9691e+02 - logprior: -1.8343e+00
Epoch 6/10
19/19 - 4s - loss: 197.0131 - loglik: -1.9513e+02 - logprior: -1.8786e+00
Epoch 7/10
19/19 - 4s - loss: 194.5154 - loglik: -1.9243e+02 - logprior: -2.0854e+00
Epoch 8/10
19/19 - 4s - loss: 193.8887 - loglik: -1.9170e+02 - logprior: -2.1895e+00
Epoch 9/10
19/19 - 4s - loss: 192.8264 - loglik: -1.9064e+02 - logprior: -2.1821e+00
Epoch 10/10
19/19 - 4s - loss: 191.8092 - loglik: -1.8962e+02 - logprior: -2.1873e+00
Fitted a model with MAP estimate = -187.5415
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 2), (58, 1), (59, 3), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (102, 2), (105, 1), (111, 2), (114, 1), (121, 2), (122, 2), (123, 1), (124, 2), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Fitting a model of length 186 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 182.8141 - loglik: -1.8008e+02 - logprior: -2.7300e+00
Epoch 2/2
19/19 - 5s - loss: 145.6540 - loglik: -1.4467e+02 - logprior: -9.8775e-01
Fitted a model with MAP estimate = -143.3767
expansions: []
discards: [ 50  70  77 128 140 154 156 161]
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 147.5379 - loglik: -1.4492e+02 - logprior: -2.6198e+00
Epoch 2/2
19/19 - 5s - loss: 141.2044 - loglik: -1.4046e+02 - logprior: -7.4113e-01
Fitted a model with MAP estimate = -142.0582
expansions: []
discards: []
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 144.6127 - loglik: -1.4275e+02 - logprior: -1.8628e+00
Epoch 2/10
22/22 - 6s - loss: 140.9925 - loglik: -1.4024e+02 - logprior: -7.5731e-01
Epoch 3/10
22/22 - 6s - loss: 137.7484 - loglik: -1.3694e+02 - logprior: -8.1268e-01
Epoch 4/10
22/22 - 6s - loss: 136.1265 - loglik: -1.3511e+02 - logprior: -1.0169e+00
Epoch 5/10
22/22 - 6s - loss: 134.8391 - loglik: -1.3387e+02 - logprior: -9.6506e-01
Epoch 6/10
22/22 - 6s - loss: 132.1547 - loglik: -1.3124e+02 - logprior: -9.1769e-01
Epoch 7/10
22/22 - 6s - loss: 131.7442 - loglik: -1.3086e+02 - logprior: -8.8368e-01
Epoch 8/10
22/22 - 6s - loss: 130.6594 - loglik: -1.2980e+02 - logprior: -8.5468e-01
Epoch 9/10
22/22 - 6s - loss: 130.0957 - loglik: -1.2928e+02 - logprior: -8.1748e-01
Epoch 10/10
22/22 - 6s - loss: 129.4083 - loglik: -1.2862e+02 - logprior: -7.8644e-01
Fitted a model with MAP estimate = -129.9427
Time for alignment: 171.5972
Computed alignments with likelihoods: ['-129.9291', '-131.7373', '-129.9427']
Best model has likelihood: -129.9291
SP score = 1.0000
Training of 3 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe27fee67f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2992a9850>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 615.0088 - loglik: -5.7860e+02 - logprior: -3.6407e+01
Epoch 2/10
10/10 - 4s - loss: 540.4540 - loglik: -5.3481e+02 - logprior: -5.6404e+00
Epoch 3/10
10/10 - 4s - loss: 486.3458 - loglik: -4.8527e+02 - logprior: -1.0780e+00
Epoch 4/10
10/10 - 4s - loss: 446.1220 - loglik: -4.4571e+02 - logprior: -4.1147e-01
Epoch 5/10
10/10 - 4s - loss: 430.2082 - loglik: -4.3031e+02 - logprior: 0.0989
Epoch 6/10
10/10 - 4s - loss: 425.3221 - loglik: -4.2578e+02 - logprior: 0.4567
Epoch 7/10
10/10 - 4s - loss: 423.0487 - loglik: -4.2380e+02 - logprior: 0.7501
Epoch 8/10
10/10 - 4s - loss: 421.9086 - loglik: -4.2284e+02 - logprior: 0.9287
Epoch 9/10
10/10 - 4s - loss: 420.5155 - loglik: -4.2150e+02 - logprior: 0.9860
Epoch 10/10
10/10 - 4s - loss: 420.7957 - loglik: -4.2180e+02 - logprior: 1.0021
Fitted a model with MAP estimate = -420.1129
expansions: [(11, 3), (19, 1), (30, 2), (31, 3), (32, 1), (41, 1), (61, 2), (62, 2), (63, 2), (75, 1), (76, 2), (77, 1), (78, 2), (86, 1), (90, 1), (91, 1), (93, 1), (101, 1), (102, 1), (121, 1), (127, 2), (129, 4), (137, 1), (139, 1), (146, 2), (147, 1), (149, 1), (152, 1), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Fitting a model of length 232 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 458.8536 - loglik: -4.1798e+02 - logprior: -4.0878e+01
Epoch 2/2
10/10 - 5s - loss: 418.7457 - loglik: -4.0537e+02 - logprior: -1.3378e+01
Fitted a model with MAP estimate = -412.1134
expansions: [(0, 4)]
discards: [  0  35  36  72 156 161 162 193 218]
Fitting a model of length 227 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 436.9387 - loglik: -4.0573e+02 - logprior: -3.1210e+01
Epoch 2/2
10/10 - 5s - loss: 405.8311 - loglik: -4.0229e+02 - logprior: -3.5406e+00
Fitted a model with MAP estimate = -400.5827
expansions: [(155, 5), (160, 1)]
discards: [1 2 3]
Fitting a model of length 230 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 433.0883 - loglik: -4.0290e+02 - logprior: -3.0188e+01
Epoch 2/10
10/10 - 5s - loss: 401.2484 - loglik: -3.9822e+02 - logprior: -3.0240e+00
Epoch 3/10
10/10 - 5s - loss: 396.0336 - loglik: -3.9867e+02 - logprior: 2.6368
Epoch 4/10
10/10 - 5s - loss: 392.4032 - loglik: -3.9743e+02 - logprior: 5.0238
Epoch 5/10
10/10 - 5s - loss: 391.8449 - loglik: -3.9825e+02 - logprior: 6.4099
Epoch 6/10
10/10 - 5s - loss: 390.3603 - loglik: -3.9761e+02 - logprior: 7.2504
Epoch 7/10
10/10 - 5s - loss: 390.4154 - loglik: -3.9820e+02 - logprior: 7.7822
Fitted a model with MAP estimate = -389.6269
Time for alignment: 124.1295
Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 615.8073 - loglik: -5.7940e+02 - logprior: -3.6406e+01
Epoch 2/10
10/10 - 4s - loss: 539.6302 - loglik: -5.3400e+02 - logprior: -5.6322e+00
Epoch 3/10
10/10 - 4s - loss: 482.2780 - loglik: -4.8114e+02 - logprior: -1.1349e+00
Epoch 4/10
10/10 - 4s - loss: 443.7463 - loglik: -4.4327e+02 - logprior: -4.7688e-01
Epoch 5/10
10/10 - 4s - loss: 430.7258 - loglik: -4.3091e+02 - logprior: 0.1795
Epoch 6/10
10/10 - 4s - loss: 426.7269 - loglik: -4.2750e+02 - logprior: 0.7715
Epoch 7/10
10/10 - 4s - loss: 426.0909 - loglik: -4.2717e+02 - logprior: 1.0815
Epoch 8/10
10/10 - 4s - loss: 424.8891 - loglik: -4.2611e+02 - logprior: 1.2229
Epoch 9/10
10/10 - 4s - loss: 424.8395 - loglik: -4.2617e+02 - logprior: 1.3272
Epoch 10/10
10/10 - 4s - loss: 422.4832 - loglik: -4.2384e+02 - logprior: 1.3603
Fitted a model with MAP estimate = -422.8872
expansions: [(11, 3), (21, 1), (30, 2), (31, 3), (32, 1), (41, 1), (45, 1), (60, 1), (61, 1), (62, 1), (63, 1), (75, 2), (76, 2), (77, 2), (86, 1), (88, 1), (90, 1), (92, 1), (101, 1), (102, 1), (120, 2), (126, 5), (127, 1), (129, 2), (130, 1), (139, 1), (148, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Fitting a model of length 231 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 461.9682 - loglik: -4.2107e+02 - logprior: -4.0895e+01
Epoch 2/2
10/10 - 5s - loss: 423.2777 - loglik: -4.0983e+02 - logprior: -1.3452e+01
Fitted a model with MAP estimate = -415.6513
expansions: [(0, 4), (185, 3)]
discards: [  0  35  36 149 157 217]
Fitting a model of length 232 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 439.2807 - loglik: -4.0790e+02 - logprior: -3.1383e+01
Epoch 2/2
10/10 - 5s - loss: 407.4494 - loglik: -4.0370e+02 - logprior: -3.7503e+00
Fitted a model with MAP estimate = -402.3644
expansions: []
discards: [  1   2   3  91  95 155 156 164 189]
Fitting a model of length 223 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 437.4302 - loglik: -4.0691e+02 - logprior: -3.0516e+01
Epoch 2/10
10/10 - 5s - loss: 406.8652 - loglik: -4.0337e+02 - logprior: -3.4916e+00
Epoch 3/10
10/10 - 5s - loss: 401.6029 - loglik: -4.0379e+02 - logprior: 2.1821
Epoch 4/10
10/10 - 5s - loss: 397.0651 - loglik: -4.0170e+02 - logprior: 4.6368
Epoch 5/10
10/10 - 5s - loss: 396.1589 - loglik: -4.0215e+02 - logprior: 5.9869
Epoch 6/10
10/10 - 5s - loss: 393.4992 - loglik: -4.0025e+02 - logprior: 6.7535
Epoch 7/10
10/10 - 5s - loss: 393.9666 - loglik: -4.0126e+02 - logprior: 7.2955
Fitted a model with MAP estimate = -392.4251
Time for alignment: 119.7988
Fitting a model of length 184 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 615.1419 - loglik: -5.7874e+02 - logprior: -3.6404e+01
Epoch 2/10
10/10 - 4s - loss: 540.6578 - loglik: -5.3503e+02 - logprior: -5.6308e+00
Epoch 3/10
10/10 - 4s - loss: 482.5582 - loglik: -4.8148e+02 - logprior: -1.0788e+00
Epoch 4/10
10/10 - 4s - loss: 445.7269 - loglik: -4.4547e+02 - logprior: -2.6138e-01
Epoch 5/10
10/10 - 4s - loss: 432.6581 - loglik: -4.3296e+02 - logprior: 0.3027
Epoch 6/10
10/10 - 4s - loss: 428.5544 - loglik: -4.2928e+02 - logprior: 0.7277
Epoch 7/10
10/10 - 4s - loss: 426.4786 - loglik: -4.2747e+02 - logprior: 0.9931
Epoch 8/10
10/10 - 4s - loss: 426.1015 - loglik: -4.2727e+02 - logprior: 1.1641
Epoch 9/10
10/10 - 4s - loss: 424.5797 - loglik: -4.2587e+02 - logprior: 1.2865
Epoch 10/10
10/10 - 4s - loss: 424.2267 - loglik: -4.2559e+02 - logprior: 1.3591
Fitted a model with MAP estimate = -424.3161
expansions: [(11, 3), (19, 1), (32, 2), (33, 1), (42, 1), (49, 1), (59, 1), (62, 2), (63, 2), (75, 1), (76, 2), (77, 2), (78, 1), (86, 1), (90, 1), (91, 1), (92, 1), (101, 1), (102, 1), (121, 1), (127, 2), (129, 3), (137, 2), (152, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: [0]
Fitting a model of length 225 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 463.7379 - loglik: -4.2275e+02 - logprior: -4.0992e+01
Epoch 2/2
10/10 - 5s - loss: 424.2134 - loglik: -4.1073e+02 - logprior: -1.3485e+01
Fitted a model with MAP estimate = -417.5758
expansions: [(0, 4)]
discards: [  0  91 153 168 185 211]
Fitting a model of length 223 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 9s - loss: 439.6851 - loglik: -4.0839e+02 - logprior: -3.1293e+01
Epoch 2/2
10/10 - 5s - loss: 407.9810 - loglik: -4.0432e+02 - logprior: -3.6647e+00
Fitted a model with MAP estimate = -402.3194
expansions: [(179, 3)]
discards: [1 2 3]
Fitting a model of length 223 on 1068 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 434.1454 - loglik: -4.0372e+02 - logprior: -3.0424e+01
Epoch 2/10
10/10 - 5s - loss: 405.2318 - loglik: -4.0200e+02 - logprior: -3.2345e+00
Epoch 3/10
10/10 - 5s - loss: 398.0308 - loglik: -4.0043e+02 - logprior: 2.3962
Epoch 4/10
10/10 - 5s - loss: 395.6426 - loglik: -4.0044e+02 - logprior: 4.7946
Epoch 5/10
10/10 - 5s - loss: 394.1092 - loglik: -4.0028e+02 - logprior: 6.1674
Epoch 6/10
10/10 - 5s - loss: 393.3026 - loglik: -4.0030e+02 - logprior: 7.0001
Epoch 7/10
10/10 - 5s - loss: 393.3322 - loglik: -4.0086e+02 - logprior: 7.5259
Fitted a model with MAP estimate = -392.4583
Time for alignment: 119.8727
Computed alignments with likelihoods: ['-389.6269', '-392.4251', '-392.4583']
Best model has likelihood: -389.6269
SP score = 0.9297
Training of 3 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2663e5790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2c3820910>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 643.7982 - loglik: -6.3105e+02 - logprior: -1.2753e+01
Epoch 2/10
10/10 - 6s - loss: 572.0398 - loglik: -5.6967e+02 - logprior: -2.3708e+00
Epoch 3/10
10/10 - 6s - loss: 508.1015 - loglik: -5.0663e+02 - logprior: -1.4693e+00
Epoch 4/10
10/10 - 6s - loss: 468.2279 - loglik: -4.6630e+02 - logprior: -1.9278e+00
Epoch 5/10
10/10 - 6s - loss: 449.1929 - loglik: -4.4694e+02 - logprior: -2.2545e+00
Epoch 6/10
10/10 - 6s - loss: 445.1160 - loglik: -4.4280e+02 - logprior: -2.3160e+00
Epoch 7/10
10/10 - 6s - loss: 440.0659 - loglik: -4.3788e+02 - logprior: -2.1835e+00
Epoch 8/10
10/10 - 6s - loss: 439.4185 - loglik: -4.3736e+02 - logprior: -2.0549e+00
Epoch 9/10
10/10 - 6s - loss: 437.4782 - loglik: -4.3546e+02 - logprior: -2.0224e+00
Epoch 10/10
10/10 - 6s - loss: 438.3268 - loglik: -4.3628e+02 - logprior: -2.0488e+00
Fitted a model with MAP estimate = -436.9140
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (37, 1), (38, 2), (39, 2), (40, 1), (50, 1), (60, 1), (63, 3), (65, 1), (66, 1), (75, 5), (76, 2), (78, 1), (80, 1), (96, 1), (100, 1), (119, 1), (120, 1), (121, 1), (122, 1), (145, 1), (146, 1), (147, 2), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 3), (171, 1), (177, 2)]
discards: [0]
Fitting a model of length 242 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 14s - loss: 432.3297 - loglik: -4.2250e+02 - logprior: -9.8314e+00
Epoch 2/2
21/21 - 9s - loss: 409.6665 - loglik: -4.0658e+02 - logprior: -3.0874e+00
Fitted a model with MAP estimate = -402.3670
expansions: []
discards: [ 13  18  28  49  83 188 219]
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 411.1873 - loglik: -4.0431e+02 - logprior: -6.8766e+00
Epoch 2/2
21/21 - 9s - loss: 402.4620 - loglik: -4.0271e+02 - logprior: 0.2475
Fitted a model with MAP estimate = -398.2926
expansions: []
discards: []
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 408.4136 - loglik: -4.0232e+02 - logprior: -6.0984e+00
Epoch 2/10
21/21 - 9s - loss: 399.1788 - loglik: -3.9970e+02 - logprior: 0.5238
Epoch 3/10
21/21 - 9s - loss: 397.1876 - loglik: -3.9848e+02 - logprior: 1.2957
Epoch 4/10
21/21 - 9s - loss: 396.0157 - loglik: -3.9769e+02 - logprior: 1.6736
Epoch 5/10
21/21 - 9s - loss: 395.2352 - loglik: -3.9707e+02 - logprior: 1.8299
Epoch 6/10
21/21 - 8s - loss: 393.8303 - loglik: -3.9582e+02 - logprior: 1.9903
Epoch 7/10
21/21 - 9s - loss: 394.8579 - loglik: -3.9701e+02 - logprior: 2.1557
Fitted a model with MAP estimate = -393.6180
Time for alignment: 197.8404
Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 644.8021 - loglik: -6.3205e+02 - logprior: -1.2749e+01
Epoch 2/10
10/10 - 6s - loss: 571.1566 - loglik: -5.6880e+02 - logprior: -2.3530e+00
Epoch 3/10
10/10 - 6s - loss: 509.6761 - loglik: -5.0819e+02 - logprior: -1.4845e+00
Epoch 4/10
10/10 - 6s - loss: 471.2645 - loglik: -4.6920e+02 - logprior: -2.0666e+00
Epoch 5/10
10/10 - 6s - loss: 454.4970 - loglik: -4.5205e+02 - logprior: -2.4494e+00
Epoch 6/10
10/10 - 6s - loss: 447.9684 - loglik: -4.4541e+02 - logprior: -2.5618e+00
Epoch 7/10
10/10 - 6s - loss: 443.4842 - loglik: -4.4102e+02 - logprior: -2.4636e+00
Epoch 8/10
10/10 - 6s - loss: 442.6566 - loglik: -4.4035e+02 - logprior: -2.3093e+00
Epoch 9/10
10/10 - 6s - loss: 442.8148 - loglik: -4.4057e+02 - logprior: -2.2403e+00
Fitted a model with MAP estimate = -441.3239
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (39, 2), (40, 1), (50, 1), (62, 1), (63, 3), (66, 1), (76, 7), (78, 2), (94, 1), (96, 1), (100, 1), (115, 1), (116, 1), (119, 1), (121, 1), (122, 1), (144, 1), (146, 2), (147, 1), (150, 1), (158, 1), (161, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Fitting a model of length 243 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 438.1883 - loglik: -4.2839e+02 - logprior: -9.7962e+00
Epoch 2/2
21/21 - 8s - loss: 413.2963 - loglik: -4.1036e+02 - logprior: -2.9319e+00
Fitted a model with MAP estimate = -408.1144
expansions: []
discards: [ 13  18  28  82 182 219]
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 417.4811 - loglik: -4.1066e+02 - logprior: -6.8171e+00
Epoch 2/2
21/21 - 9s - loss: 406.3762 - loglik: -4.0656e+02 - logprior: 0.1818
Fitted a model with MAP estimate = -404.4670
expansions: []
discards: [82]
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 414.1768 - loglik: -4.0801e+02 - logprior: -6.1653e+00
Epoch 2/10
21/21 - 9s - loss: 406.6242 - loglik: -4.0709e+02 - logprior: 0.4701
Epoch 3/10
21/21 - 9s - loss: 403.9464 - loglik: -4.0518e+02 - logprior: 1.2302
Epoch 4/10
21/21 - 8s - loss: 402.8495 - loglik: -4.0445e+02 - logprior: 1.6018
Epoch 5/10
21/21 - 9s - loss: 402.0516 - loglik: -4.0382e+02 - logprior: 1.7637
Epoch 6/10
21/21 - 8s - loss: 399.5378 - loglik: -4.0146e+02 - logprior: 1.9205
Epoch 7/10
21/21 - 9s - loss: 401.7422 - loglik: -4.0383e+02 - logprior: 2.0909
Fitted a model with MAP estimate = -400.0613
Time for alignment: 190.6908
Fitting a model of length 187 on 3127 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 9s - loss: 645.5743 - loglik: -6.3281e+02 - logprior: -1.2767e+01
Epoch 2/10
10/10 - 6s - loss: 570.3094 - loglik: -5.6796e+02 - logprior: -2.3537e+00
Epoch 3/10
10/10 - 6s - loss: 502.0477 - loglik: -5.0060e+02 - logprior: -1.4429e+00
Epoch 4/10
10/10 - 6s - loss: 461.1550 - loglik: -4.5917e+02 - logprior: -1.9887e+00
Epoch 5/10
10/10 - 5s - loss: 444.1110 - loglik: -4.4171e+02 - logprior: -2.4049e+00
Epoch 6/10
10/10 - 6s - loss: 435.9879 - loglik: -4.3344e+02 - logprior: -2.5513e+00
Epoch 7/10
10/10 - 6s - loss: 432.3167 - loglik: -4.2991e+02 - logprior: -2.4097e+00
Epoch 8/10
10/10 - 6s - loss: 431.0627 - loglik: -4.2875e+02 - logprior: -2.3149e+00
Epoch 9/10
10/10 - 6s - loss: 430.1833 - loglik: -4.2790e+02 - logprior: -2.2790e+00
Epoch 10/10
10/10 - 6s - loss: 428.3109 - loglik: -4.2600e+02 - logprior: -2.3143e+00
Fitted a model with MAP estimate = -428.5238
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (39, 2), (40, 1), (46, 1), (62, 1), (63, 3), (76, 7), (78, 1), (97, 1), (98, 1), (99, 1), (118, 2), (119, 1), (120, 1), (121, 1), (143, 1), (146, 1), (147, 2), (154, 1), (160, 1), (161, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 2), (171, 1), (177, 2)]
discards: [0]
Fitting a model of length 240 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 427.7760 - loglik: -4.1787e+02 - logprior: -9.9076e+00
Epoch 2/2
21/21 - 9s - loss: 401.1336 - loglik: -3.9804e+02 - logprior: -3.0903e+00
Fitted a model with MAP estimate = -396.4725
expansions: []
discards: [ 13  18  28  81 150 216]
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 11s - loss: 407.0300 - loglik: -4.0016e+02 - logprior: -6.8654e+00
Epoch 2/2
21/21 - 8s - loss: 394.2357 - loglik: -3.9443e+02 - logprior: 0.1922
Fitted a model with MAP estimate = -393.1128
expansions: [(93, 1)]
discards: []
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 403.3382 - loglik: -3.9728e+02 - logprior: -6.0549e+00
Epoch 2/10
21/21 - 9s - loss: 392.0319 - loglik: -3.9259e+02 - logprior: 0.5561
Epoch 3/10
21/21 - 9s - loss: 393.2576 - loglik: -3.9457e+02 - logprior: 1.3088
Fitted a model with MAP estimate = -390.5556
Time for alignment: 161.8516
Computed alignments with likelihoods: ['-393.6180', '-400.0613', '-390.5556']
Best model has likelihood: -390.5556
SP score = 0.5751
Training of 3 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f98f00d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe266793460>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 283.8246 - loglik: -2.8083e+02 - logprior: -2.9966e+00
Epoch 2/10
19/19 - 2s - loss: 253.4117 - loglik: -2.5253e+02 - logprior: -8.8288e-01
Epoch 3/10
19/19 - 2s - loss: 243.7309 - loglik: -2.4281e+02 - logprior: -9.2217e-01
Epoch 4/10
19/19 - 2s - loss: 240.9834 - loglik: -2.4017e+02 - logprior: -8.1769e-01
Epoch 5/10
19/19 - 2s - loss: 239.5898 - loglik: -2.3878e+02 - logprior: -8.0705e-01
Epoch 6/10
19/19 - 2s - loss: 239.4702 - loglik: -2.3868e+02 - logprior: -7.8736e-01
Epoch 7/10
19/19 - 2s - loss: 238.4643 - loglik: -2.3767e+02 - logprior: -7.9925e-01
Epoch 8/10
19/19 - 2s - loss: 238.1480 - loglik: -2.3734e+02 - logprior: -8.0420e-01
Epoch 9/10
19/19 - 2s - loss: 237.4296 - loglik: -2.3662e+02 - logprior: -8.1162e-01
Epoch 10/10
19/19 - 2s - loss: 237.8891 - loglik: -2.3708e+02 - logprior: -8.0893e-01
Fitted a model with MAP estimate = -236.6898
expansions: [(0, 10), (11, 2), (12, 4), (19, 1), (22, 2), (54, 1), (56, 3), (57, 1), (58, 1)]
discards: []
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 246.7542 - loglik: -2.4285e+02 - logprior: -3.9043e+00
Epoch 2/2
19/19 - 3s - loss: 238.0002 - loglik: -2.3685e+02 - logprior: -1.1477e+00
Fitted a model with MAP estimate = -235.6058
expansions: [(0, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 40 79 80]
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 242.1172 - loglik: -2.3846e+02 - logprior: -3.6600e+00
Epoch 2/2
19/19 - 3s - loss: 237.6996 - loglik: -2.3656e+02 - logprior: -1.1411e+00
Fitted a model with MAP estimate = -235.9953
expansions: [(0, 7)]
discards: [0 1 2 3 4 8]
Fitting a model of length 99 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 239.6697 - loglik: -2.3657e+02 - logprior: -3.1046e+00
Epoch 2/10
19/19 - 3s - loss: 236.3297 - loglik: -2.3543e+02 - logprior: -9.0158e-01
Epoch 3/10
19/19 - 3s - loss: 234.7891 - loglik: -2.3400e+02 - logprior: -7.8638e-01
Epoch 4/10
19/19 - 3s - loss: 234.8534 - loglik: -2.3414e+02 - logprior: -7.0956e-01
Fitted a model with MAP estimate = -233.5542
Time for alignment: 73.8148
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 284.1207 - loglik: -2.8109e+02 - logprior: -3.0275e+00
Epoch 2/10
19/19 - 2s - loss: 254.3722 - loglik: -2.5347e+02 - logprior: -8.9765e-01
Epoch 3/10
19/19 - 2s - loss: 244.6490 - loglik: -2.4372e+02 - logprior: -9.2630e-01
Epoch 4/10
19/19 - 2s - loss: 242.6041 - loglik: -2.4177e+02 - logprior: -8.3314e-01
Epoch 5/10
19/19 - 2s - loss: 241.0923 - loglik: -2.4026e+02 - logprior: -8.3567e-01
Epoch 6/10
19/19 - 2s - loss: 240.0912 - loglik: -2.3927e+02 - logprior: -8.2489e-01
Epoch 7/10
19/19 - 2s - loss: 239.4651 - loglik: -2.3866e+02 - logprior: -8.1006e-01
Epoch 8/10
19/19 - 2s - loss: 239.4531 - loglik: -2.3865e+02 - logprior: -8.0397e-01
Epoch 9/10
19/19 - 2s - loss: 239.1278 - loglik: -2.3833e+02 - logprior: -7.9991e-01
Epoch 10/10
19/19 - 2s - loss: 239.1581 - loglik: -2.3835e+02 - logprior: -8.0454e-01
Fitted a model with MAP estimate = -237.6974
expansions: [(0, 10), (16, 2), (20, 1), (23, 2), (30, 2), (57, 2), (58, 2), (59, 3), (61, 2)]
discards: []
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 248.9367 - loglik: -2.4511e+02 - logprior: -3.8229e+00
Epoch 2/2
19/19 - 3s - loss: 239.6159 - loglik: -2.3842e+02 - logprior: -1.1986e+00
Fitted a model with MAP estimate = -236.4778
expansions: [(0, 8), (26, 1), (27, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 37 46 76 77 82]
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 242.2617 - loglik: -2.3918e+02 - logprior: -3.0844e+00
Epoch 2/2
19/19 - 3s - loss: 237.7380 - loglik: -2.3681e+02 - logprior: -9.2611e-01
Fitted a model with MAP estimate = -235.8126
expansions: [(0, 8)]
discards: [ 1  2  3  4  5  6  7 80 81]
Fitting a model of length 97 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 241.2233 - loglik: -2.3776e+02 - logprior: -3.4654e+00
Epoch 2/10
19/19 - 3s - loss: 237.1007 - loglik: -2.3606e+02 - logprior: -1.0377e+00
Epoch 3/10
19/19 - 3s - loss: 235.9288 - loglik: -2.3501e+02 - logprior: -9.2259e-01
Epoch 4/10
19/19 - 3s - loss: 234.9923 - loglik: -2.3417e+02 - logprior: -8.2266e-01
Epoch 5/10
19/19 - 3s - loss: 234.0900 - loglik: -2.3332e+02 - logprior: -7.6650e-01
Epoch 6/10
19/19 - 3s - loss: 232.7788 - loglik: -2.3205e+02 - logprior: -7.2740e-01
Epoch 7/10
19/19 - 3s - loss: 232.6310 - loglik: -2.3192e+02 - logprior: -7.1532e-01
Epoch 8/10
19/19 - 3s - loss: 231.5859 - loglik: -2.3088e+02 - logprior: -7.1057e-01
Epoch 9/10
19/19 - 3s - loss: 232.2053 - loglik: -2.3152e+02 - logprior: -6.8765e-01
Fitted a model with MAP estimate = -231.6436
Time for alignment: 88.3410
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 283.1843 - loglik: -2.8018e+02 - logprior: -3.0067e+00
Epoch 2/10
19/19 - 2s - loss: 253.2595 - loglik: -2.5238e+02 - logprior: -8.8017e-01
Epoch 3/10
19/19 - 2s - loss: 243.7281 - loglik: -2.4283e+02 - logprior: -8.9419e-01
Epoch 4/10
19/19 - 2s - loss: 241.5563 - loglik: -2.4075e+02 - logprior: -8.1115e-01
Epoch 5/10
19/19 - 2s - loss: 240.2711 - loglik: -2.3947e+02 - logprior: -8.0250e-01
Epoch 6/10
19/19 - 2s - loss: 239.3815 - loglik: -2.3859e+02 - logprior: -7.8998e-01
Epoch 7/10
19/19 - 2s - loss: 238.7285 - loglik: -2.3794e+02 - logprior: -7.8890e-01
Epoch 8/10
19/19 - 2s - loss: 238.4768 - loglik: -2.3770e+02 - logprior: -7.8119e-01
Epoch 9/10
19/19 - 2s - loss: 237.8412 - loglik: -2.3705e+02 - logprior: -7.8914e-01
Epoch 10/10
19/19 - 2s - loss: 238.0178 - loglik: -2.3723e+02 - logprior: -7.8878e-01
Fitted a model with MAP estimate = -237.0150
expansions: [(0, 9), (11, 2), (12, 4), (13, 1), (14, 1), (54, 1), (57, 2), (59, 3), (61, 2)]
discards: []
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 246.8789 - loglik: -2.4278e+02 - logprior: -4.0965e+00
Epoch 2/2
19/19 - 3s - loss: 238.4329 - loglik: -2.3730e+02 - logprior: -1.1342e+00
Fitted a model with MAP estimate = -235.9813
expansions: [(0, 9)]
discards: [ 0  1  2  3  4  5  6  7  8 81]
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 241.3388 - loglik: -2.3824e+02 - logprior: -3.1003e+00
Epoch 2/2
19/19 - 3s - loss: 237.3176 - loglik: -2.3639e+02 - logprior: -9.3017e-01
Fitted a model with MAP estimate = -235.5997
expansions: [(0, 7)]
discards: [ 1  2  3  4  5  6  7  8  9 76 83 84]
Fitting a model of length 96 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 240.9078 - loglik: -2.3740e+02 - logprior: -3.5103e+00
Epoch 2/10
19/19 - 3s - loss: 237.1302 - loglik: -2.3614e+02 - logprior: -9.9138e-01
Epoch 3/10
19/19 - 3s - loss: 235.9850 - loglik: -2.3513e+02 - logprior: -8.5256e-01
Epoch 4/10
19/19 - 3s - loss: 234.6560 - loglik: -2.3391e+02 - logprior: -7.4882e-01
Epoch 5/10
19/19 - 3s - loss: 234.0895 - loglik: -2.3339e+02 - logprior: -7.0234e-01
Epoch 6/10
19/19 - 3s - loss: 233.2954 - loglik: -2.3261e+02 - logprior: -6.8273e-01
Epoch 7/10
19/19 - 3s - loss: 232.3548 - loglik: -2.3168e+02 - logprior: -6.7520e-01
Epoch 8/10
19/19 - 3s - loss: 232.3060 - loglik: -2.3165e+02 - logprior: -6.5514e-01
Epoch 9/10
19/19 - 3s - loss: 232.3892 - loglik: -2.3176e+02 - logprior: -6.2961e-01
Fitted a model with MAP estimate = -231.7406
Time for alignment: 88.7072
Computed alignments with likelihoods: ['-233.5542', '-231.6436', '-231.7406']
Best model has likelihood: -231.6436
SP score = 0.6069
Training of 3 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25d1e7430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb052f40>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 11s - loss: 770.8976 - loglik: -7.5307e+02 - logprior: -1.7823e+01
Epoch 2/10
10/10 - 7s - loss: 659.8439 - loglik: -6.5706e+02 - logprior: -2.7826e+00
Epoch 3/10
10/10 - 7s - loss: 550.3797 - loglik: -5.4884e+02 - logprior: -1.5386e+00
Epoch 4/10
10/10 - 8s - loss: 473.7112 - loglik: -4.7109e+02 - logprior: -2.6186e+00
Epoch 5/10
10/10 - 7s - loss: 447.6793 - loglik: -4.4431e+02 - logprior: -3.3661e+00
Epoch 6/10
10/10 - 7s - loss: 439.2709 - loglik: -4.3567e+02 - logprior: -3.6001e+00
Epoch 7/10
10/10 - 7s - loss: 432.9240 - loglik: -4.2938e+02 - logprior: -3.5434e+00
Epoch 8/10
10/10 - 7s - loss: 432.2166 - loglik: -4.2893e+02 - logprior: -3.2842e+00
Epoch 9/10
10/10 - 8s - loss: 431.1425 - loglik: -4.2807e+02 - logprior: -3.0684e+00
Epoch 10/10
10/10 - 7s - loss: 430.8492 - loglik: -4.2787e+02 - logprior: -2.9835e+00
Fitted a model with MAP estimate = -430.0947
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 1), (42, 1), (55, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (98, 1), (110, 1), (114, 2), (115, 3), (129, 2), (131, 1), (142, 1), (151, 1), (160, 1), (161, 1), (162, 1), (163, 1), (179, 1), (181, 2), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (207, 1), (209, 1), (212, 1)]
discards: [0]
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 11s - loss: 430.4795 - loglik: -4.1479e+02 - logprior: -1.5692e+01
Epoch 2/2
17/17 - 9s - loss: 391.8777 - loglik: -3.8753e+02 - logprior: -4.3505e+00
Fitted a model with MAP estimate = -387.2628
expansions: [(0, 9)]
discards: [  0 135 155]
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 14s - loss: 398.5479 - loglik: -3.8780e+02 - logprior: -1.0743e+01
Epoch 2/2
17/17 - 9s - loss: 381.0161 - loglik: -3.8140e+02 - logprior: 0.3794
Fitted a model with MAP estimate = -379.2542
expansions: []
discards: [1 2 3 4 5 6 7 8]
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 394.9516 - loglik: -3.8502e+02 - logprior: -9.9284e+00
Epoch 2/10
17/17 - 9s - loss: 380.6218 - loglik: -3.8188e+02 - logprior: 1.2551
Epoch 3/10
17/17 - 9s - loss: 378.9886 - loglik: -3.8181e+02 - logprior: 2.8177
Epoch 4/10
17/17 - 8s - loss: 375.4033 - loglik: -3.7903e+02 - logprior: 3.6240
Epoch 5/10
17/17 - 8s - loss: 374.1321 - loglik: -3.7813e+02 - logprior: 3.9981
Epoch 6/10
17/17 - 9s - loss: 372.4158 - loglik: -3.7665e+02 - logprior: 4.2375
Epoch 7/10
17/17 - 9s - loss: 373.8973 - loglik: -3.7841e+02 - logprior: 4.5101
Fitted a model with MAP estimate = -372.0672
Time for alignment: 208.6370
Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 12s - loss: 770.8439 - loglik: -7.5301e+02 - logprior: -1.7831e+01
Epoch 2/10
10/10 - 7s - loss: 661.5493 - loglik: -6.5876e+02 - logprior: -2.7879e+00
Epoch 3/10
10/10 - 7s - loss: 554.5213 - loglik: -5.5295e+02 - logprior: -1.5727e+00
Epoch 4/10
10/10 - 8s - loss: 477.3046 - loglik: -4.7458e+02 - logprior: -2.7264e+00
Epoch 5/10
10/10 - 7s - loss: 446.7712 - loglik: -4.4323e+02 - logprior: -3.5447e+00
Epoch 6/10
10/10 - 7s - loss: 436.7593 - loglik: -4.3305e+02 - logprior: -3.7083e+00
Epoch 7/10
10/10 - 8s - loss: 433.2138 - loglik: -4.2969e+02 - logprior: -3.5268e+00
Epoch 8/10
10/10 - 7s - loss: 431.4017 - loglik: -4.2817e+02 - logprior: -3.2332e+00
Epoch 9/10
10/10 - 7s - loss: 429.2745 - loglik: -4.2629e+02 - logprior: -2.9807e+00
Epoch 10/10
10/10 - 8s - loss: 428.8317 - loglik: -4.2594e+02 - logprior: -2.8944e+00
Fitted a model with MAP estimate = -428.7170
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 1), (43, 1), (55, 1), (57, 1), (58, 2), (60, 1), (61, 1), (87, 4), (95, 2), (97, 1), (100, 1), (114, 1), (115, 2), (116, 1), (130, 3), (131, 2), (140, 1), (141, 1), (160, 1), (161, 1), (163, 1), (173, 1), (179, 1), (180, 1), (181, 1), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 430.4061 - loglik: -4.1460e+02 - logprior: -1.5806e+01
Epoch 2/2
17/17 - 9s - loss: 391.8109 - loglik: -3.8732e+02 - logprior: -4.4894e+00
Fitted a model with MAP estimate = -387.3858
expansions: [(0, 10)]
discards: [  0 103 138 157 160]
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 398.3420 - loglik: -3.8738e+02 - logprior: -1.0959e+01
Epoch 2/2
17/17 - 9s - loss: 382.1687 - loglik: -3.8238e+02 - logprior: 0.2151
Fitted a model with MAP estimate = -379.0879
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 394.7463 - loglik: -3.8478e+02 - logprior: -9.9620e+00
Epoch 2/10
17/17 - 8s - loss: 380.6735 - loglik: -3.8190e+02 - logprior: 1.2274
Epoch 3/10
17/17 - 9s - loss: 377.7646 - loglik: -3.8056e+02 - logprior: 2.7926
Epoch 4/10
17/17 - 9s - loss: 373.8297 - loglik: -3.7743e+02 - logprior: 3.6007
Epoch 5/10
17/17 - 9s - loss: 376.5635 - loglik: -3.8053e+02 - logprior: 3.9638
Fitted a model with MAP estimate = -373.2989
Time for alignment: 193.8713
Fitting a model of length 219 on 2118 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 10s - loss: 769.9962 - loglik: -7.5217e+02 - logprior: -1.7822e+01
Epoch 2/10
10/10 - 7s - loss: 662.5879 - loglik: -6.5979e+02 - logprior: -2.8025e+00
Epoch 3/10
10/10 - 7s - loss: 553.6866 - loglik: -5.5207e+02 - logprior: -1.6127e+00
Epoch 4/10
10/10 - 8s - loss: 475.0663 - loglik: -4.7227e+02 - logprior: -2.7985e+00
Epoch 5/10
10/10 - 7s - loss: 445.3309 - loglik: -4.4165e+02 - logprior: -3.6774e+00
Epoch 6/10
10/10 - 7s - loss: 435.8450 - loglik: -4.3194e+02 - logprior: -3.9015e+00
Epoch 7/10
10/10 - 7s - loss: 432.8932 - loglik: -4.2902e+02 - logprior: -3.8708e+00
Epoch 8/10
10/10 - 7s - loss: 429.6478 - loglik: -4.2596e+02 - logprior: -3.6881e+00
Epoch 9/10
10/10 - 8s - loss: 429.9831 - loglik: -4.2649e+02 - logprior: -3.4968e+00
Fitted a model with MAP estimate = -428.0421
expansions: [(8, 1), (9, 1), (11, 1), (14, 1), (15, 1), (24, 1), (26, 1), (32, 1), (46, 1), (55, 1), (56, 1), (57, 1), (58, 1), (60, 1), (61, 1), (63, 1), (86, 1), (87, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 2), (116, 1), (123, 1), (129, 2), (131, 1), (142, 1), (151, 1), (160, 1), (161, 1), (162, 1), (163, 1), (176, 1), (178, 1), (180, 1), (184, 1), (188, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 430.9909 - loglik: -4.1524e+02 - logprior: -1.5752e+01
Epoch 2/2
17/17 - 9s - loss: 391.5565 - loglik: -3.8726e+02 - logprior: -4.2923e+00
Fitted a model with MAP estimate = -387.5821
expansions: [(0, 10)]
discards: [  0 137 155]
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 398.7660 - loglik: -3.8783e+02 - logprior: -1.0940e+01
Epoch 2/2
17/17 - 9s - loss: 382.2520 - loglik: -3.8249e+02 - logprior: 0.2385
Fitted a model with MAP estimate = -379.6483
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 394.6606 - loglik: -3.8465e+02 - logprior: -1.0007e+01
Epoch 2/10
17/17 - 9s - loss: 381.7030 - loglik: -3.8289e+02 - logprior: 1.1860
Epoch 3/10
17/17 - 9s - loss: 378.1422 - loglik: -3.8091e+02 - logprior: 2.7648
Epoch 4/10
17/17 - 8s - loss: 376.6933 - loglik: -3.8027e+02 - logprior: 3.5786
Epoch 5/10
17/17 - 9s - loss: 374.4498 - loglik: -3.7841e+02 - logprior: 3.9554
Epoch 6/10
17/17 - 9s - loss: 374.1005 - loglik: -3.7829e+02 - logprior: 4.1913
Epoch 7/10
17/17 - 9s - loss: 374.3772 - loglik: -3.7883e+02 - logprior: 4.4486
Fitted a model with MAP estimate = -372.4531
Time for alignment: 201.6919
Computed alignments with likelihoods: ['-372.0672', '-373.2989', '-372.4531']
Best model has likelihood: -372.0672
SP score = 0.9452
Training of 3 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2212704f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe24b6c0ac0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 179.4057 - loglik: -1.5939e+02 - logprior: -2.0014e+01
Epoch 2/10
10/10 - 1s - loss: 148.1114 - loglik: -1.4240e+02 - logprior: -5.7088e+00
Epoch 3/10
10/10 - 1s - loss: 130.9838 - loglik: -1.2772e+02 - logprior: -3.2652e+00
Epoch 4/10
10/10 - 1s - loss: 119.4390 - loglik: -1.1668e+02 - logprior: -2.7575e+00
Epoch 5/10
10/10 - 1s - loss: 113.8843 - loglik: -1.1119e+02 - logprior: -2.6991e+00
Epoch 6/10
10/10 - 1s - loss: 112.2829 - loglik: -1.0965e+02 - logprior: -2.6323e+00
Epoch 7/10
10/10 - 1s - loss: 111.6072 - loglik: -1.0910e+02 - logprior: -2.5117e+00
Epoch 8/10
10/10 - 1s - loss: 111.0995 - loglik: -1.0874e+02 - logprior: -2.3574e+00
Epoch 9/10
10/10 - 1s - loss: 110.9009 - loglik: -1.0863e+02 - logprior: -2.2680e+00
Epoch 10/10
10/10 - 1s - loss: 110.9478 - loglik: -1.0870e+02 - logprior: -2.2523e+00
Fitted a model with MAP estimate = -110.7933
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 135.1742 - loglik: -1.1276e+02 - logprior: -2.2414e+01
Epoch 2/2
10/10 - 1s - loss: 116.2194 - loglik: -1.0663e+02 - logprior: -9.5873e+00
Fitted a model with MAP estimate = -112.7811
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 121.6190 - loglik: -1.0385e+02 - logprior: -1.7770e+01
Epoch 2/2
10/10 - 1s - loss: 107.7475 - loglik: -1.0289e+02 - logprior: -4.8590e+00
Fitted a model with MAP estimate = -106.1135
expansions: []
discards: [0]
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.9603 - loglik: -1.0471e+02 - logprior: -2.0252e+01
Epoch 2/10
10/10 - 1s - loss: 110.0977 - loglik: -1.0431e+02 - logprior: -5.7925e+00
Epoch 3/10
10/10 - 1s - loss: 106.9481 - loglik: -1.0406e+02 - logprior: -2.8926e+00
Epoch 4/10
10/10 - 1s - loss: 105.6201 - loglik: -1.0358e+02 - logprior: -2.0445e+00
Epoch 5/10
10/10 - 1s - loss: 105.0599 - loglik: -1.0363e+02 - logprior: -1.4312e+00
Epoch 6/10
10/10 - 1s - loss: 104.9492 - loglik: -1.0378e+02 - logprior: -1.1717e+00
Epoch 7/10
10/10 - 1s - loss: 104.6842 - loglik: -1.0361e+02 - logprior: -1.0747e+00
Epoch 8/10
10/10 - 1s - loss: 104.6776 - loglik: -1.0376e+02 - logprior: -9.2120e-01
Epoch 9/10
10/10 - 1s - loss: 104.7807 - loglik: -1.0392e+02 - logprior: -8.5697e-01
Fitted a model with MAP estimate = -104.4580
Time for alignment: 30.8501
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 179.5625 - loglik: -1.5955e+02 - logprior: -2.0015e+01
Epoch 2/10
10/10 - 1s - loss: 147.9561 - loglik: -1.4225e+02 - logprior: -5.7053e+00
Epoch 3/10
10/10 - 1s - loss: 130.1085 - loglik: -1.2688e+02 - logprior: -3.2274e+00
Epoch 4/10
10/10 - 1s - loss: 119.3215 - loglik: -1.1670e+02 - logprior: -2.6220e+00
Epoch 5/10
10/10 - 1s - loss: 115.5012 - loglik: -1.1305e+02 - logprior: -2.4519e+00
Epoch 6/10
10/10 - 1s - loss: 114.2201 - loglik: -1.1190e+02 - logprior: -2.3214e+00
Epoch 7/10
10/10 - 1s - loss: 113.4444 - loglik: -1.1127e+02 - logprior: -2.1755e+00
Epoch 8/10
10/10 - 1s - loss: 113.1675 - loglik: -1.1113e+02 - logprior: -2.0413e+00
Epoch 9/10
10/10 - 1s - loss: 113.1007 - loglik: -1.1113e+02 - logprior: -1.9733e+00
Epoch 10/10
10/10 - 1s - loss: 112.9177 - loglik: -1.1095e+02 - logprior: -1.9651e+00
Fitted a model with MAP estimate = -112.8657
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (35, 4), (36, 1)]
discards: [0]
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 135.6068 - loglik: -1.1321e+02 - logprior: -2.2394e+01
Epoch 2/2
10/10 - 1s - loss: 116.7909 - loglik: -1.0717e+02 - logprior: -9.6238e+00
Fitted a model with MAP estimate = -113.1535
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 51]
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 121.9923 - loglik: -1.0419e+02 - logprior: -1.7806e+01
Epoch 2/2
10/10 - 1s - loss: 108.3813 - loglik: -1.0351e+02 - logprior: -4.8676e+00
Fitted a model with MAP estimate = -106.2300
expansions: []
discards: [0]
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 124.7488 - loglik: -1.0451e+02 - logprior: -2.0238e+01
Epoch 2/10
10/10 - 1s - loss: 110.4941 - loglik: -1.0470e+02 - logprior: -5.7893e+00
Epoch 3/10
10/10 - 1s - loss: 106.7014 - loglik: -1.0380e+02 - logprior: -2.8977e+00
Epoch 4/10
10/10 - 1s - loss: 105.7451 - loglik: -1.0368e+02 - logprior: -2.0641e+00
Epoch 5/10
10/10 - 1s - loss: 105.1939 - loglik: -1.0374e+02 - logprior: -1.4552e+00
Epoch 6/10
10/10 - 1s - loss: 104.7347 - loglik: -1.0356e+02 - logprior: -1.1792e+00
Epoch 7/10
10/10 - 1s - loss: 104.8199 - loglik: -1.0375e+02 - logprior: -1.0739e+00
Fitted a model with MAP estimate = -104.6224
Time for alignment: 26.7169
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 179.3746 - loglik: -1.5936e+02 - logprior: -2.0015e+01
Epoch 2/10
10/10 - 1s - loss: 148.1196 - loglik: -1.4241e+02 - logprior: -5.7099e+00
Epoch 3/10
10/10 - 1s - loss: 130.4048 - loglik: -1.2712e+02 - logprior: -3.2830e+00
Epoch 4/10
10/10 - 1s - loss: 118.8550 - loglik: -1.1607e+02 - logprior: -2.7888e+00
Epoch 5/10
10/10 - 1s - loss: 113.9032 - loglik: -1.1118e+02 - logprior: -2.7272e+00
Epoch 6/10
10/10 - 1s - loss: 112.1446 - loglik: -1.0949e+02 - logprior: -2.6582e+00
Epoch 7/10
10/10 - 1s - loss: 111.4980 - loglik: -1.0899e+02 - logprior: -2.5129e+00
Epoch 8/10
10/10 - 1s - loss: 111.1187 - loglik: -1.0876e+02 - logprior: -2.3607e+00
Epoch 9/10
10/10 - 1s - loss: 110.9745 - loglik: -1.0869e+02 - logprior: -2.2801e+00
Epoch 10/10
10/10 - 1s - loss: 110.9632 - loglik: -1.0870e+02 - logprior: -2.2621e+00
Fitted a model with MAP estimate = -110.7409
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 135.4382 - loglik: -1.1302e+02 - logprior: -2.2414e+01
Epoch 2/2
10/10 - 1s - loss: 115.8384 - loglik: -1.0624e+02 - logprior: -9.6028e+00
Fitted a model with MAP estimate = -112.8569
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 121.4860 - loglik: -1.0371e+02 - logprior: -1.7775e+01
Epoch 2/2
10/10 - 1s - loss: 108.1428 - loglik: -1.0328e+02 - logprior: -4.8657e+00
Fitted a model with MAP estimate = -106.1120
expansions: []
discards: [0]
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 124.7044 - loglik: -1.0446e+02 - logprior: -2.0249e+01
Epoch 2/10
10/10 - 1s - loss: 110.3490 - loglik: -1.0456e+02 - logprior: -5.7930e+00
Epoch 3/10
10/10 - 1s - loss: 106.8450 - loglik: -1.0395e+02 - logprior: -2.8982e+00
Epoch 4/10
10/10 - 1s - loss: 105.6501 - loglik: -1.0360e+02 - logprior: -2.0466e+00
Epoch 5/10
10/10 - 1s - loss: 105.2179 - loglik: -1.0378e+02 - logprior: -1.4342e+00
Epoch 6/10
10/10 - 1s - loss: 104.8196 - loglik: -1.0364e+02 - logprior: -1.1754e+00
Epoch 7/10
10/10 - 1s - loss: 104.8854 - loglik: -1.0381e+02 - logprior: -1.0802e+00
Fitted a model with MAP estimate = -104.6225
Time for alignment: 28.1003
Computed alignments with likelihoods: ['-104.4580', '-104.6224', '-104.6225']
Best model has likelihood: -104.4580
SP score = 0.9786
Training of 3 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe299076f70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2eeeaf0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 467.2343 - loglik: -4.6546e+02 - logprior: -1.7781e+00
Epoch 2/10
39/39 - 7s - loss: 388.4270 - loglik: -3.8731e+02 - logprior: -1.1138e+00
Epoch 3/10
39/39 - 8s - loss: 381.1210 - loglik: -3.8004e+02 - logprior: -1.0846e+00
Epoch 4/10
39/39 - 8s - loss: 378.5505 - loglik: -3.7750e+02 - logprior: -1.0538e+00
Epoch 5/10
39/39 - 8s - loss: 377.4823 - loglik: -3.7643e+02 - logprior: -1.0508e+00
Epoch 6/10
39/39 - 8s - loss: 376.8719 - loglik: -3.7581e+02 - logprior: -1.0633e+00
Epoch 7/10
39/39 - 8s - loss: 376.4551 - loglik: -3.7541e+02 - logprior: -1.0499e+00
Epoch 8/10
39/39 - 7s - loss: 376.4590 - loglik: -3.7540e+02 - logprior: -1.0600e+00
Fitted a model with MAP estimate = -310.8992
expansions: [(0, 13), (10, 1), (15, 1), (18, 1), (28, 1), (29, 2), (30, 3), (32, 1), (37, 1), (39, 1), (43, 1), (44, 1), (68, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 1), (108, 1), (112, 1), (126, 2), (130, 5)]
discards: []
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 363.6884 - loglik: -3.6104e+02 - logprior: -2.6503e+00
Epoch 2/2
39/39 - 10s - loss: 350.1513 - loglik: -3.4892e+02 - logprior: -1.2304e+00
Fitted a model with MAP estimate = -289.8465
expansions: [(99, 1), (160, 6)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11]
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 351.9420 - loglik: -3.5000e+02 - logprior: -1.9470e+00
Epoch 2/2
39/39 - 10s - loss: 345.6492 - loglik: -3.4488e+02 - logprior: -7.6477e-01
Fitted a model with MAP estimate = -287.4857
expansions: [(0, 14), (154, 3)]
discards: []
Fitting a model of length 190 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 16s - loss: 285.6743 - loglik: -2.8409e+02 - logprior: -1.5853e+00
Epoch 2/10
52/52 - 12s - loss: 277.7571 - loglik: -2.7656e+02 - logprior: -1.1967e+00
Epoch 3/10
52/52 - 13s - loss: 278.1032 - loglik: -2.7692e+02 - logprior: -1.1869e+00
Fitted a model with MAP estimate = -276.3923
Time for alignment: 211.6130
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 467.0362 - loglik: -4.6525e+02 - logprior: -1.7826e+00
Epoch 2/10
39/39 - 8s - loss: 386.0675 - loglik: -3.8503e+02 - logprior: -1.0343e+00
Epoch 3/10
39/39 - 8s - loss: 377.9160 - loglik: -3.7688e+02 - logprior: -1.0360e+00
Epoch 4/10
39/39 - 7s - loss: 375.7184 - loglik: -3.7468e+02 - logprior: -1.0361e+00
Epoch 5/10
39/39 - 8s - loss: 375.2840 - loglik: -3.7426e+02 - logprior: -1.0260e+00
Epoch 6/10
39/39 - 8s - loss: 374.0696 - loglik: -3.7304e+02 - logprior: -1.0261e+00
Epoch 7/10
39/39 - 8s - loss: 374.1521 - loglik: -3.7312e+02 - logprior: -1.0352e+00
Fitted a model with MAP estimate = -309.2386
expansions: [(0, 12), (10, 1), (15, 1), (18, 1), (19, 1), (27, 1), (28, 2), (32, 1), (37, 1), (42, 1), (44, 1), (56, 1), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (102, 1), (104, 1), (107, 1), (112, 1), (127, 11), (130, 3), (134, 4)]
discards: []
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 357.7088 - loglik: -3.5492e+02 - logprior: -2.7930e+00
Epoch 2/2
39/39 - 11s - loss: 341.2817 - loglik: -3.3980e+02 - logprior: -1.4814e+00
Fitted a model with MAP estimate = -283.1785
expansions: [(166, 2)]
discards: [  1   2   3   4   5   6   7   8   9  10  45 175 176 181 182 183 184]
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 350.4754 - loglik: -3.4839e+02 - logprior: -2.0875e+00
Epoch 2/2
39/39 - 10s - loss: 346.1165 - loglik: -3.4531e+02 - logprior: -8.0389e-01
Fitted a model with MAP estimate = -288.2305
expansions: [(0, 15), (156, 1), (170, 7)]
discards: []
Fitting a model of length 193 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 16s - loss: 284.2535 - loglik: -2.8255e+02 - logprior: -1.7033e+00
Epoch 2/10
52/52 - 12s - loss: 278.3920 - loglik: -2.7711e+02 - logprior: -1.2814e+00
Epoch 3/10
52/52 - 14s - loss: 275.8720 - loglik: -2.7465e+02 - logprior: -1.2222e+00
Epoch 4/10
52/52 - 12s - loss: 275.7653 - loglik: -2.7460e+02 - logprior: -1.1674e+00
Epoch 5/10
52/52 - 13s - loss: 275.2984 - loglik: -2.7419e+02 - logprior: -1.1050e+00
Epoch 6/10
52/52 - 13s - loss: 275.0244 - loglik: -2.7398e+02 - logprior: -1.0490e+00
Epoch 7/10
52/52 - 13s - loss: 271.6884 - loglik: -2.7070e+02 - logprior: -9.8929e-01
Epoch 8/10
52/52 - 13s - loss: 274.0274 - loglik: -2.7309e+02 - logprior: -9.3538e-01
Fitted a model with MAP estimate = -273.5059
Time for alignment: 272.6455
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 468.0747 - loglik: -4.6632e+02 - logprior: -1.7537e+00
Epoch 2/10
39/39 - 7s - loss: 384.8073 - loglik: -3.8384e+02 - logprior: -9.6910e-01
Epoch 3/10
39/39 - 8s - loss: 377.4191 - loglik: -3.7641e+02 - logprior: -1.0099e+00
Epoch 4/10
39/39 - 8s - loss: 375.2744 - loglik: -3.7429e+02 - logprior: -9.8348e-01
Epoch 5/10
39/39 - 8s - loss: 374.2280 - loglik: -3.7324e+02 - logprior: -9.8420e-01
Epoch 6/10
39/39 - 8s - loss: 373.5421 - loglik: -3.7255e+02 - logprior: -9.8954e-01
Epoch 7/10
39/39 - 7s - loss: 373.2609 - loglik: -3.7227e+02 - logprior: -9.9270e-01
Epoch 8/10
39/39 - 8s - loss: 373.5433 - loglik: -3.7254e+02 - logprior: -1.0011e+00
Fitted a model with MAP estimate = -308.8171
expansions: [(0, 13), (10, 1), (15, 1), (18, 1), (28, 1), (29, 2), (30, 4), (32, 1), (37, 1), (42, 1), (43, 1), (44, 1), (88, 1), (89, 1), (90, 1), (91, 1), (102, 1), (104, 1), (107, 1), (112, 1), (130, 3), (134, 4)]
discards: []
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 361.3621 - loglik: -3.5872e+02 - logprior: -2.6432e+00
Epoch 2/2
39/39 - 10s - loss: 347.3640 - loglik: -3.4607e+02 - logprior: -1.2943e+00
Fitted a model with MAP estimate = -287.8579
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  11 166 167 173 174 175 176]
Fitting a model of length 160 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 357.1022 - loglik: -3.5534e+02 - logprior: -1.7630e+00
Epoch 2/2
39/39 - 9s - loss: 353.3829 - loglik: -3.5293e+02 - logprior: -4.5519e-01
Fitted a model with MAP estimate = -294.1377
expansions: [(0, 15), (151, 7), (160, 5)]
discards: [155]
Fitting a model of length 186 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 14s - loss: 286.6686 - loglik: -2.8514e+02 - logprior: -1.5263e+00
Epoch 2/10
52/52 - 13s - loss: 280.0575 - loglik: -2.7885e+02 - logprior: -1.2079e+00
Epoch 3/10
52/52 - 12s - loss: 276.9922 - loglik: -2.7585e+02 - logprior: -1.1419e+00
Epoch 4/10
52/52 - 13s - loss: 276.6637 - loglik: -2.7558e+02 - logprior: -1.0845e+00
Epoch 5/10
52/52 - 13s - loss: 275.4290 - loglik: -2.7439e+02 - logprior: -1.0355e+00
Epoch 6/10
52/52 - 13s - loss: 276.1738 - loglik: -2.7520e+02 - logprior: -9.7439e-01
Fitted a model with MAP estimate = -274.6157
Time for alignment: 244.9274
Computed alignments with likelihoods: ['-276.3923', '-273.5059', '-274.6157']
Best model has likelihood: -273.5059
SP score = 0.1597
Training of 3 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25cd85910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe28856e1f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 816.6041 - loglik: -8.1507e+02 - logprior: -1.5304e+00
Epoch 2/10
37/37 - 20s - loss: 734.0716 - loglik: -7.3360e+02 - logprior: -4.7565e-01
Epoch 3/10
37/37 - 20s - loss: 723.3196 - loglik: -7.2288e+02 - logprior: -4.3691e-01
Epoch 4/10
37/37 - 20s - loss: 720.3222 - loglik: -7.1989e+02 - logprior: -4.2977e-01
Epoch 5/10
37/37 - 20s - loss: 715.5873 - loglik: -7.1515e+02 - logprior: -4.3917e-01
Epoch 6/10
37/37 - 20s - loss: 717.4307 - loglik: -7.1678e+02 - logprior: -6.5430e-01
Fitted a model with MAP estimate = -715.1952
expansions: [(0, 4), (34, 3), (35, 2), (36, 1), (66, 1), (73, 3), (121, 1), (135, 4), (136, 1), (197, 12), (239, 1), (240, 1)]
discards: [ 98  99 100 102]
Fitting a model of length 284 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 27s - loss: 726.9720 - loglik: -7.2458e+02 - logprior: -2.3911e+00
Epoch 2/2
37/37 - 23s - loss: 711.6139 - loglik: -7.1095e+02 - logprior: -6.6881e-01
Fitted a model with MAP estimate = -709.4783
expansions: [(0, 2), (39, 1), (42, 1), (43, 1), (107, 6), (149, 1), (152, 1), (166, 8)]
discards: [ 0 86]
Fitting a model of length 303 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 714.1187 - loglik: -7.1222e+02 - logprior: -1.8996e+00
Epoch 2/2
37/37 - 26s - loss: 705.5197 - loglik: -7.0511e+02 - logprior: -4.1016e-01
Fitted a model with MAP estimate = -704.2425
expansions: [(0, 2), (182, 3), (183, 5)]
discards: [  1   2   3 187 188 189 190]
Fitting a model of length 306 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 712.3489 - loglik: -7.1018e+02 - logprior: -2.1711e+00
Epoch 2/10
37/37 - 26s - loss: 706.2118 - loglik: -7.0588e+02 - logprior: -3.3360e-01
Epoch 3/10
37/37 - 26s - loss: 703.5811 - loglik: -7.0344e+02 - logprior: -1.4553e-01
Epoch 4/10
37/37 - 26s - loss: 697.6799 - loglik: -6.9761e+02 - logprior: -6.6481e-02
Epoch 5/10
37/37 - 26s - loss: 697.1006 - loglik: -6.9711e+02 - logprior: 0.0098
Epoch 6/10
37/37 - 26s - loss: 696.2516 - loglik: -6.9626e+02 - logprior: 0.0094
Epoch 7/10
37/37 - 27s - loss: 695.7412 - loglik: -6.9589e+02 - logprior: 0.1462
Epoch 8/10
37/37 - 26s - loss: 692.2347 - loglik: -6.9247e+02 - logprior: 0.2332
Epoch 9/10
37/37 - 26s - loss: 696.8611 - loglik: -6.9717e+02 - logprior: 0.3102
Fitted a model with MAP estimate = -693.9622
Time for alignment: 592.7858
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 24s - loss: 813.2672 - loglik: -8.1175e+02 - logprior: -1.5212e+00
Epoch 2/10
37/37 - 20s - loss: 730.7232 - loglik: -7.3026e+02 - logprior: -4.6441e-01
Epoch 3/10
37/37 - 20s - loss: 720.2264 - loglik: -7.1983e+02 - logprior: -3.9394e-01
Epoch 4/10
37/37 - 20s - loss: 715.5250 - loglik: -7.1508e+02 - logprior: -4.4402e-01
Epoch 5/10
37/37 - 20s - loss: 714.0259 - loglik: -7.1357e+02 - logprior: -4.5968e-01
Epoch 6/10
37/37 - 20s - loss: 710.6998 - loglik: -7.1008e+02 - logprior: -6.1521e-01
Epoch 7/10
37/37 - 20s - loss: 714.5283 - loglik: -7.1385e+02 - logprior: -6.7499e-01
Fitted a model with MAP estimate = -711.7508
expansions: [(0, 4), (30, 1), (32, 4), (33, 2), (35, 1), (73, 3), (88, 16), (197, 13), (239, 1), (244, 1)]
discards: [100 101 103 104 106 150]
Fitting a model of length 294 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 723.0173 - loglik: -7.2052e+02 - logprior: -2.4970e+00
Epoch 2/2
37/37 - 25s - loss: 708.1291 - loglik: -7.0748e+02 - logprior: -6.5268e-01
Fitted a model with MAP estimate = -705.4978
expansions: [(0, 2), (39, 1), (44, 1), (112, 5), (113, 3), (114, 3), (232, 4)]
discards: [  0  87 176 177 178 179 180 181 182 183 281]
Fitting a model of length 302 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 30s - loss: 715.1427 - loglik: -7.1318e+02 - logprior: -1.9642e+00
Epoch 2/2
37/37 - 26s - loss: 708.6077 - loglik: -7.0818e+02 - logprior: -4.3192e-01
Fitted a model with MAP estimate = -705.5563
expansions: [(0, 2), (31, 1), (188, 9), (189, 3), (236, 1)]
discards: [  1 115 116 117 118 213 241 242 243]
Fitting a model of length 309 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 30s - loss: 710.3573 - loglik: -7.0819e+02 - logprior: -2.1684e+00
Epoch 2/10
37/37 - 27s - loss: 705.3342 - loglik: -7.0495e+02 - logprior: -3.8302e-01
Epoch 3/10
37/37 - 27s - loss: 700.6561 - loglik: -7.0045e+02 - logprior: -2.0162e-01
Epoch 4/10
37/37 - 27s - loss: 700.5897 - loglik: -7.0045e+02 - logprior: -1.3939e-01
Epoch 5/10
37/37 - 27s - loss: 692.8005 - loglik: -6.9261e+02 - logprior: -1.9403e-01
Epoch 6/10
37/37 - 27s - loss: 694.8037 - loglik: -6.9476e+02 - logprior: -3.9276e-02
Fitted a model with MAP estimate = -693.2439
Time for alignment: 539.8360
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 819.0300 - loglik: -8.1750e+02 - logprior: -1.5330e+00
Epoch 2/10
37/37 - 20s - loss: 737.8046 - loglik: -7.3720e+02 - logprior: -6.0003e-01
Epoch 3/10
37/37 - 20s - loss: 725.9844 - loglik: -7.2540e+02 - logprior: -5.8226e-01
Epoch 4/10
37/37 - 20s - loss: 721.5599 - loglik: -7.2100e+02 - logprior: -5.5963e-01
Epoch 5/10
37/37 - 20s - loss: 721.0551 - loglik: -7.2047e+02 - logprior: -5.8082e-01
Epoch 6/10
37/37 - 20s - loss: 718.5088 - loglik: -7.1791e+02 - logprior: -6.0230e-01
Epoch 7/10
37/37 - 20s - loss: 719.0959 - loglik: -7.1849e+02 - logprior: -6.0858e-01
Fitted a model with MAP estimate = -717.7611
expansions: [(0, 4), (30, 1), (32, 1), (33, 3), (34, 2), (38, 1), (66, 1), (73, 3), (75, 3), (116, 1), (117, 1), (120, 1), (133, 11), (197, 12), (210, 2), (239, 1), (240, 1)]
discards: [ 98  99 100 101 103 156 188 189]
Fitting a model of length 295 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 728.6389 - loglik: -7.2620e+02 - logprior: -2.4340e+00
Epoch 2/2
37/37 - 25s - loss: 709.2526 - loglik: -7.0854e+02 - logprior: -7.0958e-01
Fitted a model with MAP estimate = -706.9070
expansions: [(0, 2), (45, 1), (112, 5), (154, 1), (155, 1), (156, 1), (213, 1)]
discards: [  0  86  87  88  91 180 181]
Fitting a model of length 300 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 28s - loss: 713.0374 - loglik: -7.1113e+02 - logprior: -1.9065e+00
Epoch 2/2
37/37 - 26s - loss: 706.8615 - loglik: -7.0646e+02 - logprior: -4.0172e-01
Fitted a model with MAP estimate = -704.4789
expansions: [(0, 2), (113, 2), (186, 1), (187, 2), (253, 1)]
discards: [1 2 3 4]
Fitting a model of length 304 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 711.8639 - loglik: -7.0971e+02 - logprior: -2.1547e+00
Epoch 2/10
37/37 - 26s - loss: 704.6364 - loglik: -7.0444e+02 - logprior: -2.0093e-01
Epoch 3/10
37/37 - 26s - loss: 703.3636 - loglik: -7.0335e+02 - logprior: -1.8249e-02
Epoch 4/10
37/37 - 26s - loss: 696.9967 - loglik: -6.9697e+02 - logprior: -2.4343e-02
Epoch 5/10
37/37 - 26s - loss: 696.9658 - loglik: -6.9705e+02 - logprior: 0.0821
Epoch 6/10
37/37 - 26s - loss: 694.8943 - loglik: -6.9501e+02 - logprior: 0.1128
Epoch 7/10
37/37 - 26s - loss: 694.9555 - loglik: -6.9520e+02 - logprior: 0.2450
Fitted a model with MAP estimate = -693.7037
Time for alignment: 559.7325
Computed alignments with likelihoods: ['-693.9622', '-693.2439', '-693.7037']
Best model has likelihood: -693.2439
SP score = 0.9368
Training of 3 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fa212040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2aa6479d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 386.0511 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1699 - loglik: -2.6187e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.1383 - loglik: -2.5032e+02 - logprior: -9.8201e+00
Epoch 4/10
10/10 - 1s - loss: 245.6292 - loglik: -2.4228e+02 - logprior: -3.3535e+00
Epoch 5/10
10/10 - 1s - loss: 238.1184 - loglik: -2.3794e+02 - logprior: -1.8104e-01
Epoch 6/10
10/10 - 1s - loss: 233.0570 - loglik: -2.3469e+02 - logprior: 1.6333
Epoch 7/10
10/10 - 1s - loss: 229.9471 - loglik: -2.3255e+02 - logprior: 2.6020
Epoch 8/10
10/10 - 1s - loss: 228.1648 - loglik: -2.3142e+02 - logprior: 3.2535
Epoch 9/10
10/10 - 1s - loss: 227.0800 - loglik: -2.3084e+02 - logprior: 3.7621
Epoch 10/10
10/10 - 1s - loss: 226.3574 - loglik: -2.3052e+02 - logprior: 4.1671
Fitted a model with MAP estimate = -226.0598
expansions: [(0, 6), (37, 4), (51, 4), (64, 3)]
discards: []
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 377.6247 - loglik: -2.2879e+02 - logprior: -1.4884e+02
Epoch 2/2
10/10 - 1s - loss: 268.0602 - loglik: -2.2628e+02 - logprior: -4.1779e+01
Fitted a model with MAP estimate = -248.0108
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Fitting a model of length 94 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 343.0817 - loglik: -2.2517e+02 - logprior: -1.1791e+02
Epoch 2/2
10/10 - 1s - loss: 254.4311 - loglik: -2.2452e+02 - logprior: -2.9912e+01
Fitted a model with MAP estimate = -238.8114
expansions: [(0, 4)]
discards: []
Fitting a model of length 98 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 349.1342 - loglik: -2.2350e+02 - logprior: -1.2563e+02
Epoch 2/10
10/10 - 1s - loss: 258.7870 - loglik: -2.2329e+02 - logprior: -3.5495e+01
Epoch 3/10
10/10 - 1s - loss: 234.4486 - loglik: -2.2387e+02 - logprior: -1.0574e+01
Epoch 4/10
10/10 - 1s - loss: 224.1856 - loglik: -2.2446e+02 - logprior: 0.2698
Epoch 5/10
10/10 - 1s - loss: 219.6183 - loglik: -2.2474e+02 - logprior: 5.1248
Epoch 6/10
10/10 - 1s - loss: 217.1861 - loglik: -2.2492e+02 - logprior: 7.7371
Epoch 7/10
10/10 - 1s - loss: 215.6761 - loglik: -2.2498e+02 - logprior: 9.3088
Epoch 8/10
10/10 - 1s - loss: 214.6039 - loglik: -2.2496e+02 - logprior: 10.3583
Epoch 9/10
10/10 - 1s - loss: 213.7687 - loglik: -2.2491e+02 - logprior: 11.1441
Epoch 10/10
10/10 - 1s - loss: 213.0800 - loglik: -2.2489e+02 - logprior: 11.8065
Fitted a model with MAP estimate = -212.7372
Time for alignment: 37.8534
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 386.0511 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1699 - loglik: -2.6187e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.1383 - loglik: -2.5032e+02 - logprior: -9.8201e+00
Epoch 4/10
10/10 - 1s - loss: 245.6292 - loglik: -2.4228e+02 - logprior: -3.3535e+00
Epoch 5/10
10/10 - 1s - loss: 238.1184 - loglik: -2.3794e+02 - logprior: -1.8104e-01
Epoch 6/10
10/10 - 1s - loss: 233.0570 - loglik: -2.3469e+02 - logprior: 1.6333
Epoch 7/10
10/10 - 1s - loss: 229.9471 - loglik: -2.3255e+02 - logprior: 2.6020
Epoch 8/10
10/10 - 1s - loss: 228.1648 - loglik: -2.3142e+02 - logprior: 3.2535
Epoch 9/10
10/10 - 1s - loss: 227.0800 - loglik: -2.3084e+02 - logprior: 3.7621
Epoch 10/10
10/10 - 1s - loss: 226.3574 - loglik: -2.3052e+02 - logprior: 4.1671
Fitted a model with MAP estimate = -226.0598
expansions: [(0, 6), (37, 4), (51, 4), (64, 3)]
discards: []
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 377.6247 - loglik: -2.2879e+02 - logprior: -1.4884e+02
Epoch 2/2
10/10 - 1s - loss: 268.0602 - loglik: -2.2628e+02 - logprior: -4.1779e+01
Fitted a model with MAP estimate = -248.0108
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Fitting a model of length 94 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 343.0817 - loglik: -2.2517e+02 - logprior: -1.1791e+02
Epoch 2/2
10/10 - 1s - loss: 254.4311 - loglik: -2.2452e+02 - logprior: -2.9912e+01
Fitted a model with MAP estimate = -238.8114
expansions: [(0, 4)]
discards: []
Fitting a model of length 98 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 349.1342 - loglik: -2.2350e+02 - logprior: -1.2563e+02
Epoch 2/10
10/10 - 1s - loss: 258.7870 - loglik: -2.2329e+02 - logprior: -3.5495e+01
Epoch 3/10
10/10 - 1s - loss: 234.4486 - loglik: -2.2387e+02 - logprior: -1.0574e+01
Epoch 4/10
10/10 - 1s - loss: 224.1856 - loglik: -2.2446e+02 - logprior: 0.2698
Epoch 5/10
10/10 - 1s - loss: 219.6183 - loglik: -2.2474e+02 - logprior: 5.1248
Epoch 6/10
10/10 - 1s - loss: 217.1861 - loglik: -2.2492e+02 - logprior: 7.7371
Epoch 7/10
10/10 - 1s - loss: 215.6761 - loglik: -2.2498e+02 - logprior: 9.3088
Epoch 8/10
10/10 - 1s - loss: 214.6039 - loglik: -2.2496e+02 - logprior: 10.3583
Epoch 9/10
10/10 - 1s - loss: 213.7687 - loglik: -2.2491e+02 - logprior: 11.1441
Epoch 10/10
10/10 - 1s - loss: 213.0800 - loglik: -2.2489e+02 - logprior: 11.8065
Fitted a model with MAP estimate = -212.7372
Time for alignment: 36.9360
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 386.0511 - loglik: -2.7290e+02 - logprior: -1.1315e+02
Epoch 2/10
10/10 - 1s - loss: 289.1699 - loglik: -2.6187e+02 - logprior: -2.7299e+01
Epoch 3/10
10/10 - 1s - loss: 260.1383 - loglik: -2.5032e+02 - logprior: -9.8201e+00
Epoch 4/10
10/10 - 1s - loss: 245.6292 - loglik: -2.4228e+02 - logprior: -3.3535e+00
Epoch 5/10
10/10 - 1s - loss: 238.1184 - loglik: -2.3794e+02 - logprior: -1.8104e-01
Epoch 6/10
10/10 - 1s - loss: 233.0570 - loglik: -2.3469e+02 - logprior: 1.6333
Epoch 7/10
10/10 - 1s - loss: 229.9471 - loglik: -2.3255e+02 - logprior: 2.6020
Epoch 8/10
10/10 - 1s - loss: 228.1648 - loglik: -2.3142e+02 - logprior: 3.2535
Epoch 9/10
10/10 - 1s - loss: 227.0800 - loglik: -2.3084e+02 - logprior: 3.7621
Epoch 10/10
10/10 - 1s - loss: 226.3574 - loglik: -2.3052e+02 - logprior: 4.1671
Fitted a model with MAP estimate = -226.0598
expansions: [(0, 6), (37, 4), (51, 4), (64, 3)]
discards: []
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 377.6247 - loglik: -2.2879e+02 - logprior: -1.4884e+02
Epoch 2/2
10/10 - 1s - loss: 268.0602 - loglik: -2.2628e+02 - logprior: -4.1779e+01
Fitted a model with MAP estimate = -248.0108
expansions: [(0, 4), (55, 3)]
discards: [0 1 2 3 4 5]
Fitting a model of length 94 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 343.0817 - loglik: -2.2517e+02 - logprior: -1.1791e+02
Epoch 2/2
10/10 - 1s - loss: 254.4311 - loglik: -2.2452e+02 - logprior: -2.9912e+01
Fitted a model with MAP estimate = -238.8114
expansions: [(0, 4)]
discards: []
Fitting a model of length 98 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 349.1342 - loglik: -2.2350e+02 - logprior: -1.2563e+02
Epoch 2/10
10/10 - 1s - loss: 258.7870 - loglik: -2.2329e+02 - logprior: -3.5495e+01
Epoch 3/10
10/10 - 1s - loss: 234.4486 - loglik: -2.2387e+02 - logprior: -1.0574e+01
Epoch 4/10
10/10 - 1s - loss: 224.1856 - loglik: -2.2446e+02 - logprior: 0.2698
Epoch 5/10
10/10 - 1s - loss: 219.6183 - loglik: -2.2474e+02 - logprior: 5.1248
Epoch 6/10
10/10 - 1s - loss: 217.1861 - loglik: -2.2492e+02 - logprior: 7.7371
Epoch 7/10
10/10 - 1s - loss: 215.6761 - loglik: -2.2498e+02 - logprior: 9.3088
Epoch 8/10
10/10 - 1s - loss: 214.6039 - loglik: -2.2496e+02 - logprior: 10.3583
Epoch 9/10
10/10 - 1s - loss: 213.7687 - loglik: -2.2491e+02 - logprior: 11.1441
Epoch 10/10
10/10 - 1s - loss: 213.0800 - loglik: -2.2489e+02 - logprior: 11.8065
Fitted a model with MAP estimate = -212.7372
Time for alignment: 37.0795
Computed alignments with likelihoods: ['-212.7372', '-212.7372', '-212.7372']
Best model has likelihood: -212.7372
SP score = 0.7617
Training of 3 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25c634eb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe242431a60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 997.4117 - loglik: -9.8990e+02 - logprior: -7.5105e+00
Epoch 2/10
19/19 - 17s - loss: 871.9365 - loglik: -8.7320e+02 - logprior: 1.2604
Epoch 3/10
19/19 - 17s - loss: 814.5027 - loglik: -8.1464e+02 - logprior: 0.1368
Epoch 4/10
19/19 - 17s - loss: 796.1640 - loglik: -7.9581e+02 - logprior: -3.4973e-01
Epoch 5/10
19/19 - 17s - loss: 785.3637 - loglik: -7.8500e+02 - logprior: -3.6703e-01
Epoch 6/10
19/19 - 17s - loss: 787.6766 - loglik: -7.8726e+02 - logprior: -4.1581e-01
Fitted a model with MAP estimate = -784.7626
expansions: [(30, 1), (100, 1), (101, 2), (109, 1), (114, 1), (115, 1), (117, 1), (118, 1), (120, 2), (122, 6), (124, 2), (144, 1), (146, 1), (147, 1), (164, 1), (165, 1), (167, 1), (168, 1), (169, 1), (170, 1), (173, 1), (177, 1), (178, 2), (181, 1), (190, 2), (191, 2), (199, 1), (205, 1), (209, 1), (212, 1), (220, 1), (221, 2), (222, 2), (224, 1), (226, 1), (236, 1), (237, 1), (238, 1), (259, 1), (262, 1), (264, 3), (275, 1), (300, 1), (301, 5), (303, 1)]
discards: [0]
Fitting a model of length 383 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 797.8824 - loglik: -7.8738e+02 - logprior: -1.0506e+01
Epoch 2/2
19/19 - 23s - loss: 763.1138 - loglik: -7.6110e+02 - logprior: -2.0097e+00
Fitted a model with MAP estimate = -758.7672
expansions: [(0, 2), (361, 1)]
discards: [  0 136 137 207 225 263]
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 769.2339 - loglik: -7.6297e+02 - logprior: -6.2648e+00
Epoch 2/2
19/19 - 23s - loss: 751.8602 - loglik: -7.5434e+02 - logprior: 2.4758
Fitted a model with MAP estimate = -750.4863
expansions: [(137, 1), (139, 1)]
discards: [  0  30 104]
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 771.7513 - loglik: -7.6267e+02 - logprior: -9.0768e+00
Epoch 2/10
19/19 - 22s - loss: 757.1851 - loglik: -7.5806e+02 - logprior: 0.8712
Epoch 3/10
19/19 - 22s - loss: 750.0368 - loglik: -7.5417e+02 - logprior: 4.1323
Epoch 4/10
19/19 - 23s - loss: 746.8609 - loglik: -7.5146e+02 - logprior: 4.5985
Epoch 5/10
19/19 - 22s - loss: 745.3813 - loglik: -7.5028e+02 - logprior: 4.9004
Epoch 6/10
19/19 - 22s - loss: 741.8895 - loglik: -7.4672e+02 - logprior: 4.8304
Epoch 7/10
19/19 - 22s - loss: 741.9016 - loglik: -7.4726e+02 - logprior: 5.3623
Fitted a model with MAP estimate = -741.0945
Time for alignment: 427.5214
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 21s - loss: 999.0316 - loglik: -9.9151e+02 - logprior: -7.5187e+00
Epoch 2/10
19/19 - 17s - loss: 878.6595 - loglik: -8.8008e+02 - logprior: 1.4251
Epoch 3/10
19/19 - 17s - loss: 822.3484 - loglik: -8.2331e+02 - logprior: 0.9637
Epoch 4/10
19/19 - 17s - loss: 810.0883 - loglik: -8.1083e+02 - logprior: 0.7380
Epoch 5/10
19/19 - 17s - loss: 804.2623 - loglik: -8.0501e+02 - logprior: 0.7513
Epoch 6/10
19/19 - 17s - loss: 801.2977 - loglik: -8.0194e+02 - logprior: 0.6440
Epoch 7/10
19/19 - 17s - loss: 803.9630 - loglik: -8.0459e+02 - logprior: 0.6226
Fitted a model with MAP estimate = -800.7575
expansions: [(14, 1), (40, 1), (69, 1), (94, 6), (166, 6), (170, 1), (174, 1), (175, 3), (176, 1), (190, 2), (191, 2), (199, 1), (204, 1), (205, 1), (212, 1), (221, 2), (222, 2), (223, 1), (224, 1), (236, 1), (237, 1), (238, 1), (259, 1), (262, 1), (263, 5), (280, 2), (294, 1), (301, 2), (302, 5), (303, 2)]
discards: [0]
Fitting a model of length 376 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 816.0370 - loglik: -8.0563e+02 - logprior: -1.0405e+01
Epoch 2/2
19/19 - 22s - loss: 783.6080 - loglik: -7.8168e+02 - logprior: -1.9287e+00
Fitted a model with MAP estimate = -779.5344
expansions: [(0, 2), (324, 1)]
discards: [  0 176 177 213 313 352 353]
Fitting a model of length 372 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 790.5119 - loglik: -7.8424e+02 - logprior: -6.2758e+00
Epoch 2/2
19/19 - 22s - loss: 775.6834 - loglik: -7.7817e+02 - logprior: 2.4871
Fitted a model with MAP estimate = -772.0593
expansions: [(101, 1), (302, 1), (303, 2), (304, 1)]
discards: [0]
Fitting a model of length 376 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 792.3423 - loglik: -7.8334e+02 - logprior: -9.0061e+00
Epoch 2/10
19/19 - 22s - loss: 775.9712 - loglik: -7.7695e+02 - logprior: 0.9761
Epoch 3/10
19/19 - 22s - loss: 770.2733 - loglik: -7.7422e+02 - logprior: 3.9464
Epoch 4/10
19/19 - 22s - loss: 764.2313 - loglik: -7.6893e+02 - logprior: 4.6982
Epoch 5/10
19/19 - 22s - loss: 763.1115 - loglik: -7.6812e+02 - logprior: 5.0059
Epoch 6/10
19/19 - 22s - loss: 762.1588 - loglik: -7.6743e+02 - logprior: 5.2720
Epoch 7/10
19/19 - 22s - loss: 760.3969 - loglik: -7.6594e+02 - logprior: 5.5481
Epoch 8/10
19/19 - 22s - loss: 760.8381 - loglik: -7.6668e+02 - logprior: 5.8463
Fitted a model with MAP estimate = -759.8337
Time for alignment: 462.9202
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 994.8105 - loglik: -9.8732e+02 - logprior: -7.4953e+00
Epoch 2/10
19/19 - 17s - loss: 873.0168 - loglik: -8.7415e+02 - logprior: 1.1300
Epoch 3/10
19/19 - 17s - loss: 817.2142 - loglik: -8.1753e+02 - logprior: 0.3169
Epoch 4/10
19/19 - 17s - loss: 802.4255 - loglik: -8.0260e+02 - logprior: 0.1736
Epoch 5/10
19/19 - 17s - loss: 795.2662 - loglik: -7.9534e+02 - logprior: 0.0780
Epoch 6/10
19/19 - 17s - loss: 792.7951 - loglik: -7.9269e+02 - logprior: -1.0395e-01
Epoch 7/10
19/19 - 17s - loss: 788.0705 - loglik: -7.8798e+02 - logprior: -9.1892e-02
Epoch 8/10
19/19 - 17s - loss: 788.3439 - loglik: -7.8828e+02 - logprior: -6.2932e-02
Fitted a model with MAP estimate = -788.3408
expansions: [(16, 1), (34, 1), (67, 2), (98, 1), (101, 1), (107, 1), (113, 1), (114, 1), (117, 1), (118, 1), (119, 1), (122, 6), (142, 1), (146, 1), (149, 1), (164, 1), (166, 3), (167, 1), (169, 1), (172, 1), (176, 1), (177, 2), (190, 2), (191, 2), (196, 1), (205, 1), (208, 1), (218, 1), (220, 1), (221, 1), (222, 2), (224, 1), (242, 3), (258, 1), (263, 1), (265, 6), (301, 1), (302, 2), (311, 3)]
discards: [0]
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 798.1037 - loglik: -7.8763e+02 - logprior: -1.0472e+01
Epoch 2/2
19/19 - 23s - loss: 768.1387 - loglik: -7.6643e+02 - logprior: -1.7072e+00
Fitted a model with MAP estimate = -760.7594
expansions: [(0, 3), (374, 1)]
discards: [  0 198 205 221]
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 770.2254 - loglik: -7.6419e+02 - logprior: -6.0368e+00
Epoch 2/2
19/19 - 22s - loss: 754.8553 - loglik: -7.5746e+02 - logprior: 2.6003
Fitted a model with MAP estimate = -751.7123
expansions: []
discards: [0 1]
Fitting a model of length 378 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 25s - loss: 773.0076 - loglik: -7.6381e+02 - logprior: -9.1954e+00
Epoch 2/10
19/19 - 22s - loss: 757.9802 - loglik: -7.5852e+02 - logprior: 0.5390
Epoch 3/10
19/19 - 22s - loss: 751.1098 - loglik: -7.5517e+02 - logprior: 4.0590
Epoch 4/10
19/19 - 22s - loss: 749.3899 - loglik: -7.5412e+02 - logprior: 4.7298
Epoch 5/10
19/19 - 22s - loss: 744.1575 - loglik: -7.4913e+02 - logprior: 4.9705
Epoch 6/10
19/19 - 22s - loss: 744.9987 - loglik: -7.5025e+02 - logprior: 5.2498
Fitted a model with MAP estimate = -742.9957
Time for alignment: 438.7520
Computed alignments with likelihoods: ['-741.0945', '-759.8337', '-742.9957']
Best model has likelihood: -741.0945
SP score = 0.6997
Training of 3 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2776b15e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fa9aa820>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 368.6475 - loglik: -3.1239e+02 - logprior: -5.6257e+01
Epoch 2/10
10/10 - 1s - loss: 290.6223 - loglik: -2.7726e+02 - logprior: -1.3358e+01
Epoch 3/10
10/10 - 1s - loss: 239.9758 - loglik: -2.3406e+02 - logprior: -5.9151e+00
Epoch 4/10
10/10 - 1s - loss: 211.9797 - loglik: -2.0831e+02 - logprior: -3.6659e+00
Epoch 5/10
10/10 - 1s - loss: 202.7589 - loglik: -2.0030e+02 - logprior: -2.4573e+00
Epoch 6/10
10/10 - 1s - loss: 198.9233 - loglik: -1.9717e+02 - logprior: -1.7510e+00
Epoch 7/10
10/10 - 1s - loss: 196.7452 - loglik: -1.9538e+02 - logprior: -1.3613e+00
Epoch 8/10
10/10 - 1s - loss: 194.8902 - loglik: -1.9371e+02 - logprior: -1.1827e+00
Epoch 9/10
10/10 - 1s - loss: 194.1426 - loglik: -1.9314e+02 - logprior: -1.0046e+00
Epoch 10/10
10/10 - 1s - loss: 193.6627 - loglik: -1.9286e+02 - logprior: -7.9994e-01
Fitted a model with MAP estimate = -193.4743
expansions: [(13, 3), (17, 3), (18, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (62, 1), (65, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 249.6546 - loglik: -1.8632e+02 - logprior: -6.3331e+01
Epoch 2/2
10/10 - 1s - loss: 199.2189 - loglik: -1.7459e+02 - logprior: -2.4631e+01
Fitted a model with MAP estimate = -190.3409
expansions: [(0, 2), (69, 1)]
discards: [ 0 20 98]
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.2878 - loglik: -1.7054e+02 - logprior: -4.9747e+01
Epoch 2/2
10/10 - 1s - loss: 179.0983 - loglik: -1.6828e+02 - logprior: -1.0815e+01
Fitted a model with MAP estimate = -172.6995
expansions: []
discards: [0]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.3042 - loglik: -1.7019e+02 - logprior: -6.0118e+01
Epoch 2/10
10/10 - 1s - loss: 186.1515 - loglik: -1.6946e+02 - logprior: -1.6694e+01
Epoch 3/10
10/10 - 1s - loss: 172.7115 - loglik: -1.6884e+02 - logprior: -3.8677e+00
Epoch 4/10
10/10 - 1s - loss: 168.5080 - loglik: -1.6911e+02 - logprior: 0.6032
Epoch 5/10
10/10 - 1s - loss: 166.5567 - loglik: -1.6913e+02 - logprior: 2.5687
Epoch 6/10
10/10 - 1s - loss: 164.9695 - loglik: -1.6858e+02 - logprior: 3.6087
Epoch 7/10
10/10 - 1s - loss: 164.7555 - loglik: -1.6907e+02 - logprior: 4.3168
Epoch 8/10
10/10 - 1s - loss: 164.2506 - loglik: -1.6916e+02 - logprior: 4.9091
Epoch 9/10
10/10 - 1s - loss: 163.7314 - loglik: -1.6914e+02 - logprior: 5.4126
Epoch 10/10
10/10 - 1s - loss: 163.5044 - loglik: -1.6931e+02 - logprior: 5.8047
Fitted a model with MAP estimate = -163.3195
Time for alignment: 44.8138
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 368.8341 - loglik: -3.1258e+02 - logprior: -5.6256e+01
Epoch 2/10
10/10 - 1s - loss: 290.1146 - loglik: -2.7675e+02 - logprior: -1.3363e+01
Epoch 3/10
10/10 - 1s - loss: 243.1504 - loglik: -2.3723e+02 - logprior: -5.9216e+00
Epoch 4/10
10/10 - 1s - loss: 215.1075 - loglik: -2.1140e+02 - logprior: -3.7056e+00
Epoch 5/10
10/10 - 1s - loss: 203.1716 - loglik: -2.0051e+02 - logprior: -2.6610e+00
Epoch 6/10
10/10 - 1s - loss: 198.2869 - loglik: -1.9622e+02 - logprior: -2.0719e+00
Epoch 7/10
10/10 - 1s - loss: 196.1461 - loglik: -1.9457e+02 - logprior: -1.5733e+00
Epoch 8/10
10/10 - 1s - loss: 194.4106 - loglik: -1.9305e+02 - logprior: -1.3557e+00
Epoch 9/10
10/10 - 1s - loss: 193.5253 - loglik: -1.9234e+02 - logprior: -1.1805e+00
Epoch 10/10
10/10 - 1s - loss: 193.3613 - loglik: -1.9239e+02 - logprior: -9.7206e-01
Fitted a model with MAP estimate = -193.1193
expansions: [(13, 3), (17, 3), (18, 1), (32, 1), (33, 1), (34, 2), (35, 1), (55, 2), (56, 2), (57, 1), (58, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 249.4125 - loglik: -1.8603e+02 - logprior: -6.3385e+01
Epoch 2/2
10/10 - 1s - loss: 198.9615 - loglik: -1.7431e+02 - logprior: -2.4653e+01
Fitted a model with MAP estimate = -190.0351
expansions: [(0, 2), (69, 1)]
discards: [ 0 20 98]
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 220.3259 - loglik: -1.7054e+02 - logprior: -4.9785e+01
Epoch 2/2
10/10 - 1s - loss: 178.9990 - loglik: -1.6817e+02 - logprior: -1.0829e+01
Fitted a model with MAP estimate = -172.6829
expansions: []
discards: [0]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.2397 - loglik: -1.7004e+02 - logprior: -6.0195e+01
Epoch 2/10
10/10 - 1s - loss: 186.5420 - loglik: -1.6967e+02 - logprior: -1.6870e+01
Epoch 3/10
10/10 - 1s - loss: 172.6797 - loglik: -1.6875e+02 - logprior: -3.9253e+00
Epoch 4/10
10/10 - 1s - loss: 168.6254 - loglik: -1.6923e+02 - logprior: 0.6004
Epoch 5/10
10/10 - 1s - loss: 166.1765 - loglik: -1.6876e+02 - logprior: 2.5862
Epoch 6/10
10/10 - 1s - loss: 165.3756 - loglik: -1.6901e+02 - logprior: 3.6368
Epoch 7/10
10/10 - 1s - loss: 164.6501 - loglik: -1.6899e+02 - logprior: 4.3397
Epoch 8/10
10/10 - 1s - loss: 164.1525 - loglik: -1.6908e+02 - logprior: 4.9267
Epoch 9/10
10/10 - 1s - loss: 163.9861 - loglik: -1.6941e+02 - logprior: 5.4242
Epoch 10/10
10/10 - 1s - loss: 163.2592 - loglik: -1.6908e+02 - logprior: 5.8239
Fitted a model with MAP estimate = -163.3063
Time for alignment: 42.1039
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 368.8641 - loglik: -3.1261e+02 - logprior: -5.6255e+01
Epoch 2/10
10/10 - 1s - loss: 289.6137 - loglik: -2.7626e+02 - logprior: -1.3355e+01
Epoch 3/10
10/10 - 1s - loss: 241.6913 - loglik: -2.3575e+02 - logprior: -5.9440e+00
Epoch 4/10
10/10 - 1s - loss: 213.6556 - loglik: -2.0976e+02 - logprior: -3.8975e+00
Epoch 5/10
10/10 - 1s - loss: 203.2337 - loglik: -2.0054e+02 - logprior: -2.6941e+00
Epoch 6/10
10/10 - 1s - loss: 199.5923 - loglik: -1.9764e+02 - logprior: -1.9506e+00
Epoch 7/10
10/10 - 1s - loss: 196.9010 - loglik: -1.9546e+02 - logprior: -1.4413e+00
Epoch 8/10
10/10 - 1s - loss: 195.7774 - loglik: -1.9462e+02 - logprior: -1.1601e+00
Epoch 9/10
10/10 - 1s - loss: 194.6221 - loglik: -1.9362e+02 - logprior: -1.0037e+00
Epoch 10/10
10/10 - 1s - loss: 194.4733 - loglik: -1.9363e+02 - logprior: -8.4600e-01
Fitted a model with MAP estimate = -194.1324
expansions: [(11, 4), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (54, 2), (55, 1), (56, 2), (57, 1), (58, 1), (62, 1), (78, 4), (81, 1), (86, 2), (90, 1), (91, 2)]
discards: [0]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.7841 - loglik: -1.8824e+02 - logprior: -6.3545e+01
Epoch 2/2
10/10 - 1s - loss: 200.9202 - loglik: -1.7612e+02 - logprior: -2.4799e+01
Fitted a model with MAP estimate = -192.4272
expansions: [(0, 2), (12, 1), (64, 1)]
discards: [ 0 66 68 97]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 222.3364 - loglik: -1.7235e+02 - logprior: -4.9982e+01
Epoch 2/2
10/10 - 1s - loss: 180.0647 - loglik: -1.6902e+02 - logprior: -1.1048e+01
Fitted a model with MAP estimate = -173.8443
expansions: [(69, 1)]
discards: [0]
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 231.8044 - loglik: -1.7098e+02 - logprior: -6.0824e+01
Epoch 2/10
10/10 - 1s - loss: 187.9779 - loglik: -1.6986e+02 - logprior: -1.8119e+01
Epoch 3/10
10/10 - 1s - loss: 174.1114 - loglik: -1.6963e+02 - logprior: -4.4851e+00
Epoch 4/10
10/10 - 1s - loss: 168.9079 - loglik: -1.6938e+02 - logprior: 0.4768
Epoch 5/10
10/10 - 1s - loss: 166.9585 - loglik: -1.6948e+02 - logprior: 2.5183
Epoch 6/10
10/10 - 1s - loss: 164.7671 - loglik: -1.6834e+02 - logprior: 3.5688
Epoch 7/10
10/10 - 1s - loss: 165.3239 - loglik: -1.6957e+02 - logprior: 4.2423
Fitted a model with MAP estimate = -164.5523
Time for alignment: 39.7795
Computed alignments with likelihoods: ['-163.3195', '-163.3063', '-164.5523']
Best model has likelihood: -163.3063
SP score = 0.9351
Training of 3 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1af627f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe27f91f820>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.6213 - loglik: -3.8855e+02 - logprior: -8.0671e+00
Epoch 2/10
13/13 - 2s - loss: 354.9577 - loglik: -3.5313e+02 - logprior: -1.8307e+00
Epoch 3/10
13/13 - 2s - loss: 326.8654 - loglik: -3.2534e+02 - logprior: -1.5228e+00
Epoch 4/10
13/13 - 2s - loss: 313.6422 - loglik: -3.1197e+02 - logprior: -1.6765e+00
Epoch 5/10
13/13 - 2s - loss: 308.5266 - loglik: -3.0697e+02 - logprior: -1.5588e+00
Epoch 6/10
13/13 - 2s - loss: 306.5518 - loglik: -3.0507e+02 - logprior: -1.4777e+00
Epoch 7/10
13/13 - 2s - loss: 305.4450 - loglik: -3.0391e+02 - logprior: -1.5302e+00
Epoch 8/10
13/13 - 2s - loss: 304.7248 - loglik: -3.0316e+02 - logprior: -1.5644e+00
Epoch 9/10
13/13 - 2s - loss: 303.8753 - loglik: -3.0228e+02 - logprior: -1.5964e+00
Epoch 10/10
13/13 - 2s - loss: 303.2756 - loglik: -3.0166e+02 - logprior: -1.6143e+00
Fitted a model with MAP estimate = -303.4485
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (48, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 1), (80, 1), (92, 6), (99, 2), (100, 3)]
discards: [0]
Fitting a model of length 139 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 316.9429 - loglik: -3.0760e+02 - logprior: -9.3478e+00
Epoch 2/2
13/13 - 3s - loss: 300.3513 - loglik: -2.9614e+02 - logprior: -4.2136e+00
Fitted a model with MAP estimate = -296.2967
expansions: [(0, 2)]
discards: [  0  36  80 116 117 118 119 120 126]
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 301.3821 - loglik: -2.9434e+02 - logprior: -7.0416e+00
Epoch 2/2
13/13 - 3s - loss: 293.7860 - loglik: -2.9210e+02 - logprior: -1.6835e+00
Fitted a model with MAP estimate = -291.9627
expansions: []
discards: [  0  73 122]
Fitting a model of length 129 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 304.4625 - loglik: -2.9576e+02 - logprior: -8.6980e+00
Epoch 2/10
13/13 - 2s - loss: 295.3473 - loglik: -2.9295e+02 - logprior: -2.3955e+00
Epoch 3/10
13/13 - 2s - loss: 292.0711 - loglik: -2.9115e+02 - logprior: -9.2328e-01
Epoch 4/10
13/13 - 2s - loss: 290.4564 - loglik: -2.8988e+02 - logprior: -5.7600e-01
Epoch 5/10
13/13 - 2s - loss: 290.0134 - loglik: -2.8962e+02 - logprior: -3.9719e-01
Epoch 6/10
13/13 - 2s - loss: 288.7905 - loglik: -2.8846e+02 - logprior: -3.3093e-01
Epoch 7/10
13/13 - 2s - loss: 289.2265 - loglik: -2.8892e+02 - logprior: -3.0997e-01
Fitted a model with MAP estimate = -288.6001
Time for alignment: 74.6935
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 396.4643 - loglik: -3.8840e+02 - logprior: -8.0636e+00
Epoch 2/10
13/13 - 2s - loss: 354.8055 - loglik: -3.5298e+02 - logprior: -1.8259e+00
Epoch 3/10
13/13 - 2s - loss: 324.2264 - loglik: -3.2269e+02 - logprior: -1.5351e+00
Epoch 4/10
13/13 - 2s - loss: 310.6925 - loglik: -3.0897e+02 - logprior: -1.7194e+00
Epoch 5/10
13/13 - 2s - loss: 306.6821 - loglik: -3.0510e+02 - logprior: -1.5842e+00
Epoch 6/10
13/13 - 2s - loss: 304.6966 - loglik: -3.0317e+02 - logprior: -1.5238e+00
Epoch 7/10
13/13 - 2s - loss: 303.9950 - loglik: -3.0241e+02 - logprior: -1.5851e+00
Epoch 8/10
13/13 - 2s - loss: 303.2191 - loglik: -3.0163e+02 - logprior: -1.5886e+00
Epoch 9/10
13/13 - 2s - loss: 302.7200 - loglik: -3.0113e+02 - logprior: -1.5931e+00
Epoch 10/10
13/13 - 2s - loss: 302.2098 - loglik: -3.0061e+02 - logprior: -1.5961e+00
Fitted a model with MAP estimate = -302.2177
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 2), (28, 2), (32, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 1), (82, 1), (100, 3), (101, 3)]
discards: [0]
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 315.6227 - loglik: -3.0625e+02 - logprior: -9.3733e+00
Epoch 2/2
13/13 - 3s - loss: 300.1664 - loglik: -2.9602e+02 - logprior: -4.1422e+00
Fitted a model with MAP estimate = -297.2004
expansions: [(0, 2), (123, 1)]
discards: [ 0 36 38 81]
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 300.5105 - loglik: -2.9352e+02 - logprior: -6.9954e+00
Epoch 2/2
13/13 - 3s - loss: 291.5443 - loglik: -2.8991e+02 - logprior: -1.6386e+00
Fitted a model with MAP estimate = -289.9887
expansions: []
discards: [  0  73 125]
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 302.5450 - loglik: -2.9385e+02 - logprior: -8.6936e+00
Epoch 2/10
13/13 - 2s - loss: 293.4456 - loglik: -2.9104e+02 - logprior: -2.4061e+00
Epoch 3/10
13/13 - 2s - loss: 290.3598 - loglik: -2.8945e+02 - logprior: -9.0950e-01
Epoch 4/10
13/13 - 3s - loss: 289.6133 - loglik: -2.8904e+02 - logprior: -5.6890e-01
Epoch 5/10
13/13 - 2s - loss: 288.4472 - loglik: -2.8807e+02 - logprior: -3.7279e-01
Epoch 6/10
13/13 - 3s - loss: 287.8186 - loglik: -2.8749e+02 - logprior: -3.2733e-01
Epoch 7/10
13/13 - 3s - loss: 287.2996 - loglik: -2.8700e+02 - logprior: -2.9939e-01
Epoch 8/10
13/13 - 3s - loss: 287.4195 - loglik: -2.8715e+02 - logprior: -2.6927e-01
Fitted a model with MAP estimate = -287.1907
Time for alignment: 75.8525
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 396.8963 - loglik: -3.8882e+02 - logprior: -8.0729e+00
Epoch 2/10
13/13 - 2s - loss: 355.3156 - loglik: -3.5349e+02 - logprior: -1.8232e+00
Epoch 3/10
13/13 - 2s - loss: 328.6902 - loglik: -3.2720e+02 - logprior: -1.4880e+00
Epoch 4/10
13/13 - 2s - loss: 314.8401 - loglik: -3.1311e+02 - logprior: -1.7265e+00
Epoch 5/10
13/13 - 2s - loss: 309.5911 - loglik: -3.0792e+02 - logprior: -1.6688e+00
Epoch 6/10
13/13 - 2s - loss: 305.9420 - loglik: -3.0431e+02 - logprior: -1.6338e+00
Epoch 7/10
13/13 - 2s - loss: 305.4342 - loglik: -3.0375e+02 - logprior: -1.6833e+00
Epoch 8/10
13/13 - 2s - loss: 304.8998 - loglik: -3.0321e+02 - logprior: -1.6925e+00
Epoch 9/10
13/13 - 2s - loss: 303.5892 - loglik: -3.0190e+02 - logprior: -1.6899e+00
Epoch 10/10
13/13 - 2s - loss: 303.3110 - loglik: -3.0162e+02 - logprior: -1.6892e+00
Fitted a model with MAP estimate = -303.3022
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 2), (28, 2), (51, 1), (52, 1), (55, 1), (58, 2), (64, 2), (79, 1), (85, 1), (87, 1), (88, 1), (93, 7)]
discards: [0]
Fitting a model of length 138 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 317.1184 - loglik: -3.0777e+02 - logprior: -9.3463e+00
Epoch 2/2
13/13 - 3s - loss: 300.3824 - loglik: -2.9619e+02 - logprior: -4.1885e+00
Fitted a model with MAP estimate = -297.4020
expansions: [(0, 2)]
discards: [  0  35  38  81 118 119 120 121]
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 300.9677 - loglik: -2.9396e+02 - logprior: -7.0038e+00
Epoch 2/2
13/13 - 3s - loss: 293.6642 - loglik: -2.9196e+02 - logprior: -1.7016e+00
Fitted a model with MAP estimate = -291.7739
expansions: [(116, 1), (125, 2)]
discards: [ 0 73]
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 304.1771 - loglik: -2.9523e+02 - logprior: -8.9508e+00
Epoch 2/10
13/13 - 3s - loss: 295.8446 - loglik: -2.9306e+02 - logprior: -2.7891e+00
Epoch 3/10
13/13 - 3s - loss: 291.3938 - loglik: -2.9033e+02 - logprior: -1.0616e+00
Epoch 4/10
13/13 - 3s - loss: 289.5754 - loglik: -2.8885e+02 - logprior: -7.2963e-01
Epoch 5/10
13/13 - 3s - loss: 289.3666 - loglik: -2.8883e+02 - logprior: -5.3792e-01
Epoch 6/10
13/13 - 3s - loss: 288.0788 - loglik: -2.8758e+02 - logprior: -4.9532e-01
Epoch 7/10
13/13 - 3s - loss: 288.1053 - loglik: -2.8762e+02 - logprior: -4.8405e-01
Fitted a model with MAP estimate = -287.6744
Time for alignment: 75.4972
Computed alignments with likelihoods: ['-288.6001', '-287.1907', '-287.6744']
Best model has likelihood: -287.1907
SP score = 0.8825
Training of 3 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2421956a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2a1cc91f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.6229 - loglik: -2.1404e+02 - logprior: -8.5864e+00
Epoch 2/10
13/13 - 1s - loss: 175.0978 - loglik: -1.7285e+02 - logprior: -2.2445e+00
Epoch 3/10
13/13 - 1s - loss: 147.4129 - loglik: -1.4554e+02 - logprior: -1.8703e+00
Epoch 4/10
13/13 - 1s - loss: 139.0676 - loglik: -1.3729e+02 - logprior: -1.7786e+00
Epoch 5/10
13/13 - 1s - loss: 136.3574 - loglik: -1.3474e+02 - logprior: -1.6157e+00
Epoch 6/10
13/13 - 1s - loss: 135.6875 - loglik: -1.3408e+02 - logprior: -1.6121e+00
Epoch 7/10
13/13 - 2s - loss: 135.3773 - loglik: -1.3382e+02 - logprior: -1.5569e+00
Epoch 8/10
13/13 - 1s - loss: 134.5607 - loglik: -1.3301e+02 - logprior: -1.5485e+00
Epoch 9/10
13/13 - 1s - loss: 134.5135 - loglik: -1.3299e+02 - logprior: -1.5282e+00
Epoch 10/10
13/13 - 1s - loss: 134.3625 - loglik: -1.3284e+02 - logprior: -1.5268e+00
Fitted a model with MAP estimate = -134.2387
expansions: [(0, 4), (13, 1), (15, 1), (33, 1), (35, 2), (36, 1), (37, 2), (38, 2), (43, 1), (44, 3)]
discards: []
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 140.8695 - loglik: -1.3063e+02 - logprior: -1.0240e+01
Epoch 2/2
13/13 - 1s - loss: 123.8974 - loglik: -1.2073e+02 - logprior: -3.1631e+00
Fitted a model with MAP estimate = -121.0216
expansions: [(0, 2)]
discards: [43 47 50]
Fitting a model of length 82 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 129.3034 - loglik: -1.1922e+02 - logprior: -1.0088e+01
Epoch 2/2
13/13 - 1s - loss: 120.4219 - loglik: -1.1717e+02 - logprior: -3.2552e+00
Fitted a model with MAP estimate = -118.5266
expansions: []
discards: [60]
Fitting a model of length 81 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 125.7890 - loglik: -1.1786e+02 - logprior: -7.9262e+00
Epoch 2/10
13/13 - 1s - loss: 119.4465 - loglik: -1.1719e+02 - logprior: -2.2551e+00
Epoch 3/10
13/13 - 2s - loss: 118.4248 - loglik: -1.1670e+02 - logprior: -1.7208e+00
Epoch 4/10
13/13 - 1s - loss: 117.4758 - loglik: -1.1611e+02 - logprior: -1.3633e+00
Epoch 5/10
13/13 - 1s - loss: 117.5451 - loglik: -1.1627e+02 - logprior: -1.2768e+00
Fitted a model with MAP estimate = -117.2817
Time for alignment: 47.1948
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.8467 - loglik: -2.1426e+02 - logprior: -8.5888e+00
Epoch 2/10
13/13 - 1s - loss: 174.8015 - loglik: -1.7257e+02 - logprior: -2.2349e+00
Epoch 3/10
13/13 - 1s - loss: 145.6128 - loglik: -1.4377e+02 - logprior: -1.8454e+00
Epoch 4/10
13/13 - 1s - loss: 137.3036 - loglik: -1.3554e+02 - logprior: -1.7661e+00
Epoch 5/10
13/13 - 1s - loss: 134.4792 - loglik: -1.3287e+02 - logprior: -1.6046e+00
Epoch 6/10
13/13 - 1s - loss: 133.4030 - loglik: -1.3179e+02 - logprior: -1.6115e+00
Epoch 7/10
13/13 - 1s - loss: 133.0840 - loglik: -1.3153e+02 - logprior: -1.5521e+00
Epoch 8/10
13/13 - 1s - loss: 132.7405 - loglik: -1.3119e+02 - logprior: -1.5479e+00
Epoch 9/10
13/13 - 1s - loss: 132.7993 - loglik: -1.3127e+02 - logprior: -1.5316e+00
Fitted a model with MAP estimate = -132.4968
expansions: [(0, 4), (13, 1), (16, 1), (34, 1), (35, 2), (36, 1), (37, 1), (38, 2), (43, 3), (44, 2)]
discards: []
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 138.1544 - loglik: -1.2801e+02 - logprior: -1.0141e+01
Epoch 2/2
13/13 - 1s - loss: 121.5996 - loglik: -1.1860e+02 - logprior: -3.0025e+00
Fitted a model with MAP estimate = -118.8915
expansions: [(0, 2)]
discards: [49]
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 126.5223 - loglik: -1.1650e+02 - logprior: -1.0023e+01
Epoch 2/2
13/13 - 1s - loss: 118.1942 - loglik: -1.1502e+02 - logprior: -3.1712e+00
Fitted a model with MAP estimate = -116.1374
expansions: []
discards: []
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 122.3518 - loglik: -1.1444e+02 - logprior: -7.9110e+00
Epoch 2/10
13/13 - 1s - loss: 116.1982 - loglik: -1.1397e+02 - logprior: -2.2328e+00
Epoch 3/10
13/13 - 1s - loss: 115.2934 - loglik: -1.1361e+02 - logprior: -1.6884e+00
Epoch 4/10
13/13 - 1s - loss: 114.3779 - loglik: -1.1305e+02 - logprior: -1.3326e+00
Epoch 5/10
13/13 - 1s - loss: 114.2804 - loglik: -1.1304e+02 - logprior: -1.2444e+00
Epoch 6/10
13/13 - 2s - loss: 114.2925 - loglik: -1.1310e+02 - logprior: -1.1882e+00
Fitted a model with MAP estimate = -114.0366
Time for alignment: 48.4409
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 222.6131 - loglik: -2.1402e+02 - logprior: -8.5900e+00
Epoch 2/10
13/13 - 1s - loss: 173.9901 - loglik: -1.7177e+02 - logprior: -2.2219e+00
Epoch 3/10
13/13 - 1s - loss: 146.9156 - loglik: -1.4515e+02 - logprior: -1.7623e+00
Epoch 4/10
13/13 - 1s - loss: 138.9594 - loglik: -1.3733e+02 - logprior: -1.6283e+00
Epoch 5/10
13/13 - 1s - loss: 136.5295 - loglik: -1.3504e+02 - logprior: -1.4942e+00
Epoch 6/10
13/13 - 1s - loss: 135.5128 - loglik: -1.3400e+02 - logprior: -1.5093e+00
Epoch 7/10
13/13 - 2s - loss: 134.6288 - loglik: -1.3314e+02 - logprior: -1.4884e+00
Epoch 8/10
13/13 - 1s - loss: 134.0730 - loglik: -1.3258e+02 - logprior: -1.4897e+00
Epoch 9/10
13/13 - 1s - loss: 134.1264 - loglik: -1.3267e+02 - logprior: -1.4614e+00
Fitted a model with MAP estimate = -133.9348
expansions: [(0, 4), (13, 1), (16, 1), (36, 4), (37, 1), (42, 1), (43, 2), (44, 6)]
discards: []
Fitting a model of length 85 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 138.2952 - loglik: -1.2814e+02 - logprior: -1.0150e+01
Epoch 2/2
13/13 - 1s - loss: 121.3392 - loglik: -1.1819e+02 - logprior: -3.1475e+00
Fitted a model with MAP estimate = -117.7990
expansions: [(0, 2), (45, 1)]
discards: []
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 124.8899 - loglik: -1.1473e+02 - logprior: -1.0164e+01
Epoch 2/2
13/13 - 1s - loss: 116.3696 - loglik: -1.1304e+02 - logprior: -3.3278e+00
Fitted a model with MAP estimate = -114.0590
expansions: []
discards: []
Fitting a model of length 88 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 120.4494 - loglik: -1.1238e+02 - logprior: -8.0668e+00
Epoch 2/10
13/13 - 1s - loss: 113.9422 - loglik: -1.1155e+02 - logprior: -2.3902e+00
Epoch 3/10
13/13 - 1s - loss: 112.8981 - loglik: -1.1106e+02 - logprior: -1.8407e+00
Epoch 4/10
13/13 - 1s - loss: 112.6927 - loglik: -1.1121e+02 - logprior: -1.4857e+00
Epoch 5/10
13/13 - 1s - loss: 112.3347 - loglik: -1.1093e+02 - logprior: -1.4018e+00
Epoch 6/10
13/13 - 2s - loss: 111.8544 - loglik: -1.1053e+02 - logprior: -1.3263e+00
Epoch 7/10
13/13 - 2s - loss: 112.0550 - loglik: -1.1075e+02 - logprior: -1.3056e+00
Fitted a model with MAP estimate = -111.8081
Time for alignment: 49.4340
Computed alignments with likelihoods: ['-117.2817', '-114.0366', '-111.8081']
Best model has likelihood: -111.8081
SP score = 0.4917
Training of 3 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe195c44c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25dca7670>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.5296 - loglik: -5.8657e+01 - logprior: -8.7280e-01
Epoch 2/10
41/41 - 1s - loss: 46.3153 - loglik: -4.5416e+01 - logprior: -8.9951e-01
Epoch 3/10
41/41 - 1s - loss: 45.7836 - loglik: -4.4906e+01 - logprior: -8.7772e-01
Epoch 4/10
41/41 - 1s - loss: 45.5434 - loglik: -4.4668e+01 - logprior: -8.7503e-01
Epoch 5/10
41/41 - 1s - loss: 45.4263 - loglik: -4.4556e+01 - logprior: -8.7026e-01
Epoch 6/10
41/41 - 1s - loss: 45.4691 - loglik: -4.4602e+01 - logprior: -8.6740e-01
Fitted a model with MAP estimate = -44.9596
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 45.0982 - loglik: -4.4029e+01 - logprior: -1.0697e+00
Epoch 2/2
41/41 - 1s - loss: 43.7292 - loglik: -4.2879e+01 - logprior: -8.5034e-01
Fitted a model with MAP estimate = -42.9756
expansions: []
discards: [10]
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 44.2358 - loglik: -4.3187e+01 - logprior: -1.0492e+00
Epoch 2/2
41/41 - 1s - loss: 43.6412 - loglik: -4.2806e+01 - logprior: -8.3561e-01
Fitted a model with MAP estimate = -43.0161
expansions: []
discards: []
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.7927 - loglik: -4.2113e+01 - logprior: -6.7951e-01
Epoch 2/10
58/58 - 2s - loss: 42.4815 - loglik: -4.1900e+01 - logprior: -5.8180e-01
Epoch 3/10
58/58 - 2s - loss: 42.4012 - loglik: -4.1825e+01 - logprior: -5.7577e-01
Epoch 4/10
58/58 - 2s - loss: 42.1607 - loglik: -4.1589e+01 - logprior: -5.7138e-01
Epoch 5/10
58/58 - 2s - loss: 42.1980 - loglik: -4.1628e+01 - logprior: -5.7025e-01
Fitted a model with MAP estimate = -42.1042
Time for alignment: 57.3415
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.7149 - loglik: -5.8897e+01 - logprior: -8.1805e-01
Epoch 2/10
41/41 - 1s - loss: 46.4551 - loglik: -4.5714e+01 - logprior: -7.4077e-01
Epoch 3/10
41/41 - 1s - loss: 45.6541 - loglik: -4.4963e+01 - logprior: -6.9071e-01
Epoch 4/10
41/41 - 1s - loss: 45.5684 - loglik: -4.4880e+01 - logprior: -6.8813e-01
Epoch 5/10
41/41 - 1s - loss: 45.4306 - loglik: -4.4747e+01 - logprior: -6.8375e-01
Epoch 6/10
41/41 - 1s - loss: 45.3292 - loglik: -4.4646e+01 - logprior: -6.8281e-01
Epoch 7/10
41/41 - 1s - loss: 45.2622 - loglik: -4.4582e+01 - logprior: -6.8045e-01
Epoch 8/10
41/41 - 1s - loss: 45.3800 - loglik: -4.4699e+01 - logprior: -6.8073e-01
Fitted a model with MAP estimate = -44.8993
expansions: [(4, 1), (8, 2), (9, 2), (10, 2), (11, 1), (12, 1)]
discards: [0]
Fitting a model of length 26 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 47.3543 - loglik: -4.6242e+01 - logprior: -1.1121e+00
Epoch 2/2
41/41 - 1s - loss: 43.8163 - loglik: -4.2947e+01 - logprior: -8.6906e-01
Fitted a model with MAP estimate = -42.9179
expansions: []
discards: [ 8 12 14]
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.3238 - loglik: -4.3274e+01 - logprior: -1.0498e+00
Epoch 2/2
41/41 - 1s - loss: 43.6091 - loglik: -4.2775e+01 - logprior: -8.3443e-01
Fitted a model with MAP estimate = -42.9855
expansions: []
discards: []
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 42.7300 - loglik: -4.2050e+01 - logprior: -6.7955e-01
Epoch 2/10
58/58 - 2s - loss: 42.5005 - loglik: -4.1919e+01 - logprior: -5.8184e-01
Epoch 3/10
58/58 - 2s - loss: 42.4206 - loglik: -4.1846e+01 - logprior: -5.7483e-01
Epoch 4/10
58/58 - 2s - loss: 42.1891 - loglik: -4.1617e+01 - logprior: -5.7238e-01
Epoch 5/10
58/58 - 2s - loss: 42.1609 - loglik: -4.1592e+01 - logprior: -5.6850e-01
Epoch 6/10
58/58 - 2s - loss: 42.1185 - loglik: -4.1550e+01 - logprior: -5.6897e-01
Epoch 7/10
58/58 - 2s - loss: 42.0153 - loglik: -4.1449e+01 - logprior: -5.6660e-01
Epoch 8/10
58/58 - 2s - loss: 42.0574 - loglik: -4.1492e+01 - logprior: -5.6537e-01
Fitted a model with MAP estimate = -42.0442
Time for alignment: 64.6864
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 59.8022 - loglik: -5.8940e+01 - logprior: -8.6186e-01
Epoch 2/10
41/41 - 1s - loss: 46.4257 - loglik: -4.5521e+01 - logprior: -9.0475e-01
Epoch 3/10
41/41 - 1s - loss: 45.6527 - loglik: -4.4771e+01 - logprior: -8.8141e-01
Epoch 4/10
41/41 - 1s - loss: 45.6713 - loglik: -4.4799e+01 - logprior: -8.7202e-01
Fitted a model with MAP estimate = -44.9172
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 1)]
discards: []
Fitting a model of length 24 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 45.1731 - loglik: -4.4104e+01 - logprior: -1.0690e+00
Epoch 2/2
41/41 - 1s - loss: 43.6246 - loglik: -4.2773e+01 - logprior: -8.5162e-01
Fitted a model with MAP estimate = -42.9756
expansions: []
discards: [10]
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 44.2888 - loglik: -4.3239e+01 - logprior: -1.0501e+00
Epoch 2/2
41/41 - 1s - loss: 43.5548 - loglik: -4.2721e+01 - logprior: -8.3342e-01
Fitted a model with MAP estimate = -43.0280
expansions: []
discards: []
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 6s - loss: 42.8310 - loglik: -4.2150e+01 - logprior: -6.8137e-01
Epoch 2/10
58/58 - 2s - loss: 42.4048 - loglik: -4.1822e+01 - logprior: -5.8228e-01
Epoch 3/10
58/58 - 2s - loss: 42.4178 - loglik: -4.1844e+01 - logprior: -5.7392e-01
Fitted a model with MAP estimate = -42.1877
Time for alignment: 50.7347
Computed alignments with likelihoods: ['-42.1042', '-42.0442', '-42.1877']
Best model has likelihood: -42.0442
SP score = 0.9665
Training of 3 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2c387eac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f8ed34c0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0226 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.7329 - loglik: -1.3494e+02 - logprior: -3.3790e+01
Epoch 3/10
10/10 - 1s - loss: 133.8678 - loglik: -1.1783e+02 - logprior: -1.6039e+01
Epoch 4/10
10/10 - 1s - loss: 118.0599 - loglik: -1.0844e+02 - logprior: -9.6160e+00
Epoch 5/10
10/10 - 1s - loss: 110.1340 - loglik: -1.0407e+02 - logprior: -6.0646e+00
Epoch 6/10
10/10 - 1s - loss: 106.4388 - loglik: -1.0246e+02 - logprior: -3.9836e+00
Epoch 7/10
10/10 - 0s - loss: 104.6747 - loglik: -1.0204e+02 - logprior: -2.6382e+00
Epoch 8/10
10/10 - 0s - loss: 103.6186 - loglik: -1.0187e+02 - logprior: -1.7491e+00
Epoch 9/10
10/10 - 1s - loss: 102.7236 - loglik: -1.0159e+02 - logprior: -1.1301e+00
Epoch 10/10
10/10 - 1s - loss: 101.5410 - loglik: -1.0084e+02 - logprior: -6.9865e-01
Fitted a model with MAP estimate = -101.1207
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 239.2541 - loglik: -1.0024e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 0s - loss: 152.2032 - loglik: -9.4709e+01 - logprior: -5.7494e+01
Fitted a model with MAP estimate = -137.9748
expansions: [(0, 2)]
discards: [ 0  8 20 29 43]
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 202.9596 - loglik: -9.1417e+01 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 1s - loss: 120.0048 - loglik: -9.0037e+01 - logprior: -2.9968e+01
Fitted a model with MAP estimate = -107.8781
expansions: []
discards: [0]
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 220.7513 - loglik: -9.1738e+01 - logprior: -1.2901e+02
Epoch 2/10
10/10 - 1s - loss: 128.0834 - loglik: -9.1368e+01 - logprior: -3.6716e+01
Epoch 3/10
10/10 - 1s - loss: 105.7123 - loglik: -9.1559e+01 - logprior: -1.4154e+01
Epoch 4/10
10/10 - 1s - loss: 97.8696 - loglik: -9.1901e+01 - logprior: -5.9684e+00
Epoch 5/10
10/10 - 1s - loss: 94.0255 - loglik: -9.2191e+01 - logprior: -1.8344e+00
Epoch 6/10
10/10 - 1s - loss: 91.8732 - loglik: -9.2390e+01 - logprior: 0.5172
Epoch 7/10
10/10 - 1s - loss: 90.5888 - loglik: -9.2515e+01 - logprior: 1.9267
Epoch 8/10
10/10 - 1s - loss: 89.7594 - loglik: -9.2616e+01 - logprior: 2.8567
Epoch 9/10
10/10 - 1s - loss: 89.1651 - loglik: -9.2705e+01 - logprior: 3.5403
Epoch 10/10
10/10 - 1s - loss: 88.6988 - loglik: -9.2801e+01 - logprior: 4.1025
Fitted a model with MAP estimate = -88.4735
Time for alignment: 27.9822
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 281.0226 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.7329 - loglik: -1.3494e+02 - logprior: -3.3790e+01
Epoch 3/10
10/10 - 1s - loss: 133.8678 - loglik: -1.1783e+02 - logprior: -1.6039e+01
Epoch 4/10
10/10 - 1s - loss: 118.0599 - loglik: -1.0844e+02 - logprior: -9.6160e+00
Epoch 5/10
10/10 - 1s - loss: 110.1340 - loglik: -1.0407e+02 - logprior: -6.0646e+00
Epoch 6/10
10/10 - 1s - loss: 106.4388 - loglik: -1.0246e+02 - logprior: -3.9836e+00
Epoch 7/10
10/10 - 1s - loss: 104.6747 - loglik: -1.0204e+02 - logprior: -2.6382e+00
Epoch 8/10
10/10 - 1s - loss: 103.6186 - loglik: -1.0187e+02 - logprior: -1.7491e+00
Epoch 9/10
10/10 - 1s - loss: 102.7236 - loglik: -1.0159e+02 - logprior: -1.1301e+00
Epoch 10/10
10/10 - 1s - loss: 101.5410 - loglik: -1.0084e+02 - logprior: -6.9865e-01
Fitted a model with MAP estimate = -101.1207
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 239.2541 - loglik: -1.0024e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 1s - loss: 152.2032 - loglik: -9.4709e+01 - logprior: -5.7494e+01
Fitted a model with MAP estimate = -137.9748
expansions: [(0, 2)]
discards: [ 0  8 20 29 43]
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 202.9596 - loglik: -9.1417e+01 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 1s - loss: 120.0048 - loglik: -9.0037e+01 - logprior: -2.9968e+01
Fitted a model with MAP estimate = -107.8781
expansions: []
discards: [0]
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 220.7513 - loglik: -9.1738e+01 - logprior: -1.2901e+02
Epoch 2/10
10/10 - 1s - loss: 128.0834 - loglik: -9.1368e+01 - logprior: -3.6716e+01
Epoch 3/10
10/10 - 1s - loss: 105.7123 - loglik: -9.1559e+01 - logprior: -1.4154e+01
Epoch 4/10
10/10 - 1s - loss: 97.8696 - loglik: -9.1901e+01 - logprior: -5.9684e+00
Epoch 5/10
10/10 - 1s - loss: 94.0255 - loglik: -9.2191e+01 - logprior: -1.8344e+00
Epoch 6/10
10/10 - 1s - loss: 91.8732 - loglik: -9.2390e+01 - logprior: 0.5172
Epoch 7/10
10/10 - 1s - loss: 90.5888 - loglik: -9.2515e+01 - logprior: 1.9267
Epoch 8/10
10/10 - 1s - loss: 89.7594 - loglik: -9.2616e+01 - logprior: 2.8567
Epoch 9/10
10/10 - 1s - loss: 89.1651 - loglik: -9.2705e+01 - logprior: 3.5403
Epoch 10/10
10/10 - 1s - loss: 88.6988 - loglik: -9.2801e+01 - logprior: 4.1025
Fitted a model with MAP estimate = -88.4735
Time for alignment: 26.8730
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 281.0226 - loglik: -1.5685e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 168.7329 - loglik: -1.3494e+02 - logprior: -3.3790e+01
Epoch 3/10
10/10 - 1s - loss: 133.8678 - loglik: -1.1783e+02 - logprior: -1.6039e+01
Epoch 4/10
10/10 - 1s - loss: 118.0599 - loglik: -1.0844e+02 - logprior: -9.6160e+00
Epoch 5/10
10/10 - 1s - loss: 110.1340 - loglik: -1.0407e+02 - logprior: -6.0646e+00
Epoch 6/10
10/10 - 0s - loss: 106.4388 - loglik: -1.0246e+02 - logprior: -3.9836e+00
Epoch 7/10
10/10 - 0s - loss: 104.6747 - loglik: -1.0204e+02 - logprior: -2.6382e+00
Epoch 8/10
10/10 - 1s - loss: 103.6186 - loglik: -1.0187e+02 - logprior: -1.7491e+00
Epoch 9/10
10/10 - 1s - loss: 102.7236 - loglik: -1.0159e+02 - logprior: -1.1301e+00
Epoch 10/10
10/10 - 1s - loss: 101.5410 - loglik: -1.0084e+02 - logprior: -6.9865e-01
Fitted a model with MAP estimate = -101.1207
expansions: [(9, 2), (13, 1), (15, 2), (16, 2), (23, 3), (31, 2), (32, 2)]
discards: [0]
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 239.2541 - loglik: -1.0024e+02 - logprior: -1.3901e+02
Epoch 2/2
10/10 - 1s - loss: 152.2032 - loglik: -9.4709e+01 - logprior: -5.7494e+01
Fitted a model with MAP estimate = -137.9748
expansions: [(0, 2)]
discards: [ 0  8 20 29 43]
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 202.9596 - loglik: -9.1417e+01 - logprior: -1.1154e+02
Epoch 2/2
10/10 - 1s - loss: 120.0048 - loglik: -9.0037e+01 - logprior: -2.9968e+01
Fitted a model with MAP estimate = -107.8781
expansions: []
discards: [0]
Fitting a model of length 52 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 220.7513 - loglik: -9.1738e+01 - logprior: -1.2901e+02
Epoch 2/10
10/10 - 1s - loss: 128.0834 - loglik: -9.1368e+01 - logprior: -3.6716e+01
Epoch 3/10
10/10 - 1s - loss: 105.7123 - loglik: -9.1559e+01 - logprior: -1.4154e+01
Epoch 4/10
10/10 - 1s - loss: 97.8696 - loglik: -9.1901e+01 - logprior: -5.9684e+00
Epoch 5/10
10/10 - 1s - loss: 94.0255 - loglik: -9.2191e+01 - logprior: -1.8344e+00
Epoch 6/10
10/10 - 1s - loss: 91.8732 - loglik: -9.2390e+01 - logprior: 0.5172
Epoch 7/10
10/10 - 0s - loss: 90.5888 - loglik: -9.2515e+01 - logprior: 1.9267
Epoch 8/10
10/10 - 1s - loss: 89.7594 - loglik: -9.2616e+01 - logprior: 2.8567
Epoch 9/10
10/10 - 1s - loss: 89.1651 - loglik: -9.2705e+01 - logprior: 3.5403
Epoch 10/10
10/10 - 1s - loss: 88.6988 - loglik: -9.2801e+01 - logprior: 4.1025
Fitted a model with MAP estimate = -88.4735
Time for alignment: 28.0371
Computed alignments with likelihoods: ['-88.4735', '-88.4735', '-88.4735']
Best model has likelihood: -88.4735
SP score = 0.8919
Training of 3 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe277179dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2aa2119a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.3659 - loglik: -1.9225e+02 - logprior: -3.1208e+00
Epoch 2/10
19/19 - 1s - loss: 157.1984 - loglik: -1.5597e+02 - logprior: -1.2271e+00
Epoch 3/10
19/19 - 1s - loss: 141.7352 - loglik: -1.4042e+02 - logprior: -1.3122e+00
Epoch 4/10
19/19 - 1s - loss: 139.3231 - loglik: -1.3804e+02 - logprior: -1.2798e+00
Epoch 5/10
19/19 - 1s - loss: 138.6647 - loglik: -1.3744e+02 - logprior: -1.2261e+00
Epoch 6/10
19/19 - 1s - loss: 138.5009 - loglik: -1.3730e+02 - logprior: -1.1990e+00
Epoch 7/10
19/19 - 1s - loss: 138.4251 - loglik: -1.3725e+02 - logprior: -1.1757e+00
Epoch 8/10
19/19 - 1s - loss: 138.2983 - loglik: -1.3714e+02 - logprior: -1.1577e+00
Epoch 9/10
19/19 - 1s - loss: 138.0843 - loglik: -1.3693e+02 - logprior: -1.1536e+00
Epoch 10/10
19/19 - 1s - loss: 138.2294 - loglik: -1.3708e+02 - logprior: -1.1465e+00
Fitted a model with MAP estimate = -138.5452
expansions: [(0, 3), (13, 2), (14, 1), (19, 1), (20, 1), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.3238 - loglik: -1.3700e+02 - logprior: -4.3218e+00
Epoch 2/2
19/19 - 1s - loss: 132.1531 - loglik: -1.3066e+02 - logprior: -1.4940e+00
Fitted a model with MAP estimate = -132.3604
expansions: [(0, 2)]
discards: [48 59 64]
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 134.3135 - loglik: -1.3042e+02 - logprior: -3.8948e+00
Epoch 2/2
19/19 - 1s - loss: 129.3359 - loglik: -1.2795e+02 - logprior: -1.3894e+00
Fitted a model with MAP estimate = -129.2612
expansions: []
discards: [0]
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 133.0014 - loglik: -1.2958e+02 - logprior: -3.4222e+00
Epoch 2/10
21/21 - 1s - loss: 129.8718 - loglik: -1.2813e+02 - logprior: -1.7465e+00
Epoch 3/10
21/21 - 1s - loss: 128.7088 - loglik: -1.2755e+02 - logprior: -1.1610e+00
Epoch 4/10
21/21 - 1s - loss: 128.1977 - loglik: -1.2713e+02 - logprior: -1.0660e+00
Epoch 5/10
21/21 - 1s - loss: 127.7712 - loglik: -1.2670e+02 - logprior: -1.0745e+00
Epoch 6/10
21/21 - 1s - loss: 127.4032 - loglik: -1.2634e+02 - logprior: -1.0587e+00
Epoch 7/10
21/21 - 1s - loss: 127.3593 - loglik: -1.2632e+02 - logprior: -1.0390e+00
Epoch 8/10
21/21 - 1s - loss: 127.4948 - loglik: -1.2647e+02 - logprior: -1.0246e+00
Fitted a model with MAP estimate = -127.3289
Time for alignment: 50.4180
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.5452 - loglik: -1.9243e+02 - logprior: -3.1200e+00
Epoch 2/10
19/19 - 1s - loss: 156.6914 - loglik: -1.5546e+02 - logprior: -1.2287e+00
Epoch 3/10
19/19 - 1s - loss: 142.1396 - loglik: -1.4083e+02 - logprior: -1.3076e+00
Epoch 4/10
19/19 - 1s - loss: 140.2345 - loglik: -1.3896e+02 - logprior: -1.2704e+00
Epoch 5/10
19/19 - 1s - loss: 139.5622 - loglik: -1.3835e+02 - logprior: -1.2159e+00
Epoch 6/10
19/19 - 1s - loss: 139.0824 - loglik: -1.3788e+02 - logprior: -1.1989e+00
Epoch 7/10
19/19 - 1s - loss: 138.9746 - loglik: -1.3779e+02 - logprior: -1.1867e+00
Epoch 8/10
19/19 - 1s - loss: 139.1573 - loglik: -1.3798e+02 - logprior: -1.1777e+00
Fitted a model with MAP estimate = -139.2401
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (19, 1), (33, 1), (34, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.5740 - loglik: -1.3733e+02 - logprior: -4.2420e+00
Epoch 2/2
19/19 - 1s - loss: 131.8558 - loglik: -1.3037e+02 - logprior: -1.4814e+00
Fitted a model with MAP estimate = -131.9420
expansions: [(0, 2)]
discards: [48 59 64]
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 133.9212 - loglik: -1.3001e+02 - logprior: -3.9126e+00
Epoch 2/2
19/19 - 1s - loss: 129.0190 - loglik: -1.2770e+02 - logprior: -1.3215e+00
Fitted a model with MAP estimate = -129.5532
expansions: []
discards: [0]
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 133.5062 - loglik: -1.3009e+02 - logprior: -3.4194e+00
Epoch 2/10
21/21 - 1s - loss: 130.3662 - loglik: -1.2873e+02 - logprior: -1.6396e+00
Epoch 3/10
21/21 - 1s - loss: 129.1888 - loglik: -1.2804e+02 - logprior: -1.1494e+00
Epoch 4/10
21/21 - 1s - loss: 128.8347 - loglik: -1.2775e+02 - logprior: -1.0893e+00
Epoch 5/10
21/21 - 1s - loss: 128.2566 - loglik: -1.2717e+02 - logprior: -1.0865e+00
Epoch 6/10
21/21 - 1s - loss: 128.4172 - loglik: -1.2735e+02 - logprior: -1.0689e+00
Fitted a model with MAP estimate = -128.1413
Time for alignment: 45.4160
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 195.6262 - loglik: -1.9250e+02 - logprior: -3.1223e+00
Epoch 2/10
19/19 - 1s - loss: 157.5288 - loglik: -1.5628e+02 - logprior: -1.2469e+00
Epoch 3/10
19/19 - 1s - loss: 142.3319 - loglik: -1.4101e+02 - logprior: -1.3190e+00
Epoch 4/10
19/19 - 1s - loss: 140.0824 - loglik: -1.3879e+02 - logprior: -1.2884e+00
Epoch 5/10
19/19 - 1s - loss: 139.4619 - loglik: -1.3823e+02 - logprior: -1.2351e+00
Epoch 6/10
19/19 - 1s - loss: 139.1457 - loglik: -1.3793e+02 - logprior: -1.2118e+00
Epoch 7/10
19/19 - 1s - loss: 139.0051 - loglik: -1.3781e+02 - logprior: -1.1917e+00
Epoch 8/10
19/19 - 1s - loss: 138.7837 - loglik: -1.3760e+02 - logprior: -1.1801e+00
Epoch 9/10
19/19 - 1s - loss: 138.9671 - loglik: -1.3779e+02 - logprior: -1.1732e+00
Fitted a model with MAP estimate = -139.3650
expansions: [(0, 3), (12, 1), (13, 1), (14, 1), (19, 1), (33, 1), (34, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 141.6501 - loglik: -1.3736e+02 - logprior: -4.2888e+00
Epoch 2/2
19/19 - 1s - loss: 131.9268 - loglik: -1.3044e+02 - logprior: -1.4910e+00
Fitted a model with MAP estimate = -131.8365
expansions: [(0, 2)]
discards: [48 59 64]
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 133.7675 - loglik: -1.2985e+02 - logprior: -3.9155e+00
Epoch 2/2
19/19 - 1s - loss: 129.0816 - loglik: -1.2776e+02 - logprior: -1.3226e+00
Fitted a model with MAP estimate = -129.5979
expansions: []
discards: [0]
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 133.5499 - loglik: -1.3014e+02 - logprior: -3.4147e+00
Epoch 2/10
21/21 - 1s - loss: 130.4867 - loglik: -1.2883e+02 - logprior: -1.6567e+00
Epoch 3/10
21/21 - 1s - loss: 128.9758 - loglik: -1.2783e+02 - logprior: -1.1439e+00
Epoch 4/10
21/21 - 1s - loss: 128.3475 - loglik: -1.2727e+02 - logprior: -1.0798e+00
Epoch 5/10
21/21 - 1s - loss: 127.8865 - loglik: -1.2681e+02 - logprior: -1.0789e+00
Epoch 6/10
21/21 - 1s - loss: 127.8510 - loglik: -1.2678e+02 - logprior: -1.0663e+00
Epoch 7/10
21/21 - 1s - loss: 128.0124 - loglik: -1.2697e+02 - logprior: -1.0429e+00
Fitted a model with MAP estimate = -127.7403
Time for alignment: 47.2262
Computed alignments with likelihoods: ['-127.3289', '-128.1413', '-127.7403']
Best model has likelihood: -127.3289
SP score = 0.9451
Training of 3 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe29084db20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2538e5f70>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 390.7566 - loglik: -3.8264e+02 - logprior: -8.1169e+00
Epoch 2/10
13/13 - 2s - loss: 337.9270 - loglik: -3.3605e+02 - logprior: -1.8798e+00
Epoch 3/10
13/13 - 2s - loss: 297.2130 - loglik: -2.9548e+02 - logprior: -1.7328e+00
Epoch 4/10
13/13 - 2s - loss: 283.6774 - loglik: -2.8172e+02 - logprior: -1.9612e+00
Epoch 5/10
13/13 - 2s - loss: 279.7126 - loglik: -2.7769e+02 - logprior: -2.0246e+00
Epoch 6/10
13/13 - 2s - loss: 278.1227 - loglik: -2.7616e+02 - logprior: -1.9662e+00
Epoch 7/10
13/13 - 2s - loss: 277.1984 - loglik: -2.7525e+02 - logprior: -1.9446e+00
Epoch 8/10
13/13 - 2s - loss: 277.3075 - loglik: -2.7535e+02 - logprior: -1.9579e+00
Fitted a model with MAP estimate = -276.8713
expansions: [(7, 2), (8, 3), (11, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 2), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 285.4485 - loglik: -2.7596e+02 - logprior: -9.4877e+00
Epoch 2/2
13/13 - 3s - loss: 265.0279 - loglik: -2.6104e+02 - logprior: -3.9916e+00
Fitted a model with MAP estimate = -262.6272
expansions: [(0, 3)]
discards: [ 0  9 69 95]
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 266.9234 - loglik: -2.5954e+02 - logprior: -7.3797e+00
Epoch 2/2
13/13 - 3s - loss: 258.5994 - loglik: -2.5680e+02 - logprior: -1.7998e+00
Fitted a model with MAP estimate = -256.6372
expansions: []
discards: [ 0  2 89]
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 268.8951 - loglik: -2.5966e+02 - logprior: -9.2392e+00
Epoch 2/10
13/13 - 2s - loss: 261.7494 - loglik: -2.5889e+02 - logprior: -2.8560e+00
Epoch 3/10
13/13 - 2s - loss: 257.0258 - loglik: -2.5594e+02 - logprior: -1.0844e+00
Epoch 4/10
13/13 - 2s - loss: 256.2724 - loglik: -2.5579e+02 - logprior: -4.7998e-01
Epoch 5/10
13/13 - 2s - loss: 254.7576 - loglik: -2.5439e+02 - logprior: -3.6472e-01
Epoch 6/10
13/13 - 2s - loss: 254.8335 - loglik: -2.5451e+02 - logprior: -3.2025e-01
Fitted a model with MAP estimate = -254.1900
Time for alignment: 67.7010
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 391.1543 - loglik: -3.8304e+02 - logprior: -8.1161e+00
Epoch 2/10
13/13 - 2s - loss: 338.3163 - loglik: -3.3643e+02 - logprior: -1.8825e+00
Epoch 3/10
13/13 - 2s - loss: 296.8942 - loglik: -2.9512e+02 - logprior: -1.7706e+00
Epoch 4/10
13/13 - 2s - loss: 282.7283 - loglik: -2.8067e+02 - logprior: -2.0587e+00
Epoch 5/10
13/13 - 2s - loss: 277.8299 - loglik: -2.7570e+02 - logprior: -2.1262e+00
Epoch 6/10
13/13 - 2s - loss: 277.7666 - loglik: -2.7573e+02 - logprior: -2.0374e+00
Epoch 7/10
13/13 - 2s - loss: 274.9841 - loglik: -2.7296e+02 - logprior: -2.0240e+00
Epoch 8/10
13/13 - 2s - loss: 275.7809 - loglik: -2.7375e+02 - logprior: -2.0276e+00
Fitted a model with MAP estimate = -275.6184
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (61, 2), (70, 3), (75, 1), (76, 1), (77, 2), (79, 1), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 283.4279 - loglik: -2.7397e+02 - logprior: -9.4558e+00
Epoch 2/2
13/13 - 2s - loss: 264.5676 - loglik: -2.6071e+02 - logprior: -3.8537e+00
Fitted a model with MAP estimate = -262.1141
expansions: [(0, 3)]
discards: [ 0  9 77 88 89]
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 268.1308 - loglik: -2.6087e+02 - logprior: -7.2594e+00
Epoch 2/2
13/13 - 2s - loss: 258.2360 - loglik: -2.5656e+02 - logprior: -1.6786e+00
Fitted a model with MAP estimate = -257.0031
expansions: []
discards: [0 2]
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 269.5273 - loglik: -2.6037e+02 - logprior: -9.1555e+00
Epoch 2/10
13/13 - 2s - loss: 260.5016 - loglik: -2.5780e+02 - logprior: -2.7003e+00
Epoch 3/10
13/13 - 2s - loss: 257.5770 - loglik: -2.5660e+02 - logprior: -9.7391e-01
Epoch 4/10
13/13 - 2s - loss: 255.7551 - loglik: -2.5538e+02 - logprior: -3.7352e-01
Epoch 5/10
13/13 - 2s - loss: 255.8595 - loglik: -2.5562e+02 - logprior: -2.4117e-01
Fitted a model with MAP estimate = -254.8246
Time for alignment: 62.7466
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 390.9162 - loglik: -3.8280e+02 - logprior: -8.1188e+00
Epoch 2/10
13/13 - 2s - loss: 337.8630 - loglik: -3.3599e+02 - logprior: -1.8770e+00
Epoch 3/10
13/13 - 2s - loss: 295.6089 - loglik: -2.9389e+02 - logprior: -1.7143e+00
Epoch 4/10
13/13 - 2s - loss: 283.1322 - loglik: -2.8118e+02 - logprior: -1.9552e+00
Epoch 5/10
13/13 - 2s - loss: 280.6229 - loglik: -2.7862e+02 - logprior: -1.9989e+00
Epoch 6/10
13/13 - 2s - loss: 277.6100 - loglik: -2.7566e+02 - logprior: -1.9461e+00
Epoch 7/10
13/13 - 2s - loss: 277.8148 - loglik: -2.7588e+02 - logprior: -1.9338e+00
Fitted a model with MAP estimate = -277.3401
expansions: [(7, 2), (8, 3), (11, 1), (13, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 2), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Fitting a model of length 143 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 284.6359 - loglik: -2.7516e+02 - logprior: -9.4759e+00
Epoch 2/2
13/13 - 3s - loss: 265.6680 - loglik: -2.6174e+02 - logprior: -3.9324e+00
Fitted a model with MAP estimate = -262.6343
expansions: [(0, 3)]
discards: [ 0  9 69 95]
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 267.2906 - loglik: -2.5995e+02 - logprior: -7.3370e+00
Epoch 2/2
13/13 - 3s - loss: 257.7127 - loglik: -2.5597e+02 - logprior: -1.7418e+00
Fitted a model with MAP estimate = -256.5675
expansions: []
discards: [ 0  2 24 89]
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 269.4754 - loglik: -2.6032e+02 - logprior: -9.1506e+00
Epoch 2/10
13/13 - 2s - loss: 261.3589 - loglik: -2.5867e+02 - logprior: -2.6839e+00
Epoch 3/10
13/13 - 2s - loss: 257.1085 - loglik: -2.5614e+02 - logprior: -9.6755e-01
Epoch 4/10
13/13 - 2s - loss: 256.2623 - loglik: -2.5586e+02 - logprior: -4.0052e-01
Epoch 5/10
13/13 - 2s - loss: 255.9099 - loglik: -2.5565e+02 - logprior: -2.6252e-01
Epoch 6/10
13/13 - 2s - loss: 254.6287 - loglik: -2.5441e+02 - logprior: -2.2166e-01
Epoch 7/10
13/13 - 2s - loss: 254.2289 - loglik: -2.5401e+02 - logprior: -2.1782e-01
Epoch 8/10
13/13 - 2s - loss: 254.6676 - loglik: -2.5446e+02 - logprior: -2.1136e-01
Fitted a model with MAP estimate = -253.9522
Time for alignment: 69.7332
Computed alignments with likelihoods: ['-254.1900', '-254.8246', '-253.9522']
Best model has likelihood: -253.9522
SP score = 0.9431
Training of 3 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe174520a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe17477eb80>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1083.3436 - loglik: -1.0800e+03 - logprior: -3.3873e+00
Epoch 2/10
25/25 - 29s - loss: 820.4863 - loglik: -8.1966e+02 - logprior: -8.2657e-01
Epoch 3/10
25/25 - 29s - loss: 772.2280 - loglik: -7.7034e+02 - logprior: -1.8919e+00
Epoch 4/10
25/25 - 29s - loss: 763.4021 - loglik: -7.6131e+02 - logprior: -2.0903e+00
Epoch 5/10
25/25 - 29s - loss: 764.3723 - loglik: -7.6232e+02 - logprior: -2.0548e+00
Fitted a model with MAP estimate = -760.4239
expansions: [(43, 1), (106, 1), (143, 1), (144, 1), (145, 1), (161, 1), (162, 1), (167, 2), (173, 4), (174, 2), (176, 2), (191, 3), (192, 4), (193, 1), (194, 1), (197, 1), (198, 3), (199, 1), (201, 1), (202, 1), (204, 1), (206, 1), (208, 1), (209, 1), (210, 1), (211, 1), (212, 1), (214, 1), (216, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (228, 1), (229, 1), (230, 1), (233, 1), (236, 2), (237, 1), (248, 1), (252, 1), (255, 5), (256, 1), (258, 3), (281, 1), (282, 1), (283, 1), (297, 1), (299, 1), (300, 2), (301, 2), (318, 2), (319, 2), (325, 1), (326, 1), (327, 2), (328, 1), (340, 1), (358, 1), (364, 1), (366, 1)]
discards: []
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 743.3715 - loglik: -7.3920e+02 - logprior: -4.1742e+00
Epoch 2/2
25/25 - 41s - loss: 707.1434 - loglik: -7.0651e+02 - logprior: -6.2862e-01
Fitted a model with MAP estimate = -704.5703
expansions: [(410, 1)]
discards: [174 184 185 191 214 372]
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 712.6588 - loglik: -7.0960e+02 - logprior: -3.0637e+00
Epoch 2/2
25/25 - 40s - loss: 699.6593 - loglik: -7.0070e+02 - logprior: 1.0375
Fitted a model with MAP estimate = -698.0236
expansions: [(128, 1)]
discards: [303 310 417]
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 710.5602 - loglik: -7.0811e+02 - logprior: -2.4474e+00
Epoch 2/10
25/25 - 40s - loss: 696.5510 - loglik: -6.9807e+02 - logprior: 1.5169
Epoch 3/10
25/25 - 40s - loss: 697.4313 - loglik: -6.9942e+02 - logprior: 1.9878
Fitted a model with MAP estimate = -695.7868
Time for alignment: 585.2629
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 33s - loss: 1077.5643 - loglik: -1.0742e+03 - logprior: -3.3280e+00
Epoch 2/10
25/25 - 29s - loss: 823.4124 - loglik: -8.2243e+02 - logprior: -9.7842e-01
Epoch 3/10
25/25 - 29s - loss: 768.6171 - loglik: -7.6649e+02 - logprior: -2.1261e+00
Epoch 4/10
25/25 - 29s - loss: 758.5464 - loglik: -7.5611e+02 - logprior: -2.4406e+00
Epoch 5/10
25/25 - 29s - loss: 755.2241 - loglik: -7.5276e+02 - logprior: -2.4600e+00
Epoch 6/10
25/25 - 29s - loss: 754.8007 - loglik: -7.5228e+02 - logprior: -2.5257e+00
Epoch 7/10
25/25 - 29s - loss: 754.8733 - loglik: -7.5237e+02 - logprior: -2.5025e+00
Fitted a model with MAP estimate = -753.5766
expansions: [(43, 1), (125, 1), (140, 1), (143, 1), (144, 1), (163, 1), (164, 1), (168, 2), (174, 5), (175, 3), (178, 1), (189, 1), (190, 1), (191, 3), (192, 1), (193, 2), (196, 1), (197, 1), (198, 1), (199, 1), (201, 1), (202, 1), (204, 1), (206, 1), (208, 1), (209, 1), (212, 1), (213, 1), (218, 1), (219, 1), (224, 1), (225, 3), (226, 1), (227, 1), (229, 1), (230, 1), (231, 1), (236, 1), (238, 1), (239, 1), (250, 1), (254, 1), (256, 3), (257, 2), (259, 1), (260, 1), (266, 1), (267, 1), (281, 1), (282, 1), (283, 1), (299, 3), (300, 1), (301, 1), (304, 1), (313, 1), (314, 1), (316, 1), (318, 1), (321, 2), (326, 1), (341, 1), (356, 1), (358, 1), (360, 1), (363, 1)]
discards: [327]
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 743.4487 - loglik: -7.3921e+02 - logprior: -4.2402e+00
Epoch 2/2
25/25 - 40s - loss: 711.9282 - loglik: -7.1132e+02 - logprior: -6.1090e-01
Fitted a model with MAP estimate = -708.9145
expansions: [(418, 2), (434, 1)]
discards: [175 187 212 213 217 366 397 432]
Fitting a model of length 448 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 42s - loss: 720.2620 - loglik: -7.1728e+02 - logprior: -2.9806e+00
Epoch 2/2
25/25 - 39s - loss: 707.1010 - loglik: -7.0794e+02 - logprior: 0.8342
Fitted a model with MAP estimate = -706.2031
expansions: [(210, 2), (411, 1), (426, 2)]
discards: [184]
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 715.0058 - loglik: -7.1185e+02 - logprior: -3.1574e+00
Epoch 2/10
25/25 - 40s - loss: 701.5887 - loglik: -7.0232e+02 - logprior: 0.7354
Epoch 3/10
25/25 - 40s - loss: 694.5723 - loglik: -6.9664e+02 - logprior: 2.0715
Epoch 4/10
25/25 - 40s - loss: 692.2586 - loglik: -6.9465e+02 - logprior: 2.3964
Epoch 5/10
25/25 - 40s - loss: 692.6499 - loglik: -6.9519e+02 - logprior: 2.5360
Fitted a model with MAP estimate = -691.8598
Time for alignment: 719.2902
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 1083.9767 - loglik: -1.0807e+03 - logprior: -3.3253e+00
Epoch 2/10
25/25 - 29s - loss: 825.8725 - loglik: -8.2493e+02 - logprior: -9.4177e-01
Epoch 3/10
25/25 - 29s - loss: 776.5285 - loglik: -7.7444e+02 - logprior: -2.0873e+00
Epoch 4/10
25/25 - 29s - loss: 761.3911 - loglik: -7.5916e+02 - logprior: -2.2346e+00
Epoch 5/10
25/25 - 29s - loss: 760.9426 - loglik: -7.5882e+02 - logprior: -2.1250e+00
Epoch 6/10
25/25 - 29s - loss: 755.9869 - loglik: -7.5382e+02 - logprior: -2.1651e+00
Epoch 7/10
25/25 - 29s - loss: 758.9813 - loglik: -7.5685e+02 - logprior: -2.1302e+00
Fitted a model with MAP estimate = -757.1172
expansions: [(43, 1), (71, 1), (131, 1), (143, 3), (144, 1), (162, 2), (167, 1), (172, 2), (173, 2), (174, 2), (188, 1), (189, 2), (190, 4), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (198, 1), (200, 1), (201, 1), (203, 1), (205, 1), (207, 1), (208, 1), (211, 1), (212, 1), (217, 1), (218, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (234, 1), (236, 1), (237, 1), (248, 1), (250, 1), (251, 1), (254, 5), (255, 1), (257, 4), (265, 1), (280, 1), (281, 1), (298, 1), (299, 2), (300, 1), (303, 1), (313, 2), (315, 1), (316, 1), (317, 1), (318, 2), (326, 2), (358, 1), (360, 1), (366, 1)]
discards: []
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 737.3979 - loglik: -7.3371e+02 - logprior: -3.6849e+00
Epoch 2/2
25/25 - 40s - loss: 705.4546 - loglik: -7.0556e+02 - logprior: 0.1095
Fitted a model with MAP estimate = -702.3294
expansions: [(170, 1), (345, 1), (436, 1)]
discards: [183 210 305 315 318 369]
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 708.5415 - loglik: -7.0536e+02 - logprior: -3.1790e+00
Epoch 2/2
25/25 - 40s - loss: 701.3549 - loglik: -7.0239e+02 - logprior: 1.0320
Fitted a model with MAP estimate = -697.5354
expansions: [(429, 1)]
discards: [189 430]
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 42s - loss: 709.2315 - loglik: -7.0647e+02 - logprior: -2.7565e+00
Epoch 2/10
25/25 - 40s - loss: 699.6201 - loglik: -7.0120e+02 - logprior: 1.5848
Epoch 3/10
25/25 - 40s - loss: 696.0406 - loglik: -6.9822e+02 - logprior: 2.1745
Epoch 4/10
25/25 - 40s - loss: 697.2057 - loglik: -6.9965e+02 - logprior: 2.4434
Fitted a model with MAP estimate = -694.3977
Time for alignment: 678.8048
Computed alignments with likelihoods: ['-695.7868', '-691.8598', '-694.3977']
Best model has likelihood: -691.8598
SP score = 0.8974
Training of 3 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2423f35e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe24ad183a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 325.9932 - loglik: -3.2150e+02 - logprior: -4.4882e+00
Epoch 2/10
16/16 - 3s - loss: 253.1190 - loglik: -2.5166e+02 - logprior: -1.4585e+00
Epoch 3/10
16/16 - 3s - loss: 224.2072 - loglik: -2.2239e+02 - logprior: -1.8193e+00
Epoch 4/10
16/16 - 4s - loss: 215.7067 - loglik: -2.1395e+02 - logprior: -1.7579e+00
Epoch 5/10
16/16 - 3s - loss: 209.3399 - loglik: -2.0768e+02 - logprior: -1.6554e+00
Epoch 6/10
16/16 - 3s - loss: 208.5966 - loglik: -2.0694e+02 - logprior: -1.6611e+00
Epoch 7/10
16/16 - 4s - loss: 203.8649 - loglik: -2.0218e+02 - logprior: -1.6843e+00
Epoch 8/10
16/16 - 3s - loss: 202.1053 - loglik: -2.0041e+02 - logprior: -1.6964e+00
Epoch 9/10
16/16 - 3s - loss: 201.8951 - loglik: -2.0020e+02 - logprior: -1.6937e+00
Epoch 10/10
16/16 - 3s - loss: 201.4246 - loglik: -1.9973e+02 - logprior: -1.6904e+00
Fitted a model with MAP estimate = -200.9197
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 2), (18, 1), (69, 1), (86, 2), (97, 1)]
discards: [0 2]
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 213.0125 - loglik: -2.0736e+02 - logprior: -5.6497e+00
Epoch 2/2
16/16 - 4s - loss: 200.8008 - loglik: -1.9800e+02 - logprior: -2.8016e+00
Fitted a model with MAP estimate = -198.8537
expansions: [(0, 2)]
discards: [ 0 94]
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 201.2050 - loglik: -1.9678e+02 - logprior: -4.4232e+00
Epoch 2/2
16/16 - 4s - loss: 195.1272 - loglik: -1.9347e+02 - logprior: -1.6550e+00
Fitted a model with MAP estimate = -194.8426
expansions: []
discards: [25]
Fitting a model of length 122 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 8s - loss: 200.8119 - loglik: -1.9632e+02 - logprior: -4.4921e+00
Epoch 2/10
16/16 - 3s - loss: 194.2326 - loglik: -1.9260e+02 - logprior: -1.6340e+00
Epoch 3/10
16/16 - 4s - loss: 192.8966 - loglik: -1.9151e+02 - logprior: -1.3858e+00
Epoch 4/10
16/16 - 4s - loss: 191.5927 - loglik: -1.9028e+02 - logprior: -1.3130e+00
Epoch 5/10
16/16 - 4s - loss: 191.5533 - loglik: -1.9029e+02 - logprior: -1.2642e+00
Epoch 6/10
16/16 - 3s - loss: 190.0325 - loglik: -1.8883e+02 - logprior: -1.2037e+00
Epoch 7/10
16/16 - 4s - loss: 189.7400 - loglik: -1.8860e+02 - logprior: -1.1397e+00
Epoch 8/10
16/16 - 4s - loss: 191.4539 - loglik: -1.9037e+02 - logprior: -1.0877e+00
Fitted a model with MAP estimate = -189.7076
Time for alignment: 112.2934
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 326.5076 - loglik: -3.2202e+02 - logprior: -4.4926e+00
Epoch 2/10
16/16 - 3s - loss: 255.1911 - loglik: -2.5372e+02 - logprior: -1.4711e+00
Epoch 3/10
16/16 - 4s - loss: 224.3114 - loglik: -2.2248e+02 - logprior: -1.8297e+00
Epoch 4/10
16/16 - 4s - loss: 216.0631 - loglik: -2.1436e+02 - logprior: -1.7070e+00
Epoch 5/10
16/16 - 3s - loss: 210.5540 - loglik: -2.0892e+02 - logprior: -1.6299e+00
Epoch 6/10
16/16 - 3s - loss: 207.9729 - loglik: -2.0632e+02 - logprior: -1.6498e+00
Epoch 7/10
16/16 - 4s - loss: 203.9969 - loglik: -2.0236e+02 - logprior: -1.6375e+00
Epoch 8/10
16/16 - 4s - loss: 201.8448 - loglik: -2.0020e+02 - logprior: -1.6405e+00
Epoch 9/10
16/16 - 3s - loss: 200.6361 - loglik: -1.9897e+02 - logprior: -1.6652e+00
Epoch 10/10
16/16 - 3s - loss: 199.8735 - loglik: -1.9819e+02 - logprior: -1.6823e+00
Fitted a model with MAP estimate = -199.3977
expansions: [(10, 1), (12, 1), (13, 1), (15, 1), (16, 2), (17, 1), (18, 1), (72, 1), (96, 1), (97, 1)]
discards: [0 2]
Fitting a model of length 121 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 212.7413 - loglik: -2.0712e+02 - logprior: -5.6185e+00
Epoch 2/2
16/16 - 3s - loss: 199.6882 - loglik: -1.9690e+02 - logprior: -2.7853e+00
Fitted a model with MAP estimate = -198.0628
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 122 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 200.3372 - loglik: -1.9590e+02 - logprior: -4.4418e+00
Epoch 2/2
16/16 - 4s - loss: 194.6949 - loglik: -1.9303e+02 - logprior: -1.6678e+00
Fitted a model with MAP estimate = -193.5115
expansions: [(20, 1)]
discards: [25]
Fitting a model of length 122 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 199.3228 - loglik: -1.9484e+02 - logprior: -4.4868e+00
Epoch 2/10
16/16 - 4s - loss: 195.2976 - loglik: -1.9367e+02 - logprior: -1.6278e+00
Epoch 3/10
16/16 - 4s - loss: 192.4826 - loglik: -1.9110e+02 - logprior: -1.3831e+00
Epoch 4/10
16/16 - 4s - loss: 191.7809 - loglik: -1.9047e+02 - logprior: -1.3096e+00
Epoch 5/10
16/16 - 4s - loss: 192.2264 - loglik: -1.9096e+02 - logprior: -1.2632e+00
Fitted a model with MAP estimate = -191.0123
Time for alignment: 100.7573
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 327.7193 - loglik: -3.2324e+02 - logprior: -4.4772e+00
Epoch 2/10
16/16 - 3s - loss: 254.8719 - loglik: -2.5344e+02 - logprior: -1.4290e+00
Epoch 3/10
16/16 - 4s - loss: 225.8611 - loglik: -2.2408e+02 - logprior: -1.7813e+00
Epoch 4/10
16/16 - 3s - loss: 212.7565 - loglik: -2.1093e+02 - logprior: -1.8221e+00
Epoch 5/10
16/16 - 3s - loss: 208.6853 - loglik: -2.0698e+02 - logprior: -1.7027e+00
Epoch 6/10
16/16 - 4s - loss: 203.8646 - loglik: -2.0216e+02 - logprior: -1.7007e+00
Epoch 7/10
16/16 - 4s - loss: 202.8580 - loglik: -2.0116e+02 - logprior: -1.7010e+00
Epoch 8/10
16/16 - 3s - loss: 200.5603 - loglik: -1.9887e+02 - logprior: -1.6881e+00
Epoch 9/10
16/16 - 3s - loss: 200.6729 - loglik: -1.9899e+02 - logprior: -1.6849e+00
Fitted a model with MAP estimate = -200.2781
expansions: [(10, 1), (12, 2), (14, 1), (15, 2), (16, 1), (25, 3), (57, 1), (86, 1), (97, 1)]
discards: [0 2]
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 8s - loss: 211.6792 - loglik: -2.0598e+02 - logprior: -5.7023e+00
Epoch 2/2
16/16 - 4s - loss: 198.8429 - loglik: -1.9599e+02 - logprior: -2.8540e+00
Fitted a model with MAP estimate = -197.2471
expansions: [(0, 2), (12, 1)]
discards: [ 0 31 32]
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 199.8870 - loglik: -1.9541e+02 - logprior: -4.4802e+00
Epoch 2/2
16/16 - 4s - loss: 194.1483 - loglik: -1.9245e+02 - logprior: -1.6972e+00
Fitted a model with MAP estimate = -192.9312
expansions: []
discards: []
Fitting a model of length 123 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 197.8486 - loglik: -1.9330e+02 - logprior: -4.5501e+00
Epoch 2/10
16/16 - 4s - loss: 193.0564 - loglik: -1.9137e+02 - logprior: -1.6834e+00
Epoch 3/10
16/16 - 4s - loss: 193.6204 - loglik: -1.9219e+02 - logprior: -1.4276e+00
Fitted a model with MAP estimate = -191.7427
Time for alignment: 89.8906
Computed alignments with likelihoods: ['-189.7076', '-191.0123', '-191.7427']
Best model has likelihood: -189.7076
SP score = 0.5430
Training of 3 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2fa9dc910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2c95910>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.5250 - loglik: -3.0539e+02 - logprior: -3.1346e+00
Epoch 2/10
19/19 - 2s - loss: 276.9298 - loglik: -2.7577e+02 - logprior: -1.1633e+00
Epoch 3/10
19/19 - 2s - loss: 262.1797 - loglik: -2.6071e+02 - logprior: -1.4670e+00
Epoch 4/10
19/19 - 2s - loss: 258.3421 - loglik: -2.5692e+02 - logprior: -1.4228e+00
Epoch 5/10
19/19 - 2s - loss: 257.6512 - loglik: -2.5628e+02 - logprior: -1.3663e+00
Epoch 6/10
19/19 - 2s - loss: 256.6693 - loglik: -2.5532e+02 - logprior: -1.3490e+00
Epoch 7/10
19/19 - 2s - loss: 256.4535 - loglik: -2.5512e+02 - logprior: -1.3318e+00
Epoch 8/10
19/19 - 2s - loss: 256.2820 - loglik: -2.5496e+02 - logprior: -1.3194e+00
Epoch 9/10
19/19 - 2s - loss: 256.1263 - loglik: -2.5481e+02 - logprior: -1.3173e+00
Epoch 10/10
19/19 - 2s - loss: 256.5085 - loglik: -2.5520e+02 - logprior: -1.3081e+00
Fitted a model with MAP estimate = -242.5818
expansions: [(6, 3), (7, 2), (8, 1), (10, 2), (22, 4), (33, 9), (38, 2), (39, 2), (42, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 262.6081 - loglik: -2.5867e+02 - logprior: -3.9397e+00
Epoch 2/2
19/19 - 3s - loss: 251.7076 - loglik: -2.4953e+02 - logprior: -2.1772e+00
Fitted a model with MAP estimate = -235.7642
expansions: [(0, 2), (50, 2)]
discards: [ 0  8 15 30 31 32 59 61 85 92]
Fitting a model of length 107 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 251.9838 - loglik: -2.4908e+02 - logprior: -2.9035e+00
Epoch 2/2
19/19 - 2s - loss: 247.7767 - loglik: -2.4666e+02 - logprior: -1.1126e+00
Fitted a model with MAP estimate = -233.6841
expansions: []
discards: [ 0 47 48 49 50]
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 236.6698 - loglik: -2.3413e+02 - logprior: -2.5393e+00
Epoch 2/10
23/23 - 3s - loss: 233.3023 - loglik: -2.3223e+02 - logprior: -1.0732e+00
Epoch 3/10
23/23 - 3s - loss: 232.5869 - loglik: -2.3157e+02 - logprior: -1.0170e+00
Epoch 4/10
23/23 - 3s - loss: 232.0261 - loglik: -2.3104e+02 - logprior: -9.8525e-01
Epoch 5/10
23/23 - 3s - loss: 231.7246 - loglik: -2.3075e+02 - logprior: -9.7549e-01
Epoch 6/10
23/23 - 3s - loss: 230.4457 - loglik: -2.2947e+02 - logprior: -9.7423e-01
Epoch 7/10
23/23 - 3s - loss: 230.3645 - loglik: -2.2941e+02 - logprior: -9.5451e-01
Epoch 8/10
23/23 - 3s - loss: 230.8491 - loglik: -2.2989e+02 - logprior: -9.5703e-01
Fitted a model with MAP estimate = -230.0988
Time for alignment: 86.0958
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 308.9042 - loglik: -3.0577e+02 - logprior: -3.1324e+00
Epoch 2/10
19/19 - 2s - loss: 278.1512 - loglik: -2.7700e+02 - logprior: -1.1554e+00
Epoch 3/10
19/19 - 2s - loss: 263.0752 - loglik: -2.6159e+02 - logprior: -1.4876e+00
Epoch 4/10
19/19 - 2s - loss: 259.4442 - loglik: -2.5800e+02 - logprior: -1.4470e+00
Epoch 5/10
19/19 - 2s - loss: 258.0984 - loglik: -2.5671e+02 - logprior: -1.3915e+00
Epoch 6/10
19/19 - 2s - loss: 257.0151 - loglik: -2.5564e+02 - logprior: -1.3717e+00
Epoch 7/10
19/19 - 2s - loss: 257.4007 - loglik: -2.5605e+02 - logprior: -1.3492e+00
Fitted a model with MAP estimate = -243.1745
expansions: [(6, 3), (7, 2), (8, 1), (10, 2), (21, 2), (36, 4), (38, 3), (42, 1), (43, 4), (55, 2), (58, 2), (60, 1), (63, 2), (66, 3), (67, 1), (69, 1)]
discards: [0]
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 262.8481 - loglik: -2.5889e+02 - logprior: -3.9575e+00
Epoch 2/2
19/19 - 3s - loss: 252.8219 - loglik: -2.5068e+02 - logprior: -2.1418e+00
Fitted a model with MAP estimate = -237.0791
expansions: [(0, 2)]
discards: [ 0  8 11 15 29 61 62 76 82 90 95]
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 253.9000 - loglik: -2.5103e+02 - logprior: -2.8668e+00
Epoch 2/2
19/19 - 2s - loss: 250.1672 - loglik: -2.4906e+02 - logprior: -1.1026e+00
Fitted a model with MAP estimate = -235.0371
expansions: []
discards: [ 0 43 44]
Fitting a model of length 99 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 237.8027 - loglik: -2.3524e+02 - logprior: -2.5590e+00
Epoch 2/10
23/23 - 3s - loss: 233.8293 - loglik: -2.3275e+02 - logprior: -1.0814e+00
Epoch 3/10
23/23 - 3s - loss: 232.8195 - loglik: -2.3182e+02 - logprior: -9.9587e-01
Epoch 4/10
23/23 - 3s - loss: 233.3898 - loglik: -2.3241e+02 - logprior: -9.7712e-01
Fitted a model with MAP estimate = -232.0818
Time for alignment: 65.2743
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 308.6370 - loglik: -3.0550e+02 - logprior: -3.1324e+00
Epoch 2/10
19/19 - 2s - loss: 276.8592 - loglik: -2.7570e+02 - logprior: -1.1626e+00
Epoch 3/10
19/19 - 2s - loss: 261.7982 - loglik: -2.6031e+02 - logprior: -1.4856e+00
Epoch 4/10
19/19 - 2s - loss: 257.8881 - loglik: -2.5646e+02 - logprior: -1.4269e+00
Epoch 5/10
19/19 - 2s - loss: 256.8770 - loglik: -2.5550e+02 - logprior: -1.3721e+00
Epoch 6/10
19/19 - 2s - loss: 256.5491 - loglik: -2.5520e+02 - logprior: -1.3511e+00
Epoch 7/10
19/19 - 2s - loss: 256.0155 - loglik: -2.5468e+02 - logprior: -1.3312e+00
Epoch 8/10
19/19 - 2s - loss: 255.3333 - loglik: -2.5401e+02 - logprior: -1.3240e+00
Epoch 9/10
19/19 - 2s - loss: 256.1468 - loglik: -2.5483e+02 - logprior: -1.3194e+00
Fitted a model with MAP estimate = -242.2162
expansions: [(6, 3), (7, 2), (10, 2), (21, 2), (33, 9), (38, 2), (39, 2), (42, 2), (58, 2), (60, 1), (62, 1), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 262.0426 - loglik: -2.5811e+02 - logprior: -3.9371e+00
Epoch 2/2
19/19 - 3s - loss: 251.8811 - loglik: -2.4973e+02 - logprior: -2.1484e+00
Fitted a model with MAP estimate = -236.2240
expansions: [(0, 2), (47, 2)]
discards: [ 0  8 14 28 56 58 82 89]
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 251.7502 - loglik: -2.4885e+02 - logprior: -2.9021e+00
Epoch 2/2
19/19 - 2s - loss: 247.9807 - loglik: -2.4687e+02 - logprior: -1.1077e+00
Fitted a model with MAP estimate = -233.8465
expansions: []
discards: [ 0 46 47 48]
Fitting a model of length 102 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 236.8033 - loglik: -2.3424e+02 - logprior: -2.5609e+00
Epoch 2/10
23/23 - 3s - loss: 233.7877 - loglik: -2.3270e+02 - logprior: -1.0870e+00
Epoch 3/10
23/23 - 3s - loss: 231.7794 - loglik: -2.3075e+02 - logprior: -1.0287e+00
Epoch 4/10
23/23 - 3s - loss: 231.4290 - loglik: -2.3043e+02 - logprior: -9.9912e-01
Epoch 5/10
23/23 - 3s - loss: 231.1853 - loglik: -2.3019e+02 - logprior: -9.9112e-01
Epoch 6/10
23/23 - 3s - loss: 230.6838 - loglik: -2.2970e+02 - logprior: -9.8621e-01
Epoch 7/10
23/23 - 3s - loss: 230.2918 - loglik: -2.2931e+02 - logprior: -9.7691e-01
Epoch 8/10
23/23 - 3s - loss: 230.2915 - loglik: -2.2932e+02 - logprior: -9.6650e-01
Epoch 9/10
23/23 - 3s - loss: 229.3678 - loglik: -2.2841e+02 - logprior: -9.5609e-01
Epoch 10/10
23/23 - 3s - loss: 230.2771 - loglik: -2.2933e+02 - logprior: -9.4403e-01
Fitted a model with MAP estimate = -229.7705
Time for alignment: 87.6741
Computed alignments with likelihoods: ['-230.0988', '-232.0818', '-229.7705']
Best model has likelihood: -229.7705
SP score = 0.7684
Training of 3 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1aff90a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe26eddd6d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 432.2639 - loglik: -3.8678e+02 - logprior: -4.5485e+01
Epoch 2/10
10/10 - 2s - loss: 369.5995 - loglik: -3.5930e+02 - logprior: -1.0304e+01
Epoch 3/10
10/10 - 2s - loss: 334.3708 - loglik: -3.3024e+02 - logprior: -4.1321e+00
Epoch 4/10
10/10 - 2s - loss: 314.1564 - loglik: -3.1212e+02 - logprior: -2.0395e+00
Epoch 5/10
10/10 - 2s - loss: 303.5842 - loglik: -3.0246e+02 - logprior: -1.1228e+00
Epoch 6/10
10/10 - 2s - loss: 298.3964 - loglik: -2.9768e+02 - logprior: -7.1740e-01
Epoch 7/10
10/10 - 2s - loss: 297.5918 - loglik: -2.9720e+02 - logprior: -3.9563e-01
Epoch 8/10
10/10 - 2s - loss: 295.6285 - loglik: -2.9547e+02 - logprior: -1.5552e-01
Epoch 9/10
10/10 - 2s - loss: 295.1952 - loglik: -2.9522e+02 - logprior: 0.0298
Epoch 10/10
10/10 - 2s - loss: 295.1790 - loglik: -2.9533e+02 - logprior: 0.1464
Fitted a model with MAP estimate = -294.8153
expansions: [(7, 2), (10, 1), (17, 1), (18, 1), (26, 2), (27, 2), (48, 2), (58, 2), (59, 1), (62, 2), (64, 1), (73, 1), (85, 1), (92, 6), (93, 1)]
discards: [0]
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 346.9205 - loglik: -2.9618e+02 - logprior: -5.0737e+01
Epoch 2/2
10/10 - 2s - loss: 305.1891 - loglik: -2.8612e+02 - logprior: -1.9072e+01
Fitted a model with MAP estimate = -298.5527
expansions: [(9, 3)]
discards: [ 0 57 69 75]
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 333.9854 - loglik: -2.8446e+02 - logprior: -4.9520e+01
Epoch 2/2
10/10 - 2s - loss: 298.6619 - loglik: -2.8180e+02 - logprior: -1.6864e+01
Fitted a model with MAP estimate = -291.0935
expansions: [(0, 2)]
discards: [0]
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 318.7762 - loglik: -2.8006e+02 - logprior: -3.8712e+01
Epoch 2/10
10/10 - 2s - loss: 285.5152 - loglik: -2.7801e+02 - logprior: -7.5068e+00
Epoch 3/10
10/10 - 2s - loss: 278.6009 - loglik: -2.7744e+02 - logprior: -1.1579e+00
Epoch 4/10
10/10 - 2s - loss: 275.0845 - loglik: -2.7655e+02 - logprior: 1.4656
Epoch 5/10
10/10 - 2s - loss: 274.1070 - loglik: -2.7703e+02 - logprior: 2.9275
Epoch 6/10
10/10 - 2s - loss: 273.1355 - loglik: -2.7691e+02 - logprior: 3.7744
Epoch 7/10
10/10 - 2s - loss: 272.5068 - loglik: -2.7680e+02 - logprior: 4.2943
Epoch 8/10
10/10 - 2s - loss: 272.9897 - loglik: -2.7764e+02 - logprior: 4.6544
Fitted a model with MAP estimate = -272.3126
Time for alignment: 61.6349
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.5312 - loglik: -3.8705e+02 - logprior: -4.5483e+01
Epoch 2/10
10/10 - 2s - loss: 370.0276 - loglik: -3.5972e+02 - logprior: -1.0306e+01
Epoch 3/10
10/10 - 2s - loss: 334.6214 - loglik: -3.3046e+02 - logprior: -4.1565e+00
Epoch 4/10
10/10 - 2s - loss: 311.9644 - loglik: -3.0983e+02 - logprior: -2.1369e+00
Epoch 5/10
10/10 - 2s - loss: 303.0788 - loglik: -3.0201e+02 - logprior: -1.0662e+00
Epoch 6/10
10/10 - 2s - loss: 298.9291 - loglik: -2.9840e+02 - logprior: -5.3130e-01
Epoch 7/10
10/10 - 2s - loss: 296.6939 - loglik: -2.9653e+02 - logprior: -1.5918e-01
Epoch 8/10
10/10 - 2s - loss: 296.6995 - loglik: -2.9684e+02 - logprior: 0.1374
Fitted a model with MAP estimate = -295.8421
expansions: [(10, 4), (17, 1), (18, 1), (29, 3), (40, 2), (49, 2), (59, 3), (62, 1), (65, 1), (73, 1), (85, 1), (90, 2), (91, 6)]
discards: [0]
Fitting a model of length 132 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 347.4600 - loglik: -2.9676e+02 - logprior: -5.0705e+01
Epoch 2/2
10/10 - 2s - loss: 306.2427 - loglik: -2.8702e+02 - logprior: -1.9221e+01
Fitted a model with MAP estimate = -298.7435
expansions: [(11, 1)]
discards: [ 49  59 110]
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 332.7551 - loglik: -2.8450e+02 - logprior: -4.8257e+01
Epoch 2/2
10/10 - 2s - loss: 293.8351 - loglik: -2.8123e+02 - logprior: -1.2600e+01
Fitted a model with MAP estimate = -285.8967
expansions: [(10, 1)]
discards: [36 71]
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 321.4689 - loglik: -2.8107e+02 - logprior: -4.0400e+01
Epoch 2/10
10/10 - 2s - loss: 287.6653 - loglik: -2.7986e+02 - logprior: -7.8090e+00
Epoch 3/10
10/10 - 2s - loss: 280.2272 - loglik: -2.7907e+02 - logprior: -1.1568e+00
Epoch 4/10
10/10 - 2s - loss: 276.4532 - loglik: -2.7795e+02 - logprior: 1.4935
Epoch 5/10
10/10 - 2s - loss: 275.1205 - loglik: -2.7797e+02 - logprior: 2.8446
Epoch 6/10
10/10 - 2s - loss: 274.1320 - loglik: -2.7770e+02 - logprior: 3.5659
Epoch 7/10
10/10 - 2s - loss: 273.1161 - loglik: -2.7720e+02 - logprior: 4.0808
Epoch 8/10
10/10 - 2s - loss: 273.7212 - loglik: -2.7828e+02 - logprior: 4.5557
Fitted a model with MAP estimate = -273.0647
Time for alignment: 57.8814
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 432.6125 - loglik: -3.8713e+02 - logprior: -4.5484e+01
Epoch 2/10
10/10 - 2s - loss: 370.4294 - loglik: -3.6013e+02 - logprior: -1.0300e+01
Epoch 3/10
10/10 - 2s - loss: 335.4957 - loglik: -3.3139e+02 - logprior: -4.1068e+00
Epoch 4/10
10/10 - 2s - loss: 311.5429 - loglik: -3.0946e+02 - logprior: -2.0878e+00
Epoch 5/10
10/10 - 2s - loss: 301.8193 - loglik: -3.0056e+02 - logprior: -1.2544e+00
Epoch 6/10
10/10 - 2s - loss: 298.5607 - loglik: -2.9775e+02 - logprior: -8.0832e-01
Epoch 7/10
10/10 - 2s - loss: 296.8560 - loglik: -2.9654e+02 - logprior: -3.1567e-01
Epoch 8/10
10/10 - 2s - loss: 296.3947 - loglik: -2.9640e+02 - logprior: 4.0893e-04
Epoch 9/10
10/10 - 2s - loss: 295.4031 - loglik: -2.9558e+02 - logprior: 0.1784
Epoch 10/10
10/10 - 2s - loss: 295.4106 - loglik: -2.9566e+02 - logprior: 0.2541
Fitted a model with MAP estimate = -295.1152
expansions: [(7, 2), (9, 1), (17, 1), (18, 1), (27, 1), (28, 4), (48, 2), (58, 2), (59, 1), (62, 1), (64, 1), (68, 1), (72, 1), (84, 1), (91, 6)]
discards: [0]
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 346.3672 - loglik: -2.9580e+02 - logprior: -5.0564e+01
Epoch 2/2
10/10 - 2s - loss: 304.8668 - loglik: -2.8603e+02 - logprior: -1.8841e+01
Fitted a model with MAP estimate = -298.1575
expansions: [(9, 3)]
discards: [58 70]
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 331.3367 - loglik: -2.8303e+02 - logprior: -4.8309e+01
Epoch 2/2
10/10 - 2s - loss: 292.2338 - loglik: -2.7915e+02 - logprior: -1.3085e+01
Fitted a model with MAP estimate = -284.0141
expansions: []
discards: [36]
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 319.4120 - loglik: -2.7961e+02 - logprior: -3.9800e+01
Epoch 2/10
10/10 - 2s - loss: 285.8178 - loglik: -2.7820e+02 - logprior: -7.6221e+00
Epoch 3/10
10/10 - 2s - loss: 278.7244 - loglik: -2.7768e+02 - logprior: -1.0418e+00
Epoch 4/10
10/10 - 2s - loss: 276.1198 - loglik: -2.7772e+02 - logprior: 1.6044
Epoch 5/10
10/10 - 2s - loss: 274.4742 - loglik: -2.7743e+02 - logprior: 2.9562
Epoch 6/10
10/10 - 2s - loss: 273.3216 - loglik: -2.7701e+02 - logprior: 3.6839
Epoch 7/10
10/10 - 2s - loss: 272.9980 - loglik: -2.7720e+02 - logprior: 4.2014
Epoch 8/10
10/10 - 2s - loss: 272.3702 - loglik: -2.7703e+02 - logprior: 4.6630
Epoch 9/10
10/10 - 2s - loss: 272.1617 - loglik: -2.7721e+02 - logprior: 5.0489
Epoch 10/10
10/10 - 2s - loss: 272.1708 - loglik: -2.7751e+02 - logprior: 5.3392
Fitted a model with MAP estimate = -271.8852
Time for alignment: 65.1726
Computed alignments with likelihoods: ['-272.3126', '-273.0647', '-271.8852']
Best model has likelihood: -271.8852
SP score = 0.9234
Training of 3 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb7d55b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2aa730a30>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 453.0040 - loglik: -1.8610e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3125 - loglik: -1.6038e+02 - logprior: -6.9931e+01
Epoch 3/10
10/10 - 1s - loss: 170.5663 - loglik: -1.3982e+02 - logprior: -3.0746e+01
Epoch 4/10
10/10 - 1s - loss: 142.5886 - loglik: -1.2591e+02 - logprior: -1.6682e+01
Epoch 5/10
10/10 - 1s - loss: 129.0955 - loglik: -1.2010e+02 - logprior: -8.9998e+00
Epoch 6/10
10/10 - 1s - loss: 121.7514 - loglik: -1.1805e+02 - logprior: -3.6990e+00
Epoch 7/10
10/10 - 1s - loss: 116.9155 - loglik: -1.1660e+02 - logprior: -3.1664e-01
Epoch 8/10
10/10 - 1s - loss: 114.1795 - loglik: -1.1605e+02 - logprior: 1.8670
Epoch 9/10
10/10 - 1s - loss: 112.6274 - loglik: -1.1610e+02 - logprior: 3.4742
Epoch 10/10
10/10 - 1s - loss: 111.5695 - loglik: -1.1625e+02 - logprior: 4.6800
Fitted a model with MAP estimate = -111.1080
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 464.0355 - loglik: -1.0957e+02 - logprior: -3.5446e+02
Epoch 2/2
10/10 - 1s - loss: 203.7070 - loglik: -9.7384e+01 - logprior: -1.0632e+02
Fitted a model with MAP estimate = -155.3761
expansions: []
discards: [ 0 21 37 48 53]
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 397.6619 - loglik: -9.7309e+01 - logprior: -3.0035e+02
Epoch 2/2
10/10 - 1s - loss: 209.9742 - loglik: -9.5702e+01 - logprior: -1.1427e+02
Fitted a model with MAP estimate = -180.1226
expansions: []
discards: []
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 369.6662 - loglik: -9.4843e+01 - logprior: -2.7482e+02
Epoch 2/10
10/10 - 1s - loss: 167.1217 - loglik: -9.4074e+01 - logprior: -7.3048e+01
Epoch 3/10
10/10 - 1s - loss: 116.4431 - loglik: -9.4336e+01 - logprior: -2.2107e+01
Epoch 4/10
10/10 - 1s - loss: 98.3691 - loglik: -9.4869e+01 - logprior: -3.4997e+00
Epoch 5/10
10/10 - 1s - loss: 89.2730 - loglik: -9.5348e+01 - logprior: 6.0745
Epoch 6/10
10/10 - 1s - loss: 84.0098 - loglik: -9.5714e+01 - logprior: 11.7039
Epoch 7/10
10/10 - 1s - loss: 80.6759 - loglik: -9.5976e+01 - logprior: 15.3003
Epoch 8/10
10/10 - 1s - loss: 78.3212 - loglik: -9.6147e+01 - logprior: 17.8261
Epoch 9/10
10/10 - 1s - loss: 76.5169 - loglik: -9.6286e+01 - logprior: 19.7691
Epoch 10/10
10/10 - 1s - loss: 75.0190 - loglik: -9.6408e+01 - logprior: 21.3887
Fitted a model with MAP estimate = -74.2750
Time for alignment: 29.7710
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 453.0040 - loglik: -1.8610e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3125 - loglik: -1.6038e+02 - logprior: -6.9931e+01
Epoch 3/10
10/10 - 1s - loss: 170.5663 - loglik: -1.3982e+02 - logprior: -3.0746e+01
Epoch 4/10
10/10 - 1s - loss: 142.5886 - loglik: -1.2591e+02 - logprior: -1.6682e+01
Epoch 5/10
10/10 - 1s - loss: 129.0955 - loglik: -1.2010e+02 - logprior: -8.9998e+00
Epoch 6/10
10/10 - 1s - loss: 121.7514 - loglik: -1.1805e+02 - logprior: -3.6990e+00
Epoch 7/10
10/10 - 1s - loss: 116.9155 - loglik: -1.1660e+02 - logprior: -3.1664e-01
Epoch 8/10
10/10 - 1s - loss: 114.1795 - loglik: -1.1605e+02 - logprior: 1.8670
Epoch 9/10
10/10 - 1s - loss: 112.6274 - loglik: -1.1610e+02 - logprior: 3.4742
Epoch 10/10
10/10 - 1s - loss: 111.5695 - loglik: -1.1625e+02 - logprior: 4.6800
Fitted a model with MAP estimate = -111.1080
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 464.0355 - loglik: -1.0957e+02 - logprior: -3.5446e+02
Epoch 2/2
10/10 - 1s - loss: 203.7070 - loglik: -9.7384e+01 - logprior: -1.0632e+02
Fitted a model with MAP estimate = -155.3761
expansions: []
discards: [ 0 21 37 48 53]
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 397.6619 - loglik: -9.7309e+01 - logprior: -3.0035e+02
Epoch 2/2
10/10 - 1s - loss: 209.9742 - loglik: -9.5702e+01 - logprior: -1.1427e+02
Fitted a model with MAP estimate = -180.1226
expansions: []
discards: []
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 369.6662 - loglik: -9.4843e+01 - logprior: -2.7482e+02
Epoch 2/10
10/10 - 1s - loss: 167.1217 - loglik: -9.4074e+01 - logprior: -7.3048e+01
Epoch 3/10
10/10 - 1s - loss: 116.4431 - loglik: -9.4336e+01 - logprior: -2.2107e+01
Epoch 4/10
10/10 - 1s - loss: 98.3691 - loglik: -9.4869e+01 - logprior: -3.4997e+00
Epoch 5/10
10/10 - 1s - loss: 89.2730 - loglik: -9.5348e+01 - logprior: 6.0745
Epoch 6/10
10/10 - 1s - loss: 84.0098 - loglik: -9.5714e+01 - logprior: 11.7039
Epoch 7/10
10/10 - 1s - loss: 80.6759 - loglik: -9.5976e+01 - logprior: 15.3003
Epoch 8/10
10/10 - 1s - loss: 78.3212 - loglik: -9.6147e+01 - logprior: 17.8261
Epoch 9/10
10/10 - 1s - loss: 76.5169 - loglik: -9.6286e+01 - logprior: 19.7691
Epoch 10/10
10/10 - 1s - loss: 75.0190 - loglik: -9.6408e+01 - logprior: 21.3887
Fitted a model with MAP estimate = -74.2750
Time for alignment: 30.1484
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 453.0040 - loglik: -1.8610e+02 - logprior: -2.6691e+02
Epoch 2/10
10/10 - 1s - loss: 230.3125 - loglik: -1.6038e+02 - logprior: -6.9931e+01
Epoch 3/10
10/10 - 1s - loss: 170.5663 - loglik: -1.3982e+02 - logprior: -3.0746e+01
Epoch 4/10
10/10 - 1s - loss: 142.5886 - loglik: -1.2591e+02 - logprior: -1.6682e+01
Epoch 5/10
10/10 - 1s - loss: 129.0955 - loglik: -1.2010e+02 - logprior: -8.9998e+00
Epoch 6/10
10/10 - 1s - loss: 121.7514 - loglik: -1.1805e+02 - logprior: -3.6990e+00
Epoch 7/10
10/10 - 1s - loss: 116.9155 - loglik: -1.1660e+02 - logprior: -3.1664e-01
Epoch 8/10
10/10 - 1s - loss: 114.1795 - loglik: -1.1605e+02 - logprior: 1.8670
Epoch 9/10
10/10 - 1s - loss: 112.6274 - loglik: -1.1610e+02 - logprior: 3.4742
Epoch 10/10
10/10 - 1s - loss: 111.5695 - loglik: -1.1625e+02 - logprior: 4.6800
Fitted a model with MAP estimate = -111.1080
expansions: [(0, 3), (10, 1), (17, 2), (21, 3), (23, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Fitting a model of length 72 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 464.0355 - loglik: -1.0957e+02 - logprior: -3.5446e+02
Epoch 2/2
10/10 - 1s - loss: 203.7070 - loglik: -9.7384e+01 - logprior: -1.0632e+02
Fitted a model with MAP estimate = -155.3761
expansions: []
discards: [ 0 21 37 48 53]
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 397.6619 - loglik: -9.7309e+01 - logprior: -3.0035e+02
Epoch 2/2
10/10 - 1s - loss: 209.9742 - loglik: -9.5702e+01 - logprior: -1.1427e+02
Fitted a model with MAP estimate = -180.1226
expansions: []
discards: []
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 369.6662 - loglik: -9.4843e+01 - logprior: -2.7482e+02
Epoch 2/10
10/10 - 1s - loss: 167.1217 - loglik: -9.4074e+01 - logprior: -7.3048e+01
Epoch 3/10
10/10 - 1s - loss: 116.4431 - loglik: -9.4336e+01 - logprior: -2.2107e+01
Epoch 4/10
10/10 - 1s - loss: 98.3691 - loglik: -9.4869e+01 - logprior: -3.4997e+00
Epoch 5/10
10/10 - 1s - loss: 89.2730 - loglik: -9.5348e+01 - logprior: 6.0745
Epoch 6/10
10/10 - 1s - loss: 84.0098 - loglik: -9.5714e+01 - logprior: 11.7039
Epoch 7/10
10/10 - 1s - loss: 80.6759 - loglik: -9.5976e+01 - logprior: 15.3003
Epoch 8/10
10/10 - 1s - loss: 78.3212 - loglik: -9.6147e+01 - logprior: 17.8261
Epoch 9/10
10/10 - 1s - loss: 76.5169 - loglik: -9.6286e+01 - logprior: 19.7691
Epoch 10/10
10/10 - 1s - loss: 75.0190 - loglik: -9.6408e+01 - logprior: 21.3887
Fitted a model with MAP estimate = -74.2750
Time for alignment: 29.1118
Computed alignments with likelihoods: ['-74.2750', '-74.2750', '-74.2750']
Best model has likelihood: -74.2750
SP score = 0.8233
Training of 3 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe22022a460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f89dfa60>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 13s - loss: 734.1211 - loglik: -7.2709e+02 - logprior: -7.0285e+00
Epoch 2/10
13/13 - 8s - loss: 643.2061 - loglik: -6.4221e+02 - logprior: -9.9670e-01
Epoch 3/10
13/13 - 8s - loss: 592.9825 - loglik: -5.9204e+02 - logprior: -9.3835e-01
Epoch 4/10
13/13 - 8s - loss: 580.1334 - loglik: -5.7909e+02 - logprior: -1.0399e+00
Epoch 5/10
13/13 - 8s - loss: 574.6420 - loglik: -5.7370e+02 - logprior: -9.4496e-01
Epoch 6/10
13/13 - 8s - loss: 575.3134 - loglik: -5.7445e+02 - logprior: -8.6188e-01
Fitted a model with MAP estimate = -573.1260
expansions: [(11, 1), (107, 4), (156, 1), (173, 1)]
discards: []
Fitting a model of length 207 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 582.0414 - loglik: -5.7501e+02 - logprior: -7.0288e+00
Epoch 2/2
13/13 - 8s - loss: 573.5876 - loglik: -5.7212e+02 - logprior: -1.4667e+00
Fitted a model with MAP estimate = -571.1918
expansions: []
discards: [0]
Fitting a model of length 206 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 13s - loss: 582.0615 - loglik: -5.7289e+02 - logprior: -9.1731e+00
Epoch 2/2
13/13 - 8s - loss: 575.3370 - loglik: -5.7176e+02 - logprior: -3.5785e+00
Fitted a model with MAP estimate = -573.4903
expansions: [(0, 5)]
discards: [0]
Fitting a model of length 210 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 579.7910 - loglik: -5.7275e+02 - logprior: -7.0387e+00
Epoch 2/10
13/13 - 8s - loss: 570.5956 - loglik: -5.6928e+02 - logprior: -1.3118e+00
Epoch 3/10
13/13 - 8s - loss: 570.2913 - loglik: -5.6950e+02 - logprior: -7.8628e-01
Epoch 4/10
13/13 - 8s - loss: 567.6276 - loglik: -5.6690e+02 - logprior: -7.2431e-01
Epoch 5/10
13/13 - 8s - loss: 565.8659 - loglik: -5.6532e+02 - logprior: -5.5014e-01
Epoch 6/10
13/13 - 8s - loss: 567.0937 - loglik: -5.6659e+02 - logprior: -5.0630e-01
Fitted a model with MAP estimate = -565.9568
Time for alignment: 181.3908
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 734.6052 - loglik: -7.2757e+02 - logprior: -7.0363e+00
Epoch 2/10
13/13 - 8s - loss: 641.0074 - loglik: -6.3996e+02 - logprior: -1.0451e+00
Epoch 3/10
13/13 - 8s - loss: 590.1620 - loglik: -5.8897e+02 - logprior: -1.1874e+00
Epoch 4/10
13/13 - 8s - loss: 580.4745 - loglik: -5.7883e+02 - logprior: -1.6446e+00
Epoch 5/10
13/13 - 8s - loss: 573.5722 - loglik: -5.7221e+02 - logprior: -1.3611e+00
Epoch 6/10
13/13 - 8s - loss: 570.9733 - loglik: -5.6965e+02 - logprior: -1.3267e+00
Epoch 7/10
13/13 - 8s - loss: 571.3846 - loglik: -5.7005e+02 - logprior: -1.3374e+00
Fitted a model with MAP estimate = -570.1896
expansions: [(173, 1)]
discards: [0]
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 12s - loss: 584.8514 - loglik: -5.7508e+02 - logprior: -9.7684e+00
Epoch 2/2
13/13 - 8s - loss: 575.6669 - loglik: -5.7163e+02 - logprior: -4.0335e+00
Fitted a model with MAP estimate = -574.0570
expansions: [(0, 5)]
discards: [0]
Fitting a model of length 204 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 579.4451 - loglik: -5.7190e+02 - logprior: -7.5460e+00
Epoch 2/2
13/13 - 8s - loss: 572.4109 - loglik: -5.7051e+02 - logprior: -1.9049e+00
Fitted a model with MAP estimate = -570.6806
expansions: []
discards: [1 2 3 4]
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 11s - loss: 578.2013 - loglik: -5.7089e+02 - logprior: -7.3071e+00
Epoch 2/10
13/13 - 8s - loss: 571.9423 - loglik: -5.7005e+02 - logprior: -1.8954e+00
Epoch 3/10
13/13 - 8s - loss: 570.3298 - loglik: -5.6910e+02 - logprior: -1.2322e+00
Epoch 4/10
13/13 - 8s - loss: 569.1407 - loglik: -5.6818e+02 - logprior: -9.6300e-01
Epoch 5/10
13/13 - 8s - loss: 567.9921 - loglik: -5.6712e+02 - logprior: -8.7639e-01
Epoch 6/10
13/13 - 8s - loss: 568.0540 - loglik: -5.6721e+02 - logprior: -8.4188e-01
Fitted a model with MAP estimate = -567.3990
Time for alignment: 181.2905
Fitting a model of length 200 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 12s - loss: 733.0971 - loglik: -7.2607e+02 - logprior: -7.0273e+00
Epoch 2/10
13/13 - 8s - loss: 645.0089 - loglik: -6.4398e+02 - logprior: -1.0261e+00
Epoch 3/10
13/13 - 8s - loss: 592.0309 - loglik: -5.9082e+02 - logprior: -1.2065e+00
Epoch 4/10
13/13 - 8s - loss: 576.3022 - loglik: -5.7483e+02 - logprior: -1.4771e+00
Epoch 5/10
13/13 - 8s - loss: 573.7125 - loglik: -5.7237e+02 - logprior: -1.3417e+00
Epoch 6/10
13/13 - 8s - loss: 569.5674 - loglik: -5.6831e+02 - logprior: -1.2546e+00
Epoch 7/10
13/13 - 8s - loss: 570.9768 - loglik: -5.6967e+02 - logprior: -1.3082e+00
Fitted a model with MAP estimate = -569.1745
expansions: [(183, 1)]
discards: []
Fitting a model of length 201 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 579.7385 - loglik: -5.7238e+02 - logprior: -7.3593e+00
Epoch 2/2
13/13 - 8s - loss: 570.2948 - loglik: -5.6844e+02 - logprior: -1.8522e+00
Fitted a model with MAP estimate = -569.8066
expansions: [(84, 1)]
discards: []
Fitting a model of length 202 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 11s - loss: 577.0277 - loglik: -5.6975e+02 - logprior: -7.2813e+00
Epoch 2/2
13/13 - 8s - loss: 569.5103 - loglik: -5.6775e+02 - logprior: -1.7632e+00
Fitted a model with MAP estimate = -568.5513
expansions: [(51, 1)]
discards: []
Fitting a model of length 203 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 12s - loss: 575.5066 - loglik: -5.6835e+02 - logprior: -7.1518e+00
Epoch 2/10
13/13 - 8s - loss: 569.0855 - loglik: -5.6746e+02 - logprior: -1.6265e+00
Epoch 3/10
13/13 - 8s - loss: 568.2367 - loglik: -5.6713e+02 - logprior: -1.1035e+00
Epoch 4/10
13/13 - 8s - loss: 565.7041 - loglik: -5.6484e+02 - logprior: -8.6057e-01
Epoch 5/10
13/13 - 8s - loss: 567.3690 - loglik: -5.6657e+02 - logprior: -8.0209e-01
Fitted a model with MAP estimate = -565.5538
Time for alignment: 175.8617
Computed alignments with likelihoods: ['-565.9568', '-567.3990', '-565.5538']
Best model has likelihood: -565.5538
SP score = 0.6877
Training of 3 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25d6234c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f92f6e80>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.9546 - loglik: -3.4522e+02 - logprior: -7.6735e+01
Epoch 2/10
10/10 - 1s - loss: 336.7050 - loglik: -3.1910e+02 - logprior: -1.7604e+01
Epoch 3/10
10/10 - 1s - loss: 296.6255 - loglik: -2.9019e+02 - logprior: -6.4371e+00
Epoch 4/10
10/10 - 1s - loss: 276.5388 - loglik: -2.7400e+02 - logprior: -2.5394e+00
Epoch 5/10
10/10 - 1s - loss: 269.4747 - loglik: -2.6893e+02 - logprior: -5.4888e-01
Epoch 6/10
10/10 - 1s - loss: 266.9417 - loglik: -2.6757e+02 - logprior: 0.6296
Epoch 7/10
10/10 - 1s - loss: 265.2701 - loglik: -2.6674e+02 - logprior: 1.4698
Epoch 8/10
10/10 - 1s - loss: 264.3520 - loglik: -2.6632e+02 - logprior: 1.9661
Epoch 9/10
10/10 - 1s - loss: 263.9853 - loglik: -2.6629e+02 - logprior: 2.3090
Epoch 10/10
10/10 - 1s - loss: 263.2741 - loglik: -2.6588e+02 - logprior: 2.6039
Fitted a model with MAP estimate = -263.1028
expansions: [(10, 1), (11, 4), (12, 1), (19, 2), (36, 4), (45, 3), (63, 3), (78, 2), (84, 3), (86, 1), (88, 4), (89, 1)]
discards: [0]
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 351.7171 - loglik: -2.6626e+02 - logprior: -8.5460e+01
Epoch 2/2
10/10 - 2s - loss: 285.6277 - loglik: -2.5329e+02 - logprior: -3.2336e+01
Fitted a model with MAP estimate = -273.8867
expansions: [(0, 3)]
discards: [  0  43  44  95 104]
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 316.7565 - loglik: -2.4963e+02 - logprior: -6.7124e+01
Epoch 2/2
10/10 - 1s - loss: 258.1532 - loglik: -2.4398e+02 - logprior: -1.4169e+01
Fitted a model with MAP estimate = -248.8031
expansions: [(111, 3)]
discards: [0 1]
Fitting a model of length 124 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 327.0597 - loglik: -2.4423e+02 - logprior: -8.2833e+01
Epoch 2/10
10/10 - 1s - loss: 267.9943 - loglik: -2.4173e+02 - logprior: -2.6267e+01
Epoch 3/10
10/10 - 2s - loss: 247.0511 - loglik: -2.4037e+02 - logprior: -6.6807e+00
Epoch 4/10
10/10 - 2s - loss: 237.7555 - loglik: -2.3953e+02 - logprior: 1.7737
Epoch 5/10
10/10 - 1s - loss: 234.4205 - loglik: -2.3936e+02 - logprior: 4.9433
Epoch 6/10
10/10 - 1s - loss: 232.8912 - loglik: -2.3946e+02 - logprior: 6.5648
Epoch 7/10
10/10 - 2s - loss: 231.7938 - loglik: -2.3935e+02 - logprior: 7.5588
Epoch 8/10
10/10 - 1s - loss: 230.9049 - loglik: -2.3917e+02 - logprior: 8.2606
Epoch 9/10
10/10 - 1s - loss: 230.7092 - loglik: -2.3954e+02 - logprior: 8.8343
Epoch 10/10
10/10 - 1s - loss: 230.0429 - loglik: -2.3940e+02 - logprior: 9.3534
Fitted a model with MAP estimate = -229.8652
Time for alignment: 49.1916
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.9001 - loglik: -3.4516e+02 - logprior: -7.6740e+01
Epoch 2/10
10/10 - 1s - loss: 336.0222 - loglik: -3.1842e+02 - logprior: -1.7601e+01
Epoch 3/10
10/10 - 1s - loss: 295.6197 - loglik: -2.8931e+02 - logprior: -6.3137e+00
Epoch 4/10
10/10 - 1s - loss: 277.7530 - loglik: -2.7550e+02 - logprior: -2.2491e+00
Epoch 5/10
10/10 - 1s - loss: 271.0410 - loglik: -2.7090e+02 - logprior: -1.3843e-01
Epoch 6/10
10/10 - 1s - loss: 267.3493 - loglik: -2.6840e+02 - logprior: 1.0544
Epoch 7/10
10/10 - 1s - loss: 265.8504 - loglik: -2.6764e+02 - logprior: 1.7903
Epoch 8/10
10/10 - 1s - loss: 264.1847 - loglik: -2.6660e+02 - logprior: 2.4184
Epoch 9/10
10/10 - 1s - loss: 263.3668 - loglik: -2.6614e+02 - logprior: 2.7734
Epoch 10/10
10/10 - 1s - loss: 261.8520 - loglik: -2.6490e+02 - logprior: 3.0517
Fitted a model with MAP estimate = -260.8510
expansions: [(5, 2), (6, 2), (8, 1), (12, 3), (36, 4), (45, 3), (48, 2), (63, 4), (83, 4), (86, 1), (89, 1)]
discards: [0]
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 349.1734 - loglik: -2.6362e+02 - logprior: -8.5558e+01
Epoch 2/2
10/10 - 1s - loss: 284.5299 - loglik: -2.5169e+02 - logprior: -3.2836e+01
Fitted a model with MAP estimate = -272.7640
expansions: [(0, 3), (16, 1), (17, 1), (112, 3)]
discards: [  0  43  44  79 105 106]
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 315.6917 - loglik: -2.4810e+02 - logprior: -6.7589e+01
Epoch 2/2
10/10 - 2s - loss: 256.6005 - loglik: -2.4225e+02 - logprior: -1.4347e+01
Fitted a model with MAP estimate = -247.1247
expansions: [(64, 1), (115, 3)]
discards: [ 0  1 14]
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 326.3472 - loglik: -2.4350e+02 - logprior: -8.2851e+01
Epoch 2/10
10/10 - 2s - loss: 266.6376 - loglik: -2.4010e+02 - logprior: -2.6536e+01
Epoch 3/10
10/10 - 2s - loss: 244.7066 - loglik: -2.3778e+02 - logprior: -6.9289e+00
Epoch 4/10
10/10 - 2s - loss: 235.5173 - loglik: -2.3728e+02 - logprior: 1.7605
Epoch 5/10
10/10 - 2s - loss: 231.9730 - loglik: -2.3691e+02 - logprior: 4.9376
Epoch 6/10
10/10 - 2s - loss: 230.2268 - loglik: -2.3677e+02 - logprior: 6.5435
Epoch 7/10
10/10 - 2s - loss: 229.1633 - loglik: -2.3670e+02 - logprior: 7.5355
Epoch 8/10
10/10 - 2s - loss: 228.5600 - loglik: -2.3681e+02 - logprior: 8.2503
Epoch 9/10
10/10 - 2s - loss: 228.0058 - loglik: -2.3683e+02 - logprior: 8.8261
Epoch 10/10
10/10 - 2s - loss: 227.6222 - loglik: -2.3697e+02 - logprior: 9.3465
Fitted a model with MAP estimate = -227.3286
Time for alignment: 48.6479
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 421.7662 - loglik: -3.4503e+02 - logprior: -7.6735e+01
Epoch 2/10
10/10 - 1s - loss: 336.4644 - loglik: -3.1885e+02 - logprior: -1.7614e+01
Epoch 3/10
10/10 - 1s - loss: 297.7224 - loglik: -2.9121e+02 - logprior: -6.5123e+00
Epoch 4/10
10/10 - 1s - loss: 278.2558 - loglik: -2.7568e+02 - logprior: -2.5745e+00
Epoch 5/10
10/10 - 1s - loss: 270.2282 - loglik: -2.6959e+02 - logprior: -6.3941e-01
Epoch 6/10
10/10 - 1s - loss: 267.0549 - loglik: -2.6748e+02 - logprior: 0.4264
Epoch 7/10
10/10 - 1s - loss: 265.1700 - loglik: -2.6639e+02 - logprior: 1.2182
Epoch 8/10
10/10 - 1s - loss: 264.2002 - loglik: -2.6592e+02 - logprior: 1.7223
Epoch 9/10
10/10 - 1s - loss: 262.8093 - loglik: -2.6491e+02 - logprior: 2.0979
Epoch 10/10
10/10 - 1s - loss: 262.8721 - loglik: -2.6526e+02 - logprior: 2.3898
Fitted a model with MAP estimate = -262.3936
expansions: [(5, 1), (6, 1), (10, 2), (12, 4), (18, 2), (36, 4), (45, 3), (50, 1), (59, 1), (79, 2), (83, 2), (88, 5), (89, 1)]
discards: [0]
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 348.7033 - loglik: -2.6338e+02 - logprior: -8.5324e+01
Epoch 2/2
10/10 - 2s - loss: 282.2724 - loglik: -2.5003e+02 - logprior: -3.2242e+01
Fitted a model with MAP estimate = -270.6941
expansions: [(0, 3), (58, 1), (110, 2), (112, 1)]
discards: [ 0 47 97]
Fitting a model of length 129 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 312.5801 - loglik: -2.4559e+02 - logprior: -6.6994e+01
Epoch 2/2
10/10 - 2s - loss: 252.2198 - loglik: -2.3856e+02 - logprior: -1.3658e+01
Fitted a model with MAP estimate = -242.8559
expansions: []
discards: [0 1]
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 320.7921 - loglik: -2.3964e+02 - logprior: -8.1148e+01
Epoch 2/10
10/10 - 2s - loss: 259.9277 - loglik: -2.3802e+02 - logprior: -2.1903e+01
Epoch 3/10
10/10 - 2s - loss: 240.3989 - loglik: -2.3643e+02 - logprior: -3.9691e+00
Epoch 4/10
10/10 - 2s - loss: 233.5716 - loglik: -2.3607e+02 - logprior: 2.4937
Epoch 5/10
10/10 - 2s - loss: 230.0537 - loglik: -2.3546e+02 - logprior: 5.4029
Epoch 6/10
10/10 - 2s - loss: 228.3419 - loglik: -2.3532e+02 - logprior: 6.9781
Epoch 7/10
10/10 - 2s - loss: 226.8691 - loglik: -2.3485e+02 - logprior: 7.9763
Epoch 8/10
10/10 - 2s - loss: 226.4085 - loglik: -2.3510e+02 - logprior: 8.6881
Epoch 9/10
10/10 - 2s - loss: 225.5876 - loglik: -2.3488e+02 - logprior: 9.2910
Epoch 10/10
10/10 - 2s - loss: 225.3164 - loglik: -2.3513e+02 - logprior: 9.8173
Fitted a model with MAP estimate = -225.0185
Time for alignment: 49.3489
Computed alignments with likelihoods: ['-229.8652', '-227.3286', '-225.0185']
Best model has likelihood: -225.0185
SP score = 0.8072
Training of 3 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2884f8880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b28b17f0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 97.2114 - loglik: -9.2787e+01 - logprior: -4.4241e+00
Epoch 2/10
17/17 - 1s - loss: 75.6522 - loglik: -7.4126e+01 - logprior: -1.5261e+00
Epoch 3/10
17/17 - 1s - loss: 65.4908 - loglik: -6.3857e+01 - logprior: -1.6338e+00
Epoch 4/10
17/17 - 1s - loss: 63.6758 - loglik: -6.2040e+01 - logprior: -1.6359e+00
Epoch 5/10
17/17 - 1s - loss: 63.2615 - loglik: -6.1732e+01 - logprior: -1.5290e+00
Epoch 6/10
17/17 - 1s - loss: 63.0607 - loglik: -6.1517e+01 - logprior: -1.5436e+00
Epoch 7/10
17/17 - 1s - loss: 62.8991 - loglik: -6.1370e+01 - logprior: -1.5290e+00
Epoch 8/10
17/17 - 1s - loss: 62.9136 - loglik: -6.1401e+01 - logprior: -1.5130e+00
Fitted a model with MAP estimate = -62.8738
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 70.2926 - loglik: -6.4782e+01 - logprior: -5.5108e+00
Epoch 2/2
17/17 - 1s - loss: 61.7458 - loglik: -5.9204e+01 - logprior: -2.5416e+00
Fitted a model with MAP estimate = -59.5825
expansions: []
discards: [13 16]
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 62.4126 - loglik: -5.8071e+01 - logprior: -4.3414e+00
Epoch 2/2
17/17 - 1s - loss: 58.7044 - loglik: -5.7040e+01 - logprior: -1.6643e+00
Fitted a model with MAP estimate = -58.3390
expansions: []
discards: []
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 5s - loss: 61.5910 - loglik: -5.7364e+01 - logprior: -4.2271e+00
Epoch 2/10
17/17 - 1s - loss: 58.6322 - loglik: -5.6989e+01 - logprior: -1.6435e+00
Epoch 3/10
17/17 - 1s - loss: 58.1901 - loglik: -5.6780e+01 - logprior: -1.4102e+00
Epoch 4/10
17/17 - 1s - loss: 58.0545 - loglik: -5.6707e+01 - logprior: -1.3478e+00
Epoch 5/10
17/17 - 1s - loss: 58.0831 - loglik: -5.6764e+01 - logprior: -1.3191e+00
Fitted a model with MAP estimate = -57.9429
Time for alignment: 31.7944
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 97.1465 - loglik: -9.2725e+01 - logprior: -4.4217e+00
Epoch 2/10
17/17 - 1s - loss: 74.8000 - loglik: -7.3257e+01 - logprior: -1.5430e+00
Epoch 3/10
17/17 - 1s - loss: 65.5145 - loglik: -6.3846e+01 - logprior: -1.6687e+00
Epoch 4/10
17/17 - 1s - loss: 63.6513 - loglik: -6.2028e+01 - logprior: -1.6230e+00
Epoch 5/10
17/17 - 1s - loss: 63.1825 - loglik: -6.1641e+01 - logprior: -1.5412e+00
Epoch 6/10
17/17 - 1s - loss: 63.0397 - loglik: -6.1485e+01 - logprior: -1.5548e+00
Epoch 7/10
17/17 - 1s - loss: 63.0659 - loglik: -6.1534e+01 - logprior: -1.5319e+00
Fitted a model with MAP estimate = -62.9196
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 70.0796 - loglik: -6.4575e+01 - logprior: -5.5050e+00
Epoch 2/2
17/17 - 1s - loss: 61.7458 - loglik: -5.9231e+01 - logprior: -2.5148e+00
Fitted a model with MAP estimate = -59.4788
expansions: []
discards: [13 16]
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 5s - loss: 62.2970 - loglik: -5.7964e+01 - logprior: -4.3330e+00
Epoch 2/2
17/17 - 1s - loss: 58.6374 - loglik: -5.6981e+01 - logprior: -1.6567e+00
Fitted a model with MAP estimate = -58.3264
expansions: []
discards: []
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.5732 - loglik: -5.7346e+01 - logprior: -4.2271e+00
Epoch 2/10
17/17 - 1s - loss: 58.5487 - loglik: -5.6905e+01 - logprior: -1.6437e+00
Epoch 3/10
17/17 - 1s - loss: 58.2790 - loglik: -5.6859e+01 - logprior: -1.4201e+00
Epoch 4/10
17/17 - 1s - loss: 58.1782 - loglik: -5.6833e+01 - logprior: -1.3455e+00
Epoch 5/10
17/17 - 1s - loss: 57.9634 - loglik: -5.6642e+01 - logprior: -1.3212e+00
Epoch 6/10
17/17 - 1s - loss: 57.8661 - loglik: -5.6576e+01 - logprior: -1.2900e+00
Epoch 7/10
17/17 - 1s - loss: 57.9363 - loglik: -5.6665e+01 - logprior: -1.2712e+00
Fitted a model with MAP estimate = -57.8658
Time for alignment: 30.1571
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 97.2098 - loglik: -9.2786e+01 - logprior: -4.4234e+00
Epoch 2/10
17/17 - 1s - loss: 75.1454 - loglik: -7.3612e+01 - logprior: -1.5336e+00
Epoch 3/10
17/17 - 1s - loss: 65.2649 - loglik: -6.3607e+01 - logprior: -1.6576e+00
Epoch 4/10
17/17 - 1s - loss: 63.6149 - loglik: -6.1986e+01 - logprior: -1.6286e+00
Epoch 5/10
17/17 - 1s - loss: 63.3375 - loglik: -6.1798e+01 - logprior: -1.5394e+00
Epoch 6/10
17/17 - 1s - loss: 63.0283 - loglik: -6.1478e+01 - logprior: -1.5508e+00
Epoch 7/10
17/17 - 1s - loss: 62.9556 - loglik: -6.1424e+01 - logprior: -1.5313e+00
Epoch 8/10
17/17 - 1s - loss: 62.9907 - loglik: -6.1473e+01 - logprior: -1.5174e+00
Fitted a model with MAP estimate = -62.8726
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 5s - loss: 70.1365 - loglik: -6.4625e+01 - logprior: -5.5110e+00
Epoch 2/2
17/17 - 1s - loss: 61.7742 - loglik: -5.9242e+01 - logprior: -2.5321e+00
Fitted a model with MAP estimate = -59.5442
expansions: []
discards: [13 16]
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 3s - loss: 62.3846 - loglik: -5.8047e+01 - logprior: -4.3375e+00
Epoch 2/2
17/17 - 1s - loss: 58.6513 - loglik: -5.6988e+01 - logprior: -1.6636e+00
Fitted a model with MAP estimate = -58.3337
expansions: []
discards: []
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 61.6022 - loglik: -5.7379e+01 - logprior: -4.2235e+00
Epoch 2/10
17/17 - 1s - loss: 58.6354 - loglik: -5.6984e+01 - logprior: -1.6513e+00
Epoch 3/10
17/17 - 1s - loss: 58.2262 - loglik: -5.6818e+01 - logprior: -1.4084e+00
Epoch 4/10
17/17 - 1s - loss: 58.2038 - loglik: -5.6853e+01 - logprior: -1.3511e+00
Epoch 5/10
17/17 - 1s - loss: 58.0089 - loglik: -5.6694e+01 - logprior: -1.3151e+00
Epoch 6/10
17/17 - 1s - loss: 57.7858 - loglik: -5.6494e+01 - logprior: -1.2918e+00
Epoch 7/10
17/17 - 1s - loss: 58.0203 - loglik: -5.6746e+01 - logprior: -1.2741e+00
Fitted a model with MAP estimate = -57.8692
Time for alignment: 29.9659
Computed alignments with likelihoods: ['-57.9429', '-57.8658', '-57.8692']
Best model has likelihood: -57.8658
SP score = 0.7673
Training of 3 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb64bfd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f9ed4c10>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 193.5284 - loglik: -1.8544e+02 - logprior: -8.0902e+00
Epoch 2/10
13/13 - 1s - loss: 163.6504 - loglik: -1.6144e+02 - logprior: -2.2080e+00
Epoch 3/10
13/13 - 1s - loss: 146.0143 - loglik: -1.4411e+02 - logprior: -1.9047e+00
Epoch 4/10
13/13 - 1s - loss: 139.4388 - loglik: -1.3743e+02 - logprior: -2.0051e+00
Epoch 5/10
13/13 - 1s - loss: 136.6146 - loglik: -1.3473e+02 - logprior: -1.8843e+00
Epoch 6/10
13/13 - 1s - loss: 135.4958 - loglik: -1.3369e+02 - logprior: -1.8035e+00
Epoch 7/10
13/13 - 1s - loss: 134.7432 - loglik: -1.3292e+02 - logprior: -1.8268e+00
Epoch 8/10
13/13 - 1s - loss: 134.4889 - loglik: -1.3267e+02 - logprior: -1.8158e+00
Epoch 9/10
13/13 - 1s - loss: 134.5147 - loglik: -1.3271e+02 - logprior: -1.8023e+00
Fitted a model with MAP estimate = -134.3079
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (34, 1), (41, 1), (44, 1), (45, 2), (46, 2)]
discards: [0]
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 143.5947 - loglik: -1.3407e+02 - logprior: -9.5222e+00
Epoch 2/2
13/13 - 1s - loss: 129.7166 - loglik: -1.2525e+02 - logprior: -4.4634e+00
Fitted a model with MAP estimate = -127.7260
expansions: [(0, 2)]
discards: [ 0 23 56]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 130.7955 - loglik: -1.2349e+02 - logprior: -7.3097e+00
Epoch 2/2
13/13 - 1s - loss: 124.1157 - loglik: -1.2194e+02 - logprior: -2.1752e+00
Fitted a model with MAP estimate = -123.2275
expansions: [(17, 1)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 133.6127 - loglik: -1.2435e+02 - logprior: -9.2662e+00
Epoch 2/10
13/13 - 1s - loss: 125.7603 - loglik: -1.2226e+02 - logprior: -3.5024e+00
Epoch 3/10
13/13 - 1s - loss: 122.5809 - loglik: -1.2090e+02 - logprior: -1.6840e+00
Epoch 4/10
13/13 - 1s - loss: 121.7849 - loglik: -1.2046e+02 - logprior: -1.3269e+00
Epoch 5/10
13/13 - 1s - loss: 120.7802 - loglik: -1.1959e+02 - logprior: -1.1863e+00
Epoch 6/10
13/13 - 1s - loss: 120.0528 - loglik: -1.1884e+02 - logprior: -1.2140e+00
Epoch 7/10
13/13 - 1s - loss: 120.4620 - loglik: -1.1923e+02 - logprior: -1.2280e+00
Fitted a model with MAP estimate = -119.7545
Time for alignment: 36.9880
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 193.8961 - loglik: -1.8581e+02 - logprior: -8.0907e+00
Epoch 2/10
13/13 - 1s - loss: 163.7108 - loglik: -1.6149e+02 - logprior: -2.2180e+00
Epoch 3/10
13/13 - 1s - loss: 146.5539 - loglik: -1.4460e+02 - logprior: -1.9559e+00
Epoch 4/10
13/13 - 1s - loss: 140.2697 - loglik: -1.3822e+02 - logprior: -2.0502e+00
Epoch 5/10
13/13 - 1s - loss: 136.8326 - loglik: -1.3494e+02 - logprior: -1.8956e+00
Epoch 6/10
13/13 - 1s - loss: 135.0635 - loglik: -1.3325e+02 - logprior: -1.8176e+00
Epoch 7/10
13/13 - 1s - loss: 134.1205 - loglik: -1.3227e+02 - logprior: -1.8545e+00
Epoch 8/10
13/13 - 1s - loss: 134.2026 - loglik: -1.3236e+02 - logprior: -1.8387e+00
Fitted a model with MAP estimate = -133.9275
expansions: [(12, 1), (17, 5), (18, 2), (29, 1), (32, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 142.5064 - loglik: -1.3299e+02 - logprior: -9.5161e+00
Epoch 2/2
13/13 - 1s - loss: 129.0567 - loglik: -1.2464e+02 - logprior: -4.4135e+00
Fitted a model with MAP estimate = -127.5272
expansions: [(0, 2)]
discards: [ 0 23]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 130.8282 - loglik: -1.2353e+02 - logprior: -7.3019e+00
Epoch 2/2
13/13 - 1s - loss: 124.1132 - loglik: -1.2194e+02 - logprior: -2.1777e+00
Fitted a model with MAP estimate = -123.2098
expansions: [(17, 1)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 133.5377 - loglik: -1.2429e+02 - logprior: -9.2468e+00
Epoch 2/10
13/13 - 1s - loss: 125.7465 - loglik: -1.2227e+02 - logprior: -3.4747e+00
Epoch 3/10
13/13 - 1s - loss: 122.7699 - loglik: -1.2110e+02 - logprior: -1.6736e+00
Epoch 4/10
13/13 - 1s - loss: 121.5953 - loglik: -1.2027e+02 - logprior: -1.3253e+00
Epoch 5/10
13/13 - 1s - loss: 120.6218 - loglik: -1.1943e+02 - logprior: -1.1872e+00
Epoch 6/10
13/13 - 1s - loss: 120.3273 - loglik: -1.1911e+02 - logprior: -1.2182e+00
Epoch 7/10
13/13 - 1s - loss: 120.1435 - loglik: -1.1891e+02 - logprior: -1.2323e+00
Epoch 8/10
13/13 - 1s - loss: 119.4005 - loglik: -1.1820e+02 - logprior: -1.2024e+00
Epoch 9/10
13/13 - 1s - loss: 119.8656 - loglik: -1.1867e+02 - logprior: -1.1927e+00
Fitted a model with MAP estimate = -119.5372
Time for alignment: 34.8758
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 193.4193 - loglik: -1.8533e+02 - logprior: -8.0909e+00
Epoch 2/10
13/13 - 1s - loss: 164.3573 - loglik: -1.6214e+02 - logprior: -2.2151e+00
Epoch 3/10
13/13 - 1s - loss: 147.5491 - loglik: -1.4560e+02 - logprior: -1.9473e+00
Epoch 4/10
13/13 - 1s - loss: 140.2477 - loglik: -1.3819e+02 - logprior: -2.0614e+00
Epoch 5/10
13/13 - 1s - loss: 137.2297 - loglik: -1.3532e+02 - logprior: -1.9051e+00
Epoch 6/10
13/13 - 1s - loss: 135.3373 - loglik: -1.3352e+02 - logprior: -1.8135e+00
Epoch 7/10
13/13 - 1s - loss: 135.4286 - loglik: -1.3360e+02 - logprior: -1.8257e+00
Fitted a model with MAP estimate = -134.8318
expansions: [(12, 1), (17, 5), (18, 2), (19, 2), (34, 1), (41, 1), (42, 1), (45, 1), (46, 2)]
discards: [0]
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 3s - loss: 143.4120 - loglik: -1.3389e+02 - logprior: -9.5224e+00
Epoch 2/2
13/13 - 1s - loss: 129.8988 - loglik: -1.2545e+02 - logprior: -4.4467e+00
Fitted a model with MAP estimate = -127.7484
expansions: [(0, 2)]
discards: [ 0 24 26]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 131.1768 - loglik: -1.2388e+02 - logprior: -7.2994e+00
Epoch 2/2
13/13 - 1s - loss: 124.0559 - loglik: -1.2188e+02 - logprior: -2.1780e+00
Fitted a model with MAP estimate = -123.2620
expansions: [(17, 1)]
discards: [0]
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 133.8640 - loglik: -1.2460e+02 - logprior: -9.2642e+00
Epoch 2/10
13/13 - 1s - loss: 126.0594 - loglik: -1.2256e+02 - logprior: -3.5011e+00
Epoch 3/10
13/13 - 1s - loss: 122.3806 - loglik: -1.2069e+02 - logprior: -1.6937e+00
Epoch 4/10
13/13 - 1s - loss: 121.5269 - loglik: -1.2020e+02 - logprior: -1.3273e+00
Epoch 5/10
13/13 - 1s - loss: 120.7989 - loglik: -1.1961e+02 - logprior: -1.1938e+00
Epoch 6/10
13/13 - 1s - loss: 120.3929 - loglik: -1.1917e+02 - logprior: -1.2192e+00
Epoch 7/10
13/13 - 1s - loss: 120.1355 - loglik: -1.1890e+02 - logprior: -1.2391e+00
Epoch 8/10
13/13 - 1s - loss: 119.4565 - loglik: -1.1826e+02 - logprior: -1.2007e+00
Epoch 9/10
13/13 - 1s - loss: 119.8657 - loglik: -1.1867e+02 - logprior: -1.1991e+00
Fitted a model with MAP estimate = -119.5453
Time for alignment: 33.5274
Computed alignments with likelihoods: ['-119.7545', '-119.5372', '-119.5453']
Best model has likelihood: -119.5372
SP score = 0.8824
Training of 3 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2bb0ca760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25d1ff9a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 240.9098 - loglik: -2.2837e+02 - logprior: -1.2540e+01
Epoch 2/10
11/11 - 1s - loss: 208.2881 - loglik: -2.0504e+02 - logprior: -3.2519e+00
Epoch 3/10
11/11 - 1s - loss: 182.5041 - loglik: -1.8042e+02 - logprior: -2.0797e+00
Epoch 4/10
11/11 - 1s - loss: 166.3707 - loglik: -1.6453e+02 - logprior: -1.8426e+00
Epoch 5/10
11/11 - 1s - loss: 161.3964 - loglik: -1.5984e+02 - logprior: -1.5539e+00
Epoch 6/10
11/11 - 1s - loss: 159.1627 - loglik: -1.5770e+02 - logprior: -1.4666e+00
Epoch 7/10
11/11 - 1s - loss: 157.8735 - loglik: -1.5660e+02 - logprior: -1.2713e+00
Epoch 8/10
11/11 - 1s - loss: 157.2372 - loglik: -1.5614e+02 - logprior: -1.0981e+00
Epoch 9/10
11/11 - 1s - loss: 156.9933 - loglik: -1.5599e+02 - logprior: -9.9951e-01
Epoch 10/10
11/11 - 1s - loss: 156.5359 - loglik: -1.5559e+02 - logprior: -9.5017e-01
Fitted a model with MAP estimate = -156.6377
expansions: [(0, 6), (21, 1), (27, 1), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 170.4662 - loglik: -1.5479e+02 - logprior: -1.5677e+01
Epoch 2/2
11/11 - 1s - loss: 150.5233 - loglik: -1.4618e+02 - logprior: -4.3422e+00
Fitted a model with MAP estimate = -147.3789
expansions: []
discards: [0]
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 161.9673 - loglik: -1.4789e+02 - logprior: -1.4076e+01
Epoch 2/2
11/11 - 1s - loss: 151.6244 - loglik: -1.4620e+02 - logprior: -5.4254e+00
Fitted a model with MAP estimate = -148.9677
expansions: []
discards: []
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.3213 - loglik: -1.4607e+02 - logprior: -1.2248e+01
Epoch 2/10
11/11 - 1s - loss: 147.8507 - loglik: -1.4479e+02 - logprior: -3.0645e+00
Epoch 3/10
11/11 - 1s - loss: 145.8363 - loglik: -1.4407e+02 - logprior: -1.7669e+00
Epoch 4/10
11/11 - 1s - loss: 144.5710 - loglik: -1.4345e+02 - logprior: -1.1183e+00
Epoch 5/10
11/11 - 1s - loss: 144.1987 - loglik: -1.4340e+02 - logprior: -8.0347e-01
Epoch 6/10
11/11 - 1s - loss: 143.9027 - loglik: -1.4317e+02 - logprior: -7.2887e-01
Epoch 7/10
11/11 - 1s - loss: 143.6270 - loglik: -1.4302e+02 - logprior: -6.0966e-01
Epoch 8/10
11/11 - 1s - loss: 143.1150 - loglik: -1.4254e+02 - logprior: -5.7498e-01
Epoch 9/10
11/11 - 1s - loss: 143.2521 - loglik: -1.4271e+02 - logprior: -5.3916e-01
Fitted a model with MAP estimate = -143.1830
Time for alignment: 36.2184
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 240.8315 - loglik: -2.2829e+02 - logprior: -1.2538e+01
Epoch 2/10
11/11 - 1s - loss: 208.3233 - loglik: -2.0508e+02 - logprior: -3.2449e+00
Epoch 3/10
11/11 - 1s - loss: 183.6967 - loglik: -1.8165e+02 - logprior: -2.0429e+00
Epoch 4/10
11/11 - 1s - loss: 167.0893 - loglik: -1.6532e+02 - logprior: -1.7689e+00
Epoch 5/10
11/11 - 1s - loss: 161.3363 - loglik: -1.5982e+02 - logprior: -1.5193e+00
Epoch 6/10
11/11 - 1s - loss: 158.8331 - loglik: -1.5742e+02 - logprior: -1.4157e+00
Epoch 7/10
11/11 - 1s - loss: 158.0907 - loglik: -1.5690e+02 - logprior: -1.1888e+00
Epoch 8/10
11/11 - 1s - loss: 157.3124 - loglik: -1.5626e+02 - logprior: -1.0512e+00
Epoch 9/10
11/11 - 1s - loss: 157.1771 - loglik: -1.5622e+02 - logprior: -9.5713e-01
Epoch 10/10
11/11 - 1s - loss: 156.8875 - loglik: -1.5598e+02 - logprior: -9.1128e-01
Fitted a model with MAP estimate = -156.8476
expansions: [(0, 6), (21, 1), (27, 1), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 171.5857 - loglik: -1.5591e+02 - logprior: -1.5678e+01
Epoch 2/2
11/11 - 1s - loss: 150.8449 - loglik: -1.4640e+02 - logprior: -4.4438e+00
Fitted a model with MAP estimate = -147.7239
expansions: []
discards: [ 0 37 42]
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 162.4289 - loglik: -1.4834e+02 - logprior: -1.4090e+01
Epoch 2/2
11/11 - 1s - loss: 152.0362 - loglik: -1.4660e+02 - logprior: -5.4361e+00
Fitted a model with MAP estimate = -149.2872
expansions: []
discards: []
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 158.2679 - loglik: -1.4600e+02 - logprior: -1.2264e+01
Epoch 2/10
11/11 - 1s - loss: 148.2175 - loglik: -1.4515e+02 - logprior: -3.0724e+00
Epoch 3/10
11/11 - 1s - loss: 145.8668 - loglik: -1.4411e+02 - logprior: -1.7590e+00
Epoch 4/10
11/11 - 1s - loss: 145.0368 - loglik: -1.4391e+02 - logprior: -1.1299e+00
Epoch 5/10
11/11 - 1s - loss: 143.8232 - loglik: -1.4302e+02 - logprior: -7.9984e-01
Epoch 6/10
11/11 - 1s - loss: 143.9373 - loglik: -1.4321e+02 - logprior: -7.2965e-01
Fitted a model with MAP estimate = -143.5519
Time for alignment: 34.6488
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 240.9318 - loglik: -2.2839e+02 - logprior: -1.2541e+01
Epoch 2/10
11/11 - 1s - loss: 208.3597 - loglik: -2.0511e+02 - logprior: -3.2514e+00
Epoch 3/10
11/11 - 1s - loss: 185.6770 - loglik: -1.8362e+02 - logprior: -2.0527e+00
Epoch 4/10
11/11 - 1s - loss: 168.1191 - loglik: -1.6629e+02 - logprior: -1.8242e+00
Epoch 5/10
11/11 - 1s - loss: 161.3976 - loglik: -1.5984e+02 - logprior: -1.5588e+00
Epoch 6/10
11/11 - 1s - loss: 158.8135 - loglik: -1.5737e+02 - logprior: -1.4460e+00
Epoch 7/10
11/11 - 1s - loss: 157.7785 - loglik: -1.5656e+02 - logprior: -1.2179e+00
Epoch 8/10
11/11 - 1s - loss: 156.8895 - loglik: -1.5582e+02 - logprior: -1.0664e+00
Epoch 9/10
11/11 - 1s - loss: 156.9295 - loglik: -1.5595e+02 - logprior: -9.7661e-01
Fitted a model with MAP estimate = -156.7338
expansions: [(0, 6), (21, 1), (23, 2), (28, 1), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 171.3349 - loglik: -1.5574e+02 - logprior: -1.5590e+01
Epoch 2/2
11/11 - 1s - loss: 151.1355 - loglik: -1.4677e+02 - logprior: -4.3693e+00
Fitted a model with MAP estimate = -147.5653
expansions: []
discards: [ 0 30]
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 162.0788 - loglik: -1.4800e+02 - logprior: -1.4083e+01
Epoch 2/2
11/11 - 1s - loss: 151.5611 - loglik: -1.4613e+02 - logprior: -5.4294e+00
Fitted a model with MAP estimate = -148.9681
expansions: []
discards: []
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 157.8805 - loglik: -1.4563e+02 - logprior: -1.2246e+01
Epoch 2/10
11/11 - 1s - loss: 148.0663 - loglik: -1.4500e+02 - logprior: -3.0620e+00
Epoch 3/10
11/11 - 1s - loss: 145.9536 - loglik: -1.4420e+02 - logprior: -1.7570e+00
Epoch 4/10
11/11 - 1s - loss: 144.6381 - loglik: -1.4352e+02 - logprior: -1.1189e+00
Epoch 5/10
11/11 - 1s - loss: 144.2005 - loglik: -1.4340e+02 - logprior: -7.9809e-01
Epoch 6/10
11/11 - 1s - loss: 143.8779 - loglik: -1.4314e+02 - logprior: -7.3446e-01
Epoch 7/10
11/11 - 1s - loss: 143.6633 - loglik: -1.4305e+02 - logprior: -6.1077e-01
Epoch 8/10
11/11 - 1s - loss: 143.0730 - loglik: -1.4250e+02 - logprior: -5.7241e-01
Epoch 9/10
11/11 - 1s - loss: 143.3497 - loglik: -1.4281e+02 - logprior: -5.4134e-01
Fitted a model with MAP estimate = -143.1821
Time for alignment: 34.4232
Computed alignments with likelihoods: ['-143.1830', '-143.5519', '-143.1821']
Best model has likelihood: -143.1821
SP score = 0.9930
Training of 3 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2f955be20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2770c19a0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 572.7575 - loglik: -5.6378e+02 - logprior: -8.9778e+00
Epoch 2/10
12/12 - 5s - loss: 479.9604 - loglik: -4.7834e+02 - logprior: -1.6229e+00
Epoch 3/10
12/12 - 5s - loss: 395.5457 - loglik: -3.9365e+02 - logprior: -1.8977e+00
Epoch 4/10
12/12 - 5s - loss: 360.2952 - loglik: -3.5782e+02 - logprior: -2.4773e+00
Epoch 5/10
12/12 - 5s - loss: 348.8664 - loglik: -3.4621e+02 - logprior: -2.6525e+00
Epoch 6/10
12/12 - 5s - loss: 346.2646 - loglik: -3.4375e+02 - logprior: -2.5193e+00
Epoch 7/10
12/12 - 5s - loss: 344.0242 - loglik: -3.4164e+02 - logprior: -2.3804e+00
Epoch 8/10
12/12 - 5s - loss: 343.8902 - loglik: -3.4153e+02 - logprior: -2.3562e+00
Epoch 9/10
12/12 - 5s - loss: 343.4187 - loglik: -3.4105e+02 - logprior: -2.3670e+00
Epoch 10/10
12/12 - 5s - loss: 342.6240 - loglik: -3.4027e+02 - logprior: -2.3502e+00
Fitted a model with MAP estimate = -342.9457
expansions: [(11, 1), (12, 2), (13, 3), (15, 1), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (76, 1), (83, 1), (88, 2), (91, 1), (111, 1), (114, 1), (117, 1), (120, 2), (121, 2), (123, 1), (124, 2), (150, 1), (153, 1), (154, 4), (155, 1), (158, 1), (174, 3), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Fitting a model of length 239 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 12s - loss: 348.7569 - loglik: -3.3799e+02 - logprior: -1.0771e+01
Epoch 2/2
12/12 - 7s - loss: 323.7007 - loglik: -3.1979e+02 - logprior: -3.9119e+00
Fitted a model with MAP estimate = -319.2811
expansions: [(0, 3), (191, 1), (193, 1), (214, 1)]
discards: [  0  13  14 155]
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 324.4272 - loglik: -3.1639e+02 - logprior: -8.0394e+00
Epoch 2/2
12/12 - 7s - loss: 311.4980 - loglik: -3.1026e+02 - logprior: -1.2351e+00
Fitted a model with MAP estimate = -309.8788
expansions: [(15, 1)]
discards: [0 1]
Fitting a model of length 240 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 324.0355 - loglik: -3.1387e+02 - logprior: -1.0166e+01
Epoch 2/10
12/12 - 7s - loss: 315.7782 - loglik: -3.1263e+02 - logprior: -3.1499e+00
Epoch 3/10
12/12 - 7s - loss: 311.7153 - loglik: -3.1020e+02 - logprior: -1.5173e+00
Epoch 4/10
12/12 - 7s - loss: 308.8024 - loglik: -3.0895e+02 - logprior: 0.1437
Epoch 5/10
12/12 - 7s - loss: 307.4263 - loglik: -3.0819e+02 - logprior: 0.7615
Epoch 6/10
12/12 - 7s - loss: 306.6802 - loglik: -3.0747e+02 - logprior: 0.7866
Epoch 7/10
12/12 - 7s - loss: 307.1198 - loglik: -3.0777e+02 - logprior: 0.6455
Fitted a model with MAP estimate = -306.0651
Time for alignment: 170.4290
Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 573.8734 - loglik: -5.6489e+02 - logprior: -8.9863e+00
Epoch 2/10
12/12 - 5s - loss: 477.2198 - loglik: -4.7558e+02 - logprior: -1.6357e+00
Epoch 3/10
12/12 - 5s - loss: 397.3723 - loglik: -3.9545e+02 - logprior: -1.9233e+00
Epoch 4/10
12/12 - 5s - loss: 363.1028 - loglik: -3.6059e+02 - logprior: -2.5173e+00
Epoch 5/10
12/12 - 5s - loss: 350.4326 - loglik: -3.4780e+02 - logprior: -2.6295e+00
Epoch 6/10
12/12 - 5s - loss: 347.7805 - loglik: -3.4526e+02 - logprior: -2.5167e+00
Epoch 7/10
12/12 - 5s - loss: 344.3282 - loglik: -3.4188e+02 - logprior: -2.4496e+00
Epoch 8/10
12/12 - 5s - loss: 343.5441 - loglik: -3.4108e+02 - logprior: -2.4634e+00
Epoch 9/10
12/12 - 5s - loss: 343.9736 - loglik: -3.4149e+02 - logprior: -2.4816e+00
Fitted a model with MAP estimate = -343.2272
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 2), (47, 1), (48, 1), (49, 1), (63, 1), (65, 1), (83, 1), (87, 1), (88, 1), (91, 1), (109, 1), (110, 1), (113, 1), (118, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (155, 2), (157, 1), (171, 1), (173, 2), (174, 2), (175, 1), (185, 1), (186, 1)]
discards: [0 1]
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 348.9928 - loglik: -3.3826e+02 - logprior: -1.0737e+01
Epoch 2/2
12/12 - 7s - loss: 322.5765 - loglik: -3.1876e+02 - logprior: -3.8187e+00
Fitted a model with MAP estimate = -317.7827
expansions: [(0, 3)]
discards: [  0  12 218]
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 322.7802 - loglik: -3.1473e+02 - logprior: -8.0499e+00
Epoch 2/2
12/12 - 7s - loss: 311.5289 - loglik: -3.1029e+02 - logprior: -1.2371e+00
Fitted a model with MAP estimate = -309.5065
expansions: [(194, 1)]
discards: [0]
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 12s - loss: 323.7179 - loglik: -3.1354e+02 - logprior: -1.0182e+01
Epoch 2/10
12/12 - 7s - loss: 313.0625 - loglik: -3.0981e+02 - logprior: -3.2558e+00
Epoch 3/10
12/12 - 7s - loss: 311.0780 - loglik: -3.0932e+02 - logprior: -1.7561e+00
Epoch 4/10
12/12 - 7s - loss: 307.8108 - loglik: -3.0778e+02 - logprior: -2.9654e-02
Epoch 5/10
12/12 - 7s - loss: 306.8767 - loglik: -3.0742e+02 - logprior: 0.5438
Epoch 6/10
12/12 - 7s - loss: 306.1394 - loglik: -3.0664e+02 - logprior: 0.4986
Epoch 7/10
12/12 - 7s - loss: 305.5819 - loglik: -3.0612e+02 - logprior: 0.5420
Epoch 8/10
12/12 - 7s - loss: 305.5799 - loglik: -3.0614e+02 - logprior: 0.5635
Epoch 9/10
12/12 - 7s - loss: 304.6952 - loglik: -3.0533e+02 - logprior: 0.6347
Epoch 10/10
12/12 - 7s - loss: 304.4998 - loglik: -3.0522e+02 - logprior: 0.7165
Fitted a model with MAP estimate = -304.4883
Time for alignment: 188.5838
Fitting a model of length 192 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 574.1839 - loglik: -5.6518e+02 - logprior: -9.0050e+00
Epoch 2/10
12/12 - 5s - loss: 479.8928 - loglik: -4.7822e+02 - logprior: -1.6713e+00
Epoch 3/10
12/12 - 5s - loss: 394.7202 - loglik: -3.9271e+02 - logprior: -2.0129e+00
Epoch 4/10
12/12 - 5s - loss: 357.8783 - loglik: -3.5518e+02 - logprior: -2.6944e+00
Epoch 5/10
12/12 - 5s - loss: 348.8736 - loglik: -3.4609e+02 - logprior: -2.7831e+00
Epoch 6/10
12/12 - 5s - loss: 345.6935 - loglik: -3.4305e+02 - logprior: -2.6423e+00
Epoch 7/10
12/12 - 5s - loss: 344.9863 - loglik: -3.4242e+02 - logprior: -2.5624e+00
Epoch 8/10
12/12 - 5s - loss: 342.3851 - loglik: -3.3982e+02 - logprior: -2.5645e+00
Epoch 9/10
12/12 - 5s - loss: 344.2449 - loglik: -3.4169e+02 - logprior: -2.5575e+00
Fitted a model with MAP estimate = -342.6870
expansions: [(11, 1), (12, 2), (13, 1), (14, 2), (16, 2), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (83, 1), (85, 1), (88, 1), (91, 1), (109, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (149, 1), (152, 2), (153, 1), (154, 3), (158, 1), (172, 1), (173, 3), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Fitting a model of length 242 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 10s - loss: 347.1698 - loglik: -3.3646e+02 - logprior: -1.0709e+01
Epoch 2/2
12/12 - 7s - loss: 322.3250 - loglik: -3.1851e+02 - logprior: -3.8152e+00
Fitted a model with MAP estimate = -317.5884
expansions: [(0, 3), (194, 1)]
discards: [  0  12 219]
Fitting a model of length 243 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 11s - loss: 322.0663 - loglik: -3.1405e+02 - logprior: -8.0203e+00
Epoch 2/2
12/12 - 7s - loss: 311.3216 - loglik: -3.1010e+02 - logprior: -1.2212e+00
Fitted a model with MAP estimate = -308.8144
expansions: []
discards: [ 0 26]
Fitting a model of length 241 on 3904 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 10s - loss: 323.6010 - loglik: -3.1342e+02 - logprior: -1.0180e+01
Epoch 2/10
12/12 - 7s - loss: 313.7890 - loglik: -3.1060e+02 - logprior: -3.1860e+00
Epoch 3/10
12/12 - 7s - loss: 310.8452 - loglik: -3.0924e+02 - logprior: -1.6066e+00
Epoch 4/10
12/12 - 7s - loss: 306.5413 - loglik: -3.0666e+02 - logprior: 0.1139
Epoch 5/10
12/12 - 7s - loss: 307.2614 - loglik: -3.0797e+02 - logprior: 0.7049
Fitted a model with MAP estimate = -306.2137
Time for alignment: 151.7899
Computed alignments with likelihoods: ['-306.0651', '-304.4883', '-306.2137']
Best model has likelihood: -304.4883
SP score = 0.9727
Training of 3 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe16b49a670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe27fd013d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 303.7987 - loglik: -2.7673e+02 - logprior: -2.7068e+01
Epoch 2/10
10/10 - 1s - loss: 246.2658 - loglik: -2.3939e+02 - logprior: -6.8784e+00
Epoch 3/10
10/10 - 1s - loss: 209.3190 - loglik: -2.0574e+02 - logprior: -3.5749e+00
Epoch 4/10
10/10 - 1s - loss: 191.1356 - loglik: -1.8855e+02 - logprior: -2.5813e+00
Epoch 5/10
10/10 - 1s - loss: 182.6891 - loglik: -1.8045e+02 - logprior: -2.2365e+00
Epoch 6/10
10/10 - 1s - loss: 179.4897 - loglik: -1.7742e+02 - logprior: -2.0745e+00
Epoch 7/10
10/10 - 1s - loss: 177.4231 - loglik: -1.7555e+02 - logprior: -1.8752e+00
Epoch 8/10
10/10 - 1s - loss: 175.4588 - loglik: -1.7371e+02 - logprior: -1.7513e+00
Epoch 9/10
10/10 - 1s - loss: 174.9454 - loglik: -1.7325e+02 - logprior: -1.6928e+00
Epoch 10/10
10/10 - 1s - loss: 174.4217 - loglik: -1.7276e+02 - logprior: -1.6597e+00
Fitted a model with MAP estimate = -174.5186
expansions: [(0, 3), (11, 1), (12, 1), (24, 1), (35, 3), (36, 2), (37, 1), (48, 3), (49, 2), (60, 1), (69, 1), (70, 1), (72, 3), (73, 1)]
discards: []
Fitting a model of length 107 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 202.1063 - loglik: -1.6807e+02 - logprior: -3.4039e+01
Epoch 2/2
10/10 - 1s - loss: 167.2958 - loglik: -1.5736e+02 - logprior: -9.9313e+00
Fitted a model with MAP estimate = -161.1312
expansions: []
discards: [ 0 45]
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 187.4568 - loglik: -1.5675e+02 - logprior: -3.0704e+01
Epoch 2/2
10/10 - 1s - loss: 167.4956 - loglik: -1.5549e+02 - logprior: -1.2006e+01
Fitted a model with MAP estimate = -163.5609
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 184.3429 - loglik: -1.5549e+02 - logprior: -2.8849e+01
Epoch 2/10
10/10 - 1s - loss: 161.8785 - loglik: -1.5385e+02 - logprior: -8.0316e+00
Epoch 3/10
10/10 - 1s - loss: 156.1145 - loglik: -1.5331e+02 - logprior: -2.8063e+00
Epoch 4/10
10/10 - 1s - loss: 153.8369 - loglik: -1.5276e+02 - logprior: -1.0816e+00
Epoch 5/10
10/10 - 1s - loss: 152.6268 - loglik: -1.5227e+02 - logprior: -3.5846e-01
Epoch 6/10
10/10 - 1s - loss: 152.1805 - loglik: -1.5217e+02 - logprior: -1.3370e-02
Epoch 7/10
10/10 - 1s - loss: 151.8094 - loglik: -1.5199e+02 - logprior: 0.1850
Epoch 8/10
10/10 - 1s - loss: 151.6263 - loglik: -1.5199e+02 - logprior: 0.3624
Epoch 9/10
10/10 - 1s - loss: 151.4746 - loglik: -1.5200e+02 - logprior: 0.5257
Epoch 10/10
10/10 - 1s - loss: 151.1289 - loglik: -1.5180e+02 - logprior: 0.6663
Fitted a model with MAP estimate = -151.1440
Time for alignment: 38.6927
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.6789 - loglik: -2.7661e+02 - logprior: -2.7069e+01
Epoch 2/10
10/10 - 1s - loss: 246.9187 - loglik: -2.4003e+02 - logprior: -6.8892e+00
Epoch 3/10
10/10 - 1s - loss: 208.2157 - loglik: -2.0462e+02 - logprior: -3.5942e+00
Epoch 4/10
10/10 - 1s - loss: 186.8131 - loglik: -1.8426e+02 - logprior: -2.5571e+00
Epoch 5/10
10/10 - 1s - loss: 179.6692 - loglik: -1.7750e+02 - logprior: -2.1655e+00
Epoch 6/10
10/10 - 1s - loss: 177.4458 - loglik: -1.7548e+02 - logprior: -1.9618e+00
Epoch 7/10
10/10 - 1s - loss: 176.1104 - loglik: -1.7445e+02 - logprior: -1.6645e+00
Epoch 8/10
10/10 - 1s - loss: 176.1528 - loglik: -1.7470e+02 - logprior: -1.4555e+00
Fitted a model with MAP estimate = -175.6072
expansions: [(0, 3), (11, 1), (12, 1), (33, 1), (36, 4), (49, 3), (50, 2), (69, 2), (70, 1), (72, 2), (73, 3)]
discards: []
Fitting a model of length 106 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 203.6623 - loglik: -1.6996e+02 - logprior: -3.3698e+01
Epoch 2/2
10/10 - 1s - loss: 169.4752 - loglik: -1.5956e+02 - logprior: -9.9195e+00
Fitted a model with MAP estimate = -162.8803
expansions: []
discards: [ 0 91]
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 189.4032 - loglik: -1.5859e+02 - logprior: -3.0813e+01
Epoch 2/2
10/10 - 1s - loss: 168.7176 - loglik: -1.5659e+02 - logprior: -1.2125e+01
Fitted a model with MAP estimate = -164.8298
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 185.3818 - loglik: -1.5642e+02 - logprior: -2.8966e+01
Epoch 2/10
10/10 - 1s - loss: 162.7922 - loglik: -1.5464e+02 - logprior: -8.1473e+00
Epoch 3/10
10/10 - 1s - loss: 156.8063 - loglik: -1.5389e+02 - logprior: -2.9114e+00
Epoch 4/10
10/10 - 1s - loss: 154.3313 - loglik: -1.5313e+02 - logprior: -1.2037e+00
Epoch 5/10
10/10 - 1s - loss: 153.6469 - loglik: -1.5318e+02 - logprior: -4.7075e-01
Epoch 6/10
10/10 - 1s - loss: 153.1576 - loglik: -1.5301e+02 - logprior: -1.4497e-01
Epoch 7/10
10/10 - 1s - loss: 152.4077 - loglik: -1.5246e+02 - logprior: 0.0487
Epoch 8/10
10/10 - 1s - loss: 152.3320 - loglik: -1.5256e+02 - logprior: 0.2234
Epoch 9/10
10/10 - 1s - loss: 152.1600 - loglik: -1.5255e+02 - logprior: 0.3875
Epoch 10/10
10/10 - 1s - loss: 152.6969 - loglik: -1.5322e+02 - logprior: 0.5218
Fitted a model with MAP estimate = -152.0739
Time for alignment: 36.2846
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 303.8007 - loglik: -2.7673e+02 - logprior: -2.7071e+01
Epoch 2/10
10/10 - 1s - loss: 247.0720 - loglik: -2.4019e+02 - logprior: -6.8811e+00
Epoch 3/10
10/10 - 1s - loss: 209.0968 - loglik: -2.0551e+02 - logprior: -3.5871e+00
Epoch 4/10
10/10 - 1s - loss: 188.1503 - loglik: -1.8558e+02 - logprior: -2.5700e+00
Epoch 5/10
10/10 - 1s - loss: 180.7919 - loglik: -1.7858e+02 - logprior: -2.2089e+00
Epoch 6/10
10/10 - 1s - loss: 178.1578 - loglik: -1.7612e+02 - logprior: -2.0342e+00
Epoch 7/10
10/10 - 1s - loss: 177.1874 - loglik: -1.7543e+02 - logprior: -1.7614e+00
Epoch 8/10
10/10 - 1s - loss: 176.5341 - loglik: -1.7498e+02 - logprior: -1.5587e+00
Epoch 9/10
10/10 - 1s - loss: 175.7929 - loglik: -1.7428e+02 - logprior: -1.5135e+00
Epoch 10/10
10/10 - 1s - loss: 175.6610 - loglik: -1.7415e+02 - logprior: -1.5125e+00
Fitted a model with MAP estimate = -175.5540
expansions: [(0, 3), (11, 1), (12, 1), (34, 1), (36, 5), (50, 2), (58, 1), (69, 1), (70, 1), (72, 2), (73, 3)]
discards: []
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 205.7568 - loglik: -1.7151e+02 - logprior: -3.4242e+01
Epoch 2/2
10/10 - 1s - loss: 171.1085 - loglik: -1.6106e+02 - logprior: -1.0049e+01
Fitted a model with MAP estimate = -164.7832
expansions: []
discards: [ 0 89]
Fitting a model of length 102 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 191.3924 - loglik: -1.6055e+02 - logprior: -3.0843e+01
Epoch 2/2
10/10 - 1s - loss: 170.5509 - loglik: -1.5842e+02 - logprior: -1.2130e+01
Fitted a model with MAP estimate = -166.7400
expansions: [(3, 1), (61, 2)]
discards: [0]
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 187.0013 - loglik: -1.5802e+02 - logprior: -2.8986e+01
Epoch 2/10
10/10 - 1s - loss: 163.6861 - loglik: -1.5545e+02 - logprior: -8.2353e+00
Epoch 3/10
10/10 - 1s - loss: 157.2556 - loglik: -1.5434e+02 - logprior: -2.9149e+00
Epoch 4/10
10/10 - 1s - loss: 154.9875 - loglik: -1.5383e+02 - logprior: -1.1582e+00
Epoch 5/10
10/10 - 1s - loss: 153.7816 - loglik: -1.5337e+02 - logprior: -4.0741e-01
Epoch 6/10
10/10 - 1s - loss: 153.2132 - loglik: -1.5315e+02 - logprior: -6.5580e-02
Epoch 7/10
10/10 - 1s - loss: 152.6129 - loglik: -1.5269e+02 - logprior: 0.0767
Epoch 8/10
10/10 - 1s - loss: 153.2255 - loglik: -1.5347e+02 - logprior: 0.2437
Fitted a model with MAP estimate = -152.5792
Time for alignment: 37.3877
Computed alignments with likelihoods: ['-151.1440', '-152.0739', '-152.5792']
Best model has likelihood: -151.1440
SP score = 0.8380
Training of 3 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe2b2b67280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25dc2bd90>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.2299 - loglik: -1.4614e+02 - logprior: -5.0935e+00
Epoch 2/10
16/16 - 1s - loss: 126.9461 - loglik: -1.2537e+02 - logprior: -1.5740e+00
Epoch 3/10
16/16 - 1s - loss: 114.9897 - loglik: -1.1331e+02 - logprior: -1.6786e+00
Epoch 4/10
16/16 - 1s - loss: 110.8122 - loglik: -1.0911e+02 - logprior: -1.7069e+00
Epoch 5/10
16/16 - 1s - loss: 109.3502 - loglik: -1.0770e+02 - logprior: -1.6493e+00
Epoch 6/10
16/16 - 1s - loss: 108.9876 - loglik: -1.0739e+02 - logprior: -1.5966e+00
Epoch 7/10
16/16 - 1s - loss: 109.1163 - loglik: -1.0755e+02 - logprior: -1.5614e+00
Fitted a model with MAP estimate = -108.7644
expansions: [(3, 1), (6, 1), (12, 1), (15, 2), (17, 1), (23, 5), (24, 2), (33, 1)]
discards: [0]
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 113.4301 - loglik: -1.0718e+02 - logprior: -6.2475e+00
Epoch 2/2
16/16 - 1s - loss: 105.8482 - loglik: -1.0281e+02 - logprior: -3.0363e+00
Fitted a model with MAP estimate = -104.4465
expansions: [(0, 1)]
discards: [ 0 31 32]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 106.8551 - loglik: -1.0226e+02 - logprior: -4.5998e+00
Epoch 2/2
16/16 - 1s - loss: 102.6157 - loglik: -1.0099e+02 - logprior: -1.6213e+00
Fitted a model with MAP estimate = -102.4519
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 5s - loss: 109.1475 - loglik: -1.0295e+02 - logprior: -6.2024e+00
Epoch 2/10
16/16 - 1s - loss: 104.6583 - loglik: -1.0191e+02 - logprior: -2.7521e+00
Epoch 3/10
16/16 - 1s - loss: 103.0458 - loglik: -1.0149e+02 - logprior: -1.5534e+00
Epoch 4/10
16/16 - 1s - loss: 102.3022 - loglik: -1.0100e+02 - logprior: -1.3032e+00
Epoch 5/10
16/16 - 1s - loss: 101.9228 - loglik: -1.0064e+02 - logprior: -1.2813e+00
Epoch 6/10
16/16 - 1s - loss: 101.4716 - loglik: -1.0020e+02 - logprior: -1.2763e+00
Epoch 7/10
16/16 - 1s - loss: 101.5093 - loglik: -1.0026e+02 - logprior: -1.2500e+00
Fitted a model with MAP estimate = -101.2751
Time for alignment: 43.1652
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 151.1967 - loglik: -1.4610e+02 - logprior: -5.0918e+00
Epoch 2/10
16/16 - 1s - loss: 126.5917 - loglik: -1.2502e+02 - logprior: -1.5699e+00
Epoch 3/10
16/16 - 1s - loss: 117.2159 - loglik: -1.1553e+02 - logprior: -1.6851e+00
Epoch 4/10
16/16 - 1s - loss: 112.5892 - loglik: -1.1089e+02 - logprior: -1.6971e+00
Epoch 5/10
16/16 - 1s - loss: 110.8714 - loglik: -1.0923e+02 - logprior: -1.6392e+00
Epoch 6/10
16/16 - 1s - loss: 110.3999 - loglik: -1.0880e+02 - logprior: -1.5966e+00
Epoch 7/10
16/16 - 1s - loss: 109.3111 - loglik: -1.0776e+02 - logprior: -1.5555e+00
Epoch 8/10
16/16 - 1s - loss: 109.1041 - loglik: -1.0757e+02 - logprior: -1.5351e+00
Epoch 9/10
16/16 - 1s - loss: 109.1946 - loglik: -1.0768e+02 - logprior: -1.5187e+00
Fitted a model with MAP estimate = -109.0636
expansions: [(3, 1), (6, 1), (13, 1), (15, 2), (16, 1), (23, 6), (25, 2)]
discards: [0]
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 113.8748 - loglik: -1.0763e+02 - logprior: -6.2440e+00
Epoch 2/2
16/16 - 1s - loss: 105.5346 - loglik: -1.0246e+02 - logprior: -3.0727e+00
Fitted a model with MAP estimate = -104.4801
expansions: [(0, 1)]
discards: [ 0 31 36]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 106.7084 - loglik: -1.0212e+02 - logprior: -4.5923e+00
Epoch 2/2
16/16 - 1s - loss: 103.0931 - loglik: -1.0148e+02 - logprior: -1.6100e+00
Fitted a model with MAP estimate = -102.4303
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 108.9926 - loglik: -1.0280e+02 - logprior: -6.1927e+00
Epoch 2/10
16/16 - 1s - loss: 104.8563 - loglik: -1.0206e+02 - logprior: -2.7991e+00
Epoch 3/10
16/16 - 1s - loss: 102.8435 - loglik: -1.0127e+02 - logprior: -1.5714e+00
Epoch 4/10
16/16 - 1s - loss: 102.2285 - loglik: -1.0094e+02 - logprior: -1.2901e+00
Epoch 5/10
16/16 - 1s - loss: 102.0925 - loglik: -1.0083e+02 - logprior: -1.2661e+00
Epoch 6/10
16/16 - 1s - loss: 101.5059 - loglik: -1.0026e+02 - logprior: -1.2479e+00
Epoch 7/10
16/16 - 1s - loss: 100.9824 - loglik: -9.9751e+01 - logprior: -1.2314e+00
Epoch 8/10
16/16 - 1s - loss: 101.5772 - loglik: -1.0037e+02 - logprior: -1.2108e+00
Fitted a model with MAP estimate = -101.1360
Time for alignment: 48.2112
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 150.8940 - loglik: -1.4580e+02 - logprior: -5.0926e+00
Epoch 2/10
16/16 - 1s - loss: 127.1343 - loglik: -1.2557e+02 - logprior: -1.5638e+00
Epoch 3/10
16/16 - 1s - loss: 115.9231 - loglik: -1.1427e+02 - logprior: -1.6555e+00
Epoch 4/10
16/16 - 1s - loss: 111.1589 - loglik: -1.0948e+02 - logprior: -1.6837e+00
Epoch 5/10
16/16 - 1s - loss: 109.8213 - loglik: -1.0819e+02 - logprior: -1.6341e+00
Epoch 6/10
16/16 - 1s - loss: 109.2868 - loglik: -1.0770e+02 - logprior: -1.5841e+00
Epoch 7/10
16/16 - 1s - loss: 109.1502 - loglik: -1.0760e+02 - logprior: -1.5491e+00
Epoch 8/10
16/16 - 1s - loss: 108.8211 - loglik: -1.0729e+02 - logprior: -1.5338e+00
Epoch 9/10
16/16 - 1s - loss: 108.8889 - loglik: -1.0737e+02 - logprior: -1.5229e+00
Fitted a model with MAP estimate = -108.8443
expansions: [(3, 1), (6, 1), (13, 1), (15, 2), (16, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 113.6272 - loglik: -1.0737e+02 - logprior: -6.2618e+00
Epoch 2/2
16/16 - 1s - loss: 106.0409 - loglik: -1.0293e+02 - logprior: -3.1122e+00
Fitted a model with MAP estimate = -104.5673
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 106.5693 - loglik: -1.0196e+02 - logprior: -4.6078e+00
Epoch 2/2
16/16 - 1s - loss: 103.0143 - loglik: -1.0138e+02 - logprior: -1.6327e+00
Fitted a model with MAP estimate = -102.4086
expansions: [(3, 1)]
discards: [0]
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 108.9928 - loglik: -1.0279e+02 - logprior: -6.1990e+00
Epoch 2/10
16/16 - 1s - loss: 104.9377 - loglik: -1.0216e+02 - logprior: -2.7760e+00
Epoch 3/10
16/16 - 1s - loss: 102.6770 - loglik: -1.0111e+02 - logprior: -1.5625e+00
Epoch 4/10
16/16 - 1s - loss: 102.4800 - loglik: -1.0118e+02 - logprior: -1.3004e+00
Epoch 5/10
16/16 - 1s - loss: 101.9369 - loglik: -1.0065e+02 - logprior: -1.2869e+00
Epoch 6/10
16/16 - 1s - loss: 101.4661 - loglik: -1.0020e+02 - logprior: -1.2695e+00
Epoch 7/10
16/16 - 1s - loss: 101.4218 - loglik: -1.0017e+02 - logprior: -1.2492e+00
Epoch 8/10
16/16 - 1s - loss: 101.2206 - loglik: -9.9994e+01 - logprior: -1.2271e+00
Epoch 9/10
16/16 - 1s - loss: 101.3587 - loglik: -1.0015e+02 - logprior: -1.2101e+00
Fitted a model with MAP estimate = -101.1155
Time for alignment: 48.8117
Computed alignments with likelihoods: ['-101.2751', '-101.1360', '-101.1155']
Best model has likelihood: -101.1155
SP score = 0.8918
Training of 3 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1afab3670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe25cb5fbb0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 13s - loss: 609.0101 - loglik: -6.0624e+02 - logprior: -2.7680e+00
Epoch 2/10
19/19 - 7s - loss: 567.2595 - loglik: -5.6658e+02 - logprior: -6.8075e-01
Epoch 3/10
19/19 - 8s - loss: 543.5236 - loglik: -5.4253e+02 - logprior: -9.9129e-01
Epoch 4/10
19/19 - 7s - loss: 536.3643 - loglik: -5.3542e+02 - logprior: -9.4262e-01
Epoch 5/10
19/19 - 7s - loss: 532.2571 - loglik: -5.3129e+02 - logprior: -9.6951e-01
Epoch 6/10
19/19 - 8s - loss: 530.3131 - loglik: -5.2935e+02 - logprior: -9.6769e-01
Epoch 7/10
19/19 - 8s - loss: 528.8103 - loglik: -5.2786e+02 - logprior: -9.5042e-01
Epoch 8/10
19/19 - 8s - loss: 528.9473 - loglik: -5.2799e+02 - logprior: -9.5526e-01
Fitted a model with MAP estimate = -491.5847
expansions: [(6, 1), (7, 1), (11, 1), (13, 2), (21, 2), (24, 6), (25, 1), (55, 2), (71, 2), (78, 4), (81, 1), (83, 1), (122, 1), (124, 2), (125, 3), (126, 2), (128, 2), (132, 1), (146, 4)]
discards: [  0 134 135]
Fitting a model of length 191 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 539.8931 - loglik: -5.3624e+02 - logprior: -3.6556e+00
Epoch 2/2
19/19 - 10s - loss: 528.2168 - loglik: -5.2641e+02 - logprior: -1.8048e+00
Fitted a model with MAP estimate = -481.9025
expansions: [(0, 2), (39, 1), (94, 2), (96, 2)]
discards: [  0  15  26  27  70 160 168 169 170 171 181]
Fitting a model of length 187 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 531.0988 - loglik: -5.2834e+02 - logprior: -2.7589e+00
Epoch 2/2
19/19 - 10s - loss: 526.5959 - loglik: -5.2571e+02 - logprior: -8.8625e-01
Fitted a model with MAP estimate = -481.0625
expansions: [(39, 1), (95, 3)]
discards: [ 92 150 158]
Fitting a model of length 188 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 17s - loss: 475.8108 - loglik: -4.7426e+02 - logprior: -1.5494e+00
Epoch 2/10
25/25 - 13s - loss: 469.6184 - loglik: -4.6886e+02 - logprior: -7.5507e-01
Epoch 3/10
25/25 - 13s - loss: 467.9857 - loglik: -4.6727e+02 - logprior: -7.1289e-01
Epoch 4/10
25/25 - 13s - loss: 468.8041 - loglik: -4.6812e+02 - logprior: -6.8516e-01
Fitted a model with MAP estimate = -465.9360
Time for alignment: 229.9162
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 608.9247 - loglik: -6.0616e+02 - logprior: -2.7638e+00
Epoch 2/10
19/19 - 7s - loss: 568.4941 - loglik: -5.6786e+02 - logprior: -6.3583e-01
Epoch 3/10
19/19 - 8s - loss: 543.0479 - loglik: -5.4211e+02 - logprior: -9.4045e-01
Epoch 4/10
19/19 - 8s - loss: 535.8932 - loglik: -5.3490e+02 - logprior: -9.9059e-01
Epoch 5/10
19/19 - 8s - loss: 533.4014 - loglik: -5.3243e+02 - logprior: -9.7325e-01
Epoch 6/10
19/19 - 8s - loss: 531.2751 - loglik: -5.3030e+02 - logprior: -9.7451e-01
Epoch 7/10
19/19 - 8s - loss: 531.0807 - loglik: -5.3009e+02 - logprior: -9.8981e-01
Epoch 8/10
19/19 - 8s - loss: 529.6572 - loglik: -5.2866e+02 - logprior: -9.9720e-01
Epoch 9/10
19/19 - 8s - loss: 529.0289 - loglik: -5.2803e+02 - logprior: -9.9848e-01
Epoch 10/10
19/19 - 8s - loss: 530.0970 - loglik: -5.2910e+02 - logprior: -9.9604e-01
Fitted a model with MAP estimate = -492.1662
expansions: [(4, 1), (5, 1), (10, 2), (11, 1), (20, 3), (24, 1), (25, 1), (55, 2), (67, 2), (70, 1), (78, 4), (79, 1), (91, 1), (110, 2), (122, 3), (125, 4), (146, 6)]
discards: [0]
Fitting a model of length 190 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 540.9684 - loglik: -5.3728e+02 - logprior: -3.6871e+00
Epoch 2/2
19/19 - 10s - loss: 527.3201 - loglik: -5.2550e+02 - logprior: -1.8198e+00
Fitted a model with MAP estimate = -480.2364
expansions: [(0, 2), (35, 1), (36, 1), (134, 1), (154, 1)]
discards: [  0  13  66  79 162 163 164 165 166 167 178 179 180]
Fitting a model of length 183 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 528.8513 - loglik: -5.2613e+02 - logprior: -2.7200e+00
Epoch 2/2
19/19 - 10s - loss: 523.6447 - loglik: -5.2279e+02 - logprior: -8.5918e-01
Fitted a model with MAP estimate = -478.1527
expansions: [(26, 1), (149, 4)]
discards: [163]
Fitting a model of length 187 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 15s - loss: 474.1552 - loglik: -4.7266e+02 - logprior: -1.4981e+00
Epoch 2/10
25/25 - 13s - loss: 469.3064 - loglik: -4.6856e+02 - logprior: -7.4993e-01
Epoch 3/10
25/25 - 13s - loss: 468.0150 - loglik: -4.6731e+02 - logprior: -7.0805e-01
Epoch 4/10
25/25 - 13s - loss: 467.1011 - loglik: -4.6642e+02 - logprior: -6.7666e-01
Epoch 5/10
25/25 - 12s - loss: 465.8761 - loglik: -4.6521e+02 - logprior: -6.6825e-01
Epoch 6/10
25/25 - 13s - loss: 464.4517 - loglik: -4.6380e+02 - logprior: -6.4812e-01
Epoch 7/10
25/25 - 13s - loss: 464.4580 - loglik: -4.6381e+02 - logprior: -6.4337e-01
Fitted a model with MAP estimate = -463.3741
Time for alignment: 278.1217
Fitting a model of length 155 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 609.0295 - loglik: -6.0628e+02 - logprior: -2.7544e+00
Epoch 2/10
19/19 - 7s - loss: 567.0952 - loglik: -5.6644e+02 - logprior: -6.5696e-01
Epoch 3/10
19/19 - 8s - loss: 542.4190 - loglik: -5.4141e+02 - logprior: -1.0117e+00
Epoch 4/10
19/19 - 7s - loss: 536.3019 - loglik: -5.3525e+02 - logprior: -1.0494e+00
Epoch 5/10
19/19 - 8s - loss: 533.0383 - loglik: -5.3200e+02 - logprior: -1.0388e+00
Epoch 6/10
19/19 - 8s - loss: 531.8397 - loglik: -5.3081e+02 - logprior: -1.0273e+00
Epoch 7/10
19/19 - 8s - loss: 531.0124 - loglik: -5.2999e+02 - logprior: -1.0201e+00
Epoch 8/10
19/19 - 8s - loss: 531.2260 - loglik: -5.3022e+02 - logprior: -1.0011e+00
Fitted a model with MAP estimate = -492.8473
expansions: [(6, 1), (7, 1), (11, 1), (13, 2), (21, 2), (24, 4), (25, 1), (55, 2), (56, 1), (70, 1), (81, 2), (93, 2), (106, 1), (109, 1), (121, 2), (123, 2), (124, 4), (139, 5), (140, 1)]
discards: [0]
Fitting a model of length 190 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 14s - loss: 540.6489 - loglik: -5.3694e+02 - logprior: -3.7114e+00
Epoch 2/2
19/19 - 10s - loss: 529.2323 - loglik: -5.2739e+02 - logprior: -1.8467e+00
Fitted a model with MAP estimate = -482.6669
expansions: [(0, 2), (93, 5), (149, 1), (172, 2)]
discards: [  0  15  30  31  68 146 147]
Fitting a model of length 193 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 13s - loss: 529.4180 - loglik: -5.2651e+02 - logprior: -2.9040e+00
Epoch 2/2
19/19 - 10s - loss: 524.5446 - loglik: -5.2350e+02 - logprior: -1.0461e+00
Fitted a model with MAP estimate = -480.4408
expansions: [(35, 1), (36, 1), (97, 1), (175, 1)]
discards: [0]
Fitting a model of length 196 on 17200 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 16s - loss: 475.4943 - loglik: -4.7344e+02 - logprior: -2.0561e+00
Epoch 2/10
25/25 - 13s - loss: 469.5054 - loglik: -4.6873e+02 - logprior: -7.7443e-01
Epoch 3/10
25/25 - 13s - loss: 468.2031 - loglik: -4.6758e+02 - logprior: -6.2523e-01
Epoch 4/10
25/25 - 13s - loss: 466.3714 - loglik: -4.6574e+02 - logprior: -6.3487e-01
Epoch 5/10
25/25 - 14s - loss: 464.0298 - loglik: -4.6341e+02 - logprior: -6.2034e-01
Epoch 6/10
25/25 - 14s - loss: 464.9934 - loglik: -4.6439e+02 - logprior: -6.0689e-01
Fitted a model with MAP estimate = -463.0917
Time for alignment: 258.9052
Computed alignments with likelihoods: ['-465.9360', '-463.3741', '-463.0917']
Best model has likelihood: -463.0917
SP score = 0.6447
Training of 3 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe040497e50>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe0a0056370>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe0a0056d60>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b4304790>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304130>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304a00>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe1b43046a0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304b20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304850>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43045b0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304fd0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304220>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304490>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43049a0>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43041c0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b43048b0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304040>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe1b4304a30> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe1b4304670>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b4304c10> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe0403ef820> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe141803850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe1b00a13d0>] , frozen_insertions : True , surgery_del : 0.5
 , surgery_ins : 0.5 , trainable_exchangeabilities : False , background_distribution : [0.079066 0.055941 0.041977 0.053052 0.012937 0.040767 0.071586 0.057337
 0.022355 0.062157 0.099081 0.0646   0.022951 0.042302 0.04404  0.061197
 0.053287 0.012066 0.034155 0.069147 0.       0.       0.       0.
 0.      ]

Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 491.2194 - loglik: -4.8528e+02 - logprior: -5.9372e+00
Epoch 2/10
14/14 - 4s - loss: 436.7873 - loglik: -4.3556e+02 - logprior: -1.2316e+00
Epoch 3/10
14/14 - 4s - loss: 401.6800 - loglik: -4.0038e+02 - logprior: -1.2974e+00
Epoch 4/10
14/14 - 3s - loss: 390.0619 - loglik: -3.8881e+02 - logprior: -1.2546e+00
Epoch 5/10
14/14 - 4s - loss: 384.9616 - loglik: -3.8371e+02 - logprior: -1.2496e+00
Epoch 6/10
14/14 - 3s - loss: 382.1736 - loglik: -3.8091e+02 - logprior: -1.2673e+00
Epoch 7/10
14/14 - 4s - loss: 382.0200 - loglik: -3.8078e+02 - logprior: -1.2413e+00
Epoch 8/10
14/14 - 3s - loss: 381.0859 - loglik: -3.7984e+02 - logprior: -1.2477e+00
Epoch 9/10
14/14 - 3s - loss: 379.8773 - loglik: -3.7864e+02 - logprior: -1.2364e+00
Epoch 10/10
14/14 - 4s - loss: 380.7905 - loglik: -3.7955e+02 - logprior: -1.2449e+00
Fitted a model with MAP estimate = -380.1326
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (66, 1), (69, 1), (71, 4), (89, 1), (102, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 3), (139, 1)]
discards: []
Fitting a model of length 182 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 391.0520 - loglik: -3.8387e+02 - logprior: -7.1834e+00
Epoch 2/2
14/14 - 5s - loss: 370.8767 - loglik: -3.6923e+02 - logprior: -1.6435e+00
Fitted a model with MAP estimate = -367.5617
expansions: [(127, 1), (128, 1), (130, 1)]
discards: [ 41  91  92 148 166]
Fitting a model of length 180 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 374.4153 - loglik: -3.6922e+02 - logprior: -5.1980e+00
Epoch 2/2
14/14 - 5s - loss: 367.6235 - loglik: -3.6643e+02 - logprior: -1.1936e+00
Fitted a model with MAP estimate = -366.1967
expansions: [(90, 2)]
discards: []
Fitting a model of length 182 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 9s - loss: 372.1306 - loglik: -3.6700e+02 - logprior: -5.1301e+00
Epoch 2/10
14/14 - 5s - loss: 366.6759 - loglik: -3.6549e+02 - logprior: -1.1908e+00
Epoch 3/10
14/14 - 5s - loss: 363.6135 - loglik: -3.6293e+02 - logprior: -6.8834e-01
Epoch 4/10
14/14 - 5s - loss: 363.2505 - loglik: -3.6269e+02 - logprior: -5.6383e-01
Epoch 5/10
14/14 - 5s - loss: 361.6895 - loglik: -3.6118e+02 - logprior: -5.1429e-01
Epoch 6/10
14/14 - 5s - loss: 360.9200 - loglik: -3.6043e+02 - logprior: -4.9087e-01
Epoch 7/10
14/14 - 5s - loss: 359.0506 - loglik: -3.5857e+02 - logprior: -4.7603e-01
Epoch 8/10
14/14 - 5s - loss: 359.8370 - loglik: -3.5939e+02 - logprior: -4.5043e-01
Fitted a model with MAP estimate = -358.8158
Time for alignment: 131.6378
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 490.9533 - loglik: -4.8501e+02 - logprior: -5.9429e+00
Epoch 2/10
14/14 - 3s - loss: 437.0520 - loglik: -4.3580e+02 - logprior: -1.2550e+00
Epoch 3/10
14/14 - 4s - loss: 404.9915 - loglik: -4.0371e+02 - logprior: -1.2809e+00
Epoch 4/10
14/14 - 4s - loss: 393.7497 - loglik: -3.9260e+02 - logprior: -1.1529e+00
Epoch 5/10
14/14 - 3s - loss: 387.1336 - loglik: -3.8603e+02 - logprior: -1.0988e+00
Epoch 6/10
14/14 - 3s - loss: 385.3576 - loglik: -3.8428e+02 - logprior: -1.0745e+00
Epoch 7/10
14/14 - 4s - loss: 384.0284 - loglik: -3.8297e+02 - logprior: -1.0616e+00
Epoch 8/10
14/14 - 3s - loss: 383.5909 - loglik: -3.8252e+02 - logprior: -1.0745e+00
Epoch 9/10
14/14 - 4s - loss: 383.4268 - loglik: -3.8234e+02 - logprior: -1.0880e+00
Epoch 10/10
14/14 - 3s - loss: 382.7171 - loglik: -3.8163e+02 - logprior: -1.0858e+00
Fitted a model with MAP estimate = -382.7532
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (34, 1), (52, 2), (53, 1), (56, 1), (61, 1), (65, 1), (70, 4), (74, 1), (96, 1), (103, 4), (117, 1), (120, 2), (128, 1), (129, 2), (130, 2), (131, 1), (133, 2), (139, 1)]
discards: []
Fitting a model of length 186 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 391.5303 - loglik: -3.8430e+02 - logprior: -7.2277e+00
Epoch 2/2
14/14 - 5s - loss: 370.4374 - loglik: -3.6873e+02 - logprior: -1.7039e+00
Fitted a model with MAP estimate = -367.0660
expansions: [(129, 1)]
discards: [149 162 170]
Fitting a model of length 184 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 372.7319 - loglik: -3.6738e+02 - logprior: -5.3532e+00
Epoch 2/2
14/14 - 5s - loss: 365.5050 - loglik: -3.6416e+02 - logprior: -1.3418e+00
Fitted a model with MAP estimate = -364.0913
expansions: [(127, 1)]
discards: [ 0 41]
Fitting a model of length 183 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 374.4045 - loglik: -3.6739e+02 - logprior: -7.0103e+00
Epoch 2/10
14/14 - 5s - loss: 367.0012 - loglik: -3.6421e+02 - logprior: -2.7892e+00
Epoch 3/10
14/14 - 5s - loss: 365.6638 - loglik: -3.6402e+02 - logprior: -1.6425e+00
Epoch 4/10
14/14 - 5s - loss: 362.0434 - loglik: -3.6146e+02 - logprior: -5.8091e-01
Epoch 5/10
14/14 - 5s - loss: 360.5696 - loglik: -3.6012e+02 - logprior: -4.5130e-01
Epoch 6/10
14/14 - 5s - loss: 360.4802 - loglik: -3.6003e+02 - logprior: -4.5315e-01
Epoch 7/10
14/14 - 5s - loss: 358.8760 - loglik: -3.5844e+02 - logprior: -4.3244e-01
Epoch 8/10
14/14 - 5s - loss: 358.5340 - loglik: -3.5813e+02 - logprior: -4.0171e-01
Epoch 9/10
14/14 - 5s - loss: 358.9952 - loglik: -3.5861e+02 - logprior: -3.8095e-01
Fitted a model with MAP estimate = -358.1044
Time for alignment: 134.4724
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 491.8606 - loglik: -4.8592e+02 - logprior: -5.9376e+00
Epoch 2/10
14/14 - 4s - loss: 435.6947 - loglik: -4.3447e+02 - logprior: -1.2292e+00
Epoch 3/10
14/14 - 3s - loss: 400.8945 - loglik: -3.9967e+02 - logprior: -1.2211e+00
Epoch 4/10
14/14 - 3s - loss: 390.6834 - loglik: -3.8961e+02 - logprior: -1.0755e+00
Epoch 5/10
14/14 - 3s - loss: 385.5425 - loglik: -3.8451e+02 - logprior: -1.0368e+00
Epoch 6/10
14/14 - 4s - loss: 382.7834 - loglik: -3.8172e+02 - logprior: -1.0646e+00
Epoch 7/10
14/14 - 3s - loss: 381.5000 - loglik: -3.8043e+02 - logprior: -1.0657e+00
Epoch 8/10
14/14 - 3s - loss: 381.9117 - loglik: -3.8085e+02 - logprior: -1.0621e+00
Fitted a model with MAP estimate = -381.2514
expansions: [(0, 3), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (53, 2), (54, 1), (57, 1), (58, 1), (61, 1), (69, 1), (70, 4), (96, 1), (103, 4), (117, 1), (120, 2), (128, 1), (131, 1), (133, 3), (140, 1)]
discards: []
Fitting a model of length 182 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 9s - loss: 390.0014 - loglik: -3.8287e+02 - logprior: -7.1296e+00
Epoch 2/2
14/14 - 5s - loss: 371.1596 - loglik: -3.6954e+02 - logprior: -1.6235e+00
Fitted a model with MAP estimate = -368.1511
expansions: [(128, 1)]
discards: [148 166]
Fitting a model of length 181 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 373.9437 - loglik: -3.6862e+02 - logprior: -5.3248e+00
Epoch 2/2
14/14 - 5s - loss: 366.9700 - loglik: -3.6563e+02 - logprior: -1.3358e+00
Fitted a model with MAP estimate = -365.9041
expansions: [(126, 1)]
discards: [ 0 41]
Fitting a model of length 180 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 376.0117 - loglik: -3.6902e+02 - logprior: -6.9902e+00
Epoch 2/10
14/14 - 5s - loss: 369.1894 - loglik: -3.6647e+02 - logprior: -2.7153e+00
Epoch 3/10
14/14 - 5s - loss: 366.7470 - loglik: -3.6514e+02 - logprior: -1.6058e+00
Epoch 4/10
14/14 - 5s - loss: 364.4859 - loglik: -3.6396e+02 - logprior: -5.2675e-01
Epoch 5/10
14/14 - 5s - loss: 362.2378 - loglik: -3.6183e+02 - logprior: -4.0504e-01
Epoch 6/10
14/14 - 5s - loss: 362.6090 - loglik: -3.6221e+02 - logprior: -4.0079e-01
Fitted a model with MAP estimate = -361.1265
Time for alignment: 112.1204
Computed alignments with likelihoods: ['-358.8158', '-358.1044', '-361.1265']
Best model has likelihood: -358.1044
SP score = 0.8124
