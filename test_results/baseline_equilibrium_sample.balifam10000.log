Training of 5 independent models on file PF00079.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 1634.5366 - loglik: -1.6330e+03 - logprior: -1.5383e+00
Epoch 2/10
39/39 - 31s - loss: 1510.2363 - loglik: -1.5083e+03 - logprior: -1.9586e+00
Epoch 3/10
39/39 - 32s - loss: 1498.3813 - loglik: -1.4965e+03 - logprior: -1.9123e+00
Epoch 4/10
39/39 - 32s - loss: 1497.8334 - loglik: -1.4960e+03 - logprior: -1.8407e+00
Epoch 5/10
39/39 - 33s - loss: 1494.2631 - loglik: -1.4924e+03 - logprior: -1.8486e+00
Epoch 6/10
39/39 - 34s - loss: 1494.3529 - loglik: -1.4925e+03 - logprior: -1.8686e+00
Fitted a model with MAP estimate = -1493.7199
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (16, 1), (45, 1), (51, 5), (60, 1), (61, 1), (63, 1), (65, 1), (67, 1), (68, 1), (77, 1), (78, 1), (79, 1), (83, 1), (84, 1), (85, 1), (87, 1), (89, 1), (90, 1), (93, 1), (96, 1), (98, 1), (101, 1), (111, 1), (116, 1), (117, 1), (118, 1), (119, 1), (133, 1), (138, 1), (141, 1), (157, 1), (158, 1), (162, 1), (164, 2), (165, 1), (171, 1), (174, 1), (183, 1), (187, 1), (190, 1), (192, 1), (193, 1), (194, 1), (196, 1), (197, 1), (198, 1), (213, 1), (216, 1), (217, 1), (219, 1), (228, 1), (236, 2), (238, 1), (246, 1), (262, 1), (263, 1), (264, 1), (265, 2), (267, 2), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 55s - loss: 1484.9648 - loglik: -1.4834e+03 - logprior: -1.5549e+00
Epoch 2/2
39/39 - 55s - loss: 1473.5416 - loglik: -1.4732e+03 - logprior: -3.7534e-01
Fitted a model with MAP estimate = -1470.7349
expansions: [(203, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 1475.7834 - loglik: -1.4745e+03 - logprior: -1.2833e+00
Epoch 2/2
39/39 - 61s - loss: 1472.7609 - loglik: -1.4727e+03 - logprior: -9.9278e-02
Fitted a model with MAP estimate = -1469.6362
expansions: []
discards: [336]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 1475.3909 - loglik: -1.4744e+03 - logprior: -1.0035e+00
Epoch 2/10
39/39 - 65s - loss: 1471.6979 - loglik: -1.4718e+03 - logprior: 0.0878
Epoch 3/10
39/39 - 64s - loss: 1469.3717 - loglik: -1.4696e+03 - logprior: 0.2488
Epoch 4/10
39/39 - 61s - loss: 1466.8969 - loglik: -1.4672e+03 - logprior: 0.2797
Epoch 5/10
39/39 - 61s - loss: 1465.6637 - loglik: -1.4661e+03 - logprior: 0.4240
Epoch 6/10
39/39 - 61s - loss: 1465.4960 - loglik: -1.4661e+03 - logprior: 0.5805
Epoch 7/10
39/39 - 60s - loss: 1464.5732 - loglik: -1.4653e+03 - logprior: 0.7009
Epoch 8/10
39/39 - 59s - loss: 1464.5317 - loglik: -1.4653e+03 - logprior: 0.7572
Epoch 9/10
39/39 - 57s - loss: 1463.2052 - loglik: -1.4641e+03 - logprior: 0.8534
Epoch 10/10
39/39 - 57s - loss: 1465.2473 - loglik: -1.4663e+03 - logprior: 1.0333
Fitted a model with MAP estimate = -1463.6339
Time for alignment: 1268.2465
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 1634.3295 - loglik: -1.6328e+03 - logprior: -1.5172e+00
Epoch 2/10
39/39 - 42s - loss: 1511.0082 - loglik: -1.5092e+03 - logprior: -1.8197e+00
Epoch 3/10
39/39 - 43s - loss: 1501.3190 - loglik: -1.4995e+03 - logprior: -1.8134e+00
Epoch 4/10
39/39 - 44s - loss: 1497.8811 - loglik: -1.4961e+03 - logprior: -1.7679e+00
Epoch 5/10
39/39 - 46s - loss: 1495.8121 - loglik: -1.4940e+03 - logprior: -1.7747e+00
Epoch 6/10
39/39 - 48s - loss: 1496.8435 - loglik: -1.4951e+03 - logprior: -1.7783e+00
Fitted a model with MAP estimate = -1495.2782
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (16, 1), (52, 5), (55, 1), (61, 1), (64, 1), (66, 1), (68, 1), (76, 2), (77, 2), (78, 1), (81, 1), (83, 1), (84, 1), (86, 1), (90, 1), (91, 1), (98, 1), (99, 1), (100, 1), (113, 1), (118, 1), (119, 1), (120, 1), (121, 1), (135, 1), (140, 1), (142, 1), (143, 1), (149, 1), (158, 1), (159, 1), (160, 1), (162, 1), (164, 2), (165, 1), (184, 1), (185, 1), (188, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (214, 1), (217, 1), (218, 1), (220, 1), (229, 1), (235, 1), (236, 2), (238, 1), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (279, 2), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 1484.9880 - loglik: -1.4834e+03 - logprior: -1.5897e+00
Epoch 2/2
39/39 - 72s - loss: 1473.1017 - loglik: -1.4727e+03 - logprior: -4.2402e-01
Fitted a model with MAP estimate = -1469.7461
expansions: [(205, 1)]
discards: [ 93 251 337 338]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 1475.7422 - loglik: -1.4745e+03 - logprior: -1.2158e+00
Epoch 2/2
39/39 - 71s - loss: 1471.6957 - loglik: -1.4716e+03 - logprior: -4.6863e-02
Fitted a model with MAP estimate = -1469.2697
expansions: [(144, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 75s - loss: 1474.4717 - loglik: -1.4735e+03 - logprior: -9.8952e-01
Epoch 2/10
39/39 - 70s - loss: 1470.1912 - loglik: -1.4704e+03 - logprior: 0.1706
Epoch 3/10
39/39 - 59s - loss: 1468.7012 - loglik: -1.4690e+03 - logprior: 0.2703
Epoch 4/10
39/39 - 55s - loss: 1466.5453 - loglik: -1.4669e+03 - logprior: 0.3376
Epoch 5/10
39/39 - 54s - loss: 1464.7203 - loglik: -1.4652e+03 - logprior: 0.4681
Epoch 6/10
39/39 - 55s - loss: 1463.5864 - loglik: -1.4642e+03 - logprior: 0.5873
Epoch 7/10
39/39 - 59s - loss: 1463.3397 - loglik: -1.4641e+03 - logprior: 0.7272
Epoch 8/10
39/39 - 64s - loss: 1464.3107 - loglik: -1.4652e+03 - logprior: 0.8632
Fitted a model with MAP estimate = -1462.9814
Time for alignment: 1331.0917
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 46s - loss: 1635.7179 - loglik: -1.6342e+03 - logprior: -1.5239e+00
Epoch 2/10
39/39 - 44s - loss: 1510.1781 - loglik: -1.5083e+03 - logprior: -1.8969e+00
Epoch 3/10
39/39 - 47s - loss: 1499.1372 - loglik: -1.4973e+03 - logprior: -1.8776e+00
Epoch 4/10
39/39 - 48s - loss: 1496.3179 - loglik: -1.4945e+03 - logprior: -1.8226e+00
Epoch 5/10
39/39 - 47s - loss: 1493.8556 - loglik: -1.4920e+03 - logprior: -1.8344e+00
Epoch 6/10
39/39 - 47s - loss: 1495.0292 - loglik: -1.4932e+03 - logprior: -1.8439e+00
Fitted a model with MAP estimate = -1493.2484
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (16, 1), (45, 1), (48, 1), (50, 3), (61, 2), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (77, 1), (78, 1), (83, 1), (84, 1), (85, 1), (86, 1), (90, 1), (91, 1), (94, 1), (97, 1), (99, 1), (102, 1), (105, 1), (111, 1), (116, 1), (121, 1), (134, 1), (139, 1), (141, 1), (142, 1), (149, 1), (158, 1), (159, 1), (160, 1), (162, 1), (164, 2), (165, 1), (173, 1), (184, 1), (187, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (198, 2), (199, 1), (213, 1), (217, 1), (218, 1), (219, 1), (223, 1), (228, 1), (236, 2), (262, 1), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (277, 1), (279, 1), (280, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 1484.7394 - loglik: -1.4832e+03 - logprior: -1.5662e+00
Epoch 2/2
39/39 - 73s - loss: 1472.8751 - loglik: -1.4725e+03 - logprior: -3.9881e-01
Fitted a model with MAP estimate = -1469.8419
expansions: [(204, 1)]
discards: [ 73 250]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 58s - loss: 1475.9985 - loglik: -1.4747e+03 - logprior: -1.2504e+00
Epoch 2/2
39/39 - 55s - loss: 1471.2203 - loglik: -1.4711e+03 - logprior: -7.0844e-02
Fitted a model with MAP estimate = -1469.4155
expansions: []
discards: [335 336]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 1474.9506 - loglik: -1.4739e+03 - logprior: -1.0008e+00
Epoch 2/10
39/39 - 55s - loss: 1471.3263 - loglik: -1.4714e+03 - logprior: 0.1023
Epoch 3/10
39/39 - 55s - loss: 1469.6532 - loglik: -1.4699e+03 - logprior: 0.2721
Epoch 4/10
39/39 - 55s - loss: 1465.9863 - loglik: -1.4664e+03 - logprior: 0.4009
Epoch 5/10
39/39 - 55s - loss: 1465.6423 - loglik: -1.4661e+03 - logprior: 0.4915
Epoch 6/10
39/39 - 54s - loss: 1465.1792 - loglik: -1.4658e+03 - logprior: 0.5951
Epoch 7/10
39/39 - 54s - loss: 1462.8577 - loglik: -1.4636e+03 - logprior: 0.7255
Epoch 8/10
39/39 - 54s - loss: 1465.6671 - loglik: -1.4665e+03 - logprior: 0.7959
Fitted a model with MAP estimate = -1463.7037
Time for alignment: 1219.5789
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 1636.1953 - loglik: -1.6347e+03 - logprior: -1.4829e+00
Epoch 2/10
39/39 - 37s - loss: 1510.8999 - loglik: -1.5093e+03 - logprior: -1.6393e+00
Epoch 3/10
39/39 - 37s - loss: 1499.2983 - loglik: -1.4977e+03 - logprior: -1.5725e+00
Epoch 4/10
39/39 - 37s - loss: 1496.3695 - loglik: -1.4948e+03 - logprior: -1.5262e+00
Epoch 5/10
39/39 - 37s - loss: 1495.7446 - loglik: -1.4942e+03 - logprior: -1.5346e+00
Epoch 6/10
39/39 - 37s - loss: 1495.2589 - loglik: -1.4937e+03 - logprior: -1.5489e+00
Epoch 7/10
39/39 - 39s - loss: 1493.8708 - loglik: -1.4923e+03 - logprior: -1.5778e+00
Epoch 8/10
39/39 - 40s - loss: 1494.8805 - loglik: -1.4933e+03 - logprior: -1.5884e+00
Fitted a model with MAP estimate = -1493.7149
expansions: [(0, 2), (13, 1), (14, 1), (46, 1), (52, 5), (62, 1), (63, 1), (64, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (80, 1), (84, 1), (85, 1), (86, 1), (91, 1), (92, 1), (95, 1), (100, 1), (101, 1), (107, 1), (119, 1), (120, 1), (121, 1), (122, 1), (135, 1), (140, 1), (142, 1), (143, 1), (146, 1), (157, 1), (158, 1), (159, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 1), (172, 1), (175, 1), (184, 1), (188, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (214, 1), (218, 1), (220, 1), (224, 1), (229, 1), (235, 2), (236, 2), (238, 1), (263, 1), (264, 1), (265, 1), (266, 2), (267, 3), (268, 1), (277, 1), (278, 2), (279, 1), (280, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 370 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 1485.2280 - loglik: -1.4834e+03 - logprior: -1.8176e+00
Epoch 2/2
39/39 - 66s - loss: 1472.9458 - loglik: -1.4725e+03 - logprior: -4.7912e-01
Fitted a model with MAP estimate = -1469.2754
expansions: []
discards: [  0   1 252 298 339 340]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 1476.6731 - loglik: -1.4754e+03 - logprior: -1.2957e+00
Epoch 2/2
39/39 - 67s - loss: 1472.4065 - loglik: -1.4726e+03 - logprior: 0.1928
Fitted a model with MAP estimate = -1469.6485
expansions: [(0, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 367 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 1474.8998 - loglik: -1.4738e+03 - logprior: -1.0641e+00
Epoch 2/10
39/39 - 61s - loss: 1471.4333 - loglik: -1.4716e+03 - logprior: 0.1707
Epoch 3/10
39/39 - 59s - loss: 1468.9386 - loglik: -1.4693e+03 - logprior: 0.3183
Epoch 4/10
39/39 - 57s - loss: 1466.0745 - loglik: -1.4665e+03 - logprior: 0.4131
Epoch 5/10
39/39 - 56s - loss: 1464.0349 - loglik: -1.4646e+03 - logprior: 0.5667
Epoch 6/10
39/39 - 57s - loss: 1464.0610 - loglik: -1.4646e+03 - logprior: 0.5864
Fitted a model with MAP estimate = -1463.7694
Time for alignment: 1186.3633
Fitting a model of length 289 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 1633.5521 - loglik: -1.6320e+03 - logprior: -1.5379e+00
Epoch 2/10
39/39 - 38s - loss: 1508.8051 - loglik: -1.5070e+03 - logprior: -1.8397e+00
Epoch 3/10
39/39 - 38s - loss: 1499.1226 - loglik: -1.4973e+03 - logprior: -1.7984e+00
Epoch 4/10
39/39 - 39s - loss: 1496.8219 - loglik: -1.4951e+03 - logprior: -1.7465e+00
Epoch 5/10
39/39 - 39s - loss: 1494.0087 - loglik: -1.4923e+03 - logprior: -1.7497e+00
Epoch 6/10
39/39 - 39s - loss: 1495.1559 - loglik: -1.4934e+03 - logprior: -1.7733e+00
Fitted a model with MAP estimate = -1493.2916
expansions: [(13, 1), (14, 2), (15, 1), (46, 1), (52, 3), (56, 1), (62, 1), (63, 1), (64, 1), (66, 1), (68, 1), (69, 1), (78, 1), (79, 1), (82, 1), (84, 1), (85, 1), (86, 1), (90, 1), (91, 1), (95, 1), (98, 1), (99, 1), (100, 1), (106, 1), (118, 1), (120, 1), (122, 1), (135, 1), (141, 1), (144, 1), (147, 2), (149, 1), (159, 1), (160, 1), (161, 1), (163, 1), (164, 1), (166, 1), (167, 1), (173, 1), (184, 1), (185, 1), (189, 1), (192, 1), (194, 1), (195, 1), (196, 1), (198, 1), (199, 2), (200, 1), (201, 1), (214, 1), (217, 1), (218, 1), (219, 1), (227, 1), (228, 2), (235, 3), (263, 1), (264, 2), (265, 1), (266, 2), (267, 3), (268, 1), (277, 1), (278, 2), (279, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 370 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 1484.7738 - loglik: -1.4833e+03 - logprior: -1.5114e+00
Epoch 2/2
39/39 - 60s - loss: 1472.2157 - loglik: -1.4718e+03 - logprior: -4.3449e-01
Fitted a model with MAP estimate = -1469.1958
expansions: []
discards: [ 56 181 251 339 340]
Re-initialized the encoder parameters.
Fitting a model of length 365 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 1475.7896 - loglik: -1.4746e+03 - logprior: -1.2265e+00
Epoch 2/2
39/39 - 66s - loss: 1472.2146 - loglik: -1.4722e+03 - logprior: -4.0358e-02
Fitted a model with MAP estimate = -1469.3811
expansions: [(56, 2)]
discards: [287]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 10004 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 1475.3087 - loglik: -1.4743e+03 - logprior: -9.6985e-01
Epoch 2/10
39/39 - 66s - loss: 1470.3397 - loglik: -1.4705e+03 - logprior: 0.1581
Epoch 3/10
39/39 - 69s - loss: 1469.1927 - loglik: -1.4695e+03 - logprior: 0.2861
Epoch 4/10
39/39 - 70s - loss: 1465.4778 - loglik: -1.4658e+03 - logprior: 0.3616
Epoch 5/10
39/39 - 72s - loss: 1465.7045 - loglik: -1.4662e+03 - logprior: 0.4816
Fitted a model with MAP estimate = -1464.1832
Time for alignment: 1081.2707
Computed alignments with likelihoods: ['-1463.6339', '-1462.9814', '-1463.7037', '-1463.7694', '-1464.1832']
Best model has likelihood: -1462.9814  (prior= 0.9562 )
time for generating output: 0.3112
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00079.projection.fasta
SP score = 0.9400871459694989
Training of 5 independent models on file PF01381.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 302.2505 - loglik: -2.9905e+02 - logprior: -3.2025e+00
Epoch 2/10
19/19 - 1s - loss: 281.1505 - loglik: -2.7989e+02 - logprior: -1.2605e+00
Epoch 3/10
19/19 - 1s - loss: 273.0653 - loglik: -2.7170e+02 - logprior: -1.3679e+00
Epoch 4/10
19/19 - 1s - loss: 271.4961 - loglik: -2.7012e+02 - logprior: -1.3725e+00
Epoch 5/10
19/19 - 1s - loss: 271.0479 - loglik: -2.6971e+02 - logprior: -1.3425e+00
Epoch 6/10
19/19 - 1s - loss: 270.8684 - loglik: -2.6954e+02 - logprior: -1.3315e+00
Epoch 7/10
19/19 - 1s - loss: 270.4855 - loglik: -2.6917e+02 - logprior: -1.3157e+00
Epoch 8/10
19/19 - 1s - loss: 270.5303 - loglik: -2.6922e+02 - logprior: -1.3104e+00
Fitted a model with MAP estimate = -270.1303
expansions: [(6, 3), (19, 2), (20, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 270.8900 - loglik: -2.6774e+02 - logprior: -3.1549e+00
Epoch 2/2
19/19 - 1s - loss: 264.1246 - loglik: -2.6278e+02 - logprior: -1.3407e+00
Fitted a model with MAP estimate = -262.9037
expansions: [(7, 1)]
discards: [ 1 23 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.6745 - loglik: -2.6347e+02 - logprior: -3.1995e+00
Epoch 2/2
19/19 - 1s - loss: 263.2089 - loglik: -2.6192e+02 - logprior: -1.2927e+00
Fitted a model with MAP estimate = -262.4284
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 265.3148 - loglik: -2.6213e+02 - logprior: -3.1873e+00
Epoch 2/10
19/19 - 1s - loss: 262.7859 - loglik: -2.6149e+02 - logprior: -1.2942e+00
Epoch 3/10
19/19 - 1s - loss: 262.1821 - loglik: -2.6102e+02 - logprior: -1.1607e+00
Epoch 4/10
19/19 - 1s - loss: 261.8980 - loglik: -2.6078e+02 - logprior: -1.1160e+00
Epoch 5/10
19/19 - 1s - loss: 261.6591 - loglik: -2.6056e+02 - logprior: -1.0984e+00
Epoch 6/10
19/19 - 1s - loss: 261.3447 - loglik: -2.6027e+02 - logprior: -1.0771e+00
Epoch 7/10
19/19 - 1s - loss: 261.3027 - loglik: -2.6023e+02 - logprior: -1.0680e+00
Epoch 8/10
19/19 - 1s - loss: 261.1763 - loglik: -2.6012e+02 - logprior: -1.0551e+00
Epoch 9/10
19/19 - 1s - loss: 261.3074 - loglik: -2.6026e+02 - logprior: -1.0462e+00
Fitted a model with MAP estimate = -261.0800
Time for alignment: 38.3044
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 302.2500 - loglik: -2.9905e+02 - logprior: -3.2001e+00
Epoch 2/10
19/19 - 1s - loss: 280.8687 - loglik: -2.7965e+02 - logprior: -1.2188e+00
Epoch 3/10
19/19 - 1s - loss: 272.8678 - loglik: -2.7170e+02 - logprior: -1.1674e+00
Epoch 4/10
19/19 - 1s - loss: 270.7501 - loglik: -2.6971e+02 - logprior: -1.0448e+00
Epoch 5/10
19/19 - 1s - loss: 270.1008 - loglik: -2.6906e+02 - logprior: -1.0390e+00
Epoch 6/10
19/19 - 1s - loss: 269.6451 - loglik: -2.6858e+02 - logprior: -1.0662e+00
Epoch 7/10
19/19 - 1s - loss: 269.7426 - loglik: -2.6866e+02 - logprior: -1.0853e+00
Fitted a model with MAP estimate = -269.2206
expansions: [(0, 3), (1, 1), (19, 2), (26, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 272.7786 - loglik: -2.6846e+02 - logprior: -4.3226e+00
Epoch 2/2
19/19 - 1s - loss: 264.0823 - loglik: -2.6250e+02 - logprior: -1.5844e+00
Fitted a model with MAP estimate = -262.7700
expansions: []
discards: [ 4 24 35 42]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 266.4865 - loglik: -2.6326e+02 - logprior: -3.2224e+00
Epoch 2/2
19/19 - 1s - loss: 263.2567 - loglik: -2.6195e+02 - logprior: -1.3111e+00
Fitted a model with MAP estimate = -262.4344
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 265.3714 - loglik: -2.6217e+02 - logprior: -3.2048e+00
Epoch 2/10
19/19 - 1s - loss: 262.7496 - loglik: -2.6145e+02 - logprior: -1.2962e+00
Epoch 3/10
19/19 - 1s - loss: 262.2331 - loglik: -2.6107e+02 - logprior: -1.1643e+00
Epoch 4/10
19/19 - 1s - loss: 262.0345 - loglik: -2.6092e+02 - logprior: -1.1181e+00
Epoch 5/10
19/19 - 1s - loss: 261.4374 - loglik: -2.6034e+02 - logprior: -1.0958e+00
Epoch 6/10
19/19 - 1s - loss: 261.4038 - loglik: -2.6032e+02 - logprior: -1.0838e+00
Epoch 7/10
19/19 - 1s - loss: 261.2220 - loglik: -2.6016e+02 - logprior: -1.0645e+00
Epoch 8/10
19/19 - 1s - loss: 261.2458 - loglik: -2.6019e+02 - logprior: -1.0590e+00
Fitted a model with MAP estimate = -261.1180
Time for alignment: 36.6781
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 302.3128 - loglik: -2.9911e+02 - logprior: -3.1998e+00
Epoch 2/10
19/19 - 1s - loss: 281.8043 - loglik: -2.8051e+02 - logprior: -1.2900e+00
Epoch 3/10
19/19 - 1s - loss: 272.7120 - loglik: -2.7123e+02 - logprior: -1.4817e+00
Epoch 4/10
19/19 - 1s - loss: 270.2771 - loglik: -2.6885e+02 - logprior: -1.4235e+00
Epoch 5/10
19/19 - 1s - loss: 270.0031 - loglik: -2.6862e+02 - logprior: -1.3841e+00
Epoch 6/10
19/19 - 1s - loss: 269.6027 - loglik: -2.6823e+02 - logprior: -1.3757e+00
Epoch 7/10
19/19 - 1s - loss: 269.4434 - loglik: -2.6808e+02 - logprior: -1.3634e+00
Epoch 8/10
19/19 - 1s - loss: 269.5269 - loglik: -2.6817e+02 - logprior: -1.3568e+00
Fitted a model with MAP estimate = -269.0664
expansions: [(6, 3), (10, 1), (19, 2), (23, 1), (27, 2), (28, 1), (30, 1), (31, 2), (32, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 273.4605 - loglik: -2.6939e+02 - logprior: -4.0702e+00
Epoch 2/2
19/19 - 1s - loss: 265.9955 - loglik: -2.6402e+02 - logprior: -1.9747e+00
Fitted a model with MAP estimate = -264.3478
expansions: [(0, 2)]
discards: [ 0 22 34 41]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 265.9467 - loglik: -2.6294e+02 - logprior: -3.0055e+00
Epoch 2/2
19/19 - 1s - loss: 262.9613 - loglik: -2.6176e+02 - logprior: -1.2047e+00
Fitted a model with MAP estimate = -262.1300
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 267.3658 - loglik: -2.6362e+02 - logprior: -3.7465e+00
Epoch 2/10
19/19 - 1s - loss: 262.9962 - loglik: -2.6167e+02 - logprior: -1.3307e+00
Epoch 3/10
19/19 - 1s - loss: 262.3569 - loglik: -2.6121e+02 - logprior: -1.1507e+00
Epoch 4/10
19/19 - 1s - loss: 262.1537 - loglik: -2.6104e+02 - logprior: -1.1152e+00
Epoch 5/10
19/19 - 1s - loss: 261.3320 - loglik: -2.6023e+02 - logprior: -1.0998e+00
Epoch 6/10
19/19 - 1s - loss: 261.4992 - loglik: -2.6041e+02 - logprior: -1.0903e+00
Fitted a model with MAP estimate = -261.2376
Time for alignment: 35.6692
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 302.2478 - loglik: -2.9904e+02 - logprior: -3.2074e+00
Epoch 2/10
19/19 - 1s - loss: 280.7186 - loglik: -2.7938e+02 - logprior: -1.3371e+00
Epoch 3/10
19/19 - 1s - loss: 273.4016 - loglik: -2.7191e+02 - logprior: -1.4900e+00
Epoch 4/10
19/19 - 1s - loss: 271.5674 - loglik: -2.7017e+02 - logprior: -1.4019e+00
Epoch 5/10
19/19 - 1s - loss: 271.2241 - loglik: -2.6985e+02 - logprior: -1.3762e+00
Epoch 6/10
19/19 - 1s - loss: 270.9278 - loglik: -2.6957e+02 - logprior: -1.3544e+00
Epoch 7/10
19/19 - 1s - loss: 270.7090 - loglik: -2.6937e+02 - logprior: -1.3423e+00
Epoch 8/10
19/19 - 1s - loss: 270.7538 - loglik: -2.6942e+02 - logprior: -1.3377e+00
Fitted a model with MAP estimate = -270.2726
expansions: [(6, 2), (7, 2), (16, 1), (19, 2), (28, 2), (29, 2), (30, 2), (31, 2), (32, 3)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 272.4122 - loglik: -2.6901e+02 - logprior: -3.3994e+00
Epoch 2/2
19/19 - 1s - loss: 263.9366 - loglik: -2.6243e+02 - logprior: -1.5043e+00
Fitted a model with MAP estimate = -262.7815
expansions: []
discards: [ 8 24 35 37 41 47]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 266.4347 - loglik: -2.6321e+02 - logprior: -3.2258e+00
Epoch 2/2
19/19 - 1s - loss: 263.2050 - loglik: -2.6190e+02 - logprior: -1.3094e+00
Fitted a model with MAP estimate = -262.4043
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 265.3202 - loglik: -2.6213e+02 - logprior: -3.1938e+00
Epoch 2/10
19/19 - 1s - loss: 262.7593 - loglik: -2.6147e+02 - logprior: -1.2920e+00
Epoch 3/10
19/19 - 1s - loss: 262.3710 - loglik: -2.6121e+02 - logprior: -1.1586e+00
Epoch 4/10
19/19 - 1s - loss: 261.8312 - loglik: -2.6071e+02 - logprior: -1.1223e+00
Epoch 5/10
19/19 - 1s - loss: 261.6537 - loglik: -2.6056e+02 - logprior: -1.0916e+00
Epoch 6/10
19/19 - 1s - loss: 261.3187 - loglik: -2.6024e+02 - logprior: -1.0792e+00
Epoch 7/10
19/19 - 1s - loss: 261.1331 - loglik: -2.6006e+02 - logprior: -1.0684e+00
Epoch 8/10
19/19 - 1s - loss: 261.2691 - loglik: -2.6021e+02 - logprior: -1.0562e+00
Fitted a model with MAP estimate = -261.1119
Time for alignment: 37.5519
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 302.1531 - loglik: -2.9895e+02 - logprior: -3.1997e+00
Epoch 2/10
19/19 - 1s - loss: 280.0286 - loglik: -2.7871e+02 - logprior: -1.3144e+00
Epoch 3/10
19/19 - 1s - loss: 272.1818 - loglik: -2.7070e+02 - logprior: -1.4804e+00
Epoch 4/10
19/19 - 1s - loss: 270.9316 - loglik: -2.6955e+02 - logprior: -1.3860e+00
Epoch 5/10
19/19 - 1s - loss: 270.6832 - loglik: -2.6932e+02 - logprior: -1.3598e+00
Epoch 6/10
19/19 - 1s - loss: 270.0840 - loglik: -2.6874e+02 - logprior: -1.3417e+00
Epoch 7/10
19/19 - 1s - loss: 270.4294 - loglik: -2.6910e+02 - logprior: -1.3271e+00
Fitted a model with MAP estimate = -269.7765
expansions: [(6, 3), (15, 1), (19, 2), (28, 2), (29, 2), (30, 2), (31, 2), (32, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 274.2881 - loglik: -2.7019e+02 - logprior: -4.0934e+00
Epoch 2/2
19/19 - 1s - loss: 266.2426 - loglik: -2.6422e+02 - logprior: -2.0270e+00
Fitted a model with MAP estimate = -264.4018
expansions: [(0, 2)]
discards: [ 0 23 34 36 40 46]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.3865 - loglik: -2.6338e+02 - logprior: -3.0086e+00
Epoch 2/2
19/19 - 1s - loss: 262.9388 - loglik: -2.6174e+02 - logprior: -1.2012e+00
Fitted a model with MAP estimate = -262.1502
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 267.3257 - loglik: -2.6358e+02 - logprior: -3.7462e+00
Epoch 2/10
19/19 - 1s - loss: 263.0932 - loglik: -2.6177e+02 - logprior: -1.3255e+00
Epoch 3/10
19/19 - 1s - loss: 262.4141 - loglik: -2.6127e+02 - logprior: -1.1472e+00
Epoch 4/10
19/19 - 1s - loss: 261.8598 - loglik: -2.6075e+02 - logprior: -1.1135e+00
Epoch 5/10
19/19 - 1s - loss: 261.6071 - loglik: -2.6051e+02 - logprior: -1.1018e+00
Epoch 6/10
19/19 - 1s - loss: 261.3297 - loglik: -2.6024e+02 - logprior: -1.0904e+00
Epoch 7/10
19/19 - 1s - loss: 261.4196 - loglik: -2.6035e+02 - logprior: -1.0745e+00
Fitted a model with MAP estimate = -261.1688
Time for alignment: 34.7880
Computed alignments with likelihoods: ['-261.0800', '-261.1180', '-261.2376', '-261.1119', '-261.1688']
Best model has likelihood: -261.0800  (prior= -1.0635 )
time for generating output: 0.0932
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01381.projection.fasta
SP score = 0.6329804022840091
Training of 5 independent models on file PF02878.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 737.6721 - loglik: -7.3468e+02 - logprior: -2.9901e+00
Epoch 2/10
19/19 - 3s - loss: 681.7653 - loglik: -6.8062e+02 - logprior: -1.1446e+00
Epoch 3/10
19/19 - 3s - loss: 654.8040 - loglik: -6.5354e+02 - logprior: -1.2658e+00
Epoch 4/10
19/19 - 3s - loss: 649.2885 - loglik: -6.4805e+02 - logprior: -1.2411e+00
Epoch 5/10
19/19 - 3s - loss: 647.4842 - loglik: -6.4631e+02 - logprior: -1.1760e+00
Epoch 6/10
19/19 - 3s - loss: 645.0049 - loglik: -6.4387e+02 - logprior: -1.1394e+00
Epoch 7/10
19/19 - 3s - loss: 646.1635 - loglik: -6.4506e+02 - logprior: -1.1058e+00
Fitted a model with MAP estimate = -645.1837
expansions: [(12, 2), (14, 3), (17, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 2), (93, 1), (96, 1), (101, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 648.0309 - loglik: -6.4419e+02 - logprior: -3.8401e+00
Epoch 2/2
19/19 - 4s - loss: 638.0139 - loglik: -6.3619e+02 - logprior: -1.8201e+00
Fitted a model with MAP estimate = -635.7195
expansions: [(0, 3)]
discards: [  0  12  17  38  76  92 139 140 141]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 639.8218 - loglik: -6.3694e+02 - logprior: -2.8777e+00
Epoch 2/2
19/19 - 4s - loss: 636.1884 - loglik: -6.3525e+02 - logprior: -9.4162e-01
Fitted a model with MAP estimate = -635.0253
expansions: [(137, 2)]
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 641.5699 - loglik: -6.3792e+02 - logprior: -3.6478e+00
Epoch 2/10
19/19 - 4s - loss: 637.7645 - loglik: -6.3626e+02 - logprior: -1.5065e+00
Epoch 3/10
19/19 - 4s - loss: 636.0781 - loglik: -6.3536e+02 - logprior: -7.1369e-01
Epoch 4/10
19/19 - 5s - loss: 634.9949 - loglik: -6.3458e+02 - logprior: -4.1768e-01
Epoch 5/10
19/19 - 4s - loss: 634.2199 - loglik: -6.3385e+02 - logprior: -3.7165e-01
Epoch 6/10
19/19 - 4s - loss: 633.8826 - loglik: -6.3354e+02 - logprior: -3.4661e-01
Epoch 7/10
19/19 - 4s - loss: 633.6727 - loglik: -6.3337e+02 - logprior: -3.0224e-01
Epoch 8/10
19/19 - 5s - loss: 633.9985 - loglik: -6.3375e+02 - logprior: -2.5304e-01
Fitted a model with MAP estimate = -633.3560
Time for alignment: 112.8877
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 737.9540 - loglik: -7.3497e+02 - logprior: -2.9882e+00
Epoch 2/10
19/19 - 3s - loss: 679.6841 - loglik: -6.7856e+02 - logprior: -1.1211e+00
Epoch 3/10
19/19 - 3s - loss: 655.5470 - loglik: -6.5429e+02 - logprior: -1.2567e+00
Epoch 4/10
19/19 - 3s - loss: 649.6618 - loglik: -6.4843e+02 - logprior: -1.2275e+00
Epoch 5/10
19/19 - 3s - loss: 648.0865 - loglik: -6.4692e+02 - logprior: -1.1635e+00
Epoch 6/10
19/19 - 3s - loss: 647.0961 - loglik: -6.4597e+02 - logprior: -1.1236e+00
Epoch 7/10
19/19 - 3s - loss: 646.2814 - loglik: -6.4518e+02 - logprior: -1.0969e+00
Epoch 8/10
19/19 - 3s - loss: 646.7502 - loglik: -6.4566e+02 - logprior: -1.0884e+00
Fitted a model with MAP estimate = -645.5976
expansions: [(12, 1), (14, 3), (16, 1), (17, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (94, 1), (97, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 647.3970 - loglik: -6.4359e+02 - logprior: -3.8105e+00
Epoch 2/2
19/19 - 4s - loss: 636.8942 - loglik: -6.3516e+02 - logprior: -1.7345e+00
Fitted a model with MAP estimate = -635.3521
expansions: [(0, 2)]
discards: [  0  37  75 137 138]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 637.5534 - loglik: -6.3485e+02 - logprior: -2.7025e+00
Epoch 2/2
19/19 - 4s - loss: 634.3179 - loglik: -6.3342e+02 - logprior: -8.9431e-01
Fitted a model with MAP estimate = -633.4174
expansions: []
discards: [ 0 17]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 639.3865 - loglik: -6.3572e+02 - logprior: -3.6712e+00
Epoch 2/10
19/19 - 4s - loss: 635.7280 - loglik: -6.3445e+02 - logprior: -1.2824e+00
Epoch 3/10
19/19 - 4s - loss: 634.0361 - loglik: -6.3350e+02 - logprior: -5.3659e-01
Epoch 4/10
19/19 - 5s - loss: 632.2335 - loglik: -6.3181e+02 - logprior: -4.2318e-01
Epoch 5/10
19/19 - 5s - loss: 633.0654 - loglik: -6.3268e+02 - logprior: -3.8243e-01
Fitted a model with MAP estimate = -632.0602
Time for alignment: 103.6742
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 738.1500 - loglik: -7.3516e+02 - logprior: -2.9901e+00
Epoch 2/10
19/19 - 3s - loss: 679.8591 - loglik: -6.7877e+02 - logprior: -1.0904e+00
Epoch 3/10
19/19 - 3s - loss: 655.6075 - loglik: -6.5443e+02 - logprior: -1.1800e+00
Epoch 4/10
19/19 - 3s - loss: 651.5597 - loglik: -6.5040e+02 - logprior: -1.1595e+00
Epoch 5/10
19/19 - 3s - loss: 648.5952 - loglik: -6.4751e+02 - logprior: -1.0878e+00
Epoch 6/10
19/19 - 3s - loss: 648.2648 - loglik: -6.4720e+02 - logprior: -1.0621e+00
Epoch 7/10
19/19 - 3s - loss: 647.3720 - loglik: -6.4632e+02 - logprior: -1.0502e+00
Epoch 8/10
19/19 - 3s - loss: 647.2474 - loglik: -6.4622e+02 - logprior: -1.0271e+00
Epoch 9/10
19/19 - 3s - loss: 647.1235 - loglik: -6.4610e+02 - logprior: -1.0204e+00
Epoch 10/10
19/19 - 3s - loss: 647.9911 - loglik: -6.4697e+02 - logprior: -1.0241e+00
Fitted a model with MAP estimate = -646.8824
expansions: [(12, 2), (14, 3), (16, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (94, 1), (97, 1), (101, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 648.5150 - loglik: -6.4472e+02 - logprior: -3.7983e+00
Epoch 2/2
19/19 - 5s - loss: 638.1454 - loglik: -6.3635e+02 - logprior: -1.7975e+00
Fitted a model with MAP estimate = -636.3995
expansions: [(0, 3)]
discards: [ 0 12 37 75]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 639.4036 - loglik: -6.3668e+02 - logprior: -2.7278e+00
Epoch 2/2
19/19 - 4s - loss: 636.3904 - loglik: -6.3546e+02 - logprior: -9.2572e-01
Fitted a model with MAP estimate = -635.1160
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 641.0180 - loglik: -6.3735e+02 - logprior: -3.6689e+00
Epoch 2/10
19/19 - 4s - loss: 636.4242 - loglik: -6.3509e+02 - logprior: -1.3356e+00
Epoch 3/10
19/19 - 4s - loss: 635.1763 - loglik: -6.3459e+02 - logprior: -5.8352e-01
Epoch 4/10
19/19 - 5s - loss: 635.1392 - loglik: -6.3464e+02 - logprior: -4.9747e-01
Epoch 5/10
19/19 - 4s - loss: 634.3345 - loglik: -6.3388e+02 - logprior: -4.5921e-01
Epoch 6/10
19/19 - 5s - loss: 633.3511 - loglik: -6.3293e+02 - logprior: -4.1830e-01
Epoch 7/10
19/19 - 5s - loss: 633.1344 - loglik: -6.3276e+02 - logprior: -3.7714e-01
Epoch 8/10
19/19 - 5s - loss: 633.1647 - loglik: -6.3283e+02 - logprior: -3.3587e-01
Fitted a model with MAP estimate = -633.0680
Time for alignment: 123.0937
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 737.8670 - loglik: -7.3488e+02 - logprior: -2.9887e+00
Epoch 2/10
19/19 - 3s - loss: 679.2735 - loglik: -6.7814e+02 - logprior: -1.1359e+00
Epoch 3/10
19/19 - 3s - loss: 653.5919 - loglik: -6.5229e+02 - logprior: -1.3026e+00
Epoch 4/10
19/19 - 3s - loss: 648.4144 - loglik: -6.4713e+02 - logprior: -1.2818e+00
Epoch 5/10
19/19 - 3s - loss: 645.8599 - loglik: -6.4467e+02 - logprior: -1.1918e+00
Epoch 6/10
19/19 - 3s - loss: 645.6255 - loglik: -6.4449e+02 - logprior: -1.1331e+00
Epoch 7/10
19/19 - 3s - loss: 645.7333 - loglik: -6.4463e+02 - logprior: -1.1063e+00
Fitted a model with MAP estimate = -645.0382
expansions: [(12, 2), (14, 3), (16, 1), (19, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (97, 1), (99, 1), (103, 1), (107, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 647.2250 - loglik: -6.4341e+02 - logprior: -3.8151e+00
Epoch 2/2
19/19 - 5s - loss: 636.6797 - loglik: -6.3491e+02 - logprior: -1.7677e+00
Fitted a model with MAP estimate = -635.0720
expansions: [(0, 3)]
discards: [ 0 12 16 38 76]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 638.6411 - loglik: -6.3591e+02 - logprior: -2.7272e+00
Epoch 2/2
19/19 - 4s - loss: 635.7642 - loglik: -6.3485e+02 - logprior: -9.0977e-01
Fitted a model with MAP estimate = -634.2454
expansions: []
discards: [  0   2 137]
Re-initialized the encoder parameters.
Fitting a model of length 136 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 640.0794 - loglik: -6.3639e+02 - logprior: -3.6900e+00
Epoch 2/10
19/19 - 4s - loss: 635.7352 - loglik: -6.3446e+02 - logprior: -1.2768e+00
Epoch 3/10
19/19 - 4s - loss: 634.7060 - loglik: -6.3414e+02 - logprior: -5.6102e-01
Epoch 4/10
19/19 - 4s - loss: 633.2778 - loglik: -6.3284e+02 - logprior: -4.4038e-01
Epoch 5/10
19/19 - 4s - loss: 633.9149 - loglik: -6.3352e+02 - logprior: -3.9733e-01
Fitted a model with MAP estimate = -632.4712
Time for alignment: 101.6646
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 738.1208 - loglik: -7.3513e+02 - logprior: -2.9949e+00
Epoch 2/10
19/19 - 3s - loss: 680.4575 - loglik: -6.7934e+02 - logprior: -1.1132e+00
Epoch 3/10
19/19 - 3s - loss: 657.0809 - loglik: -6.5593e+02 - logprior: -1.1498e+00
Epoch 4/10
19/19 - 3s - loss: 651.2180 - loglik: -6.5009e+02 - logprior: -1.1283e+00
Epoch 5/10
19/19 - 3s - loss: 649.5991 - loglik: -6.4850e+02 - logprior: -1.0944e+00
Epoch 6/10
19/19 - 3s - loss: 646.8211 - loglik: -6.4573e+02 - logprior: -1.0892e+00
Epoch 7/10
19/19 - 3s - loss: 646.4402 - loglik: -6.4537e+02 - logprior: -1.0689e+00
Epoch 8/10
19/19 - 3s - loss: 646.8046 - loglik: -6.4575e+02 - logprior: -1.0550e+00
Fitted a model with MAP estimate = -645.8306
expansions: [(12, 2), (13, 4), (25, 1), (26, 1), (28, 1), (29, 2), (30, 2), (40, 1), (43, 1), (45, 1), (48, 1), (55, 1), (58, 2), (69, 1), (70, 1), (71, 1), (97, 1), (101, 1), (103, 1), (107, 1), (108, 1), (109, 2), (110, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 647.3792 - loglik: -6.4356e+02 - logprior: -3.8175e+00
Epoch 2/2
19/19 - 5s - loss: 637.8622 - loglik: -6.3607e+02 - logprior: -1.7970e+00
Fitted a model with MAP estimate = -635.6538
expansions: [(0, 3)]
discards: [  0  13  15  38  76 138]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 638.9215 - loglik: -6.3615e+02 - logprior: -2.7669e+00
Epoch 2/2
19/19 - 5s - loss: 635.5129 - loglik: -6.3457e+02 - logprior: -9.4596e-01
Fitted a model with MAP estimate = -634.5380
expansions: []
discards: [  0   1   2 137]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 641.3895 - loglik: -6.3774e+02 - logprior: -3.6521e+00
Epoch 2/10
19/19 - 4s - loss: 637.5692 - loglik: -6.3605e+02 - logprior: -1.5170e+00
Epoch 3/10
19/19 - 5s - loss: 635.5143 - loglik: -6.3481e+02 - logprior: -7.0380e-01
Epoch 4/10
19/19 - 5s - loss: 634.7927 - loglik: -6.3437e+02 - logprior: -4.1819e-01
Epoch 5/10
19/19 - 5s - loss: 634.9691 - loglik: -6.3465e+02 - logprior: -3.1431e-01
Fitted a model with MAP estimate = -633.6685
Time for alignment: 105.4961
Computed alignments with likelihoods: ['-633.3560', '-632.0602', '-633.0680', '-632.4712', '-633.6685']
Best model has likelihood: -632.0602  (prior= -0.3202 )
time for generating output: 0.1598
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02878.projection.fasta
SP score = 0.8225165562913908
Training of 5 independent models on file PF00970.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 556.2225 - loglik: -5.5317e+02 - logprior: -3.0542e+00
Epoch 2/10
19/19 - 2s - loss: 518.0094 - loglik: -5.1680e+02 - logprior: -1.2081e+00
Epoch 3/10
19/19 - 2s - loss: 502.2553 - loglik: -5.0093e+02 - logprior: -1.3210e+00
Epoch 4/10
19/19 - 2s - loss: 498.6253 - loglik: -4.9731e+02 - logprior: -1.3159e+00
Epoch 5/10
19/19 - 2s - loss: 497.9916 - loglik: -4.9673e+02 - logprior: -1.2622e+00
Epoch 6/10
19/19 - 2s - loss: 497.3693 - loglik: -4.9613e+02 - logprior: -1.2382e+00
Epoch 7/10
19/19 - 2s - loss: 497.2957 - loglik: -4.9609e+02 - logprior: -1.2092e+00
Epoch 8/10
19/19 - 2s - loss: 497.2716 - loglik: -4.9606e+02 - logprior: -1.2098e+00
Epoch 9/10
19/19 - 2s - loss: 496.8574 - loglik: -4.9567e+02 - logprior: -1.1829e+00
Epoch 10/10
19/19 - 2s - loss: 496.8298 - loglik: -4.9564e+02 - logprior: -1.1930e+00
Fitted a model with MAP estimate = -496.0670
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (16, 1), (17, 1), (18, 1), (19, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (63, 3), (68, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 498.4984 - loglik: -4.9464e+02 - logprior: -3.8567e+00
Epoch 2/2
19/19 - 3s - loss: 488.9818 - loglik: -4.8777e+02 - logprior: -1.2146e+00
Fitted a model with MAP estimate = -487.0677
expansions: []
discards: [ 0 74 78 94]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 492.8733 - loglik: -4.8895e+02 - logprior: -3.9242e+00
Epoch 2/2
19/19 - 3s - loss: 489.2978 - loglik: -4.8792e+02 - logprior: -1.3825e+00
Fitted a model with MAP estimate = -486.9962
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 489.7730 - loglik: -4.8694e+02 - logprior: -2.8316e+00
Epoch 2/10
19/19 - 3s - loss: 487.0035 - loglik: -4.8596e+02 - logprior: -1.0423e+00
Epoch 3/10
19/19 - 3s - loss: 486.3060 - loglik: -4.8534e+02 - logprior: -9.7098e-01
Epoch 4/10
19/19 - 3s - loss: 485.8618 - loglik: -4.8494e+02 - logprior: -9.2185e-01
Epoch 5/10
19/19 - 3s - loss: 485.9036 - loglik: -4.8502e+02 - logprior: -8.8179e-01
Fitted a model with MAP estimate = -485.3491
Time for alignment: 77.8364
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 556.3849 - loglik: -5.5333e+02 - logprior: -3.0545e+00
Epoch 2/10
19/19 - 2s - loss: 517.7873 - loglik: -5.1661e+02 - logprior: -1.1804e+00
Epoch 3/10
19/19 - 2s - loss: 502.6721 - loglik: -5.0136e+02 - logprior: -1.3149e+00
Epoch 4/10
19/19 - 2s - loss: 499.1514 - loglik: -4.9782e+02 - logprior: -1.3323e+00
Epoch 5/10
19/19 - 2s - loss: 497.5232 - loglik: -4.9626e+02 - logprior: -1.2635e+00
Epoch 6/10
19/19 - 2s - loss: 497.5570 - loglik: -4.9631e+02 - logprior: -1.2504e+00
Fitted a model with MAP estimate = -496.1997
expansions: [(0, 2), (4, 1), (6, 1), (8, 1), (10, 1), (14, 1), (17, 1), (18, 1), (27, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 499.1796 - loglik: -4.9538e+02 - logprior: -3.7994e+00
Epoch 2/2
19/19 - 3s - loss: 489.2445 - loglik: -4.8798e+02 - logprior: -1.2681e+00
Fitted a model with MAP estimate = -487.2201
expansions: []
discards: [ 0 73 78 81 83 94]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 493.9903 - loglik: -4.9004e+02 - logprior: -3.9509e+00
Epoch 2/2
19/19 - 3s - loss: 489.8349 - loglik: -4.8840e+02 - logprior: -1.4333e+00
Fitted a model with MAP estimate = -487.7366
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 490.2663 - loglik: -4.8740e+02 - logprior: -2.8636e+00
Epoch 2/10
19/19 - 3s - loss: 487.9885 - loglik: -4.8693e+02 - logprior: -1.0619e+00
Epoch 3/10
19/19 - 3s - loss: 487.3536 - loglik: -4.8636e+02 - logprior: -9.9811e-01
Epoch 4/10
19/19 - 3s - loss: 486.4551 - loglik: -4.8551e+02 - logprior: -9.4795e-01
Epoch 5/10
19/19 - 3s - loss: 486.5937 - loglik: -4.8567e+02 - logprior: -9.2286e-01
Fitted a model with MAP estimate = -486.0492
Time for alignment: 67.2119
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 556.3494 - loglik: -5.5329e+02 - logprior: -3.0602e+00
Epoch 2/10
19/19 - 2s - loss: 518.4824 - loglik: -5.1725e+02 - logprior: -1.2345e+00
Epoch 3/10
19/19 - 2s - loss: 502.4786 - loglik: -5.0115e+02 - logprior: -1.3269e+00
Epoch 4/10
19/19 - 2s - loss: 499.4994 - loglik: -4.9818e+02 - logprior: -1.3150e+00
Epoch 5/10
19/19 - 2s - loss: 498.4326 - loglik: -4.9719e+02 - logprior: -1.2466e+00
Epoch 6/10
19/19 - 2s - loss: 498.6079 - loglik: -4.9741e+02 - logprior: -1.2017e+00
Fitted a model with MAP estimate = -497.2352
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (19, 1), (28, 1), (29, 1), (33, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 499.9680 - loglik: -4.9620e+02 - logprior: -3.7716e+00
Epoch 2/2
19/19 - 3s - loss: 490.1647 - loglik: -4.8891e+02 - logprior: -1.2502e+00
Fitted a model with MAP estimate = -487.9747
expansions: []
discards: [ 0 73 80 82 83 94]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 494.7446 - loglik: -4.9076e+02 - logprior: -3.9804e+00
Epoch 2/2
19/19 - 3s - loss: 490.2881 - loglik: -4.8882e+02 - logprior: -1.4725e+00
Fitted a model with MAP estimate = -488.3628
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 490.9636 - loglik: -4.8810e+02 - logprior: -2.8655e+00
Epoch 2/10
19/19 - 3s - loss: 488.1310 - loglik: -4.8707e+02 - logprior: -1.0581e+00
Epoch 3/10
19/19 - 3s - loss: 487.7097 - loglik: -4.8672e+02 - logprior: -9.9205e-01
Epoch 4/10
19/19 - 3s - loss: 487.2771 - loglik: -4.8634e+02 - logprior: -9.3920e-01
Epoch 5/10
19/19 - 3s - loss: 487.1132 - loglik: -4.8621e+02 - logprior: -9.0782e-01
Epoch 6/10
19/19 - 3s - loss: 486.6613 - loglik: -4.8577e+02 - logprior: -8.9228e-01
Epoch 7/10
19/19 - 3s - loss: 486.1325 - loglik: -4.8526e+02 - logprior: -8.7682e-01
Epoch 8/10
19/19 - 3s - loss: 485.9607 - loglik: -4.8511e+02 - logprior: -8.5428e-01
Epoch 9/10
19/19 - 3s - loss: 487.1419 - loglik: -4.8630e+02 - logprior: -8.3862e-01
Fitted a model with MAP estimate = -486.2028
Time for alignment: 76.6449
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 556.1694 - loglik: -5.5311e+02 - logprior: -3.0570e+00
Epoch 2/10
19/19 - 2s - loss: 519.4585 - loglik: -5.1823e+02 - logprior: -1.2261e+00
Epoch 3/10
19/19 - 2s - loss: 503.0771 - loglik: -5.0175e+02 - logprior: -1.3290e+00
Epoch 4/10
19/19 - 2s - loss: 498.9471 - loglik: -4.9762e+02 - logprior: -1.3298e+00
Epoch 5/10
19/19 - 2s - loss: 498.1238 - loglik: -4.9684e+02 - logprior: -1.2846e+00
Epoch 6/10
19/19 - 2s - loss: 497.8505 - loglik: -4.9662e+02 - logprior: -1.2322e+00
Epoch 7/10
19/19 - 2s - loss: 498.2342 - loglik: -4.9701e+02 - logprior: -1.2291e+00
Fitted a model with MAP estimate = -496.6084
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (16, 1), (17, 1), (18, 1), (21, 1), (26, 1), (29, 1), (33, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (68, 1), (71, 2), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 499.3731 - loglik: -4.9557e+02 - logprior: -3.7991e+00
Epoch 2/2
19/19 - 3s - loss: 489.1316 - loglik: -4.8789e+02 - logprior: -1.2451e+00
Fitted a model with MAP estimate = -487.0780
expansions: []
discards: [ 0 74 79 82 84 95]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 493.6113 - loglik: -4.8968e+02 - logprior: -3.9325e+00
Epoch 2/2
19/19 - 3s - loss: 489.4530 - loglik: -4.8808e+02 - logprior: -1.3742e+00
Fitted a model with MAP estimate = -487.5662
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 490.1760 - loglik: -4.8733e+02 - logprior: -2.8470e+00
Epoch 2/10
19/19 - 3s - loss: 487.3775 - loglik: -4.8632e+02 - logprior: -1.0566e+00
Epoch 3/10
19/19 - 3s - loss: 487.0999 - loglik: -4.8611e+02 - logprior: -9.8755e-01
Epoch 4/10
19/19 - 3s - loss: 486.6785 - loglik: -4.8574e+02 - logprior: -9.3878e-01
Epoch 5/10
19/19 - 3s - loss: 485.7848 - loglik: -4.8487e+02 - logprior: -9.1186e-01
Epoch 6/10
19/19 - 3s - loss: 486.1445 - loglik: -4.8526e+02 - logprior: -8.8365e-01
Fitted a model with MAP estimate = -485.5834
Time for alignment: 70.7578
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 556.3600 - loglik: -5.5330e+02 - logprior: -3.0580e+00
Epoch 2/10
19/19 - 2s - loss: 518.2421 - loglik: -5.1701e+02 - logprior: -1.2342e+00
Epoch 3/10
19/19 - 2s - loss: 502.4860 - loglik: -5.0115e+02 - logprior: -1.3312e+00
Epoch 4/10
19/19 - 2s - loss: 499.4940 - loglik: -4.9818e+02 - logprior: -1.3186e+00
Epoch 5/10
19/19 - 2s - loss: 498.3561 - loglik: -4.9710e+02 - logprior: -1.2557e+00
Epoch 6/10
19/19 - 2s - loss: 498.2618 - loglik: -4.9705e+02 - logprior: -1.2137e+00
Epoch 7/10
19/19 - 2s - loss: 498.0884 - loglik: -4.9688e+02 - logprior: -1.2127e+00
Epoch 8/10
19/19 - 2s - loss: 497.9633 - loglik: -4.9677e+02 - logprior: -1.1977e+00
Epoch 9/10
19/19 - 2s - loss: 497.7219 - loglik: -4.9652e+02 - logprior: -1.1971e+00
Epoch 10/10
19/19 - 2s - loss: 497.6579 - loglik: -4.9647e+02 - logprior: -1.1864e+00
Fitted a model with MAP estimate = -496.7860
expansions: [(0, 2), (6, 1), (7, 1), (8, 1), (10, 1), (14, 1), (18, 2), (19, 1), (28, 1), (29, 1), (35, 1), (46, 1), (59, 2), (61, 2), (62, 2), (63, 2), (64, 1), (76, 1), (77, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 499.4554 - loglik: -4.9557e+02 - logprior: -3.8819e+00
Epoch 2/2
19/19 - 3s - loss: 490.2178 - loglik: -4.8898e+02 - logprior: -1.2411e+00
Fitted a model with MAP estimate = -487.9433
expansions: []
discards: [ 0 73 80 82 83]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 494.6904 - loglik: -4.9073e+02 - logprior: -3.9590e+00
Epoch 2/2
19/19 - 3s - loss: 490.1989 - loglik: -4.8881e+02 - logprior: -1.3892e+00
Fitted a model with MAP estimate = -488.3254
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10035 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 491.0882 - loglik: -4.8823e+02 - logprior: -2.8622e+00
Epoch 2/10
19/19 - 3s - loss: 488.2499 - loglik: -4.8719e+02 - logprior: -1.0625e+00
Epoch 3/10
19/19 - 3s - loss: 487.8494 - loglik: -4.8685e+02 - logprior: -9.9637e-01
Epoch 4/10
19/19 - 3s - loss: 487.1847 - loglik: -4.8625e+02 - logprior: -9.3477e-01
Epoch 5/10
19/19 - 3s - loss: 486.8287 - loglik: -4.8592e+02 - logprior: -9.0423e-01
Epoch 6/10
19/19 - 3s - loss: 486.5459 - loglik: -4.8565e+02 - logprior: -8.9138e-01
Epoch 7/10
19/19 - 3s - loss: 486.5999 - loglik: -4.8574e+02 - logprior: -8.6369e-01
Fitted a model with MAP estimate = -486.3341
Time for alignment: 81.0934
Computed alignments with likelihoods: ['-485.3491', '-486.0492', '-486.2028', '-485.5834', '-486.3341']
Best model has likelihood: -485.3491  (prior= -0.8792 )
time for generating output: 0.1764
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00970.projection.fasta
SP score = 0.6568904035388643
Training of 5 independent models on file PF00313.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 364.9815 - loglik: -3.6177e+02 - logprior: -3.2086e+00
Epoch 2/10
19/19 - 1s - loss: 321.7928 - loglik: -3.2047e+02 - logprior: -1.3261e+00
Epoch 3/10
19/19 - 1s - loss: 305.8430 - loglik: -3.0449e+02 - logprior: -1.3519e+00
Epoch 4/10
19/19 - 1s - loss: 303.6647 - loglik: -3.0231e+02 - logprior: -1.3512e+00
Epoch 5/10
19/19 - 1s - loss: 302.5643 - loglik: -3.0127e+02 - logprior: -1.2979e+00
Epoch 6/10
19/19 - 1s - loss: 302.4229 - loglik: -3.0115e+02 - logprior: -1.2727e+00
Epoch 7/10
19/19 - 1s - loss: 301.9627 - loglik: -3.0070e+02 - logprior: -1.2632e+00
Epoch 8/10
19/19 - 1s - loss: 302.0316 - loglik: -3.0078e+02 - logprior: -1.2510e+00
Fitted a model with MAP estimate = -301.8643
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 302.4536 - loglik: -2.9831e+02 - logprior: -4.1405e+00
Epoch 2/2
19/19 - 1s - loss: 293.7805 - loglik: -2.9255e+02 - logprior: -1.2263e+00
Fitted a model with MAP estimate = -292.6652
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 298.0694 - loglik: -2.9404e+02 - logprior: -4.0260e+00
Epoch 2/2
19/19 - 1s - loss: 293.9012 - loglik: -2.9247e+02 - logprior: -1.4311e+00
Fitted a model with MAP estimate = -292.8797
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 296.1230 - loglik: -2.9294e+02 - logprior: -3.1862e+00
Epoch 2/10
19/19 - 1s - loss: 293.2340 - loglik: -2.9190e+02 - logprior: -1.3314e+00
Epoch 3/10
19/19 - 1s - loss: 292.5386 - loglik: -2.9133e+02 - logprior: -1.2114e+00
Epoch 4/10
19/19 - 1s - loss: 291.7367 - loglik: -2.9058e+02 - logprior: -1.1537e+00
Epoch 5/10
19/19 - 1s - loss: 291.5513 - loglik: -2.9042e+02 - logprior: -1.1319e+00
Epoch 6/10
19/19 - 1s - loss: 291.0416 - loglik: -2.8993e+02 - logprior: -1.1149e+00
Epoch 7/10
19/19 - 1s - loss: 291.1145 - loglik: -2.9003e+02 - logprior: -1.0845e+00
Fitted a model with MAP estimate = -290.9330
Time for alignment: 41.7398
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 365.1974 - loglik: -3.6199e+02 - logprior: -3.2047e+00
Epoch 2/10
19/19 - 1s - loss: 321.7689 - loglik: -3.2042e+02 - logprior: -1.3463e+00
Epoch 3/10
19/19 - 1s - loss: 305.7896 - loglik: -3.0444e+02 - logprior: -1.3515e+00
Epoch 4/10
19/19 - 1s - loss: 303.5579 - loglik: -3.0220e+02 - logprior: -1.3574e+00
Epoch 5/10
19/19 - 1s - loss: 302.7695 - loglik: -3.0147e+02 - logprior: -1.2972e+00
Epoch 6/10
19/19 - 1s - loss: 302.1947 - loglik: -3.0092e+02 - logprior: -1.2731e+00
Epoch 7/10
19/19 - 1s - loss: 302.2221 - loglik: -3.0096e+02 - logprior: -1.2630e+00
Fitted a model with MAP estimate = -301.9502
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 302.3501 - loglik: -2.9825e+02 - logprior: -4.1033e+00
Epoch 2/2
19/19 - 1s - loss: 293.7667 - loglik: -2.9254e+02 - logprior: -1.2314e+00
Fitted a model with MAP estimate = -292.6581
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 298.1059 - loglik: -2.9407e+02 - logprior: -4.0355e+00
Epoch 2/2
19/19 - 1s - loss: 293.9604 - loglik: -2.9253e+02 - logprior: -1.4255e+00
Fitted a model with MAP estimate = -292.9490
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 296.1436 - loglik: -2.9296e+02 - logprior: -3.1810e+00
Epoch 2/10
19/19 - 1s - loss: 293.4171 - loglik: -2.9208e+02 - logprior: -1.3328e+00
Epoch 3/10
19/19 - 1s - loss: 292.3285 - loglik: -2.9112e+02 - logprior: -1.2106e+00
Epoch 4/10
19/19 - 1s - loss: 291.8658 - loglik: -2.9070e+02 - logprior: -1.1642e+00
Epoch 5/10
19/19 - 1s - loss: 291.6935 - loglik: -2.9056e+02 - logprior: -1.1330e+00
Epoch 6/10
19/19 - 1s - loss: 290.8839 - loglik: -2.8978e+02 - logprior: -1.1065e+00
Epoch 7/10
19/19 - 1s - loss: 291.3154 - loglik: -2.9022e+02 - logprior: -1.0938e+00
Fitted a model with MAP estimate = -290.8986
Time for alignment: 40.1541
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 365.1353 - loglik: -3.6193e+02 - logprior: -3.2018e+00
Epoch 2/10
19/19 - 1s - loss: 321.3471 - loglik: -3.2001e+02 - logprior: -1.3359e+00
Epoch 3/10
19/19 - 1s - loss: 306.2830 - loglik: -3.0494e+02 - logprior: -1.3408e+00
Epoch 4/10
19/19 - 1s - loss: 303.8844 - loglik: -3.0253e+02 - logprior: -1.3511e+00
Epoch 5/10
19/19 - 1s - loss: 302.7255 - loglik: -3.0142e+02 - logprior: -1.3030e+00
Epoch 6/10
19/19 - 1s - loss: 302.3727 - loglik: -3.0110e+02 - logprior: -1.2695e+00
Epoch 7/10
19/19 - 1s - loss: 302.2841 - loglik: -3.0103e+02 - logprior: -1.2563e+00
Epoch 8/10
19/19 - 1s - loss: 302.3065 - loglik: -3.0106e+02 - logprior: -1.2472e+00
Fitted a model with MAP estimate = -301.9960
expansions: [(0, 2), (8, 1), (15, 1), (20, 1), (27, 1), (28, 3), (29, 2), (30, 1), (31, 1), (39, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 302.3829 - loglik: -2.9825e+02 - logprior: -4.1375e+00
Epoch 2/2
19/19 - 1s - loss: 293.8713 - loglik: -2.9265e+02 - logprior: -1.2221e+00
Fitted a model with MAP estimate = -292.6726
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 298.1423 - loglik: -2.9410e+02 - logprior: -4.0404e+00
Epoch 2/2
19/19 - 1s - loss: 293.9087 - loglik: -2.9248e+02 - logprior: -1.4291e+00
Fitted a model with MAP estimate = -292.9116
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 296.2362 - loglik: -2.9305e+02 - logprior: -3.1852e+00
Epoch 2/10
19/19 - 1s - loss: 293.0343 - loglik: -2.9170e+02 - logprior: -1.3297e+00
Epoch 3/10
19/19 - 1s - loss: 292.4006 - loglik: -2.9120e+02 - logprior: -1.2053e+00
Epoch 4/10
19/19 - 1s - loss: 292.0249 - loglik: -2.9086e+02 - logprior: -1.1631e+00
Epoch 5/10
19/19 - 1s - loss: 291.7396 - loglik: -2.9061e+02 - logprior: -1.1306e+00
Epoch 6/10
19/19 - 1s - loss: 290.9946 - loglik: -2.8988e+02 - logprior: -1.1103e+00
Epoch 7/10
19/19 - 1s - loss: 291.1451 - loglik: -2.9005e+02 - logprior: -1.0907e+00
Fitted a model with MAP estimate = -290.9063
Time for alignment: 40.5617
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 364.9823 - loglik: -3.6178e+02 - logprior: -3.2066e+00
Epoch 2/10
19/19 - 1s - loss: 321.8626 - loglik: -3.2053e+02 - logprior: -1.3291e+00
Epoch 3/10
19/19 - 1s - loss: 305.7334 - loglik: -3.0439e+02 - logprior: -1.3449e+00
Epoch 4/10
19/19 - 1s - loss: 303.6105 - loglik: -3.0227e+02 - logprior: -1.3406e+00
Epoch 5/10
19/19 - 1s - loss: 302.5429 - loglik: -3.0126e+02 - logprior: -1.2841e+00
Epoch 6/10
19/19 - 1s - loss: 302.5340 - loglik: -3.0127e+02 - logprior: -1.2602e+00
Epoch 7/10
19/19 - 1s - loss: 301.9856 - loglik: -3.0074e+02 - logprior: -1.2499e+00
Epoch 8/10
19/19 - 1s - loss: 301.9751 - loglik: -3.0073e+02 - logprior: -1.2413e+00
Epoch 9/10
19/19 - 1s - loss: 301.8471 - loglik: -3.0061e+02 - logprior: -1.2348e+00
Epoch 10/10
19/19 - 1s - loss: 301.7911 - loglik: -3.0056e+02 - logprior: -1.2323e+00
Fitted a model with MAP estimate = -301.7425
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 3), (29, 2), (30, 1), (31, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 302.8365 - loglik: -2.9863e+02 - logprior: -4.2025e+00
Epoch 2/2
19/19 - 1s - loss: 293.9186 - loglik: -2.9270e+02 - logprior: -1.2215e+00
Fitted a model with MAP estimate = -292.6266
expansions: []
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 298.0096 - loglik: -2.9397e+02 - logprior: -4.0428e+00
Epoch 2/2
19/19 - 1s - loss: 293.9144 - loglik: -2.9250e+02 - logprior: -1.4170e+00
Fitted a model with MAP estimate = -292.9538
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 296.2947 - loglik: -2.9311e+02 - logprior: -3.1804e+00
Epoch 2/10
19/19 - 1s - loss: 293.1888 - loglik: -2.9187e+02 - logprior: -1.3226e+00
Epoch 3/10
19/19 - 1s - loss: 292.4272 - loglik: -2.9122e+02 - logprior: -1.2027e+00
Epoch 4/10
19/19 - 1s - loss: 292.0367 - loglik: -2.9087e+02 - logprior: -1.1672e+00
Epoch 5/10
19/19 - 1s - loss: 291.6708 - loglik: -2.9055e+02 - logprior: -1.1257e+00
Epoch 6/10
19/19 - 1s - loss: 291.2343 - loglik: -2.9012e+02 - logprior: -1.1094e+00
Epoch 7/10
19/19 - 1s - loss: 291.0793 - loglik: -2.8998e+02 - logprior: -1.0953e+00
Epoch 8/10
19/19 - 1s - loss: 290.9493 - loglik: -2.8988e+02 - logprior: -1.0699e+00
Epoch 9/10
19/19 - 1s - loss: 291.1214 - loglik: -2.9006e+02 - logprior: -1.0620e+00
Fitted a model with MAP estimate = -290.8720
Time for alignment: 45.3451
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 364.9526 - loglik: -3.6174e+02 - logprior: -3.2083e+00
Epoch 2/10
19/19 - 1s - loss: 321.5015 - loglik: -3.2017e+02 - logprior: -1.3340e+00
Epoch 3/10
19/19 - 1s - loss: 305.4103 - loglik: -3.0405e+02 - logprior: -1.3651e+00
Epoch 4/10
19/19 - 1s - loss: 302.9496 - loglik: -3.0158e+02 - logprior: -1.3712e+00
Epoch 5/10
19/19 - 1s - loss: 302.2330 - loglik: -3.0091e+02 - logprior: -1.3265e+00
Epoch 6/10
19/19 - 1s - loss: 301.7445 - loglik: -3.0044e+02 - logprior: -1.3022e+00
Epoch 7/10
19/19 - 1s - loss: 301.4984 - loglik: -3.0021e+02 - logprior: -1.2924e+00
Epoch 8/10
19/19 - 1s - loss: 301.4718 - loglik: -3.0019e+02 - logprior: -1.2821e+00
Epoch 9/10
19/19 - 1s - loss: 301.2641 - loglik: -2.9999e+02 - logprior: -1.2785e+00
Epoch 10/10
19/19 - 1s - loss: 301.7065 - loglik: -3.0043e+02 - logprior: -1.2755e+00
Fitted a model with MAP estimate = -301.2302
expansions: [(0, 2), (8, 1), (15, 1), (19, 2), (28, 2), (29, 1), (30, 1), (31, 1), (32, 1), (35, 1), (36, 1), (40, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 68 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 303.0356 - loglik: -2.9882e+02 - logprior: -4.2167e+00
Epoch 2/2
19/19 - 1s - loss: 293.9558 - loglik: -2.9273e+02 - logprior: -1.2213e+00
Fitted a model with MAP estimate = -292.4892
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 298.0332 - loglik: -2.9399e+02 - logprior: -4.0397e+00
Epoch 2/2
19/19 - 1s - loss: 293.7590 - loglik: -2.9235e+02 - logprior: -1.4135e+00
Fitted a model with MAP estimate = -292.8611
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 296.0880 - loglik: -2.9290e+02 - logprior: -3.1839e+00
Epoch 2/10
19/19 - 1s - loss: 293.0785 - loglik: -2.9176e+02 - logprior: -1.3224e+00
Epoch 3/10
19/19 - 1s - loss: 292.7269 - loglik: -2.9152e+02 - logprior: -1.2072e+00
Epoch 4/10
19/19 - 1s - loss: 291.7751 - loglik: -2.9062e+02 - logprior: -1.1587e+00
Epoch 5/10
19/19 - 1s - loss: 291.7871 - loglik: -2.9066e+02 - logprior: -1.1279e+00
Fitted a model with MAP estimate = -291.3448
Time for alignment: 41.9750
Computed alignments with likelihoods: ['-290.9330', '-290.8986', '-290.9063', '-290.8720', '-291.3448']
Best model has likelihood: -290.8720  (prior= -1.0238 )
time for generating output: 0.0957
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00313.projection.fasta
SP score = 0.8142414860681114
Training of 5 independent models on file PF00009.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 1214.9780 - loglik: -1.2131e+03 - logprior: -1.8450e+00
Epoch 2/10
39/39 - 15s - loss: 1130.3190 - loglik: -1.1291e+03 - logprior: -1.1739e+00
Epoch 3/10
39/39 - 16s - loss: 1123.2744 - loglik: -1.1222e+03 - logprior: -1.1140e+00
Epoch 4/10
39/39 - 15s - loss: 1122.0757 - loglik: -1.1210e+03 - logprior: -1.0699e+00
Epoch 5/10
39/39 - 16s - loss: 1121.5690 - loglik: -1.1205e+03 - logprior: -1.0692e+00
Epoch 6/10
39/39 - 15s - loss: 1120.6226 - loglik: -1.1196e+03 - logprior: -1.0653e+00
Epoch 7/10
39/39 - 16s - loss: 1120.6523 - loglik: -1.1196e+03 - logprior: -1.0700e+00
Fitted a model with MAP estimate = -1116.7960
expansions: [(25, 1), (31, 1), (33, 2), (56, 1), (75, 3), (81, 1), (104, 1), (106, 1), (107, 1), (128, 1), (138, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1120.4846 - loglik: -1.1185e+03 - logprior: -1.9434e+00
Epoch 2/2
39/39 - 18s - loss: 1115.3076 - loglik: -1.1144e+03 - logprior: -8.7231e-01
Fitted a model with MAP estimate = -1110.2359
expansions: []
discards: [ 0 81]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1120.6365 - loglik: -1.1179e+03 - logprior: -2.6902e+00
Epoch 2/2
39/39 - 18s - loss: 1116.7991 - loglik: -1.1157e+03 - logprior: -1.1358e+00
Fitted a model with MAP estimate = -1111.3059
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1114.3669 - loglik: -1.1127e+03 - logprior: -1.6896e+00
Epoch 2/10
39/39 - 19s - loss: 1110.6543 - loglik: -1.1101e+03 - logprior: -5.9198e-01
Epoch 3/10
39/39 - 18s - loss: 1110.3401 - loglik: -1.1098e+03 - logprior: -4.9077e-01
Epoch 4/10
39/39 - 18s - loss: 1109.8259 - loglik: -1.1094e+03 - logprior: -3.9973e-01
Epoch 5/10
39/39 - 18s - loss: 1107.5052 - loglik: -1.1072e+03 - logprior: -3.2238e-01
Epoch 6/10
39/39 - 18s - loss: 1106.2400 - loglik: -1.1060e+03 - logprior: -2.4749e-01
Epoch 7/10
39/39 - 18s - loss: 1109.2001 - loglik: -1.1090e+03 - logprior: -1.6350e-01
Fitted a model with MAP estimate = -1107.4191
Time for alignment: 392.8567
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 1217.4615 - loglik: -1.2157e+03 - logprior: -1.8001e+00
Epoch 2/10
39/39 - 16s - loss: 1132.3400 - loglik: -1.1312e+03 - logprior: -1.1371e+00
Epoch 3/10
39/39 - 16s - loss: 1124.6702 - loglik: -1.1236e+03 - logprior: -1.0681e+00
Epoch 4/10
39/39 - 16s - loss: 1122.8152 - loglik: -1.1218e+03 - logprior: -1.0135e+00
Epoch 5/10
39/39 - 16s - loss: 1121.5582 - loglik: -1.1205e+03 - logprior: -1.0422e+00
Epoch 6/10
39/39 - 17s - loss: 1120.2559 - loglik: -1.1192e+03 - logprior: -1.0448e+00
Epoch 7/10
39/39 - 17s - loss: 1120.6797 - loglik: -1.1196e+03 - logprior: -1.0423e+00
Fitted a model with MAP estimate = -1116.5793
expansions: [(25, 1), (30, 4), (57, 1), (77, 1), (79, 1), (80, 1), (81, 3), (101, 1), (105, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1118.6194 - loglik: -1.1168e+03 - logprior: -1.8500e+00
Epoch 2/2
39/39 - 18s - loss: 1112.6355 - loglik: -1.1118e+03 - logprior: -8.0459e-01
Fitted a model with MAP estimate = -1108.1140
expansions: []
discards: [ 0 33]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1118.5652 - loglik: -1.1159e+03 - logprior: -2.6343e+00
Epoch 2/2
39/39 - 17s - loss: 1114.7208 - loglik: -1.1137e+03 - logprior: -1.0563e+00
Fitted a model with MAP estimate = -1109.2771
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1111.7721 - loglik: -1.1102e+03 - logprior: -1.6186e+00
Epoch 2/10
39/39 - 18s - loss: 1109.2395 - loglik: -1.1087e+03 - logprior: -5.2475e-01
Epoch 3/10
39/39 - 17s - loss: 1108.0756 - loglik: -1.1076e+03 - logprior: -4.3338e-01
Epoch 4/10
39/39 - 18s - loss: 1106.7913 - loglik: -1.1064e+03 - logprior: -3.4295e-01
Epoch 5/10
39/39 - 18s - loss: 1106.5873 - loglik: -1.1063e+03 - logprior: -2.6863e-01
Epoch 6/10
39/39 - 18s - loss: 1105.4976 - loglik: -1.1053e+03 - logprior: -1.8643e-01
Epoch 7/10
39/39 - 18s - loss: 1106.8334 - loglik: -1.1067e+03 - logprior: -1.1008e-01
Fitted a model with MAP estimate = -1105.4258
Time for alignment: 393.6539
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1216.1521 - loglik: -1.2143e+03 - logprior: -1.8607e+00
Epoch 2/10
39/39 - 16s - loss: 1137.6790 - loglik: -1.1365e+03 - logprior: -1.1350e+00
Epoch 3/10
39/39 - 17s - loss: 1124.9813 - loglik: -1.1239e+03 - logprior: -1.1136e+00
Epoch 4/10
39/39 - 17s - loss: 1121.3702 - loglik: -1.1203e+03 - logprior: -1.1102e+00
Epoch 5/10
39/39 - 16s - loss: 1121.8289 - loglik: -1.1207e+03 - logprior: -1.0971e+00
Fitted a model with MAP estimate = -1117.5485
expansions: [(20, 1), (78, 2), (80, 1), (81, 1), (102, 1), (103, 1), (104, 1), (106, 1), (107, 1), (138, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1120.6281 - loglik: -1.1187e+03 - logprior: -1.8893e+00
Epoch 2/2
39/39 - 18s - loss: 1115.6184 - loglik: -1.1148e+03 - logprior: -8.3346e-01
Fitted a model with MAP estimate = -1110.4733
expansions: [(39, 1)]
discards: [ 0 79]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1120.9113 - loglik: -1.1182e+03 - logprior: -2.6684e+00
Epoch 2/2
39/39 - 18s - loss: 1116.4658 - loglik: -1.1154e+03 - logprior: -1.0780e+00
Fitted a model with MAP estimate = -1111.3199
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1114.0837 - loglik: -1.1124e+03 - logprior: -1.6807e+00
Epoch 2/10
39/39 - 18s - loss: 1111.9998 - loglik: -1.1114e+03 - logprior: -5.7943e-01
Epoch 3/10
39/39 - 18s - loss: 1109.7190 - loglik: -1.1092e+03 - logprior: -4.8407e-01
Epoch 4/10
39/39 - 18s - loss: 1108.5137 - loglik: -1.1081e+03 - logprior: -4.0942e-01
Epoch 5/10
39/39 - 18s - loss: 1109.0079 - loglik: -1.1087e+03 - logprior: -3.3700e-01
Fitted a model with MAP estimate = -1107.8746
Time for alignment: 324.5379
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1214.3511 - loglik: -1.2125e+03 - logprior: -1.8693e+00
Epoch 2/10
39/39 - 17s - loss: 1136.5475 - loglik: -1.1354e+03 - logprior: -1.1232e+00
Epoch 3/10
39/39 - 17s - loss: 1126.7810 - loglik: -1.1257e+03 - logprior: -1.0684e+00
Epoch 4/10
39/39 - 17s - loss: 1122.7681 - loglik: -1.1217e+03 - logprior: -1.0663e+00
Epoch 5/10
39/39 - 17s - loss: 1123.2799 - loglik: -1.1222e+03 - logprior: -1.0522e+00
Fitted a model with MAP estimate = -1118.0810
expansions: [(26, 1), (30, 1), (78, 3), (80, 1), (103, 1), (104, 2), (105, 2), (106, 1), (107, 2), (137, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1120.2656 - loglik: -1.1183e+03 - logprior: -1.9770e+00
Epoch 2/2
39/39 - 18s - loss: 1113.6154 - loglik: -1.1127e+03 - logprior: -9.4623e-01
Fitted a model with MAP estimate = -1109.0925
expansions: []
discards: [  0  81 111 119]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1119.7832 - loglik: -1.1171e+03 - logprior: -2.7204e+00
Epoch 2/2
39/39 - 18s - loss: 1115.2732 - loglik: -1.1142e+03 - logprior: -1.1143e+00
Fitted a model with MAP estimate = -1110.3493
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1113.0126 - loglik: -1.1113e+03 - logprior: -1.7028e+00
Epoch 2/10
39/39 - 17s - loss: 1109.6416 - loglik: -1.1090e+03 - logprior: -6.1053e-01
Epoch 3/10
39/39 - 17s - loss: 1109.2917 - loglik: -1.1088e+03 - logprior: -5.2003e-01
Epoch 4/10
39/39 - 18s - loss: 1108.4406 - loglik: -1.1080e+03 - logprior: -4.4039e-01
Epoch 5/10
39/39 - 18s - loss: 1105.8301 - loglik: -1.1055e+03 - logprior: -3.5755e-01
Epoch 6/10
39/39 - 18s - loss: 1108.2994 - loglik: -1.1080e+03 - logprior: -2.8367e-01
Fitted a model with MAP estimate = -1106.7220
Time for alignment: 348.4505
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1213.7622 - loglik: -1.2119e+03 - logprior: -1.8887e+00
Epoch 2/10
39/39 - 15s - loss: 1127.4316 - loglik: -1.1262e+03 - logprior: -1.2690e+00
Epoch 3/10
39/39 - 15s - loss: 1120.6917 - loglik: -1.1196e+03 - logprior: -1.1411e+00
Epoch 4/10
39/39 - 16s - loss: 1118.9116 - loglik: -1.1178e+03 - logprior: -1.1150e+00
Epoch 5/10
39/39 - 16s - loss: 1117.7614 - loglik: -1.1167e+03 - logprior: -1.1042e+00
Epoch 6/10
39/39 - 16s - loss: 1117.8508 - loglik: -1.1167e+03 - logprior: -1.1030e+00
Fitted a model with MAP estimate = -1113.6113
expansions: [(25, 1), (30, 1), (31, 1), (34, 1), (56, 1), (62, 1), (75, 1), (77, 1), (80, 1), (100, 1), (101, 1), (105, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1118.4373 - loglik: -1.1166e+03 - logprior: -1.8448e+00
Epoch 2/2
39/39 - 18s - loss: 1113.7017 - loglik: -1.1129e+03 - logprior: -7.7736e-01
Fitted a model with MAP estimate = -1109.0644
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1119.3652 - loglik: -1.1167e+03 - logprior: -2.6176e+00
Epoch 2/2
39/39 - 17s - loss: 1114.4843 - loglik: -1.1135e+03 - logprior: -9.9290e-01
Fitted a model with MAP estimate = -1109.8154
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10036 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1112.9695 - loglik: -1.1115e+03 - logprior: -1.4566e+00
Epoch 2/10
39/39 - 18s - loss: 1110.3861 - loglik: -1.1098e+03 - logprior: -5.5409e-01
Epoch 3/10
39/39 - 19s - loss: 1109.1979 - loglik: -1.1088e+03 - logprior: -4.2092e-01
Epoch 4/10
39/39 - 18s - loss: 1108.0784 - loglik: -1.1077e+03 - logprior: -3.3768e-01
Epoch 5/10
39/39 - 17s - loss: 1107.8014 - loglik: -1.1075e+03 - logprior: -2.5973e-01
Epoch 6/10
39/39 - 17s - loss: 1106.1641 - loglik: -1.1060e+03 - logprior: -1.7505e-01
Epoch 7/10
39/39 - 17s - loss: 1106.7766 - loglik: -1.1067e+03 - logprior: -1.0298e-01
Fitted a model with MAP estimate = -1106.6843
Time for alignment: 373.4491
Computed alignments with likelihoods: ['-1107.4191', '-1105.4258', '-1107.8746', '-1106.7220', '-1106.6843']
Best model has likelihood: -1105.4258  (prior= -0.0667 )
time for generating output: 0.2341
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00009.projection.fasta
SP score = 0.7567539795232046
Training of 5 independent models on file PF14604.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 293.0510 - loglik: -2.8984e+02 - logprior: -3.2114e+00
Epoch 2/10
19/19 - 1s - loss: 264.4699 - loglik: -2.6304e+02 - logprior: -1.4294e+00
Epoch 3/10
19/19 - 1s - loss: 254.2356 - loglik: -2.5264e+02 - logprior: -1.5914e+00
Epoch 4/10
19/19 - 1s - loss: 252.3546 - loglik: -2.5091e+02 - logprior: -1.4446e+00
Epoch 5/10
19/19 - 1s - loss: 251.6460 - loglik: -2.5022e+02 - logprior: -1.4300e+00
Epoch 6/10
19/19 - 1s - loss: 251.1537 - loglik: -2.4973e+02 - logprior: -1.4208e+00
Epoch 7/10
19/19 - 1s - loss: 251.0896 - loglik: -2.4968e+02 - logprior: -1.4077e+00
Epoch 8/10
19/19 - 1s - loss: 251.1445 - loglik: -2.4974e+02 - logprior: -1.4001e+00
Fitted a model with MAP estimate = -250.9905
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.8652 - loglik: -2.5374e+02 - logprior: -4.1275e+00
Epoch 2/2
19/19 - 1s - loss: 250.0860 - loglik: -2.4803e+02 - logprior: -2.0524e+00
Fitted a model with MAP estimate = -248.3477
expansions: [(0, 1)]
discards: [ 0  8 17 26 27 44]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 250.9345 - loglik: -2.4768e+02 - logprior: -3.2567e+00
Epoch 2/2
19/19 - 1s - loss: 247.0038 - loglik: -2.4560e+02 - logprior: -1.4029e+00
Fitted a model with MAP estimate = -246.5320
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 249.8256 - loglik: -2.4658e+02 - logprior: -3.2456e+00
Epoch 2/10
19/19 - 1s - loss: 247.1821 - loglik: -2.4581e+02 - logprior: -1.3765e+00
Epoch 3/10
19/19 - 1s - loss: 246.8309 - loglik: -2.4555e+02 - logprior: -1.2804e+00
Epoch 4/10
19/19 - 1s - loss: 246.6678 - loglik: -2.4544e+02 - logprior: -1.2292e+00
Epoch 5/10
19/19 - 1s - loss: 246.3790 - loglik: -2.4519e+02 - logprior: -1.1928e+00
Epoch 6/10
19/19 - 1s - loss: 246.1571 - loglik: -2.4497e+02 - logprior: -1.1829e+00
Epoch 7/10
19/19 - 1s - loss: 246.2433 - loglik: -2.4508e+02 - logprior: -1.1633e+00
Fitted a model with MAP estimate = -246.1394
Time for alignment: 38.3558
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 293.0679 - loglik: -2.8985e+02 - logprior: -3.2167e+00
Epoch 2/10
19/19 - 1s - loss: 264.0762 - loglik: -2.6265e+02 - logprior: -1.4292e+00
Epoch 3/10
19/19 - 1s - loss: 254.2545 - loglik: -2.5265e+02 - logprior: -1.6005e+00
Epoch 4/10
19/19 - 1s - loss: 252.2639 - loglik: -2.5081e+02 - logprior: -1.4506e+00
Epoch 5/10
19/19 - 1s - loss: 251.3962 - loglik: -2.4996e+02 - logprior: -1.4347e+00
Epoch 6/10
19/19 - 1s - loss: 251.2240 - loglik: -2.4980e+02 - logprior: -1.4260e+00
Epoch 7/10
19/19 - 1s - loss: 251.0881 - loglik: -2.4968e+02 - logprior: -1.4125e+00
Epoch 8/10
19/19 - 1s - loss: 251.2572 - loglik: -2.4985e+02 - logprior: -1.4041e+00
Fitted a model with MAP estimate = -250.9911
expansions: [(6, 1), (7, 2), (8, 1), (13, 1), (18, 1), (20, 2), (21, 2), (26, 1), (28, 2), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.2365 - loglik: -2.5312e+02 - logprior: -4.1203e+00
Epoch 2/2
19/19 - 1s - loss: 249.7923 - loglik: -2.4778e+02 - logprior: -2.0119e+00
Fitted a model with MAP estimate = -248.2395
expansions: [(0, 1)]
discards: [ 0  8 25 26 43]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 250.5663 - loglik: -2.4731e+02 - logprior: -3.2570e+00
Epoch 2/2
19/19 - 1s - loss: 246.9526 - loglik: -2.4554e+02 - logprior: -1.4093e+00
Fitted a model with MAP estimate = -246.4761
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 249.8229 - loglik: -2.4657e+02 - logprior: -3.2512e+00
Epoch 2/10
19/19 - 1s - loss: 247.0853 - loglik: -2.4570e+02 - logprior: -1.3832e+00
Epoch 3/10
19/19 - 1s - loss: 246.8589 - loglik: -2.4558e+02 - logprior: -1.2824e+00
Epoch 4/10
19/19 - 1s - loss: 246.6070 - loglik: -2.4537e+02 - logprior: -1.2333e+00
Epoch 5/10
19/19 - 1s - loss: 246.1803 - loglik: -2.4498e+02 - logprior: -1.2010e+00
Epoch 6/10
19/19 - 1s - loss: 246.3451 - loglik: -2.4516e+02 - logprior: -1.1807e+00
Fitted a model with MAP estimate = -246.1689
Time for alignment: 36.8666
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 293.0325 - loglik: -2.8982e+02 - logprior: -3.2114e+00
Epoch 2/10
19/19 - 1s - loss: 264.3583 - loglik: -2.6293e+02 - logprior: -1.4294e+00
Epoch 3/10
19/19 - 1s - loss: 255.0710 - loglik: -2.5348e+02 - logprior: -1.5881e+00
Epoch 4/10
19/19 - 1s - loss: 253.1101 - loglik: -2.5167e+02 - logprior: -1.4420e+00
Epoch 5/10
19/19 - 1s - loss: 252.2411 - loglik: -2.5081e+02 - logprior: -1.4283e+00
Epoch 6/10
19/19 - 1s - loss: 252.0150 - loglik: -2.5060e+02 - logprior: -1.4143e+00
Epoch 7/10
19/19 - 1s - loss: 251.9167 - loglik: -2.5052e+02 - logprior: -1.3998e+00
Epoch 8/10
19/19 - 1s - loss: 251.8144 - loglik: -2.5042e+02 - logprior: -1.3912e+00
Epoch 9/10
19/19 - 1s - loss: 252.0222 - loglik: -2.5064e+02 - logprior: -1.3828e+00
Fitted a model with MAP estimate = -251.6951
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 1), (27, 1), (28, 3), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 258.0223 - loglik: -2.5389e+02 - logprior: -4.1299e+00
Epoch 2/2
19/19 - 1s - loss: 249.9007 - loglik: -2.4786e+02 - logprior: -2.0417e+00
Fitted a model with MAP estimate = -248.4720
expansions: [(0, 1)]
discards: [ 0  8 17 26 38 43]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 251.3240 - loglik: -2.4807e+02 - logprior: -3.2498e+00
Epoch 2/2
19/19 - 1s - loss: 247.5104 - loglik: -2.4611e+02 - logprior: -1.3991e+00
Fitted a model with MAP estimate = -246.9876
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 249.7178 - loglik: -2.4646e+02 - logprior: -3.2574e+00
Epoch 2/10
19/19 - 1s - loss: 247.1380 - loglik: -2.4575e+02 - logprior: -1.3878e+00
Epoch 3/10
19/19 - 1s - loss: 246.7990 - loglik: -2.4551e+02 - logprior: -1.2923e+00
Epoch 4/10
19/19 - 1s - loss: 246.4197 - loglik: -2.4518e+02 - logprior: -1.2419e+00
Epoch 5/10
19/19 - 1s - loss: 246.4541 - loglik: -2.4525e+02 - logprior: -1.2086e+00
Fitted a model with MAP estimate = -246.2335
Time for alignment: 36.9552
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 293.0592 - loglik: -2.8984e+02 - logprior: -3.2170e+00
Epoch 2/10
19/19 - 1s - loss: 264.2623 - loglik: -2.6283e+02 - logprior: -1.4334e+00
Epoch 3/10
19/19 - 1s - loss: 255.0567 - loglik: -2.5347e+02 - logprior: -1.5861e+00
Epoch 4/10
19/19 - 1s - loss: 252.8226 - loglik: -2.5139e+02 - logprior: -1.4284e+00
Epoch 5/10
19/19 - 1s - loss: 252.2678 - loglik: -2.5086e+02 - logprior: -1.4082e+00
Epoch 6/10
19/19 - 1s - loss: 252.0851 - loglik: -2.5070e+02 - logprior: -1.3898e+00
Epoch 7/10
19/19 - 1s - loss: 251.7134 - loglik: -2.5034e+02 - logprior: -1.3741e+00
Epoch 8/10
19/19 - 1s - loss: 251.8809 - loglik: -2.5052e+02 - logprior: -1.3645e+00
Fitted a model with MAP estimate = -251.6993
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 3), (28, 3), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.9173 - loglik: -2.5378e+02 - logprior: -4.1380e+00
Epoch 2/2
19/19 - 1s - loss: 250.1162 - loglik: -2.4806e+02 - logprior: -2.0572e+00
Fitted a model with MAP estimate = -248.5040
expansions: [(0, 1)]
discards: [ 0  8 17 26 29 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 251.6316 - loglik: -2.4838e+02 - logprior: -3.2512e+00
Epoch 2/2
19/19 - 1s - loss: 247.5367 - loglik: -2.4614e+02 - logprior: -1.3956e+00
Fitted a model with MAP estimate = -247.0283
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 249.7489 - loglik: -2.4650e+02 - logprior: -3.2471e+00
Epoch 2/10
19/19 - 1s - loss: 247.2385 - loglik: -2.4586e+02 - logprior: -1.3818e+00
Epoch 3/10
19/19 - 1s - loss: 246.8951 - loglik: -2.4561e+02 - logprior: -1.2861e+00
Epoch 4/10
19/19 - 1s - loss: 246.5852 - loglik: -2.4535e+02 - logprior: -1.2373e+00
Epoch 5/10
19/19 - 1s - loss: 246.1309 - loglik: -2.4493e+02 - logprior: -1.2017e+00
Epoch 6/10
19/19 - 1s - loss: 246.3446 - loglik: -2.4516e+02 - logprior: -1.1865e+00
Fitted a model with MAP estimate = -246.1690
Time for alignment: 36.9579
Fitting a model of length 40 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 293.0179 - loglik: -2.8980e+02 - logprior: -3.2144e+00
Epoch 2/10
19/19 - 1s - loss: 264.1285 - loglik: -2.6270e+02 - logprior: -1.4329e+00
Epoch 3/10
19/19 - 1s - loss: 254.8446 - loglik: -2.5325e+02 - logprior: -1.5914e+00
Epoch 4/10
19/19 - 1s - loss: 253.0449 - loglik: -2.5160e+02 - logprior: -1.4443e+00
Epoch 5/10
19/19 - 1s - loss: 252.2059 - loglik: -2.5077e+02 - logprior: -1.4319e+00
Epoch 6/10
19/19 - 1s - loss: 252.0544 - loglik: -2.5064e+02 - logprior: -1.4167e+00
Epoch 7/10
19/19 - 1s - loss: 252.0116 - loglik: -2.5061e+02 - logprior: -1.4006e+00
Epoch 8/10
19/19 - 1s - loss: 251.7733 - loglik: -2.5038e+02 - logprior: -1.3925e+00
Epoch 9/10
19/19 - 1s - loss: 251.6412 - loglik: -2.5026e+02 - logprior: -1.3836e+00
Epoch 10/10
19/19 - 1s - loss: 251.9613 - loglik: -2.5058e+02 - logprior: -1.3792e+00
Fitted a model with MAP estimate = -251.6470
expansions: [(6, 1), (7, 2), (8, 1), (13, 2), (20, 2), (21, 1), (27, 1), (28, 3), (31, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.9202 - loglik: -2.5379e+02 - logprior: -4.1254e+00
Epoch 2/2
19/19 - 1s - loss: 250.1836 - loglik: -2.4813e+02 - logprior: -2.0540e+00
Fitted a model with MAP estimate = -248.5262
expansions: [(0, 1)]
discards: [ 0  8 17 26 38 43]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 251.3548 - loglik: -2.4811e+02 - logprior: -3.2442e+00
Epoch 2/2
19/19 - 1s - loss: 247.4721 - loglik: -2.4607e+02 - logprior: -1.3991e+00
Fitted a model with MAP estimate = -246.9954
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 10008 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 249.7127 - loglik: -2.4645e+02 - logprior: -3.2594e+00
Epoch 2/10
19/19 - 1s - loss: 247.1810 - loglik: -2.4579e+02 - logprior: -1.3909e+00
Epoch 3/10
19/19 - 1s - loss: 246.7887 - loglik: -2.4550e+02 - logprior: -1.2903e+00
Epoch 4/10
19/19 - 1s - loss: 246.4520 - loglik: -2.4521e+02 - logprior: -1.2401e+00
Epoch 5/10
19/19 - 1s - loss: 246.4113 - loglik: -2.4520e+02 - logprior: -1.2085e+00
Epoch 6/10
19/19 - 1s - loss: 246.3252 - loglik: -2.4514e+02 - logprior: -1.1887e+00
Epoch 7/10
19/19 - 1s - loss: 246.3408 - loglik: -2.4517e+02 - logprior: -1.1731e+00
Fitted a model with MAP estimate = -246.1153
Time for alignment: 39.9730
Computed alignments with likelihoods: ['-246.1394', '-246.1689', '-246.2335', '-246.1690', '-246.1153']
Best model has likelihood: -246.1153  (prior= -1.1603 )
time for generating output: 0.1156
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF14604.projection.fasta
SP score = 0.7714915503306392
Training of 5 independent models on file PF00625.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 941.0798 - loglik: -9.3825e+02 - logprior: -2.8326e+00
Epoch 2/10
19/19 - 5s - loss: 861.3270 - loglik: -8.6002e+02 - logprior: -1.3106e+00
Epoch 3/10
19/19 - 5s - loss: 828.5836 - loglik: -8.2690e+02 - logprior: -1.6793e+00
Epoch 4/10
19/19 - 6s - loss: 823.1245 - loglik: -8.2138e+02 - logprior: -1.7424e+00
Epoch 5/10
19/19 - 6s - loss: 820.3867 - loglik: -8.1877e+02 - logprior: -1.6203e+00
Epoch 6/10
19/19 - 6s - loss: 819.2314 - loglik: -8.1764e+02 - logprior: -1.5885e+00
Epoch 7/10
19/19 - 6s - loss: 818.8382 - loglik: -8.1726e+02 - logprior: -1.5737e+00
Epoch 8/10
19/19 - 6s - loss: 819.3645 - loglik: -8.1778e+02 - logprior: -1.5815e+00
Fitted a model with MAP estimate = -810.9168
expansions: [(0, 2), (14, 1), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (22, 1), (23, 1), (43, 2), (44, 1), (48, 1), (49, 1), (50, 1), (51, 1), (56, 1), (65, 2), (66, 1), (68, 1), (71, 1), (73, 1), (87, 1), (88, 1), (89, 2), (99, 1), (100, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 806.2761 - loglik: -8.0392e+02 - logprior: -2.3602e+00
Epoch 2/2
39/39 - 11s - loss: 795.2069 - loglik: -7.9428e+02 - logprior: -9.2282e-01
Fitted a model with MAP estimate = -786.5315
expansions: []
discards: [  0  85 116]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 799.9551 - loglik: -7.9723e+02 - logprior: -2.7287e+00
Epoch 2/2
39/39 - 10s - loss: 795.2521 - loglik: -7.9450e+02 - logprior: -7.5387e-01
Fitted a model with MAP estimate = -786.5358
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 185 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 790.3810 - loglik: -7.8874e+02 - logprior: -1.6441e+00
Epoch 2/10
39/39 - 10s - loss: 787.3943 - loglik: -7.8690e+02 - logprior: -4.9752e-01
Epoch 3/10
39/39 - 10s - loss: 785.7166 - loglik: -7.8532e+02 - logprior: -3.9426e-01
Epoch 4/10
39/39 - 10s - loss: 785.3592 - loglik: -7.8503e+02 - logprior: -3.3266e-01
Epoch 5/10
39/39 - 11s - loss: 783.9840 - loglik: -7.8372e+02 - logprior: -2.6299e-01
Epoch 6/10
39/39 - 10s - loss: 783.4327 - loglik: -7.8325e+02 - logprior: -1.8648e-01
Epoch 7/10
39/39 - 10s - loss: 784.4996 - loglik: -7.8438e+02 - logprior: -1.1522e-01
Fitted a model with MAP estimate = -783.2258
Time for alignment: 217.8623
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 941.0671 - loglik: -9.3822e+02 - logprior: -2.8427e+00
Epoch 2/10
19/19 - 7s - loss: 862.1721 - loglik: -8.6085e+02 - logprior: -1.3241e+00
Epoch 3/10
19/19 - 7s - loss: 828.6315 - loglik: -8.2695e+02 - logprior: -1.6829e+00
Epoch 4/10
19/19 - 7s - loss: 822.0030 - loglik: -8.2022e+02 - logprior: -1.7795e+00
Epoch 5/10
19/19 - 7s - loss: 820.9441 - loglik: -8.1928e+02 - logprior: -1.6673e+00
Epoch 6/10
19/19 - 7s - loss: 820.4976 - loglik: -8.1889e+02 - logprior: -1.6091e+00
Epoch 7/10
19/19 - 7s - loss: 817.7711 - loglik: -8.1617e+02 - logprior: -1.6006e+00
Epoch 8/10
19/19 - 7s - loss: 818.7581 - loglik: -8.1719e+02 - logprior: -1.5689e+00
Fitted a model with MAP estimate = -810.6989
expansions: [(0, 2), (14, 1), (16, 2), (17, 1), (18, 2), (21, 1), (22, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (58, 1), (65, 2), (66, 1), (68, 1), (71, 2), (80, 1), (87, 1), (88, 1), (89, 2), (99, 1), (100, 1), (105, 1), (113, 1), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 807.8696 - loglik: -8.0548e+02 - logprior: -2.3920e+00
Epoch 2/2
39/39 - 11s - loss: 795.7632 - loglik: -7.9479e+02 - logprior: -9.7030e-01
Fitted a model with MAP estimate = -787.1004
expansions: []
discards: [  0  55  84  94 117]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 800.7813 - loglik: -7.9802e+02 - logprior: -2.7603e+00
Epoch 2/2
39/39 - 9s - loss: 796.2489 - loglik: -7.9549e+02 - logprior: -7.6108e-01
Fitted a model with MAP estimate = -787.4134
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 791.4022 - loglik: -7.8977e+02 - logprior: -1.6356e+00
Epoch 2/10
39/39 - 9s - loss: 787.8065 - loglik: -7.8732e+02 - logprior: -4.8642e-01
Epoch 3/10
39/39 - 9s - loss: 786.6517 - loglik: -7.8627e+02 - logprior: -3.8655e-01
Epoch 4/10
39/39 - 10s - loss: 785.7180 - loglik: -7.8540e+02 - logprior: -3.1890e-01
Epoch 5/10
39/39 - 10s - loss: 786.2178 - loglik: -7.8595e+02 - logprior: -2.6877e-01
Fitted a model with MAP estimate = -784.3972
Time for alignment: 202.6974
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 940.7997 - loglik: -9.3797e+02 - logprior: -2.8286e+00
Epoch 2/10
19/19 - 6s - loss: 864.4695 - loglik: -8.6317e+02 - logprior: -1.2963e+00
Epoch 3/10
19/19 - 6s - loss: 831.9324 - loglik: -8.3026e+02 - logprior: -1.6699e+00
Epoch 4/10
19/19 - 6s - loss: 824.8284 - loglik: -8.2308e+02 - logprior: -1.7524e+00
Epoch 5/10
19/19 - 6s - loss: 822.1832 - loglik: -8.2052e+02 - logprior: -1.6625e+00
Epoch 6/10
19/19 - 6s - loss: 822.6994 - loglik: -8.2108e+02 - logprior: -1.6191e+00
Fitted a model with MAP estimate = -813.7292
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (22, 1), (30, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (58, 1), (65, 2), (66, 1), (68, 1), (71, 2), (80, 1), (87, 2), (88, 1), (89, 1), (99, 1), (100, 1), (111, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 809.1678 - loglik: -8.0686e+02 - logprior: -2.3054e+00
Epoch 2/2
39/39 - 10s - loss: 797.4214 - loglik: -7.9646e+02 - logprior: -9.6450e-01
Fitted a model with MAP estimate = -788.7610
expansions: [(28, 1)]
discards: [  0  54  83  93 114]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 802.7858 - loglik: -8.0003e+02 - logprior: -2.7581e+00
Epoch 2/2
39/39 - 9s - loss: 797.8615 - loglik: -7.9710e+02 - logprior: -7.6378e-01
Fitted a model with MAP estimate = -789.0855
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 792.9985 - loglik: -7.9134e+02 - logprior: -1.6589e+00
Epoch 2/10
39/39 - 9s - loss: 789.6401 - loglik: -7.8912e+02 - logprior: -5.1873e-01
Epoch 3/10
39/39 - 9s - loss: 788.9214 - loglik: -7.8850e+02 - logprior: -4.1967e-01
Epoch 4/10
39/39 - 9s - loss: 786.3622 - loglik: -7.8599e+02 - logprior: -3.6756e-01
Epoch 5/10
39/39 - 9s - loss: 787.5391 - loglik: -7.8724e+02 - logprior: -2.9958e-01
Fitted a model with MAP estimate = -786.0210
Time for alignment: 181.0881
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 940.9056 - loglik: -9.3806e+02 - logprior: -2.8480e+00
Epoch 2/10
19/19 - 6s - loss: 863.4592 - loglik: -8.6213e+02 - logprior: -1.3265e+00
Epoch 3/10
19/19 - 6s - loss: 830.5945 - loglik: -8.2896e+02 - logprior: -1.6386e+00
Epoch 4/10
19/19 - 6s - loss: 825.7736 - loglik: -8.2405e+02 - logprior: -1.7224e+00
Epoch 5/10
19/19 - 7s - loss: 822.3672 - loglik: -8.2074e+02 - logprior: -1.6248e+00
Epoch 6/10
19/19 - 7s - loss: 820.3857 - loglik: -8.1881e+02 - logprior: -1.5764e+00
Epoch 7/10
19/19 - 7s - loss: 821.1156 - loglik: -8.1956e+02 - logprior: -1.5556e+00
Fitted a model with MAP estimate = -813.0518
expansions: [(0, 2), (11, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (21, 1), (23, 1), (36, 1), (43, 2), (44, 1), (48, 1), (50, 1), (52, 1), (57, 1), (66, 2), (67, 1), (69, 1), (70, 1), (71, 1), (87, 2), (88, 2), (89, 1), (90, 1), (100, 1), (112, 1), (113, 2), (114, 2), (115, 1), (116, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 188 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 808.5828 - loglik: -8.0624e+02 - logprior: -2.3395e+00
Epoch 2/2
39/39 - 11s - loss: 796.7786 - loglik: -7.9581e+02 - logprior: -9.6738e-01
Fitted a model with MAP estimate = -788.0059
expansions: []
discards: [  0  55  85 114 146]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 802.4375 - loglik: -7.9973e+02 - logprior: -2.7091e+00
Epoch 2/2
39/39 - 10s - loss: 797.4111 - loglik: -7.9666e+02 - logprior: -7.5587e-01
Fitted a model with MAP estimate = -788.5257
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 792.5535 - loglik: -7.9093e+02 - logprior: -1.6273e+00
Epoch 2/10
39/39 - 10s - loss: 789.6182 - loglik: -7.8913e+02 - logprior: -4.8739e-01
Epoch 3/10
39/39 - 10s - loss: 788.0199 - loglik: -7.8763e+02 - logprior: -3.9333e-01
Epoch 4/10
39/39 - 10s - loss: 787.4450 - loglik: -7.8712e+02 - logprior: -3.2879e-01
Epoch 5/10
39/39 - 10s - loss: 785.2564 - loglik: -7.8500e+02 - logprior: -2.5480e-01
Epoch 6/10
39/39 - 10s - loss: 786.3187 - loglik: -7.8612e+02 - logprior: -1.9429e-01
Fitted a model with MAP estimate = -785.3568
Time for alignment: 205.6568
Fitting a model of length 144 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 940.2572 - loglik: -9.3742e+02 - logprior: -2.8411e+00
Epoch 2/10
19/19 - 7s - loss: 865.4851 - loglik: -8.6415e+02 - logprior: -1.3321e+00
Epoch 3/10
19/19 - 7s - loss: 832.6639 - loglik: -8.3097e+02 - logprior: -1.6913e+00
Epoch 4/10
19/19 - 7s - loss: 824.9275 - loglik: -8.2318e+02 - logprior: -1.7450e+00
Epoch 5/10
19/19 - 7s - loss: 821.8345 - loglik: -8.2018e+02 - logprior: -1.6551e+00
Epoch 6/10
19/19 - 7s - loss: 821.3123 - loglik: -8.1970e+02 - logprior: -1.6100e+00
Epoch 7/10
19/19 - 7s - loss: 820.9346 - loglik: -8.1936e+02 - logprior: -1.5791e+00
Epoch 8/10
19/19 - 7s - loss: 821.2073 - loglik: -8.1965e+02 - logprior: -1.5619e+00
Fitted a model with MAP estimate = -812.7992
expansions: [(0, 2), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (22, 1), (30, 1), (38, 1), (44, 1), (48, 1), (50, 2), (51, 1), (57, 1), (66, 1), (67, 1), (69, 1), (70, 1), (71, 1), (80, 2), (87, 1), (88, 1), (89, 2), (91, 1), (100, 1), (105, 1), (113, 1), (114, 2), (115, 1), (116, 1), (118, 1), (122, 1), (123, 1), (135, 1), (137, 1), (138, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 808.0974 - loglik: -8.0576e+02 - logprior: -2.3418e+00
Epoch 2/2
39/39 - 10s - loss: 796.4852 - loglik: -7.9557e+02 - logprior: -9.1103e-01
Fitted a model with MAP estimate = -787.8666
expansions: [(28, 1)]
discards: [  0  64 102 115]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 801.2148 - loglik: -7.9850e+02 - logprior: -2.7139e+00
Epoch 2/2
39/39 - 9s - loss: 796.1248 - loglik: -7.9538e+02 - logprior: -7.4734e-01
Fitted a model with MAP estimate = -787.4934
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 184 on 10140 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 791.0602 - loglik: -7.8944e+02 - logprior: -1.6155e+00
Epoch 2/10
39/39 - 10s - loss: 788.0632 - loglik: -7.8758e+02 - logprior: -4.8272e-01
Epoch 3/10
39/39 - 9s - loss: 787.1373 - loglik: -7.8675e+02 - logprior: -3.9220e-01
Epoch 4/10
39/39 - 9s - loss: 785.6722 - loglik: -7.8535e+02 - logprior: -3.1732e-01
Epoch 5/10
39/39 - 9s - loss: 786.2021 - loglik: -7.8594e+02 - logprior: -2.6202e-01
Fitted a model with MAP estimate = -784.4707
Time for alignment: 200.6702
Computed alignments with likelihoods: ['-783.2258', '-784.3972', '-786.0210', '-785.3568', '-784.4707']
Best model has likelihood: -783.2258  (prior= -0.0638 )
time for generating output: 0.3708
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00625.projection.fasta
SP score = 0.4111586538579629
Training of 5 independent models on file PF00476.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 52s - loss: 1954.3109 - loglik: -1.9529e+03 - logprior: -1.3687e+00
Epoch 2/10
39/39 - 50s - loss: 1773.2378 - loglik: -1.7715e+03 - logprior: -1.6950e+00
Epoch 3/10
39/39 - 52s - loss: 1759.5641 - loglik: -1.7579e+03 - logprior: -1.7105e+00
Epoch 4/10
39/39 - 56s - loss: 1753.6925 - loglik: -1.7520e+03 - logprior: -1.6719e+00
Epoch 5/10
39/39 - 56s - loss: 1752.1912 - loglik: -1.7505e+03 - logprior: -1.7062e+00
Epoch 6/10
39/39 - 58s - loss: 1748.9812 - loglik: -1.7473e+03 - logprior: -1.7250e+00
Epoch 7/10
39/39 - 57s - loss: 1749.5397 - loglik: -1.7477e+03 - logprior: -1.8509e+00
Fitted a model with MAP estimate = -1747.6350
expansions: [(0, 5), (25, 1), (46, 1), (60, 1), (63, 2), (65, 1), (67, 1), (74, 1), (80, 1), (81, 1), (83, 2), (84, 1), (85, 1), (91, 1), (92, 1), (93, 1), (113, 1), (114, 1), (119, 1), (123, 2), (125, 3), (143, 1), (145, 1), (147, 1), (149, 1), (156, 1), (159, 1), (161, 1), (163, 1), (164, 1), (165, 1), (166, 1), (169, 1), (182, 2), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (229, 1), (230, 4), (232, 1), (255, 1), (256, 1), (257, 1), (258, 2), (259, 2), (260, 3), (278, 2), (279, 1), (280, 1), (284, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 84s - loss: 1732.5024 - loglik: -1.7301e+03 - logprior: -2.3922e+00
Epoch 2/2
39/39 - 82s - loss: 1710.2513 - loglik: -1.7093e+03 - logprior: -9.6315e-01
Fitted a model with MAP estimate = -1705.5153
expansions: []
discards: [  1   2 148 153 224 233 290 327 328 353 368 369]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 68s - loss: 1715.6512 - loglik: -1.7143e+03 - logprior: -1.3884e+00
Epoch 2/2
39/39 - 59s - loss: 1709.7285 - loglik: -1.7094e+03 - logprior: -2.8570e-01
Fitted a model with MAP estimate = -1705.9462
expansions: []
discards: [100]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 1713.5248 - loglik: -1.7124e+03 - logprior: -1.1329e+00
Epoch 2/10
39/39 - 57s - loss: 1708.9297 - loglik: -1.7090e+03 - logprior: 0.1175
Epoch 3/10
39/39 - 58s - loss: 1705.5409 - loglik: -1.7058e+03 - logprior: 0.2200
Epoch 4/10
39/39 - 58s - loss: 1703.4177 - loglik: -1.7037e+03 - logprior: 0.2653
Epoch 5/10
39/39 - 58s - loss: 1699.7123 - loglik: -1.7003e+03 - logprior: 0.5572
Epoch 6/10
39/39 - 58s - loss: 1699.4584 - loglik: -1.7002e+03 - logprior: 0.7577
Epoch 7/10
39/39 - 57s - loss: 1698.4644 - loglik: -1.6994e+03 - logprior: 0.8971
Epoch 8/10
39/39 - 58s - loss: 1698.7072 - loglik: -1.6998e+03 - logprior: 1.0651
Fitted a model with MAP estimate = -1698.3146
Time for alignment: 1470.3550
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 44s - loss: 1956.4988 - loglik: -1.9551e+03 - logprior: -1.4243e+00
Epoch 2/10
39/39 - 46s - loss: 1775.7157 - loglik: -1.7737e+03 - logprior: -2.0413e+00
Epoch 3/10
39/39 - 51s - loss: 1760.4904 - loglik: -1.7583e+03 - logprior: -2.1409e+00
Epoch 4/10
39/39 - 54s - loss: 1755.4423 - loglik: -1.7533e+03 - logprior: -2.1300e+00
Epoch 5/10
39/39 - 54s - loss: 1753.4772 - loglik: -1.7513e+03 - logprior: -2.1395e+00
Epoch 6/10
39/39 - 51s - loss: 1752.2581 - loglik: -1.7501e+03 - logprior: -2.1396e+00
Epoch 7/10
39/39 - 55s - loss: 1750.9109 - loglik: -1.7487e+03 - logprior: -2.1661e+00
Epoch 8/10
39/39 - 53s - loss: 1750.9625 - loglik: -1.7488e+03 - logprior: -2.1661e+00
Fitted a model with MAP estimate = -1749.5934
expansions: [(12, 2), (13, 1), (14, 1), (25, 1), (46, 1), (56, 1), (63, 2), (67, 1), (73, 1), (74, 1), (82, 1), (84, 2), (85, 1), (86, 1), (92, 1), (93, 1), (94, 1), (104, 1), (114, 1), (120, 1), (123, 2), (124, 1), (125, 3), (143, 1), (145, 1), (147, 1), (149, 1), (156, 1), (158, 1), (159, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (182, 1), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (231, 5), (232, 1), (236, 1), (255, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 86s - loss: 1730.7322 - loglik: -1.7289e+03 - logprior: -1.8711e+00
Epoch 2/2
39/39 - 76s - loss: 1709.6953 - loglik: -1.7089e+03 - logprior: -8.1372e-01
Fitted a model with MAP estimate = -1704.7743
expansions: []
discards: [148 151 230 287 288 349 364 365]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 93s - loss: 1714.5210 - loglik: -1.7130e+03 - logprior: -1.4858e+00
Epoch 2/2
39/39 - 87s - loss: 1709.2235 - loglik: -1.7089e+03 - logprior: -3.0677e-01
Fitted a model with MAP estimate = -1705.1077
expansions: []
discards: [100]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 87s - loss: 1712.9421 - loglik: -1.7118e+03 - logprior: -1.1844e+00
Epoch 2/10
39/39 - 75s - loss: 1708.1722 - loglik: -1.7082e+03 - logprior: -3.2072e-03
Epoch 3/10
39/39 - 78s - loss: 1704.5092 - loglik: -1.7047e+03 - logprior: 0.1779
Epoch 4/10
39/39 - 76s - loss: 1701.7682 - loglik: -1.7021e+03 - logprior: 0.3594
Epoch 5/10
39/39 - 76s - loss: 1699.8737 - loglik: -1.7004e+03 - logprior: 0.5635
Epoch 6/10
39/39 - 87s - loss: 1697.5684 - loglik: -1.6983e+03 - logprior: 0.7655
Epoch 7/10
39/39 - 86s - loss: 1698.5573 - loglik: -1.6995e+03 - logprior: 0.9191
Fitted a model with MAP estimate = -1697.4670
Time for alignment: 1705.0989
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 61s - loss: 1954.7197 - loglik: -1.9533e+03 - logprior: -1.3886e+00
Epoch 2/10
39/39 - 56s - loss: 1773.3656 - loglik: -1.7716e+03 - logprior: -1.7486e+00
Epoch 3/10
39/39 - 52s - loss: 1757.5879 - loglik: -1.7559e+03 - logprior: -1.6723e+00
Epoch 4/10
39/39 - 58s - loss: 1753.4153 - loglik: -1.7518e+03 - logprior: -1.6227e+00
Epoch 5/10
39/39 - 54s - loss: 1750.2385 - loglik: -1.7486e+03 - logprior: -1.6137e+00
Epoch 6/10
39/39 - 50s - loss: 1749.7904 - loglik: -1.7482e+03 - logprior: -1.6304e+00
Epoch 7/10
39/39 - 56s - loss: 1749.5516 - loglik: -1.7479e+03 - logprior: -1.6155e+00
Epoch 8/10
39/39 - 56s - loss: 1748.6152 - loglik: -1.7470e+03 - logprior: -1.6651e+00
Epoch 9/10
39/39 - 49s - loss: 1748.0793 - loglik: -1.7463e+03 - logprior: -1.7447e+00
Epoch 10/10
39/39 - 53s - loss: 1749.7788 - loglik: -1.7480e+03 - logprior: -1.7529e+00
Fitted a model with MAP estimate = -1746.9692
expansions: [(0, 5), (36, 1), (45, 2), (60, 1), (62, 2), (67, 1), (72, 1), (73, 1), (81, 1), (83, 3), (84, 3), (90, 1), (91, 1), (92, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (165, 1), (166, 2), (182, 2), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (231, 1), (232, 3), (233, 1), (257, 5), (258, 2), (259, 2), (260, 1), (279, 2), (280, 1), (281, 1), (284, 1), (285, 1), (287, 1), (288, 4), (289, 1), (296, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 96s - loss: 1733.7181 - loglik: -1.7314e+03 - logprior: -2.2780e+00
Epoch 2/2
39/39 - 84s - loss: 1709.8660 - loglik: -1.7089e+03 - logprior: -9.3476e-01
Fitted a model with MAP estimate = -1704.9666
expansions: []
discards: [  1   2   3  99 102 149 154 208 226 235 326 353 368 369]
Re-initialized the encoder parameters.
Fitting a model of length 375 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 81s - loss: 1716.8123 - loglik: -1.7156e+03 - logprior: -1.2599e+00
Epoch 2/2
39/39 - 75s - loss: 1711.4866 - loglik: -1.7113e+03 - logprior: -2.0987e-01
Fitted a model with MAP estimate = -1707.2721
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 94s - loss: 1713.6769 - loglik: -1.7126e+03 - logprior: -1.1245e+00
Epoch 2/10
39/39 - 87s - loss: 1708.7712 - loglik: -1.7089e+03 - logprior: 0.0912
Epoch 3/10
39/39 - 88s - loss: 1705.1226 - loglik: -1.7054e+03 - logprior: 0.2774
Epoch 4/10
39/39 - 82s - loss: 1703.0363 - loglik: -1.7033e+03 - logprior: 0.2822
Epoch 5/10
39/39 - 81s - loss: 1700.7928 - loglik: -1.7014e+03 - logprior: 0.5868
Epoch 6/10
39/39 - 78s - loss: 1698.1024 - loglik: -1.6988e+03 - logprior: 0.7423
Epoch 7/10
39/39 - 74s - loss: 1698.8297 - loglik: -1.6997e+03 - logprior: 0.8885
Fitted a model with MAP estimate = -1698.1741
Time for alignment: 1853.9478
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 1951.3406 - loglik: -1.9500e+03 - logprior: -1.3799e+00
Epoch 2/10
39/39 - 49s - loss: 1775.8490 - loglik: -1.7742e+03 - logprior: -1.6015e+00
Epoch 3/10
39/39 - 53s - loss: 1759.9960 - loglik: -1.7584e+03 - logprior: -1.5639e+00
Epoch 4/10
39/39 - 58s - loss: 1755.9227 - loglik: -1.7545e+03 - logprior: -1.4707e+00
Epoch 5/10
39/39 - 59s - loss: 1754.3033 - loglik: -1.7528e+03 - logprior: -1.5097e+00
Epoch 6/10
39/39 - 54s - loss: 1751.9581 - loglik: -1.7505e+03 - logprior: -1.4712e+00
Epoch 7/10
39/39 - 49s - loss: 1753.2272 - loglik: -1.7517e+03 - logprior: -1.5513e+00
Fitted a model with MAP estimate = -1750.2849
expansions: [(0, 5), (24, 1), (42, 1), (45, 1), (60, 1), (62, 2), (66, 1), (72, 1), (73, 1), (81, 1), (83, 3), (84, 2), (90, 1), (91, 1), (92, 1), (112, 1), (113, 1), (118, 1), (122, 2), (124, 3), (142, 1), (144, 1), (146, 1), (149, 1), (155, 1), (158, 1), (160, 1), (162, 1), (163, 1), (164, 2), (165, 1), (186, 1), (188, 1), (189, 1), (190, 1), (197, 1), (205, 1), (207, 2), (208, 2), (209, 3), (227, 1), (228, 1), (231, 4), (233, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 72s - loss: 1730.8097 - loglik: -1.7288e+03 - logprior: -1.9655e+00
Epoch 2/2
39/39 - 71s - loss: 1710.3619 - loglik: -1.7097e+03 - logprior: -6.5604e-01
Fitted a model with MAP estimate = -1705.2945
expansions: []
discards: [  1   2 102 148 153 259 260 290 325]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 1715.2252 - loglik: -1.7138e+03 - logprior: -1.4599e+00
Epoch 2/2
39/39 - 68s - loss: 1709.6984 - loglik: -1.7094e+03 - logprior: -3.3341e-01
Fitted a model with MAP estimate = -1705.7151
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 69s - loss: 1713.0740 - loglik: -1.7118e+03 - logprior: -1.2484e+00
Epoch 2/10
39/39 - 67s - loss: 1707.9069 - loglik: -1.7078e+03 - logprior: -5.9183e-02
Epoch 3/10
39/39 - 69s - loss: 1705.3900 - loglik: -1.7056e+03 - logprior: 0.1956
Epoch 4/10
39/39 - 69s - loss: 1701.0006 - loglik: -1.7014e+03 - logprior: 0.3801
Epoch 5/10
39/39 - 67s - loss: 1700.1147 - loglik: -1.7004e+03 - logprior: 0.3342
Epoch 6/10
39/39 - 65s - loss: 1700.5062 - loglik: -1.7010e+03 - logprior: 0.5350
Fitted a model with MAP estimate = -1698.4669
Time for alignment: 1378.9212
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 42s - loss: 1954.5840 - loglik: -1.9532e+03 - logprior: -1.4008e+00
Epoch 2/10
39/39 - 38s - loss: 1773.4565 - loglik: -1.7717e+03 - logprior: -1.7505e+00
Epoch 3/10
39/39 - 39s - loss: 1759.6012 - loglik: -1.7579e+03 - logprior: -1.7311e+00
Epoch 4/10
39/39 - 40s - loss: 1753.6426 - loglik: -1.7519e+03 - logprior: -1.6965e+00
Epoch 5/10
39/39 - 41s - loss: 1751.5623 - loglik: -1.7499e+03 - logprior: -1.7121e+00
Epoch 6/10
39/39 - 43s - loss: 1750.0928 - loglik: -1.7483e+03 - logprior: -1.8031e+00
Epoch 7/10
39/39 - 42s - loss: 1748.8179 - loglik: -1.7470e+03 - logprior: -1.8502e+00
Epoch 8/10
39/39 - 42s - loss: 1748.2100 - loglik: -1.7462e+03 - logprior: -1.9923e+00
Epoch 9/10
39/39 - 43s - loss: 1750.2634 - loglik: -1.7484e+03 - logprior: -1.8814e+00
Fitted a model with MAP estimate = -1747.2238
expansions: [(0, 5), (25, 1), (42, 1), (45, 1), (59, 1), (62, 2), (66, 2), (73, 1), (79, 1), (80, 1), (82, 2), (83, 1), (84, 1), (90, 1), (91, 1), (92, 1), (112, 1), (113, 1), (118, 1), (122, 2), (125, 2), (127, 1), (142, 1), (144, 1), (149, 1), (151, 1), (155, 1), (158, 1), (161, 1), (162, 1), (163, 1), (165, 1), (166, 1), (182, 2), (186, 1), (188, 2), (189, 1), (190, 1), (197, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (227, 1), (228, 1), (231, 3), (232, 1), (256, 1), (257, 3), (258, 2), (259, 2), (260, 1), (281, 1), (282, 1), (283, 1), (284, 1), (285, 1), (287, 1), (289, 1), (296, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 384 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 67s - loss: 1730.5144 - loglik: -1.7284e+03 - logprior: -2.1159e+00
Epoch 2/2
39/39 - 65s - loss: 1710.4930 - loglik: -1.7098e+03 - logprior: -6.9296e-01
Fitted a model with MAP estimate = -1705.3818
expansions: []
discards: [  1   2 148 153 224 233 324]
Re-initialized the encoder parameters.
Fitting a model of length 377 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 1714.5519 - loglik: -1.7131e+03 - logprior: -1.4260e+00
Epoch 2/2
39/39 - 66s - loss: 1709.2949 - loglik: -1.7089e+03 - logprior: -3.6812e-01
Fitted a model with MAP estimate = -1705.3351
expansions: []
discards: [100]
Re-initialized the encoder parameters.
Fitting a model of length 376 on 10010 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 1713.3093 - loglik: -1.7121e+03 - logprior: -1.2272e+00
Epoch 2/10
39/39 - 72s - loss: 1708.2053 - loglik: -1.7081e+03 - logprior: -7.2747e-02
Epoch 3/10
39/39 - 70s - loss: 1704.7831 - loglik: -1.7049e+03 - logprior: 0.1386
Epoch 4/10
39/39 - 72s - loss: 1702.0265 - loglik: -1.7024e+03 - logprior: 0.3715
Epoch 5/10
39/39 - 71s - loss: 1700.4019 - loglik: -1.7010e+03 - logprior: 0.5922
Epoch 6/10
39/39 - 71s - loss: 1698.6571 - loglik: -1.6994e+03 - logprior: 0.7625
Epoch 7/10
39/39 - 71s - loss: 1697.5176 - loglik: -1.6984e+03 - logprior: 0.9200
Epoch 8/10
39/39 - 69s - loss: 1698.3875 - loglik: -1.6995e+03 - logprior: 1.0831
Fitted a model with MAP estimate = -1697.5972
Time for alignment: 1499.6357
Computed alignments with likelihoods: ['-1698.3146', '-1697.4670', '-1698.1741', '-1698.4669', '-1697.5972']
Best model has likelihood: -1697.4670  (prior= 0.9961 )
time for generating output: 0.3628
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00476.projection.fasta
SP score = 0.9431253785584495
Training of 5 independent models on file PF02836.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1361.3964 - loglik: -1.3596e+03 - logprior: -1.8400e+00
Epoch 2/10
39/39 - 16s - loss: 1269.9752 - loglik: -1.2685e+03 - logprior: -1.4565e+00
Epoch 3/10
39/39 - 16s - loss: 1262.8430 - loglik: -1.2613e+03 - logprior: -1.5719e+00
Epoch 4/10
39/39 - 16s - loss: 1258.5310 - loglik: -1.2569e+03 - logprior: -1.6162e+00
Epoch 5/10
39/39 - 16s - loss: 1258.7390 - loglik: -1.2571e+03 - logprior: -1.6439e+00
Fitted a model with MAP estimate = -1256.6595
expansions: [(4, 1), (6, 1), (33, 1), (44, 1), (82, 2), (86, 1), (92, 1), (94, 1), (118, 1), (119, 7), (120, 3), (121, 1), (122, 2), (128, 1), (129, 1), (131, 3), (132, 2), (135, 1), (136, 1), (138, 1), (141, 3), (142, 8), (143, 1), (145, 1), (146, 1), (151, 1), (153, 1), (156, 1), (159, 1), (160, 3), (161, 1), (172, 4), (173, 1), (183, 1)]
discards: [  0 162 163 164 165 166 167 168 169 170 175 176 177 178 179 180 181 186
 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204
 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1275.5553 - loglik: -1.2729e+03 - logprior: -2.6188e+00
Epoch 2/2
39/39 - 17s - loss: 1262.0939 - loglik: -1.2609e+03 - logprior: -1.2273e+00
Fitted a model with MAP estimate = -1259.2647
expansions: [(4, 1), (87, 5), (209, 1), (229, 4), (230, 28)]
discards: [  0   1  81  82  83  84 130 131 132 137 157 180 181 195 196 197 198 199
 200 216 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 247 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1264.4235 - loglik: -1.2619e+03 - logprior: -2.5196e+00
Epoch 2/2
39/39 - 19s - loss: 1252.9866 - loglik: -1.2522e+03 - logprior: -8.2428e-01
Fitted a model with MAP estimate = -1249.8954
expansions: [(0, 2), (87, 5), (217, 3), (247, 3)]
discards: [  0   1  81  82  83  84  85 175 223 224 225 226 227 228 229 230 231 232
 233 234 235 236 237 238 239 240 241 242 243 244 245 246]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1263.3877 - loglik: -1.2619e+03 - logprior: -1.5178e+00
Epoch 2/10
39/39 - 18s - loss: 1257.3552 - loglik: -1.2570e+03 - logprior: -3.2112e-01
Epoch 3/10
39/39 - 18s - loss: 1256.1808 - loglik: -1.2560e+03 - logprior: -2.0028e-01
Epoch 4/10
39/39 - 18s - loss: 1253.8416 - loglik: -1.2537e+03 - logprior: -1.3750e-01
Epoch 5/10
39/39 - 18s - loss: 1253.5911 - loglik: -1.2535e+03 - logprior: -8.5816e-02
Epoch 6/10
39/39 - 18s - loss: 1252.5762 - loglik: -1.2526e+03 - logprior: -2.1168e-02
Epoch 7/10
39/39 - 18s - loss: 1253.3831 - loglik: -1.2534e+03 - logprior: 0.0431
Fitted a model with MAP estimate = -1252.5410
Time for alignment: 377.9263
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1358.9436 - loglik: -1.3571e+03 - logprior: -1.8134e+00
Epoch 2/10
39/39 - 16s - loss: 1271.4836 - loglik: -1.2701e+03 - logprior: -1.3820e+00
Epoch 3/10
39/39 - 16s - loss: 1263.8652 - loglik: -1.2624e+03 - logprior: -1.4868e+00
Epoch 4/10
39/39 - 17s - loss: 1262.0829 - loglik: -1.2606e+03 - logprior: -1.4763e+00
Epoch 5/10
39/39 - 16s - loss: 1261.1855 - loglik: -1.2597e+03 - logprior: -1.5005e+00
Epoch 6/10
39/39 - 17s - loss: 1259.9144 - loglik: -1.2584e+03 - logprior: -1.5357e+00
Epoch 7/10
39/39 - 17s - loss: 1259.6638 - loglik: -1.2581e+03 - logprior: -1.5412e+00
Epoch 8/10
39/39 - 17s - loss: 1259.3574 - loglik: -1.2578e+03 - logprior: -1.5428e+00
Epoch 9/10
39/39 - 17s - loss: 1259.7795 - loglik: -1.2582e+03 - logprior: -1.5401e+00
Fitted a model with MAP estimate = -1258.5867
expansions: [(4, 1), (6, 1), (31, 1), (34, 1), (78, 1), (81, 8), (83, 1), (84, 1), (87, 1), (88, 1), (89, 1), (113, 1), (114, 7), (115, 2), (116, 1), (117, 2), (126, 1), (128, 2), (129, 2), (133, 1), (136, 1), (139, 2), (140, 1), (141, 8), (145, 1), (146, 2), (157, 1), (159, 2), (160, 2), (171, 3), (172, 4), (173, 1), (184, 1)]
discards: [  0 148 149 150 151 152 153 161 162 163 164 165 166 167 168 169 186 187
 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 22s - loss: 1276.1805 - loglik: -1.2737e+03 - logprior: -2.5279e+00
Epoch 2/2
39/39 - 20s - loss: 1263.8013 - loglik: -1.2628e+03 - logprior: -1.0331e+00
Fitted a model with MAP estimate = -1261.0896
expansions: [(200, 2), (204, 1), (236, 21)]
discards: [  0   1  90 133 134 135 136 183 184 185 205 206 207 208 209 210 211 221
 222 224]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 1265.3301 - loglik: -1.2628e+03 - logprior: -2.5236e+00
Epoch 2/2
39/39 - 22s - loss: 1255.1827 - loglik: -1.2545e+03 - logprior: -6.9519e-01
Fitted a model with MAP estimate = -1252.5255
expansions: [(0, 2), (2, 1), (191, 1), (193, 7), (195, 8), (218, 4), (240, 3)]
discards: [  0 176 198 201 202 203 204 205 206 207 219 220 221 222 223 224 225 226
 227 228 229 230 231 232 233 234 235 236 237 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1260.6249 - loglik: -1.2590e+03 - logprior: -1.6008e+00
Epoch 2/10
39/39 - 22s - loss: 1252.5341 - loglik: -1.2522e+03 - logprior: -3.6409e-01
Epoch 3/10
39/39 - 22s - loss: 1250.6343 - loglik: -1.2504e+03 - logprior: -2.6578e-01
Epoch 4/10
39/39 - 22s - loss: 1250.3944 - loglik: -1.2502e+03 - logprior: -2.0418e-01
Epoch 5/10
39/39 - 22s - loss: 1248.7765 - loglik: -1.2486e+03 - logprior: -1.4585e-01
Epoch 6/10
39/39 - 22s - loss: 1248.2865 - loglik: -1.2482e+03 - logprior: -8.3211e-02
Epoch 7/10
39/39 - 22s - loss: 1250.0095 - loglik: -1.2500e+03 - logprior: -1.7629e-02
Fitted a model with MAP estimate = -1248.3550
Time for alignment: 493.6458
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1360.0471 - loglik: -1.3582e+03 - logprior: -1.8125e+00
Epoch 2/10
39/39 - 19s - loss: 1270.1494 - loglik: -1.2688e+03 - logprior: -1.3510e+00
Epoch 3/10
39/39 - 19s - loss: 1262.3907 - loglik: -1.2610e+03 - logprior: -1.4105e+00
Epoch 4/10
39/39 - 20s - loss: 1260.6305 - loglik: -1.2592e+03 - logprior: -1.4087e+00
Epoch 5/10
39/39 - 20s - loss: 1259.8610 - loglik: -1.2584e+03 - logprior: -1.4175e+00
Epoch 6/10
39/39 - 20s - loss: 1259.8202 - loglik: -1.2584e+03 - logprior: -1.4322e+00
Epoch 7/10
39/39 - 21s - loss: 1260.2587 - loglik: -1.2588e+03 - logprior: -1.4335e+00
Fitted a model with MAP estimate = -1258.5231
expansions: [(4, 1), (6, 1), (33, 1), (85, 4), (89, 2), (90, 3), (92, 2), (116, 1), (117, 5), (118, 9), (125, 1), (129, 7), (134, 1), (136, 1), (137, 1), (138, 2), (139, 2), (140, 8), (144, 1), (146, 1), (150, 1), (152, 1), (153, 1), (155, 4), (158, 1), (159, 2), (171, 5), (172, 1), (185, 1)]
discards: [  0 160 161 162 163 164 165 166 167 168 169 174 175 176 177 178 179 180
 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203
 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1277.5797 - loglik: -1.2751e+03 - logprior: -2.5047e+00
Epoch 2/2
39/39 - 24s - loss: 1261.4518 - loglik: -1.2603e+03 - logprior: -1.1102e+00
Fitted a model with MAP estimate = -1257.9133
expansions: [(222, 1), (239, 27)]
discards: [  0   1  81 100 103 134 135 136 137 138 160 184 185 186 187 202 213 214
 223]
Re-initialized the encoder parameters.
Fitting a model of length 248 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1260.2137 - loglik: -1.2577e+03 - logprior: -2.5308e+00
Epoch 2/2
39/39 - 25s - loss: 1250.7094 - loglik: -1.2498e+03 - logprior: -9.2201e-01
Fitted a model with MAP estimate = -1247.5642
expansions: [(0, 2), (2, 1), (79, 1)]
discards: [  0 173 188 189 190 191 224 225 226 227 228 229 230 231 232 233 234 235
 236 237 238 239 240 241 242 243 244 245 246 247]
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1264.5294 - loglik: -1.2630e+03 - logprior: -1.5678e+00
Epoch 2/10
39/39 - 21s - loss: 1260.0723 - loglik: -1.2598e+03 - logprior: -3.0798e-01
Epoch 3/10
39/39 - 21s - loss: 1258.6185 - loglik: -1.2584e+03 - logprior: -1.8749e-01
Epoch 4/10
39/39 - 22s - loss: 1256.4746 - loglik: -1.2564e+03 - logprior: -1.1102e-01
Epoch 5/10
39/39 - 22s - loss: 1257.8621 - loglik: -1.2578e+03 - logprior: -3.6866e-02
Fitted a model with MAP estimate = -1256.3486
Time for alignment: 459.2144
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1362.0123 - loglik: -1.3602e+03 - logprior: -1.8055e+00
Epoch 2/10
39/39 - 20s - loss: 1271.8490 - loglik: -1.2704e+03 - logprior: -1.3998e+00
Epoch 3/10
39/39 - 20s - loss: 1263.6848 - loglik: -1.2622e+03 - logprior: -1.4796e+00
Epoch 4/10
39/39 - 20s - loss: 1261.3943 - loglik: -1.2599e+03 - logprior: -1.4950e+00
Epoch 5/10
39/39 - 20s - loss: 1261.0768 - loglik: -1.2595e+03 - logprior: -1.5342e+00
Epoch 6/10
39/39 - 20s - loss: 1259.7604 - loglik: -1.2582e+03 - logprior: -1.5654e+00
Epoch 7/10
39/39 - 20s - loss: 1258.8751 - loglik: -1.2573e+03 - logprior: -1.5704e+00
Epoch 8/10
39/39 - 19s - loss: 1258.9449 - loglik: -1.2574e+03 - logprior: -1.5692e+00
Fitted a model with MAP estimate = -1258.3401
expansions: [(3, 1), (6, 1), (31, 1), (34, 1), (77, 1), (81, 6), (84, 1), (89, 1), (90, 1), (91, 1), (117, 7), (118, 2), (119, 1), (120, 1), (129, 2), (131, 3), (132, 2), (137, 1), (139, 1), (142, 3), (143, 9), (144, 1), (146, 1), (147, 2), (152, 1), (154, 2), (157, 1), (159, 2), (160, 3), (171, 1), (172, 2), (173, 3), (174, 1), (184, 1)]
discards: [  0  79 150 161 162 163 164 165 166 167 168 169 176 177 178 179 180 181
 182 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203
 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1277.1727 - loglik: -1.2747e+03 - logprior: -2.5059e+00
Epoch 2/2
39/39 - 23s - loss: 1262.9729 - loglik: -1.2619e+03 - logprior: -1.0926e+00
Fitted a model with MAP estimate = -1259.6651
expansions: [(3, 1), (4, 1), (83, 1), (88, 1), (213, 4), (222, 5), (234, 1), (236, 25)]
discards: [  0   1 131 132 133 134 159 182 183 184 199 200 201 202 214 215 216 217
 218 219 220 231]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1262.5123 - loglik: -1.2603e+03 - logprior: -2.2291e+00
Epoch 2/2
39/39 - 27s - loss: 1250.1361 - loglik: -1.2496e+03 - logprior: -5.4502e-01
Fitted a model with MAP estimate = -1246.6127
expansions: [(0, 2), (87, 3), (194, 1), (195, 3), (222, 1), (253, 2)]
discards: [  0   1   2  81  82  83  84 178 208 209 210 211 212 213 214 232 233 234
 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 1264.8917 - loglik: -1.2636e+03 - logprior: -1.3354e+00
Epoch 2/10
39/39 - 22s - loss: 1257.7266 - loglik: -1.2578e+03 - logprior: 0.0811
Epoch 3/10
39/39 - 23s - loss: 1256.1097 - loglik: -1.2563e+03 - logprior: 0.2326
Epoch 4/10
39/39 - 22s - loss: 1253.7285 - loglik: -1.2540e+03 - logprior: 0.3139
Epoch 5/10
39/39 - 22s - loss: 1253.3865 - loglik: -1.2538e+03 - logprior: 0.3791
Epoch 6/10
39/39 - 22s - loss: 1253.0741 - loglik: -1.2535e+03 - logprior: 0.4512
Epoch 7/10
39/39 - 22s - loss: 1253.4127 - loglik: -1.2539e+03 - logprior: 0.5146
Fitted a model with MAP estimate = -1252.6656
Time for alignment: 534.7562
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1360.7396 - loglik: -1.3589e+03 - logprior: -1.8410e+00
Epoch 2/10
39/39 - 19s - loss: 1269.8066 - loglik: -1.2683e+03 - logprior: -1.4821e+00
Epoch 3/10
39/39 - 20s - loss: 1261.9061 - loglik: -1.2603e+03 - logprior: -1.6033e+00
Epoch 4/10
39/39 - 20s - loss: 1259.5735 - loglik: -1.2580e+03 - logprior: -1.5976e+00
Epoch 5/10
39/39 - 20s - loss: 1259.2690 - loglik: -1.2577e+03 - logprior: -1.6105e+00
Epoch 6/10
39/39 - 21s - loss: 1258.9906 - loglik: -1.2574e+03 - logprior: -1.6238e+00
Epoch 7/10
39/39 - 21s - loss: 1258.5780 - loglik: -1.2570e+03 - logprior: -1.6278e+00
Epoch 8/10
39/39 - 21s - loss: 1258.5591 - loglik: -1.2569e+03 - logprior: -1.6260e+00
Epoch 9/10
39/39 - 21s - loss: 1260.1163 - loglik: -1.2585e+03 - logprior: -1.6275e+00
Fitted a model with MAP estimate = -1257.5839
expansions: [(4, 1), (6, 1), (31, 1), (34, 1), (83, 8), (93, 1), (120, 1), (121, 7), (122, 3), (123, 1), (124, 2), (130, 1), (131, 1), (132, 3), (133, 7), (136, 1), (137, 1), (139, 1), (142, 9), (143, 4), (145, 2), (146, 2), (159, 4), (161, 1), (171, 5), (173, 1), (183, 1), (184, 1)]
discards: [  0  77  78  79 148 149 150 151 152 153 154 155 156 162 163 164 165 167
 169 175 176 177 178 179 180 181 186 187 188 189 190 191 192 193 194 195
 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1280.7214 - loglik: -1.2781e+03 - logprior: -2.5866e+00
Epoch 2/2
39/39 - 22s - loss: 1264.4565 - loglik: -1.2633e+03 - logprior: -1.1213e+00
Fitted a model with MAP estimate = -1260.1899
expansions: [(210, 2), (213, 1), (216, 8), (222, 13), (226, 1), (228, 1), (231, 4), (232, 7)]
discards: [  0   1 133 134 135 140 154 157 158 159 162 163 187 188 196 203]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1257.4026 - loglik: -1.2550e+03 - logprior: -2.4365e+00
Epoch 2/2
39/39 - 26s - loss: 1247.2969 - loglik: -1.2466e+03 - logprior: -6.8346e-01
Fitted a model with MAP estimate = -1244.5374
expansions: [(0, 2), (148, 1), (207, 2), (216, 7), (253, 3)]
discards: [175 188 189 190 191 226 227 228 229 230 231 232 233 234 235 236 237 238
 239 240 241 242 243 244 245 246 247 248 249 250 251 252]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10008 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1261.0875 - loglik: -1.2596e+03 - logprior: -1.5253e+00
Epoch 2/10
39/39 - 22s - loss: 1254.7419 - loglik: -1.2545e+03 - logprior: -2.4923e-01
Epoch 3/10
39/39 - 20s - loss: 1251.8607 - loglik: -1.2518e+03 - logprior: -1.0709e-01
Epoch 4/10
39/39 - 20s - loss: 1251.1108 - loglik: -1.2511e+03 - logprior: -1.7477e-02
Epoch 5/10
39/39 - 19s - loss: 1251.2598 - loglik: -1.2513e+03 - logprior: 0.0527
Fitted a model with MAP estimate = -1250.3754
Time for alignment: 500.6697
Computed alignments with likelihoods: ['-1249.8954', '-1248.3550', '-1247.5642', '-1246.6127', '-1244.5374']
Best model has likelihood: -1244.5374  (prior= -0.3545 )
time for generating output: 0.2583
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02836.projection.fasta
SP score = 0.9096985748518098
Training of 5 independent models on file PF02777.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 564.3984 - loglik: -5.6128e+02 - logprior: -3.1149e+00
Epoch 2/10
19/19 - 2s - loss: 495.2217 - loglik: -4.9390e+02 - logprior: -1.3267e+00
Epoch 3/10
19/19 - 2s - loss: 472.0058 - loglik: -4.7033e+02 - logprior: -1.6745e+00
Epoch 4/10
19/19 - 2s - loss: 467.7039 - loglik: -4.6606e+02 - logprior: -1.6419e+00
Epoch 5/10
19/19 - 2s - loss: 466.7777 - loglik: -4.6522e+02 - logprior: -1.5598e+00
Epoch 6/10
19/19 - 2s - loss: 465.2604 - loglik: -4.6371e+02 - logprior: -1.5494e+00
Epoch 7/10
19/19 - 2s - loss: 465.8367 - loglik: -4.6430e+02 - logprior: -1.5370e+00
Fitted a model with MAP estimate = -464.7820
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 463.4167 - loglik: -4.5948e+02 - logprior: -3.9370e+00
Epoch 2/2
19/19 - 3s - loss: 451.3112 - loglik: -4.4933e+02 - logprior: -1.9763e+00
Fitted a model with MAP estimate = -449.0294
expansions: [(0, 2)]
discards: [ 0 42 59 60]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 451.6856 - loglik: -4.4877e+02 - logprior: -2.9168e+00
Epoch 2/2
19/19 - 3s - loss: 447.4771 - loglik: -4.4635e+02 - logprior: -1.1258e+00
Fitted a model with MAP estimate = -446.2251
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 453.2563 - loglik: -4.4938e+02 - logprior: -3.8719e+00
Epoch 2/10
19/19 - 2s - loss: 448.1013 - loglik: -4.4662e+02 - logprior: -1.4818e+00
Epoch 3/10
19/19 - 2s - loss: 446.7758 - loglik: -4.4580e+02 - logprior: -9.8007e-01
Epoch 4/10
19/19 - 3s - loss: 446.1964 - loglik: -4.4525e+02 - logprior: -9.4539e-01
Epoch 5/10
19/19 - 3s - loss: 445.2460 - loglik: -4.4432e+02 - logprior: -9.2768e-01
Epoch 6/10
19/19 - 3s - loss: 445.1939 - loglik: -4.4429e+02 - logprior: -9.0538e-01
Epoch 7/10
19/19 - 3s - loss: 444.7201 - loglik: -4.4382e+02 - logprior: -8.9602e-01
Epoch 8/10
19/19 - 3s - loss: 444.4159 - loglik: -4.4354e+02 - logprior: -8.7732e-01
Epoch 9/10
19/19 - 3s - loss: 444.8853 - loglik: -4.4403e+02 - logprior: -8.5471e-01
Fitted a model with MAP estimate = -444.4835
Time for alignment: 76.7414
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 564.3651 - loglik: -5.6125e+02 - logprior: -3.1143e+00
Epoch 2/10
19/19 - 2s - loss: 495.2253 - loglik: -4.9391e+02 - logprior: -1.3171e+00
Epoch 3/10
19/19 - 2s - loss: 471.9042 - loglik: -4.7026e+02 - logprior: -1.6479e+00
Epoch 4/10
19/19 - 2s - loss: 467.1076 - loglik: -4.6545e+02 - logprior: -1.6618e+00
Epoch 5/10
19/19 - 2s - loss: 465.4213 - loglik: -4.6384e+02 - logprior: -1.5851e+00
Epoch 6/10
19/19 - 2s - loss: 465.0602 - loglik: -4.6347e+02 - logprior: -1.5860e+00
Epoch 7/10
19/19 - 2s - loss: 464.5531 - loglik: -4.6299e+02 - logprior: -1.5659e+00
Epoch 8/10
19/19 - 2s - loss: 464.8806 - loglik: -4.6332e+02 - logprior: -1.5598e+00
Fitted a model with MAP estimate = -463.9030
expansions: [(4, 1), (6, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 459.1500 - loglik: -4.5619e+02 - logprior: -2.9568e+00
Epoch 2/2
19/19 - 3s - loss: 447.7613 - loglik: -4.4657e+02 - logprior: -1.1876e+00
Fitted a model with MAP estimate = -446.4282
expansions: []
discards: [42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 450.1477 - loglik: -4.4724e+02 - logprior: -2.9082e+00
Epoch 2/2
19/19 - 2s - loss: 447.4000 - loglik: -4.4629e+02 - logprior: -1.1086e+00
Fitted a model with MAP estimate = -446.3814
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 453.4900 - loglik: -4.4951e+02 - logprior: -3.9758e+00
Epoch 2/10
19/19 - 3s - loss: 448.5801 - loglik: -4.4640e+02 - logprior: -2.1844e+00
Epoch 3/10
19/19 - 3s - loss: 447.6964 - loglik: -4.4568e+02 - logprior: -2.0122e+00
Epoch 4/10
19/19 - 2s - loss: 447.0119 - loglik: -4.4530e+02 - logprior: -1.7090e+00
Epoch 5/10
19/19 - 3s - loss: 445.8894 - loglik: -4.4488e+02 - logprior: -1.0090e+00
Epoch 6/10
19/19 - 3s - loss: 445.5113 - loglik: -4.4453e+02 - logprior: -9.8264e-01
Epoch 7/10
19/19 - 3s - loss: 445.4711 - loglik: -4.4453e+02 - logprior: -9.4372e-01
Epoch 8/10
19/19 - 3s - loss: 445.0085 - loglik: -4.4408e+02 - logprior: -9.2790e-01
Epoch 9/10
19/19 - 3s - loss: 445.0914 - loglik: -4.4419e+02 - logprior: -9.0146e-01
Fitted a model with MAP estimate = -445.0085
Time for alignment: 78.1223
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 564.0991 - loglik: -5.6098e+02 - logprior: -3.1144e+00
Epoch 2/10
19/19 - 2s - loss: 494.8520 - loglik: -4.9353e+02 - logprior: -1.3264e+00
Epoch 3/10
19/19 - 2s - loss: 471.5965 - loglik: -4.6992e+02 - logprior: -1.6764e+00
Epoch 4/10
19/19 - 2s - loss: 467.5468 - loglik: -4.6592e+02 - logprior: -1.6263e+00
Epoch 5/10
19/19 - 2s - loss: 466.7802 - loglik: -4.6522e+02 - logprior: -1.5565e+00
Epoch 6/10
19/19 - 2s - loss: 466.4832 - loglik: -4.6493e+02 - logprior: -1.5509e+00
Epoch 7/10
19/19 - 2s - loss: 465.3398 - loglik: -4.6380e+02 - logprior: -1.5447e+00
Epoch 8/10
19/19 - 2s - loss: 464.6729 - loglik: -4.6313e+02 - logprior: -1.5458e+00
Epoch 9/10
19/19 - 2s - loss: 465.4247 - loglik: -4.6387e+02 - logprior: -1.5553e+00
Fitted a model with MAP estimate = -464.3938
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 2), (71, 2), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 463.9615 - loglik: -4.6002e+02 - logprior: -3.9445e+00
Epoch 2/2
19/19 - 3s - loss: 451.1939 - loglik: -4.4921e+02 - logprior: -1.9790e+00
Fitted a model with MAP estimate = -448.9554
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 450.8148 - loglik: -4.4789e+02 - logprior: -2.9227e+00
Epoch 2/2
19/19 - 3s - loss: 447.1062 - loglik: -4.4598e+02 - logprior: -1.1218e+00
Fitted a model with MAP estimate = -445.8988
expansions: []
discards: [ 0 91]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 453.0735 - loglik: -4.4924e+02 - logprior: -3.8371e+00
Epoch 2/10
19/19 - 3s - loss: 448.3711 - loglik: -4.4693e+02 - logprior: -1.4431e+00
Epoch 3/10
19/19 - 3s - loss: 446.8384 - loglik: -4.4586e+02 - logprior: -9.8257e-01
Epoch 4/10
19/19 - 3s - loss: 445.2371 - loglik: -4.4430e+02 - logprior: -9.3666e-01
Epoch 5/10
19/19 - 2s - loss: 445.3833 - loglik: -4.4445e+02 - logprior: -9.2873e-01
Fitted a model with MAP estimate = -444.8035
Time for alignment: 69.8673
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 564.0331 - loglik: -5.6091e+02 - logprior: -3.1181e+00
Epoch 2/10
19/19 - 2s - loss: 494.4093 - loglik: -4.9308e+02 - logprior: -1.3278e+00
Epoch 3/10
19/19 - 2s - loss: 471.2416 - loglik: -4.6955e+02 - logprior: -1.6873e+00
Epoch 4/10
19/19 - 2s - loss: 467.1489 - loglik: -4.6551e+02 - logprior: -1.6364e+00
Epoch 5/10
19/19 - 2s - loss: 467.4117 - loglik: -4.6585e+02 - logprior: -1.5591e+00
Fitted a model with MAP estimate = -465.8014
expansions: [(6, 2), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (46, 5), (47, 1), (69, 1), (70, 3), (71, 1), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 464.2853 - loglik: -4.6035e+02 - logprior: -3.9392e+00
Epoch 2/2
19/19 - 3s - loss: 451.5497 - loglik: -4.4954e+02 - logprior: -2.0128e+00
Fitted a model with MAP estimate = -448.7206
expansions: [(2, 1)]
discards: [ 0 42 59 60 91]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 454.3375 - loglik: -4.5055e+02 - logprior: -3.7866e+00
Epoch 2/2
19/19 - 3s - loss: 448.6635 - loglik: -4.4737e+02 - logprior: -1.2955e+00
Fitted a model with MAP estimate = -447.0288
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 450.2393 - loglik: -4.4725e+02 - logprior: -2.9855e+00
Epoch 2/10
19/19 - 2s - loss: 447.3813 - loglik: -4.4625e+02 - logprior: -1.1362e+00
Epoch 3/10
19/19 - 2s - loss: 446.1550 - loglik: -4.4513e+02 - logprior: -1.0298e+00
Epoch 4/10
19/19 - 3s - loss: 445.7897 - loglik: -4.4482e+02 - logprior: -9.7095e-01
Epoch 5/10
19/19 - 3s - loss: 445.5216 - loglik: -4.4459e+02 - logprior: -9.3425e-01
Epoch 6/10
19/19 - 3s - loss: 445.2366 - loglik: -4.4431e+02 - logprior: -9.2591e-01
Epoch 7/10
19/19 - 3s - loss: 444.2986 - loglik: -4.4340e+02 - logprior: -8.9607e-01
Epoch 8/10
19/19 - 3s - loss: 444.7006 - loglik: -4.4381e+02 - logprior: -8.9113e-01
Fitted a model with MAP estimate = -444.5269
Time for alignment: 69.4305
Fitting a model of length 81 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 564.5654 - loglik: -5.6145e+02 - logprior: -3.1159e+00
Epoch 2/10
19/19 - 2s - loss: 494.5033 - loglik: -4.9317e+02 - logprior: -1.3305e+00
Epoch 3/10
19/19 - 2s - loss: 471.6742 - loglik: -4.7000e+02 - logprior: -1.6778e+00
Epoch 4/10
19/19 - 2s - loss: 467.6395 - loglik: -4.6601e+02 - logprior: -1.6286e+00
Epoch 5/10
19/19 - 2s - loss: 466.5351 - loglik: -4.6497e+02 - logprior: -1.5621e+00
Epoch 6/10
19/19 - 2s - loss: 466.0706 - loglik: -4.6450e+02 - logprior: -1.5657e+00
Epoch 7/10
19/19 - 2s - loss: 464.6293 - loglik: -4.6308e+02 - logprior: -1.5482e+00
Epoch 8/10
19/19 - 2s - loss: 465.0945 - loglik: -4.6354e+02 - logprior: -1.5544e+00
Fitted a model with MAP estimate = -464.5820
expansions: [(6, 1), (7, 1), (8, 1), (9, 1), (18, 1), (19, 1), (22, 1), (23, 1), (34, 2), (36, 1), (37, 1), (39, 1), (47, 4), (48, 2), (69, 1), (70, 3), (72, 1), (77, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 463.0122 - loglik: -4.5906e+02 - logprior: -3.9530e+00
Epoch 2/2
19/19 - 3s - loss: 451.2042 - loglik: -4.4923e+02 - logprior: -1.9734e+00
Fitted a model with MAP estimate = -448.8831
expansions: [(0, 2)]
discards: [ 0 42 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 451.1041 - loglik: -4.4819e+02 - logprior: -2.9127e+00
Epoch 2/2
19/19 - 3s - loss: 447.1815 - loglik: -4.4607e+02 - logprior: -1.1104e+00
Fitted a model with MAP estimate = -446.0916
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 453.1242 - loglik: -4.4930e+02 - logprior: -3.8285e+00
Epoch 2/10
19/19 - 3s - loss: 447.9379 - loglik: -4.4654e+02 - logprior: -1.3973e+00
Epoch 3/10
19/19 - 3s - loss: 446.4260 - loglik: -4.4545e+02 - logprior: -9.7622e-01
Epoch 4/10
19/19 - 3s - loss: 445.7653 - loglik: -4.4483e+02 - logprior: -9.3987e-01
Epoch 5/10
19/19 - 3s - loss: 444.7522 - loglik: -4.4383e+02 - logprior: -9.2615e-01
Epoch 6/10
19/19 - 3s - loss: 445.4036 - loglik: -4.4451e+02 - logprior: -8.9482e-01
Fitted a model with MAP estimate = -444.5720
Time for alignment: 69.8167
Computed alignments with likelihoods: ['-444.4835', '-445.0085', '-444.8035', '-444.5269', '-444.5720']
Best model has likelihood: -444.4835  (prior= -0.8526 )
time for generating output: 0.1234
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF02777.projection.fasta
SP score = 0.9043971631205674
Training of 5 independent models on file PF07654.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 475.0355 - loglik: -4.7192e+02 - logprior: -3.1194e+00
Epoch 2/10
19/19 - 1s - loss: 441.0250 - loglik: -4.3975e+02 - logprior: -1.2783e+00
Epoch 3/10
19/19 - 1s - loss: 428.0372 - loglik: -4.2675e+02 - logprior: -1.2879e+00
Epoch 4/10
19/19 - 1s - loss: 425.0510 - loglik: -4.2375e+02 - logprior: -1.2976e+00
Epoch 5/10
19/19 - 1s - loss: 423.8217 - loglik: -4.2255e+02 - logprior: -1.2692e+00
Epoch 6/10
19/19 - 1s - loss: 422.7452 - loglik: -4.2149e+02 - logprior: -1.2589e+00
Epoch 7/10
19/19 - 1s - loss: 423.1783 - loglik: -4.2192e+02 - logprior: -1.2535e+00
Fitted a model with MAP estimate = -422.5993
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (22, 1), (31, 1), (34, 1), (35, 2), (47, 1), (49, 1), (50, 2), (53, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 425.9767 - loglik: -4.2206e+02 - logprior: -3.9185e+00
Epoch 2/2
19/19 - 1s - loss: 418.8625 - loglik: -4.1685e+02 - logprior: -2.0168e+00
Fitted a model with MAP estimate = -417.5395
expansions: [(0, 2)]
discards: [ 0 43 73]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 420.2015 - loglik: -4.1729e+02 - logprior: -2.9066e+00
Epoch 2/2
19/19 - 1s - loss: 416.8730 - loglik: -4.1573e+02 - logprior: -1.1442e+00
Fitted a model with MAP estimate = -416.2196
expansions: []
discards: [ 0 14 65]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 421.5356 - loglik: -4.1788e+02 - logprior: -3.6561e+00
Epoch 2/10
19/19 - 1s - loss: 417.8097 - loglik: -4.1663e+02 - logprior: -1.1819e+00
Epoch 3/10
19/19 - 1s - loss: 416.7993 - loglik: -4.1581e+02 - logprior: -9.8845e-01
Epoch 4/10
19/19 - 1s - loss: 415.9482 - loglik: -4.1500e+02 - logprior: -9.4566e-01
Epoch 5/10
19/19 - 1s - loss: 415.8275 - loglik: -4.1491e+02 - logprior: -9.2132e-01
Epoch 6/10
19/19 - 1s - loss: 414.7379 - loglik: -4.1383e+02 - logprior: -9.0319e-01
Epoch 7/10
19/19 - 1s - loss: 415.4696 - loglik: -4.1457e+02 - logprior: -9.0089e-01
Fitted a model with MAP estimate = -414.7597
Time for alignment: 48.8622
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 475.0125 - loglik: -4.7190e+02 - logprior: -3.1165e+00
Epoch 2/10
19/19 - 1s - loss: 440.2097 - loglik: -4.3894e+02 - logprior: -1.2678e+00
Epoch 3/10
19/19 - 1s - loss: 428.1591 - loglik: -4.2688e+02 - logprior: -1.2833e+00
Epoch 4/10
19/19 - 1s - loss: 425.0562 - loglik: -4.2376e+02 - logprior: -1.2925e+00
Epoch 5/10
19/19 - 1s - loss: 423.5612 - loglik: -4.2230e+02 - logprior: -1.2651e+00
Epoch 6/10
19/19 - 1s - loss: 423.0871 - loglik: -4.2184e+02 - logprior: -1.2442e+00
Epoch 7/10
19/19 - 1s - loss: 422.8984 - loglik: -4.2165e+02 - logprior: -1.2440e+00
Epoch 8/10
19/19 - 1s - loss: 422.7227 - loglik: -4.2148e+02 - logprior: -1.2431e+00
Epoch 9/10
19/19 - 1s - loss: 422.8408 - loglik: -4.2160e+02 - logprior: -1.2453e+00
Fitted a model with MAP estimate = -422.4991
expansions: [(7, 2), (8, 2), (9, 3), (23, 1), (30, 2), (31, 2), (34, 1), (35, 1), (37, 1), (49, 1), (50, 2), (53, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 426.5539 - loglik: -4.2263e+02 - logprior: -3.9198e+00
Epoch 2/2
19/19 - 1s - loss: 419.0560 - loglik: -4.1701e+02 - logprior: -2.0462e+00
Fitted a model with MAP estimate = -417.6883
expansions: [(0, 2)]
discards: [ 0 38 40 66 74]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 420.8188 - loglik: -4.1794e+02 - logprior: -2.8834e+00
Epoch 2/2
19/19 - 1s - loss: 417.0349 - loglik: -4.1590e+02 - logprior: -1.1302e+00
Fitted a model with MAP estimate = -416.5481
expansions: []
discards: [ 0 14]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 421.4055 - loglik: -4.1777e+02 - logprior: -3.6342e+00
Epoch 2/10
19/19 - 1s - loss: 417.8673 - loglik: -4.1671e+02 - logprior: -1.1605e+00
Epoch 3/10
19/19 - 1s - loss: 416.7364 - loglik: -4.1577e+02 - logprior: -9.7064e-01
Epoch 4/10
19/19 - 1s - loss: 416.1317 - loglik: -4.1520e+02 - logprior: -9.3126e-01
Epoch 5/10
19/19 - 1s - loss: 415.3996 - loglik: -4.1450e+02 - logprior: -9.0260e-01
Epoch 6/10
19/19 - 1s - loss: 415.4048 - loglik: -4.1452e+02 - logprior: -8.8656e-01
Fitted a model with MAP estimate = -414.9626
Time for alignment: 48.8606
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 475.3869 - loglik: -4.7227e+02 - logprior: -3.1201e+00
Epoch 2/10
19/19 - 1s - loss: 440.4157 - loglik: -4.3914e+02 - logprior: -1.2761e+00
Epoch 3/10
19/19 - 1s - loss: 427.9094 - loglik: -4.2660e+02 - logprior: -1.3125e+00
Epoch 4/10
19/19 - 1s - loss: 425.2509 - loglik: -4.2395e+02 - logprior: -1.2966e+00
Epoch 5/10
19/19 - 1s - loss: 423.7307 - loglik: -4.2246e+02 - logprior: -1.2696e+00
Epoch 6/10
19/19 - 1s - loss: 423.0807 - loglik: -4.2182e+02 - logprior: -1.2562e+00
Epoch 7/10
19/19 - 1s - loss: 422.7701 - loglik: -4.2152e+02 - logprior: -1.2477e+00
Epoch 8/10
19/19 - 1s - loss: 422.3060 - loglik: -4.2107e+02 - logprior: -1.2394e+00
Epoch 9/10
19/19 - 1s - loss: 422.6375 - loglik: -4.2140e+02 - logprior: -1.2414e+00
Fitted a model with MAP estimate = -422.3355
expansions: [(7, 2), (8, 2), (9, 3), (21, 1), (22, 1), (31, 2), (34, 1), (35, 2), (37, 1), (49, 1), (50, 1), (53, 2), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 426.6223 - loglik: -4.2269e+02 - logprior: -3.9341e+00
Epoch 2/2
19/19 - 1s - loss: 419.1766 - loglik: -4.1714e+02 - logprior: -2.0412e+00
Fitted a model with MAP estimate = -417.7145
expansions: [(0, 2)]
discards: [ 0 40 45]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 419.6337 - loglik: -4.1672e+02 - logprior: -2.9093e+00
Epoch 2/2
19/19 - 1s - loss: 416.3644 - loglik: -4.1521e+02 - logprior: -1.1579e+00
Fitted a model with MAP estimate = -415.5553
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 420.2595 - loglik: -4.1658e+02 - logprior: -3.6828e+00
Epoch 2/10
19/19 - 1s - loss: 416.5954 - loglik: -4.1539e+02 - logprior: -1.2086e+00
Epoch 3/10
19/19 - 1s - loss: 415.2271 - loglik: -4.1423e+02 - logprior: -9.9879e-01
Epoch 4/10
19/19 - 1s - loss: 414.7890 - loglik: -4.1383e+02 - logprior: -9.5987e-01
Epoch 5/10
19/19 - 1s - loss: 414.1683 - loglik: -4.1323e+02 - logprior: -9.3462e-01
Epoch 6/10
19/19 - 1s - loss: 414.0958 - loglik: -4.1318e+02 - logprior: -9.1707e-01
Epoch 7/10
19/19 - 1s - loss: 413.3717 - loglik: -4.1247e+02 - logprior: -9.0323e-01
Epoch 8/10
19/19 - 1s - loss: 413.5403 - loglik: -4.1264e+02 - logprior: -8.9752e-01
Fitted a model with MAP estimate = -413.3350
Time for alignment: 51.2420
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 474.9625 - loglik: -4.7184e+02 - logprior: -3.1206e+00
Epoch 2/10
19/19 - 1s - loss: 441.6690 - loglik: -4.4040e+02 - logprior: -1.2660e+00
Epoch 3/10
19/19 - 1s - loss: 429.2152 - loglik: -4.2795e+02 - logprior: -1.2609e+00
Epoch 4/10
19/19 - 1s - loss: 425.2682 - loglik: -4.2397e+02 - logprior: -1.2947e+00
Epoch 5/10
19/19 - 1s - loss: 424.7736 - loglik: -4.2350e+02 - logprior: -1.2686e+00
Epoch 6/10
19/19 - 1s - loss: 422.8235 - loglik: -4.2155e+02 - logprior: -1.2714e+00
Epoch 7/10
19/19 - 1s - loss: 423.2033 - loglik: -4.2195e+02 - logprior: -1.2579e+00
Fitted a model with MAP estimate = -422.9352
expansions: [(7, 2), (8, 2), (9, 2), (23, 1), (31, 1), (32, 1), (34, 1), (35, 2), (37, 1), (47, 1), (48, 1), (49, 1), (55, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 426.3389 - loglik: -4.2243e+02 - logprior: -3.9128e+00
Epoch 2/2
19/19 - 1s - loss: 419.2089 - loglik: -4.1721e+02 - logprior: -1.9998e+00
Fitted a model with MAP estimate = -418.1134
expansions: [(0, 2)]
discards: [ 0 42]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 420.2415 - loglik: -4.1733e+02 - logprior: -2.9162e+00
Epoch 2/2
19/19 - 1s - loss: 417.3612 - loglik: -4.1620e+02 - logprior: -1.1612e+00
Fitted a model with MAP estimate = -416.4556
expansions: [(8, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 421.0635 - loglik: -4.1733e+02 - logprior: -3.7285e+00
Epoch 2/10
19/19 - 1s - loss: 417.4165 - loglik: -4.1619e+02 - logprior: -1.2289e+00
Epoch 3/10
19/19 - 1s - loss: 416.4268 - loglik: -4.1544e+02 - logprior: -9.9119e-01
Epoch 4/10
19/19 - 1s - loss: 415.3542 - loglik: -4.1440e+02 - logprior: -9.5030e-01
Epoch 5/10
19/19 - 1s - loss: 414.9745 - loglik: -4.1405e+02 - logprior: -9.2480e-01
Epoch 6/10
19/19 - 1s - loss: 414.9308 - loglik: -4.1401e+02 - logprior: -9.1651e-01
Epoch 7/10
19/19 - 1s - loss: 414.5743 - loglik: -4.1367e+02 - logprior: -9.0756e-01
Epoch 8/10
19/19 - 1s - loss: 413.9747 - loglik: -4.1309e+02 - logprior: -8.8823e-01
Epoch 9/10
19/19 - 1s - loss: 414.5147 - loglik: -4.1363e+02 - logprior: -8.8066e-01
Fitted a model with MAP estimate = -414.1613
Time for alignment: 50.0485
Fitting a model of length 65 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 475.2609 - loglik: -4.7214e+02 - logprior: -3.1183e+00
Epoch 2/10
19/19 - 1s - loss: 440.5294 - loglik: -4.3926e+02 - logprior: -1.2677e+00
Epoch 3/10
19/19 - 1s - loss: 428.4358 - loglik: -4.2719e+02 - logprior: -1.2451e+00
Epoch 4/10
19/19 - 1s - loss: 425.8687 - loglik: -4.2456e+02 - logprior: -1.3096e+00
Epoch 5/10
19/19 - 1s - loss: 424.0163 - loglik: -4.2273e+02 - logprior: -1.2865e+00
Epoch 6/10
19/19 - 1s - loss: 424.2515 - loglik: -4.2298e+02 - logprior: -1.2736e+00
Fitted a model with MAP estimate = -423.5803
expansions: [(7, 2), (8, 2), (9, 2), (23, 1), (24, 1), (31, 1), (35, 2), (37, 1), (46, 1), (48, 1), (53, 1), (55, 2), (57, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 426.2798 - loglik: -4.2238e+02 - logprior: -3.8993e+00
Epoch 2/2
19/19 - 1s - loss: 419.4814 - loglik: -4.1753e+02 - logprior: -1.9469e+00
Fitted a model with MAP estimate = -418.0948
expansions: [(0, 2)]
discards: [ 0 70]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 420.7207 - loglik: -4.1784e+02 - logprior: -2.8792e+00
Epoch 2/2
19/19 - 1s - loss: 417.5101 - loglik: -4.1638e+02 - logprior: -1.1272e+00
Fitted a model with MAP estimate = -416.8870
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 421.6049 - loglik: -4.1799e+02 - logprior: -3.6119e+00
Epoch 2/10
19/19 - 1s - loss: 417.7170 - loglik: -4.1656e+02 - logprior: -1.1561e+00
Epoch 3/10
19/19 - 1s - loss: 417.0500 - loglik: -4.1608e+02 - logprior: -9.6633e-01
Epoch 4/10
19/19 - 1s - loss: 416.1194 - loglik: -4.1519e+02 - logprior: -9.3126e-01
Epoch 5/10
19/19 - 1s - loss: 415.3031 - loglik: -4.1440e+02 - logprior: -9.0557e-01
Epoch 6/10
19/19 - 1s - loss: 415.8078 - loglik: -4.1492e+02 - logprior: -8.8907e-01
Fitted a model with MAP estimate = -415.0415
Time for alignment: 43.9686
Computed alignments with likelihoods: ['-414.7597', '-414.9626', '-413.3350', '-414.1613', '-415.0415']
Best model has likelihood: -413.3350  (prior= -0.8819 )
time for generating output: 0.1063
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07654.projection.fasta
SP score = 0.826530612244898
Training of 5 independent models on file PF04082.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 1322.0458 - loglik: -1.3203e+03 - logprior: -1.7360e+00
Epoch 2/10
39/39 - 15s - loss: 1273.9039 - loglik: -1.2729e+03 - logprior: -1.0367e+00
Epoch 3/10
39/39 - 15s - loss: 1267.5790 - loglik: -1.2665e+03 - logprior: -1.0866e+00
Epoch 4/10
39/39 - 15s - loss: 1262.7773 - loglik: -1.2617e+03 - logprior: -1.1007e+00
Epoch 5/10
39/39 - 15s - loss: 1259.5228 - loglik: -1.2584e+03 - logprior: -1.1188e+00
Epoch 6/10
39/39 - 15s - loss: 1255.8572 - loglik: -1.2547e+03 - logprior: -1.1521e+00
Epoch 7/10
39/39 - 15s - loss: 1256.1036 - loglik: -1.2549e+03 - logprior: -1.1725e+00
Fitted a model with MAP estimate = -1254.2138
expansions: [(21, 1), (22, 1), (24, 4), (28, 1), (47, 2), (48, 4), (57, 1), (62, 2), (63, 2), (65, 1), (81, 1), (82, 1), (83, 2), (84, 2), (104, 1), (105, 4), (106, 2), (110, 1), (121, 1), (124, 1), (127, 1), (134, 1), (144, 1), (148, 1), (151, 1), (155, 1), (156, 3), (161, 1), (163, 1), (165, 1), (166, 1), (167, 1), (169, 1), (176, 2), (179, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1280.0072 - loglik: -1.2782e+03 - logprior: -1.8451e+00
Epoch 2/2
39/39 - 23s - loss: 1261.7665 - loglik: -1.2612e+03 - logprior: -5.3491e-01
Fitted a model with MAP estimate = -1259.3701
expansions: [(31, 2), (54, 1), (183, 1), (189, 1)]
discards: [ 26  27  28  80 105 128 133 134 179 199 226 227 241 242 243]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1267.3959 - loglik: -1.2659e+03 - logprior: -1.5195e+00
Epoch 2/2
39/39 - 22s - loss: 1263.1265 - loglik: -1.2629e+03 - logprior: -2.2594e-01
Fitted a model with MAP estimate = -1260.8117
expansions: [(61, 1), (220, 2), (234, 9)]
discards: [56 57]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1265.1379 - loglik: -1.2636e+03 - logprior: -1.5043e+00
Epoch 2/10
39/39 - 23s - loss: 1260.9894 - loglik: -1.2607e+03 - logprior: -3.3141e-01
Epoch 3/10
39/39 - 24s - loss: 1257.9631 - loglik: -1.2577e+03 - logprior: -2.6387e-01
Epoch 4/10
39/39 - 24s - loss: 1253.7120 - loglik: -1.2535e+03 - logprior: -2.3785e-01
Epoch 5/10
39/39 - 24s - loss: 1248.5264 - loglik: -1.2483e+03 - logprior: -2.1296e-01
Epoch 6/10
39/39 - 24s - loss: 1245.3044 - loglik: -1.2451e+03 - logprior: -1.9810e-01
Epoch 7/10
39/39 - 24s - loss: 1242.7305 - loglik: -1.2425e+03 - logprior: -1.9612e-01
Epoch 8/10
39/39 - 24s - loss: 1241.7637 - loglik: -1.2416e+03 - logprior: -1.8179e-01
Epoch 9/10
39/39 - 24s - loss: 1240.4248 - loglik: -1.2403e+03 - logprior: -1.6575e-01
Epoch 10/10
39/39 - 24s - loss: 1239.8091 - loglik: -1.2397e+03 - logprior: -1.3571e-01
Fitted a model with MAP estimate = -1239.3711
Time for alignment: 532.8523
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1321.6116 - loglik: -1.3199e+03 - logprior: -1.7358e+00
Epoch 2/10
39/39 - 16s - loss: 1272.8435 - loglik: -1.2718e+03 - logprior: -1.0294e+00
Epoch 3/10
39/39 - 16s - loss: 1267.3248 - loglik: -1.2663e+03 - logprior: -1.0578e+00
Epoch 4/10
39/39 - 16s - loss: 1262.9557 - loglik: -1.2619e+03 - logprior: -1.0420e+00
Epoch 5/10
39/39 - 16s - loss: 1258.9082 - loglik: -1.2579e+03 - logprior: -1.0529e+00
Epoch 6/10
39/39 - 16s - loss: 1255.8820 - loglik: -1.2548e+03 - logprior: -1.0922e+00
Epoch 7/10
39/39 - 16s - loss: 1255.2170 - loglik: -1.2541e+03 - logprior: -1.1205e+00
Epoch 8/10
39/39 - 16s - loss: 1255.2993 - loglik: -1.2542e+03 - logprior: -1.1277e+00
Fitted a model with MAP estimate = -1253.5463
expansions: [(23, 1), (24, 6), (28, 1), (47, 2), (48, 5), (59, 1), (61, 2), (62, 2), (64, 1), (80, 1), (81, 1), (82, 2), (83, 2), (103, 1), (104, 3), (105, 2), (109, 1), (120, 1), (123, 1), (126, 1), (133, 1), (145, 1), (150, 1), (156, 3), (160, 1), (161, 1), (164, 1), (166, 1), (167, 1), (169, 1), (179, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1283.7776 - loglik: -1.2819e+03 - logprior: -1.9250e+00
Epoch 2/2
39/39 - 22s - loss: 1262.5703 - loglik: -1.2620e+03 - logprior: -5.3515e-01
Fitted a model with MAP estimate = -1259.8435
expansions: [(32, 2), (194, 1), (241, 10)]
discards: [ 26  27  28  29  55  56  81 106 134 198 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 1266.1552 - loglik: -1.2645e+03 - logprior: -1.6350e+00
Epoch 2/2
39/39 - 22s - loss: 1262.1375 - loglik: -1.2617e+03 - logprior: -4.6403e-01
Fitted a model with MAP estimate = -1259.4666
expansions: [(26, 3), (217, 1)]
discards: [124 232 233 234 235 236 237 238 239 240 241]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1264.7892 - loglik: -1.2633e+03 - logprior: -1.4525e+00
Epoch 2/10
39/39 - 21s - loss: 1261.9811 - loglik: -1.2618e+03 - logprior: -1.6032e-01
Epoch 3/10
39/39 - 21s - loss: 1258.0885 - loglik: -1.2580e+03 - logprior: -5.8363e-02
Epoch 4/10
39/39 - 21s - loss: 1255.0457 - loglik: -1.2551e+03 - logprior: 0.0103
Epoch 5/10
39/39 - 21s - loss: 1249.2615 - loglik: -1.2494e+03 - logprior: 0.0898
Epoch 6/10
39/39 - 21s - loss: 1245.3763 - loglik: -1.2456e+03 - logprior: 0.2033
Epoch 7/10
39/39 - 22s - loss: 1243.1562 - loglik: -1.2434e+03 - logprior: 0.2007
Epoch 8/10
39/39 - 21s - loss: 1241.7887 - loglik: -1.2420e+03 - logprior: 0.2153
Epoch 9/10
39/39 - 21s - loss: 1241.1173 - loglik: -1.2414e+03 - logprior: 0.2393
Epoch 10/10
39/39 - 22s - loss: 1240.4209 - loglik: -1.2407e+03 - logprior: 0.2425
Fitted a model with MAP estimate = -1239.5911
Time for alignment: 532.5540
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1322.0424 - loglik: -1.3203e+03 - logprior: -1.7410e+00
Epoch 2/10
39/39 - 16s - loss: 1274.4188 - loglik: -1.2734e+03 - logprior: -1.0491e+00
Epoch 3/10
39/39 - 16s - loss: 1268.6632 - loglik: -1.2676e+03 - logprior: -1.0695e+00
Epoch 4/10
39/39 - 16s - loss: 1264.8822 - loglik: -1.2638e+03 - logprior: -1.0501e+00
Epoch 5/10
39/39 - 16s - loss: 1259.5042 - loglik: -1.2584e+03 - logprior: -1.0924e+00
Epoch 6/10
39/39 - 16s - loss: 1257.2817 - loglik: -1.2561e+03 - logprior: -1.1325e+00
Epoch 7/10
39/39 - 16s - loss: 1257.4298 - loglik: -1.2563e+03 - logprior: -1.1603e+00
Fitted a model with MAP estimate = -1255.3149
expansions: [(23, 1), (25, 4), (29, 1), (48, 3), (54, 1), (63, 2), (64, 2), (66, 1), (81, 1), (82, 2), (83, 2), (84, 3), (104, 1), (105, 4), (106, 2), (108, 1), (109, 1), (123, 1), (124, 1), (126, 1), (135, 1), (137, 1), (147, 1), (150, 1), (154, 1), (156, 1), (160, 1), (161, 1), (162, 1), (166, 1), (167, 1), (168, 1), (169, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 1280.6846 - loglik: -1.2788e+03 - logprior: -1.8672e+00
Epoch 2/2
39/39 - 22s - loss: 1262.5918 - loglik: -1.2620e+03 - logprior: -5.5531e-01
Fitted a model with MAP estimate = -1260.0203
expansions: [(31, 3), (54, 1), (55, 1)]
discards: [ 26  27  28  56  57  58  77  96  99 126 131 132 236 237 238]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 24s - loss: 1267.8507 - loglik: -1.2663e+03 - logprior: -1.5334e+00
Epoch 2/2
39/39 - 21s - loss: 1264.0891 - loglik: -1.2638e+03 - logprior: -2.4387e-01
Fitted a model with MAP estimate = -1261.2644
expansions: [(26, 3), (57, 1), (180, 1), (229, 9)]
discards: [34 35 36 72]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1265.2598 - loglik: -1.2637e+03 - logprior: -1.5116e+00
Epoch 2/10
39/39 - 21s - loss: 1261.5955 - loglik: -1.2612e+03 - logprior: -3.4768e-01
Epoch 3/10
39/39 - 21s - loss: 1258.2362 - loglik: -1.2580e+03 - logprior: -2.6935e-01
Epoch 4/10
39/39 - 21s - loss: 1253.7567 - loglik: -1.2535e+03 - logprior: -2.2672e-01
Epoch 5/10
39/39 - 21s - loss: 1249.2875 - loglik: -1.2491e+03 - logprior: -1.9538e-01
Epoch 6/10
39/39 - 21s - loss: 1245.0901 - loglik: -1.2449e+03 - logprior: -1.8795e-01
Epoch 7/10
39/39 - 21s - loss: 1244.0148 - loglik: -1.2438e+03 - logprior: -1.8774e-01
Epoch 8/10
39/39 - 21s - loss: 1242.1693 - loglik: -1.2420e+03 - logprior: -1.8371e-01
Epoch 9/10
39/39 - 21s - loss: 1242.4058 - loglik: -1.2422e+03 - logprior: -1.6121e-01
Fitted a model with MAP estimate = -1240.6806
Time for alignment: 488.3512
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1322.3627 - loglik: -1.3206e+03 - logprior: -1.7397e+00
Epoch 2/10
39/39 - 16s - loss: 1272.8866 - loglik: -1.2719e+03 - logprior: -1.0320e+00
Epoch 3/10
39/39 - 16s - loss: 1266.9934 - loglik: -1.2659e+03 - logprior: -1.0478e+00
Epoch 4/10
39/39 - 17s - loss: 1263.3412 - loglik: -1.2623e+03 - logprior: -1.0410e+00
Epoch 5/10
39/39 - 17s - loss: 1258.9285 - loglik: -1.2578e+03 - logprior: -1.0833e+00
Epoch 6/10
39/39 - 17s - loss: 1255.9060 - loglik: -1.2548e+03 - logprior: -1.1161e+00
Epoch 7/10
39/39 - 18s - loss: 1255.6086 - loglik: -1.2545e+03 - logprior: -1.1364e+00
Epoch 8/10
39/39 - 19s - loss: 1254.4813 - loglik: -1.2533e+03 - logprior: -1.1573e+00
Epoch 9/10
39/39 - 19s - loss: 1254.4082 - loglik: -1.2532e+03 - logprior: -1.1764e+00
Epoch 10/10
39/39 - 20s - loss: 1254.1161 - loglik: -1.2529e+03 - logprior: -1.1888e+00
Fitted a model with MAP estimate = -1252.9625
expansions: [(21, 1), (22, 1), (23, 1), (24, 4), (28, 1), (47, 1), (48, 5), (59, 1), (61, 2), (62, 2), (64, 1), (80, 1), (81, 1), (82, 2), (83, 2), (104, 4), (105, 2), (109, 1), (120, 1), (123, 1), (126, 1), (133, 1), (147, 1), (149, 1), (155, 3), (160, 1), (162, 1), (166, 1), (167, 1), (169, 1), (179, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1290.6346 - loglik: -1.2886e+03 - logprior: -2.0449e+00
Epoch 2/2
39/39 - 25s - loss: 1263.6510 - loglik: -1.2630e+03 - logprior: -6.0584e-01
Fitted a model with MAP estimate = -1260.5612
expansions: [(32, 3), (194, 1), (239, 10)]
discards: [ 26  27  28  55  56 105 132 133 195 196 236 237 238]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1266.4043 - loglik: -1.2648e+03 - logprior: -1.6140e+00
Epoch 2/2
39/39 - 27s - loss: 1261.9988 - loglik: -1.2615e+03 - logprior: -4.5134e-01
Fitted a model with MAP estimate = -1259.1357
expansions: [(56, 1)]
discards: [ 26  35  36  37  54 232 233 234 235 236 237 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 25s - loss: 1266.1852 - loglik: -1.2648e+03 - logprior: -1.4279e+00
Epoch 2/10
39/39 - 22s - loss: 1262.9365 - loglik: -1.2628e+03 - logprior: -1.4256e-01
Epoch 3/10
39/39 - 22s - loss: 1259.5533 - loglik: -1.2595e+03 - logprior: -2.0326e-02
Epoch 4/10
39/39 - 22s - loss: 1256.1016 - loglik: -1.2561e+03 - logprior: 0.0416
Epoch 5/10
39/39 - 22s - loss: 1250.2592 - loglik: -1.2504e+03 - logprior: 0.1372
Epoch 6/10
39/39 - 24s - loss: 1246.4891 - loglik: -1.2467e+03 - logprior: 0.1989
Epoch 7/10
39/39 - 25s - loss: 1244.5848 - loglik: -1.2448e+03 - logprior: 0.2087
Epoch 8/10
39/39 - 24s - loss: 1242.9774 - loglik: -1.2432e+03 - logprior: 0.2148
Epoch 9/10
39/39 - 23s - loss: 1241.9438 - loglik: -1.2422e+03 - logprior: 0.2411
Epoch 10/10
39/39 - 23s - loss: 1241.9529 - loglik: -1.2422e+03 - logprior: 0.2579
Fitted a model with MAP estimate = -1240.9719
Time for alignment: 617.5020
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1322.9703 - loglik: -1.3212e+03 - logprior: -1.7353e+00
Epoch 2/10
39/39 - 17s - loss: 1273.4092 - loglik: -1.2723e+03 - logprior: -1.0790e+00
Epoch 3/10
39/39 - 17s - loss: 1267.1185 - loglik: -1.2660e+03 - logprior: -1.1500e+00
Epoch 4/10
39/39 - 19s - loss: 1262.9719 - loglik: -1.2618e+03 - logprior: -1.1288e+00
Epoch 5/10
39/39 - 18s - loss: 1259.1677 - loglik: -1.2580e+03 - logprior: -1.1506e+00
Epoch 6/10
39/39 - 20s - loss: 1256.0768 - loglik: -1.2549e+03 - logprior: -1.1783e+00
Epoch 7/10
39/39 - 20s - loss: 1255.5282 - loglik: -1.2543e+03 - logprior: -1.2053e+00
Epoch 8/10
39/39 - 20s - loss: 1255.4182 - loglik: -1.2542e+03 - logprior: -1.2348e+00
Epoch 9/10
39/39 - 21s - loss: 1254.7598 - loglik: -1.2535e+03 - logprior: -1.2445e+00
Epoch 10/10
39/39 - 21s - loss: 1252.7794 - loglik: -1.2515e+03 - logprior: -1.2580e+00
Fitted a model with MAP estimate = -1253.1977
expansions: [(23, 1), (24, 4), (27, 1), (28, 1), (48, 5), (50, 1), (54, 1), (63, 2), (64, 2), (66, 1), (82, 1), (83, 1), (85, 1), (86, 1), (106, 1), (107, 4), (108, 2), (110, 1), (111, 1), (125, 1), (126, 1), (128, 1), (133, 1), (136, 1), (146, 1), (155, 1), (156, 5), (161, 1), (163, 1), (165, 1), (166, 1), (169, 1), (170, 1), (176, 2), (179, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 30s - loss: 1289.7010 - loglik: -1.2877e+03 - logprior: -1.9873e+00
Epoch 2/2
39/39 - 28s - loss: 1262.6959 - loglik: -1.2622e+03 - logprior: -5.3382e-01
Fitted a model with MAP estimate = -1259.9757
expansions: [(188, 1), (244, 10)]
discards: [ 25  26  27  78  81 133 134 197 198 199 226 227 241 242 243]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 1266.2893 - loglik: -1.2647e+03 - logprior: -1.5958e+00
Epoch 2/2
39/39 - 30s - loss: 1262.2723 - loglik: -1.2618e+03 - logprior: -4.3298e-01
Fitted a model with MAP estimate = -1259.5100
expansions: [(26, 3), (182, 1)]
discards: [ 53  56 124 231 232 233 234 235 236 237 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 10005 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1265.3817 - loglik: -1.2640e+03 - logprior: -1.4231e+00
Epoch 2/10
39/39 - 24s - loss: 1261.3121 - loglik: -1.2612e+03 - logprior: -1.0206e-01
Epoch 3/10
39/39 - 25s - loss: 1259.0901 - loglik: -1.2591e+03 - logprior: 0.0060
Epoch 4/10
39/39 - 25s - loss: 1254.1914 - loglik: -1.2543e+03 - logprior: 0.0803
Epoch 5/10
39/39 - 27s - loss: 1249.8743 - loglik: -1.2500e+03 - logprior: 0.1415
Epoch 6/10
39/39 - 25s - loss: 1245.3967 - loglik: -1.2457e+03 - logprior: 0.2549
Epoch 7/10
39/39 - 24s - loss: 1243.2285 - loglik: -1.2435e+03 - logprior: 0.2701
Epoch 8/10
39/39 - 23s - loss: 1242.7274 - loglik: -1.2430e+03 - logprior: 0.2951
Epoch 9/10
39/39 - 23s - loss: 1241.7886 - loglik: -1.2421e+03 - logprior: 0.2951
Epoch 10/10
39/39 - 23s - loss: 1239.6705 - loglik: -1.2400e+03 - logprior: 0.3250
Fitted a model with MAP estimate = -1239.8567
Time for alignment: 664.3675
Computed alignments with likelihoods: ['-1239.3711', '-1239.5911', '-1240.6806', '-1240.9719', '-1239.8567']
Best model has likelihood: -1239.3711  (prior= -0.1111 )
time for generating output: 0.2079
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF04082.projection.fasta
SP score = 0.6609778597785978
Training of 5 independent models on file PF00046.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 318.3019 - loglik: -3.1508e+02 - logprior: -3.2265e+00
Epoch 2/10
19/19 - 1s - loss: 287.4848 - loglik: -2.8595e+02 - logprior: -1.5385e+00
Epoch 3/10
19/19 - 1s - loss: 274.8065 - loglik: -2.7322e+02 - logprior: -1.5865e+00
Epoch 4/10
19/19 - 1s - loss: 272.1275 - loglik: -2.7058e+02 - logprior: -1.5490e+00
Epoch 5/10
19/19 - 1s - loss: 270.7390 - loglik: -2.6921e+02 - logprior: -1.5289e+00
Epoch 6/10
19/19 - 1s - loss: 270.1601 - loglik: -2.6866e+02 - logprior: -1.5035e+00
Epoch 7/10
19/19 - 1s - loss: 270.3825 - loglik: -2.6890e+02 - logprior: -1.4873e+00
Fitted a model with MAP estimate = -269.9832
expansions: [(5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 271.5813 - loglik: -2.6828e+02 - logprior: -3.2998e+00
Epoch 2/2
19/19 - 1s - loss: 264.4076 - loglik: -2.6301e+02 - logprior: -1.3965e+00
Fitted a model with MAP estimate = -263.2662
expansions: []
discards: [21 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.5963 - loglik: -2.6342e+02 - logprior: -3.1725e+00
Epoch 2/2
19/19 - 1s - loss: 263.5279 - loglik: -2.6226e+02 - logprior: -1.2692e+00
Fitted a model with MAP estimate = -262.9599
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.0998 - loglik: -2.6296e+02 - logprior: -3.1367e+00
Epoch 2/10
19/19 - 1s - loss: 263.2358 - loglik: -2.6199e+02 - logprior: -1.2504e+00
Epoch 3/10
19/19 - 1s - loss: 262.9733 - loglik: -2.6183e+02 - logprior: -1.1402e+00
Epoch 4/10
19/19 - 1s - loss: 262.3647 - loglik: -2.6128e+02 - logprior: -1.0867e+00
Epoch 5/10
19/19 - 1s - loss: 262.1351 - loglik: -2.6107e+02 - logprior: -1.0607e+00
Epoch 6/10
19/19 - 1s - loss: 261.7319 - loglik: -2.6068e+02 - logprior: -1.0518e+00
Epoch 7/10
19/19 - 1s - loss: 261.6930 - loglik: -2.6066e+02 - logprior: -1.0345e+00
Epoch 8/10
19/19 - 1s - loss: 261.8690 - loglik: -2.6085e+02 - logprior: -1.0188e+00
Fitted a model with MAP estimate = -261.5383
Time for alignment: 39.8882
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 318.2723 - loglik: -3.1504e+02 - logprior: -3.2290e+00
Epoch 2/10
19/19 - 1s - loss: 287.1729 - loglik: -2.8563e+02 - logprior: -1.5431e+00
Epoch 3/10
19/19 - 1s - loss: 275.3034 - loglik: -2.7376e+02 - logprior: -1.5416e+00
Epoch 4/10
19/19 - 1s - loss: 272.5717 - loglik: -2.7107e+02 - logprior: -1.5014e+00
Epoch 5/10
19/19 - 1s - loss: 271.3234 - loglik: -2.6985e+02 - logprior: -1.4702e+00
Epoch 6/10
19/19 - 1s - loss: 271.0633 - loglik: -2.6961e+02 - logprior: -1.4522e+00
Epoch 7/10
19/19 - 1s - loss: 270.7579 - loglik: -2.6932e+02 - logprior: -1.4384e+00
Epoch 8/10
19/19 - 1s - loss: 270.9664 - loglik: -2.6954e+02 - logprior: -1.4280e+00
Fitted a model with MAP estimate = -270.4927
expansions: [(5, 1), (7, 2), (9, 1), (16, 1), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 270.9507 - loglik: -2.6768e+02 - logprior: -3.2717e+00
Epoch 2/2
19/19 - 1s - loss: 264.1325 - loglik: -2.6276e+02 - logprior: -1.3765e+00
Fitted a model with MAP estimate = -263.2499
expansions: []
discards: [35 38]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.4011 - loglik: -2.6323e+02 - logprior: -3.1720e+00
Epoch 2/2
19/19 - 1s - loss: 263.7057 - loglik: -2.6244e+02 - logprior: -1.2698e+00
Fitted a model with MAP estimate = -262.9379
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.0273 - loglik: -2.6289e+02 - logprior: -3.1356e+00
Epoch 2/10
19/19 - 1s - loss: 263.3294 - loglik: -2.6208e+02 - logprior: -1.2529e+00
Epoch 3/10
19/19 - 1s - loss: 262.8214 - loglik: -2.6169e+02 - logprior: -1.1338e+00
Epoch 4/10
19/19 - 1s - loss: 262.3580 - loglik: -2.6126e+02 - logprior: -1.0931e+00
Epoch 5/10
19/19 - 1s - loss: 262.1573 - loglik: -2.6110e+02 - logprior: -1.0609e+00
Epoch 6/10
19/19 - 1s - loss: 261.7071 - loglik: -2.6065e+02 - logprior: -1.0558e+00
Epoch 7/10
19/19 - 1s - loss: 261.6327 - loglik: -2.6060e+02 - logprior: -1.0346e+00
Epoch 8/10
19/19 - 1s - loss: 261.7870 - loglik: -2.6076e+02 - logprior: -1.0267e+00
Fitted a model with MAP estimate = -261.5535
Time for alignment: 40.1726
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 318.2005 - loglik: -3.1497e+02 - logprior: -3.2278e+00
Epoch 2/10
19/19 - 1s - loss: 287.2433 - loglik: -2.8570e+02 - logprior: -1.5410e+00
Epoch 3/10
19/19 - 1s - loss: 274.6684 - loglik: -2.7308e+02 - logprior: -1.5920e+00
Epoch 4/10
19/19 - 1s - loss: 271.6604 - loglik: -2.7010e+02 - logprior: -1.5603e+00
Epoch 5/10
19/19 - 1s - loss: 270.5315 - loglik: -2.6900e+02 - logprior: -1.5284e+00
Epoch 6/10
19/19 - 1s - loss: 270.7754 - loglik: -2.6927e+02 - logprior: -1.5020e+00
Fitted a model with MAP estimate = -270.0913
expansions: [(5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 271.7336 - loglik: -2.6843e+02 - logprior: -3.3009e+00
Epoch 2/2
19/19 - 1s - loss: 264.1017 - loglik: -2.6270e+02 - logprior: -1.4000e+00
Fitted a model with MAP estimate = -263.2592
expansions: []
discards: [21 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.5499 - loglik: -2.6338e+02 - logprior: -3.1721e+00
Epoch 2/2
19/19 - 1s - loss: 263.6516 - loglik: -2.6238e+02 - logprior: -1.2698e+00
Fitted a model with MAP estimate = -262.9614
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.0616 - loglik: -2.6293e+02 - logprior: -3.1334e+00
Epoch 2/10
19/19 - 1s - loss: 263.4178 - loglik: -2.6216e+02 - logprior: -1.2602e+00
Epoch 3/10
19/19 - 1s - loss: 262.7372 - loglik: -2.6160e+02 - logprior: -1.1339e+00
Epoch 4/10
19/19 - 1s - loss: 262.5995 - loglik: -2.6151e+02 - logprior: -1.0846e+00
Epoch 5/10
19/19 - 1s - loss: 262.0289 - loglik: -2.6096e+02 - logprior: -1.0674e+00
Epoch 6/10
19/19 - 1s - loss: 261.7434 - loglik: -2.6069e+02 - logprior: -1.0553e+00
Epoch 7/10
19/19 - 1s - loss: 261.7078 - loglik: -2.6068e+02 - logprior: -1.0303e+00
Epoch 8/10
19/19 - 1s - loss: 261.7789 - loglik: -2.6075e+02 - logprior: -1.0298e+00
Fitted a model with MAP estimate = -261.5483
Time for alignment: 38.4023
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 318.0971 - loglik: -3.1487e+02 - logprior: -3.2307e+00
Epoch 2/10
19/19 - 1s - loss: 287.3246 - loglik: -2.8579e+02 - logprior: -1.5373e+00
Epoch 3/10
19/19 - 1s - loss: 274.8232 - loglik: -2.7323e+02 - logprior: -1.5903e+00
Epoch 4/10
19/19 - 1s - loss: 271.8038 - loglik: -2.7025e+02 - logprior: -1.5561e+00
Epoch 5/10
19/19 - 1s - loss: 270.7171 - loglik: -2.6919e+02 - logprior: -1.5290e+00
Epoch 6/10
19/19 - 1s - loss: 270.8315 - loglik: -2.6932e+02 - logprior: -1.5075e+00
Fitted a model with MAP estimate = -270.0968
expansions: [(5, 1), (7, 1), (10, 1), (16, 1), (17, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 271.7638 - loglik: -2.6846e+02 - logprior: -3.3021e+00
Epoch 2/2
19/19 - 1s - loss: 264.3305 - loglik: -2.6293e+02 - logprior: -1.3966e+00
Fitted a model with MAP estimate = -263.2490
expansions: []
discards: [21 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.4625 - loglik: -2.6329e+02 - logprior: -3.1711e+00
Epoch 2/2
19/19 - 1s - loss: 263.5334 - loglik: -2.6226e+02 - logprior: -1.2710e+00
Fitted a model with MAP estimate = -262.9771
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.0520 - loglik: -2.6291e+02 - logprior: -3.1386e+00
Epoch 2/10
19/19 - 1s - loss: 263.2491 - loglik: -2.6200e+02 - logprior: -1.2531e+00
Epoch 3/10
19/19 - 1s - loss: 262.8195 - loglik: -2.6169e+02 - logprior: -1.1317e+00
Epoch 4/10
19/19 - 1s - loss: 262.3598 - loglik: -2.6126e+02 - logprior: -1.0963e+00
Epoch 5/10
19/19 - 1s - loss: 262.2068 - loglik: -2.6115e+02 - logprior: -1.0613e+00
Epoch 6/10
19/19 - 1s - loss: 261.9968 - loglik: -2.6094e+02 - logprior: -1.0534e+00
Epoch 7/10
19/19 - 1s - loss: 261.6984 - loglik: -2.6066e+02 - logprior: -1.0398e+00
Epoch 8/10
19/19 - 1s - loss: 261.4540 - loglik: -2.6043e+02 - logprior: -1.0215e+00
Epoch 9/10
19/19 - 1s - loss: 261.8440 - loglik: -2.6083e+02 - logprior: -1.0123e+00
Fitted a model with MAP estimate = -261.5137
Time for alignment: 38.6406
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 318.0553 - loglik: -3.1482e+02 - logprior: -3.2325e+00
Epoch 2/10
19/19 - 1s - loss: 287.9956 - loglik: -2.8647e+02 - logprior: -1.5251e+00
Epoch 3/10
19/19 - 1s - loss: 276.0157 - loglik: -2.7449e+02 - logprior: -1.5208e+00
Epoch 4/10
19/19 - 1s - loss: 272.6297 - loglik: -2.7114e+02 - logprior: -1.4945e+00
Epoch 5/10
19/19 - 1s - loss: 271.8235 - loglik: -2.7037e+02 - logprior: -1.4566e+00
Epoch 6/10
19/19 - 1s - loss: 271.2108 - loglik: -2.6977e+02 - logprior: -1.4400e+00
Epoch 7/10
19/19 - 1s - loss: 270.6687 - loglik: -2.6923e+02 - logprior: -1.4374e+00
Epoch 8/10
19/19 - 1s - loss: 270.7701 - loglik: -2.6935e+02 - logprior: -1.4155e+00
Fitted a model with MAP estimate = -270.4911
expansions: [(5, 1), (7, 2), (9, 1), (16, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 1), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 271.6803 - loglik: -2.6841e+02 - logprior: -3.2746e+00
Epoch 2/2
19/19 - 1s - loss: 264.2198 - loglik: -2.6283e+02 - logprior: -1.3912e+00
Fitted a model with MAP estimate = -263.3128
expansions: []
discards: [20 36 39]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 266.4458 - loglik: -2.6327e+02 - logprior: -3.1714e+00
Epoch 2/2
19/19 - 1s - loss: 263.6604 - loglik: -2.6239e+02 - logprior: -1.2711e+00
Fitted a model with MAP estimate = -262.9689
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10009 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 266.0218 - loglik: -2.6289e+02 - logprior: -3.1306e+00
Epoch 2/10
19/19 - 1s - loss: 263.3894 - loglik: -2.6213e+02 - logprior: -1.2609e+00
Epoch 3/10
19/19 - 1s - loss: 262.8499 - loglik: -2.6172e+02 - logprior: -1.1293e+00
Epoch 4/10
19/19 - 1s - loss: 262.6026 - loglik: -2.6152e+02 - logprior: -1.0864e+00
Epoch 5/10
19/19 - 1s - loss: 261.7850 - loglik: -2.6072e+02 - logprior: -1.0683e+00
Epoch 6/10
19/19 - 1s - loss: 261.8354 - loglik: -2.6079e+02 - logprior: -1.0459e+00
Fitted a model with MAP estimate = -261.6729
Time for alignment: 38.5682
Computed alignments with likelihoods: ['-261.5383', '-261.5535', '-261.5483', '-261.5137', '-261.6729']
Best model has likelihood: -261.5137  (prior= -1.0008 )
time for generating output: 0.0904
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00046.projection.fasta
SP score = 0.9791666666666666
Training of 5 independent models on file PF01814.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 738.5430 - loglik: -7.3556e+02 - logprior: -2.9794e+00
Epoch 2/10
19/19 - 3s - loss: 709.9475 - loglik: -7.0894e+02 - logprior: -1.0085e+00
Epoch 3/10
19/19 - 3s - loss: 697.9077 - loglik: -6.9667e+02 - logprior: -1.2377e+00
Epoch 4/10
19/19 - 3s - loss: 694.5331 - loglik: -6.9342e+02 - logprior: -1.1160e+00
Epoch 5/10
19/19 - 3s - loss: 691.9804 - loglik: -6.9086e+02 - logprior: -1.1190e+00
Epoch 6/10
19/19 - 3s - loss: 690.5776 - loglik: -6.8945e+02 - logprior: -1.1235e+00
Epoch 7/10
19/19 - 3s - loss: 689.0276 - loglik: -6.8788e+02 - logprior: -1.1484e+00
Epoch 8/10
19/19 - 3s - loss: 688.1686 - loglik: -6.8701e+02 - logprior: -1.1590e+00
Epoch 9/10
19/19 - 3s - loss: 688.2899 - loglik: -6.8713e+02 - logprior: -1.1629e+00
Fitted a model with MAP estimate = -686.9611
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 5), (23, 2), (24, 2), (25, 1), (28, 1), (51, 1), (53, 3), (73, 1), (74, 1), (75, 3), (76, 2), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 704.0663 - loglik: -7.0034e+02 - logprior: -3.7254e+00
Epoch 2/2
19/19 - 5s - loss: 690.5270 - loglik: -6.8872e+02 - logprior: -1.8037e+00
Fitted a model with MAP estimate = -688.0276
expansions: [(0, 2)]
discards: [  0  25  26  34  99 111]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 690.8301 - loglik: -6.8808e+02 - logprior: -2.7481e+00
Epoch 2/2
19/19 - 5s - loss: 687.9574 - loglik: -6.8702e+02 - logprior: -9.3817e-01
Fitted a model with MAP estimate = -686.3085
expansions: [(27, 2)]
discards: [ 1 30]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 689.9501 - loglik: -6.8730e+02 - logprior: -2.6486e+00
Epoch 2/10
19/19 - 5s - loss: 686.8654 - loglik: -6.8599e+02 - logprior: -8.7079e-01
Epoch 3/10
19/19 - 5s - loss: 685.1980 - loglik: -6.8447e+02 - logprior: -7.3165e-01
Epoch 4/10
19/19 - 5s - loss: 685.0377 - loglik: -6.8422e+02 - logprior: -8.2015e-01
Epoch 5/10
19/19 - 5s - loss: 682.9751 - loglik: -6.8219e+02 - logprior: -7.8550e-01
Epoch 6/10
19/19 - 5s - loss: 681.4479 - loglik: -6.8067e+02 - logprior: -7.7729e-01
Epoch 7/10
19/19 - 5s - loss: 680.3406 - loglik: -6.7959e+02 - logprior: -7.5393e-01
Epoch 8/10
19/19 - 5s - loss: 678.3660 - loglik: -6.7762e+02 - logprior: -7.4439e-01
Epoch 9/10
19/19 - 5s - loss: 679.0175 - loglik: -6.7828e+02 - logprior: -7.3691e-01
Fitted a model with MAP estimate = -678.0275
Time for alignment: 127.8415
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 738.8774 - loglik: -7.3590e+02 - logprior: -2.9781e+00
Epoch 2/10
19/19 - 4s - loss: 710.2714 - loglik: -7.0925e+02 - logprior: -1.0208e+00
Epoch 3/10
19/19 - 4s - loss: 698.1984 - loglik: -6.9695e+02 - logprior: -1.2469e+00
Epoch 4/10
19/19 - 4s - loss: 694.1422 - loglik: -6.9298e+02 - logprior: -1.1605e+00
Epoch 5/10
19/19 - 3s - loss: 691.8560 - loglik: -6.9068e+02 - logprior: -1.1806e+00
Epoch 6/10
19/19 - 4s - loss: 690.0656 - loglik: -6.8890e+02 - logprior: -1.1699e+00
Epoch 7/10
19/19 - 4s - loss: 689.3322 - loglik: -6.8818e+02 - logprior: -1.1527e+00
Epoch 8/10
19/19 - 3s - loss: 688.0002 - loglik: -6.8685e+02 - logprior: -1.1455e+00
Epoch 9/10
19/19 - 4s - loss: 687.9931 - loglik: -6.8684e+02 - logprior: -1.1512e+00
Epoch 10/10
19/19 - 4s - loss: 687.2188 - loglik: -6.8606e+02 - logprior: -1.1581e+00
Fitted a model with MAP estimate = -686.9748
expansions: [(15, 1), (18, 1), (19, 1), (21, 4), (23, 2), (26, 1), (29, 1), (32, 1), (51, 2), (54, 4), (74, 1), (75, 3), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 705.7352 - loglik: -7.0201e+02 - logprior: -3.7277e+00
Epoch 2/2
19/19 - 5s - loss: 691.3874 - loglik: -6.8957e+02 - logprior: -1.8133e+00
Fitted a model with MAP estimate = -688.6347
expansions: [(0, 2), (26, 2)]
discards: [  0 108]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 690.2921 - loglik: -6.8752e+02 - logprior: -2.7702e+00
Epoch 2/2
19/19 - 5s - loss: 687.5267 - loglik: -6.8655e+02 - logprior: -9.8062e-01
Fitted a model with MAP estimate = -686.0485
expansions: [(23, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 689.3720 - loglik: -6.8665e+02 - logprior: -2.7196e+00
Epoch 2/10
19/19 - 6s - loss: 686.5771 - loglik: -6.8564e+02 - logprior: -9.3665e-01
Epoch 3/10
19/19 - 5s - loss: 685.4276 - loglik: -6.8456e+02 - logprior: -8.6691e-01
Epoch 4/10
19/19 - 6s - loss: 684.2421 - loglik: -6.8342e+02 - logprior: -8.1802e-01
Epoch 5/10
19/19 - 5s - loss: 682.1207 - loglik: -6.8134e+02 - logprior: -7.8535e-01
Epoch 6/10
19/19 - 5s - loss: 681.2184 - loglik: -6.8045e+02 - logprior: -7.6700e-01
Epoch 7/10
19/19 - 5s - loss: 680.7150 - loglik: -6.7997e+02 - logprior: -7.4974e-01
Epoch 8/10
19/19 - 6s - loss: 679.0434 - loglik: -6.7831e+02 - logprior: -7.3613e-01
Epoch 9/10
19/19 - 6s - loss: 677.3226 - loglik: -6.7660e+02 - logprior: -7.2122e-01
Epoch 10/10
19/19 - 6s - loss: 677.2158 - loglik: -6.7650e+02 - logprior: -7.1200e-01
Fitted a model with MAP estimate = -677.2118
Time for alignment: 151.0687
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 738.6882 - loglik: -7.3571e+02 - logprior: -2.9825e+00
Epoch 2/10
19/19 - 4s - loss: 710.8192 - loglik: -7.0980e+02 - logprior: -1.0149e+00
Epoch 3/10
19/19 - 4s - loss: 697.4567 - loglik: -6.9622e+02 - logprior: -1.2346e+00
Epoch 4/10
19/19 - 4s - loss: 695.0840 - loglik: -6.9394e+02 - logprior: -1.1400e+00
Epoch 5/10
19/19 - 4s - loss: 691.7625 - loglik: -6.9061e+02 - logprior: -1.1531e+00
Epoch 6/10
19/19 - 4s - loss: 690.3413 - loglik: -6.8919e+02 - logprior: -1.1529e+00
Epoch 7/10
19/19 - 4s - loss: 689.6613 - loglik: -6.8851e+02 - logprior: -1.1510e+00
Epoch 8/10
19/19 - 4s - loss: 688.0952 - loglik: -6.8694e+02 - logprior: -1.1543e+00
Epoch 9/10
19/19 - 4s - loss: 687.7137 - loglik: -6.8656e+02 - logprior: -1.1539e+00
Epoch 10/10
19/19 - 4s - loss: 687.6499 - loglik: -6.8649e+02 - logprior: -1.1606e+00
Fitted a model with MAP estimate = -687.1528
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (24, 3), (27, 2), (29, 1), (32, 1), (51, 2), (54, 4), (74, 1), (75, 3), (76, 1), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 706.1335 - loglik: -7.0242e+02 - logprior: -3.7149e+00
Epoch 2/2
19/19 - 5s - loss: 691.3315 - loglik: -6.8948e+02 - logprior: -1.8552e+00
Fitted a model with MAP estimate = -688.3143
expansions: [(0, 2)]
discards: [  0  24  25  26  37 111]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 691.2495 - loglik: -6.8850e+02 - logprior: -2.7518e+00
Epoch 2/2
19/19 - 5s - loss: 687.8831 - loglik: -6.8694e+02 - logprior: -9.4282e-01
Fitted a model with MAP estimate = -686.7154
expansions: [(25, 2)]
discards: [29]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 690.1163 - loglik: -6.8741e+02 - logprior: -2.7081e+00
Epoch 2/10
19/19 - 5s - loss: 686.7645 - loglik: -6.8585e+02 - logprior: -9.1243e-01
Epoch 3/10
19/19 - 5s - loss: 685.7385 - loglik: -6.8489e+02 - logprior: -8.5219e-01
Epoch 4/10
19/19 - 5s - loss: 684.8923 - loglik: -6.8409e+02 - logprior: -7.9973e-01
Epoch 5/10
19/19 - 5s - loss: 683.0499 - loglik: -6.8228e+02 - logprior: -7.6801e-01
Epoch 6/10
19/19 - 5s - loss: 680.8355 - loglik: -6.8008e+02 - logprior: -7.5884e-01
Epoch 7/10
19/19 - 5s - loss: 680.4606 - loglik: -6.7972e+02 - logprior: -7.4055e-01
Epoch 8/10
19/19 - 5s - loss: 678.8558 - loglik: -6.7813e+02 - logprior: -7.2765e-01
Epoch 9/10
19/19 - 5s - loss: 679.3854 - loglik: -6.7867e+02 - logprior: -7.1742e-01
Fitted a model with MAP estimate = -678.0486
Time for alignment: 140.4458
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 738.7226 - loglik: -7.3575e+02 - logprior: -2.9766e+00
Epoch 2/10
19/19 - 3s - loss: 711.0562 - loglik: -7.1004e+02 - logprior: -1.0201e+00
Epoch 3/10
19/19 - 3s - loss: 699.0096 - loglik: -6.9778e+02 - logprior: -1.2291e+00
Epoch 4/10
19/19 - 4s - loss: 695.4634 - loglik: -6.9433e+02 - logprior: -1.1305e+00
Epoch 5/10
19/19 - 4s - loss: 692.1334 - loglik: -6.9098e+02 - logprior: -1.1544e+00
Epoch 6/10
19/19 - 3s - loss: 691.0358 - loglik: -6.8987e+02 - logprior: -1.1611e+00
Epoch 7/10
19/19 - 3s - loss: 688.9219 - loglik: -6.8775e+02 - logprior: -1.1730e+00
Epoch 8/10
19/19 - 4s - loss: 687.9822 - loglik: -6.8681e+02 - logprior: -1.1754e+00
Epoch 9/10
19/19 - 4s - loss: 688.3518 - loglik: -6.8717e+02 - logprior: -1.1840e+00
Fitted a model with MAP estimate = -687.2752
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 4), (24, 3), (27, 2), (29, 1), (32, 1), (51, 2), (54, 3), (73, 1), (74, 1), (75, 1), (76, 3), (82, 1), (84, 3), (87, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 704.2980 - loglik: -7.0058e+02 - logprior: -3.7228e+00
Epoch 2/2
19/19 - 5s - loss: 690.2344 - loglik: -6.8842e+02 - logprior: -1.8192e+00
Fitted a model with MAP estimate = -687.9918
expansions: [(0, 2)]
discards: [  0  25  26  37  67 111]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 691.0323 - loglik: -6.8829e+02 - logprior: -2.7383e+00
Epoch 2/2
19/19 - 5s - loss: 687.9050 - loglik: -6.8697e+02 - logprior: -9.3212e-01
Fitted a model with MAP estimate = -686.4017
expansions: [(26, 1)]
discards: [31]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 689.8035 - loglik: -6.8711e+02 - logprior: -2.6948e+00
Epoch 2/10
19/19 - 5s - loss: 686.5496 - loglik: -6.8565e+02 - logprior: -8.9692e-01
Epoch 3/10
19/19 - 4s - loss: 685.6323 - loglik: -6.8480e+02 - logprior: -8.3103e-01
Epoch 4/10
19/19 - 4s - loss: 684.0491 - loglik: -6.8327e+02 - logprior: -7.7425e-01
Epoch 5/10
19/19 - 6s - loss: 682.8368 - loglik: -6.8210e+02 - logprior: -7.3985e-01
Epoch 6/10
19/19 - 5s - loss: 682.0259 - loglik: -6.8130e+02 - logprior: -7.2887e-01
Epoch 7/10
19/19 - 5s - loss: 679.5723 - loglik: -6.7886e+02 - logprior: -7.1371e-01
Epoch 8/10
19/19 - 5s - loss: 679.4100 - loglik: -6.7871e+02 - logprior: -7.0357e-01
Epoch 9/10
19/19 - 5s - loss: 677.9762 - loglik: -6.7728e+02 - logprior: -6.9741e-01
Epoch 10/10
19/19 - 5s - loss: 677.7783 - loglik: -6.7709e+02 - logprior: -6.8533e-01
Fitted a model with MAP estimate = -677.5278
Time for alignment: 137.3387
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 739.0790 - loglik: -7.3610e+02 - logprior: -2.9783e+00
Epoch 2/10
19/19 - 4s - loss: 709.5491 - loglik: -7.0854e+02 - logprior: -1.0048e+00
Epoch 3/10
19/19 - 4s - loss: 697.6340 - loglik: -6.9638e+02 - logprior: -1.2524e+00
Epoch 4/10
19/19 - 4s - loss: 694.1448 - loglik: -6.9302e+02 - logprior: -1.1255e+00
Epoch 5/10
19/19 - 4s - loss: 691.7345 - loglik: -6.9058e+02 - logprior: -1.1524e+00
Epoch 6/10
19/19 - 4s - loss: 690.4792 - loglik: -6.8932e+02 - logprior: -1.1566e+00
Epoch 7/10
19/19 - 4s - loss: 689.0846 - loglik: -6.8793e+02 - logprior: -1.1589e+00
Epoch 8/10
19/19 - 4s - loss: 687.2964 - loglik: -6.8613e+02 - logprior: -1.1674e+00
Epoch 9/10
19/19 - 3s - loss: 687.8484 - loglik: -6.8667e+02 - logprior: -1.1772e+00
Fitted a model with MAP estimate = -686.9000
expansions: [(15, 1), (18, 1), (19, 1), (20, 1), (21, 5), (23, 3), (25, 1), (28, 1), (31, 1), (53, 2), (73, 1), (74, 1), (75, 3), (76, 2), (82, 1), (84, 3), (86, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 703.7338 - loglik: -7.0000e+02 - logprior: -3.7294e+00
Epoch 2/2
19/19 - 5s - loss: 690.7803 - loglik: -6.8896e+02 - logprior: -1.8187e+00
Fitted a model with MAP estimate = -688.0408
expansions: [(0, 2)]
discards: [  0  24  25  26  97 109]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 690.8610 - loglik: -6.8808e+02 - logprior: -2.7822e+00
Epoch 2/2
19/19 - 5s - loss: 687.2820 - loglik: -6.8631e+02 - logprior: -9.7408e-01
Fitted a model with MAP estimate = -686.3896
expansions: [(25, 2)]
discards: [31]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 10005 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 689.8594 - loglik: -6.8715e+02 - logprior: -2.7124e+00
Epoch 2/10
19/19 - 4s - loss: 687.0910 - loglik: -6.8618e+02 - logprior: -9.1475e-01
Epoch 3/10
19/19 - 4s - loss: 685.5892 - loglik: -6.8473e+02 - logprior: -8.5685e-01
Epoch 4/10
19/19 - 4s - loss: 684.1287 - loglik: -6.8332e+02 - logprior: -8.0720e-01
Epoch 5/10
19/19 - 4s - loss: 683.3454 - loglik: -6.8257e+02 - logprior: -7.7356e-01
Epoch 6/10
19/19 - 4s - loss: 681.3910 - loglik: -6.8063e+02 - logprior: -7.5941e-01
Epoch 7/10
19/19 - 4s - loss: 680.0817 - loglik: -6.7933e+02 - logprior: -7.5444e-01
Epoch 8/10
19/19 - 4s - loss: 679.3517 - loglik: -6.7862e+02 - logprior: -7.3473e-01
Epoch 9/10
19/19 - 4s - loss: 678.9469 - loglik: -6.7822e+02 - logprior: -7.3168e-01
Epoch 10/10
19/19 - 5s - loss: 677.0770 - loglik: -6.7636e+02 - logprior: -7.1835e-01
Fitted a model with MAP estimate = -677.6897
Time for alignment: 132.2032
Computed alignments with likelihoods: ['-678.0275', '-677.2118', '-678.0486', '-677.5278', '-677.6897']
Best model has likelihood: -677.2118  (prior= -0.7067 )
time for generating output: 0.1362
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF01814.projection.fasta
SP score = 0.8932419196865817
Training of 5 independent models on file PF00155.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 1909.0231 - loglik: -1.9076e+03 - logprior: -1.4296e+00
Epoch 2/10
39/39 - 40s - loss: 1814.0513 - loglik: -1.8127e+03 - logprior: -1.3612e+00
Epoch 3/10
39/39 - 37s - loss: 1801.4706 - loglik: -1.8001e+03 - logprior: -1.4160e+00
Epoch 4/10
39/39 - 36s - loss: 1798.2625 - loglik: -1.7969e+03 - logprior: -1.3974e+00
Epoch 5/10
39/39 - 37s - loss: 1794.7137 - loglik: -1.7933e+03 - logprior: -1.3974e+00
Epoch 6/10
39/39 - 40s - loss: 1794.0010 - loglik: -1.7925e+03 - logprior: -1.4531e+00
Epoch 7/10
39/39 - 40s - loss: 1794.1897 - loglik: -1.7927e+03 - logprior: -1.4732e+00
Fitted a model with MAP estimate = -1775.5688
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (24, 1), (25, 2), (26, 1), (31, 1), (41, 1), (43, 1), (44, 2), (46, 2), (56, 1), (59, 1), (62, 1), (65, 1), (80, 1), (81, 1), (82, 2), (86, 1), (88, 1), (95, 3), (103, 1), (120, 1), (121, 1), (124, 1), (125, 1), (131, 1), (143, 2), (149, 2), (154, 2), (155, 1), (156, 2), (162, 1), (182, 1), (185, 1), (186, 1), (188, 1), (189, 1), (190, 1), (207, 3), (208, 1), (209, 1), (210, 2), (212, 1), (221, 1), (223, 1), (227, 2), (228, 1), (230, 1), (241, 2), (243, 1), (246, 1), (251, 1), (258, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 357 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 63s - loss: 1799.2434 - loglik: -1.7972e+03 - logprior: -2.0353e+00
Epoch 2/2
39/39 - 67s - loss: 1780.4441 - loglik: -1.7798e+03 - logprior: -6.0589e-01
Fitted a model with MAP estimate = -1760.0004
expansions: [(61, 1)]
discards: [  0  32  57 104 121 179 195 259 266 289 320]
Re-initialized the encoder parameters.
Fitting a model of length 347 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 71s - loss: 1785.9832 - loglik: -1.7836e+03 - logprior: -2.3853e+00
Epoch 2/2
39/39 - 66s - loss: 1779.1863 - loglik: -1.7788e+03 - logprior: -3.3724e-01
Fitted a model with MAP estimate = -1759.2431
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 1764.7654 - loglik: -1.7635e+03 - logprior: -1.2609e+00
Epoch 2/10
39/39 - 58s - loss: 1761.7656 - loglik: -1.7616e+03 - logprior: -1.3924e-01
Epoch 3/10
39/39 - 58s - loss: 1758.5413 - loglik: -1.7586e+03 - logprior: 0.0681
Epoch 4/10
39/39 - 60s - loss: 1753.9072 - loglik: -1.7540e+03 - logprior: 0.1256
Epoch 5/10
39/39 - 54s - loss: 1751.1174 - loglik: -1.7513e+03 - logprior: 0.1961
Epoch 6/10
39/39 - 53s - loss: 1750.5803 - loglik: -1.7508e+03 - logprior: 0.2237
Epoch 7/10
39/39 - 59s - loss: 1749.4493 - loglik: -1.7498e+03 - logprior: 0.3329
Epoch 8/10
39/39 - 56s - loss: 1750.2406 - loglik: -1.7506e+03 - logprior: 0.3906
Fitted a model with MAP estimate = -1748.1046
Time for alignment: 1322.6750
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 1908.5865 - loglik: -1.9072e+03 - logprior: -1.4249e+00
Epoch 2/10
39/39 - 39s - loss: 1814.3431 - loglik: -1.8130e+03 - logprior: -1.3033e+00
Epoch 3/10
39/39 - 43s - loss: 1803.4747 - loglik: -1.8022e+03 - logprior: -1.3001e+00
Epoch 4/10
39/39 - 46s - loss: 1799.3906 - loglik: -1.7981e+03 - logprior: -1.2691e+00
Epoch 5/10
39/39 - 47s - loss: 1796.5872 - loglik: -1.7953e+03 - logprior: -1.3191e+00
Epoch 6/10
39/39 - 49s - loss: 1795.4529 - loglik: -1.7941e+03 - logprior: -1.3432e+00
Epoch 7/10
39/39 - 46s - loss: 1795.1082 - loglik: -1.7937e+03 - logprior: -1.3710e+00
Epoch 8/10
39/39 - 44s - loss: 1794.8960 - loglik: -1.7935e+03 - logprior: -1.3843e+00
Epoch 9/10
39/39 - 44s - loss: 1794.3947 - loglik: -1.7930e+03 - logprior: -1.4131e+00
Epoch 10/10
39/39 - 45s - loss: 1795.1278 - loglik: -1.7937e+03 - logprior: -1.4131e+00
Fitted a model with MAP estimate = -1776.1685
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 3), (26, 1), (33, 1), (43, 2), (44, 3), (48, 2), (56, 1), (58, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (86, 1), (87, 1), (94, 1), (101, 1), (103, 1), (121, 1), (124, 2), (130, 1), (145, 1), (148, 1), (150, 1), (153, 2), (154, 1), (155, 2), (182, 1), (184, 1), (186, 1), (187, 2), (188, 1), (189, 1), (190, 1), (206, 4), (207, 1), (208, 1), (209, 1), (211, 1), (220, 1), (222, 1), (226, 1), (228, 1), (237, 1), (241, 2), (242, 1), (245, 1), (255, 1), (261, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 356 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 62s - loss: 1801.2843 - loglik: -1.7993e+03 - logprior: -2.0214e+00
Epoch 2/2
39/39 - 60s - loss: 1779.1094 - loglik: -1.7785e+03 - logprior: -5.7648e-01
Fitted a model with MAP estimate = -1758.7320
expansions: [(55, 1), (56, 1), (118, 1), (123, 1), (158, 1), (332, 1)]
discards: [  1  30  32  33  58  65 121 192 234 259 343]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 57s - loss: 1783.6616 - loglik: -1.7825e+03 - logprior: -1.1961e+00
Epoch 2/2
39/39 - 60s - loss: 1778.4115 - loglik: -1.7783e+03 - logprior: -1.5996e-01
Fitted a model with MAP estimate = -1758.2946
expansions: [(255, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 352 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 60s - loss: 1764.1287 - loglik: -1.7630e+03 - logprior: -1.0863e+00
Epoch 2/10
39/39 - 66s - loss: 1760.1978 - loglik: -1.7602e+03 - logprior: -4.8528e-04
Epoch 3/10
39/39 - 69s - loss: 1757.2196 - loglik: -1.7573e+03 - logprior: 0.1266
Epoch 4/10
39/39 - 71s - loss: 1754.0559 - loglik: -1.7543e+03 - logprior: 0.1952
Epoch 5/10
39/39 - 58s - loss: 1749.1460 - loglik: -1.7494e+03 - logprior: 0.2434
Epoch 6/10
39/39 - 48s - loss: 1749.9691 - loglik: -1.7503e+03 - logprior: 0.3282
Fitted a model with MAP estimate = -1748.0702
Time for alignment: 1341.4340
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 1908.3029 - loglik: -1.9069e+03 - logprior: -1.4180e+00
Epoch 2/10
39/39 - 30s - loss: 1815.2854 - loglik: -1.8140e+03 - logprior: -1.3029e+00
Epoch 3/10
39/39 - 30s - loss: 1803.6643 - loglik: -1.8023e+03 - logprior: -1.3563e+00
Epoch 4/10
39/39 - 30s - loss: 1798.8496 - loglik: -1.7975e+03 - logprior: -1.3514e+00
Epoch 5/10
39/39 - 31s - loss: 1796.8790 - loglik: -1.7955e+03 - logprior: -1.3709e+00
Epoch 6/10
39/39 - 33s - loss: 1795.7812 - loglik: -1.7943e+03 - logprior: -1.4430e+00
Epoch 7/10
39/39 - 33s - loss: 1795.1318 - loglik: -1.7937e+03 - logprior: -1.4237e+00
Epoch 8/10
39/39 - 34s - loss: 1795.6292 - loglik: -1.7942e+03 - logprior: -1.4269e+00
Fitted a model with MAP estimate = -1776.9825
expansions: [(0, 2), (16, 1), (17, 1), (20, 1), (21, 1), (23, 1), (24, 2), (25, 1), (30, 1), (41, 1), (42, 3), (43, 3), (55, 1), (58, 2), (60, 1), (79, 1), (80, 1), (81, 2), (85, 1), (87, 1), (94, 3), (102, 1), (104, 2), (120, 1), (123, 1), (124, 1), (130, 1), (133, 2), (144, 1), (147, 1), (149, 1), (154, 1), (155, 2), (169, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 4), (207, 1), (208, 1), (209, 2), (211, 1), (219, 1), (220, 1), (226, 2), (227, 1), (241, 2), (243, 1), (246, 1), (251, 1), (258, 1), (262, 1), (263, 1), (270, 3), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 360 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 56s - loss: 1800.7560 - loglik: -1.7987e+03 - logprior: -2.0345e+00
Epoch 2/2
39/39 - 54s - loss: 1780.5338 - loglik: -1.7799e+03 - logprior: -6.0291e-01
Fitted a model with MAP estimate = -1760.0782
expansions: [(56, 1)]
discards: [  1  32  59 105 122 136 171 261 268 290 322 347]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 49s - loss: 1784.3898 - loglik: -1.7832e+03 - logprior: -1.1822e+00
Epoch 2/2
39/39 - 46s - loss: 1779.6713 - loglik: -1.7794e+03 - logprior: -2.2694e-01
Fitted a model with MAP estimate = -1759.6745
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 1765.3641 - loglik: -1.7643e+03 - logprior: -1.0733e+00
Epoch 2/10
39/39 - 44s - loss: 1761.2760 - loglik: -1.7613e+03 - logprior: -1.2809e-02
Epoch 3/10
39/39 - 44s - loss: 1759.0271 - loglik: -1.7591e+03 - logprior: 0.0803
Epoch 4/10
39/39 - 44s - loss: 1754.6985 - loglik: -1.7548e+03 - logprior: 0.1371
Epoch 5/10
39/39 - 46s - loss: 1752.8251 - loglik: -1.7530e+03 - logprior: 0.1883
Epoch 6/10
39/39 - 47s - loss: 1749.2141 - loglik: -1.7495e+03 - logprior: 0.2655
Epoch 7/10
39/39 - 48s - loss: 1750.8313 - loglik: -1.7512e+03 - logprior: 0.3358
Fitted a model with MAP estimate = -1748.6626
Time for alignment: 1015.7650
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 1907.6134 - loglik: -1.9062e+03 - logprior: -1.4202e+00
Epoch 2/10
39/39 - 34s - loss: 1814.6088 - loglik: -1.8134e+03 - logprior: -1.2388e+00
Epoch 3/10
39/39 - 34s - loss: 1802.3041 - loglik: -1.8010e+03 - logprior: -1.2796e+00
Epoch 4/10
39/39 - 34s - loss: 1797.5997 - loglik: -1.7963e+03 - logprior: -1.2914e+00
Epoch 5/10
39/39 - 34s - loss: 1795.6801 - loglik: -1.7944e+03 - logprior: -1.3114e+00
Epoch 6/10
39/39 - 35s - loss: 1793.8853 - loglik: -1.7925e+03 - logprior: -1.3576e+00
Epoch 7/10
39/39 - 38s - loss: 1794.5026 - loglik: -1.7931e+03 - logprior: -1.3723e+00
Fitted a model with MAP estimate = -1775.7088
expansions: [(0, 2), (16, 1), (21, 1), (22, 1), (23, 3), (24, 2), (25, 1), (32, 1), (40, 1), (42, 1), (43, 2), (44, 2), (56, 1), (59, 2), (60, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (95, 2), (101, 1), (103, 2), (119, 1), (124, 2), (130, 1), (142, 1), (144, 1), (147, 1), (149, 1), (154, 1), (155, 3), (168, 1), (181, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 3), (207, 1), (208, 1), (211, 1), (220, 1), (223, 1), (224, 2), (228, 2), (240, 2), (242, 1), (245, 1), (255, 1), (261, 2), (270, 2), (271, 1), (272, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 354 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 59s - loss: 1797.2509 - loglik: -1.7953e+03 - logprior: -1.9574e+00
Epoch 2/2
39/39 - 58s - loss: 1779.6150 - loglik: -1.7791e+03 - logprior: -5.2346e-01
Fitted a model with MAP estimate = -1759.1264
expansions: [(119, 2), (159, 1), (256, 1), (305, 1), (332, 1)]
discards: [  0   1  30  33  34  81 133 196 284 317 341]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 61s - loss: 1784.1431 - loglik: -1.7828e+03 - logprior: -1.3581e+00
Epoch 2/2
39/39 - 59s - loss: 1778.6937 - loglik: -1.7786e+03 - logprior: -6.3875e-02
Fitted a model with MAP estimate = -1758.2172
expansions: [(0, 2)]
discards: [114]
Re-initialized the encoder parameters.
Fitting a model of length 350 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 59s - loss: 1764.8975 - loglik: -1.7635e+03 - logprior: -1.3492e+00
Epoch 2/10
39/39 - 57s - loss: 1761.0372 - loglik: -1.7610e+03 - logprior: -5.0755e-02
Epoch 3/10
39/39 - 60s - loss: 1757.3400 - loglik: -1.7574e+03 - logprior: 0.0847
Epoch 4/10
39/39 - 62s - loss: 1753.1991 - loglik: -1.7533e+03 - logprior: 0.1158
Epoch 5/10
39/39 - 62s - loss: 1750.3413 - loglik: -1.7505e+03 - logprior: 0.1934
Epoch 6/10
39/39 - 60s - loss: 1750.0966 - loglik: -1.7504e+03 - logprior: 0.2749
Epoch 7/10
39/39 - 57s - loss: 1747.8896 - loglik: -1.7482e+03 - logprior: 0.3472
Epoch 8/10
39/39 - 57s - loss: 1748.2981 - loglik: -1.7487e+03 - logprior: 0.4308
Fitted a model with MAP estimate = -1746.9566
Time for alignment: 1233.8627
Fitting a model of length 280 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 41s - loss: 1908.1772 - loglik: -1.9068e+03 - logprior: -1.4077e+00
Epoch 2/10
39/39 - 40s - loss: 1813.7775 - loglik: -1.8125e+03 - logprior: -1.2616e+00
Epoch 3/10
39/39 - 41s - loss: 1802.6013 - loglik: -1.8012e+03 - logprior: -1.3680e+00
Epoch 4/10
39/39 - 42s - loss: 1798.3094 - loglik: -1.7970e+03 - logprior: -1.3460e+00
Epoch 5/10
39/39 - 41s - loss: 1795.5707 - loglik: -1.7942e+03 - logprior: -1.3407e+00
Epoch 6/10
39/39 - 40s - loss: 1794.6359 - loglik: -1.7932e+03 - logprior: -1.3871e+00
Epoch 7/10
39/39 - 41s - loss: 1795.1012 - loglik: -1.7937e+03 - logprior: -1.3923e+00
Fitted a model with MAP estimate = -1776.0029
expansions: [(0, 2), (16, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 2), (25, 1), (32, 1), (33, 1), (39, 1), (41, 1), (42, 1), (44, 2), (56, 1), (59, 2), (60, 1), (61, 1), (78, 1), (79, 1), (80, 1), (81, 1), (86, 1), (87, 1), (94, 1), (102, 1), (103, 2), (119, 1), (130, 1), (145, 1), (148, 2), (153, 2), (154, 1), (155, 2), (182, 1), (183, 1), (184, 1), (185, 1), (187, 1), (188, 1), (189, 1), (206, 3), (207, 1), (208, 1), (209, 2), (211, 1), (220, 1), (222, 1), (228, 2), (240, 2), (241, 2), (242, 1), (245, 1), (250, 1), (263, 1), (271, 3), (272, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 354 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 1800.3091 - loglik: -1.7982e+03 - logprior: -2.0729e+00
Epoch 2/2
39/39 - 61s - loss: 1781.5200 - loglik: -1.7809e+03 - logprior: -5.8308e-01
Fitted a model with MAP estimate = -1760.9004
expansions: [(118, 1), (123, 1), (330, 1)]
discards: [  0  32  80 121 133 192 256 257 302 317 341]
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 60s - loss: 1787.5009 - loglik: -1.7853e+03 - logprior: -2.1847e+00
Epoch 2/2
39/39 - 56s - loss: 1781.2462 - loglik: -1.7810e+03 - logprior: -2.3264e-01
Fitted a model with MAP estimate = -1761.2232
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 347 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 62s - loss: 1766.7660 - loglik: -1.7657e+03 - logprior: -1.0930e+00
Epoch 2/10
39/39 - 57s - loss: 1763.7478 - loglik: -1.7637e+03 - logprior: -1.5133e-02
Epoch 3/10
39/39 - 50s - loss: 1760.1360 - loglik: -1.7602e+03 - logprior: 0.0487
Epoch 4/10
39/39 - 49s - loss: 1756.5837 - loglik: -1.7567e+03 - logprior: 0.1344
Epoch 5/10
39/39 - 47s - loss: 1753.6743 - loglik: -1.7539e+03 - logprior: 0.1869
Epoch 6/10
39/39 - 47s - loss: 1751.6069 - loglik: -1.7519e+03 - logprior: 0.2528
Epoch 7/10
39/39 - 45s - loss: 1752.3468 - loglik: -1.7527e+03 - logprior: 0.3053
Fitted a model with MAP estimate = -1750.4138
Time for alignment: 1171.5764
Computed alignments with likelihoods: ['-1748.1046', '-1748.0702', '-1748.6626', '-1746.9566', '-1750.4138']
Best model has likelihood: -1746.9566  (prior= 0.5508 )
time for generating output: 1.0596
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00155.projection.fasta
SP score = 0.44843080959719833
Training of 5 independent models on file PF00450.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 49s - loss: 1897.4265 - loglik: -1.8960e+03 - logprior: -1.4392e+00
Epoch 2/10
39/39 - 46s - loss: 1757.4355 - loglik: -1.7561e+03 - logprior: -1.3019e+00
Epoch 3/10
39/39 - 46s - loss: 1748.5372 - loglik: -1.7472e+03 - logprior: -1.3754e+00
Epoch 4/10
39/39 - 46s - loss: 1746.2991 - loglik: -1.7449e+03 - logprior: -1.4412e+00
Epoch 5/10
39/39 - 46s - loss: 1744.3234 - loglik: -1.7429e+03 - logprior: -1.4520e+00
Epoch 6/10
39/39 - 47s - loss: 1744.7969 - loglik: -1.7433e+03 - logprior: -1.4844e+00
Fitted a model with MAP estimate = -1741.9477
expansions: [(9, 1), (19, 2), (31, 1), (67, 1), (102, 1), (106, 2), (107, 1), (110, 1), (123, 1), (140, 1), (141, 2), (142, 1), (143, 1), (144, 3), (145, 2), (146, 1), (161, 2), (162, 2), (163, 1), (168, 1), (177, 3), (178, 1), (179, 1), (182, 5), (183, 1), (185, 1), (187, 1), (189, 1), (190, 1), (191, 1), (192, 1), (193, 2), (205, 2), (212, 1), (218, 1), (219, 2), (220, 1), (221, 1), (235, 3), (242, 1), (244, 2), (245, 2), (247, 1), (250, 2), (251, 1), (257, 1), (278, 1), (280, 3), (284, 1), (285, 2), (290, 1), (291, 2), (292, 2), (298, 1), (307, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0   1   2 194 195 196 197 198]
Re-initialized the encoder parameters.
Fitting a model of length 405 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 1735.3990 - loglik: -1.7329e+03 - logprior: -2.4549e+00
Epoch 2/2
39/39 - 76s - loss: 1720.6298 - loglik: -1.7197e+03 - logprior: -9.4884e-01
Fitted a model with MAP estimate = -1716.8646
expansions: [(3, 1), (4, 1), (233, 4)]
discards: [  0 158 161 203 216 244 259 260 262 298 299 342 349 359 360]
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 1727.5493 - loglik: -1.7255e+03 - logprior: -2.0285e+00
Epoch 2/2
39/39 - 69s - loss: 1720.9462 - loglik: -1.7206e+03 - logprior: -3.7187e-01
Fitted a model with MAP estimate = -1716.0235
expansions: [(4, 1), (232, 1), (233, 1), (261, 1)]
discards: [  0   1 210 211 234 235 237 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 1724.9810 - loglik: -1.7233e+03 - logprior: -1.7072e+00
Epoch 2/10
39/39 - 62s - loss: 1717.4399 - loglik: -1.7177e+03 - logprior: 0.2172
Epoch 3/10
39/39 - 62s - loss: 1715.0406 - loglik: -1.7152e+03 - logprior: 0.2061
Epoch 4/10
39/39 - 63s - loss: 1714.7218 - loglik: -1.7151e+03 - logprior: 0.3952
Epoch 5/10
39/39 - 65s - loss: 1713.0846 - loglik: -1.7136e+03 - logprior: 0.5184
Epoch 6/10
39/39 - 67s - loss: 1712.0125 - loglik: -1.7126e+03 - logprior: 0.5715
Epoch 7/10
39/39 - 69s - loss: 1711.4025 - loglik: -1.7122e+03 - logprior: 0.7753
Epoch 8/10
39/39 - 67s - loss: 1712.5706 - loglik: -1.7135e+03 - logprior: 0.9297
Fitted a model with MAP estimate = -1711.8547
Time for alignment: 1403.2839
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 50s - loss: 1898.7448 - loglik: -1.8972e+03 - logprior: -1.4953e+00
Epoch 2/10
39/39 - 49s - loss: 1759.3453 - loglik: -1.7578e+03 - logprior: -1.5228e+00
Epoch 3/10
39/39 - 50s - loss: 1748.1038 - loglik: -1.7464e+03 - logprior: -1.6946e+00
Epoch 4/10
39/39 - 50s - loss: 1745.2856 - loglik: -1.7436e+03 - logprior: -1.6920e+00
Epoch 5/10
39/39 - 49s - loss: 1743.2456 - loglik: -1.7415e+03 - logprior: -1.7107e+00
Epoch 6/10
39/39 - 50s - loss: 1745.8690 - loglik: -1.7441e+03 - logprior: -1.7232e+00
Fitted a model with MAP estimate = -1742.2172
expansions: [(9, 1), (19, 1), (21, 1), (31, 1), (66, 1), (68, 1), (100, 1), (103, 2), (104, 1), (105, 1), (106, 1), (113, 1), (118, 1), (137, 2), (138, 1), (139, 1), (141, 2), (142, 2), (143, 1), (158, 2), (159, 1), (160, 1), (161, 1), (165, 1), (175, 1), (176, 5), (179, 1), (180, 5), (182, 1), (184, 1), (186, 1), (187, 2), (188, 1), (189, 1), (202, 2), (204, 2), (211, 1), (219, 2), (220, 1), (221, 1), (226, 1), (228, 1), (234, 2), (237, 1), (240, 1), (242, 1), (246, 1), (254, 1), (257, 1), (274, 1), (277, 1), (279, 3), (280, 1), (281, 1), (282, 1), (283, 1), (289, 1), (290, 2), (292, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 191 192 193 194 195 206 207 208]
Re-initialized the encoder parameters.
Fitting a model of length 403 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 1733.6609 - loglik: -1.7320e+03 - logprior: -1.6569e+00
Epoch 2/2
39/39 - 74s - loss: 1720.5449 - loglik: -1.7201e+03 - logprior: -4.4755e-01
Fitted a model with MAP estimate = -1715.8318
expansions: [(110, 1), (232, 4), (248, 1), (249, 1), (296, 1)]
discards: [159 161 207 208 217 218 245 257 263 284 285 341 342 358]
Re-initialized the encoder parameters.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 88s - loss: 1725.2637 - loglik: -1.7239e+03 - logprior: -1.3140e+00
Epoch 2/2
39/39 - 86s - loss: 1720.0182 - loglik: -1.7199e+03 - logprior: -1.0555e-01
Fitted a model with MAP estimate = -1716.1788
expansions: [(5, 1), (229, 1), (230, 1), (231, 1), (281, 1), (282, 2)]
discards: [232 233 234 235 236 237 238 239 240 249 250]
Re-initialized the encoder parameters.
Fitting a model of length 393 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 96s - loss: 1724.2175 - loglik: -1.7231e+03 - logprior: -1.0989e+00
Epoch 2/10
39/39 - 95s - loss: 1719.0719 - loglik: -1.7193e+03 - logprior: 0.1914
Epoch 3/10
39/39 - 93s - loss: 1716.7737 - loglik: -1.7171e+03 - logprior: 0.3261
Epoch 4/10
39/39 - 89s - loss: 1715.9659 - loglik: -1.7164e+03 - logprior: 0.3896
Epoch 5/10
39/39 - 88s - loss: 1716.2037 - loglik: -1.7167e+03 - logprior: 0.5027
Fitted a model with MAP estimate = -1714.2657
Time for alignment: 1421.4597
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 67s - loss: 1897.8988 - loglik: -1.8964e+03 - logprior: -1.4972e+00
Epoch 2/10
39/39 - 68s - loss: 1759.2057 - loglik: -1.7577e+03 - logprior: -1.4869e+00
Epoch 3/10
39/39 - 67s - loss: 1746.7278 - loglik: -1.7451e+03 - logprior: -1.5790e+00
Epoch 4/10
39/39 - 65s - loss: 1742.9908 - loglik: -1.7414e+03 - logprior: -1.5798e+00
Epoch 5/10
39/39 - 62s - loss: 1742.6672 - loglik: -1.7411e+03 - logprior: -1.5732e+00
Epoch 6/10
39/39 - 62s - loss: 1741.2556 - loglik: -1.7396e+03 - logprior: -1.6459e+00
Epoch 7/10
39/39 - 66s - loss: 1743.0410 - loglik: -1.7414e+03 - logprior: -1.6554e+00
Fitted a model with MAP estimate = -1739.4923
expansions: [(9, 1), (19, 2), (20, 1), (34, 1), (60, 1), (62, 1), (64, 1), (75, 1), (98, 2), (101, 1), (103, 1), (104, 1), (105, 1), (111, 1), (116, 1), (117, 1), (135, 1), (137, 1), (138, 1), (139, 3), (140, 3), (155, 1), (157, 2), (158, 1), (163, 1), (172, 5), (173, 1), (174, 1), (177, 3), (178, 1), (182, 1), (184, 1), (186, 1), (187, 1), (188, 1), (189, 1), (209, 1), (217, 1), (218, 1), (219, 1), (224, 1), (232, 1), (233, 2), (239, 2), (241, 2), (242, 2), (246, 1), (249, 1), (252, 1), (255, 1), (276, 1), (277, 4), (282, 1), (283, 1), (289, 2), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  1 197 198 199 200 201 202 203 204 205 206]
Re-initialized the encoder parameters.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 96s - loss: 1731.6104 - loglik: -1.7301e+03 - logprior: -1.5178e+00
Epoch 2/2
39/39 - 89s - loss: 1720.2928 - loglik: -1.7200e+03 - logprior: -2.6542e-01
Fitted a model with MAP estimate = -1715.4063
expansions: []
discards: [161 204 205 206 277 292 335 352]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 94s - loss: 1722.8157 - loglik: -1.7216e+03 - logprior: -1.2292e+00
Epoch 2/2
39/39 - 91s - loss: 1718.9097 - loglik: -1.7190e+03 - logprior: 0.0866
Fitted a model with MAP estimate = -1715.4710
expansions: [(5, 1)]
discards: [213 288]
Re-initialized the encoder parameters.
Fitting a model of length 388 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 89s - loss: 1720.2872 - loglik: -1.7193e+03 - logprior: -1.0339e+00
Epoch 2/10
39/39 - 65s - loss: 1716.5272 - loglik: -1.7169e+03 - logprior: 0.3264
Epoch 3/10
39/39 - 61s - loss: 1716.2999 - loglik: -1.7167e+03 - logprior: 0.4292
Epoch 4/10
39/39 - 63s - loss: 1714.1797 - loglik: -1.7147e+03 - logprior: 0.5051
Epoch 5/10
39/39 - 68s - loss: 1712.6597 - loglik: -1.7134e+03 - logprior: 0.6968
Epoch 6/10
39/39 - 71s - loss: 1712.6002 - loglik: -1.7134e+03 - logprior: 0.7956
Epoch 7/10
39/39 - 74s - loss: 1715.4750 - loglik: -1.7164e+03 - logprior: 0.9678
Fitted a model with MAP estimate = -1712.5080
Time for alignment: 1696.5019
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 55s - loss: 1892.6324 - loglik: -1.8911e+03 - logprior: -1.4825e+00
Epoch 2/10
39/39 - 55s - loss: 1757.8768 - loglik: -1.7564e+03 - logprior: -1.4419e+00
Epoch 3/10
39/39 - 55s - loss: 1745.3093 - loglik: -1.7438e+03 - logprior: -1.5550e+00
Epoch 4/10
39/39 - 55s - loss: 1744.2362 - loglik: -1.7427e+03 - logprior: -1.5786e+00
Epoch 5/10
39/39 - 57s - loss: 1742.2928 - loglik: -1.7407e+03 - logprior: -1.6052e+00
Epoch 6/10
39/39 - 55s - loss: 1741.4185 - loglik: -1.7398e+03 - logprior: -1.6135e+00
Epoch 7/10
39/39 - 55s - loss: 1742.4824 - loglik: -1.7409e+03 - logprior: -1.6315e+00
Fitted a model with MAP estimate = -1739.1478
expansions: [(20, 2), (21, 1), (31, 1), (67, 1), (69, 1), (101, 1), (105, 1), (107, 1), (108, 1), (116, 1), (121, 1), (122, 1), (140, 1), (142, 1), (143, 1), (144, 3), (145, 3), (160, 2), (161, 1), (162, 1), (163, 1), (167, 1), (176, 3), (177, 4), (180, 5), (181, 1), (183, 1), (185, 1), (187, 1), (188, 1), (189, 1), (190, 1), (191, 2), (203, 2), (211, 3), (219, 2), (220, 1), (221, 1), (226, 1), (235, 2), (238, 1), (241, 1), (243, 1), (244, 2), (248, 1), (250, 1), (251, 1), (253, 1), (256, 1), (269, 1), (276, 1), (277, 1), (278, 1), (282, 2), (283, 3), (288, 1), (297, 1), (314, 1), (317, 1), (318, 1), (319, 1), (321, 1)]
discards: [  0 192 193 194 195 196 197 198 207 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 402 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 1735.5673 - loglik: -1.7332e+03 - logprior: -2.3784e+00
Epoch 2/2
39/39 - 66s - loss: 1720.9551 - loglik: -1.7200e+03 - logprior: -9.0766e-01
Fitted a model with MAP estimate = -1716.0687
expansions: [(246, 1), (247, 1), (295, 1)]
discards: [  0   1 161 207 208 218 252 262 284 297 347 348]
Re-initialized the encoder parameters.
Fitting a model of length 393 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 74s - loss: 1726.4307 - loglik: -1.7244e+03 - logprior: -2.0794e+00
Epoch 2/2
39/39 - 73s - loss: 1720.1509 - loglik: -1.7198e+03 - logprior: -3.1293e-01
Fitted a model with MAP estimate = -1715.5392
expansions: [(0, 3), (3, 2), (276, 1)]
discards: [  0 209 210 233 234 235 236 237 292]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 68s - loss: 1720.5303 - loglik: -1.7195e+03 - logprior: -1.0079e+00
Epoch 2/10
39/39 - 64s - loss: 1716.3475 - loglik: -1.7166e+03 - logprior: 0.2372
Epoch 3/10
39/39 - 64s - loss: 1716.2434 - loglik: -1.7166e+03 - logprior: 0.3592
Epoch 4/10
39/39 - 64s - loss: 1712.3988 - loglik: -1.7129e+03 - logprior: 0.5091
Epoch 5/10
39/39 - 70s - loss: 1713.8766 - loglik: -1.7145e+03 - logprior: 0.6257
Fitted a model with MAP estimate = -1712.3215
Time for alignment: 1304.1835
Fitting a model of length 326 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 1897.7772 - loglik: -1.8963e+03 - logprior: -1.4583e+00
Epoch 2/10
39/39 - 58s - loss: 1761.7288 - loglik: -1.7603e+03 - logprior: -1.4669e+00
Epoch 3/10
39/39 - 64s - loss: 1747.4219 - loglik: -1.7458e+03 - logprior: -1.6505e+00
Epoch 4/10
39/39 - 65s - loss: 1743.8702 - loglik: -1.7422e+03 - logprior: -1.6757e+00
Epoch 5/10
39/39 - 63s - loss: 1743.1152 - loglik: -1.7414e+03 - logprior: -1.7007e+00
Epoch 6/10
39/39 - 62s - loss: 1742.3187 - loglik: -1.7406e+03 - logprior: -1.7625e+00
Epoch 7/10
39/39 - 61s - loss: 1742.0803 - loglik: -1.7403e+03 - logprior: -1.7720e+00
Epoch 8/10
39/39 - 64s - loss: 1743.2073 - loglik: -1.7415e+03 - logprior: -1.7451e+00
Fitted a model with MAP estimate = -1740.4395
expansions: [(9, 1), (19, 2), (20, 1), (34, 1), (68, 1), (100, 2), (103, 1), (105, 1), (106, 1), (107, 1), (113, 1), (119, 1), (135, 1), (137, 1), (139, 1), (140, 1), (142, 2), (143, 2), (159, 2), (160, 1), (161, 1), (162, 1), (166, 1), (175, 3), (176, 1), (177, 1), (180, 1), (181, 2), (182, 1), (184, 1), (186, 1), (187, 1), (188, 1), (189, 2), (190, 1), (203, 2), (211, 3), (219, 1), (220, 1), (221, 1), (226, 1), (234, 1), (235, 3), (237, 1), (243, 1), (244, 2), (248, 1), (252, 1), (254, 1), (257, 1), (267, 1), (280, 3), (284, 1), (285, 2), (299, 1), (300, 1), (301, 1), (308, 1), (315, 1), (318, 1), (319, 1), (320, 1), (321, 1), (322, 1)]
discards: [  1 192 193 194 195 196 207 208 209]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 94s - loss: 1734.3318 - loglik: -1.7327e+03 - logprior: -1.6201e+00
Epoch 2/2
39/39 - 89s - loss: 1720.2300 - loglik: -1.7199e+03 - logprior: -3.7672e-01
Fitted a model with MAP estimate = -1716.6827
expansions: [(229, 4), (244, 6), (292, 1)]
discards: [203 213 238 239 240 248 280 281 294 338 345]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 92s - loss: 1725.3434 - loglik: -1.7241e+03 - logprior: -1.2305e+00
Epoch 2/2
39/39 - 93s - loss: 1717.8315 - loglik: -1.7178e+03 - logprior: -2.7796e-02
Fitted a model with MAP estimate = -1714.3401
expansions: [(5, 1), (245, 5), (284, 2)]
discards: [102 160 232 233 234 235 236 237 238 239 240 241 247 248 249 250 298]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 83s - loss: 1724.4451 - loglik: -1.7234e+03 - logprior: -1.0145e+00
Epoch 2/10
39/39 - 65s - loss: 1718.4323 - loglik: -1.7187e+03 - logprior: 0.2523
Epoch 3/10
39/39 - 62s - loss: 1716.2179 - loglik: -1.7167e+03 - logprior: 0.4495
Epoch 4/10
39/39 - 64s - loss: 1716.9852 - loglik: -1.7175e+03 - logprior: 0.5365
Fitted a model with MAP estimate = -1714.4163
Time for alignment: 1528.0522
Computed alignments with likelihoods: ['-1711.8547', '-1714.2657', '-1712.5080', '-1712.3215', '-1714.3401']
Best model has likelihood: -1711.8547  (prior= 1.0141 )
time for generating output: 0.3909
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00450.projection.fasta
SP score = 0.8112088930060213
Training of 5 independent models on file PF07679.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 511.0598 - loglik: -5.0800e+02 - logprior: -3.0625e+00
Epoch 2/10
19/19 - 2s - loss: 478.7157 - loglik: -4.7746e+02 - logprior: -1.2584e+00
Epoch 3/10
19/19 - 1s - loss: 465.9709 - loglik: -4.6453e+02 - logprior: -1.4360e+00
Epoch 4/10
19/19 - 2s - loss: 464.2758 - loglik: -4.6295e+02 - logprior: -1.3283e+00
Epoch 5/10
19/19 - 1s - loss: 463.5938 - loglik: -4.6226e+02 - logprior: -1.3378e+00
Epoch 6/10
19/19 - 1s - loss: 463.0772 - loglik: -4.6177e+02 - logprior: -1.3095e+00
Epoch 7/10
19/19 - 1s - loss: 463.1197 - loglik: -4.6182e+02 - logprior: -1.3032e+00
Fitted a model with MAP estimate = -462.7033
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (19, 1), (20, 1), (37, 2), (39, 1), (40, 1), (43, 2), (47, 3), (48, 1), (49, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 465.4973 - loglik: -4.6163e+02 - logprior: -3.8666e+00
Epoch 2/2
19/19 - 2s - loss: 458.4441 - loglik: -4.5647e+02 - logprior: -1.9771e+00
Fitted a model with MAP estimate = -456.9478
expansions: [(0, 2)]
discards: [ 0 10 45 62]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 458.8135 - loglik: -4.5595e+02 - logprior: -2.8591e+00
Epoch 2/2
19/19 - 2s - loss: 455.8650 - loglik: -4.5477e+02 - logprior: -1.0930e+00
Fitted a model with MAP estimate = -455.0471
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 459.9995 - loglik: -4.5629e+02 - logprior: -3.7048e+00
Epoch 2/10
19/19 - 2s - loss: 455.8717 - loglik: -4.5468e+02 - logprior: -1.1868e+00
Epoch 3/10
19/19 - 2s - loss: 454.8059 - loglik: -4.5388e+02 - logprior: -9.2187e-01
Epoch 4/10
19/19 - 2s - loss: 454.2763 - loglik: -4.5339e+02 - logprior: -8.8797e-01
Epoch 5/10
19/19 - 2s - loss: 453.9893 - loglik: -4.5312e+02 - logprior: -8.6702e-01
Epoch 6/10
19/19 - 2s - loss: 454.0641 - loglik: -4.5321e+02 - logprior: -8.5462e-01
Fitted a model with MAP estimate = -453.4894
Time for alignment: 53.4150
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 510.8700 - loglik: -5.0781e+02 - logprior: -3.0619e+00
Epoch 2/10
19/19 - 1s - loss: 480.7867 - loglik: -4.7955e+02 - logprior: -1.2336e+00
Epoch 3/10
19/19 - 1s - loss: 468.0332 - loglik: -4.6664e+02 - logprior: -1.3910e+00
Epoch 4/10
19/19 - 1s - loss: 464.9341 - loglik: -4.6363e+02 - logprior: -1.3013e+00
Epoch 5/10
19/19 - 2s - loss: 464.4281 - loglik: -4.6312e+02 - logprior: -1.3099e+00
Epoch 6/10
19/19 - 1s - loss: 463.8833 - loglik: -4.6259e+02 - logprior: -1.2979e+00
Epoch 7/10
19/19 - 1s - loss: 463.9550 - loglik: -4.6266e+02 - logprior: -1.2932e+00
Fitted a model with MAP estimate = -463.4538
expansions: [(8, 1), (9, 1), (11, 2), (13, 1), (18, 1), (19, 1), (24, 1), (37, 2), (39, 1), (40, 2), (43, 2), (47, 1), (48, 2), (49, 2), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 465.5941 - loglik: -4.6172e+02 - logprior: -3.8739e+00
Epoch 2/2
19/19 - 2s - loss: 458.5046 - loglik: -4.5654e+02 - logprior: -1.9671e+00
Fitted a model with MAP estimate = -456.9365
expansions: [(0, 2)]
discards: [ 0 44 51 66]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 458.8155 - loglik: -4.5595e+02 - logprior: -2.8665e+00
Epoch 2/2
19/19 - 2s - loss: 455.9827 - loglik: -4.5490e+02 - logprior: -1.0852e+00
Fitted a model with MAP estimate = -455.0364
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 460.1023 - loglik: -4.5641e+02 - logprior: -3.6913e+00
Epoch 2/10
19/19 - 2s - loss: 455.6702 - loglik: -4.5449e+02 - logprior: -1.1765e+00
Epoch 3/10
19/19 - 2s - loss: 454.9015 - loglik: -4.5399e+02 - logprior: -9.1440e-01
Epoch 4/10
19/19 - 2s - loss: 454.1955 - loglik: -4.5330e+02 - logprior: -8.9220e-01
Epoch 5/10
19/19 - 2s - loss: 454.1054 - loglik: -4.5324e+02 - logprior: -8.6630e-01
Epoch 6/10
19/19 - 2s - loss: 453.7335 - loglik: -4.5288e+02 - logprior: -8.5608e-01
Epoch 7/10
19/19 - 2s - loss: 453.1062 - loglik: -4.5226e+02 - logprior: -8.4560e-01
Epoch 8/10
19/19 - 2s - loss: 453.7094 - loglik: -4.5288e+02 - logprior: -8.2959e-01
Fitted a model with MAP estimate = -453.3398
Time for alignment: 57.0398
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 511.0916 - loglik: -5.0803e+02 - logprior: -3.0609e+00
Epoch 2/10
19/19 - 2s - loss: 479.5806 - loglik: -4.7833e+02 - logprior: -1.2545e+00
Epoch 3/10
19/19 - 1s - loss: 466.8396 - loglik: -4.6540e+02 - logprior: -1.4385e+00
Epoch 4/10
19/19 - 1s - loss: 464.4471 - loglik: -4.6311e+02 - logprior: -1.3407e+00
Epoch 5/10
19/19 - 1s - loss: 463.6936 - loglik: -4.6235e+02 - logprior: -1.3469e+00
Epoch 6/10
19/19 - 2s - loss: 463.4908 - loglik: -4.6217e+02 - logprior: -1.3188e+00
Epoch 7/10
19/19 - 2s - loss: 463.0779 - loglik: -4.6176e+02 - logprior: -1.3215e+00
Epoch 8/10
19/19 - 1s - loss: 462.8959 - loglik: -4.6158e+02 - logprior: -1.3168e+00
Epoch 9/10
19/19 - 1s - loss: 463.3816 - loglik: -4.6207e+02 - logprior: -1.3109e+00
Fitted a model with MAP estimate = -462.6965
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (15, 1), (24, 1), (37, 2), (39, 1), (41, 2), (43, 2), (47, 3), (48, 1), (49, 1), (60, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 465.7581 - loglik: -4.6188e+02 - logprior: -3.8766e+00
Epoch 2/2
19/19 - 2s - loss: 458.3906 - loglik: -4.5640e+02 - logprior: -1.9859e+00
Fitted a model with MAP estimate = -457.0377
expansions: [(0, 2)]
discards: [ 0 10 45 63]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 458.8183 - loglik: -4.5595e+02 - logprior: -2.8698e+00
Epoch 2/2
19/19 - 2s - loss: 455.7681 - loglik: -4.5468e+02 - logprior: -1.0907e+00
Fitted a model with MAP estimate = -455.0171
expansions: []
discards: [ 0 51]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 460.1704 - loglik: -4.5649e+02 - logprior: -3.6812e+00
Epoch 2/10
19/19 - 2s - loss: 455.8884 - loglik: -4.5473e+02 - logprior: -1.1599e+00
Epoch 3/10
19/19 - 2s - loss: 455.0614 - loglik: -4.5414e+02 - logprior: -9.2447e-01
Epoch 4/10
19/19 - 2s - loss: 454.5722 - loglik: -4.5367e+02 - logprior: -8.9939e-01
Epoch 5/10
19/19 - 2s - loss: 454.1474 - loglik: -4.5327e+02 - logprior: -8.7465e-01
Epoch 6/10
19/19 - 2s - loss: 453.7963 - loglik: -4.5294e+02 - logprior: -8.5807e-01
Epoch 7/10
19/19 - 2s - loss: 454.0284 - loglik: -4.5318e+02 - logprior: -8.4473e-01
Fitted a model with MAP estimate = -453.6354
Time for alignment: 59.3193
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 510.9550 - loglik: -5.0789e+02 - logprior: -3.0617e+00
Epoch 2/10
19/19 - 2s - loss: 480.3498 - loglik: -4.7907e+02 - logprior: -1.2776e+00
Epoch 3/10
19/19 - 2s - loss: 465.4117 - loglik: -4.6393e+02 - logprior: -1.4856e+00
Epoch 4/10
19/19 - 1s - loss: 462.6386 - loglik: -4.6124e+02 - logprior: -1.3998e+00
Epoch 5/10
19/19 - 2s - loss: 461.5755 - loglik: -4.6015e+02 - logprior: -1.4232e+00
Epoch 6/10
19/19 - 2s - loss: 461.6114 - loglik: -4.6023e+02 - logprior: -1.3829e+00
Fitted a model with MAP estimate = -460.9930
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (23, 1), (24, 1), (37, 2), (39, 1), (40, 1), (43, 2), (48, 1), (49, 2), (60, 1), (62, 1), (64, 2), (65, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 465.6976 - loglik: -4.6182e+02 - logprior: -3.8787e+00
Epoch 2/2
19/19 - 2s - loss: 458.6530 - loglik: -4.5667e+02 - logprior: -1.9849e+00
Fitted a model with MAP estimate = -456.8352
expansions: [(0, 2)]
discards: [ 0 10 45 65 84]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 458.9978 - loglik: -4.5613e+02 - logprior: -2.8631e+00
Epoch 2/2
19/19 - 2s - loss: 455.8649 - loglik: -4.5476e+02 - logprior: -1.1001e+00
Fitted a model with MAP estimate = -455.0527
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 459.9631 - loglik: -4.5625e+02 - logprior: -3.7092e+00
Epoch 2/10
19/19 - 2s - loss: 455.9552 - loglik: -4.5478e+02 - logprior: -1.1711e+00
Epoch 3/10
19/19 - 2s - loss: 454.7354 - loglik: -4.5381e+02 - logprior: -9.3011e-01
Epoch 4/10
19/19 - 2s - loss: 454.2501 - loglik: -4.5336e+02 - logprior: -8.8546e-01
Epoch 5/10
19/19 - 2s - loss: 454.1778 - loglik: -4.5331e+02 - logprior: -8.6780e-01
Epoch 6/10
19/19 - 2s - loss: 453.8601 - loglik: -4.5300e+02 - logprior: -8.5703e-01
Epoch 7/10
19/19 - 2s - loss: 453.4633 - loglik: -4.5263e+02 - logprior: -8.3827e-01
Epoch 8/10
19/19 - 2s - loss: 453.1865 - loglik: -4.5236e+02 - logprior: -8.3141e-01
Epoch 9/10
19/19 - 2s - loss: 453.6167 - loglik: -4.5280e+02 - logprior: -8.1639e-01
Fitted a model with MAP estimate = -453.2976
Time for alignment: 57.3810
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 511.1266 - loglik: -5.0807e+02 - logprior: -3.0607e+00
Epoch 2/10
19/19 - 2s - loss: 480.3208 - loglik: -4.7904e+02 - logprior: -1.2787e+00
Epoch 3/10
19/19 - 1s - loss: 465.2841 - loglik: -4.6378e+02 - logprior: -1.5078e+00
Epoch 4/10
19/19 - 1s - loss: 462.9790 - loglik: -4.6156e+02 - logprior: -1.4152e+00
Epoch 5/10
19/19 - 2s - loss: 461.6633 - loglik: -4.6023e+02 - logprior: -1.4356e+00
Epoch 6/10
19/19 - 1s - loss: 461.3473 - loglik: -4.5995e+02 - logprior: -1.3944e+00
Epoch 7/10
19/19 - 2s - loss: 461.3533 - loglik: -4.5997e+02 - logprior: -1.3809e+00
Fitted a model with MAP estimate = -460.8756
expansions: [(8, 1), (9, 3), (10, 1), (13, 1), (14, 1), (19, 1), (20, 1), (25, 1), (39, 1), (40, 2), (43, 2), (48, 1), (49, 2), (60, 1), (62, 1), (65, 1), (66, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 465.8305 - loglik: -4.6195e+02 - logprior: -3.8800e+00
Epoch 2/2
19/19 - 2s - loss: 458.5635 - loglik: -4.5658e+02 - logprior: -1.9794e+00
Fitted a model with MAP estimate = -456.9779
expansions: [(0, 2)]
discards: [ 0 10 51 65 86]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 459.1023 - loglik: -4.5625e+02 - logprior: -2.8544e+00
Epoch 2/2
19/19 - 2s - loss: 455.7317 - loglik: -4.5463e+02 - logprior: -1.1029e+00
Fitted a model with MAP estimate = -455.0619
expansions: [(54, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10010 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 460.0208 - loglik: -4.5634e+02 - logprior: -3.6767e+00
Epoch 2/10
19/19 - 2s - loss: 455.7338 - loglik: -4.5458e+02 - logprior: -1.1527e+00
Epoch 3/10
19/19 - 2s - loss: 454.7007 - loglik: -4.5378e+02 - logprior: -9.2344e-01
Epoch 4/10
19/19 - 2s - loss: 454.3373 - loglik: -4.5345e+02 - logprior: -8.8785e-01
Epoch 5/10
19/19 - 2s - loss: 454.1024 - loglik: -4.5324e+02 - logprior: -8.6404e-01
Epoch 6/10
19/19 - 2s - loss: 453.7625 - loglik: -4.5291e+02 - logprior: -8.5345e-01
Epoch 7/10
19/19 - 2s - loss: 453.5590 - loglik: -4.5271e+02 - logprior: -8.4461e-01
Epoch 8/10
19/19 - 2s - loss: 453.4169 - loglik: -4.5259e+02 - logprior: -8.2548e-01
Epoch 9/10
19/19 - 2s - loss: 452.9304 - loglik: -4.5212e+02 - logprior: -8.1382e-01
Epoch 10/10
19/19 - 2s - loss: 453.8227 - loglik: -4.5302e+02 - logprior: -8.0054e-01
Fitted a model with MAP estimate = -453.2469
Time for alignment: 62.8063
Computed alignments with likelihoods: ['-453.4894', '-453.3398', '-453.6354', '-453.2976', '-453.2469']
Best model has likelihood: -453.2469  (prior= -0.7908 )
time for generating output: 0.1261
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07679.projection.fasta
SP score = 0.8250276854928018
Training of 5 independent models on file PF07686.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 609.5316 - loglik: -6.0650e+02 - logprior: -3.0299e+00
Epoch 2/10
19/19 - 2s - loss: 579.6794 - loglik: -5.7850e+02 - logprior: -1.1834e+00
Epoch 3/10
19/19 - 2s - loss: 567.1750 - loglik: -5.6607e+02 - logprior: -1.1018e+00
Epoch 4/10
19/19 - 2s - loss: 565.0748 - loglik: -5.6409e+02 - logprior: -9.8409e-01
Epoch 5/10
19/19 - 2s - loss: 563.1376 - loglik: -5.6219e+02 - logprior: -9.4668e-01
Epoch 6/10
19/19 - 2s - loss: 562.2715 - loglik: -5.6132e+02 - logprior: -9.4942e-01
Epoch 7/10
19/19 - 2s - loss: 562.3253 - loglik: -5.6138e+02 - logprior: -9.4967e-01
Fitted a model with MAP estimate = -560.6807
expansions: [(0, 2), (21, 3), (22, 2), (23, 1), (24, 1), (39, 2), (41, 3), (42, 2), (44, 2), (53, 1), (54, 1), (74, 1), (75, 3), (76, 2), (77, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 567.0687 - loglik: -5.6301e+02 - logprior: -4.0609e+00
Epoch 2/2
19/19 - 3s - loss: 561.0094 - loglik: -5.5982e+02 - logprior: -1.1854e+00
Fitted a model with MAP estimate = -558.4380
expansions: []
discards: [ 1 27 60]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 562.6485 - loglik: -5.5978e+02 - logprior: -2.8692e+00
Epoch 2/2
19/19 - 3s - loss: 559.7045 - loglik: -5.5870e+02 - logprior: -1.0039e+00
Fitted a model with MAP estimate = -557.8014
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 110 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 561.0440 - loglik: -5.5813e+02 - logprior: -2.9102e+00
Epoch 2/10
19/19 - 3s - loss: 558.2273 - loglik: -5.5726e+02 - logprior: -9.6801e-01
Epoch 3/10
19/19 - 3s - loss: 557.7401 - loglik: -5.5696e+02 - logprior: -7.8196e-01
Epoch 4/10
19/19 - 3s - loss: 556.6130 - loglik: -5.5590e+02 - logprior: -7.1161e-01
Epoch 5/10
19/19 - 3s - loss: 556.3303 - loglik: -5.5565e+02 - logprior: -6.7856e-01
Epoch 6/10
19/19 - 3s - loss: 555.9281 - loglik: -5.5527e+02 - logprior: -6.6227e-01
Epoch 7/10
19/19 - 3s - loss: 554.7440 - loglik: -5.5410e+02 - logprior: -6.4755e-01
Epoch 8/10
19/19 - 3s - loss: 555.4060 - loglik: -5.5479e+02 - logprior: -6.1581e-01
Fitted a model with MAP estimate = -554.8759
Time for alignment: 74.2196
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 609.4146 - loglik: -6.0639e+02 - logprior: -3.0201e+00
Epoch 2/10
19/19 - 2s - loss: 578.9775 - loglik: -5.7777e+02 - logprior: -1.2083e+00
Epoch 3/10
19/19 - 2s - loss: 567.0648 - loglik: -5.6595e+02 - logprior: -1.1190e+00
Epoch 4/10
19/19 - 2s - loss: 565.0080 - loglik: -5.6399e+02 - logprior: -1.0219e+00
Epoch 5/10
19/19 - 2s - loss: 564.1215 - loglik: -5.6315e+02 - logprior: -9.7034e-01
Epoch 6/10
19/19 - 2s - loss: 563.3967 - loglik: -5.6242e+02 - logprior: -9.7254e-01
Epoch 7/10
19/19 - 2s - loss: 563.4186 - loglik: -5.6244e+02 - logprior: -9.7606e-01
Fitted a model with MAP estimate = -561.7583
expansions: [(0, 2), (7, 1), (20, 3), (21, 2), (22, 1), (23, 1), (39, 1), (42, 1), (44, 1), (53, 3), (54, 2), (74, 1), (75, 1), (76, 2), (77, 3), (78, 2), (80, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 567.2211 - loglik: -5.6315e+02 - logprior: -4.0689e+00
Epoch 2/2
19/19 - 3s - loss: 560.9498 - loglik: -5.5977e+02 - logprior: -1.1810e+00
Fitted a model with MAP estimate = -558.5611
expansions: [(72, 1)]
discards: [ 1 96]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 562.9236 - loglik: -5.6002e+02 - logprior: -2.9024e+00
Epoch 2/2
19/19 - 3s - loss: 559.7603 - loglik: -5.5875e+02 - logprior: -1.0129e+00
Fitted a model with MAP estimate = -557.8587
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 560.9588 - loglik: -5.5804e+02 - logprior: -2.9223e+00
Epoch 2/10
19/19 - 2s - loss: 558.5496 - loglik: -5.5757e+02 - logprior: -9.7941e-01
Epoch 3/10
19/19 - 3s - loss: 557.5416 - loglik: -5.5677e+02 - logprior: -7.7334e-01
Epoch 4/10
19/19 - 3s - loss: 557.2351 - loglik: -5.5652e+02 - logprior: -7.1275e-01
Epoch 5/10
19/19 - 3s - loss: 555.7370 - loglik: -5.5505e+02 - logprior: -6.8432e-01
Epoch 6/10
19/19 - 3s - loss: 555.5563 - loglik: -5.5489e+02 - logprior: -6.6959e-01
Epoch 7/10
19/19 - 3s - loss: 555.8067 - loglik: -5.5516e+02 - logprior: -6.4337e-01
Fitted a model with MAP estimate = -555.0810
Time for alignment: 71.6821
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 609.6514 - loglik: -6.0663e+02 - logprior: -3.0189e+00
Epoch 2/10
19/19 - 2s - loss: 580.2397 - loglik: -5.7903e+02 - logprior: -1.2053e+00
Epoch 3/10
19/19 - 2s - loss: 567.1607 - loglik: -5.6605e+02 - logprior: -1.1106e+00
Epoch 4/10
19/19 - 2s - loss: 564.7322 - loglik: -5.6374e+02 - logprior: -9.9699e-01
Epoch 5/10
19/19 - 2s - loss: 563.6451 - loglik: -5.6267e+02 - logprior: -9.7663e-01
Epoch 6/10
19/19 - 2s - loss: 562.5627 - loglik: -5.6160e+02 - logprior: -9.5813e-01
Epoch 7/10
19/19 - 2s - loss: 563.1992 - loglik: -5.6224e+02 - logprior: -9.5587e-01
Fitted a model with MAP estimate = -561.3190
expansions: [(0, 2), (20, 4), (21, 2), (22, 1), (39, 2), (42, 2), (44, 1), (45, 1), (51, 1), (52, 1), (53, 1), (73, 1), (74, 2), (75, 2), (76, 3), (77, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 567.4299 - loglik: -5.6337e+02 - logprior: -4.0575e+00
Epoch 2/2
19/19 - 3s - loss: 561.2369 - loglik: -5.6005e+02 - logprior: -1.1917e+00
Fitted a model with MAP estimate = -558.8977
expansions: []
discards: [ 1 27 94]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 562.9863 - loglik: -5.6011e+02 - logprior: -2.8798e+00
Epoch 2/2
19/19 - 3s - loss: 560.7125 - loglik: -5.5968e+02 - logprior: -1.0301e+00
Fitted a model with MAP estimate = -558.2980
expansions: [(51, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 560.9534 - loglik: -5.5807e+02 - logprior: -2.8860e+00
Epoch 2/10
19/19 - 3s - loss: 558.7563 - loglik: -5.5778e+02 - logprior: -9.8119e-01
Epoch 3/10
19/19 - 3s - loss: 557.6914 - loglik: -5.5690e+02 - logprior: -7.9060e-01
Epoch 4/10
19/19 - 3s - loss: 557.0700 - loglik: -5.5634e+02 - logprior: -7.3477e-01
Epoch 5/10
19/19 - 3s - loss: 556.8098 - loglik: -5.5611e+02 - logprior: -7.0103e-01
Epoch 6/10
19/19 - 3s - loss: 555.5480 - loglik: -5.5487e+02 - logprior: -6.7614e-01
Epoch 7/10
19/19 - 3s - loss: 554.9331 - loglik: -5.5427e+02 - logprior: -6.6563e-01
Epoch 8/10
19/19 - 3s - loss: 555.2112 - loglik: -5.5455e+02 - logprior: -6.5916e-01
Fitted a model with MAP estimate = -554.8757
Time for alignment: 75.2425
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 609.6237 - loglik: -6.0660e+02 - logprior: -3.0216e+00
Epoch 2/10
19/19 - 2s - loss: 582.2529 - loglik: -5.8107e+02 - logprior: -1.1840e+00
Epoch 3/10
19/19 - 2s - loss: 569.3569 - loglik: -5.6827e+02 - logprior: -1.0885e+00
Epoch 4/10
19/19 - 2s - loss: 565.2405 - loglik: -5.6424e+02 - logprior: -1.0031e+00
Epoch 5/10
19/19 - 2s - loss: 564.1991 - loglik: -5.6323e+02 - logprior: -9.7108e-01
Epoch 6/10
19/19 - 2s - loss: 563.1432 - loglik: -5.6217e+02 - logprior: -9.7533e-01
Epoch 7/10
19/19 - 2s - loss: 563.0721 - loglik: -5.6210e+02 - logprior: -9.7124e-01
Epoch 8/10
19/19 - 2s - loss: 562.4012 - loglik: -5.6143e+02 - logprior: -9.7479e-01
Epoch 9/10
19/19 - 2s - loss: 562.0838 - loglik: -5.6111e+02 - logprior: -9.7584e-01
Epoch 10/10
19/19 - 2s - loss: 562.4921 - loglik: -5.6152e+02 - logprior: -9.7042e-01
Fitted a model with MAP estimate = -560.8466
expansions: [(0, 2), (20, 4), (21, 2), (22, 1), (23, 1), (41, 3), (42, 2), (51, 1), (53, 3), (55, 2), (74, 1), (75, 2), (76, 3), (77, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 113 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 568.1740 - loglik: -5.6402e+02 - logprior: -4.1519e+00
Epoch 2/2
19/19 - 3s - loss: 561.1874 - loglik: -5.5999e+02 - logprior: -1.1963e+00
Fitted a model with MAP estimate = -558.5933
expansions: []
discards: [ 1  2 74 97]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 563.0118 - loglik: -5.6011e+02 - logprior: -2.9026e+00
Epoch 2/2
19/19 - 3s - loss: 560.0652 - loglik: -5.5909e+02 - logprior: -9.7413e-01
Fitted a model with MAP estimate = -558.0782
expansions: [(0, 1)]
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 562.6455 - loglik: -5.5821e+02 - logprior: -4.4337e+00
Epoch 2/10
19/19 - 3s - loss: 558.7867 - loglik: -5.5759e+02 - logprior: -1.1963e+00
Epoch 3/10
19/19 - 3s - loss: 558.3804 - loglik: -5.5756e+02 - logprior: -8.2088e-01
Epoch 4/10
19/19 - 3s - loss: 556.7065 - loglik: -5.5597e+02 - logprior: -7.3863e-01
Epoch 5/10
19/19 - 3s - loss: 556.2769 - loglik: -5.5557e+02 - logprior: -7.0518e-01
Epoch 6/10
19/19 - 3s - loss: 555.4213 - loglik: -5.5475e+02 - logprior: -6.7545e-01
Epoch 7/10
19/19 - 3s - loss: 555.5926 - loglik: -5.5494e+02 - logprior: -6.4946e-01
Fitted a model with MAP estimate = -555.1290
Time for alignment: 77.8733
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 609.4503 - loglik: -6.0642e+02 - logprior: -3.0290e+00
Epoch 2/10
19/19 - 2s - loss: 581.6077 - loglik: -5.8042e+02 - logprior: -1.1916e+00
Epoch 3/10
19/19 - 2s - loss: 567.7393 - loglik: -5.6662e+02 - logprior: -1.1196e+00
Epoch 4/10
19/19 - 2s - loss: 564.3491 - loglik: -5.6329e+02 - logprior: -1.0622e+00
Epoch 5/10
19/19 - 2s - loss: 563.3715 - loglik: -5.6236e+02 - logprior: -1.0078e+00
Epoch 6/10
19/19 - 2s - loss: 562.7424 - loglik: -5.6174e+02 - logprior: -1.0058e+00
Epoch 7/10
19/19 - 2s - loss: 562.0493 - loglik: -5.6105e+02 - logprior: -9.9978e-01
Epoch 8/10
19/19 - 2s - loss: 562.1662 - loglik: -5.6117e+02 - logprior: -9.9917e-01
Fitted a model with MAP estimate = -560.8192
expansions: [(0, 2), (17, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (32, 1), (39, 1), (42, 3), (44, 1), (45, 1), (53, 1), (54, 1), (74, 1), (75, 2), (76, 2), (77, 4), (78, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 567.2508 - loglik: -5.6315e+02 - logprior: -4.0961e+00
Epoch 2/2
19/19 - 3s - loss: 561.2588 - loglik: -5.6008e+02 - logprior: -1.1768e+00
Fitted a model with MAP estimate = -558.7757
expansions: []
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 563.0075 - loglik: -5.6015e+02 - logprior: -2.8578e+00
Epoch 2/2
19/19 - 3s - loss: 560.3725 - loglik: -5.5936e+02 - logprior: -1.0173e+00
Fitted a model with MAP estimate = -558.2822
expansions: [(53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10061 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 561.2392 - loglik: -5.5837e+02 - logprior: -2.8677e+00
Epoch 2/10
19/19 - 3s - loss: 558.8345 - loglik: -5.5785e+02 - logprior: -9.8595e-01
Epoch 3/10
19/19 - 3s - loss: 557.7699 - loglik: -5.5698e+02 - logprior: -7.8826e-01
Epoch 4/10
19/19 - 3s - loss: 557.0654 - loglik: -5.5634e+02 - logprior: -7.2535e-01
Epoch 5/10
19/19 - 3s - loss: 556.6313 - loglik: -5.5594e+02 - logprior: -6.8650e-01
Epoch 6/10
19/19 - 3s - loss: 555.8425 - loglik: -5.5516e+02 - logprior: -6.8467e-01
Epoch 7/10
19/19 - 3s - loss: 555.3361 - loglik: -5.5467e+02 - logprior: -6.6831e-01
Epoch 8/10
19/19 - 3s - loss: 555.6337 - loglik: -5.5498e+02 - logprior: -6.5242e-01
Fitted a model with MAP estimate = -555.1782
Time for alignment: 78.3144
Computed alignments with likelihoods: ['-554.8759', '-555.0810', '-554.8757', '-555.1290', '-555.1782']
Best model has likelihood: -554.8757  (prior= -0.6703 )
time for generating output: 0.1536
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF07686.projection.fasta
SP score = 0.8841494822986058
Training of 5 independent models on file PF00505.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 383.3586 - loglik: -3.8015e+02 - logprior: -3.2076e+00
Epoch 2/10
19/19 - 1s - loss: 349.7702 - loglik: -3.4815e+02 - logprior: -1.6251e+00
Epoch 3/10
19/19 - 1s - loss: 338.1267 - loglik: -3.3664e+02 - logprior: -1.4888e+00
Epoch 4/10
19/19 - 1s - loss: 335.2322 - loglik: -3.3369e+02 - logprior: -1.5393e+00
Epoch 5/10
19/19 - 1s - loss: 334.4433 - loglik: -3.3292e+02 - logprior: -1.5235e+00
Epoch 6/10
19/19 - 1s - loss: 333.8801 - loglik: -3.3237e+02 - logprior: -1.5082e+00
Epoch 7/10
19/19 - 1s - loss: 333.2336 - loglik: -3.3173e+02 - logprior: -1.5034e+00
Epoch 8/10
19/19 - 1s - loss: 333.7153 - loglik: -3.3222e+02 - logprior: -1.4949e+00
Fitted a model with MAP estimate = -333.0187
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (22, 2), (23, 1), (34, 1), (35, 1), (40, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 333.1156 - loglik: -3.2982e+02 - logprior: -3.2982e+00
Epoch 2/2
19/19 - 1s - loss: 325.4569 - loglik: -3.2414e+02 - logprior: -1.3136e+00
Fitted a model with MAP estimate = -324.2735
expansions: []
discards: [28]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 327.9543 - loglik: -3.2478e+02 - logprior: -3.1701e+00
Epoch 2/2
19/19 - 1s - loss: 324.8098 - loglik: -3.2356e+02 - logprior: -1.2476e+00
Fitted a model with MAP estimate = -323.9940
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 327.3211 - loglik: -3.2418e+02 - logprior: -3.1450e+00
Epoch 2/10
19/19 - 1s - loss: 324.6071 - loglik: -3.2339e+02 - logprior: -1.2146e+00
Epoch 3/10
19/19 - 1s - loss: 323.6602 - loglik: -3.2259e+02 - logprior: -1.0683e+00
Epoch 4/10
19/19 - 1s - loss: 323.2112 - loglik: -3.2218e+02 - logprior: -1.0338e+00
Epoch 5/10
19/19 - 1s - loss: 322.8261 - loglik: -3.2181e+02 - logprior: -1.0115e+00
Epoch 6/10
19/19 - 1s - loss: 322.1382 - loglik: -3.2114e+02 - logprior: -1.0022e+00
Epoch 7/10
19/19 - 1s - loss: 321.6532 - loglik: -3.2065e+02 - logprior: -1.0026e+00
Epoch 8/10
19/19 - 1s - loss: 321.9322 - loglik: -3.2096e+02 - logprior: -9.7597e-01
Fitted a model with MAP estimate = -321.5960
Time for alignment: 44.1249
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 383.3751 - loglik: -3.8017e+02 - logprior: -3.2012e+00
Epoch 2/10
19/19 - 1s - loss: 351.9301 - loglik: -3.5051e+02 - logprior: -1.4167e+00
Epoch 3/10
19/19 - 1s - loss: 340.6334 - loglik: -3.3909e+02 - logprior: -1.5472e+00
Epoch 4/10
19/19 - 1s - loss: 337.3650 - loglik: -3.3591e+02 - logprior: -1.4532e+00
Epoch 5/10
19/19 - 1s - loss: 336.1209 - loglik: -3.3462e+02 - logprior: -1.5034e+00
Epoch 6/10
19/19 - 1s - loss: 335.5426 - loglik: -3.3406e+02 - logprior: -1.4786e+00
Epoch 7/10
19/19 - 1s - loss: 335.2512 - loglik: -3.3378e+02 - logprior: -1.4702e+00
Epoch 8/10
19/19 - 1s - loss: 335.1307 - loglik: -3.3367e+02 - logprior: -1.4634e+00
Epoch 9/10
19/19 - 1s - loss: 335.3903 - loglik: -3.3393e+02 - logprior: -1.4592e+00
Fitted a model with MAP estimate = -334.7719
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (22, 5), (23, 1), (34, 1), (35, 1), (40, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 337.0969 - loglik: -3.3304e+02 - logprior: -4.0607e+00
Epoch 2/2
19/19 - 1s - loss: 327.7708 - loglik: -3.2582e+02 - logprior: -1.9466e+00
Fitted a model with MAP estimate = -326.1729
expansions: [(0, 2)]
discards: [ 0 27]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 327.6442 - loglik: -3.2466e+02 - logprior: -2.9871e+00
Epoch 2/2
19/19 - 1s - loss: 324.6981 - loglik: -3.2354e+02 - logprior: -1.1618e+00
Fitted a model with MAP estimate = -323.6655
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 329.7133 - loglik: -3.2594e+02 - logprior: -3.7750e+00
Epoch 2/10
19/19 - 1s - loss: 325.0177 - loglik: -3.2375e+02 - logprior: -1.2722e+00
Epoch 3/10
19/19 - 1s - loss: 323.6870 - loglik: -3.2263e+02 - logprior: -1.0546e+00
Epoch 4/10
19/19 - 1s - loss: 323.5128 - loglik: -3.2248e+02 - logprior: -1.0315e+00
Epoch 5/10
19/19 - 1s - loss: 322.3643 - loglik: -3.2135e+02 - logprior: -1.0187e+00
Epoch 6/10
19/19 - 1s - loss: 322.4662 - loglik: -3.2145e+02 - logprior: -1.0155e+00
Fitted a model with MAP estimate = -321.8694
Time for alignment: 43.2081
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 383.4711 - loglik: -3.8026e+02 - logprior: -3.2072e+00
Epoch 2/10
19/19 - 1s - loss: 350.6480 - loglik: -3.4904e+02 - logprior: -1.6104e+00
Epoch 3/10
19/19 - 1s - loss: 341.0028 - loglik: -3.3955e+02 - logprior: -1.4488e+00
Epoch 4/10
19/19 - 1s - loss: 337.2083 - loglik: -3.3573e+02 - logprior: -1.4825e+00
Epoch 5/10
19/19 - 1s - loss: 335.6994 - loglik: -3.3421e+02 - logprior: -1.4907e+00
Epoch 6/10
19/19 - 1s - loss: 334.3103 - loglik: -3.3281e+02 - logprior: -1.4955e+00
Epoch 7/10
19/19 - 1s - loss: 334.5076 - loglik: -3.3302e+02 - logprior: -1.4826e+00
Fitted a model with MAP estimate = -333.9792
expansions: [(16, 1), (17, 1), (18, 1), (21, 3), (22, 1), (23, 1), (24, 1), (35, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 333.1458 - loglik: -3.2985e+02 - logprior: -3.2923e+00
Epoch 2/2
19/19 - 1s - loss: 325.2482 - loglik: -3.2394e+02 - logprior: -1.3087e+00
Fitted a model with MAP estimate = -324.2178
expansions: []
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 327.7515 - loglik: -3.2457e+02 - logprior: -3.1766e+00
Epoch 2/2
19/19 - 1s - loss: 324.8530 - loglik: -3.2361e+02 - logprior: -1.2436e+00
Fitted a model with MAP estimate = -323.9958
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 327.3734 - loglik: -3.2423e+02 - logprior: -3.1445e+00
Epoch 2/10
19/19 - 1s - loss: 324.6227 - loglik: -3.2339e+02 - logprior: -1.2281e+00
Epoch 3/10
19/19 - 1s - loss: 323.8209 - loglik: -3.2275e+02 - logprior: -1.0745e+00
Epoch 4/10
19/19 - 1s - loss: 323.1424 - loglik: -3.2211e+02 - logprior: -1.0358e+00
Epoch 5/10
19/19 - 1s - loss: 322.7680 - loglik: -3.2175e+02 - logprior: -1.0169e+00
Epoch 6/10
19/19 - 1s - loss: 322.0656 - loglik: -3.2106e+02 - logprior: -1.0077e+00
Epoch 7/10
19/19 - 1s - loss: 321.7302 - loglik: -3.2073e+02 - logprior: -9.9905e-01
Epoch 8/10
19/19 - 1s - loss: 321.8004 - loglik: -3.2081e+02 - logprior: -9.8659e-01
Fitted a model with MAP estimate = -321.6087
Time for alignment: 43.6150
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 383.2704 - loglik: -3.8007e+02 - logprior: -3.1976e+00
Epoch 2/10
19/19 - 1s - loss: 351.5775 - loglik: -3.5015e+02 - logprior: -1.4294e+00
Epoch 3/10
19/19 - 1s - loss: 340.1344 - loglik: -3.3852e+02 - logprior: -1.6097e+00
Epoch 4/10
19/19 - 1s - loss: 336.5759 - loglik: -3.3503e+02 - logprior: -1.5492e+00
Epoch 5/10
19/19 - 1s - loss: 334.6788 - loglik: -3.3308e+02 - logprior: -1.5939e+00
Epoch 6/10
19/19 - 1s - loss: 334.2963 - loglik: -3.3273e+02 - logprior: -1.5711e+00
Epoch 7/10
19/19 - 1s - loss: 334.0317 - loglik: -3.3247e+02 - logprior: -1.5603e+00
Epoch 8/10
19/19 - 1s - loss: 334.4693 - loglik: -3.3292e+02 - logprior: -1.5490e+00
Fitted a model with MAP estimate = -333.5904
expansions: [(14, 1), (16, 1), (17, 1), (18, 1), (21, 2), (22, 1), (23, 1), (24, 1), (26, 1), (36, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 337.1825 - loglik: -3.3313e+02 - logprior: -4.0499e+00
Epoch 2/2
19/19 - 1s - loss: 327.6669 - loglik: -3.2575e+02 - logprior: -1.9130e+00
Fitted a model with MAP estimate = -326.0504
expansions: [(0, 2)]
discards: [ 0 25]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 327.8812 - loglik: -3.2490e+02 - logprior: -2.9831e+00
Epoch 2/2
19/19 - 1s - loss: 324.5292 - loglik: -3.2337e+02 - logprior: -1.1640e+00
Fitted a model with MAP estimate = -323.6762
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 329.7575 - loglik: -3.2599e+02 - logprior: -3.7697e+00
Epoch 2/10
19/19 - 1s - loss: 324.9581 - loglik: -3.2368e+02 - logprior: -1.2788e+00
Epoch 3/10
19/19 - 1s - loss: 323.7742 - loglik: -3.2272e+02 - logprior: -1.0585e+00
Epoch 4/10
19/19 - 1s - loss: 323.1417 - loglik: -3.2211e+02 - logprior: -1.0336e+00
Epoch 5/10
19/19 - 1s - loss: 322.7394 - loglik: -3.2172e+02 - logprior: -1.0200e+00
Epoch 6/10
19/19 - 1s - loss: 322.1289 - loglik: -3.2112e+02 - logprior: -1.0066e+00
Epoch 7/10
19/19 - 1s - loss: 321.8298 - loglik: -3.2082e+02 - logprior: -1.0085e+00
Epoch 8/10
19/19 - 1s - loss: 321.8062 - loglik: -3.2082e+02 - logprior: -9.8839e-01
Epoch 9/10
19/19 - 1s - loss: 321.6445 - loglik: -3.2067e+02 - logprior: -9.7601e-01
Epoch 10/10
19/19 - 1s - loss: 321.5715 - loglik: -3.2060e+02 - logprior: -9.6775e-01
Fitted a model with MAP estimate = -321.5365
Time for alignment: 45.4028
Fitting a model of length 55 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 383.4464 - loglik: -3.8025e+02 - logprior: -3.2004e+00
Epoch 2/10
19/19 - 1s - loss: 352.9757 - loglik: -3.5155e+02 - logprior: -1.4294e+00
Epoch 3/10
19/19 - 1s - loss: 341.0996 - loglik: -3.3950e+02 - logprior: -1.6018e+00
Epoch 4/10
19/19 - 1s - loss: 336.5493 - loglik: -3.3501e+02 - logprior: -1.5372e+00
Epoch 5/10
19/19 - 1s - loss: 334.5614 - loglik: -3.3296e+02 - logprior: -1.5980e+00
Epoch 6/10
19/19 - 1s - loss: 334.1907 - loglik: -3.3261e+02 - logprior: -1.5760e+00
Epoch 7/10
19/19 - 1s - loss: 333.6224 - loglik: -3.3205e+02 - logprior: -1.5712e+00
Epoch 8/10
19/19 - 1s - loss: 333.5760 - loglik: -3.3202e+02 - logprior: -1.5568e+00
Epoch 9/10
19/19 - 1s - loss: 333.3924 - loglik: -3.3184e+02 - logprior: -1.5553e+00
Epoch 10/10
19/19 - 1s - loss: 333.1800 - loglik: -3.3163e+02 - logprior: -1.5515e+00
Fitted a model with MAP estimate = -333.0303
expansions: [(12, 1), (16, 1), (17, 1), (18, 1), (21, 2), (22, 1), (23, 1), (24, 1), (26, 1), (32, 1), (38, 1), (41, 1), (43, 1), (44, 1), (45, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 337.1488 - loglik: -3.3309e+02 - logprior: -4.0635e+00
Epoch 2/2
19/19 - 1s - loss: 327.9774 - loglik: -3.2602e+02 - logprior: -1.9526e+00
Fitted a model with MAP estimate = -326.1909
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 327.8220 - loglik: -3.2482e+02 - logprior: -3.0054e+00
Epoch 2/2
19/19 - 1s - loss: 324.4217 - loglik: -3.2324e+02 - logprior: -1.1813e+00
Fitted a model with MAP estimate = -323.6608
expansions: []
discards: [ 0 26]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 10016 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 329.6365 - loglik: -3.2588e+02 - logprior: -3.7588e+00
Epoch 2/10
19/19 - 1s - loss: 324.9535 - loglik: -3.2368e+02 - logprior: -1.2697e+00
Epoch 3/10
19/19 - 1s - loss: 323.8541 - loglik: -3.2280e+02 - logprior: -1.0559e+00
Epoch 4/10
19/19 - 1s - loss: 323.2180 - loglik: -3.2219e+02 - logprior: -1.0311e+00
Epoch 5/10
19/19 - 1s - loss: 322.7201 - loglik: -3.2170e+02 - logprior: -1.0152e+00
Epoch 6/10
19/19 - 1s - loss: 321.9991 - loglik: -3.2099e+02 - logprior: -1.0093e+00
Epoch 7/10
19/19 - 1s - loss: 321.9309 - loglik: -3.2093e+02 - logprior: -1.0024e+00
Epoch 8/10
19/19 - 1s - loss: 321.4260 - loglik: -3.2044e+02 - logprior: -9.8453e-01
Epoch 9/10
19/19 - 1s - loss: 322.0521 - loglik: -3.2108e+02 - logprior: -9.7606e-01
Fitted a model with MAP estimate = -321.5411
Time for alignment: 47.4171
Computed alignments with likelihoods: ['-321.5960', '-321.8694', '-321.6087', '-321.5365', '-321.5411']
Best model has likelihood: -321.5365  (prior= -0.9379 )
time for generating output: 0.1066
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00505.projection.fasta
SP score = 0.9660807130477841
Training of 5 independent models on file PF13393.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1552.6864 - loglik: -1.5510e+03 - logprior: -1.6483e+00
Epoch 2/10
39/39 - 25s - loss: 1439.5779 - loglik: -1.4380e+03 - logprior: -1.5956e+00
Epoch 3/10
39/39 - 27s - loss: 1423.9305 - loglik: -1.4223e+03 - logprior: -1.6514e+00
Epoch 4/10
39/39 - 28s - loss: 1420.0789 - loglik: -1.4185e+03 - logprior: -1.5919e+00
Epoch 5/10
39/39 - 28s - loss: 1418.6548 - loglik: -1.4171e+03 - logprior: -1.5967e+00
Epoch 6/10
39/39 - 28s - loss: 1417.6591 - loglik: -1.4160e+03 - logprior: -1.6144e+00
Epoch 7/10
39/39 - 29s - loss: 1417.8350 - loglik: -1.4162e+03 - logprior: -1.6319e+00
Fitted a model with MAP estimate = -1415.4290
expansions: [(8, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (32, 2), (34, 1), (36, 1), (42, 1), (46, 1), (47, 1), (52, 1), (53, 1), (67, 1), (68, 1), (69, 2), (71, 1), (77, 1), (78, 1), (102, 1), (115, 1), (116, 1), (117, 1), (121, 5), (132, 2), (134, 1), (148, 2), (149, 1), (154, 6), (165, 2), (175, 1), (177, 1), (180, 1), (183, 1), (185, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 1), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 43s - loss: 1412.6748 - loglik: -1.4102e+03 - logprior: -2.5185e+00
Epoch 2/2
39/39 - 41s - loss: 1396.5533 - loglik: -1.3952e+03 - logprior: -1.3758e+00
Fitted a model with MAP estimate = -1390.7921
expansions: [(0, 3), (39, 1), (148, 1), (149, 1), (211, 1)]
discards: [  0  87 163 183 192 193]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 1395.6296 - loglik: -1.3942e+03 - logprior: -1.4763e+00
Epoch 2/2
39/39 - 40s - loss: 1389.6317 - loglik: -1.3892e+03 - logprior: -4.1123e-01
Fitted a model with MAP estimate = -1385.9987
expansions: []
discards: [ 0  1 43 44]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 48s - loss: 1396.6732 - loglik: -1.3946e+03 - logprior: -2.0269e+00
Epoch 2/10
39/39 - 45s - loss: 1390.6296 - loglik: -1.3904e+03 - logprior: -2.6768e-01
Epoch 3/10
39/39 - 43s - loss: 1388.6611 - loglik: -1.3887e+03 - logprior: 0.0485
Epoch 4/10
39/39 - 44s - loss: 1384.1316 - loglik: -1.3843e+03 - logprior: 0.2029
Epoch 5/10
39/39 - 43s - loss: 1383.4325 - loglik: -1.3837e+03 - logprior: 0.2881
Epoch 6/10
39/39 - 40s - loss: 1382.4297 - loglik: -1.3828e+03 - logprior: 0.3996
Epoch 7/10
39/39 - 40s - loss: 1381.2588 - loglik: -1.3817e+03 - logprior: 0.4754
Epoch 8/10
39/39 - 39s - loss: 1381.4612 - loglik: -1.3821e+03 - logprior: 0.6488
Fitted a model with MAP estimate = -1381.4441
Time for alignment: 887.5243
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 29s - loss: 1550.9276 - loglik: -1.5493e+03 - logprior: -1.6395e+00
Epoch 2/10
39/39 - 27s - loss: 1435.0431 - loglik: -1.4335e+03 - logprior: -1.5267e+00
Epoch 3/10
39/39 - 26s - loss: 1422.3445 - loglik: -1.4207e+03 - logprior: -1.6754e+00
Epoch 4/10
39/39 - 26s - loss: 1419.0621 - loglik: -1.4174e+03 - logprior: -1.6669e+00
Epoch 5/10
39/39 - 28s - loss: 1416.9260 - loglik: -1.4153e+03 - logprior: -1.6672e+00
Epoch 6/10
39/39 - 28s - loss: 1416.1682 - loglik: -1.4145e+03 - logprior: -1.6861e+00
Epoch 7/10
39/39 - 29s - loss: 1416.3584 - loglik: -1.4147e+03 - logprior: -1.6984e+00
Fitted a model with MAP estimate = -1414.1932
expansions: [(10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (30, 1), (34, 1), (37, 1), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (70, 3), (76, 1), (86, 1), (112, 1), (114, 1), (115, 1), (116, 1), (120, 4), (121, 1), (123, 1), (132, 1), (134, 1), (136, 2), (137, 1), (149, 1), (154, 1), (163, 1), (164, 2), (173, 2), (175, 1), (180, 1), (183, 1), (185, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 44s - loss: 1408.9095 - loglik: -1.4064e+03 - logprior: -2.5287e+00
Epoch 2/2
39/39 - 44s - loss: 1395.8749 - loglik: -1.3946e+03 - logprior: -1.3097e+00
Fitted a model with MAP estimate = -1390.2414
expansions: [(0, 3), (86, 1), (194, 5)]
discards: [ 0 44]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 45s - loss: 1395.2875 - loglik: -1.3938e+03 - logprior: -1.4616e+00
Epoch 2/2
39/39 - 41s - loss: 1389.6533 - loglik: -1.3893e+03 - logprior: -3.9129e-01
Fitted a model with MAP estimate = -1386.0063
expansions: [(192, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 1394.3762 - loglik: -1.3923e+03 - logprior: -2.0340e+00
Epoch 2/10
39/39 - 44s - loss: 1387.8722 - loglik: -1.3876e+03 - logprior: -2.8449e-01
Epoch 3/10
39/39 - 46s - loss: 1384.9252 - loglik: -1.3850e+03 - logprior: 0.0498
Epoch 4/10
39/39 - 44s - loss: 1382.5599 - loglik: -1.3827e+03 - logprior: 0.1454
Epoch 5/10
39/39 - 43s - loss: 1379.3822 - loglik: -1.3797e+03 - logprior: 0.2727
Epoch 6/10
39/39 - 40s - loss: 1379.5371 - loglik: -1.3799e+03 - logprior: 0.4058
Fitted a model with MAP estimate = -1378.8662
Time for alignment: 812.5417
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 1551.3978 - loglik: -1.5498e+03 - logprior: -1.6362e+00
Epoch 2/10
39/39 - 24s - loss: 1437.0591 - loglik: -1.4355e+03 - logprior: -1.5192e+00
Epoch 3/10
39/39 - 24s - loss: 1424.6400 - loglik: -1.4230e+03 - logprior: -1.6207e+00
Epoch 4/10
39/39 - 24s - loss: 1421.2291 - loglik: -1.4196e+03 - logprior: -1.6330e+00
Epoch 5/10
39/39 - 24s - loss: 1419.9506 - loglik: -1.4183e+03 - logprior: -1.6429e+00
Epoch 6/10
39/39 - 24s - loss: 1417.6166 - loglik: -1.4160e+03 - logprior: -1.6572e+00
Epoch 7/10
39/39 - 24s - loss: 1418.6995 - loglik: -1.4170e+03 - logprior: -1.6662e+00
Fitted a model with MAP estimate = -1415.9005
expansions: [(8, 1), (9, 1), (10, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (18, 1), (31, 1), (35, 1), (42, 1), (46, 1), (47, 1), (48, 1), (51, 1), (67, 1), (68, 1), (69, 2), (71, 1), (77, 1), (78, 1), (93, 1), (112, 1), (114, 1), (115, 1), (120, 4), (121, 1), (123, 1), (132, 1), (134, 1), (136, 2), (137, 1), (147, 1), (151, 1), (154, 1), (163, 2), (164, 5), (180, 1), (193, 1), (196, 1), (210, 1), (211, 3), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 1413.8018 - loglik: -1.4113e+03 - logprior: -2.5186e+00
Epoch 2/2
39/39 - 35s - loss: 1399.4601 - loglik: -1.3981e+03 - logprior: -1.3515e+00
Fitted a model with MAP estimate = -1393.7421
expansions: [(0, 3), (150, 1), (208, 1)]
discards: [  0  86 204]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 1398.7695 - loglik: -1.3973e+03 - logprior: -1.5078e+00
Epoch 2/2
39/39 - 34s - loss: 1392.9283 - loglik: -1.3925e+03 - logprior: -3.9275e-01
Fitted a model with MAP estimate = -1388.8781
expansions: [(198, 5), (242, 1)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 308 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 40s - loss: 1396.2552 - loglik: -1.3942e+03 - logprior: -2.0721e+00
Epoch 2/10
39/39 - 38s - loss: 1388.3228 - loglik: -1.3882e+03 - logprior: -1.5945e-01
Epoch 3/10
39/39 - 37s - loss: 1387.2871 - loglik: -1.3873e+03 - logprior: 0.0553
Epoch 4/10
39/39 - 35s - loss: 1382.6813 - loglik: -1.3828e+03 - logprior: 0.1547
Epoch 5/10
39/39 - 35s - loss: 1380.9525 - loglik: -1.3812e+03 - logprior: 0.2591
Epoch 6/10
39/39 - 35s - loss: 1381.2898 - loglik: -1.3817e+03 - logprior: 0.3866
Fitted a model with MAP estimate = -1380.1067
Time for alignment: 696.2046
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 28s - loss: 1551.1063 - loglik: -1.5495e+03 - logprior: -1.6490e+00
Epoch 2/10
39/39 - 25s - loss: 1436.6237 - loglik: -1.4350e+03 - logprior: -1.6250e+00
Epoch 3/10
39/39 - 26s - loss: 1422.3831 - loglik: -1.4207e+03 - logprior: -1.7025e+00
Epoch 4/10
39/39 - 26s - loss: 1419.2849 - loglik: -1.4176e+03 - logprior: -1.6681e+00
Epoch 5/10
39/39 - 26s - loss: 1417.4265 - loglik: -1.4157e+03 - logprior: -1.6963e+00
Epoch 6/10
39/39 - 26s - loss: 1416.7380 - loglik: -1.4150e+03 - logprior: -1.7240e+00
Epoch 7/10
39/39 - 25s - loss: 1416.2916 - loglik: -1.4146e+03 - logprior: -1.7266e+00
Epoch 8/10
39/39 - 24s - loss: 1416.3319 - loglik: -1.4146e+03 - logprior: -1.7318e+00
Fitted a model with MAP estimate = -1414.0241
expansions: [(8, 1), (10, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (32, 2), (34, 1), (35, 1), (42, 1), (46, 1), (47, 1), (48, 1), (51, 1), (67, 1), (68, 1), (69, 1), (70, 2), (72, 1), (76, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (122, 1), (132, 1), (136, 1), (138, 1), (146, 2), (147, 1), (150, 1), (154, 1), (155, 1), (163, 1), (164, 6), (173, 2), (175, 1), (180, 1), (183, 1), (185, 1), (192, 3), (193, 1), (196, 1), (211, 1), (212, 3), (213, 2), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 37s - loss: 1408.8888 - loglik: -1.4064e+03 - logprior: -2.4861e+00
Epoch 2/2
39/39 - 35s - loss: 1391.6134 - loglik: -1.3902e+03 - logprior: -1.3923e+00
Fitted a model with MAP estimate = -1386.5531
expansions: [(0, 3), (150, 1), (208, 1), (209, 1)]
discards: [  0  39  88 182]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 1392.3599 - loglik: -1.3909e+03 - logprior: -1.4576e+00
Epoch 2/2
39/39 - 38s - loss: 1387.6837 - loglik: -1.3873e+03 - logprior: -3.9107e-01
Fitted a model with MAP estimate = -1383.1796
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 311 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 38s - loss: 1391.7513 - loglik: -1.3897e+03 - logprior: -2.0062e+00
Epoch 2/10
39/39 - 35s - loss: 1385.8400 - loglik: -1.3857e+03 - logprior: -1.3215e-01
Epoch 3/10
39/39 - 36s - loss: 1382.7559 - loglik: -1.3828e+03 - logprior: 0.0568
Epoch 4/10
39/39 - 38s - loss: 1380.7328 - loglik: -1.3809e+03 - logprior: 0.1652
Epoch 5/10
39/39 - 42s - loss: 1378.7418 - loglik: -1.3790e+03 - logprior: 0.2584
Epoch 6/10
39/39 - 45s - loss: 1377.7458 - loglik: -1.3781e+03 - logprior: 0.3643
Epoch 7/10
39/39 - 47s - loss: 1376.9222 - loglik: -1.3774e+03 - logprior: 0.4927
Epoch 8/10
39/39 - 49s - loss: 1377.5446 - loglik: -1.3781e+03 - logprior: 0.5906
Fitted a model with MAP estimate = -1376.8332
Time for alignment: 856.1733
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 1551.7267 - loglik: -1.5501e+03 - logprior: -1.6464e+00
Epoch 2/10
39/39 - 32s - loss: 1438.4937 - loglik: -1.4369e+03 - logprior: -1.6151e+00
Epoch 3/10
39/39 - 32s - loss: 1424.9480 - loglik: -1.4233e+03 - logprior: -1.6733e+00
Epoch 4/10
39/39 - 33s - loss: 1421.0366 - loglik: -1.4194e+03 - logprior: -1.6442e+00
Epoch 5/10
39/39 - 31s - loss: 1417.7798 - loglik: -1.4161e+03 - logprior: -1.6620e+00
Epoch 6/10
39/39 - 28s - loss: 1417.4696 - loglik: -1.4158e+03 - logprior: -1.6711e+00
Epoch 7/10
39/39 - 26s - loss: 1416.6106 - loglik: -1.4149e+03 - logprior: -1.6787e+00
Epoch 8/10
39/39 - 25s - loss: 1417.5874 - loglik: -1.4159e+03 - logprior: -1.6850e+00
Fitted a model with MAP estimate = -1414.5818
expansions: [(12, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (33, 1), (34, 1), (35, 1), (37, 1), (41, 1), (45, 1), (46, 1), (51, 1), (67, 1), (70, 3), (76, 1), (102, 1), (115, 1), (116, 1), (117, 1), (120, 4), (121, 1), (131, 1), (132, 1), (135, 2), (137, 1), (147, 1), (150, 1), (153, 1), (154, 1), (155, 4), (163, 2), (165, 1), (169, 1), (177, 1), (178, 1), (180, 1), (184, 1), (185, 2), (192, 2), (193, 1), (194, 1), (197, 1), (211, 1), (212, 4), (214, 2), (226, 1), (227, 1), (230, 1), (231, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 38s - loss: 1411.7003 - loglik: -1.4093e+03 - logprior: -2.4212e+00
Epoch 2/2
39/39 - 39s - loss: 1395.0563 - loglik: -1.3937e+03 - logprior: -1.3116e+00
Fitted a model with MAP estimate = -1389.3879
expansions: [(0, 3), (85, 1), (147, 1), (194, 1)]
discards: [  0 165 244 269]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 42s - loss: 1394.2208 - loglik: -1.3928e+03 - logprior: -1.4054e+00
Epoch 2/2
39/39 - 37s - loss: 1388.9899 - loglik: -1.3887e+03 - logprior: -3.2685e-01
Fitted a model with MAP estimate = -1384.4806
expansions: [(171, 2), (209, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 10017 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 43s - loss: 1392.9380 - loglik: -1.3910e+03 - logprior: -1.9687e+00
Epoch 2/10
39/39 - 45s - loss: 1386.0750 - loglik: -1.3859e+03 - logprior: -2.1540e-01
Epoch 3/10
39/39 - 48s - loss: 1383.7612 - loglik: -1.3839e+03 - logprior: 0.1178
Epoch 4/10
39/39 - 49s - loss: 1379.8959 - loglik: -1.3801e+03 - logprior: 0.2310
Epoch 5/10
39/39 - 45s - loss: 1379.3512 - loglik: -1.3797e+03 - logprior: 0.3137
Epoch 6/10
39/39 - 46s - loss: 1376.8945 - loglik: -1.3773e+03 - logprior: 0.3955
Epoch 7/10
39/39 - 48s - loss: 1377.9280 - loglik: -1.3784e+03 - logprior: 0.5115
Fitted a model with MAP estimate = -1376.9314
Time for alignment: 899.0681
Computed alignments with likelihoods: ['-1381.4441', '-1378.8662', '-1380.1067', '-1376.8332', '-1376.9314']
Best model has likelihood: -1376.8332  (prior= 0.6311 )
time for generating output: 0.3136
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13393.projection.fasta
SP score = 0.4092755360234385
Training of 5 independent models on file PF00202.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 65s - loss: 2083.9194 - loglik: -2.0826e+03 - logprior: -1.3111e+00
Epoch 2/10
39/39 - 66s - loss: 1920.8580 - loglik: -1.9198e+03 - logprior: -1.0467e+00
Epoch 3/10
39/39 - 65s - loss: 1908.6431 - loglik: -1.9075e+03 - logprior: -1.1368e+00
Epoch 4/10
39/39 - 57s - loss: 1906.6993 - loglik: -1.9056e+03 - logprior: -1.1189e+00
Epoch 5/10
39/39 - 51s - loss: 1905.7991 - loglik: -1.9046e+03 - logprior: -1.1520e+00
Epoch 6/10
39/39 - 56s - loss: 1905.4172 - loglik: -1.9043e+03 - logprior: -1.1399e+00
Epoch 7/10
39/39 - 63s - loss: 1905.2097 - loglik: -1.9041e+03 - logprior: -1.1302e+00
Epoch 8/10
39/39 - 56s - loss: 1905.1584 - loglik: -1.9040e+03 - logprior: -1.1515e+00
Epoch 9/10
39/39 - 50s - loss: 1904.9379 - loglik: -1.9038e+03 - logprior: -1.1595e+00
Epoch 10/10
39/39 - 48s - loss: 1904.2466 - loglik: -1.9031e+03 - logprior: -1.1497e+00
Fitted a model with MAP estimate = -1884.2154
expansions: [(0, 3), (5, 2), (44, 1), (51, 1), (53, 1), (57, 1), (68, 1), (70, 1), (71, 1), (75, 2), (99, 1), (120, 2), (121, 1), (122, 1), (123, 1), (146, 1), (147, 2), (148, 1), (149, 1), (164, 1), (171, 1), (174, 1), (176, 1), (177, 1), (189, 1), (193, 2), (196, 1), (197, 1), (200, 1), (218, 1), (219, 1), (220, 1), (222, 1), (223, 1), (224, 3), (246, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (259, 1), (261, 3), (262, 1), (263, 1), (285, 3), (286, 8), (295, 2), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 87s - loss: 1891.8943 - loglik: -1.8897e+03 - logprior: -2.2341e+00
Epoch 2/2
39/39 - 72s - loss: 1873.8713 - loglik: -1.8731e+03 - logprior: -7.4428e-01
Fitted a model with MAP estimate = -1851.4510
expansions: [(0, 2), (347, 1), (348, 2)]
discards: [  1   2   8  87 136 168 171 224 266 314 338 339 340 341 342 389]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 91s - loss: 1879.3893 - loglik: -1.8772e+03 - logprior: -2.1953e+00
Epoch 2/2
39/39 - 92s - loss: 1873.6213 - loglik: -1.8734e+03 - logprior: -1.9736e-01
Fitted a model with MAP estimate = -1851.8121
expansions: [(0, 2), (259, 1), (330, 2)]
discards: [  1   2 336]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 93s - loss: 1857.0815 - loglik: -1.8551e+03 - logprior: -1.9816e+00
Epoch 2/10
39/39 - 93s - loss: 1852.1571 - loglik: -1.8524e+03 - logprior: 0.2378
Epoch 3/10
39/39 - 88s - loss: 1851.1732 - loglik: -1.8516e+03 - logprior: 0.4506
Epoch 4/10
39/39 - 72s - loss: 1850.6434 - loglik: -1.8512e+03 - logprior: 0.5608
Epoch 5/10
39/39 - 66s - loss: 1845.4309 - loglik: -1.8462e+03 - logprior: 0.7202
Epoch 6/10
39/39 - 77s - loss: 1849.8322 - loglik: -1.8507e+03 - logprior: 0.8395
Fitted a model with MAP estimate = -1847.1399
Time for alignment: 1823.8721
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 57s - loss: 2084.5552 - loglik: -2.0832e+03 - logprior: -1.3148e+00
Epoch 2/10
39/39 - 50s - loss: 1922.7775 - loglik: -1.9218e+03 - logprior: -1.0157e+00
Epoch 3/10
39/39 - 49s - loss: 1909.7019 - loglik: -1.9086e+03 - logprior: -1.0798e+00
Epoch 4/10
39/39 - 51s - loss: 1907.4812 - loglik: -1.9064e+03 - logprior: -1.1088e+00
Epoch 5/10
39/39 - 57s - loss: 1905.3328 - loglik: -1.9042e+03 - logprior: -1.1125e+00
Epoch 6/10
39/39 - 63s - loss: 1906.1426 - loglik: -1.9050e+03 - logprior: -1.1262e+00
Fitted a model with MAP estimate = -1884.9593
expansions: [(0, 3), (5, 1), (28, 1), (50, 1), (51, 1), (52, 1), (56, 1), (69, 1), (70, 1), (73, 2), (74, 2), (104, 1), (119, 1), (120, 2), (122, 1), (146, 3), (147, 1), (148, 1), (163, 1), (170, 1), (173, 1), (175, 1), (176, 1), (185, 1), (192, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (221, 1), (222, 2), (223, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (262, 3), (263, 1), (264, 1), (266, 2), (285, 6), (286, 4), (287, 1), (288, 1), (295, 3), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 399 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 89s - loss: 1890.9021 - loglik: -1.8888e+03 - logprior: -2.0972e+00
Epoch 2/2
39/39 - 92s - loss: 1872.1272 - loglik: -1.8716e+03 - logprior: -5.6797e-01
Fitted a model with MAP estimate = -1850.3212
expansions: [(0, 2), (349, 1)]
discards: [  1   4  85 137 170 223 264 315 322 392]
Re-initialized the encoder parameters.
Fitting a model of length 392 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 96s - loss: 1876.6140 - loglik: -1.8747e+03 - logprior: -1.9616e+00
Epoch 2/2
39/39 - 75s - loss: 1871.6749 - loglik: -1.8716e+03 - logprior: -5.7463e-02
Fitted a model with MAP estimate = -1849.9526
expansions: [(0, 2), (5, 1)]
discards: [  1   2 338 339]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 87s - loss: 1856.8463 - loglik: -1.8547e+03 - logprior: -2.1021e+00
Epoch 2/10
39/39 - 71s - loss: 1851.8488 - loglik: -1.8521e+03 - logprior: 0.2077
Epoch 3/10
39/39 - 70s - loss: 1848.2356 - loglik: -1.8486e+03 - logprior: 0.3160
Epoch 4/10
39/39 - 80s - loss: 1849.9763 - loglik: -1.8505e+03 - logprior: 0.4841
Fitted a model with MAP estimate = -1847.1545
Time for alignment: 1432.6198
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 64s - loss: 2083.5151 - loglik: -2.0822e+03 - logprior: -1.3099e+00
Epoch 2/10
39/39 - 66s - loss: 1920.5197 - loglik: -1.9195e+03 - logprior: -1.0208e+00
Epoch 3/10
39/39 - 63s - loss: 1908.3115 - loglik: -1.9072e+03 - logprior: -1.1431e+00
Epoch 4/10
39/39 - 59s - loss: 1905.4856 - loglik: -1.9043e+03 - logprior: -1.1835e+00
Epoch 5/10
39/39 - 62s - loss: 1904.7100 - loglik: -1.9035e+03 - logprior: -1.2149e+00
Epoch 6/10
39/39 - 64s - loss: 1903.7419 - loglik: -1.9025e+03 - logprior: -1.2688e+00
Epoch 7/10
39/39 - 64s - loss: 1903.4996 - loglik: -1.9023e+03 - logprior: -1.2421e+00
Epoch 8/10
39/39 - 64s - loss: 1904.1635 - loglik: -1.9029e+03 - logprior: -1.2569e+00
Fitted a model with MAP estimate = -1883.3540
expansions: [(0, 3), (34, 1), (47, 1), (50, 1), (52, 1), (56, 1), (60, 1), (68, 1), (69, 1), (72, 2), (98, 1), (119, 2), (120, 1), (121, 1), (122, 1), (145, 3), (147, 1), (166, 1), (169, 1), (175, 1), (176, 1), (185, 1), (192, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (244, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (257, 1), (258, 1), (259, 1), (260, 1), (262, 1), (263, 1), (265, 2), (284, 1), (285, 3), (286, 5), (287, 1), (288, 1), (295, 3), (296, 1), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 394 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 77s - loss: 1890.2887 - loglik: -1.8882e+03 - logprior: -2.0886e+00
Epoch 2/2
39/39 - 87s - loss: 1872.8372 - loglik: -1.8724e+03 - logprior: -4.7548e-01
Fitted a model with MAP estimate = -1850.7428
expansions: [(0, 2), (344, 1)]
discards: [ 83 134 220 262 317 387]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 1876.6260 - loglik: -1.8746e+03 - logprior: -1.9847e+00
Epoch 2/2
39/39 - 84s - loss: 1871.7576 - loglik: -1.8717e+03 - logprior: -4.3885e-02
Fitted a model with MAP estimate = -1850.2172
expansions: [(260, 1)]
discards: [  1   2   3 336 337]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 76s - loss: 1856.2428 - loglik: -1.8554e+03 - logprior: -8.3861e-01
Epoch 2/10
39/39 - 87s - loss: 1850.4447 - loglik: -1.8508e+03 - logprior: 0.3916
Epoch 3/10
39/39 - 92s - loss: 1851.2163 - loglik: -1.8517e+03 - logprior: 0.5147
Fitted a model with MAP estimate = -1848.5989
Time for alignment: 1476.0326
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 61s - loss: 2085.5266 - loglik: -2.0842e+03 - logprior: -1.2953e+00
Epoch 2/10
39/39 - 52s - loss: 1921.7821 - loglik: -1.9208e+03 - logprior: -1.0180e+00
Epoch 3/10
39/39 - 51s - loss: 1909.8619 - loglik: -1.9088e+03 - logprior: -1.0729e+00
Epoch 4/10
39/39 - 53s - loss: 1906.8778 - loglik: -1.9058e+03 - logprior: -1.1122e+00
Epoch 5/10
39/39 - 54s - loss: 1906.8060 - loglik: -1.9057e+03 - logprior: -1.1251e+00
Epoch 6/10
39/39 - 50s - loss: 1904.5303 - loglik: -1.9034e+03 - logprior: -1.1429e+00
Epoch 7/10
39/39 - 51s - loss: 1905.4830 - loglik: -1.9043e+03 - logprior: -1.1440e+00
Fitted a model with MAP estimate = -1884.8581
expansions: [(0, 3), (5, 1), (34, 1), (51, 1), (53, 1), (54, 2), (58, 1), (69, 1), (70, 2), (74, 2), (98, 1), (119, 2), (120, 2), (121, 2), (145, 3), (146, 1), (147, 1), (166, 1), (169, 1), (173, 1), (174, 1), (175, 1), (187, 1), (191, 2), (194, 1), (195, 1), (217, 1), (218, 1), (219, 1), (221, 1), (222, 1), (223, 3), (245, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (258, 1), (259, 1), (261, 1), (262, 1), (263, 1), (282, 1), (284, 1), (286, 2), (287, 6), (296, 2), (297, 1), (298, 3), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 397 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 75s - loss: 1890.8229 - loglik: -1.8887e+03 - logprior: -2.1489e+00
Epoch 2/2
39/39 - 74s - loss: 1872.1871 - loglik: -1.8717e+03 - logprior: -5.3206e-01
Fitted a model with MAP estimate = -1849.9648
expansions: [(0, 2), (347, 4)]
discards: [  1   2  87 136 140 171 224 337 338 339 340 341 390]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 70s - loss: 1877.3322 - loglik: -1.8753e+03 - logprior: -2.0157e+00
Epoch 2/2
39/39 - 69s - loss: 1872.2590 - loglik: -1.8723e+03 - logprior: 0.0362
Fitted a model with MAP estimate = -1850.3772
expansions: [(0, 2), (335, 3)]
discards: [  1   2 332 336 358 361]
Re-initialized the encoder parameters.
Fitting a model of length 389 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 71s - loss: 1857.6403 - loglik: -1.8556e+03 - logprior: -2.0335e+00
Epoch 2/10
39/39 - 70s - loss: 1852.9393 - loglik: -1.8531e+03 - logprior: 0.1851
Epoch 3/10
39/39 - 71s - loss: 1849.2179 - loglik: -1.8497e+03 - logprior: 0.4525
Epoch 4/10
39/39 - 73s - loss: 1847.5485 - loglik: -1.8481e+03 - logprior: 0.5692
Epoch 5/10
39/39 - 73s - loss: 1846.8928 - loglik: -1.8475e+03 - logprior: 0.6540
Epoch 6/10
39/39 - 72s - loss: 1847.9259 - loglik: -1.8487e+03 - logprior: 0.7864
Fitted a model with MAP estimate = -1846.4841
Time for alignment: 1456.0675
Fitting a model of length 319 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 53s - loss: 2083.0452 - loglik: -2.0817e+03 - logprior: -1.3248e+00
Epoch 2/10
39/39 - 51s - loss: 1919.8279 - loglik: -1.9188e+03 - logprior: -1.0189e+00
Epoch 3/10
39/39 - 50s - loss: 1907.8828 - loglik: -1.9068e+03 - logprior: -1.0701e+00
Epoch 4/10
39/39 - 51s - loss: 1905.5115 - loglik: -1.9044e+03 - logprior: -1.0790e+00
Epoch 5/10
39/39 - 51s - loss: 1904.5343 - loglik: -1.9034e+03 - logprior: -1.1216e+00
Epoch 6/10
39/39 - 50s - loss: 1904.5865 - loglik: -1.9035e+03 - logprior: -1.1333e+00
Fitted a model with MAP estimate = -1883.5698
expansions: [(0, 3), (5, 1), (34, 1), (47, 1), (51, 1), (52, 1), (56, 1), (61, 1), (69, 1), (70, 1), (74, 2), (98, 1), (119, 2), (120, 2), (121, 1), (145, 3), (147, 1), (166, 1), (169, 1), (175, 1), (176, 1), (177, 1), (192, 2), (195, 1), (196, 1), (199, 1), (219, 1), (220, 1), (221, 1), (223, 1), (224, 3), (247, 1), (248, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (259, 1), (261, 3), (262, 1), (263, 1), (285, 6), (286, 4), (287, 1), (288, 1), (296, 2), (297, 1), (298, 2), (309, 1), (313, 2), (314, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 396 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 76s - loss: 1889.2103 - loglik: -1.8870e+03 - logprior: -2.1723e+00
Epoch 2/2
39/39 - 71s - loss: 1872.6240 - loglik: -1.8721e+03 - logprior: -5.6520e-01
Fitted a model with MAP estimate = -1850.4418
expansions: [(0, 2), (345, 1)]
discards: [  1   4  86 135 221 263 311 364 389]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 65s - loss: 1876.7402 - loglik: -1.8746e+03 - logprior: -2.0933e+00
Epoch 2/2
39/39 - 61s - loss: 1871.8269 - loglik: -1.8718e+03 - logprior: -1.3325e-02
Fitted a model with MAP estimate = -1850.0497
expansions: [(0, 2), (259, 1)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 10142 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 74s - loss: 1856.3658 - loglik: -1.8542e+03 - logprior: -2.1503e+00
Epoch 2/10
39/39 - 72s - loss: 1851.3658 - loglik: -1.8515e+03 - logprior: 0.1561
Epoch 3/10
39/39 - 75s - loss: 1849.0649 - loglik: -1.8495e+03 - logprior: 0.4360
Epoch 4/10
39/39 - 74s - loss: 1847.7709 - loglik: -1.8483e+03 - logprior: 0.5681
Epoch 5/10
39/39 - 78s - loss: 1846.6643 - loglik: -1.8474e+03 - logprior: 0.6948
Epoch 6/10
39/39 - 75s - loss: 1845.3300 - loglik: -1.8461e+03 - logprior: 0.8141
Epoch 7/10
39/39 - 79s - loss: 1845.0934 - loglik: -1.8460e+03 - logprior: 0.9549
Epoch 8/10
39/39 - 79s - loss: 1845.7781 - loglik: -1.8469e+03 - logprior: 1.0927
Fitted a model with MAP estimate = -1845.6057
Time for alignment: 1521.2167
Computed alignments with likelihoods: ['-1847.1399', '-1847.1545', '-1848.5989', '-1846.4841', '-1845.6057']
Best model has likelihood: -1845.6057  (prior= 1.2357 )
time for generating output: 1.4049
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00202.projection.fasta
SP score = 0.4661604843672569
Training of 5 independent models on file PF00018.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 273.0683 - loglik: -2.6984e+02 - logprior: -3.2271e+00
Epoch 2/10
19/19 - 1s - loss: 247.8957 - loglik: -2.4648e+02 - logprior: -1.4137e+00
Epoch 3/10
19/19 - 1s - loss: 239.9387 - loglik: -2.3860e+02 - logprior: -1.3378e+00
Epoch 4/10
19/19 - 1s - loss: 238.2907 - loglik: -2.3701e+02 - logprior: -1.2783e+00
Epoch 5/10
19/19 - 1s - loss: 237.6495 - loglik: -2.3640e+02 - logprior: -1.2516e+00
Epoch 6/10
19/19 - 1s - loss: 237.2906 - loglik: -2.3605e+02 - logprior: -1.2370e+00
Epoch 7/10
19/19 - 1s - loss: 237.5948 - loglik: -2.3637e+02 - logprior: -1.2223e+00
Fitted a model with MAP estimate = -237.1079
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 240.0976 - loglik: -2.3583e+02 - logprior: -4.2654e+00
Epoch 2/2
19/19 - 1s - loss: 232.9141 - loglik: -2.3154e+02 - logprior: -1.3779e+00
Fitted a model with MAP estimate = -231.7612
expansions: []
discards: [ 0 10 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 237.2625 - loglik: -2.3316e+02 - logprior: -4.1070e+00
Epoch 2/2
19/19 - 1s - loss: 233.0468 - loglik: -2.3151e+02 - logprior: -1.5410e+00
Fitted a model with MAP estimate = -232.2013
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 234.9847 - loglik: -2.3171e+02 - logprior: -3.2731e+00
Epoch 2/10
19/19 - 1s - loss: 232.3764 - loglik: -2.3097e+02 - logprior: -1.4048e+00
Epoch 3/10
19/19 - 1s - loss: 232.0880 - loglik: -2.3079e+02 - logprior: -1.3027e+00
Epoch 4/10
19/19 - 1s - loss: 231.6573 - loglik: -2.3041e+02 - logprior: -1.2509e+00
Epoch 5/10
19/19 - 1s - loss: 231.7970 - loglik: -2.3058e+02 - logprior: -1.2192e+00
Fitted a model with MAP estimate = -231.5411
Time for alignment: 33.9727
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 273.0866 - loglik: -2.6986e+02 - logprior: -3.2235e+00
Epoch 2/10
19/19 - 1s - loss: 247.8254 - loglik: -2.4641e+02 - logprior: -1.4141e+00
Epoch 3/10
19/19 - 1s - loss: 240.4112 - loglik: -2.3909e+02 - logprior: -1.3229e+00
Epoch 4/10
19/19 - 1s - loss: 238.6445 - loglik: -2.3738e+02 - logprior: -1.2620e+00
Epoch 5/10
19/19 - 1s - loss: 237.7442 - loglik: -2.3651e+02 - logprior: -1.2330e+00
Epoch 6/10
19/19 - 1s - loss: 237.5386 - loglik: -2.3632e+02 - logprior: -1.2206e+00
Epoch 7/10
19/19 - 1s - loss: 237.4174 - loglik: -2.3621e+02 - logprior: -1.2066e+00
Epoch 8/10
19/19 - 1s - loss: 237.1998 - loglik: -2.3600e+02 - logprior: -1.1976e+00
Epoch 9/10
19/19 - 1s - loss: 237.5008 - loglik: -2.3631e+02 - logprior: -1.1900e+00
Fitted a model with MAP estimate = -237.0862
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (23, 2), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 52 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 240.3610 - loglik: -2.3600e+02 - logprior: -4.3579e+00
Epoch 2/2
19/19 - 1s - loss: 232.8809 - loglik: -2.3149e+02 - logprior: -1.3899e+00
Fitted a model with MAP estimate = -231.7328
expansions: []
discards: [ 0 11 29 32 40 42]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 237.4811 - loglik: -2.3337e+02 - logprior: -4.1078e+00
Epoch 2/2
19/19 - 1s - loss: 232.9200 - loglik: -2.3138e+02 - logprior: -1.5398e+00
Fitted a model with MAP estimate = -232.2373
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 235.0074 - loglik: -2.3173e+02 - logprior: -3.2728e+00
Epoch 2/10
19/19 - 1s - loss: 232.4650 - loglik: -2.3106e+02 - logprior: -1.4035e+00
Epoch 3/10
19/19 - 1s - loss: 231.8428 - loglik: -2.3054e+02 - logprior: -1.3022e+00
Epoch 4/10
19/19 - 1s - loss: 231.9121 - loglik: -2.3066e+02 - logprior: -1.2489e+00
Fitted a model with MAP estimate = -231.6481
Time for alignment: 33.8403
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 273.0403 - loglik: -2.6981e+02 - logprior: -3.2286e+00
Epoch 2/10
19/19 - 1s - loss: 247.8477 - loglik: -2.4643e+02 - logprior: -1.4171e+00
Epoch 3/10
19/19 - 1s - loss: 240.3072 - loglik: -2.3898e+02 - logprior: -1.3284e+00
Epoch 4/10
19/19 - 1s - loss: 238.1435 - loglik: -2.3687e+02 - logprior: -1.2734e+00
Epoch 5/10
19/19 - 1s - loss: 237.8485 - loglik: -2.3660e+02 - logprior: -1.2495e+00
Epoch 6/10
19/19 - 1s - loss: 237.2390 - loglik: -2.3600e+02 - logprior: -1.2369e+00
Epoch 7/10
19/19 - 1s - loss: 237.4268 - loglik: -2.3620e+02 - logprior: -1.2228e+00
Fitted a model with MAP estimate = -237.1110
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 240.0822 - loglik: -2.3583e+02 - logprior: -4.2561e+00
Epoch 2/2
19/19 - 1s - loss: 232.7928 - loglik: -2.3141e+02 - logprior: -1.3818e+00
Fitted a model with MAP estimate = -231.7301
expansions: []
discards: [ 0 11 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 237.3033 - loglik: -2.3321e+02 - logprior: -4.0982e+00
Epoch 2/2
19/19 - 1s - loss: 232.9370 - loglik: -2.3140e+02 - logprior: -1.5370e+00
Fitted a model with MAP estimate = -232.2258
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 234.9265 - loglik: -2.3165e+02 - logprior: -3.2743e+00
Epoch 2/10
19/19 - 1s - loss: 232.4409 - loglik: -2.3103e+02 - logprior: -1.4071e+00
Epoch 3/10
19/19 - 1s - loss: 231.9657 - loglik: -2.3066e+02 - logprior: -1.3026e+00
Epoch 4/10
19/19 - 1s - loss: 231.7997 - loglik: -2.3055e+02 - logprior: -1.2539e+00
Epoch 5/10
19/19 - 1s - loss: 231.6630 - loglik: -2.3044e+02 - logprior: -1.2196e+00
Epoch 6/10
19/19 - 1s - loss: 231.6138 - loglik: -2.3041e+02 - logprior: -1.2000e+00
Epoch 7/10
19/19 - 1s - loss: 231.7829 - loglik: -2.3059e+02 - logprior: -1.1919e+00
Fitted a model with MAP estimate = -231.4265
Time for alignment: 35.3981
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 273.2195 - loglik: -2.7000e+02 - logprior: -3.2208e+00
Epoch 2/10
19/19 - 1s - loss: 249.2224 - loglik: -2.4783e+02 - logprior: -1.3927e+00
Epoch 3/10
19/19 - 1s - loss: 240.7991 - loglik: -2.3938e+02 - logprior: -1.4158e+00
Epoch 4/10
19/19 - 1s - loss: 237.9750 - loglik: -2.3668e+02 - logprior: -1.2912e+00
Epoch 5/10
19/19 - 1s - loss: 237.3341 - loglik: -2.3607e+02 - logprior: -1.2604e+00
Epoch 6/10
19/19 - 1s - loss: 237.0872 - loglik: -2.3584e+02 - logprior: -1.2490e+00
Epoch 7/10
19/19 - 1s - loss: 237.3112 - loglik: -2.3608e+02 - logprior: -1.2347e+00
Fitted a model with MAP estimate = -236.8613
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (18, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 239.9409 - loglik: -2.3568e+02 - logprior: -4.2641e+00
Epoch 2/2
19/19 - 1s - loss: 232.7979 - loglik: -2.3142e+02 - logprior: -1.3740e+00
Fitted a model with MAP estimate = -231.7229
expansions: []
discards: [ 0 11 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 237.2928 - loglik: -2.3320e+02 - logprior: -4.0964e+00
Epoch 2/2
19/19 - 1s - loss: 232.9658 - loglik: -2.3143e+02 - logprior: -1.5384e+00
Fitted a model with MAP estimate = -232.1946
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 234.9271 - loglik: -2.3165e+02 - logprior: -3.2730e+00
Epoch 2/10
19/19 - 1s - loss: 232.3881 - loglik: -2.3098e+02 - logprior: -1.4094e+00
Epoch 3/10
19/19 - 1s - loss: 232.1335 - loglik: -2.3083e+02 - logprior: -1.3021e+00
Epoch 4/10
19/19 - 1s - loss: 231.7716 - loglik: -2.3052e+02 - logprior: -1.2535e+00
Epoch 5/10
19/19 - 1s - loss: 231.6122 - loglik: -2.3040e+02 - logprior: -1.2154e+00
Epoch 6/10
19/19 - 1s - loss: 231.6120 - loglik: -2.3041e+02 - logprior: -1.2031e+00
Epoch 7/10
19/19 - 1s - loss: 231.5330 - loglik: -2.3035e+02 - logprior: -1.1861e+00
Epoch 8/10
19/19 - 1s - loss: 231.2630 - loglik: -2.3009e+02 - logprior: -1.1732e+00
Epoch 9/10
19/19 - 1s - loss: 231.5824 - loglik: -2.3042e+02 - logprior: -1.1600e+00
Fitted a model with MAP estimate = -231.3735
Time for alignment: 35.8519
Fitting a model of length 37 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 273.1130 - loglik: -2.6989e+02 - logprior: -3.2256e+00
Epoch 2/10
19/19 - 1s - loss: 247.5079 - loglik: -2.4609e+02 - logprior: -1.4182e+00
Epoch 3/10
19/19 - 1s - loss: 239.7184 - loglik: -2.3839e+02 - logprior: -1.3305e+00
Epoch 4/10
19/19 - 1s - loss: 237.8833 - loglik: -2.3661e+02 - logprior: -1.2775e+00
Epoch 5/10
19/19 - 1s - loss: 237.7328 - loglik: -2.3648e+02 - logprior: -1.2504e+00
Epoch 6/10
19/19 - 1s - loss: 237.3271 - loglik: -2.3609e+02 - logprior: -1.2346e+00
Epoch 7/10
19/19 - 1s - loss: 237.3761 - loglik: -2.3615e+02 - logprior: -1.2222e+00
Fitted a model with MAP estimate = -237.1022
expansions: [(0, 2), (7, 2), (8, 1), (13, 1), (21, 1), (22, 2), (27, 1), (28, 2), (29, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 51 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 239.9920 - loglik: -2.3574e+02 - logprior: -4.2520e+00
Epoch 2/2
19/19 - 1s - loss: 232.8440 - loglik: -2.3146e+02 - logprior: -1.3843e+00
Fitted a model with MAP estimate = -231.7405
expansions: []
discards: [ 0 10 29 39 41]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 237.2094 - loglik: -2.3310e+02 - logprior: -4.1065e+00
Epoch 2/2
19/19 - 1s - loss: 233.0390 - loglik: -2.3150e+02 - logprior: -1.5405e+00
Fitted a model with MAP estimate = -232.2178
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10020 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 234.9438 - loglik: -2.3167e+02 - logprior: -3.2738e+00
Epoch 2/10
19/19 - 1s - loss: 232.4470 - loglik: -2.3104e+02 - logprior: -1.4034e+00
Epoch 3/10
19/19 - 1s - loss: 232.0415 - loglik: -2.3074e+02 - logprior: -1.3006e+00
Epoch 4/10
19/19 - 1s - loss: 231.8111 - loglik: -2.3056e+02 - logprior: -1.2474e+00
Epoch 5/10
19/19 - 1s - loss: 231.6590 - loglik: -2.3044e+02 - logprior: -1.2155e+00
Epoch 6/10
19/19 - 1s - loss: 231.6201 - loglik: -2.3042e+02 - logprior: -1.1972e+00
Epoch 7/10
19/19 - 1s - loss: 231.2802 - loglik: -2.3009e+02 - logprior: -1.1859e+00
Epoch 8/10
19/19 - 1s - loss: 231.4636 - loglik: -2.3030e+02 - logprior: -1.1685e+00
Fitted a model with MAP estimate = -231.3937
Time for alignment: 34.9938
Computed alignments with likelihoods: ['-231.5411', '-231.6481', '-231.4265', '-231.3735', '-231.3937']
Best model has likelihood: -231.3735  (prior= -1.1593 )
time for generating output: 0.0949
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00018.projection.fasta
SP score = 0.8358635202164437
Training of 5 independent models on file PF00687.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1034.1945 - loglik: -1.0324e+03 - logprior: -1.8010e+00
Epoch 2/10
39/39 - 9s - loss: 937.2045 - loglik: -9.3551e+02 - logprior: -1.6901e+00
Epoch 3/10
39/39 - 9s - loss: 927.1762 - loglik: -9.2542e+02 - logprior: -1.7555e+00
Epoch 4/10
39/39 - 9s - loss: 925.0644 - loglik: -9.2335e+02 - logprior: -1.7114e+00
Epoch 5/10
39/39 - 9s - loss: 923.5755 - loglik: -9.2188e+02 - logprior: -1.6998e+00
Epoch 6/10
39/39 - 9s - loss: 922.8055 - loglik: -9.2111e+02 - logprior: -1.6956e+00
Epoch 7/10
39/39 - 9s - loss: 922.0792 - loglik: -9.2039e+02 - logprior: -1.6917e+00
Epoch 8/10
39/39 - 9s - loss: 922.2288 - loglik: -9.2055e+02 - logprior: -1.6812e+00
Fitted a model with MAP estimate = -921.4152
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (39, 1), (40, 1), (41, 1), (42, 1), (51, 1), (58, 1), (60, 1), (63, 1), (66, 1), (72, 1), (74, 1), (97, 2), (100, 1), (102, 1), (103, 1), (108, 1), (110, 1), (112, 1), (113, 1), (116, 1), (123, 1), (129, 2), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 905.7714 - loglik: -9.0402e+02 - logprior: -1.7475e+00
Epoch 2/2
39/39 - 12s - loss: 892.0923 - loglik: -8.9140e+02 - logprior: -6.9400e-01
Fitted a model with MAP estimate = -890.0339
expansions: []
discards: [118]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 895.7070 - loglik: -8.9417e+02 - logprior: -1.5412e+00
Epoch 2/2
39/39 - 11s - loss: 892.0991 - loglik: -8.9162e+02 - logprior: -4.8382e-01
Fitted a model with MAP estimate = -890.0383
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 895.6191 - loglik: -8.9424e+02 - logprior: -1.3821e+00
Epoch 2/10
39/39 - 11s - loss: 891.2333 - loglik: -8.9091e+02 - logprior: -3.2481e-01
Epoch 3/10
39/39 - 11s - loss: 889.7625 - loglik: -8.8955e+02 - logprior: -2.0909e-01
Epoch 4/10
39/39 - 12s - loss: 888.0621 - loglik: -8.8795e+02 - logprior: -1.1400e-01
Epoch 5/10
39/39 - 12s - loss: 886.1343 - loglik: -8.8611e+02 - logprior: -1.9524e-02
Epoch 6/10
39/39 - 12s - loss: 885.7980 - loglik: -8.8587e+02 - logprior: 0.0744
Epoch 7/10
39/39 - 12s - loss: 884.3434 - loglik: -8.8452e+02 - logprior: 0.1744
Epoch 8/10
39/39 - 12s - loss: 884.7203 - loglik: -8.8499e+02 - logprior: 0.2744
Fitted a model with MAP estimate = -884.2366
Time for alignment: 284.3016
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1034.3032 - loglik: -1.0325e+03 - logprior: -1.7915e+00
Epoch 2/10
39/39 - 9s - loss: 936.1994 - loglik: -9.3446e+02 - logprior: -1.7443e+00
Epoch 3/10
39/39 - 9s - loss: 927.8431 - loglik: -9.2606e+02 - logprior: -1.7831e+00
Epoch 4/10
39/39 - 9s - loss: 925.5228 - loglik: -9.2382e+02 - logprior: -1.7074e+00
Epoch 5/10
39/39 - 9s - loss: 924.6933 - loglik: -9.2302e+02 - logprior: -1.6745e+00
Epoch 6/10
39/39 - 9s - loss: 922.8300 - loglik: -9.2115e+02 - logprior: -1.6773e+00
Epoch 7/10
39/39 - 9s - loss: 923.0539 - loglik: -9.2138e+02 - logprior: -1.6760e+00
Fitted a model with MAP estimate = -921.9671
expansions: [(4, 1), (5, 2), (8, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (34, 1), (39, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 2), (66, 2), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 2), (123, 1), (127, 1), (129, 1), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 196 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 906.4562 - loglik: -9.0466e+02 - logprior: -1.7927e+00
Epoch 2/2
39/39 - 13s - loss: 891.7153 - loglik: -8.9094e+02 - logprior: -7.7162e-01
Fitted a model with MAP estimate = -890.0228
expansions: []
discards: [ 80  85 120 171]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 896.8687 - loglik: -8.9527e+02 - logprior: -1.6021e+00
Epoch 2/2
39/39 - 12s - loss: 892.6624 - loglik: -8.9211e+02 - logprior: -5.5447e-01
Fitted a model with MAP estimate = -890.7349
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 895.8763 - loglik: -8.9443e+02 - logprior: -1.4457e+00
Epoch 2/10
39/39 - 12s - loss: 891.8107 - loglik: -8.9143e+02 - logprior: -3.7610e-01
Epoch 3/10
39/39 - 12s - loss: 889.9514 - loglik: -8.8970e+02 - logprior: -2.4963e-01
Epoch 4/10
39/39 - 12s - loss: 888.4315 - loglik: -8.8828e+02 - logprior: -1.5634e-01
Epoch 5/10
39/39 - 12s - loss: 886.8788 - loglik: -8.8685e+02 - logprior: -3.0628e-02
Epoch 6/10
39/39 - 12s - loss: 886.3656 - loglik: -8.8644e+02 - logprior: 0.0780
Epoch 7/10
39/39 - 12s - loss: 885.9285 - loglik: -8.8611e+02 - logprior: 0.1813
Epoch 8/10
39/39 - 13s - loss: 884.6675 - loglik: -8.8495e+02 - logprior: 0.2794
Epoch 9/10
39/39 - 12s - loss: 885.2103 - loglik: -8.8559e+02 - logprior: 0.3811
Fitted a model with MAP estimate = -884.4162
Time for alignment: 294.9022
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 1034.6354 - loglik: -1.0328e+03 - logprior: -1.8091e+00
Epoch 2/10
39/39 - 9s - loss: 935.7833 - loglik: -9.3404e+02 - logprior: -1.7386e+00
Epoch 3/10
39/39 - 9s - loss: 928.2814 - loglik: -9.2653e+02 - logprior: -1.7488e+00
Epoch 4/10
39/39 - 9s - loss: 925.8806 - loglik: -9.2420e+02 - logprior: -1.6760e+00
Epoch 5/10
39/39 - 9s - loss: 924.0190 - loglik: -9.2237e+02 - logprior: -1.6516e+00
Epoch 6/10
39/39 - 9s - loss: 923.8234 - loglik: -9.2218e+02 - logprior: -1.6412e+00
Epoch 7/10
39/39 - 9s - loss: 923.2805 - loglik: -9.2164e+02 - logprior: -1.6422e+00
Epoch 8/10
39/39 - 9s - loss: 922.3541 - loglik: -9.2072e+02 - logprior: -1.6333e+00
Epoch 9/10
39/39 - 9s - loss: 922.7736 - loglik: -9.2115e+02 - logprior: -1.6230e+00
Fitted a model with MAP estimate = -921.9219
expansions: [(4, 1), (5, 2), (9, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (40, 2), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 1), (70, 1), (75, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (123, 1), (127, 1), (129, 2), (130, 1), (134, 2), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 906.1074 - loglik: -9.0432e+02 - logprior: -1.7924e+00
Epoch 2/2
39/39 - 13s - loss: 892.2658 - loglik: -8.9154e+02 - logprior: -7.2935e-01
Fitted a model with MAP estimate = -890.4435
expansions: []
discards: [118 169]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 896.4345 - loglik: -8.9485e+02 - logprior: -1.5865e+00
Epoch 2/2
39/39 - 13s - loss: 892.6092 - loglik: -8.9208e+02 - logprior: -5.2739e-01
Fitted a model with MAP estimate = -890.4186
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 895.7242 - loglik: -8.9430e+02 - logprior: -1.4257e+00
Epoch 2/10
39/39 - 12s - loss: 891.7320 - loglik: -8.9137e+02 - logprior: -3.6086e-01
Epoch 3/10
39/39 - 12s - loss: 890.1689 - loglik: -8.8993e+02 - logprior: -2.3672e-01
Epoch 4/10
39/39 - 12s - loss: 887.8811 - loglik: -8.8775e+02 - logprior: -1.3357e-01
Epoch 5/10
39/39 - 12s - loss: 886.6153 - loglik: -8.8660e+02 - logprior: -1.4636e-02
Epoch 6/10
39/39 - 13s - loss: 886.1650 - loglik: -8.8627e+02 - logprior: 0.1023
Epoch 7/10
39/39 - 13s - loss: 885.3480 - loglik: -8.8556e+02 - logprior: 0.2079
Epoch 8/10
39/39 - 13s - loss: 885.5772 - loglik: -8.8589e+02 - logprior: 0.3170
Fitted a model with MAP estimate = -884.2006
Time for alignment: 302.9179
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1034.2201 - loglik: -1.0324e+03 - logprior: -1.8061e+00
Epoch 2/10
39/39 - 9s - loss: 936.6091 - loglik: -9.3493e+02 - logprior: -1.6818e+00
Epoch 3/10
39/39 - 9s - loss: 927.6238 - loglik: -9.2589e+02 - logprior: -1.7369e+00
Epoch 4/10
39/39 - 9s - loss: 925.4805 - loglik: -9.2381e+02 - logprior: -1.6690e+00
Epoch 5/10
39/39 - 9s - loss: 924.1992 - loglik: -9.2254e+02 - logprior: -1.6628e+00
Epoch 6/10
39/39 - 9s - loss: 923.3602 - loglik: -9.2170e+02 - logprior: -1.6554e+00
Epoch 7/10
39/39 - 9s - loss: 923.2773 - loglik: -9.2163e+02 - logprior: -1.6481e+00
Epoch 8/10
39/39 - 9s - loss: 923.0411 - loglik: -9.2140e+02 - logprior: -1.6402e+00
Epoch 9/10
39/39 - 9s - loss: 921.9857 - loglik: -9.2033e+02 - logprior: -1.6511e+00
Epoch 10/10
39/39 - 9s - loss: 922.8337 - loglik: -9.2119e+02 - logprior: -1.6390e+00
Fitted a model with MAP estimate = -921.6731
expansions: [(4, 1), (5, 2), (12, 2), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (37, 1), (40, 1), (41, 1), (42, 1), (58, 1), (59, 1), (61, 1), (63, 1), (66, 1), (74, 1), (98, 2), (99, 1), (100, 1), (102, 1), (103, 1), (111, 1), (113, 1), (114, 1), (116, 1), (120, 1), (123, 1), (127, 1), (129, 1), (130, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 193 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 906.7756 - loglik: -9.0501e+02 - logprior: -1.7685e+00
Epoch 2/2
39/39 - 12s - loss: 893.6627 - loglik: -8.9299e+02 - logprior: -6.6884e-01
Fitted a model with MAP estimate = -891.5433
expansions: []
discards: [118]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 897.1674 - loglik: -8.9563e+02 - logprior: -1.5340e+00
Epoch 2/2
39/39 - 12s - loss: 893.4022 - loglik: -8.9294e+02 - logprior: -4.6618e-01
Fitted a model with MAP estimate = -891.3100
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 896.6298 - loglik: -8.9526e+02 - logprior: -1.3675e+00
Epoch 2/10
39/39 - 12s - loss: 892.8497 - loglik: -8.9253e+02 - logprior: -3.1862e-01
Epoch 3/10
39/39 - 12s - loss: 890.9064 - loglik: -8.9071e+02 - logprior: -2.0119e-01
Epoch 4/10
39/39 - 12s - loss: 889.0422 - loglik: -8.8895e+02 - logprior: -8.9967e-02
Epoch 5/10
39/39 - 13s - loss: 887.3730 - loglik: -8.8738e+02 - logprior: 0.0099
Epoch 6/10
39/39 - 13s - loss: 887.0195 - loglik: -8.8714e+02 - logprior: 0.1178
Epoch 7/10
39/39 - 13s - loss: 885.3405 - loglik: -8.8557e+02 - logprior: 0.2325
Epoch 8/10
39/39 - 13s - loss: 885.2448 - loglik: -8.8559e+02 - logprior: 0.3471
Epoch 9/10
39/39 - 13s - loss: 884.6865 - loglik: -8.8520e+02 - logprior: 0.5145
Epoch 10/10
39/39 - 13s - loss: 884.3559 - loglik: -8.8498e+02 - logprior: 0.6225
Fitted a model with MAP estimate = -883.7449
Time for alignment: 335.8290
Fitting a model of length 153 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 1033.0901 - loglik: -1.0313e+03 - logprior: -1.7921e+00
Epoch 2/10
39/39 - 9s - loss: 932.2789 - loglik: -9.3056e+02 - logprior: -1.7182e+00
Epoch 3/10
39/39 - 9s - loss: 924.5931 - loglik: -9.2283e+02 - logprior: -1.7663e+00
Epoch 4/10
39/39 - 9s - loss: 921.5286 - loglik: -9.1983e+02 - logprior: -1.7022e+00
Epoch 5/10
39/39 - 9s - loss: 920.5488 - loglik: -9.1887e+02 - logprior: -1.6776e+00
Epoch 6/10
39/39 - 9s - loss: 919.9987 - loglik: -9.1832e+02 - logprior: -1.6756e+00
Epoch 7/10
39/39 - 9s - loss: 919.4171 - loglik: -9.1774e+02 - logprior: -1.6769e+00
Epoch 8/10
39/39 - 9s - loss: 918.8372 - loglik: -9.1716e+02 - logprior: -1.6739e+00
Epoch 9/10
39/39 - 10s - loss: 918.9414 - loglik: -9.1727e+02 - logprior: -1.6699e+00
Fitted a model with MAP estimate = -918.4478
expansions: [(4, 1), (5, 2), (7, 1), (12, 1), (13, 1), (20, 1), (26, 1), (27, 1), (35, 1), (41, 1), (42, 1), (43, 1), (52, 1), (58, 1), (59, 1), (61, 1), (63, 2), (67, 1), (75, 1), (98, 1), (100, 3), (102, 1), (107, 1), (111, 1), (113, 1), (114, 1), (116, 1), (123, 1), (127, 1), (130, 1), (131, 1), (133, 1), (134, 1), (143, 1), (144, 1), (145, 1), (146, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 194 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 905.0402 - loglik: -9.0328e+02 - logprior: -1.7585e+00
Epoch 2/2
39/39 - 14s - loss: 891.8427 - loglik: -8.9113e+02 - logprior: -7.0896e-01
Fitted a model with MAP estimate = -889.8767
expansions: []
discards: [ 80 123]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 895.7038 - loglik: -8.9415e+02 - logprior: -1.5514e+00
Epoch 2/2
39/39 - 14s - loss: 891.6174 - loglik: -8.9114e+02 - logprior: -4.7964e-01
Fitted a model with MAP estimate = -889.5013
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 192 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 894.5775 - loglik: -8.9318e+02 - logprior: -1.3952e+00
Epoch 2/10
39/39 - 15s - loss: 890.9634 - loglik: -8.9066e+02 - logprior: -3.0566e-01
Epoch 3/10
39/39 - 14s - loss: 889.2439 - loglik: -8.8906e+02 - logprior: -1.8125e-01
Epoch 4/10
39/39 - 14s - loss: 886.6902 - loglik: -8.8660e+02 - logprior: -8.9468e-02
Epoch 5/10
39/39 - 14s - loss: 886.7723 - loglik: -8.8677e+02 - logprior: -5.3698e-03
Fitted a model with MAP estimate = -885.0837
Time for alignment: 284.8702
Computed alignments with likelihoods: ['-884.2366', '-884.4162', '-884.2006', '-883.7449', '-885.0837']
Best model has likelihood: -883.7449  (prior= 0.5859 )
time for generating output: 0.1765
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00687.projection.fasta
SP score = 0.7473415567843471
Training of 5 independent models on file PF03129.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 522.8328 - loglik: -5.1973e+02 - logprior: -3.0985e+00
Epoch 2/10
19/19 - 2s - loss: 491.7130 - loglik: -4.9044e+02 - logprior: -1.2701e+00
Epoch 3/10
19/19 - 2s - loss: 477.3852 - loglik: -4.7604e+02 - logprior: -1.3485e+00
Epoch 4/10
19/19 - 2s - loss: 472.6527 - loglik: -4.7137e+02 - logprior: -1.2859e+00
Epoch 5/10
19/19 - 2s - loss: 469.7462 - loglik: -4.6856e+02 - logprior: -1.1816e+00
Epoch 6/10
19/19 - 2s - loss: 469.4853 - loglik: -4.6834e+02 - logprior: -1.1426e+00
Epoch 7/10
19/19 - 2s - loss: 469.2541 - loglik: -4.6813e+02 - logprior: -1.1218e+00
Epoch 8/10
19/19 - 2s - loss: 468.4297 - loglik: -4.6731e+02 - logprior: -1.1159e+00
Epoch 9/10
19/19 - 2s - loss: 468.8722 - loglik: -4.6776e+02 - logprior: -1.1126e+00
Fitted a model with MAP estimate = -467.7403
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (31, 1), (37, 2), (39, 1), (43, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 472.2511 - loglik: -4.6831e+02 - logprior: -3.9385e+00
Epoch 2/2
19/19 - 2s - loss: 462.5527 - loglik: -4.6072e+02 - logprior: -1.8278e+00
Fitted a model with MAP estimate = -460.1789
expansions: [(0, 2)]
discards: [ 0  7 45]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 462.3725 - loglik: -4.5954e+02 - logprior: -2.8343e+00
Epoch 2/2
19/19 - 2s - loss: 458.5430 - loglik: -4.5753e+02 - logprior: -1.0173e+00
Fitted a model with MAP estimate = -456.9468
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 462.3689 - loglik: -4.5896e+02 - logprior: -3.4052e+00
Epoch 2/10
19/19 - 2s - loss: 458.0595 - loglik: -4.5696e+02 - logprior: -1.0957e+00
Epoch 3/10
19/19 - 2s - loss: 457.0160 - loglik: -4.5607e+02 - logprior: -9.4292e-01
Epoch 4/10
19/19 - 2s - loss: 456.6218 - loglik: -4.5572e+02 - logprior: -9.0153e-01
Epoch 5/10
19/19 - 2s - loss: 455.8549 - loglik: -4.5498e+02 - logprior: -8.7817e-01
Epoch 6/10
19/19 - 2s - loss: 455.5799 - loglik: -4.5472e+02 - logprior: -8.5560e-01
Epoch 7/10
19/19 - 2s - loss: 455.7494 - loglik: -4.5492e+02 - logprior: -8.3338e-01
Fitted a model with MAP estimate = -455.3153
Time for alignment: 64.7564
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 522.7778 - loglik: -5.1968e+02 - logprior: -3.0964e+00
Epoch 2/10
19/19 - 2s - loss: 491.4453 - loglik: -4.9019e+02 - logprior: -1.2599e+00
Epoch 3/10
19/19 - 2s - loss: 477.7151 - loglik: -4.7640e+02 - logprior: -1.3102e+00
Epoch 4/10
19/19 - 2s - loss: 472.8667 - loglik: -4.7169e+02 - logprior: -1.1796e+00
Epoch 5/10
19/19 - 2s - loss: 469.8591 - loglik: -4.6873e+02 - logprior: -1.1337e+00
Epoch 6/10
19/19 - 2s - loss: 467.7318 - loglik: -4.6660e+02 - logprior: -1.1276e+00
Epoch 7/10
19/19 - 2s - loss: 467.9951 - loglik: -4.6687e+02 - logprior: -1.1254e+00
Fitted a model with MAP estimate = -466.5605
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (22, 1), (29, 1), (37, 2), (38, 1), (43, 1), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 471.0707 - loglik: -4.6713e+02 - logprior: -3.9360e+00
Epoch 2/2
19/19 - 2s - loss: 461.4343 - loglik: -4.5965e+02 - logprior: -1.7837e+00
Fitted a model with MAP estimate = -458.8997
expansions: [(0, 2)]
discards: [ 0  7 45]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 461.6098 - loglik: -4.5878e+02 - logprior: -2.8340e+00
Epoch 2/2
19/19 - 2s - loss: 458.3419 - loglik: -4.5732e+02 - logprior: -1.0207e+00
Fitted a model with MAP estimate = -456.9146
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 462.1752 - loglik: -4.5877e+02 - logprior: -3.4057e+00
Epoch 2/10
19/19 - 2s - loss: 458.0823 - loglik: -4.5697e+02 - logprior: -1.1112e+00
Epoch 3/10
19/19 - 2s - loss: 457.3770 - loglik: -4.5643e+02 - logprior: -9.4367e-01
Epoch 4/10
19/19 - 2s - loss: 456.4839 - loglik: -4.5558e+02 - logprior: -9.0761e-01
Epoch 5/10
19/19 - 2s - loss: 456.0030 - loglik: -4.5512e+02 - logprior: -8.8296e-01
Epoch 6/10
19/19 - 2s - loss: 455.8995 - loglik: -4.5504e+02 - logprior: -8.6326e-01
Epoch 7/10
19/19 - 2s - loss: 455.4870 - loglik: -4.5465e+02 - logprior: -8.3800e-01
Epoch 8/10
19/19 - 2s - loss: 455.6990 - loglik: -4.5487e+02 - logprior: -8.2727e-01
Fitted a model with MAP estimate = -455.2547
Time for alignment: 62.7852
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 522.4824 - loglik: -5.1938e+02 - logprior: -3.0983e+00
Epoch 2/10
19/19 - 2s - loss: 491.1031 - loglik: -4.8983e+02 - logprior: -1.2709e+00
Epoch 3/10
19/19 - 2s - loss: 477.4202 - loglik: -4.7606e+02 - logprior: -1.3599e+00
Epoch 4/10
19/19 - 2s - loss: 471.7281 - loglik: -4.7036e+02 - logprior: -1.3672e+00
Epoch 5/10
19/19 - 2s - loss: 470.5003 - loglik: -4.6905e+02 - logprior: -1.4552e+00
Epoch 6/10
19/19 - 2s - loss: 470.0497 - loglik: -4.6863e+02 - logprior: -1.4204e+00
Epoch 7/10
19/19 - 2s - loss: 469.7307 - loglik: -4.6831e+02 - logprior: -1.4239e+00
Epoch 8/10
19/19 - 2s - loss: 469.5372 - loglik: -4.6813e+02 - logprior: -1.4121e+00
Epoch 9/10
19/19 - 2s - loss: 469.4652 - loglik: -4.6805e+02 - logprior: -1.4113e+00
Epoch 10/10
19/19 - 2s - loss: 469.4842 - loglik: -4.6808e+02 - logprior: -1.4034e+00
Fitted a model with MAP estimate = -468.4391
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (24, 1), (30, 1), (36, 1), (37, 2), (38, 1), (50, 1), (55, 1), (57, 1), (64, 2), (68, 1), (69, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 472.0311 - loglik: -4.6807e+02 - logprior: -3.9584e+00
Epoch 2/2
19/19 - 2s - loss: 461.6738 - loglik: -4.5982e+02 - logprior: -1.8515e+00
Fitted a model with MAP estimate = -459.1322
expansions: [(0, 2)]
discards: [ 0  7 46 79]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 461.5642 - loglik: -4.5874e+02 - logprior: -2.8255e+00
Epoch 2/2
19/19 - 2s - loss: 458.5388 - loglik: -4.5752e+02 - logprior: -1.0199e+00
Fitted a model with MAP estimate = -456.9091
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 462.2361 - loglik: -4.5884e+02 - logprior: -3.3940e+00
Epoch 2/10
19/19 - 2s - loss: 458.2220 - loglik: -4.5712e+02 - logprior: -1.1012e+00
Epoch 3/10
19/19 - 2s - loss: 457.0996 - loglik: -4.5615e+02 - logprior: -9.5036e-01
Epoch 4/10
19/19 - 2s - loss: 456.5345 - loglik: -4.5563e+02 - logprior: -9.0556e-01
Epoch 5/10
19/19 - 2s - loss: 456.2304 - loglik: -4.5536e+02 - logprior: -8.7213e-01
Epoch 6/10
19/19 - 2s - loss: 455.8376 - loglik: -4.5498e+02 - logprior: -8.6117e-01
Epoch 7/10
19/19 - 2s - loss: 455.1726 - loglik: -4.5433e+02 - logprior: -8.4635e-01
Epoch 8/10
19/19 - 2s - loss: 455.5553 - loglik: -4.5474e+02 - logprior: -8.1780e-01
Fitted a model with MAP estimate = -455.2704
Time for alignment: 67.0910
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 522.4929 - loglik: -5.1939e+02 - logprior: -3.1022e+00
Epoch 2/10
19/19 - 2s - loss: 490.7349 - loglik: -4.8947e+02 - logprior: -1.2651e+00
Epoch 3/10
19/19 - 2s - loss: 477.3636 - loglik: -4.7607e+02 - logprior: -1.2962e+00
Epoch 4/10
19/19 - 2s - loss: 472.9688 - loglik: -4.7180e+02 - logprior: -1.1663e+00
Epoch 5/10
19/19 - 2s - loss: 471.2719 - loglik: -4.7017e+02 - logprior: -1.0998e+00
Epoch 6/10
19/19 - 2s - loss: 470.1206 - loglik: -4.6904e+02 - logprior: -1.0757e+00
Epoch 7/10
19/19 - 2s - loss: 470.0557 - loglik: -4.6898e+02 - logprior: -1.0741e+00
Epoch 8/10
19/19 - 2s - loss: 469.7539 - loglik: -4.6868e+02 - logprior: -1.0703e+00
Epoch 9/10
19/19 - 2s - loss: 469.7679 - loglik: -4.6870e+02 - logprior: -1.0665e+00
Fitted a model with MAP estimate = -468.5868
expansions: [(7, 2), (9, 1), (10, 1), (11, 1), (12, 1), (15, 1), (22, 1), (31, 1), (37, 2), (38, 2), (50, 1), (55, 1), (61, 2), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 472.0801 - loglik: -4.6815e+02 - logprior: -3.9326e+00
Epoch 2/2
19/19 - 2s - loss: 462.0421 - loglik: -4.6019e+02 - logprior: -1.8491e+00
Fitted a model with MAP estimate = -459.1864
expansions: [(0, 2)]
discards: [ 0  7 45 76]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 461.9854 - loglik: -4.5915e+02 - logprior: -2.8340e+00
Epoch 2/2
19/19 - 2s - loss: 458.3000 - loglik: -4.5728e+02 - logprior: -1.0211e+00
Fitted a model with MAP estimate = -456.8929
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 462.2797 - loglik: -4.5889e+02 - logprior: -3.3931e+00
Epoch 2/10
19/19 - 2s - loss: 457.8284 - loglik: -4.5673e+02 - logprior: -1.0972e+00
Epoch 3/10
19/19 - 2s - loss: 457.1837 - loglik: -4.5623e+02 - logprior: -9.5487e-01
Epoch 4/10
19/19 - 2s - loss: 456.5377 - loglik: -4.5564e+02 - logprior: -9.0257e-01
Epoch 5/10
19/19 - 2s - loss: 456.1979 - loglik: -4.5533e+02 - logprior: -8.7216e-01
Epoch 6/10
19/19 - 2s - loss: 455.9186 - loglik: -4.5505e+02 - logprior: -8.6605e-01
Epoch 7/10
19/19 - 2s - loss: 455.3811 - loglik: -4.5453e+02 - logprior: -8.5055e-01
Epoch 8/10
19/19 - 2s - loss: 455.2367 - loglik: -4.5441e+02 - logprior: -8.2294e-01
Epoch 9/10
19/19 - 2s - loss: 455.7990 - loglik: -4.5499e+02 - logprior: -8.0824e-01
Fitted a model with MAP estimate = -455.1916
Time for alignment: 69.0911
Fitting a model of length 73 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 522.3942 - loglik: -5.1930e+02 - logprior: -3.0961e+00
Epoch 2/10
19/19 - 2s - loss: 491.8295 - loglik: -4.9057e+02 - logprior: -1.2566e+00
Epoch 3/10
19/19 - 2s - loss: 476.0725 - loglik: -4.7477e+02 - logprior: -1.3059e+00
Epoch 4/10
19/19 - 2s - loss: 471.3785 - loglik: -4.7025e+02 - logprior: -1.1318e+00
Epoch 5/10
19/19 - 2s - loss: 470.0975 - loglik: -4.6901e+02 - logprior: -1.0894e+00
Epoch 6/10
19/19 - 2s - loss: 469.0360 - loglik: -4.6799e+02 - logprior: -1.0508e+00
Epoch 7/10
19/19 - 2s - loss: 469.2560 - loglik: -4.6821e+02 - logprior: -1.0411e+00
Fitted a model with MAP estimate = -468.1424
expansions: [(7, 2), (8, 1), (9, 4), (12, 1), (15, 1), (22, 1), (29, 1), (30, 2), (38, 2), (50, 1), (55, 1), (57, 1), (64, 1), (72, 1), (73, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 470.5823 - loglik: -4.6665e+02 - logprior: -3.9307e+00
Epoch 2/2
19/19 - 2s - loss: 461.4781 - loglik: -4.5967e+02 - logprior: -1.8074e+00
Fitted a model with MAP estimate = -458.8477
expansions: [(0, 2)]
discards: [ 0  7  9 12 40]
Re-initialized the encoder parameters.
Fitting a model of length 92 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 461.7760 - loglik: -4.5894e+02 - logprior: -2.8368e+00
Epoch 2/2
19/19 - 2s - loss: 458.2867 - loglik: -4.5726e+02 - logprior: -1.0253e+00
Fitted a model with MAP estimate = -456.9123
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 91 on 10055 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 462.2105 - loglik: -4.5881e+02 - logprior: -3.4049e+00
Epoch 2/10
19/19 - 2s - loss: 458.0567 - loglik: -4.5695e+02 - logprior: -1.1056e+00
Epoch 3/10
19/19 - 2s - loss: 457.4391 - loglik: -4.5649e+02 - logprior: -9.5149e-01
Epoch 4/10
19/19 - 2s - loss: 456.2622 - loglik: -4.5535e+02 - logprior: -9.1530e-01
Epoch 5/10
19/19 - 2s - loss: 456.1744 - loglik: -4.5528e+02 - logprior: -8.8963e-01
Epoch 6/10
19/19 - 2s - loss: 455.8057 - loglik: -4.5494e+02 - logprior: -8.6410e-01
Epoch 7/10
19/19 - 2s - loss: 455.4867 - loglik: -4.5464e+02 - logprior: -8.4773e-01
Epoch 8/10
19/19 - 2s - loss: 455.1091 - loglik: -4.5428e+02 - logprior: -8.2615e-01
Epoch 9/10
19/19 - 2s - loss: 455.5215 - loglik: -4.5471e+02 - logprior: -8.0840e-01
Fitted a model with MAP estimate = -455.2219
Time for alignment: 65.7907
Computed alignments with likelihoods: ['-455.3153', '-455.2547', '-455.2704', '-455.1916', '-455.2219']
Best model has likelihood: -455.1916  (prior= -0.7911 )
time for generating output: 0.1398
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF03129.projection.fasta
SP score = 0.9740502881169962
Training of 5 independent models on file PF13378.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1192.3585 - loglik: -1.1906e+03 - logprior: -1.7215e+00
Epoch 2/10
39/39 - 12s - loss: 1115.4943 - loglik: -1.1143e+03 - logprior: -1.2018e+00
Epoch 3/10
39/39 - 12s - loss: 1106.4924 - loglik: -1.1053e+03 - logprior: -1.1454e+00
Epoch 4/10
39/39 - 12s - loss: 1104.2548 - loglik: -1.1032e+03 - logprior: -1.1047e+00
Epoch 5/10
39/39 - 12s - loss: 1103.3810 - loglik: -1.1023e+03 - logprior: -1.0933e+00
Epoch 6/10
39/39 - 12s - loss: 1103.0868 - loglik: -1.1020e+03 - logprior: -1.1058e+00
Epoch 7/10
39/39 - 12s - loss: 1103.0491 - loglik: -1.1019e+03 - logprior: -1.1165e+00
Epoch 8/10
39/39 - 12s - loss: 1103.3584 - loglik: -1.1022e+03 - logprior: -1.1189e+00
Fitted a model with MAP estimate = -1097.0720
expansions: [(0, 3), (11, 1), (20, 2), (21, 1), (31, 1), (36, 1), (45, 2), (46, 1), (49, 1), (51, 1), (56, 1), (68, 1), (72, 1), (75, 1), (84, 1), (85, 1), (86, 3), (87, 2), (88, 2), (102, 1), (103, 1), (108, 1), (115, 1), (116, 1), (124, 3), (135, 1), (141, 4), (149, 2), (150, 2), (151, 2), (153, 1), (159, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 222 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 21s - loss: 1095.2203 - loglik: -1.0927e+03 - logprior: -2.5645e+00
Epoch 2/2
39/39 - 18s - loss: 1082.7551 - loglik: -1.0820e+03 - logprior: -7.2391e-01
Fitted a model with MAP estimate = -1075.1808
expansions: []
discards: [ 24  55 107 111 113 179 180 189]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1085.6554 - loglik: -1.0839e+03 - logprior: -1.7214e+00
Epoch 2/2
39/39 - 17s - loss: 1082.1890 - loglik: -1.0817e+03 - logprior: -4.7783e-01
Fitted a model with MAP estimate = -1075.0687
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1079.2576 - loglik: -1.0777e+03 - logprior: -1.5886e+00
Epoch 2/10
39/39 - 16s - loss: 1076.6686 - loglik: -1.0763e+03 - logprior: -3.3922e-01
Epoch 3/10
39/39 - 16s - loss: 1074.4894 - loglik: -1.0743e+03 - logprior: -2.2038e-01
Epoch 4/10
39/39 - 16s - loss: 1072.7562 - loglik: -1.0726e+03 - logprior: -1.6547e-01
Epoch 5/10
39/39 - 16s - loss: 1072.9514 - loglik: -1.0728e+03 - logprior: -1.1870e-01
Fitted a model with MAP estimate = -1071.7037
Time for alignment: 336.6595
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1191.9929 - loglik: -1.1903e+03 - logprior: -1.7218e+00
Epoch 2/10
39/39 - 11s - loss: 1114.1794 - loglik: -1.1129e+03 - logprior: -1.2325e+00
Epoch 3/10
39/39 - 11s - loss: 1104.9500 - loglik: -1.1037e+03 - logprior: -1.2419e+00
Epoch 4/10
39/39 - 11s - loss: 1102.9344 - loglik: -1.1017e+03 - logprior: -1.2279e+00
Epoch 5/10
39/39 - 11s - loss: 1102.0176 - loglik: -1.1008e+03 - logprior: -1.2270e+00
Epoch 6/10
39/39 - 11s - loss: 1101.7355 - loglik: -1.1005e+03 - logprior: -1.2396e+00
Epoch 7/10
39/39 - 11s - loss: 1101.6820 - loglik: -1.1004e+03 - logprior: -1.2532e+00
Epoch 8/10
39/39 - 12s - loss: 1101.6776 - loglik: -1.1004e+03 - logprior: -1.2553e+00
Epoch 9/10
39/39 - 12s - loss: 1101.7202 - loglik: -1.1005e+03 - logprior: -1.2624e+00
Fitted a model with MAP estimate = -1095.6737
expansions: [(0, 3), (12, 1), (20, 2), (21, 1), (26, 1), (36, 1), (45, 2), (46, 1), (51, 1), (52, 1), (65, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (125, 3), (126, 1), (135, 1), (137, 1), (140, 2), (149, 1), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 219 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1095.7648 - loglik: -1.0932e+03 - logprior: -2.5435e+00
Epoch 2/2
39/39 - 18s - loss: 1083.1813 - loglik: -1.0825e+03 - logprior: -6.8370e-01
Fitted a model with MAP estimate = -1075.9571
expansions: [(156, 1)]
discards: [ 55  93 112 157 177]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1085.6182 - loglik: -1.0839e+03 - logprior: -1.6897e+00
Epoch 2/2
39/39 - 18s - loss: 1082.1924 - loglik: -1.0817e+03 - logprior: -4.6238e-01
Fitted a model with MAP estimate = -1075.0775
expansions: [(181, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 216 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1079.3792 - loglik: -1.0778e+03 - logprior: -1.5614e+00
Epoch 2/10
39/39 - 18s - loss: 1075.8837 - loglik: -1.0756e+03 - logprior: -3.2730e-01
Epoch 3/10
39/39 - 18s - loss: 1074.1099 - loglik: -1.0739e+03 - logprior: -2.0705e-01
Epoch 4/10
39/39 - 17s - loss: 1072.5004 - loglik: -1.0723e+03 - logprior: -1.5854e-01
Epoch 5/10
39/39 - 16s - loss: 1072.0680 - loglik: -1.0720e+03 - logprior: -1.0601e-01
Epoch 6/10
39/39 - 17s - loss: 1071.7782 - loglik: -1.0717e+03 - logprior: -5.5871e-02
Epoch 7/10
39/39 - 17s - loss: 1071.0120 - loglik: -1.0710e+03 - logprior: 0.0171
Epoch 8/10
39/39 - 17s - loss: 1071.7709 - loglik: -1.0719e+03 - logprior: 0.0833
Fitted a model with MAP estimate = -1070.8570
Time for alignment: 400.5134
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1190.6931 - loglik: -1.1890e+03 - logprior: -1.7054e+00
Epoch 2/10
39/39 - 11s - loss: 1111.5105 - loglik: -1.1103e+03 - logprior: -1.1863e+00
Epoch 3/10
39/39 - 11s - loss: 1103.5916 - loglik: -1.1024e+03 - logprior: -1.1581e+00
Epoch 4/10
39/39 - 11s - loss: 1101.4772 - loglik: -1.1003e+03 - logprior: -1.1454e+00
Epoch 5/10
39/39 - 11s - loss: 1100.8757 - loglik: -1.0997e+03 - logprior: -1.1439e+00
Epoch 6/10
39/39 - 12s - loss: 1100.2048 - loglik: -1.0991e+03 - logprior: -1.1459e+00
Epoch 7/10
39/39 - 12s - loss: 1100.4802 - loglik: -1.0993e+03 - logprior: -1.1487e+00
Fitted a model with MAP estimate = -1094.5097
expansions: [(0, 3), (19, 2), (20, 2), (21, 1), (26, 1), (36, 1), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (72, 1), (75, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (92, 1), (108, 1), (109, 1), (113, 1), (116, 1), (125, 3), (136, 1), (142, 4), (148, 1), (149, 2), (150, 2), (151, 2), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 221 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1094.1644 - loglik: -1.0917e+03 - logprior: -2.4879e+00
Epoch 2/2
39/39 - 16s - loss: 1082.6185 - loglik: -1.0819e+03 - logprior: -6.7892e-01
Fitted a model with MAP estimate = -1075.1612
expansions: []
discards: [ 23  26  56 112 178 179]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1085.5803 - loglik: -1.0839e+03 - logprior: -1.7234e+00
Epoch 2/2
39/39 - 17s - loss: 1082.3325 - loglik: -1.0818e+03 - logprior: -4.8272e-01
Fitted a model with MAP estimate = -1075.2753
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1079.2559 - loglik: -1.0777e+03 - logprior: -1.5879e+00
Epoch 2/10
39/39 - 19s - loss: 1076.2174 - loglik: -1.0759e+03 - logprior: -3.5663e-01
Epoch 3/10
39/39 - 19s - loss: 1074.5786 - loglik: -1.0743e+03 - logprior: -2.4572e-01
Epoch 4/10
39/39 - 18s - loss: 1073.7620 - loglik: -1.0736e+03 - logprior: -1.7653e-01
Epoch 5/10
39/39 - 18s - loss: 1072.2750 - loglik: -1.0722e+03 - logprior: -1.2115e-01
Epoch 6/10
39/39 - 18s - loss: 1071.8613 - loglik: -1.0718e+03 - logprior: -7.5093e-02
Epoch 7/10
39/39 - 17s - loss: 1071.8165 - loglik: -1.0718e+03 - logprior: -1.6554e-02
Epoch 8/10
39/39 - 18s - loss: 1072.4288 - loglik: -1.0725e+03 - logprior: 0.0556
Fitted a model with MAP estimate = -1071.1822
Time for alignment: 379.2158
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1190.6210 - loglik: -1.1889e+03 - logprior: -1.7320e+00
Epoch 2/10
39/39 - 12s - loss: 1112.5986 - loglik: -1.1113e+03 - logprior: -1.2488e+00
Epoch 3/10
39/39 - 12s - loss: 1103.9762 - loglik: -1.1028e+03 - logprior: -1.1837e+00
Epoch 4/10
39/39 - 13s - loss: 1102.2303 - loglik: -1.1011e+03 - logprior: -1.1697e+00
Epoch 5/10
39/39 - 12s - loss: 1101.1946 - loglik: -1.1000e+03 - logprior: -1.1645e+00
Epoch 6/10
39/39 - 12s - loss: 1100.7151 - loglik: -1.0995e+03 - logprior: -1.1812e+00
Epoch 7/10
39/39 - 12s - loss: 1100.7041 - loglik: -1.0995e+03 - logprior: -1.1749e+00
Epoch 8/10
39/39 - 11s - loss: 1100.7168 - loglik: -1.0995e+03 - logprior: -1.1774e+00
Fitted a model with MAP estimate = -1094.7942
expansions: [(0, 3), (11, 1), (18, 2), (19, 2), (20, 1), (35, 2), (45, 2), (46, 1), (51, 1), (52, 1), (65, 2), (69, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (116, 1), (125, 3), (136, 1), (138, 1), (141, 2), (146, 2), (149, 2), (150, 2), (151, 3), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 225 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1094.6199 - loglik: -1.0920e+03 - logprior: -2.5900e+00
Epoch 2/2
39/39 - 18s - loss: 1082.3282 - loglik: -1.0816e+03 - logprior: -7.7675e-01
Fitted a model with MAP estimate = -1074.8783
expansions: []
discards: [ 25  44  57  82  96 115 180 186 191 198]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1085.5428 - loglik: -1.0838e+03 - logprior: -1.7156e+00
Epoch 2/2
39/39 - 16s - loss: 1081.9320 - loglik: -1.0815e+03 - logprior: -4.7774e-01
Fitted a model with MAP estimate = -1074.8762
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 215 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1079.0946 - loglik: -1.0775e+03 - logprior: -1.5898e+00
Epoch 2/10
39/39 - 16s - loss: 1076.1672 - loglik: -1.0758e+03 - logprior: -3.4564e-01
Epoch 3/10
39/39 - 16s - loss: 1074.3555 - loglik: -1.0741e+03 - logprior: -2.2496e-01
Epoch 4/10
39/39 - 17s - loss: 1073.0896 - loglik: -1.0729e+03 - logprior: -1.5557e-01
Epoch 5/10
39/39 - 16s - loss: 1072.0574 - loglik: -1.0719e+03 - logprior: -1.1593e-01
Epoch 6/10
39/39 - 17s - loss: 1071.5646 - loglik: -1.0715e+03 - logprior: -5.1065e-02
Epoch 7/10
39/39 - 18s - loss: 1071.8530 - loglik: -1.0719e+03 - logprior: 0.0095
Fitted a model with MAP estimate = -1071.1649
Time for alignment: 372.3078
Fitting a model of length 173 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1191.3784 - loglik: -1.1896e+03 - logprior: -1.7293e+00
Epoch 2/10
39/39 - 12s - loss: 1113.5560 - loglik: -1.1123e+03 - logprior: -1.2859e+00
Epoch 3/10
39/39 - 12s - loss: 1104.3899 - loglik: -1.1031e+03 - logprior: -1.2683e+00
Epoch 4/10
39/39 - 13s - loss: 1101.9620 - loglik: -1.1007e+03 - logprior: -1.2536e+00
Epoch 5/10
39/39 - 13s - loss: 1101.2815 - loglik: -1.1000e+03 - logprior: -1.2561e+00
Epoch 6/10
39/39 - 12s - loss: 1100.6442 - loglik: -1.0994e+03 - logprior: -1.2622e+00
Epoch 7/10
39/39 - 13s - loss: 1101.1497 - loglik: -1.0999e+03 - logprior: -1.2629e+00
Fitted a model with MAP estimate = -1094.8556
expansions: [(0, 3), (12, 1), (20, 2), (21, 1), (24, 1), (36, 1), (45, 2), (46, 1), (49, 1), (51, 1), (65, 1), (68, 1), (75, 1), (76, 2), (85, 1), (86, 1), (87, 1), (88, 1), (89, 2), (103, 1), (108, 1), (109, 1), (113, 1), (117, 1), (125, 2), (127, 1), (132, 1), (138, 1), (141, 2), (149, 3), (150, 2), (151, 3), (153, 1), (157, 1), (158, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 221 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 20s - loss: 1094.7019 - loglik: -1.0922e+03 - logprior: -2.5354e+00
Epoch 2/2
39/39 - 19s - loss: 1082.9021 - loglik: -1.0822e+03 - logprior: -7.1681e-01
Fitted a model with MAP estimate = -1075.4743
expansions: []
discards: [ 25  55  93 112 177 190 194]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1085.7714 - loglik: -1.0841e+03 - logprior: -1.7106e+00
Epoch 2/2
39/39 - 16s - loss: 1082.1431 - loglik: -1.0817e+03 - logprior: -4.6987e-01
Fitted a model with MAP estimate = -1075.2428
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 214 on 10089 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 18s - loss: 1079.2330 - loglik: -1.0776e+03 - logprior: -1.5849e+00
Epoch 2/10
39/39 - 16s - loss: 1076.6422 - loglik: -1.0763e+03 - logprior: -3.5412e-01
Epoch 3/10
39/39 - 15s - loss: 1075.1150 - loglik: -1.0749e+03 - logprior: -2.3909e-01
Epoch 4/10
39/39 - 16s - loss: 1072.9276 - loglik: -1.0727e+03 - logprior: -1.8122e-01
Epoch 5/10
39/39 - 16s - loss: 1072.9830 - loglik: -1.0728e+03 - logprior: -1.3372e-01
Fitted a model with MAP estimate = -1071.9925
Time for alignment: 324.4731
Computed alignments with likelihoods: ['-1071.7037', '-1070.8570', '-1071.1822', '-1071.1649', '-1071.9925']
Best model has likelihood: -1070.8570  (prior= 0.1031 )
time for generating output: 0.2991
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13378.projection.fasta
SP score = 0.7261561365962594
Training of 5 independent models on file PF00084.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 337.6492 - loglik: -3.3446e+02 - logprior: -3.1877e+00
Epoch 2/10
19/19 - 1s - loss: 308.9641 - loglik: -3.0757e+02 - logprior: -1.3928e+00
Epoch 3/10
19/19 - 1s - loss: 301.1421 - loglik: -2.9966e+02 - logprior: -1.4848e+00
Epoch 4/10
19/19 - 1s - loss: 299.7407 - loglik: -2.9841e+02 - logprior: -1.3267e+00
Epoch 5/10
19/19 - 1s - loss: 299.1894 - loglik: -2.9786e+02 - logprior: -1.3251e+00
Epoch 6/10
19/19 - 1s - loss: 299.3212 - loglik: -2.9802e+02 - logprior: -1.3042e+00
Fitted a model with MAP estimate = -298.9288
expansions: [(11, 5), (12, 3), (13, 1), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 306.0148 - loglik: -3.0194e+02 - logprior: -4.0701e+00
Epoch 2/2
19/19 - 1s - loss: 299.1029 - loglik: -2.9700e+02 - logprior: -2.1025e+00
Fitted a model with MAP estimate = -296.5330
expansions: []
discards: [12 13 16 40 41 50]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.1292 - loglik: -2.9584e+02 - logprior: -3.2845e+00
Epoch 2/2
19/19 - 1s - loss: 296.0198 - loglik: -2.9467e+02 - logprior: -1.3485e+00
Fitted a model with MAP estimate = -295.5740
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 298.4364 - loglik: -2.9524e+02 - logprior: -3.1958e+00
Epoch 2/10
19/19 - 1s - loss: 295.8930 - loglik: -2.9456e+02 - logprior: -1.3294e+00
Epoch 3/10
19/19 - 1s - loss: 295.4840 - loglik: -2.9425e+02 - logprior: -1.2346e+00
Epoch 4/10
19/19 - 1s - loss: 295.2004 - loglik: -2.9402e+02 - logprior: -1.1785e+00
Epoch 5/10
19/19 - 1s - loss: 294.9435 - loglik: -2.9380e+02 - logprior: -1.1412e+00
Epoch 6/10
19/19 - 1s - loss: 294.9778 - loglik: -2.9385e+02 - logprior: -1.1238e+00
Fitted a model with MAP estimate = -294.8375
Time for alignment: 37.5836
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 337.8627 - loglik: -3.3468e+02 - logprior: -3.1871e+00
Epoch 2/10
19/19 - 1s - loss: 309.5031 - loglik: -3.0811e+02 - logprior: -1.3924e+00
Epoch 3/10
19/19 - 1s - loss: 301.5974 - loglik: -3.0012e+02 - logprior: -1.4820e+00
Epoch 4/10
19/19 - 1s - loss: 299.9156 - loglik: -2.9860e+02 - logprior: -1.3167e+00
Epoch 5/10
19/19 - 1s - loss: 299.2079 - loglik: -2.9789e+02 - logprior: -1.3229e+00
Epoch 6/10
19/19 - 1s - loss: 299.3066 - loglik: -2.9801e+02 - logprior: -1.3005e+00
Fitted a model with MAP estimate = -298.9910
expansions: [(11, 2), (12, 3), (13, 4), (14, 2), (26, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 306.8385 - loglik: -3.0276e+02 - logprior: -4.0809e+00
Epoch 2/2
19/19 - 1s - loss: 299.5722 - loglik: -2.9743e+02 - logprior: -2.1375e+00
Fitted a model with MAP estimate = -296.8934
expansions: []
discards: [13 14 36 43 44 53]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 299.2166 - loglik: -2.9589e+02 - logprior: -3.3271e+00
Epoch 2/2
19/19 - 1s - loss: 295.7582 - loglik: -2.9438e+02 - logprior: -1.3757e+00
Fitted a model with MAP estimate = -295.3836
expansions: []
discards: [12 15]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 298.5989 - loglik: -2.9539e+02 - logprior: -3.2041e+00
Epoch 2/10
19/19 - 1s - loss: 295.7839 - loglik: -2.9443e+02 - logprior: -1.3497e+00
Epoch 3/10
19/19 - 1s - loss: 295.5321 - loglik: -2.9428e+02 - logprior: -1.2496e+00
Epoch 4/10
19/19 - 1s - loss: 295.0919 - loglik: -2.9390e+02 - logprior: -1.1925e+00
Epoch 5/10
19/19 - 1s - loss: 295.1139 - loglik: -2.9396e+02 - logprior: -1.1546e+00
Fitted a model with MAP estimate = -294.8992
Time for alignment: 36.6989
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 337.6124 - loglik: -3.3442e+02 - logprior: -3.1886e+00
Epoch 2/10
19/19 - 1s - loss: 309.1924 - loglik: -3.0780e+02 - logprior: -1.3920e+00
Epoch 3/10
19/19 - 1s - loss: 301.2761 - loglik: -2.9979e+02 - logprior: -1.4861e+00
Epoch 4/10
19/19 - 1s - loss: 299.6295 - loglik: -2.9830e+02 - logprior: -1.3270e+00
Epoch 5/10
19/19 - 1s - loss: 299.2583 - loglik: -2.9793e+02 - logprior: -1.3292e+00
Epoch 6/10
19/19 - 1s - loss: 298.9569 - loglik: -2.9766e+02 - logprior: -1.3017e+00
Epoch 7/10
19/19 - 1s - loss: 299.1165 - loglik: -2.9782e+02 - logprior: -1.2925e+00
Fitted a model with MAP estimate = -298.8666
expansions: [(11, 5), (12, 3), (13, 1), (18, 1), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 306.1777 - loglik: -3.0211e+02 - logprior: -4.0675e+00
Epoch 2/2
19/19 - 1s - loss: 299.2834 - loglik: -2.9716e+02 - logprior: -2.1226e+00
Fitted a model with MAP estimate = -296.7255
expansions: []
discards: [12 13 16 40 41 50]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.3330 - loglik: -2.9604e+02 - logprior: -3.2963e+00
Epoch 2/2
19/19 - 1s - loss: 295.9617 - loglik: -2.9461e+02 - logprior: -1.3546e+00
Fitted a model with MAP estimate = -295.5903
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 298.3042 - loglik: -2.9511e+02 - logprior: -3.1972e+00
Epoch 2/10
19/19 - 1s - loss: 295.9721 - loglik: -2.9464e+02 - logprior: -1.3317e+00
Epoch 3/10
19/19 - 1s - loss: 295.3568 - loglik: -2.9412e+02 - logprior: -1.2347e+00
Epoch 4/10
19/19 - 1s - loss: 295.2654 - loglik: -2.9409e+02 - logprior: -1.1755e+00
Epoch 5/10
19/19 - 1s - loss: 295.0504 - loglik: -2.9391e+02 - logprior: -1.1418e+00
Epoch 6/10
19/19 - 1s - loss: 294.9455 - loglik: -2.9382e+02 - logprior: -1.1238e+00
Epoch 7/10
19/19 - 1s - loss: 294.8626 - loglik: -2.9376e+02 - logprior: -1.1045e+00
Epoch 8/10
19/19 - 1s - loss: 294.7743 - loglik: -2.9368e+02 - logprior: -1.0954e+00
Epoch 9/10
19/19 - 1s - loss: 295.0107 - loglik: -2.9393e+02 - logprior: -1.0804e+00
Fitted a model with MAP estimate = -294.7613
Time for alignment: 41.3014
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 337.9694 - loglik: -3.3478e+02 - logprior: -3.1881e+00
Epoch 2/10
19/19 - 1s - loss: 309.4182 - loglik: -3.0802e+02 - logprior: -1.4006e+00
Epoch 3/10
19/19 - 1s - loss: 301.3375 - loglik: -2.9984e+02 - logprior: -1.5013e+00
Epoch 4/10
19/19 - 1s - loss: 299.6321 - loglik: -2.9830e+02 - logprior: -1.3368e+00
Epoch 5/10
19/19 - 1s - loss: 299.5785 - loglik: -2.9825e+02 - logprior: -1.3327e+00
Epoch 6/10
19/19 - 1s - loss: 298.9488 - loglik: -2.9764e+02 - logprior: -1.3043e+00
Epoch 7/10
19/19 - 1s - loss: 299.1649 - loglik: -2.9787e+02 - logprior: -1.2921e+00
Fitted a model with MAP estimate = -298.9240
expansions: [(11, 2), (12, 3), (13, 4), (14, 2), (26, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 306.8919 - loglik: -3.0282e+02 - logprior: -4.0762e+00
Epoch 2/2
19/19 - 1s - loss: 299.7930 - loglik: -2.9764e+02 - logprior: -2.1529e+00
Fitted a model with MAP estimate = -297.1239
expansions: []
discards: [11 13 14 36 43 44 53]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 299.2139 - loglik: -2.9589e+02 - logprior: -3.3277e+00
Epoch 2/2
19/19 - 1s - loss: 295.8938 - loglik: -2.9453e+02 - logprior: -1.3639e+00
Fitted a model with MAP estimate = -295.3925
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 298.2013 - loglik: -2.9500e+02 - logprior: -3.1971e+00
Epoch 2/10
19/19 - 1s - loss: 295.5956 - loglik: -2.9426e+02 - logprior: -1.3364e+00
Epoch 3/10
19/19 - 1s - loss: 295.3408 - loglik: -2.9410e+02 - logprior: -1.2386e+00
Epoch 4/10
19/19 - 1s - loss: 294.7836 - loglik: -2.9361e+02 - logprior: -1.1766e+00
Epoch 5/10
19/19 - 1s - loss: 294.9532 - loglik: -2.9380e+02 - logprior: -1.1496e+00
Fitted a model with MAP estimate = -294.6108
Time for alignment: 36.0545
Fitting a model of length 44 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 337.8436 - loglik: -3.3466e+02 - logprior: -3.1880e+00
Epoch 2/10
19/19 - 1s - loss: 309.3712 - loglik: -3.0798e+02 - logprior: -1.3919e+00
Epoch 3/10
19/19 - 1s - loss: 301.4227 - loglik: -2.9994e+02 - logprior: -1.4840e+00
Epoch 4/10
19/19 - 1s - loss: 299.7701 - loglik: -2.9845e+02 - logprior: -1.3192e+00
Epoch 5/10
19/19 - 1s - loss: 299.2913 - loglik: -2.9797e+02 - logprior: -1.3194e+00
Epoch 6/10
19/19 - 1s - loss: 299.1565 - loglik: -2.9786e+02 - logprior: -1.3004e+00
Epoch 7/10
19/19 - 1s - loss: 299.0870 - loglik: -2.9780e+02 - logprior: -1.2896e+00
Epoch 8/10
19/19 - 1s - loss: 298.9240 - loglik: -2.9764e+02 - logprior: -1.2802e+00
Epoch 9/10
19/19 - 1s - loss: 299.2485 - loglik: -2.9797e+02 - logprior: -1.2767e+00
Fitted a model with MAP estimate = -298.8629
expansions: [(11, 1), (12, 3), (13, 4), (14, 2), (26, 2), (29, 2), (30, 2), (34, 1), (36, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 306.6813 - loglik: -3.0262e+02 - logprior: -4.0657e+00
Epoch 2/2
19/19 - 1s - loss: 299.8062 - loglik: -2.9764e+02 - logprior: -2.1652e+00
Fitted a model with MAP estimate = -297.2490
expansions: []
discards: [12 13 35 42 43 52]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 299.3063 - loglik: -2.9595e+02 - logprior: -3.3517e+00
Epoch 2/2
19/19 - 1s - loss: 295.8881 - loglik: -2.9453e+02 - logprior: -1.3559e+00
Fitted a model with MAP estimate = -295.4099
expansions: []
discards: [14 15]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 10004 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 298.5145 - loglik: -2.9532e+02 - logprior: -3.1915e+00
Epoch 2/10
19/19 - 1s - loss: 295.9656 - loglik: -2.9464e+02 - logprior: -1.3289e+00
Epoch 3/10
19/19 - 1s - loss: 295.4384 - loglik: -2.9421e+02 - logprior: -1.2282e+00
Epoch 4/10
19/19 - 1s - loss: 295.2726 - loglik: -2.9410e+02 - logprior: -1.1762e+00
Epoch 5/10
19/19 - 1s - loss: 295.1234 - loglik: -2.9399e+02 - logprior: -1.1341e+00
Epoch 6/10
19/19 - 1s - loss: 294.9713 - loglik: -2.9384e+02 - logprior: -1.1269e+00
Epoch 7/10
19/19 - 1s - loss: 294.9239 - loglik: -2.9382e+02 - logprior: -1.1015e+00
Epoch 8/10
19/19 - 1s - loss: 295.1879 - loglik: -2.9409e+02 - logprior: -1.0977e+00
Fitted a model with MAP estimate = -294.8358
Time for alignment: 41.4701
Computed alignments with likelihoods: ['-294.8375', '-294.8992', '-294.7613', '-294.6108', '-294.8358']
Best model has likelihood: -294.6108  (prior= -1.1357 )
time for generating output: 0.1067
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00084.projection.fasta
SP score = 0.8945783132530121
Training of 5 independent models on file PF00232.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 66s - loss: 1963.4225 - loglik: -1.9620e+03 - logprior: -1.4640e+00
Epoch 2/10
39/39 - 57s - loss: 1778.8953 - loglik: -1.7775e+03 - logprior: -1.3678e+00
Epoch 3/10
39/39 - 54s - loss: 1765.4762 - loglik: -1.7641e+03 - logprior: -1.4171e+00
Epoch 4/10
39/39 - 54s - loss: 1765.0154 - loglik: -1.7635e+03 - logprior: -1.4671e+00
Epoch 5/10
39/39 - 61s - loss: 1763.4457 - loglik: -1.7620e+03 - logprior: -1.4029e+00
Epoch 6/10
39/39 - 68s - loss: 1763.2312 - loglik: -1.7618e+03 - logprior: -1.4746e+00
Epoch 7/10
39/39 - 69s - loss: 1763.2396 - loglik: -1.7619e+03 - logprior: -1.3734e+00
Fitted a model with MAP estimate = -1761.0293
expansions: [(0, 3), (38, 1), (134, 1), (136, 1), (162, 1), (163, 1), (171, 1), (175, 6), (176, 2), (177, 1), (178, 1), (179, 1), (190, 2), (191, 5), (192, 1), (193, 1), (196, 1), (197, 3), (198, 1), (200, 1), (201, 2), (202, 2), (203, 1), (206, 1), (211, 1), (215, 1), (217, 1), (218, 1), (222, 9), (225, 1), (226, 2), (227, 4), (240, 1), (242, 1), (243, 1), (244, 3), (245, 2), (247, 2), (248, 6), (250, 1), (252, 1), (253, 1), (266, 1), (268, 1), (270, 1), (284, 1), (285, 1), (287, 3), (289, 1), (300, 1), (302, 2), (303, 2), (325, 1), (328, 1), (338, 1), (348, 1), (351, 1), (352, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 124s - loss: 1734.4465 - loglik: -1.7324e+03 - logprior: -1.9968e+00
Epoch 2/2
39/39 - 123s - loss: 1717.7057 - loglik: -1.7172e+03 - logprior: -4.7223e-01
Fitted a model with MAP estimate = -1714.1494
expansions: [(217, 1), (282, 1), (284, 1), (373, 1), (452, 1)]
discards: [  2   3 185 186 187 188 213 214 238 313 314 315 316 317]
Re-initialized the encoder parameters.
Fitting a model of length 446 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 126s - loss: 1723.8251 - loglik: -1.7226e+03 - logprior: -1.1902e+00
Epoch 2/2
39/39 - 112s - loss: 1719.6497 - loglik: -1.7197e+03 - logprior: 0.0092
Fitted a model with MAP estimate = -1715.9610
expansions: [(303, 2), (304, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 129s - loss: 1721.1935 - loglik: -1.7202e+03 - logprior: -9.6817e-01
Epoch 2/10
39/39 - 114s - loss: 1716.2804 - loglik: -1.7166e+03 - logprior: 0.3095
Epoch 3/10
39/39 - 112s - loss: 1715.9478 - loglik: -1.7164e+03 - logprior: 0.4391
Epoch 4/10
39/39 - 122s - loss: 1713.8190 - loglik: -1.7144e+03 - logprior: 0.5361
Epoch 5/10
39/39 - 120s - loss: 1712.7400 - loglik: -1.7136e+03 - logprior: 0.8511
Epoch 6/10
39/39 - 110s - loss: 1713.9021 - loglik: -1.7149e+03 - logprior: 0.9835
Fitted a model with MAP estimate = -1712.0789
Time for alignment: 2140.1324
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 83s - loss: 1959.0709 - loglik: -1.9576e+03 - logprior: -1.4610e+00
Epoch 2/10
39/39 - 78s - loss: 1781.0156 - loglik: -1.7799e+03 - logprior: -1.1542e+00
Epoch 3/10
39/39 - 62s - loss: 1766.7274 - loglik: -1.7657e+03 - logprior: -1.0628e+00
Epoch 4/10
39/39 - 58s - loss: 1763.3284 - loglik: -1.7624e+03 - logprior: -9.7600e-01
Epoch 5/10
39/39 - 57s - loss: 1764.2566 - loglik: -1.7633e+03 - logprior: -9.4825e-01
Fitted a model with MAP estimate = -1760.8864
expansions: [(0, 3), (37, 1), (43, 1), (99, 1), (131, 1), (164, 1), (171, 1), (177, 1), (178, 9), (180, 1), (181, 1), (192, 2), (193, 6), (194, 2), (197, 1), (198, 3), (199, 1), (201, 1), (202, 2), (203, 2), (204, 1), (207, 1), (209, 1), (211, 2), (213, 1), (217, 1), (221, 2), (222, 5), (223, 2), (225, 1), (226, 2), (227, 3), (228, 2), (229, 1), (240, 1), (242, 1), (243, 1), (244, 2), (245, 3), (247, 2), (248, 5), (249, 1), (251, 1), (252, 2), (253, 1), (270, 1), (286, 2), (287, 2), (288, 2), (290, 1), (291, 1), (300, 1), (302, 1), (303, 1), (304, 1), (312, 1), (326, 1), (329, 1), (346, 1), (355, 6)]
discards: [  2   3 127]
Re-initialized the encoder parameters.
Fitting a model of length 461 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 98s - loss: 1737.7150 - loglik: -1.7356e+03 - logprior: -2.1188e+00
Epoch 2/2
39/39 - 96s - loss: 1718.6847 - loglik: -1.7178e+03 - logprior: -8.8260e-01
Fitted a model with MAP estimate = -1715.2147
expansions: [(252, 1), (282, 1)]
discards: [  2 187 188 189 190 191 220 239 272 288 315 316 317 318 373 459]
Re-initialized the encoder parameters.
Fitting a model of length 447 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 80s - loss: 1723.8175 - loglik: -1.7225e+03 - logprior: -1.3498e+00
Epoch 2/2
39/39 - 77s - loss: 1718.7390 - loglik: -1.7184e+03 - logprior: -3.0589e-01
Fitted a model with MAP estimate = -1716.5273
expansions: [(304, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 75s - loss: 1721.5723 - loglik: -1.7205e+03 - logprior: -1.0821e+00
Epoch 2/10
39/39 - 71s - loss: 1716.5371 - loglik: -1.7168e+03 - logprior: 0.2279
Epoch 3/10
39/39 - 71s - loss: 1717.6112 - loglik: -1.7180e+03 - logprior: 0.3634
Fitted a model with MAP estimate = -1714.4875
Time for alignment: 1285.1796
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 51s - loss: 1962.4515 - loglik: -1.9610e+03 - logprior: -1.4687e+00
Epoch 2/10
39/39 - 49s - loss: 1779.8943 - loglik: -1.7786e+03 - logprior: -1.3436e+00
Epoch 3/10
39/39 - 50s - loss: 1765.0612 - loglik: -1.7636e+03 - logprior: -1.4800e+00
Epoch 4/10
39/39 - 51s - loss: 1764.0701 - loglik: -1.7623e+03 - logprior: -1.8045e+00
Epoch 5/10
39/39 - 51s - loss: 1762.8674 - loglik: -1.7612e+03 - logprior: -1.6196e+00
Epoch 6/10
39/39 - 51s - loss: 1762.5497 - loglik: -1.7610e+03 - logprior: -1.5852e+00
Epoch 7/10
39/39 - 52s - loss: 1760.4932 - loglik: -1.7587e+03 - logprior: -1.7737e+00
Epoch 8/10
39/39 - 54s - loss: 1762.3691 - loglik: -1.7608e+03 - logprior: -1.5305e+00
Fitted a model with MAP estimate = -1760.7746
expansions: [(0, 3), (38, 1), (42, 1), (134, 1), (143, 1), (163, 1), (172, 1), (176, 6), (177, 2), (178, 1), (179, 1), (189, 1), (191, 2), (192, 5), (193, 1), (194, 1), (197, 1), (198, 3), (199, 2), (200, 1), (201, 1), (203, 1), (204, 2), (206, 1), (208, 1), (210, 1), (214, 1), (217, 1), (222, 2), (223, 7), (226, 1), (227, 2), (228, 3), (229, 2), (242, 1), (244, 1), (245, 1), (246, 3), (247, 2), (249, 2), (250, 6), (252, 1), (255, 1), (271, 1), (272, 1), (273, 1), (286, 1), (288, 1), (289, 2), (290, 2), (292, 1), (293, 1), (302, 1), (304, 2), (305, 2), (307, 2), (316, 1), (326, 1), (329, 1), (338, 1), (345, 1), (347, 1), (349, 2), (351, 1)]
discards: [  2 127]
Re-initialized the encoder parameters.
Fitting a model of length 460 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 92s - loss: 1734.8762 - loglik: -1.7330e+03 - logprior: -1.8767e+00
Epoch 2/2
39/39 - 95s - loss: 1716.5750 - loglik: -1.7159e+03 - logprior: -6.4630e-01
Fitted a model with MAP estimate = -1714.0484
expansions: [(217, 1), (281, 1), (323, 2), (324, 2)]
discards: [  2   3 185 186 187 188 213 214 272 287 314 315 316 317 318 319 320 328
 329 330 372 402]
Re-initialized the encoder parameters.
Fitting a model of length 444 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 107s - loss: 1726.2592 - loglik: -1.7250e+03 - logprior: -1.2155e+00
Epoch 2/2
39/39 - 102s - loss: 1721.4026 - loglik: -1.7214e+03 - logprior: -2.9577e-02
Fitted a model with MAP estimate = -1717.4668
expansions: [(305, 2), (309, 1), (310, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 449 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 124s - loss: 1721.0781 - loglik: -1.7200e+03 - logprior: -1.0285e+00
Epoch 2/10
39/39 - 111s - loss: 1719.6963 - loglik: -1.7199e+03 - logprior: 0.1792
Epoch 3/10
39/39 - 101s - loss: 1715.0248 - loglik: -1.7155e+03 - logprior: 0.4558
Epoch 4/10
39/39 - 104s - loss: 1715.8597 - loglik: -1.7164e+03 - logprior: 0.5765
Fitted a model with MAP estimate = -1714.1351
Time for alignment: 1727.3236
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 78s - loss: 1960.6849 - loglik: -1.9592e+03 - logprior: -1.4805e+00
Epoch 2/10
39/39 - 85s - loss: 1778.9559 - loglik: -1.7775e+03 - logprior: -1.4526e+00
Epoch 3/10
39/39 - 83s - loss: 1764.4709 - loglik: -1.7626e+03 - logprior: -1.8513e+00
Epoch 4/10
39/39 - 85s - loss: 1762.3434 - loglik: -1.7606e+03 - logprior: -1.7086e+00
Epoch 5/10
39/39 - 73s - loss: 1761.4325 - loglik: -1.7597e+03 - logprior: -1.7211e+00
Epoch 6/10
39/39 - 74s - loss: 1760.9065 - loglik: -1.7592e+03 - logprior: -1.6982e+00
Epoch 7/10
39/39 - 67s - loss: 1759.3069 - loglik: -1.7576e+03 - logprior: -1.7478e+00
Epoch 8/10
39/39 - 73s - loss: 1761.0377 - loglik: -1.7593e+03 - logprior: -1.7431e+00
Fitted a model with MAP estimate = -1759.1302
expansions: [(0, 3), (38, 1), (45, 1), (131, 1), (134, 1), (142, 1), (162, 1), (164, 1), (168, 1), (175, 4), (176, 6), (177, 1), (178, 1), (189, 2), (190, 7), (191, 1), (194, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 1), (201, 1), (202, 1), (204, 1), (206, 1), (207, 1), (208, 1), (212, 1), (215, 1), (220, 2), (221, 7), (222, 1), (225, 1), (226, 2), (227, 1), (228, 1), (229, 2), (230, 1), (241, 1), (243, 1), (244, 1), (245, 3), (246, 2), (248, 1), (249, 4), (250, 2), (251, 1), (253, 1), (255, 1), (256, 1), (270, 1), (272, 1), (286, 1), (288, 1), (289, 4), (290, 1), (291, 1), (293, 1), (299, 1), (301, 1), (302, 1), (303, 1), (311, 1), (325, 1), (328, 1), (345, 1), (351, 1), (352, 2)]
discards: [  1   2 127]
Re-initialized the encoder parameters.
Fitting a model of length 459 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 134s - loss: 1734.3181 - loglik: -1.7325e+03 - logprior: -1.8654e+00
Epoch 2/2
39/39 - 136s - loss: 1716.6420 - loglik: -1.7162e+03 - logprior: -4.2487e-01
Fitted a model with MAP estimate = -1713.4598
expansions: [(217, 1), (283, 1), (377, 1), (456, 1)]
discards: [  2   3 186 187 188 189 190 214 215 271 272 273 290 316 317 318 319 320]
Re-initialized the encoder parameters.
Fitting a model of length 445 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 107s - loss: 1724.3379 - loglik: -1.7231e+03 - logprior: -1.2413e+00
Epoch 2/2
39/39 - 104s - loss: 1718.9600 - loglik: -1.7188e+03 - logprior: -1.3160e-01
Fitted a model with MAP estimate = -1716.5885
expansions: [(0, 3), (302, 3), (303, 1)]
discards: [314]
Re-initialized the encoder parameters.
Fitting a model of length 451 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 125s - loss: 1721.9668 - loglik: -1.7205e+03 - logprior: -1.4502e+00
Epoch 2/10
39/39 - 113s - loss: 1719.4299 - loglik: -1.7194e+03 - logprior: 0.0100
Epoch 3/10
39/39 - 104s - loss: 1714.0745 - loglik: -1.7142e+03 - logprior: 0.1575
Epoch 4/10
39/39 - 104s - loss: 1714.6332 - loglik: -1.7150e+03 - logprior: 0.3624
Fitted a model with MAP estimate = -1713.4261
Time for alignment: 2091.4535
Fitting a model of length 355 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 77s - loss: 1959.8422 - loglik: -1.9583e+03 - logprior: -1.5035e+00
Epoch 2/10
39/39 - 67s - loss: 1781.4419 - loglik: -1.7800e+03 - logprior: -1.4118e+00
Epoch 3/10
39/39 - 60s - loss: 1767.5605 - loglik: -1.7659e+03 - logprior: -1.6704e+00
Epoch 4/10
39/39 - 59s - loss: 1764.7971 - loglik: -1.7632e+03 - logprior: -1.5738e+00
Epoch 5/10
39/39 - 58s - loss: 1764.5631 - loglik: -1.7630e+03 - logprior: -1.5764e+00
Epoch 6/10
39/39 - 57s - loss: 1761.9521 - loglik: -1.7604e+03 - logprior: -1.5350e+00
Epoch 7/10
39/39 - 57s - loss: 1762.8853 - loglik: -1.7613e+03 - logprior: -1.5829e+00
Fitted a model with MAP estimate = -1760.8560
expansions: [(0, 3), (38, 1), (58, 1), (133, 1), (146, 1), (162, 1), (163, 1), (164, 1), (174, 8), (175, 1), (176, 1), (177, 1), (188, 2), (189, 5), (190, 1), (191, 1), (194, 1), (195, 1), (196, 1), (197, 2), (198, 1), (199, 1), (201, 1), (202, 1), (205, 1), (207, 1), (209, 1), (213, 1), (215, 1), (216, 1), (220, 2), (221, 5), (222, 2), (225, 1), (226, 2), (227, 3), (228, 2), (229, 1), (240, 1), (242, 1), (243, 1), (244, 1), (245, 2), (246, 2), (248, 2), (249, 5), (250, 2), (251, 1), (253, 1), (269, 1), (270, 1), (271, 1), (286, 2), (287, 2), (288, 2), (290, 1), (291, 1), (300, 1), (302, 1), (303, 1), (304, 1), (326, 2), (329, 1), (338, 1), (348, 1), (351, 1), (352, 2)]
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 458 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 91s - loss: 1734.9043 - loglik: -1.7331e+03 - logprior: -1.8254e+00
Epoch 2/2
39/39 - 85s - loss: 1717.6091 - loglik: -1.7171e+03 - logprior: -4.8831e-01
Fitted a model with MAP estimate = -1714.7274
expansions: [(214, 1), (280, 1), (323, 4), (455, 1)]
discards: [  2   3 184 185 211 212 269 272 274 275 276 277 286 313 324 325 326 327
 373]
Re-initialized the encoder parameters.
Fitting a model of length 446 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 85s - loss: 1727.1516 - loglik: -1.7259e+03 - logprior: -1.2311e+00
Epoch 2/2
39/39 - 84s - loss: 1720.6200 - loglik: -1.7206e+03 - logprior: -5.9366e-02
Fitted a model with MAP estimate = -1717.8343
expansions: [(0, 3), (266, 4), (267, 1), (311, 2), (312, 1)]
discards: [306 307 308 314 315]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 10007 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 91s - loss: 1722.1799 - loglik: -1.7206e+03 - logprior: -1.5444e+00
Epoch 2/10
39/39 - 78s - loss: 1717.0291 - loglik: -1.7171e+03 - logprior: 0.0728
Epoch 3/10
39/39 - 75s - loss: 1715.9647 - loglik: -1.7160e+03 - logprior: 0.0628
Epoch 4/10
39/39 - 83s - loss: 1715.0884 - loglik: -1.7155e+03 - logprior: 0.3946
Epoch 5/10
39/39 - 91s - loss: 1712.2728 - loglik: -1.7127e+03 - logprior: 0.4597
Epoch 6/10
39/39 - 96s - loss: 1712.6931 - loglik: -1.7134e+03 - logprior: 0.6659
Fitted a model with MAP estimate = -1712.2468
Time for alignment: 1691.8844
Computed alignments with likelihoods: ['-1712.0789', '-1714.4875', '-1714.0484', '-1713.4261', '-1712.2468']
Best model has likelihood: -1712.0789  (prior= 1.1963 )
time for generating output: 0.3997
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00232.projection.fasta
SP score = 0.8697974627197863
Training of 5 independent models on file PF13522.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 845.2549 - loglik: -8.4235e+02 - logprior: -2.9035e+00
Epoch 2/10
19/19 - 4s - loss: 763.3601 - loglik: -7.6242e+02 - logprior: -9.3555e-01
Epoch 3/10
19/19 - 4s - loss: 734.4053 - loglik: -7.3336e+02 - logprior: -1.0432e+00
Epoch 4/10
19/19 - 4s - loss: 730.0606 - loglik: -7.2906e+02 - logprior: -9.9653e-01
Epoch 5/10
19/19 - 4s - loss: 728.2943 - loglik: -7.2735e+02 - logprior: -9.4883e-01
Epoch 6/10
19/19 - 4s - loss: 727.5081 - loglik: -7.2659e+02 - logprior: -9.1683e-01
Epoch 7/10
19/19 - 4s - loss: 728.0530 - loglik: -7.2716e+02 - logprior: -8.9353e-01
Fitted a model with MAP estimate = -726.2681
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 726.2736 - loglik: -7.2245e+02 - logprior: -3.8249e+00
Epoch 2/2
19/19 - 6s - loss: 715.0904 - loglik: -7.1409e+02 - logprior: -1.0027e+00
Fitted a model with MAP estimate = -712.3998
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  99 100 146]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 720.4113 - loglik: -7.1663e+02 - logprior: -3.7832e+00
Epoch 2/2
19/19 - 5s - loss: 715.2295 - loglik: -7.1421e+02 - logprior: -1.0152e+00
Fitted a model with MAP estimate = -712.9869
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 717.3764 - loglik: -7.1439e+02 - logprior: -2.9822e+00
Epoch 2/10
19/19 - 5s - loss: 713.7934 - loglik: -7.1299e+02 - logprior: -7.9892e-01
Epoch 3/10
19/19 - 5s - loss: 711.9606 - loglik: -7.1136e+02 - logprior: -6.0258e-01
Epoch 4/10
19/19 - 5s - loss: 711.6759 - loglik: -7.1119e+02 - logprior: -4.8427e-01
Epoch 5/10
19/19 - 6s - loss: 711.1778 - loglik: -7.1077e+02 - logprior: -4.0829e-01
Epoch 6/10
19/19 - 5s - loss: 710.5445 - loglik: -7.1019e+02 - logprior: -3.5605e-01
Epoch 7/10
19/19 - 5s - loss: 710.1110 - loglik: -7.0979e+02 - logprior: -3.2231e-01
Epoch 8/10
19/19 - 5s - loss: 710.6118 - loglik: -7.1033e+02 - logprior: -2.8525e-01
Fitted a model with MAP estimate = -709.8676
Time for alignment: 139.0858
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 846.2133 - loglik: -8.4331e+02 - logprior: -2.8985e+00
Epoch 2/10
19/19 - 4s - loss: 764.3593 - loglik: -7.6342e+02 - logprior: -9.4015e-01
Epoch 3/10
19/19 - 4s - loss: 736.1545 - loglik: -7.3509e+02 - logprior: -1.0677e+00
Epoch 4/10
19/19 - 4s - loss: 731.3004 - loglik: -7.3028e+02 - logprior: -1.0246e+00
Epoch 5/10
19/19 - 4s - loss: 730.0048 - loglik: -7.2902e+02 - logprior: -9.8062e-01
Epoch 6/10
19/19 - 4s - loss: 728.7029 - loglik: -7.2774e+02 - logprior: -9.6143e-01
Epoch 7/10
19/19 - 4s - loss: 728.5247 - loglik: -7.2759e+02 - logprior: -9.3792e-01
Epoch 8/10
19/19 - 4s - loss: 728.2808 - loglik: -7.2735e+02 - logprior: -9.2672e-01
Epoch 9/10
19/19 - 4s - loss: 727.5317 - loglik: -7.2661e+02 - logprior: -9.2574e-01
Epoch 10/10
19/19 - 5s - loss: 727.4507 - loglik: -7.2653e+02 - logprior: -9.2212e-01
Fitted a model with MAP estimate = -726.7773
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (27, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (99, 1), (104, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 156 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 727.6140 - loglik: -7.2365e+02 - logprior: -3.9670e+00
Epoch 2/2
19/19 - 6s - loss: 716.2026 - loglik: -7.1514e+02 - logprior: -1.0625e+00
Fitted a model with MAP estimate = -714.0102
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  99 100]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 721.7117 - loglik: -7.1787e+02 - logprior: -3.8458e+00
Epoch 2/2
19/19 - 6s - loss: 716.1529 - loglik: -7.1506e+02 - logprior: -1.0959e+00
Fitted a model with MAP estimate = -714.4280
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 718.8691 - loglik: -7.1580e+02 - logprior: -3.0701e+00
Epoch 2/10
19/19 - 5s - loss: 715.2762 - loglik: -7.1439e+02 - logprior: -8.8755e-01
Epoch 3/10
19/19 - 5s - loss: 713.4461 - loglik: -7.1275e+02 - logprior: -6.9719e-01
Epoch 4/10
19/19 - 5s - loss: 712.9862 - loglik: -7.1241e+02 - logprior: -5.7747e-01
Epoch 5/10
19/19 - 6s - loss: 712.2017 - loglik: -7.1172e+02 - logprior: -4.8107e-01
Epoch 6/10
19/19 - 6s - loss: 711.8812 - loglik: -7.1145e+02 - logprior: -4.3033e-01
Epoch 7/10
19/19 - 6s - loss: 711.5294 - loglik: -7.1115e+02 - logprior: -3.7446e-01
Epoch 8/10
19/19 - 6s - loss: 711.3593 - loglik: -7.1104e+02 - logprior: -3.1931e-01
Epoch 9/10
19/19 - 5s - loss: 711.8244 - loglik: -7.1153e+02 - logprior: -2.9422e-01
Fitted a model with MAP estimate = -711.0401
Time for alignment: 158.9694
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 845.6126 - loglik: -8.4272e+02 - logprior: -2.8969e+00
Epoch 2/10
19/19 - 4s - loss: 763.5798 - loglik: -7.6264e+02 - logprior: -9.3528e-01
Epoch 3/10
19/19 - 4s - loss: 735.3618 - loglik: -7.3431e+02 - logprior: -1.0520e+00
Epoch 4/10
19/19 - 4s - loss: 731.4372 - loglik: -7.3043e+02 - logprior: -1.0071e+00
Epoch 5/10
19/19 - 4s - loss: 729.6238 - loglik: -7.2867e+02 - logprior: -9.5427e-01
Epoch 6/10
19/19 - 4s - loss: 728.9924 - loglik: -7.2807e+02 - logprior: -9.2078e-01
Epoch 7/10
19/19 - 4s - loss: 728.9684 - loglik: -7.2807e+02 - logprior: -9.0155e-01
Epoch 8/10
19/19 - 4s - loss: 728.2395 - loglik: -7.2736e+02 - logprior: -8.7659e-01
Epoch 9/10
19/19 - 5s - loss: 728.0299 - loglik: -7.2715e+02 - logprior: -8.7981e-01
Epoch 10/10
19/19 - 4s - loss: 727.9543 - loglik: -7.2708e+02 - logprior: -8.7549e-01
Fitted a model with MAP estimate = -726.9886
expansions: [(0, 7), (7, 3), (8, 2), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (55, 2), (56, 2), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (96, 2), (97, 1), (99, 1), (105, 1), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 158 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 728.3232 - loglik: -7.2435e+02 - logprior: -3.9702e+00
Epoch 2/2
19/19 - 6s - loss: 716.5096 - loglik: -7.1542e+02 - logprior: -1.0892e+00
Fitted a model with MAP estimate = -714.0256
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  74  76 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 721.5968 - loglik: -7.1775e+02 - logprior: -3.8422e+00
Epoch 2/2
19/19 - 6s - loss: 716.4312 - loglik: -7.1533e+02 - logprior: -1.1030e+00
Fitted a model with MAP estimate = -714.5795
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 719.0939 - loglik: -7.1603e+02 - logprior: -3.0676e+00
Epoch 2/10
19/19 - 5s - loss: 714.5679 - loglik: -7.1369e+02 - logprior: -8.7293e-01
Epoch 3/10
19/19 - 6s - loss: 714.1041 - loglik: -7.1342e+02 - logprior: -6.8778e-01
Epoch 4/10
19/19 - 6s - loss: 713.3152 - loglik: -7.1275e+02 - logprior: -5.7015e-01
Epoch 5/10
19/19 - 6s - loss: 712.6363 - loglik: -7.1215e+02 - logprior: -4.8458e-01
Epoch 6/10
19/19 - 6s - loss: 711.9070 - loglik: -7.1147e+02 - logprior: -4.3435e-01
Epoch 7/10
19/19 - 6s - loss: 711.7871 - loglik: -7.1141e+02 - logprior: -3.7883e-01
Epoch 8/10
19/19 - 6s - loss: 712.1365 - loglik: -7.1181e+02 - logprior: -3.2961e-01
Fitted a model with MAP estimate = -711.3413
Time for alignment: 155.0412
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 845.6223 - loglik: -8.4272e+02 - logprior: -2.9047e+00
Epoch 2/10
19/19 - 4s - loss: 762.3993 - loglik: -7.6146e+02 - logprior: -9.3824e-01
Epoch 3/10
19/19 - 4s - loss: 734.3040 - loglik: -7.3324e+02 - logprior: -1.0622e+00
Epoch 4/10
19/19 - 4s - loss: 729.6195 - loglik: -7.2860e+02 - logprior: -1.0191e+00
Epoch 5/10
19/19 - 4s - loss: 727.9575 - loglik: -7.2696e+02 - logprior: -9.9305e-01
Epoch 6/10
19/19 - 4s - loss: 727.6494 - loglik: -7.2668e+02 - logprior: -9.7276e-01
Epoch 7/10
19/19 - 4s - loss: 727.2745 - loglik: -7.2632e+02 - logprior: -9.4991e-01
Epoch 8/10
19/19 - 4s - loss: 726.2056 - loglik: -7.2527e+02 - logprior: -9.3620e-01
Epoch 9/10
19/19 - 4s - loss: 726.4832 - loglik: -7.2555e+02 - logprior: -9.3572e-01
Fitted a model with MAP estimate = -725.5384
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (76, 3), (77, 1), (79, 1), (83, 1), (85, 1), (97, 1), (98, 1), (99, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 726.8658 - loglik: -7.2297e+02 - logprior: -3.8956e+00
Epoch 2/2
19/19 - 5s - loss: 714.8078 - loglik: -7.1378e+02 - logprior: -1.0293e+00
Fitted a model with MAP estimate = -712.3536
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22  99 100 146]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 720.1044 - loglik: -7.1632e+02 - logprior: -3.7884e+00
Epoch 2/2
19/19 - 5s - loss: 714.8627 - loglik: -7.1384e+02 - logprior: -1.0203e+00
Fitted a model with MAP estimate = -712.9950
expansions: [(0, 5)]
discards: [0 1 2 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 717.4053 - loglik: -7.1441e+02 - logprior: -2.9933e+00
Epoch 2/10
19/19 - 5s - loss: 713.2075 - loglik: -7.1236e+02 - logprior: -8.4600e-01
Epoch 3/10
19/19 - 5s - loss: 712.4534 - loglik: -7.1178e+02 - logprior: -6.7526e-01
Epoch 4/10
19/19 - 5s - loss: 711.9125 - loglik: -7.1135e+02 - logprior: -5.6680e-01
Epoch 5/10
19/19 - 5s - loss: 710.4855 - loglik: -7.1001e+02 - logprior: -4.7229e-01
Epoch 6/10
19/19 - 5s - loss: 710.2892 - loglik: -7.0988e+02 - logprior: -4.0469e-01
Epoch 7/10
19/19 - 5s - loss: 710.3859 - loglik: -7.1002e+02 - logprior: -3.6405e-01
Fitted a model with MAP estimate = -709.7852
Time for alignment: 137.1568
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 845.3306 - loglik: -8.4244e+02 - logprior: -2.8949e+00
Epoch 2/10
19/19 - 4s - loss: 761.7971 - loglik: -7.6085e+02 - logprior: -9.4298e-01
Epoch 3/10
19/19 - 4s - loss: 733.2820 - loglik: -7.3221e+02 - logprior: -1.0702e+00
Epoch 4/10
19/19 - 4s - loss: 729.0539 - loglik: -7.2803e+02 - logprior: -1.0233e+00
Epoch 5/10
19/19 - 4s - loss: 726.8853 - loglik: -7.2589e+02 - logprior: -9.9091e-01
Epoch 6/10
19/19 - 4s - loss: 726.6168 - loglik: -7.2565e+02 - logprior: -9.6337e-01
Epoch 7/10
19/19 - 4s - loss: 725.8273 - loglik: -7.2489e+02 - logprior: -9.3228e-01
Epoch 8/10
19/19 - 4s - loss: 724.7217 - loglik: -7.2380e+02 - logprior: -9.2202e-01
Epoch 9/10
19/19 - 4s - loss: 725.2034 - loglik: -7.2428e+02 - logprior: -9.2176e-01
Fitted a model with MAP estimate = -724.3193
expansions: [(0, 7), (7, 2), (8, 3), (10, 2), (34, 1), (35, 1), (37, 1), (49, 1), (53, 1), (55, 1), (58, 1), (74, 1), (75, 4), (76, 1), (83, 1), (85, 1), (97, 1), (98, 1), (100, 1), (105, 1), (112, 2), (113, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 725.5734 - loglik: -7.2171e+02 - logprior: -3.8622e+00
Epoch 2/2
19/19 - 5s - loss: 714.3477 - loglik: -7.1335e+02 - logprior: -1.0003e+00
Fitted a model with MAP estimate = -711.7352
expansions: [(0, 4)]
discards: [  1   2   3   4   5   6  22 100 146]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 719.3261 - loglik: -7.1556e+02 - logprior: -3.7621e+00
Epoch 2/2
19/19 - 5s - loss: 714.3007 - loglik: -7.1330e+02 - logprior: -9.9716e-01
Fitted a model with MAP estimate = -712.2797
expansions: [(0, 5)]
discards: [0 1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10028 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 716.9221 - loglik: -7.1396e+02 - logprior: -2.9660e+00
Epoch 2/10
19/19 - 5s - loss: 712.8690 - loglik: -7.1208e+02 - logprior: -7.8765e-01
Epoch 3/10
19/19 - 5s - loss: 711.4872 - loglik: -7.1090e+02 - logprior: -5.8643e-01
Epoch 4/10
19/19 - 5s - loss: 710.9526 - loglik: -7.1049e+02 - logprior: -4.6379e-01
Epoch 5/10
19/19 - 5s - loss: 710.3348 - loglik: -7.0994e+02 - logprior: -3.9040e-01
Epoch 6/10
19/19 - 5s - loss: 710.4391 - loglik: -7.1010e+02 - logprior: -3.4289e-01
Fitted a model with MAP estimate = -709.6311
Time for alignment: 126.7748
Computed alignments with likelihoods: ['-709.8676', '-711.0401', '-711.3413', '-709.7852', '-709.6311']
Best model has likelihood: -709.6311  (prior= -0.3275 )
time for generating output: 0.1600
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13522.projection.fasta
SP score = 0.6571195837822214
Training of 5 independent models on file PF00037.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 141.6259 - loglik: -1.3826e+02 - logprior: -3.3679e+00
Epoch 2/10
19/19 - 0s - loss: 124.7004 - loglik: -1.2333e+02 - logprior: -1.3708e+00
Epoch 3/10
19/19 - 0s - loss: 118.8820 - loglik: -1.1742e+02 - logprior: -1.4652e+00
Epoch 4/10
19/19 - 0s - loss: 117.3357 - loglik: -1.1587e+02 - logprior: -1.4612e+00
Epoch 5/10
19/19 - 0s - loss: 117.0411 - loglik: -1.1561e+02 - logprior: -1.4324e+00
Epoch 6/10
19/19 - 0s - loss: 116.9087 - loglik: -1.1548e+02 - logprior: -1.4238e+00
Epoch 7/10
19/19 - 0s - loss: 116.8375 - loglik: -1.1543e+02 - logprior: -1.4072e+00
Epoch 8/10
19/19 - 0s - loss: 116.7310 - loglik: -1.1533e+02 - logprior: -1.4017e+00
Epoch 9/10
19/19 - 0s - loss: 116.7600 - loglik: -1.1537e+02 - logprior: -1.3948e+00
Fitted a model with MAP estimate = -116.7034
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 119.8374 - loglik: -1.1511e+02 - logprior: -4.7279e+00
Epoch 2/2
19/19 - 0s - loss: 115.1421 - loglik: -1.1373e+02 - logprior: -1.4093e+00
Fitted a model with MAP estimate = -114.3622
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 116.8984 - loglik: -1.1352e+02 - logprior: -3.3815e+00
Epoch 2/2
19/19 - 0s - loss: 114.3152 - loglik: -1.1285e+02 - logprior: -1.4664e+00
Fitted a model with MAP estimate = -114.0419
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 116.5746 - loglik: -1.1327e+02 - logprior: -3.3008e+00
Epoch 2/10
19/19 - 0s - loss: 114.2415 - loglik: -1.1279e+02 - logprior: -1.4470e+00
Epoch 3/10
19/19 - 0s - loss: 113.9238 - loglik: -1.1259e+02 - logprior: -1.3376e+00
Epoch 4/10
19/19 - 0s - loss: 113.7722 - loglik: -1.1249e+02 - logprior: -1.2846e+00
Epoch 5/10
19/19 - 0s - loss: 113.6648 - loglik: -1.1240e+02 - logprior: -1.2603e+00
Epoch 6/10
19/19 - 0s - loss: 113.5473 - loglik: -1.1230e+02 - logprior: -1.2461e+00
Epoch 7/10
19/19 - 0s - loss: 113.5482 - loglik: -1.1232e+02 - logprior: -1.2324e+00
Fitted a model with MAP estimate = -113.5194
Time for alignment: 27.0413
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 141.5419 - loglik: -1.3817e+02 - logprior: -3.3686e+00
Epoch 2/10
19/19 - 0s - loss: 124.5451 - loglik: -1.2316e+02 - logprior: -1.3886e+00
Epoch 3/10
19/19 - 0s - loss: 118.4880 - loglik: -1.1699e+02 - logprior: -1.4930e+00
Epoch 4/10
19/19 - 0s - loss: 117.2481 - loglik: -1.1578e+02 - logprior: -1.4641e+00
Epoch 5/10
19/19 - 0s - loss: 117.0045 - loglik: -1.1557e+02 - logprior: -1.4363e+00
Epoch 6/10
19/19 - 0s - loss: 116.8734 - loglik: -1.1545e+02 - logprior: -1.4260e+00
Epoch 7/10
19/19 - 0s - loss: 116.8773 - loglik: -1.1547e+02 - logprior: -1.4098e+00
Fitted a model with MAP estimate = -116.7508
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 119.7595 - loglik: -1.1514e+02 - logprior: -4.6180e+00
Epoch 2/2
19/19 - 0s - loss: 115.0604 - loglik: -1.1369e+02 - logprior: -1.3681e+00
Fitted a model with MAP estimate = -114.3010
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 116.9305 - loglik: -1.1355e+02 - logprior: -3.3854e+00
Epoch 2/2
19/19 - 0s - loss: 114.3163 - loglik: -1.1285e+02 - logprior: -1.4659e+00
Fitted a model with MAP estimate = -114.0490
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 116.5644 - loglik: -1.1326e+02 - logprior: -3.3010e+00
Epoch 2/10
19/19 - 0s - loss: 114.2674 - loglik: -1.1282e+02 - logprior: -1.4500e+00
Epoch 3/10
19/19 - 0s - loss: 113.9022 - loglik: -1.1256e+02 - logprior: -1.3413e+00
Epoch 4/10
19/19 - 0s - loss: 113.7916 - loglik: -1.1250e+02 - logprior: -1.2882e+00
Epoch 5/10
19/19 - 0s - loss: 113.6268 - loglik: -1.1236e+02 - logprior: -1.2627e+00
Epoch 6/10
19/19 - 0s - loss: 113.6186 - loglik: -1.1238e+02 - logprior: -1.2422e+00
Epoch 7/10
19/19 - 0s - loss: 113.5570 - loglik: -1.1232e+02 - logprior: -1.2381e+00
Epoch 8/10
19/19 - 0s - loss: 113.5465 - loglik: -1.1232e+02 - logprior: -1.2222e+00
Epoch 9/10
19/19 - 0s - loss: 113.4299 - loglik: -1.1222e+02 - logprior: -1.2131e+00
Epoch 10/10
19/19 - 0s - loss: 113.6039 - loglik: -1.1240e+02 - logprior: -1.1996e+00
Fitted a model with MAP estimate = -113.4702
Time for alignment: 27.6067
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 141.5976 - loglik: -1.3823e+02 - logprior: -3.3703e+00
Epoch 2/10
19/19 - 0s - loss: 124.2299 - loglik: -1.2288e+02 - logprior: -1.3542e+00
Epoch 3/10
19/19 - 0s - loss: 118.6520 - loglik: -1.1720e+02 - logprior: -1.4487e+00
Epoch 4/10
19/19 - 0s - loss: 117.3218 - loglik: -1.1586e+02 - logprior: -1.4599e+00
Epoch 5/10
19/19 - 0s - loss: 116.9771 - loglik: -1.1555e+02 - logprior: -1.4285e+00
Epoch 6/10
19/19 - 0s - loss: 116.9235 - loglik: -1.1550e+02 - logprior: -1.4234e+00
Epoch 7/10
19/19 - 0s - loss: 116.8603 - loglik: -1.1545e+02 - logprior: -1.4121e+00
Epoch 8/10
19/19 - 0s - loss: 116.7180 - loglik: -1.1532e+02 - logprior: -1.3981e+00
Epoch 9/10
19/19 - 0s - loss: 116.8147 - loglik: -1.1543e+02 - logprior: -1.3894e+00
Fitted a model with MAP estimate = -116.6944
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 119.8553 - loglik: -1.1512e+02 - logprior: -4.7329e+00
Epoch 2/2
19/19 - 0s - loss: 115.1350 - loglik: -1.1372e+02 - logprior: -1.4157e+00
Fitted a model with MAP estimate = -114.3860
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 117.1125 - loglik: -1.1368e+02 - logprior: -3.4338e+00
Epoch 2/2
19/19 - 0s - loss: 114.6602 - loglik: -1.1317e+02 - logprior: -1.4861e+00
Fitted a model with MAP estimate = -114.3598
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 116.6595 - loglik: -1.1339e+02 - logprior: -3.2710e+00
Epoch 2/10
19/19 - 0s - loss: 114.3282 - loglik: -1.1288e+02 - logprior: -1.4436e+00
Epoch 3/10
19/19 - 0s - loss: 113.9536 - loglik: -1.1261e+02 - logprior: -1.3468e+00
Epoch 4/10
19/19 - 0s - loss: 113.7595 - loglik: -1.1247e+02 - logprior: -1.2933e+00
Epoch 5/10
19/19 - 0s - loss: 113.6302 - loglik: -1.1237e+02 - logprior: -1.2590e+00
Epoch 6/10
19/19 - 0s - loss: 113.5753 - loglik: -1.1233e+02 - logprior: -1.2451e+00
Epoch 7/10
19/19 - 0s - loss: 113.5393 - loglik: -1.1231e+02 - logprior: -1.2317e+00
Epoch 8/10
19/19 - 0s - loss: 113.5400 - loglik: -1.1232e+02 - logprior: -1.2187e+00
Fitted a model with MAP estimate = -113.5014
Time for alignment: 26.8054
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 141.5062 - loglik: -1.3814e+02 - logprior: -3.3655e+00
Epoch 2/10
19/19 - 0s - loss: 124.2210 - loglik: -1.2287e+02 - logprior: -1.3535e+00
Epoch 3/10
19/19 - 0s - loss: 118.7386 - loglik: -1.1731e+02 - logprior: -1.4290e+00
Epoch 4/10
19/19 - 0s - loss: 117.3124 - loglik: -1.1586e+02 - logprior: -1.4563e+00
Epoch 5/10
19/19 - 0s - loss: 117.0433 - loglik: -1.1561e+02 - logprior: -1.4315e+00
Epoch 6/10
19/19 - 0s - loss: 116.8719 - loglik: -1.1545e+02 - logprior: -1.4180e+00
Epoch 7/10
19/19 - 0s - loss: 116.8384 - loglik: -1.1543e+02 - logprior: -1.4104e+00
Epoch 8/10
19/19 - 0s - loss: 116.7659 - loglik: -1.1536e+02 - logprior: -1.4042e+00
Epoch 9/10
19/19 - 0s - loss: 116.6847 - loglik: -1.1530e+02 - logprior: -1.3875e+00
Epoch 10/10
19/19 - 0s - loss: 116.8167 - loglik: -1.1543e+02 - logprior: -1.3916e+00
Fitted a model with MAP estimate = -116.6825
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 119.9023 - loglik: -1.1512e+02 - logprior: -4.7810e+00
Epoch 2/2
19/19 - 0s - loss: 115.1718 - loglik: -1.1375e+02 - logprior: -1.4246e+00
Fitted a model with MAP estimate = -114.4036
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 117.0852 - loglik: -1.1365e+02 - logprior: -3.4346e+00
Epoch 2/2
19/19 - 0s - loss: 114.6378 - loglik: -1.1315e+02 - logprior: -1.4863e+00
Fitted a model with MAP estimate = -114.3523
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 116.6400 - loglik: -1.1337e+02 - logprior: -3.2708e+00
Epoch 2/10
19/19 - 0s - loss: 114.2971 - loglik: -1.1286e+02 - logprior: -1.4400e+00
Epoch 3/10
19/19 - 0s - loss: 113.9000 - loglik: -1.1256e+02 - logprior: -1.3389e+00
Epoch 4/10
19/19 - 0s - loss: 113.8283 - loglik: -1.1253e+02 - logprior: -1.2982e+00
Epoch 5/10
19/19 - 0s - loss: 113.6398 - loglik: -1.1238e+02 - logprior: -1.2551e+00
Epoch 6/10
19/19 - 0s - loss: 113.5593 - loglik: -1.1231e+02 - logprior: -1.2447e+00
Epoch 7/10
19/19 - 0s - loss: 113.5835 - loglik: -1.1236e+02 - logprior: -1.2266e+00
Fitted a model with MAP estimate = -113.5152
Time for alignment: 25.8340
Fitting a model of length 19 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 141.7142 - loglik: -1.3834e+02 - logprior: -3.3729e+00
Epoch 2/10
19/19 - 0s - loss: 124.4875 - loglik: -1.2310e+02 - logprior: -1.3855e+00
Epoch 3/10
19/19 - 0s - loss: 118.4353 - loglik: -1.1694e+02 - logprior: -1.4956e+00
Epoch 4/10
19/19 - 0s - loss: 117.3185 - loglik: -1.1585e+02 - logprior: -1.4659e+00
Epoch 5/10
19/19 - 0s - loss: 116.9784 - loglik: -1.1554e+02 - logprior: -1.4349e+00
Epoch 6/10
19/19 - 0s - loss: 116.9099 - loglik: -1.1549e+02 - logprior: -1.4206e+00
Epoch 7/10
19/19 - 0s - loss: 116.7742 - loglik: -1.1537e+02 - logprior: -1.4083e+00
Epoch 8/10
19/19 - 0s - loss: 116.7822 - loglik: -1.1538e+02 - logprior: -1.4007e+00
Fitted a model with MAP estimate = -116.7218
expansions: [(0, 1), (3, 1), (4, 1), (11, 1), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 119.7804 - loglik: -1.1511e+02 - logprior: -4.6659e+00
Epoch 2/2
19/19 - 0s - loss: 115.1338 - loglik: -1.1374e+02 - logprior: -1.3963e+00
Fitted a model with MAP estimate = -114.3312
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 3s - loss: 116.8907 - loglik: -1.1350e+02 - logprior: -3.3904e+00
Epoch 2/2
19/19 - 0s - loss: 114.3313 - loglik: -1.1287e+02 - logprior: -1.4639e+00
Fitted a model with MAP estimate = -114.0292
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 24 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 3s - loss: 116.5487 - loglik: -1.1325e+02 - logprior: -3.3000e+00
Epoch 2/10
19/19 - 0s - loss: 114.2544 - loglik: -1.1281e+02 - logprior: -1.4448e+00
Epoch 3/10
19/19 - 0s - loss: 113.9256 - loglik: -1.1258e+02 - logprior: -1.3415e+00
Epoch 4/10
19/19 - 0s - loss: 113.7581 - loglik: -1.1247e+02 - logprior: -1.2882e+00
Epoch 5/10
19/19 - 0s - loss: 113.7428 - loglik: -1.1248e+02 - logprior: -1.2626e+00
Epoch 6/10
19/19 - 0s - loss: 113.6182 - loglik: -1.1237e+02 - logprior: -1.2462e+00
Epoch 7/10
19/19 - 0s - loss: 113.4352 - loglik: -1.1220e+02 - logprior: -1.2325e+00
Epoch 8/10
19/19 - 0s - loss: 113.5758 - loglik: -1.1235e+02 - logprior: -1.2219e+00
Fitted a model with MAP estimate = -113.4965
Time for alignment: 25.8045
Computed alignments with likelihoods: ['-113.5194', '-113.4702', '-113.5014', '-113.5152', '-113.4965']
Best model has likelihood: -113.4702  (prior= -1.1946 )
time for generating output: 0.0719
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00037.projection.fasta
SP score = 0.8860353130016051
Training of 5 independent models on file PF00224.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1645.5480 - loglik: -1.6441e+03 - logprior: -1.4832e+00
Epoch 2/10
39/39 - 24s - loss: 1442.3832 - loglik: -1.4410e+03 - logprior: -1.4302e+00
Epoch 3/10
39/39 - 24s - loss: 1429.8108 - loglik: -1.4284e+03 - logprior: -1.4068e+00
Epoch 4/10
39/39 - 24s - loss: 1427.4709 - loglik: -1.4262e+03 - logprior: -1.3110e+00
Epoch 5/10
39/39 - 24s - loss: 1426.2052 - loglik: -1.4249e+03 - logprior: -1.2910e+00
Epoch 6/10
39/39 - 25s - loss: 1425.1312 - loglik: -1.4238e+03 - logprior: -1.2953e+00
Epoch 7/10
39/39 - 25s - loss: 1425.4131 - loglik: -1.4241e+03 - logprior: -1.3017e+00
Fitted a model with MAP estimate = -1423.3933
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (40, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (71, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (132, 3), (142, 1), (145, 2), (147, 1), (148, 1), (149, 1), (158, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (183, 1), (189, 1), (206, 1), (212, 1), (214, 2), (216, 3), (229, 2), (231, 1), (244, 1), (257, 1), (258, 1), (264, 1), (267, 1), (269, 2), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 1395.1617 - loglik: -1.3932e+03 - logprior: -1.9318e+00
Epoch 2/2
39/39 - 37s - loss: 1377.6710 - loglik: -1.3770e+03 - logprior: -6.4943e-01
Fitted a model with MAP estimate = -1374.2926
expansions: [(0, 2)]
discards: [  0   1  57  98 164 180 215]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 40s - loss: 1381.5371 - loglik: -1.3801e+03 - logprior: -1.4660e+00
Epoch 2/2
39/39 - 39s - loss: 1377.2823 - loglik: -1.3771e+03 - logprior: -1.5665e-01
Fitted a model with MAP estimate = -1373.7633
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 1381.9077 - loglik: -1.3800e+03 - logprior: -1.8688e+00
Epoch 2/10
39/39 - 45s - loss: 1376.5243 - loglik: -1.3766e+03 - logprior: 0.0717
Epoch 3/10
39/39 - 44s - loss: 1373.7159 - loglik: -1.3740e+03 - logprior: 0.2951
Epoch 4/10
39/39 - 44s - loss: 1372.8641 - loglik: -1.3734e+03 - logprior: 0.5255
Epoch 5/10
39/39 - 45s - loss: 1371.5964 - loglik: -1.3721e+03 - logprior: 0.5125
Epoch 6/10
39/39 - 45s - loss: 1371.6376 - loglik: -1.3723e+03 - logprior: 0.6449
Fitted a model with MAP estimate = -1370.5357
Time for alignment: 789.3579
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 33s - loss: 1643.7125 - loglik: -1.6422e+03 - logprior: -1.4939e+00
Epoch 2/10
39/39 - 31s - loss: 1439.7229 - loglik: -1.4383e+03 - logprior: -1.4613e+00
Epoch 3/10
39/39 - 32s - loss: 1428.4460 - loglik: -1.4270e+03 - logprior: -1.4070e+00
Epoch 4/10
39/39 - 31s - loss: 1426.0569 - loglik: -1.4247e+03 - logprior: -1.3127e+00
Epoch 5/10
39/39 - 31s - loss: 1425.0510 - loglik: -1.4237e+03 - logprior: -1.3018e+00
Epoch 6/10
39/39 - 32s - loss: 1424.5052 - loglik: -1.4232e+03 - logprior: -1.2965e+00
Epoch 7/10
39/39 - 33s - loss: 1424.0365 - loglik: -1.4227e+03 - logprior: -1.3080e+00
Epoch 8/10
39/39 - 33s - loss: 1424.0542 - loglik: -1.4227e+03 - logprior: -1.3093e+00
Fitted a model with MAP estimate = -1422.1365
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (71, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (142, 1), (147, 2), (148, 2), (158, 1), (166, 1), (168, 1), (169, 1), (171, 2), (172, 1), (179, 1), (182, 1), (188, 1), (205, 1), (209, 1), (212, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (258, 1), (264, 1), (267, 1), (269, 2), (271, 1), (272, 2), (273, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 1394.1428 - loglik: -1.3923e+03 - logprior: -1.8747e+00
Epoch 2/2
39/39 - 47s - loss: 1377.6230 - loglik: -1.3770e+03 - logprior: -5.9596e-01
Fitted a model with MAP estimate = -1373.9376
expansions: []
discards: [  0   1  97 184 213]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 41s - loss: 1383.1281 - loglik: -1.3817e+03 - logprior: -1.4675e+00
Epoch 2/2
39/39 - 37s - loss: 1377.7842 - loglik: -1.3779e+03 - logprior: 0.1316
Fitted a model with MAP estimate = -1374.7685
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 39s - loss: 1379.9412 - loglik: -1.3788e+03 - logprior: -1.1735e+00
Epoch 2/10
39/39 - 36s - loss: 1374.9612 - loglik: -1.3750e+03 - logprior: 0.0188
Epoch 3/10
39/39 - 36s - loss: 1373.9573 - loglik: -1.3743e+03 - logprior: 0.3159
Epoch 4/10
39/39 - 36s - loss: 1371.3494 - loglik: -1.3716e+03 - logprior: 0.2574
Epoch 5/10
39/39 - 36s - loss: 1371.0426 - loglik: -1.3715e+03 - logprior: 0.4950
Epoch 6/10
39/39 - 39s - loss: 1370.8915 - loglik: -1.3714e+03 - logprior: 0.5503
Epoch 7/10
39/39 - 40s - loss: 1370.4332 - loglik: -1.3712e+03 - logprior: 0.7879
Epoch 8/10
39/39 - 41s - loss: 1370.9829 - loglik: -1.3719e+03 - logprior: 0.8867
Fitted a model with MAP estimate = -1369.7844
Time for alignment: 937.1353
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 31s - loss: 1644.0500 - loglik: -1.6426e+03 - logprior: -1.4799e+00
Epoch 2/10
39/39 - 29s - loss: 1441.2552 - loglik: -1.4399e+03 - logprior: -1.4021e+00
Epoch 3/10
39/39 - 30s - loss: 1429.9332 - loglik: -1.4286e+03 - logprior: -1.3123e+00
Epoch 4/10
39/39 - 30s - loss: 1427.4351 - loglik: -1.4262e+03 - logprior: -1.2328e+00
Epoch 5/10
39/39 - 31s - loss: 1426.5865 - loglik: -1.4254e+03 - logprior: -1.2127e+00
Epoch 6/10
39/39 - 32s - loss: 1425.7631 - loglik: -1.4245e+03 - logprior: -1.2268e+00
Epoch 7/10
39/39 - 32s - loss: 1426.7401 - loglik: -1.4255e+03 - logprior: -1.2194e+00
Fitted a model with MAP estimate = -1423.8872
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (37, 1), (38, 1), (39, 1), (45, 1), (46, 1), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (105, 1), (109, 1), (110, 2), (112, 2), (113, 1), (132, 3), (133, 1), (145, 2), (147, 1), (148, 1), (159, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (188, 1), (189, 1), (206, 1), (212, 1), (214, 2), (216, 2), (227, 1), (229, 3), (230, 1), (243, 1), (265, 1), (267, 2), (269, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 47s - loss: 1394.9249 - loglik: -1.3929e+03 - logprior: -1.9853e+00
Epoch 2/2
39/39 - 45s - loss: 1377.2841 - loglik: -1.3766e+03 - logprior: -6.3512e-01
Fitted a model with MAP estimate = -1373.8423
expansions: [(0, 2)]
discards: [  0   1  97 137 141 165 181 216]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 50s - loss: 1381.7452 - loglik: -1.3804e+03 - logprior: -1.3478e+00
Epoch 2/2
39/39 - 47s - loss: 1377.3225 - loglik: -1.3770e+03 - logprior: -3.1018e-01
Fitted a model with MAP estimate = -1373.8438
expansions: []
discards: [  0   2 342]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 1383.4567 - loglik: -1.3817e+03 - logprior: -1.7120e+00
Epoch 2/10
39/39 - 45s - loss: 1377.7157 - loglik: -1.3781e+03 - logprior: 0.3728
Epoch 3/10
39/39 - 46s - loss: 1374.0184 - loglik: -1.3748e+03 - logprior: 0.8012
Epoch 4/10
39/39 - 45s - loss: 1373.4814 - loglik: -1.3744e+03 - logprior: 0.8737
Epoch 5/10
39/39 - 44s - loss: 1372.3217 - loglik: -1.3734e+03 - logprior: 1.1094
Epoch 6/10
39/39 - 44s - loss: 1372.1293 - loglik: -1.3732e+03 - logprior: 1.0554
Epoch 7/10
39/39 - 45s - loss: 1371.7098 - loglik: -1.3731e+03 - logprior: 1.3508
Epoch 8/10
39/39 - 45s - loss: 1371.9623 - loglik: -1.3735e+03 - logprior: 1.5166
Fitted a model with MAP estimate = -1371.0617
Time for alignment: 992.6066
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 32s - loss: 1646.1433 - loglik: -1.6447e+03 - logprior: -1.4716e+00
Epoch 2/10
39/39 - 30s - loss: 1442.6483 - loglik: -1.4413e+03 - logprior: -1.3493e+00
Epoch 3/10
39/39 - 30s - loss: 1430.5975 - loglik: -1.4294e+03 - logprior: -1.2334e+00
Epoch 4/10
39/39 - 30s - loss: 1428.4299 - loglik: -1.4273e+03 - logprior: -1.1365e+00
Epoch 5/10
39/39 - 31s - loss: 1426.1588 - loglik: -1.4250e+03 - logprior: -1.1223e+00
Epoch 6/10
39/39 - 32s - loss: 1427.0406 - loglik: -1.4259e+03 - logprior: -1.1275e+00
Fitted a model with MAP estimate = -1424.5281
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (19, 1), (38, 1), (39, 1), (44, 1), (45, 1), (46, 2), (48, 1), (63, 1), (67, 2), (68, 1), (70, 1), (79, 3), (82, 1), (83, 1), (84, 1), (90, 1), (93, 1), (105, 1), (109, 1), (110, 1), (111, 1), (113, 1), (132, 3), (133, 1), (145, 2), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (170, 1), (171, 3), (172, 1), (183, 1), (184, 1), (188, 1), (207, 1), (209, 1), (215, 2), (217, 3), (230, 3), (231, 1), (244, 1), (259, 1), (265, 1), (267, 2), (269, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 349 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 1395.0315 - loglik: -1.3930e+03 - logprior: -2.0032e+00
Epoch 2/2
39/39 - 47s - loss: 1377.3571 - loglik: -1.3767e+03 - logprior: -6.5507e-01
Fitted a model with MAP estimate = -1374.0083
expansions: []
discards: [  0   1  57  83  99 165 181 216]
Re-initialized the encoder parameters.
Fitting a model of length 341 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 48s - loss: 1383.5616 - loglik: -1.3821e+03 - logprior: -1.4851e+00
Epoch 2/2
39/39 - 44s - loss: 1378.5037 - loglik: -1.3786e+03 - logprior: 0.0590
Fitted a model with MAP estimate = -1374.8414
expansions: [(0, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 47s - loss: 1379.8673 - loglik: -1.3785e+03 - logprior: -1.3369e+00
Epoch 2/10
39/39 - 45s - loss: 1375.7498 - loglik: -1.3759e+03 - logprior: 0.1301
Epoch 3/10
39/39 - 44s - loss: 1373.4795 - loglik: -1.3735e+03 - logprior: 0.0656
Epoch 4/10
39/39 - 41s - loss: 1371.6292 - loglik: -1.3721e+03 - logprior: 0.4422
Epoch 5/10
39/39 - 36s - loss: 1371.8333 - loglik: -1.3722e+03 - logprior: 0.3350
Fitted a model with MAP estimate = -1370.5312
Time for alignment: 805.7452
Fitting a model of length 273 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1648.1934 - loglik: -1.6467e+03 - logprior: -1.4814e+00
Epoch 2/10
39/39 - 23s - loss: 1443.0187 - loglik: -1.4416e+03 - logprior: -1.4489e+00
Epoch 3/10
39/39 - 23s - loss: 1429.6364 - loglik: -1.4282e+03 - logprior: -1.3980e+00
Epoch 4/10
39/39 - 23s - loss: 1427.0522 - loglik: -1.4257e+03 - logprior: -1.3065e+00
Epoch 5/10
39/39 - 23s - loss: 1426.6125 - loglik: -1.4253e+03 - logprior: -1.2821e+00
Epoch 6/10
39/39 - 22s - loss: 1424.3611 - loglik: -1.4231e+03 - logprior: -1.2873e+00
Epoch 7/10
39/39 - 22s - loss: 1425.2731 - loglik: -1.4240e+03 - logprior: -1.2956e+00
Fitted a model with MAP estimate = -1423.1404
expansions: [(0, 2), (13, 1), (14, 1), (15, 1), (16, 1), (18, 2), (37, 1), (44, 1), (45, 1), (46, 2), (48, 1), (65, 1), (69, 1), (70, 1), (73, 1), (79, 3), (82, 1), (83, 1), (84, 1), (88, 1), (90, 1), (96, 1), (109, 1), (110, 1), (111, 1), (113, 1), (120, 1), (132, 1), (133, 1), (142, 1), (147, 1), (148, 1), (149, 1), (163, 1), (167, 1), (168, 1), (169, 1), (171, 2), (172, 1), (181, 1), (182, 1), (183, 1), (189, 1), (206, 1), (208, 1), (214, 2), (216, 1), (217, 1), (227, 1), (230, 2), (231, 1), (244, 1), (259, 1), (265, 1), (267, 1), (269, 1), (270, 1), (272, 3), (273, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 346 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1395.3176 - loglik: -1.3934e+03 - logprior: -1.9381e+00
Epoch 2/2
39/39 - 32s - loss: 1377.4349 - loglik: -1.3769e+03 - logprior: -5.6833e-01
Fitted a model with MAP estimate = -1374.0838
expansions: [(0, 2)]
discards: [  0   1  57  98 213]
Re-initialized the encoder parameters.
Fitting a model of length 343 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 34s - loss: 1381.5087 - loglik: -1.3801e+03 - logprior: -1.4182e+00
Epoch 2/2
39/39 - 31s - loss: 1377.2583 - loglik: -1.3771e+03 - logprior: -1.4203e-01
Fitted a model with MAP estimate = -1374.0426
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 10013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 35s - loss: 1381.3899 - loglik: -1.3795e+03 - logprior: -1.9368e+00
Epoch 2/10
39/39 - 31s - loss: 1376.4471 - loglik: -1.3765e+03 - logprior: 0.0080
Epoch 3/10
39/39 - 31s - loss: 1373.4301 - loglik: -1.3740e+03 - logprior: 0.5205
Epoch 4/10
39/39 - 31s - loss: 1372.9657 - loglik: -1.3733e+03 - logprior: 0.2954
Epoch 5/10
39/39 - 31s - loss: 1371.7122 - loglik: -1.3724e+03 - logprior: 0.7016
Epoch 6/10
39/39 - 31s - loss: 1370.7435 - loglik: -1.3714e+03 - logprior: 0.6748
Epoch 7/10
39/39 - 31s - loss: 1370.9479 - loglik: -1.3717e+03 - logprior: 0.7680
Fitted a model with MAP estimate = -1370.1948
Time for alignment: 681.4992
Computed alignments with likelihoods: ['-1370.5357', '-1369.7844', '-1371.0617', '-1370.5312', '-1370.1948']
Best model has likelihood: -1369.7844  (prior= 1.1343 )
time for generating output: 0.2317
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00224.projection.fasta
SP score = 0.9226523011439213
Training of 5 independent models on file PF13365.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 816.8431 - loglik: -8.1383e+02 - logprior: -3.0116e+00
Epoch 2/10
19/19 - 4s - loss: 746.0032 - loglik: -7.4477e+02 - logprior: -1.2356e+00
Epoch 3/10
19/19 - 4s - loss: 723.6955 - loglik: -7.2203e+02 - logprior: -1.6678e+00
Epoch 4/10
19/19 - 4s - loss: 719.8222 - loglik: -7.1832e+02 - logprior: -1.4994e+00
Epoch 5/10
19/19 - 4s - loss: 718.4575 - loglik: -7.1700e+02 - logprior: -1.4570e+00
Epoch 6/10
19/19 - 4s - loss: 716.8816 - loglik: -7.1546e+02 - logprior: -1.4190e+00
Epoch 7/10
19/19 - 4s - loss: 715.3568 - loglik: -7.1396e+02 - logprior: -1.3950e+00
Epoch 8/10
19/19 - 4s - loss: 715.4926 - loglik: -7.1411e+02 - logprior: -1.3795e+00
Fitted a model with MAP estimate = -712.2464
expansions: [(7, 2), (21, 1), (23, 1), (24, 1), (25, 2), (28, 1), (29, 2), (36, 1), (37, 2), (46, 3), (49, 1), (50, 1), (55, 1), (56, 1), (70, 1), (83, 2), (84, 4), (85, 3), (86, 1), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 720.5836 - loglik: -7.1749e+02 - logprior: -3.0922e+00
Epoch 2/2
39/39 - 6s - loss: 711.2679 - loglik: -7.0983e+02 - logprior: -1.4374e+00
Fitted a model with MAP estimate = -706.4728
expansions: []
discards: [ 29  36  47  58 104 137]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 8s - loss: 713.0068 - loglik: -7.1069e+02 - logprior: -2.3146e+00
Epoch 2/2
39/39 - 6s - loss: 710.1447 - loglik: -7.0892e+02 - logprior: -1.2257e+00
Fitted a model with MAP estimate = -706.3375
expansions: [(106, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 710.3735 - loglik: -7.0813e+02 - logprior: -2.2389e+00
Epoch 2/10
39/39 - 6s - loss: 707.2767 - loglik: -7.0613e+02 - logprior: -1.1496e+00
Epoch 3/10
39/39 - 6s - loss: 705.5822 - loglik: -7.0456e+02 - logprior: -1.0199e+00
Epoch 4/10
39/39 - 6s - loss: 704.2712 - loglik: -7.0336e+02 - logprior: -9.0891e-01
Epoch 5/10
39/39 - 6s - loss: 701.9022 - loglik: -7.0113e+02 - logprior: -7.7262e-01
Epoch 6/10
39/39 - 6s - loss: 701.1051 - loglik: -7.0044e+02 - logprior: -6.6958e-01
Epoch 7/10
39/39 - 6s - loss: 701.4134 - loglik: -7.0082e+02 - logprior: -5.9341e-01
Fitted a model with MAP estimate = -700.5621
Time for alignment: 141.8178
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 816.8109 - loglik: -8.1381e+02 - logprior: -3.0011e+00
Epoch 2/10
19/19 - 4s - loss: 746.1806 - loglik: -7.4495e+02 - logprior: -1.2305e+00
Epoch 3/10
19/19 - 4s - loss: 723.9283 - loglik: -7.2230e+02 - logprior: -1.6251e+00
Epoch 4/10
19/19 - 4s - loss: 719.5833 - loglik: -7.1810e+02 - logprior: -1.4819e+00
Epoch 5/10
19/19 - 4s - loss: 718.3045 - loglik: -7.1688e+02 - logprior: -1.4266e+00
Epoch 6/10
19/19 - 4s - loss: 717.1383 - loglik: -7.1574e+02 - logprior: -1.3965e+00
Epoch 7/10
19/19 - 4s - loss: 714.2775 - loglik: -7.1290e+02 - logprior: -1.3799e+00
Epoch 8/10
19/19 - 4s - loss: 715.2471 - loglik: -7.1389e+02 - logprior: -1.3620e+00
Fitted a model with MAP estimate = -712.3581
expansions: [(7, 2), (22, 2), (23, 2), (24, 1), (25, 2), (28, 1), (29, 2), (36, 1), (37, 2), (46, 3), (49, 1), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 721.2849 - loglik: -7.1814e+02 - logprior: -3.1474e+00
Epoch 2/2
39/39 - 6s - loss: 711.4317 - loglik: -7.0992e+02 - logprior: -1.5152e+00
Fitted a model with MAP estimate = -706.5763
expansions: []
discards: [ 24  26  31  38  49  62  68 139]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 713.0822 - loglik: -7.1076e+02 - logprior: -2.3182e+00
Epoch 2/2
39/39 - 6s - loss: 709.9792 - loglik: -7.0877e+02 - logprior: -1.2096e+00
Fitted a model with MAP estimate = -706.3386
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 710.5123 - loglik: -7.0829e+02 - logprior: -2.2264e+00
Epoch 2/10
39/39 - 6s - loss: 707.2643 - loglik: -7.0613e+02 - logprior: -1.1381e+00
Epoch 3/10
39/39 - 6s - loss: 706.1239 - loglik: -7.0512e+02 - logprior: -9.9925e-01
Epoch 4/10
39/39 - 6s - loss: 703.7809 - loglik: -7.0291e+02 - logprior: -8.7318e-01
Epoch 5/10
39/39 - 6s - loss: 702.0196 - loglik: -7.0127e+02 - logprior: -7.5105e-01
Epoch 6/10
39/39 - 6s - loss: 702.2144 - loglik: -7.0154e+02 - logprior: -6.7570e-01
Fitted a model with MAP estimate = -700.8716
Time for alignment: 135.0473
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 816.7053 - loglik: -8.1369e+02 - logprior: -3.0158e+00
Epoch 2/10
19/19 - 4s - loss: 746.9080 - loglik: -7.4567e+02 - logprior: -1.2373e+00
Epoch 3/10
19/19 - 4s - loss: 726.5757 - loglik: -7.2497e+02 - logprior: -1.6070e+00
Epoch 4/10
19/19 - 4s - loss: 720.1603 - loglik: -7.1871e+02 - logprior: -1.4476e+00
Epoch 5/10
19/19 - 4s - loss: 717.2186 - loglik: -7.1582e+02 - logprior: -1.3938e+00
Epoch 6/10
19/19 - 4s - loss: 718.0016 - loglik: -7.1664e+02 - logprior: -1.3594e+00
Fitted a model with MAP estimate = -713.7010
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (28, 1), (29, 2), (37, 1), (45, 1), (46, 3), (49, 1), (50, 1), (57, 1), (71, 1), (84, 2), (85, 4), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 720.1987 - loglik: -7.1711e+02 - logprior: -3.0883e+00
Epoch 2/2
39/39 - 6s - loss: 710.8318 - loglik: -7.0942e+02 - logprior: -1.4144e+00
Fitted a model with MAP estimate = -706.4212
expansions: []
discards: [ 23  30  37  57  59 106 137]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 713.0891 - loglik: -7.1078e+02 - logprior: -2.3041e+00
Epoch 2/2
39/39 - 6s - loss: 710.2449 - loglik: -7.0904e+02 - logprior: -1.2081e+00
Fitted a model with MAP estimate = -706.4793
expansions: []
discards: [100]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 710.7466 - loglik: -7.0853e+02 - logprior: -2.2170e+00
Epoch 2/10
39/39 - 6s - loss: 707.3031 - loglik: -7.0616e+02 - logprior: -1.1401e+00
Epoch 3/10
39/39 - 6s - loss: 706.4138 - loglik: -7.0541e+02 - logprior: -1.0044e+00
Epoch 4/10
39/39 - 6s - loss: 704.7358 - loglik: -7.0385e+02 - logprior: -8.8203e-01
Epoch 5/10
39/39 - 6s - loss: 702.4709 - loglik: -7.0171e+02 - logprior: -7.5880e-01
Epoch 6/10
39/39 - 6s - loss: 701.7864 - loglik: -7.0111e+02 - logprior: -6.7749e-01
Epoch 7/10
39/39 - 6s - loss: 700.0010 - loglik: -6.9940e+02 - logprior: -6.0593e-01
Epoch 8/10
39/39 - 6s - loss: 701.3932 - loglik: -7.0084e+02 - logprior: -5.5255e-01
Fitted a model with MAP estimate = -700.7569
Time for alignment: 139.2681
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 815.8936 - loglik: -8.1288e+02 - logprior: -3.0119e+00
Epoch 2/10
19/19 - 4s - loss: 748.7144 - loglik: -7.4748e+02 - logprior: -1.2340e+00
Epoch 3/10
19/19 - 4s - loss: 725.4809 - loglik: -7.2384e+02 - logprior: -1.6413e+00
Epoch 4/10
19/19 - 4s - loss: 717.6161 - loglik: -7.1613e+02 - logprior: -1.4837e+00
Epoch 5/10
19/19 - 4s - loss: 719.0833 - loglik: -7.1766e+02 - logprior: -1.4239e+00
Fitted a model with MAP estimate = -714.0322
expansions: [(7, 2), (22, 2), (23, 1), (24, 1), (25, 2), (28, 1), (35, 1), (37, 1), (45, 1), (46, 3), (49, 1), (50, 1), (57, 1), (71, 1), (84, 2), (85, 4), (86, 6), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 719.8525 - loglik: -7.1680e+02 - logprior: -3.0562e+00
Epoch 2/2
39/39 - 6s - loss: 710.9119 - loglik: -7.0953e+02 - logprior: -1.3845e+00
Fitted a model with MAP estimate = -706.2255
expansions: []
discards: [ 23  30  56  58 106 137]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 713.0091 - loglik: -7.1070e+02 - logprior: -2.3075e+00
Epoch 2/2
39/39 - 6s - loss: 710.2167 - loglik: -7.0900e+02 - logprior: -1.2183e+00
Fitted a model with MAP estimate = -706.3126
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 9s - loss: 710.3354 - loglik: -7.0810e+02 - logprior: -2.2347e+00
Epoch 2/10
39/39 - 6s - loss: 707.4528 - loglik: -7.0632e+02 - logprior: -1.1372e+00
Epoch 3/10
39/39 - 6s - loss: 705.8115 - loglik: -7.0480e+02 - logprior: -1.0158e+00
Epoch 4/10
39/39 - 6s - loss: 704.3705 - loglik: -7.0348e+02 - logprior: -8.9203e-01
Epoch 5/10
39/39 - 6s - loss: 701.9595 - loglik: -7.0119e+02 - logprior: -7.7033e-01
Epoch 6/10
39/39 - 6s - loss: 702.1475 - loglik: -7.0146e+02 - logprior: -6.9086e-01
Fitted a model with MAP estimate = -700.9412
Time for alignment: 122.6582
Fitting a model of length 112 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 816.4559 - loglik: -8.1345e+02 - logprior: -3.0087e+00
Epoch 2/10
19/19 - 4s - loss: 744.5172 - loglik: -7.4329e+02 - logprior: -1.2290e+00
Epoch 3/10
19/19 - 4s - loss: 725.9219 - loglik: -7.2432e+02 - logprior: -1.6025e+00
Epoch 4/10
19/19 - 4s - loss: 720.1075 - loglik: -7.1866e+02 - logprior: -1.4452e+00
Epoch 5/10
19/19 - 4s - loss: 718.4795 - loglik: -7.1708e+02 - logprior: -1.3967e+00
Epoch 6/10
19/19 - 4s - loss: 716.2827 - loglik: -7.1492e+02 - logprior: -1.3598e+00
Epoch 7/10
19/19 - 4s - loss: 715.0220 - loglik: -7.1368e+02 - logprior: -1.3401e+00
Epoch 8/10
19/19 - 4s - loss: 715.5276 - loglik: -7.1421e+02 - logprior: -1.3206e+00
Fitted a model with MAP estimate = -712.6014
expansions: [(7, 2), (21, 3), (22, 1), (25, 2), (30, 2), (35, 1), (37, 2), (46, 4), (49, 1), (50, 2), (57, 1), (71, 1), (84, 2), (85, 3), (86, 5), (106, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 721.0045 - loglik: -7.1787e+02 - logprior: -3.1330e+00
Epoch 2/2
39/39 - 6s - loss: 711.3863 - loglik: -7.0991e+02 - logprior: -1.4745e+00
Fitted a model with MAP estimate = -706.5792
expansions: []
discards: [ 23  30  37  48  60  67 138]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 9s - loss: 713.0214 - loglik: -7.1071e+02 - logprior: -2.3089e+00
Epoch 2/2
39/39 - 6s - loss: 709.8779 - loglik: -7.0865e+02 - logprior: -1.2263e+00
Fitted a model with MAP estimate = -706.3673
expansions: []
discards: [ 55 101]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10065 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 8s - loss: 710.7845 - loglik: -7.0857e+02 - logprior: -2.2135e+00
Epoch 2/10
39/39 - 6s - loss: 707.6474 - loglik: -7.0654e+02 - logprior: -1.1087e+00
Epoch 3/10
39/39 - 6s - loss: 705.7808 - loglik: -7.0479e+02 - logprior: -9.8639e-01
Epoch 4/10
39/39 - 6s - loss: 704.3223 - loglik: -7.0345e+02 - logprior: -8.6966e-01
Epoch 5/10
39/39 - 6s - loss: 702.9019 - loglik: -7.0216e+02 - logprior: -7.3723e-01
Epoch 6/10
39/39 - 6s - loss: 701.6036 - loglik: -7.0094e+02 - logprior: -6.6599e-01
Epoch 7/10
39/39 - 6s - loss: 701.2855 - loglik: -7.0068e+02 - logprior: -6.0699e-01
Epoch 8/10
39/39 - 6s - loss: 701.0964 - loglik: -7.0056e+02 - logprior: -5.3472e-01
Epoch 9/10
39/39 - 6s - loss: 700.1903 - loglik: -6.9970e+02 - logprior: -4.9465e-01
Epoch 10/10
39/39 - 6s - loss: 700.6504 - loglik: -7.0020e+02 - logprior: -4.4944e-01
Fitted a model with MAP estimate = -700.5388
Time for alignment: 158.1902
Computed alignments with likelihoods: ['-700.5621', '-700.8716', '-700.7569', '-700.9412', '-700.5388']
Best model has likelihood: -700.5388  (prior= -0.4529 )
time for generating output: 0.2403
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13365.projection.fasta
SP score = 0.16444840614413014
Training of 5 independent models on file PF00078.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 961.3503 - loglik: -9.5940e+02 - logprior: -1.9462e+00
Epoch 2/10
39/39 - 7s - loss: 919.9025 - loglik: -9.1865e+02 - logprior: -1.2487e+00
Epoch 3/10
39/39 - 7s - loss: 917.0736 - loglik: -9.1582e+02 - logprior: -1.2492e+00
Epoch 4/10
39/39 - 7s - loss: 914.2015 - loglik: -9.1297e+02 - logprior: -1.2330e+00
Epoch 5/10
39/39 - 7s - loss: 913.4850 - loglik: -9.1224e+02 - logprior: -1.2491e+00
Epoch 6/10
39/39 - 7s - loss: 914.3328 - loglik: -9.1308e+02 - logprior: -1.2562e+00
Fitted a model with MAP estimate = -912.6524
expansions: [(19, 2), (20, 1), (21, 3), (31, 1), (32, 1), (49, 1), (50, 2), (59, 2), (60, 4), (73, 1), (74, 1), (76, 2), (77, 4), (78, 2), (94, 1), (97, 1), (107, 3), (108, 1), (109, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 161 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 910.3208 - loglik: -9.0841e+02 - logprior: -1.9083e+00
Epoch 2/2
39/39 - 8s - loss: 903.0931 - loglik: -9.0240e+02 - logprior: -6.9779e-01
Fitted a model with MAP estimate = -901.7057
expansions: []
discards: [ 69  73  99 102]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 906.1146 - loglik: -9.0435e+02 - logprior: -1.7603e+00
Epoch 2/2
39/39 - 8s - loss: 903.1911 - loglik: -9.0263e+02 - logprior: -5.6479e-01
Fitted a model with MAP estimate = -901.6339
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 904.9457 - loglik: -9.0324e+02 - logprior: -1.7050e+00
Epoch 2/10
39/39 - 8s - loss: 902.9280 - loglik: -9.0242e+02 - logprior: -5.0453e-01
Epoch 3/10
39/39 - 8s - loss: 901.6982 - loglik: -9.0125e+02 - logprior: -4.4800e-01
Epoch 4/10
39/39 - 8s - loss: 899.6120 - loglik: -8.9921e+02 - logprior: -3.9719e-01
Epoch 5/10
39/39 - 8s - loss: 898.2806 - loglik: -8.9790e+02 - logprior: -3.8498e-01
Epoch 6/10
39/39 - 8s - loss: 898.3503 - loglik: -8.9800e+02 - logprior: -3.5109e-01
Fitted a model with MAP estimate = -897.7120
Time for alignment: 177.7695
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 962.8022 - loglik: -9.6086e+02 - logprior: -1.9396e+00
Epoch 2/10
39/39 - 7s - loss: 922.3517 - loglik: -9.2110e+02 - logprior: -1.2544e+00
Epoch 3/10
39/39 - 7s - loss: 917.5129 - loglik: -9.1627e+02 - logprior: -1.2441e+00
Epoch 4/10
39/39 - 7s - loss: 915.6879 - loglik: -9.1444e+02 - logprior: -1.2514e+00
Epoch 5/10
39/39 - 7s - loss: 915.0719 - loglik: -9.1382e+02 - logprior: -1.2563e+00
Epoch 6/10
39/39 - 7s - loss: 914.1298 - loglik: -9.1288e+02 - logprior: -1.2501e+00
Epoch 7/10
39/39 - 7s - loss: 913.8286 - loglik: -9.1257e+02 - logprior: -1.2580e+00
Epoch 8/10
39/39 - 7s - loss: 915.4501 - loglik: -9.1419e+02 - logprior: -1.2555e+00
Fitted a model with MAP estimate = -913.5222
expansions: [(19, 2), (21, 1), (22, 1), (32, 2), (33, 1), (34, 2), (48, 1), (50, 1), (51, 2), (55, 1), (75, 1), (77, 4), (78, 2), (79, 2), (98, 1), (102, 1), (105, 1), (106, 1), (107, 1), (109, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 915.3304 - loglik: -9.1344e+02 - logprior: -1.8913e+00
Epoch 2/2
39/39 - 8s - loss: 908.5222 - loglik: -9.0783e+02 - logprior: -6.9656e-01
Fitted a model with MAP estimate = -906.8815
expansions: [(99, 1), (100, 1)]
discards: [41 93 94 96]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 910.3144 - loglik: -9.0857e+02 - logprior: -1.7447e+00
Epoch 2/2
39/39 - 8s - loss: 907.8893 - loglik: -9.0733e+02 - logprior: -5.5626e-01
Fitted a model with MAP estimate = -906.3548
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 155 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 909.5244 - loglik: -9.0781e+02 - logprior: -1.7111e+00
Epoch 2/10
39/39 - 8s - loss: 907.2083 - loglik: -9.0671e+02 - logprior: -4.9574e-01
Epoch 3/10
39/39 - 8s - loss: 905.5897 - loglik: -9.0515e+02 - logprior: -4.3693e-01
Epoch 4/10
39/39 - 8s - loss: 903.4280 - loglik: -9.0305e+02 - logprior: -3.8234e-01
Epoch 5/10
39/39 - 8s - loss: 902.3577 - loglik: -9.0200e+02 - logprior: -3.6216e-01
Epoch 6/10
39/39 - 8s - loss: 902.2939 - loglik: -9.0197e+02 - logprior: -3.2876e-01
Epoch 7/10
39/39 - 8s - loss: 902.3009 - loglik: -9.0201e+02 - logprior: -2.8879e-01
Fitted a model with MAP estimate = -901.4214
Time for alignment: 194.2692
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 961.8193 - loglik: -9.5989e+02 - logprior: -1.9301e+00
Epoch 2/10
39/39 - 7s - loss: 921.0964 - loglik: -9.1985e+02 - logprior: -1.2493e+00
Epoch 3/10
39/39 - 7s - loss: 916.0541 - loglik: -9.1475e+02 - logprior: -1.3086e+00
Epoch 4/10
39/39 - 7s - loss: 913.9556 - loglik: -9.1264e+02 - logprior: -1.3171e+00
Epoch 5/10
39/39 - 7s - loss: 911.9128 - loglik: -9.1059e+02 - logprior: -1.3227e+00
Epoch 6/10
39/39 - 7s - loss: 911.9081 - loglik: -9.1057e+02 - logprior: -1.3350e+00
Epoch 7/10
39/39 - 7s - loss: 911.2017 - loglik: -9.0987e+02 - logprior: -1.3348e+00
Epoch 8/10
39/39 - 7s - loss: 912.2997 - loglik: -9.1096e+02 - logprior: -1.3351e+00
Fitted a model with MAP estimate = -910.9472
expansions: [(19, 2), (20, 1), (31, 4), (32, 1), (33, 1), (46, 1), (47, 1), (48, 1), (49, 2), (74, 2), (76, 1), (79, 2), (80, 8), (94, 1), (97, 1), (107, 1), (108, 1), (117, 1), (118, 9)]
discards: [ 1 58 59]
Re-initialized the encoder parameters.
Fitting a model of length 166 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 915.6811 - loglik: -9.1373e+02 - logprior: -1.9489e+00
Epoch 2/2
39/39 - 9s - loss: 907.9741 - loglik: -9.0721e+02 - logprior: -7.6079e-01
Fitted a model with MAP estimate = -905.8435
expansions: []
discards: [ 35  57  85  98  99 100 101 102 148 149 150 151 152 153 154]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 912.5040 - loglik: -9.1072e+02 - logprior: -1.7819e+00
Epoch 2/2
39/39 - 8s - loss: 909.7647 - loglik: -9.0918e+02 - logprior: -5.8012e-01
Fitted a model with MAP estimate = -907.8864
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 911.3892 - loglik: -9.0968e+02 - logprior: -1.7079e+00
Epoch 2/10
39/39 - 8s - loss: 908.8444 - loglik: -9.0835e+02 - logprior: -4.9665e-01
Epoch 3/10
39/39 - 8s - loss: 907.4932 - loglik: -9.0706e+02 - logprior: -4.3455e-01
Epoch 4/10
39/39 - 8s - loss: 905.6213 - loglik: -9.0522e+02 - logprior: -4.0017e-01
Epoch 5/10
39/39 - 8s - loss: 903.9250 - loglik: -9.0355e+02 - logprior: -3.7010e-01
Epoch 6/10
39/39 - 8s - loss: 903.5345 - loglik: -9.0320e+02 - logprior: -3.3760e-01
Epoch 7/10
39/39 - 8s - loss: 902.8298 - loglik: -9.0252e+02 - logprior: -3.0812e-01
Epoch 8/10
39/39 - 8s - loss: 903.0955 - loglik: -9.0283e+02 - logprior: -2.6905e-01
Fitted a model with MAP estimate = -902.7567
Time for alignment: 203.7920
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 960.4223 - loglik: -9.5847e+02 - logprior: -1.9519e+00
Epoch 2/10
39/39 - 7s - loss: 919.3365 - loglik: -9.1800e+02 - logprior: -1.3356e+00
Epoch 3/10
39/39 - 7s - loss: 914.9044 - loglik: -9.1356e+02 - logprior: -1.3430e+00
Epoch 4/10
39/39 - 7s - loss: 913.5653 - loglik: -9.1224e+02 - logprior: -1.3302e+00
Epoch 5/10
39/39 - 7s - loss: 911.3819 - loglik: -9.1005e+02 - logprior: -1.3362e+00
Epoch 6/10
39/39 - 7s - loss: 912.5197 - loglik: -9.1117e+02 - logprior: -1.3495e+00
Fitted a model with MAP estimate = -911.1910
expansions: [(19, 1), (21, 1), (23, 1), (32, 1), (33, 3), (34, 3), (35, 1), (46, 1), (47, 1), (48, 1), (49, 2), (55, 2), (58, 2), (59, 1), (73, 1), (76, 2), (77, 4), (78, 1), (92, 1), (93, 1), (97, 1), (106, 1), (107, 1), (109, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 163 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 909.2299 - loglik: -9.0737e+02 - logprior: -1.8554e+00
Epoch 2/2
39/39 - 8s - loss: 902.7158 - loglik: -9.0209e+02 - logprior: -6.2944e-01
Fitted a model with MAP estimate = -900.9062
expansions: []
discards: [ 59  76 101]
Re-initialized the encoder parameters.
Fitting a model of length 160 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 905.2736 - loglik: -9.0355e+02 - logprior: -1.7194e+00
Epoch 2/2
39/39 - 8s - loss: 903.1160 - loglik: -9.0258e+02 - logprior: -5.3099e-01
Fitted a model with MAP estimate = -900.9814
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 904.7356 - loglik: -9.0307e+02 - logprior: -1.6704e+00
Epoch 2/10
39/39 - 8s - loss: 902.1392 - loglik: -9.0168e+02 - logprior: -4.6142e-01
Epoch 3/10
39/39 - 8s - loss: 900.5716 - loglik: -9.0016e+02 - logprior: -4.1082e-01
Epoch 4/10
39/39 - 8s - loss: 898.4050 - loglik: -8.9804e+02 - logprior: -3.6213e-01
Epoch 5/10
39/39 - 8s - loss: 898.3361 - loglik: -8.9800e+02 - logprior: -3.4029e-01
Epoch 6/10
39/39 - 8s - loss: 897.0556 - loglik: -8.9674e+02 - logprior: -3.1161e-01
Epoch 7/10
39/39 - 8s - loss: 896.3842 - loglik: -8.9611e+02 - logprior: -2.7287e-01
Epoch 8/10
39/39 - 8s - loss: 895.4775 - loglik: -8.9523e+02 - logprior: -2.5163e-01
Epoch 9/10
39/39 - 8s - loss: 897.3917 - loglik: -8.9719e+02 - logprior: -2.0234e-01
Fitted a model with MAP estimate = -895.9404
Time for alignment: 202.2792
Fitting a model of length 128 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 961.7761 - loglik: -9.5985e+02 - logprior: -1.9279e+00
Epoch 2/10
39/39 - 7s - loss: 921.6390 - loglik: -9.2041e+02 - logprior: -1.2310e+00
Epoch 3/10
39/39 - 7s - loss: 917.9586 - loglik: -9.1675e+02 - logprior: -1.2041e+00
Epoch 4/10
39/39 - 7s - loss: 915.4961 - loglik: -9.1430e+02 - logprior: -1.1971e+00
Epoch 5/10
39/39 - 7s - loss: 914.6217 - loglik: -9.1342e+02 - logprior: -1.2051e+00
Epoch 6/10
39/39 - 7s - loss: 913.8280 - loglik: -9.1262e+02 - logprior: -1.2114e+00
Epoch 7/10
39/39 - 7s - loss: 914.2577 - loglik: -9.1304e+02 - logprior: -1.2154e+00
Fitted a model with MAP estimate = -913.3660
expansions: [(23, 1), (31, 1), (32, 15), (33, 3), (46, 1), (47, 3), (48, 2), (75, 2), (76, 3), (94, 1), (97, 3), (105, 1), (106, 1), (108, 1)]
discards: [ 1 57]
Re-initialized the encoder parameters.
Fitting a model of length 164 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 916.0432 - loglik: -9.1415e+02 - logprior: -1.8914e+00
Epoch 2/2
39/39 - 8s - loss: 908.1266 - loglik: -9.0744e+02 - logprior: -6.8245e-01
Fitted a model with MAP estimate = -906.3892
expansions: []
discards: [35 36 37 38 39 40 41 42 43 44 45 68]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 10s - loss: 911.8828 - loglik: -9.1015e+02 - logprior: -1.7372e+00
Epoch 2/2
39/39 - 8s - loss: 908.8056 - loglik: -9.0825e+02 - logprior: -5.5053e-01
Fitted a model with MAP estimate = -907.3390
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 152 on 10006 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 910.6864 - loglik: -9.0900e+02 - logprior: -1.6821e+00
Epoch 2/10
39/39 - 8s - loss: 908.5370 - loglik: -9.0805e+02 - logprior: -4.8769e-01
Epoch 3/10
39/39 - 8s - loss: 907.0619 - loglik: -9.0664e+02 - logprior: -4.2075e-01
Epoch 4/10
39/39 - 8s - loss: 904.8742 - loglik: -9.0448e+02 - logprior: -3.9029e-01
Epoch 5/10
39/39 - 8s - loss: 904.2542 - loglik: -9.0389e+02 - logprior: -3.6317e-01
Epoch 6/10
39/39 - 8s - loss: 903.3091 - loglik: -9.0297e+02 - logprior: -3.3643e-01
Epoch 7/10
39/39 - 8s - loss: 902.3205 - loglik: -9.0202e+02 - logprior: -2.9859e-01
Epoch 8/10
39/39 - 8s - loss: 903.2950 - loglik: -9.0302e+02 - logprior: -2.7207e-01
Fitted a model with MAP estimate = -902.3484
Time for alignment: 195.5196
Computed alignments with likelihoods: ['-897.7120', '-901.4214', '-902.7567', '-895.9404', '-902.3484']
Best model has likelihood: -895.9404  (prior= -0.1920 )
time for generating output: 0.1930
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00078.projection.fasta
SP score = 0.9218812275807095
Training of 5 independent models on file PF00150.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1630.4288 - loglik: -1.6290e+03 - logprior: -1.4004e+00
Epoch 2/10
39/39 - 18s - loss: 1558.8702 - loglik: -1.5582e+03 - logprior: -6.8216e-01
Epoch 3/10
39/39 - 19s - loss: 1550.8712 - loglik: -1.5501e+03 - logprior: -8.0219e-01
Epoch 4/10
39/39 - 19s - loss: 1548.9427 - loglik: -1.5481e+03 - logprior: -8.0261e-01
Epoch 5/10
39/39 - 19s - loss: 1546.8680 - loglik: -1.5460e+03 - logprior: -8.4530e-01
Epoch 6/10
39/39 - 19s - loss: 1546.3727 - loglik: -1.5455e+03 - logprior: -8.6150e-01
Epoch 7/10
39/39 - 19s - loss: 1546.5940 - loglik: -1.5457e+03 - logprior: -8.8747e-01
Fitted a model with MAP estimate = -1544.3776
expansions: [(0, 3), (9, 1), (10, 1), (19, 1), (23, 1), (43, 3), (56, 1), (61, 1), (73, 1), (84, 3), (85, 2), (86, 1), (87, 3), (95, 1), (102, 1), (115, 4), (116, 2), (117, 1), (121, 1), (153, 2), (158, 2), (176, 1), (179, 6), (183, 1), (184, 1), (206, 7), (207, 1), (214, 1)]
discards: [172]
Re-initialized the encoder parameters.
Fitting a model of length 282 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1548.6660 - loglik: -1.5466e+03 - logprior: -2.1086e+00
Epoch 2/2
39/39 - 25s - loss: 1539.2313 - loglik: -1.5387e+03 - logprior: -5.0598e-01
Fitted a model with MAP estimate = -1536.2519
expansions: [(0, 3), (101, 1), (105, 1), (107, 1), (208, 1)]
discards: [  1   2   3   4  16  51 141 142 143 193 251 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1543.3607 - loglik: -1.5410e+03 - logprior: -2.3407e+00
Epoch 2/2
39/39 - 24s - loss: 1538.7208 - loglik: -1.5384e+03 - logprior: -3.3499e-01
Fitted a model with MAP estimate = -1535.7405
expansions: [(0, 3), (186, 3), (187, 1), (248, 1)]
discards: [ 0  1  2 89 90 92 93 94]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1541.7156 - loglik: -1.5402e+03 - logprior: -1.4768e+00
Epoch 2/10
39/39 - 24s - loss: 1537.9646 - loglik: -1.5378e+03 - logprior: -1.8331e-01
Epoch 3/10
39/39 - 24s - loss: 1536.1510 - loglik: -1.5360e+03 - logprior: -1.0832e-01
Epoch 4/10
39/39 - 24s - loss: 1534.3754 - loglik: -1.5343e+03 - logprior: -2.8046e-02
Epoch 5/10
39/39 - 24s - loss: 1532.8328 - loglik: -1.5329e+03 - logprior: 0.0250
Epoch 6/10
39/39 - 24s - loss: 1533.1732 - loglik: -1.5333e+03 - logprior: 0.0824
Fitted a model with MAP estimate = -1532.0535
Time for alignment: 506.5071
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 22s - loss: 1630.5602 - loglik: -1.6292e+03 - logprior: -1.3985e+00
Epoch 2/10
39/39 - 18s - loss: 1562.9047 - loglik: -1.5622e+03 - logprior: -6.9600e-01
Epoch 3/10
39/39 - 18s - loss: 1554.3854 - loglik: -1.5535e+03 - logprior: -8.5751e-01
Epoch 4/10
39/39 - 18s - loss: 1551.3086 - loglik: -1.5504e+03 - logprior: -9.1341e-01
Epoch 5/10
39/39 - 18s - loss: 1549.1213 - loglik: -1.5482e+03 - logprior: -9.5146e-01
Epoch 6/10
39/39 - 19s - loss: 1548.2214 - loglik: -1.5473e+03 - logprior: -9.5100e-01
Epoch 7/10
39/39 - 19s - loss: 1548.4155 - loglik: -1.5475e+03 - logprior: -9.5431e-01
Fitted a model with MAP estimate = -1546.3387
expansions: [(0, 3), (9, 1), (10, 1), (23, 3), (43, 1), (51, 2), (52, 1), (81, 1), (85, 5), (86, 3), (87, 2), (88, 3), (95, 1), (98, 1), (104, 1), (115, 2), (118, 2), (121, 1), (124, 1), (125, 1), (126, 1), (149, 1), (153, 3), (178, 7), (181, 1), (206, 6), (209, 1), (214, 1)]
discards: [ 77  78 156 157 168]
Re-initialized the encoder parameters.
Fitting a model of length 281 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1551.0811 - loglik: -1.5489e+03 - logprior: -2.1475e+00
Epoch 2/2
39/39 - 25s - loss: 1540.8794 - loglik: -1.5403e+03 - logprior: -5.9084e-01
Fitted a model with MAP estimate = -1537.3281
expansions: [(0, 3), (110, 1)]
discards: [  1   2   3  29  60 102 103 104 105 147 183 191 194 215 253]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1545.0062 - loglik: -1.5426e+03 - logprior: -2.3634e+00
Epoch 2/2
39/39 - 24s - loss: 1540.5377 - loglik: -1.5401e+03 - logprior: -4.1117e-01
Fitted a model with MAP estimate = -1537.5520
expansions: [(180, 1), (183, 4), (203, 2), (204, 1), (206, 1)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1540.7643 - loglik: -1.5395e+03 - logprior: -1.2995e+00
Epoch 2/10
39/39 - 24s - loss: 1537.8782 - loglik: -1.5377e+03 - logprior: -1.7226e-01
Epoch 3/10
39/39 - 24s - loss: 1535.8333 - loglik: -1.5358e+03 - logprior: -8.1947e-02
Epoch 4/10
39/39 - 24s - loss: 1534.3842 - loglik: -1.5344e+03 - logprior: -7.0601e-03
Epoch 5/10
39/39 - 24s - loss: 1532.8711 - loglik: -1.5329e+03 - logprior: 0.0495
Epoch 6/10
39/39 - 24s - loss: 1531.1561 - loglik: -1.5313e+03 - logprior: 0.1279
Epoch 7/10
39/39 - 24s - loss: 1532.9756 - loglik: -1.5332e+03 - logprior: 0.1877
Fitted a model with MAP estimate = -1531.5599
Time for alignment: 525.7055
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1630.3547 - loglik: -1.6290e+03 - logprior: -1.3931e+00
Epoch 2/10
39/39 - 19s - loss: 1561.4796 - loglik: -1.5609e+03 - logprior: -5.7747e-01
Epoch 3/10
39/39 - 18s - loss: 1551.7043 - loglik: -1.5511e+03 - logprior: -6.4977e-01
Epoch 4/10
39/39 - 18s - loss: 1548.7266 - loglik: -1.5481e+03 - logprior: -6.4648e-01
Epoch 5/10
39/39 - 18s - loss: 1547.4501 - loglik: -1.5468e+03 - logprior: -6.4752e-01
Epoch 6/10
39/39 - 19s - loss: 1546.3811 - loglik: -1.5457e+03 - logprior: -6.5093e-01
Epoch 7/10
39/39 - 18s - loss: 1545.6281 - loglik: -1.5450e+03 - logprior: -6.6968e-01
Epoch 8/10
39/39 - 18s - loss: 1546.1090 - loglik: -1.5454e+03 - logprior: -6.6849e-01
Fitted a model with MAP estimate = -1543.9487
expansions: [(0, 4), (40, 1), (43, 1), (51, 2), (53, 1), (61, 1), (85, 8), (89, 6), (90, 1), (99, 2), (117, 4), (118, 2), (119, 1), (122, 1), (127, 1), (154, 2), (156, 3), (157, 1), (159, 1), (178, 1), (179, 4), (180, 1), (206, 7), (209, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 287 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1548.7384 - loglik: -1.5466e+03 - logprior: -2.1046e+00
Epoch 2/2
39/39 - 26s - loss: 1538.9834 - loglik: -1.5386e+03 - logprior: -4.3308e-01
Fitted a model with MAP estimate = -1535.6629
expansions: [(0, 3)]
discards: [  1   2  57  97 109 110 125 146 147 148 196 197 198 199 200 201 256 259
 260]
Re-initialized the encoder parameters.
Fitting a model of length 271 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1544.0865 - loglik: -1.5417e+03 - logprior: -2.3419e+00
Epoch 2/2
39/39 - 24s - loss: 1539.4908 - loglik: -1.5391e+03 - logprior: -3.4372e-01
Fitted a model with MAP estimate = -1536.5221
expansions: [(0, 3), (243, 1)]
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1541.8344 - loglik: -1.5393e+03 - logprior: -2.5206e+00
Epoch 2/10
39/39 - 24s - loss: 1537.4355 - loglik: -1.5371e+03 - logprior: -3.1752e-01
Epoch 3/10
39/39 - 24s - loss: 1536.1927 - loglik: -1.5361e+03 - logprior: -5.4956e-02
Epoch 4/10
39/39 - 24s - loss: 1534.1510 - loglik: -1.5342e+03 - logprior: 0.0185
Epoch 5/10
39/39 - 24s - loss: 1532.5604 - loglik: -1.5326e+03 - logprior: 0.0778
Epoch 6/10
39/39 - 24s - loss: 1532.0464 - loglik: -1.5322e+03 - logprior: 0.1546
Epoch 7/10
39/39 - 24s - loss: 1532.0521 - loglik: -1.5323e+03 - logprior: 0.2261
Fitted a model with MAP estimate = -1531.5017
Time for alignment: 542.6152
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1630.0737 - loglik: -1.6287e+03 - logprior: -1.3937e+00
Epoch 2/10
39/39 - 18s - loss: 1560.8158 - loglik: -1.5602e+03 - logprior: -6.4765e-01
Epoch 3/10
39/39 - 18s - loss: 1552.7960 - loglik: -1.5521e+03 - logprior: -7.2643e-01
Epoch 4/10
39/39 - 19s - loss: 1550.9495 - loglik: -1.5502e+03 - logprior: -7.3184e-01
Epoch 5/10
39/39 - 18s - loss: 1549.3953 - loglik: -1.5487e+03 - logprior: -7.3884e-01
Epoch 6/10
39/39 - 18s - loss: 1548.5768 - loglik: -1.5478e+03 - logprior: -7.4480e-01
Epoch 7/10
39/39 - 19s - loss: 1549.2687 - loglik: -1.5485e+03 - logprior: -7.4133e-01
Fitted a model with MAP estimate = -1547.2039
expansions: [(0, 3), (9, 2), (24, 1), (40, 1), (43, 2), (50, 2), (52, 1), (62, 1), (81, 1), (85, 1), (92, 15), (104, 1), (115, 3), (116, 2), (122, 1), (126, 1), (153, 2), (159, 2), (184, 3), (207, 6), (208, 1), (209, 1), (214, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 283 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1550.1543 - loglik: -1.5481e+03 - logprior: -2.0560e+00
Epoch 2/2
39/39 - 25s - loss: 1540.2721 - loglik: -1.5398e+03 - logprior: -4.7106e-01
Fitted a model with MAP estimate = -1537.1368
expansions: [(0, 3), (195, 3), (228, 1)]
discards: [  1   2   3   4  12  60 111 112 113 114 150 198 199 255]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1544.2865 - loglik: -1.5420e+03 - logprior: -2.2669e+00
Epoch 2/2
39/39 - 24s - loss: 1539.3409 - loglik: -1.5391e+03 - logprior: -2.8949e-01
Fitted a model with MAP estimate = -1536.6105
expansions: [(0, 3), (216, 3)]
discards: [  0  48 185 192 203 249]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 27s - loss: 1541.8571 - loglik: -1.5404e+03 - logprior: -1.4690e+00
Epoch 2/10
39/39 - 24s - loss: 1538.0087 - loglik: -1.5378e+03 - logprior: -2.1627e-01
Epoch 3/10
39/39 - 24s - loss: 1535.5404 - loglik: -1.5354e+03 - logprior: -1.2967e-01
Epoch 4/10
39/39 - 24s - loss: 1533.6505 - loglik: -1.5336e+03 - logprior: -6.0657e-02
Epoch 5/10
39/39 - 24s - loss: 1533.0416 - loglik: -1.5331e+03 - logprior: 0.0100
Epoch 6/10
39/39 - 24s - loss: 1531.2565 - loglik: -1.5313e+03 - logprior: 0.0743
Epoch 7/10
39/39 - 24s - loss: 1532.7892 - loglik: -1.5329e+03 - logprior: 0.1513
Fitted a model with MAP estimate = -1531.3356
Time for alignment: 527.1525
Fitting a model of length 229 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 21s - loss: 1630.8268 - loglik: -1.6294e+03 - logprior: -1.3936e+00
Epoch 2/10
39/39 - 18s - loss: 1561.8312 - loglik: -1.5612e+03 - logprior: -6.3354e-01
Epoch 3/10
39/39 - 18s - loss: 1552.7834 - loglik: -1.5521e+03 - logprior: -7.2883e-01
Epoch 4/10
39/39 - 19s - loss: 1550.0355 - loglik: -1.5493e+03 - logprior: -7.1988e-01
Epoch 5/10
39/39 - 19s - loss: 1548.6973 - loglik: -1.5480e+03 - logprior: -7.3034e-01
Epoch 6/10
39/39 - 18s - loss: 1547.7068 - loglik: -1.5470e+03 - logprior: -7.3969e-01
Epoch 7/10
39/39 - 18s - loss: 1547.9545 - loglik: -1.5472e+03 - logprior: -7.5111e-01
Fitted a model with MAP estimate = -1546.2967
expansions: [(0, 3), (9, 2), (10, 1), (40, 1), (42, 1), (50, 1), (52, 1), (61, 1), (86, 1), (91, 2), (92, 9), (97, 2), (115, 4), (116, 2), (117, 1), (121, 1), (153, 2), (176, 1), (178, 4), (179, 1), (206, 5), (208, 1), (214, 1)]
discards: [157 172]
Re-initialized the encoder parameters.
Fitting a model of length 275 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 28s - loss: 1550.4967 - loglik: -1.5484e+03 - logprior: -2.1198e+00
Epoch 2/2
39/39 - 24s - loss: 1541.6432 - loglik: -1.5411e+03 - logprior: -5.1857e-01
Fitted a model with MAP estimate = -1538.6106
expansions: [(190, 2)]
discards: [  1   2   3   4  12 121 142 143 144 246 247]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1544.6091 - loglik: -1.5432e+03 - logprior: -1.4251e+00
Epoch 2/2
39/39 - 23s - loss: 1541.1671 - loglik: -1.5410e+03 - logprior: -2.1461e-01
Fitted a model with MAP estimate = -1538.3201
expansions: [(0, 4), (199, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 272 on 10011 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 26s - loss: 1542.8613 - loglik: -1.5407e+03 - logprior: -2.1496e+00
Epoch 2/10
39/39 - 24s - loss: 1538.9528 - loglik: -1.5388e+03 - logprior: -1.8224e-01
Epoch 3/10
39/39 - 24s - loss: 1537.0929 - loglik: -1.5370e+03 - logprior: -5.0272e-02
Epoch 4/10
39/39 - 23s - loss: 1535.4231 - loglik: -1.5354e+03 - logprior: 0.0213
Epoch 5/10
39/39 - 24s - loss: 1533.9032 - loglik: -1.5340e+03 - logprior: 0.0903
Epoch 6/10
39/39 - 24s - loss: 1533.7532 - loglik: -1.5339e+03 - logprior: 0.1619
Epoch 7/10
39/39 - 24s - loss: 1533.0974 - loglik: -1.5333e+03 - logprior: 0.2364
Epoch 8/10
39/39 - 24s - loss: 1534.0438 - loglik: -1.5343e+03 - logprior: 0.3057
Fitted a model with MAP estimate = -1532.5626
Time for alignment: 538.8890
Computed alignments with likelihoods: ['-1532.0535', '-1531.5599', '-1531.5017', '-1531.3356', '-1532.5626']
Best model has likelihood: -1531.3356  (prior= 0.1725 )
time for generating output: 0.3595
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF00150.projection.fasta
SP score = 0.5809310986964619
Training of 5 independent models on file PF13561.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 14s - loss: 1233.2178 - loglik: -1.2314e+03 - logprior: -1.8071e+00
Epoch 2/10
39/39 - 10s - loss: 1138.5914 - loglik: -1.1370e+03 - logprior: -1.6222e+00
Epoch 3/10
39/39 - 10s - loss: 1131.2565 - loglik: -1.1296e+03 - logprior: -1.6227e+00
Epoch 4/10
39/39 - 10s - loss: 1129.5020 - loglik: -1.1279e+03 - logprior: -1.5555e+00
Epoch 5/10
39/39 - 10s - loss: 1129.3005 - loglik: -1.1278e+03 - logprior: -1.5420e+00
Epoch 6/10
39/39 - 10s - loss: 1129.1196 - loglik: -1.1276e+03 - logprior: -1.5461e+00
Epoch 7/10
39/39 - 10s - loss: 1129.0002 - loglik: -1.1274e+03 - logprior: -1.5511e+00
Epoch 8/10
39/39 - 10s - loss: 1129.1517 - loglik: -1.1276e+03 - logprior: -1.5582e+00
Fitted a model with MAP estimate = -1122.2182
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 2), (35, 1), (36, 2), (37, 2), (44, 1), (45, 1), (46, 1), (48, 1), (65, 1), (66, 2), (69, 1), (70, 1), (83, 1), (86, 1), (87, 1), (88, 1), (93, 1), (96, 1), (108, 1), (109, 2), (112, 1), (126, 1), (128, 1), (132, 1), (149, 4), (150, 2), (153, 1), (154, 1), (155, 1), (156, 1), (163, 1), (168, 1), (176, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1121.9149 - loglik: -1.1194e+03 - logprior: -2.5578e+00
Epoch 2/2
39/39 - 13s - loss: 1111.5414 - loglik: -1.1104e+03 - logprior: -1.1133e+00
Fitted a model with MAP estimate = -1103.0750
expansions: [(3, 1)]
discards: [  0  26  42  50  86 140]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1115.6680 - loglik: -1.1133e+03 - logprior: -2.3759e+00
Epoch 2/2
39/39 - 13s - loss: 1111.0413 - loglik: -1.1104e+03 - logprior: -6.7203e-01
Fitted a model with MAP estimate = -1103.1510
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1108.8594 - loglik: -1.1066e+03 - logprior: -2.2976e+00
Epoch 2/10
39/39 - 13s - loss: 1105.2363 - loglik: -1.1042e+03 - logprior: -1.0373e+00
Epoch 3/10
39/39 - 13s - loss: 1103.0363 - loglik: -1.1029e+03 - logprior: -1.7175e-01
Epoch 4/10
39/39 - 13s - loss: 1101.5721 - loglik: -1.1015e+03 - logprior: -1.0443e-01
Epoch 5/10
39/39 - 13s - loss: 1101.0676 - loglik: -1.1010e+03 - logprior: -4.3576e-02
Epoch 6/10
39/39 - 13s - loss: 1101.0885 - loglik: -1.1011e+03 - logprior: 0.0281
Fitted a model with MAP estimate = -1100.7133
Time for alignment: 294.0123
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1233.3112 - loglik: -1.2315e+03 - logprior: -1.8128e+00
Epoch 2/10
39/39 - 10s - loss: 1139.3750 - loglik: -1.1378e+03 - logprior: -1.6020e+00
Epoch 3/10
39/39 - 10s - loss: 1132.2905 - loglik: -1.1307e+03 - logprior: -1.6318e+00
Epoch 4/10
39/39 - 10s - loss: 1130.0374 - loglik: -1.1284e+03 - logprior: -1.6041e+00
Epoch 5/10
39/39 - 10s - loss: 1130.0247 - loglik: -1.1284e+03 - logprior: -1.6011e+00
Epoch 6/10
39/39 - 10s - loss: 1129.4193 - loglik: -1.1278e+03 - logprior: -1.6125e+00
Epoch 7/10
39/39 - 10s - loss: 1129.1074 - loglik: -1.1275e+03 - logprior: -1.6190e+00
Epoch 8/10
39/39 - 10s - loss: 1129.0132 - loglik: -1.1274e+03 - logprior: -1.6283e+00
Epoch 9/10
39/39 - 10s - loss: 1129.1780 - loglik: -1.1276e+03 - logprior: -1.6199e+00
Fitted a model with MAP estimate = -1122.2769
expansions: [(12, 1), (15, 1), (22, 1), (24, 2), (26, 1), (27, 1), (31, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (45, 1), (46, 1), (47, 1), (49, 1), (66, 2), (67, 2), (71, 1), (77, 1), (87, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (111, 2), (112, 1), (126, 1), (128, 1), (149, 2), (150, 3), (151, 2), (153, 3), (154, 1), (155, 1), (156, 1), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 1122.0110 - loglik: -1.1195e+03 - logprior: -2.5489e+00
Epoch 2/2
39/39 - 13s - loss: 1111.7002 - loglik: -1.1105e+03 - logprior: -1.1524e+00
Fitted a model with MAP estimate = -1103.1622
expansions: [(3, 1)]
discards: [  0  27  84 139 142 190 191 195]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1115.8251 - loglik: -1.1135e+03 - logprior: -2.3499e+00
Epoch 2/2
39/39 - 13s - loss: 1111.3540 - loglik: -1.1107e+03 - logprior: -6.6389e-01
Fitted a model with MAP estimate = -1103.2367
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1109.0735 - loglik: -1.1068e+03 - logprior: -2.2786e+00
Epoch 2/10
39/39 - 13s - loss: 1105.4437 - loglik: -1.1044e+03 - logprior: -1.0069e+00
Epoch 3/10
39/39 - 13s - loss: 1103.2083 - loglik: -1.1031e+03 - logprior: -1.5219e-01
Epoch 4/10
39/39 - 13s - loss: 1101.5862 - loglik: -1.1015e+03 - logprior: -7.8517e-02
Epoch 5/10
39/39 - 13s - loss: 1101.4042 - loglik: -1.1014e+03 - logprior: -1.1966e-02
Epoch 6/10
39/39 - 13s - loss: 1101.1180 - loglik: -1.1012e+03 - logprior: 0.0599
Epoch 7/10
39/39 - 13s - loss: 1101.0356 - loglik: -1.1012e+03 - logprior: 0.1288
Epoch 8/10
39/39 - 13s - loss: 1102.1846 - loglik: -1.1024e+03 - logprior: 0.2071
Fitted a model with MAP estimate = -1100.6705
Time for alignment: 328.1284
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 13s - loss: 1233.4757 - loglik: -1.2317e+03 - logprior: -1.8027e+00
Epoch 2/10
39/39 - 10s - loss: 1139.3728 - loglik: -1.1378e+03 - logprior: -1.6066e+00
Epoch 3/10
39/39 - 10s - loss: 1131.3318 - loglik: -1.1297e+03 - logprior: -1.6653e+00
Epoch 4/10
39/39 - 10s - loss: 1129.1414 - loglik: -1.1275e+03 - logprior: -1.6181e+00
Epoch 5/10
39/39 - 10s - loss: 1129.1499 - loglik: -1.1275e+03 - logprior: -1.6159e+00
Fitted a model with MAP estimate = -1122.0352
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (27, 1), (31, 1), (32, 1), (33, 2), (35, 1), (36, 2), (37, 2), (42, 1), (45, 1), (46, 1), (48, 1), (66, 1), (67, 2), (71, 1), (77, 1), (83, 1), (86, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (149, 2), (150, 1), (151, 3), (153, 3), (154, 1), (155, 1), (156, 1), (163, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1121.9309 - loglik: -1.1193e+03 - logprior: -2.6146e+00
Epoch 2/2
39/39 - 13s - loss: 1111.7252 - loglik: -1.1106e+03 - logprior: -1.0840e+00
Fitted a model with MAP estimate = -1103.2749
expansions: [(3, 1)]
discards: [  0  26  42  50  87 141 144 192 196]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1115.8566 - loglik: -1.1135e+03 - logprior: -2.3838e+00
Epoch 2/2
39/39 - 13s - loss: 1111.3280 - loglik: -1.1107e+03 - logprior: -6.2641e-01
Fitted a model with MAP estimate = -1103.2747
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 17s - loss: 1109.3541 - loglik: -1.1070e+03 - logprior: -2.3100e+00
Epoch 2/10
39/39 - 13s - loss: 1105.2068 - loglik: -1.1042e+03 - logprior: -1.0260e+00
Epoch 3/10
39/39 - 13s - loss: 1102.8008 - loglik: -1.1026e+03 - logprior: -1.8298e-01
Epoch 4/10
39/39 - 13s - loss: 1101.6298 - loglik: -1.1015e+03 - logprior: -1.2091e-01
Epoch 5/10
39/39 - 13s - loss: 1101.5598 - loglik: -1.1015e+03 - logprior: -5.7394e-02
Epoch 6/10
39/39 - 13s - loss: 1101.7808 - loglik: -1.1018e+03 - logprior: 0.0135
Fitted a model with MAP estimate = -1100.9164
Time for alignment: 264.6952
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1233.2804 - loglik: -1.2315e+03 - logprior: -1.8110e+00
Epoch 2/10
39/39 - 10s - loss: 1140.3346 - loglik: -1.1387e+03 - logprior: -1.5946e+00
Epoch 3/10
39/39 - 10s - loss: 1132.1857 - loglik: -1.1306e+03 - logprior: -1.6196e+00
Epoch 4/10
39/39 - 10s - loss: 1130.8217 - loglik: -1.1293e+03 - logprior: -1.5691e+00
Epoch 5/10
39/39 - 10s - loss: 1129.8027 - loglik: -1.1282e+03 - logprior: -1.5601e+00
Epoch 6/10
39/39 - 10s - loss: 1129.8600 - loglik: -1.1283e+03 - logprior: -1.5697e+00
Fitted a model with MAP estimate = -1123.0910
expansions: [(8, 1), (11, 1), (18, 1), (23, 2), (24, 1), (30, 1), (31, 1), (32, 1), (33, 2), (35, 1), (36, 2), (37, 2), (45, 1), (46, 1), (47, 1), (48, 1), (65, 1), (66, 2), (69, 1), (70, 1), (83, 1), (86, 1), (87, 1), (88, 1), (93, 1), (96, 1), (110, 2), (112, 2), (122, 1), (126, 1), (128, 1), (132, 1), (149, 1), (150, 5), (152, 2), (154, 1), (155, 1), (156, 1), (161, 1), (168, 1), (178, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1122.4381 - loglik: -1.1199e+03 - logprior: -2.5826e+00
Epoch 2/2
39/39 - 13s - loss: 1111.6346 - loglik: -1.1104e+03 - logprior: -1.1853e+00
Fitted a model with MAP estimate = -1103.2965
expansions: [(3, 1)]
discards: [  0  26  42  50  86 141 144 192 193]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1116.0146 - loglik: -1.1136e+03 - logprior: -2.4121e+00
Epoch 2/2
39/39 - 13s - loss: 1111.0864 - loglik: -1.1104e+03 - logprior: -6.7532e-01
Fitted a model with MAP estimate = -1103.2020
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 16s - loss: 1108.9294 - loglik: -1.1066e+03 - logprior: -2.3200e+00
Epoch 2/10
39/39 - 13s - loss: 1105.0488 - loglik: -1.1041e+03 - logprior: -9.8489e-01
Epoch 3/10
39/39 - 13s - loss: 1103.0940 - loglik: -1.1029e+03 - logprior: -1.7184e-01
Epoch 4/10
39/39 - 13s - loss: 1101.8864 - loglik: -1.1018e+03 - logprior: -1.2551e-01
Epoch 5/10
39/39 - 13s - loss: 1100.9204 - loglik: -1.1009e+03 - logprior: -5.3516e-02
Epoch 6/10
39/39 - 13s - loss: 1101.3278 - loglik: -1.1013e+03 - logprior: 0.0104
Fitted a model with MAP estimate = -1100.8439
Time for alignment: 273.2948
Fitting a model of length 189 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1232.5084 - loglik: -1.2307e+03 - logprior: -1.8154e+00
Epoch 2/10
39/39 - 10s - loss: 1139.4127 - loglik: -1.1378e+03 - logprior: -1.6220e+00
Epoch 3/10
39/39 - 10s - loss: 1131.8053 - loglik: -1.1301e+03 - logprior: -1.6789e+00
Epoch 4/10
39/39 - 10s - loss: 1130.1688 - loglik: -1.1286e+03 - logprior: -1.6135e+00
Epoch 5/10
39/39 - 10s - loss: 1129.2096 - loglik: -1.1276e+03 - logprior: -1.6122e+00
Epoch 6/10
39/39 - 10s - loss: 1129.5282 - loglik: -1.1279e+03 - logprior: -1.6194e+00
Fitted a model with MAP estimate = -1122.4497
expansions: [(8, 1), (11, 1), (22, 1), (25, 1), (28, 1), (29, 1), (30, 1), (32, 1), (33, 1), (35, 1), (36, 1), (38, 1), (43, 1), (45, 1), (46, 1), (48, 1), (50, 1), (66, 1), (67, 2), (71, 1), (77, 1), (83, 1), (88, 1), (89, 1), (92, 1), (94, 1), (108, 1), (110, 2), (112, 2), (122, 1), (126, 1), (133, 1), (149, 2), (150, 1), (151, 4), (153, 2), (155, 2), (156, 2), (157, 2), (163, 1), (168, 1), (176, 1), (182, 2), (183, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 17s - loss: 1122.2673 - loglik: -1.1197e+03 - logprior: -2.6030e+00
Epoch 2/2
39/39 - 13s - loss: 1111.8416 - loglik: -1.1107e+03 - logprior: -1.1547e+00
Fitted a model with MAP estimate = -1103.3577
expansions: [(3, 1)]
discards: [  0  84 138 141 188 189 190 191 199 202]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 1115.9482 - loglik: -1.1136e+03 - logprior: -2.3824e+00
Epoch 2/2
39/39 - 13s - loss: 1111.4581 - loglik: -1.1108e+03 - logprior: -6.2288e-01
Fitted a model with MAP estimate = -1103.3836
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 10077 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 15s - loss: 1109.0928 - loglik: -1.1068e+03 - logprior: -2.3058e+00
Epoch 2/10
39/39 - 13s - loss: 1105.5759 - loglik: -1.1045e+03 - logprior: -1.0278e+00
Epoch 3/10
39/39 - 13s - loss: 1103.6049 - loglik: -1.1034e+03 - logprior: -1.8134e-01
Epoch 4/10
39/39 - 13s - loss: 1101.4852 - loglik: -1.1014e+03 - logprior: -1.1911e-01
Epoch 5/10
39/39 - 13s - loss: 1101.7441 - loglik: -1.1017e+03 - logprior: -5.0718e-02
Fitted a model with MAP estimate = -1101.1504
Time for alignment: 258.9343
Computed alignments with likelihoods: ['-1100.7133', '-1100.6705', '-1100.9164', '-1100.8439', '-1101.1504']
Best model has likelihood: -1100.6705  (prior= 0.2594 )
time for generating output: 0.3469
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF13561.projection.fasta
SP score = 0.6723683158820238
Training of 5 independent models on file PF05746.10000.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7f53f07639a0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7f540099d9d0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dbb0>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dd30>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099d700>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f540099dcd0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7f540099dca0>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188160>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188610>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1889d0>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188970>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885b0>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188eb0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188400>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d90>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1885e0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1888e0>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7f550c188ac0> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7f550c188fa0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188d30> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7f53f069c310> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : False , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c1887f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7f550c188dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7f5516f75790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7f550b8ecca0> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7f550c188c70> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 0.0
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 655.0631 - loglik: -6.5199e+02 - logprior: -3.0718e+00
Epoch 2/10
19/19 - 2s - loss: 610.7523 - loglik: -6.0940e+02 - logprior: -1.3527e+00
Epoch 3/10
19/19 - 2s - loss: 587.1759 - loglik: -5.8550e+02 - logprior: -1.6776e+00
Epoch 4/10
19/19 - 2s - loss: 580.3130 - loglik: -5.7867e+02 - logprior: -1.6426e+00
Epoch 5/10
19/19 - 2s - loss: 579.1757 - loglik: -5.7754e+02 - logprior: -1.6333e+00
Epoch 6/10
19/19 - 2s - loss: 577.9663 - loglik: -5.7635e+02 - logprior: -1.6190e+00
Epoch 7/10
19/19 - 2s - loss: 577.4488 - loglik: -5.7586e+02 - logprior: -1.5900e+00
Epoch 8/10
19/19 - 2s - loss: 577.9334 - loglik: -5.7635e+02 - logprior: -1.5820e+00
Fitted a model with MAP estimate = -577.0850
expansions: [(17, 1), (18, 2), (19, 3), (20, 1), (22, 1), (23, 1), (27, 1), (28, 2), (32, 1), (34, 1), (40, 1), (43, 1), (48, 1), (49, 1), (55, 1), (62, 1), (63, 1), (65, 2), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 582.0914 - loglik: -5.7819e+02 - logprior: -3.9056e+00
Epoch 2/2
19/19 - 3s - loss: 572.2014 - loglik: -5.7032e+02 - logprior: -1.8813e+00
Fitted a model with MAP estimate = -569.9470
expansions: [(0, 2)]
discards: [ 0 15 16 37 85]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 572.6157 - loglik: -5.6974e+02 - logprior: -2.8724e+00
Epoch 2/2
19/19 - 3s - loss: 568.9672 - loglik: -5.6794e+02 - logprior: -1.0319e+00
Fitted a model with MAP estimate = -567.6122
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 574.1584 - loglik: -5.7050e+02 - logprior: -3.6603e+00
Epoch 2/10
19/19 - 3s - loss: 569.4764 - loglik: -5.6833e+02 - logprior: -1.1436e+00
Epoch 3/10
19/19 - 3s - loss: 567.1555 - loglik: -5.6618e+02 - logprior: -9.7290e-01
Epoch 4/10
19/19 - 3s - loss: 566.3802 - loglik: -5.6545e+02 - logprior: -9.2986e-01
Epoch 5/10
19/19 - 3s - loss: 565.5184 - loglik: -5.6462e+02 - logprior: -8.9560e-01
Epoch 6/10
19/19 - 3s - loss: 565.5212 - loglik: -5.6465e+02 - logprior: -8.7556e-01
Fitted a model with MAP estimate = -564.9309
Time for alignment: 79.8718
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 655.2946 - loglik: -6.5222e+02 - logprior: -3.0730e+00
Epoch 2/10
19/19 - 2s - loss: 610.2736 - loglik: -6.0892e+02 - logprior: -1.3581e+00
Epoch 3/10
19/19 - 2s - loss: 587.4561 - loglik: -5.8574e+02 - logprior: -1.7117e+00
Epoch 4/10
19/19 - 2s - loss: 581.4547 - loglik: -5.7981e+02 - logprior: -1.6439e+00
Epoch 5/10
19/19 - 2s - loss: 578.6215 - loglik: -5.7700e+02 - logprior: -1.6170e+00
Epoch 6/10
19/19 - 2s - loss: 578.2037 - loglik: -5.7662e+02 - logprior: -1.5839e+00
Epoch 7/10
19/19 - 2s - loss: 577.7537 - loglik: -5.7620e+02 - logprior: -1.5521e+00
Epoch 8/10
19/19 - 2s - loss: 577.5848 - loglik: -5.7605e+02 - logprior: -1.5372e+00
Epoch 9/10
19/19 - 2s - loss: 577.6196 - loglik: -5.7608e+02 - logprior: -1.5366e+00
Fitted a model with MAP estimate = -576.9663
expansions: [(17, 1), (18, 3), (19, 4), (22, 1), (24, 1), (28, 2), (32, 2), (34, 1), (40, 1), (44, 1), (48, 1), (49, 1), (55, 1), (62, 1), (63, 1), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 582.6926 - loglik: -5.7879e+02 - logprior: -3.8981e+00
Epoch 2/2
19/19 - 3s - loss: 571.9175 - loglik: -5.7002e+02 - logprior: -1.8934e+00
Fitted a model with MAP estimate = -569.8403
expansions: [(0, 2)]
discards: [ 0 15 16 22 23 43]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 572.7339 - loglik: -5.6985e+02 - logprior: -2.8834e+00
Epoch 2/2
19/19 - 3s - loss: 569.1063 - loglik: -5.6806e+02 - logprior: -1.0481e+00
Fitted a model with MAP estimate = -568.1346
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 574.8387 - loglik: -5.7118e+02 - logprior: -3.6614e+00
Epoch 2/10
19/19 - 3s - loss: 570.1059 - loglik: -5.6894e+02 - logprior: -1.1650e+00
Epoch 3/10
19/19 - 3s - loss: 567.7767 - loglik: -5.6680e+02 - logprior: -9.7968e-01
Epoch 4/10
19/19 - 3s - loss: 567.0326 - loglik: -5.6610e+02 - logprior: -9.3613e-01
Epoch 5/10
19/19 - 3s - loss: 566.7085 - loglik: -5.6580e+02 - logprior: -9.1257e-01
Epoch 6/10
19/19 - 3s - loss: 565.6193 - loglik: -5.6473e+02 - logprior: -8.8820e-01
Epoch 7/10
19/19 - 3s - loss: 565.2809 - loglik: -5.6441e+02 - logprior: -8.7042e-01
Epoch 8/10
19/19 - 3s - loss: 565.5278 - loglik: -5.6468e+02 - logprior: -8.5092e-01
Fitted a model with MAP estimate = -565.2401
Time for alignment: 84.2685
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 655.5783 - loglik: -6.5251e+02 - logprior: -3.0713e+00
Epoch 2/10
19/19 - 2s - loss: 610.6987 - loglik: -6.0934e+02 - logprior: -1.3565e+00
Epoch 3/10
19/19 - 2s - loss: 587.8495 - loglik: -5.8616e+02 - logprior: -1.6886e+00
Epoch 4/10
19/19 - 2s - loss: 581.8167 - loglik: -5.8021e+02 - logprior: -1.6035e+00
Epoch 5/10
19/19 - 2s - loss: 578.6702 - loglik: -5.7707e+02 - logprior: -1.6019e+00
Epoch 6/10
19/19 - 2s - loss: 578.5850 - loglik: -5.7702e+02 - logprior: -1.5680e+00
Epoch 7/10
19/19 - 2s - loss: 577.7847 - loglik: -5.7622e+02 - logprior: -1.5601e+00
Epoch 8/10
19/19 - 2s - loss: 577.2009 - loglik: -5.7565e+02 - logprior: -1.5513e+00
Epoch 9/10
19/19 - 2s - loss: 577.6801 - loglik: -5.7613e+02 - logprior: -1.5482e+00
Fitted a model with MAP estimate = -577.1589
expansions: [(17, 1), (18, 2), (19, 3), (20, 1), (22, 1), (23, 1), (27, 1), (29, 1), (32, 2), (40, 1), (43, 1), (44, 1), (50, 1), (56, 1), (63, 1), (64, 1), (66, 2), (68, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 582.3179 - loglik: -5.7841e+02 - logprior: -3.9065e+00
Epoch 2/2
19/19 - 3s - loss: 572.0507 - loglik: -5.7016e+02 - logprior: -1.8890e+00
Fitted a model with MAP estimate = -570.0553
expansions: [(0, 2)]
discards: [ 0 15 16 42]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 572.5726 - loglik: -5.6969e+02 - logprior: -2.8796e+00
Epoch 2/2
19/19 - 3s - loss: 568.7936 - loglik: -5.6774e+02 - logprior: -1.0487e+00
Fitted a model with MAP estimate = -567.5257
expansions: [(22, 1)]
discards: [ 0 83]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 574.2114 - loglik: -5.7054e+02 - logprior: -3.6697e+00
Epoch 2/10
19/19 - 3s - loss: 569.2568 - loglik: -5.6811e+02 - logprior: -1.1453e+00
Epoch 3/10
19/19 - 3s - loss: 567.1777 - loglik: -5.6621e+02 - logprior: -9.7027e-01
Epoch 4/10
19/19 - 3s - loss: 566.1110 - loglik: -5.6519e+02 - logprior: -9.1640e-01
Epoch 5/10
19/19 - 3s - loss: 565.4426 - loglik: -5.6456e+02 - logprior: -8.8603e-01
Epoch 6/10
19/19 - 3s - loss: 565.3467 - loglik: -5.6448e+02 - logprior: -8.6455e-01
Epoch 7/10
19/19 - 3s - loss: 564.5281 - loglik: -5.6368e+02 - logprior: -8.4907e-01
Epoch 8/10
19/19 - 3s - loss: 564.9432 - loglik: -5.6413e+02 - logprior: -8.1689e-01
Fitted a model with MAP estimate = -564.5259
Time for alignment: 86.8670
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 655.5921 - loglik: -6.5252e+02 - logprior: -3.0717e+00
Epoch 2/10
19/19 - 2s - loss: 611.0468 - loglik: -6.0969e+02 - logprior: -1.3535e+00
Epoch 3/10
19/19 - 2s - loss: 589.0779 - loglik: -5.8737e+02 - logprior: -1.7041e+00
Epoch 4/10
19/19 - 2s - loss: 581.6996 - loglik: -5.8006e+02 - logprior: -1.6376e+00
Epoch 5/10
19/19 - 2s - loss: 579.2111 - loglik: -5.7759e+02 - logprior: -1.6202e+00
Epoch 6/10
19/19 - 2s - loss: 578.1692 - loglik: -5.7658e+02 - logprior: -1.5845e+00
Epoch 7/10
19/19 - 2s - loss: 578.2348 - loglik: -5.7667e+02 - logprior: -1.5688e+00
Fitted a model with MAP estimate = -577.5444
expansions: [(9, 1), (17, 1), (18, 3), (19, 2), (22, 1), (24, 1), (28, 2), (32, 1), (34, 1), (40, 1), (43, 1), (48, 1), (56, 1), (63, 1), (64, 1), (66, 2), (70, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 582.5110 - loglik: -5.7864e+02 - logprior: -3.8752e+00
Epoch 2/2
19/19 - 3s - loss: 572.1303 - loglik: -5.7029e+02 - logprior: -1.8447e+00
Fitted a model with MAP estimate = -569.8517
expansions: [(0, 2)]
discards: [ 0 18 36 84]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 572.3339 - loglik: -5.6946e+02 - logprior: -2.8709e+00
Epoch 2/2
19/19 - 3s - loss: 568.6315 - loglik: -5.6759e+02 - logprior: -1.0432e+00
Fitted a model with MAP estimate = -567.4369
expansions: [(23, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 574.0305 - loglik: -5.7032e+02 - logprior: -3.7133e+00
Epoch 2/10
19/19 - 3s - loss: 569.5195 - loglik: -5.6832e+02 - logprior: -1.1993e+00
Epoch 3/10
19/19 - 3s - loss: 567.2638 - loglik: -5.6631e+02 - logprior: -9.5614e-01
Epoch 4/10
19/19 - 3s - loss: 566.8588 - loglik: -5.6593e+02 - logprior: -9.2438e-01
Epoch 5/10
19/19 - 3s - loss: 565.5545 - loglik: -5.6467e+02 - logprior: -8.8618e-01
Epoch 6/10
19/19 - 3s - loss: 565.7205 - loglik: -5.6485e+02 - logprior: -8.6988e-01
Fitted a model with MAP estimate = -564.9972
Time for alignment: 75.7023
Fitting a model of length 94 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 655.4039 - loglik: -6.5233e+02 - logprior: -3.0719e+00
Epoch 2/10
19/19 - 2s - loss: 611.1777 - loglik: -6.0982e+02 - logprior: -1.3581e+00
Epoch 3/10
19/19 - 2s - loss: 587.8287 - loglik: -5.8613e+02 - logprior: -1.6965e+00
Epoch 4/10
19/19 - 2s - loss: 580.9109 - loglik: -5.7927e+02 - logprior: -1.6445e+00
Epoch 5/10
19/19 - 2s - loss: 578.2717 - loglik: -5.7666e+02 - logprior: -1.6164e+00
Epoch 6/10
19/19 - 2s - loss: 578.0350 - loglik: -5.7646e+02 - logprior: -1.5764e+00
Epoch 7/10
19/19 - 2s - loss: 576.9532 - loglik: -5.7539e+02 - logprior: -1.5652e+00
Epoch 8/10
19/19 - 2s - loss: 576.8840 - loglik: -5.7533e+02 - logprior: -1.5494e+00
Epoch 9/10
19/19 - 2s - loss: 577.7936 - loglik: -5.7624e+02 - logprior: -1.5572e+00
Fitted a model with MAP estimate = -576.5901
expansions: [(17, 1), (18, 2), (19, 2), (20, 2), (22, 1), (23, 1), (24, 1), (28, 2), (32, 2), (34, 1), (44, 1), (48, 1), (50, 1), (56, 1), (59, 2), (62, 1), (63, 1), (69, 1), (71, 1), (74, 1), (76, 1), (79, 1), (82, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 583.2811 - loglik: -5.7937e+02 - logprior: -3.9120e+00
Epoch 2/2
19/19 - 3s - loss: 572.3641 - loglik: -5.7043e+02 - logprior: -1.9315e+00
Fitted a model with MAP estimate = -570.1116
expansions: [(0, 2)]
discards: [ 0 15 16 20 37 43 77]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 572.5535 - loglik: -5.6967e+02 - logprior: -2.8799e+00
Epoch 2/2
19/19 - 3s - loss: 568.9948 - loglik: -5.6796e+02 - logprior: -1.0365e+00
Fitted a model with MAP estimate = -567.7034
expansions: [(21, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10011 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 574.2928 - loglik: -5.7061e+02 - logprior: -3.6801e+00
Epoch 2/10
19/19 - 3s - loss: 569.5867 - loglik: -5.6843e+02 - logprior: -1.1609e+00
Epoch 3/10
19/19 - 3s - loss: 567.2841 - loglik: -5.6632e+02 - logprior: -9.6571e-01
Epoch 4/10
19/19 - 3s - loss: 566.6553 - loglik: -5.6573e+02 - logprior: -9.2860e-01
Epoch 5/10
19/19 - 3s - loss: 565.6483 - loglik: -5.6475e+02 - logprior: -8.9758e-01
Epoch 6/10
19/19 - 3s - loss: 566.0176 - loglik: -5.6515e+02 - logprior: -8.6862e-01
Fitted a model with MAP estimate = -565.1463
Time for alignment: 79.0684
Computed alignments with likelihoods: ['-564.9309', '-565.2401', '-564.5259', '-564.9972', '-565.1463']
Best model has likelihood: -564.5259  (prior= -0.7978 )
time for generating output: 0.1624
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PF05746.projection.fasta
SP score = 0.8573680063041765
