Training of 5 independent models on file cys.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7feab9f5d100>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7feab9f5d940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9d60b070>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 1118.3925 - loglik: -1.1135e+03 - logprior: -4.8766e+00
Epoch 2/10
25/25 - 6s - loss: 1005.0618 - loglik: -1.0034e+03 - logprior: -1.6863e+00
Epoch 3/10
25/25 - 6s - loss: 989.9600 - loglik: -9.8823e+02 - logprior: -1.7270e+00
Epoch 4/10
25/25 - 6s - loss: 984.4816 - loglik: -9.8289e+02 - logprior: -1.5937e+00
Epoch 5/10
25/25 - 6s - loss: 982.4953 - loglik: -9.8090e+02 - logprior: -1.5890e+00
Epoch 6/10
25/25 - 6s - loss: 982.1116 - loglik: -9.8049e+02 - logprior: -1.6183e+00
Epoch 7/10
25/25 - 6s - loss: 980.4801 - loglik: -9.7882e+02 - logprior: -1.6530e+00
Epoch 8/10
25/25 - 6s - loss: 977.5991 - loglik: -9.7591e+02 - logprior: -1.6868e+00
Epoch 9/10
25/25 - 6s - loss: 977.8150 - loglik: -9.7610e+02 - logprior: -1.7114e+00
Fitted a model with MAP estimate = -977.2149
expansions: [(9, 2), (10, 1), (11, 3), (31, 1), (32, 2), (33, 1), (35, 2), (36, 1), (47, 2), (57, 1), (58, 1), (60, 1), (62, 2), (76, 1), (81, 1), (82, 2), (83, 2), (87, 1), (90, 1), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (140, 2), (160, 1), (162, 2), (168, 2), (169, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 980.3025 - loglik: -9.7358e+02 - logprior: -6.7202e+00
Epoch 2/2
25/25 - 8s - loss: 969.3513 - loglik: -9.6686e+02 - logprior: -2.4879e+00
Fitted a model with MAP estimate = -964.2754
expansions: [(0, 3)]
discards: [  0  14  44  61  81 143 205]
Re-initialized the encoder parameters.
Fitting a model of length 215 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 971.6393 - loglik: -9.6732e+02 - logprior: -4.3219e+00
Epoch 2/2
25/25 - 7s - loss: 963.6384 - loglik: -9.6320e+02 - logprior: -4.3626e-01
Fitted a model with MAP estimate = -962.5933
expansions: []
discards: [  0   2 104]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 975.4116 - loglik: -9.6910e+02 - logprior: -6.3089e+00
Epoch 2/10
25/25 - 7s - loss: 963.3527 - loglik: -9.6201e+02 - logprior: -1.3468e+00
Epoch 3/10
25/25 - 7s - loss: 963.2106 - loglik: -9.6356e+02 - logprior: 0.3517
Epoch 4/10
25/25 - 7s - loss: 962.5726 - loglik: -9.6301e+02 - logprior: 0.4404
Epoch 5/10
25/25 - 7s - loss: 959.7530 - loglik: -9.6030e+02 - logprior: 0.5462
Epoch 6/10
25/25 - 7s - loss: 959.3528 - loglik: -9.5999e+02 - logprior: 0.6439
Epoch 7/10
25/25 - 7s - loss: 953.5675 - loglik: -9.5430e+02 - logprior: 0.7411
Epoch 8/10
25/25 - 7s - loss: 955.5931 - loglik: -9.5635e+02 - logprior: 0.7633
Fitted a model with MAP estimate = -953.6433
Time for alignment: 178.4564
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 1116.3341 - loglik: -1.1115e+03 - logprior: -4.8423e+00
Epoch 2/10
25/25 - 6s - loss: 1008.2682 - loglik: -1.0068e+03 - logprior: -1.4540e+00
Epoch 3/10
25/25 - 6s - loss: 988.8571 - loglik: -9.8720e+02 - logprior: -1.6547e+00
Epoch 4/10
25/25 - 6s - loss: 984.5814 - loglik: -9.8312e+02 - logprior: -1.4636e+00
Epoch 5/10
25/25 - 6s - loss: 985.4738 - loglik: -9.8404e+02 - logprior: -1.4321e+00
Fitted a model with MAP estimate = -981.8296
expansions: [(9, 5), (10, 2), (31, 3), (32, 2), (34, 2), (47, 3), (49, 1), (57, 1), (59, 1), (62, 1), (75, 1), (80, 1), (82, 1), (83, 2), (85, 1), (89, 1), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (123, 2), (126, 2), (137, 1), (138, 2), (139, 1), (150, 1), (159, 1), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 982.6941 - loglik: -9.7590e+02 - logprior: -6.7930e+00
Epoch 2/2
25/25 - 8s - loss: 966.9559 - loglik: -9.6452e+02 - logprior: -2.4344e+00
Fitted a model with MAP estimate = -964.4676
expansions: [(0, 3), (213, 1)]
discards: [  0  14  41  62  63 107 143 177]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 971.8166 - loglik: -9.6747e+02 - logprior: -4.3513e+00
Epoch 2/2
25/25 - 7s - loss: 964.5613 - loglik: -9.6407e+02 - logprior: -4.9031e-01
Fitted a model with MAP estimate = -962.9122
expansions: [(61, 2)]
discards: [ 0  2 45]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 972.0986 - loglik: -9.6577e+02 - logprior: -6.3314e+00
Epoch 2/10
25/25 - 7s - loss: 967.7440 - loglik: -9.6639e+02 - logprior: -1.3556e+00
Epoch 3/10
25/25 - 7s - loss: 963.6085 - loglik: -9.6395e+02 - logprior: 0.3435
Epoch 4/10
25/25 - 7s - loss: 959.5991 - loglik: -9.6001e+02 - logprior: 0.4098
Epoch 5/10
25/25 - 7s - loss: 962.7700 - loglik: -9.6329e+02 - logprior: 0.5178
Fitted a model with MAP estimate = -958.6338
Time for alignment: 134.3096
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 1114.8375 - loglik: -1.1100e+03 - logprior: -4.8675e+00
Epoch 2/10
25/25 - 6s - loss: 1008.2933 - loglik: -1.0070e+03 - logprior: -1.3166e+00
Epoch 3/10
25/25 - 6s - loss: 986.5636 - loglik: -9.8537e+02 - logprior: -1.1961e+00
Epoch 4/10
25/25 - 6s - loss: 987.4475 - loglik: -9.8640e+02 - logprior: -1.0460e+00
Fitted a model with MAP estimate = -982.9110
expansions: [(0, 6), (10, 1), (31, 3), (32, 2), (34, 2), (47, 2), (58, 1), (60, 1), (62, 2), (76, 1), (81, 1), (83, 1), (84, 2), (86, 1), (90, 1), (93, 1), (96, 1), (99, 1), (102, 1), (113, 2), (115, 1), (122, 1), (124, 1), (126, 3), (137, 1), (140, 2), (160, 1), (162, 2), (168, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 219 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 980.7096 - loglik: -9.7490e+02 - logprior: -5.8046e+00
Epoch 2/2
25/25 - 8s - loss: 963.5270 - loglik: -9.6280e+02 - logprior: -7.2639e-01
Fitted a model with MAP estimate = -962.5324
expansions: [(214, 1)]
discards: [  0   3  42  82 144 206]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 974.1284 - loglik: -9.6759e+02 - logprior: -6.5375e+00
Epoch 2/2
25/25 - 7s - loss: 965.8710 - loglik: -9.6467e+02 - logprior: -1.1996e+00
Fitted a model with MAP estimate = -963.0736
expansions: [(0, 3)]
discards: [  0  43 104]
Re-initialized the encoder parameters.
Fitting a model of length 214 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 11s - loss: 971.6046 - loglik: -9.6742e+02 - logprior: -4.1831e+00
Epoch 2/10
25/25 - 7s - loss: 962.0620 - loglik: -9.6173e+02 - logprior: -3.3134e-01
Epoch 3/10
25/25 - 7s - loss: 964.1492 - loglik: -9.6422e+02 - logprior: 0.0703
Fitted a model with MAP estimate = -961.7433
Time for alignment: 114.6672
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 8s - loss: 1116.5085 - loglik: -1.1117e+03 - logprior: -4.8429e+00
Epoch 2/10
25/25 - 6s - loss: 1005.4011 - loglik: -1.0037e+03 - logprior: -1.6605e+00
Epoch 3/10
25/25 - 6s - loss: 989.1390 - loglik: -9.8738e+02 - logprior: -1.7579e+00
Epoch 4/10
25/25 - 6s - loss: 983.3588 - loglik: -9.8173e+02 - logprior: -1.6282e+00
Epoch 5/10
25/25 - 6s - loss: 982.2780 - loglik: -9.8069e+02 - logprior: -1.5886e+00
Epoch 6/10
25/25 - 6s - loss: 979.4241 - loglik: -9.7784e+02 - logprior: -1.5812e+00
Epoch 7/10
25/25 - 6s - loss: 979.6218 - loglik: -9.7802e+02 - logprior: -1.5961e+00
Fitted a model with MAP estimate = -977.4939
expansions: [(9, 3), (10, 1), (12, 1), (13, 1), (31, 1), (32, 2), (33, 3), (34, 2), (47, 3), (49, 1), (57, 1), (59, 1), (76, 1), (81, 2), (82, 2), (83, 2), (87, 1), (90, 1), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (114, 1), (121, 1), (123, 1), (125, 3), (136, 1), (139, 1), (150, 1), (159, 1), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 982.3283 - loglik: -9.7553e+02 - logprior: -6.7972e+00
Epoch 2/2
25/25 - 8s - loss: 967.7389 - loglik: -9.6527e+02 - logprior: -2.4691e+00
Fitted a model with MAP estimate = -964.6674
expansions: [(0, 3), (213, 1)]
discards: [  0   9  42  46  62  63 104 107 144]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 973.5036 - loglik: -9.6921e+02 - logprior: -4.2952e+00
Epoch 2/2
25/25 - 7s - loss: 963.4079 - loglik: -9.6299e+02 - logprior: -4.2178e-01
Fitted a model with MAP estimate = -963.2089
expansions: [(60, 2)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 976.6284 - loglik: -9.7031e+02 - logprior: -6.3137e+00
Epoch 2/10
25/25 - 7s - loss: 962.0020 - loglik: -9.6065e+02 - logprior: -1.3468e+00
Epoch 3/10
25/25 - 7s - loss: 962.0864 - loglik: -9.6245e+02 - logprior: 0.3660
Fitted a model with MAP estimate = -961.5700
Time for alignment: 131.1047
Fitting a model of length 172 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 9s - loss: 1115.7957 - loglik: -1.1109e+03 - logprior: -4.8472e+00
Epoch 2/10
25/25 - 6s - loss: 1009.0517 - loglik: -1.0076e+03 - logprior: -1.4159e+00
Epoch 3/10
25/25 - 6s - loss: 986.5284 - loglik: -9.8502e+02 - logprior: -1.5126e+00
Epoch 4/10
25/25 - 6s - loss: 987.8935 - loglik: -9.8655e+02 - logprior: -1.3378e+00
Fitted a model with MAP estimate = -985.0590
expansions: [(9, 5), (10, 1), (31, 3), (32, 2), (34, 2), (47, 3), (57, 2), (59, 1), (76, 1), (81, 2), (82, 2), (83, 2), (90, 2), (92, 1), (95, 1), (98, 1), (101, 1), (112, 2), (115, 1), (123, 2), (126, 2), (137, 1), (140, 2), (160, 1), (162, 2), (168, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 10s - loss: 981.3270 - loglik: -9.7457e+02 - logprior: -6.7541e+00
Epoch 2/2
25/25 - 8s - loss: 967.2106 - loglik: -9.6487e+02 - logprior: -2.3361e+00
Fitted a model with MAP estimate = -964.0496
expansions: [(0, 3), (213, 1)]
discards: [  0  40  45  61  62 103 106 143 205]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 11s - loss: 971.5847 - loglik: -9.6724e+02 - logprior: -4.3413e+00
Epoch 2/2
25/25 - 7s - loss: 964.6161 - loglik: -9.6416e+02 - logprior: -4.5414e-01
Fitted a model with MAP estimate = -962.9183
expansions: [(60, 2)]
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 4316 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 10s - loss: 974.5016 - loglik: -9.6815e+02 - logprior: -6.3479e+00
Epoch 2/10
25/25 - 7s - loss: 964.2020 - loglik: -9.6284e+02 - logprior: -1.3598e+00
Epoch 3/10
25/25 - 7s - loss: 960.9242 - loglik: -9.6124e+02 - logprior: 0.3151
Epoch 4/10
25/25 - 7s - loss: 963.6492 - loglik: -9.6403e+02 - logprior: 0.3845
Fitted a model with MAP estimate = -959.9851
Time for alignment: 121.6508
Computed alignments with likelihoods: ['-953.6433', '-958.6338', '-961.7433', '-961.5700', '-959.9851']
Best model has likelihood: -953.6433  (prior= 0.8232 )
time for generating output: 0.2149
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cys.projection.fasta
SP score = 0.9241366642174872
Training of 5 independent models on file DMRL_synthase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bc73400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea44c4ef40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b606730>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 800.0338 - loglik: -7.7993e+02 - logprior: -2.0109e+01
Epoch 2/10
10/10 - 1s - loss: 733.5421 - loglik: -7.2891e+02 - logprior: -4.6362e+00
Epoch 3/10
10/10 - 1s - loss: 680.4472 - loglik: -6.7788e+02 - logprior: -2.5643e+00
Epoch 4/10
10/10 - 1s - loss: 650.6747 - loglik: -6.4841e+02 - logprior: -2.2623e+00
Epoch 5/10
10/10 - 1s - loss: 636.2983 - loglik: -6.3446e+02 - logprior: -1.8340e+00
Epoch 6/10
10/10 - 1s - loss: 630.9415 - loglik: -6.2950e+02 - logprior: -1.4409e+00
Epoch 7/10
10/10 - 1s - loss: 626.6027 - loglik: -6.2535e+02 - logprior: -1.2523e+00
Epoch 8/10
10/10 - 1s - loss: 623.9000 - loglik: -6.2286e+02 - logprior: -1.0419e+00
Epoch 9/10
10/10 - 1s - loss: 623.8693 - loglik: -6.2289e+02 - logprior: -9.7766e-01
Epoch 10/10
10/10 - 1s - loss: 622.3358 - loglik: -6.2134e+02 - logprior: -9.9392e-01
Fitted a model with MAP estimate = -621.0607
expansions: [(0, 3), (11, 1), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (76, 1), (77, 1), (78, 3), (81, 1), (96, 1), (114, 2), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 146 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 652.3695 - loglik: -6.2609e+02 - logprior: -2.6283e+01
Epoch 2/2
10/10 - 2s - loss: 612.9506 - loglik: -6.0579e+02 - logprior: -7.1635e+00
Fitted a model with MAP estimate = -606.1994
expansions: []
discards: [59 62]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 621.0930 - loglik: -6.0273e+02 - logprior: -1.8363e+01
Epoch 2/2
10/10 - 2s - loss: 604.1619 - loglik: -6.0032e+02 - logprior: -3.8451e+00
Fitted a model with MAP estimate = -601.4613
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 618.3779 - loglik: -6.0107e+02 - logprior: -1.7307e+01
Epoch 2/10
10/10 - 2s - loss: 602.9940 - loglik: -5.9949e+02 - logprior: -3.5034e+00
Epoch 3/10
10/10 - 2s - loss: 600.6512 - loglik: -5.9981e+02 - logprior: -8.4353e-01
Epoch 4/10
10/10 - 2s - loss: 598.7408 - loglik: -5.9882e+02 - logprior: 0.0828
Epoch 5/10
10/10 - 2s - loss: 597.2047 - loglik: -5.9773e+02 - logprior: 0.5304
Epoch 6/10
10/10 - 2s - loss: 596.2148 - loglik: -5.9707e+02 - logprior: 0.8583
Epoch 7/10
10/10 - 2s - loss: 594.6122 - loglik: -5.9573e+02 - logprior: 1.1233
Epoch 8/10
10/10 - 2s - loss: 592.9658 - loglik: -5.9424e+02 - logprior: 1.2749
Epoch 9/10
10/10 - 2s - loss: 591.4659 - loglik: -5.9280e+02 - logprior: 1.3320
Epoch 10/10
10/10 - 2s - loss: 590.1498 - loglik: -5.9152e+02 - logprior: 1.3724
Fitted a model with MAP estimate = -589.6442
Time for alignment: 57.6744
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 800.4924 - loglik: -7.8039e+02 - logprior: -2.0105e+01
Epoch 2/10
10/10 - 1s - loss: 732.7620 - loglik: -7.2815e+02 - logprior: -4.6146e+00
Epoch 3/10
10/10 - 1s - loss: 678.3793 - loglik: -6.7592e+02 - logprior: -2.4570e+00
Epoch 4/10
10/10 - 1s - loss: 648.6497 - loglik: -6.4663e+02 - logprior: -2.0177e+00
Epoch 5/10
10/10 - 1s - loss: 636.2745 - loglik: -6.3473e+02 - logprior: -1.5417e+00
Epoch 6/10
10/10 - 1s - loss: 630.5812 - loglik: -6.2934e+02 - logprior: -1.2362e+00
Epoch 7/10
10/10 - 1s - loss: 627.4471 - loglik: -6.2638e+02 - logprior: -1.0669e+00
Epoch 8/10
10/10 - 1s - loss: 624.7541 - loglik: -6.2390e+02 - logprior: -8.5214e-01
Epoch 9/10
10/10 - 1s - loss: 623.7144 - loglik: -6.2293e+02 - logprior: -7.8286e-01
Epoch 10/10
10/10 - 1s - loss: 622.5252 - loglik: -6.2174e+02 - logprior: -7.8401e-01
Fitted a model with MAP estimate = -621.7093
expansions: [(0, 4), (13, 1), (14, 1), (15, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 2), (114, 2), (115, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 652.5824 - loglik: -6.2630e+02 - logprior: -2.6280e+01
Epoch 2/2
10/10 - 2s - loss: 614.0644 - loglik: -6.0675e+02 - logprior: -7.3152e+00
Fitted a model with MAP estimate = -606.6707
expansions: []
discards: [ 59  62 122 145]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 622.0346 - loglik: -6.0363e+02 - logprior: -1.8407e+01
Epoch 2/2
10/10 - 2s - loss: 604.1537 - loglik: -6.0030e+02 - logprior: -3.8541e+00
Fitted a model with MAP estimate = -601.5625
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 618.5056 - loglik: -6.0118e+02 - logprior: -1.7323e+01
Epoch 2/10
10/10 - 2s - loss: 603.2086 - loglik: -5.9970e+02 - logprior: -3.5080e+00
Epoch 3/10
10/10 - 2s - loss: 600.4417 - loglik: -5.9960e+02 - logprior: -8.4341e-01
Epoch 4/10
10/10 - 2s - loss: 598.5458 - loglik: -5.9862e+02 - logprior: 0.0728
Epoch 5/10
10/10 - 2s - loss: 597.1779 - loglik: -5.9771e+02 - logprior: 0.5301
Epoch 6/10
10/10 - 2s - loss: 596.1398 - loglik: -5.9699e+02 - logprior: 0.8562
Epoch 7/10
10/10 - 2s - loss: 594.7011 - loglik: -5.9582e+02 - logprior: 1.1227
Epoch 8/10
10/10 - 2s - loss: 592.9094 - loglik: -5.9418e+02 - logprior: 1.2755
Epoch 9/10
10/10 - 2s - loss: 591.6225 - loglik: -5.9295e+02 - logprior: 1.3278
Epoch 10/10
10/10 - 2s - loss: 589.9688 - loglik: -5.9133e+02 - logprior: 1.3692
Fitted a model with MAP estimate = -589.6932
Time for alignment: 57.6558
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 800.5962 - loglik: -7.8049e+02 - logprior: -2.0106e+01
Epoch 2/10
10/10 - 1s - loss: 733.3174 - loglik: -7.2869e+02 - logprior: -4.6241e+00
Epoch 3/10
10/10 - 1s - loss: 681.1095 - loglik: -6.7856e+02 - logprior: -2.5447e+00
Epoch 4/10
10/10 - 1s - loss: 650.1248 - loglik: -6.4789e+02 - logprior: -2.2324e+00
Epoch 5/10
10/10 - 1s - loss: 636.3066 - loglik: -6.3457e+02 - logprior: -1.7311e+00
Epoch 6/10
10/10 - 1s - loss: 629.8485 - loglik: -6.2853e+02 - logprior: -1.3138e+00
Epoch 7/10
10/10 - 1s - loss: 626.3406 - loglik: -6.2524e+02 - logprior: -1.1019e+00
Epoch 8/10
10/10 - 1s - loss: 623.6508 - loglik: -6.2277e+02 - logprior: -8.8094e-01
Epoch 9/10
10/10 - 1s - loss: 622.5683 - loglik: -6.2171e+02 - logprior: -8.5193e-01
Epoch 10/10
10/10 - 1s - loss: 620.5790 - loglik: -6.1977e+02 - logprior: -8.0671e-01
Fitted a model with MAP estimate = -620.0651
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (22, 1), (26, 1), (30, 1), (31, 2), (46, 3), (47, 2), (62, 1), (72, 1), (75, 1), (76, 1), (77, 2), (78, 1), (98, 2), (114, 2), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 653.8793 - loglik: -6.2771e+02 - logprior: -2.6172e+01
Epoch 2/2
10/10 - 2s - loss: 612.9467 - loglik: -6.0577e+02 - logprior: -7.1811e+00
Fitted a model with MAP estimate = -606.4936
expansions: []
discards: [ 59  62 122]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 620.9803 - loglik: -6.0259e+02 - logprior: -1.8387e+01
Epoch 2/2
10/10 - 2s - loss: 604.5834 - loglik: -6.0072e+02 - logprior: -3.8596e+00
Fitted a model with MAP estimate = -601.5225
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 618.8502 - loglik: -6.0153e+02 - logprior: -1.7315e+01
Epoch 2/10
10/10 - 2s - loss: 602.7360 - loglik: -5.9924e+02 - logprior: -3.4916e+00
Epoch 3/10
10/10 - 2s - loss: 600.0372 - loglik: -5.9920e+02 - logprior: -8.3871e-01
Epoch 4/10
10/10 - 2s - loss: 599.0890 - loglik: -5.9918e+02 - logprior: 0.0910
Epoch 5/10
10/10 - 2s - loss: 597.4103 - loglik: -5.9795e+02 - logprior: 0.5425
Epoch 6/10
10/10 - 2s - loss: 596.0500 - loglik: -5.9692e+02 - logprior: 0.8723
Epoch 7/10
10/10 - 2s - loss: 594.3188 - loglik: -5.9545e+02 - logprior: 1.1334
Epoch 8/10
10/10 - 2s - loss: 593.5197 - loglik: -5.9480e+02 - logprior: 1.2862
Epoch 9/10
10/10 - 2s - loss: 590.7221 - loglik: -5.9206e+02 - logprior: 1.3408
Epoch 10/10
10/10 - 2s - loss: 590.7260 - loglik: -5.9210e+02 - logprior: 1.3788
Fitted a model with MAP estimate = -589.6511
Time for alignment: 57.3043
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 800.3094 - loglik: -7.8020e+02 - logprior: -2.0106e+01
Epoch 2/10
10/10 - 1s - loss: 733.3535 - loglik: -7.2873e+02 - logprior: -4.6194e+00
Epoch 3/10
10/10 - 1s - loss: 680.2700 - loglik: -6.7779e+02 - logprior: -2.4801e+00
Epoch 4/10
10/10 - 1s - loss: 650.8448 - loglik: -6.4882e+02 - logprior: -2.0241e+00
Epoch 5/10
10/10 - 1s - loss: 637.7283 - loglik: -6.3624e+02 - logprior: -1.4860e+00
Epoch 6/10
10/10 - 1s - loss: 630.0979 - loglik: -6.2897e+02 - logprior: -1.1231e+00
Epoch 7/10
10/10 - 1s - loss: 627.8873 - loglik: -6.2698e+02 - logprior: -9.0959e-01
Epoch 8/10
10/10 - 1s - loss: 625.9494 - loglik: -6.2525e+02 - logprior: -6.9355e-01
Epoch 9/10
10/10 - 1s - loss: 622.7606 - loglik: -6.2215e+02 - logprior: -6.1072e-01
Epoch 10/10
10/10 - 1s - loss: 622.2155 - loglik: -6.2158e+02 - logprior: -6.2908e-01
Fitted a model with MAP estimate = -621.4837
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (22, 1), (26, 1), (30, 1), (31, 2), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 2), (114, 2), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 653.3727 - loglik: -6.2723e+02 - logprior: -2.6140e+01
Epoch 2/2
10/10 - 2s - loss: 613.3035 - loglik: -6.0613e+02 - logprior: -7.1719e+00
Fitted a model with MAP estimate = -606.3322
expansions: []
discards: [ 59  62 122]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 621.6593 - loglik: -6.0329e+02 - logprior: -1.8367e+01
Epoch 2/2
10/10 - 2s - loss: 603.1687 - loglik: -5.9932e+02 - logprior: -3.8492e+00
Fitted a model with MAP estimate = -601.4792
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 618.9291 - loglik: -6.0162e+02 - logprior: -1.7314e+01
Epoch 2/10
10/10 - 2s - loss: 603.0675 - loglik: -5.9958e+02 - logprior: -3.4922e+00
Epoch 3/10
10/10 - 2s - loss: 600.1167 - loglik: -5.9927e+02 - logprior: -8.4781e-01
Epoch 4/10
10/10 - 2s - loss: 598.1763 - loglik: -5.9826e+02 - logprior: 0.0852
Epoch 5/10
10/10 - 2s - loss: 597.9542 - loglik: -5.9850e+02 - logprior: 0.5423
Epoch 6/10
10/10 - 2s - loss: 595.4574 - loglik: -5.9633e+02 - logprior: 0.8707
Epoch 7/10
10/10 - 2s - loss: 594.7072 - loglik: -5.9584e+02 - logprior: 1.1371
Epoch 8/10
10/10 - 2s - loss: 592.9439 - loglik: -5.9423e+02 - logprior: 1.2882
Epoch 9/10
10/10 - 2s - loss: 591.5547 - loglik: -5.9290e+02 - logprior: 1.3466
Epoch 10/10
10/10 - 2s - loss: 590.3097 - loglik: -5.9169e+02 - logprior: 1.3807
Fitted a model with MAP estimate = -589.7004
Time for alignment: 57.0876
Fitting a model of length 115 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 800.6395 - loglik: -7.8053e+02 - logprior: -2.0105e+01
Epoch 2/10
10/10 - 1s - loss: 732.7020 - loglik: -7.2808e+02 - logprior: -4.6247e+00
Epoch 3/10
10/10 - 1s - loss: 680.2831 - loglik: -6.7776e+02 - logprior: -2.5258e+00
Epoch 4/10
10/10 - 1s - loss: 649.7944 - loglik: -6.4768e+02 - logprior: -2.1127e+00
Epoch 5/10
10/10 - 1s - loss: 637.5519 - loglik: -6.3596e+02 - logprior: -1.5909e+00
Epoch 6/10
10/10 - 1s - loss: 631.2505 - loglik: -6.2999e+02 - logprior: -1.2552e+00
Epoch 7/10
10/10 - 1s - loss: 627.0085 - loglik: -6.2593e+02 - logprior: -1.0789e+00
Epoch 8/10
10/10 - 1s - loss: 625.4671 - loglik: -6.2459e+02 - logprior: -8.6956e-01
Epoch 9/10
10/10 - 1s - loss: 622.7451 - loglik: -6.2194e+02 - logprior: -7.9963e-01
Epoch 10/10
10/10 - 1s - loss: 621.9232 - loglik: -6.2110e+02 - logprior: -8.1957e-01
Fitted a model with MAP estimate = -621.0849
expansions: [(0, 4), (13, 1), (14, 1), (19, 1), (20, 1), (26, 1), (30, 1), (31, 1), (43, 1), (46, 3), (47, 2), (72, 1), (75, 2), (76, 1), (77, 2), (78, 1), (98, 2), (114, 2), (115, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 651.7309 - loglik: -6.2557e+02 - logprior: -2.6163e+01
Epoch 2/2
10/10 - 2s - loss: 613.9410 - loglik: -6.0675e+02 - logprior: -7.1908e+00
Fitted a model with MAP estimate = -606.2972
expansions: []
discards: [ 59  62 122]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 620.9576 - loglik: -6.0259e+02 - logprior: -1.8371e+01
Epoch 2/2
10/10 - 2s - loss: 604.2793 - loglik: -6.0042e+02 - logprior: -3.8571e+00
Fitted a model with MAP estimate = -601.5037
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 618.5379 - loglik: -6.0122e+02 - logprior: -1.7321e+01
Epoch 2/10
10/10 - 2s - loss: 602.9008 - loglik: -5.9941e+02 - logprior: -3.4940e+00
Epoch 3/10
10/10 - 2s - loss: 600.3829 - loglik: -5.9953e+02 - logprior: -8.5182e-01
Epoch 4/10
10/10 - 2s - loss: 598.6735 - loglik: -5.9875e+02 - logprior: 0.0795
Epoch 5/10
10/10 - 2s - loss: 597.8838 - loglik: -5.9841e+02 - logprior: 0.5315
Epoch 6/10
10/10 - 2s - loss: 594.9319 - loglik: -5.9579e+02 - logprior: 0.8632
Epoch 7/10
10/10 - 2s - loss: 595.4341 - loglik: -5.9656e+02 - logprior: 1.1263
Fitted a model with MAP estimate = -593.5836
Time for alignment: 52.3183
Computed alignments with likelihoods: ['-589.6442', '-589.6932', '-589.6511', '-589.7004', '-593.5836']
Best model has likelihood: -589.6442  (prior= 1.4028 )
time for generating output: 0.1556
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DMRL_synthase.projection.fasta
SP score = 0.9174061433447099
Training of 5 independent models on file Stap_Strp_toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea77f2a730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea66bccf10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea55d56b80>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 567.0820 - loglik: -4.9860e+02 - logprior: -6.8477e+01
Epoch 2/10
10/10 - 2s - loss: 483.2733 - loglik: -4.6604e+02 - logprior: -1.7232e+01
Epoch 3/10
10/10 - 1s - loss: 449.8411 - loglik: -4.4248e+02 - logprior: -7.3619e+00
Epoch 4/10
10/10 - 1s - loss: 435.3976 - loglik: -4.3161e+02 - logprior: -3.7907e+00
Epoch 5/10
10/10 - 1s - loss: 428.0477 - loglik: -4.2616e+02 - logprior: -1.8861e+00
Epoch 6/10
10/10 - 1s - loss: 423.7669 - loglik: -4.2281e+02 - logprior: -9.5229e-01
Epoch 7/10
10/10 - 1s - loss: 421.6533 - loglik: -4.2114e+02 - logprior: -5.0860e-01
Epoch 8/10
10/10 - 1s - loss: 420.7118 - loglik: -4.2053e+02 - logprior: -1.8506e-01
Epoch 9/10
10/10 - 1s - loss: 420.1754 - loglik: -4.2025e+02 - logprior: 0.0720
Epoch 10/10
10/10 - 1s - loss: 419.8272 - loglik: -4.2009e+02 - logprior: 0.2638
Fitted a model with MAP estimate = -419.6632
expansions: [(14, 1), (15, 2), (25, 1), (26, 2), (27, 2), (39, 1), (52, 1), (58, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 502.1007 - loglik: -4.2534e+02 - logprior: -7.6763e+01
Epoch 2/2
10/10 - 2s - loss: 445.8845 - loglik: -4.1515e+02 - logprior: -3.0730e+01
Fitted a model with MAP estimate = -436.7896
expansions: []
discards: [ 0 15 73]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 491.9574 - loglik: -4.1674e+02 - logprior: -7.5216e+01
Epoch 2/2
10/10 - 2s - loss: 438.3675 - loglik: -4.1276e+02 - logprior: -2.5611e+01
Fitted a model with MAP estimate = -427.1170
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 491.9171 - loglik: -4.1602e+02 - logprior: -7.5901e+01
Epoch 2/10
10/10 - 1s - loss: 443.1938 - loglik: -4.1327e+02 - logprior: -2.9924e+01
Epoch 3/10
10/10 - 2s - loss: 429.9630 - loglik: -4.1332e+02 - logprior: -1.6643e+01
Epoch 4/10
10/10 - 1s - loss: 417.4394 - loglik: -4.1277e+02 - logprior: -4.6711e+00
Epoch 5/10
10/10 - 1s - loss: 411.9035 - loglik: -4.1216e+02 - logprior: 0.2606
Epoch 6/10
10/10 - 1s - loss: 410.0671 - loglik: -4.1190e+02 - logprior: 1.8314
Epoch 7/10
10/10 - 1s - loss: 409.1429 - loglik: -4.1180e+02 - logprior: 2.6565
Epoch 8/10
10/10 - 1s - loss: 408.5636 - loglik: -4.1180e+02 - logprior: 3.2337
Epoch 9/10
10/10 - 1s - loss: 408.1064 - loglik: -4.1179e+02 - logprior: 3.6862
Epoch 10/10
10/10 - 1s - loss: 407.6528 - loglik: -4.1170e+02 - logprior: 4.0516
Fitted a model with MAP estimate = -407.3847
Time for alignment: 50.4525
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 567.0444 - loglik: -4.9857e+02 - logprior: -6.8477e+01
Epoch 2/10
10/10 - 1s - loss: 482.9603 - loglik: -4.6572e+02 - logprior: -1.7235e+01
Epoch 3/10
10/10 - 1s - loss: 449.8595 - loglik: -4.4246e+02 - logprior: -7.4043e+00
Epoch 4/10
10/10 - 1s - loss: 434.2914 - loglik: -4.3044e+02 - logprior: -3.8550e+00
Epoch 5/10
10/10 - 1s - loss: 426.4671 - loglik: -4.2454e+02 - logprior: -1.9301e+00
Epoch 6/10
10/10 - 1s - loss: 422.7617 - loglik: -4.2190e+02 - logprior: -8.6169e-01
Epoch 7/10
10/10 - 1s - loss: 420.8665 - loglik: -4.2053e+02 - logprior: -3.3143e-01
Epoch 8/10
10/10 - 1s - loss: 419.5473 - loglik: -4.1945e+02 - logprior: -9.4527e-02
Epoch 9/10
10/10 - 1s - loss: 418.7628 - loglik: -4.1892e+02 - logprior: 0.1580
Epoch 10/10
10/10 - 1s - loss: 418.3569 - loglik: -4.1870e+02 - logprior: 0.3462
Fitted a model with MAP estimate = -418.1835
expansions: [(9, 3), (14, 2), (24, 2), (26, 1), (27, 3), (39, 1), (52, 1), (58, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 501.2328 - loglik: -4.2461e+02 - logprior: -7.6627e+01
Epoch 2/2
10/10 - 2s - loss: 444.3014 - loglik: -4.1361e+02 - logprior: -3.0688e+01
Fitted a model with MAP estimate = -434.8258
expansions: []
discards: [ 0 16 76]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 489.8097 - loglik: -4.1462e+02 - logprior: -7.5195e+01
Epoch 2/2
10/10 - 2s - loss: 435.8824 - loglik: -4.1003e+02 - logprior: -2.5853e+01
Fitted a model with MAP estimate = -424.3539
expansions: [(0, 3)]
discards: [ 0 26]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 471.6645 - loglik: -4.1139e+02 - logprior: -6.0276e+01
Epoch 2/10
10/10 - 2s - loss: 421.3633 - loglik: -4.0674e+02 - logprior: -1.4623e+01
Epoch 3/10
10/10 - 2s - loss: 411.2628 - loglik: -4.0612e+02 - logprior: -5.1408e+00
Epoch 4/10
10/10 - 2s - loss: 406.9696 - loglik: -4.0583e+02 - logprior: -1.1405e+00
Epoch 5/10
10/10 - 2s - loss: 404.2830 - loglik: -4.0536e+02 - logprior: 1.0804
Epoch 6/10
10/10 - 2s - loss: 402.7031 - loglik: -4.0507e+02 - logprior: 2.3688
Epoch 7/10
10/10 - 2s - loss: 401.8430 - loglik: -4.0500e+02 - logprior: 3.1553
Epoch 8/10
10/10 - 2s - loss: 401.2783 - loglik: -4.0494e+02 - logprior: 3.6591
Epoch 9/10
10/10 - 2s - loss: 400.8713 - loglik: -4.0492e+02 - logprior: 4.0510
Epoch 10/10
10/10 - 2s - loss: 400.5493 - loglik: -4.0493e+02 - logprior: 4.3837
Fitted a model with MAP estimate = -400.3511
Time for alignment: 50.4493
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 567.0165 - loglik: -4.9854e+02 - logprior: -6.8479e+01
Epoch 2/10
10/10 - 1s - loss: 483.1091 - loglik: -4.6588e+02 - logprior: -1.7233e+01
Epoch 3/10
10/10 - 1s - loss: 449.6702 - loglik: -4.4228e+02 - logprior: -7.3903e+00
Epoch 4/10
10/10 - 1s - loss: 433.5617 - loglik: -4.2963e+02 - logprior: -3.9315e+00
Epoch 5/10
10/10 - 1s - loss: 426.4513 - loglik: -4.2448e+02 - logprior: -1.9700e+00
Epoch 6/10
10/10 - 1s - loss: 422.9597 - loglik: -4.2208e+02 - logprior: -8.7486e-01
Epoch 7/10
10/10 - 1s - loss: 421.2489 - loglik: -4.2093e+02 - logprior: -3.1447e-01
Epoch 8/10
10/10 - 1s - loss: 420.3759 - loglik: -4.2035e+02 - logprior: -2.7400e-02
Epoch 9/10
10/10 - 1s - loss: 419.8362 - loglik: -4.2003e+02 - logprior: 0.1967
Epoch 10/10
10/10 - 1s - loss: 419.4583 - loglik: -4.1980e+02 - logprior: 0.3455
Fitted a model with MAP estimate = -419.2997
expansions: [(9, 3), (14, 2), (25, 1), (26, 2), (27, 2), (39, 1), (52, 1), (58, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 501.5663 - loglik: -4.2490e+02 - logprior: -7.6665e+01
Epoch 2/2
10/10 - 2s - loss: 444.7919 - loglik: -4.1419e+02 - logprior: -3.0601e+01
Fitted a model with MAP estimate = -435.4036
expansions: []
discards: [31 75]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 487.3369 - loglik: -4.1423e+02 - logprior: -7.3110e+01
Epoch 2/2
10/10 - 2s - loss: 430.6955 - loglik: -4.0963e+02 - logprior: -2.1062e+01
Fitted a model with MAP estimate = -419.7911
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 472.0290 - loglik: -4.1069e+02 - logprior: -6.1338e+01
Epoch 2/10
10/10 - 2s - loss: 422.8823 - loglik: -4.0795e+02 - logprior: -1.4930e+01
Epoch 3/10
10/10 - 2s - loss: 413.0355 - loglik: -4.0777e+02 - logprior: -5.2671e+00
Epoch 4/10
10/10 - 2s - loss: 408.7311 - loglik: -4.0746e+02 - logprior: -1.2723e+00
Epoch 5/10
10/10 - 2s - loss: 406.1667 - loglik: -4.0702e+02 - logprior: 0.8489
Epoch 6/10
10/10 - 2s - loss: 404.5568 - loglik: -4.0658e+02 - logprior: 2.0232
Epoch 7/10
10/10 - 2s - loss: 403.5977 - loglik: -4.0636e+02 - logprior: 2.7677
Epoch 8/10
10/10 - 2s - loss: 402.9611 - loglik: -4.0634e+02 - logprior: 3.3815
Epoch 9/10
10/10 - 2s - loss: 402.4983 - loglik: -4.0637e+02 - logprior: 3.8767
Epoch 10/10
10/10 - 2s - loss: 402.1143 - loglik: -4.0637e+02 - logprior: 4.2613
Fitted a model with MAP estimate = -401.8780
Time for alignment: 50.8690
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 567.0667 - loglik: -4.9859e+02 - logprior: -6.8476e+01
Epoch 2/10
10/10 - 1s - loss: 483.1293 - loglik: -4.6590e+02 - logprior: -1.7231e+01
Epoch 3/10
10/10 - 1s - loss: 449.7592 - loglik: -4.4237e+02 - logprior: -7.3896e+00
Epoch 4/10
10/10 - 1s - loss: 435.4511 - loglik: -4.3158e+02 - logprior: -3.8721e+00
Epoch 5/10
10/10 - 1s - loss: 428.3276 - loglik: -4.2628e+02 - logprior: -2.0508e+00
Epoch 6/10
10/10 - 1s - loss: 423.8795 - loglik: -4.2283e+02 - logprior: -1.0537e+00
Epoch 7/10
10/10 - 1s - loss: 421.3949 - loglik: -4.2086e+02 - logprior: -5.3181e-01
Epoch 8/10
10/10 - 1s - loss: 419.8638 - loglik: -4.1963e+02 - logprior: -2.3567e-01
Epoch 9/10
10/10 - 1s - loss: 419.0004 - loglik: -4.1896e+02 - logprior: -3.4756e-02
Epoch 10/10
10/10 - 1s - loss: 418.5115 - loglik: -4.1865e+02 - logprior: 0.1396
Fitted a model with MAP estimate = -418.3161
expansions: [(14, 2), (15, 1), (25, 2), (26, 2), (27, 2), (39, 1), (52, 1), (58, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 501.4450 - loglik: -4.2465e+02 - logprior: -7.6795e+01
Epoch 2/2
10/10 - 2s - loss: 444.4974 - loglik: -4.1374e+02 - logprior: -3.0758e+01
Fitted a model with MAP estimate = -435.1561
expansions: []
discards: [ 0 13 27 74]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 490.5366 - loglik: -4.1528e+02 - logprior: -7.5253e+01
Epoch 2/2
10/10 - 1s - loss: 436.7456 - loglik: -4.1095e+02 - logprior: -2.5800e+01
Fitted a model with MAP estimate = -425.4348
expansions: [(5, 1), (7, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 491.2773 - loglik: -4.1416e+02 - logprior: -7.7113e+01
Epoch 2/10
10/10 - 2s - loss: 441.1418 - loglik: -4.1047e+02 - logprior: -3.0673e+01
Epoch 3/10
10/10 - 2s - loss: 431.1473 - loglik: -4.1016e+02 - logprior: -2.0988e+01
Epoch 4/10
10/10 - 1s - loss: 426.3896 - loglik: -4.0976e+02 - logprior: -1.6634e+01
Epoch 5/10
10/10 - 2s - loss: 420.6422 - loglik: -4.0917e+02 - logprior: -1.1469e+01
Epoch 6/10
10/10 - 2s - loss: 410.1436 - loglik: -4.0854e+02 - logprior: -1.6026e+00
Epoch 7/10
10/10 - 1s - loss: 405.7354 - loglik: -4.0797e+02 - logprior: 2.2326
Epoch 8/10
10/10 - 2s - loss: 404.9417 - loglik: -4.0807e+02 - logprior: 3.1293
Epoch 9/10
10/10 - 2s - loss: 404.3358 - loglik: -4.0809e+02 - logprior: 3.7503
Epoch 10/10
10/10 - 2s - loss: 403.9035 - loglik: -4.0807e+02 - logprior: 4.1666
Fitted a model with MAP estimate = -403.6860
Time for alignment: 50.0163
Fitting a model of length 71 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 567.0670 - loglik: -4.9859e+02 - logprior: -6.8479e+01
Epoch 2/10
10/10 - 2s - loss: 483.0544 - loglik: -4.6583e+02 - logprior: -1.7229e+01
Epoch 3/10
10/10 - 1s - loss: 449.6681 - loglik: -4.4228e+02 - logprior: -7.3892e+00
Epoch 4/10
10/10 - 1s - loss: 434.0268 - loglik: -4.3008e+02 - logprior: -3.9439e+00
Epoch 5/10
10/10 - 1s - loss: 426.8269 - loglik: -4.2473e+02 - logprior: -2.0932e+00
Epoch 6/10
10/10 - 1s - loss: 422.8273 - loglik: -4.2177e+02 - logprior: -1.0588e+00
Epoch 7/10
10/10 - 1s - loss: 420.7810 - loglik: -4.2025e+02 - logprior: -5.2622e-01
Epoch 8/10
10/10 - 1s - loss: 419.7346 - loglik: -4.1949e+02 - logprior: -2.4428e-01
Epoch 9/10
10/10 - 1s - loss: 419.0832 - loglik: -4.1906e+02 - logprior: -2.0473e-02
Epoch 10/10
10/10 - 1s - loss: 418.6863 - loglik: -4.1887e+02 - logprior: 0.1817
Fitted a model with MAP estimate = -418.5201
expansions: [(14, 2), (15, 1), (25, 2), (26, 2), (27, 2), (39, 1), (52, 1), (58, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 501.6055 - loglik: -4.2479e+02 - logprior: -7.6818e+01
Epoch 2/2
10/10 - 2s - loss: 444.8421 - loglik: -4.1408e+02 - logprior: -3.0767e+01
Fitted a model with MAP estimate = -435.4721
expansions: []
discards: [ 0 13 27 74]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 490.8782 - loglik: -4.1561e+02 - logprior: -7.5265e+01
Epoch 2/2
10/10 - 1s - loss: 437.1702 - loglik: -4.1143e+02 - logprior: -2.5742e+01
Fitted a model with MAP estimate = -425.9084
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 640 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 490.6093 - loglik: -4.1459e+02 - logprior: -7.6021e+01
Epoch 2/10
10/10 - 2s - loss: 441.9470 - loglik: -4.1183e+02 - logprior: -3.0122e+01
Epoch 3/10
10/10 - 1s - loss: 429.0753 - loglik: -4.1187e+02 - logprior: -1.7207e+01
Epoch 4/10
10/10 - 1s - loss: 416.5728 - loglik: -4.1142e+02 - logprior: -5.1566e+00
Epoch 5/10
10/10 - 1s - loss: 410.7040 - loglik: -4.1086e+02 - logprior: 0.1574
Epoch 6/10
10/10 - 1s - loss: 408.8378 - loglik: -4.1059e+02 - logprior: 1.7513
Epoch 7/10
10/10 - 1s - loss: 407.9155 - loglik: -4.1052e+02 - logprior: 2.6005
Epoch 8/10
10/10 - 1s - loss: 407.2911 - loglik: -4.1048e+02 - logprior: 3.1846
Epoch 9/10
10/10 - 1s - loss: 406.7734 - loglik: -4.1041e+02 - logprior: 3.6403
Epoch 10/10
10/10 - 1s - loss: 406.2852 - loglik: -4.1028e+02 - logprior: 3.9982
Fitted a model with MAP estimate = -406.0299
Time for alignment: 49.6294
Computed alignments with likelihoods: ['-407.3847', '-400.3511', '-401.8780', '-403.6860', '-406.0299']
Best model has likelihood: -400.3511  (prior= 4.5550 )
time for generating output: 0.1993
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Stap_Strp_toxin.projection.fasta
SP score = 0.3304664240820377
Training of 5 independent models on file ghf5.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea096b1fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bc535e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bd15580>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 1521.0692 - loglik: -1.5132e+03 - logprior: -7.9038e+00
Epoch 2/10
20/20 - 9s - loss: 1445.2693 - loglik: -1.4454e+03 - logprior: 0.1221
Epoch 3/10
20/20 - 9s - loss: 1423.8872 - loglik: -1.4239e+03 - logprior: -1.4939e-02
Epoch 4/10
20/20 - 9s - loss: 1411.9979 - loglik: -1.4122e+03 - logprior: 0.2436
Epoch 5/10
20/20 - 9s - loss: 1401.4844 - loglik: -1.4018e+03 - logprior: 0.3633
Epoch 6/10
20/20 - 9s - loss: 1400.0575 - loglik: -1.4005e+03 - logprior: 0.4253
Epoch 7/10
20/20 - 9s - loss: 1371.9408 - loglik: -1.3724e+03 - logprior: 0.4746
Epoch 8/10
20/20 - 9s - loss: 1253.2617 - loglik: -1.2523e+03 - logprior: -9.4599e-01
Epoch 9/10
20/20 - 9s - loss: 1128.6643 - loglik: -1.1241e+03 - logprior: -4.5501e+00
Epoch 10/10
20/20 - 9s - loss: 1105.1951 - loglik: -1.1006e+03 - logprior: -4.5882e+00
Fitted a model with MAP estimate = -1100.1453
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  64
  65  68  69  70  71  72 100 101 102 103 104 105 106 107 108 109 110 111
 161 181 182 183 184 185 186 187 188 189 190 214]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 8s - loss: 1573.2471 - loglik: -1.5631e+03 - logprior: -1.0186e+01
Epoch 2/2
20/20 - 5s - loss: 1534.9734 - loglik: -1.5352e+03 - logprior: 0.2242
Fitted a model with MAP estimate = -1533.3960
expansions: [(0, 171), (141, 84)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1514.1365 - loglik: -1.5062e+03 - logprior: -7.9171e+00
Epoch 2/2
20/20 - 11s - loss: 1439.0876 - loglik: -1.4397e+03 - logprior: 0.6386
Fitted a model with MAP estimate = -1419.5981
expansions: [(0, 4)]
discards: [  0 171 195 196]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 1419.8568 - loglik: -1.4124e+03 - logprior: -7.4687e+00
Epoch 2/10
20/20 - 11s - loss: 1407.8785 - loglik: -1.4080e+03 - logprior: 0.1371
Epoch 3/10
20/20 - 11s - loss: 1394.6938 - loglik: -1.3957e+03 - logprior: 0.9980
Epoch 4/10
20/20 - 11s - loss: 1404.2133 - loglik: -1.4055e+03 - logprior: 1.2945
Fitted a model with MAP estimate = -1394.1881
Time for alignment: 203.0497
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 1517.3864 - loglik: -1.5095e+03 - logprior: -7.9028e+00
Epoch 2/10
20/20 - 9s - loss: 1448.2708 - loglik: -1.4485e+03 - logprior: 0.2159
Epoch 3/10
20/20 - 9s - loss: 1420.3059 - loglik: -1.4206e+03 - logprior: 0.3268
Epoch 4/10
20/20 - 9s - loss: 1408.8049 - loglik: -1.4093e+03 - logprior: 0.4969
Epoch 5/10
20/20 - 9s - loss: 1399.8882 - loglik: -1.4004e+03 - logprior: 0.5188
Epoch 6/10
20/20 - 9s - loss: 1401.3225 - loglik: -1.4019e+03 - logprior: 0.5997
Fitted a model with MAP estimate = -1388.0627
expansions: [(0, 3), (50, 2), (52, 2), (53, 1), (86, 12), (92, 1), (93, 1), (94, 1), (116, 2), (118, 1), (124, 1), (154, 2), (172, 10), (175, 1), (176, 1), (198, 3), (200, 1), (206, 1)]
discards: [  0   1 217 218 219]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1433.9584 - loglik: -1.4260e+03 - logprior: -7.9177e+00
Epoch 2/2
20/20 - 11s - loss: 1401.0129 - loglik: -1.4010e+03 - logprior: -5.1353e-02
Fitted a model with MAP estimate = -1400.3073
expansions: [(0, 4), (81, 1), (244, 1), (261, 1)]
discards: [  1   2   4  52  83  84  95 137 180 199 200 201 202 203 204 205]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 13s - loss: 1415.7129 - loglik: -1.4057e+03 - logprior: -9.9806e+00
Epoch 2/2
20/20 - 11s - loss: 1405.3647 - loglik: -1.4053e+03 - logprior: -3.4269e-02
Fitted a model with MAP estimate = -1400.4385
expansions: [(0, 3), (84, 2), (99, 1), (196, 7)]
discards: [  3   4 227]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 15s - loss: 1415.1254 - loglik: -1.4039e+03 - logprior: -1.1199e+01
Epoch 2/10
20/20 - 11s - loss: 1402.9539 - loglik: -1.4029e+03 - logprior: -1.0185e-01
Epoch 3/10
20/20 - 11s - loss: 1391.7531 - loglik: -1.3932e+03 - logprior: 1.4024
Epoch 4/10
20/20 - 11s - loss: 1398.7573 - loglik: -1.4007e+03 - logprior: 1.8966
Fitted a model with MAP estimate = -1391.0245
Time for alignment: 186.0937
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 1521.2172 - loglik: -1.5133e+03 - logprior: -7.9001e+00
Epoch 2/10
20/20 - 9s - loss: 1453.9930 - loglik: -1.4542e+03 - logprior: 0.2352
Epoch 3/10
20/20 - 9s - loss: 1422.9470 - loglik: -1.4232e+03 - logprior: 0.2912
Epoch 4/10
20/20 - 9s - loss: 1416.4182 - loglik: -1.4169e+03 - logprior: 0.5023
Epoch 5/10
20/20 - 9s - loss: 1409.7854 - loglik: -1.4104e+03 - logprior: 0.6077
Epoch 6/10
20/20 - 9s - loss: 1396.1321 - loglik: -1.3968e+03 - logprior: 0.7032
Epoch 7/10
20/20 - 9s - loss: 1379.1688 - loglik: -1.3799e+03 - logprior: 0.6889
Epoch 8/10
20/20 - 9s - loss: 1249.7755 - loglik: -1.2488e+03 - logprior: -9.9783e-01
Epoch 9/10
20/20 - 9s - loss: 1130.8005 - loglik: -1.1256e+03 - logprior: -5.1508e+00
Epoch 10/10
20/20 - 9s - loss: 1109.4183 - loglik: -1.1042e+03 - logprior: -5.1641e+00
Fitted a model with MAP estimate = -1101.7535
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  53  54  55  56  57  58  59
  60  61  62  63  64  67  68  69  99 100 106 107 108 109 110 111 112 113
 114 115 116 161 162 163 181 182 183 184 185 186 187 188 189]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 8s - loss: 1573.8883 - loglik: -1.5612e+03 - logprior: -1.2693e+01
Epoch 2/2
20/20 - 5s - loss: 1537.6886 - loglik: -1.5348e+03 - logprior: -2.8849e+00
Fitted a model with MAP estimate = -1535.4419
expansions: [(0, 160), (138, 96)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1509.8376 - loglik: -1.5019e+03 - logprior: -7.9503e+00
Epoch 2/2
20/20 - 11s - loss: 1438.5845 - loglik: -1.4392e+03 - logprior: 0.6349
Fitted a model with MAP estimate = -1418.8448
expansions: [(0, 4)]
discards: [  0   1 193 194 195 196 197 198 199]
Re-initialized the encoder parameters.
Fitting a model of length 254 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 1424.9932 - loglik: -1.4176e+03 - logprior: -7.3800e+00
Epoch 2/10
20/20 - 10s - loss: 1404.2410 - loglik: -1.4045e+03 - logprior: 0.3039
Epoch 3/10
20/20 - 10s - loss: 1407.0500 - loglik: -1.4081e+03 - logprior: 1.0926
Fitted a model with MAP estimate = -1399.5185
Time for alignment: 189.9769
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 1517.2256 - loglik: -1.5093e+03 - logprior: -7.8977e+00
Epoch 2/10
20/20 - 9s - loss: 1450.8358 - loglik: -1.4510e+03 - logprior: 0.1776
Epoch 3/10
20/20 - 9s - loss: 1424.1431 - loglik: -1.4243e+03 - logprior: 0.1677
Epoch 4/10
20/20 - 9s - loss: 1406.9371 - loglik: -1.4075e+03 - logprior: 0.5326
Epoch 5/10
20/20 - 9s - loss: 1404.8474 - loglik: -1.4055e+03 - logprior: 0.6876
Epoch 6/10
20/20 - 9s - loss: 1393.9773 - loglik: -1.3947e+03 - logprior: 0.7713
Epoch 7/10
20/20 - 9s - loss: 1370.9486 - loglik: -1.3717e+03 - logprior: 0.7834
Epoch 8/10
20/20 - 9s - loss: 1237.5510 - loglik: -1.2365e+03 - logprior: -1.0212e+00
Epoch 9/10
20/20 - 9s - loss: 1124.0938 - loglik: -1.1192e+03 - logprior: -4.8893e+00
Epoch 10/10
20/20 - 9s - loss: 1101.8804 - loglik: -1.0978e+03 - logprior: -4.0643e+00
Fitted a model with MAP estimate = -1098.0194
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
 101 102 103 104 105 106 107 108 109 110 111 112 161 162 163 164 182 183
 184 185 186 187 188 189 214]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 8s - loss: 1575.6375 - loglik: -1.5656e+03 - logprior: -1.0079e+01
Epoch 2/2
20/20 - 5s - loss: 1537.6616 - loglik: -1.5377e+03 - logprior: 0.0624
Fitted a model with MAP estimate = -1534.5463
expansions: [(0, 124), (128, 131)]
discards: [  4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21
  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39
  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57
  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1517.1771 - loglik: -1.5086e+03 - logprior: -8.6057e+00
Epoch 2/2
20/20 - 11s - loss: 1440.4613 - loglik: -1.4409e+03 - logprior: 0.3991
Fitted a model with MAP estimate = -1421.9180
expansions: [(134, 3)]
discards: [  0   1  91 198 199 200 201]
Re-initialized the encoder parameters.
Fitting a model of length 255 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 13s - loss: 1432.0066 - loglik: -1.4211e+03 - logprior: -1.0895e+01
Epoch 2/10
20/20 - 11s - loss: 1412.0291 - loglik: -1.4088e+03 - logprior: -3.2057e+00
Epoch 3/10
20/20 - 11s - loss: 1405.1891 - loglik: -1.4045e+03 - logprior: -7.1180e-01
Epoch 4/10
20/20 - 11s - loss: 1397.0056 - loglik: -1.3984e+03 - logprior: 1.4150
Epoch 5/10
20/20 - 11s - loss: 1394.6477 - loglik: -1.3961e+03 - logprior: 1.4944
Epoch 6/10
20/20 - 11s - loss: 1389.1244 - loglik: -1.3909e+03 - logprior: 1.7425
Epoch 7/10
20/20 - 11s - loss: 1364.4558 - loglik: -1.3663e+03 - logprior: 1.8048
Epoch 8/10
20/20 - 11s - loss: 1278.3928 - loglik: -1.2797e+03 - logprior: 1.2809
Epoch 9/10
20/20 - 11s - loss: 1144.5194 - loglik: -1.1413e+03 - logprior: -3.1917e+00
Epoch 10/10
20/20 - 10s - loss: 1114.6857 - loglik: -1.1117e+03 - logprior: -3.0152e+00
Fitted a model with MAP estimate = -1105.5532
Time for alignment: 263.3249
Fitting a model of length 225 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 12s - loss: 1518.9895 - loglik: -1.5111e+03 - logprior: -7.8964e+00
Epoch 2/10
20/20 - 9s - loss: 1453.1455 - loglik: -1.4533e+03 - logprior: 0.1324
Epoch 3/10
20/20 - 9s - loss: 1418.1937 - loglik: -1.4182e+03 - logprior: 0.0527
Epoch 4/10
20/20 - 9s - loss: 1411.4883 - loglik: -1.4119e+03 - logprior: 0.3850
Epoch 5/10
20/20 - 9s - loss: 1408.2792 - loglik: -1.4088e+03 - logprior: 0.5571
Epoch 6/10
20/20 - 9s - loss: 1390.2936 - loglik: -1.3909e+03 - logprior: 0.6585
Epoch 7/10
20/20 - 9s - loss: 1379.1368 - loglik: -1.3798e+03 - logprior: 0.7058
Epoch 8/10
20/20 - 9s - loss: 1255.3937 - loglik: -1.2546e+03 - logprior: -8.0643e-01
Epoch 9/10
20/20 - 9s - loss: 1131.9626 - loglik: -1.1274e+03 - logprior: -4.5135e+00
Epoch 10/10
20/20 - 9s - loss: 1105.7087 - loglik: -1.1016e+03 - logprior: -4.1192e+00
Fitted a model with MAP estimate = -1100.0700
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  69 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 161 162
 163 181 182 183 184 185 186 187 188 214]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 9s - loss: 1574.2059 - loglik: -1.5639e+03 - logprior: -1.0335e+01
Epoch 2/2
20/20 - 5s - loss: 1536.7732 - loglik: -1.5370e+03 - logprior: 0.2538
Fitted a model with MAP estimate = -1534.3265
expansions: [(0, 170), (143, 87)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138]
Re-initialized the encoder parameters.
Fitting a model of length 261 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
20/20 - 14s - loss: 1512.4978 - loglik: -1.5046e+03 - logprior: -7.9234e+00
Epoch 2/2
20/20 - 11s - loss: 1437.4474 - loglik: -1.4380e+03 - logprior: 0.5387
Fitted a model with MAP estimate = -1415.4384
expansions: [(0, 4), (249, 1)]
discards: [  0  42  43  93  94 130 131 132 133 134 135 170 171]
Re-initialized the encoder parameters.
Fitting a model of length 253 on 2717 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 14s - loss: 1423.2185 - loglik: -1.4159e+03 - logprior: -7.3675e+00
Epoch 2/10
20/20 - 10s - loss: 1403.6750 - loglik: -1.4039e+03 - logprior: 0.2334
Epoch 3/10
20/20 - 10s - loss: 1399.6659 - loglik: -1.4007e+03 - logprior: 1.0317
Epoch 4/10
20/20 - 10s - loss: 1401.1976 - loglik: -1.4026e+03 - logprior: 1.3897
Fitted a model with MAP estimate = -1395.3308
Time for alignment: 201.0679
Computed alignments with likelihoods: ['-1100.1453', '-1388.0627', '-1101.7535', '-1098.0194', '-1100.0700']
Best model has likelihood: -1098.0194  (prior= -3.9285 )
time for generating output: 0.3231
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf5.projection.fasta
SP score = 0.021339220014716703
Training of 5 independent models on file myb_DNA-binding.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b767a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea00d68040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea093fcf40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 271.3423 - loglik: -2.6810e+02 - logprior: -3.2432e+00
Epoch 2/10
19/19 - 1s - loss: 249.7504 - loglik: -2.4831e+02 - logprior: -1.4363e+00
Epoch 3/10
19/19 - 1s - loss: 242.4139 - loglik: -2.4119e+02 - logprior: -1.2209e+00
Epoch 4/10
19/19 - 1s - loss: 240.5558 - loglik: -2.3936e+02 - logprior: -1.1902e+00
Epoch 5/10
19/19 - 1s - loss: 239.2379 - loglik: -2.3805e+02 - logprior: -1.1893e+00
Epoch 6/10
19/19 - 1s - loss: 238.7513 - loglik: -2.3759e+02 - logprior: -1.1588e+00
Epoch 7/10
19/19 - 1s - loss: 238.5150 - loglik: -2.3737e+02 - logprior: -1.1391e+00
Epoch 8/10
19/19 - 1s - loss: 237.4664 - loglik: -2.3632e+02 - logprior: -1.1342e+00
Epoch 9/10
19/19 - 1s - loss: 235.0858 - loglik: -2.3399e+02 - logprior: -1.0891e+00
Epoch 10/10
19/19 - 1s - loss: 233.8991 - loglik: -2.3287e+02 - logprior: -1.0184e+00
Fitted a model with MAP estimate = -230.7669
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (20, 1), (22, 2), (23, 1), (35, 1), (36, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 238.3168 - loglik: -2.3487e+02 - logprior: -3.4517e+00
Epoch 2/2
19/19 - 1s - loss: 230.5725 - loglik: -2.2910e+02 - logprior: -1.4697e+00
Fitted a model with MAP estimate = -226.8285
expansions: []
discards: [29 45]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 233.2135 - loglik: -2.2984e+02 - logprior: -3.3736e+00
Epoch 2/2
19/19 - 1s - loss: 230.0359 - loglik: -2.2865e+02 - logprior: -1.3854e+00
Fitted a model with MAP estimate = -226.4252
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 229.6172 - loglik: -2.2639e+02 - logprior: -3.2281e+00
Epoch 2/10
19/19 - 1s - loss: 226.2679 - loglik: -2.2491e+02 - logprior: -1.3597e+00
Epoch 3/10
19/19 - 1s - loss: 226.3548 - loglik: -2.2513e+02 - logprior: -1.2221e+00
Fitted a model with MAP estimate = -225.8491
Time for alignment: 39.0673
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 271.4772 - loglik: -2.6823e+02 - logprior: -3.2450e+00
Epoch 2/10
19/19 - 1s - loss: 250.6947 - loglik: -2.4927e+02 - logprior: -1.4264e+00
Epoch 3/10
19/19 - 1s - loss: 243.6833 - loglik: -2.4248e+02 - logprior: -1.2018e+00
Epoch 4/10
19/19 - 1s - loss: 240.6970 - loglik: -2.3954e+02 - logprior: -1.1579e+00
Epoch 5/10
19/19 - 1s - loss: 239.5235 - loglik: -2.3835e+02 - logprior: -1.1666e+00
Epoch 6/10
19/19 - 1s - loss: 238.7081 - loglik: -2.3755e+02 - logprior: -1.1562e+00
Epoch 7/10
19/19 - 1s - loss: 237.8990 - loglik: -2.3675e+02 - logprior: -1.1394e+00
Epoch 8/10
19/19 - 1s - loss: 237.9102 - loglik: -2.3675e+02 - logprior: -1.1496e+00
Fitted a model with MAP estimate = -234.0793
expansions: [(9, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (28, 2), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 239.8157 - loglik: -2.3646e+02 - logprior: -3.3570e+00
Epoch 2/2
19/19 - 1s - loss: 230.8996 - loglik: -2.2946e+02 - logprior: -1.4347e+00
Fitted a model with MAP estimate = -226.6157
expansions: []
discards: [29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 233.2021 - loglik: -2.2988e+02 - logprior: -3.3199e+00
Epoch 2/2
19/19 - 1s - loss: 230.0165 - loglik: -2.2863e+02 - logprior: -1.3895e+00
Fitted a model with MAP estimate = -226.6375
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 229.5928 - loglik: -2.2638e+02 - logprior: -3.2137e+00
Epoch 2/10
19/19 - 1s - loss: 227.2923 - loglik: -2.2591e+02 - logprior: -1.3784e+00
Epoch 3/10
19/19 - 1s - loss: 226.3154 - loglik: -2.2508e+02 - logprior: -1.2349e+00
Epoch 4/10
19/19 - 1s - loss: 226.0389 - loglik: -2.2485e+02 - logprior: -1.1903e+00
Epoch 5/10
19/19 - 1s - loss: 225.4097 - loglik: -2.2423e+02 - logprior: -1.1741e+00
Epoch 6/10
19/19 - 1s - loss: 225.4838 - loglik: -2.2432e+02 - logprior: -1.1625e+00
Fitted a model with MAP estimate = -224.6523
Time for alignment: 39.3322
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 271.5461 - loglik: -2.6830e+02 - logprior: -3.2421e+00
Epoch 2/10
19/19 - 1s - loss: 250.5986 - loglik: -2.4926e+02 - logprior: -1.3390e+00
Epoch 3/10
19/19 - 1s - loss: 244.7290 - loglik: -2.4336e+02 - logprior: -1.3652e+00
Epoch 4/10
19/19 - 1s - loss: 242.4576 - loglik: -2.4116e+02 - logprior: -1.2971e+00
Epoch 5/10
19/19 - 1s - loss: 242.0706 - loglik: -2.4080e+02 - logprior: -1.2691e+00
Epoch 6/10
19/19 - 1s - loss: 241.0904 - loglik: -2.3981e+02 - logprior: -1.2753e+00
Epoch 7/10
19/19 - 1s - loss: 240.2839 - loglik: -2.3901e+02 - logprior: -1.2628e+00
Epoch 8/10
19/19 - 1s - loss: 239.9985 - loglik: -2.3872e+02 - logprior: -1.2654e+00
Epoch 9/10
19/19 - 1s - loss: 239.3542 - loglik: -2.3808e+02 - logprior: -1.2604e+00
Epoch 10/10
19/19 - 1s - loss: 238.5927 - loglik: -2.3732e+02 - logprior: -1.2612e+00
Fitted a model with MAP estimate = -235.2901
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (24, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 247.3343 - loglik: -2.4315e+02 - logprior: -4.1814e+00
Epoch 2/2
19/19 - 1s - loss: 238.5015 - loglik: -2.3663e+02 - logprior: -1.8707e+00
Fitted a model with MAP estimate = -233.7340
expansions: [(0, 1)]
discards: [ 0 10 29]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 239.5914 - loglik: -2.3642e+02 - logprior: -3.1760e+00
Epoch 2/2
19/19 - 1s - loss: 234.0582 - loglik: -2.3280e+02 - logprior: -1.2568e+00
Fitted a model with MAP estimate = -229.7439
expansions: [(42, 1), (43, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 231.5657 - loglik: -2.2829e+02 - logprior: -3.2754e+00
Epoch 2/10
19/19 - 1s - loss: 227.0033 - loglik: -2.2566e+02 - logprior: -1.3457e+00
Epoch 3/10
19/19 - 1s - loss: 225.9722 - loglik: -2.2477e+02 - logprior: -1.2057e+00
Epoch 4/10
19/19 - 1s - loss: 226.4606 - loglik: -2.2527e+02 - logprior: -1.1895e+00
Fitted a model with MAP estimate = -225.5778
Time for alignment: 39.6302
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 271.5869 - loglik: -2.6835e+02 - logprior: -3.2413e+00
Epoch 2/10
19/19 - 1s - loss: 250.2180 - loglik: -2.4889e+02 - logprior: -1.3243e+00
Epoch 3/10
19/19 - 1s - loss: 243.9384 - loglik: -2.4257e+02 - logprior: -1.3647e+00
Epoch 4/10
19/19 - 1s - loss: 241.9858 - loglik: -2.4069e+02 - logprior: -1.2974e+00
Epoch 5/10
19/19 - 1s - loss: 241.4436 - loglik: -2.4018e+02 - logprior: -1.2599e+00
Epoch 6/10
19/19 - 1s - loss: 240.3374 - loglik: -2.3905e+02 - logprior: -1.2847e+00
Epoch 7/10
19/19 - 1s - loss: 239.4890 - loglik: -2.3821e+02 - logprior: -1.2732e+00
Epoch 8/10
19/19 - 1s - loss: 239.4239 - loglik: -2.3814e+02 - logprior: -1.2775e+00
Epoch 9/10
19/19 - 1s - loss: 238.9528 - loglik: -2.3767e+02 - logprior: -1.2752e+00
Epoch 10/10
19/19 - 1s - loss: 238.4450 - loglik: -2.3716e+02 - logprior: -1.2727e+00
Fitted a model with MAP estimate = -234.8520
expansions: [(10, 2), (12, 2), (13, 1), (14, 1), (20, 1), (22, 2), (28, 2), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 244.8095 - loglik: -2.4064e+02 - logprior: -4.1733e+00
Epoch 2/2
19/19 - 1s - loss: 233.2671 - loglik: -2.3124e+02 - logprior: -2.0267e+00
Fitted a model with MAP estimate = -228.2519
expansions: [(0, 1)]
discards: [ 0 10 29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 234.0029 - loglik: -2.3089e+02 - logprior: -3.1176e+00
Epoch 2/2
19/19 - 1s - loss: 230.1250 - loglik: -2.2874e+02 - logprior: -1.3806e+00
Fitted a model with MAP estimate = -226.6708
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 229.8780 - loglik: -2.2671e+02 - logprior: -3.1655e+00
Epoch 2/10
19/19 - 1s - loss: 226.6095 - loglik: -2.2524e+02 - logprior: -1.3686e+00
Epoch 3/10
19/19 - 1s - loss: 226.4129 - loglik: -2.2518e+02 - logprior: -1.2365e+00
Epoch 4/10
19/19 - 1s - loss: 226.2006 - loglik: -2.2500e+02 - logprior: -1.1956e+00
Epoch 5/10
19/19 - 1s - loss: 225.3546 - loglik: -2.2418e+02 - logprior: -1.1685e+00
Epoch 6/10
19/19 - 1s - loss: 225.1265 - loglik: -2.2397e+02 - logprior: -1.1543e+00
Epoch 7/10
19/19 - 1s - loss: 224.1839 - loglik: -2.2303e+02 - logprior: -1.1512e+00
Epoch 8/10
19/19 - 1s - loss: 224.3374 - loglik: -2.2319e+02 - logprior: -1.1341e+00
Fitted a model with MAP estimate = -223.5859
Time for alignment: 44.5172
Fitting a model of length 36 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 271.4746 - loglik: -2.6823e+02 - logprior: -3.2426e+00
Epoch 2/10
19/19 - 1s - loss: 250.6540 - loglik: -2.4926e+02 - logprior: -1.3934e+00
Epoch 3/10
19/19 - 1s - loss: 243.0907 - loglik: -2.4159e+02 - logprior: -1.5040e+00
Epoch 4/10
19/19 - 1s - loss: 239.9310 - loglik: -2.3845e+02 - logprior: -1.4747e+00
Epoch 5/10
19/19 - 1s - loss: 238.1147 - loglik: -2.3665e+02 - logprior: -1.4661e+00
Epoch 6/10
19/19 - 1s - loss: 237.3377 - loglik: -2.3585e+02 - logprior: -1.4865e+00
Epoch 7/10
19/19 - 1s - loss: 236.7371 - loglik: -2.3526e+02 - logprior: -1.4716e+00
Epoch 8/10
19/19 - 1s - loss: 236.2462 - loglik: -2.3477e+02 - logprior: -1.4700e+00
Epoch 9/10
19/19 - 1s - loss: 235.7601 - loglik: -2.3429e+02 - logprior: -1.4642e+00
Epoch 10/10
19/19 - 1s - loss: 235.6090 - loglik: -2.3414e+02 - logprior: -1.4608e+00
Fitted a model with MAP estimate = -232.3142
expansions: [(6, 1), (10, 1), (12, 1), (13, 1), (14, 1), (20, 1), (23, 2), (28, 1), (29, 1), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 241.5672 - loglik: -2.3741e+02 - logprior: -4.1577e+00
Epoch 2/2
19/19 - 1s - loss: 232.2509 - loglik: -2.3032e+02 - logprior: -1.9343e+00
Fitted a model with MAP estimate = -227.7424
expansions: [(0, 1)]
discards: [ 0 29]
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 233.4456 - loglik: -2.3033e+02 - logprior: -3.1146e+00
Epoch 2/2
19/19 - 1s - loss: 230.0124 - loglik: -2.2863e+02 - logprior: -1.3788e+00
Fitted a model with MAP estimate = -226.5188
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 46 on 10398 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 229.5913 - loglik: -2.2642e+02 - logprior: -3.1687e+00
Epoch 2/10
19/19 - 1s - loss: 226.7101 - loglik: -2.2533e+02 - logprior: -1.3755e+00
Epoch 3/10
19/19 - 1s - loss: 226.0801 - loglik: -2.2483e+02 - logprior: -1.2467e+00
Epoch 4/10
19/19 - 1s - loss: 225.8377 - loglik: -2.2464e+02 - logprior: -1.1955e+00
Epoch 5/10
19/19 - 1s - loss: 225.4597 - loglik: -2.2428e+02 - logprior: -1.1732e+00
Epoch 6/10
19/19 - 1s - loss: 224.8335 - loglik: -2.2367e+02 - logprior: -1.1619e+00
Epoch 7/10
19/19 - 1s - loss: 224.5619 - loglik: -2.2341e+02 - logprior: -1.1451e+00
Epoch 8/10
19/19 - 1s - loss: 223.4769 - loglik: -2.2232e+02 - logprior: -1.1468e+00
Epoch 9/10
19/19 - 1s - loss: 223.2466 - loglik: -2.2210e+02 - logprior: -1.1333e+00
Epoch 10/10
19/19 - 1s - loss: 223.0089 - loglik: -2.2186e+02 - logprior: -1.1349e+00
Fitted a model with MAP estimate = -222.4794
Time for alignment: 47.0542
Computed alignments with likelihoods: ['-225.8491', '-224.6523', '-225.5778', '-223.5859', '-222.4794']
Best model has likelihood: -222.4794  (prior= -1.1334 )
time for generating output: 0.0882
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/myb_DNA-binding.projection.fasta
SP score = 0.9679358717434869
Training of 5 independent models on file ghf10.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea77dbad60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9e76f42e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d6b78e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 1293.6339 - loglik: -1.2751e+03 - logprior: -1.8544e+01
Epoch 2/10
15/15 - 6s - loss: 1209.7603 - loglik: -1.2094e+03 - logprior: -3.6330e-01
Epoch 3/10
15/15 - 6s - loss: 1146.6793 - loglik: -1.1473e+03 - logprior: 0.5968
Epoch 4/10
15/15 - 6s - loss: 1119.9225 - loglik: -1.1203e+03 - logprior: 0.3393
Epoch 5/10
15/15 - 6s - loss: 1114.5345 - loglik: -1.1144e+03 - logprior: -1.0252e-01
Epoch 6/10
15/15 - 6s - loss: 1103.2825 - loglik: -1.1034e+03 - logprior: 0.1404
Epoch 7/10
15/15 - 6s - loss: 1105.7146 - loglik: -1.1059e+03 - logprior: 0.1859
Fitted a model with MAP estimate = -1103.3458
expansions: [(26, 3), (41, 1), (50, 2), (52, 2), (53, 1), (55, 2), (57, 1), (68, 1), (71, 2), (72, 2), (73, 5), (75, 2), (78, 1), (87, 1), (100, 1), (102, 1), (103, 1), (104, 4), (134, 2), (135, 2), (136, 5), (137, 2), (138, 2), (140, 1), (157, 1), (158, 5), (159, 2), (167, 1), (173, 8), (176, 2), (178, 1), (183, 1)]
discards: [  1   2   6 223]
Re-initialized the encoder parameters.
Fitting a model of length 289 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1117.5786 - loglik: -1.1012e+03 - logprior: -1.6340e+01
Epoch 2/2
15/15 - 9s - loss: 1081.1956 - loglik: -1.0818e+03 - logprior: 0.5785
Fitted a model with MAP estimate = -1074.7082
expansions: [(64, 1), (246, 1)]
discards: [132 167 172 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1089.5601 - loglik: -1.0743e+03 - logprior: -1.5234e+01
Epoch 2/2
15/15 - 9s - loss: 1072.7251 - loglik: -1.0742e+03 - logprior: 1.4362
Fitted a model with MAP estimate = -1068.8142
expansions: []
discards: [164 229]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 1088.0262 - loglik: -1.0732e+03 - logprior: -1.4786e+01
Epoch 2/10
15/15 - 9s - loss: 1071.5089 - loglik: -1.0735e+03 - logprior: 1.9556
Epoch 3/10
15/15 - 9s - loss: 1070.2405 - loglik: -1.0748e+03 - logprior: 4.5732
Epoch 4/10
15/15 - 9s - loss: 1060.8210 - loglik: -1.0664e+03 - logprior: 5.5837
Epoch 5/10
15/15 - 9s - loss: 1070.2714 - loglik: -1.0765e+03 - logprior: 6.2356
Fitted a model with MAP estimate = -1062.1151
Time for alignment: 154.3045
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 1290.2369 - loglik: -1.2717e+03 - logprior: -1.8494e+01
Epoch 2/10
15/15 - 6s - loss: 1216.6953 - loglik: -1.2163e+03 - logprior: -4.3754e-01
Epoch 3/10
15/15 - 6s - loss: 1147.3524 - loglik: -1.1476e+03 - logprior: 0.2880
Epoch 4/10
15/15 - 6s - loss: 1123.5631 - loglik: -1.1236e+03 - logprior: 0.0800
Epoch 5/10
15/15 - 6s - loss: 1115.3956 - loglik: -1.1152e+03 - logprior: -2.0278e-01
Epoch 6/10
15/15 - 6s - loss: 1103.3171 - loglik: -1.1032e+03 - logprior: -8.0833e-02
Epoch 7/10
15/15 - 6s - loss: 1103.2860 - loglik: -1.1032e+03 - logprior: -5.1169e-02
Epoch 8/10
15/15 - 6s - loss: 1105.1967 - loglik: -1.1051e+03 - logprior: -1.1305e-01
Fitted a model with MAP estimate = -1100.0082
expansions: [(19, 1), (20, 1), (21, 2), (22, 4), (35, 1), (38, 1), (48, 1), (50, 1), (52, 2), (55, 1), (66, 1), (69, 4), (70, 2), (71, 1), (72, 2), (73, 1), (74, 1), (87, 2), (102, 1), (103, 1), (105, 2), (112, 1), (134, 2), (135, 2), (136, 4), (137, 2), (138, 3), (139, 2), (156, 1), (157, 4), (158, 3), (173, 8), (175, 2), (177, 1), (183, 1), (197, 1)]
discards: [  1   2   6 203 223]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 1113.2609 - loglik: -1.0972e+03 - logprior: -1.6108e+01
Epoch 2/2
15/15 - 9s - loss: 1083.9265 - loglik: -1.0847e+03 - logprior: 0.7945
Fitted a model with MAP estimate = -1071.7879
expansions: [(205, 1)]
discards: [ 87 168 182 231 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1092.8542 - loglik: -1.0775e+03 - logprior: -1.5359e+01
Epoch 2/2
15/15 - 9s - loss: 1073.1239 - loglik: -1.0746e+03 - logprior: 1.4503
Fitted a model with MAP estimate = -1068.9838
expansions: [(25, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 1086.4202 - loglik: -1.0717e+03 - logprior: -1.4691e+01
Epoch 2/10
15/15 - 9s - loss: 1069.8871 - loglik: -1.0719e+03 - logprior: 1.9915
Epoch 3/10
15/15 - 9s - loss: 1067.1986 - loglik: -1.0719e+03 - logprior: 4.7479
Epoch 4/10
15/15 - 9s - loss: 1067.8910 - loglik: -1.0737e+03 - logprior: 5.7757
Fitted a model with MAP estimate = -1063.7520
Time for alignment: 152.1134
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 10s - loss: 1293.3739 - loglik: -1.2748e+03 - logprior: -1.8552e+01
Epoch 2/10
15/15 - 6s - loss: 1213.0875 - loglik: -1.2128e+03 - logprior: -3.2309e-01
Epoch 3/10
15/15 - 6s - loss: 1146.1390 - loglik: -1.1468e+03 - logprior: 0.6240
Epoch 4/10
15/15 - 6s - loss: 1120.1718 - loglik: -1.1203e+03 - logprior: 0.1536
Epoch 5/10
15/15 - 6s - loss: 1104.4141 - loglik: -1.1043e+03 - logprior: -7.2210e-02
Epoch 6/10
15/15 - 6s - loss: 1114.8788 - loglik: -1.1150e+03 - logprior: 0.1461
Fitted a model with MAP estimate = -1106.1075
expansions: [(25, 1), (44, 6), (47, 1), (50, 1), (51, 1), (54, 1), (57, 1), (72, 3), (73, 6), (75, 2), (78, 1), (80, 1), (103, 4), (104, 3), (106, 2), (110, 1), (132, 2), (133, 1), (134, 1), (135, 2), (136, 3), (137, 2), (139, 1), (156, 1), (157, 3), (159, 2), (167, 1), (173, 8), (182, 2), (183, 2)]
discards: [  1   2 223]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1117.6305 - loglik: -1.1013e+03 - logprior: -1.6322e+01
Epoch 2/2
15/15 - 9s - loss: 1084.4504 - loglik: -1.0849e+03 - logprior: 0.4337
Fitted a model with MAP estimate = -1077.1293
expansions: [(46, 2), (207, 1)]
discards: [128 136 175 231 232 233]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1091.7833 - loglik: -1.0764e+03 - logprior: -1.5426e+01
Epoch 2/2
15/15 - 9s - loss: 1075.7828 - loglik: -1.0770e+03 - logprior: 1.2281
Fitted a model with MAP estimate = -1069.7924
expansions: [(0, 3), (47, 1), (86, 1), (203, 1)]
discards: [165]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 1094.6627 - loglik: -1.0716e+03 - logprior: -2.3087e+01
Epoch 2/10
15/15 - 9s - loss: 1074.7750 - loglik: -1.0740e+03 - logprior: -7.5520e-01
Epoch 3/10
15/15 - 9s - loss: 1065.0004 - loglik: -1.0692e+03 - logprior: 4.2035
Epoch 4/10
15/15 - 9s - loss: 1070.3748 - loglik: -1.0761e+03 - logprior: 5.7051
Fitted a model with MAP estimate = -1062.3735
Time for alignment: 140.9713
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 1291.6252 - loglik: -1.2731e+03 - logprior: -1.8540e+01
Epoch 2/10
15/15 - 6s - loss: 1218.0425 - loglik: -1.2176e+03 - logprior: -4.2727e-01
Epoch 3/10
15/15 - 6s - loss: 1148.6389 - loglik: -1.1492e+03 - logprior: 0.5653
Epoch 4/10
15/15 - 6s - loss: 1119.9792 - loglik: -1.1203e+03 - logprior: 0.2759
Epoch 5/10
15/15 - 6s - loss: 1111.1398 - loglik: -1.1112e+03 - logprior: 0.0505
Epoch 6/10
15/15 - 6s - loss: 1106.0389 - loglik: -1.1062e+03 - logprior: 0.2002
Epoch 7/10
15/15 - 6s - loss: 1110.2294 - loglik: -1.1104e+03 - logprior: 0.2073
Fitted a model with MAP estimate = -1103.9735
expansions: [(19, 1), (25, 4), (40, 1), (42, 1), (49, 1), (51, 1), (53, 1), (55, 2), (57, 1), (72, 4), (73, 2), (74, 2), (75, 2), (76, 1), (77, 1), (79, 1), (89, 2), (104, 3), (105, 1), (106, 1), (108, 2), (135, 2), (136, 2), (137, 4), (138, 2), (139, 3), (140, 2), (157, 1), (158, 7), (173, 8), (183, 2), (184, 2)]
discards: [  1   2   6 204 223]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 13s - loss: 1119.9420 - loglik: -1.1036e+03 - logprior: -1.6323e+01
Epoch 2/2
15/15 - 9s - loss: 1081.5255 - loglik: -1.0818e+03 - logprior: 0.2391
Fitted a model with MAP estimate = -1075.2827
expansions: [(65, 1), (207, 2), (208, 1), (247, 1)]
discards: [ 90 138 170 184 233 234 235 236]
Re-initialized the encoder parameters.
Fitting a model of length 287 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1091.2173 - loglik: -1.0758e+03 - logprior: -1.5419e+01
Epoch 2/2
15/15 - 9s - loss: 1076.1091 - loglik: -1.0774e+03 - logprior: 1.3091
Fitted a model with MAP estimate = -1069.3917
expansions: [(251, 1)]
discards: [ 88 204 205]
Re-initialized the encoder parameters.
Fitting a model of length 285 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 12s - loss: 1094.4817 - loglik: -1.0796e+03 - logprior: -1.4871e+01
Epoch 2/10
15/15 - 9s - loss: 1064.0657 - loglik: -1.0658e+03 - logprior: 1.7262
Epoch 3/10
15/15 - 9s - loss: 1072.3086 - loglik: -1.0768e+03 - logprior: 4.5170
Fitted a model with MAP estimate = -1066.2565
Time for alignment: 136.2143
Fitting a model of length 225 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 9s - loss: 1293.1422 - loglik: -1.2746e+03 - logprior: -1.8542e+01
Epoch 2/10
15/15 - 6s - loss: 1212.3727 - loglik: -1.2120e+03 - logprior: -3.9156e-01
Epoch 3/10
15/15 - 6s - loss: 1146.8999 - loglik: -1.1473e+03 - logprior: 0.4489
Epoch 4/10
15/15 - 6s - loss: 1121.3906 - loglik: -1.1215e+03 - logprior: 0.1222
Epoch 5/10
15/15 - 6s - loss: 1115.5519 - loglik: -1.1154e+03 - logprior: -1.5218e-01
Epoch 6/10
15/15 - 6s - loss: 1102.7255 - loglik: -1.1027e+03 - logprior: -4.3474e-02
Epoch 7/10
15/15 - 6s - loss: 1111.9285 - loglik: -1.1119e+03 - logprior: 0.0121
Fitted a model with MAP estimate = -1105.2566
expansions: [(18, 1), (19, 1), (25, 3), (38, 1), (39, 1), (41, 1), (48, 1), (50, 2), (51, 1), (53, 2), (56, 1), (71, 5), (72, 3), (73, 2), (74, 1), (75, 1), (77, 1), (78, 1), (87, 1), (102, 4), (103, 2), (104, 1), (134, 2), (135, 1), (136, 7), (137, 3), (138, 2), (155, 1), (156, 4), (158, 1)]
discards: [  1   2   6 198 199 200 223]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 11s - loss: 1124.1913 - loglik: -1.1077e+03 - logprior: -1.6445e+01
Epoch 2/2
15/15 - 8s - loss: 1082.4355 - loglik: -1.0828e+03 - logprior: 0.3396
Fitted a model with MAP estimate = -1079.9593
expansions: [(207, 1)]
discards: [ 87 131 134 184 253]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 12s - loss: 1096.9548 - loglik: -1.0814e+03 - logprior: -1.5585e+01
Epoch 2/2
15/15 - 8s - loss: 1079.6167 - loglik: -1.0807e+03 - logprior: 1.0750
Fitted a model with MAP estimate = -1077.0318
expansions: [(0, 3)]
discards: [  1   4 165]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 1502 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 11s - loss: 1103.7599 - loglik: -1.0799e+03 - logprior: -2.3865e+01
Epoch 2/10
15/15 - 8s - loss: 1081.2809 - loglik: -1.0798e+03 - logprior: -1.5164e+00
Epoch 3/10
15/15 - 8s - loss: 1078.7358 - loglik: -1.0822e+03 - logprior: 3.4181
Epoch 4/10
15/15 - 8s - loss: 1073.3568 - loglik: -1.0783e+03 - logprior: 4.9929
Epoch 5/10
15/15 - 8s - loss: 1071.2971 - loglik: -1.0769e+03 - logprior: 5.5582
Epoch 6/10
15/15 - 8s - loss: 1073.6473 - loglik: -1.0796e+03 - logprior: 5.9070
Fitted a model with MAP estimate = -1068.1839
Time for alignment: 156.7118
Computed alignments with likelihoods: ['-1062.1151', '-1063.7520', '-1062.3735', '-1066.2565', '-1068.1839']
Best model has likelihood: -1062.1151  (prior= 6.5446 )
time for generating output: 0.2798
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf10.projection.fasta
SP score = 0.9042804179305696
Training of 5 independent models on file il8.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea009c4130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d0d0bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea00faaeb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 413.3027 - loglik: -3.7163e+02 - logprior: -4.1668e+01
Epoch 2/10
10/10 - 1s - loss: 364.4224 - loglik: -3.5322e+02 - logprior: -1.1202e+01
Epoch 3/10
10/10 - 1s - loss: 345.7505 - loglik: -3.4021e+02 - logprior: -5.5357e+00
Epoch 4/10
10/10 - 1s - loss: 335.4384 - loglik: -3.3193e+02 - logprior: -3.5055e+00
Epoch 5/10
10/10 - 1s - loss: 331.1908 - loglik: -3.2869e+02 - logprior: -2.4968e+00
Epoch 6/10
10/10 - 1s - loss: 328.1681 - loglik: -3.2619e+02 - logprior: -1.9807e+00
Epoch 7/10
10/10 - 1s - loss: 326.5973 - loglik: -3.2502e+02 - logprior: -1.5769e+00
Epoch 8/10
10/10 - 1s - loss: 325.5563 - loglik: -3.2431e+02 - logprior: -1.2484e+00
Epoch 9/10
10/10 - 1s - loss: 324.7728 - loglik: -3.2365e+02 - logprior: -1.1169e+00
Epoch 10/10
10/10 - 1s - loss: 323.8659 - loglik: -3.2278e+02 - logprior: -1.0793e+00
Fitted a model with MAP estimate = -323.2818
expansions: [(0, 2), (1, 1), (16, 1), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 385.1392 - loglik: -3.3043e+02 - logprior: -5.4711e+01
Epoch 2/2
10/10 - 1s - loss: 337.4901 - loglik: -3.2085e+02 - logprior: -1.6639e+01
Fitted a model with MAP estimate = -329.0651
expansions: []
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 367.9922 - loglik: -3.2089e+02 - logprior: -4.7104e+01
Epoch 2/2
10/10 - 1s - loss: 337.2878 - loglik: -3.1900e+02 - logprior: -1.8290e+01
Fitted a model with MAP estimate = -332.4250
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 362.5032 - loglik: -3.1993e+02 - logprior: -4.2574e+01
Epoch 2/10
10/10 - 1s - loss: 329.0589 - loglik: -3.1752e+02 - logprior: -1.1536e+01
Epoch 3/10
10/10 - 1s - loss: 322.0685 - loglik: -3.1766e+02 - logprior: -4.4107e+00
Epoch 4/10
10/10 - 1s - loss: 319.2179 - loglik: -3.1729e+02 - logprior: -1.9319e+00
Epoch 5/10
10/10 - 1s - loss: 317.1016 - loglik: -3.1624e+02 - logprior: -8.5648e-01
Epoch 6/10
10/10 - 1s - loss: 315.6608 - loglik: -3.1534e+02 - logprior: -3.1847e-01
Epoch 7/10
10/10 - 1s - loss: 314.2554 - loglik: -3.1444e+02 - logprior: 0.1835
Epoch 8/10
10/10 - 1s - loss: 313.1344 - loglik: -3.1368e+02 - logprior: 0.5477
Epoch 9/10
10/10 - 1s - loss: 311.8201 - loglik: -3.1251e+02 - logprior: 0.6918
Epoch 10/10
10/10 - 1s - loss: 310.4135 - loglik: -3.1117e+02 - logprior: 0.7546
Fitted a model with MAP estimate = -309.3637
Time for alignment: 29.5445
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 413.4754 - loglik: -3.7181e+02 - logprior: -4.1670e+01
Epoch 2/10
10/10 - 1s - loss: 364.1115 - loglik: -3.5290e+02 - logprior: -1.1212e+01
Epoch 3/10
10/10 - 0s - loss: 345.5604 - loglik: -3.4005e+02 - logprior: -5.5098e+00
Epoch 4/10
10/10 - 0s - loss: 336.0159 - loglik: -3.3253e+02 - logprior: -3.4873e+00
Epoch 5/10
10/10 - 0s - loss: 331.6199 - loglik: -3.2908e+02 - logprior: -2.5422e+00
Epoch 6/10
10/10 - 1s - loss: 328.8021 - loglik: -3.2679e+02 - logprior: -2.0098e+00
Epoch 7/10
10/10 - 1s - loss: 326.7018 - loglik: -3.2508e+02 - logprior: -1.6235e+00
Epoch 8/10
10/10 - 1s - loss: 325.8041 - loglik: -3.2445e+02 - logprior: -1.3489e+00
Epoch 9/10
10/10 - 1s - loss: 324.5995 - loglik: -3.2340e+02 - logprior: -1.1990e+00
Epoch 10/10
10/10 - 1s - loss: 323.6861 - loglik: -3.2254e+02 - logprior: -1.1451e+00
Fitted a model with MAP estimate = -322.8590
expansions: [(0, 2), (1, 1), (16, 1), (17, 1), (18, 2), (22, 1), (24, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 386.0992 - loglik: -3.3137e+02 - logprior: -5.4725e+01
Epoch 2/2
10/10 - 1s - loss: 337.5323 - loglik: -3.2094e+02 - logprior: -1.6592e+01
Fitted a model with MAP estimate = -329.4956
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 368.3075 - loglik: -3.2119e+02 - logprior: -4.7121e+01
Epoch 2/2
10/10 - 1s - loss: 337.5770 - loglik: -3.1927e+02 - logprior: -1.8310e+01
Fitted a model with MAP estimate = -332.8456
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 362.9765 - loglik: -3.2038e+02 - logprior: -4.2600e+01
Epoch 2/10
10/10 - 1s - loss: 329.5331 - loglik: -3.1796e+02 - logprior: -1.1568e+01
Epoch 3/10
10/10 - 1s - loss: 322.5922 - loglik: -3.1814e+02 - logprior: -4.4502e+00
Epoch 4/10
10/10 - 1s - loss: 319.6445 - loglik: -3.1767e+02 - logprior: -1.9729e+00
Epoch 5/10
10/10 - 1s - loss: 317.5448 - loglik: -3.1664e+02 - logprior: -9.0126e-01
Epoch 6/10
10/10 - 1s - loss: 315.7805 - loglik: -3.1541e+02 - logprior: -3.6478e-01
Epoch 7/10
10/10 - 1s - loss: 314.6091 - loglik: -3.1475e+02 - logprior: 0.1432
Epoch 8/10
10/10 - 1s - loss: 313.5562 - loglik: -3.1406e+02 - logprior: 0.5059
Epoch 9/10
10/10 - 1s - loss: 312.1885 - loglik: -3.1283e+02 - logprior: 0.6460
Epoch 10/10
10/10 - 1s - loss: 310.6930 - loglik: -3.1140e+02 - logprior: 0.7052
Fitted a model with MAP estimate = -309.6268
Time for alignment: 27.8697
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 413.3002 - loglik: -3.7163e+02 - logprior: -4.1670e+01
Epoch 2/10
10/10 - 1s - loss: 364.1372 - loglik: -3.5293e+02 - logprior: -1.1209e+01
Epoch 3/10
10/10 - 1s - loss: 345.8827 - loglik: -3.4036e+02 - logprior: -5.5207e+00
Epoch 4/10
10/10 - 1s - loss: 336.2022 - loglik: -3.3275e+02 - logprior: -3.4559e+00
Epoch 5/10
10/10 - 1s - loss: 332.1283 - loglik: -3.2966e+02 - logprior: -2.4684e+00
Epoch 6/10
10/10 - 1s - loss: 329.6128 - loglik: -3.2764e+02 - logprior: -1.9714e+00
Epoch 7/10
10/10 - 1s - loss: 327.1147 - loglik: -3.2553e+02 - logprior: -1.5845e+00
Epoch 8/10
10/10 - 1s - loss: 325.9664 - loglik: -3.2470e+02 - logprior: -1.2680e+00
Epoch 9/10
10/10 - 1s - loss: 324.9265 - loglik: -3.2382e+02 - logprior: -1.1069e+00
Epoch 10/10
10/10 - 1s - loss: 323.9378 - loglik: -3.2285e+02 - logprior: -1.0882e+00
Fitted a model with MAP estimate = -323.2595
expansions: [(0, 2), (1, 1), (16, 1), (17, 1), (18, 3), (20, 1), (21, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 385.9040 - loglik: -3.3118e+02 - logprior: -5.4726e+01
Epoch 2/2
10/10 - 1s - loss: 337.6818 - loglik: -3.2100e+02 - logprior: -1.6681e+01
Fitted a model with MAP estimate = -329.3695
expansions: []
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 368.3341 - loglik: -3.2120e+02 - logprior: -4.7134e+01
Epoch 2/2
10/10 - 1s - loss: 337.3324 - loglik: -3.1902e+02 - logprior: -1.8314e+01
Fitted a model with MAP estimate = -332.6349
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 362.5862 - loglik: -3.1998e+02 - logprior: -4.2604e+01
Epoch 2/10
10/10 - 1s - loss: 329.3719 - loglik: -3.1781e+02 - logprior: -1.1557e+01
Epoch 3/10
10/10 - 1s - loss: 322.4069 - loglik: -3.1796e+02 - logprior: -4.4475e+00
Epoch 4/10
10/10 - 1s - loss: 319.2244 - loglik: -3.1726e+02 - logprior: -1.9631e+00
Epoch 5/10
10/10 - 1s - loss: 317.2257 - loglik: -3.1633e+02 - logprior: -8.9020e-01
Epoch 6/10
10/10 - 1s - loss: 315.8193 - loglik: -3.1546e+02 - logprior: -3.5565e-01
Epoch 7/10
10/10 - 1s - loss: 314.4370 - loglik: -3.1459e+02 - logprior: 0.1535
Epoch 8/10
10/10 - 1s - loss: 313.1429 - loglik: -3.1366e+02 - logprior: 0.5201
Epoch 9/10
10/10 - 1s - loss: 312.0164 - loglik: -3.1268e+02 - logprior: 0.6615
Epoch 10/10
10/10 - 1s - loss: 310.4456 - loglik: -3.1116e+02 - logprior: 0.7198
Fitted a model with MAP estimate = -309.3858
Time for alignment: 28.8064
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 413.6186 - loglik: -3.7195e+02 - logprior: -4.1670e+01
Epoch 2/10
10/10 - 1s - loss: 364.1977 - loglik: -3.5299e+02 - logprior: -1.1210e+01
Epoch 3/10
10/10 - 1s - loss: 346.2083 - loglik: -3.4065e+02 - logprior: -5.5623e+00
Epoch 4/10
10/10 - 1s - loss: 336.3168 - loglik: -3.3276e+02 - logprior: -3.5603e+00
Epoch 5/10
10/10 - 1s - loss: 331.3881 - loglik: -3.2885e+02 - logprior: -2.5409e+00
Epoch 6/10
10/10 - 1s - loss: 328.7604 - loglik: -3.2675e+02 - logprior: -2.0135e+00
Epoch 7/10
10/10 - 1s - loss: 326.5416 - loglik: -3.2491e+02 - logprior: -1.6273e+00
Epoch 8/10
10/10 - 1s - loss: 325.6156 - loglik: -3.2429e+02 - logprior: -1.3206e+00
Epoch 9/10
10/10 - 1s - loss: 324.6972 - loglik: -3.2352e+02 - logprior: -1.1743e+00
Epoch 10/10
10/10 - 1s - loss: 323.3176 - loglik: -3.2215e+02 - logprior: -1.1663e+00
Fitted a model with MAP estimate = -322.7751
expansions: [(0, 2), (1, 1), (16, 1), (17, 1), (19, 2), (20, 1), (24, 1), (28, 1), (38, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 387.2469 - loglik: -3.3249e+02 - logprior: -5.4760e+01
Epoch 2/2
10/10 - 1s - loss: 338.8901 - loglik: -3.2230e+02 - logprior: -1.6586e+01
Fitted a model with MAP estimate = -330.6714
expansions: []
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 369.9685 - loglik: -3.2273e+02 - logprior: -4.7238e+01
Epoch 2/2
10/10 - 1s - loss: 337.9443 - loglik: -3.1965e+02 - logprior: -1.8299e+01
Fitted a model with MAP estimate = -333.2227
expansions: [(18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 363.4207 - loglik: -3.2089e+02 - logprior: -4.2533e+01
Epoch 2/10
10/10 - 1s - loss: 329.8338 - loglik: -3.1833e+02 - logprior: -1.1504e+01
Epoch 3/10
10/10 - 1s - loss: 322.6316 - loglik: -3.1824e+02 - logprior: -4.3880e+00
Epoch 4/10
10/10 - 1s - loss: 319.7927 - loglik: -3.1787e+02 - logprior: -1.9178e+00
Epoch 5/10
10/10 - 1s - loss: 317.2496 - loglik: -3.1639e+02 - logprior: -8.5571e-01
Epoch 6/10
10/10 - 1s - loss: 315.7240 - loglik: -3.1542e+02 - logprior: -3.0560e-01
Epoch 7/10
10/10 - 1s - loss: 314.5889 - loglik: -3.1480e+02 - logprior: 0.2092
Epoch 8/10
10/10 - 1s - loss: 312.8871 - loglik: -3.1346e+02 - logprior: 0.5748
Epoch 9/10
10/10 - 1s - loss: 312.0064 - loglik: -3.1271e+02 - logprior: 0.7076
Epoch 10/10
10/10 - 1s - loss: 310.3167 - loglik: -3.1108e+02 - logprior: 0.7709
Fitted a model with MAP estimate = -309.3665
Time for alignment: 29.3117
Fitting a model of length 52 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 413.4404 - loglik: -3.7177e+02 - logprior: -4.1669e+01
Epoch 2/10
10/10 - 1s - loss: 364.2391 - loglik: -3.5303e+02 - logprior: -1.1209e+01
Epoch 3/10
10/10 - 1s - loss: 345.6014 - loglik: -3.4009e+02 - logprior: -5.5156e+00
Epoch 4/10
10/10 - 1s - loss: 335.7823 - loglik: -3.3230e+02 - logprior: -3.4857e+00
Epoch 5/10
10/10 - 1s - loss: 331.2767 - loglik: -3.2884e+02 - logprior: -2.4389e+00
Epoch 6/10
10/10 - 1s - loss: 328.4563 - loglik: -3.2650e+02 - logprior: -1.9580e+00
Epoch 7/10
10/10 - 1s - loss: 326.9153 - loglik: -3.2532e+02 - logprior: -1.5914e+00
Epoch 8/10
10/10 - 1s - loss: 325.6742 - loglik: -3.2438e+02 - logprior: -1.2901e+00
Epoch 9/10
10/10 - 1s - loss: 324.7747 - loglik: -3.2362e+02 - logprior: -1.1497e+00
Epoch 10/10
10/10 - 1s - loss: 323.9737 - loglik: -3.2287e+02 - logprior: -1.1015e+00
Fitted a model with MAP estimate = -323.3208
expansions: [(0, 2), (16, 1), (17, 1), (18, 2), (19, 1), (21, 1), (24, 1), (35, 1), (43, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 385.2509 - loglik: -3.3043e+02 - logprior: -5.4817e+01
Epoch 2/2
10/10 - 1s - loss: 337.5623 - loglik: -3.2102e+02 - logprior: -1.6542e+01
Fitted a model with MAP estimate = -329.3297
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 368.1983 - loglik: -3.2100e+02 - logprior: -4.7198e+01
Epoch 2/2
10/10 - 1s - loss: 337.3321 - loglik: -3.1906e+02 - logprior: -1.8273e+01
Fitted a model with MAP estimate = -332.4777
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 64 on 1073 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 362.3753 - loglik: -3.1991e+02 - logprior: -4.2461e+01
Epoch 2/10
10/10 - 1s - loss: 329.1316 - loglik: -3.1765e+02 - logprior: -1.1479e+01
Epoch 3/10
10/10 - 1s - loss: 321.9640 - loglik: -3.1756e+02 - logprior: -4.4013e+00
Epoch 4/10
10/10 - 1s - loss: 319.2103 - loglik: -3.1728e+02 - logprior: -1.9326e+00
Epoch 5/10
10/10 - 1s - loss: 316.8878 - loglik: -3.1604e+02 - logprior: -8.4460e-01
Epoch 6/10
10/10 - 1s - loss: 315.6371 - loglik: -3.1532e+02 - logprior: -3.1443e-01
Epoch 7/10
10/10 - 1s - loss: 314.1071 - loglik: -3.1428e+02 - logprior: 0.1731
Epoch 8/10
10/10 - 1s - loss: 312.9951 - loglik: -3.1353e+02 - logprior: 0.5406
Epoch 9/10
10/10 - 1s - loss: 311.8896 - loglik: -3.1258e+02 - logprior: 0.6904
Epoch 10/10
10/10 - 1s - loss: 310.2865 - loglik: -3.1104e+02 - logprior: 0.7545
Fitted a model with MAP estimate = -309.1853
Time for alignment: 28.1363
Computed alignments with likelihoods: ['-309.3637', '-309.6268', '-309.3858', '-309.3665', '-309.1853']
Best model has likelihood: -309.1853  (prior= 0.7874 )
time for generating output: 0.0991
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/il8.projection.fasta
SP score = 0.8948855608929076
Training of 5 independent models on file cytb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea6f669e20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efd47070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9526ca520>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 489.0281 - loglik: -4.7583e+02 - logprior: -1.3195e+01
Epoch 2/10
11/11 - 1s - loss: 459.5270 - loglik: -4.5609e+02 - logprior: -3.4384e+00
Epoch 3/10
11/11 - 1s - loss: 440.1315 - loglik: -4.3797e+02 - logprior: -2.1609e+00
Epoch 4/10
11/11 - 1s - loss: 428.6868 - loglik: -4.2667e+02 - logprior: -2.0208e+00
Epoch 5/10
11/11 - 1s - loss: 426.1678 - loglik: -4.2422e+02 - logprior: -1.9493e+00
Epoch 6/10
11/11 - 1s - loss: 423.1007 - loglik: -4.2129e+02 - logprior: -1.8050e+00
Epoch 7/10
11/11 - 1s - loss: 422.8519 - loglik: -4.2117e+02 - logprior: -1.6837e+00
Epoch 8/10
11/11 - 1s - loss: 422.1192 - loglik: -4.2048e+02 - logprior: -1.6381e+00
Epoch 9/10
11/11 - 1s - loss: 420.5451 - loglik: -4.1890e+02 - logprior: -1.6405e+00
Epoch 10/10
11/11 - 1s - loss: 418.9153 - loglik: -4.1728e+02 - logprior: -1.6339e+00
Fitted a model with MAP estimate = -419.2103
expansions: [(8, 2), (9, 1), (10, 2), (11, 2), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 438.3135 - loglik: -4.2337e+02 - logprior: -1.4941e+01
Epoch 2/2
11/11 - 1s - loss: 423.0259 - loglik: -4.1663e+02 - logprior: -6.3992e+00
Fitted a model with MAP estimate = -419.8635
expansions: [(0, 3)]
discards: [ 0 15 16 45]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 427.9199 - loglik: -4.1611e+02 - logprior: -1.1814e+01
Epoch 2/2
11/11 - 1s - loss: 416.5415 - loglik: -4.1343e+02 - logprior: -3.1098e+00
Fitted a model with MAP estimate = -415.6015
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 430.2120 - loglik: -4.1580e+02 - logprior: -1.4415e+01
Epoch 2/10
11/11 - 1s - loss: 418.8732 - loglik: -4.1431e+02 - logprior: -4.5631e+00
Epoch 3/10
11/11 - 1s - loss: 416.0744 - loglik: -4.1392e+02 - logprior: -2.1497e+00
Epoch 4/10
11/11 - 1s - loss: 414.7968 - loglik: -4.1353e+02 - logprior: -1.2685e+00
Epoch 5/10
11/11 - 1s - loss: 413.6060 - loglik: -4.1260e+02 - logprior: -1.0030e+00
Epoch 6/10
11/11 - 1s - loss: 414.6031 - loglik: -4.1380e+02 - logprior: -7.9892e-01
Fitted a model with MAP estimate = -412.9285
Time for alignment: 44.2119
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 489.0864 - loglik: -4.7589e+02 - logprior: -1.3194e+01
Epoch 2/10
11/11 - 1s - loss: 458.6334 - loglik: -4.5520e+02 - logprior: -3.4304e+00
Epoch 3/10
11/11 - 1s - loss: 438.3258 - loglik: -4.3616e+02 - logprior: -2.1681e+00
Epoch 4/10
11/11 - 1s - loss: 428.0517 - loglik: -4.2601e+02 - logprior: -2.0440e+00
Epoch 5/10
11/11 - 1s - loss: 424.9577 - loglik: -4.2301e+02 - logprior: -1.9462e+00
Epoch 6/10
11/11 - 1s - loss: 424.3502 - loglik: -4.2255e+02 - logprior: -1.7950e+00
Epoch 7/10
11/11 - 1s - loss: 422.6404 - loglik: -4.2097e+02 - logprior: -1.6649e+00
Epoch 8/10
11/11 - 1s - loss: 420.2664 - loglik: -4.1864e+02 - logprior: -1.6255e+00
Epoch 9/10
11/11 - 1s - loss: 420.3264 - loglik: -4.1869e+02 - logprior: -1.6363e+00
Fitted a model with MAP estimate = -419.4293
expansions: [(8, 1), (10, 3), (11, 1), (12, 2), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 439.1237 - loglik: -4.2417e+02 - logprior: -1.4953e+01
Epoch 2/2
11/11 - 1s - loss: 423.1579 - loglik: -4.1672e+02 - logprior: -6.4332e+00
Fitted a model with MAP estimate = -420.6204
expansions: [(0, 2)]
discards: [ 0 14 16 45]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 426.8065 - loglik: -4.1503e+02 - logprior: -1.1778e+01
Epoch 2/2
11/11 - 1s - loss: 416.7075 - loglik: -4.1365e+02 - logprior: -3.0566e+00
Fitted a model with MAP estimate = -415.5364
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 429.3821 - loglik: -4.1519e+02 - logprior: -1.4197e+01
Epoch 2/10
11/11 - 1s - loss: 418.6085 - loglik: -4.1433e+02 - logprior: -4.2812e+00
Epoch 3/10
11/11 - 1s - loss: 416.2352 - loglik: -4.1413e+02 - logprior: -2.1066e+00
Epoch 4/10
11/11 - 1s - loss: 415.2478 - loglik: -4.1399e+02 - logprior: -1.2522e+00
Epoch 5/10
11/11 - 1s - loss: 414.7747 - loglik: -4.1378e+02 - logprior: -9.9780e-01
Epoch 6/10
11/11 - 1s - loss: 412.3708 - loglik: -4.1158e+02 - logprior: -7.9024e-01
Epoch 7/10
11/11 - 1s - loss: 413.0952 - loglik: -4.1236e+02 - logprior: -7.3486e-01
Fitted a model with MAP estimate = -411.8962
Time for alignment: 41.5388
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 490.0115 - loglik: -4.7682e+02 - logprior: -1.3191e+01
Epoch 2/10
11/11 - 1s - loss: 459.2516 - loglik: -4.5582e+02 - logprior: -3.4297e+00
Epoch 3/10
11/11 - 1s - loss: 440.8291 - loglik: -4.3868e+02 - logprior: -2.1511e+00
Epoch 4/10
11/11 - 1s - loss: 430.6352 - loglik: -4.2861e+02 - logprior: -2.0215e+00
Epoch 5/10
11/11 - 1s - loss: 426.0724 - loglik: -4.2413e+02 - logprior: -1.9418e+00
Epoch 6/10
11/11 - 1s - loss: 424.9681 - loglik: -4.2321e+02 - logprior: -1.7521e+00
Epoch 7/10
11/11 - 1s - loss: 421.8382 - loglik: -4.2023e+02 - logprior: -1.6010e+00
Epoch 8/10
11/11 - 1s - loss: 422.5691 - loglik: -4.2101e+02 - logprior: -1.5573e+00
Fitted a model with MAP estimate = -421.0863
expansions: [(8, 2), (10, 2), (12, 1), (24, 2), (25, 2), (31, 1), (32, 1), (34, 2), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 437.7099 - loglik: -4.2277e+02 - logprior: -1.4944e+01
Epoch 2/2
11/11 - 1s - loss: 422.7693 - loglik: -4.1637e+02 - logprior: -6.4036e+00
Fitted a model with MAP estimate = -419.9660
expansions: [(0, 2)]
discards: [ 0 28 31]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 426.6317 - loglik: -4.1484e+02 - logprior: -1.1794e+01
Epoch 2/2
11/11 - 1s - loss: 416.0135 - loglik: -4.1294e+02 - logprior: -3.0722e+00
Fitted a model with MAP estimate = -415.1307
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 429.4633 - loglik: -4.1521e+02 - logprior: -1.4253e+01
Epoch 2/10
11/11 - 1s - loss: 417.4462 - loglik: -4.1311e+02 - logprior: -4.3403e+00
Epoch 3/10
11/11 - 1s - loss: 416.6327 - loglik: -4.1452e+02 - logprior: -2.1141e+00
Epoch 4/10
11/11 - 1s - loss: 414.3838 - loglik: -4.1313e+02 - logprior: -1.2523e+00
Epoch 5/10
11/11 - 1s - loss: 414.3081 - loglik: -4.1329e+02 - logprior: -1.0146e+00
Epoch 6/10
11/11 - 1s - loss: 413.1372 - loglik: -4.1231e+02 - logprior: -8.2507e-01
Epoch 7/10
11/11 - 1s - loss: 412.6995 - loglik: -4.1195e+02 - logprior: -7.5127e-01
Epoch 8/10
11/11 - 1s - loss: 410.9341 - loglik: -4.1020e+02 - logprior: -7.3450e-01
Epoch 9/10
11/11 - 1s - loss: 410.4703 - loglik: -4.0976e+02 - logprior: -7.0868e-01
Epoch 10/10
11/11 - 1s - loss: 408.6353 - loglik: -4.0793e+02 - logprior: -7.0297e-01
Fitted a model with MAP estimate = -408.8446
Time for alignment: 45.2534
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 489.1586 - loglik: -4.7596e+02 - logprior: -1.3194e+01
Epoch 2/10
11/11 - 1s - loss: 457.5843 - loglik: -4.5415e+02 - logprior: -3.4349e+00
Epoch 3/10
11/11 - 1s - loss: 438.3537 - loglik: -4.3618e+02 - logprior: -2.1761e+00
Epoch 4/10
11/11 - 1s - loss: 428.0702 - loglik: -4.2600e+02 - logprior: -2.0677e+00
Epoch 5/10
11/11 - 1s - loss: 426.2838 - loglik: -4.2431e+02 - logprior: -1.9753e+00
Epoch 6/10
11/11 - 1s - loss: 423.3899 - loglik: -4.2159e+02 - logprior: -1.8013e+00
Epoch 7/10
11/11 - 1s - loss: 420.4751 - loglik: -4.1881e+02 - logprior: -1.6673e+00
Epoch 8/10
11/11 - 1s - loss: 422.4354 - loglik: -4.2080e+02 - logprior: -1.6324e+00
Fitted a model with MAP estimate = -420.4307
expansions: [(8, 3), (10, 2), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 438.1838 - loglik: -4.2323e+02 - logprior: -1.4953e+01
Epoch 2/2
11/11 - 1s - loss: 422.4919 - loglik: -4.1611e+02 - logprior: -6.3784e+00
Fitted a model with MAP estimate = -420.2523
expansions: [(0, 2)]
discards: [0 8]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 427.2617 - loglik: -4.1543e+02 - logprior: -1.1831e+01
Epoch 2/2
11/11 - 1s - loss: 416.1304 - loglik: -4.1301e+02 - logprior: -3.1238e+00
Fitted a model with MAP estimate = -415.4876
expansions: []
discards: [ 0 44]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 429.1248 - loglik: -4.1485e+02 - logprior: -1.4274e+01
Epoch 2/10
11/11 - 1s - loss: 419.5348 - loglik: -4.1516e+02 - logprior: -4.3704e+00
Epoch 3/10
11/11 - 1s - loss: 415.2799 - loglik: -4.1317e+02 - logprior: -2.1101e+00
Epoch 4/10
11/11 - 1s - loss: 416.0585 - loglik: -4.1478e+02 - logprior: -1.2765e+00
Fitted a model with MAP estimate = -414.5407
Time for alignment: 36.9847
Fitting a model of length 60 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 489.3305 - loglik: -4.7614e+02 - logprior: -1.3192e+01
Epoch 2/10
11/11 - 1s - loss: 458.5605 - loglik: -4.5513e+02 - logprior: -3.4287e+00
Epoch 3/10
11/11 - 1s - loss: 437.8633 - loglik: -4.3568e+02 - logprior: -2.1866e+00
Epoch 4/10
11/11 - 1s - loss: 427.7073 - loglik: -4.2563e+02 - logprior: -2.0772e+00
Epoch 5/10
11/11 - 1s - loss: 425.0699 - loglik: -4.2309e+02 - logprior: -1.9769e+00
Epoch 6/10
11/11 - 1s - loss: 424.6161 - loglik: -4.2281e+02 - logprior: -1.8090e+00
Epoch 7/10
11/11 - 1s - loss: 421.9014 - loglik: -4.2022e+02 - logprior: -1.6761e+00
Epoch 8/10
11/11 - 1s - loss: 420.6805 - loglik: -4.1904e+02 - logprior: -1.6320e+00
Epoch 9/10
11/11 - 1s - loss: 420.4840 - loglik: -4.1884e+02 - logprior: -1.6364e+00
Epoch 10/10
11/11 - 1s - loss: 418.7412 - loglik: -4.1710e+02 - logprior: -1.6361e+00
Fitted a model with MAP estimate = -418.8479
expansions: [(8, 2), (10, 2), (12, 1), (24, 1), (25, 1), (31, 1), (32, 1), (34, 2), (45, 1), (49, 1), (50, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 438.6100 - loglik: -4.2367e+02 - logprior: -1.4935e+01
Epoch 2/2
11/11 - 1s - loss: 423.5494 - loglik: -4.1720e+02 - logprior: -6.3447e+00
Fitted a model with MAP estimate = -420.2153
expansions: [(0, 2)]
discards: [ 0 43]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 425.1766 - loglik: -4.1341e+02 - logprior: -1.1765e+01
Epoch 2/2
11/11 - 1s - loss: 418.8623 - loglik: -4.1579e+02 - logprior: -3.0682e+00
Fitted a model with MAP estimate = -415.4618
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3206 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 429.8835 - loglik: -4.1561e+02 - logprior: -1.4274e+01
Epoch 2/10
11/11 - 1s - loss: 419.2052 - loglik: -4.1484e+02 - logprior: -4.3683e+00
Epoch 3/10
11/11 - 1s - loss: 415.2386 - loglik: -4.1312e+02 - logprior: -2.1136e+00
Epoch 4/10
11/11 - 1s - loss: 415.7499 - loglik: -4.1447e+02 - logprior: -1.2744e+00
Fitted a model with MAP estimate = -414.5307
Time for alignment: 39.7413
Computed alignments with likelihoods: ['-412.9285', '-411.8962', '-408.8446', '-414.5407', '-414.5307']
Best model has likelihood: -408.8446  (prior= -0.7008 )
time for generating output: 0.1199
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cytb.projection.fasta
SP score = 0.8268109908409659
Training of 5 independent models on file kringle.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bbf90a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efa6b940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9abe94e20>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 492.0558 - loglik: -4.5141e+02 - logprior: -4.0650e+01
Epoch 2/10
10/10 - 1s - loss: 429.1958 - loglik: -4.1841e+02 - logprior: -1.0781e+01
Epoch 3/10
10/10 - 1s - loss: 397.4002 - loglik: -3.9195e+02 - logprior: -5.4529e+00
Epoch 4/10
10/10 - 1s - loss: 375.5054 - loglik: -3.7177e+02 - logprior: -3.7366e+00
Epoch 5/10
10/10 - 1s - loss: 368.1887 - loglik: -3.6523e+02 - logprior: -2.9625e+00
Epoch 6/10
10/10 - 1s - loss: 364.6440 - loglik: -3.6218e+02 - logprior: -2.4606e+00
Epoch 7/10
10/10 - 1s - loss: 363.1998 - loglik: -3.6103e+02 - logprior: -2.1670e+00
Epoch 8/10
10/10 - 1s - loss: 362.3733 - loglik: -3.6035e+02 - logprior: -2.0200e+00
Epoch 9/10
10/10 - 1s - loss: 361.7295 - loglik: -3.5980e+02 - logprior: -1.9255e+00
Epoch 10/10
10/10 - 1s - loss: 361.3517 - loglik: -3.5947e+02 - logprior: -1.8843e+00
Fitted a model with MAP estimate = -361.1116
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 399.0830 - loglik: -3.6221e+02 - logprior: -3.6870e+01
Epoch 2/2
10/10 - 1s - loss: 359.4840 - loglik: -3.4980e+02 - logprior: -9.6835e+00
Fitted a model with MAP estimate = -354.0238
expansions: []
discards: [ 0 46 59 66 71]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 404.3676 - loglik: -3.5915e+02 - logprior: -4.5216e+01
Epoch 2/2
10/10 - 1s - loss: 371.1218 - loglik: -3.5264e+02 - logprior: -1.8482e+01
Fitted a model with MAP estimate = -366.6889
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 392.3687 - loglik: -3.5600e+02 - logprior: -3.6369e+01
Epoch 2/10
10/10 - 1s - loss: 357.7492 - loglik: -3.4869e+02 - logprior: -9.0554e+00
Epoch 3/10
10/10 - 1s - loss: 351.8576 - loglik: -3.4834e+02 - logprior: -3.5222e+00
Epoch 4/10
10/10 - 1s - loss: 349.1687 - loglik: -3.4785e+02 - logprior: -1.3160e+00
Epoch 5/10
10/10 - 1s - loss: 347.6101 - loglik: -3.4743e+02 - logprior: -1.7865e-01
Epoch 6/10
10/10 - 1s - loss: 346.4996 - loglik: -3.4692e+02 - logprior: 0.4193
Epoch 7/10
10/10 - 1s - loss: 345.7288 - loglik: -3.4646e+02 - logprior: 0.7340
Epoch 8/10
10/10 - 1s - loss: 345.2432 - loglik: -3.4618e+02 - logprior: 0.9356
Epoch 9/10
10/10 - 1s - loss: 344.8975 - loglik: -3.4600e+02 - logprior: 1.1071
Epoch 10/10
10/10 - 1s - loss: 343.8308 - loglik: -3.4509e+02 - logprior: 1.2593
Fitted a model with MAP estimate = -344.0116
Time for alignment: 31.6730
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 492.3649 - loglik: -4.5172e+02 - logprior: -4.0650e+01
Epoch 2/10
10/10 - 1s - loss: 428.8989 - loglik: -4.1812e+02 - logprior: -1.0780e+01
Epoch 3/10
10/10 - 1s - loss: 397.9325 - loglik: -3.9250e+02 - logprior: -5.4306e+00
Epoch 4/10
10/10 - 1s - loss: 376.2350 - loglik: -3.7254e+02 - logprior: -3.6923e+00
Epoch 5/10
10/10 - 1s - loss: 367.9653 - loglik: -3.6502e+02 - logprior: -2.9482e+00
Epoch 6/10
10/10 - 1s - loss: 364.8831 - loglik: -3.6242e+02 - logprior: -2.4617e+00
Epoch 7/10
10/10 - 1s - loss: 363.1442 - loglik: -3.6098e+02 - logprior: -2.1657e+00
Epoch 8/10
10/10 - 1s - loss: 362.0744 - loglik: -3.6004e+02 - logprior: -2.0336e+00
Epoch 9/10
10/10 - 1s - loss: 361.6198 - loglik: -3.5967e+02 - logprior: -1.9517e+00
Epoch 10/10
10/10 - 1s - loss: 361.3045 - loglik: -3.5940e+02 - logprior: -1.9007e+00
Fitted a model with MAP estimate = -361.0678
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 399.3638 - loglik: -3.6250e+02 - logprior: -3.6864e+01
Epoch 2/2
10/10 - 1s - loss: 359.0095 - loglik: -3.4933e+02 - logprior: -9.6817e+00
Fitted a model with MAP estimate = -354.0218
expansions: []
discards: [ 0 46 59 66 71]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 404.1838 - loglik: -3.5901e+02 - logprior: -4.5172e+01
Epoch 2/2
10/10 - 1s - loss: 371.5756 - loglik: -3.5310e+02 - logprior: -1.8479e+01
Fitted a model with MAP estimate = -366.6480
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 391.8733 - loglik: -3.5563e+02 - logprior: -3.6244e+01
Epoch 2/10
10/10 - 1s - loss: 357.8264 - loglik: -3.4879e+02 - logprior: -9.0345e+00
Epoch 3/10
10/10 - 1s - loss: 352.2036 - loglik: -3.4869e+02 - logprior: -3.5118e+00
Epoch 4/10
10/10 - 1s - loss: 349.1500 - loglik: -3.4784e+02 - logprior: -1.3101e+00
Epoch 5/10
10/10 - 1s - loss: 347.7061 - loglik: -3.4753e+02 - logprior: -1.7670e-01
Epoch 6/10
10/10 - 1s - loss: 346.8946 - loglik: -3.4730e+02 - logprior: 0.4095
Epoch 7/10
10/10 - 1s - loss: 345.4707 - loglik: -3.4618e+02 - logprior: 0.7141
Epoch 8/10
10/10 - 1s - loss: 345.1670 - loglik: -3.4609e+02 - logprior: 0.9197
Epoch 9/10
10/10 - 1s - loss: 344.7256 - loglik: -3.4582e+02 - logprior: 1.0926
Epoch 10/10
10/10 - 1s - loss: 344.3210 - loglik: -3.4557e+02 - logprior: 1.2519
Fitted a model with MAP estimate = -344.0404
Time for alignment: 31.3814
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 492.1208 - loglik: -4.5147e+02 - logprior: -4.0650e+01
Epoch 2/10
10/10 - 1s - loss: 429.3249 - loglik: -4.1854e+02 - logprior: -1.0781e+01
Epoch 3/10
10/10 - 1s - loss: 398.6690 - loglik: -3.9321e+02 - logprior: -5.4581e+00
Epoch 4/10
10/10 - 1s - loss: 376.8049 - loglik: -3.7307e+02 - logprior: -3.7325e+00
Epoch 5/10
10/10 - 1s - loss: 368.0920 - loglik: -3.6512e+02 - logprior: -2.9668e+00
Epoch 6/10
10/10 - 1s - loss: 364.3546 - loglik: -3.6185e+02 - logprior: -2.5063e+00
Epoch 7/10
10/10 - 1s - loss: 362.7820 - loglik: -3.6059e+02 - logprior: -2.1886e+00
Epoch 8/10
10/10 - 1s - loss: 361.8872 - loglik: -3.5982e+02 - logprior: -2.0667e+00
Epoch 9/10
10/10 - 1s - loss: 360.7896 - loglik: -3.5881e+02 - logprior: -1.9814e+00
Epoch 10/10
10/10 - 1s - loss: 360.8947 - loglik: -3.5896e+02 - logprior: -1.9312e+00
Fitted a model with MAP estimate = -360.4584
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (28, 1), (30, 1), (32, 2), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 399.1613 - loglik: -3.6227e+02 - logprior: -3.6891e+01
Epoch 2/2
10/10 - 1s - loss: 358.6962 - loglik: -3.4902e+02 - logprior: -9.6723e+00
Fitted a model with MAP estimate = -353.9526
expansions: []
discards: [ 0 46 59 66 71]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 404.2582 - loglik: -3.5909e+02 - logprior: -4.5171e+01
Epoch 2/2
10/10 - 1s - loss: 371.1280 - loglik: -3.5265e+02 - logprior: -1.8483e+01
Fitted a model with MAP estimate = -366.6608
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 391.9562 - loglik: -3.5571e+02 - logprior: -3.6249e+01
Epoch 2/10
10/10 - 1s - loss: 357.9681 - loglik: -3.4893e+02 - logprior: -9.0412e+00
Epoch 3/10
10/10 - 1s - loss: 352.1402 - loglik: -3.4861e+02 - logprior: -3.5277e+00
Epoch 4/10
10/10 - 1s - loss: 349.1499 - loglik: -3.4782e+02 - logprior: -1.3303e+00
Epoch 5/10
10/10 - 1s - loss: 347.5676 - loglik: -3.4737e+02 - logprior: -2.0151e-01
Epoch 6/10
10/10 - 1s - loss: 346.6907 - loglik: -3.4708e+02 - logprior: 0.3876
Epoch 7/10
10/10 - 1s - loss: 345.8060 - loglik: -3.4650e+02 - logprior: 0.6998
Epoch 8/10
10/10 - 1s - loss: 344.8904 - loglik: -3.4579e+02 - logprior: 0.9022
Epoch 9/10
10/10 - 1s - loss: 344.7634 - loglik: -3.4584e+02 - logprior: 1.0787
Epoch 10/10
10/10 - 1s - loss: 344.4490 - loglik: -3.4567e+02 - logprior: 1.2235
Fitted a model with MAP estimate = -344.0540
Time for alignment: 31.4777
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 491.9641 - loglik: -4.5131e+02 - logprior: -4.0649e+01
Epoch 2/10
10/10 - 1s - loss: 429.3999 - loglik: -4.1862e+02 - logprior: -1.0784e+01
Epoch 3/10
10/10 - 1s - loss: 398.5015 - loglik: -3.9302e+02 - logprior: -5.4828e+00
Epoch 4/10
10/10 - 1s - loss: 376.9374 - loglik: -3.7316e+02 - logprior: -3.7743e+00
Epoch 5/10
10/10 - 1s - loss: 367.9061 - loglik: -3.6489e+02 - logprior: -3.0117e+00
Epoch 6/10
10/10 - 1s - loss: 364.8574 - loglik: -3.6234e+02 - logprior: -2.5172e+00
Epoch 7/10
10/10 - 1s - loss: 362.9732 - loglik: -3.6074e+02 - logprior: -2.2300e+00
Epoch 8/10
10/10 - 1s - loss: 361.5736 - loglik: -3.5947e+02 - logprior: -2.1012e+00
Epoch 9/10
10/10 - 1s - loss: 361.7272 - loglik: -3.5971e+02 - logprior: -2.0154e+00
Fitted a model with MAP estimate = -361.1363
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (23, 1), (25, 1), (26, 1), (30, 2), (31, 1), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 399.2382 - loglik: -3.6238e+02 - logprior: -3.6863e+01
Epoch 2/2
10/10 - 1s - loss: 358.9866 - loglik: -3.4931e+02 - logprior: -9.6790e+00
Fitted a model with MAP estimate = -353.9816
expansions: []
discards: [ 0 46 59 66 71]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 403.9765 - loglik: -3.5887e+02 - logprior: -4.5111e+01
Epoch 2/2
10/10 - 1s - loss: 371.1749 - loglik: -3.5272e+02 - logprior: -1.8457e+01
Fitted a model with MAP estimate = -366.6208
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 392.0190 - loglik: -3.5591e+02 - logprior: -3.6105e+01
Epoch 2/10
10/10 - 1s - loss: 357.7760 - loglik: -3.4878e+02 - logprior: -8.9981e+00
Epoch 3/10
10/10 - 1s - loss: 351.8050 - loglik: -3.4831e+02 - logprior: -3.4969e+00
Epoch 4/10
10/10 - 1s - loss: 349.3475 - loglik: -3.4804e+02 - logprior: -1.3025e+00
Epoch 5/10
10/10 - 1s - loss: 347.6872 - loglik: -3.4751e+02 - logprior: -1.7587e-01
Epoch 6/10
10/10 - 1s - loss: 346.3793 - loglik: -3.4678e+02 - logprior: 0.4066
Epoch 7/10
10/10 - 1s - loss: 346.0368 - loglik: -3.4674e+02 - logprior: 0.7047
Epoch 8/10
10/10 - 1s - loss: 345.0623 - loglik: -3.4597e+02 - logprior: 0.9073
Epoch 9/10
10/10 - 1s - loss: 344.6329 - loglik: -3.4572e+02 - logprior: 1.0859
Epoch 10/10
10/10 - 1s - loss: 344.4474 - loglik: -3.4569e+02 - logprior: 1.2432
Fitted a model with MAP estimate = -344.0705
Time for alignment: 30.0374
Fitting a model of length 62 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 492.1858 - loglik: -4.5154e+02 - logprior: -4.0650e+01
Epoch 2/10
10/10 - 1s - loss: 429.4053 - loglik: -4.1862e+02 - logprior: -1.0786e+01
Epoch 3/10
10/10 - 1s - loss: 397.7123 - loglik: -3.9221e+02 - logprior: -5.4986e+00
Epoch 4/10
10/10 - 1s - loss: 376.0115 - loglik: -3.7223e+02 - logprior: -3.7825e+00
Epoch 5/10
10/10 - 1s - loss: 367.9725 - loglik: -3.6502e+02 - logprior: -2.9505e+00
Epoch 6/10
10/10 - 1s - loss: 364.2866 - loglik: -3.6187e+02 - logprior: -2.4118e+00
Epoch 7/10
10/10 - 1s - loss: 363.0168 - loglik: -3.6089e+02 - logprior: -2.1221e+00
Epoch 8/10
10/10 - 1s - loss: 362.1355 - loglik: -3.6015e+02 - logprior: -1.9806e+00
Epoch 9/10
10/10 - 1s - loss: 361.3163 - loglik: -3.5943e+02 - logprior: -1.8864e+00
Epoch 10/10
10/10 - 1s - loss: 361.1095 - loglik: -3.5929e+02 - logprior: -1.8165e+00
Fitted a model with MAP estimate = -360.9506
expansions: [(2, 2), (5, 2), (13, 1), (15, 1), (26, 1), (27, 1), (30, 3), (31, 1), (34, 2), (44, 2), (50, 2), (52, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 399.2647 - loglik: -3.6241e+02 - logprior: -3.6850e+01
Epoch 2/2
10/10 - 1s - loss: 359.1809 - loglik: -3.4950e+02 - logprior: -9.6774e+00
Fitted a model with MAP estimate = -353.9839
expansions: []
discards: [ 0 46 59 66 71]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 404.3709 - loglik: -3.5921e+02 - logprior: -4.5163e+01
Epoch 2/2
10/10 - 1s - loss: 370.8575 - loglik: -3.5238e+02 - logprior: -1.8475e+01
Fitted a model with MAP estimate = -366.6600
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1091 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 392.2000 - loglik: -3.5598e+02 - logprior: -3.6223e+01
Epoch 2/10
10/10 - 1s - loss: 357.8027 - loglik: -3.4877e+02 - logprior: -9.0365e+00
Epoch 3/10
10/10 - 1s - loss: 351.9563 - loglik: -3.4845e+02 - logprior: -3.5050e+00
Epoch 4/10
10/10 - 1s - loss: 349.0095 - loglik: -3.4770e+02 - logprior: -1.3129e+00
Epoch 5/10
10/10 - 1s - loss: 347.5396 - loglik: -3.4735e+02 - logprior: -1.8438e-01
Epoch 6/10
10/10 - 1s - loss: 346.7700 - loglik: -3.4718e+02 - logprior: 0.4074
Epoch 7/10
10/10 - 1s - loss: 345.8802 - loglik: -3.4660e+02 - logprior: 0.7163
Epoch 8/10
10/10 - 1s - loss: 345.1574 - loglik: -3.4608e+02 - logprior: 0.9212
Epoch 9/10
10/10 - 1s - loss: 344.7842 - loglik: -3.4588e+02 - logprior: 1.0983
Epoch 10/10
10/10 - 1s - loss: 344.2297 - loglik: -3.4548e+02 - logprior: 1.2518
Fitted a model with MAP estimate = -344.0667
Time for alignment: 31.3030
Computed alignments with likelihoods: ['-344.0116', '-344.0404', '-344.0540', '-344.0705', '-344.0667']
Best model has likelihood: -344.0116  (prior= 1.3280 )
time for generating output: 0.1108
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kringle.projection.fasta
SP score = 0.9051509769094138
Training of 5 independent models on file LIM.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea44e02ac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c82f8e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea66de34f0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 342.2793 - loglik: -3.3665e+02 - logprior: -5.6277e+00
Epoch 2/10
15/15 - 1s - loss: 310.5205 - loglik: -3.0876e+02 - logprior: -1.7639e+00
Epoch 3/10
15/15 - 1s - loss: 296.0970 - loglik: -2.9434e+02 - logprior: -1.7559e+00
Epoch 4/10
15/15 - 1s - loss: 292.4630 - loglik: -2.9077e+02 - logprior: -1.6884e+00
Epoch 5/10
15/15 - 1s - loss: 291.8571 - loglik: -2.9027e+02 - logprior: -1.5846e+00
Epoch 6/10
15/15 - 1s - loss: 291.1906 - loglik: -2.8959e+02 - logprior: -1.6026e+00
Epoch 7/10
15/15 - 1s - loss: 291.1763 - loglik: -2.8960e+02 - logprior: -1.5765e+00
Epoch 8/10
15/15 - 1s - loss: 290.5132 - loglik: -2.8896e+02 - logprior: -1.5514e+00
Epoch 9/10
15/15 - 1s - loss: 290.2907 - loglik: -2.8874e+02 - logprior: -1.5466e+00
Epoch 10/10
15/15 - 1s - loss: 290.0534 - loglik: -2.8851e+02 - logprior: -1.5387e+00
Fitted a model with MAP estimate = -289.6425
expansions: [(9, 2), (10, 3), (11, 2), (24, 2), (28, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 307.0678 - loglik: -3.0010e+02 - logprior: -6.9692e+00
Epoch 2/2
15/15 - 1s - loss: 292.8835 - loglik: -2.8947e+02 - logprior: -3.4131e+00
Fitted a model with MAP estimate = -290.7823
expansions: [(0, 2)]
discards: [ 0 11 15 31 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 297.7547 - loglik: -2.9261e+02 - logprior: -5.1418e+00
Epoch 2/2
15/15 - 1s - loss: 286.8157 - loglik: -2.8512e+02 - logprior: -1.6950e+00
Fitted a model with MAP estimate = -286.2653
expansions: [(11, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 298.9694 - loglik: -2.9227e+02 - logprior: -6.7042e+00
Epoch 2/10
15/15 - 1s - loss: 287.8858 - loglik: -2.8564e+02 - logprior: -2.2439e+00
Epoch 3/10
15/15 - 1s - loss: 285.9651 - loglik: -2.8453e+02 - logprior: -1.4352e+00
Epoch 4/10
15/15 - 1s - loss: 285.7448 - loglik: -2.8450e+02 - logprior: -1.2465e+00
Epoch 5/10
15/15 - 1s - loss: 285.4404 - loglik: -2.8425e+02 - logprior: -1.1885e+00
Epoch 6/10
15/15 - 1s - loss: 284.9516 - loglik: -2.8381e+02 - logprior: -1.1420e+00
Epoch 7/10
15/15 - 1s - loss: 284.8612 - loglik: -2.8373e+02 - logprior: -1.1269e+00
Epoch 8/10
15/15 - 1s - loss: 284.5863 - loglik: -2.8345e+02 - logprior: -1.1351e+00
Epoch 9/10
15/15 - 1s - loss: 283.8176 - loglik: -2.8270e+02 - logprior: -1.1103e+00
Epoch 10/10
15/15 - 1s - loss: 283.9065 - loglik: -2.8279e+02 - logprior: -1.1099e+00
Fitted a model with MAP estimate = -283.4692
Time for alignment: 37.7380
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 341.9216 - loglik: -3.3630e+02 - logprior: -5.6261e+00
Epoch 2/10
15/15 - 1s - loss: 310.5645 - loglik: -3.0879e+02 - logprior: -1.7776e+00
Epoch 3/10
15/15 - 1s - loss: 296.1632 - loglik: -2.9439e+02 - logprior: -1.7681e+00
Epoch 4/10
15/15 - 1s - loss: 292.9272 - loglik: -2.9125e+02 - logprior: -1.6768e+00
Epoch 5/10
15/15 - 1s - loss: 291.5984 - loglik: -2.9002e+02 - logprior: -1.5744e+00
Epoch 6/10
15/15 - 1s - loss: 291.2057 - loglik: -2.8961e+02 - logprior: -1.5897e+00
Epoch 7/10
15/15 - 1s - loss: 290.4196 - loglik: -2.8885e+02 - logprior: -1.5617e+00
Epoch 8/10
15/15 - 1s - loss: 290.6649 - loglik: -2.8912e+02 - logprior: -1.5409e+00
Fitted a model with MAP estimate = -290.1270
expansions: [(9, 2), (10, 3), (11, 2), (15, 1), (24, 2), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 306.6155 - loglik: -2.9965e+02 - logprior: -6.9652e+00
Epoch 2/2
15/15 - 1s - loss: 292.9527 - loglik: -2.8957e+02 - logprior: -3.3850e+00
Fitted a model with MAP estimate = -290.3419
expansions: []
discards: [11 15 31 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 297.8227 - loglik: -2.9179e+02 - logprior: -6.0322e+00
Epoch 2/2
15/15 - 1s - loss: 287.6196 - loglik: -2.8565e+02 - logprior: -1.9730e+00
Fitted a model with MAP estimate = -286.7688
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 295.7249 - loglik: -2.9028e+02 - logprior: -5.4400e+00
Epoch 2/10
15/15 - 1s - loss: 287.0667 - loglik: -2.8519e+02 - logprior: -1.8812e+00
Epoch 3/10
15/15 - 1s - loss: 286.3130 - loglik: -2.8490e+02 - logprior: -1.4136e+00
Epoch 4/10
15/15 - 1s - loss: 286.2254 - loglik: -2.8497e+02 - logprior: -1.2539e+00
Epoch 5/10
15/15 - 1s - loss: 285.6612 - loglik: -2.8446e+02 - logprior: -1.1966e+00
Epoch 6/10
15/15 - 1s - loss: 285.3253 - loglik: -2.8417e+02 - logprior: -1.1576e+00
Epoch 7/10
15/15 - 1s - loss: 285.0206 - loglik: -2.8388e+02 - logprior: -1.1409e+00
Epoch 8/10
15/15 - 1s - loss: 285.0056 - loglik: -2.8387e+02 - logprior: -1.1355e+00
Epoch 9/10
15/15 - 1s - loss: 284.5496 - loglik: -2.8343e+02 - logprior: -1.1159e+00
Epoch 10/10
15/15 - 1s - loss: 284.1326 - loglik: -2.8301e+02 - logprior: -1.1179e+00
Fitted a model with MAP estimate = -283.8924
Time for alignment: 36.1266
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 342.3522 - loglik: -3.3673e+02 - logprior: -5.6252e+00
Epoch 2/10
15/15 - 1s - loss: 310.8424 - loglik: -3.0907e+02 - logprior: -1.7684e+00
Epoch 3/10
15/15 - 1s - loss: 296.7116 - loglik: -2.9495e+02 - logprior: -1.7657e+00
Epoch 4/10
15/15 - 1s - loss: 292.5717 - loglik: -2.9088e+02 - logprior: -1.6929e+00
Epoch 5/10
15/15 - 1s - loss: 291.8784 - loglik: -2.9030e+02 - logprior: -1.5726e+00
Epoch 6/10
15/15 - 1s - loss: 291.1476 - loglik: -2.8956e+02 - logprior: -1.5867e+00
Epoch 7/10
15/15 - 1s - loss: 291.1118 - loglik: -2.8954e+02 - logprior: -1.5669e+00
Epoch 8/10
15/15 - 1s - loss: 290.2262 - loglik: -2.8868e+02 - logprior: -1.5454e+00
Epoch 9/10
15/15 - 1s - loss: 290.3737 - loglik: -2.8883e+02 - logprior: -1.5415e+00
Fitted a model with MAP estimate = -290.0613
expansions: [(9, 2), (10, 3), (11, 2), (15, 2), (27, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 306.4680 - loglik: -2.9951e+02 - logprior: -6.9589e+00
Epoch 2/2
15/15 - 1s - loss: 292.8404 - loglik: -2.8943e+02 - logprior: -3.4121e+00
Fitted a model with MAP estimate = -290.5702
expansions: []
discards: [11 15 21 46 50]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 297.9987 - loglik: -2.9192e+02 - logprior: -6.0765e+00
Epoch 2/2
15/15 - 1s - loss: 287.0571 - loglik: -2.8507e+02 - logprior: -1.9882e+00
Fitted a model with MAP estimate = -286.5916
expansions: [(10, 1)]
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 295.6146 - loglik: -2.9015e+02 - logprior: -5.4695e+00
Epoch 2/10
15/15 - 1s - loss: 286.9855 - loglik: -2.8513e+02 - logprior: -1.8580e+00
Epoch 3/10
15/15 - 1s - loss: 286.4537 - loglik: -2.8506e+02 - logprior: -1.3938e+00
Epoch 4/10
15/15 - 1s - loss: 285.6578 - loglik: -2.8442e+02 - logprior: -1.2368e+00
Epoch 5/10
15/15 - 1s - loss: 285.4780 - loglik: -2.8430e+02 - logprior: -1.1789e+00
Epoch 6/10
15/15 - 1s - loss: 285.2121 - loglik: -2.8408e+02 - logprior: -1.1314e+00
Epoch 7/10
15/15 - 1s - loss: 285.0043 - loglik: -2.8387e+02 - logprior: -1.1307e+00
Epoch 8/10
15/15 - 1s - loss: 284.5766 - loglik: -2.8346e+02 - logprior: -1.1119e+00
Epoch 9/10
15/15 - 1s - loss: 284.2693 - loglik: -2.8317e+02 - logprior: -1.0988e+00
Epoch 10/10
15/15 - 1s - loss: 283.8274 - loglik: -2.8273e+02 - logprior: -1.0918e+00
Fitted a model with MAP estimate = -283.6384
Time for alignment: 36.1597
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 5s - loss: 342.2827 - loglik: -3.3666e+02 - logprior: -5.6257e+00
Epoch 2/10
15/15 - 1s - loss: 311.1064 - loglik: -3.0934e+02 - logprior: -1.7693e+00
Epoch 3/10
15/15 - 1s - loss: 298.2778 - loglik: -2.9652e+02 - logprior: -1.7603e+00
Epoch 4/10
15/15 - 1s - loss: 294.0954 - loglik: -2.9241e+02 - logprior: -1.6801e+00
Epoch 5/10
15/15 - 1s - loss: 291.9606 - loglik: -2.9038e+02 - logprior: -1.5804e+00
Epoch 6/10
15/15 - 1s - loss: 291.5281 - loglik: -2.8993e+02 - logprior: -1.6009e+00
Epoch 7/10
15/15 - 1s - loss: 291.3785 - loglik: -2.8980e+02 - logprior: -1.5768e+00
Epoch 8/10
15/15 - 1s - loss: 291.2102 - loglik: -2.8965e+02 - logprior: -1.5531e+00
Epoch 9/10
15/15 - 1s - loss: 290.7447 - loglik: -2.8919e+02 - logprior: -1.5475e+00
Epoch 10/10
15/15 - 1s - loss: 290.4093 - loglik: -2.8886e+02 - logprior: -1.5385e+00
Fitted a model with MAP estimate = -290.2643
expansions: [(9, 2), (10, 3), (11, 2), (18, 1), (27, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 306.2308 - loglik: -2.9928e+02 - logprior: -6.9536e+00
Epoch 2/2
15/15 - 1s - loss: 293.0579 - loglik: -2.8968e+02 - logprior: -3.3741e+00
Fitted a model with MAP estimate = -290.6988
expansions: [(0, 2)]
discards: [ 0 11 15 45 49]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 297.4083 - loglik: -2.9227e+02 - logprior: -5.1361e+00
Epoch 2/2
15/15 - 1s - loss: 287.3053 - loglik: -2.8561e+02 - logprior: -1.6961e+00
Fitted a model with MAP estimate = -286.2530
expansions: [(11, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 59 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 298.9165 - loglik: -2.9222e+02 - logprior: -6.6979e+00
Epoch 2/10
15/15 - 1s - loss: 287.9482 - loglik: -2.8570e+02 - logprior: -2.2461e+00
Epoch 3/10
15/15 - 1s - loss: 286.0494 - loglik: -2.8462e+02 - logprior: -1.4328e+00
Epoch 4/10
15/15 - 1s - loss: 285.7611 - loglik: -2.8452e+02 - logprior: -1.2408e+00
Epoch 5/10
15/15 - 1s - loss: 285.3179 - loglik: -2.8414e+02 - logprior: -1.1814e+00
Epoch 6/10
15/15 - 1s - loss: 284.9824 - loglik: -2.8384e+02 - logprior: -1.1448e+00
Epoch 7/10
15/15 - 1s - loss: 284.3801 - loglik: -2.8325e+02 - logprior: -1.1305e+00
Epoch 8/10
15/15 - 1s - loss: 284.8136 - loglik: -2.8369e+02 - logprior: -1.1202e+00
Fitted a model with MAP estimate = -284.1759
Time for alignment: 35.7474
Fitting a model of length 45 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 342.3848 - loglik: -3.3676e+02 - logprior: -5.6266e+00
Epoch 2/10
15/15 - 1s - loss: 310.0060 - loglik: -3.0823e+02 - logprior: -1.7722e+00
Epoch 3/10
15/15 - 1s - loss: 296.5600 - loglik: -2.9479e+02 - logprior: -1.7694e+00
Epoch 4/10
15/15 - 1s - loss: 292.8421 - loglik: -2.9116e+02 - logprior: -1.6862e+00
Epoch 5/10
15/15 - 1s - loss: 291.5519 - loglik: -2.8998e+02 - logprior: -1.5731e+00
Epoch 6/10
15/15 - 1s - loss: 291.3335 - loglik: -2.8974e+02 - logprior: -1.5926e+00
Epoch 7/10
15/15 - 1s - loss: 290.6850 - loglik: -2.8912e+02 - logprior: -1.5654e+00
Epoch 8/10
15/15 - 1s - loss: 291.0933 - loglik: -2.8954e+02 - logprior: -1.5460e+00
Fitted a model with MAP estimate = -290.3918
expansions: [(9, 2), (10, 3), (11, 2), (15, 1), (27, 1), (29, 2), (30, 2), (32, 3), (34, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 62 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 5s - loss: 305.8988 - loglik: -2.9895e+02 - logprior: -6.9533e+00
Epoch 2/2
15/15 - 1s - loss: 293.1228 - loglik: -2.8977e+02 - logprior: -3.3486e+00
Fitted a model with MAP estimate = -290.4074
expansions: []
discards: [11 15 45 49]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 4s - loss: 297.8511 - loglik: -2.9182e+02 - logprior: -6.0290e+00
Epoch 2/2
15/15 - 1s - loss: 287.5140 - loglik: -2.8554e+02 - logprior: -1.9737e+00
Fitted a model with MAP estimate = -286.8241
expansions: []
discards: [36]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6428 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 3s - loss: 295.6798 - loglik: -2.9024e+02 - logprior: -5.4387e+00
Epoch 2/10
15/15 - 1s - loss: 287.3450 - loglik: -2.8546e+02 - logprior: -1.8862e+00
Epoch 3/10
15/15 - 1s - loss: 286.4875 - loglik: -2.8508e+02 - logprior: -1.4097e+00
Epoch 4/10
15/15 - 1s - loss: 285.7552 - loglik: -2.8450e+02 - logprior: -1.2558e+00
Epoch 5/10
15/15 - 1s - loss: 285.7490 - loglik: -2.8455e+02 - logprior: -1.1964e+00
Epoch 6/10
15/15 - 1s - loss: 285.4631 - loglik: -2.8431e+02 - logprior: -1.1520e+00
Epoch 7/10
15/15 - 1s - loss: 285.1387 - loglik: -2.8399e+02 - logprior: -1.1464e+00
Epoch 8/10
15/15 - 1s - loss: 284.5220 - loglik: -2.8339e+02 - logprior: -1.1234e+00
Epoch 9/10
15/15 - 1s - loss: 284.5772 - loglik: -2.8345e+02 - logprior: -1.1200e+00
Fitted a model with MAP estimate = -284.2140
Time for alignment: 34.9367
Computed alignments with likelihoods: ['-283.4692', '-283.8924', '-283.6384', '-284.1759', '-284.2140']
Best model has likelihood: -283.4692  (prior= -1.0859 )
time for generating output: 0.1095
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/LIM.projection.fasta
SP score = 0.9790322580645161
Training of 5 independent models on file annexin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9ca4b130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9bc095d30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9526bfcd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 381.5002 - loglik: -3.6717e+02 - logprior: -1.4335e+01
Epoch 2/10
10/10 - 1s - loss: 353.0837 - loglik: -3.4898e+02 - logprior: -4.1065e+00
Epoch 3/10
10/10 - 1s - loss: 336.6286 - loglik: -3.3417e+02 - logprior: -2.4608e+00
Epoch 4/10
10/10 - 2s - loss: 325.6242 - loglik: -3.2353e+02 - logprior: -2.0903e+00
Epoch 5/10
10/10 - 1s - loss: 319.4942 - loglik: -3.1766e+02 - logprior: -1.8332e+00
Epoch 6/10
10/10 - 2s - loss: 316.6524 - loglik: -3.1503e+02 - logprior: -1.6247e+00
Epoch 7/10
10/10 - 1s - loss: 315.0313 - loglik: -3.1350e+02 - logprior: -1.5245e+00
Epoch 8/10
10/10 - 1s - loss: 312.9562 - loglik: -3.1151e+02 - logprior: -1.4473e+00
Epoch 9/10
10/10 - 1s - loss: 312.2766 - loglik: -3.1086e+02 - logprior: -1.4115e+00
Epoch 10/10
10/10 - 2s - loss: 311.3410 - loglik: -3.0996e+02 - logprior: -1.3789e+00
Fitted a model with MAP estimate = -310.7735
expansions: [(0, 5), (24, 1), (27, 1), (32, 1), (33, 1), (38, 1), (39, 1), (40, 2), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 334.1360 - loglik: -3.1644e+02 - logprior: -1.7700e+01
Epoch 2/2
10/10 - 1s - loss: 310.8377 - loglik: -3.0545e+02 - logprior: -5.3877e+00
Fitted a model with MAP estimate = -307.5391
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 317.0754 - loglik: -3.0397e+02 - logprior: -1.3106e+01
Epoch 2/10
10/10 - 1s - loss: 306.1214 - loglik: -3.0254e+02 - logprior: -3.5852e+00
Epoch 3/10
10/10 - 2s - loss: 304.4909 - loglik: -3.0256e+02 - logprior: -1.9330e+00
Epoch 4/10
10/10 - 1s - loss: 303.5553 - loglik: -3.0205e+02 - logprior: -1.5004e+00
Epoch 5/10
10/10 - 2s - loss: 304.0052 - loglik: -3.0272e+02 - logprior: -1.2876e+00
Fitted a model with MAP estimate = -302.4257
Time for alignment: 38.4192
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 381.2081 - loglik: -3.6687e+02 - logprior: -1.4334e+01
Epoch 2/10
10/10 - 1s - loss: 353.6375 - loglik: -3.4954e+02 - logprior: -4.0964e+00
Epoch 3/10
10/10 - 1s - loss: 335.7646 - loglik: -3.3335e+02 - logprior: -2.4096e+00
Epoch 4/10
10/10 - 1s - loss: 326.1059 - loglik: -3.2410e+02 - logprior: -2.0069e+00
Epoch 5/10
10/10 - 1s - loss: 320.1924 - loglik: -3.1845e+02 - logprior: -1.7462e+00
Epoch 6/10
10/10 - 2s - loss: 316.5460 - loglik: -3.1499e+02 - logprior: -1.5579e+00
Epoch 7/10
10/10 - 2s - loss: 314.7819 - loglik: -3.1329e+02 - logprior: -1.4903e+00
Epoch 8/10
10/10 - 2s - loss: 313.1999 - loglik: -3.1175e+02 - logprior: -1.4464e+00
Epoch 9/10
10/10 - 2s - loss: 311.9130 - loglik: -3.1048e+02 - logprior: -1.4278e+00
Epoch 10/10
10/10 - 1s - loss: 310.6895 - loglik: -3.0927e+02 - logprior: -1.4097e+00
Fitted a model with MAP estimate = -310.4355
expansions: [(0, 5), (24, 1), (30, 1), (32, 1), (33, 1), (38, 1), (39, 1), (40, 2), (46, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 334.2560 - loglik: -3.1656e+02 - logprior: -1.7695e+01
Epoch 2/2
10/10 - 2s - loss: 311.7740 - loglik: -3.0638e+02 - logprior: -5.3935e+00
Fitted a model with MAP estimate = -307.5799
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 316.7933 - loglik: -3.0368e+02 - logprior: -1.3112e+01
Epoch 2/10
10/10 - 1s - loss: 305.9923 - loglik: -3.0240e+02 - logprior: -3.5899e+00
Epoch 3/10
10/10 - 2s - loss: 304.7188 - loglik: -3.0278e+02 - logprior: -1.9367e+00
Epoch 4/10
10/10 - 2s - loss: 303.3563 - loglik: -3.0186e+02 - logprior: -1.5007e+00
Epoch 5/10
10/10 - 2s - loss: 302.8622 - loglik: -3.0157e+02 - logprior: -1.2904e+00
Epoch 6/10
10/10 - 2s - loss: 302.4604 - loglik: -3.0139e+02 - logprior: -1.0731e+00
Epoch 7/10
10/10 - 1s - loss: 301.2825 - loglik: -3.0034e+02 - logprior: -9.4492e-01
Epoch 8/10
10/10 - 2s - loss: 300.1186 - loglik: -2.9919e+02 - logprior: -9.2311e-01
Epoch 9/10
10/10 - 1s - loss: 298.5397 - loglik: -2.9762e+02 - logprior: -9.1288e-01
Epoch 10/10
10/10 - 1s - loss: 297.9647 - loglik: -2.9706e+02 - logprior: -9.0070e-01
Fitted a model with MAP estimate = -297.4300
Time for alignment: 46.1443
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 381.2520 - loglik: -3.6692e+02 - logprior: -1.4333e+01
Epoch 2/10
10/10 - 2s - loss: 353.3253 - loglik: -3.4923e+02 - logprior: -4.0988e+00
Epoch 3/10
10/10 - 2s - loss: 336.2032 - loglik: -3.3375e+02 - logprior: -2.4534e+00
Epoch 4/10
10/10 - 2s - loss: 324.6749 - loglik: -3.2258e+02 - logprior: -2.0959e+00
Epoch 5/10
10/10 - 2s - loss: 318.8184 - loglik: -3.1695e+02 - logprior: -1.8694e+00
Epoch 6/10
10/10 - 1s - loss: 315.5211 - loglik: -3.1381e+02 - logprior: -1.7143e+00
Epoch 7/10
10/10 - 2s - loss: 314.3864 - loglik: -3.1271e+02 - logprior: -1.6745e+00
Epoch 8/10
10/10 - 1s - loss: 311.7941 - loglik: -3.1019e+02 - logprior: -1.6021e+00
Epoch 9/10
10/10 - 2s - loss: 311.2509 - loglik: -3.0971e+02 - logprior: -1.5342e+00
Epoch 10/10
10/10 - 2s - loss: 310.6105 - loglik: -3.0911e+02 - logprior: -1.4998e+00
Fitted a model with MAP estimate = -309.8687
expansions: [(0, 5), (24, 1), (27, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 333.4280 - loglik: -3.1571e+02 - logprior: -1.7719e+01
Epoch 2/2
10/10 - 1s - loss: 311.5534 - loglik: -3.0616e+02 - logprior: -5.3899e+00
Fitted a model with MAP estimate = -307.5679
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.1183 - loglik: -3.0400e+02 - logprior: -1.3114e+01
Epoch 2/10
10/10 - 1s - loss: 305.7825 - loglik: -3.0220e+02 - logprior: -3.5842e+00
Epoch 3/10
10/10 - 1s - loss: 304.6806 - loglik: -3.0275e+02 - logprior: -1.9310e+00
Epoch 4/10
10/10 - 1s - loss: 303.3849 - loglik: -3.0189e+02 - logprior: -1.4965e+00
Epoch 5/10
10/10 - 2s - loss: 302.9691 - loglik: -3.0168e+02 - logprior: -1.2878e+00
Epoch 6/10
10/10 - 1s - loss: 302.1965 - loglik: -3.0112e+02 - logprior: -1.0754e+00
Epoch 7/10
10/10 - 1s - loss: 301.2145 - loglik: -3.0027e+02 - logprior: -9.4281e-01
Epoch 8/10
10/10 - 1s - loss: 299.9363 - loglik: -2.9901e+02 - logprior: -9.1963e-01
Epoch 9/10
10/10 - 1s - loss: 298.9992 - loglik: -2.9808e+02 - logprior: -9.1545e-01
Epoch 10/10
10/10 - 2s - loss: 298.7876 - loglik: -2.9789e+02 - logprior: -8.9715e-01
Fitted a model with MAP estimate = -297.4415
Time for alignment: 47.1938
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 381.7629 - loglik: -3.6743e+02 - logprior: -1.4333e+01
Epoch 2/10
10/10 - 1s - loss: 353.4153 - loglik: -3.4931e+02 - logprior: -4.1006e+00
Epoch 3/10
10/10 - 2s - loss: 337.2118 - loglik: -3.3473e+02 - logprior: -2.4815e+00
Epoch 4/10
10/10 - 1s - loss: 325.4809 - loglik: -3.2326e+02 - logprior: -2.2161e+00
Epoch 5/10
10/10 - 1s - loss: 319.9868 - loglik: -3.1774e+02 - logprior: -2.2435e+00
Epoch 6/10
10/10 - 1s - loss: 317.6603 - loglik: -3.1536e+02 - logprior: -2.3025e+00
Epoch 7/10
10/10 - 1s - loss: 315.8004 - loglik: -3.1355e+02 - logprior: -2.2484e+00
Epoch 8/10
10/10 - 2s - loss: 314.3476 - loglik: -3.1222e+02 - logprior: -2.1281e+00
Epoch 9/10
10/10 - 1s - loss: 314.0095 - loglik: -3.1195e+02 - logprior: -2.0595e+00
Epoch 10/10
10/10 - 1s - loss: 312.0660 - loglik: -3.1001e+02 - logprior: -2.0509e+00
Fitted a model with MAP estimate = -312.1274
expansions: [(4, 2), (5, 2), (7, 2), (8, 2), (24, 1), (30, 1), (32, 1), (33, 1), (35, 1), (38, 1), (39, 1), (40, 1), (41, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 334.3733 - loglik: -3.1840e+02 - logprior: -1.5975e+01
Epoch 2/2
10/10 - 1s - loss: 313.0841 - loglik: -3.0617e+02 - logprior: -6.9115e+00
Fitted a model with MAP estimate = -309.6622
expansions: [(0, 2)]
discards: [ 0 10 13]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 315.7342 - loglik: -3.0310e+02 - logprior: -1.2630e+01
Epoch 2/2
10/10 - 1s - loss: 306.7336 - loglik: -3.0329e+02 - logprior: -3.4393e+00
Fitted a model with MAP estimate = -304.1302
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 320.3675 - loglik: -3.0527e+02 - logprior: -1.5102e+01
Epoch 2/10
10/10 - 1s - loss: 307.3060 - loglik: -3.0237e+02 - logprior: -4.9375e+00
Epoch 3/10
10/10 - 1s - loss: 304.3424 - loglik: -3.0186e+02 - logprior: -2.4854e+00
Epoch 4/10
10/10 - 1s - loss: 303.0741 - loglik: -3.0155e+02 - logprior: -1.5204e+00
Epoch 5/10
10/10 - 1s - loss: 303.1363 - loglik: -3.0189e+02 - logprior: -1.2453e+00
Fitted a model with MAP estimate = -302.1574
Time for alignment: 43.3763
Fitting a model of length 52 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 382.1226 - loglik: -3.6779e+02 - logprior: -1.4332e+01
Epoch 2/10
10/10 - 1s - loss: 352.6023 - loglik: -3.4851e+02 - logprior: -4.0928e+00
Epoch 3/10
10/10 - 2s - loss: 335.0400 - loglik: -3.3263e+02 - logprior: -2.4105e+00
Epoch 4/10
10/10 - 1s - loss: 323.2349 - loglik: -3.2120e+02 - logprior: -2.0371e+00
Epoch 5/10
10/10 - 2s - loss: 318.9871 - loglik: -3.1714e+02 - logprior: -1.8464e+00
Epoch 6/10
10/10 - 2s - loss: 314.8930 - loglik: -3.1318e+02 - logprior: -1.7068e+00
Epoch 7/10
10/10 - 2s - loss: 314.4568 - loglik: -3.1279e+02 - logprior: -1.6671e+00
Epoch 8/10
10/10 - 2s - loss: 312.3654 - loglik: -3.1076e+02 - logprior: -1.5979e+00
Epoch 9/10
10/10 - 1s - loss: 311.0269 - loglik: -3.0950e+02 - logprior: -1.5235e+00
Epoch 10/10
10/10 - 1s - loss: 310.6498 - loglik: -3.0916e+02 - logprior: -1.4898e+00
Fitted a model with MAP estimate = -309.9100
expansions: [(0, 5), (24, 1), (27, 1), (32, 1), (33, 1), (35, 1), (37, 1), (38, 1), (39, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 332.8665 - loglik: -3.1517e+02 - logprior: -1.7701e+01
Epoch 2/2
10/10 - 1s - loss: 311.7144 - loglik: -3.0633e+02 - logprior: -5.3869e+00
Fitted a model with MAP estimate = -307.5164
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 66 on 3139 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 317.2648 - loglik: -3.0416e+02 - logprior: -1.3106e+01
Epoch 2/10
10/10 - 1s - loss: 305.8529 - loglik: -3.0227e+02 - logprior: -3.5815e+00
Epoch 3/10
10/10 - 1s - loss: 304.6263 - loglik: -3.0270e+02 - logprior: -1.9309e+00
Epoch 4/10
10/10 - 1s - loss: 303.4443 - loglik: -3.0195e+02 - logprior: -1.4987e+00
Epoch 5/10
10/10 - 1s - loss: 302.8961 - loglik: -3.0161e+02 - logprior: -1.2896e+00
Epoch 6/10
10/10 - 1s - loss: 301.6108 - loglik: -3.0054e+02 - logprior: -1.0735e+00
Epoch 7/10
10/10 - 1s - loss: 301.7724 - loglik: -3.0083e+02 - logprior: -9.4481e-01
Fitted a model with MAP estimate = -300.4335
Time for alignment: 40.9058
Computed alignments with likelihoods: ['-302.4257', '-297.4300', '-297.4415', '-302.1574', '-300.4335']
Best model has likelihood: -297.4300  (prior= -0.8938 )
time for generating output: 0.2468
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/annexin.projection.fasta
SP score = 0.9959805373386926
Training of 5 independent models on file hormone_rec.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea008e6d60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea09076ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea77b68f10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 1073.4865 - loglik: -1.0673e+03 - logprior: -6.1895e+00
Epoch 2/10
23/23 - 4s - loss: 1004.0209 - loglik: -1.0034e+03 - logprior: -6.0185e-01
Epoch 3/10
23/23 - 4s - loss: 982.9724 - loglik: -9.8255e+02 - logprior: -4.2011e-01
Epoch 4/10
23/23 - 4s - loss: 979.2721 - loglik: -9.7894e+02 - logprior: -3.2795e-01
Epoch 5/10
23/23 - 4s - loss: 969.7155 - loglik: -9.6939e+02 - logprior: -3.2461e-01
Epoch 6/10
23/23 - 4s - loss: 954.3505 - loglik: -9.5397e+02 - logprior: -3.7172e-01
Epoch 7/10
23/23 - 4s - loss: 918.6317 - loglik: -9.1801e+02 - logprior: -6.1171e-01
Epoch 8/10
23/23 - 4s - loss: 824.7248 - loglik: -8.2111e+02 - logprior: -3.5984e+00
Epoch 9/10
23/23 - 4s - loss: 773.4377 - loglik: -7.6877e+02 - logprior: -4.6495e+00
Epoch 10/10
23/23 - 4s - loss: 758.8449 - loglik: -7.5420e+02 - logprior: -4.6176e+00
Fitted a model with MAP estimate = -755.0446
expansions: []
discards: [ 24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41
  42  43  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96
  97 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 1141.6567 - loglik: -1.1304e+03 - logprior: -1.1247e+01
Epoch 2/2
11/11 - 2s - loss: 1108.4323 - loglik: -1.1053e+03 - logprior: -3.1352e+00
Fitted a model with MAP estimate = -1104.0831
expansions: [(0, 114), (107, 72)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  67  68  69  70  71  72  73  74  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100 101 102 103 104 105 106]
Re-initialized the encoder parameters.
Fitting a model of length 190 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 10s - loss: 1055.2744 - loglik: -1.0490e+03 - logprior: -6.2291e+00
Epoch 2/2
23/23 - 5s - loss: 983.8218 - loglik: -9.8368e+02 - logprior: -1.3720e-01
Fitted a model with MAP estimate = -975.1981
expansions: [(0, 22), (20, 1), (67, 1), (77, 1), (97, 1), (190, 9)]
discards: [127 147 148 162 163 164 170]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 982.2460 - loglik: -9.7474e+02 - logprior: -7.5057e+00
Epoch 2/10
23/23 - 6s - loss: 967.9023 - loglik: -9.6737e+02 - logprior: -5.3450e-01
Epoch 3/10
23/23 - 6s - loss: 965.9698 - loglik: -9.6594e+02 - logprior: -2.6060e-02
Epoch 4/10
23/23 - 6s - loss: 962.4610 - loglik: -9.6262e+02 - logprior: 0.1648
Epoch 5/10
23/23 - 6s - loss: 957.2324 - loglik: -9.5770e+02 - logprior: 0.4724
Epoch 6/10
23/23 - 6s - loss: 942.8087 - loglik: -9.4328e+02 - logprior: 0.4815
Epoch 7/10
23/23 - 6s - loss: 906.6530 - loglik: -9.0692e+02 - logprior: 0.2768
Epoch 8/10
23/23 - 7s - loss: 840.6987 - loglik: -8.4040e+02 - logprior: -2.8238e-01
Epoch 9/10
23/23 - 6s - loss: 796.3288 - loglik: -7.9575e+02 - logprior: -5.5990e-01
Epoch 10/10
23/23 - 6s - loss: 761.6190 - loglik: -7.6068e+02 - logprior: -9.2400e-01
Fitted a model with MAP estimate = -748.6352
Time for alignment: 153.2645
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 1073.0791 - loglik: -1.0669e+03 - logprior: -6.2097e+00
Epoch 2/10
23/23 - 4s - loss: 1003.2629 - loglik: -1.0025e+03 - logprior: -7.1674e-01
Epoch 3/10
23/23 - 4s - loss: 985.7866 - loglik: -9.8527e+02 - logprior: -5.1204e-01
Epoch 4/10
23/23 - 4s - loss: 977.4742 - loglik: -9.7704e+02 - logprior: -4.2838e-01
Epoch 5/10
23/23 - 4s - loss: 971.6760 - loglik: -9.7135e+02 - logprior: -3.2491e-01
Epoch 6/10
23/23 - 4s - loss: 948.5643 - loglik: -9.4819e+02 - logprior: -3.6892e-01
Epoch 7/10
23/23 - 4s - loss: 915.0032 - loglik: -9.1433e+02 - logprior: -6.6126e-01
Epoch 8/10
23/23 - 4s - loss: 827.3377 - loglik: -8.2411e+02 - logprior: -3.2122e+00
Epoch 9/10
23/23 - 4s - loss: 767.5978 - loglik: -7.6274e+02 - logprior: -4.8412e+00
Epoch 10/10
23/23 - 4s - loss: 757.2426 - loglik: -7.5257e+02 - logprior: -4.6486e+00
Fitted a model with MAP estimate = -754.1151
expansions: []
discards: [ 23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40
  41  42  43  58  79  80  81  82  83  84  85  86  87  88  89  90  91  92
  93  94  95  96  97  98  99 100 104 105 106 107 108 109 110 111 148]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 1133.4073 - loglik: -1.1223e+03 - logprior: -1.1097e+01
Epoch 2/2
11/11 - 2s - loss: 1106.8689 - loglik: -1.1038e+03 - logprior: -3.1182e+00
Fitted a model with MAP estimate = -1104.4128
expansions: [(0, 133), (106, 50)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73
  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91
  92  93  94  95  96  97  98  99 100 101 102 103 104 105]
Re-initialized the encoder parameters.
Fitting a model of length 185 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 8s - loss: 1064.1693 - loglik: -1.0580e+03 - logprior: -6.1199e+00
Epoch 2/2
23/23 - 5s - loss: 985.3002 - loglik: -9.8533e+02 - logprior: 0.0267
Fitted a model with MAP estimate = -976.2867
expansions: [(0, 23), (64, 1), (65, 1), (75, 1), (185, 9)]
discards: [  3   4 133]
Re-initialized the encoder parameters.
Fitting a model of length 217 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 982.9970 - loglik: -9.7560e+02 - logprior: -7.3973e+00
Epoch 2/10
23/23 - 6s - loss: 969.7088 - loglik: -9.6915e+02 - logprior: -5.5989e-01
Epoch 3/10
23/23 - 6s - loss: 968.3250 - loglik: -9.6827e+02 - logprior: -5.7666e-02
Epoch 4/10
23/23 - 6s - loss: 964.8297 - loglik: -9.6514e+02 - logprior: 0.3165
Epoch 5/10
23/23 - 6s - loss: 956.3370 - loglik: -9.5673e+02 - logprior: 0.3945
Epoch 6/10
23/23 - 6s - loss: 948.3569 - loglik: -9.4873e+02 - logprior: 0.3785
Epoch 7/10
23/23 - 6s - loss: 906.1799 - loglik: -9.0634e+02 - logprior: 0.1671
Epoch 8/10
23/23 - 6s - loss: 843.3923 - loglik: -8.4300e+02 - logprior: -3.8188e-01
Epoch 9/10
23/23 - 6s - loss: 792.8332 - loglik: -7.9214e+02 - logprior: -6.7289e-01
Epoch 10/10
23/23 - 6s - loss: 759.8940 - loglik: -7.5886e+02 - logprior: -1.0177e+00
Fitted a model with MAP estimate = -748.7121
Time for alignment: 152.2494
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 1074.1068 - loglik: -1.0679e+03 - logprior: -6.1949e+00
Epoch 2/10
23/23 - 4s - loss: 1001.9301 - loglik: -1.0013e+03 - logprior: -6.2309e-01
Epoch 3/10
23/23 - 4s - loss: 984.3910 - loglik: -9.8397e+02 - logprior: -4.2392e-01
Epoch 4/10
23/23 - 4s - loss: 978.7996 - loglik: -9.7852e+02 - logprior: -2.7466e-01
Epoch 5/10
23/23 - 4s - loss: 971.9836 - loglik: -9.7176e+02 - logprior: -2.1657e-01
Epoch 6/10
23/23 - 4s - loss: 951.8473 - loglik: -9.5157e+02 - logprior: -2.6894e-01
Epoch 7/10
23/23 - 4s - loss: 916.1180 - loglik: -9.1555e+02 - logprior: -5.5640e-01
Epoch 8/10
23/23 - 4s - loss: 824.5115 - loglik: -8.2094e+02 - logprior: -3.5587e+00
Epoch 9/10
23/23 - 4s - loss: 773.7377 - loglik: -7.6887e+02 - logprior: -4.8437e+00
Epoch 10/10
23/23 - 4s - loss: 759.3159 - loglik: -7.5467e+02 - logprior: -4.6284e+00
Fitted a model with MAP estimate = -755.3348
expansions: []
discards: [ 23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40
  41  42  43  58  82  83  84  85  86  87  88  89  90  91  92  93 104 105
 106 107 108 109 110 111 112]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 1142.3838 - loglik: -1.1314e+03 - logprior: -1.0965e+01
Epoch 2/2
11/11 - 2s - loss: 1108.9900 - loglik: -1.1063e+03 - logprior: -2.7361e+00
Fitted a model with MAP estimate = -1104.2447
expansions: [(0, 128), (116, 58)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74
  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92
  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110
 111 112 113 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 8s - loss: 1060.1381 - loglik: -1.0540e+03 - logprior: -6.0985e+00
Epoch 2/2
23/23 - 5s - loss: 990.6365 - loglik: -9.9044e+02 - logprior: -1.9580e-01
Fitted a model with MAP estimate = -979.9392
expansions: [(0, 24), (119, 1), (149, 1), (150, 2), (152, 2), (154, 1), (155, 1), (177, 2), (189, 12)]
discards: [  1   2   3   4   5   6   7   8  54 128]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 10s - loss: 983.8621 - loglik: -9.7638e+02 - logprior: -7.4869e+00
Epoch 2/10
23/23 - 7s - loss: 965.7930 - loglik: -9.6531e+02 - logprior: -4.8767e-01
Epoch 3/10
23/23 - 7s - loss: 965.4058 - loglik: -9.6537e+02 - logprior: -3.0659e-02
Epoch 4/10
23/23 - 7s - loss: 962.6025 - loglik: -9.6299e+02 - logprior: 0.3863
Epoch 5/10
23/23 - 7s - loss: 952.1054 - loglik: -9.5261e+02 - logprior: 0.5038
Epoch 6/10
23/23 - 7s - loss: 946.8987 - loglik: -9.4731e+02 - logprior: 0.4203
Epoch 7/10
23/23 - 7s - loss: 912.8530 - loglik: -9.1328e+02 - logprior: 0.4389
Epoch 8/10
23/23 - 7s - loss: 849.8668 - loglik: -8.4967e+02 - logprior: -1.8582e-01
Epoch 9/10
23/23 - 7s - loss: 805.0831 - loglik: -8.0451e+02 - logprior: -5.5563e-01
Epoch 10/10
23/23 - 7s - loss: 766.3383 - loglik: -7.6541e+02 - logprior: -9.0655e-01
Fitted a model with MAP estimate = -750.6639
Time for alignment: 155.5723
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 7s - loss: 1072.6847 - loglik: -1.0665e+03 - logprior: -6.2056e+00
Epoch 2/10
23/23 - 4s - loss: 1004.0757 - loglik: -1.0035e+03 - logprior: -6.0961e-01
Epoch 3/10
23/23 - 4s - loss: 986.1294 - loglik: -9.8571e+02 - logprior: -4.2130e-01
Epoch 4/10
23/23 - 4s - loss: 976.3564 - loglik: -9.7604e+02 - logprior: -3.1287e-01
Epoch 5/10
23/23 - 4s - loss: 971.1943 - loglik: -9.7094e+02 - logprior: -2.5452e-01
Epoch 6/10
23/23 - 4s - loss: 954.7724 - loglik: -9.5445e+02 - logprior: -3.1852e-01
Epoch 7/10
23/23 - 4s - loss: 915.7689 - loglik: -9.1512e+02 - logprior: -6.4126e-01
Epoch 8/10
23/23 - 4s - loss: 819.6119 - loglik: -8.1553e+02 - logprior: -4.0648e+00
Epoch 9/10
23/23 - 4s - loss: 770.9414 - loglik: -7.6614e+02 - logprior: -4.7820e+00
Epoch 10/10
23/23 - 4s - loss: 756.8261 - loglik: -7.5220e+02 - logprior: -4.5989e+00
Fitted a model with MAP estimate = -754.9787
expansions: []
discards: [ 22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39
  40  41  42  43  44  45  46  47  48  49  50  51  52  53  58  82  83  84
  85  86  87  88  89  90  91  92 103 148]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 1142.4403 - loglik: -1.1314e+03 - logprior: -1.0995e+01
Epoch 2/2
11/11 - 2s - loss: 1107.4635 - loglik: -1.1046e+03 - logprior: -2.9045e+00
Fitted a model with MAP estimate = -1103.8894
expansions: [(0, 112), (113, 73)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 8s - loss: 1059.1699 - loglik: -1.0529e+03 - logprior: -6.2416e+00
Epoch 2/2
23/23 - 5s - loss: 985.7180 - loglik: -9.8562e+02 - logprior: -1.0146e-01
Fitted a model with MAP estimate = -978.0511
expansions: [(0, 22), (12, 2), (38, 1), (66, 1), (95, 1)]
discards: [130 160 161 162]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 984.0981 - loglik: -9.7643e+02 - logprior: -7.6727e+00
Epoch 2/10
23/23 - 6s - loss: 972.3522 - loglik: -9.7197e+02 - logprior: -3.8675e-01
Epoch 3/10
23/23 - 6s - loss: 968.7476 - loglik: -9.6890e+02 - logprior: 0.1566
Epoch 4/10
23/23 - 6s - loss: 963.7720 - loglik: -9.6414e+02 - logprior: 0.3665
Epoch 5/10
23/23 - 6s - loss: 960.4501 - loglik: -9.6107e+02 - logprior: 0.6254
Epoch 6/10
23/23 - 6s - loss: 948.9586 - loglik: -9.4971e+02 - logprior: 0.7573
Epoch 7/10
23/23 - 6s - loss: 908.7238 - loglik: -9.0924e+02 - logprior: 0.5295
Epoch 8/10
23/23 - 6s - loss: 842.4653 - loglik: -8.4241e+02 - logprior: -4.0797e-02
Epoch 9/10
23/23 - 6s - loss: 790.4835 - loglik: -7.8990e+02 - logprior: -5.6952e-01
Epoch 10/10
23/23 - 6s - loss: 762.0878 - loglik: -7.6117e+02 - logprior: -9.0130e-01
Fitted a model with MAP estimate = -749.7655
Time for alignment: 150.4328
Fitting a model of length 159 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 1073.2856 - loglik: -1.0671e+03 - logprior: -6.2214e+00
Epoch 2/10
23/23 - 4s - loss: 1001.7654 - loglik: -1.0011e+03 - logprior: -6.1774e-01
Epoch 3/10
23/23 - 4s - loss: 987.8777 - loglik: -9.8758e+02 - logprior: -3.0190e-01
Epoch 4/10
23/23 - 4s - loss: 978.9475 - loglik: -9.7876e+02 - logprior: -1.8852e-01
Epoch 5/10
23/23 - 4s - loss: 970.0977 - loglik: -9.6993e+02 - logprior: -1.6203e-01
Epoch 6/10
23/23 - 4s - loss: 956.0120 - loglik: -9.5575e+02 - logprior: -2.5319e-01
Epoch 7/10
23/23 - 4s - loss: 918.6743 - loglik: -9.1806e+02 - logprior: -5.9937e-01
Epoch 8/10
23/23 - 4s - loss: 830.3282 - loglik: -8.2627e+02 - logprior: -4.0465e+00
Epoch 9/10
23/23 - 4s - loss: 775.1443 - loglik: -7.7062e+02 - logprior: -4.5020e+00
Epoch 10/10
23/23 - 4s - loss: 759.2328 - loglik: -7.5463e+02 - logprior: -4.5854e+00
Fitted a model with MAP estimate = -755.2718
expansions: [(0, 1)]
discards: [ 16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33
  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  58
  59  60  61  62  63  64 104 105 106 107 108 109 110 111 112 113]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3509 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 1140.0148 - loglik: -1.1244e+03 - logprior: -1.5634e+01
Epoch 2/2
11/11 - 2s - loss: 1107.7035 - loglik: -1.1036e+03 - logprior: -4.0695e+00
Fitted a model with MAP estimate = -1103.1676
expansions: [(0, 161)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 162 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
23/23 - 8s - loss: 1075.8810 - loglik: -1.0679e+03 - logprior: -7.9432e+00
Epoch 2/2
23/23 - 4s - loss: 1001.6882 - loglik: -1.0008e+03 - logprior: -8.7278e-01
Fitted a model with MAP estimate = -987.9885
expansions: [(0, 25), (10, 2), (17, 1), (32, 1), (44, 1), (59, 1), (61, 1), (68, 1), (73, 1), (74, 1), (82, 1), (86, 1), (89, 1), (110, 1), (115, 1), (122, 2), (123, 2), (130, 1), (153, 3), (162, 8)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 218 on 3509 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 9s - loss: 988.7888 - loglik: -9.8001e+02 - logprior: -8.7789e+00
Epoch 2/10
23/23 - 6s - loss: 970.7114 - loglik: -9.6962e+02 - logprior: -1.0929e+00
Epoch 3/10
23/23 - 6s - loss: 967.8748 - loglik: -9.6785e+02 - logprior: -2.3628e-02
Epoch 4/10
23/23 - 6s - loss: 965.2708 - loglik: -9.6545e+02 - logprior: 0.1822
Epoch 5/10
23/23 - 6s - loss: 960.1808 - loglik: -9.6036e+02 - logprior: 0.1823
Epoch 6/10
23/23 - 6s - loss: 945.5170 - loglik: -9.4551e+02 - logprior: 0.0025
Epoch 7/10
23/23 - 6s - loss: 906.5013 - loglik: -9.0657e+02 - logprior: 0.0755
Epoch 8/10
23/23 - 6s - loss: 841.5247 - loglik: -8.4099e+02 - logprior: -5.2277e-01
Epoch 9/10
23/23 - 6s - loss: 796.2693 - loglik: -7.9548e+02 - logprior: -7.7086e-01
Epoch 10/10
23/23 - 6s - loss: 762.7087 - loglik: -7.6169e+02 - logprior: -1.0038e+00
Fitted a model with MAP estimate = -749.5197
Time for alignment: 149.9034
Computed alignments with likelihoods: ['-748.6352', '-748.7121', '-750.6639', '-749.7655', '-749.5197']
Best model has likelihood: -748.6352  (prior= -0.9482 )
time for generating output: 0.2231
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hormone_rec.projection.fasta
SP score = 0.014618547281863865
Training of 5 independent models on file Acetyltransf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9e7456940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9e75226d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9525c0c40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 509.1578 - loglik: -5.0801e+02 - logprior: -1.1505e+00
Epoch 2/10
29/29 - 3s - loss: 482.3104 - loglik: -4.8149e+02 - logprior: -8.1576e-01
Epoch 3/10
29/29 - 3s - loss: 478.5982 - loglik: -4.7782e+02 - logprior: -7.7470e-01
Epoch 4/10
29/29 - 3s - loss: 478.7038 - loglik: -4.7793e+02 - logprior: -7.6539e-01
Fitted a model with MAP estimate = -439.0051
expansions: [(2, 2), (12, 2), (13, 2), (14, 2), (21, 1), (38, 1), (41, 1), (43, 1), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 477.1028 - loglik: -4.7599e+02 - logprior: -1.1125e+00
Epoch 2/2
29/29 - 3s - loss: 474.7431 - loglik: -4.7403e+02 - logprior: -7.0991e-01
Fitted a model with MAP estimate = -435.8165
expansions: []
discards: [ 0 57 65]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 477.6790 - loglik: -4.7625e+02 - logprior: -1.4249e+00
Epoch 2/2
29/29 - 3s - loss: 476.0524 - loglik: -4.7528e+02 - logprior: -7.7034e-01
Fitted a model with MAP estimate = -437.0353
expansions: [(2, 1), (20, 1)]
discards: [ 0 16]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 436.3002 - loglik: -4.3553e+02 - logprior: -7.7213e-01
Epoch 2/10
42/42 - 4s - loss: 436.2735 - loglik: -4.3581e+02 - logprior: -4.6048e-01
Epoch 3/10
42/42 - 4s - loss: 435.3805 - loglik: -4.3490e+02 - logprior: -4.7743e-01
Epoch 4/10
42/42 - 4s - loss: 434.3182 - loglik: -4.3379e+02 - logprior: -5.1908e-01
Epoch 5/10
42/42 - 4s - loss: 432.7292 - loglik: -4.3214e+02 - logprior: -5.7795e-01
Epoch 6/10
42/42 - 4s - loss: 433.2507 - loglik: -4.3266e+02 - logprior: -5.6975e-01
Fitted a model with MAP estimate = -430.8980
Time for alignment: 93.8334
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 508.5085 - loglik: -5.0736e+02 - logprior: -1.1514e+00
Epoch 2/10
29/29 - 3s - loss: 484.0358 - loglik: -4.8323e+02 - logprior: -8.0824e-01
Epoch 3/10
29/29 - 3s - loss: 479.3975 - loglik: -4.7861e+02 - logprior: -7.8206e-01
Epoch 4/10
29/29 - 3s - loss: 478.2457 - loglik: -4.7747e+02 - logprior: -7.7636e-01
Epoch 5/10
29/29 - 3s - loss: 477.4620 - loglik: -4.7667e+02 - logprior: -7.8241e-01
Epoch 6/10
29/29 - 3s - loss: 474.8195 - loglik: -4.7404e+02 - logprior: -7.6255e-01
Epoch 7/10
29/29 - 3s - loss: 474.2984 - loglik: -4.7352e+02 - logprior: -7.6140e-01
Epoch 8/10
29/29 - 3s - loss: 473.3674 - loglik: -4.7259e+02 - logprior: -7.5393e-01
Epoch 9/10
29/29 - 3s - loss: 472.1513 - loglik: -4.7137e+02 - logprior: -7.5121e-01
Epoch 10/10
29/29 - 3s - loss: 470.3466 - loglik: -4.6955e+02 - logprior: -7.5277e-01
Fitted a model with MAP estimate = -442.3821
expansions: [(1, 1), (2, 1), (14, 2), (16, 2), (27, 2), (38, 1), (41, 1), (43, 1), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 7s - loss: 485.4799 - loglik: -4.8435e+02 - logprior: -1.1309e+00
Epoch 2/2
29/29 - 3s - loss: 476.0262 - loglik: -4.7529e+02 - logprior: -7.3876e-01
Fitted a model with MAP estimate = -436.1859
expansions: [(17, 2), (20, 1)]
discards: [34 56 65]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 476.2514 - loglik: -4.7517e+02 - logprior: -1.0861e+00
Epoch 2/2
29/29 - 3s - loss: 474.4075 - loglik: -4.7373e+02 - logprior: -6.8049e-01
Fitted a model with MAP estimate = -436.0170
expansions: [(22, 1)]
discards: [ 1 20]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 436.8856 - loglik: -4.3628e+02 - logprior: -6.0778e-01
Epoch 2/10
42/42 - 4s - loss: 434.5636 - loglik: -4.3407e+02 - logprior: -4.8970e-01
Epoch 3/10
42/42 - 4s - loss: 434.4651 - loglik: -4.3399e+02 - logprior: -4.7366e-01
Epoch 4/10
42/42 - 4s - loss: 434.4835 - loglik: -4.3401e+02 - logprior: -4.6704e-01
Fitted a model with MAP estimate = -433.3567
Time for alignment: 102.2909
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 508.8871 - loglik: -5.0773e+02 - logprior: -1.1529e+00
Epoch 2/10
29/29 - 3s - loss: 483.8265 - loglik: -4.8300e+02 - logprior: -8.2225e-01
Epoch 3/10
29/29 - 2s - loss: 479.5630 - loglik: -4.7877e+02 - logprior: -7.8860e-01
Epoch 4/10
29/29 - 3s - loss: 478.2517 - loglik: -4.7746e+02 - logprior: -7.8359e-01
Epoch 5/10
29/29 - 3s - loss: 477.4865 - loglik: -4.7671e+02 - logprior: -7.7006e-01
Epoch 6/10
29/29 - 3s - loss: 475.6950 - loglik: -4.7492e+02 - logprior: -7.6528e-01
Epoch 7/10
29/29 - 3s - loss: 473.8343 - loglik: -4.7304e+02 - logprior: -7.7468e-01
Epoch 8/10
29/29 - 3s - loss: 473.8333 - loglik: -4.7304e+02 - logprior: -7.6537e-01
Epoch 9/10
29/29 - 3s - loss: 472.8773 - loglik: -4.7208e+02 - logprior: -7.5996e-01
Epoch 10/10
29/29 - 3s - loss: 470.5965 - loglik: -4.6980e+02 - logprior: -7.5713e-01
Fitted a model with MAP estimate = -441.0867
expansions: [(1, 1), (2, 1), (14, 2), (15, 2), (17, 1), (22, 1), (27, 2), (38, 2), (41, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 484.9679 - loglik: -4.8383e+02 - logprior: -1.1354e+00
Epoch 2/2
29/29 - 3s - loss: 475.4944 - loglik: -4.7473e+02 - logprior: -7.6499e-01
Fitted a model with MAP estimate = -435.9916
expansions: []
discards: [21 36 49 59 63 69]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 5s - loss: 476.1742 - loglik: -4.7509e+02 - logprior: -1.0851e+00
Epoch 2/2
29/29 - 3s - loss: 474.5361 - loglik: -4.7385e+02 - logprior: -6.8768e-01
Fitted a model with MAP estimate = -435.9568
expansions: [(17, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 435.9569 - loglik: -4.3525e+02 - logprior: -7.0945e-01
Epoch 2/10
42/42 - 4s - loss: 434.9684 - loglik: -4.3441e+02 - logprior: -5.5506e-01
Epoch 3/10
42/42 - 4s - loss: 435.2801 - loglik: -4.3473e+02 - logprior: -5.5152e-01
Fitted a model with MAP estimate = -433.9893
Time for alignment: 97.8580
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 508.6914 - loglik: -5.0754e+02 - logprior: -1.1526e+00
Epoch 2/10
29/29 - 2s - loss: 483.5061 - loglik: -4.8269e+02 - logprior: -8.1920e-01
Epoch 3/10
29/29 - 3s - loss: 478.9638 - loglik: -4.7817e+02 - logprior: -7.9495e-01
Epoch 4/10
29/29 - 3s - loss: 478.0184 - loglik: -4.7721e+02 - logprior: -8.0123e-01
Epoch 5/10
29/29 - 2s - loss: 477.6273 - loglik: -4.7683e+02 - logprior: -7.8646e-01
Epoch 6/10
29/29 - 3s - loss: 475.0761 - loglik: -4.7428e+02 - logprior: -7.7877e-01
Epoch 7/10
29/29 - 3s - loss: 474.0076 - loglik: -4.7322e+02 - logprior: -7.6943e-01
Epoch 8/10
29/29 - 2s - loss: 473.9244 - loglik: -4.7313e+02 - logprior: -7.6542e-01
Epoch 9/10
29/29 - 3s - loss: 471.9448 - loglik: -4.7115e+02 - logprior: -7.6162e-01
Epoch 10/10
29/29 - 3s - loss: 470.5423 - loglik: -4.6974e+02 - logprior: -7.5859e-01
Fitted a model with MAP estimate = -442.3516
expansions: [(5, 1), (13, 1), (14, 2), (16, 2), (22, 1), (39, 2), (41, 1), (42, 1), (45, 2), (46, 2), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 485.7988 - loglik: -4.8463e+02 - logprior: -1.1698e+00
Epoch 2/2
29/29 - 3s - loss: 474.9379 - loglik: -4.7417e+02 - logprior: -7.7219e-01
Fitted a model with MAP estimate = -436.2453
expansions: [(3, 1), (21, 2)]
discards: [18 19 22 47 57 61 67]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 5s - loss: 476.0800 - loglik: -4.7500e+02 - logprior: -1.0816e+00
Epoch 2/2
29/29 - 3s - loss: 474.4587 - loglik: -4.7378e+02 - logprior: -6.7865e-01
Fitted a model with MAP estimate = -435.9441
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 437.4749 - loglik: -4.3662e+02 - logprior: -8.5088e-01
Epoch 2/10
42/42 - 4s - loss: 435.2562 - loglik: -4.3465e+02 - logprior: -6.0435e-01
Epoch 3/10
42/42 - 4s - loss: 434.8691 - loglik: -4.3433e+02 - logprior: -5.3276e-01
Epoch 4/10
42/42 - 4s - loss: 434.6275 - loglik: -4.3410e+02 - logprior: -5.2530e-01
Epoch 5/10
42/42 - 4s - loss: 434.0194 - loglik: -4.3347e+02 - logprior: -5.3430e-01
Epoch 6/10
42/42 - 4s - loss: 432.4472 - loglik: -4.3188e+02 - logprior: -5.4884e-01
Epoch 7/10
42/42 - 4s - loss: 431.3880 - loglik: -4.3081e+02 - logprior: -5.4696e-01
Epoch 8/10
42/42 - 4s - loss: 430.1769 - loglik: -4.2959e+02 - logprior: -5.4213e-01
Epoch 9/10
42/42 - 4s - loss: 430.1131 - loglik: -4.2952e+02 - logprior: -5.3851e-01
Epoch 10/10
42/42 - 4s - loss: 427.6638 - loglik: -4.2706e+02 - logprior: -5.3380e-01
Fitted a model with MAP estimate = -425.3203
Time for alignment: 123.4418
Fitting a model of length 64 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 5s - loss: 508.0165 - loglik: -5.0687e+02 - logprior: -1.1488e+00
Epoch 2/10
29/29 - 3s - loss: 482.6673 - loglik: -4.8187e+02 - logprior: -8.0051e-01
Epoch 3/10
29/29 - 3s - loss: 479.0247 - loglik: -4.7824e+02 - logprior: -7.7981e-01
Epoch 4/10
29/29 - 3s - loss: 477.3219 - loglik: -4.7653e+02 - logprior: -7.9243e-01
Epoch 5/10
29/29 - 3s - loss: 476.6930 - loglik: -4.7590e+02 - logprior: -7.8381e-01
Epoch 6/10
29/29 - 2s - loss: 474.7558 - loglik: -4.7397e+02 - logprior: -7.7201e-01
Epoch 7/10
29/29 - 3s - loss: 474.3044 - loglik: -4.7352e+02 - logprior: -7.6707e-01
Epoch 8/10
29/29 - 3s - loss: 473.0772 - loglik: -4.7229e+02 - logprior: -7.6285e-01
Epoch 9/10
29/29 - 2s - loss: 472.2776 - loglik: -4.7148e+02 - logprior: -7.6222e-01
Epoch 10/10
29/29 - 3s - loss: 469.7249 - loglik: -4.6892e+02 - logprior: -7.6032e-01
Fitted a model with MAP estimate = -441.7005
expansions: [(1, 1), (2, 1), (12, 2), (13, 2), (14, 2), (16, 2), (17, 2), (21, 1), (38, 1), (41, 1), (43, 1), (44, 2), (46, 1), (48, 1), (49, 2), (50, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 89 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 485.5589 - loglik: -4.8444e+02 - logprior: -1.1165e+00
Epoch 2/2
29/29 - 3s - loss: 474.6735 - loglik: -4.7395e+02 - logprior: -7.2102e-01
Fitted a model with MAP estimate = -436.0077
expansions: []
discards: [16 20 21 22 23 61 70]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 23143 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 6s - loss: 475.8826 - loglik: -4.7480e+02 - logprior: -1.0848e+00
Epoch 2/2
29/29 - 3s - loss: 475.2783 - loglik: -4.7459e+02 - logprior: -6.8642e-01
Fitted a model with MAP estimate = -436.0518
expansions: [(16, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 46285 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 7s - loss: 435.8900 - loglik: -4.3518e+02 - logprior: -7.1134e-01
Epoch 2/10
42/42 - 4s - loss: 435.5030 - loglik: -4.3495e+02 - logprior: -5.5096e-01
Epoch 3/10
42/42 - 4s - loss: 434.6979 - loglik: -4.3415e+02 - logprior: -5.4475e-01
Epoch 4/10
42/42 - 4s - loss: 434.4377 - loglik: -4.3390e+02 - logprior: -5.3528e-01
Epoch 5/10
42/42 - 4s - loss: 433.4635 - loglik: -4.3292e+02 - logprior: -5.2937e-01
Epoch 6/10
42/42 - 4s - loss: 431.5854 - loglik: -4.3104e+02 - logprior: -5.2575e-01
Epoch 7/10
42/42 - 4s - loss: 430.8726 - loglik: -4.3032e+02 - logprior: -5.1541e-01
Epoch 8/10
42/42 - 4s - loss: 430.2437 - loglik: -4.2968e+02 - logprior: -5.2169e-01
Epoch 9/10
42/42 - 4s - loss: 428.6285 - loglik: -4.2805e+02 - logprior: -5.2301e-01
Epoch 10/10
42/42 - 4s - loss: 426.9189 - loglik: -4.2633e+02 - logprior: -5.2148e-01
Fitted a model with MAP estimate = -424.5403
Time for alignment: 123.8362
Computed alignments with likelihoods: ['-430.8980', '-433.3567', '-433.9893', '-425.3203', '-424.5403']
Best model has likelihood: -424.5403  (prior= -0.5416 )
time for generating output: 0.1815
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Acetyltransf.projection.fasta
SP score = 0.49085516559565
Training of 5 independent models on file phoslip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea1189d4f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea11fa5970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea66a68ee0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 714.5262 - loglik: -6.6926e+02 - logprior: -4.5266e+01
Epoch 2/10
10/10 - 1s - loss: 626.5304 - loglik: -6.1568e+02 - logprior: -1.0847e+01
Epoch 3/10
10/10 - 1s - loss: 579.3655 - loglik: -5.7455e+02 - logprior: -4.8186e+00
Epoch 4/10
10/10 - 1s - loss: 553.4808 - loglik: -5.5081e+02 - logprior: -2.6666e+00
Epoch 5/10
10/10 - 1s - loss: 543.5482 - loglik: -5.4198e+02 - logprior: -1.5671e+00
Epoch 6/10
10/10 - 1s - loss: 538.5780 - loglik: -5.3755e+02 - logprior: -1.0270e+00
Epoch 7/10
10/10 - 1s - loss: 536.6904 - loglik: -5.3594e+02 - logprior: -7.5031e-01
Epoch 8/10
10/10 - 1s - loss: 535.0746 - loglik: -5.3447e+02 - logprior: -6.0537e-01
Epoch 9/10
10/10 - 1s - loss: 531.4698 - loglik: -5.3091e+02 - logprior: -5.5972e-01
Epoch 10/10
10/10 - 1s - loss: 527.2815 - loglik: -5.2670e+02 - logprior: -5.7984e-01
Fitted a model with MAP estimate = -526.7661
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (66, 1), (67, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 592.7694 - loglik: -5.4153e+02 - logprior: -5.1240e+01
Epoch 2/2
10/10 - 1s - loss: 538.1095 - loglik: -5.1807e+02 - logprior: -2.0036e+01
Fitted a model with MAP estimate = -529.5574
expansions: [(0, 3)]
discards: [ 0  9 19]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 565.7008 - loglik: -5.2535e+02 - logprior: -4.0347e+01
Epoch 2/2
10/10 - 1s - loss: 520.4807 - loglik: -5.1115e+02 - logprior: -9.3270e+00
Fitted a model with MAP estimate = -514.2762
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 573.7443 - loglik: -5.2424e+02 - logprior: -4.9501e+01
Epoch 2/10
10/10 - 1s - loss: 527.2407 - loglik: -5.1168e+02 - logprior: -1.5559e+01
Epoch 3/10
10/10 - 1s - loss: 515.3423 - loglik: -5.1078e+02 - logprior: -4.5599e+00
Epoch 4/10
10/10 - 1s - loss: 508.9090 - loglik: -5.0828e+02 - logprior: -6.3251e-01
Epoch 5/10
10/10 - 1s - loss: 506.3723 - loglik: -5.0724e+02 - logprior: 0.8644
Epoch 6/10
10/10 - 1s - loss: 503.7264 - loglik: -5.0560e+02 - logprior: 1.8774
Epoch 7/10
10/10 - 1s - loss: 502.2779 - loglik: -5.0481e+02 - logprior: 2.5299
Epoch 8/10
10/10 - 1s - loss: 500.5357 - loglik: -5.0339e+02 - logprior: 2.8575
Epoch 9/10
10/10 - 1s - loss: 500.6990 - loglik: -5.0382e+02 - logprior: 3.1267
Fitted a model with MAP estimate = -500.0144
Time for alignment: 41.6204
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 715.3358 - loglik: -6.7007e+02 - logprior: -4.5265e+01
Epoch 2/10
10/10 - 1s - loss: 625.4993 - loglik: -6.1466e+02 - logprior: -1.0841e+01
Epoch 3/10
10/10 - 1s - loss: 581.1534 - loglik: -5.7633e+02 - logprior: -4.8258e+00
Epoch 4/10
10/10 - 1s - loss: 551.0663 - loglik: -5.4827e+02 - logprior: -2.7967e+00
Epoch 5/10
10/10 - 1s - loss: 537.8905 - loglik: -5.3600e+02 - logprior: -1.8875e+00
Epoch 6/10
10/10 - 1s - loss: 531.8876 - loglik: -5.3041e+02 - logprior: -1.4728e+00
Epoch 7/10
10/10 - 1s - loss: 529.3675 - loglik: -5.2813e+02 - logprior: -1.2330e+00
Epoch 8/10
10/10 - 1s - loss: 527.4890 - loglik: -5.2638e+02 - logprior: -1.1045e+00
Epoch 9/10
10/10 - 1s - loss: 526.7491 - loglik: -5.2575e+02 - logprior: -9.9484e-01
Epoch 10/10
10/10 - 1s - loss: 526.5373 - loglik: -5.2564e+02 - logprior: -8.9217e-01
Fitted a model with MAP estimate = -526.2885
expansions: [(10, 2), (11, 2), (12, 2), (13, 1), (14, 2), (15, 1), (29, 1), (44, 2), (45, 2), (51, 2), (52, 1), (68, 4), (79, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 591.5177 - loglik: -5.4031e+02 - logprior: -5.1211e+01
Epoch 2/2
10/10 - 1s - loss: 536.4061 - loglik: -5.1630e+02 - logprior: -2.0108e+01
Fitted a model with MAP estimate = -528.1988
expansions: [(0, 3)]
discards: [ 0  9 20]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 563.8167 - loglik: -5.2347e+02 - logprior: -4.0350e+01
Epoch 2/2
10/10 - 1s - loss: 519.4074 - loglik: -5.1011e+02 - logprior: -9.2968e+00
Fitted a model with MAP estimate = -513.0106
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 571.6704 - loglik: -5.2232e+02 - logprior: -4.9353e+01
Epoch 2/10
10/10 - 1s - loss: 526.3964 - loglik: -5.1124e+02 - logprior: -1.5155e+01
Epoch 3/10
10/10 - 1s - loss: 513.4475 - loglik: -5.0914e+02 - logprior: -4.3105e+00
Epoch 4/10
10/10 - 1s - loss: 508.5319 - loglik: -5.0798e+02 - logprior: -5.4971e-01
Epoch 5/10
10/10 - 1s - loss: 505.6979 - loglik: -5.0661e+02 - logprior: 0.9153
Epoch 6/10
10/10 - 1s - loss: 503.0490 - loglik: -5.0497e+02 - logprior: 1.9217
Epoch 7/10
10/10 - 1s - loss: 501.6664 - loglik: -5.0424e+02 - logprior: 2.5753
Epoch 8/10
10/10 - 1s - loss: 500.9156 - loglik: -5.0382e+02 - logprior: 2.9076
Epoch 9/10
10/10 - 1s - loss: 499.8929 - loglik: -5.0308e+02 - logprior: 3.1852
Epoch 10/10
10/10 - 1s - loss: 499.4852 - loglik: -5.0296e+02 - logprior: 3.4733
Fitted a model with MAP estimate = -499.3973
Time for alignment: 42.9182
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 715.2794 - loglik: -6.7001e+02 - logprior: -4.5267e+01
Epoch 2/10
10/10 - 1s - loss: 626.1791 - loglik: -6.1534e+02 - logprior: -1.0840e+01
Epoch 3/10
10/10 - 1s - loss: 578.2444 - loglik: -5.7341e+02 - logprior: -4.8321e+00
Epoch 4/10
10/10 - 1s - loss: 554.1453 - loglik: -5.5149e+02 - logprior: -2.6551e+00
Epoch 5/10
10/10 - 1s - loss: 544.3480 - loglik: -5.4280e+02 - logprior: -1.5503e+00
Epoch 6/10
10/10 - 1s - loss: 540.0511 - loglik: -5.3906e+02 - logprior: -9.9317e-01
Epoch 7/10
10/10 - 1s - loss: 536.8038 - loglik: -5.3609e+02 - logprior: -7.0779e-01
Epoch 8/10
10/10 - 1s - loss: 535.0164 - loglik: -5.3445e+02 - logprior: -5.6443e-01
Epoch 9/10
10/10 - 1s - loss: 533.2867 - loglik: -5.3277e+02 - logprior: -5.1430e-01
Epoch 10/10
10/10 - 1s - loss: 528.4229 - loglik: -5.2787e+02 - logprior: -5.5261e-01
Fitted a model with MAP estimate = -527.0041
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (63, 1), (67, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 592.7256 - loglik: -5.4148e+02 - logprior: -5.1246e+01
Epoch 2/2
10/10 - 1s - loss: 538.7770 - loglik: -5.1872e+02 - logprior: -2.0058e+01
Fitted a model with MAP estimate = -529.6355
expansions: [(0, 3)]
discards: [ 0  9 19]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 565.8451 - loglik: -5.2552e+02 - logprior: -4.0330e+01
Epoch 2/2
10/10 - 1s - loss: 520.4545 - loglik: -5.1115e+02 - logprior: -9.3040e+00
Fitted a model with MAP estimate = -514.0797
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 572.8834 - loglik: -5.2336e+02 - logprior: -4.9522e+01
Epoch 2/10
10/10 - 1s - loss: 528.4772 - loglik: -5.1280e+02 - logprior: -1.5681e+01
Epoch 3/10
10/10 - 1s - loss: 514.8800 - loglik: -5.1023e+02 - logprior: -4.6508e+00
Epoch 4/10
10/10 - 1s - loss: 509.7752 - loglik: -5.0914e+02 - logprior: -6.3441e-01
Epoch 5/10
10/10 - 1s - loss: 506.3146 - loglik: -5.0719e+02 - logprior: 0.8797
Epoch 6/10
10/10 - 1s - loss: 503.4934 - loglik: -5.0538e+02 - logprior: 1.8846
Epoch 7/10
10/10 - 1s - loss: 502.2892 - loglik: -5.0481e+02 - logprior: 2.5235
Epoch 8/10
10/10 - 1s - loss: 501.0865 - loglik: -5.0395e+02 - logprior: 2.8627
Epoch 9/10
10/10 - 1s - loss: 500.8968 - loglik: -5.0402e+02 - logprior: 3.1292
Epoch 10/10
10/10 - 1s - loss: 499.6820 - loglik: -5.0309e+02 - logprior: 3.4139
Fitted a model with MAP estimate = -499.7678
Time for alignment: 43.2654
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 715.4026 - loglik: -6.7014e+02 - logprior: -4.5263e+01
Epoch 2/10
10/10 - 1s - loss: 624.5724 - loglik: -6.1374e+02 - logprior: -1.0829e+01
Epoch 3/10
10/10 - 1s - loss: 576.6600 - loglik: -5.7190e+02 - logprior: -4.7583e+00
Epoch 4/10
10/10 - 1s - loss: 551.1792 - loglik: -5.4849e+02 - logprior: -2.6875e+00
Epoch 5/10
10/10 - 1s - loss: 536.7355 - loglik: -5.3490e+02 - logprior: -1.8388e+00
Epoch 6/10
10/10 - 1s - loss: 530.5475 - loglik: -5.2920e+02 - logprior: -1.3431e+00
Epoch 7/10
10/10 - 1s - loss: 527.7396 - loglik: -5.2668e+02 - logprior: -1.0588e+00
Epoch 8/10
10/10 - 1s - loss: 526.5594 - loglik: -5.2562e+02 - logprior: -9.4093e-01
Epoch 9/10
10/10 - 1s - loss: 525.2211 - loglik: -5.2434e+02 - logprior: -8.8052e-01
Epoch 10/10
10/10 - 1s - loss: 524.5016 - loglik: -5.2369e+02 - logprior: -8.1137e-01
Fitted a model with MAP estimate = -524.6432
expansions: [(10, 2), (11, 3), (12, 3), (13, 2), (14, 1), (44, 2), (45, 2), (51, 2), (52, 1), (58, 2), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 591.0810 - loglik: -5.3989e+02 - logprior: -5.1195e+01
Epoch 2/2
10/10 - 1s - loss: 537.5673 - loglik: -5.1740e+02 - logprior: -2.0170e+01
Fitted a model with MAP estimate = -528.6968
expansions: [(0, 3)]
discards: [  0   9  75 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 566.9802 - loglik: -5.2656e+02 - logprior: -4.0420e+01
Epoch 2/2
10/10 - 1s - loss: 520.9922 - loglik: -5.1159e+02 - logprior: -9.4027e+00
Fitted a model with MAP estimate = -514.4524
expansions: []
discards: [ 0  2 15]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 573.7894 - loglik: -5.2438e+02 - logprior: -4.9410e+01
Epoch 2/10
10/10 - 1s - loss: 526.9579 - loglik: -5.1180e+02 - logprior: -1.5153e+01
Epoch 3/10
10/10 - 1s - loss: 514.9344 - loglik: -5.1054e+02 - logprior: -4.3916e+00
Epoch 4/10
10/10 - 1s - loss: 508.9465 - loglik: -5.0828e+02 - logprior: -6.6529e-01
Epoch 5/10
10/10 - 1s - loss: 506.8148 - loglik: -5.0762e+02 - logprior: 0.8087
Epoch 6/10
10/10 - 1s - loss: 503.9586 - loglik: -5.0577e+02 - logprior: 1.8088
Epoch 7/10
10/10 - 1s - loss: 502.6011 - loglik: -5.0506e+02 - logprior: 2.4591
Epoch 8/10
10/10 - 1s - loss: 501.9565 - loglik: -5.0475e+02 - logprior: 2.7988
Epoch 9/10
10/10 - 1s - loss: 500.8597 - loglik: -5.0394e+02 - logprior: 3.0828
Epoch 10/10
10/10 - 1s - loss: 500.8831 - loglik: -5.0425e+02 - logprior: 3.3663
Fitted a model with MAP estimate = -500.3537
Time for alignment: 43.5734
Fitting a model of length 96 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 714.3839 - loglik: -6.6912e+02 - logprior: -4.5264e+01
Epoch 2/10
10/10 - 1s - loss: 625.6967 - loglik: -6.1486e+02 - logprior: -1.0833e+01
Epoch 3/10
10/10 - 1s - loss: 577.4656 - loglik: -5.7266e+02 - logprior: -4.8091e+00
Epoch 4/10
10/10 - 1s - loss: 551.0906 - loglik: -5.4832e+02 - logprior: -2.7741e+00
Epoch 5/10
10/10 - 1s - loss: 538.2036 - loglik: -5.3632e+02 - logprior: -1.8814e+00
Epoch 6/10
10/10 - 1s - loss: 529.6974 - loglik: -5.2824e+02 - logprior: -1.4570e+00
Epoch 7/10
10/10 - 1s - loss: 526.7001 - loglik: -5.2549e+02 - logprior: -1.2090e+00
Epoch 8/10
10/10 - 1s - loss: 524.6122 - loglik: -5.2347e+02 - logprior: -1.1368e+00
Epoch 9/10
10/10 - 1s - loss: 524.8630 - loglik: -5.2370e+02 - logprior: -1.1635e+00
Fitted a model with MAP estimate = -523.5697
expansions: [(10, 2), (11, 3), (12, 2), (13, 3), (14, 1), (44, 2), (45, 2), (50, 1), (51, 1), (56, 1), (58, 1), (79, 1), (81, 2), (82, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 589.2046 - loglik: -5.3808e+02 - logprior: -5.1129e+01
Epoch 2/2
10/10 - 1s - loss: 534.5862 - loglik: -5.1463e+02 - logprior: -1.9953e+01
Fitted a model with MAP estimate = -526.3423
expansions: [(0, 3)]
discards: [  0   9  12 102 103]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 565.3682 - loglik: -5.2499e+02 - logprior: -4.0379e+01
Epoch 2/2
10/10 - 1s - loss: 521.4802 - loglik: -5.1202e+02 - logprior: -9.4639e+00
Fitted a model with MAP estimate = -515.0023
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 946 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 574.1895 - loglik: -5.2446e+02 - logprior: -4.9727e+01
Epoch 2/10
10/10 - 1s - loss: 528.0893 - loglik: -5.1201e+02 - logprior: -1.6084e+01
Epoch 3/10
10/10 - 1s - loss: 515.5452 - loglik: -5.1057e+02 - logprior: -4.9734e+00
Epoch 4/10
10/10 - 1s - loss: 509.3128 - loglik: -5.0856e+02 - logprior: -7.5210e-01
Epoch 5/10
10/10 - 1s - loss: 506.7621 - loglik: -5.0754e+02 - logprior: 0.7831
Epoch 6/10
10/10 - 1s - loss: 504.1032 - loglik: -5.0589e+02 - logprior: 1.7922
Epoch 7/10
10/10 - 1s - loss: 503.0571 - loglik: -5.0550e+02 - logprior: 2.4411
Epoch 8/10
10/10 - 1s - loss: 501.3361 - loglik: -5.0411e+02 - logprior: 2.7799
Epoch 9/10
10/10 - 1s - loss: 501.6710 - loglik: -5.0473e+02 - logprior: 3.0591
Fitted a model with MAP estimate = -500.8060
Time for alignment: 39.2046
Computed alignments with likelihoods: ['-500.0144', '-499.3973', '-499.7678', '-500.3537', '-500.8060']
Best model has likelihood: -499.3973  (prior= 3.6091 )
time for generating output: 0.1599
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phoslip.projection.fasta
SP score = 0.9115763683946768
Training of 5 independent models on file cah.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe952c7ec10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9bc239b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea66f35bb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 1252.5212 - loglik: -1.2301e+03 - logprior: -2.2456e+01
Epoch 2/10
14/14 - 3s - loss: 1158.5619 - loglik: -1.1566e+03 - logprior: -2.0083e+00
Epoch 3/10
14/14 - 3s - loss: 1103.9384 - loglik: -1.1034e+03 - logprior: -5.7423e-01
Epoch 4/10
14/14 - 3s - loss: 1080.5134 - loglik: -1.0803e+03 - logprior: -1.9285e-01
Epoch 5/10
14/14 - 3s - loss: 1078.1119 - loglik: -1.0781e+03 - logprior: 0.0337
Epoch 6/10
14/14 - 3s - loss: 1075.2340 - loglik: -1.0754e+03 - logprior: 0.2043
Epoch 7/10
14/14 - 3s - loss: 1069.8826 - loglik: -1.0702e+03 - logprior: 0.2954
Epoch 8/10
14/14 - 3s - loss: 1070.7679 - loglik: -1.0711e+03 - logprior: 0.3562
Fitted a model with MAP estimate = -1068.7696
expansions: [(15, 1), (17, 1), (28, 1), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 1), (44, 1), (52, 1), (53, 1), (55, 4), (64, 1), (91, 1), (92, 7), (110, 1), (112, 1), (113, 3), (114, 2), (115, 1), (116, 1), (118, 1), (119, 1), (120, 3), (130, 2), (157, 1), (163, 1), (166, 8), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1103.5500 - loglik: -1.0774e+03 - logprior: -2.6190e+01
Epoch 2/2
14/14 - 5s - loss: 1066.8674 - loglik: -1.0594e+03 - logprior: -7.4600e+00
Fitted a model with MAP estimate = -1061.7600
expansions: [(0, 4), (119, 1)]
discards: [  0  48  74 114 115 173]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1078.8298 - loglik: -1.0599e+03 - logprior: -1.8943e+01
Epoch 2/2
14/14 - 5s - loss: 1054.9514 - loglik: -1.0548e+03 - logprior: -1.9077e-01
Fitted a model with MAP estimate = -1052.0917
expansions: []
discards: [  1   2   3  76 115]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 1079.4515 - loglik: -1.0611e+03 - logprior: -1.8392e+01
Epoch 2/10
14/14 - 4s - loss: 1056.3956 - loglik: -1.0566e+03 - logprior: 0.1765
Epoch 3/10
14/14 - 4s - loss: 1051.0151 - loglik: -1.0541e+03 - logprior: 3.0360
Epoch 4/10
14/14 - 4s - loss: 1045.7087 - loglik: -1.0500e+03 - logprior: 4.2778
Epoch 5/10
14/14 - 4s - loss: 1049.6508 - loglik: -1.0548e+03 - logprior: 5.1614
Fitted a model with MAP estimate = -1046.2662
Time for alignment: 93.3061
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 1253.5035 - loglik: -1.2310e+03 - logprior: -2.2461e+01
Epoch 2/10
14/14 - 3s - loss: 1160.0216 - loglik: -1.1581e+03 - logprior: -1.9669e+00
Epoch 3/10
14/14 - 3s - loss: 1099.5037 - loglik: -1.0991e+03 - logprior: -3.9789e-01
Epoch 4/10
14/14 - 3s - loss: 1086.9315 - loglik: -1.0872e+03 - logprior: 0.2497
Epoch 5/10
14/14 - 3s - loss: 1078.4595 - loglik: -1.0789e+03 - logprior: 0.4051
Epoch 6/10
14/14 - 3s - loss: 1075.1322 - loglik: -1.0756e+03 - logprior: 0.5038
Epoch 7/10
14/14 - 3s - loss: 1074.6407 - loglik: -1.0752e+03 - logprior: 0.6031
Epoch 8/10
14/14 - 3s - loss: 1074.6417 - loglik: -1.0753e+03 - logprior: 0.6889
Fitted a model with MAP estimate = -1071.3082
expansions: [(14, 1), (15, 1), (28, 2), (29, 2), (30, 2), (31, 2), (41, 2), (42, 1), (44, 1), (52, 1), (53, 1), (54, 5), (91, 1), (92, 7), (110, 1), (112, 1), (115, 1), (116, 4), (118, 2), (119, 2), (120, 3), (154, 1), (157, 1), (163, 1), (166, 2), (167, 7)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1108.7465 - loglik: -1.0824e+03 - logprior: -2.6305e+01
Epoch 2/2
14/14 - 5s - loss: 1069.8140 - loglik: -1.0621e+03 - logprior: -7.6963e+00
Fitted a model with MAP estimate = -1066.0230
expansions: [(0, 4), (119, 1), (147, 1)]
discards: [  0  30  51 114 115 157 159 212]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 1082.6293 - loglik: -1.0636e+03 - logprior: -1.8983e+01
Epoch 2/2
14/14 - 4s - loss: 1057.0679 - loglik: -1.0568e+03 - logprior: -2.3929e-01
Fitted a model with MAP estimate = -1053.2480
expansions: [(71, 2)]
discards: [  1   2   3  75  76  77  78 115]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 1081.0564 - loglik: -1.0625e+03 - logprior: -1.8566e+01
Epoch 2/10
14/14 - 4s - loss: 1057.2507 - loglik: -1.0572e+03 - logprior: -4.7487e-02
Epoch 3/10
14/14 - 4s - loss: 1050.3121 - loglik: -1.0531e+03 - logprior: 2.7933
Epoch 4/10
14/14 - 4s - loss: 1052.4694 - loglik: -1.0565e+03 - logprior: 4.0427
Fitted a model with MAP estimate = -1049.3356
Time for alignment: 86.8003
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 1250.4313 - loglik: -1.2280e+03 - logprior: -2.2464e+01
Epoch 2/10
14/14 - 3s - loss: 1160.1555 - loglik: -1.1582e+03 - logprior: -1.9936e+00
Epoch 3/10
14/14 - 3s - loss: 1100.1984 - loglik: -1.0996e+03 - logprior: -5.9006e-01
Epoch 4/10
14/14 - 3s - loss: 1083.7844 - loglik: -1.0836e+03 - logprior: -1.7232e-01
Epoch 5/10
14/14 - 3s - loss: 1078.8633 - loglik: -1.0789e+03 - logprior: 0.0022
Epoch 6/10
14/14 - 3s - loss: 1072.7921 - loglik: -1.0730e+03 - logprior: 0.2174
Epoch 7/10
14/14 - 3s - loss: 1073.8557 - loglik: -1.0742e+03 - logprior: 0.3276
Fitted a model with MAP estimate = -1072.1345
expansions: [(14, 1), (15, 1), (28, 1), (30, 1), (31, 2), (32, 2), (33, 1), (41, 2), (42, 1), (51, 1), (52, 1), (53, 1), (55, 4), (65, 1), (82, 1), (91, 1), (111, 1), (113, 1), (114, 2), (115, 1), (116, 4), (118, 2), (119, 2), (120, 3), (130, 2), (157, 1), (163, 2), (165, 2), (166, 6), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1102.3889 - loglik: -1.0761e+03 - logprior: -2.6241e+01
Epoch 2/2
14/14 - 4s - loss: 1072.6019 - loglik: -1.0651e+03 - logprior: -7.5498e+00
Fitted a model with MAP estimate = -1065.4458
expansions: [(143, 1)]
discards: [  0  73 138 151 154 168 208 213]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 1089.4869 - loglik: -1.0639e+03 - logprior: -2.5578e+01
Epoch 2/2
14/14 - 4s - loss: 1068.1913 - loglik: -1.0623e+03 - logprior: -5.8452e+00
Fitted a model with MAP estimate = -1061.6570
expansions: [(0, 4), (68, 2)]
discards: [  0  49  72  73  74 208]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 8s - loss: 1081.7399 - loglik: -1.0634e+03 - logprior: -1.8327e+01
Epoch 2/10
14/14 - 4s - loss: 1062.8904 - loglik: -1.0633e+03 - logprior: 0.4072
Epoch 3/10
14/14 - 4s - loss: 1055.5085 - loglik: -1.0590e+03 - logprior: 3.5007
Epoch 4/10
14/14 - 4s - loss: 1052.1373 - loglik: -1.0568e+03 - logprior: 4.7037
Epoch 5/10
14/14 - 4s - loss: 1055.4493 - loglik: -1.0607e+03 - logprior: 5.2853
Fitted a model with MAP estimate = -1051.6187
Time for alignment: 86.5007
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 1254.4851 - loglik: -1.2320e+03 - logprior: -2.2459e+01
Epoch 2/10
14/14 - 3s - loss: 1156.8022 - loglik: -1.1548e+03 - logprior: -2.0016e+00
Epoch 3/10
14/14 - 3s - loss: 1102.4904 - loglik: -1.1020e+03 - logprior: -4.6336e-01
Epoch 4/10
14/14 - 3s - loss: 1079.7067 - loglik: -1.0798e+03 - logprior: 0.0559
Epoch 5/10
14/14 - 3s - loss: 1079.1227 - loglik: -1.0793e+03 - logprior: 0.1744
Epoch 6/10
14/14 - 3s - loss: 1075.0927 - loglik: -1.0754e+03 - logprior: 0.3128
Epoch 7/10
14/14 - 3s - loss: 1069.9733 - loglik: -1.0704e+03 - logprior: 0.3866
Epoch 8/10
14/14 - 3s - loss: 1072.2921 - loglik: -1.0727e+03 - logprior: 0.4128
Fitted a model with MAP estimate = -1069.0726
expansions: [(14, 1), (15, 1), (28, 1), (30, 1), (31, 2), (32, 2), (40, 2), (41, 2), (42, 1), (44, 1), (50, 1), (51, 1), (54, 3), (91, 1), (92, 7), (110, 1), (112, 1), (113, 3), (114, 2), (115, 1), (116, 1), (118, 1), (119, 1), (120, 3), (130, 2), (154, 1), (163, 1), (165, 2), (166, 6), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1102.9172 - loglik: -1.0767e+03 - logprior: -2.6219e+01
Epoch 2/2
14/14 - 4s - loss: 1068.6195 - loglik: -1.0611e+03 - logprior: -7.5060e+00
Fitted a model with MAP estimate = -1062.7036
expansions: [(0, 3), (117, 1)]
discards: [  0  48 112 113 171]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 1078.6498 - loglik: -1.0597e+03 - logprior: -1.8930e+01
Epoch 2/2
14/14 - 4s - loss: 1055.6029 - loglik: -1.0555e+03 - logprior: -1.5007e-01
Fitted a model with MAP estimate = -1052.4586
expansions: [(71, 2)]
discards: [  0   1 113]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1082.1063 - loglik: -1.0567e+03 - logprior: -2.5373e+01
Epoch 2/10
14/14 - 4s - loss: 1062.0023 - loglik: -1.0561e+03 - logprior: -5.9246e+00
Epoch 3/10
14/14 - 4s - loss: 1051.5446 - loglik: -1.0528e+03 - logprior: 1.2162
Epoch 4/10
14/14 - 4s - loss: 1049.9813 - loglik: -1.0543e+03 - logprior: 4.3679
Epoch 5/10
14/14 - 4s - loss: 1046.1770 - loglik: -1.0515e+03 - logprior: 5.3217
Epoch 6/10
14/14 - 4s - loss: 1044.3790 - loglik: -1.0501e+03 - logprior: 5.6748
Epoch 7/10
14/14 - 4s - loss: 1043.5551 - loglik: -1.0495e+03 - logprior: 5.9857
Epoch 8/10
14/14 - 4s - loss: 1040.1323 - loglik: -1.0464e+03 - logprior: 6.3047
Epoch 9/10
14/14 - 4s - loss: 1041.4532 - loglik: -1.0481e+03 - logprior: 6.6051
Fitted a model with MAP estimate = -1039.6092
Time for alignment: 107.9247
Fitting a model of length 184 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1251.9929 - loglik: -1.2295e+03 - logprior: -2.2453e+01
Epoch 2/10
14/14 - 3s - loss: 1155.1598 - loglik: -1.1531e+03 - logprior: -2.0234e+00
Epoch 3/10
14/14 - 3s - loss: 1101.0397 - loglik: -1.1004e+03 - logprior: -6.2710e-01
Epoch 4/10
14/14 - 3s - loss: 1083.8242 - loglik: -1.0836e+03 - logprior: -2.2024e-01
Epoch 5/10
14/14 - 3s - loss: 1076.5677 - loglik: -1.0766e+03 - logprior: 0.0162
Epoch 6/10
14/14 - 3s - loss: 1076.9806 - loglik: -1.0772e+03 - logprior: 0.1716
Fitted a model with MAP estimate = -1073.8082
expansions: [(14, 1), (15, 2), (28, 1), (29, 2), (30, 2), (31, 2), (39, 2), (40, 2), (41, 1), (50, 1), (51, 1), (52, 1), (54, 4), (62, 1), (80, 1), (90, 1), (110, 1), (112, 1), (113, 3), (114, 2), (115, 1), (116, 1), (118, 1), (119, 3), (120, 1), (155, 1), (157, 1), (163, 1), (166, 8), (167, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 1100.7727 - loglik: -1.0746e+03 - logprior: -2.6137e+01
Epoch 2/2
14/14 - 4s - loss: 1069.2828 - loglik: -1.0619e+03 - logprior: -7.3341e+00
Fitted a model with MAP estimate = -1064.5446
expansions: [(71, 2)]
discards: [  0  15  49  75 214]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 1088.3832 - loglik: -1.0629e+03 - logprior: -2.5516e+01
Epoch 2/2
14/14 - 4s - loss: 1071.6437 - loglik: -1.0658e+03 - logprior: -5.8594e+00
Fitted a model with MAP estimate = -1061.3613
expansions: [(0, 4)]
discards: [  0  69  74  75  76 212]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 1379 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 1080.9904 - loglik: -1.0627e+03 - logprior: -1.8329e+01
Epoch 2/10
14/14 - 4s - loss: 1063.1316 - loglik: -1.0635e+03 - logprior: 0.4086
Epoch 3/10
14/14 - 4s - loss: 1055.2449 - loglik: -1.0587e+03 - logprior: 3.4550
Epoch 4/10
14/14 - 4s - loss: 1056.1963 - loglik: -1.0609e+03 - logprior: 4.6778
Fitted a model with MAP estimate = -1053.1628
Time for alignment: 79.4124
Computed alignments with likelihoods: ['-1046.2662', '-1049.3356', '-1051.6187', '-1039.6092', '-1053.1628']
Best model has likelihood: -1039.6092  (prior= 6.7417 )
time for generating output: 0.2637
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cah.projection.fasta
SP score = 0.8803972366148531
Training of 5 independent models on file trfl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efc08040>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c6fd610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3b9535e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 1444.5112 - loglik: -1.4039e+03 - logprior: -4.0620e+01
Epoch 2/10
11/11 - 8s - loss: 1330.6611 - loglik: -1.3277e+03 - logprior: -2.9747e+00
Epoch 3/10
11/11 - 8s - loss: 1247.3159 - loglik: -1.2490e+03 - logprior: 1.6592
Epoch 4/10
11/11 - 7s - loss: 1204.1594 - loglik: -1.2068e+03 - logprior: 2.6728
Epoch 5/10
11/11 - 8s - loss: 1192.6620 - loglik: -1.1958e+03 - logprior: 3.0927
Epoch 6/10
11/11 - 8s - loss: 1173.9393 - loglik: -1.1773e+03 - logprior: 3.4030
Epoch 7/10
11/11 - 8s - loss: 1172.8948 - loglik: -1.1765e+03 - logprior: 3.6185
Epoch 8/10
11/11 - 8s - loss: 1165.6874 - loglik: -1.1693e+03 - logprior: 3.6450
Epoch 9/10
11/11 - 7s - loss: 1166.8231 - loglik: -1.1705e+03 - logprior: 3.6955
Fitted a model with MAP estimate = -1165.9988
expansions: [(22, 4), (26, 1), (43, 1), (50, 1), (52, 2), (55, 1), (63, 4), (64, 2), (76, 1), (77, 2), (78, 2), (89, 1), (102, 1), (104, 2), (106, 3), (107, 1), (132, 2), (152, 1), (153, 1), (162, 5), (181, 1), (182, 1), (183, 3), (199, 7), (200, 2), (201, 5), (222, 1), (224, 1), (225, 1)]
discards: [  0 121]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 16s - loss: 1224.3667 - loglik: -1.1773e+03 - logprior: -4.7038e+01
Epoch 2/2
11/11 - 11s - loss: 1159.5046 - loglik: -1.1467e+03 - logprior: -1.2844e+01
Fitted a model with MAP estimate = -1151.2033
expansions: [(0, 3), (221, 1), (243, 1)]
discards: [  0  59  75 127 130 131 296 298 299]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1186.0441 - loglik: -1.1510e+03 - logprior: -3.5041e+01
Epoch 2/2
11/11 - 11s - loss: 1137.5776 - loglik: -1.1371e+03 - logprior: -4.4318e-01
Fitted a model with MAP estimate = -1135.7949
expansions: [(23, 2), (248, 1), (276, 1), (295, 1), (296, 1)]
discards: [ 1  2 76 93]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 1173.2456 - loglik: -1.1402e+03 - logprior: -3.2997e+01
Epoch 2/10
11/11 - 11s - loss: 1140.3973 - loglik: -1.1414e+03 - logprior: 0.9561
Epoch 3/10
11/11 - 10s - loss: 1130.6215 - loglik: -1.1386e+03 - logprior: 7.9516
Epoch 4/10
11/11 - 11s - loss: 1129.1064 - loglik: -1.1401e+03 - logprior: 10.9766
Epoch 5/10
11/11 - 11s - loss: 1115.6519 - loglik: -1.1283e+03 - logprior: 12.6695
Epoch 6/10
11/11 - 11s - loss: 1117.6661 - loglik: -1.1313e+03 - logprior: 13.5936
Fitted a model with MAP estimate = -1117.0152
Time for alignment: 213.5384
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 1444.5961 - loglik: -1.4040e+03 - logprior: -4.0606e+01
Epoch 2/10
11/11 - 8s - loss: 1332.3031 - loglik: -1.3293e+03 - logprior: -3.0082e+00
Epoch 3/10
11/11 - 8s - loss: 1248.9161 - loglik: -1.2501e+03 - logprior: 1.1644
Epoch 4/10
11/11 - 8s - loss: 1199.1818 - loglik: -1.2008e+03 - logprior: 1.6380
Epoch 5/10
11/11 - 8s - loss: 1181.6759 - loglik: -1.1835e+03 - logprior: 1.8182
Epoch 6/10
11/11 - 8s - loss: 1173.4534 - loglik: -1.1757e+03 - logprior: 2.2286
Epoch 7/10
11/11 - 8s - loss: 1170.3608 - loglik: -1.1728e+03 - logprior: 2.4279
Epoch 8/10
11/11 - 9s - loss: 1163.1370 - loglik: -1.1656e+03 - logprior: 2.4712
Epoch 9/10
11/11 - 8s - loss: 1164.9974 - loglik: -1.1675e+03 - logprior: 2.5406
Fitted a model with MAP estimate = -1163.6802
expansions: [(19, 4), (21, 2), (24, 1), (35, 1), (42, 1), (46, 1), (48, 1), (63, 1), (65, 1), (78, 1), (79, 2), (80, 1), (91, 1), (93, 1), (103, 3), (104, 1), (105, 1), (107, 1), (109, 1), (152, 2), (153, 1), (161, 1), (168, 3), (179, 1), (180, 1), (181, 3), (198, 1), (199, 1), (200, 2), (201, 1), (203, 6), (221, 1), (224, 1), (225, 1), (226, 1), (228, 1)]
discards: [  0   1 131 209]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 1222.8038 - loglik: -1.1755e+03 - logprior: -4.7294e+01
Epoch 2/2
11/11 - 11s - loss: 1166.0138 - loglik: -1.1531e+03 - logprior: -1.2891e+01
Fitted a model with MAP estimate = -1155.1165
expansions: [(0, 3), (213, 1), (269, 1)]
discards: [  0  20  91 121 122 271 288 290 291]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1188.8470 - loglik: -1.1537e+03 - logprior: -3.5194e+01
Epoch 2/2
11/11 - 10s - loss: 1146.0172 - loglik: -1.1456e+03 - logprior: -4.5723e-01
Fitted a model with MAP estimate = -1139.6506
expansions: [(237, 1), (239, 1), (247, 2), (287, 1), (288, 1)]
discards: [  0 243 244 245]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 1196.5073 - loglik: -1.1517e+03 - logprior: -4.4833e+01
Epoch 2/10
11/11 - 11s - loss: 1154.0005 - loglik: -1.1440e+03 - logprior: -9.9934e+00
Epoch 3/10
11/11 - 10s - loss: 1144.0820 - loglik: -1.1458e+03 - logprior: 1.7252
Epoch 4/10
11/11 - 9s - loss: 1124.0764 - loglik: -1.1337e+03 - logprior: 9.6244
Epoch 5/10
11/11 - 10s - loss: 1128.9860 - loglik: -1.1410e+03 - logprior: 12.0606
Fitted a model with MAP estimate = -1125.8819
Time for alignment: 201.2861
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 11s - loss: 1446.2286 - loglik: -1.4056e+03 - logprior: -4.0621e+01
Epoch 2/10
11/11 - 9s - loss: 1327.9091 - loglik: -1.3249e+03 - logprior: -2.9797e+00
Epoch 3/10
11/11 - 8s - loss: 1236.7913 - loglik: -1.2383e+03 - logprior: 1.4681
Epoch 4/10
11/11 - 8s - loss: 1196.2183 - loglik: -1.1984e+03 - logprior: 2.1774
Epoch 5/10
11/11 - 8s - loss: 1181.6650 - loglik: -1.1844e+03 - logprior: 2.7616
Epoch 6/10
11/11 - 8s - loss: 1178.0399 - loglik: -1.1813e+03 - logprior: 3.2349
Epoch 7/10
11/11 - 7s - loss: 1173.2384 - loglik: -1.1767e+03 - logprior: 3.4527
Epoch 8/10
11/11 - 8s - loss: 1167.3638 - loglik: -1.1710e+03 - logprior: 3.6096
Epoch 9/10
11/11 - 8s - loss: 1168.7415 - loglik: -1.1725e+03 - logprior: 3.7408
Fitted a model with MAP estimate = -1166.9281
expansions: [(22, 4), (26, 1), (35, 1), (36, 1), (41, 1), (45, 1), (47, 1), (49, 2), (61, 4), (62, 1), (75, 1), (76, 1), (77, 1), (88, 1), (101, 3), (102, 1), (103, 1), (105, 1), (161, 2), (162, 5), (179, 1), (180, 1), (181, 3), (197, 3), (198, 1), (199, 2), (201, 2), (202, 5), (221, 1), (223, 1), (224, 1), (225, 2)]
discards: [  0   1 211]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 1226.8375 - loglik: -1.1800e+03 - logprior: -4.6848e+01
Epoch 2/2
11/11 - 10s - loss: 1164.4083 - loglik: -1.1519e+03 - logprior: -1.2544e+01
Fitted a model with MAP estimate = -1155.0085
expansions: [(0, 4), (215, 1)]
discards: [  0  75  76 121 122 276 292 294 295]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1184.7354 - loglik: -1.1498e+03 - logprior: -3.4960e+01
Epoch 2/2
11/11 - 10s - loss: 1145.8265 - loglik: -1.1454e+03 - logprior: -3.9732e-01
Fitted a model with MAP estimate = -1141.2025
expansions: [(23, 2), (194, 2), (245, 1), (291, 1), (292, 1)]
discards: [ 1  2 61]
Re-initialized the encoder parameters.
Fitting a model of length 298 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 1180.9468 - loglik: -1.1479e+03 - logprior: -3.3060e+01
Epoch 2/10
11/11 - 11s - loss: 1142.2161 - loglik: -1.1431e+03 - logprior: 0.8483
Epoch 3/10
11/11 - 11s - loss: 1133.4506 - loglik: -1.1412e+03 - logprior: 7.7600
Epoch 4/10
11/11 - 12s - loss: 1131.9010 - loglik: -1.1428e+03 - logprior: 10.8526
Epoch 5/10
11/11 - 10s - loss: 1126.8566 - loglik: -1.1394e+03 - logprior: 12.5199
Epoch 6/10
11/11 - 11s - loss: 1119.6295 - loglik: -1.1330e+03 - logprior: 13.4170
Epoch 7/10
11/11 - 11s - loss: 1119.9644 - loglik: -1.1341e+03 - logprior: 14.1338
Fitted a model with MAP estimate = -1118.8276
Time for alignment: 219.8656
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 1443.0574 - loglik: -1.4025e+03 - logprior: -4.0603e+01
Epoch 2/10
11/11 - 8s - loss: 1332.3470 - loglik: -1.3294e+03 - logprior: -2.9546e+00
Epoch 3/10
11/11 - 8s - loss: 1249.7748 - loglik: -1.2519e+03 - logprior: 2.0938
Epoch 4/10
11/11 - 8s - loss: 1200.4911 - loglik: -1.2037e+03 - logprior: 3.1622
Epoch 5/10
11/11 - 8s - loss: 1188.7692 - loglik: -1.1926e+03 - logprior: 3.8600
Epoch 6/10
11/11 - 8s - loss: 1176.4742 - loglik: -1.1806e+03 - logprior: 4.1097
Epoch 7/10
11/11 - 8s - loss: 1170.9396 - loglik: -1.1752e+03 - logprior: 4.2795
Epoch 8/10
11/11 - 8s - loss: 1163.7317 - loglik: -1.1682e+03 - logprior: 4.4454
Epoch 9/10
11/11 - 8s - loss: 1167.2140 - loglik: -1.1718e+03 - logprior: 4.5552
Fitted a model with MAP estimate = -1164.7361
expansions: [(22, 4), (37, 1), (43, 1), (50, 1), (53, 2), (58, 1), (60, 1), (62, 2), (63, 2), (64, 3), (76, 3), (77, 3), (78, 1), (102, 3), (103, 1), (104, 1), (129, 1), (162, 1), (164, 5), (181, 7), (199, 11), (219, 1), (221, 1), (225, 1)]
discards: [  0   1 209]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 13s - loss: 1225.0122 - loglik: -1.1781e+03 - logprior: -4.6943e+01
Epoch 2/2
11/11 - 11s - loss: 1165.9004 - loglik: -1.1533e+03 - logprior: -1.2583e+01
Fitted a model with MAP estimate = -1155.8022
expansions: [(0, 4), (220, 1), (247, 3)]
discards: [  0  58  74  75  78  96 127 128 129 198 293 296]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 15s - loss: 1189.7174 - loglik: -1.1546e+03 - logprior: -3.5124e+01
Epoch 2/2
11/11 - 10s - loss: 1143.2598 - loglik: -1.1428e+03 - logprior: -4.2926e-01
Fitted a model with MAP estimate = -1138.4061
expansions: [(23, 2), (240, 2), (241, 1)]
discards: [  1   2  77 245]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 14s - loss: 1178.9445 - loglik: -1.1455e+03 - logprior: -3.3462e+01
Epoch 2/10
11/11 - 11s - loss: 1140.3042 - loglik: -1.1407e+03 - logprior: 0.4383
Epoch 3/10
11/11 - 11s - loss: 1134.4061 - loglik: -1.1417e+03 - logprior: 7.3311
Epoch 4/10
11/11 - 10s - loss: 1127.7511 - loglik: -1.1381e+03 - logprior: 10.3986
Epoch 5/10
11/11 - 11s - loss: 1126.7640 - loglik: -1.1388e+03 - logprior: 12.0365
Epoch 6/10
11/11 - 11s - loss: 1124.0146 - loglik: -1.1370e+03 - logprior: 12.9923
Epoch 7/10
11/11 - 11s - loss: 1114.7145 - loglik: -1.1284e+03 - logprior: 13.7226
Epoch 8/10
11/11 - 11s - loss: 1120.7802 - loglik: -1.1353e+03 - logprior: 14.4724
Fitted a model with MAP estimate = -1115.5923
Time for alignment: 233.0397
Fitting a model of length 244 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 12s - loss: 1453.8741 - loglik: -1.4133e+03 - logprior: -4.0614e+01
Epoch 2/10
11/11 - 8s - loss: 1320.3571 - loglik: -1.3173e+03 - logprior: -3.0084e+00
Epoch 3/10
11/11 - 8s - loss: 1247.5525 - loglik: -1.2489e+03 - logprior: 1.3233
Epoch 4/10
11/11 - 8s - loss: 1198.7596 - loglik: -1.2011e+03 - logprior: 2.3255
Epoch 5/10
11/11 - 8s - loss: 1182.7892 - loglik: -1.1856e+03 - logprior: 2.8580
Epoch 6/10
11/11 - 9s - loss: 1172.0269 - loglik: -1.1751e+03 - logprior: 3.0496
Epoch 7/10
11/11 - 8s - loss: 1167.7118 - loglik: -1.1710e+03 - logprior: 3.2841
Epoch 8/10
11/11 - 8s - loss: 1167.6831 - loglik: -1.1712e+03 - logprior: 3.4685
Epoch 9/10
11/11 - 8s - loss: 1159.3447 - loglik: -1.1628e+03 - logprior: 3.5007
Epoch 10/10
11/11 - 9s - loss: 1171.2563 - loglik: -1.1749e+03 - logprior: 3.6075
Fitted a model with MAP estimate = -1162.4857
expansions: [(19, 3), (20, 5), (23, 1), (35, 1), (41, 1), (48, 1), (54, 1), (62, 4), (63, 1), (76, 1), (77, 2), (78, 2), (88, 1), (101, 3), (104, 1), (108, 5), (120, 2), (137, 1), (161, 2), (163, 5), (180, 1), (181, 1), (182, 1), (183, 2), (199, 4), (200, 2), (202, 2), (203, 6), (221, 1), (223, 1), (224, 1), (225, 2)]
discards: [  0   1 130 131 210]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1224.1865 - loglik: -1.1776e+03 - logprior: -4.6589e+01
Epoch 2/2
11/11 - 11s - loss: 1157.4569 - loglik: -1.1452e+03 - logprior: -1.2299e+01
Fitted a model with MAP estimate = -1148.6680
expansions: [(0, 4), (253, 1)]
discards: [  0  19  21  77  94 125 126 135 136 137 151 284 300 302 303]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 14s - loss: 1186.2521 - loglik: -1.1517e+03 - logprior: -3.4520e+01
Epoch 2/2
11/11 - 11s - loss: 1141.3921 - loglik: -1.1414e+03 - logprior: -2.7988e-03
Fitted a model with MAP estimate = -1136.6280
expansions: [(293, 1), (294, 1)]
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 296 on 837 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 13s - loss: 1175.1770 - loglik: -1.1422e+03 - logprior: -3.3013e+01
Epoch 2/10
11/11 - 12s - loss: 1138.3813 - loglik: -1.1393e+03 - logprior: 0.9139
Epoch 3/10
11/11 - 11s - loss: 1132.8325 - loglik: -1.1405e+03 - logprior: 7.6820
Epoch 4/10
11/11 - 11s - loss: 1126.7267 - loglik: -1.1377e+03 - logprior: 10.9821
Epoch 5/10
11/11 - 11s - loss: 1122.2177 - loglik: -1.1349e+03 - logprior: 12.6889
Epoch 6/10
11/11 - 11s - loss: 1118.4330 - loglik: -1.1321e+03 - logprior: 13.6373
Epoch 7/10
11/11 - 10s - loss: 1118.2725 - loglik: -1.1327e+03 - logprior: 14.4464
Epoch 8/10
11/11 - 10s - loss: 1116.8409 - loglik: -1.1320e+03 - logprior: 15.1225
Epoch 9/10
11/11 - 11s - loss: 1115.1345 - loglik: -1.1307e+03 - logprior: 15.6048
Epoch 10/10
11/11 - 12s - loss: 1105.4813 - loglik: -1.1216e+03 - logprior: 16.0721
Fitted a model with MAP estimate = -1112.0892
Time for alignment: 265.1416
Computed alignments with likelihoods: ['-1117.0152', '-1125.8819', '-1118.8276', '-1115.5923', '-1112.0892']
Best model has likelihood: -1112.0892  (prior= 16.3913 )
time for generating output: 0.4899
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/trfl.projection.fasta
SP score = 0.9259902710215427
Training of 5 independent models on file serpin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea55843bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cdcec400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cd9cffd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 1659.9857 - loglik: -1.6540e+03 - logprior: -5.9425e+00
Epoch 2/10
21/21 - 15s - loss: 1536.1012 - loglik: -1.5356e+03 - logprior: -4.7666e-01
Epoch 3/10
21/21 - 15s - loss: 1486.5037 - loglik: -1.4845e+03 - logprior: -2.0355e+00
Epoch 4/10
21/21 - 15s - loss: 1477.8661 - loglik: -1.4760e+03 - logprior: -1.8499e+00
Epoch 5/10
21/21 - 15s - loss: 1470.5065 - loglik: -1.4688e+03 - logprior: -1.7327e+00
Epoch 6/10
21/21 - 15s - loss: 1473.9905 - loglik: -1.4721e+03 - logprior: -1.8445e+00
Fitted a model with MAP estimate = -1466.5301
expansions: [(13, 1), (14, 2), (15, 1), (43, 1), (45, 1), (51, 4), (54, 1), (60, 2), (61, 2), (62, 1), (64, 1), (65, 1), (74, 1), (75, 1), (76, 1), (79, 1), (80, 2), (81, 2), (83, 1), (85, 1), (86, 1), (89, 1), (92, 1), (93, 1), (94, 1), (101, 1), (112, 1), (113, 1), (114, 1), (115, 1), (135, 1), (138, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 2), (160, 1), (161, 4), (180, 1), (183, 2), (191, 1), (192, 1), (194, 2), (195, 1), (196, 1), (210, 1), (211, 1), (213, 1), (214, 1), (216, 1), (225, 1), (230, 2), (231, 3), (233, 1), (235, 1), (257, 1), (258, 3), (259, 2), (261, 2), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 1475.3232 - loglik: -1.4698e+03 - logprior: -5.5723e+00
Epoch 2/2
21/21 - 23s - loss: 1451.2235 - loglik: -1.4522e+03 - logprior: 0.9840
Fitted a model with MAP estimate = -1444.4258
expansions: [(76, 1), (143, 1)]
discards: [104 203 205 250 297 336]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 1455.3881 - loglik: -1.4508e+03 - logprior: -4.6082e+00
Epoch 2/2
21/21 - 22s - loss: 1446.0450 - loglik: -1.4478e+03 - logprior: 1.7524
Fitted a model with MAP estimate = -1442.4700
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 1450.5667 - loglik: -1.4462e+03 - logprior: -4.4061e+00
Epoch 2/10
21/21 - 23s - loss: 1447.8583 - loglik: -1.4500e+03 - logprior: 2.1151
Epoch 3/10
21/21 - 22s - loss: 1440.4884 - loglik: -1.4433e+03 - logprior: 2.8199
Epoch 4/10
21/21 - 22s - loss: 1440.1058 - loglik: -1.4434e+03 - logprior: 3.2917
Epoch 5/10
21/21 - 22s - loss: 1433.0496 - loglik: -1.4366e+03 - logprior: 3.5045
Epoch 6/10
21/21 - 22s - loss: 1432.8240 - loglik: -1.4366e+03 - logprior: 3.7476
Epoch 7/10
21/21 - 23s - loss: 1429.6904 - loglik: -1.4337e+03 - logprior: 3.9797
Epoch 8/10
21/21 - 22s - loss: 1425.0804 - loglik: -1.4292e+03 - logprior: 4.0900
Epoch 9/10
21/21 - 22s - loss: 1404.7609 - loglik: -1.4088e+03 - logprior: 4.0686
Epoch 10/10
21/21 - 22s - loss: 1342.6987 - loglik: -1.3463e+03 - logprior: 3.6089
Fitted a model with MAP estimate = -1284.0377
Time for alignment: 477.5995
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 1659.1910 - loglik: -1.6532e+03 - logprior: -5.9635e+00
Epoch 2/10
21/21 - 15s - loss: 1525.1381 - loglik: -1.5245e+03 - logprior: -6.2341e-01
Epoch 3/10
21/21 - 15s - loss: 1488.1683 - loglik: -1.4860e+03 - logprior: -2.1879e+00
Epoch 4/10
21/21 - 15s - loss: 1477.5848 - loglik: -1.4755e+03 - logprior: -2.1045e+00
Epoch 5/10
21/21 - 15s - loss: 1472.1946 - loglik: -1.4703e+03 - logprior: -1.9189e+00
Epoch 6/10
21/21 - 15s - loss: 1468.4205 - loglik: -1.4665e+03 - logprior: -1.9627e+00
Epoch 7/10
21/21 - 15s - loss: 1460.8138 - loglik: -1.4588e+03 - logprior: -2.0350e+00
Epoch 8/10
21/21 - 15s - loss: 1450.3201 - loglik: -1.4482e+03 - logprior: -2.1413e+00
Epoch 9/10
21/21 - 15s - loss: 1442.9194 - loglik: -1.4403e+03 - logprior: -2.6126e+00
Epoch 10/10
21/21 - 15s - loss: 1423.9773 - loglik: -1.4214e+03 - logprior: -2.5676e+00
Fitted a model with MAP estimate = -1404.1534
expansions: [(13, 1), (15, 2), (50, 1), (52, 3), (54, 1), (60, 1), (65, 1), (66, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (84, 1), (85, 1), (86, 1), (91, 1), (92, 1), (93, 1), (94, 1), (101, 1), (110, 1), (114, 1), (116, 1), (129, 1), (135, 1), (138, 1), (141, 1), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 2), (158, 2), (159, 2), (179, 1), (182, 1), (189, 1), (190, 1), (191, 2), (192, 1), (193, 1), (194, 1), (208, 1), (209, 1), (212, 1), (215, 1), (219, 1), (228, 1), (229, 1), (230, 2), (232, 1), (255, 1), (256, 1), (257, 1), (258, 2), (259, 2), (260, 2), (261, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 359 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 25s - loss: 1622.9916 - loglik: -1.6160e+03 - logprior: -7.0169e+00
Epoch 2/2
21/21 - 22s - loss: 1485.1082 - loglik: -1.4848e+03 - logprior: -2.8052e-01
Fitted a model with MAP estimate = -1461.9034
expansions: [(14, 1), (60, 1), (70, 1), (71, 2), (99, 1), (106, 1), (118, 1), (139, 1), (197, 1), (204, 1), (244, 3), (325, 1), (326, 1)]
discards: [  1   4  17  57  61 100 107 119 192 193 194 245 247 248 249 250 319 320
 321 322 323 329]
Re-initialized the encoder parameters.
Fitting a model of length 353 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 24s - loss: 1468.2081 - loglik: -1.4636e+03 - logprior: -4.6357e+00
Epoch 2/2
21/21 - 21s - loss: 1447.0740 - loglik: -1.4489e+03 - logprior: 1.8001
Fitted a model with MAP estimate = -1446.6402
expansions: [(0, 3), (56, 2), (59, 1), (244, 1), (245, 1), (246, 1), (318, 1), (320, 1)]
discards: [248]
Re-initialized the encoder parameters.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 1457.2560 - loglik: -1.4510e+03 - logprior: -6.2116e+00
Epoch 2/10
21/21 - 22s - loss: 1443.9059 - loglik: -1.4463e+03 - logprior: 2.3814
Epoch 3/10
21/21 - 22s - loss: 1446.0372 - loglik: -1.4493e+03 - logprior: 3.2952
Fitted a model with MAP estimate = -1439.2851
Time for alignment: 374.3209
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 1654.7397 - loglik: -1.6488e+03 - logprior: -5.9707e+00
Epoch 2/10
21/21 - 15s - loss: 1536.6985 - loglik: -1.5361e+03 - logprior: -6.0050e-01
Epoch 3/10
21/21 - 15s - loss: 1484.9202 - loglik: -1.4827e+03 - logprior: -2.1866e+00
Epoch 4/10
21/21 - 15s - loss: 1478.2075 - loglik: -1.4762e+03 - logprior: -2.0257e+00
Epoch 5/10
21/21 - 15s - loss: 1473.3378 - loglik: -1.4714e+03 - logprior: -1.8871e+00
Epoch 6/10
21/21 - 15s - loss: 1463.6840 - loglik: -1.4617e+03 - logprior: -1.9638e+00
Epoch 7/10
21/21 - 15s - loss: 1462.6029 - loglik: -1.4606e+03 - logprior: -1.9844e+00
Epoch 8/10
21/21 - 15s - loss: 1452.9590 - loglik: -1.4509e+03 - logprior: -2.0977e+00
Epoch 9/10
21/21 - 15s - loss: 1438.8612 - loglik: -1.4364e+03 - logprior: -2.4403e+00
Epoch 10/10
21/21 - 15s - loss: 1412.8455 - loglik: -1.4103e+03 - logprior: -2.5375e+00
Fitted a model with MAP estimate = -1388.3732
expansions: [(67, 1), (76, 1), (77, 1), (80, 1), (83, 2), (85, 1), (87, 1), (91, 1), (94, 1), (96, 1), (102, 1), (103, 1), (116, 1), (118, 1), (136, 1), (137, 1), (140, 1), (143, 1), (146, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (161, 1), (162, 1), (171, 1), (181, 1), (184, 1), (188, 1), (190, 1), (191, 1), (192, 2), (193, 1), (194, 1), (195, 1), (209, 1), (210, 1), (212, 1), (213, 1), (215, 1), (219, 1), (230, 3), (232, 1), (234, 1), (258, 2), (259, 1), (261, 1), (262, 1)]
discards: [ 63  64 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278
 279 280 281 282]
Re-initialized the encoder parameters.
Fitting a model of length 315 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 21s - loss: 1668.1825 - loglik: -1.6612e+03 - logprior: -7.0033e+00
Epoch 2/2
21/21 - 18s - loss: 1517.5667 - loglik: -1.5170e+03 - logprior: -5.6811e-01
Fitted a model with MAP estimate = -1488.3663
expansions: [(13, 1), (14, 2), (15, 1), (55, 2), (63, 1), (64, 3), (83, 1), (84, 1), (94, 1), (182, 2), (183, 1), (228, 2), (280, 1), (303, 1), (304, 4), (305, 3), (306, 2), (312, 1), (313, 1), (315, 16)]
discards: [  1   2   3   4   5  87 178 180 229 231 232 233 234 235 281 307 309 314]
Re-initialized the encoder parameters.
Fitting a model of length 344 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 23s - loss: 1477.6567 - loglik: -1.4726e+03 - logprior: -5.0760e+00
Epoch 2/2
21/21 - 20s - loss: 1456.2915 - loglik: -1.4580e+03 - logprior: 1.6746
Fitted a model with MAP estimate = -1450.3908
expansions: [(0, 4), (15, 1), (51, 2), (57, 1), (63, 1), (64, 1), (65, 1), (94, 1), (234, 3), (235, 1), (310, 1), (334, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 362 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 26s - loss: 1459.1729 - loglik: -1.4529e+03 - logprior: -6.2868e+00
Epoch 2/10
21/21 - 22s - loss: 1449.2148 - loglik: -1.4516e+03 - logprior: 2.3383
Epoch 3/10
21/21 - 22s - loss: 1439.6107 - loglik: -1.4429e+03 - logprior: 3.2453
Epoch 4/10
21/21 - 22s - loss: 1438.2466 - loglik: -1.4419e+03 - logprior: 3.6568
Epoch 5/10
21/21 - 22s - loss: 1436.9810 - loglik: -1.4409e+03 - logprior: 3.9635
Epoch 6/10
21/21 - 22s - loss: 1439.1400 - loglik: -1.4434e+03 - logprior: 4.2175
Fitted a model with MAP estimate = -1431.7845
Time for alignment: 425.5644
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 1655.9698 - loglik: -1.6500e+03 - logprior: -5.9491e+00
Epoch 2/10
21/21 - 15s - loss: 1533.4255 - loglik: -1.5328e+03 - logprior: -6.2507e-01
Epoch 3/10
21/21 - 15s - loss: 1488.8169 - loglik: -1.4867e+03 - logprior: -2.0893e+00
Epoch 4/10
21/21 - 15s - loss: 1475.1089 - loglik: -1.4732e+03 - logprior: -1.9244e+00
Epoch 5/10
21/21 - 15s - loss: 1475.5720 - loglik: -1.4737e+03 - logprior: -1.8239e+00
Fitted a model with MAP estimate = -1470.4743
expansions: [(13, 1), (14, 2), (15, 1), (50, 1), (52, 2), (54, 3), (55, 2), (60, 2), (61, 1), (62, 1), (65, 1), (66, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 2), (81, 1), (85, 1), (86, 1), (90, 1), (93, 1), (94, 1), (95, 1), (101, 1), (103, 1), (110, 1), (112, 1), (114, 1), (116, 1), (129, 1), (135, 1), (138, 1), (141, 2), (153, 1), (154, 1), (155, 1), (157, 1), (158, 1), (160, 1), (161, 2), (180, 1), (183, 2), (189, 1), (190, 1), (191, 2), (192, 1), (193, 1), (194, 1), (195, 1), (209, 1), (212, 1), (214, 1), (219, 1), (223, 1), (229, 2), (230, 2), (235, 1), (240, 1), (256, 1), (257, 1), (258, 1), (259, 2), (260, 3), (261, 1), (272, 2), (273, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 366 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 27s - loss: 1474.3754 - loglik: -1.4661e+03 - logprior: -8.2622e+00
Epoch 2/2
21/21 - 22s - loss: 1446.5919 - loglik: -1.4447e+03 - logprior: -1.9144e+00
Fitted a model with MAP estimate = -1447.0204
expansions: [(0, 4)]
discards: [  0  56 181 295 336 337]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 1452.7322 - loglik: -1.4478e+03 - logprior: -4.9476e+00
Epoch 2/2
21/21 - 22s - loss: 1446.3445 - loglik: -1.4480e+03 - logprior: 1.6535
Fitted a model with MAP estimate = -1442.7042
expansions: []
discards: [1 2 3]
Re-initialized the encoder parameters.
Fitting a model of length 361 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 1449.8844 - loglik: -1.4455e+03 - logprior: -4.3950e+00
Epoch 2/10
21/21 - 21s - loss: 1453.3943 - loglik: -1.4555e+03 - logprior: 2.0842
Fitted a model with MAP estimate = -1442.6925
Time for alignment: 282.0600
Fitting a model of length 283 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 18s - loss: 1655.5774 - loglik: -1.6496e+03 - logprior: -5.9530e+00
Epoch 2/10
21/21 - 15s - loss: 1529.1754 - loglik: -1.5285e+03 - logprior: -6.7912e-01
Epoch 3/10
21/21 - 15s - loss: 1496.7764 - loglik: -1.4946e+03 - logprior: -2.1449e+00
Epoch 4/10
21/21 - 15s - loss: 1475.9939 - loglik: -1.4741e+03 - logprior: -1.8662e+00
Epoch 5/10
21/21 - 15s - loss: 1476.1001 - loglik: -1.4743e+03 - logprior: -1.8460e+00
Fitted a model with MAP estimate = -1470.6649
expansions: [(13, 1), (14, 2), (15, 1), (50, 1), (52, 2), (53, 3), (56, 1), (59, 1), (61, 2), (62, 1), (63, 1), (65, 2), (67, 1), (76, 1), (77, 1), (78, 1), (81, 1), (82, 2), (83, 2), (85, 1), (86, 1), (87, 1), (92, 1), (93, 1), (95, 1), (101, 1), (102, 1), (111, 1), (113, 1), (115, 1), (117, 1), (130, 1), (136, 1), (139, 1), (142, 1), (145, 1), (154, 1), (155, 1), (156, 1), (158, 3), (159, 1), (160, 2), (161, 1), (169, 1), (179, 1), (182, 2), (188, 1), (189, 1), (190, 1), (192, 1), (193, 1), (194, 1), (196, 1), (207, 1), (208, 1), (210, 1), (211, 1), (221, 1), (222, 2), (228, 2), (229, 2), (231, 1), (235, 1), (255, 1), (256, 1), (257, 1), (262, 1), (271, 1), (272, 2), (273, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 368 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 1464.5399 - loglik: -1.4592e+03 - logprior: -5.3677e+00
Epoch 2/2
21/21 - 23s - loss: 1454.1655 - loglik: -1.4552e+03 - logprior: 1.0307
Fitted a model with MAP estimate = -1443.7793
expansions: []
discards: [ 56  59 208 290 300]
Re-initialized the encoder parameters.
Fitting a model of length 363 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 26s - loss: 1451.5057 - loglik: -1.4469e+03 - logprior: -4.5669e+00
Epoch 2/2
21/21 - 22s - loss: 1447.2959 - loglik: -1.4491e+03 - logprior: 1.7997
Fitted a model with MAP estimate = -1442.0940
expansions: [(336, 2)]
discards: [56]
Re-initialized the encoder parameters.
Fitting a model of length 364 on 3144 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 25s - loss: 1455.1541 - loglik: -1.4507e+03 - logprior: -4.4596e+00
Epoch 2/10
21/21 - 22s - loss: 1438.7334 - loglik: -1.4408e+03 - logprior: 2.1011
Epoch 3/10
21/21 - 22s - loss: 1443.9838 - loglik: -1.4469e+03 - logprior: 2.9248
Fitted a model with MAP estimate = -1438.6688
Time for alignment: 304.5511
Computed alignments with likelihoods: ['-1284.0377', '-1404.1534', '-1388.3732', '-1442.6925', '-1438.6688']
Best model has likelihood: -1284.0377  (prior= 3.0621 )
time for generating output: 0.3376
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/serpin.projection.fasta
SP score = 0.8349178055060408
Training of 5 independent models on file ghf13.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea55e0bbb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea77e7faf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea6f614250>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1735.6252 - loglik: -1.7339e+03 - logprior: -1.6912e+00
Epoch 2/10
39/39 - 20s - loss: 1638.6993 - loglik: -1.6374e+03 - logprior: -1.2655e+00
Epoch 3/10
39/39 - 20s - loss: 1627.1392 - loglik: -1.6258e+03 - logprior: -1.3833e+00
Epoch 4/10
39/39 - 20s - loss: 1624.1954 - loglik: -1.6228e+03 - logprior: -1.3905e+00
Epoch 5/10
39/39 - 20s - loss: 1620.2389 - loglik: -1.6189e+03 - logprior: -1.3720e+00
Epoch 6/10
39/39 - 20s - loss: 1618.1195 - loglik: -1.6168e+03 - logprior: -1.3352e+00
Epoch 7/10
39/39 - 20s - loss: 1612.9915 - loglik: -1.6116e+03 - logprior: -1.3538e+00
Epoch 8/10
39/39 - 20s - loss: 1598.2476 - loglik: -1.5969e+03 - logprior: -1.3553e+00
Epoch 9/10
39/39 - 20s - loss: 1475.2168 - loglik: -1.4713e+03 - logprior: -3.8906e+00
Epoch 10/10
39/39 - 20s - loss: 1307.2793 - loglik: -1.3009e+03 - logprior: -6.2857e+00
Fitted a model with MAP estimate = -1150.4300
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94 107 108 109 110 111 112 113 121 122 123 124 125 126
 127 128 130 131 132 133 134 135 136 137 161 162 201 202 203 225 232 233
 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1830.0140 - loglik: -1.8277e+03 - logprior: -2.3316e+00
Epoch 2/2
39/39 - 9s - loss: 1811.3975 - loglik: -1.8105e+03 - logprior: -8.5943e-01
Fitted a model with MAP estimate = -1552.0342
expansions: [(116, 236)]
discards: [  0  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38
  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56
  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74
  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92
  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110
 111 112 113 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 257 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1729.8848 - loglik: -1.7274e+03 - logprior: -2.5246e+00
Epoch 2/2
39/39 - 21s - loss: 1634.4015 - loglik: -1.6329e+03 - logprior: -1.4810e+00
Fitted a model with MAP estimate = -1398.6527
expansions: [(42, 1), (54, 1), (55, 2), (56, 2), (79, 1), (80, 3), (81, 1), (94, 2), (96, 2), (101, 4), (113, 2), (122, 1), (124, 3), (125, 2), (147, 1), (149, 13), (154, 4), (167, 6), (168, 3), (179, 2), (182, 3), (185, 1), (223, 2), (254, 1), (257, 3)]
discards: [107 108 109 110 111 193]
Re-initialized the encoder parameters.
Fitting a model of length 317 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 35s - loss: 1386.6288 - loglik: -1.3850e+03 - logprior: -1.5790e+00
Epoch 2/10
43/43 - 32s - loss: 1372.4133 - loglik: -1.3718e+03 - logprior: -6.5205e-01
Epoch 3/10
43/43 - 32s - loss: 1376.1245 - loglik: -1.3755e+03 - logprior: -6.1092e-01
Fitted a model with MAP estimate = -1371.2092
Time for alignment: 473.7655
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1737.3134 - loglik: -1.7356e+03 - logprior: -1.6874e+00
Epoch 2/10
39/39 - 20s - loss: 1637.8558 - loglik: -1.6365e+03 - logprior: -1.3194e+00
Epoch 3/10
39/39 - 20s - loss: 1627.2522 - loglik: -1.6259e+03 - logprior: -1.3908e+00
Epoch 4/10
39/39 - 20s - loss: 1624.2583 - loglik: -1.6229e+03 - logprior: -1.3736e+00
Epoch 5/10
39/39 - 20s - loss: 1620.7421 - loglik: -1.6194e+03 - logprior: -1.3521e+00
Epoch 6/10
39/39 - 20s - loss: 1617.6359 - loglik: -1.6163e+03 - logprior: -1.3594e+00
Epoch 7/10
39/39 - 20s - loss: 1613.2260 - loglik: -1.6119e+03 - logprior: -1.3548e+00
Epoch 8/10
39/39 - 20s - loss: 1593.0774 - loglik: -1.5917e+03 - logprior: -1.3990e+00
Epoch 9/10
39/39 - 20s - loss: 1448.5939 - loglik: -1.4444e+03 - logprior: -4.1724e+00
Epoch 10/10
39/39 - 20s - loss: 1298.1022 - loglik: -1.2922e+03 - logprior: -5.7996e+00
Fitted a model with MAP estimate = -1149.0314
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  91  95 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120
 121 122 123 124 125 126 127 129 130 131 134 135 136 201 202 203 204 205
 206 225 232 233 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1831.8350 - loglik: -1.8297e+03 - logprior: -2.1722e+00
Epoch 2/2
39/39 - 10s - loss: 1815.7391 - loglik: -1.8147e+03 - logprior: -9.9854e-01
Fitted a model with MAP estimate = -1556.2349
expansions: [(0, 44), (13, 100), (112, 133)]
discards: [ 16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33
  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51
  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69
  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87
  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105
 106 107 108 109 110 111]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1723.8616 - loglik: -1.7214e+03 - logprior: -2.4209e+00
Epoch 2/2
39/39 - 26s - loss: 1636.3420 - loglik: -1.6348e+03 - logprior: -1.5538e+00
Fitted a model with MAP estimate = -1401.3310
expansions: [(79, 1), (110, 1), (115, 1), (116, 3), (117, 1), (130, 3), (135, 1), (144, 1), (145, 5), (195, 1), (196, 2), (197, 2), (198, 1), (233, 1), (265, 1), (270, 1), (274, 1), (291, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32 263]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 1398.0575 - loglik: -1.3963e+03 - logprior: -1.7273e+00
Epoch 2/10
43/43 - 27s - loss: 1379.7106 - loglik: -1.3791e+03 - logprior: -6.4151e-01
Epoch 3/10
43/43 - 28s - loss: 1383.5637 - loglik: -1.3830e+03 - logprior: -5.2361e-01
Fitted a model with MAP estimate = -1380.7429
Time for alignment: 476.3786
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1739.7881 - loglik: -1.7381e+03 - logprior: -1.6990e+00
Epoch 2/10
39/39 - 20s - loss: 1641.4584 - loglik: -1.6400e+03 - logprior: -1.4223e+00
Epoch 3/10
39/39 - 20s - loss: 1628.6407 - loglik: -1.6271e+03 - logprior: -1.4987e+00
Epoch 4/10
39/39 - 20s - loss: 1624.8982 - loglik: -1.6234e+03 - logprior: -1.5182e+00
Epoch 5/10
39/39 - 20s - loss: 1620.6145 - loglik: -1.6191e+03 - logprior: -1.5225e+00
Epoch 6/10
39/39 - 20s - loss: 1619.2034 - loglik: -1.6177e+03 - logprior: -1.5287e+00
Epoch 7/10
39/39 - 20s - loss: 1614.5117 - loglik: -1.6130e+03 - logprior: -1.5102e+00
Epoch 8/10
39/39 - 20s - loss: 1599.6765 - loglik: -1.5981e+03 - logprior: -1.5163e+00
Epoch 9/10
39/39 - 20s - loss: 1498.8038 - loglik: -1.4953e+03 - logprior: -3.4521e+00
Epoch 10/10
39/39 - 20s - loss: 1313.7584 - loglik: -1.3073e+03 - logprior: -6.4008e+00
Fitted a model with MAP estimate = -1150.1400
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  81  91  92 102 103 104 105 106 107 108 109 110 111 113
 114 115 116 117 118 119 120 121 122 123 124 126 127 128 129 130 131 132
 133 147 148 160 165 166 200 201 202 225 232 233 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 1832.0891 - loglik: -1.8302e+03 - logprior: -1.9346e+00
Epoch 2/2
39/39 - 9s - loss: 1813.5231 - loglik: -1.8131e+03 - logprior: -4.0817e-01
Fitted a model with MAP estimate = -1554.4133
expansions: [(5, 3), (14, 1), (122, 247)]
discards: [  1  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33
  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51
  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69
  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87
  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105
 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1724.7804 - loglik: -1.7231e+03 - logprior: -1.7110e+00
Epoch 2/2
39/39 - 23s - loss: 1626.5032 - loglik: -1.6252e+03 - logprior: -1.2640e+00
Fitted a model with MAP estimate = -1391.6995
expansions: [(6, 1), (41, 1), (53, 1), (55, 1), (80, 4), (81, 2), (95, 1), (100, 1), (130, 1), (131, 2), (132, 1), (169, 1), (176, 1), (177, 3), (188, 2), (189, 4), (190, 2), (191, 2), (192, 1), (203, 1), (212, 1), (213, 4), (214, 2), (218, 1), (219, 1), (220, 2), (234, 1), (235, 1), (239, 2), (241, 1), (242, 1), (267, 3)]
discards: [108 109 110 111 112 113 114 115 116 155 156 157 158 159]
Re-initialized the encoder parameters.
Fitting a model of length 306 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 33s - loss: 1387.7252 - loglik: -1.3861e+03 - logprior: -1.6344e+00
Epoch 2/10
43/43 - 30s - loss: 1374.6891 - loglik: -1.3739e+03 - logprior: -7.9928e-01
Epoch 3/10
43/43 - 30s - loss: 1368.7709 - loglik: -1.3681e+03 - logprior: -6.9360e-01
Epoch 4/10
43/43 - 30s - loss: 1372.6219 - loglik: -1.3720e+03 - logprior: -6.5403e-01
Fitted a model with MAP estimate = -1369.8586
Time for alignment: 502.7965
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 23s - loss: 1735.2576 - loglik: -1.7336e+03 - logprior: -1.6661e+00
Epoch 2/10
39/39 - 20s - loss: 1639.7499 - loglik: -1.6386e+03 - logprior: -1.1725e+00
Epoch 3/10
39/39 - 20s - loss: 1625.6925 - loglik: -1.6244e+03 - logprior: -1.3151e+00
Epoch 4/10
39/39 - 20s - loss: 1622.5104 - loglik: -1.6212e+03 - logprior: -1.3320e+00
Epoch 5/10
39/39 - 20s - loss: 1619.3096 - loglik: -1.6180e+03 - logprior: -1.3157e+00
Epoch 6/10
39/39 - 20s - loss: 1616.9493 - loglik: -1.6156e+03 - logprior: -1.3038e+00
Epoch 7/10
39/39 - 20s - loss: 1611.7346 - loglik: -1.6104e+03 - logprior: -1.3034e+00
Epoch 8/10
39/39 - 20s - loss: 1600.6608 - loglik: -1.5993e+03 - logprior: -1.3312e+00
Epoch 9/10
39/39 - 20s - loss: 1497.9277 - loglik: -1.4951e+03 - logprior: -2.7436e+00
Epoch 10/10
39/39 - 20s - loss: 1310.2216 - loglik: -1.3045e+03 - logprior: -5.7123e+00
Fitted a model with MAP estimate = -1149.0113
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  61  62  63  64  65  66  67  68  69  73  74  75  76  84  91 106 108
 109 110 111 112 113 114 121 122 123 124 125 126 127 128 129 131 132 133
 135 136 137 138 166 167 201 202 203 225 232 233 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1830.4260 - loglik: -1.8283e+03 - logprior: -2.1353e+00
Epoch 2/2
39/39 - 10s - loss: 1814.8759 - loglik: -1.8144e+03 - logprior: -4.3897e-01
Fitted a model with MAP estimate = -1555.5984
expansions: [(0, 59), (6, 10), (10, 12), (12, 57), (13, 2), (140, 149)]
discards: [ 18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 31s - loss: 1695.9215 - loglik: -1.6929e+03 - logprior: -2.9853e+00
Epoch 2/2
39/39 - 28s - loss: 1614.1448 - loglik: -1.6129e+03 - logprior: -1.2118e+00
Fitted a model with MAP estimate = -1383.6098
expansions: [(89, 1), (90, 4), (112, 3), (113, 1), (115, 1), (116, 1), (118, 2), (210, 1), (211, 1), (212, 2), (213, 1), (241, 1), (277, 3), (279, 1), (307, 3)]
discards: [  0  29  30  31  32  33  34  70 190 191 192 193 194 195 196 197 198 199
 200 201 202 203 204 205 206 207 242 245 246 247 248 249 250 251 252 253
 254 255]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 1391.3889 - loglik: -1.3893e+03 - logprior: -2.0565e+00
Epoch 2/10
43/43 - 28s - loss: 1372.2025 - loglik: -1.3714e+03 - logprior: -7.6260e-01
Epoch 3/10
43/43 - 29s - loss: 1372.9095 - loglik: -1.3722e+03 - logprior: -7.0176e-01
Fitted a model with MAP estimate = -1374.9201
Time for alignment: 489.8395
Fitting a model of length 244 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 24s - loss: 1736.7341 - loglik: -1.7350e+03 - logprior: -1.7008e+00
Epoch 2/10
39/39 - 20s - loss: 1640.9958 - loglik: -1.6397e+03 - logprior: -1.2668e+00
Epoch 3/10
39/39 - 20s - loss: 1628.6772 - loglik: -1.6273e+03 - logprior: -1.3993e+00
Epoch 4/10
39/39 - 20s - loss: 1625.0410 - loglik: -1.6236e+03 - logprior: -1.4293e+00
Epoch 5/10
39/39 - 20s - loss: 1620.7572 - loglik: -1.6193e+03 - logprior: -1.4363e+00
Epoch 6/10
39/39 - 20s - loss: 1617.9708 - loglik: -1.6165e+03 - logprior: -1.4521e+00
Epoch 7/10
39/39 - 20s - loss: 1614.6761 - loglik: -1.6132e+03 - logprior: -1.4495e+00
Epoch 8/10
39/39 - 20s - loss: 1600.8087 - loglik: -1.5993e+03 - logprior: -1.4767e+00
Epoch 9/10
39/39 - 20s - loss: 1507.9684 - loglik: -1.5049e+03 - logprior: -2.9942e+00
Epoch 10/10
39/39 - 20s - loss: 1317.0757 - loglik: -1.3102e+03 - logprior: -6.8051e+00
Fitted a model with MAP estimate = -1151.3628
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  91  94  95 107 108 109 110 111 112 113 114 115 116 117
 118 119 120 121 122 123 124 125 126 127 129 130 131 134 135 136 146 147
 148 149 150 155 158 159 160 161 169 186 200 201 202 225 232 233 238 239]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1831.8173 - loglik: -1.8298e+03 - logprior: -1.9954e+00
Epoch 2/2
39/39 - 10s - loss: 1817.1719 - loglik: -1.8164e+03 - logprior: -8.1466e-01
Fitted a model with MAP estimate = -1560.2977
expansions: [(0, 113), (118, 158)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108
 109 110 111 112 113 114 115 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1725.5841 - loglik: -1.7227e+03 - logprior: -2.9173e+00
Epoch 2/2
39/39 - 23s - loss: 1622.0890 - loglik: -1.6210e+03 - logprior: -1.1289e+00
Fitted a model with MAP estimate = -1389.4858
expansions: [(8, 1), (14, 1), (30, 1), (40, 1), (43, 1), (51, 1), (79, 4), (80, 2), (88, 1), (111, 3), (171, 3), (190, 1), (203, 5), (210, 3), (242, 3), (272, 3)]
discards: [  0 102 103 104 105 106 107 108 109 152 153]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 12607 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 31s - loss: 1388.5441 - loglik: -1.3864e+03 - logprior: -2.1181e+00
Epoch 2/10
43/43 - 29s - loss: 1381.1060 - loglik: -1.3802e+03 - logprior: -9.2325e-01
Epoch 3/10
43/43 - 28s - loss: 1378.2782 - loglik: -1.3775e+03 - logprior: -8.2161e-01
Epoch 4/10
43/43 - 28s - loss: 1369.8097 - loglik: -1.3691e+03 - logprior: -7.0239e-01
Epoch 5/10
43/43 - 29s - loss: 1374.2914 - loglik: -1.3737e+03 - logprior: -6.3267e-01
Fitted a model with MAP estimate = -1370.6971
Time for alignment: 526.9229
Computed alignments with likelihoods: ['-1150.4300', '-1149.0314', '-1150.1400', '-1149.0113', '-1151.3628']
Best model has likelihood: -1149.0113  (prior= -4.3470 )
time for generating output: 0.4043
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf13.projection.fasta
SP score = 0.046051555943103166
Training of 5 independent models on file cryst.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3b993a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c808730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b4dfbb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 513.7337 - loglik: -4.7559e+02 - logprior: -3.8143e+01
Epoch 2/10
10/10 - 1s - loss: 460.2278 - loglik: -4.5014e+02 - logprior: -1.0087e+01
Epoch 3/10
10/10 - 1s - loss: 433.7116 - loglik: -4.2854e+02 - logprior: -5.1703e+00
Epoch 4/10
10/10 - 1s - loss: 417.8439 - loglik: -4.1435e+02 - logprior: -3.4941e+00
Epoch 5/10
10/10 - 1s - loss: 409.5574 - loglik: -4.0688e+02 - logprior: -2.6742e+00
Epoch 6/10
10/10 - 1s - loss: 406.8906 - loglik: -4.0458e+02 - logprior: -2.3092e+00
Epoch 7/10
10/10 - 1s - loss: 402.9854 - loglik: -4.0111e+02 - logprior: -1.8738e+00
Epoch 8/10
10/10 - 1s - loss: 401.6629 - loglik: -4.0005e+02 - logprior: -1.6091e+00
Epoch 9/10
10/10 - 1s - loss: 400.2251 - loglik: -3.9869e+02 - logprior: -1.5326e+00
Epoch 10/10
10/10 - 1s - loss: 399.8633 - loglik: -3.9834e+02 - logprior: -1.5162e+00
Fitted a model with MAP estimate = -399.2390
expansions: [(0, 2), (12, 1), (15, 3), (20, 1), (23, 1), (26, 1), (30, 1), (35, 2), (46, 1), (55, 3), (56, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 456.8445 - loglik: -4.0681e+02 - logprior: -5.0033e+01
Epoch 2/2
10/10 - 1s - loss: 408.3344 - loglik: -3.9318e+02 - logprior: -1.5157e+01
Fitted a model with MAP estimate = -399.7201
expansions: []
discards: [ 0 19 45]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 437.7575 - loglik: -3.9437e+02 - logprior: -4.3389e+01
Epoch 2/2
10/10 - 1s - loss: 408.4841 - loglik: -3.9147e+02 - logprior: -1.7011e+01
Fitted a model with MAP estimate = -403.8182
expansions: []
discards: [66]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 431.6703 - loglik: -3.9204e+02 - logprior: -3.9626e+01
Epoch 2/10
10/10 - 1s - loss: 401.0985 - loglik: -3.9027e+02 - logprior: -1.0828e+01
Epoch 3/10
10/10 - 1s - loss: 393.7411 - loglik: -3.8964e+02 - logprior: -4.1021e+00
Epoch 4/10
10/10 - 1s - loss: 390.7177 - loglik: -3.8876e+02 - logprior: -1.9598e+00
Epoch 5/10
10/10 - 1s - loss: 389.0477 - loglik: -3.8817e+02 - logprior: -8.7331e-01
Epoch 6/10
10/10 - 1s - loss: 387.3922 - loglik: -3.8732e+02 - logprior: -7.2175e-02
Epoch 7/10
10/10 - 1s - loss: 385.6628 - loglik: -3.8598e+02 - logprior: 0.3214
Epoch 8/10
10/10 - 1s - loss: 384.9811 - loglik: -3.8543e+02 - logprior: 0.4518
Epoch 9/10
10/10 - 1s - loss: 383.9694 - loglik: -3.8456e+02 - logprior: 0.5890
Epoch 10/10
10/10 - 1s - loss: 383.8417 - loglik: -3.8457e+02 - logprior: 0.7324
Fitted a model with MAP estimate = -383.4125
Time for alignment: 44.3152
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 513.6421 - loglik: -4.7550e+02 - logprior: -3.8141e+01
Epoch 2/10
10/10 - 1s - loss: 459.7848 - loglik: -4.4970e+02 - logprior: -1.0087e+01
Epoch 3/10
10/10 - 1s - loss: 433.1513 - loglik: -4.2803e+02 - logprior: -5.1249e+00
Epoch 4/10
10/10 - 1s - loss: 419.1133 - loglik: -4.1581e+02 - logprior: -3.3058e+00
Epoch 5/10
10/10 - 1s - loss: 411.4511 - loglik: -4.0900e+02 - logprior: -2.4547e+00
Epoch 6/10
10/10 - 1s - loss: 406.4352 - loglik: -4.0424e+02 - logprior: -2.1905e+00
Epoch 7/10
10/10 - 1s - loss: 403.9892 - loglik: -4.0214e+02 - logprior: -1.8477e+00
Epoch 8/10
10/10 - 1s - loss: 402.1551 - loglik: -4.0053e+02 - logprior: -1.6218e+00
Epoch 9/10
10/10 - 1s - loss: 400.8737 - loglik: -3.9930e+02 - logprior: -1.5751e+00
Epoch 10/10
10/10 - 1s - loss: 400.2141 - loglik: -3.9864e+02 - logprior: -1.5747e+00
Fitted a model with MAP estimate = -399.6004
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (24, 3), (25, 1), (38, 1), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 456.4392 - loglik: -4.0643e+02 - logprior: -5.0010e+01
Epoch 2/2
10/10 - 1s - loss: 407.1672 - loglik: -3.9201e+02 - logprior: -1.5153e+01
Fitted a model with MAP estimate = -399.1147
expansions: []
discards: [ 0 18 26 70]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 436.9755 - loglik: -3.9369e+02 - logprior: -4.3287e+01
Epoch 2/2
10/10 - 1s - loss: 407.9724 - loglik: -3.9114e+02 - logprior: -1.6837e+01
Fitted a model with MAP estimate = -402.9963
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 430.9090 - loglik: -3.9132e+02 - logprior: -3.9588e+01
Epoch 2/10
10/10 - 1s - loss: 400.6473 - loglik: -3.8985e+02 - logprior: -1.0800e+01
Epoch 3/10
10/10 - 1s - loss: 392.9083 - loglik: -3.8890e+02 - logprior: -4.0113e+00
Epoch 4/10
10/10 - 1s - loss: 390.5103 - loglik: -3.8864e+02 - logprior: -1.8671e+00
Epoch 5/10
10/10 - 1s - loss: 387.9866 - loglik: -3.8720e+02 - logprior: -7.8196e-01
Epoch 6/10
10/10 - 1s - loss: 386.5143 - loglik: -3.8653e+02 - logprior: 0.0202
Epoch 7/10
10/10 - 1s - loss: 385.2718 - loglik: -3.8567e+02 - logprior: 0.4020
Epoch 8/10
10/10 - 1s - loss: 383.8577 - loglik: -3.8439e+02 - logprior: 0.5347
Epoch 9/10
10/10 - 1s - loss: 383.7766 - loglik: -3.8444e+02 - logprior: 0.6680
Epoch 10/10
10/10 - 1s - loss: 382.7589 - loglik: -3.8357e+02 - logprior: 0.8145
Fitted a model with MAP estimate = -382.5113
Time for alignment: 42.5038
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 513.4824 - loglik: -4.7534e+02 - logprior: -3.8143e+01
Epoch 2/10
10/10 - 1s - loss: 460.4995 - loglik: -4.5041e+02 - logprior: -1.0086e+01
Epoch 3/10
10/10 - 1s - loss: 433.0274 - loglik: -4.2789e+02 - logprior: -5.1389e+00
Epoch 4/10
10/10 - 1s - loss: 417.4632 - loglik: -4.1405e+02 - logprior: -3.4138e+00
Epoch 5/10
10/10 - 1s - loss: 410.2443 - loglik: -4.0759e+02 - logprior: -2.6549e+00
Epoch 6/10
10/10 - 1s - loss: 406.0982 - loglik: -4.0373e+02 - logprior: -2.3683e+00
Epoch 7/10
10/10 - 1s - loss: 403.9190 - loglik: -4.0195e+02 - logprior: -1.9633e+00
Epoch 8/10
10/10 - 1s - loss: 402.1896 - loglik: -4.0051e+02 - logprior: -1.6781e+00
Epoch 9/10
10/10 - 1s - loss: 400.9611 - loglik: -3.9935e+02 - logprior: -1.6101e+00
Epoch 10/10
10/10 - 1s - loss: 400.3994 - loglik: -3.9879e+02 - logprior: -1.6072e+00
Fitted a model with MAP estimate = -400.0414
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (26, 1), (30, 1), (38, 1), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 457.3813 - loglik: -4.0725e+02 - logprior: -5.0131e+01
Epoch 2/2
10/10 - 1s - loss: 409.2127 - loglik: -3.9407e+02 - logprior: -1.5140e+01
Fitted a model with MAP estimate = -400.8617
expansions: [(33, 1)]
discards: [ 0 18 26 68]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 438.9048 - loglik: -3.9546e+02 - logprior: -4.3448e+01
Epoch 2/2
10/10 - 1s - loss: 409.8516 - loglik: -3.9293e+02 - logprior: -1.6921e+01
Fitted a model with MAP estimate = -404.6900
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 433.4730 - loglik: -3.9386e+02 - logprior: -3.9611e+01
Epoch 2/10
10/10 - 1s - loss: 400.6822 - loglik: -3.8987e+02 - logprior: -1.0813e+01
Epoch 3/10
10/10 - 1s - loss: 393.5814 - loglik: -3.8953e+02 - logprior: -4.0538e+00
Epoch 4/10
10/10 - 1s - loss: 390.5541 - loglik: -3.8863e+02 - logprior: -1.9201e+00
Epoch 5/10
10/10 - 1s - loss: 388.9690 - loglik: -3.8814e+02 - logprior: -8.2849e-01
Epoch 6/10
10/10 - 1s - loss: 387.1796 - loglik: -3.8715e+02 - logprior: -2.8014e-02
Epoch 7/10
10/10 - 1s - loss: 385.8246 - loglik: -3.8619e+02 - logprior: 0.3645
Epoch 8/10
10/10 - 1s - loss: 384.7357 - loglik: -3.8522e+02 - logprior: 0.4879
Epoch 9/10
10/10 - 1s - loss: 383.5573 - loglik: -3.8417e+02 - logprior: 0.6193
Epoch 10/10
10/10 - 1s - loss: 383.9238 - loglik: -3.8469e+02 - logprior: 0.7661
Fitted a model with MAP estimate = -383.0523
Time for alignment: 43.2974
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 513.8599 - loglik: -4.7572e+02 - logprior: -3.8144e+01
Epoch 2/10
10/10 - 1s - loss: 459.9831 - loglik: -4.4990e+02 - logprior: -1.0084e+01
Epoch 3/10
10/10 - 1s - loss: 433.5804 - loglik: -4.2848e+02 - logprior: -5.1037e+00
Epoch 4/10
10/10 - 1s - loss: 418.2232 - loglik: -4.1483e+02 - logprior: -3.3927e+00
Epoch 5/10
10/10 - 1s - loss: 410.3178 - loglik: -4.0774e+02 - logprior: -2.5758e+00
Epoch 6/10
10/10 - 1s - loss: 405.7532 - loglik: -4.0346e+02 - logprior: -2.2954e+00
Epoch 7/10
10/10 - 1s - loss: 403.3230 - loglik: -4.0141e+02 - logprior: -1.9108e+00
Epoch 8/10
10/10 - 1s - loss: 401.4764 - loglik: -3.9987e+02 - logprior: -1.6028e+00
Epoch 9/10
10/10 - 1s - loss: 400.0217 - loglik: -3.9849e+02 - logprior: -1.5255e+00
Epoch 10/10
10/10 - 1s - loss: 399.6506 - loglik: -3.9810e+02 - logprior: -1.5435e+00
Fitted a model with MAP estimate = -399.1744
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (30, 1), (38, 1), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 457.5285 - loglik: -4.0737e+02 - logprior: -5.0156e+01
Epoch 2/2
10/10 - 1s - loss: 409.1322 - loglik: -3.9402e+02 - logprior: -1.5115e+01
Fitted a model with MAP estimate = -400.9098
expansions: [(32, 3)]
discards: [ 0 19 68]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 438.6782 - loglik: -3.9529e+02 - logprior: -4.3385e+01
Epoch 2/2
10/10 - 1s - loss: 408.6289 - loglik: -3.9175e+02 - logprior: -1.6880e+01
Fitted a model with MAP estimate = -403.6072
expansions: []
discards: [25]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 431.3135 - loglik: -3.9179e+02 - logprior: -3.9528e+01
Epoch 2/10
10/10 - 1s - loss: 400.7149 - loglik: -3.8998e+02 - logprior: -1.0731e+01
Epoch 3/10
10/10 - 1s - loss: 392.7547 - loglik: -3.8875e+02 - logprior: -4.0049e+00
Epoch 4/10
10/10 - 1s - loss: 390.0373 - loglik: -3.8817e+02 - logprior: -1.8718e+00
Epoch 5/10
10/10 - 1s - loss: 388.4032 - loglik: -3.8762e+02 - logprior: -7.8300e-01
Epoch 6/10
10/10 - 1s - loss: 386.7969 - loglik: -3.8682e+02 - logprior: 0.0218
Epoch 7/10
10/10 - 1s - loss: 384.9611 - loglik: -3.8537e+02 - logprior: 0.4088
Epoch 8/10
10/10 - 1s - loss: 384.5966 - loglik: -3.8513e+02 - logprior: 0.5364
Epoch 9/10
10/10 - 1s - loss: 383.2091 - loglik: -3.8388e+02 - logprior: 0.6689
Epoch 10/10
10/10 - 1s - loss: 382.9679 - loglik: -3.8378e+02 - logprior: 0.8132
Fitted a model with MAP estimate = -382.6538
Time for alignment: 43.4538
Fitting a model of length 65 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 513.5094 - loglik: -4.7537e+02 - logprior: -3.8143e+01
Epoch 2/10
10/10 - 1s - loss: 460.2487 - loglik: -4.5017e+02 - logprior: -1.0081e+01
Epoch 3/10
10/10 - 1s - loss: 433.0838 - loglik: -4.2797e+02 - logprior: -5.1169e+00
Epoch 4/10
10/10 - 1s - loss: 416.5631 - loglik: -4.1311e+02 - logprior: -3.4521e+00
Epoch 5/10
10/10 - 1s - loss: 409.7709 - loglik: -4.0709e+02 - logprior: -2.6804e+00
Epoch 6/10
10/10 - 1s - loss: 405.7774 - loglik: -4.0342e+02 - logprior: -2.3592e+00
Epoch 7/10
10/10 - 1s - loss: 403.9055 - loglik: -4.0194e+02 - logprior: -1.9611e+00
Epoch 8/10
10/10 - 1s - loss: 402.1554 - loglik: -4.0048e+02 - logprior: -1.6697e+00
Epoch 9/10
10/10 - 1s - loss: 399.9632 - loglik: -3.9836e+02 - logprior: -1.5982e+00
Epoch 10/10
10/10 - 1s - loss: 400.1108 - loglik: -3.9849e+02 - logprior: -1.6156e+00
Fitted a model with MAP estimate = -399.4866
expansions: [(0, 2), (12, 1), (15, 2), (18, 1), (20, 2), (24, 2), (26, 1), (30, 1), (32, 1), (46, 1), (55, 3), (58, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 457.3550 - loglik: -4.0731e+02 - logprior: -5.0048e+01
Epoch 2/2
10/10 - 1s - loss: 408.3136 - loglik: -3.9315e+02 - logprior: -1.5162e+01
Fitted a model with MAP estimate = -399.6558
expansions: []
discards: [ 0 19 27 34 70]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 438.7387 - loglik: -3.9541e+02 - logprior: -4.3324e+01
Epoch 2/2
10/10 - 1s - loss: 409.0698 - loglik: -3.9213e+02 - logprior: -1.6940e+01
Fitted a model with MAP estimate = -404.1065
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1160 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 432.2875 - loglik: -3.9263e+02 - logprior: -3.9656e+01
Epoch 2/10
10/10 - 1s - loss: 400.7099 - loglik: -3.8985e+02 - logprior: -1.0858e+01
Epoch 3/10
10/10 - 1s - loss: 393.5858 - loglik: -3.8952e+02 - logprior: -4.0679e+00
Epoch 4/10
10/10 - 1s - loss: 390.6622 - loglik: -3.8874e+02 - logprior: -1.9184e+00
Epoch 5/10
10/10 - 1s - loss: 388.7270 - loglik: -3.8790e+02 - logprior: -8.2704e-01
Epoch 6/10
10/10 - 1s - loss: 387.3685 - loglik: -3.8734e+02 - logprior: -2.8438e-02
Epoch 7/10
10/10 - 1s - loss: 385.9944 - loglik: -3.8635e+02 - logprior: 0.3619
Epoch 8/10
10/10 - 1s - loss: 384.2421 - loglik: -3.8474e+02 - logprior: 0.4987
Epoch 9/10
10/10 - 1s - loss: 384.3160 - loglik: -3.8495e+02 - logprior: 0.6339
Fitted a model with MAP estimate = -383.7166
Time for alignment: 40.9255
Computed alignments with likelihoods: ['-383.4125', '-382.5113', '-383.0523', '-382.6538', '-383.7166']
Best model has likelihood: -382.5113  (prior= 0.8800 )
time for generating output: 0.1725
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cryst.projection.fasta
SP score = 0.9242723933314496
Training of 5 independent models on file Ald_Xan_dh_2.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cdc559d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bbcf3d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ab3ed220>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 609.4617 - loglik: -5.9848e+02 - logprior: -1.0980e+01
Epoch 2/10
19/19 - 4s - loss: 543.4761 - loglik: -5.4132e+02 - logprior: -2.1546e+00
Epoch 3/10
19/19 - 5s - loss: 527.5370 - loglik: -5.2579e+02 - logprior: -1.7436e+00
Epoch 4/10
19/19 - 6s - loss: 522.8426 - loglik: -5.2138e+02 - logprior: -1.4618e+00
Epoch 5/10
19/19 - 4s - loss: 516.7195 - loglik: -5.1544e+02 - logprior: -1.2741e+00
Epoch 6/10
19/19 - 6s - loss: 518.6528 - loglik: -5.1739e+02 - logprior: -1.2593e+00
Fitted a model with MAP estimate = -516.1475
expansions: [(10, 3), (11, 2), (12, 2), (27, 2), (29, 1), (43, 1), (47, 3), (48, 5), (49, 3), (56, 1), (71, 1), (72, 3), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 531.9860 - loglik: -5.1845e+02 - logprior: -1.3534e+01
Epoch 2/2
19/19 - 5s - loss: 511.7780 - loglik: -5.0738e+02 - logprior: -4.4015e+00
Fitted a model with MAP estimate = -507.4023
expansions: [(0, 2)]
discards: [ 0 13 14 16 57 58 59]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 517.1987 - loglik: -5.0782e+02 - logprior: -9.3774e+00
Epoch 2/2
19/19 - 5s - loss: 505.2065 - loglik: -5.0399e+02 - logprior: -1.2160e+00
Fitted a model with MAP estimate = -504.7392
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 519.1542 - loglik: -5.0784e+02 - logprior: -1.1313e+01
Epoch 2/10
19/19 - 5s - loss: 508.3677 - loglik: -5.0679e+02 - logprior: -1.5733e+00
Epoch 3/10
19/19 - 4s - loss: 503.8353 - loglik: -5.0357e+02 - logprior: -2.6237e-01
Epoch 4/10
19/19 - 5s - loss: 503.3486 - loglik: -5.0349e+02 - logprior: 0.1389
Epoch 5/10
19/19 - 5s - loss: 500.3412 - loglik: -5.0064e+02 - logprior: 0.3030
Epoch 6/10
19/19 - 6s - loss: 500.4957 - loglik: -5.0088e+02 - logprior: 0.3850
Fitted a model with MAP estimate = -497.9832
Time for alignment: 107.9909
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 610.2866 - loglik: -5.9931e+02 - logprior: -1.0975e+01
Epoch 2/10
19/19 - 5s - loss: 546.7632 - loglik: -5.4461e+02 - logprior: -2.1567e+00
Epoch 3/10
19/19 - 5s - loss: 527.1896 - loglik: -5.2546e+02 - logprior: -1.7327e+00
Epoch 4/10
19/19 - 6s - loss: 522.1403 - loglik: -5.2070e+02 - logprior: -1.4364e+00
Epoch 5/10
19/19 - 5s - loss: 520.6675 - loglik: -5.1942e+02 - logprior: -1.2404e+00
Epoch 6/10
19/19 - 5s - loss: 518.3616 - loglik: -5.1710e+02 - logprior: -1.2609e+00
Epoch 7/10
19/19 - 4s - loss: 514.5904 - loglik: -5.1332e+02 - logprior: -1.2653e+00
Epoch 8/10
19/19 - 4s - loss: 514.9881 - loglik: -5.1371e+02 - logprior: -1.2724e+00
Fitted a model with MAP estimate = -512.9695
expansions: [(10, 3), (11, 2), (12, 2), (27, 2), (29, 1), (46, 1), (47, 5), (48, 3), (49, 1), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 536.8354 - loglik: -5.2325e+02 - logprior: -1.3590e+01
Epoch 2/2
19/19 - 4s - loss: 512.0056 - loglik: -5.0749e+02 - logprior: -4.5123e+00
Fitted a model with MAP estimate = -508.5348
expansions: [(0, 2)]
discards: [ 0 14 16 63 64]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 515.3187 - loglik: -5.0590e+02 - logprior: -9.4203e+00
Epoch 2/2
19/19 - 5s - loss: 508.1758 - loglik: -5.0690e+02 - logprior: -1.2735e+00
Fitted a model with MAP estimate = -504.7562
expansions: []
discards: [ 0 14 62]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 519.7676 - loglik: -5.0849e+02 - logprior: -1.1282e+01
Epoch 2/10
19/19 - 4s - loss: 505.9413 - loglik: -5.0437e+02 - logprior: -1.5696e+00
Epoch 3/10
19/19 - 5s - loss: 506.1639 - loglik: -5.0590e+02 - logprior: -2.6245e-01
Fitted a model with MAP estimate = -503.9051
Time for alignment: 99.2432
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 608.8045 - loglik: -5.9783e+02 - logprior: -1.0975e+01
Epoch 2/10
19/19 - 6s - loss: 549.4842 - loglik: -5.4725e+02 - logprior: -2.2380e+00
Epoch 3/10
19/19 - 6s - loss: 529.3403 - loglik: -5.2736e+02 - logprior: -1.9750e+00
Epoch 4/10
19/19 - 5s - loss: 522.2440 - loglik: -5.2049e+02 - logprior: -1.7500e+00
Epoch 5/10
19/19 - 5s - loss: 517.6934 - loglik: -5.1614e+02 - logprior: -1.5486e+00
Epoch 6/10
19/19 - 5s - loss: 519.5069 - loglik: -5.1795e+02 - logprior: -1.5576e+00
Fitted a model with MAP estimate = -517.2230
expansions: [(7, 1), (9, 3), (10, 2), (11, 2), (30, 1), (31, 1), (41, 1), (42, 1), (47, 1), (48, 3), (49, 1), (50, 1), (57, 1), (72, 1), (73, 3), (74, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 532.8810 - loglik: -5.1934e+02 - logprior: -1.3543e+01
Epoch 2/2
19/19 - 5s - loss: 510.5905 - loglik: -5.0626e+02 - logprior: -4.3319e+00
Fitted a model with MAP estimate = -507.7519
expansions: [(0, 2)]
discards: [ 0 14 16 61]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 516.6662 - loglik: -5.0727e+02 - logprior: -9.3968e+00
Epoch 2/2
19/19 - 6s - loss: 506.5219 - loglik: -5.0527e+02 - logprior: -1.2561e+00
Fitted a model with MAP estimate = -504.7961
expansions: []
discards: [ 0 14]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 520.3575 - loglik: -5.0913e+02 - logprior: -1.1231e+01
Epoch 2/10
19/19 - 5s - loss: 506.3047 - loglik: -5.0473e+02 - logprior: -1.5728e+00
Epoch 3/10
19/19 - 4s - loss: 504.3455 - loglik: -5.0410e+02 - logprior: -2.4952e-01
Epoch 4/10
19/19 - 5s - loss: 502.3358 - loglik: -5.0246e+02 - logprior: 0.1297
Epoch 5/10
19/19 - 6s - loss: 501.9127 - loglik: -5.0222e+02 - logprior: 0.3049
Epoch 6/10
19/19 - 5s - loss: 499.0434 - loglik: -4.9943e+02 - logprior: 0.3864
Epoch 7/10
19/19 - 5s - loss: 496.9272 - loglik: -4.9732e+02 - logprior: 0.3985
Epoch 8/10
19/19 - 4s - loss: 491.4119 - loglik: -4.9181e+02 - logprior: 0.3998
Epoch 9/10
19/19 - 5s - loss: 481.4101 - loglik: -4.8174e+02 - logprior: 0.3384
Epoch 10/10
19/19 - 5s - loss: 468.4823 - loglik: -4.6867e+02 - logprior: 0.2020
Fitted a model with MAP estimate = -460.1980
Time for alignment: 125.9186
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 608.6959 - loglik: -5.9772e+02 - logprior: -1.0976e+01
Epoch 2/10
19/19 - 5s - loss: 549.5800 - loglik: -5.4732e+02 - logprior: -2.2562e+00
Epoch 3/10
19/19 - 5s - loss: 526.3370 - loglik: -5.2432e+02 - logprior: -2.0152e+00
Epoch 4/10
19/19 - 5s - loss: 517.4908 - loglik: -5.1561e+02 - logprior: -1.8801e+00
Epoch 5/10
19/19 - 4s - loss: 517.7813 - loglik: -5.1603e+02 - logprior: -1.7445e+00
Fitted a model with MAP estimate = -516.1045
expansions: [(10, 3), (11, 2), (12, 2), (15, 1), (26, 1), (29, 1), (43, 1), (47, 3), (48, 5), (49, 2), (57, 1), (60, 1), (71, 1), (72, 1), (73, 1), (74, 1), (76, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 531.4006 - loglik: -5.1779e+02 - logprior: -1.3614e+01
Epoch 2/2
19/19 - 4s - loss: 509.4354 - loglik: -5.0509e+02 - logprior: -4.3403e+00
Fitted a model with MAP estimate = -507.3699
expansions: [(0, 2)]
discards: [ 0 13 14 16 60 67]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 517.6285 - loglik: -5.0825e+02 - logprior: -9.3773e+00
Epoch 2/2
19/19 - 5s - loss: 505.6512 - loglik: -5.0442e+02 - logprior: -1.2334e+00
Fitted a model with MAP estimate = -504.4842
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 519.5220 - loglik: -5.0834e+02 - logprior: -1.1181e+01
Epoch 2/10
19/19 - 6s - loss: 507.2061 - loglik: -5.0562e+02 - logprior: -1.5887e+00
Epoch 3/10
19/19 - 4s - loss: 501.9772 - loglik: -5.0172e+02 - logprior: -2.5995e-01
Epoch 4/10
19/19 - 5s - loss: 503.5611 - loglik: -5.0366e+02 - logprior: 0.1012
Fitted a model with MAP estimate = -501.5855
Time for alignment: 88.5863
Fitting a model of length 87 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 610.0643 - loglik: -5.9909e+02 - logprior: -1.0979e+01
Epoch 2/10
19/19 - 5s - loss: 541.5168 - loglik: -5.3935e+02 - logprior: -2.1682e+00
Epoch 3/10
19/19 - 6s - loss: 527.6997 - loglik: -5.2593e+02 - logprior: -1.7707e+00
Epoch 4/10
19/19 - 5s - loss: 519.8166 - loglik: -5.1828e+02 - logprior: -1.5385e+00
Epoch 5/10
19/19 - 4s - loss: 518.5159 - loglik: -5.1717e+02 - logprior: -1.3486e+00
Epoch 6/10
19/19 - 6s - loss: 517.1401 - loglik: -5.1578e+02 - logprior: -1.3574e+00
Epoch 7/10
19/19 - 4s - loss: 512.9731 - loglik: -5.1159e+02 - logprior: -1.3813e+00
Epoch 8/10
19/19 - 5s - loss: 513.9329 - loglik: -5.1249e+02 - logprior: -1.4358e+00
Fitted a model with MAP estimate = -511.9596
expansions: [(10, 3), (11, 2), (12, 2), (27, 1), (28, 1), (29, 1), (46, 1), (47, 5), (48, 3), (49, 2), (50, 2), (56, 1), (71, 1), (72, 3), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 535.8047 - loglik: -5.2217e+02 - logprior: -1.3638e+01
Epoch 2/2
19/19 - 5s - loss: 512.4386 - loglik: -5.0786e+02 - logprior: -4.5750e+00
Fitted a model with MAP estimate = -508.2688
expansions: [(0, 2)]
discards: [ 0 14 16 63 64 65 68 70]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 515.3351 - loglik: -5.0595e+02 - logprior: -9.3823e+00
Epoch 2/2
19/19 - 5s - loss: 508.0066 - loglik: -5.0677e+02 - logprior: -1.2324e+00
Fitted a model with MAP estimate = -504.8645
expansions: []
discards: [ 0 14]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 2589 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 520.3050 - loglik: -5.0905e+02 - logprior: -1.1255e+01
Epoch 2/10
19/19 - 4s - loss: 505.5546 - loglik: -5.0399e+02 - logprior: -1.5625e+00
Epoch 3/10
19/19 - 5s - loss: 505.5729 - loglik: -5.0532e+02 - logprior: -2.4998e-01
Fitted a model with MAP estimate = -503.8776
Time for alignment: 97.5897
Computed alignments with likelihoods: ['-497.9832', '-503.9051', '-460.1980', '-501.5855', '-503.8776']
Best model has likelihood: -460.1980  (prior= 0.1318 )
time for generating output: 0.5219
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Ald_Xan_dh_2.projection.fasta
SP score = 0.05595193390912505
Training of 5 independent models on file ace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efa57ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c931700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea559aab80>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 46s - loss: 2333.8213 - loglik: -2.3325e+03 - logprior: -1.3483e+00
Epoch 2/10
49/49 - 41s - loss: 2208.5469 - loglik: -2.2090e+03 - logprior: 0.5029
Epoch 3/10
49/49 - 41s - loss: 2189.8027 - loglik: -2.1903e+03 - logprior: 0.5242
Epoch 4/10
49/49 - 41s - loss: 2180.8660 - loglik: -2.1815e+03 - logprior: 0.6003
Epoch 5/10
49/49 - 41s - loss: 2173.5950 - loglik: -2.1741e+03 - logprior: 0.4677
Epoch 6/10
49/49 - 41s - loss: 2081.2632 - loglik: -2.0812e+03 - logprior: -1.4971e-02
Epoch 7/10
49/49 - 41s - loss: 1796.9309 - loglik: -1.7862e+03 - logprior: -1.0757e+01
Epoch 8/10
49/49 - 41s - loss: 1745.3071 - loglik: -1.7382e+03 - logprior: -7.0374e+00
Epoch 9/10
49/49 - 41s - loss: 1749.1689 - loglik: -1.7389e+03 - logprior: -1.0215e+01
Fitted a model with MAP estimate = -1743.5409
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 384]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 15s - loss: 2566.0369 - loglik: -2.5593e+03 - logprior: -6.7729e+00
Epoch 2/2
24/24 - 12s - loss: 2496.3645 - loglik: -2.4972e+03 - logprior: 0.8778
Fitted a model with MAP estimate = -2502.2781
expansions: [(0, 173), (178, 240)]
discards: [ 15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32
  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50
  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68
  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86
  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104
 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122
 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140
 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158
 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176
 177]
Re-initialized the encoder parameters.
Fitting a model of length 428 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 48s - loss: 2316.7700 - loglik: -2.3134e+03 - logprior: -3.3933e+00
Epoch 2/2
49/49 - 45s - loss: 2187.1809 - loglik: -2.1877e+03 - logprior: 0.5287
Fitted a model with MAP estimate = -2181.6959
expansions: [(0, 5), (29, 3), (31, 1), (74, 2), (80, 1), (95, 2), (117, 1), (118, 1), (122, 1), (125, 1), (147, 2), (148, 1), (220, 1), (222, 1), (223, 1), (238, 1), (239, 1), (254, 3), (255, 2), (256, 2), (278, 1), (279, 1), (282, 1), (303, 1), (304, 5), (307, 1), (310, 3), (312, 4), (313, 1), (328, 4), (329, 1), (330, 1), (338, 1), (340, 1), (341, 2), (361, 3), (363, 1), (380, 2), (381, 3), (383, 1), (386, 1), (389, 1), (414, 5), (419, 5)]
discards: [  2   3   4   5   6   7   8   9  10  11  12 422 423 424 425 426 427]
Re-initialized the encoder parameters.
Fitting a model of length 494 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 61s - loss: 2181.8853 - loglik: -2.1787e+03 - logprior: -3.1571e+00
Epoch 2/10
49/49 - 57s - loss: 2164.4089 - loglik: -2.1664e+03 - logprior: 1.9806
Epoch 3/10
49/49 - 57s - loss: 2160.3638 - loglik: -2.1626e+03 - logprior: 2.2549
Epoch 4/10
49/49 - 57s - loss: 2164.5266 - loglik: -2.1671e+03 - logprior: 2.5713
Fitted a model with MAP estimate = -2147.8439
Time for alignment: 833.1385
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 2340.3989 - loglik: -2.3391e+03 - logprior: -1.3339e+00
Epoch 2/10
49/49 - 41s - loss: 2199.2705 - loglik: -2.1997e+03 - logprior: 0.4552
Epoch 3/10
49/49 - 41s - loss: 2195.0837 - loglik: -2.1957e+03 - logprior: 0.5996
Epoch 4/10
49/49 - 41s - loss: 2186.5496 - loglik: -2.1872e+03 - logprior: 0.6577
Epoch 5/10
49/49 - 41s - loss: 2166.8975 - loglik: -2.1675e+03 - logprior: 0.6089
Epoch 6/10
49/49 - 41s - loss: 2083.0742 - loglik: -2.0826e+03 - logprior: -4.2988e-01
Epoch 7/10
49/49 - 41s - loss: 1788.3761 - loglik: -1.7764e+03 - logprior: -1.1917e+01
Epoch 8/10
49/49 - 41s - loss: 1752.1764 - loglik: -1.7432e+03 - logprior: -8.9499e+00
Epoch 9/10
49/49 - 41s - loss: 1734.5494 - loglik: -1.7283e+03 - logprior: -6.1887e+00
Epoch 10/10
49/49 - 41s - loss: 1744.9016 - loglik: -1.7375e+03 - logprior: -7.3825e+00
Fitted a model with MAP estimate = -1743.3148
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 272 314 385]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 15s - loss: 2551.7297 - loglik: -2.5452e+03 - logprior: -6.4932e+00
Epoch 2/2
24/24 - 12s - loss: 2511.8096 - loglik: -2.5127e+03 - logprior: 0.8414
Fitted a model with MAP estimate = -2501.2800
expansions: [(0, 136), (175, 275)]
discards: [ 13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30
  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48
  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66
  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84
  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102
 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120
 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138
 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156
 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174]
Re-initialized the encoder parameters.
Fitting a model of length 424 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 47s - loss: 2335.1216 - loglik: -2.3305e+03 - logprior: -4.6531e+00
Epoch 2/2
49/49 - 44s - loss: 2198.7200 - loglik: -2.1984e+03 - logprior: -3.3703e-01
Fitted a model with MAP estimate = -2194.6670
expansions: [(0, 6), (19, 1), (20, 1), (22, 1), (25, 2), (26, 3), (27, 2), (29, 1), (50, 1), (53, 1), (54, 4), (55, 2), (57, 1), (58, 1), (59, 3), (60, 5), (61, 5), (69, 3), (70, 9), (81, 1), (86, 1), (87, 1), (90, 2), (91, 1), (92, 2), (113, 2), (164, 1), (184, 2), (185, 1), (188, 1), (201, 2), (203, 1), (252, 1), (279, 3), (313, 1), (314, 1), (325, 1), (326, 1), (328, 1), (332, 1), (346, 1), (352, 1), (368, 2), (369, 3), (375, 1), (398, 1), (403, 1), (406, 1), (411, 1), (413, 1), (414, 1)]
discards: [  2   3   4   5   6   7   8   9  10  11  12 417 418 419 420 421 422 423]
Re-initialized the encoder parameters.
Fitting a model of length 500 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 62s - loss: 2192.6257 - loglik: -2.1890e+03 - logprior: -3.6339e+00
Epoch 2/10
49/49 - 57s - loss: 2163.9429 - loglik: -2.1663e+03 - logprior: 2.3687
Epoch 3/10
49/49 - 58s - loss: 2155.1001 - loglik: -2.1577e+03 - logprior: 2.6093
Epoch 4/10
49/49 - 57s - loss: 2158.3665 - loglik: -2.1613e+03 - logprior: 2.9564
Fitted a model with MAP estimate = -2145.6822
Time for alignment: 873.5141
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 2342.0598 - loglik: -2.3406e+03 - logprior: -1.4446e+00
Epoch 2/10
49/49 - 41s - loss: 2197.2793 - loglik: -2.1977e+03 - logprior: 0.3847
Epoch 3/10
49/49 - 41s - loss: 2198.8894 - loglik: -2.1992e+03 - logprior: 0.2648
Fitted a model with MAP estimate = -2188.8694
expansions: [(0, 5), (142, 1), (194, 1), (212, 1), (214, 3), (226, 1), (227, 1), (229, 4), (230, 4), (231, 3), (232, 1), (233, 1), (236, 2), (237, 2), (238, 1), (239, 1), (246, 1), (249, 1), (250, 4), (251, 2), (252, 1), (268, 1), (269, 2), (270, 3), (271, 1), (272, 2), (284, 1), (288, 2), (289, 2), (290, 1), (291, 3), (292, 1), (293, 3), (294, 2), (296, 6), (306, 1), (309, 4), (310, 1), (320, 1), (322, 1), (323, 1), (324, 1), (325, 1), (342, 5), (362, 2), (363, 4), (364, 1), (365, 1), (366, 1), (368, 1), (394, 8)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 33 34 92]
Re-initialized the encoder parameters.
Fitting a model of length 492 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 2181.4363 - loglik: -2.1796e+03 - logprior: -1.8820e+00
Epoch 2/2
49/49 - 56s - loss: 2167.8762 - loglik: -2.1700e+03 - logprior: 2.1619
Fitted a model with MAP estimate = -2163.0084
expansions: [(0, 6), (346, 1), (478, 1), (483, 5)]
discards: [  1   2   3   4   5   6   7   8  24 322 487 488 489 490 491]
Re-initialized the encoder parameters.
Fitting a model of length 490 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 2175.9614 - loglik: -2.1743e+03 - logprior: -1.6781e+00
Epoch 2/2
49/49 - 56s - loss: 2169.0442 - loglik: -2.1722e+03 - logprior: 3.1082
Fitted a model with MAP estimate = -2163.5001
expansions: [(0, 6), (326, 2), (338, 1), (483, 1), (484, 1), (490, 6)]
discards: [0 1 2 3 4 5 6]
Re-initialized the encoder parameters.
Fitting a model of length 500 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 61s - loss: 2166.5366 - loglik: -2.1661e+03 - logprior: -4.1122e-01
Epoch 2/10
49/49 - 58s - loss: 2168.3774 - loglik: -2.1721e+03 - logprior: 3.7135
Fitted a model with MAP estimate = -2159.6366
Time for alignment: 637.9408
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 44s - loss: 2337.1233 - loglik: -2.3358e+03 - logprior: -1.3586e+00
Epoch 2/10
49/49 - 41s - loss: 2210.7551 - loglik: -2.2109e+03 - logprior: 0.1409
Epoch 3/10
49/49 - 41s - loss: 2187.9636 - loglik: -2.1881e+03 - logprior: 0.0965
Epoch 4/10
49/49 - 41s - loss: 2173.4241 - loglik: -2.1736e+03 - logprior: 0.1407
Epoch 5/10
49/49 - 41s - loss: 2181.5730 - loglik: -2.1818e+03 - logprior: 0.2322
Fitted a model with MAP estimate = -2151.6872
expansions: [(0, 3), (132, 1), (142, 1), (164, 2), (183, 1), (211, 1), (213, 1), (214, 1), (228, 1), (231, 4), (232, 3), (233, 1), (234, 1), (236, 1), (242, 2), (244, 1), (245, 1), (250, 1), (252, 1), (253, 1), (254, 2), (255, 2), (256, 5), (257, 1), (273, 1), (274, 1), (275, 2), (276, 1), (277, 2), (283, 1), (295, 6), (296, 2), (299, 3), (302, 3), (303, 3), (306, 1), (311, 2), (312, 1), (313, 3), (314, 2), (315, 1), (317, 1), (323, 1), (324, 1), (325, 1), (326, 1), (345, 2), (347, 1), (366, 3), (393, 2), (394, 3), (398, 1), (399, 10), (400, 1), (404, 6)]
discards: [ 1  2  3  4  5 34 35]
Re-initialized the encoder parameters.
Fitting a model of length 505 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 62s - loss: 2211.7466 - loglik: -2.2091e+03 - logprior: -2.6283e+00
Epoch 2/2
49/49 - 58s - loss: 2167.0032 - loglik: -2.1683e+03 - logprior: 1.3291
Fitted a model with MAP estimate = -2165.2575
expansions: [(0, 5), (280, 1), (333, 1), (334, 2), (345, 1), (351, 1), (358, 1), (420, 3), (423, 1), (494, 4)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  14  32 104 162 246 257
 341 377 441 475 476 477 495 496 497 498 499 500 501 502 503 504]
Re-initialized the encoder parameters.
Fitting a model of length 491 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 59s - loss: 2170.8799 - loglik: -2.1693e+03 - logprior: -1.5381e+00
Epoch 2/2
49/49 - 56s - loss: 2176.4922 - loglik: -2.1801e+03 - logprior: 3.5672
Fitted a model with MAP estimate = -2163.0025
expansions: [(0, 6), (343, 1), (490, 2), (491, 6)]
discards: [  1   2   3   4   5   6   7   8 324 413]
Re-initialized the encoder parameters.
Fitting a model of length 496 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 60s - loss: 2172.9204 - loglik: -2.1716e+03 - logprior: -1.3422e+00
Epoch 2/10
49/49 - 56s - loss: 2163.8184 - loglik: -2.1677e+03 - logprior: 3.8793
Epoch 3/10
49/49 - 57s - loss: 2158.3215 - loglik: -2.1628e+03 - logprior: 4.4797
Epoch 4/10
49/49 - 57s - loss: 2144.7625 - loglik: -2.1499e+03 - logprior: 5.1481
Epoch 5/10
49/49 - 57s - loss: 2140.4966 - loglik: -2.1459e+03 - logprior: 5.4368
Epoch 6/10
49/49 - 57s - loss: 2002.0837 - loglik: -2.0070e+03 - logprior: 4.8953
Epoch 7/10
49/49 - 57s - loss: 1769.4323 - loglik: -1.7697e+03 - logprior: 0.3132
Epoch 8/10
49/49 - 57s - loss: 1729.8038 - loglik: -1.7285e+03 - logprior: -1.2504e+00
Epoch 9/10
49/49 - 57s - loss: 1729.6036 - loglik: -1.7309e+03 - logprior: 1.3360
Epoch 10/10
49/49 - 57s - loss: 1722.3507 - loglik: -1.7226e+03 - logprior: 0.3228
Fitted a model with MAP estimate = -1723.0810
Time for alignment: 1179.9874
Fitting a model of length 404 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 45s - loss: 2345.0012 - loglik: -2.3438e+03 - logprior: -1.1684e+00
Epoch 2/10
49/49 - 41s - loss: 2198.1790 - loglik: -2.1987e+03 - logprior: 0.4984
Epoch 3/10
49/49 - 41s - loss: 2187.9822 - loglik: -2.1885e+03 - logprior: 0.4908
Epoch 4/10
49/49 - 41s - loss: 2182.1924 - loglik: -2.1828e+03 - logprior: 0.6300
Epoch 5/10
49/49 - 41s - loss: 2166.4219 - loglik: -2.1671e+03 - logprior: 0.7165
Epoch 6/10
49/49 - 41s - loss: 2102.0547 - loglik: -2.1024e+03 - logprior: 0.3492
Epoch 7/10
49/49 - 41s - loss: 1793.9686 - loglik: -1.7853e+03 - logprior: -8.6791e+00
Epoch 8/10
49/49 - 41s - loss: 1748.9858 - loglik: -1.7413e+03 - logprior: -7.6489e+00
Epoch 9/10
49/49 - 41s - loss: 1746.0319 - loglik: -1.7359e+03 - logprior: -1.0080e+01
Epoch 10/10
49/49 - 41s - loss: 1740.5861 - loglik: -1.7336e+03 - logprior: -6.9938e+00
Fitted a model with MAP estimate = -1739.6447
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 274 350]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 3989 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 14s - loss: 2556.3577 - loglik: -2.5500e+03 - logprior: -6.3646e+00
Epoch 2/2
24/24 - 12s - loss: 2509.2507 - loglik: -2.5096e+03 - logprior: 0.3235
Fitted a model with MAP estimate = -2498.8993
expansions: [(0, 165), (175, 233)]
discards: [ 27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44
  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62
  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80
  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98
  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116
 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134
 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152
 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170
 171 172 173 174]
Re-initialized the encoder parameters.
Fitting a model of length 425 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
49/49 - 48s - loss: 2327.9841 - loglik: -2.3242e+03 - logprior: -3.7607e+00
Epoch 2/2
49/49 - 44s - loss: 2188.3853 - loglik: -2.1884e+03 - logprior: 0.0620
Fitted a model with MAP estimate = -2184.1764
expansions: [(0, 5), (30, 1), (31, 3), (32, 2), (34, 1), (94, 2), (95, 1), (115, 1), (116, 1), (120, 1), (121, 1), (122, 2), (143, 2), (144, 1), (216, 1), (219, 1), (222, 1), (231, 1), (233, 1), (234, 1), (250, 3), (251, 1), (252, 1), (273, 1), (274, 1), (277, 1), (299, 1), (300, 2), (304, 2), (308, 4), (322, 1), (323, 1), (325, 4), (326, 2), (327, 2), (328, 1), (336, 1), (337, 1), (338, 1), (357, 2), (358, 1), (360, 1), (377, 3), (378, 3), (380, 1), (383, 1), (385, 1), (406, 1), (411, 2), (412, 2), (414, 1), (416, 5)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13 419 420 421 422 423 424]
Re-initialized the encoder parameters.
Fitting a model of length 492 on 3989 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
49/49 - 59s - loss: 2174.5620 - loglik: -2.1713e+03 - logprior: -3.2880e+00
Epoch 2/10
49/49 - 56s - loss: 2183.1663 - loglik: -2.1852e+03 - logprior: 2.0758
Fitted a model with MAP estimate = -2164.1953
Time for alignment: 755.1796
Computed alignments with likelihoods: ['-1743.5409', '-1743.3148', '-2159.6366', '-1723.0810', '-1739.6447']
Best model has likelihood: -1723.0810  (prior= 1.5105 )
time for generating output: 0.4693
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ace.projection.fasta
SP score = 0.05334755934915977
Training of 5 independent models on file tRNA-synt_2b.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bc910d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea5594a760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b61d160>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1006.1078 - loglik: -1.0042e+03 - logprior: -1.9042e+00
Epoch 2/10
39/39 - 8s - loss: 939.9650 - loglik: -9.3845e+02 - logprior: -1.5184e+00
Epoch 3/10
39/39 - 8s - loss: 932.8897 - loglik: -9.3138e+02 - logprior: -1.5084e+00
Epoch 4/10
39/39 - 8s - loss: 930.6647 - loglik: -9.2915e+02 - logprior: -1.5079e+00
Epoch 5/10
39/39 - 8s - loss: 929.5072 - loglik: -9.2800e+02 - logprior: -1.5017e+00
Epoch 6/10
39/39 - 8s - loss: 928.0612 - loglik: -9.2656e+02 - logprior: -1.4972e+00
Epoch 7/10
39/39 - 8s - loss: 926.7920 - loglik: -9.2528e+02 - logprior: -1.5005e+00
Epoch 8/10
39/39 - 8s - loss: 925.0061 - loglik: -9.2350e+02 - logprior: -1.4917e+00
Epoch 9/10
39/39 - 8s - loss: 921.3914 - loglik: -9.1987e+02 - logprior: -1.4992e+00
Epoch 10/10
39/39 - 8s - loss: 916.9383 - loglik: -9.1536e+02 - logprior: -1.5482e+00
Fitted a model with MAP estimate = -907.0223
expansions: [(9, 1), (10, 2), (11, 2), (12, 2), (22, 1), (23, 2), (26, 2), (32, 1), (39, 2), (40, 1), (43, 1), (44, 1), (54, 1), (58, 2), (59, 1), (69, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (98, 2), (99, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (122, 1), (126, 1), (133, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 937.7642 - loglik: -9.3580e+02 - logprior: -1.9593e+00
Epoch 2/2
39/39 - 10s - loss: 910.7633 - loglik: -9.0987e+02 - logprior: -8.8869e-01
Fitted a model with MAP estimate = -898.0278
expansions: []
discards: [ 12  14  31  36  78  94 115 128 174]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 912.7614 - loglik: -9.1107e+02 - logprior: -1.6948e+00
Epoch 2/2
39/39 - 10s - loss: 910.1608 - loglik: -9.0962e+02 - logprior: -5.3848e-01
Fitted a model with MAP estimate = -898.4020
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 14s - loss: 902.0540 - loglik: -8.9981e+02 - logprior: -2.2400e+00
Epoch 2/10
41/41 - 10s - loss: 896.9988 - loglik: -8.9618e+02 - logprior: -8.2141e-01
Epoch 3/10
41/41 - 10s - loss: 896.0241 - loglik: -8.9576e+02 - logprior: -2.6423e-01
Epoch 4/10
41/41 - 10s - loss: 895.1229 - loglik: -8.9489e+02 - logprior: -2.2612e-01
Epoch 5/10
41/41 - 10s - loss: 893.5370 - loglik: -8.9338e+02 - logprior: -1.5056e-01
Epoch 6/10
41/41 - 10s - loss: 894.3922 - loglik: -8.9429e+02 - logprior: -9.0249e-02
Fitted a model with MAP estimate = -891.7310
Time for alignment: 243.6265
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1006.7270 - loglik: -1.0048e+03 - logprior: -1.9332e+00
Epoch 2/10
39/39 - 8s - loss: 942.1520 - loglik: -9.4062e+02 - logprior: -1.5355e+00
Epoch 3/10
39/39 - 8s - loss: 934.6169 - loglik: -9.3306e+02 - logprior: -1.5508e+00
Epoch 4/10
39/39 - 8s - loss: 932.9326 - loglik: -9.3140e+02 - logprior: -1.5284e+00
Epoch 5/10
39/39 - 8s - loss: 930.8116 - loglik: -9.2927e+02 - logprior: -1.5329e+00
Epoch 6/10
39/39 - 8s - loss: 929.8958 - loglik: -9.2837e+02 - logprior: -1.5185e+00
Epoch 7/10
39/39 - 8s - loss: 928.4889 - loglik: -9.2696e+02 - logprior: -1.5143e+00
Epoch 8/10
39/39 - 8s - loss: 927.1061 - loglik: -9.2556e+02 - logprior: -1.5286e+00
Epoch 9/10
39/39 - 8s - loss: 924.0261 - loglik: -9.2247e+02 - logprior: -1.5380e+00
Epoch 10/10
39/39 - 8s - loss: 919.6760 - loglik: -9.1810e+02 - logprior: -1.5472e+00
Fitted a model with MAP estimate = -908.7938
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (14, 2), (23, 1), (26, 2), (39, 2), (40, 2), (43, 1), (44, 1), (54, 1), (55, 1), (56, 2), (57, 1), (58, 2), (68, 2), (70, 2), (80, 1), (92, 1), (93, 1), (95, 2), (99, 1), (103, 2), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (122, 1), (126, 1), (133, 2)]
discards: [ 2 45]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 939.4843 - loglik: -9.3747e+02 - logprior: -2.0141e+00
Epoch 2/2
39/39 - 10s - loss: 915.5450 - loglik: -9.1456e+02 - logprior: -9.8299e-01
Fitted a model with MAP estimate = -903.7136
expansions: [(61, 1)]
discards: [ 13  20  35  52  73  78  91  95 106 124 137 175]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 917.4925 - loglik: -9.1572e+02 - logprior: -1.7757e+00
Epoch 2/2
39/39 - 10s - loss: 914.3035 - loglik: -9.1368e+02 - logprior: -6.2369e-01
Fitted a model with MAP estimate = -902.9475
expansions: [(2, 1), (98, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 14s - loss: 905.7974 - loglik: -9.0349e+02 - logprior: -2.3055e+00
Epoch 2/10
41/41 - 10s - loss: 900.5005 - loglik: -8.9959e+02 - logprior: -9.1033e-01
Epoch 3/10
41/41 - 10s - loss: 899.6030 - loglik: -8.9923e+02 - logprior: -3.7614e-01
Epoch 4/10
41/41 - 10s - loss: 897.6635 - loglik: -8.9738e+02 - logprior: -2.7531e-01
Epoch 5/10
41/41 - 10s - loss: 898.0194 - loglik: -8.9782e+02 - logprior: -1.9552e-01
Fitted a model with MAP estimate = -895.9072
Time for alignment: 234.7569
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1006.2958 - loglik: -1.0043e+03 - logprior: -1.9472e+00
Epoch 2/10
39/39 - 8s - loss: 939.5228 - loglik: -9.3796e+02 - logprior: -1.5622e+00
Epoch 3/10
39/39 - 8s - loss: 931.8358 - loglik: -9.3026e+02 - logprior: -1.5786e+00
Epoch 4/10
39/39 - 8s - loss: 929.9422 - loglik: -9.2837e+02 - logprior: -1.5706e+00
Epoch 5/10
39/39 - 8s - loss: 928.8111 - loglik: -9.2723e+02 - logprior: -1.5723e+00
Epoch 6/10
39/39 - 8s - loss: 928.2289 - loglik: -9.2666e+02 - logprior: -1.5658e+00
Epoch 7/10
39/39 - 8s - loss: 926.4539 - loglik: -9.2487e+02 - logprior: -1.5687e+00
Epoch 8/10
39/39 - 8s - loss: 924.3837 - loglik: -9.2278e+02 - logprior: -1.5839e+00
Epoch 9/10
39/39 - 8s - loss: 922.5864 - loglik: -9.2098e+02 - logprior: -1.5894e+00
Epoch 10/10
39/39 - 8s - loss: 918.2643 - loglik: -9.1663e+02 - logprior: -1.6067e+00
Fitted a model with MAP estimate = -907.4583
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (17, 1), (22, 2), (23, 1), (25, 1), (38, 1), (39, 2), (44, 1), (45, 1), (55, 1), (57, 1), (58, 2), (59, 1), (69, 1), (71, 2), (80, 1), (89, 2), (92, 1), (93, 1), (95, 1), (99, 2), (103, 2), (104, 1), (105, 1), (107, 1), (109, 1), (118, 1), (119, 1), (126, 2), (131, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 934.5596 - loglik: -9.3261e+02 - logprior: -1.9474e+00
Epoch 2/2
39/39 - 10s - loss: 911.6868 - loglik: -9.1081e+02 - logprior: -8.7390e-01
Fitted a model with MAP estimate = -898.5539
expansions: []
discards: [ 13  29  50  76  93 114 129 136 166]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 913.4977 - loglik: -9.1175e+02 - logprior: -1.7433e+00
Epoch 2/2
39/39 - 10s - loss: 910.5217 - loglik: -9.0993e+02 - logprior: -5.8888e-01
Fitted a model with MAP estimate = -898.1181
expansions: [(2, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 900.5229 - loglik: -8.9820e+02 - logprior: -2.3264e+00
Epoch 2/10
41/41 - 10s - loss: 897.4375 - loglik: -8.9649e+02 - logprior: -9.4975e-01
Epoch 3/10
41/41 - 10s - loss: 896.9149 - loglik: -8.9650e+02 - logprior: -4.1613e-01
Epoch 4/10
41/41 - 10s - loss: 896.5748 - loglik: -8.9623e+02 - logprior: -3.3821e-01
Epoch 5/10
41/41 - 10s - loss: 893.2692 - loglik: -8.9300e+02 - logprior: -2.6701e-01
Epoch 6/10
41/41 - 10s - loss: 891.0245 - loglik: -8.9083e+02 - logprior: -1.8858e-01
Epoch 7/10
41/41 - 10s - loss: 893.6406 - loglik: -8.9349e+02 - logprior: -1.4068e-01
Fitted a model with MAP estimate = -888.9018
Time for alignment: 253.6214
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1005.5653 - loglik: -1.0036e+03 - logprior: -1.9215e+00
Epoch 2/10
39/39 - 8s - loss: 941.3771 - loglik: -9.3985e+02 - logprior: -1.5283e+00
Epoch 3/10
39/39 - 8s - loss: 933.9672 - loglik: -9.3242e+02 - logprior: -1.5424e+00
Epoch 4/10
39/39 - 8s - loss: 932.8414 - loglik: -9.3135e+02 - logprior: -1.4896e+00
Epoch 5/10
39/39 - 8s - loss: 931.3964 - loglik: -9.2991e+02 - logprior: -1.4821e+00
Epoch 6/10
39/39 - 8s - loss: 929.9192 - loglik: -9.2844e+02 - logprior: -1.4670e+00
Epoch 7/10
39/39 - 8s - loss: 928.3055 - loglik: -9.2683e+02 - logprior: -1.4635e+00
Epoch 8/10
39/39 - 8s - loss: 926.0316 - loglik: -9.2454e+02 - logprior: -1.4774e+00
Epoch 9/10
39/39 - 8s - loss: 923.1603 - loglik: -9.2166e+02 - logprior: -1.4789e+00
Epoch 10/10
39/39 - 8s - loss: 914.2636 - loglik: -9.1273e+02 - logprior: -1.5062e+00
Fitted a model with MAP estimate = -904.0109
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 2), (22, 1), (23, 1), (33, 1), (39, 2), (40, 3), (43, 1), (44, 1), (55, 2), (56, 1), (57, 2), (58, 1), (71, 1), (80, 1), (89, 1), (92, 1), (96, 1), (98, 1), (99, 1), (103, 1), (104, 1), (105, 1), (106, 1), (109, 1), (118, 1), (119, 1), (126, 1), (133, 2)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 953.1526 - loglik: -9.5117e+02 - logprior: -1.9824e+00
Epoch 2/2
39/39 - 10s - loss: 917.1405 - loglik: -9.1621e+02 - logprior: -9.2531e-01
Fitted a model with MAP estimate = -903.5535
expansions: [(2, 1), (132, 1)]
discards: [  0  13  16  72 133 170]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 918.9429 - loglik: -9.1618e+02 - logprior: -2.7618e+00
Epoch 2/2
39/39 - 10s - loss: 913.7489 - loglik: -9.1257e+02 - logprior: -1.1804e+00
Fitted a model with MAP estimate = -902.1356
expansions: [(2, 1)]
discards: [ 0 47]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 904.3323 - loglik: -9.0224e+02 - logprior: -2.0911e+00
Epoch 2/10
41/41 - 10s - loss: 901.5558 - loglik: -9.0099e+02 - logprior: -5.6798e-01
Epoch 3/10
41/41 - 10s - loss: 899.3231 - loglik: -8.9882e+02 - logprior: -5.0622e-01
Epoch 4/10
41/41 - 10s - loss: 899.9056 - loglik: -8.9948e+02 - logprior: -4.2320e-01
Fitted a model with MAP estimate = -898.1772
Time for alignment: 223.7133
Fitting a model of length 136 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1006.0436 - loglik: -1.0041e+03 - logprior: -1.9308e+00
Epoch 2/10
39/39 - 8s - loss: 942.1974 - loglik: -9.4073e+02 - logprior: -1.4706e+00
Epoch 3/10
39/39 - 8s - loss: 934.3694 - loglik: -9.3290e+02 - logprior: -1.4700e+00
Epoch 4/10
39/39 - 8s - loss: 933.1121 - loglik: -9.3166e+02 - logprior: -1.4473e+00
Epoch 5/10
39/39 - 8s - loss: 931.4902 - loglik: -9.3005e+02 - logprior: -1.4361e+00
Epoch 6/10
39/39 - 8s - loss: 929.6966 - loglik: -9.2826e+02 - logprior: -1.4329e+00
Epoch 7/10
39/39 - 8s - loss: 928.0363 - loglik: -9.2658e+02 - logprior: -1.4489e+00
Epoch 8/10
39/39 - 8s - loss: 926.2955 - loglik: -9.2482e+02 - logprior: -1.4629e+00
Epoch 9/10
39/39 - 8s - loss: 922.7263 - loglik: -9.2123e+02 - logprior: -1.4738e+00
Epoch 10/10
39/39 - 8s - loss: 917.4086 - loglik: -9.1590e+02 - logprior: -1.4828e+00
Fitted a model with MAP estimate = -907.6363
expansions: [(7, 1), (9, 1), (10, 1), (11, 2), (12, 1), (22, 2), (23, 2), (26, 2), (34, 2), (39, 2), (40, 1), (43, 1), (44, 1), (55, 1), (57, 1), (58, 1), (59, 1), (69, 1), (71, 2), (89, 2), (94, 1), (101, 1), (103, 1), (104, 2), (105, 3), (107, 2), (109, 1), (118, 2), (122, 1), (126, 2), (131, 1)]
discards: [2]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 943.0623 - loglik: -9.4111e+02 - logprior: -1.9543e+00
Epoch 2/2
39/39 - 11s - loss: 914.2897 - loglik: -9.1342e+02 - logprior: -8.6954e-01
Fitted a model with MAP estimate = -902.2980
expansions: [(105, 1)]
discards: [ 13  28  31  36  46  95 115 136 143 157 168]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 916.2225 - loglik: -9.1448e+02 - logprior: -1.7400e+00
Epoch 2/2
39/39 - 10s - loss: 912.8922 - loglik: -9.1234e+02 - logprior: -5.4722e-01
Fitted a model with MAP estimate = -901.2790
expansions: [(2, 1)]
discards: [ 0 58]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 11293 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 13s - loss: 905.4572 - loglik: -9.0333e+02 - logprior: -2.1290e+00
Epoch 2/10
41/41 - 10s - loss: 902.5975 - loglik: -9.0170e+02 - logprior: -9.0179e-01
Epoch 3/10
41/41 - 10s - loss: 900.1802 - loglik: -8.9976e+02 - logprior: -4.1398e-01
Epoch 4/10
41/41 - 10s - loss: 899.2499 - loglik: -8.9895e+02 - logprior: -2.9681e-01
Epoch 5/10
41/41 - 10s - loss: 897.9869 - loglik: -8.9778e+02 - logprior: -1.9742e-01
Epoch 6/10
41/41 - 10s - loss: 895.1063 - loglik: -8.9497e+02 - logprior: -1.2710e-01
Epoch 7/10
41/41 - 10s - loss: 895.1167 - loglik: -8.9503e+02 - logprior: -7.2428e-02
Fitted a model with MAP estimate = -892.6697
Time for alignment: 251.9460
Computed alignments with likelihoods: ['-891.7310', '-895.9072', '-888.9018', '-898.1772', '-892.6697']
Best model has likelihood: -888.9018  (prior= -0.1650 )
time for generating output: 0.2919
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tRNA-synt_2b.projection.fasta
SP score = 0.5417690417690417
Training of 5 independent models on file ChtBD.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9bc7dc730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cdce99d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea229897f0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 284.2678 - loglik: -2.2536e+02 - logprior: -5.8904e+01
Epoch 2/10
10/10 - 0s - loss: 206.3951 - loglik: -1.8981e+02 - logprior: -1.6584e+01
Epoch 3/10
10/10 - 0s - loss: 177.9471 - loglik: -1.6968e+02 - logprior: -8.2636e+00
Epoch 4/10
10/10 - 0s - loss: 167.1504 - loglik: -1.6195e+02 - logprior: -5.2032e+00
Epoch 5/10
10/10 - 0s - loss: 161.3475 - loglik: -1.5767e+02 - logprior: -3.6749e+00
Epoch 6/10
10/10 - 0s - loss: 158.7564 - loglik: -1.5583e+02 - logprior: -2.9219e+00
Epoch 7/10
10/10 - 0s - loss: 157.4755 - loglik: -1.5499e+02 - logprior: -2.4867e+00
Epoch 8/10
10/10 - 0s - loss: 156.8758 - loglik: -1.5472e+02 - logprior: -2.1548e+00
Epoch 9/10
10/10 - 0s - loss: 156.5005 - loglik: -1.5459e+02 - logprior: -1.9100e+00
Epoch 10/10
10/10 - 0s - loss: 155.7050 - loglik: -1.5396e+02 - logprior: -1.7422e+00
Fitted a model with MAP estimate = -155.7966
expansions: [(0, 4), (10, 2), (21, 1), (25, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 253.0607 - loglik: -1.7381e+02 - logprior: -7.9246e+01
Epoch 2/2
10/10 - 0s - loss: 181.7685 - loglik: -1.5622e+02 - logprior: -2.5549e+01
Fitted a model with MAP estimate = -167.4929
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 230.7368 - loglik: -1.6509e+02 - logprior: -6.5648e+01
Epoch 2/2
10/10 - 0s - loss: 175.8642 - loglik: -1.5286e+02 - logprior: -2.3004e+01
Fitted a model with MAP estimate = -163.7761
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 230.8982 - loglik: -1.6522e+02 - logprior: -6.5683e+01
Epoch 2/10
10/10 - 0s - loss: 180.9155 - loglik: -1.5371e+02 - logprior: -2.7209e+01
Epoch 3/10
10/10 - 0s - loss: 165.9510 - loglik: -1.5012e+02 - logprior: -1.5835e+01
Epoch 4/10
10/10 - 0s - loss: 155.8786 - loglik: -1.4952e+02 - logprior: -6.3595e+00
Epoch 5/10
10/10 - 0s - loss: 151.3428 - loglik: -1.4848e+02 - logprior: -2.8616e+00
Epoch 6/10
10/10 - 0s - loss: 148.9950 - loglik: -1.4720e+02 - logprior: -1.7938e+00
Epoch 7/10
10/10 - 0s - loss: 147.8815 - loglik: -1.4662e+02 - logprior: -1.2570e+00
Epoch 8/10
10/10 - 0s - loss: 147.1779 - loglik: -1.4629e+02 - logprior: -8.9169e-01
Epoch 9/10
10/10 - 0s - loss: 146.8226 - loglik: -1.4620e+02 - logprior: -6.1999e-01
Epoch 10/10
10/10 - 0s - loss: 146.2659 - loglik: -1.4583e+02 - logprior: -4.3820e-01
Fitted a model with MAP estimate = -146.1148
Time for alignment: 25.7467
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 284.3590 - loglik: -2.2546e+02 - logprior: -5.8903e+01
Epoch 2/10
10/10 - 0s - loss: 206.3134 - loglik: -1.8973e+02 - logprior: -1.6582e+01
Epoch 3/10
10/10 - 0s - loss: 178.2173 - loglik: -1.6996e+02 - logprior: -8.2617e+00
Epoch 4/10
10/10 - 0s - loss: 166.8619 - loglik: -1.6166e+02 - logprior: -5.2036e+00
Epoch 5/10
10/10 - 0s - loss: 161.5630 - loglik: -1.5789e+02 - logprior: -3.6749e+00
Epoch 6/10
10/10 - 0s - loss: 158.6964 - loglik: -1.5577e+02 - logprior: -2.9211e+00
Epoch 7/10
10/10 - 0s - loss: 157.7212 - loglik: -1.5523e+02 - logprior: -2.4894e+00
Epoch 8/10
10/10 - 0s - loss: 156.6234 - loglik: -1.5446e+02 - logprior: -2.1598e+00
Epoch 9/10
10/10 - 0s - loss: 156.4721 - loglik: -1.5456e+02 - logprior: -1.9123e+00
Epoch 10/10
10/10 - 0s - loss: 155.7956 - loglik: -1.5405e+02 - logprior: -1.7447e+00
Fitted a model with MAP estimate = -155.7969
expansions: [(0, 4), (10, 2), (21, 1), (25, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 253.0186 - loglik: -1.7377e+02 - logprior: -7.9246e+01
Epoch 2/2
10/10 - 0s - loss: 182.1162 - loglik: -1.5657e+02 - logprior: -2.5548e+01
Fitted a model with MAP estimate = -167.4906
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 230.6755 - loglik: -1.6502e+02 - logprior: -6.5651e+01
Epoch 2/2
10/10 - 0s - loss: 175.8941 - loglik: -1.5289e+02 - logprior: -2.3008e+01
Fitted a model with MAP estimate = -163.7587
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 230.8783 - loglik: -1.6519e+02 - logprior: -6.5684e+01
Epoch 2/10
10/10 - 0s - loss: 181.0098 - loglik: -1.5379e+02 - logprior: -2.7216e+01
Epoch 3/10
10/10 - 0s - loss: 165.8924 - loglik: -1.5005e+02 - logprior: -1.5847e+01
Epoch 4/10
10/10 - 0s - loss: 156.0529 - loglik: -1.4968e+02 - logprior: -6.3702e+00
Epoch 5/10
10/10 - 0s - loss: 151.1249 - loglik: -1.4825e+02 - logprior: -2.8693e+00
Epoch 6/10
10/10 - 0s - loss: 149.1927 - loglik: -1.4740e+02 - logprior: -1.7918e+00
Epoch 7/10
10/10 - 0s - loss: 147.7637 - loglik: -1.4650e+02 - logprior: -1.2599e+00
Epoch 8/10
10/10 - 0s - loss: 147.1754 - loglik: -1.4629e+02 - logprior: -8.8629e-01
Epoch 9/10
10/10 - 0s - loss: 146.7057 - loglik: -1.4609e+02 - logprior: -6.1907e-01
Epoch 10/10
10/10 - 0s - loss: 146.2608 - loglik: -1.4582e+02 - logprior: -4.3702e-01
Fitted a model with MAP estimate = -146.1081
Time for alignment: 24.9451
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 284.4360 - loglik: -2.2553e+02 - logprior: -5.8903e+01
Epoch 2/10
10/10 - 0s - loss: 206.2323 - loglik: -1.8965e+02 - logprior: -1.6583e+01
Epoch 3/10
10/10 - 0s - loss: 177.9019 - loglik: -1.6964e+02 - logprior: -8.2658e+00
Epoch 4/10
10/10 - 0s - loss: 166.7645 - loglik: -1.6154e+02 - logprior: -5.2216e+00
Epoch 5/10
10/10 - 0s - loss: 161.3645 - loglik: -1.5768e+02 - logprior: -3.6816e+00
Epoch 6/10
10/10 - 0s - loss: 158.9669 - loglik: -1.5604e+02 - logprior: -2.9214e+00
Epoch 7/10
10/10 - 0s - loss: 157.3678 - loglik: -1.5489e+02 - logprior: -2.4785e+00
Epoch 8/10
10/10 - 0s - loss: 156.8271 - loglik: -1.5468e+02 - logprior: -2.1463e+00
Epoch 9/10
10/10 - 0s - loss: 156.3278 - loglik: -1.5442e+02 - logprior: -1.9025e+00
Epoch 10/10
10/10 - 0s - loss: 155.8187 - loglik: -1.5408e+02 - logprior: -1.7396e+00
Fitted a model with MAP estimate = -155.8111
expansions: [(0, 4), (10, 2), (21, 1), (25, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 253.1308 - loglik: -1.7388e+02 - logprior: -7.9248e+01
Epoch 2/2
10/10 - 0s - loss: 181.9353 - loglik: -1.5638e+02 - logprior: -2.5552e+01
Fitted a model with MAP estimate = -167.4923
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 230.8097 - loglik: -1.6516e+02 - logprior: -6.5647e+01
Epoch 2/2
10/10 - 0s - loss: 175.9094 - loglik: -1.5291e+02 - logprior: -2.3004e+01
Fitted a model with MAP estimate = -163.8166
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 230.7174 - loglik: -1.6503e+02 - logprior: -6.5685e+01
Epoch 2/10
10/10 - 0s - loss: 181.0973 - loglik: -1.5389e+02 - logprior: -2.7206e+01
Epoch 3/10
10/10 - 0s - loss: 165.8378 - loglik: -1.5003e+02 - logprior: -1.5807e+01
Epoch 4/10
10/10 - 0s - loss: 155.8709 - loglik: -1.4952e+02 - logprior: -6.3521e+00
Epoch 5/10
10/10 - 0s - loss: 151.3703 - loglik: -1.4852e+02 - logprior: -2.8514e+00
Epoch 6/10
10/10 - 0s - loss: 148.9938 - loglik: -1.4720e+02 - logprior: -1.7894e+00
Epoch 7/10
10/10 - 0s - loss: 147.7614 - loglik: -1.4650e+02 - logprior: -1.2575e+00
Epoch 8/10
10/10 - 0s - loss: 147.2860 - loglik: -1.4639e+02 - logprior: -8.9193e-01
Epoch 9/10
10/10 - 0s - loss: 146.8138 - loglik: -1.4620e+02 - logprior: -6.1534e-01
Epoch 10/10
10/10 - 0s - loss: 146.2923 - loglik: -1.4586e+02 - logprior: -4.3575e-01
Fitted a model with MAP estimate = -146.1111
Time for alignment: 25.4598
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 284.3094 - loglik: -2.2541e+02 - logprior: -5.8903e+01
Epoch 2/10
10/10 - 0s - loss: 206.5543 - loglik: -1.8998e+02 - logprior: -1.6578e+01
Epoch 3/10
10/10 - 0s - loss: 178.4276 - loglik: -1.7016e+02 - logprior: -8.2687e+00
Epoch 4/10
10/10 - 0s - loss: 167.2608 - loglik: -1.6202e+02 - logprior: -5.2405e+00
Epoch 5/10
10/10 - 0s - loss: 161.3427 - loglik: -1.5765e+02 - logprior: -3.6889e+00
Epoch 6/10
10/10 - 0s - loss: 158.7969 - loglik: -1.5586e+02 - logprior: -2.9345e+00
Epoch 7/10
10/10 - 0s - loss: 157.6243 - loglik: -1.5513e+02 - logprior: -2.4925e+00
Epoch 8/10
10/10 - 0s - loss: 156.6936 - loglik: -1.5453e+02 - logprior: -2.1614e+00
Epoch 9/10
10/10 - 0s - loss: 156.3137 - loglik: -1.5440e+02 - logprior: -1.9148e+00
Epoch 10/10
10/10 - 0s - loss: 155.9019 - loglik: -1.5415e+02 - logprior: -1.7511e+00
Fitted a model with MAP estimate = -155.7909
expansions: [(0, 4), (10, 2), (21, 1), (25, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 253.1922 - loglik: -1.7394e+02 - logprior: -7.9251e+01
Epoch 2/2
10/10 - 0s - loss: 181.8768 - loglik: -1.5632e+02 - logprior: -2.5553e+01
Fitted a model with MAP estimate = -167.5030
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 230.7797 - loglik: -1.6513e+02 - logprior: -6.5649e+01
Epoch 2/2
10/10 - 0s - loss: 175.7898 - loglik: -1.5278e+02 - logprior: -2.3010e+01
Fitted a model with MAP estimate = -163.7693
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 230.8297 - loglik: -1.6514e+02 - logprior: -6.5689e+01
Epoch 2/10
10/10 - 0s - loss: 181.1013 - loglik: -1.5388e+02 - logprior: -2.7221e+01
Epoch 3/10
10/10 - 0s - loss: 165.7549 - loglik: -1.4990e+02 - logprior: -1.5858e+01
Epoch 4/10
10/10 - 0s - loss: 156.0286 - loglik: -1.4965e+02 - logprior: -6.3825e+00
Epoch 5/10
10/10 - 0s - loss: 151.2764 - loglik: -1.4841e+02 - logprior: -2.8613e+00
Epoch 6/10
10/10 - 0s - loss: 148.9501 - loglik: -1.4715e+02 - logprior: -1.7955e+00
Epoch 7/10
10/10 - 0s - loss: 147.9284 - loglik: -1.4667e+02 - logprior: -1.2596e+00
Epoch 8/10
10/10 - 0s - loss: 147.1834 - loglik: -1.4629e+02 - logprior: -8.9411e-01
Epoch 9/10
10/10 - 0s - loss: 146.8743 - loglik: -1.4625e+02 - logprior: -6.1807e-01
Epoch 10/10
10/10 - 0s - loss: 146.2877 - loglik: -1.4585e+02 - logprior: -4.3503e-01
Fitted a model with MAP estimate = -146.1202
Time for alignment: 25.0122
Fitting a model of length 32 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 284.2089 - loglik: -2.2530e+02 - logprior: -5.8904e+01
Epoch 2/10
10/10 - 0s - loss: 206.4526 - loglik: -1.8987e+02 - logprior: -1.6585e+01
Epoch 3/10
10/10 - 0s - loss: 177.7522 - loglik: -1.6948e+02 - logprior: -8.2731e+00
Epoch 4/10
10/10 - 0s - loss: 166.5423 - loglik: -1.6131e+02 - logprior: -5.2332e+00
Epoch 5/10
10/10 - 0s - loss: 161.5458 - loglik: -1.5786e+02 - logprior: -3.6832e+00
Epoch 6/10
10/10 - 0s - loss: 158.7730 - loglik: -1.5585e+02 - logprior: -2.9177e+00
Epoch 7/10
10/10 - 0s - loss: 157.4302 - loglik: -1.5496e+02 - logprior: -2.4682e+00
Epoch 8/10
10/10 - 0s - loss: 156.7069 - loglik: -1.5457e+02 - logprior: -2.1382e+00
Epoch 9/10
10/10 - 0s - loss: 156.4362 - loglik: -1.5454e+02 - logprior: -1.8954e+00
Epoch 10/10
10/10 - 0s - loss: 155.9981 - loglik: -1.5426e+02 - logprior: -1.7348e+00
Fitted a model with MAP estimate = -155.8226
expansions: [(0, 4), (10, 2), (21, 1), (25, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 253.0158 - loglik: -1.7377e+02 - logprior: -7.9245e+01
Epoch 2/2
10/10 - 0s - loss: 181.9760 - loglik: -1.5642e+02 - logprior: -2.5553e+01
Fitted a model with MAP estimate = -167.5001
expansions: [(0, 2)]
discards: [14]
Re-initialized the encoder parameters.
Fitting a model of length 41 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 230.8428 - loglik: -1.6520e+02 - logprior: -6.5647e+01
Epoch 2/2
10/10 - 0s - loss: 175.8745 - loglik: -1.5287e+02 - logprior: -2.3001e+01
Fitted a model with MAP estimate = -163.7984
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 40 on 774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 230.8840 - loglik: -1.6520e+02 - logprior: -6.5685e+01
Epoch 2/10
10/10 - 0s - loss: 180.9768 - loglik: -1.5377e+02 - logprior: -2.7208e+01
Epoch 3/10
10/10 - 0s - loss: 165.8108 - loglik: -1.5000e+02 - logprior: -1.5812e+01
Epoch 4/10
10/10 - 0s - loss: 156.0898 - loglik: -1.4973e+02 - logprior: -6.3565e+00
Epoch 5/10
10/10 - 0s - loss: 151.1262 - loglik: -1.4827e+02 - logprior: -2.8584e+00
Epoch 6/10
10/10 - 0s - loss: 149.0573 - loglik: -1.4726e+02 - logprior: -1.7921e+00
Epoch 7/10
10/10 - 0s - loss: 147.8577 - loglik: -1.4659e+02 - logprior: -1.2631e+00
Epoch 8/10
10/10 - 0s - loss: 147.2349 - loglik: -1.4634e+02 - logprior: -8.9789e-01
Epoch 9/10
10/10 - 0s - loss: 146.7712 - loglik: -1.4615e+02 - logprior: -6.2292e-01
Epoch 10/10
10/10 - 0s - loss: 146.2402 - loglik: -1.4580e+02 - logprior: -4.4229e-01
Fitted a model with MAP estimate = -146.1171
Time for alignment: 25.1538
Computed alignments with likelihoods: ['-146.1148', '-146.1081', '-146.1111', '-146.1202', '-146.1171']
Best model has likelihood: -146.1081  (prior= -0.3675 )
time for generating output: 0.0889
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ChtBD.projection.fasta
SP score = 0.9444444444444444
Training of 5 independent models on file adh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea093cda30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea55e64af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea55e64820>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 754.0658 - loglik: -7.5127e+02 - logprior: -2.7949e+00
Epoch 2/10
20/20 - 3s - loss: 709.7831 - loglik: -7.0856e+02 - logprior: -1.2179e+00
Epoch 3/10
20/20 - 3s - loss: 692.9419 - loglik: -6.9153e+02 - logprior: -1.4134e+00
Epoch 4/10
20/20 - 3s - loss: 687.4155 - loglik: -6.8604e+02 - logprior: -1.3694e+00
Epoch 5/10
20/20 - 3s - loss: 684.4186 - loglik: -6.8302e+02 - logprior: -1.3961e+00
Epoch 6/10
20/20 - 3s - loss: 682.6846 - loglik: -6.8133e+02 - logprior: -1.3514e+00
Epoch 7/10
20/20 - 3s - loss: 679.1490 - loglik: -6.7780e+02 - logprior: -1.3347e+00
Epoch 8/10
20/20 - 3s - loss: 675.0292 - loglik: -6.7370e+02 - logprior: -1.3156e+00
Epoch 9/10
20/20 - 3s - loss: 669.8600 - loglik: -6.6853e+02 - logprior: -1.3036e+00
Epoch 10/10
20/20 - 3s - loss: 658.2310 - loglik: -6.5691e+02 - logprior: -1.2903e+00
Fitted a model with MAP estimate = -646.7012
expansions: [(5, 1), (8, 1), (10, 2), (13, 2), (18, 1), (20, 1), (23, 2), (35, 1), (36, 2), (38, 1), (42, 1), (50, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 2), (62, 1), (75, 1), (76, 2), (77, 1), (81, 2), (83, 1), (86, 1), (94, 1), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 137 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 706.8060 - loglik: -7.0418e+02 - logprior: -2.6213e+00
Epoch 2/2
40/40 - 5s - loss: 676.1832 - loglik: -6.7529e+02 - logprior: -8.9114e-01
Fitted a model with MAP estimate = -628.4989
expansions: [(100, 3), (101, 2), (110, 1), (126, 1)]
discards: [ 12  47  74  83 102 103 104 105 106 107]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 10s - loss: 675.5215 - loglik: -6.7369e+02 - logprior: -1.8280e+00
Epoch 2/2
40/40 - 5s - loss: 672.4420 - loglik: -6.7179e+02 - logprior: -6.5402e-01
Fitted a model with MAP estimate = -627.4336
expansions: []
discards: [ 30  94 100]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 624.6072 - loglik: -6.2359e+02 - logprior: -1.0204e+00
Epoch 2/10
57/57 - 6s - loss: 621.0422 - loglik: -6.2035e+02 - logprior: -6.8866e-01
Epoch 3/10
57/57 - 6s - loss: 621.0221 - loglik: -6.2037e+02 - logprior: -6.5315e-01
Epoch 4/10
57/57 - 6s - loss: 619.0057 - loglik: -6.1837e+02 - logprior: -6.3120e-01
Epoch 5/10
57/57 - 6s - loss: 615.8787 - loglik: -6.1525e+02 - logprior: -6.1475e-01
Epoch 6/10
57/57 - 6s - loss: 613.1506 - loglik: -6.1253e+02 - logprior: -5.9672e-01
Epoch 7/10
57/57 - 6s - loss: 603.1204 - loglik: -6.0250e+02 - logprior: -5.7653e-01
Epoch 8/10
57/57 - 6s - loss: 581.6675 - loglik: -5.8103e+02 - logprior: -5.8004e-01
Epoch 9/10
57/57 - 6s - loss: 544.3470 - loglik: -5.4363e+02 - logprior: -6.3553e-01
Epoch 10/10
57/57 - 6s - loss: 503.1177 - loglik: -5.0244e+02 - logprior: -5.6897e-01
Fitted a model with MAP estimate = -492.1679
Time for alignment: 165.0291
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 754.2975 - loglik: -7.5150e+02 - logprior: -2.7972e+00
Epoch 2/10
20/20 - 3s - loss: 709.0844 - loglik: -7.0785e+02 - logprior: -1.2375e+00
Epoch 3/10
20/20 - 3s - loss: 690.4406 - loglik: -6.8901e+02 - logprior: -1.4332e+00
Epoch 4/10
20/20 - 3s - loss: 686.1674 - loglik: -6.8482e+02 - logprior: -1.3441e+00
Epoch 5/10
20/20 - 3s - loss: 684.1688 - loglik: -6.8281e+02 - logprior: -1.3507e+00
Epoch 6/10
20/20 - 3s - loss: 681.5865 - loglik: -6.8027e+02 - logprior: -1.3085e+00
Epoch 7/10
20/20 - 3s - loss: 678.6676 - loglik: -6.7735e+02 - logprior: -1.3022e+00
Epoch 8/10
20/20 - 3s - loss: 675.1348 - loglik: -6.7384e+02 - logprior: -1.2767e+00
Epoch 9/10
20/20 - 3s - loss: 669.3019 - loglik: -6.6801e+02 - logprior: -1.2727e+00
Epoch 10/10
20/20 - 3s - loss: 657.6096 - loglik: -6.5633e+02 - logprior: -1.2479e+00
Fitted a model with MAP estimate = -645.0251
expansions: [(5, 1), (8, 1), (10, 2), (13, 2), (19, 1), (24, 2), (31, 1), (37, 2), (39, 1), (40, 1), (47, 1), (57, 2), (58, 2), (59, 2), (60, 2), (61, 2), (62, 1), (77, 1), (78, 2), (79, 1), (81, 2), (86, 1), (93, 1), (94, 1), (95, 1), (96, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 704.6423 - loglik: -7.0200e+02 - logprior: -2.6440e+00
Epoch 2/2
40/40 - 5s - loss: 675.1955 - loglik: -6.7425e+02 - logprior: -9.4956e-01
Fitted a model with MAP estimate = -627.7082
expansions: [(101, 2), (114, 1), (128, 1)]
discards: [ 12  31  47  74  76  79  85 102 105 106 108 109 125]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 675.9010 - loglik: -6.7408e+02 - logprior: -1.8170e+00
Epoch 2/2
40/40 - 5s - loss: 672.3762 - loglik: -6.7175e+02 - logprior: -6.2923e-01
Fitted a model with MAP estimate = -626.7201
expansions: [(95, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 624.2836 - loglik: -6.2327e+02 - logprior: -1.0169e+00
Epoch 2/10
57/57 - 6s - loss: 621.0275 - loglik: -6.2033e+02 - logprior: -6.9630e-01
Epoch 3/10
57/57 - 6s - loss: 620.9826 - loglik: -6.2031e+02 - logprior: -6.6880e-01
Epoch 4/10
57/57 - 6s - loss: 617.9525 - loglik: -6.1730e+02 - logprior: -6.4877e-01
Epoch 5/10
57/57 - 6s - loss: 616.6146 - loglik: -6.1598e+02 - logprior: -6.2338e-01
Epoch 6/10
57/57 - 6s - loss: 613.3696 - loglik: -6.1274e+02 - logprior: -6.0616e-01
Epoch 7/10
57/57 - 6s - loss: 603.6838 - loglik: -6.0306e+02 - logprior: -5.8219e-01
Epoch 8/10
57/57 - 7s - loss: 583.1315 - loglik: -5.8250e+02 - logprior: -5.7156e-01
Epoch 9/10
57/57 - 6s - loss: 549.1708 - loglik: -5.4847e+02 - logprior: -6.1623e-01
Epoch 10/10
57/57 - 6s - loss: 504.7474 - loglik: -5.0409e+02 - logprior: -5.4552e-01
Fitted a model with MAP estimate = -492.1306
Time for alignment: 165.1724
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 753.8943 - loglik: -7.5110e+02 - logprior: -2.7947e+00
Epoch 2/10
20/20 - 3s - loss: 708.5662 - loglik: -7.0734e+02 - logprior: -1.2231e+00
Epoch 3/10
20/20 - 3s - loss: 690.3109 - loglik: -6.8887e+02 - logprior: -1.4404e+00
Epoch 4/10
20/20 - 3s - loss: 685.7380 - loglik: -6.8439e+02 - logprior: -1.3503e+00
Epoch 5/10
20/20 - 3s - loss: 683.5364 - loglik: -6.8220e+02 - logprior: -1.3282e+00
Epoch 6/10
20/20 - 3s - loss: 681.4223 - loglik: -6.8014e+02 - logprior: -1.2763e+00
Epoch 7/10
20/20 - 3s - loss: 677.3826 - loglik: -6.7612e+02 - logprior: -1.2479e+00
Epoch 8/10
20/20 - 3s - loss: 674.1728 - loglik: -6.7292e+02 - logprior: -1.2320e+00
Epoch 9/10
20/20 - 3s - loss: 669.4200 - loglik: -6.6817e+02 - logprior: -1.2314e+00
Epoch 10/10
20/20 - 3s - loss: 658.0126 - loglik: -6.5676e+02 - logprior: -1.2245e+00
Fitted a model with MAP estimate = -645.4700
expansions: [(5, 1), (8, 1), (10, 2), (13, 2), (18, 1), (20, 1), (23, 2), (35, 1), (36, 2), (38, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 2), (61, 2), (64, 2), (77, 1), (78, 2), (79, 1), (81, 2), (83, 1), (86, 1), (94, 1), (95, 1), (96, 1), (103, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 704.5395 - loglik: -7.0185e+02 - logprior: -2.6855e+00
Epoch 2/2
40/40 - 5s - loss: 674.2687 - loglik: -6.7320e+02 - logprior: -1.0654e+00
Fitted a model with MAP estimate = -626.5758
expansions: [(101, 1), (142, 2)]
discards: [ 12  31  47  73  75  83  88 102 105 108 109]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 675.4265 - loglik: -6.7347e+02 - logprior: -1.9592e+00
Epoch 2/2
40/40 - 5s - loss: 671.5350 - loglik: -6.7073e+02 - logprior: -8.0458e-01
Fitted a model with MAP estimate = -626.1748
expansions: [(101, 2)]
discards: [118 119 120 131]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 624.5331 - loglik: -6.2344e+02 - logprior: -1.0915e+00
Epoch 2/10
57/57 - 6s - loss: 620.3054 - loglik: -6.1958e+02 - logprior: -7.2690e-01
Epoch 3/10
57/57 - 7s - loss: 621.6083 - loglik: -6.2091e+02 - logprior: -6.9125e-01
Fitted a model with MAP estimate = -619.0088
Time for alignment: 122.3611
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 7s - loss: 754.3474 - loglik: -7.5155e+02 - logprior: -2.7969e+00
Epoch 2/10
20/20 - 3s - loss: 709.5604 - loglik: -7.0831e+02 - logprior: -1.2488e+00
Epoch 3/10
20/20 - 3s - loss: 690.5216 - loglik: -6.8906e+02 - logprior: -1.4556e+00
Epoch 4/10
20/20 - 3s - loss: 686.6275 - loglik: -6.8526e+02 - logprior: -1.3638e+00
Epoch 5/10
20/20 - 3s - loss: 684.2115 - loglik: -6.8285e+02 - logprior: -1.3563e+00
Epoch 6/10
20/20 - 3s - loss: 681.8865 - loglik: -6.8056e+02 - logprior: -1.3170e+00
Epoch 7/10
20/20 - 3s - loss: 678.5253 - loglik: -6.7723e+02 - logprior: -1.2785e+00
Epoch 8/10
20/20 - 3s - loss: 674.6463 - loglik: -6.7338e+02 - logprior: -1.2520e+00
Epoch 9/10
20/20 - 3s - loss: 670.4323 - loglik: -6.6915e+02 - logprior: -1.2588e+00
Epoch 10/10
20/20 - 3s - loss: 659.3373 - loglik: -6.5807e+02 - logprior: -1.2378e+00
Fitted a model with MAP estimate = -645.5148
expansions: [(5, 1), (8, 1), (10, 2), (13, 2), (19, 1), (24, 2), (31, 1), (37, 2), (39, 1), (40, 1), (47, 1), (51, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 2), (62, 1), (77, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 1), (95, 1), (96, 1), (103, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 703.0023 - loglik: -7.0029e+02 - logprior: -2.7082e+00
Epoch 2/2
40/40 - 5s - loss: 673.7023 - loglik: -6.7273e+02 - logprior: -9.6979e-01
Fitted a model with MAP estimate = -626.6365
expansions: [(99, 1), (141, 2)]
discards: [ 12  31  47  74  83 100 103 107 108]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 675.1254 - loglik: -6.7318e+02 - logprior: -1.9457e+00
Epoch 2/2
40/40 - 5s - loss: 671.4896 - loglik: -6.7070e+02 - logprior: -7.8791e-01
Fitted a model with MAP estimate = -626.7683
expansions: []
discards: [132]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 623.7267 - loglik: -6.2267e+02 - logprior: -1.0565e+00
Epoch 2/10
57/57 - 7s - loss: 621.8891 - loglik: -6.2120e+02 - logprior: -6.8830e-01
Epoch 3/10
57/57 - 6s - loss: 619.6764 - loglik: -6.1901e+02 - logprior: -6.6107e-01
Epoch 4/10
57/57 - 6s - loss: 617.9393 - loglik: -6.1729e+02 - logprior: -6.4041e-01
Epoch 5/10
57/57 - 6s - loss: 616.0848 - loglik: -6.1545e+02 - logprior: -6.1686e-01
Epoch 6/10
57/57 - 6s - loss: 612.1682 - loglik: -6.1154e+02 - logprior: -6.0342e-01
Epoch 7/10
57/57 - 7s - loss: 602.4626 - loglik: -6.0183e+02 - logprior: -5.9286e-01
Epoch 8/10
57/57 - 6s - loss: 578.2761 - loglik: -5.7758e+02 - logprior: -6.3840e-01
Epoch 9/10
57/57 - 6s - loss: 541.5927 - loglik: -5.4077e+02 - logprior: -7.3828e-01
Epoch 10/10
57/57 - 6s - loss: 500.5456 - loglik: -4.9975e+02 - logprior: -6.7915e-01
Fitted a model with MAP estimate = -490.6002
Time for alignment: 166.7835
Fitting a model of length 103 on 10666 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
20/20 - 6s - loss: 754.0302 - loglik: -7.5124e+02 - logprior: -2.7931e+00
Epoch 2/10
20/20 - 3s - loss: 708.8878 - loglik: -7.0766e+02 - logprior: -1.2267e+00
Epoch 3/10
20/20 - 3s - loss: 690.3637 - loglik: -6.8892e+02 - logprior: -1.4473e+00
Epoch 4/10
20/20 - 3s - loss: 685.5959 - loglik: -6.8424e+02 - logprior: -1.3515e+00
Epoch 5/10
20/20 - 3s - loss: 683.0985 - loglik: -6.8175e+02 - logprior: -1.3416e+00
Epoch 6/10
20/20 - 3s - loss: 681.2532 - loglik: -6.7996e+02 - logprior: -1.2831e+00
Epoch 7/10
20/20 - 3s - loss: 677.5678 - loglik: -6.7631e+02 - logprior: -1.2465e+00
Epoch 8/10
20/20 - 3s - loss: 674.8565 - loglik: -6.7360e+02 - logprior: -1.2351e+00
Epoch 9/10
20/20 - 3s - loss: 669.2916 - loglik: -6.6804e+02 - logprior: -1.2299e+00
Epoch 10/10
20/20 - 3s - loss: 660.2216 - loglik: -6.5896e+02 - logprior: -1.2320e+00
Fitted a model with MAP estimate = -643.9608
expansions: [(5, 1), (8, 2), (10, 2), (13, 2), (19, 1), (20, 1), (23, 2), (37, 2), (39, 1), (40, 1), (47, 1), (57, 2), (58, 2), (59, 1), (60, 1), (61, 2), (64, 2), (76, 1), (78, 2), (79, 2), (81, 2), (83, 1), (86, 1), (94, 1), (95, 1), (96, 1), (103, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 9s - loss: 702.1893 - loglik: -6.9947e+02 - logprior: -2.7196e+00
Epoch 2/2
40/40 - 5s - loss: 673.6298 - loglik: -6.7265e+02 - logprior: -9.8367e-01
Fitted a model with MAP estimate = -626.4532
expansions: [(143, 2)]
discards: [  8  13  31  48  77  83  88 105 109 110 129]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 10666 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 8s - loss: 674.8383 - loglik: -6.7291e+02 - logprior: -1.9319e+00
Epoch 2/2
40/40 - 5s - loss: 671.1794 - loglik: -6.7038e+02 - logprior: -8.0127e-01
Fitted a model with MAP estimate = -626.5976
expansions: []
discards: [119 120 131]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 21331 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
57/57 - 9s - loss: 624.5232 - loglik: -6.2346e+02 - logprior: -1.0658e+00
Epoch 2/10
57/57 - 6s - loss: 620.3721 - loglik: -6.1966e+02 - logprior: -7.1113e-01
Epoch 3/10
57/57 - 7s - loss: 621.3582 - loglik: -6.2068e+02 - logprior: -6.7942e-01
Fitted a model with MAP estimate = -618.9989
Time for alignment: 119.6836
Computed alignments with likelihoods: ['-492.1679', '-492.1306', '-619.0088', '-490.6002', '-618.9989']
Best model has likelihood: -490.6002  (prior= -0.6311 )
time for generating output: 0.2986
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/adh.projection.fasta
SP score = 0.3189261744966443
Training of 5 independent models on file Sulfotransfer.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9167ca580>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea5e329670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea66ef7c40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 1379.6537 - loglik: -1.3701e+03 - logprior: -9.5782e+00
Epoch 2/10
19/19 - 6s - loss: 1314.1769 - loglik: -1.3135e+03 - logprior: -6.3672e-01
Epoch 3/10
19/19 - 6s - loss: 1283.4282 - loglik: -1.2828e+03 - logprior: -6.5725e-01
Epoch 4/10
19/19 - 6s - loss: 1276.5398 - loglik: -1.2762e+03 - logprior: -3.1771e-01
Epoch 5/10
19/19 - 6s - loss: 1266.0273 - loglik: -1.2659e+03 - logprior: -1.3848e-01
Epoch 6/10
19/19 - 6s - loss: 1249.6791 - loglik: -1.2497e+03 - logprior: 0.0383
Epoch 7/10
19/19 - 6s - loss: 1223.9861 - loglik: -1.2240e+03 - logprior: 0.0064
Epoch 8/10
19/19 - 6s - loss: 1127.5426 - loglik: -1.1256e+03 - logprior: -1.9165e+00
Epoch 9/10
19/19 - 6s - loss: 990.7585 - loglik: -9.8511e+02 - logprior: -5.6338e+00
Epoch 10/10
19/19 - 6s - loss: 973.0881 - loglik: -9.6683e+02 - logprior: -6.2373e+00
Fitted a model with MAP estimate = -971.1406
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  74  75  76  77  78  79  80  81  82 102 133 134 135 136
 137 138 139 140 141 142 143 144 154 171 173 174 175 176 177 178 179 180
 181 182 197]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1428.0040 - loglik: -1.4162e+03 - logprior: -1.1811e+01
Epoch 2/2
19/19 - 4s - loss: 1396.0359 - loglik: -1.3958e+03 - logprior: -1.9340e-01
Fitted a model with MAP estimate = -1390.5114
expansions: [(0, 122), (123, 109)]
discards: [  4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21
  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39
  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57
  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113 114 115 116 117 118 119 120 121 122]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 1369.6575 - loglik: -1.3584e+03 - logprior: -1.1254e+01
Epoch 2/2
19/19 - 8s - loss: 1292.5071 - loglik: -1.2919e+03 - logprior: -6.0607e-01
Fitted a model with MAP estimate = -1275.3458
expansions: [(124, 4), (144, 1), (160, 1), (178, 1), (235, 16)]
discards: [  0 190 203 204 205 206 207 208 209 210 211 212 213 220 221 222 223 224
 230 231 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 1296.7339 - loglik: -1.2840e+03 - logprior: -1.2735e+01
Epoch 2/10
19/19 - 8s - loss: 1277.1271 - loglik: -1.2741e+03 - logprior: -2.9910e+00
Epoch 3/10
19/19 - 8s - loss: 1269.6632 - loglik: -1.2704e+03 - logprior: 0.7113
Epoch 4/10
19/19 - 8s - loss: 1264.3512 - loglik: -1.2656e+03 - logprior: 1.2671
Epoch 5/10
19/19 - 8s - loss: 1257.5055 - loglik: -1.2589e+03 - logprior: 1.3946
Epoch 6/10
19/19 - 8s - loss: 1248.1544 - loglik: -1.2497e+03 - logprior: 1.5367
Epoch 7/10
19/19 - 8s - loss: 1219.7803 - loglik: -1.2213e+03 - logprior: 1.5622
Epoch 8/10
19/19 - 8s - loss: 1131.8303 - loglik: -1.1324e+03 - logprior: 0.5297
Epoch 9/10
19/19 - 8s - loss: 999.0662 - loglik: -9.9707e+02 - logprior: -1.9872e+00
Epoch 10/10
19/19 - 8s - loss: 971.3198 - loglik: -9.6916e+02 - logprior: -2.1383e+00
Fitted a model with MAP estimate = -966.1679
Time for alignment: 194.2342
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 1376.8959 - loglik: -1.3673e+03 - logprior: -9.5807e+00
Epoch 2/10
19/19 - 6s - loss: 1311.0787 - loglik: -1.3103e+03 - logprior: -7.5659e-01
Epoch 3/10
19/19 - 6s - loss: 1285.6090 - loglik: -1.2849e+03 - logprior: -6.6109e-01
Epoch 4/10
19/19 - 6s - loss: 1275.2448 - loglik: -1.2750e+03 - logprior: -2.8905e-01
Epoch 5/10
19/19 - 6s - loss: 1267.7759 - loglik: -1.2678e+03 - logprior: -1.5936e-02
Epoch 6/10
19/19 - 6s - loss: 1254.6570 - loglik: -1.2547e+03 - logprior: 0.0284
Epoch 7/10
19/19 - 6s - loss: 1221.4281 - loglik: -1.2214e+03 - logprior: -5.0250e-02
Epoch 8/10
19/19 - 6s - loss: 1127.9353 - loglik: -1.1262e+03 - logprior: -1.7706e+00
Epoch 9/10
19/19 - 6s - loss: 991.4694 - loglik: -9.8569e+02 - logprior: -5.7633e+00
Epoch 10/10
19/19 - 6s - loss: 972.0485 - loglik: -9.6591e+02 - logprior: -6.1211e+00
Fitted a model with MAP estimate = -970.9524
expansions: [(0, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  74  75  76  77  78  79  80  81  82 132 133
 134 135 136 137 138 139 140 141 142 171 172 173 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1425.4238 - loglik: -1.4142e+03 - logprior: -1.1237e+01
Epoch 2/2
19/19 - 4s - loss: 1395.0160 - loglik: -1.3944e+03 - logprior: -5.8528e-01
Fitted a model with MAP estimate = -1390.6450
expansions: [(0, 150)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108
 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1389.8098 - loglik: -1.3762e+03 - logprior: -1.3645e+01
Epoch 2/2
19/19 - 4s - loss: 1329.2008 - loglik: -1.3271e+03 - logprior: -2.1130e+00
Fitted a model with MAP estimate = -1313.9294
expansions: [(109, 7), (110, 3), (113, 3), (151, 38)]
discards: [  0 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 1319.4131 - loglik: -1.3062e+03 - logprior: -1.3197e+01
Epoch 2/10
19/19 - 6s - loss: 1293.4170 - loglik: -1.2902e+03 - logprior: -3.1948e+00
Epoch 3/10
19/19 - 6s - loss: 1283.7012 - loglik: -1.2834e+03 - logprior: -2.6423e-01
Epoch 4/10
19/19 - 6s - loss: 1280.5200 - loglik: -1.2810e+03 - logprior: 0.4967
Epoch 5/10
19/19 - 6s - loss: 1274.5300 - loglik: -1.2752e+03 - logprior: 0.6853
Epoch 6/10
19/19 - 6s - loss: 1263.1099 - loglik: -1.2640e+03 - logprior: 0.8807
Epoch 7/10
19/19 - 6s - loss: 1238.3002 - loglik: -1.2393e+03 - logprior: 0.9937
Epoch 8/10
19/19 - 6s - loss: 1151.1240 - loglik: -1.1505e+03 - logprior: -5.7931e-01
Epoch 9/10
19/19 - 6s - loss: 999.7584 - loglik: -9.9582e+02 - logprior: -3.9205e+00
Epoch 10/10
19/19 - 6s - loss: 972.3201 - loglik: -9.6864e+02 - logprior: -3.6628e+00
Fitted a model with MAP estimate = -969.3878
Time for alignment: 164.3343
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 1378.8573 - loglik: -1.3693e+03 - logprior: -9.5846e+00
Epoch 2/10
19/19 - 6s - loss: 1313.3594 - loglik: -1.3126e+03 - logprior: -7.1171e-01
Epoch 3/10
19/19 - 6s - loss: 1286.7330 - loglik: -1.2861e+03 - logprior: -5.9280e-01
Epoch 4/10
19/19 - 6s - loss: 1278.7911 - loglik: -1.2785e+03 - logprior: -2.9739e-01
Epoch 5/10
19/19 - 6s - loss: 1268.8129 - loglik: -1.2688e+03 - logprior: -4.9906e-02
Epoch 6/10
19/19 - 6s - loss: 1257.0273 - loglik: -1.2570e+03 - logprior: -6.7372e-02
Epoch 7/10
19/19 - 6s - loss: 1222.8046 - loglik: -1.2227e+03 - logprior: -9.6577e-02
Epoch 8/10
19/19 - 6s - loss: 1124.5712 - loglik: -1.1228e+03 - logprior: -1.7340e+00
Epoch 9/10
19/19 - 6s - loss: 988.6503 - loglik: -9.8310e+02 - logprior: -5.5384e+00
Epoch 10/10
19/19 - 6s - loss: 974.2512 - loglik: -9.6847e+02 - logprior: -5.7679e+00
Fitted a model with MAP estimate = -970.1488
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  75  76  77  78  79  80  81  82  83 120 121 122
 123 124 125 126 127 132 153 154 155 171 172 173 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1426.8398 - loglik: -1.4161e+03 - logprior: -1.0704e+01
Epoch 2/2
19/19 - 4s - loss: 1397.6176 - loglik: -1.3976e+03 - logprior: -5.1722e-02
Fitted a model with MAP estimate = -1390.1569
expansions: [(0, 137), (129, 95)]
discards: [  3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20
  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38
  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56
  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74
  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92
  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110
 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 1365.5381 - loglik: -1.3560e+03 - logprior: -9.5634e+00
Epoch 2/2
19/19 - 8s - loss: 1287.7664 - loglik: -1.2876e+03 - logprior: -1.6386e-01
Fitted a model with MAP estimate = -1273.6734
expansions: [(76, 2), (235, 17)]
discards: [  0 181 182 183 184 185 188 189 190 197 198 199 200 201 202 203 204 205
 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223
 224 225 226 227 228 229 230 231 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 207 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 1302.8351 - loglik: -1.2899e+03 - logprior: -1.2890e+01
Epoch 2/10
19/19 - 7s - loss: 1283.9849 - loglik: -1.2806e+03 - logprior: -3.3858e+00
Epoch 3/10
19/19 - 7s - loss: 1280.0093 - loglik: -1.2804e+03 - logprior: 0.4221
Epoch 4/10
19/19 - 7s - loss: 1276.8684 - loglik: -1.2780e+03 - logprior: 1.1392
Epoch 5/10
19/19 - 6s - loss: 1267.4065 - loglik: -1.2687e+03 - logprior: 1.3366
Epoch 6/10
19/19 - 7s - loss: 1255.7598 - loglik: -1.2572e+03 - logprior: 1.4383
Epoch 7/10
19/19 - 7s - loss: 1226.7769 - loglik: -1.2281e+03 - logprior: 1.3540
Epoch 8/10
19/19 - 7s - loss: 1130.4844 - loglik: -1.1301e+03 - logprior: -3.6118e-01
Epoch 9/10
19/19 - 7s - loss: 996.1797 - loglik: -9.9311e+02 - logprior: -3.0540e+00
Epoch 10/10
19/19 - 6s - loss: 976.1517 - loglik: -9.7315e+02 - logprior: -2.9821e+00
Fitted a model with MAP estimate = -969.1508
Time for alignment: 183.3427
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 9s - loss: 1378.5754 - loglik: -1.3690e+03 - logprior: -9.5883e+00
Epoch 2/10
19/19 - 6s - loss: 1311.3221 - loglik: -1.3106e+03 - logprior: -7.1477e-01
Epoch 3/10
19/19 - 6s - loss: 1285.4875 - loglik: -1.2846e+03 - logprior: -8.3787e-01
Epoch 4/10
19/19 - 6s - loss: 1275.2816 - loglik: -1.2748e+03 - logprior: -4.7773e-01
Epoch 5/10
19/19 - 6s - loss: 1268.4163 - loglik: -1.2682e+03 - logprior: -2.4959e-01
Epoch 6/10
19/19 - 6s - loss: 1253.7080 - loglik: -1.2534e+03 - logprior: -2.7533e-01
Epoch 7/10
19/19 - 6s - loss: 1229.2131 - loglik: -1.2289e+03 - logprior: -3.3050e-01
Epoch 8/10
19/19 - 6s - loss: 1126.1787 - loglik: -1.1240e+03 - logprior: -2.1478e+00
Epoch 9/10
19/19 - 6s - loss: 990.0226 - loglik: -9.8390e+02 - logprior: -6.1086e+00
Epoch 10/10
19/19 - 6s - loss: 974.2579 - loglik: -9.6817e+02 - logprior: -6.0678e+00
Fitted a model with MAP estimate = -970.1479
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  75  76  77  78  79  80  81  82  83 121 122 123
 124 125 126 127 128 132 133 134 135 142 171 172 173 174 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1429.3152 - loglik: -1.4172e+03 - logprior: -1.2107e+01
Epoch 2/2
19/19 - 4s - loss: 1395.5538 - loglik: -1.3953e+03 - logprior: -2.2395e-01
Fitted a model with MAP estimate = -1390.5443
expansions: [(0, 120), (127, 112)]
discards: [  3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20
  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38
  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56
  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74
  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92
  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110
 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 11s - loss: 1369.5336 - loglik: -1.3590e+03 - logprior: -1.0517e+01
Epoch 2/2
19/19 - 8s - loss: 1294.8420 - loglik: -1.2946e+03 - logprior: -2.7448e-01
Fitted a model with MAP estimate = -1281.2695
expansions: [(235, 18)]
discards: [  0 202 203 204 205 206 207 208 209 210 211 212 213 219 220 221 222 223
 224 225 226 227 228 229 230 231 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 224 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 1300.3663 - loglik: -1.2873e+03 - logprior: -1.3040e+01
Epoch 2/10
19/19 - 7s - loss: 1279.5892 - loglik: -1.2760e+03 - logprior: -3.5555e+00
Epoch 3/10
19/19 - 7s - loss: 1273.8773 - loglik: -1.2742e+03 - logprior: 0.3695
Epoch 4/10
19/19 - 7s - loss: 1269.7588 - loglik: -1.2708e+03 - logprior: 1.0478
Epoch 5/10
19/19 - 7s - loss: 1263.9900 - loglik: -1.2652e+03 - logprior: 1.2523
Epoch 6/10
19/19 - 7s - loss: 1253.3352 - loglik: -1.2548e+03 - logprior: 1.4237
Epoch 7/10
19/19 - 7s - loss: 1226.5222 - loglik: -1.2279e+03 - logprior: 1.4281
Epoch 8/10
19/19 - 7s - loss: 1138.0968 - loglik: -1.1384e+03 - logprior: 0.2993
Epoch 9/10
19/19 - 7s - loss: 1008.3776 - loglik: -1.0056e+03 - logprior: -2.8067e+00
Epoch 10/10
19/19 - 7s - loss: 976.2783 - loglik: -9.7355e+02 - logprior: -2.7085e+00
Fitted a model with MAP estimate = -969.4483
Time for alignment: 188.4981
Fitting a model of length 198 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 10s - loss: 1378.3539 - loglik: -1.3688e+03 - logprior: -9.5921e+00
Epoch 2/10
19/19 - 6s - loss: 1313.9136 - loglik: -1.3132e+03 - logprior: -7.2383e-01
Epoch 3/10
19/19 - 6s - loss: 1284.5605 - loglik: -1.2837e+03 - logprior: -8.3628e-01
Epoch 4/10
19/19 - 6s - loss: 1277.1429 - loglik: -1.2766e+03 - logprior: -5.1335e-01
Epoch 5/10
19/19 - 6s - loss: 1269.0648 - loglik: -1.2687e+03 - logprior: -3.3137e-01
Epoch 6/10
19/19 - 6s - loss: 1253.5050 - loglik: -1.2533e+03 - logprior: -2.2355e-01
Epoch 7/10
19/19 - 6s - loss: 1220.3289 - loglik: -1.2200e+03 - logprior: -2.7146e-01
Epoch 8/10
19/19 - 6s - loss: 1130.3655 - loglik: -1.1283e+03 - logprior: -2.0942e+00
Epoch 9/10
19/19 - 6s - loss: 994.1567 - loglik: -9.8857e+02 - logprior: -5.5703e+00
Epoch 10/10
19/19 - 6s - loss: 970.0496 - loglik: -9.6413e+02 - logprior: -5.9039e+00
Fitted a model with MAP estimate = -970.5910
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  73  74  75  76  77  78  79  80  81 120 121 122 123 124
 125 126 127 131 132 133 134 135 136 137 138 139 140 141 142 148 171 172
 173 174 175 176]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1428.5729 - loglik: -1.4170e+03 - logprior: -1.1578e+01
Epoch 2/2
19/19 - 4s - loss: 1395.0657 - loglik: -1.3945e+03 - logprior: -5.3617e-01
Fitted a model with MAP estimate = -1390.6593
expansions: [(0, 131), (122, 99)]
discards: [  5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22
  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40
  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58
  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76
  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94
  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112
 113 114 115 116 117 118 119 120 121]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 1369.5507 - loglik: -1.3593e+03 - logprior: -1.0218e+01
Epoch 2/2
19/19 - 8s - loss: 1289.8171 - loglik: -1.2897e+03 - logprior: -1.0665e-01
Fitted a model with MAP estimate = -1276.1192
expansions: [(22, 1), (100, 2), (130, 2), (235, 17)]
discards: [  0 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 187
 188 202 203 204 205 206 207 208 209 211 220 221 222 223 224 228 229 230
 231 232 233 234]
Re-initialized the encoder parameters.
Fitting a model of length 217 on 2489 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 11s - loss: 1302.2271 - loglik: -1.2893e+03 - logprior: -1.2943e+01
Epoch 2/10
19/19 - 7s - loss: 1278.9681 - loglik: -1.2753e+03 - logprior: -3.6710e+00
Epoch 3/10
19/19 - 7s - loss: 1274.1274 - loglik: -1.2745e+03 - logprior: 0.3837
Epoch 4/10
19/19 - 7s - loss: 1268.4302 - loglik: -1.2696e+03 - logprior: 1.2021
Epoch 5/10
19/19 - 7s - loss: 1265.3741 - loglik: -1.2668e+03 - logprior: 1.4154
Epoch 6/10
19/19 - 7s - loss: 1253.5645 - loglik: -1.2551e+03 - logprior: 1.5118
Epoch 7/10
19/19 - 7s - loss: 1229.3181 - loglik: -1.2307e+03 - logprior: 1.4232
Epoch 8/10
19/19 - 7s - loss: 1122.9722 - loglik: -1.1227e+03 - logprior: -2.7992e-01
Epoch 9/10
19/19 - 7s - loss: 998.5356 - loglik: -9.9499e+02 - logprior: -3.5312e+00
Epoch 10/10
19/19 - 7s - loss: 974.0045 - loglik: -9.7081e+02 - logprior: -3.1795e+00
Fitted a model with MAP estimate = -968.7607
Time for alignment: 186.3350
Computed alignments with likelihoods: ['-966.1679', '-969.3878', '-969.1508', '-969.4483', '-968.7607']
Best model has likelihood: -966.1679  (prior= -1.6839 )
time for generating output: 0.3087
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Sulfotransfer.projection.fasta
SP score = 0.024634655532359082
Training of 5 independent models on file hom.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efa9fdf0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9d614790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea095fe550>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 313.9383 - loglik: -3.1070e+02 - logprior: -3.2334e+00
Epoch 2/10
19/19 - 1s - loss: 279.0268 - loglik: -2.7751e+02 - logprior: -1.5196e+00
Epoch 3/10
19/19 - 1s - loss: 266.2807 - loglik: -2.6470e+02 - logprior: -1.5800e+00
Epoch 4/10
19/19 - 1s - loss: 263.2524 - loglik: -2.6171e+02 - logprior: -1.5452e+00
Epoch 5/10
19/19 - 1s - loss: 262.1967 - loglik: -2.6069e+02 - logprior: -1.5071e+00
Epoch 6/10
19/19 - 1s - loss: 261.6030 - loglik: -2.6011e+02 - logprior: -1.4885e+00
Epoch 7/10
19/19 - 1s - loss: 260.6277 - loglik: -2.5914e+02 - logprior: -1.4804e+00
Epoch 8/10
19/19 - 1s - loss: 260.0967 - loglik: -2.5861e+02 - logprior: -1.4751e+00
Epoch 9/10
19/19 - 1s - loss: 259.5930 - loglik: -2.5810e+02 - logprior: -1.4802e+00
Epoch 10/10
19/19 - 1s - loss: 258.6787 - loglik: -2.5718e+02 - logprior: -1.4901e+00
Fitted a model with MAP estimate = -245.2317
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 2), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 265.6559 - loglik: -2.6236e+02 - logprior: -3.2956e+00
Epoch 2/2
19/19 - 1s - loss: 254.5365 - loglik: -2.5311e+02 - logprior: -1.4228e+00
Fitted a model with MAP estimate = -239.6898
expansions: []
discards: [12 36 39 42]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 257.7805 - loglik: -2.5462e+02 - logprior: -3.1597e+00
Epoch 2/2
19/19 - 1s - loss: 253.5618 - loglik: -2.5230e+02 - logprior: -1.2627e+00
Fitted a model with MAP estimate = -239.4305
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 242.2893 - loglik: -2.3972e+02 - logprior: -2.5657e+00
Epoch 2/10
21/21 - 1s - loss: 238.5524 - loglik: -2.3731e+02 - logprior: -1.2403e+00
Epoch 3/10
21/21 - 1s - loss: 238.6013 - loglik: -2.3744e+02 - logprior: -1.1563e+00
Fitted a model with MAP estimate = -237.7660
Time for alignment: 39.9426
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 313.7965 - loglik: -3.1057e+02 - logprior: -3.2301e+00
Epoch 2/10
19/19 - 1s - loss: 278.4048 - loglik: -2.7690e+02 - logprior: -1.5064e+00
Epoch 3/10
19/19 - 1s - loss: 265.9282 - loglik: -2.6435e+02 - logprior: -1.5758e+00
Epoch 4/10
19/19 - 1s - loss: 263.1176 - loglik: -2.6158e+02 - logprior: -1.5394e+00
Epoch 5/10
19/19 - 1s - loss: 262.3059 - loglik: -2.6080e+02 - logprior: -1.5080e+00
Epoch 6/10
19/19 - 1s - loss: 261.6690 - loglik: -2.6017e+02 - logprior: -1.4911e+00
Epoch 7/10
19/19 - 1s - loss: 260.4382 - loglik: -2.5895e+02 - logprior: -1.4803e+00
Epoch 8/10
19/19 - 1s - loss: 260.5746 - loglik: -2.5909e+02 - logprior: -1.4769e+00
Fitted a model with MAP estimate = -246.1633
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 2), (30, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 61 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 264.8946 - loglik: -2.6160e+02 - logprior: -3.2933e+00
Epoch 2/2
19/19 - 1s - loss: 254.4917 - loglik: -2.5307e+02 - logprior: -1.4224e+00
Fitted a model with MAP estimate = -239.6976
expansions: []
discards: [11 36 39 42]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 258.2101 - loglik: -2.5505e+02 - logprior: -3.1632e+00
Epoch 2/2
19/19 - 1s - loss: 253.3795 - loglik: -2.5212e+02 - logprior: -1.2557e+00
Fitted a model with MAP estimate = -239.4194
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 5s - loss: 242.3700 - loglik: -2.3980e+02 - logprior: -2.5657e+00
Epoch 2/10
21/21 - 1s - loss: 238.3186 - loglik: -2.3708e+02 - logprior: -1.2358e+00
Epoch 3/10
21/21 - 1s - loss: 238.2230 - loglik: -2.3707e+02 - logprior: -1.1524e+00
Epoch 4/10
21/21 - 1s - loss: 238.1287 - loglik: -2.3703e+02 - logprior: -1.0965e+00
Epoch 5/10
21/21 - 1s - loss: 236.9771 - loglik: -2.3589e+02 - logprior: -1.0884e+00
Epoch 6/10
21/21 - 1s - loss: 236.5767 - loglik: -2.3550e+02 - logprior: -1.0667e+00
Epoch 7/10
21/21 - 1s - loss: 235.6678 - loglik: -2.3457e+02 - logprior: -1.0855e+00
Epoch 8/10
21/21 - 1s - loss: 235.4441 - loglik: -2.3433e+02 - logprior: -1.1003e+00
Epoch 9/10
21/21 - 1s - loss: 233.8474 - loglik: -2.3271e+02 - logprior: -1.1272e+00
Epoch 10/10
21/21 - 1s - loss: 233.1118 - loglik: -2.3196e+02 - logprior: -1.1368e+00
Fitted a model with MAP estimate = -232.2140
Time for alignment: 46.2136
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 313.9515 - loglik: -3.1072e+02 - logprior: -3.2352e+00
Epoch 2/10
19/19 - 1s - loss: 279.8355 - loglik: -2.7830e+02 - logprior: -1.5365e+00
Epoch 3/10
19/19 - 1s - loss: 266.2857 - loglik: -2.6467e+02 - logprior: -1.6141e+00
Epoch 4/10
19/19 - 1s - loss: 262.8984 - loglik: -2.6131e+02 - logprior: -1.5919e+00
Epoch 5/10
19/19 - 1s - loss: 261.6208 - loglik: -2.6007e+02 - logprior: -1.5463e+00
Epoch 6/10
19/19 - 1s - loss: 261.0734 - loglik: -2.5953e+02 - logprior: -1.5355e+00
Epoch 7/10
19/19 - 1s - loss: 260.3188 - loglik: -2.5879e+02 - logprior: -1.5199e+00
Epoch 8/10
19/19 - 1s - loss: 260.1654 - loglik: -2.5864e+02 - logprior: -1.5162e+00
Epoch 9/10
19/19 - 1s - loss: 258.8794 - loglik: -2.5734e+02 - logprior: -1.5251e+00
Epoch 10/10
19/19 - 1s - loss: 258.4103 - loglik: -2.5686e+02 - logprior: -1.5329e+00
Fitted a model with MAP estimate = -244.7622
expansions: [(5, 1), (6, 1), (7, 1), (9, 1), (13, 1), (21, 1), (24, 1), (28, 1), (29, 2), (30, 1), (31, 2), (32, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 265.2829 - loglik: -2.6203e+02 - logprior: -3.2554e+00
Epoch 2/2
19/19 - 1s - loss: 254.4250 - loglik: -2.5304e+02 - logprior: -1.3816e+00
Fitted a model with MAP estimate = -239.6978
expansions: []
discards: [37 42 45]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.7332 - loglik: -2.5459e+02 - logprior: -3.1455e+00
Epoch 2/2
19/19 - 1s - loss: 253.3592 - loglik: -2.5210e+02 - logprior: -1.2568e+00
Fitted a model with MAP estimate = -239.5099
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 242.1891 - loglik: -2.3961e+02 - logprior: -2.5777e+00
Epoch 2/10
21/21 - 1s - loss: 238.2839 - loglik: -2.3704e+02 - logprior: -1.2394e+00
Epoch 3/10
21/21 - 1s - loss: 238.2347 - loglik: -2.3708e+02 - logprior: -1.1528e+00
Epoch 4/10
21/21 - 1s - loss: 237.7014 - loglik: -2.3658e+02 - logprior: -1.1148e+00
Epoch 5/10
21/21 - 1s - loss: 237.4073 - loglik: -2.3631e+02 - logprior: -1.0916e+00
Epoch 6/10
21/21 - 1s - loss: 236.8767 - loglik: -2.3578e+02 - logprior: -1.0948e+00
Epoch 7/10
21/21 - 1s - loss: 235.5337 - loglik: -2.3443e+02 - logprior: -1.0937e+00
Epoch 8/10
21/21 - 1s - loss: 234.9862 - loglik: -2.3388e+02 - logprior: -1.0957e+00
Epoch 9/10
21/21 - 1s - loss: 234.2353 - loglik: -2.3310e+02 - logprior: -1.1206e+00
Epoch 10/10
21/21 - 1s - loss: 233.4271 - loglik: -2.3227e+02 - logprior: -1.1414e+00
Fitted a model with MAP estimate = -232.3332
Time for alignment: 46.1447
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 313.9809 - loglik: -3.1074e+02 - logprior: -3.2388e+00
Epoch 2/10
19/19 - 1s - loss: 279.4607 - loglik: -2.7792e+02 - logprior: -1.5399e+00
Epoch 3/10
19/19 - 1s - loss: 266.0690 - loglik: -2.6449e+02 - logprior: -1.5793e+00
Epoch 4/10
19/19 - 1s - loss: 262.9869 - loglik: -2.6141e+02 - logprior: -1.5796e+00
Epoch 5/10
19/19 - 1s - loss: 262.4566 - loglik: -2.6094e+02 - logprior: -1.5188e+00
Epoch 6/10
19/19 - 1s - loss: 261.2418 - loglik: -2.5974e+02 - logprior: -1.4933e+00
Epoch 7/10
19/19 - 1s - loss: 260.4759 - loglik: -2.5898e+02 - logprior: -1.4927e+00
Epoch 8/10
19/19 - 1s - loss: 259.3389 - loglik: -2.5783e+02 - logprior: -1.5038e+00
Epoch 9/10
19/19 - 1s - loss: 258.6013 - loglik: -2.5707e+02 - logprior: -1.5204e+00
Epoch 10/10
19/19 - 1s - loss: 258.0576 - loglik: -2.5652e+02 - logprior: -1.5231e+00
Fitted a model with MAP estimate = -244.3544
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (28, 1), (29, 2), (30, 2), (31, 1), (41, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 60 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 265.6559 - loglik: -2.6238e+02 - logprior: -3.2770e+00
Epoch 2/2
19/19 - 1s - loss: 254.5244 - loglik: -2.5313e+02 - logprior: -1.3931e+00
Fitted a model with MAP estimate = -239.7008
expansions: []
discards: [12 38 42]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 257.7961 - loglik: -2.5465e+02 - logprior: -3.1496e+00
Epoch 2/2
19/19 - 1s - loss: 253.3930 - loglik: -2.5214e+02 - logprior: -1.2548e+00
Fitted a model with MAP estimate = -239.4340
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 242.2997 - loglik: -2.3974e+02 - logprior: -2.5634e+00
Epoch 2/10
21/21 - 1s - loss: 238.6537 - loglik: -2.3742e+02 - logprior: -1.2290e+00
Epoch 3/10
21/21 - 1s - loss: 238.3944 - loglik: -2.3725e+02 - logprior: -1.1475e+00
Epoch 4/10
21/21 - 1s - loss: 236.9720 - loglik: -2.3587e+02 - logprior: -1.1017e+00
Epoch 5/10
21/21 - 1s - loss: 237.7472 - loglik: -2.3666e+02 - logprior: -1.0814e+00
Fitted a model with MAP estimate = -236.7265
Time for alignment: 41.6531
Fitting a model of length 45 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 313.7570 - loglik: -3.1053e+02 - logprior: -3.2315e+00
Epoch 2/10
19/19 - 1s - loss: 278.5716 - loglik: -2.7704e+02 - logprior: -1.5300e+00
Epoch 3/10
19/19 - 1s - loss: 266.5910 - loglik: -2.6501e+02 - logprior: -1.5791e+00
Epoch 4/10
19/19 - 1s - loss: 263.8281 - loglik: -2.6231e+02 - logprior: -1.5181e+00
Epoch 5/10
19/19 - 1s - loss: 262.6452 - loglik: -2.6117e+02 - logprior: -1.4737e+00
Epoch 6/10
19/19 - 1s - loss: 261.8762 - loglik: -2.6041e+02 - logprior: -1.4580e+00
Epoch 7/10
19/19 - 1s - loss: 261.4013 - loglik: -2.5995e+02 - logprior: -1.4424e+00
Epoch 8/10
19/19 - 1s - loss: 260.3337 - loglik: -2.5887e+02 - logprior: -1.4539e+00
Epoch 9/10
19/19 - 1s - loss: 259.6559 - loglik: -2.5816e+02 - logprior: -1.4809e+00
Epoch 10/10
19/19 - 1s - loss: 258.6555 - loglik: -2.5715e+02 - logprior: -1.4943e+00
Fitted a model with MAP estimate = -245.1853
expansions: [(5, 1), (6, 1), (7, 1), (8, 1), (9, 2), (21, 1), (24, 1), (27, 3), (28, 2), (29, 2), (30, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 62 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 266.2810 - loglik: -2.6298e+02 - logprior: -3.2991e+00
Epoch 2/2
19/19 - 1s - loss: 254.6086 - loglik: -2.5316e+02 - logprior: -1.4441e+00
Fitted a model with MAP estimate = -239.8209
expansions: []
discards: [12 36 39 43 45]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 257.8912 - loglik: -2.5474e+02 - logprior: -3.1557e+00
Epoch 2/2
19/19 - 1s - loss: 253.5910 - loglik: -2.5233e+02 - logprior: -1.2620e+00
Fitted a model with MAP estimate = -239.5190
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 57 on 12037 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 242.2592 - loglik: -2.3969e+02 - logprior: -2.5717e+00
Epoch 2/10
21/21 - 1s - loss: 238.5399 - loglik: -2.3731e+02 - logprior: -1.2330e+00
Epoch 3/10
21/21 - 1s - loss: 238.2260 - loglik: -2.3708e+02 - logprior: -1.1474e+00
Epoch 4/10
21/21 - 1s - loss: 237.7151 - loglik: -2.3661e+02 - logprior: -1.1038e+00
Epoch 5/10
21/21 - 1s - loss: 237.2653 - loglik: -2.3618e+02 - logprior: -1.0792e+00
Epoch 6/10
21/21 - 1s - loss: 236.5280 - loglik: -2.3544e+02 - logprior: -1.0787e+00
Epoch 7/10
21/21 - 1s - loss: 235.6791 - loglik: -2.3459e+02 - logprior: -1.0805e+00
Epoch 8/10
21/21 - 1s - loss: 235.2870 - loglik: -2.3418e+02 - logprior: -1.0995e+00
Epoch 9/10
21/21 - 1s - loss: 234.2254 - loglik: -2.3309e+02 - logprior: -1.1243e+00
Epoch 10/10
21/21 - 1s - loss: 233.1337 - loglik: -2.3197e+02 - logprior: -1.1449e+00
Fitted a model with MAP estimate = -232.3213
Time for alignment: 46.9490
Computed alignments with likelihoods: ['-237.7660', '-232.2140', '-232.3332', '-236.7265', '-232.3213']
Best model has likelihood: -232.2140  (prior= -1.1535 )
time for generating output: 0.1005
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hom.projection.fasta
SP score = 0.9240180296200902
Training of 5 independent models on file OTCace.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea11f83bb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe939460130>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea66b13a90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 867.4007 - loglik: -8.6271e+02 - logprior: -4.6921e+00
Epoch 2/10
27/27 - 4s - loss: 785.3361 - loglik: -7.8347e+02 - logprior: -1.8663e+00
Epoch 3/10
27/27 - 4s - loss: 769.8785 - loglik: -7.6800e+02 - logprior: -1.8762e+00
Epoch 4/10
27/27 - 4s - loss: 765.1612 - loglik: -7.6327e+02 - logprior: -1.8922e+00
Epoch 5/10
27/27 - 4s - loss: 764.1961 - loglik: -7.6234e+02 - logprior: -1.8539e+00
Epoch 6/10
27/27 - 4s - loss: 763.5059 - loglik: -7.6165e+02 - logprior: -1.8508e+00
Epoch 7/10
27/27 - 4s - loss: 762.0016 - loglik: -7.6014e+02 - logprior: -1.8546e+00
Epoch 8/10
27/27 - 4s - loss: 761.5302 - loglik: -7.5965e+02 - logprior: -1.8701e+00
Epoch 9/10
27/27 - 4s - loss: 760.4731 - loglik: -7.5857e+02 - logprior: -1.8997e+00
Epoch 10/10
27/27 - 4s - loss: 759.3616 - loglik: -7.5740e+02 - logprior: -1.9499e+00
Fitted a model with MAP estimate = -758.7219
expansions: [(0, 2), (9, 2), (18, 1), (25, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 1), (67, 1), (68, 1), (71, 1), (72, 1), (73, 1), (78, 1), (79, 1), (81, 1), (100, 2), (101, 1), (102, 1), (103, 2), (104, 2), (105, 1), (113, 1), (114, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 161 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 765.0249 - loglik: -7.5859e+02 - logprior: -6.4370e+00
Epoch 2/2
27/27 - 4s - loss: 742.0054 - loglik: -7.4072e+02 - logprior: -1.2868e+00
Fitted a model with MAP estimate = -741.1654
expansions: [(43, 1)]
discards: [  0  12  31  48 131 136 138]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 751.3899 - loglik: -7.4517e+02 - logprior: -6.2219e+00
Epoch 2/2
27/27 - 4s - loss: 743.4771 - loglik: -7.4234e+02 - logprior: -1.1361e+00
Fitted a model with MAP estimate = -741.7488
expansions: []
discards: [44 45]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 748.0692 - loglik: -7.4395e+02 - logprior: -4.1145e+00
Epoch 2/10
27/27 - 4s - loss: 742.8337 - loglik: -7.4212e+02 - logprior: -7.1631e-01
Epoch 3/10
27/27 - 4s - loss: 742.5333 - loglik: -7.4213e+02 - logprior: -4.0249e-01
Epoch 4/10
27/27 - 4s - loss: 740.7944 - loglik: -7.4050e+02 - logprior: -2.9606e-01
Epoch 5/10
27/27 - 4s - loss: 739.9350 - loglik: -7.3976e+02 - logprior: -1.7415e-01
Epoch 6/10
27/27 - 4s - loss: 738.7083 - loglik: -7.3859e+02 - logprior: -1.1824e-01
Epoch 7/10
27/27 - 4s - loss: 737.9651 - loglik: -7.3791e+02 - logprior: -5.0456e-02
Epoch 8/10
27/27 - 4s - loss: 737.1202 - loglik: -7.3711e+02 - logprior: 7.9983e-05
Epoch 9/10
27/27 - 4s - loss: 736.9114 - loglik: -7.3697e+02 - logprior: 0.0619
Epoch 10/10
27/27 - 4s - loss: 735.7205 - loglik: -7.3585e+02 - logprior: 0.1386
Fitted a model with MAP estimate = -735.1965
Time for alignment: 128.9925
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 868.6859 - loglik: -8.6399e+02 - logprior: -4.6964e+00
Epoch 2/10
27/27 - 4s - loss: 786.9705 - loglik: -7.8514e+02 - logprior: -1.8326e+00
Epoch 3/10
27/27 - 4s - loss: 772.2836 - loglik: -7.7049e+02 - logprior: -1.7886e+00
Epoch 4/10
27/27 - 4s - loss: 768.5438 - loglik: -7.6674e+02 - logprior: -1.7978e+00
Epoch 5/10
27/27 - 4s - loss: 766.2716 - loglik: -7.6443e+02 - logprior: -1.8375e+00
Epoch 6/10
27/27 - 4s - loss: 762.9540 - loglik: -7.6109e+02 - logprior: -1.8612e+00
Epoch 7/10
27/27 - 4s - loss: 763.2273 - loglik: -7.6132e+02 - logprior: -1.8984e+00
Fitted a model with MAP estimate = -762.3182
expansions: [(0, 2), (10, 1), (19, 1), (21, 1), (24, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (45, 1), (47, 2), (67, 2), (68, 1), (69, 1), (71, 1), (72, 1), (73, 1), (78, 1), (81, 1), (83, 2), (99, 1), (100, 1), (102, 1), (103, 2), (104, 2), (105, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 164 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 762.7568 - loglik: -7.5647e+02 - logprior: -6.2880e+00
Epoch 2/2
27/27 - 5s - loss: 742.6887 - loglik: -7.4132e+02 - logprior: -1.3655e+00
Fitted a model with MAP estimate = -741.7668
expansions: []
discards: [  0  29  48  67  89 113 139 141]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 751.9187 - loglik: -7.4566e+02 - logprior: -6.2571e+00
Epoch 2/2
27/27 - 4s - loss: 745.0986 - loglik: -7.4386e+02 - logprior: -1.2424e+00
Fitted a model with MAP estimate = -743.0464
expansions: []
discards: [44 45]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 749.0363 - loglik: -7.4489e+02 - logprior: -4.1495e+00
Epoch 2/10
27/27 - 4s - loss: 744.2304 - loglik: -7.4347e+02 - logprior: -7.5596e-01
Epoch 3/10
27/27 - 4s - loss: 743.5084 - loglik: -7.4306e+02 - logprior: -4.4535e-01
Epoch 4/10
27/27 - 4s - loss: 744.0156 - loglik: -7.4369e+02 - logprior: -3.2874e-01
Fitted a model with MAP estimate = -741.6057
Time for alignment: 93.4220
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 868.8509 - loglik: -8.6416e+02 - logprior: -4.6949e+00
Epoch 2/10
27/27 - 4s - loss: 789.4618 - loglik: -7.8760e+02 - logprior: -1.8636e+00
Epoch 3/10
27/27 - 4s - loss: 773.1195 - loglik: -7.7136e+02 - logprior: -1.7555e+00
Epoch 4/10
27/27 - 4s - loss: 766.9618 - loglik: -7.6520e+02 - logprior: -1.7632e+00
Epoch 5/10
27/27 - 4s - loss: 766.9856 - loglik: -7.6524e+02 - logprior: -1.7431e+00
Fitted a model with MAP estimate = -765.4916
expansions: [(0, 2), (10, 1), (19, 1), (21, 1), (24, 2), (26, 1), (36, 2), (37, 4), (38, 1), (39, 1), (41, 1), (42, 1), (47, 1), (67, 1), (68, 1), (69, 1), (71, 2), (73, 1), (78, 1), (82, 1), (83, 2), (100, 1), (103, 1), (104, 2), (105, 2), (106, 1), (114, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 762.6871 - loglik: -7.5658e+02 - logprior: -6.1107e+00
Epoch 2/2
27/27 - 4s - loss: 746.6714 - loglik: -7.4536e+02 - logprior: -1.3155e+00
Fitted a model with MAP estimate = -745.5335
expansions: [(154, 1)]
discards: [  0  29  47  48  89  93 110 136 138]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 756.7856 - loglik: -7.5049e+02 - logprior: -6.2921e+00
Epoch 2/2
27/27 - 4s - loss: 748.8077 - loglik: -7.4760e+02 - logprior: -1.2030e+00
Fitted a model with MAP estimate = -747.5135
expansions: [(88, 1)]
discards: [145]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 753.1028 - loglik: -7.4897e+02 - logprior: -4.1361e+00
Epoch 2/10
27/27 - 4s - loss: 748.6906 - loglik: -7.4794e+02 - logprior: -7.4880e-01
Epoch 3/10
27/27 - 4s - loss: 747.1578 - loglik: -7.4672e+02 - logprior: -4.3203e-01
Epoch 4/10
27/27 - 4s - loss: 746.4093 - loglik: -7.4609e+02 - logprior: -3.1968e-01
Epoch 5/10
27/27 - 4s - loss: 744.8225 - loglik: -7.4460e+02 - logprior: -2.2396e-01
Epoch 6/10
27/27 - 4s - loss: 743.4224 - loglik: -7.4326e+02 - logprior: -1.5873e-01
Epoch 7/10
27/27 - 4s - loss: 743.5743 - loglik: -7.4349e+02 - logprior: -8.3584e-02
Fitted a model with MAP estimate = -742.1516
Time for alignment: 97.3142
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 6s - loss: 870.4600 - loglik: -8.6576e+02 - logprior: -4.7036e+00
Epoch 2/10
27/27 - 4s - loss: 787.4824 - loglik: -7.8569e+02 - logprior: -1.7904e+00
Epoch 3/10
27/27 - 4s - loss: 773.5820 - loglik: -7.7182e+02 - logprior: -1.7616e+00
Epoch 4/10
27/27 - 4s - loss: 768.8135 - loglik: -7.6708e+02 - logprior: -1.7308e+00
Epoch 5/10
27/27 - 4s - loss: 765.5329 - loglik: -7.6379e+02 - logprior: -1.7366e+00
Epoch 6/10
27/27 - 4s - loss: 764.1877 - loglik: -7.6241e+02 - logprior: -1.7717e+00
Epoch 7/10
27/27 - 4s - loss: 763.5668 - loglik: -7.6175e+02 - logprior: -1.8101e+00
Epoch 8/10
27/27 - 4s - loss: 762.2557 - loglik: -7.6042e+02 - logprior: -1.8237e+00
Epoch 9/10
27/27 - 4s - loss: 760.9891 - loglik: -7.5914e+02 - logprior: -1.8368e+00
Epoch 10/10
27/27 - 4s - loss: 760.3276 - loglik: -7.5847e+02 - logprior: -1.8482e+00
Fitted a model with MAP estimate = -758.8153
expansions: [(0, 2), (9, 1), (21, 1), (24, 2), (26, 1), (36, 2), (37, 2), (38, 2), (39, 1), (40, 1), (42, 1), (43, 1), (48, 1), (68, 1), (69, 2), (71, 1), (72, 1), (73, 1), (78, 1), (79, 1), (81, 1), (100, 1), (101, 1), (102, 1), (103, 2), (104, 2), (105, 1), (115, 1), (116, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 159 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 8s - loss: 769.9464 - loglik: -7.6364e+02 - logprior: -6.3044e+00
Epoch 2/2
27/27 - 4s - loss: 745.9279 - loglik: -7.4472e+02 - logprior: -1.2120e+00
Fitted a model with MAP estimate = -743.6597
expansions: [(42, 1)]
discards: [  0  28 134 136]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 751.8406 - loglik: -7.4571e+02 - logprior: -6.1281e+00
Epoch 2/2
27/27 - 4s - loss: 744.3866 - loglik: -7.4344e+02 - logprior: -9.4417e-01
Fitted a model with MAP estimate = -743.1436
expansions: [(46, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 157 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 748.9258 - loglik: -7.4496e+02 - logprior: -3.9690e+00
Epoch 2/10
27/27 - 4s - loss: 742.6901 - loglik: -7.4212e+02 - logprior: -5.7344e-01
Epoch 3/10
27/27 - 4s - loss: 742.6169 - loglik: -7.4233e+02 - logprior: -2.8688e-01
Epoch 4/10
27/27 - 4s - loss: 740.7845 - loglik: -7.4063e+02 - logprior: -1.5367e-01
Epoch 5/10
27/27 - 4s - loss: 740.5507 - loglik: -7.4050e+02 - logprior: -4.7666e-02
Epoch 6/10
27/27 - 4s - loss: 739.2823 - loglik: -7.3929e+02 - logprior: 0.0087
Epoch 7/10
27/27 - 4s - loss: 737.4116 - loglik: -7.3739e+02 - logprior: -1.7002e-02
Epoch 8/10
27/27 - 4s - loss: 738.2151 - loglik: -7.3826e+02 - logprior: 0.0473
Fitted a model with MAP estimate = -736.6225
Time for alignment: 120.1158
Fitting a model of length 122 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 8s - loss: 869.0487 - loglik: -8.6435e+02 - logprior: -4.7027e+00
Epoch 2/10
27/27 - 4s - loss: 785.4283 - loglik: -7.8373e+02 - logprior: -1.6962e+00
Epoch 3/10
27/27 - 4s - loss: 773.0480 - loglik: -7.7142e+02 - logprior: -1.6234e+00
Epoch 4/10
27/27 - 4s - loss: 768.2115 - loglik: -7.6655e+02 - logprior: -1.6613e+00
Epoch 5/10
27/27 - 4s - loss: 767.7384 - loglik: -7.6607e+02 - logprior: -1.6612e+00
Epoch 6/10
27/27 - 4s - loss: 765.6300 - loglik: -7.6396e+02 - logprior: -1.6676e+00
Epoch 7/10
27/27 - 4s - loss: 765.5216 - loglik: -7.6383e+02 - logprior: -1.6901e+00
Epoch 8/10
27/27 - 4s - loss: 763.3647 - loglik: -7.6161e+02 - logprior: -1.7472e+00
Epoch 9/10
27/27 - 4s - loss: 762.9102 - loglik: -7.6110e+02 - logprior: -1.7985e+00
Epoch 10/10
27/27 - 4s - loss: 760.8499 - loglik: -7.5900e+02 - logprior: -1.8345e+00
Fitted a model with MAP estimate = -759.9733
expansions: [(0, 2), (10, 1), (21, 1), (24, 2), (26, 1), (35, 1), (36, 1), (37, 4), (38, 1), (39, 1), (43, 1), (45, 1), (47, 1), (67, 1), (68, 1), (69, 1), (71, 2), (72, 1), (73, 1), (78, 1), (79, 1), (81, 1), (100, 1), (101, 1), (102, 1), (103, 2), (104, 2), (105, 1), (114, 1), (115, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 160 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 768.6019 - loglik: -7.6216e+02 - logprior: -6.4464e+00
Epoch 2/2
27/27 - 4s - loss: 745.2078 - loglik: -7.4397e+02 - logprior: -1.2388e+00
Fitted a model with MAP estimate = -743.2017
expansions: []
discards: [  0  28  46  47  93 135 137]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
27/27 - 7s - loss: 753.0587 - loglik: -7.4694e+02 - logprior: -6.1154e+00
Epoch 2/2
27/27 - 4s - loss: 746.3237 - loglik: -7.4532e+02 - logprior: -9.9996e-01
Fitted a model with MAP estimate = -744.4787
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 153 on 4795 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
27/27 - 7s - loss: 749.0930 - loglik: -7.4510e+02 - logprior: -3.9922e+00
Epoch 2/10
27/27 - 4s - loss: 744.8911 - loglik: -7.4429e+02 - logprior: -6.0309e-01
Epoch 3/10
27/27 - 4s - loss: 743.1365 - loglik: -7.4285e+02 - logprior: -2.8856e-01
Epoch 4/10
27/27 - 4s - loss: 743.5823 - loglik: -7.4341e+02 - logprior: -1.7051e-01
Fitted a model with MAP estimate = -741.8921
Time for alignment: 102.3324
Computed alignments with likelihoods: ['-735.1965', '-741.6057', '-742.1516', '-736.6225', '-741.8921']
Best model has likelihood: -735.1965  (prior= 0.1644 )
time for generating output: 0.2844
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/OTCace.projection.fasta
SP score = 0.507641196013289
Training of 5 independent models on file lyase_1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c0fdb80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9eff21b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ab4fe7c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 1608.4713 - loglik: -1.6063e+03 - logprior: -2.1286e+00
Epoch 2/10
34/34 - 14s - loss: 1484.2089 - loglik: -1.4826e+03 - logprior: -1.6351e+00
Epoch 3/10
34/34 - 14s - loss: 1465.3315 - loglik: -1.4636e+03 - logprior: -1.7465e+00
Epoch 4/10
34/34 - 14s - loss: 1460.6647 - loglik: -1.4590e+03 - logprior: -1.6733e+00
Epoch 5/10
34/34 - 14s - loss: 1455.6979 - loglik: -1.4540e+03 - logprior: -1.6440e+00
Epoch 6/10
34/34 - 14s - loss: 1453.6299 - loglik: -1.4520e+03 - logprior: -1.6394e+00
Epoch 7/10
34/34 - 14s - loss: 1450.0200 - loglik: -1.4484e+03 - logprior: -1.6436e+00
Epoch 8/10
34/34 - 14s - loss: 1434.8734 - loglik: -1.4332e+03 - logprior: -1.6987e+00
Epoch 9/10
34/34 - 14s - loss: 1425.8468 - loglik: -1.4241e+03 - logprior: -1.7687e+00
Epoch 10/10
34/34 - 14s - loss: 1403.3650 - loglik: -1.4015e+03 - logprior: -1.8102e+00
Fitted a model with MAP estimate = -1382.3091
expansions: [(13, 1), (14, 1), (27, 1), (29, 1), (30, 2), (39, 1), (48, 1), (51, 1), (54, 1), (65, 1), (66, 2), (67, 1), (91, 1), (93, 2), (94, 1), (99, 1), (100, 2), (101, 1), (102, 1), (104, 1), (109, 1), (131, 1), (134, 1), (139, 1), (142, 1), (143, 2), (144, 1), (147, 3), (164, 1), (167, 1), (172, 2), (175, 1), (178, 1), (188, 1), (190, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 284 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 21s - loss: 1629.9412 - loglik: -1.6272e+03 - logprior: -2.7813e+00
Epoch 2/2
34/34 - 18s - loss: 1498.3014 - loglik: -1.4967e+03 - logprior: -1.6071e+00
Fitted a model with MAP estimate = -1478.6972
expansions: [(0, 34), (109, 1), (113, 1), (159, 1), (169, 1), (170, 1), (198, 1), (207, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74 105 106 111 160 175 176 177 179 199 242 243 244 245 246 247
 248 275 276 277]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 17s - loss: 1502.2631 - loglik: -1.5001e+03 - logprior: -2.2087e+00
Epoch 2/2
34/34 - 14s - loss: 1481.0632 - loglik: -1.4805e+03 - logprior: -5.4845e-01
Fitted a model with MAP estimate = -1480.7910
expansions: [(9, 1), (17, 2), (22, 1), (23, 1), (38, 1), (39, 7), (40, 33), (44, 6), (136, 1), (199, 4), (225, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 290 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 1460.0465 - loglik: -1.4570e+03 - logprior: -3.0153e+00
Epoch 2/10
34/34 - 19s - loss: 1435.2927 - loglik: -1.4346e+03 - logprior: -6.6033e-01
Epoch 3/10
34/34 - 19s - loss: 1428.3623 - loglik: -1.4284e+03 - logprior: 0.0511
Epoch 4/10
34/34 - 19s - loss: 1426.3944 - loglik: -1.4266e+03 - logprior: 0.2238
Epoch 5/10
34/34 - 19s - loss: 1416.5721 - loglik: -1.4169e+03 - logprior: 0.3693
Epoch 6/10
34/34 - 19s - loss: 1421.6199 - loglik: -1.4221e+03 - logprior: 0.4636
Fitted a model with MAP estimate = -1415.5757
Time for alignment: 413.6933
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 1611.3828 - loglik: -1.6092e+03 - logprior: -2.1353e+00
Epoch 2/10
34/34 - 14s - loss: 1485.9341 - loglik: -1.4843e+03 - logprior: -1.6309e+00
Epoch 3/10
34/34 - 14s - loss: 1465.5857 - loglik: -1.4638e+03 - logprior: -1.7854e+00
Epoch 4/10
34/34 - 14s - loss: 1463.1735 - loglik: -1.4614e+03 - logprior: -1.7469e+00
Epoch 5/10
34/34 - 14s - loss: 1460.8821 - loglik: -1.4592e+03 - logprior: -1.7192e+00
Epoch 6/10
34/34 - 14s - loss: 1456.8361 - loglik: -1.4551e+03 - logprior: -1.7276e+00
Epoch 7/10
34/34 - 14s - loss: 1449.9998 - loglik: -1.4482e+03 - logprior: -1.7434e+00
Epoch 8/10
34/34 - 14s - loss: 1440.3979 - loglik: -1.4386e+03 - logprior: -1.7712e+00
Epoch 9/10
34/34 - 14s - loss: 1426.5227 - loglik: -1.4246e+03 - logprior: -1.8641e+00
Epoch 10/10
34/34 - 14s - loss: 1399.9430 - loglik: -1.3980e+03 - logprior: -1.8785e+00
Fitted a model with MAP estimate = -1381.6934
expansions: [(9, 1), (12, 1), (14, 1), (15, 1), (16, 1), (21, 1), (22, 2), (29, 1), (30, 2), (39, 1), (49, 1), (51, 2), (56, 3), (57, 1), (61, 1), (65, 1), (67, 2), (92, 1), (94, 1), (96, 2), (99, 2), (101, 2), (102, 4), (104, 1), (109, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (165, 1), (166, 1), (167, 1), (172, 1), (173, 1), (175, 1), (178, 1), (185, 1), (186, 4), (187, 1), (199, 1), (203, 1), (204, 2), (208, 1), (211, 1), (225, 1), (229, 1), (230, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 1621.2244 - loglik: -1.6185e+03 - logprior: -2.7704e+00
Epoch 2/2
34/34 - 20s - loss: 1483.3773 - loglik: -1.4813e+03 - logprior: -2.0296e+00
Fitted a model with MAP estimate = -1465.6034
expansions: [(0, 66), (114, 2), (118, 1), (119, 2), (121, 1), (122, 1), (181, 1), (182, 1), (184, 2), (185, 2), (209, 1), (218, 2), (232, 4), (236, 4), (237, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 115 116 126 127 135 186 188 189 190 191 192 210 240
 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260
 261 262]
Re-initialized the encoder parameters.
Fitting a model of length 247 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 18s - loss: 1511.3900 - loglik: -1.5093e+03 - logprior: -2.0640e+00
Epoch 2/2
34/34 - 15s - loss: 1467.1826 - loglik: -1.4664e+03 - logprior: -8.0646e-01
Fitted a model with MAP estimate = -1461.3651
expansions: [(0, 6), (4, 13), (5, 5), (9, 5), (10, 3), (11, 2), (12, 2), (13, 2), (16, 1), (19, 2), (20, 6), (21, 1), (22, 2), (23, 1), (44, 1), (45, 1), (46, 1), (140, 1), (199, 3), (208, 1), (209, 8), (218, 2)]
discards: [  0  24  25  26  27  28  29  30  31  32  33  78 173 192]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 1452.1897 - loglik: -1.4504e+03 - logprior: -1.7626e+00
Epoch 2/10
34/34 - 20s - loss: 1423.1173 - loglik: -1.4229e+03 - logprior: -1.9301e-01
Epoch 3/10
34/34 - 20s - loss: 1420.7371 - loglik: -1.4207e+03 - logprior: -1.3860e-02
Epoch 4/10
34/34 - 20s - loss: 1413.6683 - loglik: -1.4138e+03 - logprior: 0.1059
Epoch 5/10
34/34 - 20s - loss: 1413.0292 - loglik: -1.4132e+03 - logprior: 0.1456
Epoch 6/10
34/34 - 20s - loss: 1407.2119 - loglik: -1.4074e+03 - logprior: 0.2159
Epoch 7/10
34/34 - 20s - loss: 1403.4705 - loglik: -1.4037e+03 - logprior: 0.2810
Epoch 8/10
34/34 - 20s - loss: 1392.3824 - loglik: -1.3926e+03 - logprior: 0.2734
Epoch 9/10
34/34 - 20s - loss: 1337.2769 - loglik: -1.3374e+03 - logprior: 0.1867
Epoch 10/10
34/34 - 20s - loss: 1236.5084 - loglik: -1.2354e+03 - logprior: -1.0501e+00
Fitted a model with MAP estimate = -1183.2854
Time for alignment: 514.0325
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 19s - loss: 1609.6160 - loglik: -1.6075e+03 - logprior: -2.1148e+00
Epoch 2/10
34/34 - 14s - loss: 1482.7872 - loglik: -1.4812e+03 - logprior: -1.5395e+00
Epoch 3/10
34/34 - 14s - loss: 1469.6567 - loglik: -1.4680e+03 - logprior: -1.6801e+00
Epoch 4/10
34/34 - 14s - loss: 1463.7523 - loglik: -1.4621e+03 - logprior: -1.6794e+00
Epoch 5/10
34/34 - 14s - loss: 1456.0963 - loglik: -1.4544e+03 - logprior: -1.7165e+00
Epoch 6/10
34/34 - 14s - loss: 1457.0856 - loglik: -1.4554e+03 - logprior: -1.7261e+00
Fitted a model with MAP estimate = -1451.2493
expansions: [(13, 2), (14, 1), (15, 3), (16, 1), (18, 2), (22, 2), (28, 1), (29, 2), (37, 1), (50, 2), (56, 1), (64, 1), (65, 1), (66, 1), (67, 1), (92, 1), (93, 1), (94, 2), (95, 2), (99, 1), (100, 1), (101, 1), (102, 1), (104, 1), (109, 1), (110, 1), (134, 1), (139, 1), (142, 2), (143, 1), (144, 2), (147, 1), (165, 1), (167, 1), (171, 2), (172, 1), (175, 1), (178, 1), (185, 3), (186, 3), (188, 2), (190, 1), (203, 1), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 23s - loss: 1449.8799 - loglik: -1.4466e+03 - logprior: -3.3015e+00
Epoch 2/2
34/34 - 20s - loss: 1417.8856 - loglik: -1.4166e+03 - logprior: -1.2837e+00
Fitted a model with MAP estimate = -1414.9586
expansions: [(25, 1), (84, 1), (183, 1), (257, 1)]
discards: [ 31  40  81  82 118 120 243 258 259 260]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 1426.1481 - loglik: -1.4242e+03 - logprior: -1.9551e+00
Epoch 2/2
34/34 - 19s - loss: 1419.5013 - loglik: -1.4193e+03 - logprior: -2.1457e-01
Fitted a model with MAP estimate = -1417.9501
expansions: [(77, 1)]
discards: [80]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 1423.5168 - loglik: -1.4217e+03 - logprior: -1.8077e+00
Epoch 2/10
34/34 - 19s - loss: 1420.1741 - loglik: -1.4202e+03 - logprior: 0.0184
Epoch 3/10
34/34 - 20s - loss: 1416.8778 - loglik: -1.4171e+03 - logprior: 0.2476
Epoch 4/10
34/34 - 19s - loss: 1415.1716 - loglik: -1.4155e+03 - logprior: 0.3518
Epoch 5/10
34/34 - 19s - loss: 1411.7729 - loglik: -1.4123e+03 - logprior: 0.5278
Epoch 6/10
34/34 - 19s - loss: 1411.3207 - loglik: -1.4120e+03 - logprior: 0.6473
Epoch 7/10
34/34 - 19s - loss: 1406.4312 - loglik: -1.4072e+03 - logprior: 0.7299
Epoch 8/10
34/34 - 20s - loss: 1396.3850 - loglik: -1.3971e+03 - logprior: 0.7630
Epoch 9/10
34/34 - 19s - loss: 1357.5980 - loglik: -1.3582e+03 - logprior: 0.6443
Epoch 10/10
34/34 - 19s - loss: 1270.4258 - loglik: -1.2702e+03 - logprior: -1.4235e-01
Fitted a model with MAP estimate = -1203.9820
Time for alignment: 466.6789
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 17s - loss: 1610.8027 - loglik: -1.6087e+03 - logprior: -2.1082e+00
Epoch 2/10
34/34 - 14s - loss: 1482.8842 - loglik: -1.4814e+03 - logprior: -1.5083e+00
Epoch 3/10
34/34 - 14s - loss: 1467.7432 - loglik: -1.4661e+03 - logprior: -1.6888e+00
Epoch 4/10
34/34 - 14s - loss: 1461.3602 - loglik: -1.4597e+03 - logprior: -1.6945e+00
Epoch 5/10
34/34 - 14s - loss: 1457.4344 - loglik: -1.4557e+03 - logprior: -1.6816e+00
Epoch 6/10
34/34 - 14s - loss: 1454.1392 - loglik: -1.4525e+03 - logprior: -1.6632e+00
Epoch 7/10
34/34 - 14s - loss: 1448.3455 - loglik: -1.4467e+03 - logprior: -1.6587e+00
Epoch 8/10
34/34 - 14s - loss: 1438.2544 - loglik: -1.4365e+03 - logprior: -1.6947e+00
Epoch 9/10
34/34 - 14s - loss: 1429.0012 - loglik: -1.4272e+03 - logprior: -1.7735e+00
Epoch 10/10
34/34 - 14s - loss: 1406.4623 - loglik: -1.4046e+03 - logprior: -1.8636e+00
Fitted a model with MAP estimate = -1387.5298
expansions: [(9, 1), (12, 1), (16, 1), (18, 5), (21, 1), (22, 2), (29, 1), (30, 2), (49, 1), (50, 1), (51, 1), (56, 4), (65, 1), (68, 1), (69, 2), (93, 2), (94, 1), (95, 1), (96, 1), (100, 1), (101, 1), (102, 1), (103, 1), (105, 1), (110, 1), (130, 1), (131, 1), (137, 1), (139, 1), (143, 3), (144, 1), (165, 1), (167, 3), (168, 1), (171, 1), (172, 2), (178, 1), (179, 1), (188, 1), (190, 1), (203, 1), (204, 3), (208, 1), (211, 1), (225, 1), (228, 1), (230, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 1610.4889 - loglik: -1.6078e+03 - logprior: -2.7065e+00
Epoch 2/2
34/34 - 19s - loss: 1474.3502 - loglik: -1.4726e+03 - logprior: -1.7149e+00
Fitted a model with MAP estimate = -1457.1864
expansions: [(0, 56), (132, 1), (168, 1), (181, 1), (182, 2), (184, 1), (208, 1), (222, 1), (261, 4)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  91  92
 119 129 166 187 188 189 190 209 213 215 220 263 264]
Re-initialized the encoder parameters.
Fitting a model of length 265 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 20s - loss: 1484.9302 - loglik: -1.4828e+03 - logprior: -2.1261e+00
Epoch 2/2
34/34 - 17s - loss: 1451.8320 - loglik: -1.4513e+03 - logprior: -5.0348e-01
Fitted a model with MAP estimate = -1448.3351
expansions: [(4, 4), (5, 1), (6, 3), (7, 6), (8, 2), (14, 2), (16, 1), (17, 1), (19, 3), (23, 1), (29, 2), (31, 3), (44, 2), (155, 1)]
discards: [  0 228 229]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 22s - loss: 1449.0659 - loglik: -1.4462e+03 - logprior: -2.8705e+00
Epoch 2/10
34/34 - 19s - loss: 1432.5165 - loglik: -1.4320e+03 - logprior: -4.8294e-01
Epoch 3/10
34/34 - 19s - loss: 1426.9838 - loglik: -1.4272e+03 - logprior: 0.2157
Epoch 4/10
34/34 - 19s - loss: 1428.3104 - loglik: -1.4287e+03 - logprior: 0.3478
Fitted a model with MAP estimate = -1421.7149
Time for alignment: 396.5133
Fitting a model of length 236 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 18s - loss: 1609.3937 - loglik: -1.6073e+03 - logprior: -2.1276e+00
Epoch 2/10
34/34 - 14s - loss: 1484.6296 - loglik: -1.4830e+03 - logprior: -1.6293e+00
Epoch 3/10
34/34 - 14s - loss: 1467.9497 - loglik: -1.4663e+03 - logprior: -1.6861e+00
Epoch 4/10
34/34 - 14s - loss: 1467.4491 - loglik: -1.4659e+03 - logprior: -1.5914e+00
Epoch 5/10
34/34 - 14s - loss: 1460.7703 - loglik: -1.4592e+03 - logprior: -1.5571e+00
Epoch 6/10
34/34 - 14s - loss: 1461.1045 - loglik: -1.4595e+03 - logprior: -1.5580e+00
Fitted a model with MAP estimate = -1456.6026
expansions: [(13, 2), (14, 1), (15, 3), (17, 6), (22, 1), (28, 3), (41, 1), (50, 2), (54, 2), (55, 1), (56, 1), (64, 1), (66, 2), (67, 1), (92, 1), (93, 1), (94, 1), (99, 1), (100, 2), (102, 1), (104, 1), (107, 1), (111, 1), (131, 1), (137, 1), (139, 1), (142, 1), (143, 3), (144, 1), (147, 1), (164, 4), (167, 1), (171, 1), (172, 2), (175, 1), (184, 2), (185, 1), (186, 2), (188, 1), (190, 1), (199, 1), (203, 1), (204, 1), (208, 1), (211, 1), (225, 1), (228, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 304 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 1452.6543 - loglik: -1.4494e+03 - logprior: -3.3031e+00
Epoch 2/2
34/34 - 20s - loss: 1425.8733 - loglik: -1.4246e+03 - logprior: -1.2812e+00
Fitted a model with MAP estimate = -1422.6600
expansions: [(43, 1), (68, 1), (211, 1), (239, 1)]
discards: [ 19  20  75  89 186 187]
Re-initialized the encoder parameters.
Fitting a model of length 302 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 24s - loss: 1430.9791 - loglik: -1.4289e+03 - logprior: -2.0810e+00
Epoch 2/2
34/34 - 20s - loss: 1422.4366 - loglik: -1.4221e+03 - logprior: -3.5841e-01
Fitted a model with MAP estimate = -1422.2304
expansions: [(19, 1)]
discards: [16 66]
Re-initialized the encoder parameters.
Fitting a model of length 301 on 7632 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 23s - loss: 1428.3671 - loglik: -1.4264e+03 - logprior: -1.9689e+00
Epoch 2/10
34/34 - 20s - loss: 1422.2958 - loglik: -1.4221e+03 - logprior: -1.8607e-01
Epoch 3/10
34/34 - 20s - loss: 1419.9418 - loglik: -1.4200e+03 - logprior: 0.0604
Epoch 4/10
34/34 - 20s - loss: 1422.5543 - loglik: -1.4227e+03 - logprior: 0.1841
Fitted a model with MAP estimate = -1416.9298
Time for alignment: 356.8885
Computed alignments with likelihoods: ['-1382.3091', '-1183.2854', '-1203.9820', '-1387.5298', '-1416.9298']
Best model has likelihood: -1183.2854  (prior= -2.0979 )
time for generating output: 0.3663
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/lyase_1.projection.fasta
SP score = 0.0
Training of 5 independent models on file toxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f83178e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea11d06a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9e753e370>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 458.1082 - loglik: -3.6987e+02 - logprior: -8.8241e+01
Epoch 2/10
10/10 - 1s - loss: 360.7769 - loglik: -3.3704e+02 - logprior: -2.3739e+01
Epoch 3/10
10/10 - 1s - loss: 329.8086 - loglik: -3.1859e+02 - logprior: -1.1219e+01
Epoch 4/10
10/10 - 1s - loss: 316.6403 - loglik: -3.0994e+02 - logprior: -6.7025e+00
Epoch 5/10
10/10 - 1s - loss: 309.2092 - loglik: -3.0481e+02 - logprior: -4.3971e+00
Epoch 6/10
10/10 - 1s - loss: 304.3210 - loglik: -3.0116e+02 - logprior: -3.1650e+00
Epoch 7/10
10/10 - 1s - loss: 301.8067 - loglik: -2.9947e+02 - logprior: -2.3374e+00
Epoch 8/10
10/10 - 1s - loss: 300.1873 - loglik: -2.9831e+02 - logprior: -1.8760e+00
Epoch 9/10
10/10 - 1s - loss: 299.0969 - loglik: -2.9752e+02 - logprior: -1.5766e+00
Epoch 10/10
10/10 - 1s - loss: 298.2838 - loglik: -2.9691e+02 - logprior: -1.3719e+00
Fitted a model with MAP estimate = -297.8909
expansions: [(7, 2), (8, 2), (13, 1), (16, 1), (24, 3), (26, 2), (33, 1), (36, 1), (37, 1), (38, 2), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 414.5125 - loglik: -3.1537e+02 - logprior: -9.9147e+01
Epoch 2/2
10/10 - 1s - loss: 337.7802 - loglik: -2.9693e+02 - logprior: -4.0846e+01
Fitted a model with MAP estimate = -324.7573
expansions: [(0, 2), (47, 1)]
discards: [ 0 29 30 34 51]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 380.1226 - loglik: -3.0108e+02 - logprior: -7.9042e+01
Epoch 2/2
10/10 - 1s - loss: 310.3078 - loglik: -2.8959e+02 - logprior: -2.0716e+01
Fitted a model with MAP estimate = -299.4945
expansions: [(9, 1), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 396.1062 - loglik: -2.9983e+02 - logprior: -9.6272e+01
Epoch 2/10
10/10 - 1s - loss: 323.9991 - loglik: -2.8997e+02 - logprior: -3.4032e+01
Epoch 3/10
10/10 - 1s - loss: 301.0569 - loglik: -2.8821e+02 - logprior: -1.2849e+01
Epoch 4/10
10/10 - 1s - loss: 290.5894 - loglik: -2.8633e+02 - logprior: -4.2588e+00
Epoch 5/10
10/10 - 1s - loss: 285.4386 - loglik: -2.8422e+02 - logprior: -1.2197e+00
Epoch 6/10
10/10 - 1s - loss: 282.6547 - loglik: -2.8290e+02 - logprior: 0.2454
Epoch 7/10
10/10 - 1s - loss: 281.1526 - loglik: -2.8232e+02 - logprior: 1.1663
Epoch 8/10
10/10 - 1s - loss: 280.1176 - loglik: -2.8202e+02 - logprior: 1.9025
Epoch 9/10
10/10 - 1s - loss: 279.2690 - loglik: -2.8176e+02 - logprior: 2.4951
Epoch 10/10
10/10 - 1s - loss: 278.4585 - loglik: -2.8139e+02 - logprior: 2.9309
Fitted a model with MAP estimate = -277.9872
Time for alignment: 30.0799
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 458.1082 - loglik: -3.6987e+02 - logprior: -8.8241e+01
Epoch 2/10
10/10 - 1s - loss: 360.7769 - loglik: -3.3704e+02 - logprior: -2.3739e+01
Epoch 3/10
10/10 - 1s - loss: 329.8086 - loglik: -3.1859e+02 - logprior: -1.1219e+01
Epoch 4/10
10/10 - 1s - loss: 316.6403 - loglik: -3.0994e+02 - logprior: -6.7025e+00
Epoch 5/10
10/10 - 1s - loss: 309.2092 - loglik: -3.0481e+02 - logprior: -4.3971e+00
Epoch 6/10
10/10 - 1s - loss: 304.3210 - loglik: -3.0116e+02 - logprior: -3.1650e+00
Epoch 7/10
10/10 - 1s - loss: 301.8067 - loglik: -2.9947e+02 - logprior: -2.3374e+00
Epoch 8/10
10/10 - 1s - loss: 300.1874 - loglik: -2.9831e+02 - logprior: -1.8760e+00
Epoch 9/10
10/10 - 1s - loss: 299.0969 - loglik: -2.9752e+02 - logprior: -1.5766e+00
Epoch 10/10
10/10 - 1s - loss: 298.2836 - loglik: -2.9691e+02 - logprior: -1.3719e+00
Fitted a model with MAP estimate = -297.8910
expansions: [(7, 2), (8, 2), (13, 1), (16, 1), (24, 3), (26, 2), (33, 1), (36, 1), (37, 1), (38, 2), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 414.5126 - loglik: -3.1537e+02 - logprior: -9.9147e+01
Epoch 2/2
10/10 - 1s - loss: 337.7802 - loglik: -2.9693e+02 - logprior: -4.0846e+01
Fitted a model with MAP estimate = -324.7574
expansions: [(0, 2), (47, 1)]
discards: [ 0 29 30 34 51]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 380.1226 - loglik: -3.0108e+02 - logprior: -7.9042e+01
Epoch 2/2
10/10 - 1s - loss: 310.3078 - loglik: -2.8959e+02 - logprior: -2.0716e+01
Fitted a model with MAP estimate = -299.4946
expansions: [(9, 1), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 396.1063 - loglik: -2.9983e+02 - logprior: -9.6272e+01
Epoch 2/10
10/10 - 1s - loss: 323.9991 - loglik: -2.8997e+02 - logprior: -3.4032e+01
Epoch 3/10
10/10 - 1s - loss: 301.0569 - loglik: -2.8821e+02 - logprior: -1.2849e+01
Epoch 4/10
10/10 - 1s - loss: 290.5894 - loglik: -2.8633e+02 - logprior: -4.2588e+00
Epoch 5/10
10/10 - 1s - loss: 285.4386 - loglik: -2.8422e+02 - logprior: -1.2197e+00
Epoch 6/10
10/10 - 1s - loss: 282.6547 - loglik: -2.8290e+02 - logprior: 0.2454
Epoch 7/10
10/10 - 1s - loss: 281.1526 - loglik: -2.8232e+02 - logprior: 1.1663
Epoch 8/10
10/10 - 1s - loss: 280.1176 - loglik: -2.8202e+02 - logprior: 1.9025
Epoch 9/10
10/10 - 1s - loss: 279.2690 - loglik: -2.8176e+02 - logprior: 2.4951
Epoch 10/10
10/10 - 1s - loss: 278.4587 - loglik: -2.8139e+02 - logprior: 2.9308
Fitted a model with MAP estimate = -277.9871
Time for alignment: 28.8615
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 458.1082 - loglik: -3.6987e+02 - logprior: -8.8241e+01
Epoch 2/10
10/10 - 1s - loss: 360.7769 - loglik: -3.3704e+02 - logprior: -2.3739e+01
Epoch 3/10
10/10 - 1s - loss: 329.8086 - loglik: -3.1859e+02 - logprior: -1.1219e+01
Epoch 4/10
10/10 - 1s - loss: 316.6403 - loglik: -3.0994e+02 - logprior: -6.7025e+00
Epoch 5/10
10/10 - 1s - loss: 309.2092 - loglik: -3.0481e+02 - logprior: -4.3971e+00
Epoch 6/10
10/10 - 1s - loss: 304.3210 - loglik: -3.0116e+02 - logprior: -3.1650e+00
Epoch 7/10
10/10 - 1s - loss: 301.8067 - loglik: -2.9947e+02 - logprior: -2.3375e+00
Epoch 8/10
10/10 - 1s - loss: 300.1874 - loglik: -2.9831e+02 - logprior: -1.8760e+00
Epoch 9/10
10/10 - 1s - loss: 299.0970 - loglik: -2.9752e+02 - logprior: -1.5766e+00
Epoch 10/10
10/10 - 1s - loss: 298.2836 - loglik: -2.9691e+02 - logprior: -1.3719e+00
Fitted a model with MAP estimate = -297.8908
expansions: [(7, 2), (8, 2), (13, 1), (16, 1), (24, 3), (26, 2), (33, 1), (36, 1), (37, 1), (38, 2), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 414.5126 - loglik: -3.1537e+02 - logprior: -9.9147e+01
Epoch 2/2
10/10 - 1s - loss: 337.7802 - loglik: -2.9693e+02 - logprior: -4.0846e+01
Fitted a model with MAP estimate = -324.7574
expansions: [(0, 2), (47, 1)]
discards: [ 0 29 30 34 51]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 380.1226 - loglik: -3.0108e+02 - logprior: -7.9042e+01
Epoch 2/2
10/10 - 1s - loss: 310.3079 - loglik: -2.8959e+02 - logprior: -2.0716e+01
Fitted a model with MAP estimate = -299.4946
expansions: [(9, 1), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 396.1063 - loglik: -2.9983e+02 - logprior: -9.6272e+01
Epoch 2/10
10/10 - 1s - loss: 323.9992 - loglik: -2.8997e+02 - logprior: -3.4032e+01
Epoch 3/10
10/10 - 1s - loss: 301.0569 - loglik: -2.8821e+02 - logprior: -1.2849e+01
Epoch 4/10
10/10 - 1s - loss: 290.5894 - loglik: -2.8633e+02 - logprior: -4.2588e+00
Epoch 5/10
10/10 - 1s - loss: 285.4386 - loglik: -2.8422e+02 - logprior: -1.2197e+00
Epoch 6/10
10/10 - 1s - loss: 282.6547 - loglik: -2.8290e+02 - logprior: 0.2454
Epoch 7/10
10/10 - 1s - loss: 281.1526 - loglik: -2.8232e+02 - logprior: 1.1663
Epoch 8/10
10/10 - 1s - loss: 280.1176 - loglik: -2.8202e+02 - logprior: 1.9025
Epoch 9/10
10/10 - 1s - loss: 279.2690 - loglik: -2.8176e+02 - logprior: 2.4951
Epoch 10/10
10/10 - 1s - loss: 278.4586 - loglik: -2.8139e+02 - logprior: 2.9309
Fitted a model with MAP estimate = -277.9869
Time for alignment: 30.0342
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 458.1082 - loglik: -3.6987e+02 - logprior: -8.8241e+01
Epoch 2/10
10/10 - 1s - loss: 360.7769 - loglik: -3.3704e+02 - logprior: -2.3739e+01
Epoch 3/10
10/10 - 1s - loss: 329.8086 - loglik: -3.1859e+02 - logprior: -1.1219e+01
Epoch 4/10
10/10 - 1s - loss: 316.6403 - loglik: -3.0994e+02 - logprior: -6.7025e+00
Epoch 5/10
10/10 - 1s - loss: 309.2092 - loglik: -3.0481e+02 - logprior: -4.3971e+00
Epoch 6/10
10/10 - 1s - loss: 304.3210 - loglik: -3.0116e+02 - logprior: -3.1650e+00
Epoch 7/10
10/10 - 1s - loss: 301.8067 - loglik: -2.9947e+02 - logprior: -2.3374e+00
Epoch 8/10
10/10 - 1s - loss: 300.1874 - loglik: -2.9831e+02 - logprior: -1.8760e+00
Epoch 9/10
10/10 - 1s - loss: 299.0970 - loglik: -2.9752e+02 - logprior: -1.5766e+00
Epoch 10/10
10/10 - 1s - loss: 298.2837 - loglik: -2.9691e+02 - logprior: -1.3719e+00
Fitted a model with MAP estimate = -297.8906
expansions: [(7, 2), (8, 2), (13, 1), (16, 1), (24, 3), (26, 2), (33, 1), (36, 1), (37, 1), (38, 2), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 414.5126 - loglik: -3.1537e+02 - logprior: -9.9147e+01
Epoch 2/2
10/10 - 1s - loss: 337.7802 - loglik: -2.9693e+02 - logprior: -4.0846e+01
Fitted a model with MAP estimate = -324.7574
expansions: [(0, 2), (47, 1)]
discards: [ 0 29 30 34 51]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 380.1227 - loglik: -3.0108e+02 - logprior: -7.9042e+01
Epoch 2/2
10/10 - 1s - loss: 310.3079 - loglik: -2.8959e+02 - logprior: -2.0716e+01
Fitted a model with MAP estimate = -299.4946
expansions: [(9, 1), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 396.1063 - loglik: -2.9983e+02 - logprior: -9.6272e+01
Epoch 2/10
10/10 - 1s - loss: 323.9992 - loglik: -2.8997e+02 - logprior: -3.4032e+01
Epoch 3/10
10/10 - 1s - loss: 301.0569 - loglik: -2.8821e+02 - logprior: -1.2849e+01
Epoch 4/10
10/10 - 1s - loss: 290.5894 - loglik: -2.8633e+02 - logprior: -4.2588e+00
Epoch 5/10
10/10 - 1s - loss: 285.4386 - loglik: -2.8422e+02 - logprior: -1.2197e+00
Epoch 6/10
10/10 - 1s - loss: 282.6548 - loglik: -2.8290e+02 - logprior: 0.2454
Epoch 7/10
10/10 - 1s - loss: 281.1526 - loglik: -2.8232e+02 - logprior: 1.1663
Epoch 8/10
10/10 - 1s - loss: 280.1176 - loglik: -2.8202e+02 - logprior: 1.9025
Epoch 9/10
10/10 - 1s - loss: 279.2690 - loglik: -2.8176e+02 - logprior: 2.4951
Epoch 10/10
10/10 - 1s - loss: 278.4586 - loglik: -2.8139e+02 - logprior: 2.9309
Fitted a model with MAP estimate = -277.9871
Time for alignment: 28.0919
Fitting a model of length 49 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 458.1082 - loglik: -3.6987e+02 - logprior: -8.8241e+01
Epoch 2/10
10/10 - 1s - loss: 360.7769 - loglik: -3.3704e+02 - logprior: -2.3739e+01
Epoch 3/10
10/10 - 1s - loss: 329.8086 - loglik: -3.1859e+02 - logprior: -1.1219e+01
Epoch 4/10
10/10 - 1s - loss: 316.6403 - loglik: -3.0994e+02 - logprior: -6.7025e+00
Epoch 5/10
10/10 - 1s - loss: 309.2092 - loglik: -3.0481e+02 - logprior: -4.3971e+00
Epoch 6/10
10/10 - 1s - loss: 304.3211 - loglik: -3.0116e+02 - logprior: -3.1650e+00
Epoch 7/10
10/10 - 1s - loss: 301.8067 - loglik: -2.9947e+02 - logprior: -2.3374e+00
Epoch 8/10
10/10 - 1s - loss: 300.1874 - loglik: -2.9831e+02 - logprior: -1.8760e+00
Epoch 9/10
10/10 - 1s - loss: 299.0970 - loglik: -2.9752e+02 - logprior: -1.5766e+00
Epoch 10/10
10/10 - 1s - loss: 298.2838 - loglik: -2.9691e+02 - logprior: -1.3719e+00
Fitted a model with MAP estimate = -297.8909
expansions: [(7, 2), (8, 2), (13, 1), (16, 1), (24, 3), (26, 2), (33, 1), (36, 1), (37, 1), (38, 2), (40, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 414.5126 - loglik: -3.1537e+02 - logprior: -9.9147e+01
Epoch 2/2
10/10 - 1s - loss: 337.7802 - loglik: -2.9693e+02 - logprior: -4.0846e+01
Fitted a model with MAP estimate = -324.7574
expansions: [(0, 2), (47, 1)]
discards: [ 0 29 30 34 51]
Re-initialized the encoder parameters.
Fitting a model of length 63 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 380.1226 - loglik: -3.0108e+02 - logprior: -7.9042e+01
Epoch 2/2
10/10 - 1s - loss: 310.3078 - loglik: -2.8959e+02 - logprior: -2.0716e+01
Fitted a model with MAP estimate = -299.4946
expansions: [(9, 1), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 65 on 508 sequences.
Batch size= 508 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 396.1063 - loglik: -2.9983e+02 - logprior: -9.6272e+01
Epoch 2/10
10/10 - 1s - loss: 323.9991 - loglik: -2.8997e+02 - logprior: -3.4032e+01
Epoch 3/10
10/10 - 1s - loss: 301.0569 - loglik: -2.8821e+02 - logprior: -1.2849e+01
Epoch 4/10
10/10 - 1s - loss: 290.5894 - loglik: -2.8633e+02 - logprior: -4.2588e+00
Epoch 5/10
10/10 - 1s - loss: 285.4386 - loglik: -2.8422e+02 - logprior: -1.2197e+00
Epoch 6/10
10/10 - 1s - loss: 282.6547 - loglik: -2.8290e+02 - logprior: 0.2454
Epoch 7/10
10/10 - 1s - loss: 281.1526 - loglik: -2.8232e+02 - logprior: 1.1663
Epoch 8/10
10/10 - 1s - loss: 280.1175 - loglik: -2.8202e+02 - logprior: 1.9025
Epoch 9/10
10/10 - 1s - loss: 279.2689 - loglik: -2.8176e+02 - logprior: 2.4951
Epoch 10/10
10/10 - 1s - loss: 278.4585 - loglik: -2.8139e+02 - logprior: 2.9308
Fitted a model with MAP estimate = -277.9871
Time for alignment: 28.2269
Computed alignments with likelihoods: ['-277.9872', '-277.9871', '-277.9869', '-277.9871', '-277.9871']
Best model has likelihood: -277.9869  (prior= 3.1210 )
time for generating output: 0.1039
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/toxin.projection.fasta
SP score = 0.8719301437147535
Training of 5 independent models on file msb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d65a730>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea6f594af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe951fcba90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 630.1534 - loglik: -6.2246e+02 - logprior: -7.6964e+00
Epoch 2/10
13/13 - 1s - loss: 596.9141 - loglik: -5.9503e+02 - logprior: -1.8812e+00
Epoch 3/10
13/13 - 1s - loss: 572.2814 - loglik: -5.7064e+02 - logprior: -1.6390e+00
Epoch 4/10
13/13 - 1s - loss: 563.9288 - loglik: -5.6212e+02 - logprior: -1.8061e+00
Epoch 5/10
13/13 - 1s - loss: 561.1381 - loglik: -5.5942e+02 - logprior: -1.7177e+00
Epoch 6/10
13/13 - 1s - loss: 560.4777 - loglik: -5.5886e+02 - logprior: -1.6190e+00
Epoch 7/10
13/13 - 1s - loss: 558.5345 - loglik: -5.5691e+02 - logprior: -1.6201e+00
Epoch 8/10
13/13 - 1s - loss: 558.5165 - loglik: -5.5690e+02 - logprior: -1.6133e+00
Epoch 9/10
13/13 - 1s - loss: 556.5380 - loglik: -5.5495e+02 - logprior: -1.5819e+00
Epoch 10/10
13/13 - 1s - loss: 553.8416 - loglik: -5.5226e+02 - logprior: -1.5709e+00
Fitted a model with MAP estimate = -551.6029
expansions: [(9, 1), (10, 1), (13, 1), (26, 1), (28, 3), (29, 4), (39, 1), (40, 1), (44, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 578.3967 - loglik: -5.6938e+02 - logprior: -9.0181e+00
Epoch 2/2
13/13 - 2s - loss: 561.4150 - loglik: -5.5737e+02 - logprior: -4.0416e+00
Fitted a model with MAP estimate = -559.1006
expansions: [(0, 2), (41, 1)]
discards: [ 0 34 64 93]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 564.1653 - loglik: -5.5734e+02 - logprior: -6.8265e+00
Epoch 2/2
13/13 - 1s - loss: 555.9874 - loglik: -5.5417e+02 - logprior: -1.8138e+00
Fitted a model with MAP estimate = -555.3262
expansions: []
discards: [ 0 88]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 565.9764 - loglik: -5.5735e+02 - logprior: -8.6232e+00
Epoch 2/10
13/13 - 1s - loss: 558.4503 - loglik: -5.5588e+02 - logprior: -2.5728e+00
Epoch 3/10
13/13 - 1s - loss: 555.4472 - loglik: -5.5420e+02 - logprior: -1.2443e+00
Epoch 4/10
13/13 - 1s - loss: 555.4841 - loglik: -5.5458e+02 - logprior: -9.0475e-01
Fitted a model with MAP estimate = -554.7337
Time for alignment: 47.0936
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 630.4213 - loglik: -6.2272e+02 - logprior: -7.6964e+00
Epoch 2/10
13/13 - 1s - loss: 597.6068 - loglik: -5.9573e+02 - logprior: -1.8791e+00
Epoch 3/10
13/13 - 1s - loss: 572.2120 - loglik: -5.7057e+02 - logprior: -1.6378e+00
Epoch 4/10
13/13 - 1s - loss: 563.5559 - loglik: -5.6173e+02 - logprior: -1.8240e+00
Epoch 5/10
13/13 - 1s - loss: 561.9744 - loglik: -5.6026e+02 - logprior: -1.7111e+00
Epoch 6/10
13/13 - 1s - loss: 560.0632 - loglik: -5.5844e+02 - logprior: -1.6190e+00
Epoch 7/10
13/13 - 1s - loss: 558.4762 - loglik: -5.5685e+02 - logprior: -1.6193e+00
Epoch 8/10
13/13 - 1s - loss: 558.2164 - loglik: -5.5660e+02 - logprior: -1.6120e+00
Epoch 9/10
13/13 - 1s - loss: 556.6166 - loglik: -5.5503e+02 - logprior: -1.5794e+00
Epoch 10/10
13/13 - 1s - loss: 554.3693 - loglik: -5.5279e+02 - logprior: -1.5714e+00
Fitted a model with MAP estimate = -552.2046
expansions: [(9, 1), (10, 1), (11, 1), (12, 2), (28, 1), (29, 1), (30, 3), (40, 1), (41, 1), (43, 1), (51, 2), (63, 2), (64, 1), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 578.4024 - loglik: -5.6938e+02 - logprior: -9.0258e+00
Epoch 2/2
13/13 - 2s - loss: 561.0967 - loglik: -5.5702e+02 - logprior: -4.0732e+00
Fitted a model with MAP estimate = -559.0914
expansions: [(0, 2), (41, 1)]
discards: [ 0 14 64 90 93]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 564.7642 - loglik: -5.5793e+02 - logprior: -6.8385e+00
Epoch 2/2
13/13 - 1s - loss: 555.3602 - loglik: -5.5354e+02 - logprior: -1.8251e+00
Fitted a model with MAP estimate = -555.3750
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 565.8650 - loglik: -5.5720e+02 - logprior: -8.6626e+00
Epoch 2/10
13/13 - 1s - loss: 558.3920 - loglik: -5.5577e+02 - logprior: -2.6212e+00
Epoch 3/10
13/13 - 1s - loss: 556.1716 - loglik: -5.5492e+02 - logprior: -1.2529e+00
Epoch 4/10
13/13 - 1s - loss: 555.3064 - loglik: -5.5439e+02 - logprior: -9.1609e-01
Epoch 5/10
13/13 - 1s - loss: 554.4008 - loglik: -5.5358e+02 - logprior: -8.2262e-01
Epoch 6/10
13/13 - 1s - loss: 553.4703 - loglik: -5.5268e+02 - logprior: -7.9193e-01
Epoch 7/10
13/13 - 1s - loss: 552.8057 - loglik: -5.5203e+02 - logprior: -7.6969e-01
Epoch 8/10
13/13 - 1s - loss: 553.3413 - loglik: -5.5258e+02 - logprior: -7.5379e-01
Fitted a model with MAP estimate = -551.6086
Time for alignment: 53.0322
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 631.5782 - loglik: -6.2388e+02 - logprior: -7.6935e+00
Epoch 2/10
13/13 - 1s - loss: 596.0795 - loglik: -5.9421e+02 - logprior: -1.8712e+00
Epoch 3/10
13/13 - 1s - loss: 571.8922 - loglik: -5.7025e+02 - logprior: -1.6375e+00
Epoch 4/10
13/13 - 1s - loss: 564.3383 - loglik: -5.6252e+02 - logprior: -1.8216e+00
Epoch 5/10
13/13 - 1s - loss: 561.0688 - loglik: -5.5935e+02 - logprior: -1.7156e+00
Epoch 6/10
13/13 - 1s - loss: 560.2131 - loglik: -5.5860e+02 - logprior: -1.6067e+00
Epoch 7/10
13/13 - 1s - loss: 558.0626 - loglik: -5.5645e+02 - logprior: -1.6098e+00
Epoch 8/10
13/13 - 1s - loss: 558.6264 - loglik: -5.5702e+02 - logprior: -1.5975e+00
Fitted a model with MAP estimate = -557.3694
expansions: [(9, 1), (10, 1), (13, 1), (26, 1), (28, 3), (29, 4), (30, 1), (39, 1), (40, 1), (42, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 571.9720 - loglik: -5.6299e+02 - logprior: -8.9862e+00
Epoch 2/2
13/13 - 2s - loss: 560.1298 - loglik: -5.5617e+02 - logprior: -3.9584e+00
Fitted a model with MAP estimate = -558.3200
expansions: [(0, 2)]
discards: [ 0 65 94]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 563.2501 - loglik: -5.5645e+02 - logprior: -6.8048e+00
Epoch 2/2
13/13 - 2s - loss: 556.4238 - loglik: -5.5462e+02 - logprior: -1.8035e+00
Fitted a model with MAP estimate = -555.2548
expansions: []
discards: [ 0 89]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 566.1856 - loglik: -5.5756e+02 - logprior: -8.6266e+00
Epoch 2/10
13/13 - 1s - loss: 557.9630 - loglik: -5.5537e+02 - logprior: -2.5942e+00
Epoch 3/10
13/13 - 1s - loss: 555.9928 - loglik: -5.5475e+02 - logprior: -1.2406e+00
Epoch 4/10
13/13 - 1s - loss: 555.6212 - loglik: -5.5471e+02 - logprior: -9.0975e-01
Epoch 5/10
13/13 - 1s - loss: 554.3807 - loglik: -5.5357e+02 - logprior: -8.0591e-01
Epoch 6/10
13/13 - 2s - loss: 553.9653 - loglik: -5.5318e+02 - logprior: -7.8462e-01
Epoch 7/10
13/13 - 1s - loss: 553.1016 - loglik: -5.5233e+02 - logprior: -7.6716e-01
Epoch 8/10
13/13 - 1s - loss: 551.4984 - loglik: -5.5075e+02 - logprior: -7.4670e-01
Epoch 9/10
13/13 - 1s - loss: 551.4819 - loglik: -5.5076e+02 - logprior: -7.1701e-01
Epoch 10/10
13/13 - 1s - loss: 548.5942 - loglik: -5.4789e+02 - logprior: -6.9575e-01
Fitted a model with MAP estimate = -548.0976
Time for alignment: 52.4718
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 631.3256 - loglik: -6.2363e+02 - logprior: -7.6939e+00
Epoch 2/10
13/13 - 1s - loss: 594.9532 - loglik: -5.9308e+02 - logprior: -1.8764e+00
Epoch 3/10
13/13 - 1s - loss: 572.0632 - loglik: -5.7042e+02 - logprior: -1.6469e+00
Epoch 4/10
13/13 - 1s - loss: 562.5880 - loglik: -5.6079e+02 - logprior: -1.7966e+00
Epoch 5/10
13/13 - 1s - loss: 562.7533 - loglik: -5.6107e+02 - logprior: -1.6777e+00
Fitted a model with MAP estimate = -560.7308
expansions: [(9, 1), (10, 1), (13, 2), (26, 1), (28, 3), (29, 4), (30, 1), (31, 1), (39, 1), (40, 1), (42, 1), (50, 1), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 572.0849 - loglik: -5.6312e+02 - logprior: -8.9641e+00
Epoch 2/2
13/13 - 2s - loss: 560.0018 - loglik: -5.5608e+02 - logprior: -3.9189e+00
Fitted a model with MAP estimate = -558.1997
expansions: [(0, 2)]
discards: [ 0 14 95]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 563.7113 - loglik: -5.5691e+02 - logprior: -6.7993e+00
Epoch 2/2
13/13 - 2s - loss: 556.9904 - loglik: -5.5518e+02 - logprior: -1.8084e+00
Fitted a model with MAP estimate = -555.2030
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 566.4679 - loglik: -5.5786e+02 - logprior: -8.6088e+00
Epoch 2/10
13/13 - 2s - loss: 556.8467 - loglik: -5.5428e+02 - logprior: -2.5714e+00
Epoch 3/10
13/13 - 2s - loss: 556.0613 - loglik: -5.5482e+02 - logprior: -1.2368e+00
Epoch 4/10
13/13 - 1s - loss: 555.2514 - loglik: -5.5434e+02 - logprior: -9.1010e-01
Epoch 5/10
13/13 - 2s - loss: 554.1437 - loglik: -5.5335e+02 - logprior: -7.9071e-01
Epoch 6/10
13/13 - 2s - loss: 554.3934 - loglik: -5.5362e+02 - logprior: -7.7003e-01
Fitted a model with MAP estimate = -553.0479
Time for alignment: 43.4452
Fitting a model of length 86 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 630.3125 - loglik: -6.2262e+02 - logprior: -7.6935e+00
Epoch 2/10
13/13 - 1s - loss: 597.0074 - loglik: -5.9513e+02 - logprior: -1.8792e+00
Epoch 3/10
13/13 - 1s - loss: 571.8583 - loglik: -5.7020e+02 - logprior: -1.6617e+00
Epoch 4/10
13/13 - 1s - loss: 562.6918 - loglik: -5.6085e+02 - logprior: -1.8363e+00
Epoch 5/10
13/13 - 1s - loss: 562.6008 - loglik: -5.6087e+02 - logprior: -1.7275e+00
Epoch 6/10
13/13 - 1s - loss: 559.5215 - loglik: -5.5790e+02 - logprior: -1.6143e+00
Epoch 7/10
13/13 - 1s - loss: 558.8592 - loglik: -5.5724e+02 - logprior: -1.6182e+00
Epoch 8/10
13/13 - 1s - loss: 558.5333 - loglik: -5.5692e+02 - logprior: -1.6105e+00
Epoch 9/10
13/13 - 1s - loss: 556.8361 - loglik: -5.5525e+02 - logprior: -1.5766e+00
Epoch 10/10
13/13 - 1s - loss: 554.5341 - loglik: -5.5296e+02 - logprior: -1.5634e+00
Fitted a model with MAP estimate = -552.2528
expansions: [(9, 1), (10, 1), (13, 1), (20, 1), (28, 3), (29, 4), (39, 1), (40, 1), (42, 1), (51, 2), (62, 2), (68, 1), (69, 5), (70, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 577.6840 - loglik: -5.6867e+02 - logprior: -9.0121e+00
Epoch 2/2
13/13 - 2s - loss: 561.0262 - loglik: -5.5699e+02 - logprior: -4.0324e+00
Fitted a model with MAP estimate = -559.0048
expansions: [(0, 2), (41, 1)]
discards: [ 0 34 64 93]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 564.4365 - loglik: -5.5762e+02 - logprior: -6.8205e+00
Epoch 2/2
13/13 - 1s - loss: 556.2524 - loglik: -5.5444e+02 - logprior: -1.8133e+00
Fitted a model with MAP estimate = -555.3076
expansions: []
discards: [ 0 88]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 4884 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 566.0206 - loglik: -5.5738e+02 - logprior: -8.6399e+00
Epoch 2/10
13/13 - 1s - loss: 559.0121 - loglik: -5.5642e+02 - logprior: -2.5876e+00
Epoch 3/10
13/13 - 1s - loss: 555.5841 - loglik: -5.5434e+02 - logprior: -1.2392e+00
Epoch 4/10
13/13 - 1s - loss: 554.6562 - loglik: -5.5375e+02 - logprior: -9.1037e-01
Epoch 5/10
13/13 - 1s - loss: 554.8667 - loglik: -5.5405e+02 - logprior: -8.1375e-01
Fitted a model with MAP estimate = -554.0468
Time for alignment: 47.8307
Computed alignments with likelihoods: ['-551.6029', '-551.6086', '-548.0976', '-553.0479', '-552.2528']
Best model has likelihood: -548.0976  (prior= -0.6813 )
time for generating output: 0.1592
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/msb.projection.fasta
SP score = 0.9236641221374046
Training of 5 independent models on file rnasemam.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d045fd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9bc3a7af0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea77d91ac0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 789.2406 - loglik: -7.0367e+02 - logprior: -8.5574e+01
Epoch 2/10
10/10 - 1s - loss: 677.6925 - loglik: -6.5783e+02 - logprior: -1.9865e+01
Epoch 3/10
10/10 - 1s - loss: 625.2638 - loglik: -6.1730e+02 - logprior: -7.9603e+00
Epoch 4/10
10/10 - 1s - loss: 596.4167 - loglik: -5.9215e+02 - logprior: -4.2687e+00
Epoch 5/10
10/10 - 1s - loss: 582.9333 - loglik: -5.8103e+02 - logprior: -1.9030e+00
Epoch 6/10
10/10 - 1s - loss: 575.7653 - loglik: -5.7492e+02 - logprior: -8.4389e-01
Epoch 7/10
10/10 - 1s - loss: 572.3050 - loglik: -5.7196e+02 - logprior: -3.4299e-01
Epoch 8/10
10/10 - 1s - loss: 570.6383 - loglik: -5.7077e+02 - logprior: 0.1304
Epoch 9/10
10/10 - 1s - loss: 569.7511 - loglik: -5.7032e+02 - logprior: 0.5684
Epoch 10/10
10/10 - 1s - loss: 569.1845 - loglik: -5.7003e+02 - logprior: 0.8499
Fitted a model with MAP estimate = -568.9377
expansions: [(8, 2), (13, 2), (14, 1), (24, 1), (29, 1), (40, 2), (41, 2), (49, 2), (50, 1), (61, 1), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 672.9631 - loglik: -5.7725e+02 - logprior: -9.5716e+01
Epoch 2/2
10/10 - 1s - loss: 591.1475 - loglik: -5.5485e+02 - logprior: -3.6294e+01
Fitted a model with MAP estimate = -578.2041
expansions: [(0, 7)]
discards: [  0  46 112]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 632.8168 - loglik: -5.5736e+02 - logprior: -7.5452e+01
Epoch 2/2
10/10 - 1s - loss: 564.0897 - loglik: -5.4784e+02 - logprior: -1.6252e+01
Fitted a model with MAP estimate = -554.3030
expansions: []
discards: [  1   2   3   4   5   6  21 116]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 628.9227 - loglik: -5.5521e+02 - logprior: -7.3711e+01
Epoch 2/10
10/10 - 1s - loss: 563.8141 - loglik: -5.4841e+02 - logprior: -1.5400e+01
Epoch 3/10
10/10 - 1s - loss: 551.0804 - loglik: -5.4799e+02 - logprior: -3.0948e+00
Epoch 4/10
10/10 - 1s - loss: 544.5758 - loglik: -5.4683e+02 - logprior: 2.2591
Epoch 5/10
10/10 - 1s - loss: 539.7462 - loglik: -5.4491e+02 - logprior: 5.1642
Epoch 6/10
10/10 - 1s - loss: 536.2938 - loglik: -5.4319e+02 - logprior: 6.8969
Epoch 7/10
10/10 - 1s - loss: 534.8053 - loglik: -5.4287e+02 - logprior: 8.0624
Epoch 8/10
10/10 - 1s - loss: 533.8700 - loglik: -5.4282e+02 - logprior: 8.9557
Epoch 9/10
10/10 - 1s - loss: 533.1818 - loglik: -5.4283e+02 - logprior: 9.6463
Epoch 10/10
10/10 - 1s - loss: 532.6066 - loglik: -5.4284e+02 - logprior: 10.2321
Fitted a model with MAP estimate = -532.3219
Time for alignment: 44.5730
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 789.2406 - loglik: -7.0367e+02 - logprior: -8.5574e+01
Epoch 2/10
10/10 - 1s - loss: 677.6925 - loglik: -6.5783e+02 - logprior: -1.9865e+01
Epoch 3/10
10/10 - 1s - loss: 625.2638 - loglik: -6.1730e+02 - logprior: -7.9603e+00
Epoch 4/10
10/10 - 1s - loss: 596.4167 - loglik: -5.9215e+02 - logprior: -4.2687e+00
Epoch 5/10
10/10 - 1s - loss: 582.9333 - loglik: -5.8103e+02 - logprior: -1.9030e+00
Epoch 6/10
10/10 - 1s - loss: 575.7653 - loglik: -5.7492e+02 - logprior: -8.4389e-01
Epoch 7/10
10/10 - 1s - loss: 572.3049 - loglik: -5.7196e+02 - logprior: -3.4299e-01
Epoch 8/10
10/10 - 1s - loss: 570.6383 - loglik: -5.7077e+02 - logprior: 0.1304
Epoch 9/10
10/10 - 1s - loss: 569.7511 - loglik: -5.7032e+02 - logprior: 0.5684
Epoch 10/10
10/10 - 1s - loss: 569.1844 - loglik: -5.7003e+02 - logprior: 0.8499
Fitted a model with MAP estimate = -568.9374
expansions: [(8, 2), (13, 2), (14, 1), (24, 1), (29, 1), (40, 2), (41, 2), (49, 2), (50, 1), (61, 1), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 672.9631 - loglik: -5.7725e+02 - logprior: -9.5716e+01
Epoch 2/2
10/10 - 1s - loss: 591.1475 - loglik: -5.5485e+02 - logprior: -3.6294e+01
Fitted a model with MAP estimate = -578.2041
expansions: [(0, 7)]
discards: [  0  46 112]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 632.8168 - loglik: -5.5736e+02 - logprior: -7.5452e+01
Epoch 2/2
10/10 - 1s - loss: 564.0897 - loglik: -5.4784e+02 - logprior: -1.6252e+01
Fitted a model with MAP estimate = -554.3030
expansions: []
discards: [  1   2   3   4   5   6  21 116]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 628.9227 - loglik: -5.5521e+02 - logprior: -7.3711e+01
Epoch 2/10
10/10 - 1s - loss: 563.8141 - loglik: -5.4841e+02 - logprior: -1.5400e+01
Epoch 3/10
10/10 - 1s - loss: 551.0804 - loglik: -5.4799e+02 - logprior: -3.0948e+00
Epoch 4/10
10/10 - 1s - loss: 544.5758 - loglik: -5.4683e+02 - logprior: 2.2591
Epoch 5/10
10/10 - 1s - loss: 539.7462 - loglik: -5.4491e+02 - logprior: 5.1642
Epoch 6/10
10/10 - 1s - loss: 536.2938 - loglik: -5.4319e+02 - logprior: 6.8969
Epoch 7/10
10/10 - 1s - loss: 534.8053 - loglik: -5.4287e+02 - logprior: 8.0625
Epoch 8/10
10/10 - 1s - loss: 533.8702 - loglik: -5.4283e+02 - logprior: 8.9558
Epoch 9/10
10/10 - 1s - loss: 533.1819 - loglik: -5.4283e+02 - logprior: 9.6463
Epoch 10/10
10/10 - 1s - loss: 532.6066 - loglik: -5.4284e+02 - logprior: 10.2320
Fitted a model with MAP estimate = -532.3222
Time for alignment: 44.4722
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 789.2406 - loglik: -7.0367e+02 - logprior: -8.5574e+01
Epoch 2/10
10/10 - 1s - loss: 677.6925 - loglik: -6.5783e+02 - logprior: -1.9865e+01
Epoch 3/10
10/10 - 1s - loss: 625.2638 - loglik: -6.1730e+02 - logprior: -7.9603e+00
Epoch 4/10
10/10 - 1s - loss: 596.4167 - loglik: -5.9215e+02 - logprior: -4.2687e+00
Epoch 5/10
10/10 - 1s - loss: 582.9333 - loglik: -5.8103e+02 - logprior: -1.9030e+00
Epoch 6/10
10/10 - 1s - loss: 575.7653 - loglik: -5.7492e+02 - logprior: -8.4389e-01
Epoch 7/10
10/10 - 1s - loss: 572.3049 - loglik: -5.7196e+02 - logprior: -3.4300e-01
Epoch 8/10
10/10 - 1s - loss: 570.6384 - loglik: -5.7077e+02 - logprior: 0.1304
Epoch 9/10
10/10 - 1s - loss: 569.7513 - loglik: -5.7032e+02 - logprior: 0.5684
Epoch 10/10
10/10 - 1s - loss: 569.1847 - loglik: -5.7003e+02 - logprior: 0.8499
Fitted a model with MAP estimate = -568.9377
expansions: [(8, 2), (13, 2), (14, 1), (24, 1), (29, 1), (40, 2), (41, 2), (49, 2), (50, 1), (61, 1), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 672.9632 - loglik: -5.7725e+02 - logprior: -9.5716e+01
Epoch 2/2
10/10 - 1s - loss: 591.1475 - loglik: -5.5485e+02 - logprior: -3.6294e+01
Fitted a model with MAP estimate = -578.2041
expansions: [(0, 7)]
discards: [  0  46 112]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 632.8168 - loglik: -5.5736e+02 - logprior: -7.5452e+01
Epoch 2/2
10/10 - 1s - loss: 564.0897 - loglik: -5.4784e+02 - logprior: -1.6252e+01
Fitted a model with MAP estimate = -554.3029
expansions: []
discards: [  1   2   3   4   5   6  21 116]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 628.9227 - loglik: -5.5521e+02 - logprior: -7.3711e+01
Epoch 2/10
10/10 - 1s - loss: 563.8140 - loglik: -5.4841e+02 - logprior: -1.5400e+01
Epoch 3/10
10/10 - 1s - loss: 551.0804 - loglik: -5.4799e+02 - logprior: -3.0948e+00
Epoch 4/10
10/10 - 1s - loss: 544.5758 - loglik: -5.4683e+02 - logprior: 2.2592
Epoch 5/10
10/10 - 1s - loss: 539.7462 - loglik: -5.4491e+02 - logprior: 5.1642
Epoch 6/10
10/10 - 1s - loss: 536.2938 - loglik: -5.4319e+02 - logprior: 6.8969
Epoch 7/10
10/10 - 1s - loss: 534.8052 - loglik: -5.4287e+02 - logprior: 8.0625
Epoch 8/10
10/10 - 1s - loss: 533.8701 - loglik: -5.4282e+02 - logprior: 8.9558
Epoch 9/10
10/10 - 1s - loss: 533.1818 - loglik: -5.4283e+02 - logprior: 9.6464
Epoch 10/10
10/10 - 1s - loss: 532.6068 - loglik: -5.4284e+02 - logprior: 10.2321
Fitted a model with MAP estimate = -532.3217
Time for alignment: 42.5877
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 789.2406 - loglik: -7.0367e+02 - logprior: -8.5574e+01
Epoch 2/10
10/10 - 1s - loss: 677.6925 - loglik: -6.5783e+02 - logprior: -1.9865e+01
Epoch 3/10
10/10 - 1s - loss: 625.2638 - loglik: -6.1730e+02 - logprior: -7.9603e+00
Epoch 4/10
10/10 - 1s - loss: 596.4167 - loglik: -5.9215e+02 - logprior: -4.2687e+00
Epoch 5/10
10/10 - 1s - loss: 582.9333 - loglik: -5.8103e+02 - logprior: -1.9030e+00
Epoch 6/10
10/10 - 1s - loss: 575.7653 - loglik: -5.7492e+02 - logprior: -8.4389e-01
Epoch 7/10
10/10 - 1s - loss: 572.3050 - loglik: -5.7196e+02 - logprior: -3.4299e-01
Epoch 8/10
10/10 - 1s - loss: 570.6382 - loglik: -5.7077e+02 - logprior: 0.1304
Epoch 9/10
10/10 - 1s - loss: 569.7513 - loglik: -5.7032e+02 - logprior: 0.5684
Epoch 10/10
10/10 - 1s - loss: 569.1845 - loglik: -5.7003e+02 - logprior: 0.8499
Fitted a model with MAP estimate = -568.9376
expansions: [(8, 2), (13, 2), (14, 1), (24, 1), (29, 1), (40, 2), (41, 2), (49, 2), (50, 1), (61, 1), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 672.9632 - loglik: -5.7725e+02 - logprior: -9.5716e+01
Epoch 2/2
10/10 - 1s - loss: 591.1475 - loglik: -5.5485e+02 - logprior: -3.6294e+01
Fitted a model with MAP estimate = -578.2041
expansions: [(0, 7)]
discards: [  0  46 112]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 632.8168 - loglik: -5.5736e+02 - logprior: -7.5452e+01
Epoch 2/2
10/10 - 1s - loss: 564.0897 - loglik: -5.4784e+02 - logprior: -1.6252e+01
Fitted a model with MAP estimate = -554.3030
expansions: []
discards: [  1   2   3   4   5   6  21 116]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 628.9226 - loglik: -5.5521e+02 - logprior: -7.3711e+01
Epoch 2/10
10/10 - 1s - loss: 563.8140 - loglik: -5.4841e+02 - logprior: -1.5400e+01
Epoch 3/10
10/10 - 1s - loss: 551.0804 - loglik: -5.4799e+02 - logprior: -3.0948e+00
Epoch 4/10
10/10 - 1s - loss: 544.5758 - loglik: -5.4683e+02 - logprior: 2.2591
Epoch 5/10
10/10 - 1s - loss: 539.7462 - loglik: -5.4491e+02 - logprior: 5.1642
Epoch 6/10
10/10 - 1s - loss: 536.2938 - loglik: -5.4319e+02 - logprior: 6.8969
Epoch 7/10
10/10 - 1s - loss: 534.8053 - loglik: -5.4287e+02 - logprior: 8.0625
Epoch 8/10
10/10 - 1s - loss: 533.8700 - loglik: -5.4282e+02 - logprior: 8.9558
Epoch 9/10
10/10 - 1s - loss: 533.1819 - loglik: -5.4283e+02 - logprior: 9.6464
Epoch 10/10
10/10 - 1s - loss: 532.6068 - loglik: -5.4284e+02 - logprior: 10.2321
Fitted a model with MAP estimate = -532.3219
Time for alignment: 44.1191
Fitting a model of length 98 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 789.2406 - loglik: -7.0367e+02 - logprior: -8.5574e+01
Epoch 2/10
10/10 - 1s - loss: 677.6925 - loglik: -6.5783e+02 - logprior: -1.9865e+01
Epoch 3/10
10/10 - 1s - loss: 625.2638 - loglik: -6.1730e+02 - logprior: -7.9603e+00
Epoch 4/10
10/10 - 1s - loss: 596.4167 - loglik: -5.9215e+02 - logprior: -4.2687e+00
Epoch 5/10
10/10 - 1s - loss: 582.9333 - loglik: -5.8103e+02 - logprior: -1.9030e+00
Epoch 6/10
10/10 - 1s - loss: 575.7653 - loglik: -5.7492e+02 - logprior: -8.4389e-01
Epoch 7/10
10/10 - 1s - loss: 572.3050 - loglik: -5.7196e+02 - logprior: -3.4299e-01
Epoch 8/10
10/10 - 1s - loss: 570.6384 - loglik: -5.7077e+02 - logprior: 0.1304
Epoch 9/10
10/10 - 1s - loss: 569.7512 - loglik: -5.7032e+02 - logprior: 0.5684
Epoch 10/10
10/10 - 1s - loss: 569.1844 - loglik: -5.7003e+02 - logprior: 0.8499
Fitted a model with MAP estimate = -568.9374
expansions: [(8, 2), (13, 2), (14, 1), (24, 1), (29, 1), (40, 2), (41, 2), (49, 2), (50, 1), (61, 1), (62, 1), (63, 1), (68, 1), (77, 1), (80, 4), (89, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 672.9631 - loglik: -5.7725e+02 - logprior: -9.5716e+01
Epoch 2/2
10/10 - 1s - loss: 591.1475 - loglik: -5.5485e+02 - logprior: -3.6294e+01
Fitted a model with MAP estimate = -578.2041
expansions: [(0, 7)]
discards: [  0  46 112]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 632.8168 - loglik: -5.5736e+02 - logprior: -7.5452e+01
Epoch 2/2
10/10 - 1s - loss: 564.0897 - loglik: -5.4784e+02 - logprior: -1.6252e+01
Fitted a model with MAP estimate = -554.3029
expansions: []
discards: [  1   2   3   4   5   6  21 116]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 498 sequences.
Batch size= 498 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 628.9227 - loglik: -5.5521e+02 - logprior: -7.3711e+01
Epoch 2/10
10/10 - 1s - loss: 563.8141 - loglik: -5.4841e+02 - logprior: -1.5400e+01
Epoch 3/10
10/10 - 1s - loss: 551.0804 - loglik: -5.4799e+02 - logprior: -3.0948e+00
Epoch 4/10
10/10 - 1s - loss: 544.5757 - loglik: -5.4683e+02 - logprior: 2.2592
Epoch 5/10
10/10 - 1s - loss: 539.7460 - loglik: -5.4491e+02 - logprior: 5.1642
Epoch 6/10
10/10 - 1s - loss: 536.2936 - loglik: -5.4319e+02 - logprior: 6.8969
Epoch 7/10
10/10 - 1s - loss: 534.8054 - loglik: -5.4287e+02 - logprior: 8.0623
Epoch 8/10
10/10 - 1s - loss: 533.8701 - loglik: -5.4282e+02 - logprior: 8.9556
Epoch 9/10
10/10 - 1s - loss: 533.1820 - loglik: -5.4283e+02 - logprior: 9.6463
Epoch 10/10
10/10 - 1s - loss: 532.6068 - loglik: -5.4284e+02 - logprior: 10.2321
Fitted a model with MAP estimate = -532.3220
Time for alignment: 42.6294
Computed alignments with likelihoods: ['-532.3219', '-532.3222', '-532.3217', '-532.3219', '-532.3220']
Best model has likelihood: -532.3217  (prior= 10.5192 )
time for generating output: 0.1376
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rnasemam.projection.fasta
SP score = 0.8865671641791045
Training of 5 independent models on file rub.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe938db14c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d1c6400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea22cf0400>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 302.5858 - loglik: -2.7091e+02 - logprior: -3.1678e+01
Epoch 2/10
10/10 - 0s - loss: 253.7817 - loglik: -2.4480e+02 - logprior: -8.9790e+00
Epoch 3/10
10/10 - 0s - loss: 228.8848 - loglik: -2.2396e+02 - logprior: -4.9249e+00
Epoch 4/10
10/10 - 0s - loss: 213.6674 - loglik: -2.0975e+02 - logprior: -3.9193e+00
Epoch 5/10
10/10 - 0s - loss: 207.9605 - loglik: -2.0433e+02 - logprior: -3.6309e+00
Epoch 6/10
10/10 - 0s - loss: 205.1205 - loglik: -2.0197e+02 - logprior: -3.1513e+00
Epoch 7/10
10/10 - 0s - loss: 203.2028 - loglik: -2.0041e+02 - logprior: -2.7914e+00
Epoch 8/10
10/10 - 0s - loss: 201.9435 - loglik: -1.9925e+02 - logprior: -2.6898e+00
Epoch 9/10
10/10 - 0s - loss: 201.3357 - loglik: -1.9871e+02 - logprior: -2.6260e+00
Epoch 10/10
10/10 - 0s - loss: 200.8890 - loglik: -1.9835e+02 - logprior: -2.5380e+00
Fitted a model with MAP estimate = -200.7764
expansions: [(0, 2), (3, 2), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 50 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 249.5984 - loglik: -2.0744e+02 - logprior: -4.2157e+01
Epoch 2/2
10/10 - 0s - loss: 207.4894 - loglik: -1.9386e+02 - logprior: -1.3624e+01
Fitted a model with MAP estimate = -200.2616
expansions: []
discards: [ 0 31 35]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 234.3428 - loglik: -1.9829e+02 - logprior: -3.6050e+01
Epoch 2/2
10/10 - 0s - loss: 207.5723 - loglik: -1.9313e+02 - logprior: -1.4441e+01
Fitted a model with MAP estimate = -202.8532
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 228.6974 - loglik: -1.9675e+02 - logprior: -3.1950e+01
Epoch 2/10
10/10 - 0s - loss: 201.0646 - loglik: -1.9188e+02 - logprior: -9.1812e+00
Epoch 3/10
10/10 - 0s - loss: 195.5699 - loglik: -1.9100e+02 - logprior: -4.5662e+00
Epoch 4/10
10/10 - 0s - loss: 193.5056 - loglik: -1.9042e+02 - logprior: -3.0884e+00
Epoch 5/10
10/10 - 0s - loss: 191.8925 - loglik: -1.8986e+02 - logprior: -2.0304e+00
Epoch 6/10
10/10 - 0s - loss: 190.6231 - loglik: -1.8920e+02 - logprior: -1.4239e+00
Epoch 7/10
10/10 - 0s - loss: 189.7087 - loglik: -1.8850e+02 - logprior: -1.2046e+00
Epoch 8/10
10/10 - 0s - loss: 188.8902 - loglik: -1.8785e+02 - logprior: -1.0344e+00
Epoch 9/10
10/10 - 0s - loss: 188.7228 - loglik: -1.8781e+02 - logprior: -9.1256e-01
Epoch 10/10
10/10 - 0s - loss: 188.2705 - loglik: -1.8740e+02 - logprior: -8.6439e-01
Fitted a model with MAP estimate = -188.0834
Time for alignment: 26.4267
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 302.6284 - loglik: -2.7095e+02 - logprior: -3.1678e+01
Epoch 2/10
10/10 - 0s - loss: 253.9189 - loglik: -2.4495e+02 - logprior: -8.9721e+00
Epoch 3/10
10/10 - 0s - loss: 229.1886 - loglik: -2.2428e+02 - logprior: -4.9096e+00
Epoch 4/10
10/10 - 0s - loss: 213.5692 - loglik: -2.0963e+02 - logprior: -3.9427e+00
Epoch 5/10
10/10 - 0s - loss: 207.8564 - loglik: -2.0420e+02 - logprior: -3.6532e+00
Epoch 6/10
10/10 - 0s - loss: 205.0287 - loglik: -2.0187e+02 - logprior: -3.1617e+00
Epoch 7/10
10/10 - 0s - loss: 203.1522 - loglik: -2.0036e+02 - logprior: -2.7941e+00
Epoch 8/10
10/10 - 0s - loss: 201.8302 - loglik: -1.9913e+02 - logprior: -2.6955e+00
Epoch 9/10
10/10 - 0s - loss: 201.3645 - loglik: -1.9874e+02 - logprior: -2.6264e+00
Epoch 10/10
10/10 - 0s - loss: 200.8955 - loglik: -1.9836e+02 - logprior: -2.5358e+00
Fitted a model with MAP estimate = -200.7741
expansions: [(0, 2), (3, 2), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 50 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 249.7751 - loglik: -2.0762e+02 - logprior: -4.2158e+01
Epoch 2/2
10/10 - 0s - loss: 207.5870 - loglik: -1.9395e+02 - logprior: -1.3633e+01
Fitted a model with MAP estimate = -200.3774
expansions: []
discards: [ 0 31 35]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 234.5228 - loglik: -1.9847e+02 - logprior: -3.6057e+01
Epoch 2/2
10/10 - 0s - loss: 207.4813 - loglik: -1.9305e+02 - logprior: -1.4435e+01
Fitted a model with MAP estimate = -202.8468
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 228.7143 - loglik: -1.9677e+02 - logprior: -3.1946e+01
Epoch 2/10
10/10 - 0s - loss: 201.1381 - loglik: -1.9196e+02 - logprior: -9.1760e+00
Epoch 3/10
10/10 - 0s - loss: 195.4599 - loglik: -1.9089e+02 - logprior: -4.5653e+00
Epoch 4/10
10/10 - 0s - loss: 193.5057 - loglik: -1.9042e+02 - logprior: -3.0869e+00
Epoch 5/10
10/10 - 0s - loss: 191.7794 - loglik: -1.8975e+02 - logprior: -2.0326e+00
Epoch 6/10
10/10 - 0s - loss: 190.8383 - loglik: -1.8942e+02 - logprior: -1.4165e+00
Epoch 7/10
10/10 - 0s - loss: 189.6546 - loglik: -1.8845e+02 - logprior: -1.2071e+00
Epoch 8/10
10/10 - 0s - loss: 189.0463 - loglik: -1.8801e+02 - logprior: -1.0320e+00
Epoch 9/10
10/10 - 0s - loss: 188.6834 - loglik: -1.8777e+02 - logprior: -9.1293e-01
Epoch 10/10
10/10 - 0s - loss: 188.2450 - loglik: -1.8738e+02 - logprior: -8.6615e-01
Fitted a model with MAP estimate = -188.0939
Time for alignment: 26.9007
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 302.3409 - loglik: -2.7066e+02 - logprior: -3.1678e+01
Epoch 2/10
10/10 - 0s - loss: 253.7739 - loglik: -2.4479e+02 - logprior: -8.9829e+00
Epoch 3/10
10/10 - 0s - loss: 228.4593 - loglik: -2.2352e+02 - logprior: -4.9426e+00
Epoch 4/10
10/10 - 0s - loss: 213.1045 - loglik: -2.0915e+02 - logprior: -3.9553e+00
Epoch 5/10
10/10 - 0s - loss: 207.8366 - loglik: -2.0423e+02 - logprior: -3.6025e+00
Epoch 6/10
10/10 - 0s - loss: 205.4054 - loglik: -2.0238e+02 - logprior: -3.0208e+00
Epoch 7/10
10/10 - 0s - loss: 203.5253 - loglik: -2.0095e+02 - logprior: -2.5760e+00
Epoch 8/10
10/10 - 0s - loss: 202.2295 - loglik: -1.9980e+02 - logprior: -2.4280e+00
Epoch 9/10
10/10 - 0s - loss: 201.4551 - loglik: -1.9910e+02 - logprior: -2.3537e+00
Epoch 10/10
10/10 - 0s - loss: 201.1176 - loglik: -1.9884e+02 - logprior: -2.2724e+00
Fitted a model with MAP estimate = -200.7608
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 251.8053 - loglik: -2.0956e+02 - logprior: -4.2250e+01
Epoch 2/2
10/10 - 0s - loss: 209.2463 - loglik: -1.9551e+02 - logprior: -1.3735e+01
Fitted a model with MAP estimate = -201.2790
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 226.1997 - loglik: -1.9639e+02 - logprior: -2.9809e+01
Epoch 2/2
10/10 - 0s - loss: 199.8832 - loglik: -1.9148e+02 - logprior: -8.4069e+00
Fitted a model with MAP estimate = -196.3419
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 223.6494 - loglik: -1.9546e+02 - logprior: -2.8193e+01
Epoch 2/10
10/10 - 0s - loss: 199.2012 - loglik: -1.9124e+02 - logprior: -7.9649e+00
Epoch 3/10
10/10 - 0s - loss: 195.1158 - loglik: -1.9115e+02 - logprior: -3.9631e+00
Epoch 4/10
10/10 - 0s - loss: 193.3067 - loglik: -1.9086e+02 - logprior: -2.4486e+00
Epoch 5/10
10/10 - 0s - loss: 191.9725 - loglik: -1.9024e+02 - logprior: -1.7353e+00
Epoch 6/10
10/10 - 0s - loss: 190.8031 - loglik: -1.8940e+02 - logprior: -1.4016e+00
Epoch 7/10
10/10 - 0s - loss: 189.9264 - loglik: -1.8872e+02 - logprior: -1.2025e+00
Epoch 8/10
10/10 - 0s - loss: 189.4114 - loglik: -1.8837e+02 - logprior: -1.0393e+00
Epoch 9/10
10/10 - 0s - loss: 188.7336 - loglik: -1.8780e+02 - logprior: -9.3361e-01
Epoch 10/10
10/10 - 0s - loss: 188.4906 - loglik: -1.8763e+02 - logprior: -8.5893e-01
Fitted a model with MAP estimate = -188.3229
Time for alignment: 24.6048
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 302.4672 - loglik: -2.7079e+02 - logprior: -3.1677e+01
Epoch 2/10
10/10 - 0s - loss: 253.9545 - loglik: -2.4499e+02 - logprior: -8.9660e+00
Epoch 3/10
10/10 - 0s - loss: 228.7080 - loglik: -2.2379e+02 - logprior: -4.9181e+00
Epoch 4/10
10/10 - 0s - loss: 213.0486 - loglik: -2.0909e+02 - logprior: -3.9633e+00
Epoch 5/10
10/10 - 0s - loss: 207.4965 - loglik: -2.0390e+02 - logprior: -3.5979e+00
Epoch 6/10
10/10 - 0s - loss: 204.8781 - loglik: -2.0187e+02 - logprior: -3.0024e+00
Epoch 7/10
10/10 - 0s - loss: 203.2001 - loglik: -2.0063e+02 - logprior: -2.5680e+00
Epoch 8/10
10/10 - 0s - loss: 202.1810 - loglik: -1.9975e+02 - logprior: -2.4254e+00
Epoch 9/10
10/10 - 0s - loss: 201.3988 - loglik: -1.9904e+02 - logprior: -2.3564e+00
Epoch 10/10
10/10 - 0s - loss: 200.8949 - loglik: -1.9863e+02 - logprior: -2.2620e+00
Fitted a model with MAP estimate = -200.7436
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 251.8479 - loglik: -2.0957e+02 - logprior: -4.2274e+01
Epoch 2/2
10/10 - 0s - loss: 209.0638 - loglik: -1.9532e+02 - logprior: -1.3741e+01
Fitted a model with MAP estimate = -201.2642
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 226.2093 - loglik: -1.9640e+02 - logprior: -2.9809e+01
Epoch 2/2
10/10 - 0s - loss: 199.6831 - loglik: -1.9128e+02 - logprior: -8.4064e+00
Fitted a model with MAP estimate = -196.3362
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 223.8613 - loglik: -1.9567e+02 - logprior: -2.8189e+01
Epoch 2/10
10/10 - 0s - loss: 199.0022 - loglik: -1.9103e+02 - logprior: -7.9731e+00
Epoch 3/10
10/10 - 0s - loss: 195.1183 - loglik: -1.9116e+02 - logprior: -3.9587e+00
Epoch 4/10
10/10 - 0s - loss: 193.3336 - loglik: -1.9088e+02 - logprior: -2.4502e+00
Epoch 5/10
10/10 - 0s - loss: 191.8977 - loglik: -1.9016e+02 - logprior: -1.7342e+00
Epoch 6/10
10/10 - 0s - loss: 190.7085 - loglik: -1.8930e+02 - logprior: -1.4062e+00
Epoch 7/10
10/10 - 0s - loss: 190.0313 - loglik: -1.8883e+02 - logprior: -1.1993e+00
Epoch 8/10
10/10 - 0s - loss: 189.2610 - loglik: -1.8821e+02 - logprior: -1.0458e+00
Epoch 9/10
10/10 - 0s - loss: 188.8504 - loglik: -1.8792e+02 - logprior: -9.2942e-01
Epoch 10/10
10/10 - 0s - loss: 188.4376 - loglik: -1.8757e+02 - logprior: -8.6204e-01
Fitted a model with MAP estimate = -188.3192
Time for alignment: 26.5754
Fitting a model of length 37 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 302.6303 - loglik: -2.7095e+02 - logprior: -3.1678e+01
Epoch 2/10
10/10 - 0s - loss: 253.7570 - loglik: -2.4478e+02 - logprior: -8.9739e+00
Epoch 3/10
10/10 - 0s - loss: 228.2986 - loglik: -2.2338e+02 - logprior: -4.9231e+00
Epoch 4/10
10/10 - 0s - loss: 212.8379 - loglik: -2.0890e+02 - logprior: -3.9356e+00
Epoch 5/10
10/10 - 0s - loss: 208.0603 - loglik: -2.0445e+02 - logprior: -3.6107e+00
Epoch 6/10
10/10 - 0s - loss: 205.3748 - loglik: -2.0231e+02 - logprior: -3.0617e+00
Epoch 7/10
10/10 - 0s - loss: 203.9839 - loglik: -2.0137e+02 - logprior: -2.6145e+00
Epoch 8/10
10/10 - 0s - loss: 202.7201 - loglik: -2.0027e+02 - logprior: -2.4518e+00
Epoch 9/10
10/10 - 0s - loss: 201.8282 - loglik: -1.9947e+02 - logprior: -2.3561e+00
Epoch 10/10
10/10 - 0s - loss: 201.0373 - loglik: -1.9876e+02 - logprior: -2.2734e+00
Fitted a model with MAP estimate = -200.8354
expansions: [(0, 3), (11, 1), (15, 1), (21, 1), (22, 1), (23, 2), (24, 2), (26, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 49 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 251.7467 - loglik: -2.0950e+02 - logprior: -4.2245e+01
Epoch 2/2
10/10 - 0s - loss: 209.2215 - loglik: -1.9548e+02 - logprior: -1.3738e+01
Fitted a model with MAP estimate = -201.3001
expansions: []
discards: [30 33]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 226.2380 - loglik: -1.9643e+02 - logprior: -2.9812e+01
Epoch 2/2
10/10 - 0s - loss: 199.7144 - loglik: -1.9131e+02 - logprior: -8.4029e+00
Fitted a model with MAP estimate = -196.3470
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 47 on 1435 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 223.8058 - loglik: -1.9561e+02 - logprior: -2.8192e+01
Epoch 2/10
10/10 - 0s - loss: 198.9915 - loglik: -1.9102e+02 - logprior: -7.9695e+00
Epoch 3/10
10/10 - 0s - loss: 195.3696 - loglik: -1.9141e+02 - logprior: -3.9618e+00
Epoch 4/10
10/10 - 0s - loss: 193.0693 - loglik: -1.9062e+02 - logprior: -2.4502e+00
Epoch 5/10
10/10 - 0s - loss: 191.9437 - loglik: -1.9021e+02 - logprior: -1.7336e+00
Epoch 6/10
10/10 - 0s - loss: 190.8475 - loglik: -1.8944e+02 - logprior: -1.4103e+00
Epoch 7/10
10/10 - 0s - loss: 189.9734 - loglik: -1.8877e+02 - logprior: -1.2019e+00
Epoch 8/10
10/10 - 0s - loss: 189.1604 - loglik: -1.8811e+02 - logprior: -1.0475e+00
Epoch 9/10
10/10 - 0s - loss: 188.8902 - loglik: -1.8796e+02 - logprior: -9.3156e-01
Epoch 10/10
10/10 - 0s - loss: 188.4766 - loglik: -1.8761e+02 - logprior: -8.6732e-01
Fitted a model with MAP estimate = -188.3310
Time for alignment: 24.5083
Computed alignments with likelihoods: ['-188.0834', '-188.0939', '-188.3229', '-188.3192', '-188.3310']
Best model has likelihood: -188.0834  (prior= -0.8312 )
time for generating output: 0.0894
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rub.projection.fasta
SP score = 0.9918367346938776
Training of 5 independent models on file cyclo.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea00c7efd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cd887c70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea22ee7dc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 904.3906 - loglik: -8.9907e+02 - logprior: -5.3224e+00
Epoch 2/10
15/15 - 3s - loss: 822.6447 - loglik: -8.2137e+02 - logprior: -1.2793e+00
Epoch 3/10
15/15 - 3s - loss: 778.3653 - loglik: -7.7688e+02 - logprior: -1.4880e+00
Epoch 4/10
15/15 - 3s - loss: 765.4281 - loglik: -7.6385e+02 - logprior: -1.5788e+00
Epoch 5/10
15/15 - 3s - loss: 759.7981 - loglik: -7.5833e+02 - logprior: -1.4652e+00
Epoch 6/10
15/15 - 3s - loss: 757.6122 - loglik: -7.5614e+02 - logprior: -1.4683e+00
Epoch 7/10
15/15 - 3s - loss: 753.8979 - loglik: -7.5244e+02 - logprior: -1.4501e+00
Epoch 8/10
15/15 - 3s - loss: 753.2452 - loglik: -7.5178e+02 - logprior: -1.4564e+00
Epoch 9/10
15/15 - 3s - loss: 751.4944 - loglik: -7.5004e+02 - logprior: -1.4536e+00
Epoch 10/10
15/15 - 3s - loss: 751.0087 - loglik: -7.4954e+02 - logprior: -1.4624e+00
Fitted a model with MAP estimate = -750.3946
expansions: [(7, 3), (10, 1), (14, 1), (18, 1), (24, 1), (27, 1), (49, 2), (55, 1), (59, 1), (64, 2), (65, 2), (69, 1), (91, 1), (92, 2), (105, 2), (114, 4), (116, 3), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 9s - loss: 760.0084 - loglik: -7.5346e+02 - logprior: -6.5452e+00
Epoch 2/2
15/15 - 4s - loss: 742.0527 - loglik: -7.3901e+02 - logprior: -3.0433e+00
Fitted a model with MAP estimate = -740.8120
expansions: [(0, 2)]
discards: [  0   7  57  66  67  77  84 109 125]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 748.6420 - loglik: -7.4374e+02 - logprior: -4.9009e+00
Epoch 2/2
15/15 - 4s - loss: 741.3100 - loglik: -7.3992e+02 - logprior: -1.3894e+00
Fitted a model with MAP estimate = -741.2015
expansions: [(65, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 749.3429 - loglik: -7.4292e+02 - logprior: -6.4236e+00
Epoch 2/10
15/15 - 4s - loss: 741.6973 - loglik: -7.3959e+02 - logprior: -2.1095e+00
Epoch 3/10
15/15 - 4s - loss: 738.6754 - loglik: -7.3772e+02 - logprior: -9.5047e-01
Epoch 4/10
15/15 - 4s - loss: 737.8981 - loglik: -7.3714e+02 - logprior: -7.5546e-01
Epoch 5/10
15/15 - 4s - loss: 737.1644 - loglik: -7.3645e+02 - logprior: -7.0968e-01
Epoch 6/10
15/15 - 4s - loss: 736.4902 - loglik: -7.3578e+02 - logprior: -7.0729e-01
Epoch 7/10
15/15 - 4s - loss: 736.6937 - loglik: -7.3604e+02 - logprior: -6.5459e-01
Fitted a model with MAP estimate = -735.0819
Time for alignment: 115.0902
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 904.1932 - loglik: -8.9886e+02 - logprior: -5.3284e+00
Epoch 2/10
15/15 - 3s - loss: 821.8951 - loglik: -8.2062e+02 - logprior: -1.2706e+00
Epoch 3/10
15/15 - 3s - loss: 782.3489 - loglik: -7.8088e+02 - logprior: -1.4685e+00
Epoch 4/10
15/15 - 3s - loss: 765.2834 - loglik: -7.6369e+02 - logprior: -1.5919e+00
Epoch 5/10
15/15 - 3s - loss: 761.0363 - loglik: -7.5956e+02 - logprior: -1.4764e+00
Epoch 6/10
15/15 - 3s - loss: 758.5967 - loglik: -7.5714e+02 - logprior: -1.4583e+00
Epoch 7/10
15/15 - 3s - loss: 758.7146 - loglik: -7.5727e+02 - logprior: -1.4376e+00
Fitted a model with MAP estimate = -756.9705
expansions: [(7, 3), (10, 1), (14, 1), (24, 3), (25, 1), (49, 2), (60, 1), (62, 1), (66, 2), (69, 1), (91, 1), (104, 1), (106, 2), (112, 1), (114, 1), (115, 1), (116, 4), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 156 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 762.4474 - loglik: -7.5592e+02 - logprior: -6.5242e+00
Epoch 2/2
15/15 - 4s - loss: 744.8135 - loglik: -7.4179e+02 - logprior: -3.0226e+00
Fitted a model with MAP estimate = -743.0297
expansions: [(0, 2)]
discards: [  0   7  29  58  83 123]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 748.5961 - loglik: -7.4368e+02 - logprior: -4.9198e+00
Epoch 2/2
15/15 - 4s - loss: 740.9064 - loglik: -7.3949e+02 - logprior: -1.4149e+00
Fitted a model with MAP estimate = -740.7906
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 750.4084 - loglik: -7.4397e+02 - logprior: -6.4390e+00
Epoch 2/10
15/15 - 4s - loss: 742.5579 - loglik: -7.4052e+02 - logprior: -2.0426e+00
Epoch 3/10
15/15 - 4s - loss: 743.4273 - loglik: -7.4249e+02 - logprior: -9.4150e-01
Fitted a model with MAP estimate = -740.6309
Time for alignment: 87.2790
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 902.5042 - loglik: -8.9719e+02 - logprior: -5.3166e+00
Epoch 2/10
15/15 - 3s - loss: 821.5675 - loglik: -8.2029e+02 - logprior: -1.2728e+00
Epoch 3/10
15/15 - 3s - loss: 776.8785 - loglik: -7.7549e+02 - logprior: -1.3860e+00
Epoch 4/10
15/15 - 3s - loss: 767.2933 - loglik: -7.6577e+02 - logprior: -1.5206e+00
Epoch 5/10
15/15 - 3s - loss: 760.9813 - loglik: -7.5953e+02 - logprior: -1.4523e+00
Epoch 6/10
15/15 - 3s - loss: 760.0970 - loglik: -7.5862e+02 - logprior: -1.4715e+00
Epoch 7/10
15/15 - 3s - loss: 758.4819 - loglik: -7.5702e+02 - logprior: -1.4554e+00
Epoch 8/10
15/15 - 3s - loss: 756.3814 - loglik: -7.5494e+02 - logprior: -1.4332e+00
Epoch 9/10
15/15 - 3s - loss: 756.4941 - loglik: -7.5505e+02 - logprior: -1.4427e+00
Fitted a model with MAP estimate = -755.4741
expansions: [(7, 3), (12, 1), (16, 1), (24, 2), (27, 1), (49, 2), (60, 1), (66, 3), (69, 1), (91, 2), (92, 2), (112, 1), (114, 2), (115, 1), (117, 3), (118, 1), (119, 2)]
discards: [ 0 76]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 761.6154 - loglik: -7.5505e+02 - logprior: -6.5666e+00
Epoch 2/2
15/15 - 4s - loss: 745.6409 - loglik: -7.4265e+02 - logprior: -2.9948e+00
Fitted a model with MAP estimate = -742.9366
expansions: [(0, 2)]
discards: [  0   7  65  66  67 107]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 751.1599 - loglik: -7.4626e+02 - logprior: -4.9010e+00
Epoch 2/2
15/15 - 4s - loss: 743.8778 - loglik: -7.4250e+02 - logprior: -1.3791e+00
Fitted a model with MAP estimate = -743.1227
expansions: [(65, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 751.2564 - loglik: -7.4484e+02 - logprior: -6.4146e+00
Epoch 2/10
15/15 - 4s - loss: 743.1396 - loglik: -7.4104e+02 - logprior: -2.0987e+00
Epoch 3/10
15/15 - 4s - loss: 739.6367 - loglik: -7.3871e+02 - logprior: -9.2611e-01
Epoch 4/10
15/15 - 4s - loss: 741.1307 - loglik: -7.4038e+02 - logprior: -7.5153e-01
Fitted a model with MAP estimate = -739.1266
Time for alignment: 99.8345
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 6s - loss: 903.6207 - loglik: -8.9831e+02 - logprior: -5.3077e+00
Epoch 2/10
15/15 - 3s - loss: 821.2717 - loglik: -8.2000e+02 - logprior: -1.2704e+00
Epoch 3/10
15/15 - 3s - loss: 777.4470 - loglik: -7.7599e+02 - logprior: -1.4543e+00
Epoch 4/10
15/15 - 3s - loss: 765.6052 - loglik: -7.6406e+02 - logprior: -1.5488e+00
Epoch 5/10
15/15 - 3s - loss: 761.5156 - loglik: -7.6010e+02 - logprior: -1.4163e+00
Epoch 6/10
15/15 - 3s - loss: 758.2726 - loglik: -7.5686e+02 - logprior: -1.4149e+00
Epoch 7/10
15/15 - 3s - loss: 755.1851 - loglik: -7.5378e+02 - logprior: -1.3972e+00
Epoch 8/10
15/15 - 3s - loss: 757.5339 - loglik: -7.5616e+02 - logprior: -1.3698e+00
Fitted a model with MAP estimate = -755.3830
expansions: [(7, 3), (10, 1), (14, 1), (24, 2), (25, 1), (26, 1), (60, 1), (66, 3), (69, 1), (91, 1), (92, 2), (112, 1), (114, 2), (115, 2), (116, 4), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 155 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 8s - loss: 761.2489 - loglik: -7.5472e+02 - logprior: -6.5321e+00
Epoch 2/2
15/15 - 4s - loss: 746.1223 - loglik: -7.4313e+02 - logprior: -2.9915e+00
Fitted a model with MAP estimate = -743.5613
expansions: [(0, 2)]
discards: [  0   7 106 137 138]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 748.1500 - loglik: -7.4322e+02 - logprior: -4.9261e+00
Epoch 2/2
15/15 - 4s - loss: 739.4570 - loglik: -7.3808e+02 - logprior: -1.3762e+00
Fitted a model with MAP estimate = -738.7844
expansions: [(75, 1)]
discards: [ 0 65 66 81]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 752.1345 - loglik: -7.4575e+02 - logprior: -6.3835e+00
Epoch 2/10
15/15 - 4s - loss: 744.5826 - loglik: -7.4257e+02 - logprior: -2.0136e+00
Epoch 3/10
15/15 - 4s - loss: 741.3986 - loglik: -7.4049e+02 - logprior: -9.1096e-01
Epoch 4/10
15/15 - 4s - loss: 742.2628 - loglik: -7.4154e+02 - logprior: -7.2205e-01
Fitted a model with MAP estimate = -740.8851
Time for alignment: 94.4684
Fitting a model of length 128 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 7s - loss: 903.6826 - loglik: -8.9836e+02 - logprior: -5.3263e+00
Epoch 2/10
15/15 - 3s - loss: 820.7549 - loglik: -8.1949e+02 - logprior: -1.2666e+00
Epoch 3/10
15/15 - 3s - loss: 776.4276 - loglik: -7.7500e+02 - logprior: -1.4251e+00
Epoch 4/10
15/15 - 3s - loss: 762.7664 - loglik: -7.6128e+02 - logprior: -1.4845e+00
Epoch 5/10
15/15 - 3s - loss: 760.4905 - loglik: -7.5916e+02 - logprior: -1.3246e+00
Epoch 6/10
15/15 - 3s - loss: 757.4697 - loglik: -7.5615e+02 - logprior: -1.3174e+00
Epoch 7/10
15/15 - 3s - loss: 756.8239 - loglik: -7.5553e+02 - logprior: -1.2900e+00
Epoch 8/10
15/15 - 3s - loss: 755.2728 - loglik: -7.5400e+02 - logprior: -1.2638e+00
Epoch 9/10
15/15 - 3s - loss: 754.0987 - loglik: -7.5282e+02 - logprior: -1.2707e+00
Epoch 10/10
15/15 - 3s - loss: 753.4832 - loglik: -7.5221e+02 - logprior: -1.2687e+00
Fitted a model with MAP estimate = -753.0861
expansions: [(7, 3), (10, 1), (16, 1), (24, 2), (27, 1), (56, 1), (60, 2), (65, 2), (69, 1), (91, 2), (92, 2), (112, 1), (114, 3), (116, 3), (119, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 759.8575 - loglik: -7.5332e+02 - logprior: -6.5390e+00
Epoch 2/2
15/15 - 4s - loss: 745.3298 - loglik: -7.4234e+02 - logprior: -2.9904e+00
Fitted a model with MAP estimate = -743.0338
expansions: [(0, 2)]
discards: [  0   7  81 106]
Re-initialized the encoder parameters.
Fitting a model of length 152 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
15/15 - 7s - loss: 747.6467 - loglik: -7.4274e+02 - logprior: -4.9049e+00
Epoch 2/2
15/15 - 4s - loss: 741.0604 - loglik: -7.3968e+02 - logprior: -1.3817e+00
Fitted a model with MAP estimate = -740.2884
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 6288 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
15/15 - 8s - loss: 749.0629 - loglik: -7.4265e+02 - logprior: -6.4109e+00
Epoch 2/10
15/15 - 4s - loss: 743.6377 - loglik: -7.4161e+02 - logprior: -2.0307e+00
Epoch 3/10
15/15 - 4s - loss: 739.8670 - loglik: -7.3894e+02 - logprior: -9.2644e-01
Epoch 4/10
15/15 - 4s - loss: 740.4660 - loglik: -7.3972e+02 - logprior: -7.4134e-01
Fitted a model with MAP estimate = -739.3818
Time for alignment: 102.1767
Computed alignments with likelihoods: ['-735.0819', '-740.6309', '-739.1266', '-738.7844', '-739.3818']
Best model has likelihood: -735.0819  (prior= -0.6492 )
time for generating output: 0.1740
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyclo.projection.fasta
SP score = 0.9126254180602007
Training of 5 independent models on file gpdh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea5e1208b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe938121820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8d2467700>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 664.3252 - loglik: -6.6147e+02 - logprior: -2.8596e+00
Epoch 2/10
34/34 - 4s - loss: 574.8352 - loglik: -5.7303e+02 - logprior: -1.8050e+00
Epoch 3/10
34/34 - 5s - loss: 567.2926 - loglik: -5.6555e+02 - logprior: -1.7431e+00
Epoch 4/10
34/34 - 4s - loss: 564.4421 - loglik: -5.6273e+02 - logprior: -1.7099e+00
Epoch 5/10
34/34 - 4s - loss: 565.2179 - loglik: -5.6350e+02 - logprior: -1.7115e+00
Fitted a model with MAP estimate = -564.7737
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 2), (27, 2), (40, 1), (41, 1), (45, 1), (48, 1), (49, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (66, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 554.2609 - loglik: -5.5133e+02 - logprior: -2.9276e+00
Epoch 2/2
34/34 - 5s - loss: 543.5170 - loglik: -5.4226e+02 - logprior: -1.2521e+00
Fitted a model with MAP estimate = -543.1084
expansions: []
discards: [ 20  34 140]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 10s - loss: 547.1705 - loglik: -5.4455e+02 - logprior: -2.6203e+00
Epoch 2/2
34/34 - 5s - loss: 542.3431 - loglik: -5.4127e+02 - logprior: -1.0762e+00
Fitted a model with MAP estimate = -543.2991
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 545.7461 - loglik: -5.4323e+02 - logprior: -2.5190e+00
Epoch 2/10
34/34 - 5s - loss: 544.8588 - loglik: -5.4390e+02 - logprior: -9.5341e-01
Epoch 3/10
34/34 - 5s - loss: 544.7681 - loglik: -5.4393e+02 - logprior: -8.3700e-01
Epoch 4/10
34/34 - 5s - loss: 540.7599 - loglik: -5.3999e+02 - logprior: -7.6990e-01
Epoch 5/10
34/34 - 5s - loss: 540.7554 - loglik: -5.4008e+02 - logprior: -6.7344e-01
Epoch 6/10
34/34 - 5s - loss: 542.2520 - loglik: -5.4162e+02 - logprior: -6.2485e-01
Fitted a model with MAP estimate = -539.8929
Time for alignment: 111.5290
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 667.5264 - loglik: -6.6468e+02 - logprior: -2.8456e+00
Epoch 2/10
34/34 - 4s - loss: 575.1940 - loglik: -5.7339e+02 - logprior: -1.8005e+00
Epoch 3/10
34/34 - 4s - loss: 568.7729 - loglik: -5.6702e+02 - logprior: -1.7523e+00
Epoch 4/10
34/34 - 4s - loss: 566.1654 - loglik: -5.6446e+02 - logprior: -1.7026e+00
Epoch 5/10
34/34 - 4s - loss: 566.2573 - loglik: -5.6456e+02 - logprior: -1.6984e+00
Fitted a model with MAP estimate = -565.1563
expansions: [(0, 2), (16, 4), (26, 2), (27, 2), (40, 1), (41, 1), (42, 1), (44, 1), (47, 1), (53, 1), (55, 1), (56, 1), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (85, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 555.1073 - loglik: -5.5213e+02 - logprior: -2.9746e+00
Epoch 2/2
34/34 - 5s - loss: 544.0745 - loglik: -5.4295e+02 - logprior: -1.1261e+00
Fitted a model with MAP estimate = -543.7006
expansions: [(17, 1)]
discards: [ 33 138]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 548.4814 - loglik: -5.4587e+02 - logprior: -2.6069e+00
Epoch 2/2
34/34 - 5s - loss: 542.3970 - loglik: -5.4132e+02 - logprior: -1.0725e+00
Fitted a model with MAP estimate = -543.3441
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 547.0418 - loglik: -5.4453e+02 - logprior: -2.5164e+00
Epoch 2/10
34/34 - 5s - loss: 543.6724 - loglik: -5.4271e+02 - logprior: -9.5957e-01
Epoch 3/10
34/34 - 5s - loss: 542.3228 - loglik: -5.4148e+02 - logprior: -8.3856e-01
Epoch 4/10
34/34 - 5s - loss: 542.3121 - loglik: -5.4155e+02 - logprior: -7.5917e-01
Epoch 5/10
34/34 - 5s - loss: 542.6640 - loglik: -5.4198e+02 - logprior: -6.8220e-01
Fitted a model with MAP estimate = -540.8162
Time for alignment: 105.4241
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 667.6045 - loglik: -6.6473e+02 - logprior: -2.8765e+00
Epoch 2/10
34/34 - 5s - loss: 577.6574 - loglik: -5.7581e+02 - logprior: -1.8477e+00
Epoch 3/10
34/34 - 5s - loss: 567.4451 - loglik: -5.6561e+02 - logprior: -1.8343e+00
Epoch 4/10
34/34 - 5s - loss: 567.0203 - loglik: -5.6521e+02 - logprior: -1.8035e+00
Epoch 5/10
34/34 - 4s - loss: 566.6993 - loglik: -5.6490e+02 - logprior: -1.7966e+00
Epoch 6/10
34/34 - 4s - loss: 564.4453 - loglik: -5.6264e+02 - logprior: -1.8020e+00
Epoch 7/10
34/34 - 4s - loss: 565.5511 - loglik: -5.6373e+02 - logprior: -1.8168e+00
Fitted a model with MAP estimate = -564.3073
expansions: [(0, 2), (15, 1), (16, 1), (17, 2), (18, 1), (26, 2), (27, 1), (41, 1), (42, 1), (45, 1), (46, 1), (48, 1), (54, 1), (55, 1), (56, 2), (57, 1), (64, 1), (65, 1), (75, 1), (78, 1), (81, 1), (84, 1), (108, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 554.5056 - loglik: -5.5155e+02 - logprior: -2.9596e+00
Epoch 2/2
34/34 - 5s - loss: 543.5973 - loglik: -5.4247e+02 - logprior: -1.1309e+00
Fitted a model with MAP estimate = -542.4379
expansions: []
discards: [ 33 140]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 546.8003 - loglik: -5.4420e+02 - logprior: -2.6050e+00
Epoch 2/2
34/34 - 5s - loss: 540.9312 - loglik: -5.3986e+02 - logprior: -1.0674e+00
Fitted a model with MAP estimate = -542.8394
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 149 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 545.8137 - loglik: -5.4331e+02 - logprior: -2.5013e+00
Epoch 2/10
34/34 - 5s - loss: 543.9604 - loglik: -5.4302e+02 - logprior: -9.3872e-01
Epoch 3/10
34/34 - 5s - loss: 542.2462 - loglik: -5.4143e+02 - logprior: -8.1799e-01
Epoch 4/10
34/34 - 5s - loss: 542.1162 - loglik: -5.4138e+02 - logprior: -7.3103e-01
Epoch 5/10
34/34 - 5s - loss: 540.3058 - loglik: -5.3964e+02 - logprior: -6.5855e-01
Epoch 6/10
34/34 - 5s - loss: 542.3737 - loglik: -5.4178e+02 - logprior: -5.9198e-01
Fitted a model with MAP estimate = -539.3691
Time for alignment: 119.7192
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 665.1808 - loglik: -6.6232e+02 - logprior: -2.8621e+00
Epoch 2/10
34/34 - 4s - loss: 576.4843 - loglik: -5.7474e+02 - logprior: -1.7430e+00
Epoch 3/10
34/34 - 4s - loss: 569.3508 - loglik: -5.6766e+02 - logprior: -1.6874e+00
Epoch 4/10
34/34 - 4s - loss: 567.9599 - loglik: -5.6632e+02 - logprior: -1.6425e+00
Epoch 5/10
34/34 - 5s - loss: 564.6512 - loglik: -5.6302e+02 - logprior: -1.6246e+00
Epoch 6/10
34/34 - 4s - loss: 567.0878 - loglik: -5.6543e+02 - logprior: -1.6545e+00
Fitted a model with MAP estimate = -564.8193
expansions: [(0, 2), (15, 1), (16, 4), (26, 2), (27, 2), (40, 1), (45, 3), (48, 1), (49, 1), (53, 1), (55, 1), (56, 1), (57, 1), (60, 1), (63, 1), (64, 1), (78, 1), (81, 1), (84, 1), (107, 1), (108, 1), (110, 1), (111, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 150 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 555.5697 - loglik: -5.5260e+02 - logprior: -2.9690e+00
Epoch 2/2
34/34 - 5s - loss: 542.0087 - loglik: -5.4078e+02 - logprior: -1.2330e+00
Fitted a model with MAP estimate = -543.2452
expansions: []
discards: [34 58]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 546.8757 - loglik: -5.4426e+02 - logprior: -2.6147e+00
Epoch 2/2
34/34 - 5s - loss: 544.1125 - loglik: -5.4305e+02 - logprior: -1.0627e+00
Fitted a model with MAP estimate = -543.4525
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 9s - loss: 546.6614 - loglik: -5.4416e+02 - logprior: -2.5045e+00
Epoch 2/10
34/34 - 5s - loss: 544.1248 - loglik: -5.4318e+02 - logprior: -9.4717e-01
Epoch 3/10
34/34 - 5s - loss: 540.3448 - loglik: -5.3951e+02 - logprior: -8.2970e-01
Epoch 4/10
34/34 - 5s - loss: 545.0445 - loglik: -5.4430e+02 - logprior: -7.4223e-01
Fitted a model with MAP estimate = -541.7124
Time for alignment: 106.9132
Fitting a model of length 119 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 7s - loss: 663.4642 - loglik: -6.6061e+02 - logprior: -2.8517e+00
Epoch 2/10
34/34 - 4s - loss: 574.1697 - loglik: -5.7235e+02 - logprior: -1.8179e+00
Epoch 3/10
34/34 - 5s - loss: 567.2582 - loglik: -5.6554e+02 - logprior: -1.7203e+00
Epoch 4/10
34/34 - 4s - loss: 566.5858 - loglik: -5.6492e+02 - logprior: -1.6590e+00
Epoch 5/10
34/34 - 5s - loss: 564.9426 - loglik: -5.6328e+02 - logprior: -1.6574e+00
Epoch 6/10
34/34 - 4s - loss: 564.8520 - loglik: -5.6319e+02 - logprior: -1.6600e+00
Epoch 7/10
34/34 - 4s - loss: 564.8939 - loglik: -5.6322e+02 - logprior: -1.6697e+00
Fitted a model with MAP estimate = -563.8912
expansions: [(0, 2), (15, 1), (16, 2), (17, 3), (26, 2), (27, 2), (40, 1), (41, 1), (44, 1), (45, 1), (47, 1), (53, 1), (54, 1), (55, 2), (56, 1), (63, 1), (64, 1), (78, 1), (81, 1), (84, 1), (109, 1), (110, 1), (111, 2), (112, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 151 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 8s - loss: 554.8428 - loglik: -5.5188e+02 - logprior: -2.9650e+00
Epoch 2/2
34/34 - 5s - loss: 543.2554 - loglik: -5.4218e+02 - logprior: -1.0773e+00
Fitted a model with MAP estimate = -543.2589
expansions: []
discards: [ 20  35 140]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
34/34 - 9s - loss: 546.7615 - loglik: -5.4419e+02 - logprior: -2.5734e+00
Epoch 2/2
34/34 - 5s - loss: 543.2107 - loglik: -5.4215e+02 - logprior: -1.0651e+00
Fitted a model with MAP estimate = -543.4558
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 148 on 7691 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
34/34 - 8s - loss: 546.8890 - loglik: -5.4438e+02 - logprior: -2.5085e+00
Epoch 2/10
34/34 - 5s - loss: 542.9827 - loglik: -5.4205e+02 - logprior: -9.3426e-01
Epoch 3/10
34/34 - 5s - loss: 544.7792 - loglik: -5.4396e+02 - logprior: -8.2202e-01
Fitted a model with MAP estimate = -542.6239
Time for alignment: 103.6892
Computed alignments with likelihoods: ['-539.8929', '-540.8162', '-539.3691', '-541.7124', '-542.6239']
Best model has likelihood: -539.3691  (prior= -0.5727 )
time for generating output: 0.3221
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gpdh.projection.fasta
SP score = 0.6275780153331174
Training of 5 independent models on file sodcu.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9bc44bc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efa3e190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe938204d30>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 820.6686 - loglik: -7.9996e+02 - logprior: -2.0709e+01
Epoch 2/10
10/10 - 2s - loss: 760.3913 - loglik: -7.5564e+02 - logprior: -4.7490e+00
Epoch 3/10
10/10 - 2s - loss: 717.0780 - loglik: -7.1459e+02 - logprior: -2.4907e+00
Epoch 4/10
10/10 - 2s - loss: 691.4667 - loglik: -6.8953e+02 - logprior: -1.9410e+00
Epoch 5/10
10/10 - 2s - loss: 679.1057 - loglik: -6.7728e+02 - logprior: -1.8241e+00
Epoch 6/10
10/10 - 2s - loss: 672.9640 - loglik: -6.7116e+02 - logprior: -1.8040e+00
Epoch 7/10
10/10 - 2s - loss: 668.9976 - loglik: -6.6722e+02 - logprior: -1.7762e+00
Epoch 8/10
10/10 - 2s - loss: 665.8798 - loglik: -6.6414e+02 - logprior: -1.7370e+00
Epoch 9/10
10/10 - 2s - loss: 663.2667 - loglik: -6.6163e+02 - logprior: -1.6374e+00
Epoch 10/10
10/10 - 2s - loss: 662.9278 - loglik: -6.6130e+02 - logprior: -1.6278e+00
Fitted a model with MAP estimate = -662.3917
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (21, 1), (22, 1), (23, 1), (24, 1), (26, 1), (53, 1), (55, 2), (62, 1), (71, 1), (79, 1), (81, 1), (82, 2), (85, 1), (98, 1), (99, 1), (101, 4), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 691.5016 - loglik: -6.7279e+02 - logprior: -1.8708e+01
Epoch 2/2
10/10 - 2s - loss: 658.7241 - loglik: -6.5430e+02 - logprior: -4.4268e+00
Fitted a model with MAP estimate = -654.3078
expansions: []
discards: [  0  66 125]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 683.1186 - loglik: -6.5986e+02 - logprior: -2.3263e+01
Epoch 2/2
10/10 - 2s - loss: 663.1143 - loglik: -6.5403e+02 - logprior: -9.0873e+00
Fitted a model with MAP estimate = -659.3644
expansions: [(0, 5)]
discards: [  0 122]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 678.0284 - loglik: -6.5966e+02 - logprior: -1.8372e+01
Epoch 2/10
10/10 - 2s - loss: 655.5470 - loglik: -6.5149e+02 - logprior: -4.0522e+00
Epoch 3/10
10/10 - 2s - loss: 653.3925 - loglik: -6.5221e+02 - logprior: -1.1858e+00
Epoch 4/10
10/10 - 2s - loss: 650.9545 - loglik: -6.5078e+02 - logprior: -1.7819e-01
Epoch 5/10
10/10 - 2s - loss: 650.2130 - loglik: -6.5051e+02 - logprior: 0.3004
Epoch 6/10
10/10 - 2s - loss: 647.4061 - loglik: -6.4800e+02 - logprior: 0.5908
Epoch 7/10
10/10 - 2s - loss: 647.3265 - loglik: -6.4816e+02 - logprior: 0.8338
Epoch 8/10
10/10 - 2s - loss: 644.4389 - loglik: -6.4540e+02 - logprior: 0.9622
Epoch 9/10
10/10 - 2s - loss: 643.4962 - loglik: -6.4451e+02 - logprior: 1.0210
Epoch 10/10
10/10 - 2s - loss: 641.6610 - loglik: -6.4271e+02 - logprior: 1.0527
Fitted a model with MAP estimate = -641.7622
Time for alignment: 64.8439
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 821.6538 - loglik: -8.0094e+02 - logprior: -2.0718e+01
Epoch 2/10
10/10 - 2s - loss: 758.2812 - loglik: -7.5353e+02 - logprior: -4.7485e+00
Epoch 3/10
10/10 - 2s - loss: 717.4108 - loglik: -7.1499e+02 - logprior: -2.4232e+00
Epoch 4/10
10/10 - 2s - loss: 691.6121 - loglik: -6.8979e+02 - logprior: -1.8266e+00
Epoch 5/10
10/10 - 2s - loss: 679.3400 - loglik: -6.7765e+02 - logprior: -1.6860e+00
Epoch 6/10
10/10 - 2s - loss: 674.6086 - loglik: -6.7298e+02 - logprior: -1.6242e+00
Epoch 7/10
10/10 - 2s - loss: 671.1459 - loglik: -6.6971e+02 - logprior: -1.4327e+00
Epoch 8/10
10/10 - 2s - loss: 669.7223 - loglik: -6.6842e+02 - logprior: -1.2985e+00
Epoch 9/10
10/10 - 2s - loss: 667.4157 - loglik: -6.6618e+02 - logprior: -1.2332e+00
Epoch 10/10
10/10 - 2s - loss: 665.7723 - loglik: -6.6454e+02 - logprior: -1.2324e+00
Fitted a model with MAP estimate = -665.1190
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 1), (25, 1), (27, 1), (29, 2), (56, 1), (63, 1), (80, 3), (81, 2), (82, 2), (83, 1), (87, 1), (97, 4), (98, 1), (102, 2), (103, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 692.7284 - loglik: -6.7391e+02 - logprior: -1.8821e+01
Epoch 2/2
10/10 - 2s - loss: 662.5163 - loglik: -6.5789e+02 - logprior: -4.6285e+00
Fitted a model with MAP estimate = -656.8886
expansions: []
discards: [  0  38  97 121 122 123 124 129]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 688.2355 - loglik: -6.6493e+02 - logprior: -2.3305e+01
Epoch 2/2
10/10 - 2s - loss: 668.2108 - loglik: -6.5910e+02 - logprior: -9.1140e+00
Fitted a model with MAP estimate = -664.2482
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 679.3690 - loglik: -6.6097e+02 - logprior: -1.8394e+01
Epoch 2/10
10/10 - 2s - loss: 660.1223 - loglik: -6.5609e+02 - logprior: -4.0279e+00
Epoch 3/10
10/10 - 2s - loss: 655.9830 - loglik: -6.5480e+02 - logprior: -1.1860e+00
Epoch 4/10
10/10 - 2s - loss: 653.7030 - loglik: -6.5353e+02 - logprior: -1.7559e-01
Epoch 5/10
10/10 - 2s - loss: 652.3268 - loglik: -6.5260e+02 - logprior: 0.2775
Epoch 6/10
10/10 - 2s - loss: 650.7051 - loglik: -6.5130e+02 - logprior: 0.5946
Epoch 7/10
10/10 - 2s - loss: 649.2784 - loglik: -6.5009e+02 - logprior: 0.8137
Epoch 8/10
10/10 - 2s - loss: 648.1403 - loglik: -6.4907e+02 - logprior: 0.9337
Epoch 9/10
10/10 - 2s - loss: 645.6318 - loglik: -6.4662e+02 - logprior: 0.9897
Epoch 10/10
10/10 - 2s - loss: 644.6185 - loglik: -6.4562e+02 - logprior: 1.0020
Fitted a model with MAP estimate = -644.4540
Time for alignment: 64.8771
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 821.0802 - loglik: -8.0037e+02 - logprior: -2.0709e+01
Epoch 2/10
10/10 - 2s - loss: 760.0034 - loglik: -7.5524e+02 - logprior: -4.7601e+00
Epoch 3/10
10/10 - 2s - loss: 717.0573 - loglik: -7.1448e+02 - logprior: -2.5751e+00
Epoch 4/10
10/10 - 2s - loss: 688.7767 - loglik: -6.8672e+02 - logprior: -2.0528e+00
Epoch 5/10
10/10 - 2s - loss: 677.5228 - loglik: -6.7571e+02 - logprior: -1.8159e+00
Epoch 6/10
10/10 - 2s - loss: 672.4686 - loglik: -6.7084e+02 - logprior: -1.6274e+00
Epoch 7/10
10/10 - 2s - loss: 668.6838 - loglik: -6.6712e+02 - logprior: -1.5627e+00
Epoch 8/10
10/10 - 2s - loss: 667.5099 - loglik: -6.6597e+02 - logprior: -1.5376e+00
Epoch 9/10
10/10 - 2s - loss: 665.2365 - loglik: -6.6372e+02 - logprior: -1.5175e+00
Epoch 10/10
10/10 - 2s - loss: 663.9310 - loglik: -6.6237e+02 - logprior: -1.5608e+00
Fitted a model with MAP estimate = -663.4417
expansions: [(8, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 3), (26, 1), (53, 1), (60, 1), (62, 1), (79, 2), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (86, 1), (98, 1), (101, 2), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 144 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 689.2096 - loglik: -6.7041e+02 - logprior: -1.8803e+01
Epoch 2/2
10/10 - 2s - loss: 662.3205 - loglik: -6.5782e+02 - logprior: -4.5051e+00
Fitted a model with MAP estimate = -656.2114
expansions: []
discards: [ 0 93]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 683.6585 - loglik: -6.6035e+02 - logprior: -2.3307e+01
Epoch 2/2
10/10 - 2s - loss: 663.0817 - loglik: -6.5398e+02 - logprior: -9.1049e+00
Fitted a model with MAP estimate = -660.0605
expansions: [(0, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 147 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 675.9334 - loglik: -6.5752e+02 - logprior: -1.8414e+01
Epoch 2/10
10/10 - 2s - loss: 656.5564 - loglik: -6.5251e+02 - logprior: -4.0443e+00
Epoch 3/10
10/10 - 2s - loss: 653.3197 - loglik: -6.5211e+02 - logprior: -1.2107e+00
Epoch 4/10
10/10 - 2s - loss: 649.8174 - loglik: -6.4961e+02 - logprior: -2.1029e-01
Epoch 5/10
10/10 - 2s - loss: 648.5905 - loglik: -6.4881e+02 - logprior: 0.2165
Epoch 6/10
10/10 - 2s - loss: 647.8959 - loglik: -6.4842e+02 - logprior: 0.5239
Epoch 7/10
10/10 - 2s - loss: 646.6633 - loglik: -6.4746e+02 - logprior: 0.7947
Epoch 8/10
10/10 - 2s - loss: 643.6656 - loglik: -6.4459e+02 - logprior: 0.9232
Epoch 9/10
10/10 - 2s - loss: 642.5569 - loglik: -6.4352e+02 - logprior: 0.9626
Epoch 10/10
10/10 - 2s - loss: 642.1022 - loglik: -6.4309e+02 - logprior: 0.9936
Fitted a model with MAP estimate = -641.0786
Time for alignment: 64.0975
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 820.1722 - loglik: -7.9947e+02 - logprior: -2.0706e+01
Epoch 2/10
10/10 - 2s - loss: 760.3880 - loglik: -7.5564e+02 - logprior: -4.7452e+00
Epoch 3/10
10/10 - 2s - loss: 718.7398 - loglik: -7.1625e+02 - logprior: -2.4883e+00
Epoch 4/10
10/10 - 2s - loss: 690.2745 - loglik: -6.8831e+02 - logprior: -1.9673e+00
Epoch 5/10
10/10 - 2s - loss: 679.7116 - loglik: -6.7793e+02 - logprior: -1.7846e+00
Epoch 6/10
10/10 - 2s - loss: 675.6356 - loglik: -6.7403e+02 - logprior: -1.6035e+00
Epoch 7/10
10/10 - 2s - loss: 671.8799 - loglik: -6.7035e+02 - logprior: -1.5286e+00
Epoch 8/10
10/10 - 2s - loss: 668.9791 - loglik: -6.6746e+02 - logprior: -1.5137e+00
Epoch 9/10
10/10 - 2s - loss: 666.7921 - loglik: -6.6527e+02 - logprior: -1.5152e+00
Epoch 10/10
10/10 - 2s - loss: 666.1297 - loglik: -6.6457e+02 - logprior: -1.5555e+00
Fitted a model with MAP estimate = -664.8940
expansions: [(8, 1), (9, 1), (14, 1), (17, 2), (23, 1), (24, 4), (26, 1), (53, 1), (55, 2), (62, 1), (79, 2), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (86, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 694.7589 - loglik: -6.7590e+02 - logprior: -1.8858e+01
Epoch 2/2
10/10 - 2s - loss: 663.8328 - loglik: -6.5919e+02 - logprior: -4.6378e+00
Fitted a model with MAP estimate = -659.4096
expansions: []
discards: [ 0 31 67 95]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 687.2903 - loglik: -6.6392e+02 - logprior: -2.3368e+01
Epoch 2/2
10/10 - 2s - loss: 666.3283 - loglik: -6.5717e+02 - logprior: -9.1581e+00
Fitted a model with MAP estimate = -663.4674
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 679.4414 - loglik: -6.6098e+02 - logprior: -1.8465e+01
Epoch 2/10
10/10 - 2s - loss: 659.8505 - loglik: -6.5576e+02 - logprior: -4.0880e+00
Epoch 3/10
10/10 - 2s - loss: 656.2425 - loglik: -6.5498e+02 - logprior: -1.2672e+00
Epoch 4/10
10/10 - 2s - loss: 654.0006 - loglik: -6.5375e+02 - logprior: -2.4526e-01
Epoch 5/10
10/10 - 2s - loss: 652.8171 - loglik: -6.5301e+02 - logprior: 0.1943
Epoch 6/10
10/10 - 2s - loss: 652.1121 - loglik: -6.5261e+02 - logprior: 0.5030
Epoch 7/10
10/10 - 2s - loss: 648.4246 - loglik: -6.4919e+02 - logprior: 0.7659
Epoch 8/10
10/10 - 2s - loss: 648.6041 - loglik: -6.4947e+02 - logprior: 0.8721
Fitted a model with MAP estimate = -646.8984
Time for alignment: 59.5242
Fitting a model of length 116 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 820.9497 - loglik: -8.0024e+02 - logprior: -2.0708e+01
Epoch 2/10
10/10 - 2s - loss: 761.1102 - loglik: -7.5636e+02 - logprior: -4.7529e+00
Epoch 3/10
10/10 - 2s - loss: 717.1927 - loglik: -7.1468e+02 - logprior: -2.5153e+00
Epoch 4/10
10/10 - 2s - loss: 692.4661 - loglik: -6.9044e+02 - logprior: -2.0221e+00
Epoch 5/10
10/10 - 2s - loss: 680.8999 - loglik: -6.7904e+02 - logprior: -1.8629e+00
Epoch 6/10
10/10 - 2s - loss: 675.8981 - loglik: -6.7419e+02 - logprior: -1.7088e+00
Epoch 7/10
10/10 - 2s - loss: 672.3869 - loglik: -6.7083e+02 - logprior: -1.5535e+00
Epoch 8/10
10/10 - 2s - loss: 670.0165 - loglik: -6.6852e+02 - logprior: -1.4978e+00
Epoch 9/10
10/10 - 2s - loss: 669.1350 - loglik: -6.6763e+02 - logprior: -1.4993e+00
Epoch 10/10
10/10 - 2s - loss: 665.9401 - loglik: -6.6443e+02 - logprior: -1.5089e+00
Fitted a model with MAP estimate = -666.2740
expansions: [(6, 2), (7, 1), (9, 1), (10, 1), (17, 2), (23, 1), (24, 4), (26, 1), (53, 1), (55, 1), (62, 1), (79, 1), (80, 1), (81, 1), (82, 2), (83, 1), (87, 1), (92, 1), (102, 1), (103, 1), (104, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 143 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 694.3912 - loglik: -6.7552e+02 - logprior: -1.8869e+01
Epoch 2/2
10/10 - 2s - loss: 665.0131 - loglik: -6.6039e+02 - logprior: -4.6279e+00
Fitted a model with MAP estimate = -660.0190
expansions: []
discards: [ 0  6 33]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 687.4535 - loglik: -6.6404e+02 - logprior: -2.3417e+01
Epoch 2/2
10/10 - 2s - loss: 666.9221 - loglik: -6.5772e+02 - logprior: -9.2034e+00
Fitted a model with MAP estimate = -664.1053
expansions: [(0, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 145 on 2038 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 680.2234 - loglik: -6.6168e+02 - logprior: -1.8540e+01
Epoch 2/10
10/10 - 2s - loss: 660.2526 - loglik: -6.5610e+02 - logprior: -4.1545e+00
Epoch 3/10
10/10 - 2s - loss: 655.8948 - loglik: -6.5458e+02 - logprior: -1.3120e+00
Epoch 4/10
10/10 - 2s - loss: 654.4675 - loglik: -6.5417e+02 - logprior: -2.9841e-01
Epoch 5/10
10/10 - 2s - loss: 652.7352 - loglik: -6.5284e+02 - logprior: 0.1034
Epoch 6/10
10/10 - 2s - loss: 651.8749 - loglik: -6.5228e+02 - logprior: 0.4031
Epoch 7/10
10/10 - 2s - loss: 648.9880 - loglik: -6.4964e+02 - logprior: 0.6501
Epoch 8/10
10/10 - 2s - loss: 648.2696 - loglik: -6.4905e+02 - logprior: 0.7850
Epoch 9/10
10/10 - 2s - loss: 646.2859 - loglik: -6.4712e+02 - logprior: 0.8327
Epoch 10/10
10/10 - 2s - loss: 645.9360 - loglik: -6.4680e+02 - logprior: 0.8660
Fitted a model with MAP estimate = -644.9645
Time for alignment: 62.3063
Computed alignments with likelihoods: ['-641.7622', '-644.4540', '-641.0786', '-646.8984', '-644.9645']
Best model has likelihood: -641.0786  (prior= 1.0074 )
time for generating output: 0.1763
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodcu.projection.fasta
SP score = 0.883399209486166
Training of 5 independent models on file DEATH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d739e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea0969f910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9bc1917f0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 502.5906 - loglik: -4.6528e+02 - logprior: -3.7313e+01
Epoch 2/10
10/10 - 1s - loss: 464.0912 - loglik: -4.5453e+02 - logprior: -9.5631e+00
Epoch 3/10
10/10 - 1s - loss: 448.9841 - loglik: -4.4467e+02 - logprior: -4.3172e+00
Epoch 4/10
10/10 - 1s - loss: 436.5911 - loglik: -4.3403e+02 - logprior: -2.5623e+00
Epoch 5/10
10/10 - 1s - loss: 427.5017 - loglik: -4.2533e+02 - logprior: -2.1742e+00
Epoch 6/10
10/10 - 1s - loss: 420.6202 - loglik: -4.1888e+02 - logprior: -1.7422e+00
Epoch 7/10
10/10 - 1s - loss: 415.2772 - loglik: -4.1419e+02 - logprior: -1.0838e+00
Epoch 8/10
10/10 - 1s - loss: 409.6904 - loglik: -4.0885e+02 - logprior: -8.3851e-01
Epoch 9/10
10/10 - 1s - loss: 404.8549 - loglik: -4.0406e+02 - logprior: -7.8694e-01
Epoch 10/10
10/10 - 1s - loss: 398.1019 - loglik: -3.9737e+02 - logprior: -7.2497e-01
Fitted a model with MAP estimate = -393.0721
expansions: [(0, 3), (6, 2), (21, 2), (29, 3), (34, 1), (41, 1), (45, 2), (55, 1), (56, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 517.7714 - loglik: -4.6798e+02 - logprior: -4.9795e+01
Epoch 2/2
10/10 - 1s - loss: 458.3839 - loglik: -4.4252e+02 - logprior: -1.5864e+01
Fitted a model with MAP estimate = -442.9853
expansions: [(0, 5), (35, 2)]
discards: [ 2  3  4  5  6 11 27 28 29 30 31 32 33 39 40 56]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 474.8676 - loglik: -4.3302e+02 - logprior: -4.1849e+01
Epoch 2/2
10/10 - 1s - loss: 438.4853 - loglik: -4.2542e+02 - logprior: -1.3069e+01
Fitted a model with MAP estimate = -431.2644
expansions: [(26, 1), (30, 2), (32, 1), (33, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 465.5895 - loglik: -4.2450e+02 - logprior: -4.1088e+01
Epoch 2/10
10/10 - 1s - loss: 438.1474 - loglik: -4.2210e+02 - logprior: -1.6050e+01
Epoch 3/10
10/10 - 1s - loss: 428.9797 - loglik: -4.1993e+02 - logprior: -9.0489e+00
Epoch 4/10
10/10 - 1s - loss: 421.4185 - loglik: -4.1844e+02 - logprior: -2.9782e+00
Epoch 5/10
10/10 - 1s - loss: 417.3051 - loglik: -4.1675e+02 - logprior: -5.5005e-01
Epoch 6/10
10/10 - 1s - loss: 413.7969 - loglik: -4.1412e+02 - logprior: 0.3257
Epoch 7/10
10/10 - 1s - loss: 410.0026 - loglik: -4.1087e+02 - logprior: 0.8719
Epoch 8/10
10/10 - 1s - loss: 405.3926 - loglik: -4.0650e+02 - logprior: 1.1115
Epoch 9/10
10/10 - 1s - loss: 401.4868 - loglik: -4.0269e+02 - logprior: 1.2109
Epoch 10/10
10/10 - 1s - loss: 391.7385 - loglik: -3.9298e+02 - logprior: 1.2433
Fitted a model with MAP estimate = -382.1835
Time for alignment: 38.6609
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 502.5540 - loglik: -4.6524e+02 - logprior: -3.7314e+01
Epoch 2/10
10/10 - 1s - loss: 463.3678 - loglik: -4.5380e+02 - logprior: -9.5685e+00
Epoch 3/10
10/10 - 1s - loss: 445.9846 - loglik: -4.4162e+02 - logprior: -4.3657e+00
Epoch 4/10
10/10 - 1s - loss: 434.1981 - loglik: -4.3151e+02 - logprior: -2.6851e+00
Epoch 5/10
10/10 - 1s - loss: 426.9557 - loglik: -4.2479e+02 - logprior: -2.1687e+00
Epoch 6/10
10/10 - 1s - loss: 420.9977 - loglik: -4.1952e+02 - logprior: -1.4806e+00
Epoch 7/10
10/10 - 1s - loss: 415.0472 - loglik: -4.1418e+02 - logprior: -8.6857e-01
Epoch 8/10
10/10 - 1s - loss: 409.2074 - loglik: -4.0846e+02 - logprior: -7.4722e-01
Epoch 9/10
10/10 - 1s - loss: 403.5835 - loglik: -4.0292e+02 - logprior: -6.5587e-01
Epoch 10/10
10/10 - 1s - loss: 395.1415 - loglik: -3.9457e+02 - logprior: -5.6134e-01
Fitted a model with MAP estimate = -389.5670
expansions: [(0, 3), (6, 2), (22, 1), (25, 1), (31, 2), (45, 2), (49, 1), (55, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 520.7588 - loglik: -4.7072e+02 - logprior: -5.0038e+01
Epoch 2/2
10/10 - 1s - loss: 461.7833 - loglik: -4.4567e+02 - logprior: -1.6117e+01
Fitted a model with MAP estimate = -446.3293
expansions: [(0, 5), (39, 4)]
discards: [ 2  3  4  5  6 11 28 29 32 33 34 35 36]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 475.6207 - loglik: -4.3370e+02 - logprior: -4.1919e+01
Epoch 2/2
10/10 - 1s - loss: 438.2576 - loglik: -4.2535e+02 - logprior: -1.2905e+01
Fitted a model with MAP estimate = -430.6083
expansions: [(28, 1), (29, 1), (30, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 81 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 465.3512 - loglik: -4.2427e+02 - logprior: -4.1077e+01
Epoch 2/10
10/10 - 1s - loss: 438.1474 - loglik: -4.2206e+02 - logprior: -1.6088e+01
Epoch 3/10
10/10 - 1s - loss: 429.4292 - loglik: -4.2041e+02 - logprior: -9.0181e+00
Epoch 4/10
10/10 - 1s - loss: 421.7509 - loglik: -4.1876e+02 - logprior: -2.9861e+00
Epoch 5/10
10/10 - 1s - loss: 417.2983 - loglik: -4.1669e+02 - logprior: -6.0734e-01
Epoch 6/10
10/10 - 1s - loss: 414.4271 - loglik: -4.1465e+02 - logprior: 0.2290
Epoch 7/10
10/10 - 1s - loss: 410.7676 - loglik: -4.1151e+02 - logprior: 0.7456
Epoch 8/10
10/10 - 1s - loss: 407.0050 - loglik: -4.0799e+02 - logprior: 0.9881
Epoch 9/10
10/10 - 1s - loss: 402.8262 - loglik: -4.0389e+02 - logprior: 1.0661
Epoch 10/10
10/10 - 1s - loss: 395.3335 - loglik: -3.9644e+02 - logprior: 1.1102
Fitted a model with MAP estimate = -388.4338
Time for alignment: 36.4142
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 502.5186 - loglik: -4.6521e+02 - logprior: -3.7312e+01
Epoch 2/10
10/10 - 1s - loss: 463.8348 - loglik: -4.5427e+02 - logprior: -9.5643e+00
Epoch 3/10
10/10 - 1s - loss: 448.2523 - loglik: -4.4387e+02 - logprior: -4.3844e+00
Epoch 4/10
10/10 - 1s - loss: 436.3097 - loglik: -4.3359e+02 - logprior: -2.7211e+00
Epoch 5/10
10/10 - 1s - loss: 427.8123 - loglik: -4.2561e+02 - logprior: -2.2059e+00
Epoch 6/10
10/10 - 1s - loss: 420.5583 - loglik: -4.1885e+02 - logprior: -1.7025e+00
Epoch 7/10
10/10 - 1s - loss: 414.6136 - loglik: -4.1354e+02 - logprior: -1.0685e+00
Epoch 8/10
10/10 - 1s - loss: 408.7017 - loglik: -4.0782e+02 - logprior: -8.7405e-01
Epoch 9/10
10/10 - 1s - loss: 402.7608 - loglik: -4.0193e+02 - logprior: -8.2771e-01
Epoch 10/10
10/10 - 1s - loss: 395.7804 - loglik: -3.9500e+02 - logprior: -7.7059e-01
Fitted a model with MAP estimate = -390.2843
expansions: [(0, 3), (6, 2), (22, 1), (29, 2), (31, 1), (34, 1), (41, 1), (44, 2), (52, 1), (55, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 519.9917 - loglik: -4.7013e+02 - logprior: -4.9858e+01
Epoch 2/2
10/10 - 1s - loss: 459.8931 - loglik: -4.4408e+02 - logprior: -1.5811e+01
Fitted a model with MAP estimate = -443.9093
expansions: [(0, 5), (34, 3)]
discards: [ 2  3  4  5  6 11 28 29 30 31 32 37 38]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 474.5408 - loglik: -4.3269e+02 - logprior: -4.1852e+01
Epoch 2/2
10/10 - 1s - loss: 438.2773 - loglik: -4.2520e+02 - logprior: -1.3081e+01
Fitted a model with MAP estimate = -430.7466
expansions: [(25, 1), (26, 2), (28, 1), (32, 1), (34, 1)]
discards: [ 0 50]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 465.0627 - loglik: -4.2403e+02 - logprior: -4.1034e+01
Epoch 2/10
10/10 - 1s - loss: 436.7989 - loglik: -4.2067e+02 - logprior: -1.6133e+01
Epoch 3/10
10/10 - 1s - loss: 427.5186 - loglik: -4.1832e+02 - logprior: -9.1968e+00
Epoch 4/10
10/10 - 1s - loss: 419.8454 - loglik: -4.1675e+02 - logprior: -3.0920e+00
Epoch 5/10
10/10 - 1s - loss: 415.6405 - loglik: -4.1504e+02 - logprior: -6.0057e-01
Epoch 6/10
10/10 - 1s - loss: 412.6570 - loglik: -4.1292e+02 - logprior: 0.2605
Epoch 7/10
10/10 - 1s - loss: 409.2655 - loglik: -4.1006e+02 - logprior: 0.7942
Epoch 8/10
10/10 - 1s - loss: 405.0413 - loglik: -4.0608e+02 - logprior: 1.0420
Epoch 9/10
10/10 - 1s - loss: 401.2436 - loglik: -4.0239e+02 - logprior: 1.1549
Epoch 10/10
10/10 - 1s - loss: 393.7576 - loglik: -3.9497e+02 - logprior: 1.2163
Fitted a model with MAP estimate = -385.6846
Time for alignment: 36.6179
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 502.6730 - loglik: -4.6536e+02 - logprior: -3.7313e+01
Epoch 2/10
10/10 - 1s - loss: 462.9684 - loglik: -4.5340e+02 - logprior: -9.5640e+00
Epoch 3/10
10/10 - 1s - loss: 446.2806 - loglik: -4.4197e+02 - logprior: -4.3083e+00
Epoch 4/10
10/10 - 1s - loss: 434.6537 - loglik: -4.3214e+02 - logprior: -2.5175e+00
Epoch 5/10
10/10 - 1s - loss: 426.4749 - loglik: -4.2446e+02 - logprior: -2.0150e+00
Epoch 6/10
10/10 - 1s - loss: 420.5961 - loglik: -4.1923e+02 - logprior: -1.3606e+00
Epoch 7/10
10/10 - 1s - loss: 415.0399 - loglik: -4.1435e+02 - logprior: -6.8783e-01
Epoch 8/10
10/10 - 1s - loss: 409.7472 - loglik: -4.0921e+02 - logprior: -5.3759e-01
Epoch 9/10
10/10 - 1s - loss: 404.3358 - loglik: -4.0384e+02 - logprior: -4.9414e-01
Epoch 10/10
10/10 - 1s - loss: 397.0717 - loglik: -3.9662e+02 - logprior: -4.5042e-01
Fitted a model with MAP estimate = -392.0557
expansions: [(0, 3), (6, 2), (22, 1), (25, 1), (31, 2), (43, 2), (44, 2), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 518.5659 - loglik: -4.6863e+02 - logprior: -4.9936e+01
Epoch 2/2
10/10 - 1s - loss: 459.6898 - loglik: -4.4367e+02 - logprior: -1.6017e+01
Fitted a model with MAP estimate = -444.6105
expansions: [(0, 6), (31, 2), (39, 2)]
discards: [ 2  3  4  5  6 11 28 29 32 33 34 35 40 41 42 52]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 475.3465 - loglik: -4.3333e+02 - logprior: -4.2013e+01
Epoch 2/2
10/10 - 1s - loss: 439.8664 - loglik: -4.2664e+02 - logprior: -1.3223e+01
Fitted a model with MAP estimate = -432.9304
expansions: [(33, 5)]
discards: [ 0  6 29 37]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 466.6606 - loglik: -4.2577e+02 - logprior: -4.0895e+01
Epoch 2/10
10/10 - 1s - loss: 437.9557 - loglik: -4.2255e+02 - logprior: -1.5407e+01
Epoch 3/10
10/10 - 1s - loss: 428.1574 - loglik: -4.2130e+02 - logprior: -6.8567e+00
Epoch 4/10
10/10 - 1s - loss: 421.7743 - loglik: -4.1974e+02 - logprior: -2.0302e+00
Epoch 5/10
10/10 - 1s - loss: 418.5583 - loglik: -4.1800e+02 - logprior: -5.5821e-01
Epoch 6/10
10/10 - 1s - loss: 415.5798 - loglik: -4.1571e+02 - logprior: 0.1285
Epoch 7/10
10/10 - 1s - loss: 411.9571 - loglik: -4.1249e+02 - logprior: 0.5372
Epoch 8/10
10/10 - 1s - loss: 408.7305 - loglik: -4.0949e+02 - logprior: 0.7642
Epoch 9/10
10/10 - 1s - loss: 404.3393 - loglik: -4.0524e+02 - logprior: 0.9009
Epoch 10/10
10/10 - 1s - loss: 397.2043 - loglik: -3.9815e+02 - logprior: 0.9490
Fitted a model with MAP estimate = -389.6472
Time for alignment: 36.9538
Fitting a model of length 65 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 502.4377 - loglik: -4.6513e+02 - logprior: -3.7312e+01
Epoch 2/10
10/10 - 1s - loss: 463.5620 - loglik: -4.5399e+02 - logprior: -9.5687e+00
Epoch 3/10
10/10 - 1s - loss: 445.8759 - loglik: -4.4149e+02 - logprior: -4.3825e+00
Epoch 4/10
10/10 - 1s - loss: 434.0198 - loglik: -4.3129e+02 - logprior: -2.7340e+00
Epoch 5/10
10/10 - 1s - loss: 426.3625 - loglik: -4.2415e+02 - logprior: -2.2130e+00
Epoch 6/10
10/10 - 1s - loss: 420.5626 - loglik: -4.1903e+02 - logprior: -1.5278e+00
Epoch 7/10
10/10 - 1s - loss: 415.2196 - loglik: -4.1436e+02 - logprior: -8.5484e-01
Epoch 8/10
10/10 - 1s - loss: 409.2462 - loglik: -4.0854e+02 - logprior: -6.9932e-01
Epoch 9/10
10/10 - 1s - loss: 404.6760 - loglik: -4.0402e+02 - logprior: -6.5350e-01
Epoch 10/10
10/10 - 1s - loss: 397.3685 - loglik: -3.9681e+02 - logprior: -5.5698e-01
Fitted a model with MAP estimate = -391.8401
expansions: [(0, 3), (6, 2), (22, 1), (25, 1), (27, 2), (45, 2), (49, 1), (59, 1), (65, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 82 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 517.9086 - loglik: -4.6799e+02 - logprior: -4.9923e+01
Epoch 2/2
10/10 - 1s - loss: 458.7077 - loglik: -4.4296e+02 - logprior: -1.5743e+01
Fitted a model with MAP estimate = -443.4612
expansions: [(0, 5), (35, 3)]
discards: [ 2  3  4  5  6 11 28 29 32 36 37 38]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 473.8739 - loglik: -4.3201e+02 - logprior: -4.1862e+01
Epoch 2/2
10/10 - 1s - loss: 437.3165 - loglik: -4.2447e+02 - logprior: -1.2850e+01
Fitted a model with MAP estimate = -430.0625
expansions: [(30, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 1183 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 465.0080 - loglik: -4.2394e+02 - logprior: -4.1065e+01
Epoch 2/10
10/10 - 1s - loss: 437.6647 - loglik: -4.2152e+02 - logprior: -1.6149e+01
Epoch 3/10
10/10 - 1s - loss: 428.6881 - loglik: -4.1949e+02 - logprior: -9.1934e+00
Epoch 4/10
10/10 - 1s - loss: 420.9043 - loglik: -4.1773e+02 - logprior: -3.1725e+00
Epoch 5/10
10/10 - 1s - loss: 416.4451 - loglik: -4.1562e+02 - logprior: -8.2325e-01
Epoch 6/10
10/10 - 1s - loss: 413.5593 - loglik: -4.1359e+02 - logprior: 0.0343
Epoch 7/10
10/10 - 1s - loss: 409.8818 - loglik: -4.1048e+02 - logprior: 0.6011
Epoch 8/10
10/10 - 1s - loss: 405.3693 - loglik: -4.0622e+02 - logprior: 0.8513
Epoch 9/10
10/10 - 1s - loss: 400.2376 - loglik: -4.0118e+02 - logprior: 0.9496
Epoch 10/10
10/10 - 1s - loss: 393.2811 - loglik: -3.9429e+02 - logprior: 1.0177
Fitted a model with MAP estimate = -386.1429
Time for alignment: 36.9754
Computed alignments with likelihoods: ['-382.1835', '-388.4338', '-385.6846', '-389.6472', '-386.1429']
Best model has likelihood: -382.1835  (prior= 1.1928 )
time for generating output: 0.1492
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/DEATH.projection.fasta
SP score = 0.3472584856396867
Training of 5 independent models on file aat.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea5e34ac10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cd813700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efe6c8e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 1995.6935 - loglik: -1.9945e+03 - logprior: -1.2424e+00
Epoch 2/10
43/43 - 27s - loss: 1883.9532 - loglik: -1.8823e+03 - logprior: -1.6433e+00
Epoch 3/10
43/43 - 27s - loss: 1872.0516 - loglik: -1.8704e+03 - logprior: -1.6767e+00
Epoch 4/10
43/43 - 27s - loss: 1865.6708 - loglik: -1.8641e+03 - logprior: -1.6125e+00
Epoch 5/10
43/43 - 27s - loss: 1860.4709 - loglik: -1.8589e+03 - logprior: -1.5733e+00
Epoch 6/10
43/43 - 27s - loss: 1852.3865 - loglik: -1.8508e+03 - logprior: -1.5961e+00
Epoch 7/10
43/43 - 27s - loss: 1841.2314 - loglik: -1.8396e+03 - logprior: -1.5649e+00
Epoch 8/10
43/43 - 27s - loss: 1810.5027 - loglik: -1.8089e+03 - logprior: -1.5761e+00
Epoch 9/10
43/43 - 27s - loss: 1679.9744 - loglik: -1.6779e+03 - logprior: -2.0156e+00
Epoch 10/10
43/43 - 27s - loss: 1475.7509 - loglik: -1.4714e+03 - logprior: -4.2952e+00
Fitted a model with MAP estimate = -1729.2603
expansions: []
discards: [ 30  46  66  67  80  81  82 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 159 160 161 162 163
 164 168 169 170 171 172 173 174 229 230 267]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 26s - loss: 2124.3430 - loglik: -2.1231e+03 - logprior: -1.2287e+00
Epoch 2/2
43/43 - 21s - loss: 2096.7854 - loglik: -2.0965e+03 - logprior: -2.8760e-01
Fitted a model with MAP estimate = -1950.8272
expansions: [(0, 176), (232, 156)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231]
Re-initialized the encoder parameters.
Fitting a model of length 332 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 39s - loss: 1952.2601 - loglik: -1.9511e+03 - logprior: -1.1505e+00
Epoch 2/2
43/43 - 36s - loss: 1853.4591 - loglik: -1.8528e+03 - logprior: -6.6264e-01
Fitted a model with MAP estimate = -1774.5820
expansions: [(18, 1), (30, 1), (48, 1), (113, 1), (154, 1), (187, 2), (225, 1), (249, 1), (250, 2), (252, 1), (269, 1), (270, 1), (275, 2), (288, 2), (296, 1), (299, 1), (308, 1), (312, 1), (313, 1), (323, 1), (324, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 356 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 59s - loss: 1753.2468 - loglik: -1.7522e+03 - logprior: -1.0061e+00
Epoch 2/10
61/61 - 55s - loss: 1745.7892 - loglik: -1.7452e+03 - logprior: -5.9699e-01
Epoch 3/10
61/61 - 55s - loss: 1742.3835 - loglik: -1.7418e+03 - logprior: -5.8168e-01
Epoch 4/10
61/61 - 56s - loss: 1736.3383 - loglik: -1.7357e+03 - logprior: -5.8010e-01
Epoch 5/10
61/61 - 55s - loss: 1732.5906 - loglik: -1.7320e+03 - logprior: -5.6866e-01
Epoch 6/10
61/61 - 55s - loss: 1723.1750 - loglik: -1.7226e+03 - logprior: -5.5975e-01
Epoch 7/10
61/61 - 55s - loss: 1712.0448 - loglik: -1.7115e+03 - logprior: -5.3480e-01
Epoch 8/10
61/61 - 55s - loss: 1691.8785 - loglik: -1.6913e+03 - logprior: -5.5802e-01
Epoch 9/10
61/61 - 55s - loss: 1537.2119 - loglik: -1.5364e+03 - logprior: -7.4040e-01
Epoch 10/10
61/61 - 55s - loss: 1423.6948 - loglik: -1.4224e+03 - logprior: -1.1819e+00
Fitted a model with MAP estimate = -1346.0608
Time for alignment: 1185.6211
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 1994.2157 - loglik: -1.9930e+03 - logprior: -1.2442e+00
Epoch 2/10
43/43 - 27s - loss: 1882.2032 - loglik: -1.8807e+03 - logprior: -1.5247e+00
Epoch 3/10
43/43 - 27s - loss: 1869.4263 - loglik: -1.8679e+03 - logprior: -1.5627e+00
Epoch 4/10
43/43 - 27s - loss: 1863.9609 - loglik: -1.8624e+03 - logprior: -1.5176e+00
Epoch 5/10
43/43 - 27s - loss: 1858.2780 - loglik: -1.8568e+03 - logprior: -1.4969e+00
Epoch 6/10
43/43 - 27s - loss: 1852.2251 - loglik: -1.8507e+03 - logprior: -1.4898e+00
Epoch 7/10
43/43 - 27s - loss: 1838.7579 - loglik: -1.8372e+03 - logprior: -1.4986e+00
Epoch 8/10
43/43 - 27s - loss: 1810.6028 - loglik: -1.8091e+03 - logprior: -1.5156e+00
Epoch 9/10
43/43 - 27s - loss: 1696.4246 - loglik: -1.6945e+03 - logprior: -1.8550e+00
Epoch 10/10
43/43 - 27s - loss: 1477.4974 - loglik: -1.4728e+03 - logprior: -4.6351e+00
Fitted a model with MAP estimate = -1757.8030
expansions: [(0, 1)]
discards: [  0  66  67 113 114 115 116 117 118 119 120 121 122 123 136 137 138 139
 140 161 162 163 164 165 166 167 168 169 170 171 172 173 229 230 267]
Re-initialized the encoder parameters.
Fitting a model of length 245 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 26s - loss: 2121.7366 - loglik: -2.1205e+03 - logprior: -1.2781e+00
Epoch 2/2
43/43 - 23s - loss: 2100.5173 - loglik: -2.1010e+03 - logprior: 0.4419
Fitted a model with MAP estimate = -1950.7811
expansions: [(0, 215)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215
 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 22s - loss: 2007.1500 - loglik: -2.0052e+03 - logprior: -1.9492e+00
Epoch 2/2
43/43 - 19s - loss: 1931.3905 - loglik: -1.9294e+03 - logprior: -1.9990e+00
Fitted a model with MAP estimate = -1837.9020
expansions: [(24, 1), (116, 1)]
discards: [144 145 152 159 160 161 162 163 164 165 166 167 168 169 170 171 179 181
 182 188 189 190 191 192 193 197 198 199 200 214 215 216 217 218]
Re-initialized the encoder parameters.
Fitting a model of length 187 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 25s - loss: 1847.0204 - loglik: -1.8454e+03 - logprior: -1.6233e+00
Epoch 2/10
61/61 - 22s - loss: 1837.6495 - loglik: -1.8360e+03 - logprior: -1.6459e+00
Epoch 3/10
61/61 - 22s - loss: 1834.6968 - loglik: -1.8331e+03 - logprior: -1.6309e+00
Epoch 4/10
61/61 - 22s - loss: 1828.4032 - loglik: -1.8268e+03 - logprior: -1.6288e+00
Epoch 5/10
61/61 - 22s - loss: 1823.5204 - loglik: -1.8219e+03 - logprior: -1.6426e+00
Epoch 6/10
61/61 - 22s - loss: 1812.0740 - loglik: -1.8104e+03 - logprior: -1.6479e+00
Epoch 7/10
61/61 - 22s - loss: 1789.6615 - loglik: -1.7880e+03 - logprior: -1.6540e+00
Epoch 8/10
61/61 - 22s - loss: 1750.6195 - loglik: -1.7487e+03 - logprior: -1.8288e+00
Epoch 9/10
61/61 - 22s - loss: 1530.0793 - loglik: -1.5259e+03 - logprior: -4.0547e+00
Epoch 10/10
61/61 - 22s - loss: 1415.8064 - loglik: -1.4116e+03 - logprior: -4.0475e+00
Fitted a model with MAP estimate = -1364.0032
Time for alignment: 772.5589
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 1995.1831 - loglik: -1.9939e+03 - logprior: -1.2532e+00
Epoch 2/10
43/43 - 27s - loss: 1881.0658 - loglik: -1.8794e+03 - logprior: -1.6787e+00
Epoch 3/10
43/43 - 27s - loss: 1870.9913 - loglik: -1.8693e+03 - logprior: -1.7270e+00
Epoch 4/10
43/43 - 27s - loss: 1863.0967 - loglik: -1.8614e+03 - logprior: -1.7053e+00
Epoch 5/10
43/43 - 27s - loss: 1857.2329 - loglik: -1.8556e+03 - logprior: -1.6638e+00
Epoch 6/10
43/43 - 27s - loss: 1852.1045 - loglik: -1.8504e+03 - logprior: -1.6423e+00
Epoch 7/10
43/43 - 27s - loss: 1839.2627 - loglik: -1.8376e+03 - logprior: -1.5990e+00
Epoch 8/10
43/43 - 27s - loss: 1812.7146 - loglik: -1.8110e+03 - logprior: -1.6736e+00
Epoch 9/10
43/43 - 27s - loss: 1706.8585 - loglik: -1.7049e+03 - logprior: -1.8725e+00
Epoch 10/10
43/43 - 27s - loss: 1480.6602 - loglik: -1.4758e+03 - logprior: -4.8085e+00
Fitted a model with MAP estimate = -1752.6842
expansions: []
discards: [ 30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45 115 116
 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134
 135 136 137 138 162 163 164 165 166 167 168 169 170 171 172 173 174 229
 230 240 267]
Re-initialized the encoder parameters.
Fitting a model of length 222 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 23s - loss: 2122.6824 - loglik: -2.1210e+03 - logprior: -1.6690e+00
Epoch 2/2
43/43 - 19s - loss: 2094.9021 - loglik: -2.0943e+03 - logprior: -5.5770e-01
Fitted a model with MAP estimate = -1944.7682
expansions: [(0, 145), (95, 2), (96, 2), (222, 179)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139
 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157
 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175
 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193
 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213 214 215 216 217 218 219 220 221]
Re-initialized the encoder parameters.
Fitting a model of length 360 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 43s - loss: 1945.4869 - loglik: -1.9441e+03 - logprior: -1.3477e+00
Epoch 2/2
43/43 - 41s - loss: 1854.9906 - loglik: -1.8543e+03 - logprior: -6.5682e-01
Fitted a model with MAP estimate = -1778.0232
expansions: [(320, 1), (338, 1), (348, 2)]
discards: [  0  34  35  36  37  68  93  94 120 121 153 231 250]
Re-initialized the encoder parameters.
Fitting a model of length 351 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 57s - loss: 1760.6283 - loglik: -1.7597e+03 - logprior: -9.5945e-01
Epoch 2/10
61/61 - 54s - loss: 1751.0859 - loglik: -1.7505e+03 - logprior: -5.9559e-01
Epoch 3/10
61/61 - 54s - loss: 1745.5483 - loglik: -1.7450e+03 - logprior: -5.4331e-01
Epoch 4/10
61/61 - 54s - loss: 1739.3473 - loglik: -1.7388e+03 - logprior: -5.1182e-01
Epoch 5/10
61/61 - 53s - loss: 1735.7418 - loglik: -1.7352e+03 - logprior: -4.9035e-01
Epoch 6/10
61/61 - 54s - loss: 1723.7191 - loglik: -1.7232e+03 - logprior: -4.6125e-01
Epoch 7/10
61/61 - 54s - loss: 1719.6993 - loglik: -1.7192e+03 - logprior: -4.5072e-01
Epoch 8/10
61/61 - 54s - loss: 1692.8066 - loglik: -1.6923e+03 - logprior: -4.5178e-01
Epoch 9/10
61/61 - 54s - loss: 1579.0831 - loglik: -1.5784e+03 - logprior: -5.8986e-01
Epoch 10/10
61/61 - 54s - loss: 1446.2944 - loglik: -1.4451e+03 - logprior: -1.0566e+00
Fitted a model with MAP estimate = -1352.6556
Time for alignment: 1183.9345
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 1994.9860 - loglik: -1.9937e+03 - logprior: -1.2760e+00
Epoch 2/10
43/43 - 27s - loss: 1881.0625 - loglik: -1.8794e+03 - logprior: -1.7086e+00
Epoch 3/10
43/43 - 27s - loss: 1870.3334 - loglik: -1.8686e+03 - logprior: -1.7068e+00
Epoch 4/10
43/43 - 27s - loss: 1866.8843 - loglik: -1.8652e+03 - logprior: -1.6579e+00
Epoch 5/10
43/43 - 27s - loss: 1858.7396 - loglik: -1.8571e+03 - logprior: -1.6412e+00
Epoch 6/10
43/43 - 27s - loss: 1853.7329 - loglik: -1.8521e+03 - logprior: -1.6366e+00
Epoch 7/10
43/43 - 27s - loss: 1839.7516 - loglik: -1.8381e+03 - logprior: -1.6142e+00
Epoch 8/10
43/43 - 27s - loss: 1809.9969 - loglik: -1.8083e+03 - logprior: -1.6455e+00
Epoch 9/10
43/43 - 27s - loss: 1689.4669 - loglik: -1.6875e+03 - logprior: -1.9009e+00
Epoch 10/10
43/43 - 27s - loss: 1475.3965 - loglik: -1.4711e+03 - logprior: -4.2387e+00
Fitted a model with MAP estimate = -1727.9065
expansions: []
discards: [ 30  45  66  67 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 137 138 139 140 141 142 160 161 162 163 164 168 169 170 171 172
 173 174 229 230 267]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 24s - loss: 2125.0967 - loglik: -2.1238e+03 - logprior: -1.3189e+00
Epoch 2/2
43/43 - 21s - loss: 2093.6184 - loglik: -2.0933e+03 - logprior: -3.1954e-01
Fitted a model with MAP estimate = -1946.5507
expansions: [(0, 170), (117, 8), (119, 9), (130, 57), (132, 29)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 137 138 139 140 141 142 143 144 145 146 147
 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165
 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183
 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201
 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219
 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237]
Re-initialized the encoder parameters.
Fitting a model of length 295 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 33s - loss: 1991.8749 - loglik: -1.9906e+03 - logprior: -1.3028e+00
Epoch 2/2
43/43 - 29s - loss: 1911.2499 - loglik: -1.9101e+03 - logprior: -1.1469e+00
Fitted a model with MAP estimate = -1822.6085
expansions: [(171, 1), (176, 1), (177, 1), (181, 3), (183, 2), (184, 2), (185, 1), (192, 6), (193, 11), (194, 34), (195, 1), (196, 1), (207, 15), (229, 1), (234, 1), (268, 1), (271, 1), (273, 2), (274, 1)]
discards: [  0  59  60  62  64  68  70  71  73  75  77  79  84  86  87  88  89  98
 126 127 128 134 135 136]
Re-initialized the encoder parameters.
Fitting a model of length 357 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 58s - loss: 1771.4933 - loglik: -1.7704e+03 - logprior: -1.0525e+00
Epoch 2/10
61/61 - 55s - loss: 1751.8772 - loglik: -1.7511e+03 - logprior: -7.3782e-01
Epoch 3/10
61/61 - 55s - loss: 1749.5686 - loglik: -1.7489e+03 - logprior: -7.0890e-01
Epoch 4/10
61/61 - 55s - loss: 1739.9496 - loglik: -1.7392e+03 - logprior: -6.9544e-01
Epoch 5/10
61/61 - 56s - loss: 1739.4297 - loglik: -1.7387e+03 - logprior: -6.9255e-01
Epoch 6/10
61/61 - 56s - loss: 1726.6173 - loglik: -1.7259e+03 - logprior: -6.6250e-01
Epoch 7/10
61/61 - 55s - loss: 1716.9910 - loglik: -1.7163e+03 - logprior: -6.4553e-01
Epoch 8/10
61/61 - 55s - loss: 1688.8698 - loglik: -1.6881e+03 - logprior: -6.6884e-01
Epoch 9/10
61/61 - 55s - loss: 1514.5692 - loglik: -1.5135e+03 - logprior: -9.9775e-01
Epoch 10/10
61/61 - 56s - loss: 1417.4347 - loglik: -1.4159e+03 - logprior: -1.3572e+00
Fitted a model with MAP estimate = -1346.6801
Time for alignment: 1162.9639
Fitting a model of length 279 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
43/43 - 30s - loss: 1995.8429 - loglik: -1.9946e+03 - logprior: -1.2455e+00
Epoch 2/10
43/43 - 27s - loss: 1883.8118 - loglik: -1.8822e+03 - logprior: -1.6441e+00
Epoch 3/10
43/43 - 27s - loss: 1872.8966 - loglik: -1.8712e+03 - logprior: -1.6687e+00
Epoch 4/10
43/43 - 27s - loss: 1866.5161 - loglik: -1.8648e+03 - logprior: -1.6698e+00
Epoch 5/10
43/43 - 27s - loss: 1861.1589 - loglik: -1.8595e+03 - logprior: -1.6608e+00
Epoch 6/10
43/43 - 27s - loss: 1854.1053 - loglik: -1.8524e+03 - logprior: -1.6404e+00
Epoch 7/10
43/43 - 27s - loss: 1843.2128 - loglik: -1.8416e+03 - logprior: -1.6208e+00
Epoch 8/10
43/43 - 27s - loss: 1816.6499 - loglik: -1.8149e+03 - logprior: -1.6843e+00
Epoch 9/10
43/43 - 27s - loss: 1711.1641 - loglik: -1.7091e+03 - logprior: -1.9797e+00
Epoch 10/10
43/43 - 27s - loss: 1482.3395 - loglik: -1.4765e+03 - logprior: -5.7644e+00
Fitted a model with MAP estimate = -1739.5716
expansions: [(0, 1)]
discards: [  7   8   9  10  11  30  57  58  59  60  61  62  63  64  65  66  67  94
  98  99 100 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126
 127 128 129 130 131 132 133 134 135 136 137 138 162 163 164 165 166 167
 168 169 170 171 172 173 174 175 176 177 178 179 223 224 225 229 230 262
 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 20s - loss: 2119.7952 - loglik: -2.1185e+03 - logprior: -1.2718e+00
Epoch 2/2
43/43 - 16s - loss: 2099.2695 - loglik: -2.0992e+03 - logprior: -9.1301e-02
Fitted a model with MAP estimate = -1952.4385
expansions: [(0, 165), (192, 176)]
discards: [ 26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43
  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79
  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97
  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115
 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133
 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169
 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187
 188 189 190 191]
Re-initialized the encoder parameters.
Fitting a model of length 367 on 12550 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
43/43 - 45s - loss: 1994.6595 - loglik: -1.9926e+03 - logprior: -2.0108e+00
Epoch 2/2
43/43 - 42s - loss: 1898.1682 - loglik: -1.8959e+03 - logprior: -2.2842e+00
Fitted a model with MAP estimate = -1807.9247
expansions: [(121, 1), (166, 3), (249, 1), (259, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 216 217 272 273
 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309
 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327
 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345
 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363
 364 365 366]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 25100 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
61/61 - 22s - loss: 1866.7529 - loglik: -1.8657e+03 - logprior: -1.0091e+00
Epoch 2/10
61/61 - 20s - loss: 1852.9926 - loglik: -1.8526e+03 - logprior: -4.4010e-01
Epoch 3/10
61/61 - 20s - loss: 1850.9828 - loglik: -1.8506e+03 - logprior: -3.3492e-01
Epoch 4/10
61/61 - 20s - loss: 1842.6990 - loglik: -1.8424e+03 - logprior: -2.9374e-01
Epoch 5/10
61/61 - 20s - loss: 1847.0862 - loglik: -1.8468e+03 - logprior: -2.6774e-01
Fitted a model with MAP estimate = -1837.6278
Time for alignment: 721.1220
Computed alignments with likelihoods: ['-1346.0608', '-1364.0032', '-1352.6556', '-1346.6801', '-1739.5716']
Best model has likelihood: -1346.0608  (prior= -1.2988 )
time for generating output: 0.3862
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aat.projection.fasta
SP score = 0.03142991422864528
Training of 5 independent models on file ins.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe938384280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cd921250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cd921c40>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 441.7678 - loglik: -3.8510e+02 - logprior: -5.6670e+01
Epoch 2/10
10/10 - 1s - loss: 373.9024 - loglik: -3.5827e+02 - logprior: -1.5635e+01
Epoch 3/10
10/10 - 1s - loss: 351.3866 - loglik: -3.4378e+02 - logprior: -7.6039e+00
Epoch 4/10
10/10 - 1s - loss: 340.3953 - loglik: -3.3598e+02 - logprior: -4.4173e+00
Epoch 5/10
10/10 - 1s - loss: 336.7957 - loglik: -3.3396e+02 - logprior: -2.8359e+00
Epoch 6/10
10/10 - 1s - loss: 335.2606 - loglik: -3.3335e+02 - logprior: -1.9055e+00
Epoch 7/10
10/10 - 1s - loss: 332.0478 - loglik: -3.3059e+02 - logprior: -1.4598e+00
Epoch 8/10
10/10 - 1s - loss: 331.1923 - loglik: -3.2994e+02 - logprior: -1.2542e+00
Epoch 9/10
10/10 - 1s - loss: 329.3003 - loglik: -3.2815e+02 - logprior: -1.1502e+00
Epoch 10/10
10/10 - 1s - loss: 329.7585 - loglik: -3.2872e+02 - logprior: -1.0393e+00
Fitted a model with MAP estimate = -329.4730
expansions: [(13, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 404.9660 - loglik: -3.4030e+02 - logprior: -6.4666e+01
Epoch 2/2
10/10 - 1s - loss: 359.5201 - loglik: -3.3237e+02 - logprior: -2.7150e+01
Fitted a model with MAP estimate = -353.8324
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 386.6360 - loglik: -3.3313e+02 - logprior: -5.3505e+01
Epoch 2/2
10/10 - 1s - loss: 344.4671 - loglik: -3.2978e+02 - logprior: -1.4684e+01
Fitted a model with MAP estimate = -339.3249
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 397.7949 - loglik: -3.3457e+02 - logprior: -6.3227e+01
Epoch 2/10
10/10 - 1s - loss: 357.4102 - loglik: -3.3195e+02 - logprior: -2.5459e+01
Epoch 3/10
10/10 - 1s - loss: 344.6472 - loglik: -3.3184e+02 - logprior: -1.2804e+01
Epoch 4/10
10/10 - 1s - loss: 335.7883 - loglik: -3.3105e+02 - logprior: -4.7414e+00
Epoch 5/10
10/10 - 1s - loss: 332.5389 - loglik: -3.3027e+02 - logprior: -2.2731e+00
Epoch 6/10
10/10 - 1s - loss: 330.5168 - loglik: -3.2916e+02 - logprior: -1.3584e+00
Epoch 7/10
10/10 - 1s - loss: 328.6380 - loglik: -3.2786e+02 - logprior: -7.7986e-01
Epoch 8/10
10/10 - 1s - loss: 327.6594 - loglik: -3.2722e+02 - logprior: -4.3517e-01
Epoch 9/10
10/10 - 1s - loss: 327.2938 - loglik: -3.2706e+02 - logprior: -2.3248e-01
Epoch 10/10
10/10 - 1s - loss: 327.6144 - loglik: -3.2756e+02 - logprior: -5.1405e-02
Fitted a model with MAP estimate = -326.5281
Time for alignment: 43.5788
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 441.2957 - loglik: -3.8462e+02 - logprior: -5.6671e+01
Epoch 2/10
10/10 - 1s - loss: 374.7485 - loglik: -3.5912e+02 - logprior: -1.5626e+01
Epoch 3/10
10/10 - 1s - loss: 350.1517 - loglik: -3.4255e+02 - logprior: -7.5998e+00
Epoch 4/10
10/10 - 1s - loss: 340.5783 - loglik: -3.3615e+02 - logprior: -4.4310e+00
Epoch 5/10
10/10 - 1s - loss: 336.4955 - loglik: -3.3364e+02 - logprior: -2.8528e+00
Epoch 6/10
10/10 - 1s - loss: 335.4727 - loglik: -3.3355e+02 - logprior: -1.9184e+00
Epoch 7/10
10/10 - 1s - loss: 333.6025 - loglik: -3.3221e+02 - logprior: -1.3911e+00
Epoch 8/10
10/10 - 1s - loss: 332.2021 - loglik: -3.3104e+02 - logprior: -1.1601e+00
Epoch 9/10
10/10 - 1s - loss: 330.6292 - loglik: -3.2961e+02 - logprior: -1.0203e+00
Epoch 10/10
10/10 - 1s - loss: 331.5665 - loglik: -3.3062e+02 - logprior: -9.4580e-01
Fitted a model with MAP estimate = -330.8809
expansions: [(11, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 406.3886 - loglik: -3.4225e+02 - logprior: -6.4142e+01
Epoch 2/2
10/10 - 1s - loss: 361.2573 - loglik: -3.3428e+02 - logprior: -2.6973e+01
Fitted a model with MAP estimate = -355.0034
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 49 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 386.6711 - loglik: -3.3395e+02 - logprior: -5.2722e+01
Epoch 2/2
10/10 - 1s - loss: 347.2086 - loglik: -3.3268e+02 - logprior: -1.4532e+01
Fitted a model with MAP estimate = -340.7240
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 397.7254 - loglik: -3.3459e+02 - logprior: -6.3134e+01
Epoch 2/10
10/10 - 1s - loss: 359.0815 - loglik: -3.3428e+02 - logprior: -2.4804e+01
Epoch 3/10
10/10 - 1s - loss: 343.9769 - loglik: -3.3250e+02 - logprior: -1.1482e+01
Epoch 4/10
10/10 - 1s - loss: 337.9935 - loglik: -3.3358e+02 - logprior: -4.4124e+00
Epoch 5/10
10/10 - 1s - loss: 332.6593 - loglik: -3.3041e+02 - logprior: -2.2519e+00
Epoch 6/10
10/10 - 1s - loss: 332.2977 - loglik: -3.3091e+02 - logprior: -1.3904e+00
Epoch 7/10
10/10 - 1s - loss: 330.8903 - loglik: -3.3003e+02 - logprior: -8.5764e-01
Epoch 8/10
10/10 - 1s - loss: 329.1309 - loglik: -3.2862e+02 - logprior: -5.1316e-01
Epoch 9/10
10/10 - 1s - loss: 329.0659 - loglik: -3.2875e+02 - logprior: -3.1912e-01
Epoch 10/10
10/10 - 1s - loss: 326.7315 - loglik: -3.2659e+02 - logprior: -1.3475e-01
Fitted a model with MAP estimate = -327.7417
Time for alignment: 43.8468
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 440.5559 - loglik: -3.8388e+02 - logprior: -5.6672e+01
Epoch 2/10
10/10 - 1s - loss: 375.0087 - loglik: -3.5938e+02 - logprior: -1.5627e+01
Epoch 3/10
10/10 - 1s - loss: 349.7579 - loglik: -3.4215e+02 - logprior: -7.6056e+00
Epoch 4/10
10/10 - 1s - loss: 342.2644 - loglik: -3.3785e+02 - logprior: -4.4151e+00
Epoch 5/10
10/10 - 1s - loss: 338.2981 - loglik: -3.3546e+02 - logprior: -2.8346e+00
Epoch 6/10
10/10 - 1s - loss: 333.4798 - loglik: -3.3157e+02 - logprior: -1.9119e+00
Epoch 7/10
10/10 - 1s - loss: 333.7866 - loglik: -3.3237e+02 - logprior: -1.4158e+00
Fitted a model with MAP estimate = -332.5123
expansions: [(10, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 402.7564 - loglik: -3.3881e+02 - logprior: -6.3944e+01
Epoch 2/2
10/10 - 1s - loss: 359.2736 - loglik: -3.3235e+02 - logprior: -2.6921e+01
Fitted a model with MAP estimate = -354.1405
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 385.9448 - loglik: -3.3356e+02 - logprior: -5.2388e+01
Epoch 2/2
10/10 - 1s - loss: 348.0610 - loglik: -3.3338e+02 - logprior: -1.4678e+01
Fitted a model with MAP estimate = -340.9598
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 399.0103 - loglik: -3.3562e+02 - logprior: -6.3386e+01
Epoch 2/10
10/10 - 1s - loss: 358.5115 - loglik: -3.3264e+02 - logprior: -2.5868e+01
Epoch 3/10
10/10 - 1s - loss: 347.3211 - loglik: -3.3330e+02 - logprior: -1.4022e+01
Epoch 4/10
10/10 - 1s - loss: 336.9915 - loglik: -3.3172e+02 - logprior: -5.2734e+00
Epoch 5/10
10/10 - 1s - loss: 335.3120 - loglik: -3.3291e+02 - logprior: -2.4043e+00
Epoch 6/10
10/10 - 1s - loss: 331.0685 - loglik: -3.2964e+02 - logprior: -1.4300e+00
Epoch 7/10
10/10 - 1s - loss: 331.2039 - loglik: -3.3038e+02 - logprior: -8.2579e-01
Fitted a model with MAP estimate = -329.8489
Time for alignment: 35.0806
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 441.3225 - loglik: -3.8465e+02 - logprior: -5.6674e+01
Epoch 2/10
10/10 - 1s - loss: 374.0045 - loglik: -3.5836e+02 - logprior: -1.5642e+01
Epoch 3/10
10/10 - 1s - loss: 349.5797 - loglik: -3.4198e+02 - logprior: -7.6024e+00
Epoch 4/10
10/10 - 1s - loss: 342.2066 - loglik: -3.3778e+02 - logprior: -4.4223e+00
Epoch 5/10
10/10 - 1s - loss: 337.6247 - loglik: -3.3478e+02 - logprior: -2.8407e+00
Epoch 6/10
10/10 - 1s - loss: 334.5666 - loglik: -3.3268e+02 - logprior: -1.8899e+00
Epoch 7/10
10/10 - 1s - loss: 332.5936 - loglik: -3.3121e+02 - logprior: -1.3820e+00
Epoch 8/10
10/10 - 1s - loss: 332.9120 - loglik: -3.3177e+02 - logprior: -1.1454e+00
Fitted a model with MAP estimate = -331.7645
expansions: [(11, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 403.0034 - loglik: -3.3899e+02 - logprior: -6.4017e+01
Epoch 2/2
10/10 - 1s - loss: 360.9393 - loglik: -3.3400e+02 - logprior: -2.6942e+01
Fitted a model with MAP estimate = -354.8266
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 388.6263 - loglik: -3.3605e+02 - logprior: -5.2581e+01
Epoch 2/2
10/10 - 1s - loss: 346.1503 - loglik: -3.3144e+02 - logprior: -1.4709e+01
Fitted a model with MAP estimate = -341.2225
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 399.1953 - loglik: -3.3595e+02 - logprior: -6.3249e+01
Epoch 2/10
10/10 - 1s - loss: 358.8087 - loglik: -3.3347e+02 - logprior: -2.5335e+01
Epoch 3/10
10/10 - 1s - loss: 345.3182 - loglik: -3.3281e+02 - logprior: -1.2511e+01
Epoch 4/10
10/10 - 1s - loss: 337.3124 - loglik: -3.3261e+02 - logprior: -4.7066e+00
Epoch 5/10
10/10 - 1s - loss: 333.8561 - loglik: -3.3153e+02 - logprior: -2.3240e+00
Epoch 6/10
10/10 - 1s - loss: 332.2079 - loglik: -3.3080e+02 - logprior: -1.4044e+00
Epoch 7/10
10/10 - 1s - loss: 330.9304 - loglik: -3.3011e+02 - logprior: -8.1947e-01
Epoch 8/10
10/10 - 1s - loss: 329.3105 - loglik: -3.2885e+02 - logprior: -4.6214e-01
Epoch 9/10
10/10 - 1s - loss: 328.1103 - loglik: -3.2782e+02 - logprior: -2.9297e-01
Epoch 10/10
10/10 - 1s - loss: 328.4143 - loglik: -3.2826e+02 - logprior: -1.4886e-01
Fitted a model with MAP estimate = -327.8061
Time for alignment: 38.7234
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 441.4893 - loglik: -3.8482e+02 - logprior: -5.6672e+01
Epoch 2/10
10/10 - 1s - loss: 375.2682 - loglik: -3.5963e+02 - logprior: -1.5643e+01
Epoch 3/10
10/10 - 1s - loss: 349.8599 - loglik: -3.4226e+02 - logprior: -7.5968e+00
Epoch 4/10
10/10 - 1s - loss: 341.8847 - loglik: -3.3745e+02 - logprior: -4.4313e+00
Epoch 5/10
10/10 - 1s - loss: 337.6677 - loglik: -3.3483e+02 - logprior: -2.8373e+00
Epoch 6/10
10/10 - 1s - loss: 334.8065 - loglik: -3.3291e+02 - logprior: -1.9004e+00
Epoch 7/10
10/10 - 1s - loss: 332.7275 - loglik: -3.3133e+02 - logprior: -1.3979e+00
Epoch 8/10
10/10 - 1s - loss: 332.2884 - loglik: -3.3112e+02 - logprior: -1.1656e+00
Epoch 9/10
10/10 - 1s - loss: 331.7143 - loglik: -3.3067e+02 - logprior: -1.0376e+00
Epoch 10/10
10/10 - 1s - loss: 330.8217 - loglik: -3.2987e+02 - logprior: -9.4864e-01
Fitted a model with MAP estimate = -330.8658
expansions: [(11, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 48 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 406.1240 - loglik: -3.4189e+02 - logprior: -6.4232e+01
Epoch 2/2
10/10 - 1s - loss: 361.3805 - loglik: -3.3438e+02 - logprior: -2.7001e+01
Fitted a model with MAP estimate = -354.9836
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 50 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 388.1281 - loglik: -3.3506e+02 - logprior: -5.3066e+01
Epoch 2/2
10/10 - 1s - loss: 346.8958 - loglik: -3.3211e+02 - logprior: -1.4788e+01
Fitted a model with MAP estimate = -341.0602
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 47 on 793 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 399.5168 - loglik: -3.3638e+02 - logprior: -6.3142e+01
Epoch 2/10
10/10 - 1s - loss: 357.4315 - loglik: -3.3261e+02 - logprior: -2.4822e+01
Epoch 3/10
10/10 - 1s - loss: 345.5428 - loglik: -3.3403e+02 - logprior: -1.1513e+01
Epoch 4/10
10/10 - 1s - loss: 335.7121 - loglik: -3.3131e+02 - logprior: -4.4065e+00
Epoch 5/10
10/10 - 1s - loss: 335.2447 - loglik: -3.3300e+02 - logprior: -2.2441e+00
Epoch 6/10
10/10 - 1s - loss: 331.9241 - loglik: -3.3054e+02 - logprior: -1.3791e+00
Epoch 7/10
10/10 - 1s - loss: 329.6866 - loglik: -3.2883e+02 - logprior: -8.5106e-01
Epoch 8/10
10/10 - 1s - loss: 329.6156 - loglik: -3.2910e+02 - logprior: -5.0936e-01
Epoch 9/10
10/10 - 1s - loss: 327.9027 - loglik: -3.2759e+02 - logprior: -3.0969e-01
Epoch 10/10
10/10 - 1s - loss: 328.1915 - loglik: -3.2808e+02 - logprior: -1.0950e-01
Fitted a model with MAP estimate = -327.7606
Time for alignment: 40.8248
Computed alignments with likelihoods: ['-326.5281', '-327.7417', '-329.8489', '-327.8061', '-327.7606']
Best model has likelihood: -326.5281  (prior= 0.0372 )
time for generating output: 0.1013
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ins.projection.fasta
SP score = 0.9029126213592233
Training of 5 independent models on file sti.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cd8fce50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9d6188a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea66ab9130>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1015.1769 - loglik: -9.4842e+02 - logprior: -6.6758e+01
Epoch 2/10
10/10 - 2s - loss: 912.5597 - loglik: -8.9923e+02 - logprior: -1.3325e+01
Epoch 3/10
10/10 - 2s - loss: 860.6697 - loglik: -8.5625e+02 - logprior: -4.4199e+00
Epoch 4/10
10/10 - 2s - loss: 829.4056 - loglik: -8.2809e+02 - logprior: -1.3104e+00
Epoch 5/10
10/10 - 2s - loss: 812.7904 - loglik: -8.1307e+02 - logprior: 0.2780
Epoch 6/10
10/10 - 2s - loss: 804.1557 - loglik: -8.0529e+02 - logprior: 1.1386
Epoch 7/10
10/10 - 2s - loss: 800.6715 - loglik: -8.0226e+02 - logprior: 1.5932
Epoch 8/10
10/10 - 2s - loss: 799.3259 - loglik: -8.0113e+02 - logprior: 1.8042
Epoch 9/10
10/10 - 2s - loss: 798.6019 - loglik: -8.0059e+02 - logprior: 1.9884
Epoch 10/10
10/10 - 2s - loss: 797.3374 - loglik: -7.9949e+02 - logprior: 2.1490
Fitted a model with MAP estimate = -797.3130
expansions: [(11, 3), (12, 2), (16, 3), (17, 1), (24, 1), (26, 2), (38, 2), (39, 1), (55, 1), (62, 1), (65, 1), (71, 1), (78, 3), (80, 2), (89, 1), (90, 1), (103, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 883.0672 - loglik: -8.0777e+02 - logprior: -7.5299e+01
Epoch 2/2
10/10 - 3s - loss: 815.1627 - loglik: -7.8787e+02 - logprior: -2.7291e+01
Fitted a model with MAP estimate = -803.6974
expansions: [(0, 2), (65, 1)]
discards: [  0  11  50 163]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 841.4640 - loglik: -7.8319e+02 - logprior: -5.8276e+01
Epoch 2/2
10/10 - 3s - loss: 787.2498 - loglik: -7.7713e+02 - logprior: -1.0123e+01
Fitted a model with MAP estimate = -778.7210
expansions: [(36, 1)]
discards: [  0  24 142]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 852.7863 - loglik: -7.8103e+02 - logprior: -7.1754e+01
Epoch 2/10
10/10 - 3s - loss: 797.0870 - loglik: -7.7851e+02 - logprior: -1.8578e+01
Epoch 3/10
10/10 - 3s - loss: 777.2689 - loglik: -7.7555e+02 - logprior: -1.7188e+00
Epoch 4/10
10/10 - 3s - loss: 770.1342 - loglik: -7.7454e+02 - logprior: 4.4094
Epoch 5/10
10/10 - 3s - loss: 764.9902 - loglik: -7.7193e+02 - logprior: 6.9360
Epoch 6/10
10/10 - 3s - loss: 761.3766 - loglik: -7.6956e+02 - logprior: 8.1826
Epoch 7/10
10/10 - 3s - loss: 759.6559 - loglik: -7.6871e+02 - logprior: 9.0563
Epoch 8/10
10/10 - 3s - loss: 758.6321 - loglik: -7.6847e+02 - logprior: 9.8416
Epoch 9/10
10/10 - 3s - loss: 757.4861 - loglik: -7.6802e+02 - logprior: 10.5387
Epoch 10/10
10/10 - 3s - loss: 756.8843 - loglik: -7.6801e+02 - logprior: 11.1299
Fitted a model with MAP estimate = -756.8549
Time for alignment: 74.9288
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1015.1149 - loglik: -9.4836e+02 - logprior: -6.6756e+01
Epoch 2/10
10/10 - 2s - loss: 911.7228 - loglik: -8.9840e+02 - logprior: -1.3324e+01
Epoch 3/10
10/10 - 2s - loss: 858.9741 - loglik: -8.5476e+02 - logprior: -4.2187e+00
Epoch 4/10
10/10 - 2s - loss: 828.3589 - loglik: -8.2748e+02 - logprior: -8.8043e-01
Epoch 5/10
10/10 - 2s - loss: 813.2541 - loglik: -8.1420e+02 - logprior: 0.9481
Epoch 6/10
10/10 - 2s - loss: 806.7836 - loglik: -8.0862e+02 - logprior: 1.8374
Epoch 7/10
10/10 - 2s - loss: 803.5110 - loglik: -8.0582e+02 - logprior: 2.3098
Epoch 8/10
10/10 - 2s - loss: 802.1219 - loglik: -8.0478e+02 - logprior: 2.6613
Epoch 9/10
10/10 - 2s - loss: 801.4017 - loglik: -8.0427e+02 - logprior: 2.8671
Epoch 10/10
10/10 - 2s - loss: 801.1404 - loglik: -8.0419e+02 - logprior: 3.0549
Fitted a model with MAP estimate = -800.7434
expansions: [(11, 3), (12, 2), (16, 3), (17, 1), (24, 1), (26, 2), (38, 2), (39, 1), (55, 1), (65, 2), (78, 2), (79, 3), (80, 2), (89, 1), (90, 1), (112, 3), (113, 2), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 887.0361 - loglik: -8.1199e+02 - logprior: -7.5041e+01
Epoch 2/2
10/10 - 3s - loss: 820.4087 - loglik: -7.9330e+02 - logprior: -2.7111e+01
Fitted a model with MAP estimate = -808.3882
expansions: [(0, 2)]
discards: [  0  11  50 142 152 162]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 846.6998 - loglik: -7.8849e+02 - logprior: -5.8212e+01
Epoch 2/2
10/10 - 3s - loss: 792.7784 - loglik: -7.8256e+02 - logprior: -1.0216e+01
Fitted a model with MAP estimate = -784.3457
expansions: [(137, 1)]
discards: [ 0 32]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 857.8109 - loglik: -7.8581e+02 - logprior: -7.1998e+01
Epoch 2/10
10/10 - 3s - loss: 800.9642 - loglik: -7.8212e+02 - logprior: -1.8844e+01
Epoch 3/10
10/10 - 3s - loss: 782.7941 - loglik: -7.8081e+02 - logprior: -1.9842e+00
Epoch 4/10
10/10 - 3s - loss: 774.9115 - loglik: -7.7907e+02 - logprior: 4.1594
Epoch 5/10
10/10 - 3s - loss: 769.1747 - loglik: -7.7582e+02 - logprior: 6.6479
Epoch 6/10
10/10 - 3s - loss: 766.4789 - loglik: -7.7436e+02 - logprior: 7.8796
Epoch 7/10
10/10 - 3s - loss: 764.2692 - loglik: -7.7299e+02 - logprior: 8.7207
Epoch 8/10
10/10 - 3s - loss: 762.6490 - loglik: -7.7213e+02 - logprior: 9.4864
Epoch 9/10
10/10 - 3s - loss: 762.3732 - loglik: -7.7255e+02 - logprior: 10.1744
Epoch 10/10
10/10 - 3s - loss: 761.5604 - loglik: -7.7230e+02 - logprior: 10.7445
Fitted a model with MAP estimate = -761.4391
Time for alignment: 71.8206
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1015.0364 - loglik: -9.4828e+02 - logprior: -6.6758e+01
Epoch 2/10
10/10 - 2s - loss: 912.0436 - loglik: -8.9873e+02 - logprior: -1.3317e+01
Epoch 3/10
10/10 - 2s - loss: 860.2742 - loglik: -8.5610e+02 - logprior: -4.1770e+00
Epoch 4/10
10/10 - 2s - loss: 831.3403 - loglik: -8.3039e+02 - logprior: -9.5316e-01
Epoch 5/10
10/10 - 2s - loss: 817.4916 - loglik: -8.1826e+02 - logprior: 0.7684
Epoch 6/10
10/10 - 2s - loss: 808.0690 - loglik: -8.0970e+02 - logprior: 1.6351
Epoch 7/10
10/10 - 2s - loss: 804.0074 - loglik: -8.0588e+02 - logprior: 1.8753
Epoch 8/10
10/10 - 2s - loss: 801.8209 - loglik: -8.0384e+02 - logprior: 2.0206
Epoch 9/10
10/10 - 2s - loss: 800.1809 - loglik: -8.0239e+02 - logprior: 2.2072
Epoch 10/10
10/10 - 2s - loss: 799.3705 - loglik: -8.0176e+02 - logprior: 2.3863
Fitted a model with MAP estimate = -798.9639
expansions: [(11, 3), (12, 2), (16, 3), (17, 1), (26, 2), (38, 2), (39, 3), (55, 1), (64, 2), (78, 3), (79, 1), (80, 2), (89, 1), (90, 1), (110, 3), (114, 2), (115, 1), (121, 1), (123, 1), (126, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 885.1927 - loglik: -8.0999e+02 - logprior: -7.5199e+01
Epoch 2/2
10/10 - 3s - loss: 815.6053 - loglik: -7.8843e+02 - logprior: -2.7180e+01
Fitted a model with MAP estimate = -803.3594
expansions: [(0, 2), (35, 1), (129, 1)]
discards: [  0  11 137 163]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 840.2200 - loglik: -7.8207e+02 - logprior: -5.8150e+01
Epoch 2/2
10/10 - 3s - loss: 785.5479 - loglik: -7.7549e+02 - logprior: -1.0060e+01
Fitted a model with MAP estimate = -777.7465
expansions: []
discards: [ 0 51]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 852.0607 - loglik: -7.8103e+02 - logprior: -7.1032e+01
Epoch 2/10
10/10 - 3s - loss: 794.9725 - loglik: -7.7777e+02 - logprior: -1.7204e+01
Epoch 3/10
10/10 - 3s - loss: 776.5559 - loglik: -7.7539e+02 - logprior: -1.1652e+00
Epoch 4/10
10/10 - 3s - loss: 769.7552 - loglik: -7.7423e+02 - logprior: 4.4758
Epoch 5/10
10/10 - 3s - loss: 764.7660 - loglik: -7.7167e+02 - logprior: 6.9015
Epoch 6/10
10/10 - 3s - loss: 760.6846 - loglik: -7.6878e+02 - logprior: 8.0982
Epoch 7/10
10/10 - 3s - loss: 759.4684 - loglik: -7.6840e+02 - logprior: 8.9375
Epoch 8/10
10/10 - 3s - loss: 757.8971 - loglik: -7.6760e+02 - logprior: 9.7005
Epoch 9/10
10/10 - 3s - loss: 757.3810 - loglik: -7.6778e+02 - logprior: 10.4004
Epoch 10/10
10/10 - 3s - loss: 756.9005 - loglik: -7.6789e+02 - logprior: 10.9870
Fitted a model with MAP estimate = -756.3672
Time for alignment: 74.3997
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1014.9321 - loglik: -9.4817e+02 - logprior: -6.6760e+01
Epoch 2/10
10/10 - 2s - loss: 912.9053 - loglik: -8.9959e+02 - logprior: -1.3314e+01
Epoch 3/10
10/10 - 2s - loss: 861.0305 - loglik: -8.5676e+02 - logprior: -4.2730e+00
Epoch 4/10
10/10 - 2s - loss: 831.6717 - loglik: -8.3052e+02 - logprior: -1.1471e+00
Epoch 5/10
10/10 - 2s - loss: 814.6290 - loglik: -8.1513e+02 - logprior: 0.4975
Epoch 6/10
10/10 - 2s - loss: 805.4219 - loglik: -8.0679e+02 - logprior: 1.3732
Epoch 7/10
10/10 - 2s - loss: 801.5006 - loglik: -8.0331e+02 - logprior: 1.8079
Epoch 8/10
10/10 - 2s - loss: 799.8850 - loglik: -8.0191e+02 - logprior: 2.0278
Epoch 9/10
10/10 - 2s - loss: 798.1630 - loglik: -8.0039e+02 - logprior: 2.2294
Epoch 10/10
10/10 - 2s - loss: 799.0151 - loglik: -8.0138e+02 - logprior: 2.3662
Fitted a model with MAP estimate = -797.7570
expansions: [(11, 3), (12, 2), (16, 3), (17, 1), (26, 2), (38, 2), (39, 1), (55, 1), (66, 2), (67, 1), (71, 1), (78, 4), (80, 2), (89, 1), (90, 2), (103, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 882.6759 - loglik: -8.0749e+02 - logprior: -7.5190e+01
Epoch 2/2
10/10 - 3s - loss: 814.8049 - loglik: -7.8766e+02 - logprior: -2.7149e+01
Fitted a model with MAP estimate = -802.3564
expansions: [(0, 2), (35, 1)]
discards: [  0  11 116 165]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 839.0842 - loglik: -7.8083e+02 - logprior: -5.8257e+01
Epoch 2/2
10/10 - 3s - loss: 785.8868 - loglik: -7.7570e+02 - logprior: -1.0185e+01
Fitted a model with MAP estimate = -776.9478
expansions: []
discards: [ 0 50 83]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 850.7523 - loglik: -7.7952e+02 - logprior: -7.1228e+01
Epoch 2/10
10/10 - 3s - loss: 794.5958 - loglik: -7.7724e+02 - logprior: -1.7358e+01
Epoch 3/10
10/10 - 3s - loss: 776.1934 - loglik: -7.7493e+02 - logprior: -1.2584e+00
Epoch 4/10
10/10 - 3s - loss: 769.5023 - loglik: -7.7387e+02 - logprior: 4.3665
Epoch 5/10
10/10 - 3s - loss: 763.8454 - loglik: -7.7065e+02 - logprior: 6.8005
Epoch 6/10
10/10 - 3s - loss: 761.3427 - loglik: -7.6936e+02 - logprior: 8.0160
Epoch 7/10
10/10 - 3s - loss: 758.5153 - loglik: -7.6739e+02 - logprior: 8.8715
Epoch 8/10
10/10 - 3s - loss: 758.2745 - loglik: -7.6792e+02 - logprior: 9.6439
Epoch 9/10
10/10 - 3s - loss: 756.7523 - loglik: -7.6707e+02 - logprior: 10.3222
Epoch 10/10
10/10 - 3s - loss: 756.5341 - loglik: -7.6745e+02 - logprior: 10.9161
Fitted a model with MAP estimate = -756.3472
Time for alignment: 72.5302
Fitting a model of length 136 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1014.9017 - loglik: -9.4815e+02 - logprior: -6.6756e+01
Epoch 2/10
10/10 - 2s - loss: 911.8962 - loglik: -8.9856e+02 - logprior: -1.3332e+01
Epoch 3/10
10/10 - 2s - loss: 860.2902 - loglik: -8.5589e+02 - logprior: -4.4004e+00
Epoch 4/10
10/10 - 2s - loss: 831.5571 - loglik: -8.3043e+02 - logprior: -1.1234e+00
Epoch 5/10
10/10 - 2s - loss: 815.7418 - loglik: -8.1629e+02 - logprior: 0.5471
Epoch 6/10
10/10 - 2s - loss: 806.3787 - loglik: -8.0752e+02 - logprior: 1.1441
Epoch 7/10
10/10 - 2s - loss: 802.3690 - loglik: -8.0406e+02 - logprior: 1.6869
Epoch 8/10
10/10 - 2s - loss: 800.3314 - loglik: -8.0235e+02 - logprior: 2.0188
Epoch 9/10
10/10 - 2s - loss: 799.5701 - loglik: -8.0178e+02 - logprior: 2.2100
Epoch 10/10
10/10 - 2s - loss: 798.6247 - loglik: -8.0099e+02 - logprior: 2.3638
Fitted a model with MAP estimate = -798.4298
expansions: [(11, 3), (12, 2), (16, 4), (26, 2), (38, 2), (39, 1), (51, 1), (58, 1), (67, 1), (71, 1), (78, 4), (80, 2), (89, 1), (90, 1), (110, 2), (114, 3), (115, 1), (120, 2), (122, 1), (128, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 883.6569 - loglik: -8.0841e+02 - logprior: -7.5248e+01
Epoch 2/2
10/10 - 3s - loss: 816.6396 - loglik: -7.8947e+02 - logprior: -2.7168e+01
Fitted a model with MAP estimate = -803.9326
expansions: [(0, 2), (35, 1), (128, 1)]
discards: [ 0 11]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 840.9471 - loglik: -7.8285e+02 - logprior: -5.8096e+01
Epoch 2/2
10/10 - 3s - loss: 786.6122 - loglik: -7.7670e+02 - logprior: -9.9158e+00
Fitted a model with MAP estimate = -778.3798
expansions: []
discards: [  0 143 164]
Re-initialized the encoder parameters.
Fitting a model of length 172 on 613 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 851.9855 - loglik: -7.8097e+02 - logprior: -7.1015e+01
Epoch 2/10
10/10 - 3s - loss: 795.0040 - loglik: -7.7790e+02 - logprior: -1.7107e+01
Epoch 3/10
10/10 - 3s - loss: 777.4816 - loglik: -7.7630e+02 - logprior: -1.1796e+00
Epoch 4/10
10/10 - 3s - loss: 770.3693 - loglik: -7.7478e+02 - logprior: 4.4067
Epoch 5/10
10/10 - 3s - loss: 764.9116 - loglik: -7.7175e+02 - logprior: 6.8437
Epoch 6/10
10/10 - 3s - loss: 762.0551 - loglik: -7.7010e+02 - logprior: 8.0497
Epoch 7/10
10/10 - 3s - loss: 759.8190 - loglik: -7.6871e+02 - logprior: 8.8885
Epoch 8/10
10/10 - 3s - loss: 759.1309 - loglik: -7.6880e+02 - logprior: 9.6661
Epoch 9/10
10/10 - 3s - loss: 758.3102 - loglik: -7.6865e+02 - logprior: 10.3434
Epoch 10/10
10/10 - 3s - loss: 758.1142 - loglik: -7.6904e+02 - logprior: 10.9279
Fitted a model with MAP estimate = -757.4533
Time for alignment: 72.5831
Computed alignments with likelihoods: ['-756.8549', '-761.4391', '-756.3672', '-756.3472', '-757.4533']
Best model has likelihood: -756.3472  (prior= 11.2058 )
time for generating output: 0.1815
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sti.projection.fasta
SP score = 0.7563546187228767
Training of 5 independent models on file glob.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea559f5610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe952157ee0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe952ca6190>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 580.3946 - loglik: -5.7048e+02 - logprior: -9.9195e+00
Epoch 2/10
12/12 - 1s - loss: 540.7703 - loglik: -5.3836e+02 - logprior: -2.4059e+00
Epoch 3/10
12/12 - 1s - loss: 515.1734 - loglik: -5.1351e+02 - logprior: -1.6670e+00
Epoch 4/10
12/12 - 1s - loss: 505.8737 - loglik: -5.0427e+02 - logprior: -1.6054e+00
Epoch 5/10
12/12 - 1s - loss: 502.9398 - loglik: -5.0136e+02 - logprior: -1.5825e+00
Epoch 6/10
12/12 - 1s - loss: 500.5270 - loglik: -4.9900e+02 - logprior: -1.5278e+00
Epoch 7/10
12/12 - 1s - loss: 499.2210 - loglik: -4.9773e+02 - logprior: -1.4863e+00
Epoch 8/10
12/12 - 1s - loss: 497.3749 - loglik: -4.9585e+02 - logprior: -1.5174e+00
Epoch 9/10
12/12 - 1s - loss: 495.1351 - loglik: -4.9358e+02 - logprior: -1.5534e+00
Epoch 10/10
12/12 - 1s - loss: 492.8631 - loglik: -4.9127e+02 - logprior: -1.5914e+00
Fitted a model with MAP estimate = -492.2902
expansions: [(6, 3), (10, 2), (11, 2), (12, 2), (13, 1), (21, 1), (36, 2), (49, 1), (50, 3), (52, 1), (59, 5), (61, 1), (67, 2), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 510.2965 - loglik: -4.9892e+02 - logprior: -1.1378e+01
Epoch 2/2
12/12 - 1s - loss: 490.4198 - loglik: -4.8570e+02 - logprior: -4.7158e+00
Fitted a model with MAP estimate = -487.4568
expansions: [(0, 8)]
discards: [ 0 15 18 78 90]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 494.7820 - loglik: -4.8546e+02 - logprior: -9.3226e+00
Epoch 2/2
12/12 - 1s - loss: 483.0787 - loglik: -4.8057e+02 - logprior: -2.5063e+00
Fitted a model with MAP estimate = -481.8072
expansions: []
discards: [1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 490.7460 - loglik: -4.8189e+02 - logprior: -8.8598e+00
Epoch 2/10
12/12 - 1s - loss: 482.5476 - loglik: -4.8028e+02 - logprior: -2.2636e+00
Epoch 3/10
12/12 - 1s - loss: 481.4871 - loglik: -4.8019e+02 - logprior: -1.2999e+00
Epoch 4/10
12/12 - 1s - loss: 479.2201 - loglik: -4.7836e+02 - logprior: -8.5961e-01
Epoch 5/10
12/12 - 1s - loss: 479.9076 - loglik: -4.7920e+02 - logprior: -7.1107e-01
Fitted a model with MAP estimate = -478.8329
Time for alignment: 47.6198
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 580.2646 - loglik: -5.7035e+02 - logprior: -9.9160e+00
Epoch 2/10
12/12 - 1s - loss: 541.7930 - loglik: -5.3939e+02 - logprior: -2.4032e+00
Epoch 3/10
12/12 - 1s - loss: 517.7697 - loglik: -5.1610e+02 - logprior: -1.6698e+00
Epoch 4/10
12/12 - 1s - loss: 507.6928 - loglik: -5.0609e+02 - logprior: -1.5982e+00
Epoch 5/10
12/12 - 1s - loss: 504.4174 - loglik: -5.0284e+02 - logprior: -1.5798e+00
Epoch 6/10
12/12 - 1s - loss: 502.4741 - loglik: -5.0092e+02 - logprior: -1.5535e+00
Epoch 7/10
12/12 - 1s - loss: 499.4563 - loglik: -4.9792e+02 - logprior: -1.5382e+00
Epoch 8/10
12/12 - 1s - loss: 498.5837 - loglik: -4.9703e+02 - logprior: -1.5481e+00
Epoch 9/10
12/12 - 1s - loss: 496.1364 - loglik: -4.9454e+02 - logprior: -1.5912e+00
Epoch 10/10
12/12 - 1s - loss: 494.5260 - loglik: -4.9291e+02 - logprior: -1.6121e+00
Fitted a model with MAP estimate = -493.3963
expansions: [(7, 2), (8, 1), (9, 2), (10, 3), (11, 2), (21, 1), (29, 1), (36, 3), (49, 1), (50, 3), (52, 1), (59, 1), (60, 4), (61, 2), (68, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 5s - loss: 512.8359 - loglik: -5.0148e+02 - logprior: -1.1353e+01
Epoch 2/2
12/12 - 1s - loss: 490.5275 - loglik: -4.8580e+02 - logprior: -4.7262e+00
Fitted a model with MAP estimate = -487.8906
expansions: [(0, 8)]
discards: [ 0 12 14 15 18 80 81]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 495.0935 - loglik: -4.8582e+02 - logprior: -9.2731e+00
Epoch 2/2
12/12 - 1s - loss: 483.5848 - loglik: -4.8112e+02 - logprior: -2.4669e+00
Fitted a model with MAP estimate = -482.2614
expansions: [(20, 2)]
discards: [ 1  2  3  4  5  6  7 52]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 491.0947 - loglik: -4.8233e+02 - logprior: -8.7636e+00
Epoch 2/10
12/12 - 1s - loss: 483.8502 - loglik: -4.8170e+02 - logprior: -2.1532e+00
Epoch 3/10
12/12 - 1s - loss: 481.2242 - loglik: -4.7998e+02 - logprior: -1.2462e+00
Epoch 4/10
12/12 - 1s - loss: 480.6389 - loglik: -4.7975e+02 - logprior: -8.8987e-01
Epoch 5/10
12/12 - 1s - loss: 479.2702 - loglik: -4.7860e+02 - logprior: -6.7070e-01
Epoch 6/10
12/12 - 1s - loss: 477.1320 - loglik: -4.7649e+02 - logprior: -6.3719e-01
Epoch 7/10
12/12 - 1s - loss: 477.7245 - loglik: -4.7711e+02 - logprior: -6.1533e-01
Fitted a model with MAP estimate = -475.7183
Time for alignment: 49.7853
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 580.4274 - loglik: -5.7051e+02 - logprior: -9.9168e+00
Epoch 2/10
12/12 - 1s - loss: 541.9130 - loglik: -5.3951e+02 - logprior: -2.4052e+00
Epoch 3/10
12/12 - 1s - loss: 516.3099 - loglik: -5.1465e+02 - logprior: -1.6565e+00
Epoch 4/10
12/12 - 1s - loss: 507.1410 - loglik: -5.0554e+02 - logprior: -1.6001e+00
Epoch 5/10
12/12 - 1s - loss: 502.0635 - loglik: -5.0052e+02 - logprior: -1.5401e+00
Epoch 6/10
12/12 - 1s - loss: 499.9713 - loglik: -4.9850e+02 - logprior: -1.4667e+00
Epoch 7/10
12/12 - 1s - loss: 497.7650 - loglik: -4.9633e+02 - logprior: -1.4327e+00
Epoch 8/10
12/12 - 1s - loss: 496.5490 - loglik: -4.9506e+02 - logprior: -1.4804e+00
Epoch 9/10
12/12 - 1s - loss: 494.3800 - loglik: -4.9286e+02 - logprior: -1.5111e+00
Epoch 10/10
12/12 - 1s - loss: 492.8476 - loglik: -4.9131e+02 - logprior: -1.5269e+00
Fitted a model with MAP estimate = -491.1668
expansions: [(6, 3), (10, 4), (13, 1), (21, 1), (36, 4), (49, 1), (50, 3), (52, 1), (58, 2), (59, 5), (61, 1), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 107 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 510.9264 - loglik: -4.9951e+02 - logprior: -1.1418e+01
Epoch 2/2
12/12 - 1s - loss: 489.0565 - loglik: -4.8432e+02 - logprior: -4.7409e+00
Fitted a model with MAP estimate = -485.5109
expansions: [(0, 7), (48, 1)]
discards: [ 0 75 76 79]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 491.2906 - loglik: -4.8201e+02 - logprior: -9.2811e+00
Epoch 2/2
12/12 - 2s - loss: 480.6934 - loglik: -4.7828e+02 - logprior: -2.4090e+00
Fitted a model with MAP estimate = -478.9783
expansions: []
discards: [1 2 3 4 5 6]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 487.4798 - loglik: -4.7864e+02 - logprior: -8.8363e+00
Epoch 2/10
12/12 - 1s - loss: 480.6585 - loglik: -4.7841e+02 - logprior: -2.2512e+00
Epoch 3/10
12/12 - 1s - loss: 477.6154 - loglik: -4.7633e+02 - logprior: -1.2840e+00
Epoch 4/10
12/12 - 1s - loss: 477.2747 - loglik: -4.7642e+02 - logprior: -8.5607e-01
Epoch 5/10
12/12 - 1s - loss: 476.9175 - loglik: -4.7622e+02 - logprior: -6.9337e-01
Epoch 6/10
12/12 - 1s - loss: 475.5506 - loglik: -4.7493e+02 - logprior: -6.1995e-01
Epoch 7/10
12/12 - 1s - loss: 474.8233 - loglik: -4.7418e+02 - logprior: -6.4310e-01
Epoch 8/10
12/12 - 1s - loss: 471.3555 - loglik: -4.7071e+02 - logprior: -6.4349e-01
Epoch 9/10
12/12 - 1s - loss: 471.1214 - loglik: -4.7047e+02 - logprior: -6.4390e-01
Epoch 10/10
12/12 - 1s - loss: 468.2188 - loglik: -4.6756e+02 - logprior: -6.5703e-01
Fitted a model with MAP estimate = -466.9716
Time for alignment: 52.3363
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 580.4229 - loglik: -5.7050e+02 - logprior: -9.9218e+00
Epoch 2/10
12/12 - 1s - loss: 541.5000 - loglik: -5.3908e+02 - logprior: -2.4226e+00
Epoch 3/10
12/12 - 1s - loss: 515.3936 - loglik: -5.1369e+02 - logprior: -1.7046e+00
Epoch 4/10
12/12 - 1s - loss: 507.1018 - loglik: -5.0544e+02 - logprior: -1.6612e+00
Epoch 5/10
12/12 - 1s - loss: 503.2606 - loglik: -5.0157e+02 - logprior: -1.6847e+00
Epoch 6/10
12/12 - 1s - loss: 498.6668 - loglik: -4.9703e+02 - logprior: -1.6375e+00
Epoch 7/10
12/12 - 1s - loss: 498.1194 - loglik: -4.9651e+02 - logprior: -1.6017e+00
Epoch 8/10
12/12 - 1s - loss: 496.2028 - loglik: -4.9459e+02 - logprior: -1.6082e+00
Epoch 9/10
12/12 - 1s - loss: 493.7611 - loglik: -4.9212e+02 - logprior: -1.6322e+00
Epoch 10/10
12/12 - 1s - loss: 492.1801 - loglik: -4.9053e+02 - logprior: -1.6425e+00
Fitted a model with MAP estimate = -491.1229
expansions: [(7, 2), (8, 1), (10, 2), (11, 1), (12, 2), (13, 1), (21, 1), (29, 1), (36, 3), (49, 1), (50, 3), (52, 1), (59, 1), (60, 4), (61, 3), (64, 1), (70, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 511.0420 - loglik: -4.9966e+02 - logprior: -1.1385e+01
Epoch 2/2
12/12 - 1s - loss: 489.9494 - loglik: -4.8520e+02 - logprior: -4.7508e+00
Fitted a model with MAP estimate = -486.1149
expansions: [(0, 7)]
discards: [ 0 12 13 81 82 85]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 493.8371 - loglik: -4.8461e+02 - logprior: -9.2313e+00
Epoch 2/2
12/12 - 1s - loss: 482.5899 - loglik: -4.8023e+02 - logprior: -2.3571e+00
Fitted a model with MAP estimate = -481.1544
expansions: []
discards: [ 1  2  3  4  5  6 52]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 4s - loss: 490.6990 - loglik: -4.8194e+02 - logprior: -8.7632e+00
Epoch 2/10
12/12 - 1s - loss: 482.7368 - loglik: -4.8054e+02 - logprior: -2.1989e+00
Epoch 3/10
12/12 - 1s - loss: 482.1506 - loglik: -4.8089e+02 - logprior: -1.2622e+00
Epoch 4/10
12/12 - 1s - loss: 478.5383 - loglik: -4.7770e+02 - logprior: -8.3795e-01
Epoch 5/10
12/12 - 1s - loss: 480.2241 - loglik: -4.7959e+02 - logprior: -6.3754e-01
Fitted a model with MAP estimate = -478.7133
Time for alignment: 44.5473
Fitting a model of length 80 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 581.0129 - loglik: -5.7110e+02 - logprior: -9.9174e+00
Epoch 2/10
12/12 - 1s - loss: 541.7944 - loglik: -5.3939e+02 - logprior: -2.4049e+00
Epoch 3/10
12/12 - 1s - loss: 516.6817 - loglik: -5.1504e+02 - logprior: -1.6388e+00
Epoch 4/10
12/12 - 1s - loss: 508.1572 - loglik: -5.0658e+02 - logprior: -1.5748e+00
Epoch 5/10
12/12 - 1s - loss: 504.8235 - loglik: -5.0330e+02 - logprior: -1.5179e+00
Epoch 6/10
12/12 - 1s - loss: 501.3243 - loglik: -4.9985e+02 - logprior: -1.4736e+00
Epoch 7/10
12/12 - 1s - loss: 501.2160 - loglik: -4.9979e+02 - logprior: -1.4199e+00
Epoch 8/10
12/12 - 1s - loss: 498.6349 - loglik: -4.9721e+02 - logprior: -1.4188e+00
Epoch 9/10
12/12 - 1s - loss: 497.0023 - loglik: -4.9553e+02 - logprior: -1.4644e+00
Epoch 10/10
12/12 - 1s - loss: 493.8808 - loglik: -4.9232e+02 - logprior: -1.5584e+00
Fitted a model with MAP estimate = -492.8141
expansions: [(6, 3), (10, 3), (13, 1), (21, 1), (29, 1), (36, 4), (49, 1), (50, 3), (51, 1), (58, 3), (59, 2), (61, 1), (71, 1), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 509.7699 - loglik: -4.9843e+02 - logprior: -1.1337e+01
Epoch 2/2
12/12 - 1s - loss: 490.1425 - loglik: -4.8548e+02 - logprior: -4.6599e+00
Fitted a model with MAP estimate = -487.2370
expansions: [(0, 8), (48, 1)]
discards: [ 0 77]
Re-initialized the encoder parameters.
Fitting a model of length 112 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 4s - loss: 493.5334 - loglik: -4.8420e+02 - logprior: -9.3308e+00
Epoch 2/2
12/12 - 1s - loss: 482.7122 - loglik: -4.8019e+02 - logprior: -2.5192e+00
Fitted a model with MAP estimate = -480.4996
expansions: []
discards: [1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 3983 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 488.9743 - loglik: -4.8010e+02 - logprior: -8.8722e+00
Epoch 2/10
12/12 - 1s - loss: 481.5052 - loglik: -4.7920e+02 - logprior: -2.3063e+00
Epoch 3/10
12/12 - 1s - loss: 479.2297 - loglik: -4.7794e+02 - logprior: -1.2874e+00
Epoch 4/10
12/12 - 1s - loss: 479.5416 - loglik: -4.7868e+02 - logprior: -8.5812e-01
Fitted a model with MAP estimate = -478.2539
Time for alignment: 44.8853
Computed alignments with likelihoods: ['-478.8329', '-475.7183', '-466.9716', '-478.7133', '-478.2539']
Best model has likelihood: -466.9716  (prior= -0.6689 )
time for generating output: 0.1672
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/glob.projection.fasta
SP score = 0.8907431801725567
Training of 5 independent models on file az.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe939319940>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea669c02e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91692de50>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 618.6508 - loglik: -5.7841e+02 - logprior: -4.0245e+01
Epoch 2/10
10/10 - 1s - loss: 564.7746 - loglik: -5.5467e+02 - logprior: -1.0101e+01
Epoch 3/10
10/10 - 1s - loss: 538.2576 - loglik: -5.3410e+02 - logprior: -4.1604e+00
Epoch 4/10
10/10 - 1s - loss: 525.7119 - loglik: -5.2370e+02 - logprior: -2.0101e+00
Epoch 5/10
10/10 - 1s - loss: 517.9554 - loglik: -5.1693e+02 - logprior: -1.0275e+00
Epoch 6/10
10/10 - 1s - loss: 513.2325 - loglik: -5.1263e+02 - logprior: -6.0435e-01
Epoch 7/10
10/10 - 1s - loss: 511.1597 - loglik: -5.1076e+02 - logprior: -3.9547e-01
Epoch 8/10
10/10 - 1s - loss: 508.7194 - loglik: -5.0848e+02 - logprior: -2.4163e-01
Epoch 9/10
10/10 - 1s - loss: 507.1151 - loglik: -5.0694e+02 - logprior: -1.6911e-01
Epoch 10/10
10/10 - 1s - loss: 505.9855 - loglik: -5.0583e+02 - logprior: -1.5425e-01
Fitted a model with MAP estimate = -505.5048
expansions: [(0, 3), (5, 1), (8, 1), (36, 2), (43, 11), (53, 3), (61, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 572.4390 - loglik: -5.2019e+02 - logprior: -5.2251e+01
Epoch 2/2
10/10 - 1s - loss: 522.9690 - loglik: -5.0759e+02 - logprior: -1.5379e+01
Fitted a model with MAP estimate = -514.5451
expansions: [(11, 1)]
discards: [ 0  2 54 55 72 83]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 553.2944 - loglik: -5.0772e+02 - logprior: -4.5570e+01
Epoch 2/2
10/10 - 1s - loss: 522.8478 - loglik: -5.0532e+02 - logprior: -1.7523e+01
Fitted a model with MAP estimate = -517.5034
expansions: [(0, 4)]
discards: [ 0 53]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 546.1448 - loglik: -5.0550e+02 - logprior: -4.0641e+01
Epoch 2/10
10/10 - 1s - loss: 513.4603 - loglik: -5.0334e+02 - logprior: -1.0120e+01
Epoch 3/10
10/10 - 1s - loss: 506.5085 - loglik: -5.0319e+02 - logprior: -3.3139e+00
Epoch 4/10
10/10 - 1s - loss: 502.7203 - loglik: -5.0194e+02 - logprior: -7.8400e-01
Epoch 5/10
10/10 - 1s - loss: 501.3995 - loglik: -5.0185e+02 - logprior: 0.4506
Epoch 6/10
10/10 - 1s - loss: 499.6326 - loglik: -5.0073e+02 - logprior: 1.0960
Epoch 7/10
10/10 - 1s - loss: 497.6548 - loglik: -4.9908e+02 - logprior: 1.4304
Epoch 8/10
10/10 - 1s - loss: 496.6847 - loglik: -4.9831e+02 - logprior: 1.6252
Epoch 9/10
10/10 - 1s - loss: 495.8917 - loglik: -4.9768e+02 - logprior: 1.7857
Epoch 10/10
10/10 - 1s - loss: 495.0224 - loglik: -4.9695e+02 - logprior: 1.9282
Fitted a model with MAP estimate = -494.3830
Time for alignment: 42.1228
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 618.4578 - loglik: -5.7821e+02 - logprior: -4.0244e+01
Epoch 2/10
10/10 - 1s - loss: 565.1364 - loglik: -5.5504e+02 - logprior: -1.0099e+01
Epoch 3/10
10/10 - 1s - loss: 539.3744 - loglik: -5.3516e+02 - logprior: -4.2143e+00
Epoch 4/10
10/10 - 1s - loss: 526.7777 - loglik: -5.2476e+02 - logprior: -2.0219e+00
Epoch 5/10
10/10 - 1s - loss: 519.8748 - loglik: -5.1897e+02 - logprior: -9.0870e-01
Epoch 6/10
10/10 - 1s - loss: 515.1491 - loglik: -5.1474e+02 - logprior: -4.0546e-01
Epoch 7/10
10/10 - 1s - loss: 512.1738 - loglik: -5.1196e+02 - logprior: -2.1436e-01
Epoch 8/10
10/10 - 1s - loss: 510.0224 - loglik: -5.0994e+02 - logprior: -8.3020e-02
Epoch 9/10
10/10 - 1s - loss: 508.3348 - loglik: -5.0835e+02 - logprior: 0.0219
Epoch 10/10
10/10 - 1s - loss: 506.8456 - loglik: -5.0694e+02 - logprior: 0.0959
Fitted a model with MAP estimate = -506.6328
expansions: [(0, 3), (8, 3), (36, 1), (38, 1), (43, 12), (54, 5), (61, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 102 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 573.3326 - loglik: -5.2139e+02 - logprior: -5.1945e+01
Epoch 2/2
10/10 - 1s - loss: 524.3524 - loglik: -5.0903e+02 - logprior: -1.5318e+01
Fitted a model with MAP estimate = -514.3408
expansions: []
discards: [ 0  1  2 58 74 75 87]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 544.2992 - loglik: -5.0699e+02 - logprior: -3.7309e+01
Epoch 2/2
10/10 - 1s - loss: 512.9451 - loglik: -5.0395e+02 - logprior: -8.9906e+00
Fitted a model with MAP estimate = -508.3484
expansions: [(0, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 553.9825 - loglik: -5.0385e+02 - logprior: -5.0128e+01
Epoch 2/10
10/10 - 1s - loss: 516.9786 - loglik: -5.0290e+02 - logprior: -1.4077e+01
Epoch 3/10
10/10 - 1s - loss: 507.9462 - loglik: -5.0279e+02 - logprior: -5.1547e+00
Epoch 4/10
10/10 - 1s - loss: 503.7049 - loglik: -5.0203e+02 - logprior: -1.6723e+00
Epoch 5/10
10/10 - 1s - loss: 501.4753 - loglik: -5.0144e+02 - logprior: -3.3608e-02
Epoch 6/10
10/10 - 1s - loss: 499.2612 - loglik: -5.0003e+02 - logprior: 0.7670
Epoch 7/10
10/10 - 1s - loss: 497.8351 - loglik: -4.9898e+02 - logprior: 1.1511
Epoch 8/10
10/10 - 1s - loss: 496.4441 - loglik: -4.9781e+02 - logprior: 1.3663
Epoch 9/10
10/10 - 1s - loss: 494.9368 - loglik: -4.9651e+02 - logprior: 1.5732
Epoch 10/10
10/10 - 1s - loss: 494.9095 - loglik: -4.9668e+02 - logprior: 1.7684
Fitted a model with MAP estimate = -493.9299
Time for alignment: 42.3033
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 618.5607 - loglik: -5.7831e+02 - logprior: -4.0247e+01
Epoch 2/10
10/10 - 1s - loss: 564.6952 - loglik: -5.5459e+02 - logprior: -1.0110e+01
Epoch 3/10
10/10 - 1s - loss: 540.5404 - loglik: -5.3636e+02 - logprior: -4.1762e+00
Epoch 4/10
10/10 - 1s - loss: 527.9156 - loglik: -5.2593e+02 - logprior: -1.9807e+00
Epoch 5/10
10/10 - 1s - loss: 519.4149 - loglik: -5.1851e+02 - logprior: -9.0569e-01
Epoch 6/10
10/10 - 1s - loss: 515.9908 - loglik: -5.1556e+02 - logprior: -4.3325e-01
Epoch 7/10
10/10 - 1s - loss: 511.9283 - loglik: -5.1172e+02 - logprior: -2.0837e-01
Epoch 8/10
10/10 - 1s - loss: 510.4167 - loglik: -5.1034e+02 - logprior: -6.9661e-02
Epoch 9/10
10/10 - 1s - loss: 506.8058 - loglik: -5.0680e+02 - logprior: -1.1779e-03
Epoch 10/10
10/10 - 1s - loss: 507.5464 - loglik: -5.0756e+02 - logprior: 0.0122
Fitted a model with MAP estimate = -506.0496
expansions: [(0, 3), (8, 3), (36, 1), (38, 1), (43, 14), (53, 3), (61, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 102 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 574.0150 - loglik: -5.2208e+02 - logprior: -5.1937e+01
Epoch 2/2
10/10 - 1s - loss: 523.1223 - loglik: -5.0772e+02 - logprior: -1.5407e+01
Fitted a model with MAP estimate = -513.3971
expansions: []
discards: [ 0  1  2 76 87]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 543.1364 - loglik: -5.0572e+02 - logprior: -3.7414e+01
Epoch 2/2
10/10 - 1s - loss: 511.5489 - loglik: -5.0250e+02 - logprior: -9.0499e+00
Fitted a model with MAP estimate = -507.0255
expansions: [(0, 3)]
discards: [55]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 553.9550 - loglik: -5.0377e+02 - logprior: -5.0184e+01
Epoch 2/10
10/10 - 1s - loss: 515.9840 - loglik: -5.0180e+02 - logprior: -1.4183e+01
Epoch 3/10
10/10 - 1s - loss: 506.9188 - loglik: -5.0167e+02 - logprior: -5.2503e+00
Epoch 4/10
10/10 - 1s - loss: 502.7968 - loglik: -5.0112e+02 - logprior: -1.6777e+00
Epoch 5/10
10/10 - 1s - loss: 500.3591 - loglik: -5.0034e+02 - logprior: -1.5454e-02
Epoch 6/10
10/10 - 1s - loss: 498.3757 - loglik: -4.9919e+02 - logprior: 0.8133
Epoch 7/10
10/10 - 1s - loss: 496.7840 - loglik: -4.9800e+02 - logprior: 1.2169
Epoch 8/10
10/10 - 1s - loss: 495.5748 - loglik: -4.9705e+02 - logprior: 1.4807
Epoch 9/10
10/10 - 1s - loss: 494.0567 - loglik: -4.9576e+02 - logprior: 1.7098
Epoch 10/10
10/10 - 1s - loss: 494.0134 - loglik: -4.9591e+02 - logprior: 1.9006
Fitted a model with MAP estimate = -493.0670
Time for alignment: 41.2385
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 618.8366 - loglik: -5.7859e+02 - logprior: -4.0247e+01
Epoch 2/10
10/10 - 1s - loss: 564.5204 - loglik: -5.5441e+02 - logprior: -1.0106e+01
Epoch 3/10
10/10 - 1s - loss: 538.9233 - loglik: -5.3472e+02 - logprior: -4.2008e+00
Epoch 4/10
10/10 - 1s - loss: 525.5803 - loglik: -5.2356e+02 - logprior: -2.0191e+00
Epoch 5/10
10/10 - 1s - loss: 518.9146 - loglik: -5.1794e+02 - logprior: -9.7487e-01
Epoch 6/10
10/10 - 1s - loss: 515.3732 - loglik: -5.1485e+02 - logprior: -5.1764e-01
Epoch 7/10
10/10 - 1s - loss: 511.3327 - loglik: -5.1099e+02 - logprior: -3.3749e-01
Epoch 8/10
10/10 - 1s - loss: 509.1461 - loglik: -5.0889e+02 - logprior: -2.5023e-01
Epoch 9/10
10/10 - 1s - loss: 508.1252 - loglik: -5.0793e+02 - logprior: -1.9261e-01
Epoch 10/10
10/10 - 1s - loss: 506.4656 - loglik: -5.0627e+02 - logprior: -1.9673e-01
Fitted a model with MAP estimate = -506.0021
expansions: [(0, 3), (5, 1), (7, 1), (8, 1), (36, 1), (38, 1), (44, 11), (54, 5), (61, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 101 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 572.3871 - loglik: -5.2035e+02 - logprior: -5.2039e+01
Epoch 2/2
10/10 - 1s - loss: 522.1266 - loglik: -5.0675e+02 - logprior: -1.5372e+01
Fitted a model with MAP estimate = -513.3939
expansions: []
discards: [ 0  1  2 56 57 74 75 86]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 544.1813 - loglik: -5.0696e+02 - logprior: -3.7224e+01
Epoch 2/2
10/10 - 1s - loss: 512.7694 - loglik: -5.0376e+02 - logprior: -9.0136e+00
Fitted a model with MAP estimate = -508.4913
expansions: [(0, 3)]
discards: [53 54]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 555.8251 - loglik: -5.0562e+02 - logprior: -5.0204e+01
Epoch 2/10
10/10 - 1s - loss: 517.9482 - loglik: -5.0392e+02 - logprior: -1.4027e+01
Epoch 3/10
10/10 - 1s - loss: 508.4522 - loglik: -5.0339e+02 - logprior: -5.0618e+00
Epoch 4/10
10/10 - 1s - loss: 504.0040 - loglik: -5.0248e+02 - logprior: -1.5245e+00
Epoch 5/10
10/10 - 1s - loss: 501.7773 - loglik: -5.0187e+02 - logprior: 0.0939
Epoch 6/10
10/10 - 1s - loss: 499.8044 - loglik: -5.0068e+02 - logprior: 0.8765
Epoch 7/10
10/10 - 1s - loss: 498.5601 - loglik: -4.9980e+02 - logprior: 1.2385
Epoch 8/10
10/10 - 1s - loss: 497.1251 - loglik: -4.9860e+02 - logprior: 1.4759
Epoch 9/10
10/10 - 1s - loss: 496.0020 - loglik: -4.9769e+02 - logprior: 1.6935
Epoch 10/10
10/10 - 1s - loss: 494.9332 - loglik: -4.9680e+02 - logprior: 1.8739
Fitted a model with MAP estimate = -494.5433
Time for alignment: 40.4348
Fitting a model of length 76 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 618.7339 - loglik: -5.7849e+02 - logprior: -4.0242e+01
Epoch 2/10
10/10 - 1s - loss: 564.0137 - loglik: -5.5392e+02 - logprior: -1.0096e+01
Epoch 3/10
10/10 - 1s - loss: 537.3138 - loglik: -5.3315e+02 - logprior: -4.1616e+00
Epoch 4/10
10/10 - 1s - loss: 522.9948 - loglik: -5.2095e+02 - logprior: -2.0430e+00
Epoch 5/10
10/10 - 1s - loss: 517.4335 - loglik: -5.1633e+02 - logprior: -1.1020e+00
Epoch 6/10
10/10 - 1s - loss: 512.5596 - loglik: -5.1192e+02 - logprior: -6.3686e-01
Epoch 7/10
10/10 - 1s - loss: 510.1318 - loglik: -5.0969e+02 - logprior: -4.3656e-01
Epoch 8/10
10/10 - 1s - loss: 508.5424 - loglik: -5.0820e+02 - logprior: -3.3706e-01
Epoch 9/10
10/10 - 1s - loss: 506.6921 - loglik: -5.0640e+02 - logprior: -2.9231e-01
Epoch 10/10
10/10 - 1s - loss: 506.0805 - loglik: -5.0585e+02 - logprior: -2.3116e-01
Fitted a model with MAP estimate = -505.3863
expansions: [(0, 3), (5, 1), (7, 1), (8, 1), (36, 1), (43, 11), (53, 3), (61, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 98 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 571.0800 - loglik: -5.1882e+02 - logprior: -5.2258e+01
Epoch 2/2
10/10 - 1s - loss: 523.1241 - loglik: -5.0773e+02 - logprior: -1.5392e+01
Fitted a model with MAP estimate = -513.3796
expansions: []
discards: [ 0  1  2 72 83]
Re-initialized the encoder parameters.
Fitting a model of length 93 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 542.9291 - loglik: -5.0565e+02 - logprior: -3.7275e+01
Epoch 2/2
10/10 - 1s - loss: 512.5839 - loglik: -5.0355e+02 - logprior: -9.0377e+00
Fitted a model with MAP estimate = -507.8718
expansions: [(0, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 1086 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 554.4052 - loglik: -5.0426e+02 - logprior: -5.0148e+01
Epoch 2/10
10/10 - 1s - loss: 515.7057 - loglik: -5.0166e+02 - logprior: -1.4041e+01
Epoch 3/10
10/10 - 1s - loss: 507.4671 - loglik: -5.0238e+02 - logprior: -5.0828e+00
Epoch 4/10
10/10 - 1s - loss: 502.7692 - loglik: -5.0122e+02 - logprior: -1.5473e+00
Epoch 5/10
10/10 - 1s - loss: 500.9019 - loglik: -5.0100e+02 - logprior: 0.0963
Epoch 6/10
10/10 - 1s - loss: 499.3152 - loglik: -5.0022e+02 - logprior: 0.9039
Epoch 7/10
10/10 - 1s - loss: 496.9875 - loglik: -4.9829e+02 - logprior: 1.2993
Epoch 8/10
10/10 - 1s - loss: 495.8910 - loglik: -4.9745e+02 - logprior: 1.5643
Epoch 9/10
10/10 - 1s - loss: 495.4404 - loglik: -4.9724e+02 - logprior: 1.7994
Epoch 10/10
10/10 - 1s - loss: 493.6902 - loglik: -4.9567e+02 - logprior: 1.9868
Fitted a model with MAP estimate = -493.7006
Time for alignment: 40.4391
Computed alignments with likelihoods: ['-494.3830', '-493.9299', '-493.0670', '-494.5433', '-493.7006']
Best model has likelihood: -493.0670  (prior= 1.9792 )
time for generating output: 0.1726
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/az.projection.fasta
SP score = 0.7464664081005805
Training of 5 independent models on file ghf11.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea22a7ea00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9392a4880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f8698c70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 8s - loss: 1030.9634 - loglik: -9.5318e+02 - logprior: -7.7784e+01
Epoch 2/10
10/10 - 2s - loss: 888.5504 - loglik: -8.7328e+02 - logprior: -1.5268e+01
Epoch 3/10
10/10 - 2s - loss: 811.2285 - loglik: -8.0578e+02 - logprior: -5.4497e+00
Epoch 4/10
10/10 - 2s - loss: 762.4806 - loglik: -7.5908e+02 - logprior: -3.4039e+00
Epoch 5/10
10/10 - 2s - loss: 743.4682 - loglik: -7.4113e+02 - logprior: -2.3373e+00
Epoch 6/10
10/10 - 2s - loss: 735.9891 - loglik: -7.3485e+02 - logprior: -1.1389e+00
Epoch 7/10
10/10 - 2s - loss: 732.8566 - loglik: -7.3275e+02 - logprior: -1.0605e-01
Epoch 8/10
10/10 - 2s - loss: 731.0380 - loglik: -7.3142e+02 - logprior: 0.3852
Epoch 9/10
10/10 - 2s - loss: 730.5084 - loglik: -7.3123e+02 - logprior: 0.7203
Epoch 10/10
10/10 - 2s - loss: 728.4630 - loglik: -7.2962e+02 - logprior: 1.1583
Fitted a model with MAP estimate = -729.0374
expansions: [(17, 4), (20, 1), (22, 1), (26, 1), (39, 5), (44, 1), (45, 2), (58, 1), (77, 1), (78, 3), (79, 1), (90, 2), (91, 1), (94, 1), (100, 2), (110, 1), (118, 2), (124, 3), (126, 1), (128, 1), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 827.1237 - loglik: -7.3892e+02 - logprior: -8.8201e+01
Epoch 2/2
10/10 - 2s - loss: 738.7471 - loglik: -7.0687e+02 - logprior: -3.1877e+01
Fitted a model with MAP estimate = -725.5658
expansions: [(0, 3), (19, 1), (21, 1), (90, 1)]
discards: [  0  47  48  57  94 124]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 777.7294 - loglik: -7.0860e+02 - logprior: -6.9127e+01
Epoch 2/2
10/10 - 2s - loss: 706.3271 - loglik: -6.9460e+02 - logprior: -1.1722e+01
Fitted a model with MAP estimate = -695.4377
expansions: []
discards: [  0   1 113]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 788.1164 - loglik: -7.0311e+02 - logprior: -8.5007e+01
Epoch 2/10
10/10 - 2s - loss: 719.5175 - loglik: -6.9414e+02 - logprior: -2.5373e+01
Epoch 3/10
10/10 - 2s - loss: 696.5320 - loglik: -6.9232e+02 - logprior: -4.2081e+00
Epoch 4/10
10/10 - 2s - loss: 685.3453 - loglik: -6.9112e+02 - logprior: 5.7791
Epoch 5/10
10/10 - 2s - loss: 678.5326 - loglik: -6.8789e+02 - logprior: 9.3543
Epoch 6/10
10/10 - 2s - loss: 676.6031 - loglik: -6.8774e+02 - logprior: 11.1407
Epoch 7/10
10/10 - 2s - loss: 674.8732 - loglik: -6.8710e+02 - logprior: 12.2304
Epoch 8/10
10/10 - 2s - loss: 673.3918 - loglik: -6.8647e+02 - logprior: 13.0765
Epoch 9/10
10/10 - 2s - loss: 675.2886 - loglik: -6.8909e+02 - logprior: 13.8012
Fitted a model with MAP estimate = -672.4058
Time for alignment: 69.2361
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1031.2455 - loglik: -9.5346e+02 - logprior: -7.7783e+01
Epoch 2/10
10/10 - 2s - loss: 889.1118 - loglik: -8.7385e+02 - logprior: -1.5258e+01
Epoch 3/10
10/10 - 2s - loss: 813.5477 - loglik: -8.0803e+02 - logprior: -5.5130e+00
Epoch 4/10
10/10 - 2s - loss: 762.8195 - loglik: -7.5892e+02 - logprior: -3.8969e+00
Epoch 5/10
10/10 - 2s - loss: 743.9512 - loglik: -7.4100e+02 - logprior: -2.9552e+00
Epoch 6/10
10/10 - 2s - loss: 736.5774 - loglik: -7.3497e+02 - logprior: -1.6042e+00
Epoch 7/10
10/10 - 2s - loss: 732.6728 - loglik: -7.3171e+02 - logprior: -9.6225e-01
Epoch 8/10
10/10 - 2s - loss: 732.0649 - loglik: -7.3156e+02 - logprior: -5.0477e-01
Epoch 9/10
10/10 - 2s - loss: 730.0513 - loglik: -7.2973e+02 - logprior: -3.1960e-01
Epoch 10/10
10/10 - 2s - loss: 728.4941 - loglik: -7.2848e+02 - logprior: -1.5174e-02
Fitted a model with MAP estimate = -728.7126
expansions: [(17, 3), (18, 2), (19, 1), (21, 1), (23, 2), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (53, 1), (58, 1), (77, 1), (78, 3), (79, 1), (90, 2), (91, 1), (94, 1), (100, 2), (110, 1), (118, 2), (124, 3), (126, 1), (128, 1), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 823.9109 - loglik: -7.3577e+02 - logprior: -8.8136e+01
Epoch 2/2
10/10 - 2s - loss: 734.9313 - loglik: -7.0383e+02 - logprior: -3.1102e+01
Fitted a model with MAP estimate = -720.5765
expansions: [(0, 3), (91, 1)]
discards: [  0  49  95 125]
Re-initialized the encoder parameters.
Fitting a model of length 184 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 774.2939 - loglik: -7.0542e+02 - logprior: -6.8874e+01
Epoch 2/2
10/10 - 2s - loss: 703.2698 - loglik: -6.9175e+02 - logprior: -1.1518e+01
Fitted a model with MAP estimate = -693.6420
expansions: []
discards: [  0   1 114]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 787.6654 - loglik: -7.0288e+02 - logprior: -8.4783e+01
Epoch 2/10
10/10 - 2s - loss: 718.1767 - loglik: -6.9309e+02 - logprior: -2.5091e+01
Epoch 3/10
10/10 - 2s - loss: 695.5490 - loglik: -6.9157e+02 - logprior: -3.9750e+00
Epoch 4/10
10/10 - 2s - loss: 683.2310 - loglik: -6.8926e+02 - logprior: 6.0306
Epoch 5/10
10/10 - 2s - loss: 678.7621 - loglik: -6.8836e+02 - logprior: 9.5964
Epoch 6/10
10/10 - 2s - loss: 675.5664 - loglik: -6.8697e+02 - logprior: 11.4043
Epoch 7/10
10/10 - 2s - loss: 674.4340 - loglik: -6.8694e+02 - logprior: 12.5024
Epoch 8/10
10/10 - 2s - loss: 673.8308 - loglik: -6.8717e+02 - logprior: 13.3422
Epoch 9/10
10/10 - 2s - loss: 673.3261 - loglik: -6.8742e+02 - logprior: 14.0990
Epoch 10/10
10/10 - 2s - loss: 670.0593 - loglik: -6.8487e+02 - logprior: 14.8066
Fitted a model with MAP estimate = -671.0856
Time for alignment: 67.6577
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 1030.8767 - loglik: -9.5309e+02 - logprior: -7.7782e+01
Epoch 2/10
10/10 - 2s - loss: 888.4932 - loglik: -8.7323e+02 - logprior: -1.5262e+01
Epoch 3/10
10/10 - 2s - loss: 812.0766 - loglik: -8.0663e+02 - logprior: -5.4478e+00
Epoch 4/10
10/10 - 2s - loss: 762.9626 - loglik: -7.5953e+02 - logprior: -3.4322e+00
Epoch 5/10
10/10 - 2s - loss: 743.0051 - loglik: -7.4063e+02 - logprior: -2.3757e+00
Epoch 6/10
10/10 - 2s - loss: 736.1586 - loglik: -7.3514e+02 - logprior: -1.0166e+00
Epoch 7/10
10/10 - 2s - loss: 733.6929 - loglik: -7.3351e+02 - logprior: -1.8683e-01
Epoch 8/10
10/10 - 2s - loss: 731.2501 - loglik: -7.3154e+02 - logprior: 0.2879
Epoch 9/10
10/10 - 2s - loss: 729.9633 - loglik: -7.3066e+02 - logprior: 0.6940
Epoch 10/10
10/10 - 2s - loss: 729.2490 - loglik: -7.3032e+02 - logprior: 1.0705
Fitted a model with MAP estimate = -729.4805
expansions: [(20, 1), (22, 1), (26, 1), (39, 2), (40, 4), (44, 1), (45, 2), (58, 1), (77, 1), (78, 3), (79, 1), (90, 2), (91, 1), (94, 1), (100, 2), (110, 1), (118, 2), (124, 3), (126, 1), (128, 1), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 829.7461 - loglik: -7.4129e+02 - logprior: -8.8458e+01
Epoch 2/2
10/10 - 2s - loss: 743.6202 - loglik: -7.1169e+02 - logprior: -3.1929e+01
Fitted a model with MAP estimate = -730.5190
expansions: [(0, 3), (15, 3), (87, 1)]
discards: [  0  42  44  45  54  91 121]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 784.0562 - loglik: -7.1458e+02 - logprior: -6.9473e+01
Epoch 2/2
10/10 - 2s - loss: 710.5609 - loglik: -6.9834e+02 - logprior: -1.2216e+01
Fitted a model with MAP estimate = -700.6722
expansions: [(16, 1), (17, 1), (18, 2)]
discards: [  0   1 110]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 792.0214 - loglik: -7.0698e+02 - logprior: -8.5046e+01
Epoch 2/10
10/10 - 2s - loss: 719.1973 - loglik: -6.9291e+02 - logprior: -2.6286e+01
Epoch 3/10
10/10 - 2s - loss: 697.7405 - loglik: -6.9213e+02 - logprior: -5.6101e+00
Epoch 4/10
10/10 - 2s - loss: 684.2623 - loglik: -6.8991e+02 - logprior: 5.6466
Epoch 5/10
10/10 - 2s - loss: 679.4014 - loglik: -6.8889e+02 - logprior: 9.4857
Epoch 6/10
10/10 - 2s - loss: 675.7361 - loglik: -6.8703e+02 - logprior: 11.2927
Epoch 7/10
10/10 - 2s - loss: 674.6160 - loglik: -6.8699e+02 - logprior: 12.3746
Epoch 8/10
10/10 - 2s - loss: 673.9000 - loglik: -6.8710e+02 - logprior: 13.2028
Epoch 9/10
10/10 - 2s - loss: 671.0115 - loglik: -6.8495e+02 - logprior: 13.9436
Epoch 10/10
10/10 - 2s - loss: 672.5689 - loglik: -6.8721e+02 - logprior: 14.6428
Fitted a model with MAP estimate = -671.0754
Time for alignment: 69.2688
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1031.3600 - loglik: -9.5358e+02 - logprior: -7.7782e+01
Epoch 2/10
10/10 - 2s - loss: 888.7150 - loglik: -8.7346e+02 - logprior: -1.5257e+01
Epoch 3/10
10/10 - 2s - loss: 812.2112 - loglik: -8.0672e+02 - logprior: -5.4945e+00
Epoch 4/10
10/10 - 2s - loss: 762.4098 - loglik: -7.5857e+02 - logprior: -3.8437e+00
Epoch 5/10
10/10 - 2s - loss: 742.6257 - loglik: -7.3967e+02 - logprior: -2.9581e+00
Epoch 6/10
10/10 - 2s - loss: 735.0620 - loglik: -7.3342e+02 - logprior: -1.6379e+00
Epoch 7/10
10/10 - 2s - loss: 732.7581 - loglik: -7.3205e+02 - logprior: -7.0762e-01
Epoch 8/10
10/10 - 2s - loss: 730.2566 - loglik: -7.3010e+02 - logprior: -1.6071e-01
Epoch 9/10
10/10 - 2s - loss: 731.1131 - loglik: -7.3133e+02 - logprior: 0.2133
Fitted a model with MAP estimate = -729.7813
expansions: [(20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (53, 1), (58, 1), (77, 1), (78, 3), (79, 1), (90, 2), (91, 1), (94, 1), (100, 2), (110, 1), (118, 2), (124, 3), (126, 1), (128, 2), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 829.2503 - loglik: -7.4056e+02 - logprior: -8.8693e+01
Epoch 2/2
10/10 - 2s - loss: 742.9712 - loglik: -7.1112e+02 - logprior: -3.1846e+01
Fitted a model with MAP estimate = -730.1344
expansions: [(0, 3), (84, 1)]
discards: [  0  42  88 118 156]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 783.7329 - loglik: -7.1418e+02 - logprior: -6.9554e+01
Epoch 2/2
10/10 - 2s - loss: 713.7472 - loglik: -7.0158e+02 - logprior: -1.2170e+01
Fitted a model with MAP estimate = -704.1172
expansions: [(16, 1), (17, 4)]
discards: [  0   1 107]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 796.2572 - loglik: -7.1100e+02 - logprior: -8.5261e+01
Epoch 2/10
10/10 - 2s - loss: 725.1532 - loglik: -6.9841e+02 - logprior: -2.6738e+01
Epoch 3/10
10/10 - 2s - loss: 701.8324 - loglik: -6.9560e+02 - logprior: -6.2280e+00
Epoch 4/10
10/10 - 2s - loss: 688.4855 - loglik: -6.9379e+02 - logprior: 5.3034
Epoch 5/10
10/10 - 2s - loss: 680.8836 - loglik: -6.9006e+02 - logprior: 9.1735
Epoch 6/10
10/10 - 2s - loss: 679.8116 - loglik: -6.9080e+02 - logprior: 10.9888
Epoch 7/10
10/10 - 2s - loss: 677.4116 - loglik: -6.8945e+02 - logprior: 12.0404
Epoch 8/10
10/10 - 2s - loss: 676.0756 - loglik: -6.8894e+02 - logprior: 12.8636
Epoch 9/10
10/10 - 2s - loss: 673.1159 - loglik: -6.8672e+02 - logprior: 13.6010
Epoch 10/10
10/10 - 2s - loss: 672.8377 - loglik: -6.8713e+02 - logprior: 14.2945
Fitted a model with MAP estimate = -673.1357
Time for alignment: 64.1902
Fitting a model of length 145 on 521 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1030.4062 - loglik: -9.5262e+02 - logprior: -7.7783e+01
Epoch 2/10
10/10 - 2s - loss: 889.8630 - loglik: -8.7461e+02 - logprior: -1.5256e+01
Epoch 3/10
10/10 - 2s - loss: 811.3828 - loglik: -8.0589e+02 - logprior: -5.4920e+00
Epoch 4/10
10/10 - 2s - loss: 761.9518 - loglik: -7.5812e+02 - logprior: -3.8313e+00
Epoch 5/10
10/10 - 2s - loss: 743.2452 - loglik: -7.4028e+02 - logprior: -2.9648e+00
Epoch 6/10
10/10 - 2s - loss: 735.8271 - loglik: -7.3421e+02 - logprior: -1.6194e+00
Epoch 7/10
10/10 - 2s - loss: 732.3365 - loglik: -7.3164e+02 - logprior: -6.9220e-01
Epoch 8/10
10/10 - 2s - loss: 731.0460 - loglik: -7.3086e+02 - logprior: -1.8731e-01
Epoch 9/10
10/10 - 2s - loss: 730.1769 - loglik: -7.3039e+02 - logprior: 0.2099
Epoch 10/10
10/10 - 2s - loss: 729.1288 - loglik: -7.2975e+02 - logprior: 0.6180
Fitted a model with MAP estimate = -729.2175
expansions: [(20, 1), (22, 1), (26, 1), (39, 2), (40, 1), (41, 1), (44, 1), (53, 1), (58, 1), (77, 1), (78, 3), (79, 1), (90, 2), (91, 1), (94, 1), (100, 2), (110, 1), (118, 2), (124, 3), (126, 1), (128, 1), (129, 1), (133, 1), (137, 1), (138, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 827.6833 - loglik: -7.3909e+02 - logprior: -8.8592e+01
Epoch 2/2
10/10 - 2s - loss: 744.4139 - loglik: -7.1265e+02 - logprior: -3.1761e+01
Fitted a model with MAP estimate = -730.0965
expansions: [(0, 3), (84, 1)]
discards: [  0  42  88 118]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 784.3276 - loglik: -7.1480e+02 - logprior: -6.9527e+01
Epoch 2/2
10/10 - 2s - loss: 713.0524 - loglik: -7.0094e+02 - logprior: -1.2116e+01
Fitted a model with MAP estimate = -703.9315
expansions: [(16, 1), (17, 4)]
discards: [  0   1 107]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 521 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 796.1293 - loglik: -7.1096e+02 - logprior: -8.5165e+01
Epoch 2/10
10/10 - 2s - loss: 725.2622 - loglik: -6.9867e+02 - logprior: -2.6595e+01
Epoch 3/10
10/10 - 2s - loss: 701.1486 - loglik: -6.9513e+02 - logprior: -6.0182e+00
Epoch 4/10
10/10 - 2s - loss: 686.9010 - loglik: -6.9231e+02 - logprior: 5.4047
Epoch 5/10
10/10 - 2s - loss: 682.2959 - loglik: -6.9158e+02 - logprior: 9.2816
Epoch 6/10
10/10 - 2s - loss: 678.9587 - loglik: -6.9000e+02 - logprior: 11.0407
Epoch 7/10
10/10 - 2s - loss: 677.1452 - loglik: -6.8927e+02 - logprior: 12.1293
Epoch 8/10
10/10 - 2s - loss: 675.9969 - loglik: -6.8895e+02 - logprior: 12.9493
Epoch 9/10
10/10 - 2s - loss: 673.0328 - loglik: -6.8673e+02 - logprior: 13.6991
Epoch 10/10
10/10 - 2s - loss: 673.1796 - loglik: -6.8758e+02 - logprior: 14.3981
Fitted a model with MAP estimate = -673.0566
Time for alignment: 67.1616
Computed alignments with likelihoods: ['-672.4058', '-671.0856', '-671.0754', '-673.1357', '-673.0566']
Best model has likelihood: -671.0754  (prior= 14.9822 )
time for generating output: 0.1729
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf11.projection.fasta
SP score = 0.9136771300448431
Training of 5 independent models on file subt.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe952475820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b555160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cd7f0280>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 23s - loss: 1665.9534 - loglik: -1.6639e+03 - logprior: -2.0320e+00
Epoch 2/10
33/33 - 20s - loss: 1571.5023 - loglik: -1.5709e+03 - logprior: -6.0267e-01
Epoch 3/10
33/33 - 21s - loss: 1569.3535 - loglik: -1.5689e+03 - logprior: -4.4257e-01
Epoch 4/10
33/33 - 21s - loss: 1558.7046 - loglik: -1.5583e+03 - logprior: -3.8445e-01
Epoch 5/10
33/33 - 21s - loss: 1552.1934 - loglik: -1.5518e+03 - logprior: -3.5036e-01
Epoch 6/10
33/33 - 21s - loss: 1544.9834 - loglik: -1.5446e+03 - logprior: -3.2964e-01
Epoch 7/10
33/33 - 21s - loss: 1512.2144 - loglik: -1.5113e+03 - logprior: -8.9451e-01
Epoch 8/10
33/33 - 20s - loss: 1328.4637 - loglik: -1.3241e+03 - logprior: -4.2853e+00
Epoch 9/10
33/33 - 21s - loss: 1228.3606 - loglik: -1.2237e+03 - logprior: -4.5914e+00
Epoch 10/10
33/33 - 21s - loss: 1212.8575 - loglik: -1.2084e+03 - logprior: -4.3969e+00
Fitted a model with MAP estimate = -1203.3431
expansions: [(0, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  67  68  69  70  71  72  73 101 102 103 104 105 121 129 130
 131 132 186 187 188 189 190 191 192 193 194 195 196 229]
Re-initialized the encoder parameters.
Fitting a model of length 146 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 15s - loss: 1752.4938 - loglik: -1.7495e+03 - logprior: -2.9594e+00
Epoch 2/2
33/33 - 12s - loss: 1726.7318 - loglik: -1.7266e+03 - logprior: -1.0520e-01
Fitted a model with MAP estimate = -1711.3511
expansions: [(0, 79), (146, 162)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37
  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55
  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73
  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91
  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109
 110 111 112 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128
 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145]
Re-initialized the encoder parameters.
Fitting a model of length 244 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 25s - loss: 1660.6466 - loglik: -1.6578e+03 - logprior: -2.8478e+00
Epoch 2/2
33/33 - 22s - loss: 1568.2709 - loglik: -1.5677e+03 - logprior: -5.2594e-01
Fitted a model with MAP estimate = -1560.3277
expansions: [(0, 5), (7, 1), (9, 2), (26, 1), (34, 3), (59, 2), (61, 2), (62, 2), (80, 1), (173, 5), (218, 1), (244, 4)]
discards: [  3 109 110 189 238 239 240 241 242]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 28s - loss: 1566.1124 - loglik: -1.5628e+03 - logprior: -3.3421e+00
Epoch 2/10
33/33 - 25s - loss: 1557.3730 - loglik: -1.5569e+03 - logprior: -4.6050e-01
Epoch 3/10
33/33 - 25s - loss: 1545.9297 - loglik: -1.5457e+03 - logprior: -2.5236e-01
Epoch 4/10
33/33 - 25s - loss: 1550.9720 - loglik: -1.5508e+03 - logprior: -1.7672e-01
Fitted a model with MAP estimate = -1544.7928
Time for alignment: 465.7185
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 24s - loss: 1665.8176 - loglik: -1.6638e+03 - logprior: -2.0171e+00
Epoch 2/10
33/33 - 21s - loss: 1579.8113 - loglik: -1.5792e+03 - logprior: -6.0500e-01
Epoch 3/10
33/33 - 21s - loss: 1560.0853 - loglik: -1.5596e+03 - logprior: -5.1678e-01
Epoch 4/10
33/33 - 21s - loss: 1562.4773 - loglik: -1.5621e+03 - logprior: -4.2260e-01
Fitted a model with MAP estimate = -1557.0668
expansions: [(0, 5), (5, 1), (8, 1), (9, 1), (34, 2), (71, 1), (72, 1), (73, 3), (77, 1), (108, 1), (112, 1), (115, 1), (117, 1), (131, 2), (154, 3), (183, 1), (210, 1), (216, 1), (221, 3), (230, 3)]
discards: [225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 1566.3322 - loglik: -1.5636e+03 - logprior: -2.7618e+00
Epoch 2/2
33/33 - 25s - loss: 1553.0272 - loglik: -1.5528e+03 - logprior: -2.7306e-01
Fitted a model with MAP estimate = -1553.4696
expansions: [(126, 4), (260, 4)]
discards: [  1  16 177 254 255 256 257 258 259]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 28s - loss: 1560.8734 - loglik: -1.5588e+03 - logprior: -2.0273e+00
Epoch 2/2
33/33 - 24s - loss: 1554.6903 - loglik: -1.5546e+03 - logprior: -1.0529e-01
Fitted a model with MAP estimate = -1553.2011
expansions: [(0, 4), (252, 1), (254, 1), (259, 4)]
discards: [  5   6   7 124 125 126 255 256 257 258]
Re-initialized the encoder parameters.
Fitting a model of length 259 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 1561.5361 - loglik: -1.5585e+03 - logprior: -3.0852e+00
Epoch 2/10
33/33 - 24s - loss: 1554.5304 - loglik: -1.5545e+03 - logprior: -3.3111e-02
Epoch 3/10
33/33 - 24s - loss: 1555.9899 - loglik: -1.5562e+03 - logprior: 0.1669
Fitted a model with MAP estimate = -1549.8955
Time for alignment: 362.4406
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 24s - loss: 1668.3105 - loglik: -1.6663e+03 - logprior: -2.0220e+00
Epoch 2/10
33/33 - 21s - loss: 1572.2273 - loglik: -1.5717e+03 - logprior: -5.2988e-01
Epoch 3/10
33/33 - 21s - loss: 1562.3009 - loglik: -1.5618e+03 - logprior: -4.7767e-01
Epoch 4/10
33/33 - 20s - loss: 1558.8419 - loglik: -1.5584e+03 - logprior: -4.2586e-01
Epoch 5/10
33/33 - 21s - loss: 1549.2268 - loglik: -1.5488e+03 - logprior: -3.8616e-01
Epoch 6/10
33/33 - 21s - loss: 1547.9056 - loglik: -1.5475e+03 - logprior: -3.5859e-01
Epoch 7/10
33/33 - 21s - loss: 1507.2793 - loglik: -1.5064e+03 - logprior: -8.3327e-01
Epoch 8/10
33/33 - 20s - loss: 1330.0663 - loglik: -1.3262e+03 - logprior: -3.8685e+00
Epoch 9/10
33/33 - 21s - loss: 1229.8809 - loglik: -1.2253e+03 - logprior: -4.5327e+00
Epoch 10/10
33/33 - 21s - loss: 1204.2819 - loglik: -1.1999e+03 - logprior: -4.3332e+00
Fitted a model with MAP estimate = -1201.8309
expansions: []
discards: [  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  28  29  30
  31  32  48  49  50  51  52  53  54  55  56  62  63  64  65 101 102 103
 104 105 120 128 129 130 131 184 185 186 187 188 189 190 191 192 193 194
 195 196 229]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 17s - loss: 1744.7214 - loglik: -1.7420e+03 - logprior: -2.7483e+00
Epoch 2/2
33/33 - 14s - loss: 1730.0929 - loglik: -1.7294e+03 - logprior: -7.1562e-01
Fitted a model with MAP estimate = -1698.0862
expansions: [(0, 80), (173, 141)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  28  29  30  44  48  49  50  51  52  53  54  55  68  69  82 112 113 114
 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132
 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150
 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168
 169 170 171 172]
Re-initialized the encoder parameters.
Fitting a model of length 300 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 34s - loss: 1658.8931 - loglik: -1.6560e+03 - logprior: -2.9170e+00
Epoch 2/2
33/33 - 30s - loss: 1575.8270 - loglik: -1.5747e+03 - logprior: -1.1101e+00
Fitted a model with MAP estimate = -1569.1189
expansions: [(70, 2), (71, 2), (90, 3), (103, 10), (136, 8), (165, 1)]
discards: [  1   5   6   7   8  38  39  40  41  42  43  44  45  46  47  48  49  50
  51  52  53  54  55  56  57  58  80  81  82  83 130 212 213 214 215 216
 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234
 235 236 237 238 239 240 241 242 243 244 245 246 295 296 297 298 299]
Re-initialized the encoder parameters.
Fitting a model of length 255 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 27s - loss: 1574.7208 - loglik: -1.5727e+03 - logprior: -2.0676e+00
Epoch 2/10
33/33 - 24s - loss: 1567.4388 - loglik: -1.5674e+03 - logprior: -2.6988e-02
Epoch 3/10
33/33 - 24s - loss: 1561.3340 - loglik: -1.5616e+03 - logprior: 0.2500
Epoch 4/10
33/33 - 24s - loss: 1553.3684 - loglik: -1.5538e+03 - logprior: 0.4081
Epoch 5/10
33/33 - 24s - loss: 1545.7072 - loglik: -1.5462e+03 - logprior: 0.5239
Epoch 6/10
33/33 - 24s - loss: 1537.1915 - loglik: -1.5378e+03 - logprior: 0.6375
Epoch 7/10
33/33 - 24s - loss: 1505.8385 - loglik: -1.5060e+03 - logprior: 0.1947
Epoch 8/10
33/33 - 24s - loss: 1360.3271 - loglik: -1.3583e+03 - logprior: -1.9515e+00
Epoch 9/10
33/33 - 24s - loss: 1223.8217 - loglik: -1.2201e+03 - logprior: -3.6507e+00
Epoch 10/10
33/33 - 24s - loss: 1196.9396 - loglik: -1.1940e+03 - logprior: -2.9112e+00
Fitted a model with MAP estimate = -1191.9500
Time for alignment: 636.4154
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 23s - loss: 1665.8398 - loglik: -1.6638e+03 - logprior: -2.0006e+00
Epoch 2/10
33/33 - 21s - loss: 1577.8925 - loglik: -1.5773e+03 - logprior: -5.4411e-01
Epoch 3/10
33/33 - 21s - loss: 1562.0040 - loglik: -1.5616e+03 - logprior: -4.4845e-01
Epoch 4/10
33/33 - 20s - loss: 1559.0217 - loglik: -1.5586e+03 - logprior: -3.9480e-01
Epoch 5/10
33/33 - 21s - loss: 1555.6072 - loglik: -1.5552e+03 - logprior: -3.5756e-01
Epoch 6/10
33/33 - 20s - loss: 1538.2511 - loglik: -1.5379e+03 - logprior: -3.1496e-01
Epoch 7/10
33/33 - 21s - loss: 1504.7756 - loglik: -1.5038e+03 - logprior: -9.6682e-01
Epoch 8/10
33/33 - 21s - loss: 1316.1069 - loglik: -1.3119e+03 - logprior: -4.1488e+00
Epoch 9/10
33/33 - 21s - loss: 1226.7563 - loglik: -1.2222e+03 - logprior: -4.4918e+00
Epoch 10/10
33/33 - 21s - loss: 1205.7867 - loglik: -1.2013e+03 - logprior: -4.4190e+00
Fitted a model with MAP estimate = -1203.5127
expansions: [(0, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  67  68  69  70  71  72 102 103 104 105 106 121 129 130 131
 132 185 186 187 188 189 190 191 192 193 194 195 196 221 229]
Re-initialized the encoder parameters.
Fitting a model of length 144 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 16s - loss: 1746.9531 - loglik: -1.7442e+03 - logprior: -2.7053e+00
Epoch 2/2
33/33 - 12s - loss: 1730.3358 - loglik: -1.7302e+03 - logprior: -1.7288e-01
Fitted a model with MAP estimate = -1697.5269
expansions: [(0, 96), (11, 9), (40, 9), (61, 6), (113, 3), (144, 74)]
discards: [ 86  87  88  89  90  91  92 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143]
Re-initialized the encoder parameters.
Fitting a model of length 307 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 34s - loss: 1642.7255 - loglik: -1.6391e+03 - logprior: -3.5995e+00
Epoch 2/2
33/33 - 31s - loss: 1581.7625 - loglik: -1.5803e+03 - logprior: -1.4176e+00
Fitted a model with MAP estimate = -1572.0882
expansions: [(0, 5), (101, 6), (152, 2), (153, 5), (178, 2), (179, 6), (203, 36), (205, 5)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
 111 181 182 183 184 185 186 275 276 277 278 279 280 281 282 283 284 285
 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303
 304 305 306]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 33s - loss: 1577.6487 - loglik: -1.5749e+03 - logprior: -2.7594e+00
Epoch 2/10
33/33 - 30s - loss: 1562.5145 - loglik: -1.5622e+03 - logprior: -3.2552e-01
Epoch 3/10
33/33 - 30s - loss: 1552.0229 - loglik: -1.5520e+03 - logprior: -2.1905e-02
Epoch 4/10
33/33 - 30s - loss: 1546.3105 - loglik: -1.5464e+03 - logprior: 0.1398
Epoch 5/10
33/33 - 30s - loss: 1541.7048 - loglik: -1.5420e+03 - logprior: 0.2919
Epoch 6/10
33/33 - 30s - loss: 1533.7562 - loglik: -1.5342e+03 - logprior: 0.4278
Epoch 7/10
33/33 - 30s - loss: 1494.9028 - loglik: -1.4950e+03 - logprior: 0.1621
Epoch 8/10
33/33 - 30s - loss: 1347.7123 - loglik: -1.3464e+03 - logprior: -1.2675e+00
Epoch 9/10
33/33 - 30s - loss: 1225.7727 - loglik: -1.2237e+03 - logprior: -2.0663e+00
Epoch 10/10
33/33 - 30s - loss: 1199.1743 - loglik: -1.1976e+03 - logprior: -1.5257e+00
Fitted a model with MAP estimate = -1192.5875
Time for alignment: 695.3811
Fitting a model of length 230 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 25s - loss: 1669.7606 - loglik: -1.6677e+03 - logprior: -2.0200e+00
Epoch 2/10
33/33 - 21s - loss: 1574.6733 - loglik: -1.5741e+03 - logprior: -5.9866e-01
Epoch 3/10
33/33 - 21s - loss: 1557.2009 - loglik: -1.5567e+03 - logprior: -4.6722e-01
Epoch 4/10
33/33 - 21s - loss: 1560.1932 - loglik: -1.5598e+03 - logprior: -4.1107e-01
Fitted a model with MAP estimate = -1555.8274
expansions: [(0, 5), (9, 1), (34, 6), (61, 1), (64, 2), (66, 1), (72, 1), (74, 1), (79, 2), (88, 1), (108, 1), (110, 1), (112, 1), (115, 1), (132, 2), (155, 3), (163, 4), (204, 1), (210, 1), (220, 2), (221, 3), (230, 3)]
discards: [225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 29s - loss: 1563.4253 - loglik: -1.5606e+03 - logprior: -2.7965e+00
Epoch 2/2
33/33 - 26s - loss: 1552.3192 - loglik: -1.5519e+03 - logprior: -4.5239e-01
Fitted a model with MAP estimate = -1551.1850
expansions: [(270, 4)]
discards: [  1  15  41  42 183 194 195 196 257 265 266 267 268 269]
Re-initialized the encoder parameters.
Fitting a model of length 260 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 27s - loss: 1560.2532 - loglik: -1.5582e+03 - logprior: -2.0407e+00
Epoch 2/2
33/33 - 25s - loss: 1556.2524 - loglik: -1.5562e+03 - logprior: -1.0144e-01
Fitted a model with MAP estimate = -1552.4126
expansions: [(0, 4), (254, 1), (260, 4)]
discards: [  4   5   6  71 256 257 258]
Re-initialized the encoder parameters.
Fitting a model of length 262 on 7517 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 29s - loss: 1560.8652 - loglik: -1.5578e+03 - logprior: -3.0332e+00
Epoch 2/10
33/33 - 25s - loss: 1559.2229 - loglik: -1.5592e+03 - logprior: -5.4773e-02
Epoch 3/10
33/33 - 25s - loss: 1542.5759 - loglik: -1.5427e+03 - logprior: 0.1516
Epoch 4/10
33/33 - 25s - loss: 1554.9315 - loglik: -1.5552e+03 - logprior: 0.2369
Fitted a model with MAP estimate = -1545.2312
Time for alignment: 393.3614
Computed alignments with likelihoods: ['-1203.3431', '-1549.8955', '-1191.9500', '-1192.5875', '-1545.2312']
Best model has likelihood: -1191.9500  (prior= -2.6445 )
time for generating output: 0.3002
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/subt.projection.fasta
SP score = 0.12026802984249793
Training of 5 independent models on file profilin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea11e301c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f8715a30>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cde44970>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 766.9534 - loglik: -7.0502e+02 - logprior: -6.1929e+01
Epoch 2/10
10/10 - 1s - loss: 677.1885 - loglik: -6.6286e+02 - logprior: -1.4333e+01
Epoch 3/10
10/10 - 1s - loss: 634.1246 - loglik: -6.2792e+02 - logprior: -6.2092e+00
Epoch 4/10
10/10 - 1s - loss: 611.7340 - loglik: -6.0842e+02 - logprior: -3.3151e+00
Epoch 5/10
10/10 - 1s - loss: 598.0010 - loglik: -5.9615e+02 - logprior: -1.8504e+00
Epoch 6/10
10/10 - 1s - loss: 590.2552 - loglik: -5.8920e+02 - logprior: -1.0501e+00
Epoch 7/10
10/10 - 1s - loss: 586.8421 - loglik: -5.8627e+02 - logprior: -5.7472e-01
Epoch 8/10
10/10 - 1s - loss: 583.9047 - loglik: -5.8342e+02 - logprior: -4.8352e-01
Epoch 9/10
10/10 - 1s - loss: 581.8533 - loglik: -5.8132e+02 - logprior: -5.3406e-01
Epoch 10/10
10/10 - 1s - loss: 582.1218 - loglik: -5.8164e+02 - logprior: -4.7778e-01
Fitted a model with MAP estimate = -581.3887
expansions: [(8, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 1), (40, 1), (45, 1), (51, 1), (63, 1), (64, 1), (71, 3), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 662.6703 - loglik: -5.9330e+02 - logprior: -6.9374e+01
Epoch 2/2
10/10 - 1s - loss: 598.4662 - loglik: -5.7222e+02 - logprior: -2.6246e+01
Fitted a model with MAP estimate = -587.5754
expansions: [(0, 8)]
discards: [  0  13  87 109]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 624.5356 - loglik: -5.6982e+02 - logprior: -5.4713e+01
Epoch 2/2
10/10 - 2s - loss: 576.8131 - loglik: -5.6494e+02 - logprior: -1.1870e+01
Fitted a model with MAP estimate = -569.0506
expansions: []
discards: [1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 619.4394 - loglik: -5.6588e+02 - logprior: -5.3555e+01
Epoch 2/10
10/10 - 1s - loss: 574.7191 - loglik: -5.6348e+02 - logprior: -1.1240e+01
Epoch 3/10
10/10 - 1s - loss: 565.9220 - loglik: -5.6360e+02 - logprior: -2.3215e+00
Epoch 4/10
10/10 - 1s - loss: 560.6463 - loglik: -5.6204e+02 - logprior: 1.3978
Epoch 5/10
10/10 - 1s - loss: 556.9749 - loglik: -5.6038e+02 - logprior: 3.4035
Epoch 6/10
10/10 - 1s - loss: 553.0385 - loglik: -5.5752e+02 - logprior: 4.4817
Epoch 7/10
10/10 - 1s - loss: 549.7167 - loglik: -5.5469e+02 - logprior: 4.9727
Epoch 8/10
10/10 - 1s - loss: 549.3412 - loglik: -5.5477e+02 - logprior: 5.4323
Epoch 9/10
10/10 - 1s - loss: 547.3676 - loglik: -5.5331e+02 - logprior: 5.9476
Epoch 10/10
10/10 - 1s - loss: 546.7777 - loglik: -5.5320e+02 - logprior: 6.4279
Fitted a model with MAP estimate = -545.9876
Time for alignment: 49.6758
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 766.8699 - loglik: -7.0494e+02 - logprior: -6.1930e+01
Epoch 2/10
10/10 - 1s - loss: 676.8829 - loglik: -6.6256e+02 - logprior: -1.4323e+01
Epoch 3/10
10/10 - 1s - loss: 632.2594 - loglik: -6.2613e+02 - logprior: -6.1264e+00
Epoch 4/10
10/10 - 1s - loss: 608.6140 - loglik: -6.0529e+02 - logprior: -3.3230e+00
Epoch 5/10
10/10 - 1s - loss: 596.5616 - loglik: -5.9461e+02 - logprior: -1.9550e+00
Epoch 6/10
10/10 - 1s - loss: 588.8736 - loglik: -5.8768e+02 - logprior: -1.1886e+00
Epoch 7/10
10/10 - 1s - loss: 585.5110 - loglik: -5.8484e+02 - logprior: -6.7185e-01
Epoch 8/10
10/10 - 1s - loss: 582.2298 - loglik: -5.8166e+02 - logprior: -5.6736e-01
Epoch 9/10
10/10 - 1s - loss: 582.3651 - loglik: -5.8185e+02 - logprior: -5.1485e-01
Fitted a model with MAP estimate = -581.6076
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (54, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 661.3788 - loglik: -5.9201e+02 - logprior: -6.9367e+01
Epoch 2/2
10/10 - 1s - loss: 599.0711 - loglik: -5.7303e+02 - logprior: -2.6039e+01
Fitted a model with MAP estimate = -587.8716
expansions: [(0, 7)]
discards: [  0 108]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 624.1540 - loglik: -5.6942e+02 - logprior: -5.4731e+01
Epoch 2/2
10/10 - 2s - loss: 575.3526 - loglik: -5.6350e+02 - logprior: -1.1856e+01
Fitted a model with MAP estimate = -568.7045
expansions: []
discards: [ 1  2  3  4  5  6 19]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 619.6534 - loglik: -5.6615e+02 - logprior: -5.3503e+01
Epoch 2/10
10/10 - 1s - loss: 576.0188 - loglik: -5.6479e+02 - logprior: -1.1227e+01
Epoch 3/10
10/10 - 1s - loss: 566.6287 - loglik: -5.6430e+02 - logprior: -2.3313e+00
Epoch 4/10
10/10 - 1s - loss: 561.9460 - loglik: -5.6337e+02 - logprior: 1.4278
Epoch 5/10
10/10 - 1s - loss: 556.4510 - loglik: -5.5991e+02 - logprior: 3.4598
Epoch 6/10
10/10 - 1s - loss: 552.2149 - loglik: -5.5672e+02 - logprior: 4.5071
Epoch 7/10
10/10 - 1s - loss: 549.7156 - loglik: -5.5480e+02 - logprior: 5.0824
Epoch 8/10
10/10 - 1s - loss: 547.5197 - loglik: -5.5299e+02 - logprior: 5.4694
Epoch 9/10
10/10 - 1s - loss: 547.1071 - loglik: -5.5303e+02 - logprior: 5.9291
Epoch 10/10
10/10 - 1s - loss: 545.9511 - loglik: -5.5230e+02 - logprior: 6.3500
Fitted a model with MAP estimate = -545.5987
Time for alignment: 47.0415
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 766.7264 - loglik: -7.0480e+02 - logprior: -6.1931e+01
Epoch 2/10
10/10 - 1s - loss: 677.5724 - loglik: -6.6324e+02 - logprior: -1.4329e+01
Epoch 3/10
10/10 - 1s - loss: 633.2429 - loglik: -6.2706e+02 - logprior: -6.1807e+00
Epoch 4/10
10/10 - 1s - loss: 608.4266 - loglik: -6.0504e+02 - logprior: -3.3904e+00
Epoch 5/10
10/10 - 1s - loss: 596.0236 - loglik: -5.9410e+02 - logprior: -1.9191e+00
Epoch 6/10
10/10 - 1s - loss: 588.5450 - loglik: -5.8748e+02 - logprior: -1.0674e+00
Epoch 7/10
10/10 - 1s - loss: 585.0949 - loglik: -5.8448e+02 - logprior: -6.0940e-01
Epoch 8/10
10/10 - 1s - loss: 582.1901 - loglik: -5.8176e+02 - logprior: -4.2924e-01
Epoch 9/10
10/10 - 1s - loss: 582.4344 - loglik: -5.8208e+02 - logprior: -3.5476e-01
Fitted a model with MAP estimate = -581.4593
expansions: [(8, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (54, 1), (64, 1), (71, 3), (78, 1), (79, 1), (83, 1), (91, 1), (92, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 661.3719 - loglik: -5.9197e+02 - logprior: -6.9401e+01
Epoch 2/2
10/10 - 1s - loss: 598.1785 - loglik: -5.7183e+02 - logprior: -2.6347e+01
Fitted a model with MAP estimate = -587.7267
expansions: [(0, 8)]
discards: [  0  87 114]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 623.9706 - loglik: -5.6915e+02 - logprior: -5.4816e+01
Epoch 2/2
10/10 - 2s - loss: 576.2870 - loglik: -5.6436e+02 - logprior: -1.1922e+01
Fitted a model with MAP estimate = -568.9793
expansions: []
discards: [ 1  2  3  4  5  6  7 20]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 620.1714 - loglik: -5.6666e+02 - logprior: -5.3507e+01
Epoch 2/10
10/10 - 1s - loss: 575.9618 - loglik: -5.6477e+02 - logprior: -1.1192e+01
Epoch 3/10
10/10 - 1s - loss: 566.7551 - loglik: -5.6443e+02 - logprior: -2.3219e+00
Epoch 4/10
10/10 - 1s - loss: 560.9977 - loglik: -5.6242e+02 - logprior: 1.4250
Epoch 5/10
10/10 - 1s - loss: 558.1591 - loglik: -5.6162e+02 - logprior: 3.4575
Epoch 6/10
10/10 - 1s - loss: 553.5386 - loglik: -5.5805e+02 - logprior: 4.5099
Epoch 7/10
10/10 - 1s - loss: 550.0084 - loglik: -5.5507e+02 - logprior: 5.0664
Epoch 8/10
10/10 - 1s - loss: 547.9313 - loglik: -5.5341e+02 - logprior: 5.4769
Epoch 9/10
10/10 - 1s - loss: 547.0270 - loglik: -5.5298e+02 - logprior: 5.9536
Epoch 10/10
10/10 - 1s - loss: 546.9054 - loglik: -5.5329e+02 - logprior: 6.3870
Fitted a model with MAP estimate = -546.0907
Time for alignment: 48.9167
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 767.0768 - loglik: -7.0515e+02 - logprior: -6.1931e+01
Epoch 2/10
10/10 - 1s - loss: 677.2293 - loglik: -6.6290e+02 - logprior: -1.4331e+01
Epoch 3/10
10/10 - 1s - loss: 633.0436 - loglik: -6.2689e+02 - logprior: -6.1564e+00
Epoch 4/10
10/10 - 1s - loss: 611.1915 - loglik: -6.0782e+02 - logprior: -3.3696e+00
Epoch 5/10
10/10 - 1s - loss: 597.2459 - loglik: -5.9525e+02 - logprior: -1.9985e+00
Epoch 6/10
10/10 - 1s - loss: 589.0223 - loglik: -5.8793e+02 - logprior: -1.0886e+00
Epoch 7/10
10/10 - 1s - loss: 585.5943 - loglik: -5.8508e+02 - logprior: -5.1002e-01
Epoch 8/10
10/10 - 1s - loss: 583.5454 - loglik: -5.8304e+02 - logprior: -5.0867e-01
Epoch 9/10
10/10 - 1s - loss: 582.2492 - loglik: -5.8171e+02 - logprior: -5.3617e-01
Epoch 10/10
10/10 - 1s - loss: 581.6273 - loglik: -5.8122e+02 - logprior: -4.0223e-01
Fitted a model with MAP estimate = -581.3123
expansions: [(11, 1), (12, 2), (13, 5), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (60, 2), (62, 1), (63, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 662.8044 - loglik: -5.9341e+02 - logprior: -6.9391e+01
Epoch 2/2
10/10 - 1s - loss: 599.1613 - loglik: -5.7302e+02 - logprior: -2.6145e+01
Fitted a model with MAP estimate = -588.3362
expansions: [(0, 8)]
discards: [  0  73 109]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 625.9376 - loglik: -5.7122e+02 - logprior: -5.4717e+01
Epoch 2/2
10/10 - 2s - loss: 575.6830 - loglik: -5.6388e+02 - logprior: -1.1806e+01
Fitted a model with MAP estimate = -569.1386
expansions: []
discards: [1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 619.4213 - loglik: -5.6590e+02 - logprior: -5.3524e+01
Epoch 2/10
10/10 - 1s - loss: 574.3013 - loglik: -5.6306e+02 - logprior: -1.1240e+01
Epoch 3/10
10/10 - 1s - loss: 566.6846 - loglik: -5.6437e+02 - logprior: -2.3185e+00
Epoch 4/10
10/10 - 1s - loss: 559.8080 - loglik: -5.6125e+02 - logprior: 1.4378
Epoch 5/10
10/10 - 1s - loss: 556.7357 - loglik: -5.6019e+02 - logprior: 3.4583
Epoch 6/10
10/10 - 1s - loss: 551.8732 - loglik: -5.5632e+02 - logprior: 4.4439
Epoch 7/10
10/10 - 1s - loss: 548.6393 - loglik: -5.5365e+02 - logprior: 5.0112
Epoch 8/10
10/10 - 1s - loss: 547.4520 - loglik: -5.5290e+02 - logprior: 5.4444
Epoch 9/10
10/10 - 1s - loss: 546.1093 - loglik: -5.5205e+02 - logprior: 5.9472
Epoch 10/10
10/10 - 1s - loss: 545.6133 - loglik: -5.5199e+02 - logprior: 6.3747
Fitted a model with MAP estimate = -545.0478
Time for alignment: 48.1324
Fitting a model of length 100 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 766.6932 - loglik: -7.0476e+02 - logprior: -6.1930e+01
Epoch 2/10
10/10 - 1s - loss: 677.2262 - loglik: -6.6289e+02 - logprior: -1.4335e+01
Epoch 3/10
10/10 - 1s - loss: 633.9397 - loglik: -6.2779e+02 - logprior: -6.1486e+00
Epoch 4/10
10/10 - 1s - loss: 610.2084 - loglik: -6.0692e+02 - logprior: -3.2854e+00
Epoch 5/10
10/10 - 1s - loss: 596.9003 - loglik: -5.9491e+02 - logprior: -1.9857e+00
Epoch 6/10
10/10 - 1s - loss: 588.2645 - loglik: -5.8699e+02 - logprior: -1.2692e+00
Epoch 7/10
10/10 - 1s - loss: 585.0765 - loglik: -5.8431e+02 - logprior: -7.6409e-01
Epoch 8/10
10/10 - 1s - loss: 583.7770 - loglik: -5.8310e+02 - logprior: -6.7162e-01
Epoch 9/10
10/10 - 1s - loss: 581.7153 - loglik: -5.8100e+02 - logprior: -7.1380e-01
Epoch 10/10
10/10 - 1s - loss: 581.3310 - loglik: -5.8068e+02 - logprior: -6.5376e-01
Fitted a model with MAP estimate = -581.0363
expansions: [(8, 1), (12, 3), (13, 1), (14, 3), (19, 2), (37, 1), (38, 1), (39, 2), (45, 1), (51, 1), (54, 1), (64, 1), (69, 1), (70, 1), (78, 1), (79, 1), (83, 1), (87, 4), (88, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 662.1804 - loglik: -5.9280e+02 - logprior: -6.9377e+01
Epoch 2/2
10/10 - 1s - loss: 596.7435 - loglik: -5.7056e+02 - logprior: -2.6182e+01
Fitted a model with MAP estimate = -586.5535
expansions: [(0, 7)]
discards: [  0  13  14  17  26 110]
Re-initialized the encoder parameters.
Fitting a model of length 128 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 626.2047 - loglik: -5.7151e+02 - logprior: -5.4692e+01
Epoch 2/2
10/10 - 2s - loss: 577.5553 - loglik: -5.6576e+02 - logprior: -1.1797e+01
Fitted a model with MAP estimate = -570.6287
expansions: []
discards: [1 2 3 4 5 6]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 687 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 620.6646 - loglik: -5.6716e+02 - logprior: -5.3504e+01
Epoch 2/10
10/10 - 1s - loss: 577.0562 - loglik: -5.6587e+02 - logprior: -1.1184e+01
Epoch 3/10
10/10 - 1s - loss: 567.2015 - loglik: -5.6491e+02 - logprior: -2.2955e+00
Epoch 4/10
10/10 - 1s - loss: 562.3487 - loglik: -5.6381e+02 - logprior: 1.4641
Epoch 5/10
10/10 - 1s - loss: 558.4054 - loglik: -5.6191e+02 - logprior: 3.5075
Epoch 6/10
10/10 - 1s - loss: 553.7129 - loglik: -5.5824e+02 - logprior: 4.5293
Epoch 7/10
10/10 - 1s - loss: 550.0116 - loglik: -5.5509e+02 - logprior: 5.0782
Epoch 8/10
10/10 - 1s - loss: 548.3546 - loglik: -5.5381e+02 - logprior: 5.4586
Epoch 9/10
10/10 - 1s - loss: 547.4454 - loglik: -5.5338e+02 - logprior: 5.9345
Epoch 10/10
10/10 - 1s - loss: 547.1835 - loglik: -5.5355e+02 - logprior: 6.3698
Fitted a model with MAP estimate = -546.4778
Time for alignment: 48.3122
Computed alignments with likelihoods: ['-545.9876', '-545.5987', '-546.0907', '-545.0478', '-546.4778']
Best model has likelihood: -545.0478  (prior= 6.5647 )
time for generating output: 0.1420
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/profilin.projection.fasta
SP score = 0.9289176090468497
Training of 5 independent models on file rrm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea5e698b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea44be0a60>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f839bf70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 400.9322 - loglik: -3.9875e+02 - logprior: -2.1872e+00
Epoch 2/10
22/22 - 1s - loss: 368.3939 - loglik: -3.6708e+02 - logprior: -1.3127e+00
Epoch 3/10
22/22 - 1s - loss: 361.5878 - loglik: -3.6020e+02 - logprior: -1.3839e+00
Epoch 4/10
22/22 - 1s - loss: 359.5319 - loglik: -3.5824e+02 - logprior: -1.2934e+00
Epoch 5/10
22/22 - 1s - loss: 358.8373 - loglik: -3.5753e+02 - logprior: -1.3026e+00
Epoch 6/10
22/22 - 1s - loss: 357.6573 - loglik: -3.5638e+02 - logprior: -1.2733e+00
Epoch 7/10
22/22 - 1s - loss: 357.1628 - loglik: -3.5589e+02 - logprior: -1.2600e+00
Epoch 8/10
22/22 - 1s - loss: 356.0857 - loglik: -3.5482e+02 - logprior: -1.2519e+00
Epoch 9/10
22/22 - 1s - loss: 355.4285 - loglik: -3.5416e+02 - logprior: -1.2456e+00
Epoch 10/10
22/22 - 1s - loss: 355.7126 - loglik: -3.5445e+02 - logprior: -1.2425e+00
Fitted a model with MAP estimate = -345.6873
expansions: [(8, 1), (9, 2), (12, 1), (14, 2), (20, 2), (21, 2), (23, 1), (25, 1), (40, 1), (41, 1), (45, 2), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 365.2133 - loglik: -3.6238e+02 - logprior: -2.8345e+00
Epoch 2/2
22/22 - 1s - loss: 353.9761 - loglik: -3.5253e+02 - logprior: -1.4490e+00
Fitted a model with MAP estimate = -339.9781
expansions: [(0, 2)]
discards: [ 0  9 18 26 29 58 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 353.4321 - loglik: -3.5133e+02 - logprior: -2.1001e+00
Epoch 2/2
22/22 - 1s - loss: 350.3665 - loglik: -3.4936e+02 - logprior: -1.0032e+00
Fitted a model with MAP estimate = -338.8577
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 339.3347 - loglik: -3.3820e+02 - logprior: -1.1341e+00
Epoch 2/10
32/32 - 2s - loss: 336.6199 - loglik: -3.3582e+02 - logprior: -7.9541e-01
Epoch 3/10
32/32 - 2s - loss: 336.5383 - loglik: -3.3577e+02 - logprior: -7.6874e-01
Epoch 4/10
32/32 - 2s - loss: 335.7209 - loglik: -3.3495e+02 - logprior: -7.6592e-01
Epoch 5/10
32/32 - 2s - loss: 335.7266 - loglik: -3.3496e+02 - logprior: -7.5942e-01
Fitted a model with MAP estimate = -334.9214
Time for alignment: 60.9639
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 400.6441 - loglik: -3.9846e+02 - logprior: -2.1890e+00
Epoch 2/10
22/22 - 1s - loss: 368.5949 - loglik: -3.6727e+02 - logprior: -1.3208e+00
Epoch 3/10
22/22 - 1s - loss: 360.9256 - loglik: -3.5952e+02 - logprior: -1.4069e+00
Epoch 4/10
22/22 - 1s - loss: 360.1117 - loglik: -3.5881e+02 - logprior: -1.3031e+00
Epoch 5/10
22/22 - 1s - loss: 358.6283 - loglik: -3.5732e+02 - logprior: -1.3035e+00
Epoch 6/10
22/22 - 1s - loss: 358.1365 - loglik: -3.5686e+02 - logprior: -1.2714e+00
Epoch 7/10
22/22 - 1s - loss: 357.0684 - loglik: -3.5580e+02 - logprior: -1.2613e+00
Epoch 8/10
22/22 - 1s - loss: 356.5509 - loglik: -3.5528e+02 - logprior: -1.2540e+00
Epoch 9/10
22/22 - 1s - loss: 356.1100 - loglik: -3.5484e+02 - logprior: -1.2475e+00
Epoch 10/10
22/22 - 1s - loss: 355.6968 - loglik: -3.5443e+02 - logprior: -1.2446e+00
Fitted a model with MAP estimate = -346.1776
expansions: [(8, 1), (9, 2), (11, 2), (13, 2), (20, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (45, 2), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 365.3291 - loglik: -3.6249e+02 - logprior: -2.8378e+00
Epoch 2/2
22/22 - 1s - loss: 353.9361 - loglik: -3.5248e+02 - logprior: -1.4597e+00
Fitted a model with MAP estimate = -339.8854
expansions: [(0, 2)]
discards: [ 0  9 14 18 26 58 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 353.2871 - loglik: -3.5119e+02 - logprior: -2.0998e+00
Epoch 2/2
22/22 - 1s - loss: 350.5058 - loglik: -3.4951e+02 - logprior: -1.0004e+00
Fitted a model with MAP estimate = -338.7447
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 339.0075 - loglik: -3.3788e+02 - logprior: -1.1295e+00
Epoch 2/10
32/32 - 2s - loss: 336.9853 - loglik: -3.3619e+02 - logprior: -7.9276e-01
Epoch 3/10
32/32 - 2s - loss: 336.4449 - loglik: -3.3567e+02 - logprior: -7.7786e-01
Epoch 4/10
32/32 - 2s - loss: 336.0009 - loglik: -3.3523e+02 - logprior: -7.6255e-01
Epoch 5/10
32/32 - 2s - loss: 335.4735 - loglik: -3.3471e+02 - logprior: -7.6025e-01
Epoch 6/10
32/32 - 2s - loss: 334.6897 - loglik: -3.3393e+02 - logprior: -7.5095e-01
Epoch 7/10
32/32 - 2s - loss: 334.5047 - loglik: -3.3374e+02 - logprior: -7.5070e-01
Epoch 8/10
32/32 - 2s - loss: 333.0182 - loglik: -3.3225e+02 - logprior: -7.4424e-01
Epoch 9/10
32/32 - 2s - loss: 332.6935 - loglik: -3.3191e+02 - logprior: -7.4696e-01
Epoch 10/10
32/32 - 2s - loss: 332.1757 - loglik: -3.3139e+02 - logprior: -7.4393e-01
Fitted a model with MAP estimate = -331.6283
Time for alignment: 67.7840
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 5s - loss: 400.6789 - loglik: -3.9849e+02 - logprior: -2.1893e+00
Epoch 2/10
22/22 - 1s - loss: 368.5096 - loglik: -3.6719e+02 - logprior: -1.3182e+00
Epoch 3/10
22/22 - 1s - loss: 360.4602 - loglik: -3.5906e+02 - logprior: -1.4041e+00
Epoch 4/10
22/22 - 1s - loss: 359.5278 - loglik: -3.5822e+02 - logprior: -1.3069e+00
Epoch 5/10
22/22 - 1s - loss: 358.5363 - loglik: -3.5723e+02 - logprior: -1.3058e+00
Epoch 6/10
22/22 - 1s - loss: 357.9429 - loglik: -3.5666e+02 - logprior: -1.2741e+00
Epoch 7/10
22/22 - 1s - loss: 357.0816 - loglik: -3.5581e+02 - logprior: -1.2624e+00
Epoch 8/10
22/22 - 1s - loss: 356.4210 - loglik: -3.5515e+02 - logprior: -1.2529e+00
Epoch 9/10
22/22 - 1s - loss: 356.0594 - loglik: -3.5479e+02 - logprior: -1.2483e+00
Epoch 10/10
22/22 - 1s - loss: 355.6514 - loglik: -3.5439e+02 - logprior: -1.2454e+00
Fitted a model with MAP estimate = -345.8769
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (20, 2), (22, 2), (23, 1), (25, 1), (40, 1), (41, 1), (45, 2), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 365.1740 - loglik: -3.6233e+02 - logprior: -2.8391e+00
Epoch 2/2
22/22 - 1s - loss: 354.0847 - loglik: -3.5263e+02 - logprior: -1.4591e+00
Fitted a model with MAP estimate = -339.8601
expansions: [(0, 2)]
discards: [ 0  9 17 25 30 58 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 353.2125 - loglik: -3.5111e+02 - logprior: -2.1015e+00
Epoch 2/2
22/22 - 1s - loss: 350.7410 - loglik: -3.4974e+02 - logprior: -1.0043e+00
Fitted a model with MAP estimate = -338.7342
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 6s - loss: 338.9480 - loglik: -3.3782e+02 - logprior: -1.1315e+00
Epoch 2/10
32/32 - 2s - loss: 337.1320 - loglik: -3.3634e+02 - logprior: -7.9249e-01
Epoch 3/10
32/32 - 2s - loss: 336.2551 - loglik: -3.3548e+02 - logprior: -7.7551e-01
Epoch 4/10
32/32 - 2s - loss: 335.6972 - loglik: -3.3492e+02 - logprior: -7.6868e-01
Epoch 5/10
32/32 - 2s - loss: 335.9724 - loglik: -3.3521e+02 - logprior: -7.5525e-01
Fitted a model with MAP estimate = -334.9096
Time for alignment: 59.4945
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 400.5899 - loglik: -3.9840e+02 - logprior: -2.1877e+00
Epoch 2/10
22/22 - 1s - loss: 368.5839 - loglik: -3.6727e+02 - logprior: -1.3136e+00
Epoch 3/10
22/22 - 1s - loss: 361.5564 - loglik: -3.6015e+02 - logprior: -1.4046e+00
Epoch 4/10
22/22 - 1s - loss: 359.4041 - loglik: -3.5809e+02 - logprior: -1.3105e+00
Epoch 5/10
22/22 - 1s - loss: 359.2439 - loglik: -3.5793e+02 - logprior: -1.3122e+00
Epoch 6/10
22/22 - 1s - loss: 358.2372 - loglik: -3.5695e+02 - logprior: -1.2846e+00
Epoch 7/10
22/22 - 1s - loss: 357.4363 - loglik: -3.5615e+02 - logprior: -1.2738e+00
Epoch 8/10
22/22 - 1s - loss: 356.7237 - loglik: -3.5545e+02 - logprior: -1.2635e+00
Epoch 9/10
22/22 - 1s - loss: 356.2599 - loglik: -3.5498e+02 - logprior: -1.2595e+00
Epoch 10/10
22/22 - 1s - loss: 356.0982 - loglik: -3.5482e+02 - logprior: -1.2547e+00
Fitted a model with MAP estimate = -346.0596
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (14, 2), (21, 1), (22, 1), (25, 1), (40, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 364.1658 - loglik: -3.6135e+02 - logprior: -2.8165e+00
Epoch 2/2
22/22 - 1s - loss: 354.0193 - loglik: -3.5261e+02 - logprior: -1.4041e+00
Fitted a model with MAP estimate = -339.8821
expansions: [(0, 2)]
discards: [ 0  9 17 19 66]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 353.1043 - loglik: -3.5101e+02 - logprior: -2.0977e+00
Epoch 2/2
22/22 - 1s - loss: 350.4723 - loglik: -3.4947e+02 - logprior: -9.9931e-01
Fitted a model with MAP estimate = -338.6744
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 339.4581 - loglik: -3.3832e+02 - logprior: -1.1362e+00
Epoch 2/10
32/32 - 2s - loss: 336.4149 - loglik: -3.3562e+02 - logprior: -7.9522e-01
Epoch 3/10
32/32 - 2s - loss: 336.4894 - loglik: -3.3571e+02 - logprior: -7.7436e-01
Fitted a model with MAP estimate = -336.0210
Time for alignment: 54.6161
Fitting a model of length 56 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 4s - loss: 400.6200 - loglik: -3.9843e+02 - logprior: -2.1898e+00
Epoch 2/10
22/22 - 1s - loss: 369.2054 - loglik: -3.6789e+02 - logprior: -1.3112e+00
Epoch 3/10
22/22 - 1s - loss: 361.4407 - loglik: -3.6005e+02 - logprior: -1.3923e+00
Epoch 4/10
22/22 - 1s - loss: 359.8369 - loglik: -3.5852e+02 - logprior: -1.3140e+00
Epoch 5/10
22/22 - 1s - loss: 358.9983 - loglik: -3.5768e+02 - logprior: -1.3107e+00
Epoch 6/10
22/22 - 1s - loss: 358.3145 - loglik: -3.5703e+02 - logprior: -1.2803e+00
Epoch 7/10
22/22 - 1s - loss: 357.4650 - loglik: -3.5618e+02 - logprior: -1.2724e+00
Epoch 8/10
22/22 - 1s - loss: 356.7922 - loglik: -3.5551e+02 - logprior: -1.2622e+00
Epoch 9/10
22/22 - 1s - loss: 356.3154 - loglik: -3.5504e+02 - logprior: -1.2588e+00
Epoch 10/10
22/22 - 1s - loss: 355.6343 - loglik: -3.5436e+02 - logprior: -1.2544e+00
Fitted a model with MAP estimate = -346.2155
expansions: [(8, 1), (9, 2), (11, 1), (13, 2), (14, 2), (21, 2), (22, 2), (24, 1), (40, 1), (41, 1), (44, 1), (47, 2), (48, 1), (49, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 4s - loss: 364.9986 - loglik: -3.6217e+02 - logprior: -2.8316e+00
Epoch 2/2
22/22 - 1s - loss: 353.9526 - loglik: -3.5250e+02 - logprior: -1.4568e+00
Fitted a model with MAP estimate = -339.9548
expansions: [(0, 2)]
discards: [ 0  9 17 19 29 31 68]
Re-initialized the encoder parameters.
Fitting a model of length 72 on 13805 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 5s - loss: 353.3251 - loglik: -3.5123e+02 - logprior: -2.0999e+00
Epoch 2/2
22/22 - 1s - loss: 350.3468 - loglik: -3.4934e+02 - logprior: -1.0032e+00
Fitted a model with MAP estimate = -338.7925
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 71 on 27610 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
32/32 - 5s - loss: 339.6290 - loglik: -3.3850e+02 - logprior: -1.1321e+00
Epoch 2/10
32/32 - 2s - loss: 336.0905 - loglik: -3.3530e+02 - logprior: -7.9425e-01
Epoch 3/10
32/32 - 2s - loss: 336.5635 - loglik: -3.3579e+02 - logprior: -7.7099e-01
Fitted a model with MAP estimate = -336.0008
Time for alignment: 53.8614
Computed alignments with likelihoods: ['-334.9214', '-331.6283', '-334.9096', '-336.0210', '-336.0008']
Best model has likelihood: -331.6283  (prior= -0.7204 )
time for generating output: 0.1318
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rrm.projection.fasta
SP score = 0.8668628687548174
Training of 5 independent models on file KAS.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea77eb4fa0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea669c5a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c29e910>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 960.0071 - loglik: -9.4702e+02 - logprior: -1.2983e+01
Epoch 2/10
17/17 - 3s - loss: 808.5950 - loglik: -8.0676e+02 - logprior: -1.8355e+00
Epoch 3/10
17/17 - 4s - loss: 762.5945 - loglik: -7.6100e+02 - logprior: -1.5896e+00
Epoch 4/10
17/17 - 4s - loss: 753.0746 - loglik: -7.5203e+02 - logprior: -1.0393e+00
Epoch 5/10
17/17 - 4s - loss: 748.7142 - loglik: -7.4773e+02 - logprior: -9.7876e-01
Epoch 6/10
17/17 - 4s - loss: 748.5945 - loglik: -7.4763e+02 - logprior: -9.6094e-01
Epoch 7/10
17/17 - 4s - loss: 743.9814 - loglik: -7.4299e+02 - logprior: -9.8506e-01
Epoch 8/10
17/17 - 4s - loss: 742.3309 - loglik: -7.4127e+02 - logprior: -1.0563e+00
Epoch 9/10
17/17 - 4s - loss: 741.0688 - loglik: -7.3995e+02 - logprior: -1.1125e+00
Epoch 10/10
17/17 - 4s - loss: 741.2288 - loglik: -7.4006e+02 - logprior: -1.1640e+00
Fitted a model with MAP estimate = -740.9397
expansions: [(50, 1), (59, 1), (80, 1), (89, 1), (96, 1), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 771.0832 - loglik: -7.5359e+02 - logprior: -1.7491e+01
Epoch 2/2
17/17 - 4s - loss: 750.2760 - loglik: -7.4419e+02 - logprior: -6.0860e+00
Fitted a model with MAP estimate = -749.4074
expansions: [(0, 20)]
discards: [ 0 49 54 59]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 766.7281 - loglik: -7.5325e+02 - logprior: -1.3473e+01
Epoch 2/2
17/17 - 4s - loss: 744.2344 - loglik: -7.4221e+02 - logprior: -2.0217e+00
Fitted a model with MAP estimate = -738.2526
expansions: [(68, 1), (72, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 57 58 59 60 61
 62 63 64 65]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 784.9116 - loglik: -7.6935e+02 - logprior: -1.5561e+01
Epoch 2/10
17/17 - 3s - loss: 766.2767 - loglik: -7.6476e+02 - logprior: -1.5177e+00
Epoch 3/10
17/17 - 4s - loss: 760.2630 - loglik: -7.6059e+02 - logprior: 0.3306
Epoch 4/10
17/17 - 3s - loss: 764.6822 - loglik: -7.6544e+02 - logprior: 0.7564
Fitted a model with MAP estimate = -761.0671
Time for alignment: 94.2725
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 960.7133 - loglik: -9.4774e+02 - logprior: -1.2971e+01
Epoch 2/10
17/17 - 4s - loss: 809.5629 - loglik: -8.0777e+02 - logprior: -1.7938e+00
Epoch 3/10
17/17 - 4s - loss: 761.8974 - loglik: -7.6047e+02 - logprior: -1.4303e+00
Epoch 4/10
17/17 - 4s - loss: 751.4858 - loglik: -7.5048e+02 - logprior: -1.0017e+00
Epoch 5/10
17/17 - 4s - loss: 747.4748 - loglik: -7.4648e+02 - logprior: -9.9781e-01
Epoch 6/10
17/17 - 4s - loss: 746.2307 - loglik: -7.4522e+02 - logprior: -1.0135e+00
Epoch 7/10
17/17 - 4s - loss: 741.1001 - loglik: -7.4004e+02 - logprior: -1.0543e+00
Epoch 8/10
17/17 - 4s - loss: 742.2281 - loglik: -7.4110e+02 - logprior: -1.1261e+00
Fitted a model with MAP estimate = -740.4820
expansions: [(25, 1), (49, 1), (50, 1), (80, 1), (89, 1), (108, 1), (136, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 767.1354 - loglik: -7.4986e+02 - logprior: -1.7273e+01
Epoch 2/2
17/17 - 4s - loss: 746.3851 - loglik: -7.4044e+02 - logprior: -5.9414e+00
Fitted a model with MAP estimate = -746.5159
expansions: [(0, 19)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 8s - loss: 757.8798 - loglik: -7.4464e+02 - logprior: -1.3235e+01
Epoch 2/2
17/17 - 5s - loss: 735.0202 - loglik: -7.3328e+02 - logprior: -1.7442e+00
Fitted a model with MAP estimate = -731.7672
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 57 58 59 60 61 62
 63 64 65]
Re-initialized the encoder parameters.
Fitting a model of length 159 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 776.7946 - loglik: -7.6153e+02 - logprior: -1.5264e+01
Epoch 2/10
17/17 - 4s - loss: 760.4709 - loglik: -7.5926e+02 - logprior: -1.2086e+00
Epoch 3/10
17/17 - 3s - loss: 761.4947 - loglik: -7.6204e+02 - logprior: 0.5408
Fitted a model with MAP estimate = -758.5267
Time for alignment: 82.7139
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 960.2927 - loglik: -9.4729e+02 - logprior: -1.2999e+01
Epoch 2/10
17/17 - 4s - loss: 808.7824 - loglik: -8.0691e+02 - logprior: -1.8758e+00
Epoch 3/10
17/17 - 4s - loss: 760.1319 - loglik: -7.5855e+02 - logprior: -1.5856e+00
Epoch 4/10
17/17 - 4s - loss: 748.6090 - loglik: -7.4752e+02 - logprior: -1.0908e+00
Epoch 5/10
17/17 - 4s - loss: 753.9812 - loglik: -7.5295e+02 - logprior: -1.0341e+00
Fitted a model with MAP estimate = -746.4214
expansions: [(31, 1), (50, 1), (73, 1), (82, 1), (89, 1), (108, 1), (137, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 9s - loss: 766.2350 - loglik: -7.4904e+02 - logprior: -1.7200e+01
Epoch 2/2
17/17 - 4s - loss: 749.2876 - loglik: -7.4340e+02 - logprior: -5.8826e+00
Fitted a model with MAP estimate = -746.0609
expansions: [(0, 19)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 756.4420 - loglik: -7.4321e+02 - logprior: -1.3235e+01
Epoch 2/2
17/17 - 4s - loss: 737.9179 - loglik: -7.3596e+02 - logprior: -1.9579e+00
Fitted a model with MAP estimate = -733.3925
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 57 58 59 60 61 62
 63 64 65]
Re-initialized the encoder parameters.
Fitting a model of length 159 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 778.5885 - loglik: -7.6308e+02 - logprior: -1.5509e+01
Epoch 2/10
17/17 - 4s - loss: 759.2292 - loglik: -7.5775e+02 - logprior: -1.4809e+00
Epoch 3/10
17/17 - 4s - loss: 761.7948 - loglik: -7.6230e+02 - logprior: 0.5068
Fitted a model with MAP estimate = -758.6194
Time for alignment: 71.6367
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 960.7067 - loglik: -9.4772e+02 - logprior: -1.2989e+01
Epoch 2/10
17/17 - 4s - loss: 811.3043 - loglik: -8.0952e+02 - logprior: -1.7868e+00
Epoch 3/10
17/17 - 4s - loss: 762.4570 - loglik: -7.6090e+02 - logprior: -1.5616e+00
Epoch 4/10
17/17 - 4s - loss: 754.7302 - loglik: -7.5364e+02 - logprior: -1.0903e+00
Epoch 5/10
17/17 - 4s - loss: 745.8259 - loglik: -7.4484e+02 - logprior: -9.8230e-01
Epoch 6/10
17/17 - 4s - loss: 747.1787 - loglik: -7.4622e+02 - logprior: -9.5726e-01
Fitted a model with MAP estimate = -744.5425
expansions: [(25, 1), (32, 1), (49, 1), (65, 1), (82, 1), (90, 1), (131, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 769.3546 - loglik: -7.5206e+02 - logprior: -1.7292e+01
Epoch 2/2
17/17 - 4s - loss: 750.6893 - loglik: -7.4468e+02 - logprior: -6.0133e+00
Fitted a model with MAP estimate = -748.3444
expansions: [(0, 20)]
discards: [ 0 50]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 756.9788 - loglik: -7.4370e+02 - logprior: -1.3279e+01
Epoch 2/2
17/17 - 4s - loss: 742.9796 - loglik: -7.4087e+02 - logprior: -2.1099e+00
Fitted a model with MAP estimate = -739.1981
expansions: [(69, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 7s - loss: 757.1669 - loglik: -7.4169e+02 - logprior: -1.5477e+01
Epoch 2/10
17/17 - 4s - loss: 743.5300 - loglik: -7.4209e+02 - logprior: -1.4431e+00
Epoch 3/10
17/17 - 4s - loss: 738.2034 - loglik: -7.3873e+02 - logprior: 0.5311
Epoch 4/10
17/17 - 4s - loss: 741.0068 - loglik: -7.4196e+02 - logprior: 0.9554
Fitted a model with MAP estimate = -738.2718
Time for alignment: 78.9091
Fitting a model of length 162 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 959.0635 - loglik: -9.4605e+02 - logprior: -1.3014e+01
Epoch 2/10
17/17 - 4s - loss: 809.6882 - loglik: -8.0779e+02 - logprior: -1.9023e+00
Epoch 3/10
17/17 - 4s - loss: 762.6001 - loglik: -7.6115e+02 - logprior: -1.4518e+00
Epoch 4/10
17/17 - 4s - loss: 756.4036 - loglik: -7.5527e+02 - logprior: -1.1303e+00
Epoch 5/10
17/17 - 4s - loss: 748.3746 - loglik: -7.4731e+02 - logprior: -1.0597e+00
Epoch 6/10
17/17 - 4s - loss: 746.0380 - loglik: -7.4499e+02 - logprior: -1.0486e+00
Epoch 7/10
17/17 - 4s - loss: 742.9357 - loglik: -7.4182e+02 - logprior: -1.1114e+00
Epoch 8/10
17/17 - 4s - loss: 744.2059 - loglik: -7.4303e+02 - logprior: -1.1771e+00
Fitted a model with MAP estimate = -742.1293
expansions: [(20, 1), (50, 1), (59, 1), (80, 1), (83, 1), (117, 1), (136, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 769.6255 - loglik: -7.5226e+02 - logprior: -1.7364e+01
Epoch 2/2
17/17 - 4s - loss: 751.4653 - loglik: -7.4546e+02 - logprior: -6.0062e+00
Fitted a model with MAP estimate = -749.3415
expansions: [(0, 20)]
discards: [ 0 50]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 7s - loss: 760.1356 - loglik: -7.4684e+02 - logprior: -1.3297e+01
Epoch 2/2
17/17 - 4s - loss: 741.8321 - loglik: -7.3982e+02 - logprior: -2.0119e+00
Fitted a model with MAP estimate = -734.3503
expansions: [(69, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 58 59 60 61 62
 63 64 65 66]
Re-initialized the encoder parameters.
Fitting a model of length 159 on 2070 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 8s - loss: 778.0908 - loglik: -7.6321e+02 - logprior: -1.4877e+01
Epoch 2/10
17/17 - 3s - loss: 760.8577 - loglik: -7.5966e+02 - logprior: -1.1932e+00
Epoch 3/10
17/17 - 4s - loss: 758.5933 - loglik: -7.5914e+02 - logprior: 0.5515
Epoch 4/10
17/17 - 4s - loss: 758.8950 - loglik: -7.5983e+02 - logprior: 0.9379
Fitted a model with MAP estimate = -757.1127
Time for alignment: 86.7813
Computed alignments with likelihoods: ['-738.2526', '-731.7672', '-733.3925', '-738.2718', '-734.3503']
Best model has likelihood: -731.7672  (prior= -0.4195 )
time for generating output: 0.3043
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/KAS.projection.fasta
SP score = 0.2085516663173339
Training of 5 independent models on file GEL.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c744cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b30b340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b30b280>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 472.6618 - loglik: -4.5240e+02 - logprior: -2.0266e+01
Epoch 2/10
10/10 - 1s - loss: 443.5248 - loglik: -4.3808e+02 - logprior: -5.4444e+00
Epoch 3/10
10/10 - 1s - loss: 427.9855 - loglik: -4.2502e+02 - logprior: -2.9620e+00
Epoch 4/10
10/10 - 1s - loss: 420.3476 - loglik: -4.1804e+02 - logprior: -2.3084e+00
Epoch 5/10
10/10 - 1s - loss: 415.1804 - loglik: -4.1316e+02 - logprior: -2.0236e+00
Epoch 6/10
10/10 - 1s - loss: 412.0348 - loglik: -4.1036e+02 - logprior: -1.6775e+00
Epoch 7/10
10/10 - 1s - loss: 410.7661 - loglik: -4.0933e+02 - logprior: -1.4295e+00
Epoch 8/10
10/10 - 1s - loss: 408.6902 - loglik: -4.0735e+02 - logprior: -1.3410e+00
Epoch 9/10
10/10 - 1s - loss: 408.4128 - loglik: -4.0713e+02 - logprior: -1.2755e+00
Epoch 10/10
10/10 - 1s - loss: 407.5539 - loglik: -4.0628e+02 - logprior: -1.2680e+00
Fitted a model with MAP estimate = -406.8261
expansions: [(0, 2), (8, 1), (23, 3), (41, 2), (42, 1), (43, 2), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 439.3925 - loglik: -4.1282e+02 - logprior: -2.6576e+01
Epoch 2/2
10/10 - 1s - loss: 415.2313 - loglik: -4.0699e+02 - logprior: -8.2435e+00
Fitted a model with MAP estimate = -410.6906
expansions: []
discards: [ 0 27 50 52 55]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 429.6181 - loglik: -4.0640e+02 - logprior: -2.3218e+01
Epoch 2/2
10/10 - 1s - loss: 415.3244 - loglik: -4.0618e+02 - logprior: -9.1431e+00
Fitted a model with MAP estimate = -412.2129
expansions: [(0, 2)]
discards: [ 0 58]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 425.3900 - loglik: -4.0515e+02 - logprior: -2.0235e+01
Epoch 2/10
10/10 - 1s - loss: 410.6959 - loglik: -4.0533e+02 - logprior: -5.3673e+00
Epoch 3/10
10/10 - 1s - loss: 406.9375 - loglik: -4.0462e+02 - logprior: -2.3205e+00
Epoch 4/10
10/10 - 1s - loss: 405.8347 - loglik: -4.0442e+02 - logprior: -1.4097e+00
Epoch 5/10
10/10 - 1s - loss: 404.5457 - loglik: -4.0349e+02 - logprior: -1.0537e+00
Epoch 6/10
10/10 - 1s - loss: 404.3673 - loglik: -4.0370e+02 - logprior: -6.6948e-01
Epoch 7/10
10/10 - 1s - loss: 403.1906 - loglik: -4.0276e+02 - logprior: -4.3119e-01
Epoch 8/10
10/10 - 1s - loss: 402.4130 - loglik: -4.0206e+02 - logprior: -3.4777e-01
Epoch 9/10
10/10 - 1s - loss: 401.6838 - loglik: -4.0139e+02 - logprior: -2.8703e-01
Epoch 10/10
10/10 - 1s - loss: 401.2625 - loglik: -4.0103e+02 - logprior: -2.2952e-01
Fitted a model with MAP estimate = -400.6196
Time for alignment: 36.4293
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 472.6485 - loglik: -4.5238e+02 - logprior: -2.0265e+01
Epoch 2/10
10/10 - 1s - loss: 443.8087 - loglik: -4.3837e+02 - logprior: -5.4403e+00
Epoch 3/10
10/10 - 1s - loss: 426.3290 - loglik: -4.2339e+02 - logprior: -2.9360e+00
Epoch 4/10
10/10 - 1s - loss: 417.8415 - loglik: -4.1554e+02 - logprior: -2.3049e+00
Epoch 5/10
10/10 - 1s - loss: 413.2729 - loglik: -4.1125e+02 - logprior: -2.0215e+00
Epoch 6/10
10/10 - 1s - loss: 411.8474 - loglik: -4.1021e+02 - logprior: -1.6336e+00
Epoch 7/10
10/10 - 1s - loss: 409.5687 - loglik: -4.0820e+02 - logprior: -1.3625e+00
Epoch 8/10
10/10 - 1s - loss: 409.8478 - loglik: -4.0853e+02 - logprior: -1.3173e+00
Fitted a model with MAP estimate = -408.6477
expansions: [(0, 2), (8, 1), (10, 1), (23, 1), (41, 2), (42, 2), (44, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 436.6488 - loglik: -4.1038e+02 - logprior: -2.6270e+01
Epoch 2/2
10/10 - 1s - loss: 414.4379 - loglik: -4.0652e+02 - logprior: -7.9186e+00
Fitted a model with MAP estimate = -410.2870
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 429.1886 - loglik: -4.0605e+02 - logprior: -2.3138e+01
Epoch 2/2
10/10 - 1s - loss: 415.2167 - loglik: -4.0611e+02 - logprior: -9.1018e+00
Fitted a model with MAP estimate = -412.2032
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 425.8081 - loglik: -4.0561e+02 - logprior: -2.0200e+01
Epoch 2/10
10/10 - 1s - loss: 409.4387 - loglik: -4.0408e+02 - logprior: -5.3597e+00
Epoch 3/10
10/10 - 1s - loss: 407.2840 - loglik: -4.0493e+02 - logprior: -2.3489e+00
Epoch 4/10
10/10 - 1s - loss: 405.7642 - loglik: -4.0434e+02 - logprior: -1.4211e+00
Epoch 5/10
10/10 - 1s - loss: 405.2965 - loglik: -4.0422e+02 - logprior: -1.0766e+00
Epoch 6/10
10/10 - 1s - loss: 403.4890 - loglik: -4.0278e+02 - logprior: -7.0551e-01
Epoch 7/10
10/10 - 1s - loss: 402.9977 - loglik: -4.0256e+02 - logprior: -4.3979e-01
Epoch 8/10
10/10 - 1s - loss: 402.0903 - loglik: -4.0174e+02 - logprior: -3.4892e-01
Epoch 9/10
10/10 - 1s - loss: 401.9969 - loglik: -4.0171e+02 - logprior: -2.8765e-01
Epoch 10/10
10/10 - 1s - loss: 401.4210 - loglik: -4.0117e+02 - logprior: -2.4417e-01
Fitted a model with MAP estimate = -400.6614
Time for alignment: 34.2226
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 472.7039 - loglik: -4.5244e+02 - logprior: -2.0265e+01
Epoch 2/10
10/10 - 1s - loss: 442.9465 - loglik: -4.3750e+02 - logprior: -5.4417e+00
Epoch 3/10
10/10 - 1s - loss: 425.2528 - loglik: -4.2232e+02 - logprior: -2.9317e+00
Epoch 4/10
10/10 - 1s - loss: 417.9974 - loglik: -4.1570e+02 - logprior: -2.2959e+00
Epoch 5/10
10/10 - 1s - loss: 412.8615 - loglik: -4.1085e+02 - logprior: -2.0134e+00
Epoch 6/10
10/10 - 1s - loss: 411.9298 - loglik: -4.1029e+02 - logprior: -1.6339e+00
Epoch 7/10
10/10 - 1s - loss: 409.8651 - loglik: -4.0846e+02 - logprior: -1.3985e+00
Epoch 8/10
10/10 - 1s - loss: 409.2157 - loglik: -4.0789e+02 - logprior: -1.3234e+00
Epoch 9/10
10/10 - 1s - loss: 408.2706 - loglik: -4.0697e+02 - logprior: -1.2996e+00
Epoch 10/10
10/10 - 1s - loss: 407.6443 - loglik: -4.0634e+02 - logprior: -1.3027e+00
Fitted a model with MAP estimate = -407.2046
expansions: [(0, 2), (8, 1), (10, 1), (39, 1), (41, 2), (42, 1), (43, 1), (44, 1), (45, 2), (46, 1), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 438.2578 - loglik: -4.1171e+02 - logprior: -2.6547e+01
Epoch 2/2
10/10 - 1s - loss: 414.4416 - loglik: -4.0633e+02 - logprior: -8.1118e+00
Fitted a model with MAP estimate = -410.2747
expansions: []
discards: [ 0 51 53 61]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 429.8465 - loglik: -4.0658e+02 - logprior: -2.3269e+01
Epoch 2/2
10/10 - 1s - loss: 415.4300 - loglik: -4.0626e+02 - logprior: -9.1730e+00
Fitted a model with MAP estimate = -412.3966
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 425.3995 - loglik: -4.0508e+02 - logprior: -2.0321e+01
Epoch 2/10
10/10 - 1s - loss: 410.0651 - loglik: -4.0463e+02 - logprior: -5.4314e+00
Epoch 3/10
10/10 - 1s - loss: 407.0051 - loglik: -4.0461e+02 - logprior: -2.3986e+00
Epoch 4/10
10/10 - 1s - loss: 405.8255 - loglik: -4.0434e+02 - logprior: -1.4826e+00
Epoch 5/10
10/10 - 1s - loss: 404.7448 - loglik: -4.0360e+02 - logprior: -1.1393e+00
Epoch 6/10
10/10 - 1s - loss: 403.8334 - loglik: -4.0304e+02 - logprior: -7.8762e-01
Epoch 7/10
10/10 - 1s - loss: 403.2862 - loglik: -4.0276e+02 - logprior: -5.2040e-01
Epoch 8/10
10/10 - 1s - loss: 402.2557 - loglik: -4.0181e+02 - logprior: -4.4300e-01
Epoch 9/10
10/10 - 1s - loss: 401.7211 - loglik: -4.0134e+02 - logprior: -3.7937e-01
Epoch 10/10
10/10 - 1s - loss: 400.9066 - loglik: -4.0059e+02 - logprior: -3.1660e-01
Fitted a model with MAP estimate = -400.7141
Time for alignment: 37.4481
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 472.6488 - loglik: -4.5238e+02 - logprior: -2.0265e+01
Epoch 2/10
10/10 - 1s - loss: 443.3103 - loglik: -4.3787e+02 - logprior: -5.4395e+00
Epoch 3/10
10/10 - 1s - loss: 427.2640 - loglik: -4.2433e+02 - logprior: -2.9304e+00
Epoch 4/10
10/10 - 1s - loss: 418.8785 - loglik: -4.1656e+02 - logprior: -2.3143e+00
Epoch 5/10
10/10 - 1s - loss: 414.5550 - loglik: -4.1253e+02 - logprior: -2.0266e+00
Epoch 6/10
10/10 - 1s - loss: 412.2871 - loglik: -4.1064e+02 - logprior: -1.6440e+00
Epoch 7/10
10/10 - 1s - loss: 410.9640 - loglik: -4.0956e+02 - logprior: -1.3997e+00
Epoch 8/10
10/10 - 1s - loss: 409.0543 - loglik: -4.0771e+02 - logprior: -1.3375e+00
Epoch 9/10
10/10 - 1s - loss: 408.5073 - loglik: -4.0721e+02 - logprior: -1.2920e+00
Epoch 10/10
10/10 - 1s - loss: 407.0844 - loglik: -4.0582e+02 - logprior: -1.2589e+00
Fitted a model with MAP estimate = -406.9019
expansions: [(0, 2), (7, 2), (8, 2), (23, 1), (41, 2), (42, 1), (43, 2), (44, 1), (45, 2), (46, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 439.6401 - loglik: -4.1318e+02 - logprior: -2.6458e+01
Epoch 2/2
10/10 - 1s - loss: 414.6102 - loglik: -4.0650e+02 - logprior: -8.1115e+00
Fitted a model with MAP estimate = -410.9967
expansions: []
discards: [ 0 10 12 53 56]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 429.9559 - loglik: -4.0681e+02 - logprior: -2.3143e+01
Epoch 2/2
10/10 - 1s - loss: 414.9272 - loglik: -4.0584e+02 - logprior: -9.0893e+00
Fitted a model with MAP estimate = -412.1633
expansions: [(0, 2)]
discards: [ 0 50]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 425.8936 - loglik: -4.0571e+02 - logprior: -2.0182e+01
Epoch 2/10
10/10 - 1s - loss: 410.5850 - loglik: -4.0524e+02 - logprior: -5.3411e+00
Epoch 3/10
10/10 - 1s - loss: 407.1661 - loglik: -4.0483e+02 - logprior: -2.3322e+00
Epoch 4/10
10/10 - 1s - loss: 406.1155 - loglik: -4.0468e+02 - logprior: -1.4323e+00
Epoch 5/10
10/10 - 1s - loss: 404.3216 - loglik: -4.0328e+02 - logprior: -1.0423e+00
Epoch 6/10
10/10 - 1s - loss: 404.3210 - loglik: -4.0368e+02 - logprior: -6.4170e-01
Epoch 7/10
10/10 - 1s - loss: 403.0936 - loglik: -4.0268e+02 - logprior: -4.1433e-01
Epoch 8/10
10/10 - 1s - loss: 402.2541 - loglik: -4.0193e+02 - logprior: -3.2482e-01
Epoch 9/10
10/10 - 1s - loss: 401.8053 - loglik: -4.0153e+02 - logprior: -2.6837e-01
Epoch 10/10
10/10 - 1s - loss: 400.9888 - loglik: -4.0076e+02 - logprior: -2.2772e-01
Fitted a model with MAP estimate = -400.6377
Time for alignment: 35.9610
Fitting a model of length 62 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 472.9670 - loglik: -4.5270e+02 - logprior: -2.0266e+01
Epoch 2/10
10/10 - 1s - loss: 443.1161 - loglik: -4.3767e+02 - logprior: -5.4411e+00
Epoch 3/10
10/10 - 1s - loss: 424.4766 - loglik: -4.2154e+02 - logprior: -2.9326e+00
Epoch 4/10
10/10 - 1s - loss: 417.2255 - loglik: -4.1492e+02 - logprior: -2.3097e+00
Epoch 5/10
10/10 - 1s - loss: 413.8997 - loglik: -4.1187e+02 - logprior: -2.0300e+00
Epoch 6/10
10/10 - 1s - loss: 411.5263 - loglik: -4.0990e+02 - logprior: -1.6207e+00
Epoch 7/10
10/10 - 1s - loss: 410.1808 - loglik: -4.0880e+02 - logprior: -1.3764e+00
Epoch 8/10
10/10 - 1s - loss: 409.6203 - loglik: -4.0829e+02 - logprior: -1.3278e+00
Epoch 9/10
10/10 - 1s - loss: 408.1819 - loglik: -4.0690e+02 - logprior: -1.2802e+00
Epoch 10/10
10/10 - 1s - loss: 408.1562 - loglik: -4.0691e+02 - logprior: -1.2448e+00
Fitted a model with MAP estimate = -407.4769
expansions: [(0, 2), (8, 1), (9, 1), (10, 1), (39, 2), (41, 2), (42, 3), (44, 2), (48, 1), (51, 1), (53, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 439.3442 - loglik: -4.1276e+02 - logprior: -2.6580e+01
Epoch 2/2
10/10 - 1s - loss: 415.3581 - loglik: -4.0721e+02 - logprior: -8.1497e+00
Fitted a model with MAP estimate = -411.3188
expansions: []
discards: [ 0 12 45 51]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 430.7571 - loglik: -4.0756e+02 - logprior: -2.3197e+01
Epoch 2/2
10/10 - 1s - loss: 415.4394 - loglik: -4.0633e+02 - logprior: -9.1120e+00
Fitted a model with MAP estimate = -412.6621
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2195 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 425.7907 - loglik: -4.0552e+02 - logprior: -2.0267e+01
Epoch 2/10
10/10 - 1s - loss: 410.4569 - loglik: -4.0508e+02 - logprior: -5.3721e+00
Epoch 3/10
10/10 - 1s - loss: 407.0065 - loglik: -4.0468e+02 - logprior: -2.3235e+00
Epoch 4/10
10/10 - 1s - loss: 405.7810 - loglik: -4.0437e+02 - logprior: -1.4063e+00
Epoch 5/10
10/10 - 1s - loss: 404.9529 - loglik: -4.0390e+02 - logprior: -1.0479e+00
Epoch 6/10
10/10 - 1s - loss: 404.0270 - loglik: -4.0335e+02 - logprior: -6.7234e-01
Epoch 7/10
10/10 - 1s - loss: 403.2025 - loglik: -4.0278e+02 - logprior: -4.2370e-01
Epoch 8/10
10/10 - 1s - loss: 402.3976 - loglik: -4.0204e+02 - logprior: -3.5892e-01
Epoch 9/10
10/10 - 1s - loss: 401.8822 - loglik: -4.0157e+02 - logprior: -3.0694e-01
Epoch 10/10
10/10 - 1s - loss: 401.2307 - loglik: -4.0098e+02 - logprior: -2.4630e-01
Fitted a model with MAP estimate = -400.8462
Time for alignment: 35.4117
Computed alignments with likelihoods: ['-400.6196', '-400.6614', '-400.7141', '-400.6377', '-400.8462']
Best model has likelihood: -400.6196  (prior= -0.2102 )
time for generating output: 0.1562
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/GEL.projection.fasta
SP score = 0.6238805970149254
Training of 5 independent models on file hr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b70a460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea66c92070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bfc8310>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 392.5518 - loglik: -3.8106e+02 - logprior: -1.1493e+01
Epoch 2/10
11/11 - 1s - loss: 342.7832 - loglik: -3.3953e+02 - logprior: -3.2542e+00
Epoch 3/10
11/11 - 1s - loss: 310.0116 - loglik: -3.0760e+02 - logprior: -2.4087e+00
Epoch 4/10
11/11 - 1s - loss: 294.4745 - loglik: -2.9245e+02 - logprior: -2.0251e+00
Epoch 5/10
11/11 - 1s - loss: 290.4593 - loglik: -2.8865e+02 - logprior: -1.8056e+00
Epoch 6/10
11/11 - 1s - loss: 288.6000 - loglik: -2.8681e+02 - logprior: -1.7936e+00
Epoch 7/10
11/11 - 1s - loss: 287.4418 - loglik: -2.8574e+02 - logprior: -1.7039e+00
Epoch 8/10
11/11 - 1s - loss: 287.0331 - loglik: -2.8531e+02 - logprior: -1.7180e+00
Epoch 9/10
11/11 - 1s - loss: 286.0620 - loglik: -2.8433e+02 - logprior: -1.7275e+00
Epoch 10/10
11/11 - 1s - loss: 285.0555 - loglik: -2.8331e+02 - logprior: -1.7413e+00
Fitted a model with MAP estimate = -285.2521
expansions: [(0, 3), (15, 2), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 1), (35, 1), (38, 2), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 73 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 306.2223 - loglik: -2.9255e+02 - logprior: -1.3669e+01
Epoch 2/2
11/11 - 1s - loss: 280.6606 - loglik: -2.7651e+02 - logprior: -4.1507e+00
Fitted a model with MAP estimate = -277.4539
expansions: []
discards: [ 0 34 39]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 298.7962 - loglik: -2.8571e+02 - logprior: -1.3085e+01
Epoch 2/2
11/11 - 1s - loss: 281.5878 - loglik: -2.7611e+02 - logprior: -5.4773e+00
Fitted a model with MAP estimate = -278.5083
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 295.0337 - loglik: -2.8426e+02 - logprior: -1.0769e+01
Epoch 2/10
11/11 - 1s - loss: 278.6323 - loglik: -2.7569e+02 - logprior: -2.9443e+00
Epoch 3/10
11/11 - 1s - loss: 276.1126 - loglik: -2.7436e+02 - logprior: -1.7554e+00
Epoch 4/10
11/11 - 1s - loss: 275.9611 - loglik: -2.7442e+02 - logprior: -1.5421e+00
Epoch 5/10
11/11 - 1s - loss: 275.0244 - loglik: -2.7354e+02 - logprior: -1.4851e+00
Epoch 6/10
11/11 - 1s - loss: 274.2935 - loglik: -2.7297e+02 - logprior: -1.3268e+00
Epoch 7/10
11/11 - 1s - loss: 274.6967 - loglik: -2.7348e+02 - logprior: -1.2135e+00
Fitted a model with MAP estimate = -273.7201
Time for alignment: 31.5988
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 392.9028 - loglik: -3.8141e+02 - logprior: -1.1491e+01
Epoch 2/10
11/11 - 1s - loss: 341.9198 - loglik: -3.3869e+02 - logprior: -3.2321e+00
Epoch 3/10
11/11 - 1s - loss: 309.9601 - loglik: -3.0758e+02 - logprior: -2.3787e+00
Epoch 4/10
11/11 - 1s - loss: 294.7879 - loglik: -2.9276e+02 - logprior: -2.0260e+00
Epoch 5/10
11/11 - 1s - loss: 289.4908 - loglik: -2.8769e+02 - logprior: -1.8015e+00
Epoch 6/10
11/11 - 1s - loss: 287.2260 - loglik: -2.8539e+02 - logprior: -1.8310e+00
Epoch 7/10
11/11 - 1s - loss: 286.4616 - loglik: -2.8473e+02 - logprior: -1.7278e+00
Epoch 8/10
11/11 - 1s - loss: 285.9904 - loglik: -2.8428e+02 - logprior: -1.7052e+00
Epoch 9/10
11/11 - 1s - loss: 285.0018 - loglik: -2.8331e+02 - logprior: -1.6936e+00
Epoch 10/10
11/11 - 1s - loss: 284.3222 - loglik: -2.8262e+02 - logprior: -1.6990e+00
Fitted a model with MAP estimate = -284.3546
expansions: [(0, 3), (15, 2), (28, 1), (29, 3), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (37, 1), (40, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 305.1722 - loglik: -2.9147e+02 - logprior: -1.3703e+01
Epoch 2/2
11/11 - 1s - loss: 280.9853 - loglik: -2.7676e+02 - logprior: -4.2253e+00
Fitted a model with MAP estimate = -277.4445
expansions: []
discards: [ 0 35 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 299.5832 - loglik: -2.8648e+02 - logprior: -1.3106e+01
Epoch 2/2
11/11 - 1s - loss: 280.7037 - loglik: -2.7520e+02 - logprior: -5.5030e+00
Fitted a model with MAP estimate = -278.4932
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 295.3642 - loglik: -2.8458e+02 - logprior: -1.0785e+01
Epoch 2/10
11/11 - 1s - loss: 278.1659 - loglik: -2.7522e+02 - logprior: -2.9459e+00
Epoch 3/10
11/11 - 1s - loss: 277.1989 - loglik: -2.7544e+02 - logprior: -1.7569e+00
Epoch 4/10
11/11 - 1s - loss: 274.6775 - loglik: -2.7313e+02 - logprior: -1.5487e+00
Epoch 5/10
11/11 - 1s - loss: 275.0133 - loglik: -2.7353e+02 - logprior: -1.4861e+00
Fitted a model with MAP estimate = -274.7669
Time for alignment: 29.3650
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 393.1607 - loglik: -3.8167e+02 - logprior: -1.1490e+01
Epoch 2/10
11/11 - 1s - loss: 341.3722 - loglik: -3.3813e+02 - logprior: -3.2436e+00
Epoch 3/10
11/11 - 1s - loss: 308.1775 - loglik: -3.0577e+02 - logprior: -2.4068e+00
Epoch 4/10
11/11 - 1s - loss: 293.6996 - loglik: -2.9166e+02 - logprior: -2.0409e+00
Epoch 5/10
11/11 - 1s - loss: 290.2873 - loglik: -2.8846e+02 - logprior: -1.8282e+00
Epoch 6/10
11/11 - 1s - loss: 287.7346 - loglik: -2.8589e+02 - logprior: -1.8404e+00
Epoch 7/10
11/11 - 1s - loss: 287.4188 - loglik: -2.8566e+02 - logprior: -1.7529e+00
Epoch 8/10
11/11 - 1s - loss: 285.8893 - loglik: -2.8415e+02 - logprior: -1.7401e+00
Epoch 9/10
11/11 - 1s - loss: 286.1360 - loglik: -2.8441e+02 - logprior: -1.7265e+00
Fitted a model with MAP estimate = -285.4192
expansions: [(0, 3), (15, 2), (26, 1), (27, 1), (28, 3), (29, 1), (30, 1), (31, 1), (34, 1), (37, 1), (40, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 304.4031 - loglik: -2.9081e+02 - logprior: -1.3597e+01
Epoch 2/2
11/11 - 1s - loss: 280.6457 - loglik: -2.7655e+02 - logprior: -4.0937e+00
Fitted a model with MAP estimate = -277.1794
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 298.3476 - loglik: -2.8526e+02 - logprior: -1.3087e+01
Epoch 2/2
11/11 - 1s - loss: 281.7543 - loglik: -2.7628e+02 - logprior: -5.4754e+00
Fitted a model with MAP estimate = -278.4692
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 295.2647 - loglik: -2.8451e+02 - logprior: -1.0750e+01
Epoch 2/10
11/11 - 1s - loss: 278.1542 - loglik: -2.7522e+02 - logprior: -2.9302e+00
Epoch 3/10
11/11 - 1s - loss: 276.3696 - loglik: -2.7462e+02 - logprior: -1.7474e+00
Epoch 4/10
11/11 - 1s - loss: 275.7133 - loglik: -2.7418e+02 - logprior: -1.5362e+00
Epoch 5/10
11/11 - 1s - loss: 274.8482 - loglik: -2.7337e+02 - logprior: -1.4765e+00
Epoch 6/10
11/11 - 1s - loss: 274.3899 - loglik: -2.7307e+02 - logprior: -1.3194e+00
Epoch 7/10
11/11 - 1s - loss: 274.7279 - loglik: -2.7352e+02 - logprior: -1.2079e+00
Fitted a model with MAP estimate = -273.7149
Time for alignment: 31.3360
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 393.2657 - loglik: -3.8177e+02 - logprior: -1.1493e+01
Epoch 2/10
11/11 - 1s - loss: 342.0917 - loglik: -3.3886e+02 - logprior: -3.2314e+00
Epoch 3/10
11/11 - 1s - loss: 310.6154 - loglik: -3.0823e+02 - logprior: -2.3832e+00
Epoch 4/10
11/11 - 1s - loss: 295.5393 - loglik: -2.9346e+02 - logprior: -2.0792e+00
Epoch 5/10
11/11 - 1s - loss: 289.8164 - loglik: -2.8797e+02 - logprior: -1.8441e+00
Epoch 6/10
11/11 - 1s - loss: 287.4140 - loglik: -2.8552e+02 - logprior: -1.8946e+00
Epoch 7/10
11/11 - 1s - loss: 286.5725 - loglik: -2.8472e+02 - logprior: -1.8544e+00
Epoch 8/10
11/11 - 1s - loss: 285.4716 - loglik: -2.8362e+02 - logprior: -1.8511e+00
Epoch 9/10
11/11 - 1s - loss: 284.6443 - loglik: -2.8278e+02 - logprior: -1.8571e+00
Epoch 10/10
11/11 - 1s - loss: 284.6277 - loglik: -2.8274e+02 - logprior: -1.8856e+00
Fitted a model with MAP estimate = -284.0195
expansions: [(0, 3), (15, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 2), (31, 1), (32, 2), (33, 1), (36, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 306.8961 - loglik: -2.9323e+02 - logprior: -1.3667e+01
Epoch 2/2
11/11 - 1s - loss: 281.0423 - loglik: -2.7683e+02 - logprior: -4.2116e+00
Fitted a model with MAP estimate = -277.5108
expansions: []
discards: [ 0 36 39 44]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 298.8975 - loglik: -2.8578e+02 - logprior: -1.3121e+01
Epoch 2/2
11/11 - 1s - loss: 281.7252 - loglik: -2.7621e+02 - logprior: -5.5109e+00
Fitted a model with MAP estimate = -278.4723
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 295.5815 - loglik: -2.8480e+02 - logprior: -1.0782e+01
Epoch 2/10
11/11 - 1s - loss: 278.0058 - loglik: -2.7505e+02 - logprior: -2.9526e+00
Epoch 3/10
11/11 - 1s - loss: 276.2156 - loglik: -2.7446e+02 - logprior: -1.7587e+00
Epoch 4/10
11/11 - 1s - loss: 275.5995 - loglik: -2.7405e+02 - logprior: -1.5477e+00
Epoch 5/10
11/11 - 1s - loss: 275.1501 - loglik: -2.7366e+02 - logprior: -1.4900e+00
Epoch 6/10
11/11 - 1s - loss: 274.6079 - loglik: -2.7327e+02 - logprior: -1.3350e+00
Epoch 7/10
11/11 - 1s - loss: 273.8607 - loglik: -2.7264e+02 - logprior: -1.2158e+00
Epoch 8/10
11/11 - 1s - loss: 273.3551 - loglik: -2.7215e+02 - logprior: -1.2054e+00
Epoch 9/10
11/11 - 1s - loss: 272.9862 - loglik: -2.7176e+02 - logprior: -1.2200e+00
Epoch 10/10
11/11 - 1s - loss: 272.4717 - loglik: -2.7126e+02 - logprior: -1.2135e+00
Fitted a model with MAP estimate = -272.0380
Time for alignment: 33.0567
Fitting a model of length 56 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 392.4454 - loglik: -3.8095e+02 - logprior: -1.1490e+01
Epoch 2/10
11/11 - 1s - loss: 343.4121 - loglik: -3.4017e+02 - logprior: -3.2376e+00
Epoch 3/10
11/11 - 1s - loss: 312.0993 - loglik: -3.0969e+02 - logprior: -2.4073e+00
Epoch 4/10
11/11 - 1s - loss: 297.2141 - loglik: -2.9514e+02 - logprior: -2.0697e+00
Epoch 5/10
11/11 - 1s - loss: 290.9402 - loglik: -2.8913e+02 - logprior: -1.8102e+00
Epoch 6/10
11/11 - 1s - loss: 289.6352 - loglik: -2.8780e+02 - logprior: -1.8385e+00
Epoch 7/10
11/11 - 1s - loss: 287.7459 - loglik: -2.8599e+02 - logprior: -1.7551e+00
Epoch 8/10
11/11 - 1s - loss: 286.9791 - loglik: -2.8523e+02 - logprior: -1.7448e+00
Epoch 9/10
11/11 - 1s - loss: 285.7465 - loglik: -2.8399e+02 - logprior: -1.7505e+00
Epoch 10/10
11/11 - 1s - loss: 285.6398 - loglik: -2.8388e+02 - logprior: -1.7570e+00
Fitted a model with MAP estimate = -285.2877
expansions: [(0, 3), (15, 2), (26, 1), (27, 1), (28, 3), (29, 1), (30, 1), (31, 1), (34, 1), (37, 1), (43, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 72 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 304.8864 - loglik: -2.9123e+02 - logprior: -1.3659e+01
Epoch 2/2
11/11 - 1s - loss: 280.8783 - loglik: -2.7676e+02 - logprior: -4.1199e+00
Fitted a model with MAP estimate = -277.2440
expansions: []
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 299.3556 - loglik: -2.8626e+02 - logprior: -1.3094e+01
Epoch 2/2
11/11 - 1s - loss: 280.8712 - loglik: -2.7541e+02 - logprior: -5.4647e+00
Fitted a model with MAP estimate = -278.4657
expansions: [(0, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 3707 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 294.5323 - loglik: -2.8377e+02 - logprior: -1.0763e+01
Epoch 2/10
11/11 - 1s - loss: 279.0033 - loglik: -2.7607e+02 - logprior: -2.9343e+00
Epoch 3/10
11/11 - 1s - loss: 276.3741 - loglik: -2.7462e+02 - logprior: -1.7491e+00
Epoch 4/10
11/11 - 1s - loss: 275.2256 - loglik: -2.7369e+02 - logprior: -1.5367e+00
Epoch 5/10
11/11 - 1s - loss: 275.2653 - loglik: -2.7379e+02 - logprior: -1.4783e+00
Fitted a model with MAP estimate = -274.7726
Time for alignment: 29.1727
Computed alignments with likelihoods: ['-273.7201', '-274.7669', '-273.7149', '-272.0380', '-274.7726']
Best model has likelihood: -272.0380  (prior= -1.2021 )
time for generating output: 0.1127
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hr.projection.fasta
SP score = 0.9957142857142857
Training of 5 independent models on file rvp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d125310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8d20bc520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c70adc0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 428.4745 - loglik: -4.2775e+02 - logprior: -7.2223e-01
Epoch 2/10
42/42 - 3s - loss: 336.1476 - loglik: -3.3519e+02 - logprior: -9.5440e-01
Epoch 3/10
42/42 - 3s - loss: 333.4589 - loglik: -3.3253e+02 - logprior: -9.2915e-01
Epoch 4/10
42/42 - 3s - loss: 333.0418 - loglik: -3.3212e+02 - logprior: -9.2094e-01
Epoch 5/10
42/42 - 3s - loss: 332.6305 - loglik: -3.3171e+02 - logprior: -9.1152e-01
Epoch 6/10
42/42 - 3s - loss: 332.3689 - loglik: -3.3145e+02 - logprior: -9.0556e-01
Epoch 7/10
42/42 - 3s - loss: 332.4484 - loglik: -3.3154e+02 - logprior: -8.9596e-01
Fitted a model with MAP estimate = -329.5797
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (29, 1), (31, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (59, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 6s - loss: 304.0302 - loglik: -3.0308e+02 - logprior: -9.4937e-01
Epoch 2/2
42/42 - 4s - loss: 289.4551 - loglik: -2.8872e+02 - logprior: -7.3009e-01
Fitted a model with MAP estimate = -286.8823
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 287.6693 - loglik: -2.8706e+02 - logprior: -6.0884e-01
Epoch 2/10
59/59 - 5s - loss: 285.8347 - loglik: -2.8532e+02 - logprior: -5.1324e-01
Epoch 3/10
59/59 - 5s - loss: 285.3848 - loglik: -2.8488e+02 - logprior: -5.0102e-01
Epoch 4/10
59/59 - 5s - loss: 284.9138 - loglik: -2.8441e+02 - logprior: -4.9248e-01
Epoch 5/10
59/59 - 5s - loss: 285.0665 - loglik: -2.8457e+02 - logprior: -4.8618e-01
Fitted a model with MAP estimate = -284.8850
Time for alignment: 112.5186
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 429.3369 - loglik: -4.2861e+02 - logprior: -7.2248e-01
Epoch 2/10
42/42 - 3s - loss: 335.6478 - loglik: -3.3469e+02 - logprior: -9.5275e-01
Epoch 3/10
42/42 - 3s - loss: 332.6590 - loglik: -3.3173e+02 - logprior: -9.2865e-01
Epoch 4/10
42/42 - 3s - loss: 332.7696 - loglik: -3.3185e+02 - logprior: -9.1999e-01
Fitted a model with MAP estimate = -329.4001
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (31, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 7s - loss: 303.7472 - loglik: -3.0279e+02 - logprior: -9.5795e-01
Epoch 2/2
42/42 - 4s - loss: 289.3146 - loglik: -2.8858e+02 - logprior: -7.3124e-01
Fitted a model with MAP estimate = -286.8687
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 287.5760 - loglik: -2.8697e+02 - logprior: -6.0792e-01
Epoch 2/10
59/59 - 5s - loss: 285.9198 - loglik: -2.8541e+02 - logprior: -5.1050e-01
Epoch 3/10
59/59 - 5s - loss: 285.2491 - loglik: -2.8474e+02 - logprior: -5.0201e-01
Epoch 4/10
59/59 - 5s - loss: 285.4427 - loglik: -2.8494e+02 - logprior: -4.9289e-01
Fitted a model with MAP estimate = -285.0071
Time for alignment: 97.5894
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 429.0577 - loglik: -4.2833e+02 - logprior: -7.2430e-01
Epoch 2/10
42/42 - 3s - loss: 335.8484 - loglik: -3.3489e+02 - logprior: -9.5508e-01
Epoch 3/10
42/42 - 3s - loss: 333.3540 - loglik: -3.3242e+02 - logprior: -9.3164e-01
Epoch 4/10
42/42 - 3s - loss: 332.7023 - loglik: -3.3178e+02 - logprior: -9.2254e-01
Epoch 5/10
42/42 - 3s - loss: 332.5451 - loglik: -3.3163e+02 - logprior: -9.1182e-01
Epoch 6/10
42/42 - 3s - loss: 331.8132 - loglik: -3.3090e+02 - logprior: -9.0706e-01
Epoch 7/10
42/42 - 3s - loss: 332.0634 - loglik: -3.3115e+02 - logprior: -8.9847e-01
Fitted a model with MAP estimate = -329.1752
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (33, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 303.2223 - loglik: -3.0226e+02 - logprior: -9.5967e-01
Epoch 2/2
42/42 - 4s - loss: 290.1629 - loglik: -2.8943e+02 - logprior: -7.3037e-01
Fitted a model with MAP estimate = -286.9080
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 287.7572 - loglik: -2.8715e+02 - logprior: -6.1084e-01
Epoch 2/10
59/59 - 5s - loss: 285.8451 - loglik: -2.8533e+02 - logprior: -5.1335e-01
Epoch 3/10
59/59 - 5s - loss: 285.3151 - loglik: -2.8481e+02 - logprior: -5.0231e-01
Epoch 4/10
59/59 - 5s - loss: 285.2087 - loglik: -2.8471e+02 - logprior: -4.9410e-01
Epoch 5/10
59/59 - 5s - loss: 284.9998 - loglik: -2.8450e+02 - logprior: -4.8903e-01
Epoch 6/10
59/59 - 5s - loss: 284.9406 - loglik: -2.8444e+02 - logprior: -4.8046e-01
Epoch 7/10
59/59 - 5s - loss: 284.7391 - loglik: -2.8424e+02 - logprior: -4.7524e-01
Epoch 8/10
59/59 - 5s - loss: 284.7614 - loglik: -2.8427e+02 - logprior: -4.7196e-01
Fitted a model with MAP estimate = -284.5738
Time for alignment: 126.9411
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 429.2131 - loglik: -4.2849e+02 - logprior: -7.2294e-01
Epoch 2/10
42/42 - 3s - loss: 335.7517 - loglik: -3.3479e+02 - logprior: -9.5741e-01
Epoch 3/10
42/42 - 3s - loss: 332.9743 - loglik: -3.3204e+02 - logprior: -9.3128e-01
Epoch 4/10
42/42 - 3s - loss: 332.5915 - loglik: -3.3166e+02 - logprior: -9.2265e-01
Epoch 5/10
42/42 - 3s - loss: 332.6276 - loglik: -3.3171e+02 - logprior: -9.1250e-01
Fitted a model with MAP estimate = -329.3427
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (30, 1), (33, 1), (47, 1), (48, 1), (49, 1), (54, 1), (55, 1), (56, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 304.0029 - loglik: -3.0305e+02 - logprior: -9.5736e-01
Epoch 2/2
42/42 - 4s - loss: 289.4052 - loglik: -2.8867e+02 - logprior: -7.3239e-01
Fitted a model with MAP estimate = -286.8688
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 288.0349 - loglik: -2.8742e+02 - logprior: -6.1106e-01
Epoch 2/10
59/59 - 5s - loss: 285.5667 - loglik: -2.8505e+02 - logprior: -5.1316e-01
Epoch 3/10
59/59 - 5s - loss: 285.4333 - loglik: -2.8493e+02 - logprior: -5.0226e-01
Epoch 4/10
59/59 - 5s - loss: 284.8708 - loglik: -2.8437e+02 - logprior: -4.9447e-01
Epoch 5/10
59/59 - 5s - loss: 285.2776 - loglik: -2.8478e+02 - logprior: -4.8779e-01
Fitted a model with MAP estimate = -284.9048
Time for alignment: 104.3576
Fitting a model of length 76 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
42/42 - 6s - loss: 428.8400 - loglik: -4.2812e+02 - logprior: -7.2308e-01
Epoch 2/10
42/42 - 3s - loss: 336.4329 - loglik: -3.3548e+02 - logprior: -9.5215e-01
Epoch 3/10
42/42 - 3s - loss: 332.8656 - loglik: -3.3193e+02 - logprior: -9.2991e-01
Epoch 4/10
42/42 - 3s - loss: 333.1115 - loglik: -3.3219e+02 - logprior: -9.2011e-01
Fitted a model with MAP estimate = -329.6589
expansions: [(1, 1), (5, 1), (7, 1), (8, 1), (12, 1), (26, 1), (27, 1), (28, 1), (29, 1), (47, 1), (48, 1), (49, 1), (55, 1), (56, 1), (59, 1), (63, 1), (68, 1), (69, 1), (71, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 46841 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
42/42 - 8s - loss: 304.0469 - loglik: -3.0310e+02 - logprior: -9.5163e-01
Epoch 2/2
42/42 - 4s - loss: 289.2408 - loglik: -2.8850e+02 - logprior: -7.3528e-01
Fitted a model with MAP estimate = -286.7056
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 93681 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
59/59 - 8s - loss: 287.5253 - loglik: -2.8691e+02 - logprior: -6.1202e-01
Epoch 2/10
59/59 - 5s - loss: 285.4938 - loglik: -2.8498e+02 - logprior: -5.1011e-01
Epoch 3/10
59/59 - 5s - loss: 285.7030 - loglik: -2.8520e+02 - logprior: -4.9880e-01
Fitted a model with MAP estimate = -285.1088
Time for alignment: 92.2395
Computed alignments with likelihoods: ['-284.8850', '-285.0071', '-284.5738', '-284.9048', '-285.1088']
Best model has likelihood: -284.5738  (prior= -0.4709 )
time for generating output: 0.1482
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rvp.projection.fasta
SP score = 0.18335588633288227
Training of 5 independent models on file mmp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe90551d7f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bbd40a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe952a14310>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 934.2911 - loglik: -9.1096e+02 - logprior: -2.3333e+01
Epoch 2/10
14/14 - 3s - loss: 867.6993 - loglik: -8.6420e+02 - logprior: -3.5009e+00
Epoch 3/10
14/14 - 3s - loss: 832.9030 - loglik: -8.3139e+02 - logprior: -1.5132e+00
Epoch 4/10
14/14 - 3s - loss: 822.1984 - loglik: -8.2101e+02 - logprior: -1.1837e+00
Epoch 5/10
14/14 - 3s - loss: 813.5485 - loglik: -8.1253e+02 - logprior: -1.0199e+00
Epoch 6/10
14/14 - 3s - loss: 812.3968 - loglik: -8.1157e+02 - logprior: -8.2377e-01
Epoch 7/10
14/14 - 3s - loss: 811.6366 - loglik: -8.1092e+02 - logprior: -7.1464e-01
Epoch 8/10
14/14 - 3s - loss: 808.6535 - loglik: -8.0799e+02 - logprior: -6.6496e-01
Epoch 9/10
14/14 - 3s - loss: 809.6353 - loglik: -8.0898e+02 - logprior: -6.5133e-01
Fitted a model with MAP estimate = -807.0096
expansions: [(5, 1), (11, 1), (16, 5), (17, 1), (24, 1), (36, 3), (41, 1), (42, 1), (44, 1), (45, 1), (66, 2), (76, 1), (78, 5), (101, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 156 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 830.9946 - loglik: -8.0985e+02 - logprior: -2.1148e+01
Epoch 2/2
14/14 - 3s - loss: 806.2197 - loglik: -8.0334e+02 - logprior: -2.8817e+00
Fitted a model with MAP estimate = -801.8332
expansions: [(17, 1), (84, 1)]
discards: [ 1 18 46 61 62]
Re-initialized the encoder parameters.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 827.4938 - loglik: -8.0684e+02 - logprior: -2.0654e+01
Epoch 2/2
14/14 - 3s - loss: 806.8477 - loglik: -8.0410e+02 - logprior: -2.7522e+00
Fitted a model with MAP estimate = -803.5102
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 153 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 820.4954 - loglik: -7.9993e+02 - logprior: -2.0567e+01
Epoch 2/10
14/14 - 3s - loss: 808.6520 - loglik: -8.0619e+02 - logprior: -2.4668e+00
Epoch 3/10
14/14 - 3s - loss: 801.3940 - loglik: -8.0184e+02 - logprior: 0.4484
Epoch 4/10
14/14 - 3s - loss: 799.6180 - loglik: -8.0120e+02 - logprior: 1.5858
Epoch 5/10
14/14 - 3s - loss: 799.0364 - loglik: -8.0117e+02 - logprior: 2.1344
Epoch 6/10
14/14 - 3s - loss: 794.9440 - loglik: -7.9742e+02 - logprior: 2.4766
Epoch 7/10
14/14 - 3s - loss: 796.5073 - loglik: -7.9923e+02 - logprior: 2.7192
Fitted a model with MAP estimate = -794.5899
Time for alignment: 81.0967
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 932.8505 - loglik: -9.0952e+02 - logprior: -2.3329e+01
Epoch 2/10
14/14 - 3s - loss: 869.4116 - loglik: -8.6591e+02 - logprior: -3.5014e+00
Epoch 3/10
14/14 - 3s - loss: 833.5566 - loglik: -8.3207e+02 - logprior: -1.4835e+00
Epoch 4/10
14/14 - 3s - loss: 818.5737 - loglik: -8.1746e+02 - logprior: -1.1123e+00
Epoch 5/10
14/14 - 3s - loss: 814.4297 - loglik: -8.1355e+02 - logprior: -8.8329e-01
Epoch 6/10
14/14 - 3s - loss: 812.9434 - loglik: -8.1235e+02 - logprior: -5.8732e-01
Epoch 7/10
14/14 - 3s - loss: 810.0148 - loglik: -8.0951e+02 - logprior: -5.0804e-01
Epoch 8/10
14/14 - 3s - loss: 809.0107 - loglik: -8.0858e+02 - logprior: -4.2831e-01
Epoch 9/10
14/14 - 3s - loss: 807.6907 - loglik: -8.0727e+02 - logprior: -4.2138e-01
Epoch 10/10
14/14 - 3s - loss: 806.6581 - loglik: -8.0623e+02 - logprior: -4.2947e-01
Fitted a model with MAP estimate = -806.8355
expansions: [(5, 1), (11, 1), (16, 5), (17, 1), (18, 1), (36, 1), (38, 2), (43, 1), (45, 1), (66, 2), (76, 1), (78, 2), (79, 4), (80, 1), (102, 1), (109, 1), (110, 3), (111, 1), (113, 1)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 835.2476 - loglik: -8.1403e+02 - logprior: -2.1221e+01
Epoch 2/2
14/14 - 3s - loss: 805.1447 - loglik: -8.0218e+02 - logprior: -2.9677e+00
Fitted a model with MAP estimate = -803.5527
expansions: [(16, 1)]
discards: [47 58 59 95]
Re-initialized the encoder parameters.
Fitting a model of length 151 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 827.4583 - loglik: -8.0670e+02 - logprior: -2.0763e+01
Epoch 2/2
14/14 - 3s - loss: 805.8170 - loglik: -8.0309e+02 - logprior: -2.7266e+00
Fitted a model with MAP estimate = -803.9294
expansions: [(56, 3), (79, 1)]
discards: [57]
Re-initialized the encoder parameters.
Fitting a model of length 154 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 823.8182 - loglik: -8.0340e+02 - logprior: -2.0419e+01
Epoch 2/10
14/14 - 3s - loss: 803.3920 - loglik: -8.0095e+02 - logprior: -2.4429e+00
Epoch 3/10
14/14 - 3s - loss: 800.8046 - loglik: -8.0127e+02 - logprior: 0.4633
Epoch 4/10
14/14 - 3s - loss: 796.8519 - loglik: -7.9847e+02 - logprior: 1.6204
Epoch 5/10
14/14 - 3s - loss: 795.3271 - loglik: -7.9755e+02 - logprior: 2.2244
Epoch 6/10
14/14 - 3s - loss: 796.7518 - loglik: -7.9929e+02 - logprior: 2.5435
Fitted a model with MAP estimate = -794.4958
Time for alignment: 81.3666
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 934.9410 - loglik: -9.1160e+02 - logprior: -2.3341e+01
Epoch 2/10
14/14 - 3s - loss: 863.2289 - loglik: -8.5975e+02 - logprior: -3.4779e+00
Epoch 3/10
14/14 - 3s - loss: 831.0422 - loglik: -8.2973e+02 - logprior: -1.3128e+00
Epoch 4/10
14/14 - 3s - loss: 817.6231 - loglik: -8.1678e+02 - logprior: -8.4493e-01
Epoch 5/10
14/14 - 3s - loss: 813.7520 - loglik: -8.1313e+02 - logprior: -6.2067e-01
Epoch 6/10
14/14 - 3s - loss: 812.1994 - loglik: -8.1187e+02 - logprior: -3.2501e-01
Epoch 7/10
14/14 - 3s - loss: 813.2921 - loglik: -8.1299e+02 - logprior: -3.0032e-01
Fitted a model with MAP estimate = -809.6877
expansions: [(12, 1), (16, 4), (18, 1), (35, 1), (37, 2), (38, 2), (67, 2), (77, 1), (79, 5), (100, 1), (102, 1), (109, 1), (110, 3), (111, 1), (113, 1)]
discards: [ 0  1 45]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 844.7846 - loglik: -8.1690e+02 - logprior: -2.7885e+01
Epoch 2/2
14/14 - 3s - loss: 816.7238 - loglik: -8.0708e+02 - logprior: -9.6452e+00
Fitted a model with MAP estimate = -813.6128
expansions: [(0, 29)]
discards: [ 0 45]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 828.4107 - loglik: -8.0734e+02 - logprior: -2.1073e+01
Epoch 2/2
14/14 - 4s - loss: 808.2132 - loglik: -8.0556e+02 - logprior: -2.6499e+00
Fitted a model with MAP estimate = -804.2567
expansions: [(104, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 830.3926 - loglik: -8.0496e+02 - logprior: -2.5429e+01
Epoch 2/10
14/14 - 3s - loss: 805.9357 - loglik: -8.0239e+02 - logprior: -3.5439e+00
Epoch 3/10
14/14 - 3s - loss: 803.2564 - loglik: -8.0366e+02 - logprior: 0.4034
Epoch 4/10
14/14 - 3s - loss: 797.8956 - loglik: -7.9946e+02 - logprior: 1.5693
Epoch 5/10
14/14 - 3s - loss: 797.8554 - loglik: -8.0001e+02 - logprior: 2.1550
Epoch 6/10
14/14 - 3s - loss: 799.8506 - loglik: -8.0235e+02 - logprior: 2.4977
Fitted a model with MAP estimate = -797.1203
Time for alignment: 71.8613
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 5s - loss: 930.7671 - loglik: -9.0744e+02 - logprior: -2.3329e+01
Epoch 2/10
14/14 - 3s - loss: 867.3541 - loglik: -8.6390e+02 - logprior: -3.4555e+00
Epoch 3/10
14/14 - 3s - loss: 830.3900 - loglik: -8.2904e+02 - logprior: -1.3547e+00
Epoch 4/10
14/14 - 3s - loss: 818.7802 - loglik: -8.1794e+02 - logprior: -8.3632e-01
Epoch 5/10
14/14 - 3s - loss: 815.8506 - loglik: -8.1526e+02 - logprior: -5.9080e-01
Epoch 6/10
14/14 - 3s - loss: 813.2783 - loglik: -8.1293e+02 - logprior: -3.5079e-01
Epoch 7/10
14/14 - 3s - loss: 813.0972 - loglik: -8.1282e+02 - logprior: -2.7299e-01
Epoch 8/10
14/14 - 3s - loss: 809.7824 - loglik: -8.0951e+02 - logprior: -2.7316e-01
Epoch 9/10
14/14 - 3s - loss: 807.7915 - loglik: -8.0747e+02 - logprior: -3.2027e-01
Epoch 10/10
14/14 - 3s - loss: 807.3981 - loglik: -8.0706e+02 - logprior: -3.3247e-01
Fitted a model with MAP estimate = -808.0958
expansions: [(12, 2), (16, 5), (17, 1), (34, 1), (36, 3), (37, 2), (44, 1), (45, 1), (66, 1), (67, 1), (77, 1), (79, 2), (80, 4), (101, 1), (102, 2), (111, 3), (112, 1), (113, 2)]
discards: [1]
Re-initialized the encoder parameters.
Fitting a model of length 157 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 8s - loss: 833.2271 - loglik: -8.1195e+02 - logprior: -2.1273e+01
Epoch 2/2
14/14 - 3s - loss: 805.8963 - loglik: -8.0277e+02 - logprior: -3.1218e+00
Fitted a model with MAP estimate = -801.8971
expansions: [(58, 1)]
discards: [ 12  13  45  48  60  61  62  98 101]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 6s - loss: 829.4139 - loglik: -8.0855e+02 - logprior: -2.0860e+01
Epoch 2/2
14/14 - 3s - loss: 805.2025 - loglik: -8.0233e+02 - logprior: -2.8762e+00
Fitted a model with MAP estimate = -803.5382
expansions: [(12, 1), (54, 2)]
discards: [55 93]
Re-initialized the encoder parameters.
Fitting a model of length 150 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 823.7464 - loglik: -8.0298e+02 - logprior: -2.0767e+01
Epoch 2/10
14/14 - 3s - loss: 806.8472 - loglik: -8.0408e+02 - logprior: -2.7627e+00
Epoch 3/10
14/14 - 3s - loss: 801.0959 - loglik: -8.0132e+02 - logprior: 0.2281
Epoch 4/10
14/14 - 3s - loss: 797.7968 - loglik: -7.9920e+02 - logprior: 1.4057
Epoch 5/10
14/14 - 3s - loss: 800.6265 - loglik: -8.0257e+02 - logprior: 1.9452
Fitted a model with MAP estimate = -797.2165
Time for alignment: 76.3382
Fitting a model of length 124 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 934.2579 - loglik: -9.1093e+02 - logprior: -2.3328e+01
Epoch 2/10
14/14 - 3s - loss: 863.9805 - loglik: -8.6049e+02 - logprior: -3.4948e+00
Epoch 3/10
14/14 - 3s - loss: 829.1865 - loglik: -8.2771e+02 - logprior: -1.4768e+00
Epoch 4/10
14/14 - 3s - loss: 819.0818 - loglik: -8.1786e+02 - logprior: -1.2223e+00
Epoch 5/10
14/14 - 3s - loss: 811.3021 - loglik: -8.1024e+02 - logprior: -1.0591e+00
Epoch 6/10
14/14 - 3s - loss: 815.4447 - loglik: -8.1462e+02 - logprior: -8.2636e-01
Fitted a model with MAP estimate = -812.1034
expansions: [(10, 1), (11, 1), (16, 4), (18, 1), (35, 1), (36, 3), (38, 1), (43, 1), (44, 1), (66, 1), (70, 1), (79, 5), (100, 1), (102, 1), (109, 2), (110, 3), (111, 1), (113, 1)]
discards: [ 0  1 45 46 47 67]
Re-initialized the encoder parameters.
Fitting a model of length 148 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 848.1083 - loglik: -8.2010e+02 - logprior: -2.8006e+01
Epoch 2/2
14/14 - 3s - loss: 817.9728 - loglik: -8.0804e+02 - logprior: -9.9305e+00
Fitted a model with MAP estimate = -815.3610
expansions: [(0, 25), (55, 3), (57, 1)]
discards: [ 0 43 59]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
14/14 - 7s - loss: 831.4617 - loglik: -8.1019e+02 - logprior: -2.1267e+01
Epoch 2/2
14/14 - 4s - loss: 806.5589 - loglik: -8.0370e+02 - logprior: -2.8573e+00
Fitted a model with MAP estimate = -803.3458
expansions: [(79, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 81
 86 87]
Re-initialized the encoder parameters.
Fitting a model of length 149 on 1427 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 834.9508 - loglik: -8.0930e+02 - logprior: -2.5646e+01
Epoch 2/10
14/14 - 3s - loss: 807.5590 - loglik: -8.0381e+02 - logprior: -3.7520e+00
Epoch 3/10
14/14 - 3s - loss: 803.5405 - loglik: -8.0382e+02 - logprior: 0.2776
Epoch 4/10
14/14 - 3s - loss: 805.0984 - loglik: -8.0654e+02 - logprior: 1.4441
Fitted a model with MAP estimate = -801.5249
Time for alignment: 63.2325
Computed alignments with likelihoods: ['-794.5899', '-794.4958', '-797.1203', '-797.2165', '-801.5249']
Best model has likelihood: -794.4958  (prior= 2.7162 )
time for generating output: 0.1730
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mmp.projection.fasta
SP score = 0.9655317360235393
Training of 5 independent models on file icd.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9abf2a160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efabdc40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3b99d160>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 24s - loss: 1795.6378 - loglik: -1.7929e+03 - logprior: -2.7145e+00
Epoch 2/10
29/29 - 19s - loss: 1620.6958 - loglik: -1.6191e+03 - logprior: -1.6073e+00
Epoch 3/10
29/29 - 19s - loss: 1584.9524 - loglik: -1.5832e+03 - logprior: -1.7470e+00
Epoch 4/10
29/29 - 19s - loss: 1579.3048 - loglik: -1.5776e+03 - logprior: -1.7445e+00
Epoch 5/10
29/29 - 19s - loss: 1575.1549 - loglik: -1.5734e+03 - logprior: -1.7713e+00
Epoch 6/10
29/29 - 19s - loss: 1571.7772 - loglik: -1.5700e+03 - logprior: -1.7907e+00
Epoch 7/10
29/29 - 19s - loss: 1569.5679 - loglik: -1.5678e+03 - logprior: -1.7959e+00
Epoch 8/10
29/29 - 19s - loss: 1567.2733 - loglik: -1.5654e+03 - logprior: -1.8196e+00
Epoch 9/10
29/29 - 19s - loss: 1571.8580 - loglik: -1.5700e+03 - logprior: -1.8500e+00
Fitted a model with MAP estimate = -1566.8415
expansions: [(16, 1), (17, 1), (24, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (49, 2), (50, 3), (69, 1), (76, 1), (87, 1), (88, 1), (89, 2), (96, 2), (116, 8), (120, 3), (121, 1), (123, 1), (127, 1), (141, 1), (143, 1), (150, 1), (151, 1), (153, 1), (154, 1), (171, 1), (172, 1), (183, 1), (184, 1), (185, 2), (191, 1), (204, 1), (215, 1), (217, 2), (218, 2), (248, 1), (249, 1), (251, 2), (258, 1), (260, 2), (261, 1), (262, 1), (269, 2), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 349 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 1565.9723 - loglik: -1.5611e+03 - logprior: -4.9151e+00
Epoch 2/2
29/29 - 27s - loss: 1534.0027 - loglik: -1.5319e+03 - logprior: -2.1313e+00
Fitted a model with MAP estimate = -1528.6521
expansions: [(0, 2)]
discards: [  0  62  63 116 142 143 144 145 146 147 151]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1535.8704 - loglik: -1.5330e+03 - logprior: -2.8909e+00
Epoch 2/2
29/29 - 25s - loss: 1532.5322 - loglik: -1.5325e+03 - logprior: -6.6330e-02
Fitted a model with MAP estimate = -1528.8750
expansions: [(325, 2)]
discards: [  0 299 329]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 1538.3259 - loglik: -1.5342e+03 - logprior: -4.1703e+00
Epoch 2/10
29/29 - 25s - loss: 1531.1814 - loglik: -1.5308e+03 - logprior: -4.1126e-01
Epoch 3/10
29/29 - 25s - loss: 1526.9047 - loglik: -1.5277e+03 - logprior: 0.8230
Epoch 4/10
29/29 - 25s - loss: 1525.5699 - loglik: -1.5266e+03 - logprior: 0.9928
Epoch 5/10
29/29 - 25s - loss: 1521.4115 - loglik: -1.5226e+03 - logprior: 1.1693
Epoch 6/10
29/29 - 25s - loss: 1525.8478 - loglik: -1.5265e+03 - logprior: 0.7061
Fitted a model with MAP estimate = -1520.9488
Time for alignment: 541.9760
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 1793.9318 - loglik: -1.7912e+03 - logprior: -2.7315e+00
Epoch 2/10
29/29 - 19s - loss: 1615.2351 - loglik: -1.6136e+03 - logprior: -1.6072e+00
Epoch 3/10
29/29 - 19s - loss: 1593.8701 - loglik: -1.5921e+03 - logprior: -1.7504e+00
Epoch 4/10
29/29 - 19s - loss: 1583.2468 - loglik: -1.5814e+03 - logprior: -1.7989e+00
Epoch 5/10
29/29 - 19s - loss: 1579.9939 - loglik: -1.5782e+03 - logprior: -1.7947e+00
Epoch 6/10
29/29 - 19s - loss: 1581.0015 - loglik: -1.5792e+03 - logprior: -1.8100e+00
Fitted a model with MAP estimate = -1576.2459
expansions: [(16, 1), (17, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 1), (36, 2), (41, 1), (48, 2), (49, 1), (50, 1), (73, 1), (76, 1), (87, 2), (88, 1), (89, 1), (121, 3), (123, 1), (124, 1), (128, 1), (142, 1), (148, 1), (155, 1), (156, 1), (165, 1), (172, 1), (173, 1), (182, 1), (185, 2), (186, 1), (191, 1), (192, 1), (205, 1), (216, 1), (218, 2), (219, 2), (234, 1), (250, 1), (252, 1), (253, 3), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1567.6072 - loglik: -1.5628e+03 - logprior: -4.7911e+00
Epoch 2/2
29/29 - 26s - loss: 1545.7792 - loglik: -1.5438e+03 - logprior: -1.9918e+00
Fitted a model with MAP estimate = -1540.6039
expansions: [(0, 2)]
discards: [  0  37  63 104 142 219 301 302 327]
Re-initialized the encoder parameters.
Fitting a model of length 335 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1547.1525 - loglik: -1.5443e+03 - logprior: -2.8113e+00
Epoch 2/2
29/29 - 25s - loss: 1540.3640 - loglik: -1.5403e+03 - logprior: -9.4567e-02
Fitted a model with MAP estimate = -1539.9223
expansions: [(79, 3)]
discards: [  0 324]
Re-initialized the encoder parameters.
Fitting a model of length 336 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 1550.3153 - loglik: -1.5461e+03 - logprior: -4.1944e+00
Epoch 2/10
29/29 - 25s - loss: 1536.1421 - loglik: -1.5358e+03 - logprior: -3.6044e-01
Epoch 3/10
29/29 - 25s - loss: 1540.8679 - loglik: -1.5413e+03 - logprior: 0.3987
Fitted a model with MAP estimate = -1537.0417
Time for alignment: 402.1379
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 23s - loss: 1796.7654 - loglik: -1.7941e+03 - logprior: -2.7094e+00
Epoch 2/10
29/29 - 19s - loss: 1617.9640 - loglik: -1.6163e+03 - logprior: -1.6429e+00
Epoch 3/10
29/29 - 19s - loss: 1586.3401 - loglik: -1.5845e+03 - logprior: -1.8278e+00
Epoch 4/10
29/29 - 19s - loss: 1585.7996 - loglik: -1.5840e+03 - logprior: -1.7538e+00
Epoch 5/10
29/29 - 19s - loss: 1578.6182 - loglik: -1.5768e+03 - logprior: -1.7752e+00
Epoch 6/10
29/29 - 19s - loss: 1577.7102 - loglik: -1.5759e+03 - logprior: -1.8486e+00
Epoch 7/10
29/29 - 19s - loss: 1573.1981 - loglik: -1.5713e+03 - logprior: -1.8630e+00
Epoch 8/10
29/29 - 19s - loss: 1574.9294 - loglik: -1.5730e+03 - logprior: -1.9070e+00
Fitted a model with MAP estimate = -1571.7308
expansions: [(16, 1), (17, 1), (24, 2), (28, 1), (29, 1), (30, 1), (31, 1), (39, 1), (50, 3), (70, 1), (77, 1), (88, 1), (89, 1), (90, 2), (97, 1), (122, 2), (124, 1), (125, 1), (129, 1), (140, 1), (142, 1), (152, 1), (155, 3), (163, 1), (166, 2), (173, 1), (182, 1), (185, 1), (186, 1), (191, 1), (192, 1), (205, 1), (218, 2), (219, 2), (220, 1), (233, 1), (249, 1), (250, 2), (252, 1), (256, 1), (258, 1), (260, 2), (261, 1), (262, 1), (270, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1564.2639 - loglik: -1.5594e+03 - logprior: -4.8323e+00
Epoch 2/2
29/29 - 25s - loss: 1537.2479 - loglik: -1.5354e+03 - logprior: -1.8341e+00
Fitted a model with MAP estimate = -1533.5383
expansions: [(0, 2), (58, 1), (60, 1), (327, 1)]
discards: [  0 196 260 296]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1538.7303 - loglik: -1.5359e+03 - logprior: -2.8583e+00
Epoch 2/2
29/29 - 25s - loss: 1530.7036 - loglik: -1.5307e+03 - logprior: -1.3102e-02
Fitted a model with MAP estimate = -1529.7702
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 30s - loss: 1538.7765 - loglik: -1.5347e+03 - logprior: -4.1239e+00
Epoch 2/10
29/29 - 25s - loss: 1530.6304 - loglik: -1.5303e+03 - logprior: -3.7211e-01
Epoch 3/10
29/29 - 25s - loss: 1530.0779 - loglik: -1.5309e+03 - logprior: 0.8325
Epoch 4/10
29/29 - 25s - loss: 1528.5986 - loglik: -1.5296e+03 - logprior: 1.0125
Epoch 5/10
29/29 - 25s - loss: 1522.8004 - loglik: -1.5240e+03 - logprior: 1.2153
Epoch 6/10
29/29 - 25s - loss: 1521.4049 - loglik: -1.5228e+03 - logprior: 1.3929
Epoch 7/10
29/29 - 25s - loss: 1521.6710 - loglik: -1.5226e+03 - logprior: 0.8917
Fitted a model with MAP estimate = -1520.4158
Time for alignment: 544.3177
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 1795.1700 - loglik: -1.7925e+03 - logprior: -2.7186e+00
Epoch 2/10
29/29 - 19s - loss: 1614.2839 - loglik: -1.6126e+03 - logprior: -1.6915e+00
Epoch 3/10
29/29 - 19s - loss: 1584.5529 - loglik: -1.5828e+03 - logprior: -1.7773e+00
Epoch 4/10
29/29 - 19s - loss: 1575.8458 - loglik: -1.5741e+03 - logprior: -1.7475e+00
Epoch 5/10
29/29 - 19s - loss: 1578.3259 - loglik: -1.5766e+03 - logprior: -1.7302e+00
Fitted a model with MAP estimate = -1573.5066
expansions: [(16, 1), (17, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (46, 1), (48, 2), (49, 2), (65, 4), (72, 1), (76, 1), (89, 1), (90, 3), (97, 2), (121, 3), (123, 1), (124, 1), (128, 1), (142, 1), (144, 1), (149, 1), (154, 2), (163, 1), (172, 1), (173, 1), (184, 1), (185, 1), (186, 1), (193, 1), (202, 1), (206, 1), (218, 2), (219, 2), (220, 1), (233, 1), (249, 1), (250, 1), (252, 1), (256, 1), (258, 1), (260, 2), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 346 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 29s - loss: 1563.4664 - loglik: -1.5587e+03 - logprior: -4.7938e+00
Epoch 2/2
29/29 - 26s - loss: 1534.7325 - loglik: -1.5328e+03 - logprior: -1.9136e+00
Fitted a model with MAP estimate = -1530.4494
expansions: [(0, 2), (26, 1), (61, 1)]
discards: [  0  39 112 147 188 266 330 331]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 1538.0411 - loglik: -1.5351e+03 - logprior: -2.9444e+00
Epoch 2/2
29/29 - 26s - loss: 1529.6335 - loglik: -1.5293e+03 - logprior: -2.8976e-01
Fitted a model with MAP estimate = -1529.2799
expansions: [(328, 2)]
discards: [  0 121]
Re-initialized the encoder parameters.
Fitting a model of length 342 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 29s - loss: 1538.2990 - loglik: -1.5339e+03 - logprior: -4.3695e+00
Epoch 2/10
29/29 - 26s - loss: 1530.5889 - loglik: -1.5300e+03 - logprior: -5.5318e-01
Epoch 3/10
29/29 - 26s - loss: 1530.1650 - loglik: -1.5308e+03 - logprior: 0.6009
Epoch 4/10
29/29 - 26s - loss: 1528.7828 - loglik: -1.5292e+03 - logprior: 0.4456
Epoch 5/10
29/29 - 26s - loss: 1522.4528 - loglik: -1.5234e+03 - logprior: 0.9842
Epoch 6/10
29/29 - 26s - loss: 1523.7596 - loglik: -1.5246e+03 - logprior: 0.8798
Fitted a model with MAP estimate = -1521.2104
Time for alignment: 466.8975
Fitting a model of length 279 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 22s - loss: 1797.1765 - loglik: -1.7945e+03 - logprior: -2.7241e+00
Epoch 2/10
29/29 - 19s - loss: 1617.5616 - loglik: -1.6159e+03 - logprior: -1.6297e+00
Epoch 3/10
29/29 - 19s - loss: 1589.7255 - loglik: -1.5879e+03 - logprior: -1.8325e+00
Epoch 4/10
29/29 - 19s - loss: 1581.8257 - loglik: -1.5800e+03 - logprior: -1.8170e+00
Epoch 5/10
29/29 - 19s - loss: 1580.5859 - loglik: -1.5788e+03 - logprior: -1.8140e+00
Epoch 6/10
29/29 - 19s - loss: 1577.0903 - loglik: -1.5752e+03 - logprior: -1.8487e+00
Epoch 7/10
29/29 - 19s - loss: 1575.2834 - loglik: -1.5734e+03 - logprior: -1.8815e+00
Epoch 8/10
29/29 - 19s - loss: 1573.1722 - loglik: -1.5713e+03 - logprior: -1.9100e+00
Epoch 9/10
29/29 - 19s - loss: 1576.8708 - loglik: -1.5749e+03 - logprior: -1.9447e+00
Fitted a model with MAP estimate = -1573.3010
expansions: [(16, 1), (17, 1), (24, 1), (28, 1), (29, 1), (30, 1), (31, 1), (37, 1), (39, 1), (48, 1), (49, 1), (64, 1), (73, 1), (76, 1), (89, 2), (90, 2), (91, 1), (97, 2), (119, 2), (120, 2), (121, 1), (123, 1), (124, 1), (125, 1), (128, 1), (142, 2), (149, 1), (153, 1), (155, 2), (163, 1), (166, 2), (173, 1), (184, 1), (185, 1), (186, 2), (192, 1), (205, 1), (216, 1), (218, 2), (219, 3), (249, 1), (250, 1), (251, 3), (258, 1), (260, 1), (261, 1), (263, 1), (264, 1), (269, 3), (270, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 347 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 30s - loss: 1569.7864 - loglik: -1.5649e+03 - logprior: -4.8450e+00
Epoch 2/2
29/29 - 26s - loss: 1536.6788 - loglik: -1.5346e+03 - logprior: -2.0869e+00
Fitted a model with MAP estimate = -1535.7133
expansions: [(0, 2), (25, 1)]
discards: [  0 103 106 115 171 188 202 265 304 330 332]
Re-initialized the encoder parameters.
Fitting a model of length 339 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 28s - loss: 1543.7716 - loglik: -1.5409e+03 - logprior: -2.8973e+00
Epoch 2/2
29/29 - 25s - loss: 1536.4570 - loglik: -1.5362e+03 - logprior: -2.5149e-01
Fitted a model with MAP estimate = -1534.0680
expansions: [(325, 2)]
discards: [  0  41 328]
Re-initialized the encoder parameters.
Fitting a model of length 338 on 5678 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 28s - loss: 1542.6130 - loglik: -1.5384e+03 - logprior: -4.2578e+00
Epoch 2/10
29/29 - 25s - loss: 1535.1509 - loglik: -1.5347e+03 - logprior: -4.5552e-01
Epoch 3/10
29/29 - 25s - loss: 1533.0504 - loglik: -1.5337e+03 - logprior: 0.6768
Epoch 4/10
29/29 - 25s - loss: 1536.7034 - loglik: -1.5373e+03 - logprior: 0.5950
Fitted a model with MAP estimate = -1530.0618
Time for alignment: 486.5407
Computed alignments with likelihoods: ['-1520.9488', '-1537.0417', '-1520.4158', '-1521.2104', '-1530.0618']
Best model has likelihood: -1520.4158  (prior= 1.1959 )
time for generating output: 0.3437
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/icd.projection.fasta
SP score = 0.8819102749638206
Training of 5 independent models on file seatoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea228359a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea22fd2b80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bd69640>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 746.3381 - loglik: -2.5756e+02 - logprior: -4.8878e+02
Epoch 2/10
10/10 - 0s - loss: 362.1268 - loglik: -2.2710e+02 - logprior: -1.3503e+02
Epoch 3/10
10/10 - 0s - loss: 269.4843 - loglik: -2.0712e+02 - logprior: -6.2361e+01
Epoch 4/10
10/10 - 0s - loss: 230.5065 - loglik: -1.9588e+02 - logprior: -3.4624e+01
Epoch 5/10
10/10 - 0s - loss: 210.5292 - loglik: -1.9064e+02 - logprior: -1.9891e+01
Epoch 6/10
10/10 - 0s - loss: 199.8646 - loglik: -1.8905e+02 - logprior: -1.0814e+01
Epoch 7/10
10/10 - 0s - loss: 193.6377 - loglik: -1.8887e+02 - logprior: -4.7717e+00
Epoch 8/10
10/10 - 0s - loss: 189.7215 - loglik: -1.8885e+02 - logprior: -8.6675e-01
Epoch 9/10
10/10 - 0s - loss: 187.0890 - loglik: -1.8881e+02 - logprior: 1.7182
Epoch 10/10
10/10 - 0s - loss: 185.2189 - loglik: -1.8876e+02 - logprior: 3.5392
Fitted a model with MAP estimate = -184.3809
expansions: [(0, 3), (7, 1), (15, 1), (17, 2), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 847.0464 - loglik: -1.9458e+02 - logprior: -6.5246e+02
Epoch 2/2
10/10 - 0s - loss: 380.3632 - loglik: -1.7668e+02 - logprior: -2.0368e+02
Fitted a model with MAP estimate = -291.5886
expansions: []
discards: [22]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 634.5132 - loglik: -1.7909e+02 - logprior: -4.5543e+02
Epoch 2/2
10/10 - 0s - loss: 292.2665 - loglik: -1.7091e+02 - logprior: -1.2135e+02
Fitted a model with MAP estimate = -240.6786
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 605.8312 - loglik: -1.7782e+02 - logprior: -4.2801e+02
Epoch 2/10
10/10 - 0s - loss: 284.7143 - loglik: -1.7073e+02 - logprior: -1.1399e+02
Epoch 3/10
10/10 - 0s - loss: 216.9594 - loglik: -1.6958e+02 - logprior: -4.7376e+01
Epoch 4/10
10/10 - 0s - loss: 188.5363 - loglik: -1.6967e+02 - logprior: -1.8864e+01
Epoch 5/10
10/10 - 0s - loss: 172.5945 - loglik: -1.6960e+02 - logprior: -2.9961e+00
Epoch 6/10
10/10 - 0s - loss: 163.2418 - loglik: -1.6966e+02 - logprior: 6.4208
Epoch 7/10
10/10 - 0s - loss: 157.2863 - loglik: -1.6962e+02 - logprior: 12.3348
Epoch 8/10
10/10 - 0s - loss: 153.2050 - loglik: -1.6961e+02 - logprior: 16.4019
Epoch 9/10
10/10 - 0s - loss: 150.1824 - loglik: -1.6964e+02 - logprior: 19.4589
Epoch 10/10
10/10 - 0s - loss: 147.7574 - loglik: -1.6968e+02 - logprior: 21.9196
Fitted a model with MAP estimate = -146.5672
Time for alignment: 25.6437
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 746.3381 - loglik: -2.5756e+02 - logprior: -4.8878e+02
Epoch 2/10
10/10 - 0s - loss: 362.1268 - loglik: -2.2710e+02 - logprior: -1.3503e+02
Epoch 3/10
10/10 - 0s - loss: 269.4843 - loglik: -2.0712e+02 - logprior: -6.2361e+01
Epoch 4/10
10/10 - 0s - loss: 230.5065 - loglik: -1.9588e+02 - logprior: -3.4624e+01
Epoch 5/10
10/10 - 0s - loss: 210.5292 - loglik: -1.9064e+02 - logprior: -1.9891e+01
Epoch 6/10
10/10 - 0s - loss: 199.8646 - loglik: -1.8905e+02 - logprior: -1.0814e+01
Epoch 7/10
10/10 - 0s - loss: 193.6377 - loglik: -1.8887e+02 - logprior: -4.7717e+00
Epoch 8/10
10/10 - 0s - loss: 189.7215 - loglik: -1.8885e+02 - logprior: -8.6675e-01
Epoch 9/10
10/10 - 0s - loss: 187.0889 - loglik: -1.8881e+02 - logprior: 1.7182
Epoch 10/10
10/10 - 0s - loss: 185.2189 - loglik: -1.8876e+02 - logprior: 3.5392
Fitted a model with MAP estimate = -184.3815
expansions: [(0, 3), (7, 1), (15, 1), (17, 2), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 847.0464 - loglik: -1.9458e+02 - logprior: -6.5246e+02
Epoch 2/2
10/10 - 0s - loss: 380.3632 - loglik: -1.7668e+02 - logprior: -2.0368e+02
Fitted a model with MAP estimate = -291.5886
expansions: []
discards: [22]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 634.5132 - loglik: -1.7909e+02 - logprior: -4.5543e+02
Epoch 2/2
10/10 - 0s - loss: 292.2665 - loglik: -1.7091e+02 - logprior: -1.2135e+02
Fitted a model with MAP estimate = -240.6786
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 605.8311 - loglik: -1.7782e+02 - logprior: -4.2801e+02
Epoch 2/10
10/10 - 0s - loss: 284.7143 - loglik: -1.7073e+02 - logprior: -1.1399e+02
Epoch 3/10
10/10 - 0s - loss: 216.9594 - loglik: -1.6958e+02 - logprior: -4.7376e+01
Epoch 4/10
10/10 - 0s - loss: 188.5362 - loglik: -1.6967e+02 - logprior: -1.8864e+01
Epoch 5/10
10/10 - 0s - loss: 172.5943 - loglik: -1.6960e+02 - logprior: -2.9960e+00
Epoch 6/10
10/10 - 0s - loss: 163.2415 - loglik: -1.6966e+02 - logprior: 6.4210
Epoch 7/10
10/10 - 0s - loss: 157.2861 - loglik: -1.6962e+02 - logprior: 12.3350
Epoch 8/10
10/10 - 0s - loss: 153.2048 - loglik: -1.6961e+02 - logprior: 16.4022
Epoch 9/10
10/10 - 0s - loss: 150.1822 - loglik: -1.6964e+02 - logprior: 19.4592
Epoch 10/10
10/10 - 0s - loss: 147.7571 - loglik: -1.6968e+02 - logprior: 21.9199
Fitted a model with MAP estimate = -146.5667
Time for alignment: 25.3572
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 746.3381 - loglik: -2.5756e+02 - logprior: -4.8878e+02
Epoch 2/10
10/10 - 0s - loss: 362.1268 - loglik: -2.2710e+02 - logprior: -1.3503e+02
Epoch 3/10
10/10 - 0s - loss: 269.4843 - loglik: -2.0712e+02 - logprior: -6.2361e+01
Epoch 4/10
10/10 - 0s - loss: 230.5065 - loglik: -1.9588e+02 - logprior: -3.4624e+01
Epoch 5/10
10/10 - 0s - loss: 210.5291 - loglik: -1.9064e+02 - logprior: -1.9891e+01
Epoch 6/10
10/10 - 0s - loss: 199.8646 - loglik: -1.8905e+02 - logprior: -1.0814e+01
Epoch 7/10
10/10 - 0s - loss: 193.6377 - loglik: -1.8887e+02 - logprior: -4.7717e+00
Epoch 8/10
10/10 - 0s - loss: 189.7215 - loglik: -1.8885e+02 - logprior: -8.6675e-01
Epoch 9/10
10/10 - 0s - loss: 187.0889 - loglik: -1.8881e+02 - logprior: 1.7182
Epoch 10/10
10/10 - 0s - loss: 185.2189 - loglik: -1.8876e+02 - logprior: 3.5392
Fitted a model with MAP estimate = -184.3810
expansions: [(0, 3), (7, 1), (15, 1), (17, 2), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 847.0465 - loglik: -1.9458e+02 - logprior: -6.5246e+02
Epoch 2/2
10/10 - 0s - loss: 380.3632 - loglik: -1.7668e+02 - logprior: -2.0368e+02
Fitted a model with MAP estimate = -291.5886
expansions: []
discards: [22]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 634.5132 - loglik: -1.7909e+02 - logprior: -4.5543e+02
Epoch 2/2
10/10 - 0s - loss: 292.2665 - loglik: -1.7091e+02 - logprior: -1.2135e+02
Fitted a model with MAP estimate = -240.6786
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 605.8310 - loglik: -1.7782e+02 - logprior: -4.2801e+02
Epoch 2/10
10/10 - 0s - loss: 284.7142 - loglik: -1.7072e+02 - logprior: -1.1399e+02
Epoch 3/10
10/10 - 0s - loss: 216.9594 - loglik: -1.6958e+02 - logprior: -4.7376e+01
Epoch 4/10
10/10 - 0s - loss: 188.5363 - loglik: -1.6967e+02 - logprior: -1.8864e+01
Epoch 5/10
10/10 - 0s - loss: 172.5944 - loglik: -1.6960e+02 - logprior: -2.9959e+00
Epoch 6/10
10/10 - 0s - loss: 163.2419 - loglik: -1.6966e+02 - logprior: 6.4203
Epoch 7/10
10/10 - 0s - loss: 157.2865 - loglik: -1.6962e+02 - logprior: 12.3351
Epoch 8/10
10/10 - 0s - loss: 153.2051 - loglik: -1.6961e+02 - logprior: 16.4017
Epoch 9/10
10/10 - 0s - loss: 150.1825 - loglik: -1.6964e+02 - logprior: 19.4589
Epoch 10/10
10/10 - 0s - loss: 147.7573 - loglik: -1.6968e+02 - logprior: 21.9199
Fitted a model with MAP estimate = -146.5674
Time for alignment: 24.7264
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 746.3381 - loglik: -2.5756e+02 - logprior: -4.8878e+02
Epoch 2/10
10/10 - 0s - loss: 362.1268 - loglik: -2.2710e+02 - logprior: -1.3503e+02
Epoch 3/10
10/10 - 0s - loss: 269.4843 - loglik: -2.0712e+02 - logprior: -6.2361e+01
Epoch 4/10
10/10 - 0s - loss: 230.5065 - loglik: -1.9588e+02 - logprior: -3.4624e+01
Epoch 5/10
10/10 - 0s - loss: 210.5291 - loglik: -1.9064e+02 - logprior: -1.9891e+01
Epoch 6/10
10/10 - 0s - loss: 199.8646 - loglik: -1.8905e+02 - logprior: -1.0814e+01
Epoch 7/10
10/10 - 0s - loss: 193.6377 - loglik: -1.8887e+02 - logprior: -4.7717e+00
Epoch 8/10
10/10 - 0s - loss: 189.7215 - loglik: -1.8885e+02 - logprior: -8.6674e-01
Epoch 9/10
10/10 - 0s - loss: 187.0889 - loglik: -1.8881e+02 - logprior: 1.7182
Epoch 10/10
10/10 - 0s - loss: 185.2189 - loglik: -1.8876e+02 - logprior: 3.5392
Fitted a model with MAP estimate = -184.3814
expansions: [(0, 3), (7, 1), (15, 1), (17, 2), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 847.0464 - loglik: -1.9458e+02 - logprior: -6.5246e+02
Epoch 2/2
10/10 - 0s - loss: 380.3632 - loglik: -1.7668e+02 - logprior: -2.0368e+02
Fitted a model with MAP estimate = -291.5886
expansions: []
discards: [22]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 634.5132 - loglik: -1.7909e+02 - logprior: -4.5543e+02
Epoch 2/2
10/10 - 0s - loss: 292.2665 - loglik: -1.7091e+02 - logprior: -1.2135e+02
Fitted a model with MAP estimate = -240.6786
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 605.8312 - loglik: -1.7782e+02 - logprior: -4.2801e+02
Epoch 2/10
10/10 - 0s - loss: 284.7143 - loglik: -1.7073e+02 - logprior: -1.1399e+02
Epoch 3/10
10/10 - 0s - loss: 216.9594 - loglik: -1.6958e+02 - logprior: -4.7376e+01
Epoch 4/10
10/10 - 0s - loss: 188.5363 - loglik: -1.6967e+02 - logprior: -1.8864e+01
Epoch 5/10
10/10 - 0s - loss: 172.5944 - loglik: -1.6960e+02 - logprior: -2.9961e+00
Epoch 6/10
10/10 - 0s - loss: 163.2418 - loglik: -1.6966e+02 - logprior: 6.4208
Epoch 7/10
10/10 - 0s - loss: 157.2863 - loglik: -1.6962e+02 - logprior: 12.3348
Epoch 8/10
10/10 - 0s - loss: 153.2050 - loglik: -1.6961e+02 - logprior: 16.4020
Epoch 9/10
10/10 - 0s - loss: 150.1824 - loglik: -1.6964e+02 - logprior: 19.4590
Epoch 10/10
10/10 - 0s - loss: 147.7573 - loglik: -1.6968e+02 - logprior: 21.9197
Fitted a model with MAP estimate = -146.5672
Time for alignment: 23.9024
Warning: You are aligning 93 sequences, although learnMSA is designed for large scale alignments. We recommend to have a sufficiently deep training dataset of at least 100 sequences for accurate results.
Fitting a model of length 34 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 746.3381 - loglik: -2.5756e+02 - logprior: -4.8878e+02
Epoch 2/10
10/10 - 0s - loss: 362.1268 - loglik: -2.2710e+02 - logprior: -1.3503e+02
Epoch 3/10
10/10 - 0s - loss: 269.4843 - loglik: -2.0712e+02 - logprior: -6.2361e+01
Epoch 4/10
10/10 - 0s - loss: 230.5065 - loglik: -1.9588e+02 - logprior: -3.4624e+01
Epoch 5/10
10/10 - 0s - loss: 210.5291 - loglik: -1.9064e+02 - logprior: -1.9891e+01
Epoch 6/10
10/10 - 0s - loss: 199.8646 - loglik: -1.8905e+02 - logprior: -1.0814e+01
Epoch 7/10
10/10 - 0s - loss: 193.6377 - loglik: -1.8887e+02 - logprior: -4.7717e+00
Epoch 8/10
10/10 - 0s - loss: 189.7215 - loglik: -1.8885e+02 - logprior: -8.6675e-01
Epoch 9/10
10/10 - 0s - loss: 187.0888 - loglik: -1.8881e+02 - logprior: 1.7182
Epoch 10/10
10/10 - 0s - loss: 185.2189 - loglik: -1.8876e+02 - logprior: 3.5392
Fitted a model with MAP estimate = -184.3815
expansions: [(0, 3), (7, 1), (15, 1), (17, 2), (26, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 44 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 847.0464 - loglik: -1.9458e+02 - logprior: -6.5246e+02
Epoch 2/2
10/10 - 0s - loss: 380.3632 - loglik: -1.7668e+02 - logprior: -2.0368e+02
Fitted a model with MAP estimate = -291.5886
expansions: []
discards: [22]
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 634.5132 - loglik: -1.7909e+02 - logprior: -4.5543e+02
Epoch 2/2
10/10 - 0s - loss: 292.2665 - loglik: -1.7091e+02 - logprior: -1.2135e+02
Fitted a model with MAP estimate = -240.6786
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 43 on 93 sequences.
Batch size= 93 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 605.8311 - loglik: -1.7782e+02 - logprior: -4.2801e+02
Epoch 2/10
10/10 - 0s - loss: 284.7143 - loglik: -1.7072e+02 - logprior: -1.1399e+02
Epoch 3/10
10/10 - 0s - loss: 216.9594 - loglik: -1.6958e+02 - logprior: -4.7376e+01
Epoch 4/10
10/10 - 0s - loss: 188.5363 - loglik: -1.6967e+02 - logprior: -1.8864e+01
Epoch 5/10
10/10 - 0s - loss: 172.5945 - loglik: -1.6960e+02 - logprior: -2.9960e+00
Epoch 6/10
10/10 - 0s - loss: 163.2420 - loglik: -1.6966e+02 - logprior: 6.4204
Epoch 7/10
10/10 - 0s - loss: 157.2866 - loglik: -1.6962e+02 - logprior: 12.3349
Epoch 8/10
10/10 - 0s - loss: 153.2052 - loglik: -1.6961e+02 - logprior: 16.4017
Epoch 9/10
10/10 - 0s - loss: 150.1826 - loglik: -1.6964e+02 - logprior: 19.4588
Epoch 10/10
10/10 - 0s - loss: 147.7574 - loglik: -1.6968e+02 - logprior: 21.9197
Fitted a model with MAP estimate = -146.5673
Time for alignment: 23.4852
Computed alignments with likelihoods: ['-146.5672', '-146.5667', '-146.5674', '-146.5672', '-146.5673']
Best model has likelihood: -146.5667  (prior= 23.1145 )
time for generating output: 0.0882
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/seatoxin.projection.fasta
SP score = 0.7523148148148148
Training of 5 independent models on file int.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9394cb8e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea118980d0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f841b5e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 955.9545 - loglik: -9.5177e+02 - logprior: -4.1849e+00
Epoch 2/10
16/16 - 4s - loss: 917.8398 - loglik: -9.1683e+02 - logprior: -1.0120e+00
Epoch 3/10
16/16 - 4s - loss: 895.4427 - loglik: -8.9419e+02 - logprior: -1.2535e+00
Epoch 4/10
16/16 - 4s - loss: 885.7322 - loglik: -8.8449e+02 - logprior: -1.2377e+00
Epoch 5/10
16/16 - 4s - loss: 883.4884 - loglik: -8.8234e+02 - logprior: -1.1467e+00
Epoch 6/10
16/16 - 4s - loss: 879.9379 - loglik: -8.7877e+02 - logprior: -1.1638e+00
Epoch 7/10
16/16 - 4s - loss: 877.2332 - loglik: -8.7607e+02 - logprior: -1.1510e+00
Epoch 8/10
16/16 - 4s - loss: 873.5861 - loglik: -8.7241e+02 - logprior: -1.1669e+00
Epoch 9/10
16/16 - 4s - loss: 872.0328 - loglik: -8.7085e+02 - logprior: -1.1651e+00
Epoch 10/10
16/16 - 4s - loss: 866.4772 - loglik: -8.6529e+02 - logprior: -1.1702e+00
Fitted a model with MAP estimate = -863.5723
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 3), (29, 1), (42, 1), (43, 1), (48, 2), (51, 1), (53, 1), (55, 1), (57, 1), (71, 1), (72, 2), (73, 2), (74, 1), (75, 1), (94, 3), (95, 1), (96, 1), (102, 1), (116, 1), (117, 1), (122, 2), (125, 1), (129, 1), (139, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 12s - loss: 893.9379 - loglik: -8.9023e+02 - logprior: -3.7084e+00
Epoch 2/2
33/33 - 6s - loss: 875.8634 - loglik: -8.7486e+02 - logprior: -9.9842e-01
Fitted a model with MAP estimate = -874.0140
expansions: [(117, 1), (175, 2)]
discards: [  0  32  33  57  95 173 174]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 883.9628 - loglik: -8.8040e+02 - logprior: -3.5639e+00
Epoch 2/2
33/33 - 6s - loss: 877.5754 - loglik: -8.7675e+02 - logprior: -8.2451e-01
Fitted a model with MAP estimate = -875.1374
expansions: [(0, 1), (171, 2)]
discards: [115 169 170]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 879.5658 - loglik: -8.7715e+02 - logprior: -2.4117e+00
Epoch 2/10
33/33 - 6s - loss: 874.8430 - loglik: -8.7435e+02 - logprior: -4.9599e-01
Epoch 3/10
33/33 - 6s - loss: 873.4169 - loglik: -8.7309e+02 - logprior: -3.2986e-01
Epoch 4/10
33/33 - 6s - loss: 869.1704 - loglik: -8.6890e+02 - logprior: -2.6685e-01
Epoch 5/10
33/33 - 6s - loss: 866.6406 - loglik: -8.6642e+02 - logprior: -2.1213e-01
Epoch 6/10
33/33 - 6s - loss: 864.3809 - loglik: -8.6422e+02 - logprior: -1.4657e-01
Epoch 7/10
33/33 - 6s - loss: 857.4744 - loglik: -8.5735e+02 - logprior: -1.0725e-01
Epoch 8/10
33/33 - 6s - loss: 844.9037 - loglik: -8.4480e+02 - logprior: -8.6064e-02
Epoch 9/10
33/33 - 6s - loss: 774.8896 - loglik: -7.7431e+02 - logprior: -5.4483e-01
Epoch 10/10
33/33 - 6s - loss: 699.3391 - loglik: -6.9806e+02 - logprior: -1.2321e+00
Fitted a model with MAP estimate = -684.0993
Time for alignment: 171.7897
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 9s - loss: 956.1951 - loglik: -9.5202e+02 - logprior: -4.1742e+00
Epoch 2/10
16/16 - 4s - loss: 918.1293 - loglik: -9.1714e+02 - logprior: -9.8408e-01
Epoch 3/10
16/16 - 4s - loss: 895.4474 - loglik: -8.9427e+02 - logprior: -1.1774e+00
Epoch 4/10
16/16 - 4s - loss: 887.4419 - loglik: -8.8625e+02 - logprior: -1.1930e+00
Epoch 5/10
16/16 - 4s - loss: 881.8784 - loglik: -8.8076e+02 - logprior: -1.1118e+00
Epoch 6/10
16/16 - 4s - loss: 880.0302 - loglik: -8.7891e+02 - logprior: -1.1137e+00
Epoch 7/10
16/16 - 4s - loss: 877.6843 - loglik: -8.7659e+02 - logprior: -1.0908e+00
Epoch 8/10
16/16 - 4s - loss: 872.8483 - loglik: -8.7173e+02 - logprior: -1.1122e+00
Epoch 9/10
16/16 - 4s - loss: 872.6940 - loglik: -8.7158e+02 - logprior: -1.1043e+00
Epoch 10/10
16/16 - 4s - loss: 867.6363 - loglik: -8.6651e+02 - logprior: -1.1064e+00
Fitted a model with MAP estimate = -864.2519
expansions: [(13, 1), (14, 1), (28, 1), (29, 3), (30, 1), (43, 1), (45, 1), (49, 2), (50, 1), (51, 1), (52, 1), (53, 1), (69, 1), (73, 1), (74, 3), (94, 4), (96, 1), (97, 1), (99, 1), (105, 1), (112, 1), (115, 1), (119, 1), (122, 1), (125, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 893.7573 - loglik: -8.9005e+02 - logprior: -3.7092e+00
Epoch 2/2
33/33 - 6s - loss: 877.0422 - loglik: -8.7605e+02 - logprior: -9.8907e-01
Fitted a model with MAP estimate = -873.8415
expansions: [(174, 2)]
discards: [  0  32  33 115 171 172 173]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 883.1969 - loglik: -8.7964e+02 - logprior: -3.5593e+00
Epoch 2/2
33/33 - 6s - loss: 876.7100 - loglik: -8.7593e+02 - logprior: -7.8379e-01
Fitted a model with MAP estimate = -874.9546
expansions: [(0, 1), (169, 3)]
discards: [112 167 168]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 10s - loss: 878.5363 - loglik: -8.7614e+02 - logprior: -2.3991e+00
Epoch 2/10
33/33 - 6s - loss: 874.9184 - loglik: -8.7445e+02 - logprior: -4.6940e-01
Epoch 3/10
33/33 - 6s - loss: 873.2308 - loglik: -8.7296e+02 - logprior: -2.6810e-01
Epoch 4/10
33/33 - 6s - loss: 868.8662 - loglik: -8.6867e+02 - logprior: -1.9559e-01
Epoch 5/10
33/33 - 6s - loss: 866.7455 - loglik: -8.6660e+02 - logprior: -1.3774e-01
Epoch 6/10
33/33 - 6s - loss: 863.1891 - loglik: -8.6310e+02 - logprior: -8.0963e-02
Epoch 7/10
33/33 - 6s - loss: 857.4681 - loglik: -8.5743e+02 - logprior: -2.6558e-02
Epoch 8/10
33/33 - 6s - loss: 844.1240 - loglik: -8.4409e+02 - logprior: -1.0485e-02
Epoch 9/10
33/33 - 6s - loss: 773.8331 - loglik: -7.7321e+02 - logprior: -5.9588e-01
Epoch 10/10
33/33 - 6s - loss: 702.3362 - loglik: -7.0096e+02 - logprior: -1.3326e+00
Fitted a model with MAP estimate = -686.0027
Time for alignment: 170.9804
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 955.5679 - loglik: -9.5140e+02 - logprior: -4.1720e+00
Epoch 2/10
16/16 - 4s - loss: 919.6523 - loglik: -9.1866e+02 - logprior: -9.8735e-01
Epoch 3/10
16/16 - 4s - loss: 895.7482 - loglik: -8.9456e+02 - logprior: -1.1888e+00
Epoch 4/10
16/16 - 4s - loss: 888.2125 - loglik: -8.8703e+02 - logprior: -1.1794e+00
Epoch 5/10
16/16 - 4s - loss: 882.8676 - loglik: -8.8178e+02 - logprior: -1.0870e+00
Epoch 6/10
16/16 - 4s - loss: 878.3301 - loglik: -8.7721e+02 - logprior: -1.1162e+00
Epoch 7/10
16/16 - 4s - loss: 878.9895 - loglik: -8.7790e+02 - logprior: -1.0773e+00
Fitted a model with MAP estimate = -875.5709
expansions: [(13, 1), (14, 1), (23, 3), (28, 3), (29, 1), (42, 1), (44, 1), (48, 2), (51, 1), (54, 1), (56, 2), (57, 1), (73, 2), (74, 3), (94, 5), (98, 1), (112, 1), (115, 1), (119, 1), (122, 1), (125, 1), (127, 1), (128, 1), (129, 1), (139, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 885.8533 - loglik: -8.8213e+02 - logprior: -3.7223e+00
Epoch 2/2
33/33 - 6s - loss: 875.3686 - loglik: -8.7435e+02 - logprior: -1.0226e+00
Fitted a model with MAP estimate = -872.5873
expansions: [(94, 1), (178, 2)]
discards: [  0  25  33  34  71 175 176 177]
Re-initialized the encoder parameters.
Fitting a model of length 173 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 882.5227 - loglik: -8.7895e+02 - logprior: -3.5692e+00
Epoch 2/2
33/33 - 6s - loss: 875.7036 - loglik: -8.7490e+02 - logprior: -8.0452e-01
Fitted a model with MAP estimate = -873.8569
expansions: [(0, 1), (33, 1), (173, 2)]
discards: [152 171 172]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 879.2941 - loglik: -8.7684e+02 - logprior: -2.4495e+00
Epoch 2/10
33/33 - 6s - loss: 872.7964 - loglik: -8.7227e+02 - logprior: -5.2437e-01
Epoch 3/10
33/33 - 6s - loss: 871.2545 - loglik: -8.7092e+02 - logprior: -3.3681e-01
Epoch 4/10
33/33 - 6s - loss: 868.0372 - loglik: -8.6777e+02 - logprior: -2.6335e-01
Epoch 5/10
33/33 - 6s - loss: 864.7584 - loglik: -8.6455e+02 - logprior: -2.0636e-01
Epoch 6/10
33/33 - 6s - loss: 863.8150 - loglik: -8.6365e+02 - logprior: -1.5122e-01
Epoch 7/10
33/33 - 6s - loss: 855.8973 - loglik: -8.5578e+02 - logprior: -1.0226e-01
Epoch 8/10
33/33 - 6s - loss: 840.5330 - loglik: -8.4041e+02 - logprior: -1.0035e-01
Epoch 9/10
33/33 - 6s - loss: 765.8844 - loglik: -7.6520e+02 - logprior: -6.5759e-01
Epoch 10/10
33/33 - 6s - loss: 697.9173 - loglik: -6.9675e+02 - logprior: -1.1199e+00
Fitted a model with MAP estimate = -684.0378
Time for alignment: 160.1868
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 955.2956 - loglik: -9.5111e+02 - logprior: -4.1858e+00
Epoch 2/10
16/16 - 4s - loss: 918.8824 - loglik: -9.1788e+02 - logprior: -1.0051e+00
Epoch 3/10
16/16 - 4s - loss: 899.5081 - loglik: -8.9826e+02 - logprior: -1.2446e+00
Epoch 4/10
16/16 - 4s - loss: 888.1890 - loglik: -8.8692e+02 - logprior: -1.2634e+00
Epoch 5/10
16/16 - 4s - loss: 885.1985 - loglik: -8.8402e+02 - logprior: -1.1706e+00
Epoch 6/10
16/16 - 4s - loss: 878.9730 - loglik: -8.7778e+02 - logprior: -1.1874e+00
Epoch 7/10
16/16 - 4s - loss: 878.4765 - loglik: -8.7731e+02 - logprior: -1.1552e+00
Epoch 8/10
16/16 - 4s - loss: 875.1066 - loglik: -8.7392e+02 - logprior: -1.1764e+00
Epoch 9/10
16/16 - 4s - loss: 872.4469 - loglik: -8.7123e+02 - logprior: -1.1997e+00
Epoch 10/10
16/16 - 4s - loss: 866.7520 - loglik: -8.6553e+02 - logprior: -1.2080e+00
Fitted a model with MAP estimate = -862.0532
expansions: [(13, 1), (14, 1), (22, 1), (23, 1), (28, 3), (29, 1), (42, 1), (44, 1), (49, 1), (51, 1), (53, 1), (57, 2), (58, 2), (69, 1), (73, 2), (74, 3), (90, 1), (94, 3), (95, 1), (99, 1), (113, 1), (116, 1), (120, 1), (123, 1), (126, 1), (128, 1), (129, 1), (139, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 176 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 11s - loss: 894.8693 - loglik: -8.9114e+02 - logprior: -3.7324e+00
Epoch 2/2
33/33 - 6s - loss: 875.8552 - loglik: -8.7486e+02 - logprior: -9.9910e-01
Fitted a model with MAP estimate = -873.3048
expansions: [(176, 2)]
discards: [  0  32  33  70  73 174 175]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 882.3541 - loglik: -8.7879e+02 - logprior: -3.5604e+00
Epoch 2/2
33/33 - 6s - loss: 877.7531 - loglik: -8.7695e+02 - logprior: -8.0235e-01
Fitted a model with MAP estimate = -875.0691
expansions: [(0, 1), (30, 2), (171, 3)]
discards: [112 169 170]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 878.8916 - loglik: -8.7648e+02 - logprior: -2.4104e+00
Epoch 2/10
33/33 - 6s - loss: 873.7776 - loglik: -8.7328e+02 - logprior: -4.9755e-01
Epoch 3/10
33/33 - 6s - loss: 870.7172 - loglik: -8.7040e+02 - logprior: -3.1630e-01
Epoch 4/10
33/33 - 6s - loss: 870.5099 - loglik: -8.7026e+02 - logprior: -2.4621e-01
Epoch 5/10
33/33 - 6s - loss: 865.5055 - loglik: -8.6530e+02 - logprior: -1.9900e-01
Epoch 6/10
33/33 - 6s - loss: 862.9405 - loglik: -8.6281e+02 - logprior: -1.2154e-01
Epoch 7/10
33/33 - 6s - loss: 856.4960 - loglik: -8.5641e+02 - logprior: -7.5055e-02
Epoch 8/10
33/33 - 6s - loss: 842.8301 - loglik: -8.4275e+02 - logprior: -5.7596e-02
Epoch 9/10
33/33 - 6s - loss: 776.6832 - loglik: -7.7619e+02 - logprior: -4.6157e-01
Epoch 10/10
33/33 - 6s - loss: 700.7739 - loglik: -6.9958e+02 - logprior: -1.1513e+00
Fitted a model with MAP estimate = -684.9990
Time for alignment: 171.0176
Fitting a model of length 139 on 7572 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 955.3660 - loglik: -9.5118e+02 - logprior: -4.1879e+00
Epoch 2/10
16/16 - 4s - loss: 917.2049 - loglik: -9.1619e+02 - logprior: -1.0150e+00
Epoch 3/10
16/16 - 4s - loss: 896.9500 - loglik: -8.9568e+02 - logprior: -1.2661e+00
Epoch 4/10
16/16 - 4s - loss: 887.3042 - loglik: -8.8600e+02 - logprior: -1.2986e+00
Epoch 5/10
16/16 - 4s - loss: 883.7593 - loglik: -8.8256e+02 - logprior: -1.2008e+00
Epoch 6/10
16/16 - 4s - loss: 877.3026 - loglik: -8.7609e+02 - logprior: -1.2044e+00
Epoch 7/10
16/16 - 4s - loss: 876.8223 - loglik: -8.7564e+02 - logprior: -1.1713e+00
Epoch 8/10
16/16 - 4s - loss: 874.1652 - loglik: -8.7297e+02 - logprior: -1.1892e+00
Epoch 9/10
16/16 - 4s - loss: 872.1021 - loglik: -8.7090e+02 - logprior: -1.1927e+00
Epoch 10/10
16/16 - 4s - loss: 868.6108 - loglik: -8.6740e+02 - logprior: -1.1920e+00
Fitted a model with MAP estimate = -862.7684
expansions: [(13, 1), (14, 1), (23, 1), (27, 1), (28, 3), (29, 1), (42, 1), (44, 1), (49, 1), (50, 1), (56, 2), (69, 1), (73, 2), (74, 2), (91, 1), (94, 2), (95, 4), (99, 1), (113, 1), (116, 1), (117, 1), (119, 1), (122, 1), (125, 1), (129, 1), (139, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 174 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 894.9022 - loglik: -8.9116e+02 - logprior: -3.7375e+00
Epoch 2/2
33/33 - 6s - loss: 876.4711 - loglik: -8.7546e+02 - logprior: -1.0101e+00
Fitted a model with MAP estimate = -874.7847
expansions: [(57, 2), (174, 2)]
discards: [  0  32  33  68 114 115 116 172 173]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 883.5070 - loglik: -8.7991e+02 - logprior: -3.5924e+00
Epoch 2/2
33/33 - 6s - loss: 879.6211 - loglik: -8.7871e+02 - logprior: -9.1294e-01
Fitted a model with MAP estimate = -876.0716
expansions: [(0, 1), (169, 2)]
discards: [ 88 167 168]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 7572 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 9s - loss: 879.9875 - loglik: -8.7756e+02 - logprior: -2.4258e+00
Epoch 2/10
33/33 - 6s - loss: 876.0750 - loglik: -8.7557e+02 - logprior: -5.0519e-01
Epoch 3/10
33/33 - 6s - loss: 873.5929 - loglik: -8.7327e+02 - logprior: -3.1833e-01
Epoch 4/10
33/33 - 6s - loss: 871.6149 - loglik: -8.7136e+02 - logprior: -2.5431e-01
Epoch 5/10
33/33 - 6s - loss: 866.5977 - loglik: -8.6641e+02 - logprior: -1.8405e-01
Epoch 6/10
33/33 - 6s - loss: 863.9200 - loglik: -8.6378e+02 - logprior: -1.3247e-01
Epoch 7/10
33/33 - 6s - loss: 861.0343 - loglik: -8.6095e+02 - logprior: -6.8233e-02
Epoch 8/10
33/33 - 6s - loss: 841.1066 - loglik: -8.4102e+02 - logprior: -6.7652e-02
Epoch 9/10
33/33 - 6s - loss: 768.8458 - loglik: -7.6816e+02 - logprior: -6.5620e-01
Epoch 10/10
33/33 - 6s - loss: 696.9926 - loglik: -6.9562e+02 - logprior: -1.3307e+00
Fitted a model with MAP estimate = -682.9751
Time for alignment: 168.2703
Computed alignments with likelihoods: ['-684.0993', '-686.0027', '-684.0378', '-684.9990', '-682.9751']
Best model has likelihood: -682.9751  (prior= -1.3990 )
time for generating output: 0.1820
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/int.projection.fasta
SP score = 0.0200445434298441
Training of 5 independent models on file phc.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe938f3cc70>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9d67b7d00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8d243ceb0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 566.6307 - loglik: -5.5170e+02 - logprior: -1.4934e+01
Epoch 2/10
10/10 - 1s - loss: 522.0997 - loglik: -5.1819e+02 - logprior: -3.9088e+00
Epoch 3/10
10/10 - 1s - loss: 499.4388 - loglik: -4.9720e+02 - logprior: -2.2359e+00
Epoch 4/10
10/10 - 1s - loss: 483.7333 - loglik: -4.8191e+02 - logprior: -1.8192e+00
Epoch 5/10
10/10 - 1s - loss: 479.6628 - loglik: -4.7810e+02 - logprior: -1.5647e+00
Epoch 6/10
10/10 - 1s - loss: 476.9593 - loglik: -4.7544e+02 - logprior: -1.5157e+00
Epoch 7/10
10/10 - 1s - loss: 474.1694 - loglik: -4.7269e+02 - logprior: -1.4810e+00
Epoch 8/10
10/10 - 1s - loss: 475.5109 - loglik: -4.7407e+02 - logprior: -1.4410e+00
Fitted a model with MAP estimate = -473.1245
expansions: [(10, 1), (35, 2), (36, 1), (37, 1), (44, 1), (48, 1), (55, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 497.7617 - loglik: -4.8049e+02 - logprior: -1.7272e+01
Epoch 2/2
10/10 - 1s - loss: 476.3038 - loglik: -4.6897e+02 - logprior: -7.3331e+00
Fitted a model with MAP estimate = -474.2516
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 491.5370 - loglik: -4.7458e+02 - logprior: -1.6953e+01
Epoch 2/2
10/10 - 1s - loss: 474.0658 - loglik: -4.6800e+02 - logprior: -6.0698e+00
Fitted a model with MAP estimate = -471.6361
expansions: [(0, 21)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 483.1971 - loglik: -4.6883e+02 - logprior: -1.4363e+01
Epoch 2/10
10/10 - 1s - loss: 465.2693 - loglik: -4.6088e+02 - logprior: -4.3858e+00
Epoch 3/10
10/10 - 1s - loss: 457.6072 - loglik: -4.5512e+02 - logprior: -2.4836e+00
Epoch 4/10
10/10 - 1s - loss: 455.8577 - loglik: -4.5408e+02 - logprior: -1.7819e+00
Epoch 5/10
10/10 - 1s - loss: 452.4155 - loglik: -4.5090e+02 - logprior: -1.5182e+00
Epoch 6/10
10/10 - 1s - loss: 453.1872 - loglik: -4.5180e+02 - logprior: -1.3812e+00
Fitted a model with MAP estimate = -452.8968
Time for alignment: 42.3615
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 563.7865 - loglik: -5.4885e+02 - logprior: -1.4937e+01
Epoch 2/10
10/10 - 1s - loss: 526.8635 - loglik: -5.2294e+02 - logprior: -3.9242e+00
Epoch 3/10
10/10 - 1s - loss: 499.1812 - loglik: -4.9691e+02 - logprior: -2.2696e+00
Epoch 4/10
10/10 - 1s - loss: 484.3354 - loglik: -4.8250e+02 - logprior: -1.8399e+00
Epoch 5/10
10/10 - 1s - loss: 480.0710 - loglik: -4.7843e+02 - logprior: -1.6366e+00
Epoch 6/10
10/10 - 1s - loss: 478.9350 - loglik: -4.7746e+02 - logprior: -1.4712e+00
Epoch 7/10
10/10 - 1s - loss: 476.6828 - loglik: -4.7536e+02 - logprior: -1.3237e+00
Epoch 8/10
10/10 - 1s - loss: 476.5488 - loglik: -4.7527e+02 - logprior: -1.2799e+00
Epoch 9/10
10/10 - 1s - loss: 471.4779 - loglik: -4.7019e+02 - logprior: -1.2876e+00
Epoch 10/10
10/10 - 1s - loss: 473.4792 - loglik: -4.7213e+02 - logprior: -1.3422e+00
Fitted a model with MAP estimate = -470.4610
expansions: [(23, 1), (38, 1), (39, 1), (40, 1), (48, 1), (56, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 500.3361 - loglik: -4.8299e+02 - logprior: -1.7343e+01
Epoch 2/2
10/10 - 1s - loss: 481.3615 - loglik: -4.7390e+02 - logprior: -7.4622e+00
Fitted a model with MAP estimate = -477.9553
expansions: [(35, 1)]
discards: [ 0 40 61]
Re-initialized the encoder parameters.
Fitting a model of length 73 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 497.4554 - loglik: -4.8031e+02 - logprior: -1.7143e+01
Epoch 2/2
10/10 - 1s - loss: 480.3145 - loglik: -4.7361e+02 - logprior: -6.7011e+00
Fitted a model with MAP estimate = -476.4130
expansions: [(0, 21)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 94 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 487.8776 - loglik: -4.7323e+02 - logprior: -1.4645e+01
Epoch 2/10
10/10 - 1s - loss: 468.0586 - loglik: -4.6360e+02 - logprior: -4.4605e+00
Epoch 3/10
10/10 - 1s - loss: 459.9582 - loglik: -4.5729e+02 - logprior: -2.6631e+00
Epoch 4/10
10/10 - 1s - loss: 461.1848 - loglik: -4.5931e+02 - logprior: -1.8743e+00
Fitted a model with MAP estimate = -458.3307
Time for alignment: 39.2043
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 567.6034 - loglik: -5.5267e+02 - logprior: -1.4932e+01
Epoch 2/10
10/10 - 1s - loss: 523.8318 - loglik: -5.1991e+02 - logprior: -3.9214e+00
Epoch 3/10
10/10 - 1s - loss: 495.8455 - loglik: -4.9359e+02 - logprior: -2.2585e+00
Epoch 4/10
10/10 - 1s - loss: 485.3417 - loglik: -4.8351e+02 - logprior: -1.8312e+00
Epoch 5/10
10/10 - 1s - loss: 479.4440 - loglik: -4.7780e+02 - logprior: -1.6390e+00
Epoch 6/10
10/10 - 1s - loss: 476.5265 - loglik: -4.7496e+02 - logprior: -1.5626e+00
Epoch 7/10
10/10 - 1s - loss: 474.5978 - loglik: -4.7313e+02 - logprior: -1.4688e+00
Epoch 8/10
10/10 - 1s - loss: 474.4342 - loglik: -4.7305e+02 - logprior: -1.3829e+00
Epoch 9/10
10/10 - 1s - loss: 473.1507 - loglik: -4.7175e+02 - logprior: -1.3988e+00
Epoch 10/10
10/10 - 1s - loss: 469.1108 - loglik: -4.6769e+02 - logprior: -1.4133e+00
Fitted a model with MAP estimate = -470.0919
expansions: [(11, 1), (35, 2), (38, 1), (40, 1), (41, 1), (47, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 77 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 497.9679 - loglik: -4.8074e+02 - logprior: -1.7230e+01
Epoch 2/2
10/10 - 1s - loss: 477.7289 - loglik: -4.7039e+02 - logprior: -7.3385e+00
Fitted a model with MAP estimate = -474.0503
expansions: []
discards: [ 0 10]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 490.9473 - loglik: -4.7399e+02 - logprior: -1.6957e+01
Epoch 2/2
10/10 - 1s - loss: 474.6837 - loglik: -4.6832e+02 - logprior: -6.3650e+00
Fitted a model with MAP estimate = -472.2807
expansions: [(0, 20)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 95 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 482.6423 - loglik: -4.6816e+02 - logprior: -1.4482e+01
Epoch 2/10
10/10 - 1s - loss: 466.7970 - loglik: -4.6244e+02 - logprior: -4.3556e+00
Epoch 3/10
10/10 - 1s - loss: 458.8152 - loglik: -4.5631e+02 - logprior: -2.5031e+00
Epoch 4/10
10/10 - 1s - loss: 458.1907 - loglik: -4.5637e+02 - logprior: -1.8208e+00
Epoch 5/10
10/10 - 1s - loss: 453.9429 - loglik: -4.5239e+02 - logprior: -1.5503e+00
Epoch 6/10
10/10 - 1s - loss: 453.8748 - loglik: -4.5239e+02 - logprior: -1.4859e+00
Epoch 7/10
10/10 - 1s - loss: 453.9641 - loglik: -4.5272e+02 - logprior: -1.2422e+00
Fitted a model with MAP estimate = -452.8791
Time for alignment: 43.1138
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 565.5095 - loglik: -5.5057e+02 - logprior: -1.4938e+01
Epoch 2/10
10/10 - 1s - loss: 524.6209 - loglik: -5.2070e+02 - logprior: -3.9199e+00
Epoch 3/10
10/10 - 1s - loss: 501.9584 - loglik: -4.9970e+02 - logprior: -2.2543e+00
Epoch 4/10
10/10 - 1s - loss: 485.4097 - loglik: -4.8351e+02 - logprior: -1.8960e+00
Epoch 5/10
10/10 - 1s - loss: 479.3010 - loglik: -4.7757e+02 - logprior: -1.7347e+00
Epoch 6/10
10/10 - 1s - loss: 476.4420 - loglik: -4.7478e+02 - logprior: -1.6647e+00
Epoch 7/10
10/10 - 1s - loss: 475.5991 - loglik: -4.7404e+02 - logprior: -1.5591e+00
Epoch 8/10
10/10 - 1s - loss: 476.3812 - loglik: -4.7487e+02 - logprior: -1.5134e+00
Fitted a model with MAP estimate = -472.9512
expansions: [(27, 1), (36, 1), (37, 1), (38, 1), (40, 2), (41, 3), (44, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 80 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 498.0399 - loglik: -4.8077e+02 - logprior: -1.7266e+01
Epoch 2/2
10/10 - 1s - loss: 478.3036 - loglik: -4.7096e+02 - logprior: -7.3397e+00
Fitted a model with MAP estimate = -472.3104
expansions: []
discards: [ 0 43 44 45]
Re-initialized the encoder parameters.
Fitting a model of length 76 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 491.4391 - loglik: -4.7449e+02 - logprior: -1.6944e+01
Epoch 2/2
10/10 - 1s - loss: 472.4398 - loglik: -4.6618e+02 - logprior: -6.2565e+00
Fitted a model with MAP estimate = -471.9439
expansions: [(0, 21)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 97 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 484.6681 - loglik: -4.7017e+02 - logprior: -1.4500e+01
Epoch 2/10
10/10 - 1s - loss: 464.3110 - loglik: -4.6001e+02 - logprior: -4.2963e+00
Epoch 3/10
10/10 - 1s - loss: 458.1037 - loglik: -4.5558e+02 - logprior: -2.5255e+00
Epoch 4/10
10/10 - 1s - loss: 455.2734 - loglik: -4.5349e+02 - logprior: -1.7797e+00
Epoch 5/10
10/10 - 1s - loss: 454.2556 - loglik: -4.5269e+02 - logprior: -1.5643e+00
Epoch 6/10
10/10 - 1s - loss: 453.6148 - loglik: -4.5219e+02 - logprior: -1.4262e+00
Epoch 7/10
10/10 - 1s - loss: 452.3553 - loglik: -4.5106e+02 - logprior: -1.2912e+00
Epoch 8/10
10/10 - 1s - loss: 452.6560 - loglik: -4.5146e+02 - logprior: -1.1922e+00
Fitted a model with MAP estimate = -451.5414
Time for alignment: 43.7610
Fitting a model of length 68 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 568.5289 - loglik: -5.5359e+02 - logprior: -1.4937e+01
Epoch 2/10
10/10 - 1s - loss: 523.2436 - loglik: -5.1933e+02 - logprior: -3.9154e+00
Epoch 3/10
10/10 - 1s - loss: 497.6131 - loglik: -4.9536e+02 - logprior: -2.2502e+00
Epoch 4/10
10/10 - 1s - loss: 484.5558 - loglik: -4.8266e+02 - logprior: -1.8985e+00
Epoch 5/10
10/10 - 1s - loss: 478.8591 - loglik: -4.7709e+02 - logprior: -1.7691e+00
Epoch 6/10
10/10 - 1s - loss: 480.0294 - loglik: -4.7844e+02 - logprior: -1.5923e+00
Fitted a model with MAP estimate = -475.7806
expansions: [(20, 1), (21, 1), (37, 1), (39, 1), (48, 1), (54, 1), (56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 499.5819 - loglik: -4.8220e+02 - logprior: -1.7380e+01
Epoch 2/2
10/10 - 1s - loss: 483.1651 - loglik: -4.7563e+02 - logprior: -7.5316e+00
Fitted a model with MAP estimate = -478.5940
expansions: [(9, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 496.4743 - loglik: -4.7930e+02 - logprior: -1.7178e+01
Epoch 2/2
10/10 - 1s - loss: 476.5067 - loglik: -4.7013e+02 - logprior: -6.3781e+00
Fitted a model with MAP estimate = -474.9099
expansions: [(0, 21), (35, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 2957 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 486.4250 - loglik: -4.7167e+02 - logprior: -1.4755e+01
Epoch 2/10
10/10 - 1s - loss: 466.7737 - loglik: -4.6230e+02 - logprior: -4.4746e+00
Epoch 3/10
10/10 - 1s - loss: 458.0038 - loglik: -4.5524e+02 - logprior: -2.7649e+00
Epoch 4/10
10/10 - 1s - loss: 459.8430 - loglik: -4.5786e+02 - logprior: -1.9849e+00
Fitted a model with MAP estimate = -456.4268
Time for alignment: 34.3519
Computed alignments with likelihoods: ['-452.8968', '-458.3307', '-452.8791', '-451.5414', '-456.4268']
Best model has likelihood: -451.5414  (prior= -1.1673 )
time for generating output: 0.1674
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/phc.projection.fasta
SP score = 0.3836003770028275
Training of 5 independent models on file ricin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d4921f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea55a3ae50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d6eddf0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1184.0703 - loglik: -1.1312e+03 - logprior: -5.2827e+01
Epoch 2/10
10/10 - 2s - loss: 1085.9700 - loglik: -1.0771e+03 - logprior: -8.8281e+00
Epoch 3/10
10/10 - 2s - loss: 1030.9602 - loglik: -1.0293e+03 - logprior: -1.6866e+00
Epoch 4/10
10/10 - 2s - loss: 999.2057 - loglik: -9.9967e+02 - logprior: 0.4677
Epoch 5/10
10/10 - 2s - loss: 986.7562 - loglik: -9.8861e+02 - logprior: 1.8522
Epoch 6/10
10/10 - 2s - loss: 978.3264 - loglik: -9.8124e+02 - logprior: 2.9161
Epoch 7/10
10/10 - 2s - loss: 972.3566 - loglik: -9.7559e+02 - logprior: 3.2306
Epoch 8/10
10/10 - 2s - loss: 969.5670 - loglik: -9.7301e+02 - logprior: 3.4469
Epoch 9/10
10/10 - 2s - loss: 963.2767 - loglik: -9.6681e+02 - logprior: 3.5338
Epoch 10/10
10/10 - 2s - loss: 961.5999 - loglik: -9.6500e+02 - logprior: 3.4053
Fitted a model with MAP estimate = -962.1082
expansions: [(15, 1), (18, 2), (20, 2), (22, 2), (23, 2), (25, 1), (29, 3), (43, 1), (44, 2), (46, 1), (52, 2), (76, 2), (81, 2), (100, 2), (113, 1), (115, 1), (116, 2), (144, 4), (152, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1034.0029 - loglik: -9.8637e+02 - logprior: -4.7630e+01
Epoch 2/2
10/10 - 2s - loss: 973.4078 - loglik: -9.6530e+02 - logprior: -8.1102e+00
Fitted a model with MAP estimate = -964.2340
expansions: [(0, 16)]
discards: [  0  15  20  23  28  30  39  40  41  69 102 123 143 174]
Re-initialized the encoder parameters.
Fitting a model of length 203 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 1020.4329 - loglik: -9.7330e+02 - logprior: -4.7135e+01
Epoch 2/2
10/10 - 2s - loss: 970.8989 - loglik: -9.6331e+02 - logprior: -7.5925e+00
Fitted a model with MAP estimate = -963.3741
expansions: [(177, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24 101]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1037.4949 - loglik: -9.7767e+02 - logprior: -5.9828e+01
Epoch 2/10
10/10 - 2s - loss: 994.8232 - loglik: -9.7437e+02 - logprior: -2.0449e+01
Epoch 3/10
10/10 - 2s - loss: 983.5709 - loglik: -9.7413e+02 - logprior: -9.4389e+00
Epoch 4/10
10/10 - 2s - loss: 971.4871 - loglik: -9.7259e+02 - logprior: 1.1044
Epoch 5/10
10/10 - 2s - loss: 962.7337 - loglik: -9.6891e+02 - logprior: 6.1802
Epoch 6/10
10/10 - 2s - loss: 962.0013 - loglik: -9.6976e+02 - logprior: 7.7608
Epoch 7/10
10/10 - 2s - loss: 956.1300 - loglik: -9.6475e+02 - logprior: 8.6239
Epoch 8/10
10/10 - 2s - loss: 950.8548 - loglik: -9.6006e+02 - logprior: 9.2072
Epoch 9/10
10/10 - 2s - loss: 947.1851 - loglik: -9.5665e+02 - logprior: 9.4628
Epoch 10/10
10/10 - 2s - loss: 947.0812 - loglik: -9.5669e+02 - logprior: 9.6063
Fitted a model with MAP estimate = -944.6368
Time for alignment: 71.8197
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1183.5457 - loglik: -1.1307e+03 - logprior: -5.2829e+01
Epoch 2/10
10/10 - 2s - loss: 1085.8932 - loglik: -1.0771e+03 - logprior: -8.8197e+00
Epoch 3/10
10/10 - 2s - loss: 1025.5898 - loglik: -1.0241e+03 - logprior: -1.4935e+00
Epoch 4/10
10/10 - 2s - loss: 1002.9921 - loglik: -1.0039e+03 - logprior: 0.8998
Epoch 5/10
10/10 - 2s - loss: 988.6575 - loglik: -9.9098e+02 - logprior: 2.3211
Epoch 6/10
10/10 - 2s - loss: 982.7493 - loglik: -9.8589e+02 - logprior: 3.1452
Epoch 7/10
10/10 - 2s - loss: 975.2456 - loglik: -9.7872e+02 - logprior: 3.4727
Epoch 8/10
10/10 - 2s - loss: 970.2460 - loglik: -9.7393e+02 - logprior: 3.6836
Epoch 9/10
10/10 - 2s - loss: 966.9670 - loglik: -9.7067e+02 - logprior: 3.7060
Epoch 10/10
10/10 - 2s - loss: 965.5850 - loglik: -9.6917e+02 - logprior: 3.5837
Fitted a model with MAP estimate = -964.4320
expansions: [(15, 1), (21, 2), (22, 1), (23, 3), (25, 1), (29, 2), (43, 1), (44, 2), (46, 1), (52, 2), (76, 3), (81, 2), (103, 2), (105, 2), (114, 3), (116, 2), (145, 4), (146, 2), (152, 1), (154, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 205 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 1035.5393 - loglik: -9.8784e+02 - logprior: -4.7702e+01
Epoch 2/2
10/10 - 2s - loss: 972.6716 - loglik: -9.6416e+02 - logprior: -8.5111e+00
Fitted a model with MAP estimate = -962.6055
expansions: []
discards: [  0  15  22  28  66  92  93  94 101 125 128 139 140 176 180 181]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1032.2258 - loglik: -9.7217e+02 - logprior: -6.0054e+01
Epoch 2/2
10/10 - 2s - loss: 988.8380 - loglik: -9.6758e+02 - logprior: -2.1262e+01
Fitted a model with MAP estimate = -981.0590
expansions: [(0, 20), (34, 1), (87, 2), (106, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1014.6906 - loglik: -9.6797e+02 - logprior: -4.6718e+01
Epoch 2/10
10/10 - 3s - loss: 970.2723 - loglik: -9.6403e+02 - logprior: -6.2376e+00
Epoch 3/10
10/10 - 3s - loss: 958.1039 - loglik: -9.6013e+02 - logprior: 2.0305
Epoch 4/10
10/10 - 3s - loss: 955.4196 - loglik: -9.6090e+02 - logprior: 5.4767
Epoch 5/10
10/10 - 3s - loss: 950.8286 - loglik: -9.5862e+02 - logprior: 7.7899
Epoch 6/10
10/10 - 3s - loss: 946.3086 - loglik: -9.5535e+02 - logprior: 9.0393
Epoch 7/10
10/10 - 3s - loss: 945.6877 - loglik: -9.5543e+02 - logprior: 9.7418
Epoch 8/10
10/10 - 3s - loss: 939.4800 - loglik: -9.4986e+02 - logprior: 10.3851
Epoch 9/10
10/10 - 3s - loss: 936.7563 - loglik: -9.4755e+02 - logprior: 10.7953
Epoch 10/10
10/10 - 3s - loss: 934.5394 - loglik: -9.4561e+02 - logprior: 11.0698
Fitted a model with MAP estimate = -932.4945
Time for alignment: 75.7224
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1184.1182 - loglik: -1.1313e+03 - logprior: -5.2824e+01
Epoch 2/10
10/10 - 2s - loss: 1086.9089 - loglik: -1.0781e+03 - logprior: -8.8546e+00
Epoch 3/10
10/10 - 2s - loss: 1027.8542 - loglik: -1.0261e+03 - logprior: -1.7756e+00
Epoch 4/10
10/10 - 2s - loss: 998.4432 - loglik: -9.9887e+02 - logprior: 0.4298
Epoch 5/10
10/10 - 2s - loss: 989.7689 - loglik: -9.9169e+02 - logprior: 1.9170
Epoch 6/10
10/10 - 2s - loss: 981.8026 - loglik: -9.8456e+02 - logprior: 2.7568
Epoch 7/10
10/10 - 2s - loss: 974.3228 - loglik: -9.7739e+02 - logprior: 3.0709
Epoch 8/10
10/10 - 2s - loss: 969.2604 - loglik: -9.7245e+02 - logprior: 3.1943
Epoch 9/10
10/10 - 2s - loss: 965.8910 - loglik: -9.6908e+02 - logprior: 3.1909
Epoch 10/10
10/10 - 2s - loss: 963.1157 - loglik: -9.6631e+02 - logprior: 3.1922
Fitted a model with MAP estimate = -963.0540
expansions: [(9, 2), (15, 1), (22, 1), (23, 3), (30, 2), (31, 1), (44, 1), (45, 2), (46, 1), (52, 2), (76, 2), (81, 1), (82, 1), (107, 3), (114, 1), (117, 2), (119, 1), (145, 3), (146, 1), (151, 2), (152, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1046.6145 - loglik: -9.8600e+02 - logprior: -6.0613e+01
Epoch 2/2
10/10 - 2s - loss: 987.4734 - loglik: -9.6595e+02 - logprior: -2.1527e+01
Fitted a model with MAP estimate = -975.6443
expansions: [(0, 18)]
discards: [  0   8   9  16  27  28  56  65  92 171 172 173 181 184]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1017.9098 - loglik: -9.7076e+02 - logprior: -4.7151e+01
Epoch 2/2
10/10 - 3s - loss: 971.8470 - loglik: -9.6412e+02 - logprior: -7.7303e+00
Fitted a model with MAP estimate = -963.4814
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]
Re-initialized the encoder parameters.
Fitting a model of length 188 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1018.8906 - loglik: -9.6468e+02 - logprior: -5.4207e+01
Epoch 2/10
10/10 - 2s - loss: 971.3379 - loglik: -9.6191e+02 - logprior: -9.4252e+00
Epoch 3/10
10/10 - 2s - loss: 962.2923 - loglik: -9.6308e+02 - logprior: 0.7859
Epoch 4/10
10/10 - 2s - loss: 955.9049 - loglik: -9.6064e+02 - logprior: 4.7402
Epoch 5/10
10/10 - 2s - loss: 951.7933 - loglik: -9.5865e+02 - logprior: 6.8601
Epoch 6/10
10/10 - 2s - loss: 949.0719 - loglik: -9.5719e+02 - logprior: 8.1227
Epoch 7/10
10/10 - 2s - loss: 947.2551 - loglik: -9.5624e+02 - logprior: 8.9897
Epoch 8/10
10/10 - 2s - loss: 941.0374 - loglik: -9.5064e+02 - logprior: 9.6048
Epoch 9/10
10/10 - 2s - loss: 936.9821 - loglik: -9.4684e+02 - logprior: 9.8622
Epoch 10/10
10/10 - 2s - loss: 936.2362 - loglik: -9.4610e+02 - logprior: 9.8671
Fitted a model with MAP estimate = -933.9721
Time for alignment: 72.0190
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1182.7458 - loglik: -1.1299e+03 - logprior: -5.2842e+01
Epoch 2/10
10/10 - 2s - loss: 1087.8623 - loglik: -1.0791e+03 - logprior: -8.8042e+00
Epoch 3/10
10/10 - 2s - loss: 1026.7269 - loglik: -1.0252e+03 - logprior: -1.5675e+00
Epoch 4/10
10/10 - 2s - loss: 1001.0038 - loglik: -1.0019e+03 - logprior: 0.8862
Epoch 5/10
10/10 - 2s - loss: 988.7962 - loglik: -9.9118e+02 - logprior: 2.3839
Epoch 6/10
10/10 - 2s - loss: 983.4901 - loglik: -9.8674e+02 - logprior: 3.2459
Epoch 7/10
10/10 - 2s - loss: 976.0468 - loglik: -9.7954e+02 - logprior: 3.4983
Epoch 8/10
10/10 - 2s - loss: 971.7642 - loglik: -9.7545e+02 - logprior: 3.6878
Epoch 9/10
10/10 - 2s - loss: 966.6146 - loglik: -9.7037e+02 - logprior: 3.7573
Epoch 10/10
10/10 - 2s - loss: 964.8924 - loglik: -9.6860e+02 - logprior: 3.7060
Fitted a model with MAP estimate = -964.5409
expansions: [(0, 3), (12, 1), (20, 1), (22, 2), (23, 1), (30, 3), (44, 1), (45, 2), (46, 1), (52, 2), (76, 3), (81, 1), (82, 1), (104, 2), (116, 4), (145, 4), (146, 2), (152, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 202 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 1058.7898 - loglik: -9.9040e+02 - logprior: -6.8386e+01
Epoch 2/2
10/10 - 2s - loss: 986.0189 - loglik: -9.7014e+02 - logprior: -1.5879e+01
Fitted a model with MAP estimate = -971.6955
expansions: []
discards: [  0   1   2   3  15  38  39  40  67  94 126 142 174 178 179 186]
Re-initialized the encoder parameters.
Fitting a model of length 186 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1037.2366 - loglik: -9.7752e+02 - logprior: -5.9716e+01
Epoch 2/2
10/10 - 2s - loss: 990.4828 - loglik: -9.6966e+02 - logprior: -2.0824e+01
Fitted a model with MAP estimate = -984.0301
expansions: [(0, 16)]
discards: [ 0 49]
Re-initialized the encoder parameters.
Fitting a model of length 200 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 1019.6942 - loglik: -9.7211e+02 - logprior: -4.7586e+01
Epoch 2/10
10/10 - 2s - loss: 975.9976 - loglik: -9.6851e+02 - logprior: -7.4866e+00
Epoch 3/10
10/10 - 2s - loss: 962.2826 - loglik: -9.6332e+02 - logprior: 1.0409
Epoch 4/10
10/10 - 2s - loss: 955.9287 - loglik: -9.6061e+02 - logprior: 4.6849
Epoch 5/10
10/10 - 2s - loss: 953.3640 - loglik: -9.6011e+02 - logprior: 6.7449
Epoch 6/10
10/10 - 2s - loss: 950.4678 - loglik: -9.5839e+02 - logprior: 7.9225
Epoch 7/10
10/10 - 2s - loss: 946.9263 - loglik: -9.5570e+02 - logprior: 8.7772
Epoch 8/10
10/10 - 2s - loss: 941.5875 - loglik: -9.5103e+02 - logprior: 9.4443
Epoch 9/10
10/10 - 2s - loss: 939.3060 - loglik: -9.4913e+02 - logprior: 9.8298
Epoch 10/10
10/10 - 2s - loss: 933.3873 - loglik: -9.4339e+02 - logprior: 10.0038
Fitted a model with MAP estimate = -934.8154
Time for alignment: 74.3387
Fitting a model of length 167 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 1183.6760 - loglik: -1.1308e+03 - logprior: -5.2828e+01
Epoch 2/10
10/10 - 2s - loss: 1086.5256 - loglik: -1.0777e+03 - logprior: -8.8338e+00
Epoch 3/10
10/10 - 2s - loss: 1028.4564 - loglik: -1.0266e+03 - logprior: -1.8389e+00
Epoch 4/10
10/10 - 2s - loss: 998.9474 - loglik: -9.9928e+02 - logprior: 0.3360
Epoch 5/10
10/10 - 2s - loss: 987.4544 - loglik: -9.8920e+02 - logprior: 1.7412
Epoch 6/10
10/10 - 2s - loss: 980.5500 - loglik: -9.8308e+02 - logprior: 2.5268
Epoch 7/10
10/10 - 2s - loss: 972.8289 - loglik: -9.7558e+02 - logprior: 2.7484
Epoch 8/10
10/10 - 2s - loss: 968.6033 - loglik: -9.7152e+02 - logprior: 2.9224
Epoch 9/10
10/10 - 2s - loss: 962.5696 - loglik: -9.6547e+02 - logprior: 2.9035
Epoch 10/10
10/10 - 2s - loss: 962.6337 - loglik: -9.6546e+02 - logprior: 2.8283
Fitted a model with MAP estimate = -961.6815
expansions: [(0, 2), (18, 1), (20, 2), (22, 2), (23, 1), (24, 1), (33, 1), (44, 1), (45, 2), (46, 1), (52, 2), (76, 1), (80, 1), (81, 2), (82, 1), (103, 1), (104, 1), (114, 1), (116, 3), (145, 4), (146, 1), (150, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 202 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1053.0061 - loglik: -9.8500e+02 - logprior: -6.8004e+01
Epoch 2/2
10/10 - 2s - loss: 975.8846 - loglik: -9.6049e+02 - logprior: -1.5399e+01
Fitted a model with MAP estimate = -964.4844
expansions: []
discards: [  0   1   2  23  66  99 172 183 184]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 1027.4229 - loglik: -9.6814e+02 - logprior: -5.9287e+01
Epoch 2/2
10/10 - 2s - loss: 980.4487 - loglik: -9.6023e+02 - logprior: -2.0219e+01
Fitted a model with MAP estimate = -974.1180
expansions: [(0, 18), (123, 1)]
discards: [  0  24 120]
Re-initialized the encoder parameters.
Fitting a model of length 209 on 747 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 1010.6071 - loglik: -9.6358e+02 - logprior: -4.7026e+01
Epoch 2/10
10/10 - 3s - loss: 966.6495 - loglik: -9.5995e+02 - logprior: -6.7038e+00
Epoch 3/10
10/10 - 3s - loss: 952.9165 - loglik: -9.5455e+02 - logprior: 1.6370
Epoch 4/10
10/10 - 3s - loss: 948.2716 - loglik: -9.5338e+02 - logprior: 5.1086
Epoch 5/10
10/10 - 3s - loss: 948.6625 - loglik: -9.5594e+02 - logprior: 7.2829
Fitted a model with MAP estimate = -944.3982
Time for alignment: 61.4253
Computed alignments with likelihoods: ['-944.6368', '-932.4945', '-933.9721', '-934.8154', '-944.3982']
Best model has likelihood: -932.4945  (prior= 11.2161 )
time for generating output: 0.2942
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ricin.projection.fasta
SP score = 0.7558639562157936
Training of 5 independent models on file PDZ.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9055b49a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea779a22b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea6f08a0d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 476.7986 - loglik: -4.7369e+02 - logprior: -3.1054e+00
Epoch 2/10
19/19 - 2s - loss: 446.0841 - loglik: -4.4495e+02 - logprior: -1.1377e+00
Epoch 3/10
19/19 - 2s - loss: 435.3153 - loglik: -4.3410e+02 - logprior: -1.2160e+00
Epoch 4/10
19/19 - 2s - loss: 432.0423 - loglik: -4.3090e+02 - logprior: -1.1432e+00
Epoch 5/10
19/19 - 2s - loss: 430.3737 - loglik: -4.2925e+02 - logprior: -1.1148e+00
Epoch 6/10
19/19 - 2s - loss: 428.5711 - loglik: -4.2747e+02 - logprior: -1.0921e+00
Epoch 7/10
19/19 - 2s - loss: 426.5918 - loglik: -4.2549e+02 - logprior: -1.0876e+00
Epoch 8/10
19/19 - 2s - loss: 425.9158 - loglik: -4.2481e+02 - logprior: -1.0900e+00
Epoch 9/10
19/19 - 2s - loss: 424.1079 - loglik: -4.2301e+02 - logprior: -1.0837e+00
Epoch 10/10
19/19 - 2s - loss: 424.0598 - loglik: -4.2294e+02 - logprior: -1.0978e+00
Fitted a model with MAP estimate = -408.1058
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (15, 1), (16, 4), (18, 3), (23, 1), (42, 1), (46, 2), (47, 1), (49, 2), (52, 2), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 440.6197 - loglik: -4.3647e+02 - logprior: -4.1540e+00
Epoch 2/2
19/19 - 2s - loss: 427.2305 - loglik: -4.2588e+02 - logprior: -1.3549e+00
Fitted a model with MAP estimate = -405.6146
expansions: [(67, 1)]
discards: [ 0  1 24 25 63 74]
Re-initialized the encoder parameters.
Fitting a model of length 85 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 429.2816 - loglik: -4.2642e+02 - logprior: -2.8573e+00
Epoch 2/2
19/19 - 2s - loss: 425.0547 - loglik: -4.2411e+02 - logprior: -9.4915e-01
Fitted a model with MAP estimate = -404.8470
expansions: [(0, 2), (74, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 87 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 8s - loss: 407.0291 - loglik: -4.0522e+02 - logprior: -1.8090e+00
Epoch 2/10
23/23 - 2s - loss: 403.4826 - loglik: -4.0255e+02 - logprior: -9.2980e-01
Epoch 3/10
23/23 - 2s - loss: 403.1882 - loglik: -4.0230e+02 - logprior: -8.8216e-01
Epoch 4/10
23/23 - 2s - loss: 401.5287 - loglik: -4.0067e+02 - logprior: -8.5122e-01
Epoch 5/10
23/23 - 2s - loss: 400.9941 - loglik: -4.0015e+02 - logprior: -8.3960e-01
Epoch 6/10
23/23 - 2s - loss: 399.2215 - loglik: -3.9839e+02 - logprior: -8.2746e-01
Epoch 7/10
23/23 - 2s - loss: 397.9533 - loglik: -3.9713e+02 - logprior: -8.0947e-01
Epoch 8/10
23/23 - 2s - loss: 395.7383 - loglik: -3.9491e+02 - logprior: -8.0940e-01
Epoch 9/10
23/23 - 2s - loss: 395.6322 - loglik: -3.9482e+02 - logprior: -7.8970e-01
Epoch 10/10
23/23 - 2s - loss: 394.0844 - loglik: -3.9327e+02 - logprior: -7.8780e-01
Fitted a model with MAP estimate = -393.4454
Time for alignment: 69.8585
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 476.9305 - loglik: -4.7383e+02 - logprior: -3.1032e+00
Epoch 2/10
19/19 - 2s - loss: 445.0383 - loglik: -4.4391e+02 - logprior: -1.1306e+00
Epoch 3/10
19/19 - 2s - loss: 434.5801 - loglik: -4.3337e+02 - logprior: -1.2058e+00
Epoch 4/10
19/19 - 1s - loss: 431.7734 - loglik: -4.3061e+02 - logprior: -1.1592e+00
Epoch 5/10
19/19 - 2s - loss: 430.0995 - loglik: -4.2897e+02 - logprior: -1.1214e+00
Epoch 6/10
19/19 - 2s - loss: 428.3816 - loglik: -4.2727e+02 - logprior: -1.1088e+00
Epoch 7/10
19/19 - 2s - loss: 427.5334 - loglik: -4.2644e+02 - logprior: -1.0786e+00
Epoch 8/10
19/19 - 2s - loss: 425.6568 - loglik: -4.2455e+02 - logprior: -1.0883e+00
Epoch 9/10
19/19 - 2s - loss: 424.7477 - loglik: -4.2363e+02 - logprior: -1.1007e+00
Epoch 10/10
19/19 - 2s - loss: 424.6605 - loglik: -4.2356e+02 - logprior: -1.0848e+00
Fitted a model with MAP estimate = -407.8445
expansions: [(0, 2), (3, 1), (5, 2), (7, 1), (15, 2), (16, 4), (18, 2), (23, 1), (47, 2), (48, 3), (49, 1), (52, 1), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 438.1930 - loglik: -4.3398e+02 - logprior: -4.2141e+00
Epoch 2/2
19/19 - 2s - loss: 425.6259 - loglik: -4.2421e+02 - logprior: -1.4109e+00
Fitted a model with MAP estimate = -404.3832
expansions: [(7, 1), (30, 1)]
discards: [ 0  1 12 24 25 26 27 63]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 429.0821 - loglik: -4.2620e+02 - logprior: -2.8819e+00
Epoch 2/2
19/19 - 2s - loss: 424.7049 - loglik: -4.2373e+02 - logprior: -9.6969e-01
Fitted a model with MAP estimate = -404.6147
expansions: [(0, 2), (22, 4), (23, 1)]
discards: [9]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 406.9789 - loglik: -4.0466e+02 - logprior: -2.3157e+00
Epoch 2/10
23/23 - 2s - loss: 403.1656 - loglik: -4.0211e+02 - logprior: -1.0541e+00
Epoch 3/10
23/23 - 2s - loss: 402.3421 - loglik: -4.0137e+02 - logprior: -9.7152e-01
Epoch 4/10
23/23 - 2s - loss: 400.7956 - loglik: -3.9987e+02 - logprior: -9.1898e-01
Epoch 5/10
23/23 - 2s - loss: 399.8393 - loglik: -3.9893e+02 - logprior: -9.0848e-01
Epoch 6/10
23/23 - 2s - loss: 398.1879 - loglik: -3.9728e+02 - logprior: -8.9620e-01
Epoch 7/10
23/23 - 2s - loss: 396.8747 - loglik: -3.9598e+02 - logprior: -8.8374e-01
Epoch 8/10
23/23 - 2s - loss: 395.1241 - loglik: -3.9424e+02 - logprior: -8.6467e-01
Epoch 9/10
23/23 - 2s - loss: 394.2385 - loglik: -3.9337e+02 - logprior: -8.4840e-01
Epoch 10/10
23/23 - 2s - loss: 393.6870 - loglik: -3.9281e+02 - logprior: -8.5083e-01
Fitted a model with MAP estimate = -392.9944
Time for alignment: 69.1809
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 476.4793 - loglik: -4.7337e+02 - logprior: -3.1090e+00
Epoch 2/10
19/19 - 2s - loss: 444.5523 - loglik: -4.4341e+02 - logprior: -1.1379e+00
Epoch 3/10
19/19 - 2s - loss: 433.7390 - loglik: -4.3252e+02 - logprior: -1.2222e+00
Epoch 4/10
19/19 - 2s - loss: 431.1377 - loglik: -4.2999e+02 - logprior: -1.1454e+00
Epoch 5/10
19/19 - 2s - loss: 429.7260 - loglik: -4.2861e+02 - logprior: -1.1084e+00
Epoch 6/10
19/19 - 2s - loss: 428.0105 - loglik: -4.2692e+02 - logprior: -1.0866e+00
Epoch 7/10
19/19 - 2s - loss: 426.8299 - loglik: -4.2574e+02 - logprior: -1.0833e+00
Epoch 8/10
19/19 - 2s - loss: 425.4240 - loglik: -4.2433e+02 - logprior: -1.0822e+00
Epoch 9/10
19/19 - 2s - loss: 424.4122 - loglik: -4.2331e+02 - logprior: -1.0816e+00
Epoch 10/10
19/19 - 2s - loss: 423.5155 - loglik: -4.2241e+02 - logprior: -1.0868e+00
Fitted a model with MAP estimate = -407.8812
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (15, 2), (16, 4), (18, 3), (22, 1), (24, 1), (46, 2), (47, 1), (49, 2), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 90 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 439.4470 - loglik: -4.3530e+02 - logprior: -4.1473e+00
Epoch 2/2
19/19 - 2s - loss: 426.9290 - loglik: -4.2555e+02 - logprior: -1.3743e+00
Fitted a model with MAP estimate = -405.1385
expansions: [(22, 1), (68, 1)]
discards: [ 0  1 24 25 26 27 28 64]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 429.6199 - loglik: -4.2676e+02 - logprior: -2.8567e+00
Epoch 2/2
19/19 - 2s - loss: 424.9608 - loglik: -4.2402e+02 - logprior: -9.4218e-01
Fitted a model with MAP estimate = -404.9806
expansions: [(0, 2), (23, 4), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 90 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 406.7990 - loglik: -4.0498e+02 - logprior: -1.8184e+00
Epoch 2/10
23/23 - 2s - loss: 403.3300 - loglik: -4.0236e+02 - logprior: -9.6581e-01
Epoch 3/10
23/23 - 2s - loss: 402.1237 - loglik: -4.0118e+02 - logprior: -9.4514e-01
Epoch 4/10
23/23 - 2s - loss: 400.8249 - loglik: -3.9992e+02 - logprior: -9.0284e-01
Epoch 5/10
23/23 - 2s - loss: 400.6223 - loglik: -3.9973e+02 - logprior: -8.8820e-01
Epoch 6/10
23/23 - 2s - loss: 398.2213 - loglik: -3.9734e+02 - logprior: -8.7058e-01
Epoch 7/10
23/23 - 2s - loss: 396.5343 - loglik: -3.9565e+02 - logprior: -8.6805e-01
Epoch 8/10
23/23 - 2s - loss: 395.6845 - loglik: -3.9482e+02 - logprior: -8.4780e-01
Epoch 9/10
23/23 - 2s - loss: 394.0694 - loglik: -3.9320e+02 - logprior: -8.4196e-01
Epoch 10/10
23/23 - 2s - loss: 393.2368 - loglik: -3.9238e+02 - logprior: -8.3345e-01
Fitted a model with MAP estimate = -392.5846
Time for alignment: 68.7163
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 476.5502 - loglik: -4.7344e+02 - logprior: -3.1070e+00
Epoch 2/10
19/19 - 2s - loss: 444.9926 - loglik: -4.4385e+02 - logprior: -1.1415e+00
Epoch 3/10
19/19 - 2s - loss: 434.3515 - loglik: -4.3314e+02 - logprior: -1.2147e+00
Epoch 4/10
19/19 - 2s - loss: 431.0828 - loglik: -4.2992e+02 - logprior: -1.1625e+00
Epoch 5/10
19/19 - 2s - loss: 429.0372 - loglik: -4.2790e+02 - logprior: -1.1333e+00
Epoch 6/10
19/19 - 2s - loss: 427.6503 - loglik: -4.2654e+02 - logprior: -1.1072e+00
Epoch 7/10
19/19 - 2s - loss: 426.0087 - loglik: -4.2491e+02 - logprior: -1.0920e+00
Epoch 8/10
19/19 - 2s - loss: 425.3231 - loglik: -4.2422e+02 - logprior: -1.0856e+00
Epoch 9/10
19/19 - 2s - loss: 424.1379 - loglik: -4.2304e+02 - logprior: -1.0846e+00
Epoch 10/10
19/19 - 2s - loss: 423.0952 - loglik: -4.2199e+02 - logprior: -1.0901e+00
Fitted a model with MAP estimate = -407.1469
expansions: [(0, 2), (3, 1), (6, 1), (16, 2), (17, 4), (18, 3), (22, 1), (24, 1), (46, 2), (47, 1), (49, 3), (52, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 89 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 438.6904 - loglik: -4.3451e+02 - logprior: -4.1831e+00
Epoch 2/2
19/19 - 2s - loss: 425.8825 - loglik: -4.2455e+02 - logprior: -1.3288e+00
Fitted a model with MAP estimate = -404.4842
expansions: []
discards: [ 0  1 24 25 26 62]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 428.6540 - loglik: -4.2576e+02 - logprior: -2.8974e+00
Epoch 2/2
19/19 - 2s - loss: 424.9464 - loglik: -4.2397e+02 - logprior: -9.7520e-01
Fitted a model with MAP estimate = -404.5205
expansions: [(0, 2), (5, 1), (22, 3), (26, 1)]
discards: [18 24]
Re-initialized the encoder parameters.
Fitting a model of length 88 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 406.7368 - loglik: -4.0441e+02 - logprior: -2.3285e+00
Epoch 2/10
23/23 - 2s - loss: 403.1622 - loglik: -4.0210e+02 - logprior: -1.0620e+00
Epoch 3/10
23/23 - 2s - loss: 401.5184 - loglik: -4.0053e+02 - logprior: -9.8679e-01
Epoch 4/10
23/23 - 2s - loss: 401.4372 - loglik: -4.0051e+02 - logprior: -9.2780e-01
Epoch 5/10
23/23 - 2s - loss: 400.2864 - loglik: -3.9937e+02 - logprior: -9.1129e-01
Epoch 6/10
23/23 - 2s - loss: 397.6129 - loglik: -3.9671e+02 - logprior: -8.9194e-01
Epoch 7/10
23/23 - 2s - loss: 397.2574 - loglik: -3.9636e+02 - logprior: -8.8373e-01
Epoch 8/10
23/23 - 2s - loss: 395.9317 - loglik: -3.9504e+02 - logprior: -8.6955e-01
Epoch 9/10
23/23 - 2s - loss: 394.5017 - loglik: -3.9362e+02 - logprior: -8.5674e-01
Epoch 10/10
23/23 - 2s - loss: 393.4682 - loglik: -3.9259e+02 - logprior: -8.4855e-01
Fitted a model with MAP estimate = -393.1363
Time for alignment: 70.3328
Fitting a model of length 64 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 476.2735 - loglik: -4.7316e+02 - logprior: -3.1092e+00
Epoch 2/10
19/19 - 1s - loss: 445.4875 - loglik: -4.4436e+02 - logprior: -1.1294e+00
Epoch 3/10
19/19 - 1s - loss: 434.5128 - loglik: -4.3334e+02 - logprior: -1.1750e+00
Epoch 4/10
19/19 - 2s - loss: 431.2640 - loglik: -4.3014e+02 - logprior: -1.1212e+00
Epoch 5/10
19/19 - 1s - loss: 429.7261 - loglik: -4.2864e+02 - logprior: -1.0839e+00
Epoch 6/10
19/19 - 2s - loss: 427.9676 - loglik: -4.2690e+02 - logprior: -1.0631e+00
Epoch 7/10
19/19 - 2s - loss: 426.5303 - loglik: -4.2546e+02 - logprior: -1.0650e+00
Epoch 8/10
19/19 - 1s - loss: 425.3425 - loglik: -4.2428e+02 - logprior: -1.0537e+00
Epoch 9/10
19/19 - 1s - loss: 423.9566 - loglik: -4.2288e+02 - logprior: -1.0619e+00
Epoch 10/10
19/19 - 2s - loss: 423.8119 - loglik: -4.2274e+02 - logprior: -1.0575e+00
Fitted a model with MAP estimate = -407.8342
expansions: [(0, 2), (3, 1), (4, 1), (5, 1), (6, 1), (15, 2), (16, 3), (17, 2), (24, 1), (46, 2), (47, 1), (49, 2), (53, 1), (55, 2), (58, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 439.5471 - loglik: -4.3536e+02 - logprior: -4.1828e+00
Epoch 2/2
19/19 - 2s - loss: 427.5412 - loglik: -4.2618e+02 - logprior: -1.3610e+00
Fitted a model with MAP estimate = -405.6990
expansions: [(0, 2), (22, 1), (29, 1), (65, 1)]
discards: [ 0  1 24 25 26 61]
Re-initialized the encoder parameters.
Fitting a model of length 86 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 429.8294 - loglik: -4.2671e+02 - logprior: -3.1221e+00
Epoch 2/2
19/19 - 2s - loss: 424.9300 - loglik: -4.2375e+02 - logprior: -1.1822e+00
Fitted a model with MAP estimate = -404.8431
expansions: [(25, 4), (75, 1)]
discards: [1 2]
Re-initialized the encoder parameters.
Fitting a model of length 89 on 14950 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 406.4771 - loglik: -4.0471e+02 - logprior: -1.7667e+00
Epoch 2/10
23/23 - 2s - loss: 403.7974 - loglik: -4.0297e+02 - logprior: -8.2574e-01
Epoch 3/10
23/23 - 2s - loss: 402.0385 - loglik: -4.0121e+02 - logprior: -8.2570e-01
Epoch 4/10
23/23 - 2s - loss: 400.7592 - loglik: -3.9996e+02 - logprior: -7.9311e-01
Epoch 5/10
23/23 - 2s - loss: 400.4232 - loglik: -3.9964e+02 - logprior: -7.7832e-01
Epoch 6/10
23/23 - 2s - loss: 398.3753 - loglik: -3.9760e+02 - logprior: -7.6935e-01
Epoch 7/10
23/23 - 2s - loss: 396.4918 - loglik: -3.9571e+02 - logprior: -7.6519e-01
Epoch 8/10
23/23 - 2s - loss: 395.3506 - loglik: -3.9458e+02 - logprior: -7.4836e-01
Epoch 9/10
23/23 - 2s - loss: 394.8557 - loglik: -3.9409e+02 - logprior: -7.3986e-01
Epoch 10/10
23/23 - 2s - loss: 393.2339 - loglik: -3.9247e+02 - logprior: -7.3346e-01
Fitted a model with MAP estimate = -392.6868
Time for alignment: 67.5387
Computed alignments with likelihoods: ['-393.4454', '-392.9944', '-392.5846', '-393.1363', '-392.6868']
Best model has likelihood: -392.5846  (prior= -0.7812 )
time for generating output: 0.1305
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/PDZ.projection.fasta
SP score = 0.868663594470046
Training of 5 independent models on file asp.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9bc4d17c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3c2647f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cdb47cd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 1611.2948 - loglik: -1.6056e+03 - logprior: -5.7426e+00
Epoch 2/10
22/22 - 12s - loss: 1504.5640 - loglik: -1.5040e+03 - logprior: -5.8210e-01
Epoch 3/10
22/22 - 12s - loss: 1459.8917 - loglik: -1.4584e+03 - logprior: -1.4800e+00
Epoch 4/10
22/22 - 12s - loss: 1451.8524 - loglik: -1.4506e+03 - logprior: -1.2669e+00
Epoch 5/10
22/22 - 12s - loss: 1449.5303 - loglik: -1.4483e+03 - logprior: -1.2113e+00
Epoch 6/10
22/22 - 12s - loss: 1435.5406 - loglik: -1.4343e+03 - logprior: -1.2199e+00
Epoch 7/10
22/22 - 12s - loss: 1421.8533 - loglik: -1.4205e+03 - logprior: -1.3261e+00
Epoch 8/10
22/22 - 12s - loss: 1389.7781 - loglik: -1.3880e+03 - logprior: -1.7519e+00
Epoch 9/10
22/22 - 12s - loss: 1300.0376 - loglik: -1.2945e+03 - logprior: -5.4819e+00
Epoch 10/10
22/22 - 12s - loss: 1192.4484 - loglik: -1.1814e+03 - logprior: -1.1006e+01
Fitted a model with MAP estimate = -1166.3189
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  35  48  49  50  63  64  65  66  72
  73  74  86  87  88  89  90  97  98  99 100 101 104 106 107 108 109 110
 111 112 113 114 115 119 120 121 122 123 124 125 126 146 149 150 151 178
 195 196 197 198 199 221 228 229 230 231 232 233 234 235 236 237 238 239
 246 247 248]
Re-initialized the encoder parameters.
Fitting a model of length 159 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 10s - loss: 1685.4750 - loglik: -1.6776e+03 - logprior: -7.8551e+00
Epoch 2/2
22/22 - 7s - loss: 1649.8153 - loglik: -1.6504e+03 - logprior: 0.6039
Fitted a model with MAP estimate = -1648.3788
expansions: [(0, 111), (150, 30), (156, 79)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 14s - loss: 1601.6129 - loglik: -1.5950e+03 - logprior: -6.6100e+00
Epoch 2/2
22/22 - 11s - loss: 1507.3210 - loglik: -1.5057e+03 - logprior: -1.5969e+00
Fitted a model with MAP estimate = -1488.7383
expansions: [(166, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  97  98  99 100 101 102 103 104 105 106 107 108 109 110 116
 117 118 119 120 121 122 123 175 176 177 178 179 180 181 182 183 184 185
 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203
 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221
 222 223 224 225 226 227 228 229 230]
Re-initialized the encoder parameters.
Fitting a model of length 61 on 3262 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 8s - loss: 1634.0341 - loglik: -1.6190e+03 - logprior: -1.5054e+01
Epoch 2/10
11/11 - 3s - loss: 1619.4421 - loglik: -1.6130e+03 - logprior: -6.4842e+00
Epoch 3/10
11/11 - 3s - loss: 1607.2450 - loglik: -1.6030e+03 - logprior: -4.2927e+00
Epoch 4/10
11/11 - 3s - loss: 1597.2468 - loglik: -1.5949e+03 - logprior: -2.3276e+00
Epoch 5/10
11/11 - 3s - loss: 1599.3667 - loglik: -1.5981e+03 - logprior: -1.2610e+00
Fitted a model with MAP estimate = -1597.5705
Time for alignment: 215.0112
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 1618.2753 - loglik: -1.6125e+03 - logprior: -5.7509e+00
Epoch 2/10
22/22 - 12s - loss: 1500.5886 - loglik: -1.5000e+03 - logprior: -5.9616e-01
Epoch 3/10
22/22 - 12s - loss: 1461.6593 - loglik: -1.4602e+03 - logprior: -1.4983e+00
Epoch 4/10
22/22 - 12s - loss: 1451.1465 - loglik: -1.4498e+03 - logprior: -1.3067e+00
Epoch 5/10
22/22 - 12s - loss: 1446.0907 - loglik: -1.4447e+03 - logprior: -1.3445e+00
Epoch 6/10
22/22 - 12s - loss: 1439.8175 - loglik: -1.4384e+03 - logprior: -1.3651e+00
Epoch 7/10
22/22 - 12s - loss: 1426.1619 - loglik: -1.4247e+03 - logprior: -1.4540e+00
Epoch 8/10
22/22 - 12s - loss: 1391.6422 - loglik: -1.3897e+03 - logprior: -1.8946e+00
Epoch 9/10
22/22 - 12s - loss: 1303.0298 - loglik: -1.2976e+03 - logprior: -5.4516e+00
Epoch 10/10
22/22 - 12s - loss: 1199.7429 - loglik: -1.1881e+03 - logprior: -1.1586e+01
Fitted a model with MAP estimate = -1175.2964
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  37  49  50  51  52  53  54  55  56  57
  58  59  60  61  62  63  64  65  66  72  73  74  86  87  88  89  90  97
  98  99 100 101 104 106 107 108 109 110 111 112 113 114 115 119 120 121
 122 123 124 125 126 146 147 148 149 150 151 179 180 195 196 197 198 199
 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217
 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235
 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 9s - loss: 1691.0157 - loglik: -1.6831e+03 - logprior: -7.9174e+00
Epoch 2/2
22/22 - 6s - loss: 1652.0282 - loglik: -1.6517e+03 - logprior: -3.2484e-01
Fitted a model with MAP estimate = -1649.4587
expansions: [(0, 122), (110, 144)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 18s - loss: 1611.4591 - loglik: -1.6044e+03 - logprior: -7.0297e+00
Epoch 2/2
22/22 - 14s - loss: 1518.8817 - loglik: -1.5170e+03 - logprior: -1.8698e+00
Fitted a model with MAP estimate = -1502.5761
expansions: [(198, 2), (220, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 206 207
 208 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253
 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 8s - loss: 1553.3438 - loglik: -1.5452e+03 - logprior: -8.1344e+00
Epoch 2/10
22/22 - 6s - loss: 1541.0355 - loglik: -1.5394e+03 - logprior: -1.6243e+00
Epoch 3/10
22/22 - 6s - loss: 1537.9391 - loglik: -1.5369e+03 - logprior: -1.0881e+00
Epoch 4/10
22/22 - 6s - loss: 1534.3048 - loglik: -1.5331e+03 - logprior: -1.2012e+00
Epoch 5/10
22/22 - 6s - loss: 1526.1992 - loglik: -1.5249e+03 - logprior: -1.2582e+00
Epoch 6/10
22/22 - 5s - loss: 1522.8152 - loglik: -1.5215e+03 - logprior: -1.3020e+00
Epoch 7/10
22/22 - 6s - loss: 1471.0740 - loglik: -1.4691e+03 - logprior: -1.9669e+00
Epoch 8/10
22/22 - 6s - loss: 1310.2642 - loglik: -1.3054e+03 - logprior: -4.8541e+00
Epoch 9/10
22/22 - 6s - loss: 1190.8583 - loglik: -1.1852e+03 - logprior: -5.6444e+00
Epoch 10/10
22/22 - 6s - loss: 1171.8611 - loglik: -1.1665e+03 - logprior: -5.3505e+00
Fitted a model with MAP estimate = -1164.1006
Time for alignment: 262.4767
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 1614.0043 - loglik: -1.6083e+03 - logprior: -5.7490e+00
Epoch 2/10
22/22 - 12s - loss: 1496.3071 - loglik: -1.4957e+03 - logprior: -5.9941e-01
Epoch 3/10
22/22 - 12s - loss: 1465.3038 - loglik: -1.4637e+03 - logprior: -1.5565e+00
Epoch 4/10
22/22 - 12s - loss: 1448.4426 - loglik: -1.4471e+03 - logprior: -1.3625e+00
Epoch 5/10
22/22 - 12s - loss: 1450.7959 - loglik: -1.4494e+03 - logprior: -1.3477e+00
Fitted a model with MAP estimate = -1445.2032
expansions: [(12, 1), (13, 1), (14, 1), (31, 2), (32, 1), (34, 3), (35, 2), (45, 2), (48, 1), (49, 1), (50, 1), (70, 1), (71, 1), (76, 1), (77, 2), (81, 1), (82, 1), (83, 1), (99, 1), (104, 1), (105, 2), (107, 1), (109, 1), (120, 2), (143, 2), (144, 1), (149, 1), (150, 1), (152, 1), (156, 1), (157, 1), (176, 1), (179, 3), (180, 1), (181, 1), (184, 3), (185, 2), (194, 3), (206, 1), (207, 1), (208, 1), (209, 1), (213, 2), (224, 1), (225, 1), (236, 1), (237, 1), (238, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 316 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 22s - loss: 1459.3590 - loglik: -1.4512e+03 - logprior: -8.1581e+00
Epoch 2/2
22/22 - 17s - loss: 1437.2422 - loglik: -1.4351e+03 - logprior: -2.1538e+00
Fitted a model with MAP estimate = -1433.7966
expansions: [(0, 3), (121, 1)]
discards: [  0  34  40  56 175 195 220 231]
Re-initialized the encoder parameters.
Fitting a model of length 312 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 20s - loss: 1441.7104 - loglik: -1.4369e+03 - logprior: -4.8242e+00
Epoch 2/2
22/22 - 17s - loss: 1429.8574 - loglik: -1.4309e+03 - logprior: 1.0860
Fitted a model with MAP estimate = -1430.8097
expansions: []
discards: [0 1 2]
Re-initialized the encoder parameters.
Fitting a model of length 309 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 20s - loss: 1450.9501 - loglik: -1.4435e+03 - logprior: -7.4453e+00
Epoch 2/10
22/22 - 16s - loss: 1434.1447 - loglik: -1.4325e+03 - logprior: -1.5952e+00
Epoch 3/10
22/22 - 16s - loss: 1433.8719 - loglik: -1.4340e+03 - logprior: 0.1301
Epoch 4/10
22/22 - 16s - loss: 1427.9543 - loglik: -1.4304e+03 - logprior: 2.4343
Epoch 5/10
22/22 - 16s - loss: 1418.8684 - loglik: -1.4215e+03 - logprior: 2.6627
Epoch 6/10
22/22 - 16s - loss: 1414.9884 - loglik: -1.4177e+03 - logprior: 2.7514
Epoch 7/10
22/22 - 16s - loss: 1394.0359 - loglik: -1.3968e+03 - logprior: 2.7895
Epoch 8/10
22/22 - 16s - loss: 1341.1274 - loglik: -1.3433e+03 - logprior: 2.1443
Epoch 9/10
22/22 - 16s - loss: 1252.8120 - loglik: -1.2517e+03 - logprior: -1.0948e+00
Epoch 10/10
22/22 - 16s - loss: 1174.2069 - loglik: -1.1706e+03 - logprior: -3.6262e+00
Fitted a model with MAP estimate = -1161.3883
Time for alignment: 353.1532
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 17s - loss: 1610.9219 - loglik: -1.6052e+03 - logprior: -5.7502e+00
Epoch 2/10
22/22 - 12s - loss: 1506.5739 - loglik: -1.5060e+03 - logprior: -6.0826e-01
Epoch 3/10
22/22 - 12s - loss: 1464.1278 - loglik: -1.4627e+03 - logprior: -1.4268e+00
Epoch 4/10
22/22 - 12s - loss: 1453.4963 - loglik: -1.4521e+03 - logprior: -1.3510e+00
Epoch 5/10
22/22 - 12s - loss: 1452.6188 - loglik: -1.4513e+03 - logprior: -1.3461e+00
Epoch 6/10
22/22 - 12s - loss: 1440.4601 - loglik: -1.4391e+03 - logprior: -1.3686e+00
Epoch 7/10
22/22 - 12s - loss: 1420.3546 - loglik: -1.4189e+03 - logprior: -1.4434e+00
Epoch 8/10
22/22 - 12s - loss: 1391.5077 - loglik: -1.3896e+03 - logprior: -1.8795e+00
Epoch 9/10
22/22 - 12s - loss: 1298.2572 - loglik: -1.2921e+03 - logprior: -6.1773e+00
Epoch 10/10
22/22 - 12s - loss: 1192.9165 - loglik: -1.1816e+03 - logprior: -1.1332e+01
Fitted a model with MAP estimate = -1170.3437
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  37  49  50  51  52  64  65  66  67  68
  69  70  71  72  73  86  87  88  89  90  97  98  99 100 101 102 105 108
 109 110 111 112 113 114 115 120 121 122 123 124 125 126 127 146 149 164
 165 180 196 197 198 199 200 206 221 222 223 224 225 226 227 228 229 230
 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248
 249 250 251]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 9s - loss: 1688.2395 - loglik: -1.6807e+03 - logprior: -7.5192e+00
Epoch 2/2
22/22 - 6s - loss: 1651.8457 - loglik: -1.6521e+03 - logprior: 0.2326
Fitted a model with MAP estimate = -1653.4742
expansions: [(0, 123), (141, 157)]
discards: [  2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37
  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55
  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73
  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91
  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140]
Re-initialized the encoder parameters.
Fitting a model of length 282 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 17s - loss: 1602.4159 - loglik: -1.5938e+03 - logprior: -8.6637e+00
Epoch 2/2
22/22 - 14s - loss: 1472.8998 - loglik: -1.4721e+03 - logprior: -8.3646e-01
Fitted a model with MAP estimate = -1451.6656
expansions: [(14, 1), (15, 1), (32, 2), (33, 1), (36, 1), (38, 1), (48, 1), (53, 1), (75, 1), (76, 2), (83, 1), (89, 1), (106, 1), (150, 1), (158, 1), (163, 1), (167, 1), (170, 2), (204, 3), (205, 1), (206, 1), (254, 1), (266, 1), (268, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 310 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 21s - loss: 1457.5981 - loglik: -1.4494e+03 - logprior: -8.2010e+00
Epoch 2/10
22/22 - 16s - loss: 1429.9120 - loglik: -1.4292e+03 - logprior: -7.3684e-01
Epoch 3/10
22/22 - 16s - loss: 1435.0958 - loglik: -1.4364e+03 - logprior: 1.2962
Fitted a model with MAP estimate = -1428.7108
Time for alignment: 261.8754
Fitting a model of length 252 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 15s - loss: 1610.8448 - loglik: -1.6051e+03 - logprior: -5.7459e+00
Epoch 2/10
22/22 - 12s - loss: 1502.5707 - loglik: -1.5020e+03 - logprior: -5.8603e-01
Epoch 3/10
22/22 - 12s - loss: 1459.7814 - loglik: -1.4583e+03 - logprior: -1.5276e+00
Epoch 4/10
22/22 - 12s - loss: 1456.1244 - loglik: -1.4547e+03 - logprior: -1.3944e+00
Epoch 5/10
22/22 - 12s - loss: 1444.0984 - loglik: -1.4427e+03 - logprior: -1.3558e+00
Epoch 6/10
22/22 - 12s - loss: 1442.1844 - loglik: -1.4408e+03 - logprior: -1.3479e+00
Epoch 7/10
22/22 - 12s - loss: 1421.5206 - loglik: -1.4201e+03 - logprior: -1.4402e+00
Epoch 8/10
22/22 - 12s - loss: 1393.9438 - loglik: -1.3921e+03 - logprior: -1.8192e+00
Epoch 9/10
22/22 - 12s - loss: 1315.3186 - loglik: -1.3109e+03 - logprior: -4.3786e+00
Epoch 10/10
22/22 - 12s - loss: 1199.7573 - loglik: -1.1891e+03 - logprior: -1.0684e+01
Fitted a model with MAP estimate = -1167.2071
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  37  49  53  65  66  67  68  69  70
  71  72  73  74  86  87  88  89  90 105 106 107 108 109 110 111 112 113
 114 115 116 120 121 122 123 124 125 126 127 146 179 195 196 197 198 199
 200 201 202 203 204 205 206 228 229 230 231 232 233 234 235 236 237 238
 239 246 247 248]
Re-initialized the encoder parameters.
Fitting a model of length 158 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 9s - loss: 1688.0219 - loglik: -1.6804e+03 - logprior: -7.6130e+00
Epoch 2/2
22/22 - 7s - loss: 1652.2234 - loglik: -1.6528e+03 - logprior: 0.5414
Fitted a model with MAP estimate = -1649.5980
expansions: [(0, 92), (155, 70), (158, 75)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147]
Re-initialized the encoder parameters.
Fitting a model of length 247 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 14s - loss: 1611.3325 - loglik: -1.6048e+03 - logprior: -6.5428e+00
Epoch 2/2
22/22 - 12s - loss: 1520.6559 - loglik: -1.5195e+03 - logprior: -1.1254e+00
Fitted a model with MAP estimate = -1508.6357
expansions: [(39, 1), (56, 1), (57, 1), (78, 2), (79, 2), (85, 3), (86, 4), (93, 78), (129, 1), (153, 2), (154, 2)]
discards: [  0 168 169 170 171 172 202 203 204 205 206 207 208 209 210 211 212 213
 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246]
Re-initialized the encoder parameters.
Fitting a model of length 293 on 3262 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 19s - loss: 1495.1437 - loglik: -1.4866e+03 - logprior: -8.5312e+00
Epoch 2/10
22/22 - 15s - loss: 1455.2433 - loglik: -1.4531e+03 - logprior: -2.1118e+00
Epoch 3/10
22/22 - 15s - loss: 1434.0122 - loglik: -1.4346e+03 - logprior: 0.5878
Epoch 4/10
22/22 - 15s - loss: 1435.6311 - loglik: -1.4366e+03 - logprior: 0.9641
Fitted a model with MAP estimate = -1430.2124
Time for alignment: 264.2852
Computed alignments with likelihoods: ['-1166.3189', '-1164.1006', '-1161.3883', '-1170.3437', '-1167.2071']
Best model has likelihood: -1161.3883  (prior= -3.6264 )
time for generating output: 0.3125
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/asp.projection.fasta
SP score = 0.07011753340846885
Training of 5 independent models on file bowman.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9d614fbb0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9d67bee80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea22ae60a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 252.4607 - loglik: -1.6007e+02 - logprior: -9.2395e+01
Epoch 2/10
10/10 - 0s - loss: 167.8701 - loglik: -1.4127e+02 - logprior: -2.6599e+01
Epoch 3/10
10/10 - 1s - loss: 144.8348 - loglik: -1.3152e+02 - logprior: -1.3319e+01
Epoch 4/10
10/10 - 1s - loss: 134.8612 - loglik: -1.2665e+02 - logprior: -8.2124e+00
Epoch 5/10
10/10 - 0s - loss: 129.7188 - loglik: -1.2415e+02 - logprior: -5.5726e+00
Epoch 6/10
10/10 - 0s - loss: 127.2022 - loglik: -1.2307e+02 - logprior: -4.1320e+00
Epoch 7/10
10/10 - 1s - loss: 125.7395 - loglik: -1.2241e+02 - logprior: -3.3311e+00
Epoch 8/10
10/10 - 1s - loss: 124.7548 - loglik: -1.2186e+02 - logprior: -2.8987e+00
Epoch 9/10
10/10 - 1s - loss: 124.2823 - loglik: -1.2169e+02 - logprior: -2.5900e+00
Epoch 10/10
10/10 - 0s - loss: 123.9845 - loglik: -1.2170e+02 - logprior: -2.2815e+00
Fitted a model with MAP estimate = -123.8531
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 253.8745 - loglik: -1.3019e+02 - logprior: -1.2368e+02
Epoch 2/2
10/10 - 0s - loss: 159.6338 - loglik: -1.1948e+02 - logprior: -4.0158e+01
Fitted a model with MAP estimate = -140.9387
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 210.3733 - loglik: -1.2343e+02 - logprior: -8.6946e+01
Epoch 2/10
10/10 - 1s - loss: 141.7709 - loglik: -1.1673e+02 - logprior: -2.5043e+01
Epoch 3/10
10/10 - 0s - loss: 128.1349 - loglik: -1.1582e+02 - logprior: -1.2311e+01
Epoch 4/10
10/10 - 0s - loss: 122.8691 - loglik: -1.1568e+02 - logprior: -7.1842e+00
Epoch 5/10
10/10 - 1s - loss: 119.4364 - loglik: -1.1490e+02 - logprior: -4.5368e+00
Epoch 6/10
10/10 - 1s - loss: 117.4083 - loglik: -1.1426e+02 - logprior: -3.1481e+00
Epoch 7/10
10/10 - 0s - loss: 116.2682 - loglik: -1.1395e+02 - logprior: -2.3184e+00
Epoch 8/10
10/10 - 1s - loss: 115.5287 - loglik: -1.1382e+02 - logprior: -1.7076e+00
Epoch 9/10
10/10 - 0s - loss: 115.0554 - loglik: -1.1381e+02 - logprior: -1.2446e+00
Epoch 10/10
10/10 - 0s - loss: 114.6740 - loglik: -1.1378e+02 - logprior: -8.9762e-01
Fitted a model with MAP estimate = -114.4969
Time for alignment: 20.4922
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 252.4607 - loglik: -1.6007e+02 - logprior: -9.2395e+01
Epoch 2/10
10/10 - 1s - loss: 167.8701 - loglik: -1.4127e+02 - logprior: -2.6599e+01
Epoch 3/10
10/10 - 1s - loss: 144.8348 - loglik: -1.3152e+02 - logprior: -1.3319e+01
Epoch 4/10
10/10 - 1s - loss: 134.8612 - loglik: -1.2665e+02 - logprior: -8.2124e+00
Epoch 5/10
10/10 - 1s - loss: 129.7188 - loglik: -1.2415e+02 - logprior: -5.5726e+00
Epoch 6/10
10/10 - 1s - loss: 127.2022 - loglik: -1.2307e+02 - logprior: -4.1320e+00
Epoch 7/10
10/10 - 1s - loss: 125.7395 - loglik: -1.2241e+02 - logprior: -3.3311e+00
Epoch 8/10
10/10 - 1s - loss: 124.7548 - loglik: -1.2186e+02 - logprior: -2.8987e+00
Epoch 9/10
10/10 - 1s - loss: 124.2823 - loglik: -1.2169e+02 - logprior: -2.5900e+00
Epoch 10/10
10/10 - 1s - loss: 123.9845 - loglik: -1.2170e+02 - logprior: -2.2815e+00
Fitted a model with MAP estimate = -123.8533
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 253.8745 - loglik: -1.3019e+02 - logprior: -1.2368e+02
Epoch 2/2
10/10 - 0s - loss: 159.6338 - loglik: -1.1948e+02 - logprior: -4.0158e+01
Fitted a model with MAP estimate = -140.9387
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 210.3733 - loglik: -1.2343e+02 - logprior: -8.6946e+01
Epoch 2/10
10/10 - 1s - loss: 141.7709 - loglik: -1.1673e+02 - logprior: -2.5043e+01
Epoch 3/10
10/10 - 1s - loss: 128.1349 - loglik: -1.1582e+02 - logprior: -1.2311e+01
Epoch 4/10
10/10 - 1s - loss: 122.8692 - loglik: -1.1568e+02 - logprior: -7.1842e+00
Epoch 5/10
10/10 - 1s - loss: 119.4365 - loglik: -1.1490e+02 - logprior: -4.5368e+00
Epoch 6/10
10/10 - 1s - loss: 117.4083 - loglik: -1.1426e+02 - logprior: -3.1481e+00
Epoch 7/10
10/10 - 1s - loss: 116.2686 - loglik: -1.1395e+02 - logprior: -2.3184e+00
Epoch 8/10
10/10 - 1s - loss: 115.5292 - loglik: -1.1382e+02 - logprior: -1.7077e+00
Epoch 9/10
10/10 - 1s - loss: 115.0556 - loglik: -1.1381e+02 - logprior: -1.2447e+00
Epoch 10/10
10/10 - 1s - loss: 114.6741 - loglik: -1.1378e+02 - logprior: -8.9763e-01
Fitted a model with MAP estimate = -114.4968
Time for alignment: 24.6648
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 252.4607 - loglik: -1.6007e+02 - logprior: -9.2395e+01
Epoch 2/10
10/10 - 1s - loss: 167.8701 - loglik: -1.4127e+02 - logprior: -2.6599e+01
Epoch 3/10
10/10 - 1s - loss: 144.8348 - loglik: -1.3152e+02 - logprior: -1.3319e+01
Epoch 4/10
10/10 - 1s - loss: 134.8612 - loglik: -1.2665e+02 - logprior: -8.2124e+00
Epoch 5/10
10/10 - 0s - loss: 129.7188 - loglik: -1.2415e+02 - logprior: -5.5726e+00
Epoch 6/10
10/10 - 1s - loss: 127.2022 - loglik: -1.2307e+02 - logprior: -4.1320e+00
Epoch 7/10
10/10 - 0s - loss: 125.7395 - loglik: -1.2241e+02 - logprior: -3.3311e+00
Epoch 8/10
10/10 - 1s - loss: 124.7548 - loglik: -1.2186e+02 - logprior: -2.8987e+00
Epoch 9/10
10/10 - 1s - loss: 124.2823 - loglik: -1.2169e+02 - logprior: -2.5900e+00
Epoch 10/10
10/10 - 1s - loss: 123.9845 - loglik: -1.2170e+02 - logprior: -2.2815e+00
Fitted a model with MAP estimate = -123.8531
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 253.8745 - loglik: -1.3019e+02 - logprior: -1.2368e+02
Epoch 2/2
10/10 - 1s - loss: 159.6338 - loglik: -1.1948e+02 - logprior: -4.0158e+01
Fitted a model with MAP estimate = -140.9387
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 210.3733 - loglik: -1.2343e+02 - logprior: -8.6946e+01
Epoch 2/10
10/10 - 1s - loss: 141.7709 - loglik: -1.1673e+02 - logprior: -2.5043e+01
Epoch 3/10
10/10 - 1s - loss: 128.1349 - loglik: -1.1582e+02 - logprior: -1.2311e+01
Epoch 4/10
10/10 - 1s - loss: 122.8691 - loglik: -1.1568e+02 - logprior: -7.1842e+00
Epoch 5/10
10/10 - 1s - loss: 119.4365 - loglik: -1.1490e+02 - logprior: -4.5368e+00
Epoch 6/10
10/10 - 1s - loss: 117.4083 - loglik: -1.1426e+02 - logprior: -3.1481e+00
Epoch 7/10
10/10 - 1s - loss: 116.2685 - loglik: -1.1395e+02 - logprior: -2.3184e+00
Epoch 8/10
10/10 - 1s - loss: 115.5290 - loglik: -1.1382e+02 - logprior: -1.7076e+00
Epoch 9/10
10/10 - 1s - loss: 115.0555 - loglik: -1.1381e+02 - logprior: -1.2446e+00
Epoch 10/10
10/10 - 1s - loss: 114.6740 - loglik: -1.1378e+02 - logprior: -8.9763e-01
Fitted a model with MAP estimate = -114.4969
Time for alignment: 23.5446
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 252.4607 - loglik: -1.6007e+02 - logprior: -9.2395e+01
Epoch 2/10
10/10 - 1s - loss: 167.8701 - loglik: -1.4127e+02 - logprior: -2.6599e+01
Epoch 3/10
10/10 - 0s - loss: 144.8348 - loglik: -1.3152e+02 - logprior: -1.3319e+01
Epoch 4/10
10/10 - 1s - loss: 134.8612 - loglik: -1.2665e+02 - logprior: -8.2124e+00
Epoch 5/10
10/10 - 0s - loss: 129.7188 - loglik: -1.2415e+02 - logprior: -5.5726e+00
Epoch 6/10
10/10 - 1s - loss: 127.2022 - loglik: -1.2307e+02 - logprior: -4.1320e+00
Epoch 7/10
10/10 - 1s - loss: 125.7395 - loglik: -1.2241e+02 - logprior: -3.3311e+00
Epoch 8/10
10/10 - 1s - loss: 124.7549 - loglik: -1.2186e+02 - logprior: -2.8987e+00
Epoch 9/10
10/10 - 1s - loss: 124.2823 - loglik: -1.2169e+02 - logprior: -2.5900e+00
Epoch 10/10
10/10 - 1s - loss: 123.9845 - loglik: -1.2170e+02 - logprior: -2.2815e+00
Fitted a model with MAP estimate = -123.8533
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 253.8745 - loglik: -1.3019e+02 - logprior: -1.2368e+02
Epoch 2/2
10/10 - 1s - loss: 159.6338 - loglik: -1.1948e+02 - logprior: -4.0158e+01
Fitted a model with MAP estimate = -140.9387
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 210.3733 - loglik: -1.2343e+02 - logprior: -8.6946e+01
Epoch 2/10
10/10 - 1s - loss: 141.7710 - loglik: -1.1673e+02 - logprior: -2.5043e+01
Epoch 3/10
10/10 - 1s - loss: 128.1349 - loglik: -1.1582e+02 - logprior: -1.2311e+01
Epoch 4/10
10/10 - 0s - loss: 122.8691 - loglik: -1.1568e+02 - logprior: -7.1842e+00
Epoch 5/10
10/10 - 1s - loss: 119.4364 - loglik: -1.1490e+02 - logprior: -4.5368e+00
Epoch 6/10
10/10 - 0s - loss: 117.4083 - loglik: -1.1426e+02 - logprior: -3.1481e+00
Epoch 7/10
10/10 - 0s - loss: 116.2684 - loglik: -1.1395e+02 - logprior: -2.3184e+00
Epoch 8/10
10/10 - 0s - loss: 115.5289 - loglik: -1.1382e+02 - logprior: -1.7076e+00
Epoch 9/10
10/10 - 0s - loss: 115.0555 - loglik: -1.1381e+02 - logprior: -1.2446e+00
Epoch 10/10
10/10 - 1s - loss: 114.6741 - loglik: -1.1378e+02 - logprior: -8.9762e-01
Fitted a model with MAP estimate = -114.4967
Time for alignment: 23.4237
Fitting a model of length 20 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 252.4607 - loglik: -1.6007e+02 - logprior: -9.2395e+01
Epoch 2/10
10/10 - 0s - loss: 167.8701 - loglik: -1.4127e+02 - logprior: -2.6599e+01
Epoch 3/10
10/10 - 0s - loss: 144.8348 - loglik: -1.3152e+02 - logprior: -1.3319e+01
Epoch 4/10
10/10 - 1s - loss: 134.8612 - loglik: -1.2665e+02 - logprior: -8.2124e+00
Epoch 5/10
10/10 - 0s - loss: 129.7188 - loglik: -1.2415e+02 - logprior: -5.5726e+00
Epoch 6/10
10/10 - 0s - loss: 127.2022 - loglik: -1.2307e+02 - logprior: -4.1320e+00
Epoch 7/10
10/10 - 0s - loss: 125.7395 - loglik: -1.2241e+02 - logprior: -3.3311e+00
Epoch 8/10
10/10 - 1s - loss: 124.7548 - loglik: -1.2186e+02 - logprior: -2.8987e+00
Epoch 9/10
10/10 - 0s - loss: 124.2823 - loglik: -1.2169e+02 - logprior: -2.5900e+00
Epoch 10/10
10/10 - 0s - loss: 123.9845 - loglik: -1.2170e+02 - logprior: -2.2815e+00
Fitted a model with MAP estimate = -123.8534
expansions: [(0, 4), (13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 253.8745 - loglik: -1.3019e+02 - logprior: -1.2368e+02
Epoch 2/2
10/10 - 1s - loss: 159.6338 - loglik: -1.1948e+02 - logprior: -4.0158e+01
Fitted a model with MAP estimate = -140.9387
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 499 sequences.
Batch size= 499 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 210.3733 - loglik: -1.2343e+02 - logprior: -8.6946e+01
Epoch 2/10
10/10 - 0s - loss: 141.7709 - loglik: -1.1673e+02 - logprior: -2.5043e+01
Epoch 3/10
10/10 - 1s - loss: 128.1349 - loglik: -1.1582e+02 - logprior: -1.2311e+01
Epoch 4/10
10/10 - 1s - loss: 122.8691 - loglik: -1.1568e+02 - logprior: -7.1842e+00
Epoch 5/10
10/10 - 0s - loss: 119.4365 - loglik: -1.1490e+02 - logprior: -4.5368e+00
Epoch 6/10
10/10 - 0s - loss: 117.4083 - loglik: -1.1426e+02 - logprior: -3.1481e+00
Epoch 7/10
10/10 - 0s - loss: 116.2684 - loglik: -1.1395e+02 - logprior: -2.3184e+00
Epoch 8/10
10/10 - 0s - loss: 115.5290 - loglik: -1.1382e+02 - logprior: -1.7076e+00
Epoch 9/10
10/10 - 1s - loss: 115.0554 - loglik: -1.1381e+02 - logprior: -1.2446e+00
Epoch 10/10
10/10 - 0s - loss: 114.6740 - loglik: -1.1378e+02 - logprior: -8.9763e-01
Fitted a model with MAP estimate = -114.4968
Time for alignment: 22.9010
Computed alignments with likelihoods: ['-114.4969', '-114.4968', '-114.4969', '-114.4967', '-114.4968']
Best model has likelihood: -114.4967  (prior= -0.7428 )
time for generating output: 0.1087
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/bowman.projection.fasta
SP score = 0.9418604651162791
Training of 5 independent models on file oxidored_q6.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea6f529a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea009d7910>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea44ec4a90>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 718.7888 - loglik: -7.0648e+02 - logprior: -1.2305e+01
Epoch 2/10
11/11 - 2s - loss: 661.6512 - loglik: -6.5870e+02 - logprior: -2.9472e+00
Epoch 3/10
11/11 - 2s - loss: 621.0413 - loglik: -6.1916e+02 - logprior: -1.8814e+00
Epoch 4/10
11/11 - 2s - loss: 606.5255 - loglik: -6.0469e+02 - logprior: -1.8395e+00
Epoch 5/10
11/11 - 2s - loss: 599.3043 - loglik: -5.9743e+02 - logprior: -1.8770e+00
Epoch 6/10
11/11 - 2s - loss: 596.7687 - loglik: -5.9482e+02 - logprior: -1.9495e+00
Epoch 7/10
11/11 - 2s - loss: 593.0089 - loglik: -5.9115e+02 - logprior: -1.8579e+00
Epoch 8/10
11/11 - 2s - loss: 593.6293 - loglik: -5.9179e+02 - logprior: -1.8315e+00
Fitted a model with MAP estimate = -591.6550
expansions: [(8, 2), (9, 3), (10, 1), (12, 2), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 2), (62, 2), (63, 2), (65, 1), (66, 1), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 609.8337 - loglik: -5.9572e+02 - logprior: -1.4113e+01
Epoch 2/2
11/11 - 2s - loss: 580.5414 - loglik: -5.7465e+02 - logprior: -5.8893e+00
Fitted a model with MAP estimate = -578.5675
expansions: [(0, 23)]
discards: [  0   7  73  77  79  87 110]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 9s - loss: 581.7119 - loglik: -5.7427e+02 - logprior: -7.4403e+00
Epoch 2/2
22/22 - 3s - loss: 565.8310 - loglik: -5.6415e+02 - logprior: -1.6855e+00
Fitted a model with MAP estimate = -565.4694
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 589.9921 - loglik: -5.7597e+02 - logprior: -1.4021e+01
Epoch 2/10
11/11 - 2s - loss: 577.6210 - loglik: -5.7258e+02 - logprior: -5.0439e+00
Epoch 3/10
11/11 - 2s - loss: 574.2661 - loglik: -5.7216e+02 - logprior: -2.1065e+00
Epoch 4/10
11/11 - 2s - loss: 570.6039 - loglik: -5.6996e+02 - logprior: -6.3987e-01
Epoch 5/10
11/11 - 2s - loss: 571.3043 - loglik: -5.7104e+02 - logprior: -2.6732e-01
Fitted a model with MAP estimate = -570.0968
Time for alignment: 61.0792
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 719.1464 - loglik: -7.0684e+02 - logprior: -1.2309e+01
Epoch 2/10
11/11 - 2s - loss: 661.5615 - loglik: -6.5861e+02 - logprior: -2.9481e+00
Epoch 3/10
11/11 - 2s - loss: 622.4029 - loglik: -6.2047e+02 - logprior: -1.9283e+00
Epoch 4/10
11/11 - 2s - loss: 603.1324 - loglik: -6.0118e+02 - logprior: -1.9546e+00
Epoch 5/10
11/11 - 2s - loss: 597.6921 - loglik: -5.9569e+02 - logprior: -2.0030e+00
Epoch 6/10
11/11 - 2s - loss: 596.5932 - loglik: -5.9454e+02 - logprior: -2.0517e+00
Epoch 7/10
11/11 - 2s - loss: 593.4454 - loglik: -5.9148e+02 - logprior: -1.9651e+00
Epoch 8/10
11/11 - 2s - loss: 591.1400 - loglik: -5.8924e+02 - logprior: -1.9016e+00
Epoch 9/10
11/11 - 2s - loss: 591.5735 - loglik: -5.8965e+02 - logprior: -1.9242e+00
Fitted a model with MAP estimate = -589.8826
expansions: [(8, 2), (9, 3), (10, 2), (11, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 2), (62, 2), (63, 2), (64, 2), (65, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 609.3225 - loglik: -5.9518e+02 - logprior: -1.4139e+01
Epoch 2/2
11/11 - 2s - loss: 581.3144 - loglik: -5.7544e+02 - logprior: -5.8715e+00
Fitted a model with MAP estimate = -578.3931
expansions: [(0, 23)]
discards: [ 0  8 73 77 79 85]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 580.6362 - loglik: -5.7324e+02 - logprior: -7.3980e+00
Epoch 2/2
22/22 - 3s - loss: 567.9695 - loglik: -5.6638e+02 - logprior: -1.5895e+00
Fitted a model with MAP estimate = -565.0374
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 589.8539 - loglik: -5.7591e+02 - logprior: -1.3947e+01
Epoch 2/10
11/11 - 2s - loss: 577.7523 - loglik: -5.7279e+02 - logprior: -4.9665e+00
Epoch 3/10
11/11 - 2s - loss: 572.8911 - loglik: -5.7090e+02 - logprior: -1.9945e+00
Epoch 4/10
11/11 - 2s - loss: 571.4839 - loglik: -5.7088e+02 - logprior: -6.0360e-01
Epoch 5/10
11/11 - 2s - loss: 569.2266 - loglik: -5.6899e+02 - logprior: -2.3793e-01
Epoch 6/10
11/11 - 2s - loss: 570.2850 - loglik: -5.7018e+02 - logprior: -1.0018e-01
Fitted a model with MAP estimate = -569.2179
Time for alignment: 63.4930
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 718.4996 - loglik: -7.0619e+02 - logprior: -1.2308e+01
Epoch 2/10
11/11 - 2s - loss: 663.2734 - loglik: -6.6033e+02 - logprior: -2.9430e+00
Epoch 3/10
11/11 - 2s - loss: 624.5867 - loglik: -6.2265e+02 - logprior: -1.9411e+00
Epoch 4/10
11/11 - 2s - loss: 606.5552 - loglik: -6.0452e+02 - logprior: -2.0320e+00
Epoch 5/10
11/11 - 2s - loss: 596.3968 - loglik: -5.9429e+02 - logprior: -2.1025e+00
Epoch 6/10
11/11 - 2s - loss: 596.8487 - loglik: -5.9467e+02 - logprior: -2.1746e+00
Fitted a model with MAP estimate = -594.4287
expansions: [(8, 2), (9, 2), (10, 1), (12, 1), (17, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 2), (62, 2), (63, 2), (64, 3), (65, 2), (68, 1), (81, 1), (82, 1), (83, 1), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 117 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 610.5300 - loglik: -5.9634e+02 - logprior: -1.4186e+01
Epoch 2/2
11/11 - 2s - loss: 583.8777 - loglik: -5.7791e+02 - logprior: -5.9693e+00
Fitted a model with MAP estimate = -580.0640
expansions: [(0, 24)]
discards: [  0  72  76  78  82  85 110]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 582.5322 - loglik: -5.7510e+02 - logprior: -7.4366e+00
Epoch 2/2
22/22 - 3s - loss: 567.3497 - loglik: -5.6571e+02 - logprior: -1.6354e+00
Fitted a model with MAP estimate = -565.4427
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 6s - loss: 590.1989 - loglik: -5.7667e+02 - logprior: -1.3525e+01
Epoch 2/10
11/11 - 2s - loss: 573.0502 - loglik: -5.6935e+02 - logprior: -3.6996e+00
Epoch 3/10
11/11 - 2s - loss: 572.0149 - loglik: -5.7075e+02 - logprior: -1.2684e+00
Epoch 4/10
11/11 - 2s - loss: 570.8764 - loglik: -5.7032e+02 - logprior: -5.5278e-01
Epoch 5/10
11/11 - 2s - loss: 571.3358 - loglik: -5.7105e+02 - logprior: -2.8556e-01
Fitted a model with MAP estimate = -569.6263
Time for alignment: 57.8412
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 718.4799 - loglik: -7.0618e+02 - logprior: -1.2303e+01
Epoch 2/10
11/11 - 2s - loss: 661.6419 - loglik: -6.5870e+02 - logprior: -2.9398e+00
Epoch 3/10
11/11 - 2s - loss: 619.8125 - loglik: -6.1793e+02 - logprior: -1.8779e+00
Epoch 4/10
11/11 - 2s - loss: 605.5795 - loglik: -6.0371e+02 - logprior: -1.8702e+00
Epoch 5/10
11/11 - 2s - loss: 596.9937 - loglik: -5.9504e+02 - logprior: -1.9564e+00
Epoch 6/10
11/11 - 2s - loss: 596.3093 - loglik: -5.9426e+02 - logprior: -2.0462e+00
Epoch 7/10
11/11 - 2s - loss: 592.9405 - loglik: -5.9097e+02 - logprior: -1.9653e+00
Epoch 8/10
11/11 - 2s - loss: 591.7571 - loglik: -5.8983e+02 - logprior: -1.9281e+00
Epoch 9/10
11/11 - 2s - loss: 590.9387 - loglik: -5.8896e+02 - logprior: -1.9791e+00
Epoch 10/10
11/11 - 2s - loss: 589.1485 - loglik: -5.8715e+02 - logprior: -1.9963e+00
Fitted a model with MAP estimate = -588.4650
expansions: [(8, 2), (9, 3), (10, 2), (11, 1), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (65, 1), (66, 1), (67, 2), (68, 1), (81, 1), (83, 2), (84, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 609.1394 - loglik: -5.9498e+02 - logprior: -1.4156e+01
Epoch 2/2
11/11 - 2s - loss: 583.1251 - loglik: -5.7728e+02 - logprior: -5.8411e+00
Fitted a model with MAP estimate = -579.1047
expansions: [(0, 23)]
discards: [  0  14  75  85 108]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 6s - loss: 581.2940 - loglik: -5.7388e+02 - logprior: -7.4108e+00
Epoch 2/2
22/22 - 3s - loss: 568.4752 - loglik: -5.6688e+02 - logprior: -1.5938e+00
Fitted a model with MAP estimate = -565.7944
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 7s - loss: 585.4838 - loglik: -5.7369e+02 - logprior: -1.1793e+01
Epoch 2/10
11/11 - 2s - loss: 572.8767 - loglik: -5.7023e+02 - logprior: -2.6477e+00
Epoch 3/10
11/11 - 2s - loss: 571.6943 - loglik: -5.7066e+02 - logprior: -1.0317e+00
Epoch 4/10
11/11 - 2s - loss: 570.0179 - loglik: -5.6952e+02 - logprior: -4.9507e-01
Epoch 5/10
11/11 - 2s - loss: 569.5847 - loglik: -5.6930e+02 - logprior: -2.8111e-01
Epoch 6/10
11/11 - 2s - loss: 568.3807 - loglik: -5.6820e+02 - logprior: -1.7447e-01
Epoch 7/10
11/11 - 2s - loss: 567.2563 - loglik: -5.6716e+02 - logprior: -9.7208e-02
Epoch 8/10
11/11 - 2s - loss: 567.0460 - loglik: -5.6699e+02 - logprior: -5.3990e-02
Epoch 9/10
11/11 - 2s - loss: 565.8311 - loglik: -5.6579e+02 - logprior: -3.9010e-02
Epoch 10/10
11/11 - 2s - loss: 564.3204 - loglik: -5.6426e+02 - logprior: -5.5494e-02
Fitted a model with MAP estimate = -563.0686
Time for alignment: 74.7079
Fitting a model of length 89 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 719.0931 - loglik: -7.0679e+02 - logprior: -1.2305e+01
Epoch 2/10
11/11 - 2s - loss: 662.7115 - loglik: -6.5977e+02 - logprior: -2.9430e+00
Epoch 3/10
11/11 - 2s - loss: 619.5094 - loglik: -6.1764e+02 - logprior: -1.8691e+00
Epoch 4/10
11/11 - 2s - loss: 606.6240 - loglik: -6.0478e+02 - logprior: -1.8399e+00
Epoch 5/10
11/11 - 2s - loss: 599.8387 - loglik: -5.9798e+02 - logprior: -1.8593e+00
Epoch 6/10
11/11 - 2s - loss: 595.2717 - loglik: -5.9335e+02 - logprior: -1.9166e+00
Epoch 7/10
11/11 - 2s - loss: 595.2451 - loglik: -5.9342e+02 - logprior: -1.8197e+00
Epoch 8/10
11/11 - 2s - loss: 594.2546 - loglik: -5.9248e+02 - logprior: -1.7723e+00
Epoch 9/10
11/11 - 2s - loss: 591.3346 - loglik: -5.8953e+02 - logprior: -1.8027e+00
Epoch 10/10
11/11 - 2s - loss: 590.3111 - loglik: -5.8849e+02 - logprior: -1.8158e+00
Fitted a model with MAP estimate = -589.8586
expansions: [(8, 2), (9, 3), (10, 1), (12, 2), (33, 1), (34, 1), (38, 1), (39, 1), (40, 1), (61, 1), (62, 2), (63, 1), (65, 1), (66, 1), (67, 2), (68, 1), (84, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 114 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 5s - loss: 608.0891 - loglik: -5.9397e+02 - logprior: -1.4122e+01
Epoch 2/2
11/11 - 2s - loss: 582.7898 - loglik: -5.7702e+02 - logprior: -5.7738e+00
Fitted a model with MAP estimate = -577.5734
expansions: [(0, 23)]
discards: [ 0  8 75 85]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 3348 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
22/22 - 7s - loss: 580.5876 - loglik: -5.7319e+02 - logprior: -7.4017e+00
Epoch 2/2
22/22 - 3s - loss: 566.2584 - loglik: -5.6459e+02 - logprior: -1.6636e+00
Fitted a model with MAP estimate = -565.1142
expansions: []
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 3348 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 5s - loss: 591.9752 - loglik: -5.7798e+02 - logprior: -1.3998e+01
Epoch 2/10
11/11 - 2s - loss: 576.0427 - loglik: -5.7100e+02 - logprior: -5.0384e+00
Epoch 3/10
11/11 - 2s - loss: 573.9180 - loglik: -5.7182e+02 - logprior: -2.0982e+00
Epoch 4/10
11/11 - 2s - loss: 571.3442 - loglik: -5.7070e+02 - logprior: -6.4719e-01
Epoch 5/10
11/11 - 2s - loss: 568.4182 - loglik: -5.6815e+02 - logprior: -2.6390e-01
Epoch 6/10
11/11 - 2s - loss: 571.0591 - loglik: -5.7093e+02 - logprior: -1.3079e-01
Fitted a model with MAP estimate = -569.2544
Time for alignment: 65.1085
Computed alignments with likelihoods: ['-565.4694', '-565.0374', '-565.4427', '-563.0686', '-565.1142']
Best model has likelihood: -563.0686  (prior= -0.0674 )
time for generating output: 0.2289
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/oxidored_q6.projection.fasta
SP score = 0.5740310077519379
Training of 5 independent models on file aldosered.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8d20c4be0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8fcac3f10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8f403f910>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1591.8845 - loglik: -1.5902e+03 - logprior: -1.6487e+00
Epoch 2/10
39/39 - 16s - loss: 1477.1470 - loglik: -1.4756e+03 - logprior: -1.5247e+00
Epoch 3/10
39/39 - 16s - loss: 1464.2937 - loglik: -1.4627e+03 - logprior: -1.6193e+00
Epoch 4/10
39/39 - 16s - loss: 1459.6169 - loglik: -1.4580e+03 - logprior: -1.6016e+00
Epoch 5/10
39/39 - 16s - loss: 1455.3240 - loglik: -1.4537e+03 - logprior: -1.6058e+00
Epoch 6/10
39/39 - 16s - loss: 1449.8820 - loglik: -1.4483e+03 - logprior: -1.5929e+00
Epoch 7/10
39/39 - 16s - loss: 1442.5012 - loglik: -1.4409e+03 - logprior: -1.5725e+00
Epoch 8/10
39/39 - 16s - loss: 1424.6836 - loglik: -1.4231e+03 - logprior: -1.5846e+00
Epoch 9/10
39/39 - 16s - loss: 1393.4283 - loglik: -1.3917e+03 - logprior: -1.6439e+00
Epoch 10/10
39/39 - 16s - loss: 1239.2274 - loglik: -1.2337e+03 - logprior: -5.4898e+00
Fitted a model with MAP estimate = -1209.2170
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92 106 107 108 109 110 111 112 113 114 148 151 152 153 154 155
 156 157 175 176 177 178 179 180 212 213 217]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 11s - loss: 1702.7947 - loglik: -1.7005e+03 - logprior: -2.3065e+00
Epoch 2/2
39/39 - 9s - loss: 1660.9573 - loglik: -1.6601e+03 - logprior: -8.9148e-01
Fitted a model with MAP estimate = -1512.5162
expansions: [(0, 116), (14, 1), (16, 4), (17, 24), (48, 24), (105, 22)]
discards: [ 93  96  97  98 101]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 29s - loss: 1533.5559 - loglik: -1.5310e+03 - logprior: -2.6044e+00
Epoch 2/2
39/39 - 23s - loss: 1451.8752 - loglik: -1.4510e+03 - logprior: -9.0131e-01
Fitted a model with MAP estimate = -1341.6736
expansions: [(72, 1), (149, 5), (202, 1), (207, 3), (210, 4)]
discards: [  0 135 197 198 199 282 283 284 285 286 287 288 289 290]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 30s - loss: 1345.9684 - loglik: -1.3443e+03 - logprior: -1.6850e+00
Epoch 2/10
45/45 - 26s - loss: 1332.7029 - loglik: -1.3321e+03 - logprior: -5.7109e-01
Epoch 3/10
45/45 - 27s - loss: 1328.5607 - loglik: -1.3281e+03 - logprior: -5.0345e-01
Epoch 4/10
45/45 - 26s - loss: 1326.6465 - loglik: -1.3262e+03 - logprior: -4.2670e-01
Epoch 5/10
45/45 - 26s - loss: 1320.4082 - loglik: -1.3200e+03 - logprior: -3.5070e-01
Epoch 6/10
45/45 - 26s - loss: 1314.3799 - loglik: -1.3141e+03 - logprior: -2.8773e-01
Epoch 7/10
45/45 - 27s - loss: 1304.6083 - loglik: -1.3044e+03 - logprior: -2.0485e-01
Epoch 8/10
45/45 - 26s - loss: 1290.1155 - loglik: -1.2897e+03 - logprior: -3.6056e-01
Epoch 9/10
45/45 - 26s - loss: 1218.3816 - loglik: -1.2179e+03 - logprior: -4.6368e-01
Epoch 10/10
45/45 - 26s - loss: 1105.7522 - loglik: -1.1041e+03 - logprior: -1.5956e+00
Fitted a model with MAP estimate = -1075.1418
Time for alignment: 597.1702
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1586.4778 - loglik: -1.5848e+03 - logprior: -1.6457e+00
Epoch 2/10
39/39 - 16s - loss: 1469.2925 - loglik: -1.4679e+03 - logprior: -1.3774e+00
Epoch 3/10
39/39 - 16s - loss: 1457.6948 - loglik: -1.4563e+03 - logprior: -1.4403e+00
Epoch 4/10
39/39 - 16s - loss: 1453.1176 - loglik: -1.4517e+03 - logprior: -1.4126e+00
Epoch 5/10
39/39 - 16s - loss: 1449.1158 - loglik: -1.4477e+03 - logprior: -1.3829e+00
Epoch 6/10
39/39 - 16s - loss: 1443.1758 - loglik: -1.4418e+03 - logprior: -1.3545e+00
Epoch 7/10
39/39 - 16s - loss: 1436.1853 - loglik: -1.4348e+03 - logprior: -1.3449e+00
Epoch 8/10
39/39 - 16s - loss: 1418.0537 - loglik: -1.4167e+03 - logprior: -1.3750e+00
Epoch 9/10
39/39 - 16s - loss: 1390.4131 - loglik: -1.3890e+03 - logprior: -1.4216e+00
Epoch 10/10
39/39 - 16s - loss: 1246.2710 - loglik: -1.2414e+03 - logprior: -4.8337e+00
Fitted a model with MAP estimate = -1210.0158
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95 108 109 110 111 112 113 114 115 116 135 139 161
 163 164 165 180 181 182 183 184 185 186 187 188 189 213 214 215 216 217]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 1702.2662 - loglik: -1.7000e+03 - logprior: -2.2488e+00
Epoch 2/2
39/39 - 8s - loss: 1667.3323 - loglik: -1.6664e+03 - logprior: -9.4439e-01
Fitted a model with MAP estimate = -1524.7546
expansions: [(0, 118), (14, 6), (15, 2), (27, 23), (28, 13), (29, 3), (30, 1), (31, 2), (32, 18), (98, 26)]
discards: [33 34 35 36 37 38 39 40 41 89 92]
Re-initialized the encoder parameters.
Fitting a model of length 299 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1531.8566 - loglik: -1.5292e+03 - logprior: -2.6699e+00
Epoch 2/2
39/39 - 24s - loss: 1446.9497 - loglik: -1.4459e+03 - logprior: -1.0484e+00
Fitted a model with MAP estimate = -1336.3099
expansions: [(141, 2), (156, 2), (205, 2), (210, 3), (213, 2), (214, 1)]
discards: [  0 197 199 200 201 202 286 287 288 289 290 291 292 293 294 295 296 297
 298]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 1337.3926 - loglik: -1.3357e+03 - logprior: -1.6927e+00
Epoch 2/10
45/45 - 26s - loss: 1327.3638 - loglik: -1.3267e+03 - logprior: -6.5231e-01
Epoch 3/10
45/45 - 27s - loss: 1321.4265 - loglik: -1.3209e+03 - logprior: -5.7032e-01
Epoch 4/10
45/45 - 26s - loss: 1318.8917 - loglik: -1.3183e+03 - logprior: -5.8315e-01
Epoch 5/10
45/45 - 26s - loss: 1315.0975 - loglik: -1.3147e+03 - logprior: -4.0418e-01
Epoch 6/10
45/45 - 26s - loss: 1312.7086 - loglik: -1.3123e+03 - logprior: -3.8962e-01
Epoch 7/10
45/45 - 27s - loss: 1302.5959 - loglik: -1.3022e+03 - logprior: -3.2953e-01
Epoch 8/10
45/45 - 26s - loss: 1280.1854 - loglik: -1.2798e+03 - logprior: -3.3533e-01
Epoch 9/10
45/45 - 26s - loss: 1204.6436 - loglik: -1.2039e+03 - logprior: -7.1056e-01
Epoch 10/10
45/45 - 26s - loss: 1096.9097 - loglik: -1.0950e+03 - logprior: -1.8253e+00
Fitted a model with MAP estimate = -1075.5341
Time for alignment: 599.9955
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 20s - loss: 1595.6213 - loglik: -1.5939e+03 - logprior: -1.6730e+00
Epoch 2/10
39/39 - 16s - loss: 1479.4309 - loglik: -1.4779e+03 - logprior: -1.5636e+00
Epoch 3/10
39/39 - 16s - loss: 1464.8503 - loglik: -1.4632e+03 - logprior: -1.6735e+00
Epoch 4/10
39/39 - 16s - loss: 1460.3458 - loglik: -1.4587e+03 - logprior: -1.6105e+00
Epoch 5/10
39/39 - 16s - loss: 1456.0486 - loglik: -1.4544e+03 - logprior: -1.5907e+00
Epoch 6/10
39/39 - 16s - loss: 1450.6604 - loglik: -1.4491e+03 - logprior: -1.5694e+00
Epoch 7/10
39/39 - 16s - loss: 1444.0540 - loglik: -1.4425e+03 - logprior: -1.5470e+00
Epoch 8/10
39/39 - 16s - loss: 1426.6764 - loglik: -1.4251e+03 - logprior: -1.5709e+00
Epoch 9/10
39/39 - 16s - loss: 1399.8043 - loglik: -1.3982e+03 - logprior: -1.5900e+00
Epoch 10/10
39/39 - 16s - loss: 1239.6783 - loglik: -1.2344e+03 - logprior: -5.2274e+00
Fitted a model with MAP estimate = -1210.2212
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98 106 110 111 112 113 114 115 116 124
 149 152 153 154 155 176 177 178 179 180 192 213 214 215 216 217]
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 1702.1384 - loglik: -1.7003e+03 - logprior: -1.8142e+00
Epoch 2/2
39/39 - 9s - loss: 1663.5392 - loglik: -1.6630e+03 - logprior: -4.8995e-01
Fitted a model with MAP estimate = -1519.8521
expansions: [(0, 88), (1, 4), (2, 23), (5, 5), (10, 47), (17, 24), (18, 2), (19, 4), (100, 16)]
discards: [20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 94 95]
Re-initialized the encoder parameters.
Fitting a model of length 292 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 26s - loss: 1539.0747 - loglik: -1.5369e+03 - logprior: -2.1594e+00
Epoch 2/2
39/39 - 23s - loss: 1453.2682 - loglik: -1.4523e+03 - logprior: -9.7132e-01
Fitted a model with MAP estimate = -1342.6792
expansions: [(14, 1), (15, 1), (23, 1), (52, 2), (67, 2), (71, 3), (89, 2), (147, 3), (148, 2), (149, 2), (150, 2), (159, 2), (160, 2), (162, 1), (163, 1), (164, 1), (165, 1)]
discards: [  0 101 102 103 104 105 106 190 192 193 194 195 196 197 198 199 200 201
 202 203 204 205 216 217 286 287 288]
Re-initialized the encoder parameters.
Fitting a model of length 294 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 30s - loss: 1341.1284 - loglik: -1.3394e+03 - logprior: -1.7106e+00
Epoch 2/10
45/45 - 27s - loss: 1329.0887 - loglik: -1.3284e+03 - logprior: -6.4659e-01
Epoch 3/10
45/45 - 27s - loss: 1326.3665 - loglik: -1.3258e+03 - logprior: -5.9916e-01
Epoch 4/10
45/45 - 26s - loss: 1319.7739 - loglik: -1.3192e+03 - logprior: -5.5937e-01
Epoch 5/10
45/45 - 28s - loss: 1316.0225 - loglik: -1.3155e+03 - logprior: -5.2397e-01
Epoch 6/10
45/45 - 26s - loss: 1311.4783 - loglik: -1.3110e+03 - logprior: -4.5370e-01
Epoch 7/10
45/45 - 27s - loss: 1299.8915 - loglik: -1.2994e+03 - logprior: -4.5595e-01
Epoch 8/10
45/45 - 26s - loss: 1278.4454 - loglik: -1.2780e+03 - logprior: -4.2792e-01
Epoch 9/10
45/45 - 26s - loss: 1196.6981 - loglik: -1.1958e+03 - logprior: -8.9279e-01
Epoch 10/10
45/45 - 27s - loss: 1094.4170 - loglik: -1.0924e+03 - logprior: -1.9817e+00
Fitted a model with MAP estimate = -1074.6113
Time for alignment: 602.8920
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1590.9462 - loglik: -1.5893e+03 - logprior: -1.6415e+00
Epoch 2/10
39/39 - 16s - loss: 1474.9229 - loglik: -1.4735e+03 - logprior: -1.4190e+00
Epoch 3/10
39/39 - 16s - loss: 1461.9438 - loglik: -1.4603e+03 - logprior: -1.6320e+00
Epoch 4/10
39/39 - 16s - loss: 1457.7689 - loglik: -1.4562e+03 - logprior: -1.5783e+00
Epoch 5/10
39/39 - 16s - loss: 1453.8185 - loglik: -1.4523e+03 - logprior: -1.5571e+00
Epoch 6/10
39/39 - 16s - loss: 1448.2843 - loglik: -1.4467e+03 - logprior: -1.5412e+00
Epoch 7/10
39/39 - 16s - loss: 1441.5731 - loglik: -1.4400e+03 - logprior: -1.5418e+00
Epoch 8/10
39/39 - 16s - loss: 1424.1720 - loglik: -1.4226e+03 - logprior: -1.5606e+00
Epoch 9/10
39/39 - 16s - loss: 1399.5751 - loglik: -1.3979e+03 - logprior: -1.6004e+00
Epoch 10/10
39/39 - 16s - loss: 1266.6201 - loglik: -1.2616e+03 - logprior: -4.9709e+00
Fitted a model with MAP estimate = -1213.5285
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  94  95  96
  97 106 107 108 109 110 111 112 113 114 117 118 119 120 121 122 123 124
 125 150 153 154 155 156 157 158 159 176 177 178 179 180 192 214 215 216
 217]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 1701.6312 - loglik: -1.6996e+03 - logprior: -1.9968e+00
Epoch 2/2
39/39 - 9s - loss: 1676.4214 - loglik: -1.6759e+03 - logprior: -5.5144e-01
Fitted a model with MAP estimate = -1539.9696
expansions: [(0, 31), (3, 10), (5, 63), (6, 13), (16, 1), (17, 71)]
discards: [22 23 24 25 26 27 28 29 30 31 32 33 34 35 59 91]
Re-initialized the encoder parameters.
Fitting a model of length 270 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 25s - loss: 1565.9402 - loglik: -1.5633e+03 - logprior: -2.6574e+00
Epoch 2/2
39/39 - 21s - loss: 1471.0339 - loglik: -1.4699e+03 - logprior: -1.0864e+00
Fitted a model with MAP estimate = -1351.8817
expansions: [(45, 1), (85, 2), (117, 4), (151, 3), (152, 2), (153, 2), (154, 1), (165, 3), (166, 1), (168, 1), (169, 1), (170, 1), (171, 1), (182, 1), (183, 1), (190, 2), (195, 3), (234, 1), (265, 1)]
discards: [  0  13  14  31  32  77  78  79  80  81  82 185 186 187]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 1347.5510 - loglik: -1.3459e+03 - logprior: -1.6785e+00
Epoch 2/10
45/45 - 26s - loss: 1333.5088 - loglik: -1.3329e+03 - logprior: -5.7592e-01
Epoch 3/10
45/45 - 26s - loss: 1328.3989 - loglik: -1.3279e+03 - logprior: -5.4421e-01
Epoch 4/10
45/45 - 26s - loss: 1320.8605 - loglik: -1.3204e+03 - logprior: -4.7190e-01
Epoch 5/10
45/45 - 26s - loss: 1324.4974 - loglik: -1.3240e+03 - logprior: -4.3810e-01
Fitted a model with MAP estimate = -1315.8540
Time for alignment: 456.7454
Fitting a model of length 224 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 19s - loss: 1590.8600 - loglik: -1.5892e+03 - logprior: -1.6837e+00
Epoch 2/10
39/39 - 16s - loss: 1472.8184 - loglik: -1.4712e+03 - logprior: -1.5719e+00
Epoch 3/10
39/39 - 16s - loss: 1461.7975 - loglik: -1.4602e+03 - logprior: -1.6435e+00
Epoch 4/10
39/39 - 16s - loss: 1457.6995 - loglik: -1.4561e+03 - logprior: -1.6042e+00
Epoch 5/10
39/39 - 16s - loss: 1452.7921 - loglik: -1.4512e+03 - logprior: -1.5974e+00
Epoch 6/10
39/39 - 16s - loss: 1446.4266 - loglik: -1.4448e+03 - logprior: -1.5869e+00
Epoch 7/10
39/39 - 16s - loss: 1439.8424 - loglik: -1.4382e+03 - logprior: -1.5760e+00
Epoch 8/10
39/39 - 16s - loss: 1422.0675 - loglik: -1.4205e+03 - logprior: -1.5733e+00
Epoch 9/10
39/39 - 16s - loss: 1392.0829 - loglik: -1.3904e+03 - logprior: -1.6004e+00
Epoch 10/10
39/39 - 16s - loss: 1241.1550 - loglik: -1.2356e+03 - logprior: -5.5020e+00
Fitted a model with MAP estimate = -1209.3031
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98 106 107 108 109 110 111 112 113 114
 133 150 153 154 155 156 157 175 176 177 178 179 180 209 210 211 212 213
 217]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 1702.1412 - loglik: -1.7001e+03 - logprior: -2.0419e+00
Epoch 2/2
39/39 - 9s - loss: 1666.5918 - loglik: -1.6658e+03 - logprior: -7.8708e-01
Fitted a model with MAP estimate = -1519.0921
expansions: [(0, 61), (2, 43), (6, 27), (11, 1), (23, 17), (33, 1), (43, 23), (97, 21)]
discards: [88 89 90]
Re-initialized the encoder parameters.
Fitting a model of length 288 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 27s - loss: 1544.1990 - loglik: -1.5417e+03 - logprior: -2.4660e+00
Epoch 2/2
39/39 - 23s - loss: 1462.1017 - loglik: -1.4610e+03 - logprior: -1.1262e+00
Fitted a model with MAP estimate = -1349.1729
expansions: [(70, 2), (81, 3), (144, 4), (151, 3), (155, 3), (156, 2), (157, 2), (158, 2), (159, 1), (161, 2), (202, 2), (205, 1), (206, 3), (209, 1), (243, 1), (244, 1)]
discards: [  0  13  14  15  40  41  42  43  44  45  46  47  61 119 120 121 122 123
 197 198 199 200 233 234 277 278 279 280 281 282]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 13277 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
45/45 - 29s - loss: 1340.5817 - loglik: -1.3389e+03 - logprior: -1.6996e+00
Epoch 2/10
45/45 - 26s - loss: 1331.8153 - loglik: -1.3311e+03 - logprior: -7.2224e-01
Epoch 3/10
45/45 - 26s - loss: 1325.8777 - loglik: -1.3252e+03 - logprior: -6.4282e-01
Epoch 4/10
45/45 - 26s - loss: 1321.5458 - loglik: -1.3209e+03 - logprior: -6.0811e-01
Epoch 5/10
45/45 - 26s - loss: 1317.3673 - loglik: -1.3169e+03 - logprior: -5.0438e-01
Epoch 6/10
45/45 - 26s - loss: 1312.2241 - loglik: -1.3118e+03 - logprior: -4.4661e-01
Epoch 7/10
45/45 - 26s - loss: 1302.6149 - loglik: -1.3022e+03 - logprior: -3.9991e-01
Epoch 8/10
45/45 - 26s - loss: 1290.5348 - loglik: -1.2901e+03 - logprior: -3.8509e-01
Epoch 9/10
45/45 - 26s - loss: 1226.5903 - loglik: -1.2260e+03 - logprior: -5.8745e-01
Epoch 10/10
45/45 - 26s - loss: 1111.0273 - loglik: -1.1092e+03 - logprior: -1.8002e+00
Fitted a model with MAP estimate = -1077.3808
Time for alignment: 594.5059
Computed alignments with likelihoods: ['-1075.1418', '-1075.5341', '-1074.6113', '-1213.5285', '-1077.3808']
Best model has likelihood: -1074.6113  (prior= -1.8945 )
time for generating output: 0.2705
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aldosered.projection.fasta
SP score = 0.062188227469238445
Training of 5 independent models on file sdr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe952bba820>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efa93dc0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efa93cd0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 929.5567 - loglik: -9.2852e+02 - logprior: -1.0324e+00
Epoch 2/10
30/30 - 6s - loss: 859.3879 - loglik: -8.5826e+02 - logprior: -1.1276e+00
Epoch 3/10
30/30 - 7s - loss: 846.2126 - loglik: -8.4512e+02 - logprior: -1.0924e+00
Epoch 4/10
30/30 - 6s - loss: 844.2468 - loglik: -8.4317e+02 - logprior: -1.0703e+00
Epoch 5/10
30/30 - 6s - loss: 843.5389 - loglik: -8.4249e+02 - logprior: -1.0367e+00
Epoch 6/10
30/30 - 7s - loss: 840.3139 - loglik: -8.3928e+02 - logprior: -1.0190e+00
Epoch 7/10
30/30 - 7s - loss: 837.5055 - loglik: -8.3648e+02 - logprior: -1.0088e+00
Epoch 8/10
30/30 - 6s - loss: 834.6939 - loglik: -8.3366e+02 - logprior: -9.9850e-01
Epoch 9/10
30/30 - 7s - loss: 829.9008 - loglik: -8.2887e+02 - logprior: -9.9094e-01
Epoch 10/10
30/30 - 6s - loss: 822.0443 - loglik: -8.2100e+02 - logprior: -9.8944e-01
Fitted a model with MAP estimate = -817.8790
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (26, 1), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (48, 1), (52, 1), (54, 1), (56, 1), (71, 2), (72, 2), (75, 1), (78, 2), (89, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (117, 2), (126, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 850.9375 - loglik: -8.4977e+02 - logprior: -1.1670e+00
Epoch 2/2
61/61 - 11s - loss: 828.3469 - loglik: -8.2751e+02 - logprior: -8.3909e-01
Fitted a model with MAP estimate = -791.4955
expansions: []
discards: [ 18  47  51  58  93  95 104 134 149 151 155 157]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 16s - loss: 830.8693 - loglik: -8.2989e+02 - logprior: -9.7712e-01
Epoch 2/2
61/61 - 10s - loss: 828.7169 - loglik: -8.2802e+02 - logprior: -6.9748e-01
Fitted a model with MAP estimate = -791.2880
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 791.1248 - loglik: -7.9048e+02 - logprior: -6.4177e-01
Epoch 2/10
87/87 - 14s - loss: 789.6908 - loglik: -7.8915e+02 - logprior: -5.3451e-01
Epoch 3/10
87/87 - 14s - loss: 788.1797 - loglik: -7.8766e+02 - logprior: -5.1005e-01
Epoch 4/10
87/87 - 14s - loss: 787.1905 - loglik: -7.8669e+02 - logprior: -4.8972e-01
Epoch 5/10
87/87 - 14s - loss: 783.1610 - loglik: -7.8266e+02 - logprior: -4.7177e-01
Epoch 6/10
87/87 - 14s - loss: 781.6835 - loglik: -7.8118e+02 - logprior: -4.6011e-01
Epoch 7/10
87/87 - 14s - loss: 779.7020 - loglik: -7.7919e+02 - logprior: -4.4311e-01
Epoch 8/10
87/87 - 14s - loss: 763.4931 - loglik: -7.6296e+02 - logprior: -4.3858e-01
Epoch 9/10
87/87 - 14s - loss: 759.6926 - loglik: -7.5911e+02 - logprior: -4.3928e-01
Epoch 10/10
87/87 - 14s - loss: 696.5560 - loglik: -6.9588e+02 - logprior: -4.8469e-01
Fitted a model with MAP estimate = -660.7130
Time for alignment: 375.1510
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 10s - loss: 929.2144 - loglik: -9.2818e+02 - logprior: -1.0364e+00
Epoch 2/10
30/30 - 7s - loss: 858.5421 - loglik: -8.5745e+02 - logprior: -1.0943e+00
Epoch 3/10
30/30 - 7s - loss: 846.6351 - loglik: -8.4556e+02 - logprior: -1.0692e+00
Epoch 4/10
30/30 - 6s - loss: 843.3779 - loglik: -8.4231e+02 - logprior: -1.0600e+00
Epoch 5/10
30/30 - 6s - loss: 843.4696 - loglik: -8.4243e+02 - logprior: -1.0337e+00
Fitted a model with MAP estimate = -804.5686
expansions: [(14, 1), (15, 1), (16, 1), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (48, 1), (51, 1), (52, 1), (56, 1), (72, 2), (74, 2), (75, 1), (78, 2), (89, 1), (92, 1), (93, 1), (94, 1), (99, 1), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 15s - loss: 836.2472 - loglik: -8.3514e+02 - logprior: -1.1023e+00
Epoch 2/2
61/61 - 11s - loss: 828.7452 - loglik: -8.2793e+02 - logprior: -8.0978e-01
Fitted a model with MAP estimate = -790.9764
expansions: []
discards: [ 25  48  51  57  93  96 103 134 148 152 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 831.2426 - loglik: -8.3025e+02 - logprior: -9.8801e-01
Epoch 2/2
61/61 - 10s - loss: 828.0601 - loglik: -8.2735e+02 - logprior: -7.1285e-01
Fitted a model with MAP estimate = -791.5157
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 791.1842 - loglik: -7.9054e+02 - logprior: -6.4574e-01
Epoch 2/10
87/87 - 14s - loss: 789.8893 - loglik: -7.8934e+02 - logprior: -5.4244e-01
Epoch 3/10
87/87 - 14s - loss: 787.5613 - loglik: -7.8703e+02 - logprior: -5.2116e-01
Epoch 4/10
87/87 - 14s - loss: 787.2803 - loglik: -7.8676e+02 - logprior: -5.0264e-01
Epoch 5/10
87/87 - 14s - loss: 784.6187 - loglik: -7.8410e+02 - logprior: -4.8782e-01
Epoch 6/10
87/87 - 13s - loss: 781.2088 - loglik: -7.8069e+02 - logprior: -4.7421e-01
Epoch 7/10
87/87 - 14s - loss: 778.4000 - loglik: -7.7787e+02 - logprior: -4.5758e-01
Epoch 8/10
87/87 - 14s - loss: 763.2048 - loglik: -7.6265e+02 - logprior: -4.5623e-01
Epoch 9/10
87/87 - 14s - loss: 759.5402 - loglik: -7.5894e+02 - logprior: -4.5917e-01
Epoch 10/10
87/87 - 14s - loss: 694.5318 - loglik: -6.9383e+02 - logprior: -5.0811e-01
Fitted a model with MAP estimate = -659.6549
Time for alignment: 342.0279
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 11s - loss: 929.3812 - loglik: -9.2834e+02 - logprior: -1.0390e+00
Epoch 2/10
30/30 - 6s - loss: 858.2549 - loglik: -8.5716e+02 - logprior: -1.0954e+00
Epoch 3/10
30/30 - 7s - loss: 846.1481 - loglik: -8.4508e+02 - logprior: -1.0712e+00
Epoch 4/10
30/30 - 6s - loss: 843.7440 - loglik: -8.4266e+02 - logprior: -1.0761e+00
Epoch 5/10
30/30 - 6s - loss: 842.5531 - loglik: -8.4149e+02 - logprior: -1.0503e+00
Epoch 6/10
30/30 - 6s - loss: 839.9172 - loglik: -8.3887e+02 - logprior: -1.0309e+00
Epoch 7/10
30/30 - 7s - loss: 837.8127 - loglik: -8.3677e+02 - logprior: -1.0170e+00
Epoch 8/10
30/30 - 6s - loss: 834.7609 - loglik: -8.3372e+02 - logprior: -1.0091e+00
Epoch 9/10
30/30 - 7s - loss: 828.5966 - loglik: -8.2755e+02 - logprior: -9.9931e-01
Epoch 10/10
30/30 - 6s - loss: 821.8231 - loglik: -8.2077e+02 - logprior: -9.9704e-01
Fitted a model with MAP estimate = -818.1392
expansions: [(14, 1), (15, 1), (16, 2), (20, 1), (26, 2), (29, 1), (35, 1), (36, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (52, 1), (54, 1), (55, 1), (56, 1), (72, 1), (73, 2), (75, 1), (78, 1), (89, 1), (92, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 1), (116, 2), (117, 2), (125, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 851.0443 - loglik: -8.4988e+02 - logprior: -1.1683e+00
Epoch 2/2
61/61 - 11s - loss: 829.6449 - loglik: -8.2881e+02 - logprior: -8.3815e-01
Fitted a model with MAP estimate = -791.4143
expansions: [(93, 2)]
discards: [ 18  31  48  52  59  95  97 133 148 153 155]
Re-initialized the encoder parameters.
Fitting a model of length 169 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 830.6041 - loglik: -8.2962e+02 - logprior: -9.8071e-01
Epoch 2/2
61/61 - 10s - loss: 828.3901 - loglik: -8.2768e+02 - logprior: -7.0599e-01
Fitted a model with MAP estimate = -791.3262
expansions: []
discards: [89]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 18s - loss: 791.0372 - loglik: -7.9039e+02 - logprior: -6.4475e-01
Epoch 2/10
87/87 - 14s - loss: 789.7249 - loglik: -7.8920e+02 - logprior: -5.2777e-01
Epoch 3/10
87/87 - 14s - loss: 787.7112 - loglik: -7.8720e+02 - logprior: -5.0927e-01
Epoch 4/10
87/87 - 14s - loss: 788.0291 - loglik: -7.8752e+02 - logprior: -4.8999e-01
Fitted a model with MAP estimate = -784.7266
Time for alignment: 292.8057
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 10s - loss: 929.3953 - loglik: -9.2837e+02 - logprior: -1.0288e+00
Epoch 2/10
30/30 - 6s - loss: 859.2396 - loglik: -8.5815e+02 - logprior: -1.0927e+00
Epoch 3/10
30/30 - 6s - loss: 847.1139 - loglik: -8.4604e+02 - logprior: -1.0739e+00
Epoch 4/10
30/30 - 6s - loss: 844.8095 - loglik: -8.4374e+02 - logprior: -1.0641e+00
Epoch 5/10
30/30 - 6s - loss: 843.0345 - loglik: -8.4199e+02 - logprior: -1.0402e+00
Epoch 6/10
30/30 - 6s - loss: 840.7445 - loglik: -8.3970e+02 - logprior: -1.0269e+00
Epoch 7/10
30/30 - 6s - loss: 837.3362 - loglik: -8.3630e+02 - logprior: -1.0160e+00
Epoch 8/10
30/30 - 6s - loss: 834.6422 - loglik: -8.3361e+02 - logprior: -1.0055e+00
Epoch 9/10
30/30 - 7s - loss: 829.7715 - loglik: -8.2873e+02 - logprior: -9.9902e-01
Epoch 10/10
30/30 - 6s - loss: 820.4988 - loglik: -8.1944e+02 - logprior: -9.9766e-01
Fitted a model with MAP estimate = -818.6744
expansions: [(14, 1), (15, 2), (16, 2), (20, 1), (26, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (48, 1), (51, 1), (52, 1), (54, 1), (72, 1), (74, 2), (75, 1), (78, 1), (89, 1), (92, 1), (93, 1), (94, 1), (97, 1), (102, 2), (113, 3), (114, 2), (116, 2), (126, 1), (127, 1), (128, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 179 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 852.3066 - loglik: -8.5113e+02 - logprior: -1.1808e+00
Epoch 2/2
61/61 - 11s - loss: 828.2407 - loglik: -8.2739e+02 - logprior: -8.4739e-01
Fitted a model with MAP estimate = -791.8755
expansions: []
discards: [ 15  19  32  49  53  60  97 134 149 152 155]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 14s - loss: 830.9011 - loglik: -8.2993e+02 - logprior: -9.6971e-01
Epoch 2/2
61/61 - 10s - loss: 828.6232 - loglik: -8.2793e+02 - logprior: -6.9138e-01
Fitted a model with MAP estimate = -791.3552
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 17s - loss: 791.6210 - loglik: -7.9099e+02 - logprior: -6.3154e-01
Epoch 2/10
87/87 - 14s - loss: 789.5731 - loglik: -7.8905e+02 - logprior: -5.2428e-01
Epoch 3/10
87/87 - 14s - loss: 787.6694 - loglik: -7.8716e+02 - logprior: -5.0228e-01
Epoch 4/10
87/87 - 14s - loss: 786.5667 - loglik: -7.8607e+02 - logprior: -4.8315e-01
Epoch 5/10
87/87 - 14s - loss: 784.7711 - loglik: -7.8427e+02 - logprior: -4.7144e-01
Epoch 6/10
87/87 - 14s - loss: 781.5423 - loglik: -7.8105e+02 - logprior: -4.5010e-01
Epoch 7/10
87/87 - 14s - loss: 778.9937 - loglik: -7.7849e+02 - logprior: -4.3418e-01
Epoch 8/10
87/87 - 14s - loss: 763.6628 - loglik: -7.6313e+02 - logprior: -4.3055e-01
Epoch 9/10
87/87 - 14s - loss: 759.8605 - loglik: -7.5929e+02 - logprior: -4.3130e-01
Epoch 10/10
87/87 - 14s - loss: 695.7115 - loglik: -6.9504e+02 - logprior: -4.8475e-01
Fitted a model with MAP estimate = -660.8003
Time for alignment: 374.0529
Fitting a model of length 134 on 25079 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
30/30 - 9s - loss: 928.9463 - loglik: -9.2791e+02 - logprior: -1.0337e+00
Epoch 2/10
30/30 - 6s - loss: 857.7275 - loglik: -8.5663e+02 - logprior: -1.0998e+00
Epoch 3/10
30/30 - 6s - loss: 845.4286 - loglik: -8.4434e+02 - logprior: -1.0859e+00
Epoch 4/10
30/30 - 6s - loss: 841.9397 - loglik: -8.4084e+02 - logprior: -1.0960e+00
Epoch 5/10
30/30 - 7s - loss: 841.1053 - loglik: -8.4003e+02 - logprior: -1.0706e+00
Epoch 6/10
30/30 - 6s - loss: 838.1747 - loglik: -8.3711e+02 - logprior: -1.0552e+00
Epoch 7/10
30/30 - 7s - loss: 836.3348 - loglik: -8.3527e+02 - logprior: -1.0448e+00
Epoch 8/10
30/30 - 6s - loss: 832.9059 - loglik: -8.3184e+02 - logprior: -1.0359e+00
Epoch 9/10
30/30 - 6s - loss: 827.5643 - loglik: -8.2649e+02 - logprior: -1.0280e+00
Epoch 10/10
30/30 - 7s - loss: 820.7197 - loglik: -8.1964e+02 - logprior: -1.0266e+00
Fitted a model with MAP estimate = -816.0119
expansions: [(14, 1), (15, 2), (16, 2), (20, 1), (22, 2), (29, 1), (33, 1), (35, 1), (37, 1), (38, 2), (40, 2), (41, 1), (43, 2), (52, 1), (54, 1), (55, 1), (56, 1), (72, 1), (74, 2), (75, 1), (78, 2), (82, 1), (92, 1), (93, 1), (94, 1), (97, 1), (102, 2), (111, 1), (113, 2), (114, 2), (116, 1), (117, 2), (125, 1), (127, 1), (128, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 15s - loss: 850.8862 - loglik: -8.4971e+02 - logprior: -1.1738e+00
Epoch 2/2
61/61 - 11s - loss: 828.5738 - loglik: -8.2772e+02 - logprior: -8.5068e-01
Fitted a model with MAP estimate = -791.6537
expansions: []
discards: [ 15  19  27  49  53  60  97 105 135 150 153 158]
Re-initialized the encoder parameters.
Fitting a model of length 168 on 25079 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
61/61 - 13s - loss: 830.8661 - loglik: -8.2990e+02 - logprior: -9.7088e-01
Epoch 2/2
61/61 - 10s - loss: 828.5674 - loglik: -8.2788e+02 - logprior: -6.8936e-01
Fitted a model with MAP estimate = -791.2503
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 168 on 50157 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
87/87 - 16s - loss: 791.8437 - loglik: -7.9120e+02 - logprior: -6.3875e-01
Epoch 2/10
87/87 - 14s - loss: 789.1986 - loglik: -7.8867e+02 - logprior: -5.2371e-01
Epoch 3/10
87/87 - 14s - loss: 787.3599 - loglik: -7.8685e+02 - logprior: -5.0399e-01
Epoch 4/10
87/87 - 14s - loss: 787.1033 - loglik: -7.8661e+02 - logprior: -4.8232e-01
Epoch 5/10
87/87 - 14s - loss: 785.2656 - loglik: -7.8477e+02 - logprior: -4.6550e-01
Epoch 6/10
87/87 - 14s - loss: 781.5069 - loglik: -7.8101e+02 - logprior: -4.4968e-01
Epoch 7/10
87/87 - 13s - loss: 778.2469 - loglik: -7.7774e+02 - logprior: -4.3792e-01
Epoch 8/10
87/87 - 14s - loss: 762.7457 - loglik: -7.6222e+02 - logprior: -4.2878e-01
Epoch 9/10
87/87 - 14s - loss: 760.5015 - loglik: -7.5992e+02 - logprior: -4.3521e-01
Epoch 10/10
87/87 - 14s - loss: 695.8831 - loglik: -6.9521e+02 - logprior: -4.8432e-01
Fitted a model with MAP estimate = -660.5725
Time for alignment: 373.4999
Computed alignments with likelihoods: ['-660.7130', '-659.6549', '-784.7266', '-660.8003', '-660.5725']
Best model has likelihood: -659.6549  (prior= -0.5599 )
time for generating output: 0.2441
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sdr.projection.fasta
SP score = 0.015841474857868302
Training of 5 independent models on file p450.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b4295b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efb614f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea77edd6a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 2568.9978 - loglik: -2.5679e+03 - logprior: -1.0688e+00
Epoch 2/10
40/40 - 39s - loss: 2461.7761 - loglik: -2.4619e+03 - logprior: 0.1361
Epoch 3/10
40/40 - 39s - loss: 2452.8511 - loglik: -2.4531e+03 - logprior: 0.2265
Epoch 4/10
40/40 - 39s - loss: 2443.8267 - loglik: -2.4441e+03 - logprior: 0.2765
Epoch 5/10
40/40 - 39s - loss: 2431.3560 - loglik: -2.4316e+03 - logprior: 0.2897
Epoch 6/10
40/40 - 39s - loss: 2412.4536 - loglik: -2.4128e+03 - logprior: 0.3166
Epoch 7/10
40/40 - 39s - loss: 2339.7183 - loglik: -2.3395e+03 - logprior: -2.3461e-01
Epoch 8/10
40/40 - 39s - loss: 1971.7692 - loglik: -1.9671e+03 - logprior: -4.6527e+00
Epoch 9/10
40/40 - 39s - loss: 1845.0795 - loglik: -1.8410e+03 - logprior: -4.0160e+00
Epoch 10/10
40/40 - 39s - loss: 1828.6713 - loglik: -1.8249e+03 - logprior: -3.7208e+00
Fitted a model with MAP estimate = -1570.3665
expansions: []
discards: [ 11  32 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245
 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263
 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281
 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299
 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317
 318 319 320 321 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 228 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 29s - loss: 2696.0791 - loglik: -2.6953e+03 - logprior: -7.5113e-01
Epoch 2/2
40/40 - 22s - loss: 2665.2993 - loglik: -2.6659e+03 - logprior: 0.6323
Fitted a model with MAP estimate = -1932.7277
expansions: [(0, 139), (228, 297)]
discards: [  0   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22
  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40
  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58
  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76
  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94
  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112
 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130
 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148
 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166
 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184
 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202
 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220
 221 222 223 224 225 226 227]
Re-initialized the encoder parameters.
Fitting a model of length 441 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 66s - loss: 2517.1760 - loglik: -2.5158e+03 - logprior: -1.3529e+00
Epoch 2/2
40/40 - 63s - loss: 2376.0850 - loglik: -2.3755e+03 - logprior: -6.0816e-01
Fitted a model with MAP estimate = -1738.4127
expansions: [(23, 1), (61, 1), (207, 2), (262, 1), (306, 1), (312, 1), (320, 1), (340, 2), (348, 1), (353, 1), (365, 1), (382, 1), (383, 2), (402, 1), (408, 1), (423, 1), (424, 2), (430, 2), (434, 1), (441, 2)]
discards: [  0 137 138 139 140 141 142 143 144 164 165]
Re-initialized the encoder parameters.
Fitting a model of length 456 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 99s - loss: 1729.7673 - loglik: -1.7290e+03 - logprior: -7.5896e-01
Epoch 2/10
113/113 - 94s - loss: 1723.9254 - loglik: -1.7234e+03 - logprior: -5.4843e-01
Epoch 3/10
113/113 - 94s - loss: 1727.6190 - loglik: -1.7271e+03 - logprior: -5.4867e-01
Fitted a model with MAP estimate = -1719.9815
Time for alignment: 1179.7702
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 2579.7898 - loglik: -2.5785e+03 - logprior: -1.2429e+00
Epoch 2/10
40/40 - 39s - loss: 2471.5261 - loglik: -2.4698e+03 - logprior: -1.7171e+00
Epoch 3/10
40/40 - 39s - loss: 2458.4380 - loglik: -2.4558e+03 - logprior: -2.5974e+00
Epoch 4/10
40/40 - 39s - loss: 2447.8740 - loglik: -2.4456e+03 - logprior: -2.3054e+00
Epoch 5/10
40/40 - 39s - loss: 2430.7092 - loglik: -2.4283e+03 - logprior: -2.3682e+00
Epoch 6/10
40/40 - 39s - loss: 2405.5366 - loglik: -2.4033e+03 - logprior: -2.2523e+00
Epoch 7/10
40/40 - 39s - loss: 2302.6069 - loglik: -2.3003e+03 - logprior: -2.2319e+00
Epoch 8/10
40/40 - 39s - loss: 1929.3684 - loglik: -1.9249e+03 - logprior: -4.4460e+00
Epoch 9/10
40/40 - 39s - loss: 1844.6035 - loglik: -1.8403e+03 - logprior: -4.2105e+00
Epoch 10/10
40/40 - 39s - loss: 1830.0225 - loglik: -1.8258e+03 - logprior: -4.1529e+00
Fitted a model with MAP estimate = -1570.3054
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  32 244 245 246
 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264
 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282
 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300
 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318
 319 320 321 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 26s - loss: 2689.7322 - loglik: -2.6881e+03 - logprior: -1.5843e+00
Epoch 2/2
40/40 - 23s - loss: 2667.5417 - loglik: -2.6680e+03 - logprior: 0.4660
Fitted a model with MAP estimate = -1930.6315
expansions: [(0, 234), (229, 180)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  79  80  81
  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99
 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117
 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135
 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153
 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189
 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207
 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225
 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 442 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 67s - loss: 2513.2603 - loglik: -2.5117e+03 - logprior: -1.5509e+00
Epoch 2/2
40/40 - 63s - loss: 2388.7910 - loglik: -2.3876e+03 - logprior: -1.1710e+00
Fitted a model with MAP estimate = -1746.6121
expansions: [(15, 1), (22, 1), (24, 1), (39, 2), (58, 1), (64, 1), (96, 1), (115, 1), (116, 1), (117, 1), (126, 1), (151, 2), (160, 1), (207, 2), (278, 1), (279, 2), (280, 3), (281, 1), (282, 1), (284, 1), (289, 1), (307, 1), (312, 1), (316, 1), (321, 1), (325, 1), (333, 1), (348, 4), (349, 2), (370, 1), (372, 1), (389, 2), (393, 1), (395, 1)]
discards: [  0 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424
 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 96s - loss: 1732.6580 - loglik: -1.7319e+03 - logprior: -8.0218e-01
Epoch 2/10
113/113 - 93s - loss: 1724.5685 - loglik: -1.7240e+03 - logprior: -5.3082e-01
Epoch 3/10
113/113 - 93s - loss: 1730.0591 - loglik: -1.7296e+03 - logprior: -4.0938e-01
Fitted a model with MAP estimate = -1720.8677
Time for alignment: 1174.6120
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 44s - loss: 2562.4155 - loglik: -2.5609e+03 - logprior: -1.5600e+00
Epoch 2/10
40/40 - 39s - loss: 2438.5825 - loglik: -2.4361e+03 - logprior: -2.4676e+00
Epoch 3/10
40/40 - 39s - loss: 2426.5947 - loglik: -2.4240e+03 - logprior: -2.5744e+00
Epoch 4/10
40/40 - 39s - loss: 2416.5303 - loglik: -2.4139e+03 - logprior: -2.5790e+00
Epoch 5/10
40/40 - 39s - loss: 2402.6658 - loglik: -2.4001e+03 - logprior: -2.5289e+00
Epoch 6/10
40/40 - 39s - loss: 2375.5464 - loglik: -2.3730e+03 - logprior: -2.4817e+00
Epoch 7/10
40/40 - 39s - loss: 2255.0667 - loglik: -2.2525e+03 - logprior: -2.5589e+00
Epoch 8/10
40/40 - 39s - loss: 1926.2566 - loglik: -1.9224e+03 - logprior: -3.8362e+00
Epoch 9/10
40/40 - 39s - loss: 1843.7048 - loglik: -1.8398e+03 - logprior: -3.8906e+00
Epoch 10/10
40/40 - 39s - loss: 1830.1650 - loglik: -1.8262e+03 - logprior: -3.8551e+00
Fitted a model with MAP estimate = -1571.6300
expansions: []
discards: [ 11 178 179 180 181 182 183 184 185 186 187 191 247 248 249 250 251 252
 253 254 255 256 257 258 259 261 262 263 264 265 266 267 268 269 270 271
 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289
 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307
 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325
 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 27s - loss: 2691.6001 - loglik: -2.6907e+03 - logprior: -8.6052e-01
Epoch 2/2
40/40 - 23s - loss: 2664.5210 - loglik: -2.6654e+03 - logprior: 0.8894
Fitted a model with MAP estimate = -1931.6286
expansions: [(0, 137), (236, 288)]
discards: [  0   1   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37
  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55
  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73
  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91
  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109
 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127
 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145
 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163
 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181
 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199
 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217
 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235]
Re-initialized the encoder parameters.
Fitting a model of length 427 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 62s - loss: 2512.4954 - loglik: -2.5112e+03 - logprior: -1.3250e+00
Epoch 2/2
40/40 - 59s - loss: 2377.4199 - loglik: -2.3769e+03 - logprior: -5.2078e-01
Fitted a model with MAP estimate = -1739.3134
expansions: [(180, 3), (246, 1), (298, 1), (302, 1), (303, 1), (305, 2), (310, 1), (328, 1), (329, 2), (337, 1), (342, 1), (354, 1), (360, 1), (369, 2), (370, 3), (384, 1), (392, 1), (399, 1), (411, 2), (415, 1), (416, 2), (420, 1), (427, 2)]
discards: [  0  87 136 137 138 195]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 21013 sequences.
Batch size= 128 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
113/113 - 98s - loss: 1734.2877 - loglik: -1.7335e+03 - logprior: -7.5088e-01
Epoch 2/10
113/113 - 93s - loss: 1722.8191 - loglik: -1.7223e+03 - logprior: -4.8809e-01
Epoch 3/10
113/113 - 94s - loss: 1723.2220 - loglik: -1.7227e+03 - logprior: -5.1239e-01
Fitted a model with MAP estimate = -1720.1332
Time for alignment: 1167.8581
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 2574.5757 - loglik: -2.5735e+03 - logprior: -1.0562e+00
Epoch 2/10
40/40 - 39s - loss: 2469.5532 - loglik: -2.4696e+03 - logprior: 0.0827
Epoch 3/10
40/40 - 39s - loss: 2459.8711 - loglik: -2.4600e+03 - logprior: 0.1555
Epoch 4/10
40/40 - 39s - loss: 2450.9038 - loglik: -2.4511e+03 - logprior: 0.1664
Epoch 5/10
40/40 - 39s - loss: 2437.7251 - loglik: -2.4379e+03 - logprior: 0.2042
Epoch 6/10
40/40 - 39s - loss: 2419.1121 - loglik: -2.4193e+03 - logprior: 0.1720
Epoch 7/10
40/40 - 39s - loss: 2358.5984 - loglik: -2.3586e+03 - logprior: 0.0276
Epoch 8/10
40/40 - 39s - loss: 1983.1057 - loglik: -1.9778e+03 - logprior: -5.2413e+00
Epoch 9/10
40/40 - 39s - loss: 1846.5657 - loglik: -1.8423e+03 - logprior: -4.1750e+00
Epoch 10/10
40/40 - 39s - loss: 1828.5535 - loglik: -1.8247e+03 - logprior: -3.7960e+00
Fitted a model with MAP estimate = -1570.2493
expansions: []
discards: [ 11  93  94 123 233 234 235 236 237 238 239 240 241 242 243 244 245 246
 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264
 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282
 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300
 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318
 319 320 321 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 229 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 25s - loss: 2696.2571 - loglik: -2.6955e+03 - logprior: -7.4059e-01
Epoch 2/2
40/40 - 23s - loss: 2664.9343 - loglik: -2.6655e+03 - logprior: 0.6047
Fitted a model with MAP estimate = -1932.8380
expansions: [(229, 315)]
discards: [  0   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22
  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40
  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58
  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76
  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94
  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112
 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130
 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148
 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166
 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184
 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202
 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220
 221 222 223 224 225 226 227 228]
Re-initialized the encoder parameters.
Fitting a model of length 320 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 42s - loss: 2562.6838 - loglik: -2.5602e+03 - logprior: -2.4670e+00
Epoch 2/2
40/40 - 37s - loss: 2463.4470 - loglik: -2.4611e+03 - logprior: -2.3270e+00
Fitted a model with MAP estimate = -1799.0871
expansions: [(0, 2), (299, 2), (301, 1), (302, 1), (303, 3), (305, 1), (320, 3)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197
 198 199 200 201 202]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 19s - loss: 1856.5966 - loglik: -1.8550e+03 - logprior: -1.5830e+00
Epoch 2/10
56/56 - 17s - loss: 1852.9939 - loglik: -1.8514e+03 - logprior: -1.5590e+00
Epoch 3/10
56/56 - 17s - loss: 1847.6064 - loglik: -1.8460e+03 - logprior: -1.5707e+00
Epoch 4/10
56/56 - 17s - loss: 1849.5276 - loglik: -1.8480e+03 - logprior: -1.5678e+00
Fitted a model with MAP estimate = -1842.8324
Time for alignment: 827.0807
Fitting a model of length 330 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
40/40 - 42s - loss: 2557.8464 - loglik: -2.5561e+03 - logprior: -1.7329e+00
Epoch 2/10
40/40 - 39s - loss: 2435.3921 - loglik: -2.4325e+03 - logprior: -2.8431e+00
Epoch 3/10
40/40 - 39s - loss: 2425.1404 - loglik: -2.4223e+03 - logprior: -2.8548e+00
Epoch 4/10
40/40 - 39s - loss: 2416.8911 - loglik: -2.4141e+03 - logprior: -2.8234e+00
Epoch 5/10
40/40 - 39s - loss: 2403.1611 - loglik: -2.4004e+03 - logprior: -2.7820e+00
Epoch 6/10
40/40 - 39s - loss: 2382.1204 - loglik: -2.3793e+03 - logprior: -2.7724e+00
Epoch 7/10
40/40 - 39s - loss: 2273.0024 - loglik: -2.2700e+03 - logprior: -3.0069e+00
Epoch 8/10
40/40 - 39s - loss: 1937.8640 - loglik: -1.9335e+03 - logprior: -4.2999e+00
Epoch 9/10
40/40 - 39s - loss: 1844.6451 - loglik: -1.8404e+03 - logprior: -4.1560e+00
Epoch 10/10
40/40 - 39s - loss: 1830.2415 - loglik: -1.8261e+03 - logprior: -4.0924e+00
Fitted a model with MAP estimate = -1571.6441
expansions: []
discards: [ 11 162 163 164 180 181 182 183 184 185 186 187 188 189 193 194 248 249
 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267
 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285
 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303
 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321
 322 323 324 325 326 327 328 329]
Re-initialized the encoder parameters.
Fitting a model of length 232 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 28s - loss: 2693.0542 - loglik: -2.6922e+03 - logprior: -8.4722e-01
Epoch 2/2
40/40 - 23s - loss: 2655.8884 - loglik: -2.6560e+03 - logprior: 0.1357
Fitted a model with MAP estimate = -1912.6224
expansions: [(0, 136), (232, 201)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161
 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179
 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208
 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226
 227 228 229 230 231]
Re-initialized the encoder parameters.
Fitting a model of length 348 on 10507 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
40/40 - 46s - loss: 2550.6326 - loglik: -2.5488e+03 - logprior: -1.8728e+00
Epoch 2/2
40/40 - 42s - loss: 2454.8201 - loglik: -2.4536e+03 - logprior: -1.1761e+00
Fitted a model with MAP estimate = -1787.3936
expansions: [(17, 18), (19, 10), (20, 9), (21, 11), (22, 47), (23, 4), (24, 9), (25, 5), (26, 1), (32, 1), (36, 1), (37, 2), (38, 1), (39, 1), (41, 1), (42, 1), (43, 1), (69, 2), (73, 3), (83, 1), (119, 1), (122, 1), (267, 1)]
discards: [  0 254 255 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286
 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304
 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322
 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340
 341 342 343 344 345 346 347]
Re-initialized the encoder parameters.
Fitting a model of length 401 on 21013 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
56/56 - 77s - loss: 1763.1022 - loglik: -1.7619e+03 - logprior: -1.1550e+00
Epoch 2/10
56/56 - 74s - loss: 1731.3503 - loglik: -1.7307e+03 - logprior: -6.4939e-01
Epoch 3/10
56/56 - 74s - loss: 1742.3551 - loglik: -1.7418e+03 - logprior: -5.3134e-01
Fitted a model with MAP estimate = -1735.5645
Time for alignment: 1027.6153
Computed alignments with likelihoods: ['-1570.3665', '-1570.3054', '-1571.6300', '-1570.2493', '-1571.6441']
Best model has likelihood: -1570.2493  (prior= -1.8901 )
time for generating output: 0.3737
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/p450.projection.fasta
SP score = 0.009705942833641784
Training of 5 independent models on file hla.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea0919f160>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d05b310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9059ec430>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 958.9203 - loglik: -9.5604e+02 - logprior: -2.8761e+00
Epoch 2/10
19/19 - 4s - loss: 790.2051 - loglik: -7.8862e+02 - logprior: -1.5824e+00
Epoch 3/10
19/19 - 4s - loss: 724.2425 - loglik: -7.2211e+02 - logprior: -2.1309e+00
Epoch 4/10
19/19 - 4s - loss: 714.9777 - loglik: -7.1276e+02 - logprior: -2.2161e+00
Epoch 5/10
19/19 - 4s - loss: 711.0457 - loglik: -7.0889e+02 - logprior: -2.1540e+00
Epoch 6/10
19/19 - 4s - loss: 710.1996 - loglik: -7.0810e+02 - logprior: -2.0993e+00
Epoch 7/10
19/19 - 4s - loss: 708.2033 - loglik: -7.0613e+02 - logprior: -2.0727e+00
Epoch 8/10
19/19 - 4s - loss: 706.5474 - loglik: -7.0449e+02 - logprior: -2.0566e+00
Epoch 9/10
19/19 - 4s - loss: 703.9148 - loglik: -7.0185e+02 - logprior: -2.0549e+00
Epoch 10/10
19/19 - 4s - loss: 701.2353 - loglik: -6.9918e+02 - logprior: -2.0431e+00
Fitted a model with MAP estimate = -628.7319
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (35, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 2), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (105, 1), (114, 1), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 689.0220 - loglik: -6.8598e+02 - logprior: -3.0385e+00
Epoch 2/2
19/19 - 5s - loss: 652.8865 - loglik: -6.5167e+02 - logprior: -1.2176e+00
Fitted a model with MAP estimate = -584.1277
expansions: []
discards: [ 50 151]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 654.9391 - loglik: -6.5197e+02 - logprior: -2.9663e+00
Epoch 2/2
19/19 - 5s - loss: 650.6226 - loglik: -6.4957e+02 - logprior: -1.0533e+00
Fitted a model with MAP estimate = -583.2966
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 11s - loss: 585.6210 - loglik: -5.8349e+02 - logprior: -2.1260e+00
Epoch 2/10
22/22 - 6s - loss: 580.5167 - loglik: -5.7957e+02 - logprior: -9.4165e-01
Epoch 3/10
22/22 - 6s - loss: 579.9883 - loglik: -5.7909e+02 - logprior: -8.9707e-01
Epoch 4/10
22/22 - 6s - loss: 580.8676 - loglik: -5.8002e+02 - logprior: -8.4264e-01
Fitted a model with MAP estimate = -578.4460
Time for alignment: 137.8498
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 959.0197 - loglik: -9.5615e+02 - logprior: -2.8719e+00
Epoch 2/10
19/19 - 4s - loss: 788.8149 - loglik: -7.8722e+02 - logprior: -1.5922e+00
Epoch 3/10
19/19 - 4s - loss: 723.7860 - loglik: -7.2165e+02 - logprior: -2.1353e+00
Epoch 4/10
19/19 - 4s - loss: 714.0503 - loglik: -7.1179e+02 - logprior: -2.2567e+00
Epoch 5/10
19/19 - 4s - loss: 712.6498 - loglik: -7.1047e+02 - logprior: -2.1780e+00
Epoch 6/10
19/19 - 4s - loss: 709.7554 - loglik: -7.0763e+02 - logprior: -2.1270e+00
Epoch 7/10
19/19 - 4s - loss: 708.1137 - loglik: -7.0602e+02 - logprior: -2.0891e+00
Epoch 8/10
19/19 - 4s - loss: 706.7682 - loglik: -7.0468e+02 - logprior: -2.0834e+00
Epoch 9/10
19/19 - 4s - loss: 703.6808 - loglik: -7.0160e+02 - logprior: -2.0731e+00
Epoch 10/10
19/19 - 4s - loss: 701.8801 - loglik: -6.9981e+02 - logprior: -2.0588e+00
Fitted a model with MAP estimate = -628.9073
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 2), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (101, 1), (104, 1), (114, 1), (120, 1), (121, 2), (122, 2), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 688.7925 - loglik: -6.8575e+02 - logprior: -3.0430e+00
Epoch 2/2
19/19 - 5s - loss: 652.4849 - loglik: -6.5126e+02 - logprior: -1.2231e+00
Fitted a model with MAP estimate = -584.2048
expansions: []
discards: [149 152]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 655.4709 - loglik: -6.5250e+02 - logprior: -2.9688e+00
Epoch 2/2
19/19 - 5s - loss: 648.9031 - loglik: -6.4786e+02 - logprior: -1.0381e+00
Fitted a model with MAP estimate = -583.4873
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 583.9786 - loglik: -5.8190e+02 - logprior: -2.0820e+00
Epoch 2/10
22/22 - 6s - loss: 583.9352 - loglik: -5.8303e+02 - logprior: -9.0782e-01
Epoch 3/10
22/22 - 6s - loss: 577.8104 - loglik: -5.7691e+02 - logprior: -9.0404e-01
Epoch 4/10
22/22 - 6s - loss: 580.6450 - loglik: -5.7981e+02 - logprior: -8.3645e-01
Fitted a model with MAP estimate = -578.4515
Time for alignment: 135.3199
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 958.5410 - loglik: -9.5567e+02 - logprior: -2.8720e+00
Epoch 2/10
19/19 - 4s - loss: 791.7350 - loglik: -7.9016e+02 - logprior: -1.5789e+00
Epoch 3/10
19/19 - 4s - loss: 724.9741 - loglik: -7.2282e+02 - logprior: -2.1554e+00
Epoch 4/10
19/19 - 4s - loss: 716.4673 - loglik: -7.1421e+02 - logprior: -2.2562e+00
Epoch 5/10
19/19 - 4s - loss: 711.6055 - loglik: -7.0942e+02 - logprior: -2.1810e+00
Epoch 6/10
19/19 - 4s - loss: 711.5591 - loglik: -7.0945e+02 - logprior: -2.1106e+00
Epoch 7/10
19/19 - 4s - loss: 710.8511 - loglik: -7.0877e+02 - logprior: -2.0812e+00
Epoch 8/10
19/19 - 4s - loss: 706.3088 - loglik: -7.0423e+02 - logprior: -2.0783e+00
Epoch 9/10
19/19 - 4s - loss: 705.2850 - loglik: -7.0320e+02 - logprior: -2.0734e+00
Epoch 10/10
19/19 - 4s - loss: 702.9530 - loglik: -7.0086e+02 - logprior: -2.0893e+00
Fitted a model with MAP estimate = -629.9426
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (40, 1), (53, 2), (54, 1), (55, 1), (58, 1), (59, 1), (61, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (105, 1), (114, 1), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 689.2538 - loglik: -6.8630e+02 - logprior: -2.9546e+00
Epoch 2/2
19/19 - 5s - loss: 653.2237 - loglik: -6.5207e+02 - logprior: -1.1496e+00
Fitted a model with MAP estimate = -584.1629
expansions: []
discards: [150]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 655.4066 - loglik: -6.5244e+02 - logprior: -2.9646e+00
Epoch 2/2
19/19 - 5s - loss: 650.1636 - loglik: -6.4910e+02 - logprior: -1.0665e+00
Fitted a model with MAP estimate = -583.7976
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 585.0193 - loglik: -5.8289e+02 - logprior: -2.1257e+00
Epoch 2/10
22/22 - 6s - loss: 580.1793 - loglik: -5.7923e+02 - logprior: -9.4974e-01
Epoch 3/10
22/22 - 6s - loss: 580.7670 - loglik: -5.7986e+02 - logprior: -9.0962e-01
Fitted a model with MAP estimate = -579.1415
Time for alignment: 128.6111
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 958.8359 - loglik: -9.5596e+02 - logprior: -2.8744e+00
Epoch 2/10
19/19 - 4s - loss: 790.4065 - loglik: -7.8882e+02 - logprior: -1.5853e+00
Epoch 3/10
19/19 - 4s - loss: 725.4140 - loglik: -7.2331e+02 - logprior: -2.0993e+00
Epoch 4/10
19/19 - 4s - loss: 715.6943 - loglik: -7.1350e+02 - logprior: -2.1977e+00
Epoch 5/10
19/19 - 4s - loss: 711.9576 - loglik: -7.0981e+02 - logprior: -2.1453e+00
Epoch 6/10
19/19 - 4s - loss: 710.9416 - loglik: -7.0884e+02 - logprior: -2.1020e+00
Epoch 7/10
19/19 - 4s - loss: 709.0989 - loglik: -7.0703e+02 - logprior: -2.0659e+00
Epoch 8/10
19/19 - 4s - loss: 707.6691 - loglik: -7.0561e+02 - logprior: -2.0548e+00
Epoch 9/10
19/19 - 4s - loss: 703.2598 - loglik: -7.0120e+02 - logprior: -2.0496e+00
Epoch 10/10
19/19 - 4s - loss: 702.9481 - loglik: -7.0089e+02 - logprior: -2.0460e+00
Fitted a model with MAP estimate = -629.2332
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (41, 2), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 2), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (101, 1), (102, 1), (104, 1), (114, 1), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 688.8481 - loglik: -6.8582e+02 - logprior: -3.0323e+00
Epoch 2/2
19/19 - 5s - loss: 652.5205 - loglik: -6.5130e+02 - logprior: -1.2230e+00
Fitted a model with MAP estimate = -584.2603
expansions: []
discards: [ 50 151]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 655.6390 - loglik: -6.5269e+02 - logprior: -2.9455e+00
Epoch 2/2
19/19 - 5s - loss: 649.7661 - loglik: -6.4872e+02 - logprior: -1.0482e+00
Fitted a model with MAP estimate = -583.4295
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 585.1247 - loglik: -5.8302e+02 - logprior: -2.1068e+00
Epoch 2/10
22/22 - 6s - loss: 580.7879 - loglik: -5.7986e+02 - logprior: -9.3196e-01
Epoch 3/10
22/22 - 6s - loss: 581.3826 - loglik: -5.8047e+02 - logprior: -9.1222e-01
Fitted a model with MAP estimate = -579.0541
Time for alignment: 128.1631
Fitting a model of length 142 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 959.0069 - loglik: -9.5614e+02 - logprior: -2.8660e+00
Epoch 2/10
19/19 - 4s - loss: 789.4263 - loglik: -7.8786e+02 - logprior: -1.5630e+00
Epoch 3/10
19/19 - 4s - loss: 723.1255 - loglik: -7.2099e+02 - logprior: -2.1345e+00
Epoch 4/10
19/19 - 4s - loss: 714.0439 - loglik: -7.1179e+02 - logprior: -2.2496e+00
Epoch 5/10
19/19 - 4s - loss: 712.8132 - loglik: -7.1066e+02 - logprior: -2.1547e+00
Epoch 6/10
19/19 - 4s - loss: 709.2419 - loglik: -7.0711e+02 - logprior: -2.1292e+00
Epoch 7/10
19/19 - 4s - loss: 708.4614 - loglik: -7.0637e+02 - logprior: -2.0864e+00
Epoch 8/10
19/19 - 4s - loss: 705.0842 - loglik: -7.0300e+02 - logprior: -2.0818e+00
Epoch 9/10
19/19 - 4s - loss: 705.1728 - loglik: -7.0309e+02 - logprior: -2.0766e+00
Fitted a model with MAP estimate = -628.9969
expansions: [(7, 1), (8, 1), (9, 1), (14, 1), (16, 1), (17, 1), (18, 1), (32, 1), (33, 1), (34, 1), (53, 1), (54, 1), (55, 1), (56, 1), (58, 1), (59, 2), (71, 1), (75, 1), (76, 1), (92, 1), (93, 1), (97, 1), (100, 1), (102, 1), (104, 1), (114, 1), (120, 1), (121, 1), (122, 2), (124, 1), (125, 1), (129, 1), (130, 1), (131, 1), (132, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 179 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 688.0090 - loglik: -6.8499e+02 - logprior: -3.0227e+00
Epoch 2/2
19/19 - 5s - loss: 652.9603 - loglik: -6.5179e+02 - logprior: -1.1748e+00
Fitted a model with MAP estimate = -583.9305
expansions: []
discards: [151]
Re-initialized the encoder parameters.
Fitting a model of length 178 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 655.0800 - loglik: -6.5212e+02 - logprior: -2.9556e+00
Epoch 2/2
19/19 - 5s - loss: 650.3716 - loglik: -6.4932e+02 - logprior: -1.0521e+00
Fitted a model with MAP estimate = -583.5760
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 13465 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
22/22 - 9s - loss: 584.8952 - loglik: -5.8280e+02 - logprior: -2.0947e+00
Epoch 2/10
22/22 - 6s - loss: 580.9448 - loglik: -5.8002e+02 - logprior: -9.2508e-01
Epoch 3/10
22/22 - 6s - loss: 581.7791 - loglik: -5.8090e+02 - logprior: -8.7946e-01
Fitted a model with MAP estimate = -579.0986
Time for alignment: 124.1953
Computed alignments with likelihoods: ['-578.4460', '-578.4515', '-579.1415', '-579.0541', '-579.0986']
Best model has likelihood: -578.4460  (prior= -0.8190 )
time for generating output: 0.1807
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hla.projection.fasta
SP score = 1.0
Training of 5 independent models on file ltn.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cdf9b850>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea55b4aa90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8d20ab040>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 1186.0303 - loglik: -1.1539e+03 - logprior: -3.2120e+01
Epoch 2/10
12/12 - 3s - loss: 1102.4963 - loglik: -1.0991e+03 - logprior: -3.3881e+00
Epoch 3/10
12/12 - 3s - loss: 1038.6447 - loglik: -1.0381e+03 - logprior: -5.3403e-01
Epoch 4/10
12/12 - 3s - loss: 1011.9357 - loglik: -1.0124e+03 - logprior: 0.4247
Epoch 5/10
12/12 - 3s - loss: 1004.7857 - loglik: -1.0056e+03 - logprior: 0.8632
Epoch 6/10
12/12 - 3s - loss: 999.8613 - loglik: -1.0011e+03 - logprior: 1.2192
Epoch 7/10
12/12 - 3s - loss: 991.3525 - loglik: -9.9258e+02 - logprior: 1.2328
Epoch 8/10
12/12 - 3s - loss: 985.5908 - loglik: -9.8682e+02 - logprior: 1.2282
Epoch 9/10
12/12 - 3s - loss: 975.3395 - loglik: -9.7648e+02 - logprior: 1.1414
Epoch 10/10
12/12 - 3s - loss: 964.9443 - loglik: -9.6590e+02 - logprior: 0.9560
Fitted a model with MAP estimate = -960.0077
expansions: [(0, 2), (25, 1), (31, 2), (32, 2), (33, 1), (42, 1), (46, 1), (61, 1), (63, 1), (64, 2), (76, 1), (77, 2), (78, 2), (79, 2), (81, 1), (87, 1), (88, 1), (90, 1), (91, 1), (92, 1), (100, 1), (101, 1), (127, 8), (129, 2), (137, 1), (145, 1), (146, 3), (148, 2), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 236 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 9s - loss: 1116.8978 - loglik: -1.0747e+03 - logprior: -4.2199e+01
Epoch 2/2
12/12 - 4s - loss: 1017.3266 - loglik: -1.0096e+03 - logprior: -7.7364e+00
Fitted a model with MAP estimate = -997.1409
expansions: [(108, 1)]
discards: [  2   3   4  36  39  75  91 100 101 110 131 164 165 166 167 168 190 191
 194 222 223]
Re-initialized the encoder parameters.
Fitting a model of length 216 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1027.9169 - loglik: -9.9969e+02 - logprior: -2.8222e+01
Epoch 2/2
12/12 - 3s - loss: 985.8965 - loglik: -9.8385e+02 - logprior: -2.0512e+00
Fitted a model with MAP estimate = -983.1347
expansions: [(10, 3), (90, 1), (174, 2), (176, 1), (204, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 224 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 1014.4285 - loglik: -9.8779e+02 - logprior: -2.6643e+01
Epoch 2/10
12/12 - 3s - loss: 979.0136 - loglik: -9.7818e+02 - logprior: -8.3127e-01
Epoch 3/10
12/12 - 3s - loss: 975.2559 - loglik: -9.7926e+02 - logprior: 4.0063
Epoch 4/10
12/12 - 3s - loss: 972.1689 - loglik: -9.7825e+02 - logprior: 6.0861
Epoch 5/10
12/12 - 3s - loss: 971.1721 - loglik: -9.7835e+02 - logprior: 7.1828
Epoch 6/10
12/12 - 3s - loss: 964.8430 - loglik: -9.7265e+02 - logprior: 7.8092
Epoch 7/10
12/12 - 3s - loss: 964.4413 - loglik: -9.7264e+02 - logprior: 8.2038
Epoch 8/10
12/12 - 3s - loss: 957.5330 - loglik: -9.6605e+02 - logprior: 8.5221
Epoch 9/10
12/12 - 3s - loss: 950.1567 - loglik: -9.5877e+02 - logprior: 8.6183
Epoch 10/10
12/12 - 3s - loss: 933.3533 - loglik: -9.4185e+02 - logprior: 8.5031
Fitted a model with MAP estimate = -926.3968
Time for alignment: 100.4644
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 1187.5045 - loglik: -1.1554e+03 - logprior: -3.2118e+01
Epoch 2/10
12/12 - 3s - loss: 1103.3801 - loglik: -1.1000e+03 - logprior: -3.4252e+00
Epoch 3/10
12/12 - 3s - loss: 1038.5028 - loglik: -1.0378e+03 - logprior: -7.3172e-01
Epoch 4/10
12/12 - 3s - loss: 1014.1915 - loglik: -1.0142e+03 - logprior: 1.2233e-04
Epoch 5/10
12/12 - 3s - loss: 1001.1332 - loglik: -1.0017e+03 - logprior: 0.6167
Epoch 6/10
12/12 - 3s - loss: 996.5729 - loglik: -9.9745e+02 - logprior: 0.8769
Epoch 7/10
12/12 - 3s - loss: 992.1152 - loglik: -9.9305e+02 - logprior: 0.9326
Epoch 8/10
12/12 - 3s - loss: 986.1447 - loglik: -9.8714e+02 - logprior: 0.9941
Epoch 9/10
12/12 - 3s - loss: 976.7205 - loglik: -9.7764e+02 - logprior: 0.9263
Epoch 10/10
12/12 - 3s - loss: 970.7825 - loglik: -9.7151e+02 - logprior: 0.7341
Fitted a model with MAP estimate = -962.5307
expansions: [(0, 3), (19, 1), (31, 2), (32, 2), (33, 1), (42, 1), (45, 1), (59, 1), (60, 1), (62, 1), (63, 2), (75, 1), (76, 2), (77, 1), (78, 2), (88, 2), (89, 1), (100, 1), (101, 1), (120, 1), (126, 5), (127, 1), (129, 1), (131, 1), (138, 2), (145, 1), (146, 3), (148, 2), (154, 1), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 234 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1106.1410 - loglik: -1.0641e+03 - logprior: -4.2064e+01
Epoch 2/2
12/12 - 4s - loss: 1013.5364 - loglik: -1.0061e+03 - logprior: -7.4759e+00
Fitted a model with MAP estimate = -996.4835
expansions: [(75, 1), (108, 1), (116, 1)]
discards: [  0   1   2   3   4   5  37  73  76  92 114 129 154 184 188 189 192 220
 221]
Re-initialized the encoder parameters.
Fitting a model of length 218 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 1036.2701 - loglik: -9.9928e+02 - logprior: -3.6992e+01
Epoch 2/2
12/12 - 3s - loss: 997.3218 - loglik: -9.8615e+02 - logprior: -1.1169e+01
Fitted a model with MAP estimate = -991.4553
expansions: [(0, 6), (8, 3), (70, 1), (106, 1), (175, 2), (178, 1), (206, 1)]
discards: [  0  30 145]
Re-initialized the encoder parameters.
Fitting a model of length 230 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 8s - loss: 1016.3151 - loglik: -9.8893e+02 - logprior: -2.7384e+01
Epoch 2/10
12/12 - 4s - loss: 979.2447 - loglik: -9.7860e+02 - logprior: -6.4928e-01
Epoch 3/10
12/12 - 4s - loss: 974.1667 - loglik: -9.7832e+02 - logprior: 4.1508
Epoch 4/10
12/12 - 4s - loss: 973.7814 - loglik: -9.8000e+02 - logprior: 6.2199
Epoch 5/10
12/12 - 4s - loss: 966.6019 - loglik: -9.7392e+02 - logprior: 7.3206
Epoch 6/10
12/12 - 4s - loss: 964.4355 - loglik: -9.7231e+02 - logprior: 7.8705
Epoch 7/10
12/12 - 4s - loss: 960.1212 - loglik: -9.6840e+02 - logprior: 8.2795
Epoch 8/10
12/12 - 4s - loss: 956.3682 - loglik: -9.6490e+02 - logprior: 8.5332
Epoch 9/10
12/12 - 4s - loss: 946.1104 - loglik: -9.5475e+02 - logprior: 8.6435
Epoch 10/10
12/12 - 4s - loss: 931.8200 - loglik: -9.4029e+02 - logprior: 8.4741
Fitted a model with MAP estimate = -922.7636
Time for alignment: 103.3912
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1188.8704 - loglik: -1.1567e+03 - logprior: -3.2121e+01
Epoch 2/10
12/12 - 3s - loss: 1100.6158 - loglik: -1.0972e+03 - logprior: -3.4167e+00
Epoch 3/10
12/12 - 3s - loss: 1044.8301 - loglik: -1.0442e+03 - logprior: -6.4586e-01
Epoch 4/10
12/12 - 3s - loss: 1013.7437 - loglik: -1.0139e+03 - logprior: 0.1753
Epoch 5/10
12/12 - 3s - loss: 1001.7514 - loglik: -1.0024e+03 - logprior: 0.6007
Epoch 6/10
12/12 - 3s - loss: 996.3001 - loglik: -9.9730e+02 - logprior: 0.9992
Epoch 7/10
12/12 - 3s - loss: 993.4600 - loglik: -9.9452e+02 - logprior: 1.0603
Epoch 8/10
12/12 - 3s - loss: 985.1885 - loglik: -9.8619e+02 - logprior: 1.0029
Epoch 9/10
12/12 - 3s - loss: 978.0248 - loglik: -9.7884e+02 - logprior: 0.8172
Epoch 10/10
12/12 - 3s - loss: 964.7720 - loglik: -9.6539e+02 - logprior: 0.6244
Fitted a model with MAP estimate = -961.1267
expansions: [(0, 2), (19, 1), (30, 1), (31, 1), (40, 1), (42, 1), (45, 1), (61, 1), (63, 1), (64, 2), (75, 2), (76, 2), (77, 2), (78, 2), (86, 1), (88, 2), (90, 1), (101, 1), (102, 1), (104, 1), (121, 1), (127, 1), (129, 3), (137, 1), (147, 2), (149, 1), (152, 1), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 228 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 7s - loss: 1111.8865 - loglik: -1.0696e+03 - logprior: -4.2248e+01
Epoch 2/2
12/12 - 4s - loss: 1015.8181 - loglik: -1.0079e+03 - logprior: -7.8891e+00
Fitted a model with MAP estimate = -998.9658
expansions: [(35, 2), (116, 1), (154, 5), (159, 1), (184, 1)]
discards: [  2   3   4  36  37  38  73  89  95 107 161 182 185 188 214 215]
Re-initialized the encoder parameters.
Fitting a model of length 222 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 1026.6942 - loglik: -9.9871e+02 - logprior: -2.7984e+01
Epoch 2/2
12/12 - 3s - loss: 983.2906 - loglik: -9.8158e+02 - logprior: -1.7155e+00
Fitted a model with MAP estimate = -980.8663
expansions: [(10, 3), (32, 1), (210, 1)]
discards: [123 147 148 149]
Re-initialized the encoder parameters.
Fitting a model of length 223 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 7s - loss: 1015.3835 - loglik: -9.8872e+02 - logprior: -2.6667e+01
Epoch 2/10
12/12 - 3s - loss: 978.2939 - loglik: -9.7735e+02 - logprior: -9.4645e-01
Epoch 3/10
12/12 - 3s - loss: 976.1663 - loglik: -9.8011e+02 - logprior: 3.9456
Epoch 4/10
12/12 - 3s - loss: 976.6284 - loglik: -9.8266e+02 - logprior: 6.0318
Fitted a model with MAP estimate = -971.2823
Time for alignment: 78.2279
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1189.5558 - loglik: -1.1574e+03 - logprior: -3.2112e+01
Epoch 2/10
12/12 - 3s - loss: 1097.9882 - loglik: -1.0946e+03 - logprior: -3.4267e+00
Epoch 3/10
12/12 - 3s - loss: 1040.1957 - loglik: -1.0395e+03 - logprior: -6.6010e-01
Epoch 4/10
12/12 - 3s - loss: 1010.6170 - loglik: -1.0110e+03 - logprior: 0.4168
Epoch 5/10
12/12 - 3s - loss: 1008.2209 - loglik: -1.0092e+03 - logprior: 0.9633
Epoch 6/10
12/12 - 3s - loss: 997.0598 - loglik: -9.9831e+02 - logprior: 1.2517
Epoch 7/10
12/12 - 3s - loss: 994.4671 - loglik: -9.9585e+02 - logprior: 1.3830
Epoch 8/10
12/12 - 3s - loss: 987.6219 - loglik: -9.8897e+02 - logprior: 1.3478
Epoch 9/10
12/12 - 3s - loss: 975.3506 - loglik: -9.7648e+02 - logprior: 1.1368
Epoch 10/10
12/12 - 3s - loss: 968.8969 - loglik: -9.6974e+02 - logprior: 0.8503
Fitted a model with MAP estimate = -960.0730
expansions: [(0, 2), (19, 1), (30, 2), (31, 3), (41, 1), (42, 1), (48, 1), (60, 1), (62, 1), (63, 2), (75, 1), (76, 2), (77, 1), (78, 2), (87, 1), (90, 1), (101, 1), (102, 2), (120, 1), (127, 1), (129, 3), (137, 1), (146, 3), (154, 2), (158, 1), (173, 2), (174, 1), (175, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 226 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 1115.7892 - loglik: -1.0733e+03 - logprior: -4.2446e+01
Epoch 2/2
12/12 - 4s - loss: 1019.2180 - loglik: -1.0112e+03 - logprior: -7.9910e+00
Fitted a model with MAP estimate = -1000.2327
expansions: [(107, 1), (115, 1), (158, 1), (179, 2), (180, 2)]
discards: [  2   3   4  35  39  40  75  91 128 160 181 182 183 184 212 213]
Re-initialized the encoder parameters.
Fitting a model of length 217 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 1028.1766 - loglik: -9.9994e+02 - logprior: -2.8240e+01
Epoch 2/2
12/12 - 3s - loss: 989.0043 - loglik: -9.8702e+02 - logprior: -1.9828e+00
Fitted a model with MAP estimate = -983.8838
expansions: [(10, 3), (205, 1)]
discards: [177]
Re-initialized the encoder parameters.
Fitting a model of length 220 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1013.4732 - loglik: -9.8656e+02 - logprior: -2.6912e+01
Epoch 2/10
12/12 - 3s - loss: 984.4708 - loglik: -9.8335e+02 - logprior: -1.1256e+00
Epoch 3/10
12/12 - 3s - loss: 975.3383 - loglik: -9.7908e+02 - logprior: 3.7372
Epoch 4/10
12/12 - 3s - loss: 976.1690 - loglik: -9.8197e+02 - logprior: 5.8002
Fitted a model with MAP estimate = -972.9462
Time for alignment: 78.0284
Fitting a model of length 184 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 5s - loss: 1188.8522 - loglik: -1.1567e+03 - logprior: -3.2114e+01
Epoch 2/10
12/12 - 3s - loss: 1099.6274 - loglik: -1.0963e+03 - logprior: -3.3721e+00
Epoch 3/10
12/12 - 3s - loss: 1038.9073 - loglik: -1.0383e+03 - logprior: -6.0655e-01
Epoch 4/10
12/12 - 3s - loss: 1012.0781 - loglik: -1.0124e+03 - logprior: 0.3068
Epoch 5/10
12/12 - 3s - loss: 999.6811 - loglik: -1.0007e+03 - logprior: 0.9864
Epoch 6/10
12/12 - 3s - loss: 997.4903 - loglik: -9.9879e+02 - logprior: 1.3001
Epoch 7/10
12/12 - 3s - loss: 990.1894 - loglik: -9.9146e+02 - logprior: 1.2731
Epoch 8/10
12/12 - 3s - loss: 983.5607 - loglik: -9.8488e+02 - logprior: 1.3207
Epoch 9/10
12/12 - 3s - loss: 972.0630 - loglik: -9.7329e+02 - logprior: 1.2287
Epoch 10/10
12/12 - 3s - loss: 962.2012 - loglik: -9.6318e+02 - logprior: 0.9792
Fitted a model with MAP estimate = -955.9856
expansions: [(0, 3), (10, 2), (18, 1), (30, 2), (32, 1), (43, 1), (59, 1), (60, 1), (62, 1), (63, 2), (75, 1), (76, 2), (77, 1), (78, 2), (88, 3), (89, 1), (100, 1), (101, 1), (118, 1), (126, 6), (127, 1), (129, 2), (137, 1), (146, 4), (148, 1), (153, 2), (173, 2), (174, 1), (175, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 233 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 6s - loss: 1124.9612 - loglik: -1.0824e+03 - logprior: -4.2611e+01
Epoch 2/2
12/12 - 4s - loss: 1024.3126 - loglik: -1.0156e+03 - logprior: -8.7333e+00
Fitted a model with MAP estimate = -1001.9613
expansions: [(0, 7), (37, 1), (95, 1), (107, 1), (164, 1), (165, 1)]
discards: [  0   1   2   3  14  38  72  75  91  97 112 114 115 129 154 155 156 159
 166 187 188 189 191 192 219 220]
Re-initialized the encoder parameters.
Fitting a model of length 219 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
12/12 - 8s - loss: 1031.7114 - loglik: -1.0006e+03 - logprior: -3.1120e+01
Epoch 2/2
12/12 - 3s - loss: 990.3681 - loglik: -9.8778e+02 - logprior: -2.5917e+00
Fitted a model with MAP estimate = -984.3444
expansions: [(75, 1), (77, 1), (108, 1), (111, 2), (178, 3), (179, 2), (207, 1)]
discards: [1 2 3 4 5]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 1068 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
12/12 - 6s - loss: 1012.7463 - loglik: -9.8614e+02 - logprior: -2.6603e+01
Epoch 2/10
12/12 - 4s - loss: 986.2017 - loglik: -9.8548e+02 - logprior: -7.2371e-01
Epoch 3/10
12/12 - 3s - loss: 974.2481 - loglik: -9.7828e+02 - logprior: 4.0298
Epoch 4/10
12/12 - 3s - loss: 974.0330 - loglik: -9.8006e+02 - logprior: 6.0319
Epoch 5/10
12/12 - 3s - loss: 968.8413 - loglik: -9.7579e+02 - logprior: 6.9512
Epoch 6/10
12/12 - 3s - loss: 966.1411 - loglik: -9.7377e+02 - logprior: 7.6348
Epoch 7/10
12/12 - 3s - loss: 961.2058 - loglik: -9.6941e+02 - logprior: 8.2095
Epoch 8/10
12/12 - 3s - loss: 957.9443 - loglik: -9.6642e+02 - logprior: 8.4819
Epoch 9/10
12/12 - 3s - loss: 945.9577 - loglik: -9.5451e+02 - logprior: 8.5507
Epoch 10/10
12/12 - 3s - loss: 937.0791 - loglik: -9.4545e+02 - logprior: 8.3766
Fitted a model with MAP estimate = -925.4533
Time for alignment: 99.3580
Computed alignments with likelihoods: ['-926.3968', '-922.7636', '-961.1267', '-960.0730', '-925.4533']
Best model has likelihood: -922.7636  (prior= 8.3066 )
time for generating output: 0.2352
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ltn.projection.fasta
SP score = 0.9340084273481039
Training of 5 independent models on file aadh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b26bb20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe938e78190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea6f06dac0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 1242.5985 - loglik: -1.2353e+03 - logprior: -7.3001e+00
Epoch 2/10
21/21 - 6s - loss: 1119.0118 - loglik: -1.1174e+03 - logprior: -1.6544e+00
Epoch 3/10
21/21 - 6s - loss: 1083.8717 - loglik: -1.0820e+03 - logprior: -1.8675e+00
Epoch 4/10
21/21 - 7s - loss: 1076.7583 - loglik: -1.0751e+03 - logprior: -1.7001e+00
Epoch 5/10
21/21 - 6s - loss: 1070.9731 - loglik: -1.0694e+03 - logprior: -1.5639e+00
Epoch 6/10
21/21 - 7s - loss: 1071.0154 - loglik: -1.0695e+03 - logprior: -1.5485e+00
Fitted a model with MAP estimate = -1067.8623
expansions: [(16, 1), (17, 1), (19, 1), (20, 3), (21, 2), (22, 1), (36, 1), (37, 1), (39, 1), (40, 1), (50, 1), (60, 1), (63, 3), (66, 1), (76, 5), (77, 2), (78, 2), (97, 1), (100, 2), (116, 1), (119, 2), (121, 1), (122, 1), (147, 1), (148, 2), (155, 1), (161, 2), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 1064.2528 - loglik: -1.0575e+03 - logprior: -6.7907e+00
Epoch 2/2
21/21 - 8s - loss: 1040.5549 - loglik: -1.0404e+03 - logprior: -1.8536e-01
Fitted a model with MAP estimate = -1038.3858
expansions: []
discards: [ 24  79  84 131 151 216]
Re-initialized the encoder parameters.
Fitting a model of length 234 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 15s - loss: 1048.2350 - loglik: -1.0418e+03 - logprior: -6.4020e+00
Epoch 2/2
21/21 - 9s - loss: 1040.3134 - loglik: -1.0405e+03 - logprior: 0.2321
Fitted a model with MAP estimate = -1037.9523
expansions: []
discards: [180]
Re-initialized the encoder parameters.
Fitting a model of length 233 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 1048.2784 - loglik: -1.0421e+03 - logprior: -6.1538e+00
Epoch 2/10
21/21 - 8s - loss: 1038.4642 - loglik: -1.0390e+03 - logprior: 0.5564
Epoch 3/10
21/21 - 8s - loss: 1035.0100 - loglik: -1.0364e+03 - logprior: 1.4037
Epoch 4/10
21/21 - 8s - loss: 1037.0615 - loglik: -1.0388e+03 - logprior: 1.7749
Fitted a model with MAP estimate = -1033.4541
Time for alignment: 148.7575
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 10s - loss: 1243.4412 - loglik: -1.2362e+03 - logprior: -7.2783e+00
Epoch 2/10
21/21 - 6s - loss: 1115.3792 - loglik: -1.1136e+03 - logprior: -1.8035e+00
Epoch 3/10
21/21 - 6s - loss: 1078.5869 - loglik: -1.0763e+03 - logprior: -2.3288e+00
Epoch 4/10
21/21 - 6s - loss: 1072.9720 - loglik: -1.0709e+03 - logprior: -2.0289e+00
Epoch 5/10
21/21 - 6s - loss: 1071.7290 - loglik: -1.0698e+03 - logprior: -1.9463e+00
Epoch 6/10
21/21 - 6s - loss: 1068.1309 - loglik: -1.0662e+03 - logprior: -1.9664e+00
Epoch 7/10
21/21 - 6s - loss: 1065.8297 - loglik: -1.0638e+03 - logprior: -2.0021e+00
Epoch 8/10
21/21 - 6s - loss: 1064.7976 - loglik: -1.0628e+03 - logprior: -2.0414e+00
Epoch 9/10
21/21 - 6s - loss: 1064.4564 - loglik: -1.0624e+03 - logprior: -2.0649e+00
Epoch 10/10
21/21 - 7s - loss: 1061.2219 - loglik: -1.0591e+03 - logprior: -2.0961e+00
Fitted a model with MAP estimate = -1062.2081
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (36, 1), (37, 1), (39, 1), (40, 2), (50, 1), (60, 1), (63, 3), (66, 1), (75, 9), (77, 1), (96, 1), (99, 2), (118, 1), (119, 1), (120, 1), (121, 1), (146, 1), (147, 2), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 1), (181, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1077.3330 - loglik: -1.0676e+03 - logprior: -9.7752e+00
Epoch 2/2
21/21 - 9s - loss: 1046.9535 - loglik: -1.0438e+03 - logprior: -3.1188e+00
Fitted a model with MAP estimate = -1042.8831
expansions: [(0, 5)]
discards: [  0  18  28  53 133 217]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 1050.6154 - loglik: -1.0442e+03 - logprior: -6.4626e+00
Epoch 2/2
21/21 - 9s - loss: 1039.0421 - loglik: -1.0391e+03 - logprior: 0.0722
Fitted a model with MAP estimate = -1036.9720
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 1046.8268 - loglik: -1.0409e+03 - logprior: -5.9636e+00
Epoch 2/10
21/21 - 8s - loss: 1039.3932 - loglik: -1.0401e+03 - logprior: 0.7159
Epoch 3/10
21/21 - 9s - loss: 1035.4916 - loglik: -1.0371e+03 - logprior: 1.5885
Epoch 4/10
21/21 - 8s - loss: 1035.3838 - loglik: -1.0374e+03 - logprior: 1.9681
Epoch 5/10
21/21 - 9s - loss: 1031.5703 - loglik: -1.0337e+03 - logprior: 2.1691
Epoch 6/10
21/21 - 8s - loss: 1031.5065 - loglik: -1.0338e+03 - logprior: 2.3386
Epoch 7/10
21/21 - 9s - loss: 1029.5120 - loglik: -1.0320e+03 - logprior: 2.4713
Epoch 8/10
21/21 - 9s - loss: 1025.6143 - loglik: -1.0281e+03 - logprior: 2.5281
Epoch 9/10
21/21 - 9s - loss: 1024.7484 - loglik: -1.0272e+03 - logprior: 2.4402
Epoch 10/10
21/21 - 9s - loss: 1024.6979 - loglik: -1.0273e+03 - logprior: 2.5893
Fitted a model with MAP estimate = -1022.0436
Time for alignment: 228.3752
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 9s - loss: 1243.0177 - loglik: -1.2357e+03 - logprior: -7.2833e+00
Epoch 2/10
21/21 - 6s - loss: 1120.7244 - loglik: -1.1189e+03 - logprior: -1.7971e+00
Epoch 3/10
21/21 - 6s - loss: 1078.2395 - loglik: -1.0759e+03 - logprior: -2.2942e+00
Epoch 4/10
21/21 - 6s - loss: 1068.3910 - loglik: -1.0663e+03 - logprior: -2.0983e+00
Epoch 5/10
21/21 - 6s - loss: 1064.7709 - loglik: -1.0627e+03 - logprior: -2.0550e+00
Epoch 6/10
21/21 - 7s - loss: 1062.7998 - loglik: -1.0607e+03 - logprior: -2.0781e+00
Epoch 7/10
21/21 - 6s - loss: 1061.5299 - loglik: -1.0595e+03 - logprior: -2.0668e+00
Epoch 8/10
21/21 - 7s - loss: 1061.3064 - loglik: -1.0592e+03 - logprior: -2.0815e+00
Epoch 9/10
21/21 - 6s - loss: 1058.7632 - loglik: -1.0567e+03 - logprior: -2.1062e+00
Epoch 10/10
21/21 - 6s - loss: 1057.8544 - loglik: -1.0557e+03 - logprior: -2.1189e+00
Fitted a model with MAP estimate = -1057.7442
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (62, 1), (63, 3), (75, 8), (97, 1), (100, 2), (119, 1), (120, 1), (121, 1), (122, 1), (124, 1), (143, 1), (146, 1), (147, 2), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (172, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 13s - loss: 1066.8209 - loglik: -1.0570e+03 - logprior: -9.7730e+00
Epoch 2/2
21/21 - 8s - loss: 1037.5389 - loglik: -1.0345e+03 - logprior: -3.0796e+00
Fitted a model with MAP estimate = -1033.6770
expansions: [(0, 5), (97, 1), (226, 1)]
discards: [  0  18  28  83 131 216]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 1042.0457 - loglik: -1.0357e+03 - logprior: -6.3126e+00
Epoch 2/2
21/21 - 8s - loss: 1026.8920 - loglik: -1.0271e+03 - logprior: 0.2305
Fitted a model with MAP estimate = -1026.7071
expansions: []
discards: [1 2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 236 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 1035.9625 - loglik: -1.0301e+03 - logprior: -5.8133e+00
Epoch 2/10
21/21 - 9s - loss: 1028.9924 - loglik: -1.0298e+03 - logprior: 0.8512
Epoch 3/10
21/21 - 8s - loss: 1024.9224 - loglik: -1.0266e+03 - logprior: 1.7096
Epoch 4/10
21/21 - 9s - loss: 1025.5576 - loglik: -1.0277e+03 - logprior: 2.0976
Fitted a model with MAP estimate = -1023.2788
Time for alignment: 174.9848
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 11s - loss: 1241.7874 - loglik: -1.2345e+03 - logprior: -7.2731e+00
Epoch 2/10
21/21 - 6s - loss: 1121.2565 - loglik: -1.1195e+03 - logprior: -1.7306e+00
Epoch 3/10
21/21 - 6s - loss: 1077.6010 - loglik: -1.0753e+03 - logprior: -2.3114e+00
Epoch 4/10
21/21 - 6s - loss: 1071.9534 - loglik: -1.0698e+03 - logprior: -2.1191e+00
Epoch 5/10
21/21 - 6s - loss: 1066.6321 - loglik: -1.0646e+03 - logprior: -2.0422e+00
Epoch 6/10
21/21 - 6s - loss: 1066.5381 - loglik: -1.0645e+03 - logprior: -2.0647e+00
Epoch 7/10
21/21 - 6s - loss: 1064.4437 - loglik: -1.0623e+03 - logprior: -2.1349e+00
Epoch 8/10
21/21 - 6s - loss: 1060.7417 - loglik: -1.0585e+03 - logprior: -2.1996e+00
Epoch 9/10
21/21 - 7s - loss: 1060.6730 - loglik: -1.0585e+03 - logprior: -2.2081e+00
Epoch 10/10
21/21 - 6s - loss: 1059.6378 - loglik: -1.0574e+03 - logprior: -2.2428e+00
Fitted a model with MAP estimate = -1059.0435
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (37, 1), (39, 3), (40, 1), (50, 1), (62, 1), (63, 4), (70, 1), (75, 5), (76, 1), (78, 2), (80, 1), (96, 1), (99, 2), (118, 1), (119, 1), (120, 1), (121, 1), (143, 1), (146, 1), (147, 2), (158, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (177, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 1071.1572 - loglik: -1.0614e+03 - logprior: -9.7523e+00
Epoch 2/2
21/21 - 9s - loss: 1043.5151 - loglik: -1.0405e+03 - logprior: -3.0299e+00
Fitted a model with MAP estimate = -1037.1977
expansions: [(0, 5)]
discards: [  0  18  28 133 218]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 1043.2778 - loglik: -1.0369e+03 - logprior: -6.3932e+00
Epoch 2/2
21/21 - 9s - loss: 1034.8428 - loglik: -1.0350e+03 - logprior: 0.1190
Fitted a model with MAP estimate = -1030.5632
expansions: []
discards: [ 1  2  3  4 82]
Re-initialized the encoder parameters.
Fitting a model of length 237 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 12s - loss: 1042.1208 - loglik: -1.0362e+03 - logprior: -5.9280e+00
Epoch 2/10
21/21 - 8s - loss: 1029.3225 - loglik: -1.0301e+03 - logprior: 0.7719
Epoch 3/10
21/21 - 9s - loss: 1032.7302 - loglik: -1.0344e+03 - logprior: 1.6383
Fitted a model with MAP estimate = -1028.8644
Time for alignment: 166.3441
Fitting a model of length 187 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 10s - loss: 1242.1619 - loglik: -1.2349e+03 - logprior: -7.2788e+00
Epoch 2/10
21/21 - 6s - loss: 1114.7689 - loglik: -1.1130e+03 - logprior: -1.8094e+00
Epoch 3/10
21/21 - 7s - loss: 1075.2981 - loglik: -1.0728e+03 - logprior: -2.4903e+00
Epoch 4/10
21/21 - 6s - loss: 1070.2795 - loglik: -1.0680e+03 - logprior: -2.2669e+00
Epoch 5/10
21/21 - 6s - loss: 1066.0330 - loglik: -1.0638e+03 - logprior: -2.2496e+00
Epoch 6/10
21/21 - 7s - loss: 1064.8711 - loglik: -1.0626e+03 - logprior: -2.2852e+00
Epoch 7/10
21/21 - 6s - loss: 1064.2654 - loglik: -1.0620e+03 - logprior: -2.2918e+00
Epoch 8/10
21/21 - 7s - loss: 1061.9119 - loglik: -1.0596e+03 - logprior: -2.3204e+00
Epoch 9/10
21/21 - 6s - loss: 1059.2897 - loglik: -1.0569e+03 - logprior: -2.3428e+00
Epoch 10/10
21/21 - 7s - loss: 1062.9388 - loglik: -1.0606e+03 - logprior: -2.3668e+00
Fitted a model with MAP estimate = -1059.7194
expansions: [(14, 1), (17, 2), (18, 1), (19, 1), (20, 2), (21, 3), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (40, 1), (50, 1), (60, 1), (63, 4), (65, 1), (74, 1), (75, 4), (76, 1), (78, 2), (80, 1), (96, 1), (99, 2), (118, 1), (119, 1), (120, 1), (121, 1), (146, 1), (147, 2), (154, 1), (160, 1), (163, 2), (164, 1), (165, 1), (167, 1), (169, 2), (170, 1), (171, 1), (178, 1), (179, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 12s - loss: 1069.2759 - loglik: -1.0596e+03 - logprior: -9.7086e+00
Epoch 2/2
21/21 - 9s - loss: 1044.3800 - loglik: -1.0414e+03 - logprior: -2.9454e+00
Fitted a model with MAP estimate = -1037.6201
expansions: [(0, 5)]
discards: [  0  18  28  82 133 217]
Re-initialized the encoder parameters.
Fitting a model of length 240 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
21/21 - 11s - loss: 1046.0721 - loglik: -1.0397e+03 - logprior: -6.3683e+00
Epoch 2/2
21/21 - 9s - loss: 1030.2860 - loglik: -1.0304e+03 - logprior: 0.1332
Fitted a model with MAP estimate = -1031.5891
expansions: []
discards: [ 1  2  3  4 87]
Re-initialized the encoder parameters.
Fitting a model of length 235 on 3127 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 13s - loss: 1043.1617 - loglik: -1.0372e+03 - logprior: -5.9287e+00
Epoch 2/10
21/21 - 9s - loss: 1031.0055 - loglik: -1.0317e+03 - logprior: 0.7325
Epoch 3/10
21/21 - 8s - loss: 1030.4170 - loglik: -1.0320e+03 - logprior: 1.5930
Epoch 4/10
21/21 - 9s - loss: 1030.2614 - loglik: -1.0323e+03 - logprior: 1.9901
Epoch 5/10
21/21 - 9s - loss: 1027.3928 - loglik: -1.0296e+03 - logprior: 2.1864
Epoch 6/10
21/21 - 9s - loss: 1025.2042 - loglik: -1.0276e+03 - logprior: 2.3485
Epoch 7/10
21/21 - 8s - loss: 1025.5532 - loglik: -1.0280e+03 - logprior: 2.4945
Fitted a model with MAP estimate = -1022.5397
Time for alignment: 201.0256
Computed alignments with likelihoods: ['-1033.4541', '-1022.0436', '-1023.2788', '-1028.8644', '-1022.5397']
Best model has likelihood: -1022.0436  (prior= 2.6501 )
time for generating output: 0.3673
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/aadh.projection.fasta
SP score = 0.2943890039540576
Training of 5 independent models on file gluts.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea6f6c4e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea5e049250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8c91e8130>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 569.0213 - loglik: -5.6601e+02 - logprior: -3.0076e+00
Epoch 2/10
19/19 - 2s - loss: 537.8594 - loglik: -5.3699e+02 - logprior: -8.6446e-01
Epoch 3/10
19/19 - 2s - loss: 529.3226 - loglik: -5.2847e+02 - logprior: -8.5043e-01
Epoch 4/10
19/19 - 2s - loss: 528.0732 - loglik: -5.2730e+02 - logprior: -7.7324e-01
Epoch 5/10
19/19 - 2s - loss: 525.1367 - loglik: -5.2438e+02 - logprior: -7.5696e-01
Epoch 6/10
19/19 - 2s - loss: 523.6550 - loglik: -5.2291e+02 - logprior: -7.4299e-01
Epoch 7/10
19/19 - 2s - loss: 522.9901 - loglik: -5.2227e+02 - logprior: -7.1311e-01
Epoch 8/10
19/19 - 2s - loss: 520.3517 - loglik: -5.1963e+02 - logprior: -7.1166e-01
Epoch 9/10
19/19 - 2s - loss: 518.5941 - loglik: -5.1787e+02 - logprior: -7.0646e-01
Epoch 10/10
19/19 - 2s - loss: 508.9753 - loglik: -5.0822e+02 - logprior: -7.2986e-01
Fitted a model with MAP estimate = -499.8041
expansions: [(0, 9), (13, 3), (16, 2), (20, 1), (22, 1), (57, 3), (58, 1), (59, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 550.1496 - loglik: -5.4617e+02 - logprior: -3.9759e+00
Epoch 2/2
19/19 - 3s - loss: 528.9431 - loglik: -5.2772e+02 - logprior: -1.2264e+00
Fitted a model with MAP estimate = -523.3075
expansions: [(0, 11), (23, 2)]
discards: [ 7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 28 75 79]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 529.6869 - loglik: -5.2593e+02 - logprior: -3.7558e+00
Epoch 2/2
19/19 - 3s - loss: 525.7921 - loglik: -5.2459e+02 - logprior: -1.2066e+00
Fitted a model with MAP estimate = -521.9984
expansions: [(0, 7), (17, 2)]
discards: [ 1  2  3  4  5  6  7  8  9 10]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 8s - loss: 526.4383 - loglik: -5.2281e+02 - logprior: -3.6306e+00
Epoch 2/10
19/19 - 3s - loss: 522.6792 - loglik: -5.2138e+02 - logprior: -1.2943e+00
Epoch 3/10
19/19 - 3s - loss: 522.6234 - loglik: -5.2160e+02 - logprior: -1.0229e+00
Epoch 4/10
19/19 - 3s - loss: 520.2584 - loglik: -5.1943e+02 - logprior: -8.3138e-01
Epoch 5/10
19/19 - 3s - loss: 519.1384 - loglik: -5.1840e+02 - logprior: -7.3114e-01
Epoch 6/10
19/19 - 3s - loss: 516.9783 - loglik: -5.1628e+02 - logprior: -6.9554e-01
Epoch 7/10
19/19 - 2s - loss: 515.7835 - loglik: -5.1511e+02 - logprior: -6.6427e-01
Epoch 8/10
19/19 - 3s - loss: 515.5468 - loglik: -5.1489e+02 - logprior: -6.4403e-01
Epoch 9/10
19/19 - 2s - loss: 512.7369 - loglik: -5.1206e+02 - logprior: -6.5696e-01
Epoch 10/10
19/19 - 3s - loss: 504.0052 - loglik: -5.0332e+02 - logprior: -6.6185e-01
Fitted a model with MAP estimate = -495.4183
Time for alignment: 87.6422
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 568.6307 - loglik: -5.6563e+02 - logprior: -3.0050e+00
Epoch 2/10
19/19 - 2s - loss: 539.2112 - loglik: -5.3833e+02 - logprior: -8.7952e-01
Epoch 3/10
19/19 - 2s - loss: 529.1821 - loglik: -5.2830e+02 - logprior: -8.8375e-01
Epoch 4/10
19/19 - 2s - loss: 527.5527 - loglik: -5.2676e+02 - logprior: -7.8876e-01
Epoch 5/10
19/19 - 2s - loss: 525.7095 - loglik: -5.2492e+02 - logprior: -7.8099e-01
Epoch 6/10
19/19 - 2s - loss: 523.8530 - loglik: -5.2309e+02 - logprior: -7.5550e-01
Epoch 7/10
19/19 - 2s - loss: 522.4977 - loglik: -5.2174e+02 - logprior: -7.5211e-01
Epoch 8/10
19/19 - 2s - loss: 520.7494 - loglik: -5.1999e+02 - logprior: -7.4278e-01
Epoch 9/10
19/19 - 2s - loss: 518.2857 - loglik: -5.1753e+02 - logprior: -7.3938e-01
Epoch 10/10
19/19 - 2s - loss: 507.9455 - loglik: -5.0715e+02 - logprior: -7.6703e-01
Fitted a model with MAP estimate = -498.3328
expansions: [(0, 8), (11, 2), (13, 2), (20, 1), (30, 2), (54, 1), (57, 2), (61, 2), (62, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 553.8698 - loglik: -5.4982e+02 - logprior: -4.0456e+00
Epoch 2/2
19/19 - 3s - loss: 530.2385 - loglik: -5.2896e+02 - logprior: -1.2736e+00
Fitted a model with MAP estimate = -524.4358
expansions: [(0, 14), (24, 1)]
discards: [ 6  7  8  9 10 11 12 13 14 15 16 17 18 21 44 73 82]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 529.9352 - loglik: -5.2614e+02 - logprior: -3.7999e+00
Epoch 2/2
19/19 - 3s - loss: 525.0452 - loglik: -5.2374e+02 - logprior: -1.3026e+00
Fitted a model with MAP estimate = -521.3628
expansions: [(0, 6), (18, 5), (20, 2), (25, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 525.5681 - loglik: -5.2228e+02 - logprior: -3.2833e+00
Epoch 2/10
19/19 - 3s - loss: 521.3035 - loglik: -5.2027e+02 - logprior: -1.0325e+00
Epoch 3/10
19/19 - 3s - loss: 520.8928 - loglik: -5.1999e+02 - logprior: -9.0159e-01
Epoch 4/10
19/19 - 3s - loss: 519.1238 - loglik: -5.1844e+02 - logprior: -6.7938e-01
Epoch 5/10
19/19 - 3s - loss: 518.0177 - loglik: -5.1739e+02 - logprior: -6.2503e-01
Epoch 6/10
19/19 - 3s - loss: 516.2403 - loglik: -5.1563e+02 - logprior: -6.0492e-01
Epoch 7/10
19/19 - 3s - loss: 515.5585 - loglik: -5.1496e+02 - logprior: -5.8534e-01
Epoch 8/10
19/19 - 3s - loss: 512.4064 - loglik: -5.1182e+02 - logprior: -5.7114e-01
Epoch 9/10
19/19 - 3s - loss: 512.1218 - loglik: -5.1155e+02 - logprior: -5.5718e-01
Epoch 10/10
19/19 - 3s - loss: 501.9486 - loglik: -5.0135e+02 - logprior: -5.7959e-01
Fitted a model with MAP estimate = -495.0512
Time for alignment: 88.4720
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 568.7411 - loglik: -5.6573e+02 - logprior: -3.0069e+00
Epoch 2/10
19/19 - 2s - loss: 538.7357 - loglik: -5.3787e+02 - logprior: -8.6726e-01
Epoch 3/10
19/19 - 2s - loss: 530.6656 - loglik: -5.2982e+02 - logprior: -8.4373e-01
Epoch 4/10
19/19 - 2s - loss: 527.8222 - loglik: -5.2709e+02 - logprior: -7.3260e-01
Epoch 5/10
19/19 - 2s - loss: 526.3818 - loglik: -5.2568e+02 - logprior: -6.9406e-01
Epoch 6/10
19/19 - 2s - loss: 525.5361 - loglik: -5.2486e+02 - logprior: -6.6912e-01
Epoch 7/10
19/19 - 2s - loss: 522.4774 - loglik: -5.2181e+02 - logprior: -6.5492e-01
Epoch 8/10
19/19 - 2s - loss: 520.8215 - loglik: -5.2015e+02 - logprior: -6.5498e-01
Epoch 9/10
19/19 - 2s - loss: 518.4816 - loglik: -5.1779e+02 - logprior: -6.7067e-01
Epoch 10/10
19/19 - 2s - loss: 507.7560 - loglik: -5.0702e+02 - logprior: -7.0687e-01
Fitted a model with MAP estimate = -499.0439
expansions: [(0, 8), (10, 2), (14, 2), (15, 3), (16, 1), (35, 1), (57, 2), (59, 2), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 550.8595 - loglik: -5.4683e+02 - logprior: -4.0310e+00
Epoch 2/2
19/19 - 3s - loss: 528.5687 - loglik: -5.2739e+02 - logprior: -1.1735e+00
Fitted a model with MAP estimate = -522.9778
expansions: [(0, 12), (25, 1)]
discards: [ 7  8  9 10 11 12 13 14 15 20 26 75 79 80]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 528.6708 - loglik: -5.2490e+02 - logprior: -3.7755e+00
Epoch 2/2
19/19 - 3s - loss: 523.5007 - loglik: -5.2229e+02 - logprior: -1.2102e+00
Fitted a model with MAP estimate = -520.4320
expansions: [(0, 7), (19, 1)]
discards: [ 1  2  3  4  5  6  7  8  9 10 11]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 525.3621 - loglik: -5.2179e+02 - logprior: -3.5760e+00
Epoch 2/10
19/19 - 3s - loss: 521.5507 - loglik: -5.2031e+02 - logprior: -1.2418e+00
Epoch 3/10
19/19 - 3s - loss: 520.7850 - loglik: -5.1982e+02 - logprior: -9.6505e-01
Epoch 4/10
19/19 - 3s - loss: 519.3505 - loglik: -5.1852e+02 - logprior: -8.2452e-01
Epoch 5/10
19/19 - 3s - loss: 518.1903 - loglik: -5.1748e+02 - logprior: -7.0888e-01
Epoch 6/10
19/19 - 3s - loss: 516.6870 - loglik: -5.1599e+02 - logprior: -6.9255e-01
Epoch 7/10
19/19 - 3s - loss: 515.0800 - loglik: -5.1440e+02 - logprior: -6.7369e-01
Epoch 8/10
19/19 - 3s - loss: 513.7850 - loglik: -5.1311e+02 - logprior: -6.6655e-01
Epoch 9/10
19/19 - 3s - loss: 512.3242 - loglik: -5.1166e+02 - logprior: -6.5156e-01
Epoch 10/10
19/19 - 3s - loss: 503.5066 - loglik: -5.0282e+02 - logprior: -6.6064e-01
Fitted a model with MAP estimate = -495.6733
Time for alignment: 88.0411
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 568.8102 - loglik: -5.6581e+02 - logprior: -3.0036e+00
Epoch 2/10
19/19 - 2s - loss: 537.7189 - loglik: -5.3686e+02 - logprior: -8.6165e-01
Epoch 3/10
19/19 - 2s - loss: 528.9968 - loglik: -5.2814e+02 - logprior: -8.5722e-01
Epoch 4/10
19/19 - 2s - loss: 526.7228 - loglik: -5.2597e+02 - logprior: -7.5183e-01
Epoch 5/10
19/19 - 2s - loss: 525.7437 - loglik: -5.2500e+02 - logprior: -7.3537e-01
Epoch 6/10
19/19 - 2s - loss: 523.6541 - loglik: -5.2292e+02 - logprior: -7.2677e-01
Epoch 7/10
19/19 - 2s - loss: 521.5853 - loglik: -5.2085e+02 - logprior: -7.2271e-01
Epoch 8/10
19/19 - 2s - loss: 521.4077 - loglik: -5.2066e+02 - logprior: -7.2925e-01
Epoch 9/10
19/19 - 2s - loss: 516.8298 - loglik: -5.1607e+02 - logprior: -7.4531e-01
Epoch 10/10
19/19 - 2s - loss: 508.4485 - loglik: -5.0766e+02 - logprior: -7.6673e-01
Fitted a model with MAP estimate = -500.0497
expansions: [(0, 8), (11, 1), (13, 2), (20, 1), (21, 1), (55, 1), (57, 2), (58, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 548.6841 - loglik: -5.4451e+02 - logprior: -4.1743e+00
Epoch 2/2
19/19 - 3s - loss: 528.4575 - loglik: -5.2730e+02 - logprior: -1.1561e+00
Fitted a model with MAP estimate = -522.9782
expansions: [(0, 10)]
discards: [ 7  8  9 10 11 12 13 14 15 16]
Re-initialized the encoder parameters.
Fitting a model of length 96 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 528.7170 - loglik: -5.2501e+02 - logprior: -3.7118e+00
Epoch 2/2
19/19 - 3s - loss: 524.0760 - loglik: -5.2291e+02 - logprior: -1.1688e+00
Fitted a model with MAP estimate = -520.9061
expansions: [(0, 6), (14, 4), (20, 1)]
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 525.8318 - loglik: -5.2221e+02 - logprior: -3.6253e+00
Epoch 2/10
19/19 - 3s - loss: 521.5627 - loglik: -5.2030e+02 - logprior: -1.2650e+00
Epoch 3/10
19/19 - 3s - loss: 520.2417 - loglik: -5.1923e+02 - logprior: -1.0136e+00
Epoch 4/10
19/19 - 3s - loss: 519.6608 - loglik: -5.1870e+02 - logprior: -9.5586e-01
Epoch 5/10
19/19 - 3s - loss: 516.9239 - loglik: -5.1617e+02 - logprior: -7.5507e-01
Epoch 6/10
19/19 - 3s - loss: 516.6732 - loglik: -5.1601e+02 - logprior: -6.5688e-01
Epoch 7/10
19/19 - 3s - loss: 513.6586 - loglik: -5.1301e+02 - logprior: -6.3745e-01
Epoch 8/10
19/19 - 3s - loss: 514.1793 - loglik: -5.1356e+02 - logprior: -6.1168e-01
Fitted a model with MAP estimate = -512.1835
Time for alignment: 82.5282
Fitting a model of length 77 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 568.6212 - loglik: -5.6562e+02 - logprior: -3.0058e+00
Epoch 2/10
19/19 - 2s - loss: 539.0214 - loglik: -5.3816e+02 - logprior: -8.6289e-01
Epoch 3/10
19/19 - 2s - loss: 529.8359 - loglik: -5.2898e+02 - logprior: -8.5451e-01
Epoch 4/10
19/19 - 2s - loss: 527.0643 - loglik: -5.2631e+02 - logprior: -7.5342e-01
Epoch 5/10
19/19 - 2s - loss: 525.3524 - loglik: -5.2462e+02 - logprior: -7.2336e-01
Epoch 6/10
19/19 - 2s - loss: 522.6453 - loglik: -5.2194e+02 - logprior: -7.0212e-01
Epoch 7/10
19/19 - 2s - loss: 523.3478 - loglik: -5.2266e+02 - logprior: -6.7923e-01
Fitted a model with MAP estimate = -519.2795
expansions: [(0, 9), (16, 2), (20, 1), (22, 1), (57, 3), (58, 2), (59, 3), (61, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 100 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 532.5240 - loglik: -5.2885e+02 - logprior: -3.6787e+00
Epoch 2/2
19/19 - 3s - loss: 524.3853 - loglik: -5.2335e+02 - logprior: -1.0395e+00
Fitted a model with MAP estimate = -521.4414
expansions: [(0, 8)]
discards: [ 0  1  2  3  4  5  6 73 74 75 77 78 79]
Re-initialized the encoder parameters.
Fitting a model of length 95 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 528.3099 - loglik: -5.2525e+02 - logprior: -3.0583e+00
Epoch 2/2
19/19 - 3s - loss: 524.1374 - loglik: -5.2326e+02 - logprior: -8.7507e-01
Fitted a model with MAP estimate = -521.3368
expansions: [(0, 8)]
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 94 on 10099 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 6s - loss: 526.3885 - loglik: -5.2293e+02 - logprior: -3.4574e+00
Epoch 2/10
19/19 - 3s - loss: 522.9738 - loglik: -5.2194e+02 - logprior: -1.0327e+00
Epoch 3/10
19/19 - 3s - loss: 521.5795 - loglik: -5.2070e+02 - logprior: -8.7424e-01
Epoch 4/10
19/19 - 2s - loss: 519.9239 - loglik: -5.1920e+02 - logprior: -7.1697e-01
Epoch 5/10
19/19 - 3s - loss: 518.8896 - loglik: -5.1823e+02 - logprior: -6.6001e-01
Epoch 6/10
19/19 - 3s - loss: 517.8237 - loglik: -5.1720e+02 - logprior: -6.1507e-01
Epoch 7/10
19/19 - 2s - loss: 515.3117 - loglik: -5.1468e+02 - logprior: -6.2292e-01
Epoch 8/10
19/19 - 3s - loss: 513.4030 - loglik: -5.1276e+02 - logprior: -6.2686e-01
Epoch 9/10
19/19 - 3s - loss: 511.7284 - loglik: -5.1108e+02 - logprior: -6.3079e-01
Epoch 10/10
19/19 - 2s - loss: 503.7723 - loglik: -5.0311e+02 - logprior: -6.4287e-01
Fitted a model with MAP estimate = -495.2510
Time for alignment: 79.9511
Computed alignments with likelihoods: ['-495.4183', '-495.0512', '-495.6733', '-500.0497', '-495.2510']
Best model has likelihood: -495.0512  (prior= -0.6112 )
time for generating output: 0.2005
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/gluts.projection.fasta
SP score = 0.3867894736842105
Training of 5 independent models on file tms.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea091f1370>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9414df880>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea11d66520>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 1504.1274 - loglik: -1.4922e+03 - logprior: -1.1892e+01
Epoch 2/10
17/17 - 6s - loss: 1322.2192 - loglik: -1.3204e+03 - logprior: -1.7970e+00
Epoch 3/10
17/17 - 7s - loss: 1230.4681 - loglik: -1.2272e+03 - logprior: -3.2556e+00
Epoch 4/10
17/17 - 6s - loss: 1216.5264 - loglik: -1.2132e+03 - logprior: -3.3382e+00
Epoch 5/10
17/17 - 7s - loss: 1205.5396 - loglik: -1.2026e+03 - logprior: -2.9521e+00
Epoch 6/10
17/17 - 7s - loss: 1204.6196 - loglik: -1.2018e+03 - logprior: -2.8220e+00
Epoch 7/10
17/17 - 6s - loss: 1201.1602 - loglik: -1.1982e+03 - logprior: -2.9307e+00
Epoch 8/10
17/17 - 7s - loss: 1202.1865 - loglik: -1.1992e+03 - logprior: -2.9908e+00
Fitted a model with MAP estimate = -1199.2416
expansions: [(8, 1), (9, 1), (11, 1), (13, 3), (22, 1), (23, 1), (32, 1), (42, 1), (45, 1), (46, 1), (53, 1), (55, 1), (57, 1), (59, 1), (60, 1), (62, 1), (90, 1), (95, 2), (97, 1), (100, 1), (114, 1), (115, 1), (116, 1), (128, 1), (129, 2), (131, 2), (140, 1), (141, 1), (155, 1), (160, 1), (161, 1), (162, 1), (179, 1), (181, 2), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1201.8785 - loglik: -1.1863e+03 - logprior: -1.5575e+01
Epoch 2/2
17/17 - 9s - loss: 1166.7153 - loglik: -1.1623e+03 - logprior: -4.4448e+00
Fitted a model with MAP estimate = -1161.8584
expansions: [(0, 8)]
discards: [  0  16 155 159]
Re-initialized the encoder parameters.
Fitting a model of length 271 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1170.4702 - loglik: -1.1598e+03 - logprior: -1.0675e+01
Epoch 2/2
17/17 - 9s - loss: 1156.1013 - loglik: -1.1567e+03 - logprior: 0.5667
Fitted a model with MAP estimate = -1153.8945
expansions: []
discards: [1 2 3 4 5 6 7]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 15s - loss: 1168.6936 - loglik: -1.1588e+03 - logprior: -9.8511e+00
Epoch 2/10
17/17 - 9s - loss: 1155.8491 - loglik: -1.1572e+03 - logprior: 1.3093
Epoch 3/10
17/17 - 8s - loss: 1151.0010 - loglik: -1.1540e+03 - logprior: 2.9936
Epoch 4/10
17/17 - 9s - loss: 1153.7544 - loglik: -1.1576e+03 - logprior: 3.8070
Fitted a model with MAP estimate = -1150.8368
Time for alignment: 161.3655
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 1504.9021 - loglik: -1.4930e+03 - logprior: -1.1877e+01
Epoch 2/10
17/17 - 7s - loss: 1320.3899 - loglik: -1.3186e+03 - logprior: -1.7732e+00
Epoch 3/10
17/17 - 7s - loss: 1230.9042 - loglik: -1.2278e+03 - logprior: -3.1006e+00
Epoch 4/10
17/17 - 7s - loss: 1209.0674 - loglik: -1.2057e+03 - logprior: -3.3394e+00
Epoch 5/10
17/17 - 6s - loss: 1207.0599 - loglik: -1.2041e+03 - logprior: -2.9340e+00
Epoch 6/10
17/17 - 7s - loss: 1204.0660 - loglik: -1.2013e+03 - logprior: -2.7438e+00
Epoch 7/10
17/17 - 6s - loss: 1201.7903 - loglik: -1.1990e+03 - logprior: -2.7842e+00
Epoch 8/10
17/17 - 6s - loss: 1200.7177 - loglik: -1.1979e+03 - logprior: -2.8527e+00
Epoch 9/10
17/17 - 6s - loss: 1198.5127 - loglik: -1.1956e+03 - logprior: -2.8890e+00
Epoch 10/10
17/17 - 7s - loss: 1202.2402 - loglik: -1.1993e+03 - logprior: -2.9371e+00
Fitted a model with MAP estimate = -1198.5292
expansions: [(8, 1), (9, 1), (11, 1), (13, 3), (24, 1), (26, 1), (32, 1), (42, 1), (45, 1), (46, 1), (55, 1), (58, 1), (60, 1), (61, 1), (62, 2), (86, 1), (95, 2), (96, 1), (97, 1), (109, 1), (113, 1), (114, 2), (115, 1), (129, 2), (131, 2), (140, 1), (141, 1), (155, 1), (160, 1), (161, 1), (162, 1), (176, 1), (180, 1), (181, 1), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1203.4641 - loglik: -1.1878e+03 - logprior: -1.5707e+01
Epoch 2/2
17/17 - 9s - loss: 1167.1761 - loglik: -1.1626e+03 - logprior: -4.6035e+00
Fitted a model with MAP estimate = -1162.4549
expansions: [(0, 9)]
discards: [  0  16 138 156 160]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1172.6494 - loglik: -1.1620e+03 - logprior: -1.0662e+01
Epoch 2/2
17/17 - 9s - loss: 1155.5729 - loglik: -1.1561e+03 - logprior: 0.5655
Fitted a model with MAP estimate = -1153.9840
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1168.4050 - loglik: -1.1585e+03 - logprior: -9.8821e+00
Epoch 2/10
17/17 - 9s - loss: 1157.0310 - loglik: -1.1583e+03 - logprior: 1.2867
Epoch 3/10
17/17 - 8s - loss: 1150.4269 - loglik: -1.1535e+03 - logprior: 3.0505
Epoch 4/10
17/17 - 9s - loss: 1152.6501 - loglik: -1.1565e+03 - logprior: 3.8461
Fitted a model with MAP estimate = -1150.9532
Time for alignment: 173.9479
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 10s - loss: 1503.5438 - loglik: -1.4917e+03 - logprior: -1.1884e+01
Epoch 2/10
17/17 - 6s - loss: 1322.2712 - loglik: -1.3207e+03 - logprior: -1.5786e+00
Epoch 3/10
17/17 - 7s - loss: 1229.7152 - loglik: -1.2270e+03 - logprior: -2.7028e+00
Epoch 4/10
17/17 - 7s - loss: 1211.1199 - loglik: -1.2081e+03 - logprior: -3.0103e+00
Epoch 5/10
17/17 - 7s - loss: 1208.9288 - loglik: -1.2062e+03 - logprior: -2.7136e+00
Epoch 6/10
17/17 - 7s - loss: 1203.6761 - loglik: -1.2012e+03 - logprior: -2.4975e+00
Epoch 7/10
17/17 - 6s - loss: 1200.1215 - loglik: -1.1976e+03 - logprior: -2.5257e+00
Epoch 8/10
17/17 - 7s - loss: 1201.7831 - loglik: -1.1992e+03 - logprior: -2.5962e+00
Fitted a model with MAP estimate = -1199.1837
expansions: [(8, 3), (10, 1), (13, 1), (14, 1), (23, 1), (32, 1), (33, 2), (45, 1), (46, 1), (55, 1), (58, 1), (60, 1), (61, 1), (63, 1), (87, 3), (95, 2), (96, 1), (97, 1), (114, 2), (115, 3), (122, 1), (131, 2), (140, 1), (141, 1), (156, 1), (160, 1), (163, 1), (177, 1), (180, 1), (181, 1), (182, 1), (184, 1), (188, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 268 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 13s - loss: 1202.1235 - loglik: -1.1865e+03 - logprior: -1.5655e+01
Epoch 2/2
17/17 - 9s - loss: 1165.4824 - loglik: -1.1610e+03 - logprior: -4.4841e+00
Fitted a model with MAP estimate = -1162.0366
expansions: [(0, 11)]
discards: [  0  40 104 137 160]
Re-initialized the encoder parameters.
Fitting a model of length 274 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1172.0518 - loglik: -1.1612e+03 - logprior: -1.0839e+01
Epoch 2/2
17/17 - 9s - loss: 1155.3175 - loglik: -1.1557e+03 - logprior: 0.4137
Fitted a model with MAP estimate = -1153.4829
expansions: []
discards: [ 1  2  3  4  5  6  7  8  9 10]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 12s - loss: 1169.4258 - loglik: -1.1595e+03 - logprior: -9.9650e+00
Epoch 2/10
17/17 - 9s - loss: 1154.9155 - loglik: -1.1561e+03 - logprior: 1.2062
Epoch 3/10
17/17 - 9s - loss: 1152.4131 - loglik: -1.1554e+03 - logprior: 3.0211
Epoch 4/10
17/17 - 8s - loss: 1152.7032 - loglik: -1.1565e+03 - logprior: 3.8200
Fitted a model with MAP estimate = -1150.6856
Time for alignment: 161.8665
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1503.6904 - loglik: -1.4918e+03 - logprior: -1.1880e+01
Epoch 2/10
17/17 - 7s - loss: 1325.1672 - loglik: -1.3234e+03 - logprior: -1.7719e+00
Epoch 3/10
17/17 - 7s - loss: 1233.3649 - loglik: -1.2302e+03 - logprior: -3.1292e+00
Epoch 4/10
17/17 - 7s - loss: 1210.8552 - loglik: -1.2075e+03 - logprior: -3.3422e+00
Epoch 5/10
17/17 - 7s - loss: 1202.7865 - loglik: -1.1999e+03 - logprior: -2.9179e+00
Epoch 6/10
17/17 - 6s - loss: 1204.0724 - loglik: -1.2014e+03 - logprior: -2.7135e+00
Fitted a model with MAP estimate = -1201.0091
expansions: [(8, 1), (9, 1), (11, 1), (13, 3), (22, 1), (23, 1), (32, 1), (42, 1), (45, 1), (54, 1), (55, 1), (57, 1), (58, 1), (60, 1), (63, 1), (86, 1), (87, 1), (95, 2), (98, 1), (100, 1), (114, 2), (115, 3), (122, 1), (128, 2), (130, 1), (141, 1), (155, 1), (160, 1), (163, 1), (165, 1), (179, 1), (181, 2), (184, 1), (191, 1), (192, 1), (196, 1), (197, 1), (201, 1), (205, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 267 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1201.2408 - loglik: -1.1857e+03 - logprior: -1.5501e+01
Epoch 2/2
17/17 - 9s - loss: 1165.2872 - loglik: -1.1610e+03 - logprior: -4.3139e+00
Fitted a model with MAP estimate = -1161.5709
expansions: [(0, 9)]
discards: [  0  16 136 156]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1171.5791 - loglik: -1.1608e+03 - logprior: -1.0770e+01
Epoch 2/2
17/17 - 9s - loss: 1156.4966 - loglik: -1.1569e+03 - logprior: 0.4418
Fitted a model with MAP estimate = -1154.2145
expansions: []
discards: [1 2 3 4 5 6 7 8]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1168.5562 - loglik: -1.1586e+03 - logprior: -9.9711e+00
Epoch 2/10
17/17 - 9s - loss: 1156.9384 - loglik: -1.1581e+03 - logprior: 1.1297
Epoch 3/10
17/17 - 8s - loss: 1152.5757 - loglik: -1.1555e+03 - logprior: 2.9245
Epoch 4/10
17/17 - 9s - loss: 1152.5713 - loglik: -1.1563e+03 - logprior: 3.7246
Epoch 5/10
17/17 - 8s - loss: 1147.5201 - loglik: -1.1517e+03 - logprior: 4.1678
Epoch 6/10
17/17 - 9s - loss: 1150.7693 - loglik: -1.1552e+03 - logprior: 4.4489
Fitted a model with MAP estimate = -1148.0000
Time for alignment: 165.3212
Fitting a model of length 219 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1504.0552 - loglik: -1.4922e+03 - logprior: -1.1867e+01
Epoch 2/10
17/17 - 6s - loss: 1325.1415 - loglik: -1.3234e+03 - logprior: -1.7105e+00
Epoch 3/10
17/17 - 7s - loss: 1235.3966 - loglik: -1.2326e+03 - logprior: -2.8277e+00
Epoch 4/10
17/17 - 6s - loss: 1217.2277 - loglik: -1.2141e+03 - logprior: -3.0782e+00
Epoch 5/10
17/17 - 7s - loss: 1208.6229 - loglik: -1.2057e+03 - logprior: -2.8800e+00
Epoch 6/10
17/17 - 6s - loss: 1206.2635 - loglik: -1.2034e+03 - logprior: -2.8423e+00
Epoch 7/10
17/17 - 7s - loss: 1203.3977 - loglik: -1.2005e+03 - logprior: -2.8743e+00
Epoch 8/10
17/17 - 7s - loss: 1203.0450 - loglik: -1.2001e+03 - logprior: -2.9246e+00
Epoch 9/10
17/17 - 7s - loss: 1203.7853 - loglik: -1.2008e+03 - logprior: -2.9879e+00
Fitted a model with MAP estimate = -1200.1457
expansions: [(8, 1), (9, 1), (11, 1), (13, 1), (15, 1), (22, 1), (23, 1), (32, 2), (45, 1), (46, 1), (53, 1), (55, 1), (57, 1), (60, 1), (63, 1), (69, 1), (86, 1), (95, 2), (96, 1), (97, 1), (114, 1), (115, 2), (116, 1), (117, 1), (129, 2), (131, 1), (140, 1), (141, 1), (156, 1), (160, 1), (162, 1), (163, 1), (179, 1), (181, 2), (184, 1), (191, 1), (192, 1), (195, 1), (196, 1), (197, 1), (200, 1), (209, 1), (212, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 266 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1202.9344 - loglik: -1.1874e+03 - logprior: -1.5566e+01
Epoch 2/2
17/17 - 9s - loss: 1166.0175 - loglik: -1.1617e+03 - logprior: -4.3420e+00
Fitted a model with MAP estimate = -1162.3315
expansions: [(0, 10)]
discards: [  0 137 155]
Re-initialized the encoder parameters.
Fitting a model of length 273 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 12s - loss: 1170.5391 - loglik: -1.1598e+03 - logprior: -1.0781e+01
Epoch 2/2
17/17 - 9s - loss: 1156.1641 - loglik: -1.1566e+03 - logprior: 0.4517
Fitted a model with MAP estimate = -1153.4963
expansions: []
discards: [1 2 3 4 5 6 7 8 9]
Re-initialized the encoder parameters.
Fitting a model of length 264 on 2118 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 11s - loss: 1167.8251 - loglik: -1.1579e+03 - logprior: -9.9031e+00
Epoch 2/10
17/17 - 9s - loss: 1157.4354 - loglik: -1.1587e+03 - logprior: 1.2470
Epoch 3/10
17/17 - 8s - loss: 1150.5129 - loglik: -1.1535e+03 - logprior: 3.0096
Epoch 4/10
17/17 - 9s - loss: 1154.9736 - loglik: -1.1588e+03 - logprior: 3.8477
Fitted a model with MAP estimate = -1150.8605
Time for alignment: 166.8811
Computed alignments with likelihoods: ['-1150.8368', '-1150.9532', '-1150.6856', '-1148.0000', '-1150.8605']
Best model has likelihood: -1148.0000  (prior= 4.5447 )
time for generating output: 0.2631
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tms.projection.fasta
SP score = 0.9494446572194561
Training of 5 independent models on file kunitz.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ab9f8760>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea11a27310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8e3c7bc10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 340.5356 - loglik: -3.2052e+02 - logprior: -2.0021e+01
Epoch 2/10
10/10 - 1s - loss: 303.1592 - loglik: -2.9743e+02 - logprior: -5.7297e+00
Epoch 3/10
10/10 - 1s - loss: 285.3135 - loglik: -2.8205e+02 - logprior: -3.2683e+00
Epoch 4/10
10/10 - 1s - loss: 274.1469 - loglik: -2.7145e+02 - logprior: -2.7015e+00
Epoch 5/10
10/10 - 1s - loss: 268.7240 - loglik: -2.6610e+02 - logprior: -2.6246e+00
Epoch 6/10
10/10 - 1s - loss: 266.7899 - loglik: -2.6425e+02 - logprior: -2.5417e+00
Epoch 7/10
10/10 - 1s - loss: 265.2336 - loglik: -2.6282e+02 - logprior: -2.4136e+00
Epoch 8/10
10/10 - 1s - loss: 264.1411 - loglik: -2.6184e+02 - logprior: -2.3037e+00
Epoch 9/10
10/10 - 1s - loss: 263.5295 - loglik: -2.6128e+02 - logprior: -2.2434e+00
Epoch 10/10
10/10 - 1s - loss: 263.0759 - loglik: -2.6084e+02 - logprior: -2.2308e+00
Fitted a model with MAP estimate = -262.9430
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 296.6064 - loglik: -2.7417e+02 - logprior: -2.2437e+01
Epoch 2/2
10/10 - 1s - loss: 271.4053 - loglik: -2.6170e+02 - logprior: -9.7093e+00
Fitted a model with MAP estimate = -267.2960
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 282.6577 - loglik: -2.6489e+02 - logprior: -1.7771e+01
Epoch 2/2
10/10 - 1s - loss: 263.5544 - loglik: -2.5869e+02 - logprior: -4.8641e+00
Fitted a model with MAP estimate = -260.9079
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 285.7431 - loglik: -2.6549e+02 - logprior: -2.0254e+01
Epoch 2/10
10/10 - 1s - loss: 265.4493 - loglik: -2.5953e+02 - logprior: -5.9172e+00
Epoch 3/10
10/10 - 1s - loss: 261.7655 - loglik: -2.5860e+02 - logprior: -3.1680e+00
Epoch 4/10
10/10 - 1s - loss: 260.1130 - loglik: -2.5806e+02 - logprior: -2.0551e+00
Epoch 5/10
10/10 - 1s - loss: 259.3964 - loglik: -2.5797e+02 - logprior: -1.4278e+00
Epoch 6/10
10/10 - 0s - loss: 258.7509 - loglik: -2.5755e+02 - logprior: -1.2017e+00
Epoch 7/10
10/10 - 1s - loss: 257.7440 - loglik: -2.5674e+02 - logprior: -1.0047e+00
Epoch 8/10
10/10 - 1s - loss: 257.3218 - loglik: -2.5639e+02 - logprior: -9.3349e-01
Epoch 9/10
10/10 - 1s - loss: 256.5542 - loglik: -2.5566e+02 - logprior: -8.9085e-01
Epoch 10/10
10/10 - 1s - loss: 256.6639 - loglik: -2.5579e+02 - logprior: -8.7099e-01
Fitted a model with MAP estimate = -256.0314
Time for alignment: 32.5682
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 340.5948 - loglik: -3.2058e+02 - logprior: -2.0019e+01
Epoch 2/10
10/10 - 1s - loss: 303.5600 - loglik: -2.9783e+02 - logprior: -5.7302e+00
Epoch 3/10
10/10 - 1s - loss: 285.2504 - loglik: -2.8196e+02 - logprior: -3.2903e+00
Epoch 4/10
10/10 - 1s - loss: 273.9789 - loglik: -2.7121e+02 - logprior: -2.7673e+00
Epoch 5/10
10/10 - 1s - loss: 268.4320 - loglik: -2.6576e+02 - logprior: -2.6761e+00
Epoch 6/10
10/10 - 1s - loss: 265.9023 - loglik: -2.6331e+02 - logprior: -2.5884e+00
Epoch 7/10
10/10 - 1s - loss: 264.8350 - loglik: -2.6236e+02 - logprior: -2.4755e+00
Epoch 8/10
10/10 - 1s - loss: 263.8193 - loglik: -2.6148e+02 - logprior: -2.3419e+00
Epoch 9/10
10/10 - 1s - loss: 263.3076 - loglik: -2.6105e+02 - logprior: -2.2599e+00
Epoch 10/10
10/10 - 1s - loss: 262.9567 - loglik: -2.6071e+02 - logprior: -2.2394e+00
Fitted a model with MAP estimate = -262.6690
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (17, 1), (18, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 296.7707 - loglik: -2.7433e+02 - logprior: -2.2437e+01
Epoch 2/2
10/10 - 1s - loss: 271.3591 - loglik: -2.6165e+02 - logprior: -9.7064e+00
Fitted a model with MAP estimate = -267.3120
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 282.5475 - loglik: -2.6478e+02 - logprior: -1.7766e+01
Epoch 2/2
10/10 - 1s - loss: 263.3234 - loglik: -2.5846e+02 - logprior: -4.8637e+00
Fitted a model with MAP estimate = -260.9194
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 285.6172 - loglik: -2.6537e+02 - logprior: -2.0251e+01
Epoch 2/10
10/10 - 1s - loss: 265.3967 - loglik: -2.5948e+02 - logprior: -5.9148e+00
Epoch 3/10
10/10 - 1s - loss: 261.7996 - loglik: -2.5863e+02 - logprior: -3.1645e+00
Epoch 4/10
10/10 - 1s - loss: 260.2328 - loglik: -2.5819e+02 - logprior: -2.0396e+00
Epoch 5/10
10/10 - 1s - loss: 259.3276 - loglik: -2.5789e+02 - logprior: -1.4323e+00
Epoch 6/10
10/10 - 1s - loss: 258.9803 - loglik: -2.5777e+02 - logprior: -1.2059e+00
Epoch 7/10
10/10 - 1s - loss: 257.4769 - loglik: -2.5648e+02 - logprior: -9.9847e-01
Epoch 8/10
10/10 - 1s - loss: 257.6584 - loglik: -2.5672e+02 - logprior: -9.3568e-01
Fitted a model with MAP estimate = -256.9294
Time for alignment: 28.0193
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 340.3956 - loglik: -3.2038e+02 - logprior: -2.0019e+01
Epoch 2/10
10/10 - 1s - loss: 303.3067 - loglik: -2.9758e+02 - logprior: -5.7308e+00
Epoch 3/10
10/10 - 1s - loss: 285.4517 - loglik: -2.8216e+02 - logprior: -3.2913e+00
Epoch 4/10
10/10 - 1s - loss: 273.6118 - loglik: -2.7084e+02 - logprior: -2.7738e+00
Epoch 5/10
10/10 - 1s - loss: 267.9946 - loglik: -2.6530e+02 - logprior: -2.6935e+00
Epoch 6/10
10/10 - 1s - loss: 266.4789 - loglik: -2.6390e+02 - logprior: -2.5803e+00
Epoch 7/10
10/10 - 1s - loss: 264.5276 - loglik: -2.6208e+02 - logprior: -2.4450e+00
Epoch 8/10
10/10 - 1s - loss: 263.8707 - loglik: -2.6153e+02 - logprior: -2.3395e+00
Epoch 9/10
10/10 - 1s - loss: 262.9869 - loglik: -2.6071e+02 - logprior: -2.2744e+00
Epoch 10/10
10/10 - 1s - loss: 262.7249 - loglik: -2.6047e+02 - logprior: -2.2546e+00
Fitted a model with MAP estimate = -262.5126
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (20, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 296.5262 - loglik: -2.7408e+02 - logprior: -2.2443e+01
Epoch 2/2
10/10 - 1s - loss: 271.7139 - loglik: -2.6201e+02 - logprior: -9.7074e+00
Fitted a model with MAP estimate = -267.3188
expansions: [(0, 2)]
discards: [ 0  6 13 16 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 282.8502 - loglik: -2.6508e+02 - logprior: -1.7773e+01
Epoch 2/2
10/10 - 1s - loss: 263.3829 - loglik: -2.5851e+02 - logprior: -4.8687e+00
Fitted a model with MAP estimate = -260.9638
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 285.7029 - loglik: -2.6544e+02 - logprior: -2.0263e+01
Epoch 2/10
10/10 - 1s - loss: 265.8190 - loglik: -2.5990e+02 - logprior: -5.9212e+00
Epoch 3/10
10/10 - 1s - loss: 261.3306 - loglik: -2.5817e+02 - logprior: -3.1633e+00
Epoch 4/10
10/10 - 1s - loss: 260.2787 - loglik: -2.5824e+02 - logprior: -2.0369e+00
Epoch 5/10
10/10 - 1s - loss: 259.3431 - loglik: -2.5791e+02 - logprior: -1.4310e+00
Epoch 6/10
10/10 - 1s - loss: 258.9316 - loglik: -2.5773e+02 - logprior: -1.2018e+00
Epoch 7/10
10/10 - 1s - loss: 257.7378 - loglik: -2.5674e+02 - logprior: -9.9487e-01
Epoch 8/10
10/10 - 1s - loss: 257.3342 - loglik: -2.5640e+02 - logprior: -9.2994e-01
Epoch 9/10
10/10 - 1s - loss: 256.9660 - loglik: -2.5608e+02 - logprior: -8.8707e-01
Epoch 10/10
10/10 - 1s - loss: 256.1641 - loglik: -2.5530e+02 - logprior: -8.5912e-01
Fitted a model with MAP estimate = -256.0487
Time for alignment: 29.7064
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 340.3798 - loglik: -3.2036e+02 - logprior: -2.0021e+01
Epoch 2/10
10/10 - 1s - loss: 303.3980 - loglik: -2.9767e+02 - logprior: -5.7314e+00
Epoch 3/10
10/10 - 1s - loss: 285.3775 - loglik: -2.8206e+02 - logprior: -3.3145e+00
Epoch 4/10
10/10 - 1s - loss: 273.9872 - loglik: -2.7118e+02 - logprior: -2.8056e+00
Epoch 5/10
10/10 - 1s - loss: 268.3604 - loglik: -2.6566e+02 - logprior: -2.7032e+00
Epoch 6/10
10/10 - 1s - loss: 266.3265 - loglik: -2.6372e+02 - logprior: -2.6021e+00
Epoch 7/10
10/10 - 1s - loss: 264.8759 - loglik: -2.6241e+02 - logprior: -2.4624e+00
Epoch 8/10
10/10 - 1s - loss: 264.2513 - loglik: -2.6191e+02 - logprior: -2.3441e+00
Epoch 9/10
10/10 - 1s - loss: 263.1078 - loglik: -2.6082e+02 - logprior: -2.2871e+00
Epoch 10/10
10/10 - 1s - loss: 263.3344 - loglik: -2.6105e+02 - logprior: -2.2778e+00
Fitted a model with MAP estimate = -262.7418
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (19, 1), (20, 1), (22, 2), (23, 1), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 296.7965 - loglik: -2.7436e+02 - logprior: -2.2435e+01
Epoch 2/2
10/10 - 1s - loss: 271.8339 - loglik: -2.6212e+02 - logprior: -9.7136e+00
Fitted a model with MAP estimate = -267.4220
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 32]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 282.4550 - loglik: -2.6468e+02 - logprior: -1.7779e+01
Epoch 2/2
10/10 - 1s - loss: 263.3867 - loglik: -2.5852e+02 - logprior: -4.8664e+00
Fitted a model with MAP estimate = -260.9182
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 285.7585 - loglik: -2.6550e+02 - logprior: -2.0258e+01
Epoch 2/10
10/10 - 1s - loss: 265.5289 - loglik: -2.5961e+02 - logprior: -5.9201e+00
Epoch 3/10
10/10 - 1s - loss: 261.3683 - loglik: -2.5820e+02 - logprior: -3.1661e+00
Epoch 4/10
10/10 - 1s - loss: 260.3484 - loglik: -2.5830e+02 - logprior: -2.0492e+00
Epoch 5/10
10/10 - 1s - loss: 259.4065 - loglik: -2.5797e+02 - logprior: -1.4339e+00
Epoch 6/10
10/10 - 1s - loss: 258.8176 - loglik: -2.5761e+02 - logprior: -1.2033e+00
Epoch 7/10
10/10 - 1s - loss: 257.7051 - loglik: -2.5670e+02 - logprior: -1.0028e+00
Epoch 8/10
10/10 - 1s - loss: 257.5173 - loglik: -2.5658e+02 - logprior: -9.3460e-01
Epoch 9/10
10/10 - 1s - loss: 256.7094 - loglik: -2.5582e+02 - logprior: -8.8952e-01
Epoch 10/10
10/10 - 1s - loss: 256.2985 - loglik: -2.5543e+02 - logprior: -8.6693e-01
Fitted a model with MAP estimate = -256.0436
Time for alignment: 30.6121
Fitting a model of length 42 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 340.3129 - loglik: -3.2029e+02 - logprior: -2.0020e+01
Epoch 2/10
10/10 - 1s - loss: 303.4936 - loglik: -2.9777e+02 - logprior: -5.7275e+00
Epoch 3/10
10/10 - 1s - loss: 285.0912 - loglik: -2.8182e+02 - logprior: -3.2716e+00
Epoch 4/10
10/10 - 1s - loss: 273.7909 - loglik: -2.7106e+02 - logprior: -2.7308e+00
Epoch 5/10
10/10 - 1s - loss: 268.7389 - loglik: -2.6608e+02 - logprior: -2.6578e+00
Epoch 6/10
10/10 - 1s - loss: 266.2378 - loglik: -2.6366e+02 - logprior: -2.5746e+00
Epoch 7/10
10/10 - 1s - loss: 264.8358 - loglik: -2.6239e+02 - logprior: -2.4420e+00
Epoch 8/10
10/10 - 1s - loss: 264.1246 - loglik: -2.6181e+02 - logprior: -2.3147e+00
Epoch 9/10
10/10 - 1s - loss: 263.6288 - loglik: -2.6139e+02 - logprior: -2.2409e+00
Epoch 10/10
10/10 - 1s - loss: 263.0351 - loglik: -2.6080e+02 - logprior: -2.2365e+00
Fitted a model with MAP estimate = -262.8908
expansions: [(6, 2), (7, 1), (10, 2), (11, 2), (12, 2), (18, 1), (19, 1), (22, 1), (31, 2), (34, 1), (35, 1), (36, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 58 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 296.6395 - loglik: -2.7421e+02 - logprior: -2.2431e+01
Epoch 2/2
10/10 - 1s - loss: 271.7494 - loglik: -2.6204e+02 - logprior: -9.7052e+00
Fitted a model with MAP estimate = -267.3134
expansions: [(0, 2)]
discards: [ 0  6 13 15 18 42]
Re-initialized the encoder parameters.
Fitting a model of length 54 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 282.7483 - loglik: -2.6498e+02 - logprior: -1.7771e+01
Epoch 2/2
10/10 - 1s - loss: 263.1432 - loglik: -2.5828e+02 - logprior: -4.8643e+00
Fitted a model with MAP estimate = -260.9151
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 2266 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 285.8323 - loglik: -2.6556e+02 - logprior: -2.0275e+01
Epoch 2/10
10/10 - 1s - loss: 265.4635 - loglik: -2.5954e+02 - logprior: -5.9255e+00
Epoch 3/10
10/10 - 0s - loss: 261.5378 - loglik: -2.5837e+02 - logprior: -3.1662e+00
Epoch 4/10
10/10 - 1s - loss: 260.2245 - loglik: -2.5818e+02 - logprior: -2.0447e+00
Epoch 5/10
10/10 - 1s - loss: 259.3420 - loglik: -2.5791e+02 - logprior: -1.4355e+00
Epoch 6/10
10/10 - 1s - loss: 258.4515 - loglik: -2.5724e+02 - logprior: -1.2055e+00
Epoch 7/10
10/10 - 1s - loss: 258.3762 - loglik: -2.5737e+02 - logprior: -1.0072e+00
Epoch 8/10
10/10 - 0s - loss: 257.1387 - loglik: -2.5620e+02 - logprior: -9.3401e-01
Epoch 9/10
10/10 - 1s - loss: 256.8612 - loglik: -2.5596e+02 - logprior: -8.9415e-01
Epoch 10/10
10/10 - 1s - loss: 256.1912 - loglik: -2.5532e+02 - logprior: -8.7127e-01
Fitted a model with MAP estimate = -256.0314
Time for alignment: 29.0391
Computed alignments with likelihoods: ['-256.0314', '-256.9294', '-256.0487', '-256.0436', '-256.0314']
Best model has likelihood: -256.0314  (prior= -0.8573 )
time for generating output: 0.0937
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/kunitz.projection.fasta
SP score = 0.9893069306930693
Training of 5 independent models on file rhv.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea5e6c6f40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea00d9a250>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9abde1f10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 998.7189 - loglik: -9.9695e+02 - logprior: -1.7732e+00
Epoch 2/10
39/39 - 7s - loss: 919.3433 - loglik: -9.1824e+02 - logprior: -1.1008e+00
Epoch 3/10
39/39 - 7s - loss: 912.1703 - loglik: -9.1112e+02 - logprior: -1.0507e+00
Epoch 4/10
39/39 - 7s - loss: 911.0104 - loglik: -9.1002e+02 - logprior: -9.8675e-01
Epoch 5/10
39/39 - 7s - loss: 910.0662 - loglik: -9.0910e+02 - logprior: -9.6413e-01
Epoch 6/10
39/39 - 7s - loss: 909.1071 - loglik: -9.0814e+02 - logprior: -9.6488e-01
Epoch 7/10
39/39 - 7s - loss: 908.0947 - loglik: -9.0712e+02 - logprior: -9.6128e-01
Epoch 8/10
39/39 - 7s - loss: 907.4803 - loglik: -9.0650e+02 - logprior: -9.6802e-01
Epoch 9/10
39/39 - 8s - loss: 906.6891 - loglik: -9.0570e+02 - logprior: -9.7311e-01
Epoch 10/10
39/39 - 7s - loss: 906.2058 - loglik: -9.0522e+02 - logprior: -9.6801e-01
Fitted a model with MAP estimate = -727.1913
expansions: [(0, 16), (18, 1), (19, 1), (28, 1), (29, 1), (30, 3), (37, 2), (43, 1), (44, 1), (69, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 1), (107, 1), (108, 1), (112, 1), (125, 8), (126, 1), (130, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 894.6874 - loglik: -8.9209e+02 - logprior: -2.5997e+00
Epoch 2/2
39/39 - 10s - loss: 878.3606 - loglik: -8.7702e+02 - logprior: -1.3415e+00
Fitted a model with MAP estimate = -703.4979
expansions: [(30, 1), (51, 2), (165, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14 176]
Re-initialized the encoder parameters.
Fitting a model of length 171 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 15s - loss: 885.0181 - loglik: -8.8213e+02 - logprior: -2.8912e+00
Epoch 2/2
39/39 - 9s - loss: 878.9119 - loglik: -8.7807e+02 - logprior: -8.4293e-01
Fitted a model with MAP estimate = -706.2764
expansions: [(0, 19)]
discards: [153]
Re-initialized the encoder parameters.
Fitting a model of length 189 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 701.9057 - loglik: -7.0042e+02 - logprior: -1.4904e+00
Epoch 2/10
52/52 - 12s - loss: 697.8955 - loglik: -6.9673e+02 - logprior: -1.1658e+00
Epoch 3/10
52/52 - 13s - loss: 695.0321 - loglik: -6.9391e+02 - logprior: -1.1232e+00
Epoch 4/10
52/52 - 13s - loss: 695.8474 - loglik: -6.9479e+02 - logprior: -1.0567e+00
Fitted a model with MAP estimate = -694.8627
Time for alignment: 235.9967
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 997.7619 - loglik: -9.9603e+02 - logprior: -1.7271e+00
Epoch 2/10
39/39 - 8s - loss: 920.6550 - loglik: -9.1972e+02 - logprior: -9.3229e-01
Epoch 3/10
39/39 - 7s - loss: 912.6628 - loglik: -9.1171e+02 - logprior: -9.5114e-01
Epoch 4/10
39/39 - 7s - loss: 910.3805 - loglik: -9.0948e+02 - logprior: -9.0235e-01
Epoch 5/10
39/39 - 8s - loss: 909.7136 - loglik: -9.0883e+02 - logprior: -8.7936e-01
Epoch 6/10
39/39 - 7s - loss: 908.0444 - loglik: -9.0715e+02 - logprior: -8.8928e-01
Epoch 7/10
39/39 - 8s - loss: 906.7371 - loglik: -9.0582e+02 - logprior: -9.1066e-01
Epoch 8/10
39/39 - 7s - loss: 903.7454 - loglik: -9.0278e+02 - logprior: -9.5624e-01
Epoch 9/10
39/39 - 8s - loss: 902.3463 - loglik: -9.0135e+02 - logprior: -9.8192e-01
Epoch 10/10
39/39 - 8s - loss: 902.4639 - loglik: -9.0144e+02 - logprior: -1.0061e+00
Fitted a model with MAP estimate = -725.2836
expansions: [(0, 12), (11, 1), (19, 1), (27, 1), (28, 1), (30, 2), (33, 3), (38, 2), (45, 3), (68, 4), (71, 2), (88, 1), (89, 1), (90, 1), (91, 1), (103, 2), (104, 2), (107, 2), (114, 1), (126, 10), (130, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 191 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 14s - loss: 888.0190 - loglik: -8.8527e+02 - logprior: -2.7525e+00
Epoch 2/2
39/39 - 10s - loss: 868.5733 - loglik: -8.6714e+02 - logprior: -1.4346e+00
Fitted a model with MAP estimate = -698.6891
expansions: [(55, 2), (173, 1), (174, 2)]
discards: [  1   2   3   4   5   6   7   8   9  46  52  53  69  95  96  97 101 139
 142 147 183]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 876.1343 - loglik: -8.7427e+02 - logprior: -1.8689e+00
Epoch 2/2
39/39 - 10s - loss: 871.3116 - loglik: -8.7056e+02 - logprior: -7.5564e-01
Fitted a model with MAP estimate = -701.9720
expansions: [(44, 1)]
discards: [158]
Re-initialized the encoder parameters.
Fitting a model of length 175 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 14s - loss: 697.6308 - loglik: -6.9651e+02 - logprior: -1.1163e+00
Epoch 2/10
52/52 - 12s - loss: 693.3916 - loglik: -6.9272e+02 - logprior: -6.6814e-01
Epoch 3/10
52/52 - 11s - loss: 695.9047 - loglik: -6.9529e+02 - logprior: -6.1704e-01
Fitted a model with MAP estimate = -693.1455
Time for alignment: 223.2535
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 998.7532 - loglik: -9.9700e+02 - logprior: -1.7569e+00
Epoch 2/10
39/39 - 7s - loss: 916.4589 - loglik: -9.1539e+02 - logprior: -1.0635e+00
Epoch 3/10
39/39 - 7s - loss: 909.1817 - loglik: -9.0814e+02 - logprior: -1.0357e+00
Epoch 4/10
39/39 - 7s - loss: 908.1133 - loglik: -9.0710e+02 - logprior: -1.0076e+00
Epoch 5/10
39/39 - 7s - loss: 907.2292 - loglik: -9.0624e+02 - logprior: -9.8661e-01
Epoch 6/10
39/39 - 7s - loss: 905.3401 - loglik: -9.0438e+02 - logprior: -9.5337e-01
Epoch 7/10
39/39 - 7s - loss: 904.6277 - loglik: -9.0365e+02 - logprior: -9.7167e-01
Epoch 8/10
39/39 - 7s - loss: 903.0424 - loglik: -9.0204e+02 - logprior: -9.8531e-01
Epoch 9/10
39/39 - 7s - loss: 903.1305 - loglik: -9.0213e+02 - logprior: -9.8745e-01
Fitted a model with MAP estimate = -725.1854
expansions: [(0, 13), (10, 1), (11, 1), (18, 1), (28, 1), (29, 1), (30, 1), (31, 5), (33, 2), (42, 2), (43, 1), (44, 1), (55, 1), (70, 2), (87, 1), (88, 1), (89, 1), (90, 1), (102, 1), (107, 2), (125, 8), (126, 1), (130, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 186 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 891.9767 - loglik: -8.8937e+02 - logprior: -2.6066e+00
Epoch 2/2
39/39 - 10s - loss: 874.5029 - loglik: -8.7330e+02 - logprior: -1.1990e+00
Fitted a model with MAP estimate = -702.0855
expansions: [(169, 4)]
discards: [  1   2   3   4   5   6   7   8   9  10  69 145 180]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 877.5405 - loglik: -8.7565e+02 - logprior: -1.8855e+00
Epoch 2/2
39/39 - 10s - loss: 872.9510 - loglik: -8.7222e+02 - logprior: -7.3462e-01
Fitted a model with MAP estimate = -703.4463
expansions: [(51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 178 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 16s - loss: 699.8016 - loglik: -6.9871e+02 - logprior: -1.0964e+00
Epoch 2/10
52/52 - 11s - loss: 693.2916 - loglik: -6.9265e+02 - logprior: -6.4506e-01
Epoch 3/10
52/52 - 12s - loss: 695.5660 - loglik: -6.9495e+02 - logprior: -6.1653e-01
Fitted a model with MAP estimate = -693.0995
Time for alignment: 214.4733
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 997.9061 - loglik: -9.9617e+02 - logprior: -1.7378e+00
Epoch 2/10
39/39 - 8s - loss: 917.5925 - loglik: -9.1669e+02 - logprior: -9.0279e-01
Epoch 3/10
39/39 - 7s - loss: 908.6525 - loglik: -9.0770e+02 - logprior: -9.4902e-01
Epoch 4/10
39/39 - 7s - loss: 906.4767 - loglik: -9.0555e+02 - logprior: -9.2652e-01
Epoch 5/10
39/39 - 7s - loss: 905.0744 - loglik: -9.0414e+02 - logprior: -9.3179e-01
Epoch 6/10
39/39 - 7s - loss: 903.6198 - loglik: -9.0267e+02 - logprior: -9.3860e-01
Epoch 7/10
39/39 - 7s - loss: 902.6797 - loglik: -9.0173e+02 - logprior: -9.4224e-01
Epoch 8/10
39/39 - 7s - loss: 902.1050 - loglik: -9.0116e+02 - logprior: -9.3769e-01
Epoch 9/10
39/39 - 7s - loss: 900.9999 - loglik: -9.0005e+02 - logprior: -9.3980e-01
Epoch 10/10
39/39 - 7s - loss: 900.9384 - loglik: -8.9996e+02 - logprior: -9.5986e-01
Fitted a model with MAP estimate = -723.5680
expansions: [(0, 6), (10, 1), (11, 1), (18, 1), (28, 1), (30, 2), (31, 1), (32, 2), (33, 1), (34, 1), (36, 1), (37, 2), (43, 1), (44, 2), (71, 1), (88, 1), (89, 1), (90, 1), (91, 1), (103, 2), (104, 1), (107, 2), (108, 1), (125, 7), (127, 3), (134, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 886.3859 - loglik: -8.8324e+02 - logprior: -3.1464e+00
Epoch 2/2
39/39 - 10s - loss: 867.5108 - loglik: -8.6611e+02 - logprior: -1.4042e+00
Fitted a model with MAP estimate = -697.2511
expansions: []
discards: [  0   1   2   3   4   5  45  50  65 131 138 169 182]
Re-initialized the encoder parameters.
Fitting a model of length 170 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 12s - loss: 875.6369 - loglik: -8.7363e+02 - logprior: -2.0105e+00
Epoch 2/2
39/39 - 9s - loss: 871.2739 - loglik: -8.7076e+02 - logprior: -5.0884e-01
Fitted a model with MAP estimate = -701.4636
expansions: [(0, 16), (151, 1), (152, 3), (170, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 195 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 16s - loss: 695.0816 - loglik: -6.9355e+02 - logprior: -1.5288e+00
Epoch 2/10
52/52 - 12s - loss: 688.2256 - loglik: -6.8707e+02 - logprior: -1.1505e+00
Epoch 3/10
52/52 - 13s - loss: 690.5674 - loglik: -6.8947e+02 - logprior: -1.0972e+00
Fitted a model with MAP estimate = -687.6503
Time for alignment: 222.7579
Fitting a model of length 134 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 10s - loss: 997.3314 - loglik: -9.9561e+02 - logprior: -1.7250e+00
Epoch 2/10
39/39 - 7s - loss: 917.4028 - loglik: -9.1653e+02 - logprior: -8.7016e-01
Epoch 3/10
39/39 - 7s - loss: 911.6213 - loglik: -9.1084e+02 - logprior: -7.7708e-01
Epoch 4/10
39/39 - 7s - loss: 909.9139 - loglik: -9.0919e+02 - logprior: -7.2387e-01
Epoch 5/10
39/39 - 7s - loss: 909.3129 - loglik: -9.0859e+02 - logprior: -7.2142e-01
Epoch 6/10
39/39 - 7s - loss: 907.8439 - loglik: -9.0713e+02 - logprior: -7.1132e-01
Epoch 7/10
39/39 - 7s - loss: 907.0953 - loglik: -9.0635e+02 - logprior: -7.3324e-01
Epoch 8/10
39/39 - 7s - loss: 906.1903 - loglik: -9.0545e+02 - logprior: -7.2561e-01
Epoch 9/10
39/39 - 7s - loss: 905.3238 - loglik: -9.0460e+02 - logprior: -7.1012e-01
Epoch 10/10
39/39 - 8s - loss: 905.5474 - loglik: -9.0485e+02 - logprior: -6.7984e-01
Fitted a model with MAP estimate = -726.5845
expansions: [(0, 16), (10, 1), (11, 1), (18, 1), (28, 1), (30, 3), (37, 2), (44, 3), (69, 1), (71, 1), (88, 1), (89, 2), (90, 1), (103, 1), (107, 3), (125, 5), (127, 1), (130, 4), (134, 5)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 187 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 890.2180 - loglik: -8.8766e+02 - logprior: -2.5620e+00
Epoch 2/2
39/39 - 10s - loss: 872.4006 - loglik: -8.7117e+02 - logprior: -1.2332e+00
Fitted a model with MAP estimate = -700.4553
expansions: []
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  51  52  70 175
 176 177 182 183]
Re-initialized the encoder parameters.
Fitting a model of length 165 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 13s - loss: 882.1714 - loglik: -8.8031e+02 - logprior: -1.8653e+00
Epoch 2/2
39/39 - 9s - loss: 878.0746 - loglik: -8.7738e+02 - logprior: -6.9272e-01
Fitted a model with MAP estimate = -706.0059
expansions: [(0, 19)]
discards: [162 163 164]
Re-initialized the encoder parameters.
Fitting a model of length 181 on 17976 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
52/52 - 15s - loss: 706.2917 - loglik: -7.0484e+02 - logprior: -1.4473e+00
Epoch 2/10
52/52 - 12s - loss: 698.3557 - loglik: -6.9744e+02 - logprior: -9.1187e-01
Epoch 3/10
52/52 - 11s - loss: 699.6736 - loglik: -6.9883e+02 - logprior: -8.3884e-01
Fitted a model with MAP estimate = -699.4175
Time for alignment: 220.2032
Computed alignments with likelihoods: ['-694.8627', '-693.1455', '-693.0995', '-687.6503', '-699.4175']
Best model has likelihood: -687.6503  (prior= -1.0683 )
time for generating output: 0.5899
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/rhv.projection.fasta
SP score = 0.1863034574721402
Training of 5 independent models on file blm.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea66807b20>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8d24971f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c8082e0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 22s - loss: 1682.3179 - loglik: -1.6808e+03 - logprior: -1.5152e+00
Epoch 2/10
37/37 - 20s - loss: 1602.1918 - loglik: -1.6017e+03 - logprior: -5.0307e-01
Epoch 3/10
37/37 - 20s - loss: 1592.4606 - loglik: -1.5920e+03 - logprior: -4.5247e-01
Epoch 4/10
37/37 - 20s - loss: 1586.1140 - loglik: -1.5857e+03 - logprior: -4.4539e-01
Epoch 5/10
37/37 - 20s - loss: 1582.3683 - loglik: -1.5819e+03 - logprior: -4.8388e-01
Epoch 6/10
37/37 - 20s - loss: 1573.0540 - loglik: -1.5726e+03 - logprior: -4.7349e-01
Epoch 7/10
37/37 - 20s - loss: 1553.2937 - loglik: -1.5527e+03 - logprior: -5.6627e-01
Epoch 8/10
37/37 - 20s - loss: 1455.0508 - loglik: -1.4524e+03 - logprior: -2.6321e+00
Epoch 9/10
37/37 - 20s - loss: 1288.2810 - loglik: -1.2824e+03 - logprior: -5.8017e+00
Epoch 10/10
37/37 - 20s - loss: 1252.1112 - loglik: -1.2466e+03 - logprior: -5.4503e+00
Fitted a model with MAP estimate = -1238.9498
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 110 117
 118 119 120 121 122 123 124 132 133 134 135 136 137 138 139 140 141 148
 150 151 152 153 154 155 173 174 175 176 191 193 230]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 12s - loss: 1764.4215 - loglik: -1.7619e+03 - logprior: -2.4960e+00
Epoch 2/2
37/37 - 9s - loss: 1730.4061 - loglik: -1.7296e+03 - logprior: -7.7414e-01
Fitted a model with MAP estimate = -1720.4407
expansions: [(0, 162), (1, 2), (115, 114)]
discards: [  4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21
  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39
  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57
  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113 114]
Re-initialized the encoder parameters.
Fitting a model of length 282 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 29s - loss: 1671.6787 - loglik: -1.6674e+03 - logprior: -4.2505e+00
Epoch 2/2
37/37 - 23s - loss: 1593.6515 - loglik: -1.5930e+03 - logprior: -6.8897e-01
Fitted a model with MAP estimate = -1584.7914
expansions: [(0, 3), (32, 1), (34, 1), (40, 1), (80, 2), (83, 2), (185, 3), (227, 8), (229, 3), (257, 1), (261, 1), (271, 1), (273, 1), (274, 1), (275, 1), (282, 4)]
discards: [101 102 103 104 105 106 162 165 251 252 253]
Re-initialized the encoder parameters.
Fitting a model of length 305 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 1589.6484 - loglik: -1.5865e+03 - logprior: -3.1045e+00
Epoch 2/10
37/37 - 26s - loss: 1579.1692 - loglik: -1.5786e+03 - logprior: -5.7720e-01
Epoch 3/10
37/37 - 26s - loss: 1572.7418 - loglik: -1.5726e+03 - logprior: -1.5907e-01
Epoch 4/10
37/37 - 26s - loss: 1569.8625 - loglik: -1.5697e+03 - logprior: -2.0042e-01
Epoch 5/10
37/37 - 26s - loss: 1567.4677 - loglik: -1.5673e+03 - logprior: -1.4186e-01
Epoch 6/10
37/37 - 26s - loss: 1553.7419 - loglik: -1.5537e+03 - logprior: -4.7610e-02
Epoch 7/10
37/37 - 26s - loss: 1532.5251 - loglik: -1.5322e+03 - logprior: -2.8175e-01
Epoch 8/10
37/37 - 26s - loss: 1428.1715 - loglik: -1.4270e+03 - logprior: -1.1897e+00
Epoch 9/10
37/37 - 26s - loss: 1291.3783 - loglik: -1.2872e+03 - logprior: -4.1072e+00
Epoch 10/10
37/37 - 26s - loss: 1254.1914 - loglik: -1.2491e+03 - logprior: -5.0031e+00
Fitted a model with MAP estimate = -1242.2107
Time for alignment: 630.3407
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 1683.2620 - loglik: -1.6818e+03 - logprior: -1.4988e+00
Epoch 2/10
37/37 - 20s - loss: 1597.2878 - loglik: -1.5968e+03 - logprior: -4.4095e-01
Epoch 3/10
37/37 - 20s - loss: 1588.1412 - loglik: -1.5877e+03 - logprior: -3.9060e-01
Epoch 4/10
37/37 - 20s - loss: 1583.4900 - loglik: -1.5832e+03 - logprior: -3.3468e-01
Epoch 5/10
37/37 - 20s - loss: 1576.9340 - loglik: -1.5765e+03 - logprior: -4.2880e-01
Epoch 6/10
37/37 - 20s - loss: 1571.3879 - loglik: -1.5710e+03 - logprior: -3.9745e-01
Epoch 7/10
37/37 - 20s - loss: 1553.5261 - loglik: -1.5531e+03 - logprior: -4.0464e-01
Epoch 8/10
37/37 - 20s - loss: 1482.2344 - loglik: -1.4810e+03 - logprior: -1.2423e+00
Epoch 9/10
37/37 - 20s - loss: 1303.7239 - loglik: -1.2984e+03 - logprior: -5.2368e+00
Epoch 10/10
37/37 - 20s - loss: 1258.6127 - loglik: -1.2536e+03 - logprior: -4.9401e+00
Fitted a model with MAP estimate = -1254.3741
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 133 134 135 152 153
 159 170 171 172 173 189 190 198 227 228 229 230]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 14s - loss: 1768.0098 - loglik: -1.7653e+03 - logprior: -2.7307e+00
Epoch 2/2
37/37 - 9s - loss: 1732.8563 - loglik: -1.7323e+03 - logprior: -5.3449e-01
Fitted a model with MAP estimate = -1719.5386
expansions: [(0, 164), (116, 104)]
discards: [  4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21
  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39
  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57
  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 272 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 25s - loss: 1673.4198 - loglik: -1.6696e+03 - logprior: -3.8275e+00
Epoch 2/2
37/37 - 22s - loss: 1600.2479 - loglik: -1.5991e+03 - logprior: -1.1421e+00
Fitted a model with MAP estimate = -1596.1480
expansions: [(0, 3), (11, 1), (166, 40), (207, 2), (209, 2), (217, 4), (220, 3), (221, 1), (229, 3), (260, 1), (261, 1), (262, 1), (264, 1), (265, 1)]
discards: [  0 106 107 108 109 110 111 120 121 122 123 124 125 127 128 188 189 190
 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205]
Re-initialized the encoder parameters.
Fitting a model of length 303 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 1598.4152 - loglik: -1.5958e+03 - logprior: -2.6141e+00
Epoch 2/10
37/37 - 26s - loss: 1574.0842 - loglik: -1.5732e+03 - logprior: -8.4226e-01
Epoch 3/10
37/37 - 26s - loss: 1573.0348 - loglik: -1.5723e+03 - logprior: -6.9200e-01
Epoch 4/10
37/37 - 26s - loss: 1566.5038 - loglik: -1.5659e+03 - logprior: -6.2300e-01
Epoch 5/10
37/37 - 26s - loss: 1563.5309 - loglik: -1.5630e+03 - logprior: -5.6780e-01
Epoch 6/10
37/37 - 26s - loss: 1548.5074 - loglik: -1.5480e+03 - logprior: -5.4005e-01
Epoch 7/10
37/37 - 26s - loss: 1533.7660 - loglik: -1.5332e+03 - logprior: -5.8815e-01
Epoch 8/10
37/37 - 26s - loss: 1443.9797 - loglik: -1.4426e+03 - logprior: -1.3316e+00
Epoch 9/10
37/37 - 26s - loss: 1303.1737 - loglik: -1.2989e+03 - logprior: -4.1932e+00
Epoch 10/10
37/37 - 26s - loss: 1262.7610 - loglik: -1.2582e+03 - logprior: -4.5291e+00
Fitted a model with MAP estimate = -1246.2017
Time for alignment: 624.5770
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 1683.5494 - loglik: -1.6820e+03 - logprior: -1.5258e+00
Epoch 2/10
37/37 - 20s - loss: 1598.2339 - loglik: -1.5978e+03 - logprior: -4.8037e-01
Epoch 3/10
37/37 - 20s - loss: 1587.2292 - loglik: -1.5868e+03 - logprior: -4.0696e-01
Epoch 4/10
37/37 - 20s - loss: 1585.9816 - loglik: -1.5856e+03 - logprior: -3.4787e-01
Epoch 5/10
37/37 - 20s - loss: 1578.4606 - loglik: -1.5780e+03 - logprior: -4.9785e-01
Epoch 6/10
37/37 - 20s - loss: 1571.5586 - loglik: -1.5712e+03 - logprior: -3.0262e-01
Epoch 7/10
37/37 - 20s - loss: 1554.0419 - loglik: -1.5536e+03 - logprior: -3.8496e-01
Epoch 8/10
37/37 - 20s - loss: 1466.8508 - loglik: -1.4650e+03 - logprior: -1.8663e+00
Epoch 9/10
37/37 - 20s - loss: 1301.1644 - loglik: -1.2957e+03 - logprior: -5.4391e+00
Epoch 10/10
37/37 - 20s - loss: 1262.5504 - loglik: -1.2573e+03 - logprior: -5.2212e+00
Fitted a model with MAP estimate = -1247.6632
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91 101 102 103 105 106 122 123 124 125 126 127 128 129 130 131 132
 133 134 135 136 137 153 154 169 182 183 184 185 186 187 188 189 190 191
 192 193 194 195 196 197 198 199 200 238]
Re-initialized the encoder parameters.
Fitting a model of length 118 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 13s - loss: 1761.2610 - loglik: -1.7591e+03 - logprior: -2.1312e+00
Epoch 2/2
37/37 - 9s - loss: 1725.9861 - loglik: -1.7254e+03 - logprior: -6.2418e-01
Fitted a model with MAP estimate = -1716.3306
expansions: [(0, 203), (118, 73)]
discards: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18
  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36
  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54
  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72
  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90
  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108
 109 110 111 112 113 114 115 116 117]
Re-initialized the encoder parameters.
Fitting a model of length 277 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 26s - loss: 1674.8713 - loglik: -1.6716e+03 - logprior: -3.2454e+00
Epoch 2/2
37/37 - 22s - loss: 1594.9613 - loglik: -1.5944e+03 - logprior: -5.3939e-01
Fitted a model with MAP estimate = -1585.4734
expansions: [(0, 3), (30, 1), (32, 2), (36, 1), (40, 1), (54, 1), (76, 2), (95, 1), (107, 1), (113, 1), (146, 1), (163, 1), (164, 2), (236, 2), (237, 1), (257, 1), (258, 1), (277, 4)]
discards: [171]
Re-initialized the encoder parameters.
Fitting a model of length 303 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 1587.4702 - loglik: -1.5846e+03 - logprior: -2.8918e+00
Epoch 2/10
37/37 - 26s - loss: 1577.1550 - loglik: -1.5768e+03 - logprior: -3.7023e-01
Epoch 3/10
37/37 - 26s - loss: 1571.8839 - loglik: -1.5718e+03 - logprior: -1.0664e-01
Epoch 4/10
37/37 - 26s - loss: 1571.8615 - loglik: -1.5718e+03 - logprior: -1.4118e-02
Epoch 5/10
37/37 - 26s - loss: 1563.2695 - loglik: -1.5630e+03 - logprior: -2.4040e-01
Epoch 6/10
37/37 - 26s - loss: 1549.8884 - loglik: -1.5499e+03 - logprior: 0.0653
Epoch 7/10
37/37 - 26s - loss: 1525.0574 - loglik: -1.5252e+03 - logprior: 0.1392
Epoch 8/10
37/37 - 26s - loss: 1414.9869 - loglik: -1.4139e+03 - logprior: -1.0386e+00
Epoch 9/10
37/37 - 26s - loss: 1272.3931 - loglik: -1.2683e+03 - logprior: -4.0437e+00
Epoch 10/10
37/37 - 26s - loss: 1234.8618 - loglik: -1.2306e+03 - logprior: -4.2317e+00
Fitted a model with MAP estimate = -1221.6429
Time for alignment: 626.2127
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 25s - loss: 1683.3038 - loglik: -1.6818e+03 - logprior: -1.5035e+00
Epoch 2/10
37/37 - 20s - loss: 1597.1969 - loglik: -1.5968e+03 - logprior: -3.6873e-01
Epoch 3/10
37/37 - 20s - loss: 1586.0231 - loglik: -1.5857e+03 - logprior: -2.9309e-01
Epoch 4/10
37/37 - 20s - loss: 1584.5339 - loglik: -1.5843e+03 - logprior: -2.5960e-01
Epoch 5/10
37/37 - 20s - loss: 1574.5597 - loglik: -1.5743e+03 - logprior: -2.3087e-01
Epoch 6/10
37/37 - 20s - loss: 1568.4130 - loglik: -1.5680e+03 - logprior: -4.0135e-01
Epoch 7/10
37/37 - 20s - loss: 1551.4171 - loglik: -1.5512e+03 - logprior: -1.6803e-01
Epoch 8/10
37/37 - 20s - loss: 1451.4612 - loglik: -1.4492e+03 - logprior: -2.2804e+00
Epoch 9/10
37/37 - 20s - loss: 1292.5948 - loglik: -1.2872e+03 - logprior: -5.3764e+00
Epoch 10/10
37/37 - 20s - loss: 1254.8090 - loglik: -1.2498e+03 - logprior: -4.9618e+00
Fitted a model with MAP estimate = -1248.5905
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132
 133 134 135 170 171 172 173 188 189 190 191 192]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 12s - loss: 1766.8699 - loglik: -1.7647e+03 - logprior: -2.1288e+00
Epoch 2/2
37/37 - 9s - loss: 1727.1116 - loglik: -1.7267e+03 - logprior: -3.9319e-01
Fitted a model with MAP estimate = -1714.8066
expansions: [(0, 99), (1, 15), (4, 1), (7, 19), (116, 141)]
discards: [  8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25
  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43
  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61
  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79
  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97
  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115]
Re-initialized the encoder parameters.
Fitting a model of length 283 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 26s - loss: 1673.2848 - loglik: -1.6692e+03 - logprior: -4.1288e+00
Epoch 2/2
37/37 - 23s - loss: 1590.4277 - loglik: -1.5894e+03 - logprior: -1.0545e+00
Fitted a model with MAP estimate = -1583.8861
expansions: [(30, 2), (131, 1), (157, 1), (176, 1), (177, 3), (217, 4), (225, 11), (242, 2), (243, 1), (261, 1), (271, 1), (273, 1), (276, 1)]
discards: [  0  77  78  79  80  81  82  83  84  87  93  94  95  96  97  98  99 100
 101 102 103 282]
Re-initialized the encoder parameters.
Fitting a model of length 291 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 28s - loss: 1590.1277 - loglik: -1.5876e+03 - logprior: -2.5525e+00
Epoch 2/10
37/37 - 24s - loss: 1579.3149 - loglik: -1.5790e+03 - logprior: -3.3190e-01
Epoch 3/10
37/37 - 24s - loss: 1577.5231 - loglik: -1.5774e+03 - logprior: -1.2830e-01
Epoch 4/10
37/37 - 24s - loss: 1572.7527 - loglik: -1.5727e+03 - logprior: -3.8080e-02
Epoch 5/10
37/37 - 24s - loss: 1570.1667 - loglik: -1.5701e+03 - logprior: -1.5078e-02
Epoch 6/10
37/37 - 24s - loss: 1560.9163 - loglik: -1.5610e+03 - logprior: 0.0848
Epoch 7/10
37/37 - 24s - loss: 1538.9288 - loglik: -1.5390e+03 - logprior: 0.0811
Epoch 8/10
37/37 - 24s - loss: 1439.0907 - loglik: -1.4380e+03 - logprior: -1.0167e+00
Epoch 9/10
37/37 - 24s - loss: 1288.1301 - loglik: -1.2845e+03 - logprior: -3.5541e+00
Epoch 10/10
37/37 - 24s - loss: 1246.5691 - loglik: -1.2426e+03 - logprior: -3.9536e+00
Fitted a model with MAP estimate = -1229.6807
Time for alignment: 612.8037
Fitting a model of length 254 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 23s - loss: 1681.8812 - loglik: -1.6804e+03 - logprior: -1.5014e+00
Epoch 2/10
37/37 - 20s - loss: 1602.6949 - loglik: -1.6021e+03 - logprior: -5.7394e-01
Epoch 3/10
37/37 - 20s - loss: 1590.4840 - loglik: -1.5899e+03 - logprior: -5.6225e-01
Epoch 4/10
37/37 - 20s - loss: 1585.0833 - loglik: -1.5845e+03 - logprior: -5.4170e-01
Epoch 5/10
37/37 - 20s - loss: 1579.4895 - loglik: -1.5790e+03 - logprior: -5.2972e-01
Epoch 6/10
37/37 - 20s - loss: 1572.5034 - loglik: -1.5720e+03 - logprior: -4.9698e-01
Epoch 7/10
37/37 - 20s - loss: 1557.1732 - loglik: -1.5567e+03 - logprior: -4.5867e-01
Epoch 8/10
37/37 - 20s - loss: 1454.4131 - loglik: -1.4516e+03 - logprior: -2.7905e+00
Epoch 9/10
37/37 - 20s - loss: 1295.1942 - loglik: -1.2891e+03 - logprior: -6.0037e+00
Epoch 10/10
37/37 - 20s - loss: 1260.6520 - loglik: -1.2549e+03 - logprior: -5.7386e+00
Fitted a model with MAP estimate = -1251.2491
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  92  99 100
 101 102 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134
 135 136 137 138 139 140 141 150 151 152 153 154 157 158 159 160 161 162
 163 164 170 171 172 173 188 189 190 191 192 193 194 195 196 197 198 199]
Re-initialized the encoder parameters.
Fitting a model of length 110 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 12s - loss: 1761.3894 - loglik: -1.7593e+03 - logprior: -2.0739e+00
Epoch 2/2
37/37 - 9s - loss: 1725.9930 - loglik: -1.7254e+03 - logprior: -5.5874e-01
Fitted a model with MAP estimate = -1720.1055
expansions: [(0, 142), (110, 130)]
discards: [  4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21
  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39
  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57
  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109]
Re-initialized the encoder parameters.
Fitting a model of length 276 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
37/37 - 25s - loss: 1668.0787 - loglik: -1.6651e+03 - logprior: -2.9859e+00
Epoch 2/2
37/37 - 22s - loss: 1585.0353 - loglik: -1.5844e+03 - logprior: -5.8873e-01
Fitted a model with MAP estimate = -1577.5171
expansions: [(0, 2), (29, 1), (36, 1), (39, 1), (83, 1), (123, 1), (229, 3), (262, 1), (264, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 289 on 9105 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
37/37 - 29s - loss: 1581.9871 - loglik: -1.5789e+03 - logprior: -3.0788e+00
Epoch 2/10
37/37 - 24s - loss: 1576.7100 - loglik: -1.5761e+03 - logprior: -6.3411e-01
Epoch 3/10
37/37 - 24s - loss: 1569.9976 - loglik: -1.5697e+03 - logprior: -2.9482e-01
Epoch 4/10
37/37 - 24s - loss: 1569.1592 - loglik: -1.5689e+03 - logprior: -2.5835e-01
Epoch 5/10
37/37 - 24s - loss: 1563.3243 - loglik: -1.5631e+03 - logprior: -2.0091e-01
Epoch 6/10
37/37 - 24s - loss: 1554.3005 - loglik: -1.5540e+03 - logprior: -2.5263e-01
Epoch 7/10
37/37 - 24s - loss: 1538.7231 - loglik: -1.5384e+03 - logprior: -2.6687e-01
Epoch 8/10
37/37 - 24s - loss: 1450.7144 - loglik: -1.4494e+03 - logprior: -1.3015e+00
Epoch 9/10
37/37 - 24s - loss: 1302.0338 - loglik: -1.2983e+03 - logprior: -3.6999e+00
Epoch 10/10
37/37 - 24s - loss: 1257.5923 - loglik: -1.2536e+03 - logprior: -3.9121e+00
Fitted a model with MAP estimate = -1239.9448
Time for alignment: 606.4615
Computed alignments with likelihoods: ['-1238.9498', '-1246.2017', '-1221.6429', '-1229.6807', '-1239.9448']
Best model has likelihood: -1221.6429  (prior= -4.2153 )
time for generating output: 0.2599
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blm.projection.fasta
SP score = 0.05915100904662491
Training of 5 independent models on file cyt3.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe916853340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ab648a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3ba86af0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 659.1701 - loglik: -5.4600e+02 - logprior: -1.1317e+02
Epoch 2/10
10/10 - 1s - loss: 555.9457 - loglik: -5.2857e+02 - logprior: -2.7380e+01
Epoch 3/10
10/10 - 1s - loss: 525.3506 - loglik: -5.1542e+02 - logprior: -9.9280e+00
Epoch 4/10
10/10 - 1s - loss: 508.0221 - loglik: -5.0458e+02 - logprior: -3.4401e+00
Epoch 5/10
10/10 - 1s - loss: 493.8530 - loglik: -4.9342e+02 - logprior: -4.3089e-01
Epoch 6/10
10/10 - 1s - loss: 477.1699 - loglik: -4.7810e+02 - logprior: 0.9274
Epoch 7/10
10/10 - 1s - loss: 454.1950 - loglik: -4.5480e+02 - logprior: 0.6055
Epoch 8/10
10/10 - 1s - loss: 433.5045 - loglik: -4.3165e+02 - logprior: -1.8479e+00
Epoch 9/10
10/10 - 1s - loss: 421.4804 - loglik: -4.1965e+02 - logprior: -1.8311e+00
Epoch 10/10
10/10 - 1s - loss: 411.3107 - loglik: -4.0987e+02 - logprior: -1.4345e+00
Fitted a model with MAP estimate = -407.4484
expansions: []
discards: [41 42 43 44 45 48 49 50 51 52 53 54 55 56 57 58 59 60 61]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 671.1919 - loglik: -5.6708e+02 - logprior: -1.0412e+02
Epoch 2/2
10/10 - 1s - loss: 573.0103 - loglik: -5.4666e+02 - logprior: -2.6348e+01
Fitted a model with MAP estimate = -555.3078
expansions: [(0, 51), (57, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 655.4240 - loglik: -5.4019e+02 - logprior: -1.1523e+02
Epoch 2/2
10/10 - 1s - loss: 549.3180 - loglik: -5.2176e+02 - logprior: -2.7556e+01
Fitted a model with MAP estimate = -530.5844
expansions: [(0, 4), (54, 9)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 91 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 645.0281 - loglik: -5.1544e+02 - logprior: -1.2959e+02
Epoch 2/10
10/10 - 1s - loss: 535.3283 - loglik: -5.0023e+02 - logprior: -3.5097e+01
Epoch 3/10
10/10 - 1s - loss: 509.1654 - loglik: -4.9719e+02 - logprior: -1.1980e+01
Epoch 4/10
10/10 - 1s - loss: 496.5426 - loglik: -4.9369e+02 - logprior: -2.8543e+00
Epoch 5/10
10/10 - 1s - loss: 486.8315 - loglik: -4.8812e+02 - logprior: 1.2933
Epoch 6/10
10/10 - 1s - loss: 475.9271 - loglik: -4.7904e+02 - logprior: 3.1135
Epoch 7/10
10/10 - 1s - loss: 459.6113 - loglik: -4.6336e+02 - logprior: 3.7475
Epoch 8/10
10/10 - 1s - loss: 440.3840 - loglik: -4.4427e+02 - logprior: 3.8833
Epoch 9/10
10/10 - 1s - loss: 422.0800 - loglik: -4.2640e+02 - logprior: 4.3229
Epoch 10/10
10/10 - 1s - loss: 404.8379 - loglik: -4.0951e+02 - logprior: 4.6718
Fitted a model with MAP estimate = -395.9860
Time for alignment: 33.9870
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 659.1701 - loglik: -5.4600e+02 - logprior: -1.1317e+02
Epoch 2/10
10/10 - 1s - loss: 555.9457 - loglik: -5.2857e+02 - logprior: -2.7380e+01
Epoch 3/10
10/10 - 1s - loss: 525.3506 - loglik: -5.1542e+02 - logprior: -9.9280e+00
Epoch 4/10
10/10 - 1s - loss: 508.0221 - loglik: -5.0458e+02 - logprior: -3.4401e+00
Epoch 5/10
10/10 - 1s - loss: 493.8530 - loglik: -4.9342e+02 - logprior: -4.3089e-01
Epoch 6/10
10/10 - 1s - loss: 477.1697 - loglik: -4.7810e+02 - logprior: 0.9274
Epoch 7/10
10/10 - 1s - loss: 454.1956 - loglik: -4.5480e+02 - logprior: 0.6056
Epoch 8/10
10/10 - 1s - loss: 433.5047 - loglik: -4.3165e+02 - logprior: -1.8478e+00
Epoch 9/10
10/10 - 1s - loss: 421.4808 - loglik: -4.1965e+02 - logprior: -1.8311e+00
Epoch 10/10
10/10 - 1s - loss: 411.3155 - loglik: -4.0988e+02 - logprior: -1.4346e+00
Fitted a model with MAP estimate = -407.4513
expansions: []
discards: [41 42 43 44 45 48 49 50 51 52 53 54 55 56 57 58 59 60 61]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 671.1918 - loglik: -5.6708e+02 - logprior: -1.0412e+02
Epoch 2/2
10/10 - 1s - loss: 573.0103 - loglik: -5.4666e+02 - logprior: -2.6348e+01
Fitted a model with MAP estimate = -555.3075
expansions: [(0, 51), (57, 20)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 655.4925 - loglik: -5.4022e+02 - logprior: -1.1527e+02
Epoch 2/2
10/10 - 1s - loss: 549.4662 - loglik: -5.2196e+02 - logprior: -2.7507e+01
Fitted a model with MAP estimate = -531.0226
expansions: [(0, 5), (54, 9)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 93 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 646.1663 - loglik: -5.1673e+02 - logprior: -1.2943e+02
Epoch 2/10
10/10 - 1s - loss: 537.0270 - loglik: -5.0179e+02 - logprior: -3.5236e+01
Epoch 3/10
10/10 - 1s - loss: 509.6051 - loglik: -4.9750e+02 - logprior: -1.2102e+01
Epoch 4/10
10/10 - 1s - loss: 496.3943 - loglik: -4.9357e+02 - logprior: -2.8257e+00
Epoch 5/10
10/10 - 1s - loss: 486.1720 - loglik: -4.8752e+02 - logprior: 1.3526
Epoch 6/10
10/10 - 1s - loss: 474.6194 - loglik: -4.7802e+02 - logprior: 3.4027
Epoch 7/10
10/10 - 1s - loss: 456.7118 - loglik: -4.6079e+02 - logprior: 4.0759
Epoch 8/10
10/10 - 1s - loss: 435.7106 - loglik: -4.3991e+02 - logprior: 4.2050
Epoch 9/10
10/10 - 1s - loss: 417.7452 - loglik: -4.2235e+02 - logprior: 4.6095
Epoch 10/10
10/10 - 1s - loss: 401.0394 - loglik: -4.0596e+02 - logprior: 4.9253
Fitted a model with MAP estimate = -392.1709
Time for alignment: 39.8424
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 659.1701 - loglik: -5.4600e+02 - logprior: -1.1317e+02
Epoch 2/10
10/10 - 1s - loss: 555.9457 - loglik: -5.2857e+02 - logprior: -2.7380e+01
Epoch 3/10
10/10 - 1s - loss: 525.3506 - loglik: -5.1542e+02 - logprior: -9.9280e+00
Epoch 4/10
10/10 - 1s - loss: 508.0221 - loglik: -5.0458e+02 - logprior: -3.4401e+00
Epoch 5/10
10/10 - 1s - loss: 493.8530 - loglik: -4.9342e+02 - logprior: -4.3089e-01
Epoch 6/10
10/10 - 1s - loss: 477.1698 - loglik: -4.7810e+02 - logprior: 0.9274
Epoch 7/10
10/10 - 1s - loss: 454.1959 - loglik: -4.5480e+02 - logprior: 0.6055
Epoch 8/10
10/10 - 1s - loss: 433.5046 - loglik: -4.3165e+02 - logprior: -1.8478e+00
Epoch 9/10
10/10 - 1s - loss: 421.4820 - loglik: -4.1965e+02 - logprior: -1.8311e+00
Epoch 10/10
10/10 - 1s - loss: 411.3136 - loglik: -4.0988e+02 - logprior: -1.4345e+00
Fitted a model with MAP estimate = -407.4454
expansions: []
discards: [41 42 43 44 45 48 49 50 51 52 53 54 55 56 57 58 59 60 61]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 671.1919 - loglik: -5.6708e+02 - logprior: -1.0412e+02
Epoch 2/2
10/10 - 1s - loss: 573.0103 - loglik: -5.4666e+02 - logprior: -2.6348e+01
Fitted a model with MAP estimate = -555.3078
expansions: [(0, 52), (57, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 655.4206 - loglik: -5.4015e+02 - logprior: -1.1527e+02
Epoch 2/2
10/10 - 1s - loss: 549.3509 - loglik: -5.2188e+02 - logprior: -2.7475e+01
Fitted a model with MAP estimate = -531.0554
expansions: [(55, 9)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 616.4919 - loglik: -5.1616e+02 - logprior: -1.0033e+02
Epoch 2/10
10/10 - 1s - loss: 524.4944 - loglik: -4.9993e+02 - logprior: -2.4564e+01
Epoch 3/10
10/10 - 1s - loss: 504.4656 - loglik: -4.9570e+02 - logprior: -8.7613e+00
Epoch 4/10
10/10 - 1s - loss: 494.3976 - loglik: -4.9238e+02 - logprior: -2.0186e+00
Epoch 5/10
10/10 - 1s - loss: 485.7672 - loglik: -4.8733e+02 - logprior: 1.5643
Epoch 6/10
10/10 - 1s - loss: 476.0766 - loglik: -4.7940e+02 - logprior: 3.3206
Epoch 7/10
10/10 - 1s - loss: 460.6954 - loglik: -4.6461e+02 - logprior: 3.9136
Epoch 8/10
10/10 - 1s - loss: 441.6773 - loglik: -4.4565e+02 - logprior: 3.9705
Epoch 9/10
10/10 - 1s - loss: 425.2700 - loglik: -4.2945e+02 - logprior: 4.1840
Epoch 10/10
10/10 - 1s - loss: 408.3422 - loglik: -4.1295e+02 - logprior: 4.6127
Fitted a model with MAP estimate = -397.8103
Time for alignment: 37.1405
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 659.1701 - loglik: -5.4600e+02 - logprior: -1.1317e+02
Epoch 2/10
10/10 - 1s - loss: 555.9457 - loglik: -5.2857e+02 - logprior: -2.7380e+01
Epoch 3/10
10/10 - 1s - loss: 525.3506 - loglik: -5.1542e+02 - logprior: -9.9280e+00
Epoch 4/10
10/10 - 1s - loss: 508.0221 - loglik: -5.0458e+02 - logprior: -3.4401e+00
Epoch 5/10
10/10 - 1s - loss: 493.8530 - loglik: -4.9342e+02 - logprior: -4.3089e-01
Epoch 6/10
10/10 - 1s - loss: 477.1698 - loglik: -4.7810e+02 - logprior: 0.9274
Epoch 7/10
10/10 - 1s - loss: 454.1951 - loglik: -4.5480e+02 - logprior: 0.6055
Epoch 8/10
10/10 - 1s - loss: 433.5041 - loglik: -4.3165e+02 - logprior: -1.8479e+00
Epoch 9/10
10/10 - 1s - loss: 421.4807 - loglik: -4.1965e+02 - logprior: -1.8311e+00
Epoch 10/10
10/10 - 1s - loss: 411.3138 - loglik: -4.0988e+02 - logprior: -1.4345e+00
Fitted a model with MAP estimate = -407.4478
expansions: []
discards: [41 42 43 44 45 48 49 50 51 52 53 54 55 56 57 58 59 60 61]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 671.1919 - loglik: -5.6708e+02 - logprior: -1.0412e+02
Epoch 2/2
10/10 - 1s - loss: 573.0103 - loglik: -5.4666e+02 - logprior: -2.6348e+01
Fitted a model with MAP estimate = -555.3078
expansions: [(0, 51), (57, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48]
Re-initialized the encoder parameters.
Fitting a model of length 78 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 655.4240 - loglik: -5.4019e+02 - logprior: -1.1523e+02
Epoch 2/2
10/10 - 1s - loss: 549.3182 - loglik: -5.2176e+02 - logprior: -2.7556e+01
Fitted a model with MAP estimate = -530.5846
expansions: [(0, 4), (54, 9)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 91 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 645.0284 - loglik: -5.1544e+02 - logprior: -1.2959e+02
Epoch 2/10
10/10 - 1s - loss: 535.3286 - loglik: -5.0023e+02 - logprior: -3.5097e+01
Epoch 3/10
10/10 - 1s - loss: 509.1656 - loglik: -4.9719e+02 - logprior: -1.1980e+01
Epoch 4/10
10/10 - 1s - loss: 496.5428 - loglik: -4.9369e+02 - logprior: -2.8542e+00
Epoch 5/10
10/10 - 1s - loss: 486.8315 - loglik: -4.8812e+02 - logprior: 1.2933
Epoch 6/10
10/10 - 1s - loss: 475.9276 - loglik: -4.7904e+02 - logprior: 3.1135
Epoch 7/10
10/10 - 1s - loss: 459.6119 - loglik: -4.6336e+02 - logprior: 3.7475
Epoch 8/10
10/10 - 1s - loss: 440.3862 - loglik: -4.4427e+02 - logprior: 3.8833
Epoch 9/10
10/10 - 1s - loss: 422.0838 - loglik: -4.2640e+02 - logprior: 4.3229
Epoch 10/10
10/10 - 1s - loss: 404.8385 - loglik: -4.0951e+02 - logprior: 4.6719
Fitted a model with MAP estimate = -395.9977
Time for alignment: 36.4508
Fitting a model of length 76 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 659.1701 - loglik: -5.4600e+02 - logprior: -1.1317e+02
Epoch 2/10
10/10 - 1s - loss: 555.9457 - loglik: -5.2857e+02 - logprior: -2.7380e+01
Epoch 3/10
10/10 - 1s - loss: 525.3506 - loglik: -5.1542e+02 - logprior: -9.9280e+00
Epoch 4/10
10/10 - 1s - loss: 508.0221 - loglik: -5.0458e+02 - logprior: -3.4401e+00
Epoch 5/10
10/10 - 1s - loss: 493.8530 - loglik: -4.9342e+02 - logprior: -4.3089e-01
Epoch 6/10
10/10 - 1s - loss: 477.1698 - loglik: -4.7810e+02 - logprior: 0.9274
Epoch 7/10
10/10 - 1s - loss: 454.1954 - loglik: -4.5480e+02 - logprior: 0.6056
Epoch 8/10
10/10 - 1s - loss: 433.5055 - loglik: -4.3166e+02 - logprior: -1.8478e+00
Epoch 9/10
10/10 - 1s - loss: 421.4828 - loglik: -4.1965e+02 - logprior: -1.8311e+00
Epoch 10/10
10/10 - 1s - loss: 411.3139 - loglik: -4.0988e+02 - logprior: -1.4345e+00
Fitted a model with MAP estimate = -407.4436
expansions: []
discards: [41 42 43 44 45 48 49 50 51 52 53 54 55 56 57 58 59 60 61]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 671.1919 - loglik: -5.6708e+02 - logprior: -1.0412e+02
Epoch 2/2
10/10 - 1s - loss: 573.0103 - loglik: -5.4666e+02 - logprior: -2.6348e+01
Fitted a model with MAP estimate = -555.3077
expansions: [(0, 52), (57, 19)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48]
Re-initialized the encoder parameters.
Fitting a model of length 79 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 655.4206 - loglik: -5.4015e+02 - logprior: -1.1527e+02
Epoch 2/2
10/10 - 1s - loss: 549.3508 - loglik: -5.2188e+02 - logprior: -2.7475e+01
Fitted a model with MAP estimate = -531.0552
expansions: [(55, 9)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 385 sequences.
Batch size= 385 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 616.4918 - loglik: -5.1616e+02 - logprior: -1.0033e+02
Epoch 2/10
10/10 - 1s - loss: 524.4946 - loglik: -4.9993e+02 - logprior: -2.4564e+01
Epoch 3/10
10/10 - 1s - loss: 504.4657 - loglik: -4.9570e+02 - logprior: -8.7614e+00
Epoch 4/10
10/10 - 1s - loss: 494.3976 - loglik: -4.9238e+02 - logprior: -2.0186e+00
Epoch 5/10
10/10 - 1s - loss: 485.7670 - loglik: -4.8733e+02 - logprior: 1.5643
Epoch 6/10
10/10 - 1s - loss: 476.0768 - loglik: -4.7940e+02 - logprior: 3.3206
Epoch 7/10
10/10 - 1s - loss: 460.6960 - loglik: -4.6461e+02 - logprior: 3.9135
Epoch 8/10
10/10 - 1s - loss: 441.6765 - loglik: -4.4564e+02 - logprior: 3.9706
Epoch 9/10
10/10 - 1s - loss: 425.2723 - loglik: -4.2945e+02 - logprior: 4.1839
Epoch 10/10
10/10 - 1s - loss: 408.3405 - loglik: -4.1295e+02 - logprior: 4.6126
Fitted a model with MAP estimate = -397.8139
Time for alignment: 34.2862
Computed alignments with likelihoods: ['-395.9860', '-392.1709', '-397.8103', '-395.9977', '-397.8139']
Best model has likelihood: -392.1709  (prior= 4.9961 )
time for generating output: 0.1343
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/cyt3.projection.fasta
SP score = 0.206172028890348
Training of 5 independent models on file mofe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3b832700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9055e7460>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea00d9c1c0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 23s - loss: 2014.8564 - loglik: -2.0073e+03 - logprior: -7.5447e+00
Epoch 2/10
19/19 - 17s - loss: 1892.4132 - loglik: -1.8936e+03 - logprior: 1.1453
Epoch 3/10
19/19 - 17s - loss: 1834.3760 - loglik: -1.8349e+03 - logprior: 0.4898
Epoch 4/10
19/19 - 17s - loss: 1821.2744 - loglik: -1.8217e+03 - logprior: 0.4633
Epoch 5/10
19/19 - 17s - loss: 1812.7352 - loglik: -1.8133e+03 - logprior: 0.5774
Epoch 6/10
19/19 - 17s - loss: 1807.2380 - loglik: -1.8077e+03 - logprior: 0.4937
Epoch 7/10
19/19 - 17s - loss: 1791.7438 - loglik: -1.7922e+03 - logprior: 0.4525
Epoch 8/10
19/19 - 17s - loss: 1793.3927 - loglik: -1.7937e+03 - logprior: 0.2799
Fitted a model with MAP estimate = -1785.4990
expansions: [(30, 1), (68, 1), (69, 1), (98, 2), (99, 2), (114, 1), (115, 1), (120, 1), (125, 2), (126, 3), (127, 4), (145, 1), (148, 1), (159, 1), (166, 1), (168, 1), (169, 1), (170, 1), (171, 1), (174, 1), (177, 2), (178, 2), (179, 1), (180, 1), (189, 1), (191, 1), (199, 1), (206, 1), (213, 2), (222, 1), (223, 2), (225, 1), (239, 2), (241, 2), (265, 4), (283, 1), (302, 3), (303, 1), (311, 3), (312, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 1837.8826 - loglik: -1.8271e+03 - logprior: -1.0819e+01
Epoch 2/2
19/19 - 22s - loss: 1792.4685 - loglik: -1.7902e+03 - logprior: -2.2392e+00
Fitted a model with MAP estimate = -1785.8729
expansions: [(0, 3), (283, 1), (313, 4)]
discards: [  0  29 100 101 145 205 287]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 1794.8347 - loglik: -1.7886e+03 - logprior: -6.2802e+00
Epoch 2/2
19/19 - 22s - loss: 1770.0670 - loglik: -1.7721e+03 - logprior: 2.0688
Fitted a model with MAP estimate = -1775.0186
expansions: [(141, 1)]
discards: [  0   1 143 312]
Re-initialized the encoder parameters.
Fitting a model of length 378 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 1793.9342 - loglik: -1.7844e+03 - logprior: -9.5336e+00
Epoch 2/10
19/19 - 22s - loss: 1784.6036 - loglik: -1.7846e+03 - logprior: 0.0181
Epoch 3/10
19/19 - 22s - loss: 1772.7603 - loglik: -1.7765e+03 - logprior: 3.7196
Epoch 4/10
19/19 - 22s - loss: 1770.7797 - loglik: -1.7754e+03 - logprior: 4.5732
Epoch 5/10
19/19 - 22s - loss: 1767.3765 - loglik: -1.7722e+03 - logprior: 4.8599
Epoch 6/10
19/19 - 22s - loss: 1755.3344 - loglik: -1.7605e+03 - logprior: 5.1290
Epoch 7/10
19/19 - 22s - loss: 1760.2992 - loglik: -1.7657e+03 - logprior: 5.3931
Fitted a model with MAP estimate = -1754.7693
Time for alignment: 461.1578
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 2018.3038 - loglik: -2.0108e+03 - logprior: -7.5341e+00
Epoch 2/10
19/19 - 17s - loss: 1888.8792 - loglik: -1.8899e+03 - logprior: 1.0256
Epoch 3/10
19/19 - 17s - loss: 1844.4672 - loglik: -1.8448e+03 - logprior: 0.2957
Epoch 4/10
19/19 - 17s - loss: 1820.2990 - loglik: -1.8206e+03 - logprior: 0.3086
Epoch 5/10
19/19 - 17s - loss: 1818.2626 - loglik: -1.8188e+03 - logprior: 0.5043
Epoch 6/10
19/19 - 17s - loss: 1811.1252 - loglik: -1.8116e+03 - logprior: 0.4738
Epoch 7/10
19/19 - 17s - loss: 1806.8696 - loglik: -1.8073e+03 - logprior: 0.4175
Epoch 8/10
19/19 - 17s - loss: 1796.3470 - loglik: -1.7967e+03 - logprior: 0.3559
Epoch 9/10
19/19 - 17s - loss: 1790.1285 - loglik: -1.7898e+03 - logprior: -2.8674e-01
Epoch 10/10
19/19 - 17s - loss: 1766.7412 - loglik: -1.7662e+03 - logprior: -4.9147e-01
Fitted a model with MAP estimate = -1750.6854
expansions: [(14, 2), (32, 1), (57, 1), (67, 1), (96, 1), (97, 1), (98, 2), (105, 1), (111, 1), (112, 2), (116, 1), (117, 1), (118, 1), (120, 2), (121, 2), (123, 5), (141, 1), (145, 1), (156, 1), (163, 1), (164, 2), (165, 2), (168, 1), (176, 1), (179, 1), (180, 1), (188, 1), (189, 1), (190, 1), (198, 1), (204, 1), (218, 3), (219, 1), (220, 1), (221, 2), (223, 1), (236, 1), (237, 1), (238, 2), (265, 5), (295, 4), (301, 1), (302, 2), (303, 1), (311, 3), (314, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 390 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 1945.6749 - loglik: -1.9349e+03 - logprior: -1.0768e+01
Epoch 2/2
19/19 - 23s - loss: 1840.3411 - loglik: -1.8381e+03 - logprior: -2.2703e+00
Fitted a model with MAP estimate = -1822.3908
expansions: [(146, 2), (204, 1), (227, 1), (252, 2), (390, 51)]
discards: [  0   1   2   7  13  14  38  59 105 121 137 140 147 148 158 159 160 161
 193 225 254 262 285 286 287 288 289 310 311 312 313 314 315 316 317 318
 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336
 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354
 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372
 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389]
Re-initialized the encoder parameters.
Fitting a model of length 340 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 22s - loss: 1864.0658 - loglik: -1.8535e+03 - logprior: -1.0580e+01
Epoch 2/2
19/19 - 19s - loss: 1828.3629 - loglik: -1.8266e+03 - logprior: -1.7244e+00
Fitted a model with MAP estimate = -1820.3678
expansions: [(0, 27), (4, 1), (9, 1), (32, 1), (146, 3), (240, 1), (248, 1), (269, 3), (287, 22), (318, 1)]
discards: [  0 236]
Re-initialized the encoder parameters.
Fitting a model of length 399 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 27s - loss: 1819.6891 - loglik: -1.8132e+03 - logprior: -6.4854e+00
Epoch 2/10
19/19 - 24s - loss: 1792.1357 - loglik: -1.7943e+03 - logprior: 2.1735
Epoch 3/10
19/19 - 24s - loss: 1786.9736 - loglik: -1.7904e+03 - logprior: 3.4311
Epoch 4/10
19/19 - 24s - loss: 1781.9813 - loglik: -1.7859e+03 - logprior: 3.9568
Epoch 5/10
19/19 - 24s - loss: 1791.4138 - loglik: -1.7958e+03 - logprior: 4.3967
Fitted a model with MAP estimate = -1777.8322
Time for alignment: 449.5152
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 2021.0477 - loglik: -2.0135e+03 - logprior: -7.5690e+00
Epoch 2/10
19/19 - 17s - loss: 1891.2568 - loglik: -1.8925e+03 - logprior: 1.2532
Epoch 3/10
19/19 - 17s - loss: 1839.9248 - loglik: -1.8405e+03 - logprior: 0.5883
Epoch 4/10
19/19 - 17s - loss: 1831.8137 - loglik: -1.8323e+03 - logprior: 0.4688
Epoch 5/10
19/19 - 17s - loss: 1805.1232 - loglik: -1.8056e+03 - logprior: 0.5100
Epoch 6/10
19/19 - 17s - loss: 1814.7633 - loglik: -1.8153e+03 - logprior: 0.5073
Fitted a model with MAP estimate = -1806.9153
expansions: [(65, 1), (99, 1), (108, 1), (115, 1), (116, 1), (119, 1), (120, 3), (121, 1), (123, 6), (144, 1), (145, 1), (147, 2), (165, 2), (167, 1), (168, 1), (169, 2), (170, 1), (173, 1), (176, 2), (177, 1), (178, 1), (180, 1), (190, 3), (198, 1), (204, 1), (207, 1), (218, 1), (220, 1), (221, 1), (222, 1), (224, 1), (227, 1), (242, 3), (264, 1), (265, 3), (266, 1), (275, 1), (302, 4), (303, 1), (311, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 381 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 27s - loss: 1828.3425 - loglik: -1.8177e+03 - logprior: -1.0670e+01
Epoch 2/2
19/19 - 23s - loss: 1793.0989 - loglik: -1.7908e+03 - logprior: -2.2606e+00
Fitted a model with MAP estimate = -1788.4217
expansions: [(0, 2), (314, 1), (375, 1)]
discards: [  0 127 161 192 203]
Re-initialized the encoder parameters.
Fitting a model of length 380 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 25s - loss: 1789.6547 - loglik: -1.7832e+03 - logprior: -6.4989e+00
Epoch 2/2
19/19 - 23s - loss: 1784.7504 - loglik: -1.7872e+03 - logprior: 2.4031
Fitted a model with MAP estimate = -1776.0529
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 379 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 1793.4712 - loglik: -1.7840e+03 - logprior: -9.4509e+00
Epoch 2/10
19/19 - 22s - loss: 1781.4110 - loglik: -1.7815e+03 - logprior: 0.0947
Epoch 3/10
19/19 - 22s - loss: 1779.4825 - loglik: -1.7834e+03 - logprior: 3.9243
Epoch 4/10
19/19 - 22s - loss: 1777.0715 - loglik: -1.7817e+03 - logprior: 4.6143
Epoch 5/10
19/19 - 22s - loss: 1762.1918 - loglik: -1.7672e+03 - logprior: 4.9691
Epoch 6/10
19/19 - 22s - loss: 1763.6127 - loglik: -1.7688e+03 - logprior: 5.2290
Fitted a model with MAP estimate = -1762.0040
Time for alignment: 402.2527
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 20s - loss: 2019.7391 - loglik: -2.0122e+03 - logprior: -7.5188e+00
Epoch 2/10
19/19 - 17s - loss: 1890.0045 - loglik: -1.8910e+03 - logprior: 0.9955
Epoch 3/10
19/19 - 17s - loss: 1837.6978 - loglik: -1.8381e+03 - logprior: 0.3906
Epoch 4/10
19/19 - 17s - loss: 1823.9402 - loglik: -1.8241e+03 - logprior: 0.1642
Epoch 5/10
19/19 - 17s - loss: 1814.2350 - loglik: -1.8145e+03 - logprior: 0.2420
Epoch 6/10
19/19 - 17s - loss: 1810.6643 - loglik: -1.8109e+03 - logprior: 0.2165
Epoch 7/10
19/19 - 17s - loss: 1801.5087 - loglik: -1.8016e+03 - logprior: 0.0575
Epoch 8/10
19/19 - 17s - loss: 1790.3300 - loglik: -1.7903e+03 - logprior: -3.2638e-02
Epoch 9/10
19/19 - 17s - loss: 1781.0027 - loglik: -1.7807e+03 - logprior: -2.5922e-01
Epoch 10/10
19/19 - 17s - loss: 1758.3826 - loglik: -1.7576e+03 - logprior: -7.6315e-01
Fitted a model with MAP estimate = -1742.1049
expansions: [(14, 1), (33, 1), (38, 1), (67, 1), (95, 2), (96, 1), (113, 2), (114, 1), (118, 1), (119, 1), (120, 1), (123, 1), (124, 3), (125, 5), (143, 1), (144, 1), (146, 1), (157, 1), (164, 1), (166, 3), (169, 1), (176, 1), (177, 1), (178, 2), (189, 1), (190, 1), (191, 1), (195, 1), (198, 1), (201, 1), (218, 1), (220, 1), (221, 1), (222, 2), (224, 1), (242, 3), (260, 1), (263, 2), (264, 3), (273, 1), (281, 1), (282, 1), (296, 2), (302, 2), (303, 1), (311, 3), (312, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 387 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 1956.9346 - loglik: -1.9456e+03 - logprior: -1.1383e+01
Epoch 2/2
19/19 - 23s - loss: 1848.0472 - loglik: -1.8453e+03 - logprior: -2.7210e+00
Fitted a model with MAP estimate = -1822.2770
expansions: [(103, 1), (228, 2)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  28  29  30  31  32
  34  39  40  41  42 100 101 121 136 146 147 148 149 150 151 210 224 226
 267 306 307 308 309 310 314 315 316 317 318 319 320 321 322 323 324 325
 326 327 328 329 330 331 332 349 350 351 352 353 354 355 359 360 361 363
 364 365 366 367 368]
Re-initialized the encoder parameters.
Fitting a model of length 313 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 20s - loss: 1861.9979 - loglik: -1.8512e+03 - logprior: -1.0813e+01
Epoch 2/2
19/19 - 16s - loss: 1833.1422 - loglik: -1.8336e+03 - logprior: 0.4183
Fitted a model with MAP estimate = -1825.5769
expansions: [(0, 17), (16, 4), (20, 1), (21, 3), (114, 2), (116, 3), (117, 1), (232, 1), (271, 14), (272, 5), (273, 2), (275, 1), (289, 6), (291, 5), (294, 7)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 1816.3174 - loglik: -1.8099e+03 - logprior: -6.4212e+00
Epoch 2/10
19/19 - 23s - loss: 1783.3810 - loglik: -1.7861e+03 - logprior: 2.7096
Epoch 3/10
19/19 - 23s - loss: 1772.5288 - loglik: -1.7762e+03 - logprior: 3.6621
Epoch 4/10
19/19 - 23s - loss: 1773.5773 - loglik: -1.7776e+03 - logprior: 3.9810
Fitted a model with MAP estimate = -1769.1256
Time for alignment: 413.0501
Fitting a model of length 320 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 22s - loss: 2018.0751 - loglik: -2.0105e+03 - logprior: -7.5490e+00
Epoch 2/10
19/19 - 17s - loss: 1893.4111 - loglik: -1.8946e+03 - logprior: 1.1792
Epoch 3/10
19/19 - 17s - loss: 1831.5099 - loglik: -1.8320e+03 - logprior: 0.5140
Epoch 4/10
19/19 - 17s - loss: 1822.8090 - loglik: -1.8234e+03 - logprior: 0.5505
Epoch 5/10
19/19 - 17s - loss: 1814.0511 - loglik: -1.8147e+03 - logprior: 0.6255
Epoch 6/10
19/19 - 17s - loss: 1808.7307 - loglik: -1.8093e+03 - logprior: 0.5700
Epoch 7/10
19/19 - 17s - loss: 1797.5450 - loglik: -1.7982e+03 - logprior: 0.6232
Epoch 8/10
19/19 - 17s - loss: 1798.3864 - loglik: -1.7989e+03 - logprior: 0.5050
Fitted a model with MAP estimate = -1790.0372
expansions: [(32, 1), (68, 1), (98, 2), (99, 3), (113, 2), (114, 2), (117, 1), (118, 1), (119, 1), (122, 5), (124, 3), (125, 2), (141, 1), (142, 1), (144, 1), (163, 1), (165, 2), (167, 1), (168, 1), (175, 2), (176, 1), (177, 1), (188, 1), (189, 2), (190, 1), (195, 1), (197, 1), (204, 1), (211, 1), (221, 1), (222, 2), (224, 1), (237, 2), (238, 2), (265, 9), (295, 5), (301, 5), (303, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 391 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 1842.0886 - loglik: -1.8313e+03 - logprior: -1.0828e+01
Epoch 2/2
19/19 - 24s - loss: 1795.9899 - loglik: -1.7933e+03 - logprior: -2.7298e+00
Fitted a model with MAP estimate = -1784.3184
expansions: [(0, 3), (140, 1)]
discards: [  0 102 103 206 223 318 319 320 356 357]
Re-initialized the encoder parameters.
Fitting a model of length 385 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 26s - loss: 1795.1971 - loglik: -1.7888e+03 - logprior: -6.4358e+00
Epoch 2/2
19/19 - 23s - loss: 1776.3506 - loglik: -1.7788e+03 - logprior: 2.4375
Fitted a model with MAP estimate = -1775.2406
expansions: [(363, 2)]
discards: [  0   1 146 147 148]
Re-initialized the encoder parameters.
Fitting a model of length 382 on 2567 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 26s - loss: 1794.2572 - loglik: -1.7846e+03 - logprior: -9.6144e+00
Epoch 2/10
19/19 - 22s - loss: 1781.8951 - loglik: -1.7819e+03 - logprior: 0.0046
Epoch 3/10
19/19 - 23s - loss: 1772.0144 - loglik: -1.7756e+03 - logprior: 3.5853
Epoch 4/10
19/19 - 22s - loss: 1773.8035 - loglik: -1.7783e+03 - logprior: 4.5105
Fitted a model with MAP estimate = -1769.4012
Time for alignment: 397.5006
Computed alignments with likelihoods: ['-1754.7693', '-1750.6854', '-1762.0040', '-1742.1049', '-1769.4012']
Best model has likelihood: -1742.1049  (prior= -1.7744 )
time for generating output: 0.3954
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/mofe.projection.fasta
SP score = 0.3323317307692308
Training of 5 independent models on file ghf22.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bbf15e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea5e0954c0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c29e340>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 687.9759 - loglik: -6.3171e+02 - logprior: -5.6266e+01
Epoch 2/10
10/10 - 1s - loss: 605.4421 - loglik: -5.9205e+02 - logprior: -1.3392e+01
Epoch 3/10
10/10 - 1s - loss: 560.5901 - loglik: -5.5454e+02 - logprior: -6.0478e+00
Epoch 4/10
10/10 - 1s - loss: 531.5651 - loglik: -5.2767e+02 - logprior: -3.8962e+00
Epoch 5/10
10/10 - 1s - loss: 519.1871 - loglik: -5.1642e+02 - logprior: -2.7656e+00
Epoch 6/10
10/10 - 1s - loss: 512.3376 - loglik: -5.1032e+02 - logprior: -2.0221e+00
Epoch 7/10
10/10 - 1s - loss: 510.1396 - loglik: -5.0846e+02 - logprior: -1.6761e+00
Epoch 8/10
10/10 - 1s - loss: 507.9724 - loglik: -5.0657e+02 - logprior: -1.3993e+00
Epoch 9/10
10/10 - 1s - loss: 507.7389 - loglik: -5.0652e+02 - logprior: -1.2184e+00
Epoch 10/10
10/10 - 1s - loss: 506.6641 - loglik: -5.0560e+02 - logprior: -1.0601e+00
Fitted a model with MAP estimate = -506.8532
expansions: [(13, 3), (16, 1), (17, 1), (18, 4), (31, 1), (32, 1), (35, 1), (54, 2), (56, 2), (57, 1), (60, 1), (62, 1), (76, 1), (77, 4), (83, 1), (86, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 569.4796 - loglik: -5.0601e+02 - logprior: -6.3472e+01
Epoch 2/2
10/10 - 1s - loss: 513.7829 - loglik: -4.8910e+02 - logprior: -2.4687e+01
Fitted a model with MAP estimate = -504.8279
expansions: [(0, 2)]
discards: [ 0 98]
Re-initialized the encoder parameters.
Fitting a model of length 124 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 538.1126 - loglik: -4.8801e+02 - logprior: -5.0103e+01
Epoch 2/2
10/10 - 1s - loss: 492.7969 - loglik: -4.8156e+02 - logprior: -1.1233e+01
Fitted a model with MAP estimate = -486.9606
expansions: []
discards: [ 0 19]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 548.7169 - loglik: -4.8790e+02 - logprior: -6.0815e+01
Epoch 2/10
10/10 - 1s - loss: 502.0720 - loglik: -4.8453e+02 - logprior: -1.7540e+01
Epoch 3/10
10/10 - 1s - loss: 487.8573 - loglik: -4.8331e+02 - logprior: -4.5470e+00
Epoch 4/10
10/10 - 1s - loss: 483.0810 - loglik: -4.8301e+02 - logprior: -7.1314e-02
Epoch 5/10
10/10 - 1s - loss: 480.3997 - loglik: -4.8217e+02 - logprior: 1.7751
Epoch 6/10
10/10 - 1s - loss: 478.4291 - loglik: -4.8125e+02 - logprior: 2.8254
Epoch 7/10
10/10 - 1s - loss: 477.7720 - loglik: -4.8140e+02 - logprior: 3.6334
Epoch 8/10
10/10 - 1s - loss: 477.3194 - loglik: -4.8157e+02 - logprior: 4.2484
Epoch 9/10
10/10 - 1s - loss: 476.8688 - loglik: -4.8154e+02 - logprior: 4.6693
Epoch 10/10
10/10 - 1s - loss: 476.2490 - loglik: -4.8125e+02 - logprior: 5.0012
Fitted a model with MAP estimate = -476.2890
Time for alignment: 46.7778
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 688.2118 - loglik: -6.3195e+02 - logprior: -5.6265e+01
Epoch 2/10
10/10 - 1s - loss: 604.6757 - loglik: -5.9129e+02 - logprior: -1.3383e+01
Epoch 3/10
10/10 - 1s - loss: 560.1247 - loglik: -5.5413e+02 - logprior: -5.9973e+00
Epoch 4/10
10/10 - 1s - loss: 532.3577 - loglik: -5.2852e+02 - logprior: -3.8351e+00
Epoch 5/10
10/10 - 1s - loss: 520.1058 - loglik: -5.1739e+02 - logprior: -2.7118e+00
Epoch 6/10
10/10 - 1s - loss: 514.0023 - loglik: -5.1195e+02 - logprior: -2.0521e+00
Epoch 7/10
10/10 - 1s - loss: 510.0860 - loglik: -5.0837e+02 - logprior: -1.7139e+00
Epoch 8/10
10/10 - 1s - loss: 509.4551 - loglik: -5.0794e+02 - logprior: -1.5158e+00
Epoch 9/10
10/10 - 1s - loss: 508.1747 - loglik: -5.0686e+02 - logprior: -1.3184e+00
Epoch 10/10
10/10 - 1s - loss: 508.1005 - loglik: -5.0695e+02 - logprior: -1.1500e+00
Fitted a model with MAP estimate = -507.6914
expansions: [(11, 4), (17, 3), (19, 1), (32, 1), (33, 1), (34, 2), (53, 1), (54, 2), (55, 2), (56, 1), (57, 1), (60, 1), (65, 1), (76, 1), (77, 4), (83, 1), (86, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 571.6083 - loglik: -5.0819e+02 - logprior: -6.3417e+01
Epoch 2/2
10/10 - 1s - loss: 514.6375 - loglik: -4.9001e+02 - logprior: -2.4629e+01
Fitted a model with MAP estimate = -504.7892
expansions: [(0, 2), (13, 1)]
discards: [ 0 20 21 22 44 70]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 539.5865 - loglik: -4.8960e+02 - logprior: -4.9990e+01
Epoch 2/2
10/10 - 1s - loss: 494.5505 - loglik: -4.8339e+02 - logprior: -1.1160e+01
Fitted a model with MAP estimate = -488.4501
expansions: []
discards: [ 0 98]
Re-initialized the encoder parameters.
Fitting a model of length 121 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 550.7067 - loglik: -4.9012e+02 - logprior: -6.0585e+01
Epoch 2/10
10/10 - 1s - loss: 503.4373 - loglik: -4.8610e+02 - logprior: -1.7335e+01
Epoch 3/10
10/10 - 1s - loss: 488.2270 - loglik: -4.8389e+02 - logprior: -4.3347e+00
Epoch 4/10
10/10 - 1s - loss: 484.2002 - loglik: -4.8431e+02 - logprior: 0.1131
Epoch 5/10
10/10 - 1s - loss: 481.8346 - loglik: -4.8378e+02 - logprior: 1.9495
Epoch 6/10
10/10 - 1s - loss: 479.7407 - loglik: -4.8273e+02 - logprior: 2.9898
Epoch 7/10
10/10 - 1s - loss: 479.0322 - loglik: -4.8283e+02 - logprior: 3.7959
Epoch 8/10
10/10 - 1s - loss: 478.7210 - loglik: -4.8312e+02 - logprior: 4.4023
Epoch 9/10
10/10 - 1s - loss: 478.7557 - loglik: -4.8358e+02 - logprior: 4.8231
Fitted a model with MAP estimate = -478.1357
Time for alignment: 42.1912
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 687.8622 - loglik: -6.3160e+02 - logprior: -5.6266e+01
Epoch 2/10
10/10 - 1s - loss: 604.6850 - loglik: -5.9130e+02 - logprior: -1.3380e+01
Epoch 3/10
10/10 - 1s - loss: 555.3347 - loglik: -5.4933e+02 - logprior: -6.0093e+00
Epoch 4/10
10/10 - 1s - loss: 529.4326 - loglik: -5.2560e+02 - logprior: -3.8295e+00
Epoch 5/10
10/10 - 1s - loss: 518.5248 - loglik: -5.1591e+02 - logprior: -2.6134e+00
Epoch 6/10
10/10 - 1s - loss: 511.3590 - loglik: -5.0933e+02 - logprior: -2.0317e+00
Epoch 7/10
10/10 - 1s - loss: 509.0260 - loglik: -5.0726e+02 - logprior: -1.7627e+00
Epoch 8/10
10/10 - 1s - loss: 506.7549 - loglik: -5.0514e+02 - logprior: -1.6145e+00
Epoch 9/10
10/10 - 1s - loss: 507.0556 - loglik: -5.0567e+02 - logprior: -1.3897e+00
Fitted a model with MAP estimate = -506.3721
expansions: [(11, 4), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (52, 1), (54, 2), (55, 1), (56, 1), (57, 1), (60, 1), (62, 1), (72, 1), (77, 3), (78, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 570.2192 - loglik: -5.0675e+02 - logprior: -6.3471e+01
Epoch 2/2
10/10 - 1s - loss: 514.8260 - loglik: -4.9035e+02 - logprior: -2.4472e+01
Fitted a model with MAP estimate = -506.1992
expansions: [(0, 2), (12, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 538.1654 - loglik: -4.8834e+02 - logprior: -4.9827e+01
Epoch 2/2
10/10 - 1s - loss: 493.2552 - loglik: -4.8236e+02 - logprior: -1.0891e+01
Fitted a model with MAP estimate = -487.0869
expansions: []
discards: [ 0 45 99]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 549.5852 - loglik: -4.8921e+02 - logprior: -6.0371e+01
Epoch 2/10
10/10 - 1s - loss: 501.5324 - loglik: -4.8443e+02 - logprior: -1.7101e+01
Epoch 3/10
10/10 - 1s - loss: 487.8215 - loglik: -4.8361e+02 - logprior: -4.2073e+00
Epoch 4/10
10/10 - 1s - loss: 483.0278 - loglik: -4.8325e+02 - logprior: 0.2207
Epoch 5/10
10/10 - 1s - loss: 480.4181 - loglik: -4.8246e+02 - logprior: 2.0466
Epoch 6/10
10/10 - 1s - loss: 478.3706 - loglik: -4.8145e+02 - logprior: 3.0763
Epoch 7/10
10/10 - 1s - loss: 478.3879 - loglik: -4.8227e+02 - logprior: 3.8844
Fitted a model with MAP estimate = -477.8222
Time for alignment: 38.3341
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 688.6317 - loglik: -6.3237e+02 - logprior: -5.6266e+01
Epoch 2/10
10/10 - 1s - loss: 605.3705 - loglik: -5.9199e+02 - logprior: -1.3376e+01
Epoch 3/10
10/10 - 1s - loss: 557.6089 - loglik: -5.5168e+02 - logprior: -5.9271e+00
Epoch 4/10
10/10 - 1s - loss: 528.2722 - loglik: -5.2455e+02 - logprior: -3.7245e+00
Epoch 5/10
10/10 - 1s - loss: 516.2809 - loglik: -5.1379e+02 - logprior: -2.4940e+00
Epoch 6/10
10/10 - 1s - loss: 513.0532 - loglik: -5.1121e+02 - logprior: -1.8441e+00
Epoch 7/10
10/10 - 1s - loss: 509.1612 - loglik: -5.0773e+02 - logprior: -1.4264e+00
Epoch 8/10
10/10 - 1s - loss: 508.0854 - loglik: -5.0685e+02 - logprior: -1.2347e+00
Epoch 9/10
10/10 - 1s - loss: 507.3119 - loglik: -5.0619e+02 - logprior: -1.1219e+00
Epoch 10/10
10/10 - 1s - loss: 506.5752 - loglik: -5.0559e+02 - logprior: -9.8082e-01
Fitted a model with MAP estimate = -506.5559
expansions: [(11, 4), (25, 1), (32, 1), (33, 1), (34, 2), (35, 1), (52, 1), (54, 2), (55, 2), (56, 1), (62, 1), (76, 1), (77, 3), (78, 1), (86, 1), (87, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 571.1502 - loglik: -5.0763e+02 - logprior: -6.3524e+01
Epoch 2/2
10/10 - 1s - loss: 516.5009 - loglik: -4.9187e+02 - logprior: -2.4628e+01
Fitted a model with MAP estimate = -507.6679
expansions: [(0, 2), (12, 1)]
discards: [ 0 96]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 539.8195 - loglik: -4.8991e+02 - logprior: -4.9909e+01
Epoch 2/2
10/10 - 1s - loss: 494.5363 - loglik: -4.8347e+02 - logprior: -1.1062e+01
Fitted a model with MAP estimate = -488.8075
expansions: [(69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 550.1221 - loglik: -4.8926e+02 - logprior: -6.0861e+01
Epoch 2/10
10/10 - 1s - loss: 502.8681 - loglik: -4.8455e+02 - logprior: -1.8318e+01
Epoch 3/10
10/10 - 1s - loss: 488.9448 - loglik: -4.8427e+02 - logprior: -4.6760e+00
Epoch 4/10
10/10 - 1s - loss: 482.2833 - loglik: -4.8253e+02 - logprior: 0.2445
Epoch 5/10
10/10 - 1s - loss: 480.1448 - loglik: -4.8230e+02 - logprior: 2.1539
Epoch 6/10
10/10 - 1s - loss: 478.0716 - loglik: -4.8125e+02 - logprior: 3.1796
Epoch 7/10
10/10 - 1s - loss: 477.2354 - loglik: -4.8122e+02 - logprior: 3.9836
Epoch 8/10
10/10 - 1s - loss: 477.3499 - loglik: -4.8195e+02 - logprior: 4.6021
Fitted a model with MAP estimate = -476.7366
Time for alignment: 39.7092
Fitting a model of length 96 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 687.6132 - loglik: -6.3135e+02 - logprior: -5.6266e+01
Epoch 2/10
10/10 - 1s - loss: 607.0167 - loglik: -5.9362e+02 - logprior: -1.3395e+01
Epoch 3/10
10/10 - 1s - loss: 557.5432 - loglik: -5.5144e+02 - logprior: -6.0993e+00
Epoch 4/10
10/10 - 1s - loss: 529.0355 - loglik: -5.2503e+02 - logprior: -4.0031e+00
Epoch 5/10
10/10 - 1s - loss: 516.8829 - loglik: -5.1423e+02 - logprior: -2.6512e+00
Epoch 6/10
10/10 - 1s - loss: 511.3077 - loglik: -5.0936e+02 - logprior: -1.9519e+00
Epoch 7/10
10/10 - 1s - loss: 509.2559 - loglik: -5.0742e+02 - logprior: -1.8307e+00
Epoch 8/10
10/10 - 1s - loss: 506.8492 - loglik: -5.0507e+02 - logprior: -1.7831e+00
Epoch 9/10
10/10 - 1s - loss: 506.5728 - loglik: -5.0504e+02 - logprior: -1.5292e+00
Epoch 10/10
10/10 - 1s - loss: 505.9289 - loglik: -5.0459e+02 - logprior: -1.3354e+00
Fitted a model with MAP estimate = -505.9286
expansions: [(13, 3), (16, 1), (17, 1), (18, 3), (32, 1), (33, 1), (34, 2), (35, 1), (52, 1), (54, 2), (56, 2), (57, 1), (60, 1), (62, 1), (69, 1), (77, 4), (80, 1), (86, 1), (90, 1), (91, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 569.1300 - loglik: -5.0594e+02 - logprior: -6.3194e+01
Epoch 2/2
10/10 - 1s - loss: 511.9789 - loglik: -4.8756e+02 - logprior: -2.4420e+01
Fitted a model with MAP estimate = -503.6913
expansions: [(0, 2)]
discards: [ 0 22 23]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 537.9533 - loglik: -4.8825e+02 - logprior: -4.9704e+01
Epoch 2/2
10/10 - 1s - loss: 492.5834 - loglik: -4.8180e+02 - logprior: -1.0780e+01
Fitted a model with MAP estimate = -487.0010
expansions: []
discards: [  0  45 100]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 760 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 550.1525 - loglik: -4.8988e+02 - logprior: -6.0270e+01
Epoch 2/10
10/10 - 1s - loss: 500.9366 - loglik: -4.8393e+02 - logprior: -1.7009e+01
Epoch 3/10
10/10 - 1s - loss: 488.5978 - loglik: -4.8449e+02 - logprior: -4.1049e+00
Epoch 4/10
10/10 - 1s - loss: 483.1575 - loglik: -4.8351e+02 - logprior: 0.3515
Epoch 5/10
10/10 - 1s - loss: 480.5552 - loglik: -4.8273e+02 - logprior: 2.1798
Epoch 6/10
10/10 - 1s - loss: 479.4512 - loglik: -4.8267e+02 - logprior: 3.2170
Epoch 7/10
10/10 - 1s - loss: 477.7465 - loglik: -4.8177e+02 - logprior: 4.0217
Epoch 8/10
10/10 - 1s - loss: 477.8297 - loglik: -4.8245e+02 - logprior: 4.6167
Fitted a model with MAP estimate = -477.5331
Time for alignment: 39.9783
Computed alignments with likelihoods: ['-476.2890', '-478.1357', '-477.8222', '-476.7366', '-477.5331']
Best model has likelihood: -476.2890  (prior= 5.1745 )
time for generating output: 0.1465
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf22.projection.fasta
SP score = 0.9381481029644992
Training of 5 independent models on file flav.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8daf23e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8c9497220>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b2ee490>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 792.3531 - loglik: -7.8429e+02 - logprior: -8.0663e+00
Epoch 2/10
13/13 - 2s - loss: 749.1556 - loglik: -7.4734e+02 - logprior: -1.8136e+00
Epoch 3/10
13/13 - 2s - loss: 723.0831 - loglik: -7.2163e+02 - logprior: -1.4568e+00
Epoch 4/10
13/13 - 2s - loss: 709.5413 - loglik: -7.0789e+02 - logprior: -1.6467e+00
Epoch 5/10
13/13 - 2s - loss: 702.3328 - loglik: -7.0073e+02 - logprior: -1.6055e+00
Epoch 6/10
13/13 - 2s - loss: 698.6371 - loglik: -6.9707e+02 - logprior: -1.5688e+00
Epoch 7/10
13/13 - 2s - loss: 695.0381 - loglik: -6.9342e+02 - logprior: -1.6129e+00
Epoch 8/10
13/13 - 2s - loss: 693.8121 - loglik: -6.9222e+02 - logprior: -1.5881e+00
Epoch 9/10
13/13 - 2s - loss: 691.3336 - loglik: -6.8976e+02 - logprior: -1.5620e+00
Epoch 10/10
13/13 - 2s - loss: 690.2477 - loglik: -6.8867e+02 - logprior: -1.5688e+00
Fitted a model with MAP estimate = -688.6587
expansions: [(12, 1), (15, 1), (17, 1), (19, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (29, 2), (51, 1), (52, 1), (55, 1), (58, 2), (64, 1), (79, 1), (82, 1), (87, 1), (88, 1), (100, 3), (101, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 714.7197 - loglik: -7.0540e+02 - logprior: -9.3238e+00
Epoch 2/2
13/13 - 3s - loss: 695.1858 - loglik: -6.9123e+02 - logprior: -3.9556e+00
Fitted a model with MAP estimate = -691.9271
expansions: [(0, 2)]
discards: [ 0 38]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 695.1858 - loglik: -6.8820e+02 - logprior: -6.9886e+00
Epoch 2/2
13/13 - 3s - loss: 687.5191 - loglik: -6.8586e+02 - logprior: -1.6570e+00
Fitted a model with MAP estimate = -685.5375
expansions: []
discards: [ 0 73]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 8s - loss: 698.5333 - loglik: -6.8977e+02 - logprior: -8.7628e+00
Epoch 2/10
13/13 - 2s - loss: 688.0598 - loglik: -6.8574e+02 - logprior: -2.3187e+00
Epoch 3/10
13/13 - 2s - loss: 685.9891 - loglik: -6.8511e+02 - logprior: -8.7933e-01
Epoch 4/10
13/13 - 2s - loss: 685.1603 - loglik: -6.8472e+02 - logprior: -4.3482e-01
Epoch 5/10
13/13 - 2s - loss: 682.1649 - loglik: -6.8188e+02 - logprior: -2.8008e-01
Epoch 6/10
13/13 - 2s - loss: 683.1512 - loglik: -6.8295e+02 - logprior: -1.9856e-01
Fitted a model with MAP estimate = -680.9723
Time for alignment: 71.3086
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 791.9441 - loglik: -7.8387e+02 - logprior: -8.0759e+00
Epoch 2/10
13/13 - 2s - loss: 749.0859 - loglik: -7.4727e+02 - logprior: -1.8149e+00
Epoch 3/10
13/13 - 2s - loss: 722.7833 - loglik: -7.2131e+02 - logprior: -1.4693e+00
Epoch 4/10
13/13 - 2s - loss: 709.8108 - loglik: -7.0814e+02 - logprior: -1.6650e+00
Epoch 5/10
13/13 - 2s - loss: 704.0735 - loglik: -7.0255e+02 - logprior: -1.5258e+00
Epoch 6/10
13/13 - 2s - loss: 700.9426 - loglik: -6.9951e+02 - logprior: -1.4339e+00
Epoch 7/10
13/13 - 2s - loss: 698.7968 - loglik: -6.9731e+02 - logprior: -1.4853e+00
Epoch 8/10
13/13 - 2s - loss: 696.7247 - loglik: -6.9520e+02 - logprior: -1.5160e+00
Epoch 9/10
13/13 - 2s - loss: 694.2379 - loglik: -6.9273e+02 - logprior: -1.5014e+00
Epoch 10/10
13/13 - 2s - loss: 692.8725 - loglik: -6.9136e+02 - logprior: -1.5031e+00
Fitted a model with MAP estimate = -691.1541
expansions: [(12, 1), (19, 1), (20, 1), (21, 2), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (33, 1), (51, 1), (52, 1), (55, 1), (57, 1), (64, 1), (81, 1), (88, 3), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 716.5495 - loglik: -7.0717e+02 - logprior: -9.3825e+00
Epoch 2/2
13/13 - 2s - loss: 695.9949 - loglik: -6.9199e+02 - logprior: -4.0001e+00
Fitted a model with MAP estimate = -693.4239
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 696.5231 - loglik: -6.8947e+02 - logprior: -7.0524e+00
Epoch 2/2
13/13 - 3s - loss: 688.7908 - loglik: -6.8709e+02 - logprior: -1.7037e+00
Fitted a model with MAP estimate = -687.0849
expansions: []
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 699.1771 - loglik: -6.9038e+02 - logprior: -8.7952e+00
Epoch 2/10
13/13 - 2s - loss: 689.6041 - loglik: -6.8728e+02 - logprior: -2.3190e+00
Epoch 3/10
13/13 - 2s - loss: 686.7680 - loglik: -6.8586e+02 - logprior: -9.1224e-01
Epoch 4/10
13/13 - 2s - loss: 686.7481 - loglik: -6.8627e+02 - logprior: -4.7442e-01
Epoch 5/10
13/13 - 2s - loss: 685.1848 - loglik: -6.8488e+02 - logprior: -3.0050e-01
Epoch 6/10
13/13 - 2s - loss: 683.6630 - loglik: -6.8343e+02 - logprior: -2.2649e-01
Epoch 7/10
13/13 - 2s - loss: 681.1970 - loglik: -6.8100e+02 - logprior: -1.8914e-01
Epoch 8/10
13/13 - 2s - loss: 680.3878 - loglik: -6.8022e+02 - logprior: -1.6659e-01
Epoch 9/10
13/13 - 2s - loss: 678.4460 - loglik: -6.7829e+02 - logprior: -1.5011e-01
Epoch 10/10
13/13 - 2s - loss: 677.0839 - loglik: -6.7693e+02 - logprior: -1.4221e-01
Fitted a model with MAP estimate = -675.4611
Time for alignment: 79.3719
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 791.8585 - loglik: -7.8379e+02 - logprior: -8.0706e+00
Epoch 2/10
13/13 - 2s - loss: 749.6168 - loglik: -7.4781e+02 - logprior: -1.8102e+00
Epoch 3/10
13/13 - 2s - loss: 722.2998 - loglik: -7.2083e+02 - logprior: -1.4679e+00
Epoch 4/10
13/13 - 2s - loss: 709.9052 - loglik: -7.0820e+02 - logprior: -1.7032e+00
Epoch 5/10
13/13 - 2s - loss: 702.9649 - loglik: -7.0130e+02 - logprior: -1.6671e+00
Epoch 6/10
13/13 - 2s - loss: 699.4283 - loglik: -6.9787e+02 - logprior: -1.5558e+00
Epoch 7/10
13/13 - 2s - loss: 697.0404 - loglik: -6.9546e+02 - logprior: -1.5772e+00
Epoch 8/10
13/13 - 2s - loss: 694.2557 - loglik: -6.9266e+02 - logprior: -1.5912e+00
Epoch 9/10
13/13 - 2s - loss: 692.5569 - loglik: -6.9096e+02 - logprior: -1.5927e+00
Epoch 10/10
13/13 - 2s - loss: 690.9362 - loglik: -6.8930e+02 - logprior: -1.6236e+00
Fitted a model with MAP estimate = -688.5448
expansions: [(6, 1), (11, 1), (18, 1), (21, 1), (23, 1), (24, 1), (25, 2), (26, 1), (27, 1), (33, 1), (51, 1), (52, 1), (55, 1), (59, 3), (64, 2), (80, 1), (82, 1), (83, 1), (84, 1), (100, 3), (101, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 135 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 718.6228 - loglik: -7.0927e+02 - logprior: -9.3485e+00
Epoch 2/2
13/13 - 3s - loss: 696.1173 - loglik: -6.9216e+02 - logprior: -3.9580e+00
Fitted a model with MAP estimate = -692.7415
expansions: [(0, 2)]
discards: [  0  80 126]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 695.7286 - loglik: -6.8871e+02 - logprior: -7.0157e+00
Epoch 2/2
13/13 - 2s - loss: 688.3516 - loglik: -6.8665e+02 - logprior: -1.6965e+00
Fitted a model with MAP estimate = -686.6910
expansions: []
discards: [ 0 75 76]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 699.7106 - loglik: -6.9093e+02 - logprior: -8.7773e+00
Epoch 2/10
13/13 - 2s - loss: 690.4413 - loglik: -6.8813e+02 - logprior: -2.3074e+00
Epoch 3/10
13/13 - 2s - loss: 687.6815 - loglik: -6.8684e+02 - logprior: -8.4325e-01
Epoch 4/10
13/13 - 2s - loss: 685.8608 - loglik: -6.8544e+02 - logprior: -4.2235e-01
Epoch 5/10
13/13 - 2s - loss: 685.3140 - loglik: -6.8506e+02 - logprior: -2.5113e-01
Epoch 6/10
13/13 - 2s - loss: 683.6813 - loglik: -6.8350e+02 - logprior: -1.7841e-01
Epoch 7/10
13/13 - 2s - loss: 681.9119 - loglik: -6.8177e+02 - logprior: -1.3373e-01
Epoch 8/10
13/13 - 2s - loss: 680.3034 - loglik: -6.8018e+02 - logprior: -1.1507e-01
Epoch 9/10
13/13 - 2s - loss: 677.9909 - loglik: -6.7787e+02 - logprior: -1.1163e-01
Epoch 10/10
13/13 - 2s - loss: 676.4083 - loglik: -6.7629e+02 - logprior: -1.1327e-01
Fitted a model with MAP estimate = -675.1298
Time for alignment: 80.3694
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 792.4485 - loglik: -7.8438e+02 - logprior: -8.0682e+00
Epoch 2/10
13/13 - 2s - loss: 749.0345 - loglik: -7.4722e+02 - logprior: -1.8110e+00
Epoch 3/10
13/13 - 2s - loss: 720.7794 - loglik: -7.1933e+02 - logprior: -1.4538e+00
Epoch 4/10
13/13 - 2s - loss: 707.8493 - loglik: -7.0619e+02 - logprior: -1.6588e+00
Epoch 5/10
13/13 - 2s - loss: 703.2495 - loglik: -7.0169e+02 - logprior: -1.5557e+00
Epoch 6/10
13/13 - 2s - loss: 698.9700 - loglik: -6.9753e+02 - logprior: -1.4322e+00
Epoch 7/10
13/13 - 2s - loss: 697.8036 - loglik: -6.9635e+02 - logprior: -1.4533e+00
Epoch 8/10
13/13 - 2s - loss: 696.2343 - loglik: -6.9477e+02 - logprior: -1.4609e+00
Epoch 9/10
13/13 - 2s - loss: 693.8816 - loglik: -6.9241e+02 - logprior: -1.4653e+00
Epoch 10/10
13/13 - 2s - loss: 692.0365 - loglik: -6.9053e+02 - logprior: -1.4935e+00
Fitted a model with MAP estimate = -691.1082
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 2), (24, 1), (27, 3), (28, 2), (33, 1), (51, 1), (52, 1), (55, 1), (58, 2), (64, 1), (80, 1), (82, 1), (92, 8), (99, 1), (100, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 718.2187 - loglik: -7.0886e+02 - logprior: -9.3573e+00
Epoch 2/2
13/13 - 3s - loss: 697.9844 - loglik: -6.9390e+02 - logprior: -4.0862e+00
Fitted a model with MAP estimate = -693.9433
expansions: [(0, 2)]
discards: [  0  27  34  74 116 117 129 130]
Re-initialized the encoder parameters.
Fitting a model of length 133 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 699.0892 - loglik: -6.9210e+02 - logprior: -6.9894e+00
Epoch 2/2
13/13 - 3s - loss: 688.1241 - loglik: -6.8642e+02 - logprior: -1.6992e+00
Fitted a model with MAP estimate = -687.3710
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 699.1621 - loglik: -6.9034e+02 - logprior: -8.8220e+00
Epoch 2/10
13/13 - 2s - loss: 689.0918 - loglik: -6.8673e+02 - logprior: -2.3599e+00
Epoch 3/10
13/13 - 2s - loss: 686.6198 - loglik: -6.8569e+02 - logprior: -9.3169e-01
Epoch 4/10
13/13 - 2s - loss: 685.4724 - loglik: -6.8498e+02 - logprior: -4.8987e-01
Epoch 5/10
13/13 - 2s - loss: 684.8639 - loglik: -6.8453e+02 - logprior: -3.2994e-01
Epoch 6/10
13/13 - 2s - loss: 681.5219 - loglik: -6.8128e+02 - logprior: -2.3927e-01
Epoch 7/10
13/13 - 3s - loss: 681.3877 - loglik: -6.8116e+02 - logprior: -2.2406e-01
Epoch 8/10
13/13 - 2s - loss: 680.1991 - loglik: -6.7999e+02 - logprior: -1.9959e-01
Epoch 9/10
13/13 - 2s - loss: 677.2363 - loglik: -6.7703e+02 - logprior: -2.0233e-01
Epoch 10/10
13/13 - 2s - loss: 676.1681 - loglik: -6.7595e+02 - logprior: -2.1017e-01
Fitted a model with MAP estimate = -674.6782
Time for alignment: 80.8205
Fitting a model of length 108 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 791.2524 - loglik: -7.8318e+02 - logprior: -8.0719e+00
Epoch 2/10
13/13 - 2s - loss: 748.9835 - loglik: -7.4717e+02 - logprior: -1.8134e+00
Epoch 3/10
13/13 - 2s - loss: 720.6132 - loglik: -7.1913e+02 - logprior: -1.4850e+00
Epoch 4/10
13/13 - 2s - loss: 708.2087 - loglik: -7.0656e+02 - logprior: -1.6427e+00
Epoch 5/10
13/13 - 2s - loss: 703.4596 - loglik: -7.0199e+02 - logprior: -1.4696e+00
Epoch 6/10
13/13 - 2s - loss: 699.3589 - loglik: -6.9799e+02 - logprior: -1.3698e+00
Epoch 7/10
13/13 - 2s - loss: 697.3052 - loglik: -6.9590e+02 - logprior: -1.3963e+00
Epoch 8/10
13/13 - 2s - loss: 696.4626 - loglik: -6.9508e+02 - logprior: -1.3803e+00
Epoch 9/10
13/13 - 2s - loss: 693.9346 - loglik: -6.9256e+02 - logprior: -1.3712e+00
Epoch 10/10
13/13 - 2s - loss: 693.0455 - loglik: -6.9164e+02 - logprior: -1.3941e+00
Fitted a model with MAP estimate = -691.4463
expansions: [(12, 1), (19, 1), (20, 1), (21, 1), (23, 2), (24, 1), (27, 3), (28, 2), (33, 1), (51, 1), (52, 1), (55, 1), (58, 2), (64, 1), (82, 4), (83, 1), (93, 1), (99, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 134 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 717.1034 - loglik: -7.0775e+02 - logprior: -9.3572e+00
Epoch 2/2
13/13 - 3s - loss: 698.5848 - loglik: -6.9451e+02 - logprior: -4.0750e+00
Fitted a model with MAP estimate = -694.5881
expansions: [(0, 2)]
discards: [  0  27  35  74 102 124]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 698.7698 - loglik: -6.9177e+02 - logprior: -7.0034e+00
Epoch 2/2
13/13 - 2s - loss: 689.4721 - loglik: -6.8780e+02 - logprior: -1.6722e+00
Fitted a model with MAP estimate = -688.3834
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 4612 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 700.4749 - loglik: -6.9168e+02 - logprior: -8.7945e+00
Epoch 2/10
13/13 - 2s - loss: 691.5875 - loglik: -6.8926e+02 - logprior: -2.3319e+00
Epoch 3/10
13/13 - 2s - loss: 687.6165 - loglik: -6.8671e+02 - logprior: -9.1024e-01
Epoch 4/10
13/13 - 2s - loss: 687.8788 - loglik: -6.8739e+02 - logprior: -4.8331e-01
Fitted a model with MAP estimate = -686.4584
Time for alignment: 64.4212
Computed alignments with likelihoods: ['-680.9723', '-675.4611', '-675.1298', '-674.6782', '-686.4584']
Best model has likelihood: -674.6782  (prior= -0.2464 )
time for generating output: 0.1880
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/flav.projection.fasta
SP score = 0.842456608811749
Training of 5 independent models on file sodfe.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea6683b9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea0902c9a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea66d399d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 444.4781 - loglik: -4.3588e+02 - logprior: -8.5957e+00
Epoch 2/10
13/13 - 1s - loss: 390.4959 - loglik: -3.8825e+02 - logprior: -2.2442e+00
Epoch 3/10
13/13 - 1s - loss: 363.5508 - loglik: -3.6183e+02 - logprior: -1.7204e+00
Epoch 4/10
13/13 - 1s - loss: 355.8214 - loglik: -3.5433e+02 - logprior: -1.4943e+00
Epoch 5/10
13/13 - 1s - loss: 354.0898 - loglik: -3.5267e+02 - logprior: -1.4144e+00
Epoch 6/10
13/13 - 1s - loss: 352.5483 - loglik: -3.5118e+02 - logprior: -1.3686e+00
Epoch 7/10
13/13 - 1s - loss: 352.2808 - loglik: -3.5094e+02 - logprior: -1.3417e+00
Epoch 8/10
13/13 - 1s - loss: 351.6451 - loglik: -3.5032e+02 - logprior: -1.3209e+00
Epoch 9/10
13/13 - 1s - loss: 351.4511 - loglik: -3.5013e+02 - logprior: -1.3214e+00
Epoch 10/10
13/13 - 1s - loss: 350.4414 - loglik: -3.4913e+02 - logprior: -1.3112e+00
Fitted a model with MAP estimate = -350.6570
expansions: [(0, 4), (13, 1), (35, 1), (36, 3), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 360.9310 - loglik: -3.5079e+02 - logprior: -1.0137e+01
Epoch 2/2
13/13 - 1s - loss: 342.3098 - loglik: -3.3926e+02 - logprior: -3.0490e+00
Fitted a model with MAP estimate = -339.5515
expansions: [(0, 2)]
discards: [47]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 349.9934 - loglik: -3.3997e+02 - logprior: -1.0023e+01
Epoch 2/2
13/13 - 1s - loss: 338.3851 - loglik: -3.3513e+02 - logprior: -3.2501e+00
Fitted a model with MAP estimate = -336.8708
expansions: []
discards: [48]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 344.8293 - loglik: -3.3692e+02 - logprior: -7.9076e+00
Epoch 2/10
13/13 - 1s - loss: 337.2585 - loglik: -3.3505e+02 - logprior: -2.2048e+00
Epoch 3/10
13/13 - 1s - loss: 336.2895 - loglik: -3.3463e+02 - logprior: -1.6605e+00
Epoch 4/10
13/13 - 1s - loss: 334.6674 - loglik: -3.3334e+02 - logprior: -1.3305e+00
Epoch 5/10
13/13 - 2s - loss: 334.8571 - loglik: -3.3365e+02 - logprior: -1.2096e+00
Fitted a model with MAP estimate = -334.5016
Time for alignment: 46.9165
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 444.6106 - loglik: -4.3602e+02 - logprior: -8.5944e+00
Epoch 2/10
13/13 - 1s - loss: 391.5954 - loglik: -3.8935e+02 - logprior: -2.2474e+00
Epoch 3/10
13/13 - 1s - loss: 365.3568 - loglik: -3.6359e+02 - logprior: -1.7703e+00
Epoch 4/10
13/13 - 2s - loss: 356.1194 - loglik: -3.5454e+02 - logprior: -1.5766e+00
Epoch 5/10
13/13 - 1s - loss: 354.4619 - loglik: -3.5297e+02 - logprior: -1.4910e+00
Epoch 6/10
13/13 - 1s - loss: 352.1668 - loglik: -3.5070e+02 - logprior: -1.4664e+00
Epoch 7/10
13/13 - 1s - loss: 351.8916 - loglik: -3.5046e+02 - logprior: -1.4327e+00
Epoch 8/10
13/13 - 1s - loss: 351.0474 - loglik: -3.4962e+02 - logprior: -1.4238e+00
Epoch 9/10
13/13 - 1s - loss: 351.0499 - loglik: -3.4962e+02 - logprior: -1.4243e+00
Fitted a model with MAP estimate = -350.4743
expansions: [(0, 5), (13, 1), (16, 1), (34, 1), (35, 2), (36, 1), (37, 2), (39, 1), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 359.8556 - loglik: -3.4973e+02 - logprior: -1.0126e+01
Epoch 2/2
13/13 - 1s - loss: 341.7574 - loglik: -3.3860e+02 - logprior: -3.1604e+00
Fitted a model with MAP estimate = -338.1923
expansions: [(0, 2)]
discards: [43 48 58]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 348.9187 - loglik: -3.3890e+02 - logprior: -1.0016e+01
Epoch 2/2
13/13 - 1s - loss: 339.0501 - loglik: -3.3589e+02 - logprior: -3.1555e+00
Fitted a model with MAP estimate = -336.9962
expansions: [(0, 2)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 346.9308 - loglik: -3.3790e+02 - logprior: -9.0304e+00
Epoch 2/10
13/13 - 1s - loss: 336.9924 - loglik: -3.3468e+02 - logprior: -2.3167e+00
Epoch 3/10
13/13 - 1s - loss: 335.9019 - loglik: -3.3431e+02 - logprior: -1.5905e+00
Epoch 4/10
13/13 - 1s - loss: 335.8749 - loglik: -3.3452e+02 - logprior: -1.3576e+00
Epoch 5/10
13/13 - 1s - loss: 334.3562 - loglik: -3.3316e+02 - logprior: -1.1986e+00
Epoch 6/10
13/13 - 1s - loss: 335.0975 - loglik: -3.3392e+02 - logprior: -1.1778e+00
Fitted a model with MAP estimate = -334.2260
Time for alignment: 47.5227
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 444.8029 - loglik: -4.3621e+02 - logprior: -8.5964e+00
Epoch 2/10
13/13 - 1s - loss: 393.3582 - loglik: -3.9109e+02 - logprior: -2.2674e+00
Epoch 3/10
13/13 - 1s - loss: 364.3916 - loglik: -3.6260e+02 - logprior: -1.7884e+00
Epoch 4/10
13/13 - 1s - loss: 356.1071 - loglik: -3.5454e+02 - logprior: -1.5665e+00
Epoch 5/10
13/13 - 1s - loss: 353.1462 - loglik: -3.5171e+02 - logprior: -1.4379e+00
Epoch 6/10
13/13 - 1s - loss: 352.7595 - loglik: -3.5137e+02 - logprior: -1.3882e+00
Epoch 7/10
13/13 - 1s - loss: 351.1237 - loglik: -3.4976e+02 - logprior: -1.3636e+00
Epoch 8/10
13/13 - 1s - loss: 350.9256 - loglik: -3.4955e+02 - logprior: -1.3696e+00
Epoch 9/10
13/13 - 1s - loss: 350.3885 - loglik: -3.4902e+02 - logprior: -1.3623e+00
Epoch 10/10
13/13 - 1s - loss: 349.6573 - loglik: -3.4829e+02 - logprior: -1.3635e+00
Fitted a model with MAP estimate = -349.5528
expansions: [(0, 4), (13, 1), (32, 1), (33, 1), (35, 2), (36, 1), (37, 2), (44, 7)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 360.4325 - loglik: -3.5022e+02 - logprior: -1.0211e+01
Epoch 2/2
13/13 - 1s - loss: 341.4155 - loglik: -3.3828e+02 - logprior: -3.1395e+00
Fitted a model with MAP estimate = -338.1520
expansions: [(0, 2)]
discards: [43 47 60 61]
Re-initialized the encoder parameters.
Fitting a model of length 82 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 349.9472 - loglik: -3.3991e+02 - logprior: -1.0032e+01
Epoch 2/2
13/13 - 1s - loss: 339.1295 - loglik: -3.3584e+02 - logprior: -3.2893e+00
Fitted a model with MAP estimate = -337.2704
expansions: [(59, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 345.5387 - loglik: -3.3761e+02 - logprior: -7.9336e+00
Epoch 2/10
13/13 - 1s - loss: 335.5804 - loglik: -3.3331e+02 - logprior: -2.2748e+00
Epoch 3/10
13/13 - 1s - loss: 334.5210 - loglik: -3.3275e+02 - logprior: -1.7741e+00
Epoch 4/10
13/13 - 2s - loss: 334.1538 - loglik: -3.3273e+02 - logprior: -1.4198e+00
Epoch 5/10
13/13 - 1s - loss: 333.8235 - loglik: -3.3250e+02 - logprior: -1.3179e+00
Epoch 6/10
13/13 - 1s - loss: 332.9940 - loglik: -3.3173e+02 - logprior: -1.2671e+00
Epoch 7/10
13/13 - 1s - loss: 332.9727 - loglik: -3.3174e+02 - logprior: -1.2305e+00
Epoch 8/10
13/13 - 1s - loss: 331.5252 - loglik: -3.3031e+02 - logprior: -1.2145e+00
Epoch 9/10
13/13 - 1s - loss: 331.0172 - loglik: -3.2983e+02 - logprior: -1.1861e+00
Epoch 10/10
13/13 - 1s - loss: 331.6224 - loglik: -3.3045e+02 - logprior: -1.1656e+00
Fitted a model with MAP estimate = -330.8585
Time for alignment: 53.4054
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 445.3415 - loglik: -4.3674e+02 - logprior: -8.5980e+00
Epoch 2/10
13/13 - 1s - loss: 389.4325 - loglik: -3.8719e+02 - logprior: -2.2391e+00
Epoch 3/10
13/13 - 1s - loss: 362.5752 - loglik: -3.6089e+02 - logprior: -1.6889e+00
Epoch 4/10
13/13 - 1s - loss: 356.1067 - loglik: -3.5464e+02 - logprior: -1.4642e+00
Epoch 5/10
13/13 - 2s - loss: 354.0995 - loglik: -3.5271e+02 - logprior: -1.3836e+00
Epoch 6/10
13/13 - 1s - loss: 352.6883 - loglik: -3.5134e+02 - logprior: -1.3480e+00
Epoch 7/10
13/13 - 1s - loss: 352.5355 - loglik: -3.5123e+02 - logprior: -1.3056e+00
Epoch 8/10
13/13 - 1s - loss: 351.7708 - loglik: -3.5047e+02 - logprior: -1.2961e+00
Epoch 9/10
13/13 - 1s - loss: 351.0355 - loglik: -3.4975e+02 - logprior: -1.2811e+00
Epoch 10/10
13/13 - 1s - loss: 350.7041 - loglik: -3.4942e+02 - logprior: -1.2747e+00
Fitted a model with MAP estimate = -350.6728
expansions: [(0, 4), (13, 1), (36, 4), (37, 2), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 359.7043 - loglik: -3.4957e+02 - logprior: -1.0136e+01
Epoch 2/2
13/13 - 1s - loss: 343.0147 - loglik: -3.3996e+02 - logprior: -3.0526e+00
Fitted a model with MAP estimate = -339.4521
expansions: [(0, 2)]
discards: [49]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 349.6979 - loglik: -3.3967e+02 - logprior: -1.0032e+01
Epoch 2/2
13/13 - 1s - loss: 338.1208 - loglik: -3.3487e+02 - logprior: -3.2516e+00
Fitted a model with MAP estimate = -336.7954
expansions: []
discards: [48]
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 344.5193 - loglik: -3.3661e+02 - logprior: -7.9079e+00
Epoch 2/10
13/13 - 1s - loss: 337.4946 - loglik: -3.3529e+02 - logprior: -2.2036e+00
Epoch 3/10
13/13 - 1s - loss: 335.3387 - loglik: -3.3368e+02 - logprior: -1.6618e+00
Epoch 4/10
13/13 - 1s - loss: 335.7797 - loglik: -3.3446e+02 - logprior: -1.3199e+00
Fitted a model with MAP estimate = -334.9453
Time for alignment: 44.2986
Fitting a model of length 65 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 444.9748 - loglik: -4.3638e+02 - logprior: -8.5945e+00
Epoch 2/10
13/13 - 1s - loss: 391.6832 - loglik: -3.8944e+02 - logprior: -2.2462e+00
Epoch 3/10
13/13 - 1s - loss: 365.9510 - loglik: -3.6419e+02 - logprior: -1.7589e+00
Epoch 4/10
13/13 - 1s - loss: 357.0928 - loglik: -3.5554e+02 - logprior: -1.5533e+00
Epoch 5/10
13/13 - 1s - loss: 354.1177 - loglik: -3.5267e+02 - logprior: -1.4420e+00
Epoch 6/10
13/13 - 1s - loss: 352.6836 - loglik: -3.5131e+02 - logprior: -1.3759e+00
Epoch 7/10
13/13 - 1s - loss: 351.6641 - loglik: -3.5031e+02 - logprior: -1.3547e+00
Epoch 8/10
13/13 - 1s - loss: 351.6641 - loglik: -3.5032e+02 - logprior: -1.3397e+00
Fitted a model with MAP estimate = -351.0707
expansions: [(0, 4), (13, 1), (34, 1), (36, 2), (37, 3), (38, 2), (43, 3), (44, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 83 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 360.5147 - loglik: -3.5041e+02 - logprior: -1.0100e+01
Epoch 2/2
13/13 - 1s - loss: 341.3189 - loglik: -3.3829e+02 - logprior: -3.0275e+00
Fitted a model with MAP estimate = -339.1220
expansions: [(0, 2)]
discards: [49]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 348.5877 - loglik: -3.3858e+02 - logprior: -1.0010e+01
Epoch 2/2
13/13 - 1s - loss: 338.7780 - loglik: -3.3557e+02 - logprior: -3.2055e+00
Fitted a model with MAP estimate = -336.5710
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4455 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 344.8664 - loglik: -3.3698e+02 - logprior: -7.8896e+00
Epoch 2/10
13/13 - 1s - loss: 336.2869 - loglik: -3.3409e+02 - logprior: -2.1955e+00
Epoch 3/10
13/13 - 1s - loss: 335.4363 - loglik: -3.3378e+02 - logprior: -1.6539e+00
Epoch 4/10
13/13 - 1s - loss: 334.8387 - loglik: -3.3353e+02 - logprior: -1.3058e+00
Epoch 5/10
13/13 - 1s - loss: 334.0463 - loglik: -3.3283e+02 - logprior: -1.2116e+00
Epoch 6/10
13/13 - 1s - loss: 334.5309 - loglik: -3.3337e+02 - logprior: -1.1615e+00
Fitted a model with MAP estimate = -333.6134
Time for alignment: 44.7595
Computed alignments with likelihoods: ['-334.5016', '-334.2260', '-330.8585', '-334.9453', '-333.6134']
Best model has likelihood: -330.8585  (prior= -1.1541 )
time for generating output: 0.1817
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/sodfe.projection.fasta
SP score = 0.5103500188182161
Training of 5 independent models on file zf-CCHH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8fc8871f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efa3eac0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c6006a0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 127.1336 - loglik: -1.2630e+02 - logprior: -8.3083e-01
Epoch 2/10
41/41 - 1s - loss: 112.9666 - loglik: -1.1216e+02 - logprior: -8.0680e-01
Epoch 3/10
41/41 - 1s - loss: 112.2948 - loglik: -1.1150e+02 - logprior: -7.9329e-01
Epoch 4/10
41/41 - 1s - loss: 112.2484 - loglik: -1.1146e+02 - logprior: -7.8196e-01
Epoch 5/10
41/41 - 1s - loss: 112.0360 - loglik: -1.1125e+02 - logprior: -7.8268e-01
Epoch 6/10
41/41 - 1s - loss: 111.8858 - loglik: -1.1110e+02 - logprior: -7.8034e-01
Epoch 7/10
41/41 - 1s - loss: 111.7945 - loglik: -1.1101e+02 - logprior: -7.7761e-01
Epoch 8/10
41/41 - 1s - loss: 111.7607 - loglik: -1.1097e+02 - logprior: -7.7835e-01
Epoch 9/10
41/41 - 1s - loss: 111.7766 - loglik: -1.1098e+02 - logprior: -7.7550e-01
Fitted a model with MAP estimate = -110.3567
expansions: [(8, 2), (9, 2), (10, 2), (11, 1), (12, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 27 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 113.9650 - loglik: -1.1295e+02 - logprior: -1.0169e+00
Epoch 2/2
41/41 - 1s - loss: 110.4397 - loglik: -1.0966e+02 - logprior: -7.8231e-01
Fitted a model with MAP estimate = -108.5800
expansions: []
discards: [ 8 12 14 20]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 112.2318 - loglik: -1.1125e+02 - logprior: -9.7677e-01
Epoch 2/2
41/41 - 1s - loss: 110.2357 - loglik: -1.0949e+02 - logprior: -7.4017e-01
Fitted a model with MAP estimate = -108.5650
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 8s - loss: 109.1424 - loglik: -1.0853e+02 - logprior: -6.0942e-01
Epoch 2/10
58/58 - 2s - loss: 107.8951 - loglik: -1.0739e+02 - logprior: -5.0440e-01
Epoch 3/10
58/58 - 2s - loss: 107.8348 - loglik: -1.0733e+02 - logprior: -4.9922e-01
Epoch 4/10
58/58 - 2s - loss: 107.5661 - loglik: -1.0706e+02 - logprior: -4.9715e-01
Epoch 5/10
58/58 - 2s - loss: 107.6176 - loglik: -1.0711e+02 - logprior: -4.9642e-01
Fitted a model with MAP estimate = -107.4751
Time for alignment: 60.6868
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 127.2074 - loglik: -1.2638e+02 - logprior: -8.2556e-01
Epoch 2/10
41/41 - 1s - loss: 113.0652 - loglik: -1.1225e+02 - logprior: -8.1118e-01
Epoch 3/10
41/41 - 1s - loss: 112.3177 - loglik: -1.1152e+02 - logprior: -7.9364e-01
Epoch 4/10
41/41 - 1s - loss: 112.2239 - loglik: -1.1143e+02 - logprior: -7.8645e-01
Epoch 5/10
41/41 - 1s - loss: 112.0822 - loglik: -1.1130e+02 - logprior: -7.7938e-01
Epoch 6/10
41/41 - 1s - loss: 111.9632 - loglik: -1.1117e+02 - logprior: -7.8235e-01
Epoch 7/10
41/41 - 1s - loss: 111.8516 - loglik: -1.1106e+02 - logprior: -7.7797e-01
Epoch 8/10
41/41 - 1s - loss: 111.8109 - loglik: -1.1102e+02 - logprior: -7.7888e-01
Epoch 9/10
41/41 - 1s - loss: 111.6775 - loglik: -1.1088e+02 - logprior: -7.7662e-01
Epoch 10/10
41/41 - 1s - loss: 111.5755 - loglik: -1.1078e+02 - logprior: -7.7723e-01
Fitted a model with MAP estimate = -110.4230
expansions: [(4, 1), (9, 2), (10, 1), (11, 1), (12, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 113.4794 - loglik: -1.1247e+02 - logprior: -1.0048e+00
Epoch 2/2
41/41 - 1s - loss: 110.4474 - loglik: -1.0968e+02 - logprior: -7.6679e-01
Fitted a model with MAP estimate = -108.5533
expansions: []
discards: [11 18]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 112.0982 - loglik: -1.1112e+02 - logprior: -9.7614e-01
Epoch 2/2
41/41 - 1s - loss: 110.2754 - loglik: -1.0954e+02 - logprior: -7.3938e-01
Fitted a model with MAP estimate = -108.5971
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 108.9839 - loglik: -1.0837e+02 - logprior: -6.0869e-01
Epoch 2/10
58/58 - 2s - loss: 107.9179 - loglik: -1.0741e+02 - logprior: -5.0493e-01
Epoch 3/10
58/58 - 2s - loss: 107.9283 - loglik: -1.0742e+02 - logprior: -5.0118e-01
Fitted a model with MAP estimate = -107.6894
Time for alignment: 58.7904
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 127.3269 - loglik: -1.2649e+02 - logprior: -8.3230e-01
Epoch 2/10
41/41 - 1s - loss: 113.0422 - loglik: -1.1224e+02 - logprior: -8.0597e-01
Epoch 3/10
41/41 - 1s - loss: 112.2835 - loglik: -1.1149e+02 - logprior: -7.9230e-01
Epoch 4/10
41/41 - 1s - loss: 112.2095 - loglik: -1.1142e+02 - logprior: -7.8710e-01
Epoch 5/10
41/41 - 1s - loss: 112.0613 - loglik: -1.1127e+02 - logprior: -7.8424e-01
Epoch 6/10
41/41 - 1s - loss: 111.9922 - loglik: -1.1120e+02 - logprior: -7.8005e-01
Epoch 7/10
41/41 - 1s - loss: 111.8372 - loglik: -1.1105e+02 - logprior: -7.7819e-01
Epoch 8/10
41/41 - 1s - loss: 111.7713 - loglik: -1.1098e+02 - logprior: -7.7751e-01
Epoch 9/10
41/41 - 1s - loss: 111.7027 - loglik: -1.1091e+02 - logprior: -7.7647e-01
Epoch 10/10
41/41 - 1s - loss: 111.5673 - loglik: -1.1077e+02 - logprior: -7.7551e-01
Fitted a model with MAP estimate = -110.4602
expansions: [(4, 1), (9, 1), (10, 2), (11, 1), (12, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 113.4910 - loglik: -1.1249e+02 - logprior: -1.0024e+00
Epoch 2/2
41/41 - 1s - loss: 110.3564 - loglik: -1.0959e+02 - logprior: -7.6750e-01
Fitted a model with MAP estimate = -108.5496
expansions: []
discards: [12 18]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 6s - loss: 112.1144 - loglik: -1.1114e+02 - logprior: -9.7508e-01
Epoch 2/2
41/41 - 1s - loss: 110.2887 - loglik: -1.0955e+02 - logprior: -7.3959e-01
Fitted a model with MAP estimate = -108.5628
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 5s - loss: 109.1773 - loglik: -1.0857e+02 - logprior: -6.0858e-01
Epoch 2/10
58/58 - 2s - loss: 107.9410 - loglik: -1.0744e+02 - logprior: -5.0429e-01
Epoch 3/10
58/58 - 1s - loss: 107.7479 - loglik: -1.0724e+02 - logprior: -5.0127e-01
Epoch 4/10
58/58 - 2s - loss: 107.6750 - loglik: -1.0717e+02 - logprior: -4.9653e-01
Epoch 5/10
58/58 - 2s - loss: 107.6557 - loglik: -1.0715e+02 - logprior: -4.9475e-01
Epoch 6/10
58/58 - 2s - loss: 107.6373 - loglik: -1.0713e+02 - logprior: -4.9345e-01
Epoch 7/10
58/58 - 2s - loss: 107.3129 - loglik: -1.0680e+02 - logprior: -4.9222e-01
Epoch 8/10
58/58 - 2s - loss: 107.4395 - loglik: -1.0692e+02 - logprior: -4.9066e-01
Fitted a model with MAP estimate = -107.1676
Time for alignment: 65.5513
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 127.1374 - loglik: -1.2631e+02 - logprior: -8.3041e-01
Epoch 2/10
41/41 - 1s - loss: 113.0371 - loglik: -1.1223e+02 - logprior: -8.0618e-01
Epoch 3/10
41/41 - 1s - loss: 112.3332 - loglik: -1.1154e+02 - logprior: -7.9466e-01
Epoch 4/10
41/41 - 1s - loss: 112.2284 - loglik: -1.1144e+02 - logprior: -7.8398e-01
Epoch 5/10
41/41 - 1s - loss: 111.9941 - loglik: -1.1121e+02 - logprior: -7.8323e-01
Epoch 6/10
41/41 - 1s - loss: 111.9992 - loglik: -1.1121e+02 - logprior: -7.8120e-01
Fitted a model with MAP estimate = -110.3968
expansions: [(4, 1), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 113.3696 - loglik: -1.1236e+02 - logprior: -1.0057e+00
Epoch 2/2
41/41 - 1s - loss: 110.2714 - loglik: -1.0950e+02 - logprior: -7.6847e-01
Fitted a model with MAP estimate = -108.6160
expansions: []
discards: [11 15]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 5s - loss: 112.0005 - loglik: -1.1102e+02 - logprior: -9.7612e-01
Epoch 2/2
41/41 - 1s - loss: 110.2959 - loglik: -1.0955e+02 - logprior: -7.4030e-01
Fitted a model with MAP estimate = -108.6140
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 109.1789 - loglik: -1.0857e+02 - logprior: -6.0889e-01
Epoch 2/10
58/58 - 2s - loss: 107.9792 - loglik: -1.0747e+02 - logprior: -5.0558e-01
Epoch 3/10
58/58 - 2s - loss: 107.7202 - loglik: -1.0722e+02 - logprior: -4.9817e-01
Epoch 4/10
58/58 - 2s - loss: 107.6213 - loglik: -1.0712e+02 - logprior: -4.9865e-01
Epoch 5/10
58/58 - 2s - loss: 107.6679 - loglik: -1.0716e+02 - logprior: -4.9615e-01
Fitted a model with MAP estimate = -107.4861
Time for alignment: 56.0702
Fitting a model of length 18 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
41/41 - 4s - loss: 127.2560 - loglik: -1.2642e+02 - logprior: -8.3267e-01
Epoch 2/10
41/41 - 1s - loss: 113.0408 - loglik: -1.1223e+02 - logprior: -8.0996e-01
Epoch 3/10
41/41 - 1s - loss: 112.2359 - loglik: -1.1144e+02 - logprior: -7.9198e-01
Epoch 4/10
41/41 - 1s - loss: 112.3305 - loglik: -1.1154e+02 - logprior: -7.8709e-01
Fitted a model with MAP estimate = -110.4128
expansions: [(4, 1), (9, 2), (10, 1), (11, 2), (12, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 25 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 113.4068 - loglik: -1.1240e+02 - logprior: -1.0056e+00
Epoch 2/2
41/41 - 1s - loss: 110.1992 - loglik: -1.0943e+02 - logprior: -7.6933e-01
Fitted a model with MAP estimate = -108.6231
expansions: []
discards: [11 15]
Re-initialized the encoder parameters.
Fitting a model of length 23 on 44173 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
41/41 - 4s - loss: 112.0816 - loglik: -1.1111e+02 - logprior: -9.7587e-01
Epoch 2/2
41/41 - 1s - loss: 110.1892 - loglik: -1.0945e+02 - logprior: -7.3894e-01
Fitted a model with MAP estimate = -108.6072
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 23 on 88345 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
58/58 - 4s - loss: 109.1705 - loglik: -1.0856e+02 - logprior: -6.0761e-01
Epoch 2/10
58/58 - 1s - loss: 107.7815 - loglik: -1.0727e+02 - logprior: -5.0538e-01
Epoch 3/10
58/58 - 1s - loss: 107.9030 - loglik: -1.0740e+02 - logprior: -5.0042e-01
Fitted a model with MAP estimate = -107.6875
Time for alignment: 50.4845
Computed alignments with likelihoods: ['-107.4751', '-107.6894', '-107.1676', '-107.4861', '-107.6875']
Best model has likelihood: -107.1676  (prior= -0.4892 )
time for generating output: 0.0796
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/zf-CCHH.projection.fasta
SP score = 0.966464502318944
Training of 5 independent models on file scorptoxin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9d6084c40>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe927b5a790>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe927b5a760>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 438.6913 - loglik: -3.1451e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 317.7151 - loglik: -2.8391e+02 - logprior: -3.3802e+01
Epoch 3/10
10/10 - 1s - loss: 281.6205 - loglik: -2.6558e+02 - logprior: -1.6043e+01
Epoch 4/10
10/10 - 0s - loss: 265.0416 - loglik: -2.5547e+02 - logprior: -9.5700e+00
Epoch 5/10
10/10 - 0s - loss: 255.9734 - loglik: -2.4993e+02 - logprior: -6.0437e+00
Epoch 6/10
10/10 - 0s - loss: 250.8732 - loglik: -2.4677e+02 - logprior: -4.1010e+00
Epoch 7/10
10/10 - 0s - loss: 248.2170 - loglik: -2.4540e+02 - logprior: -2.8147e+00
Epoch 8/10
10/10 - 1s - loss: 246.8697 - loglik: -2.4488e+02 - logprior: -1.9901e+00
Epoch 9/10
10/10 - 1s - loss: 246.0100 - loglik: -2.4455e+02 - logprior: -1.4601e+00
Epoch 10/10
10/10 - 0s - loss: 245.3965 - loglik: -2.4437e+02 - logprior: -1.0215e+00
Fitted a model with MAP estimate = -245.1350
expansions: [(9, 2), (13, 1), (14, 2), (15, 1), (16, 1), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 398.5441 - loglik: -2.5943e+02 - logprior: -1.3912e+02
Epoch 2/2
10/10 - 1s - loss: 301.2188 - loglik: -2.4393e+02 - logprior: -5.7289e+01
Fitted a model with MAP estimate = -284.8862
expansions: [(0, 2)]
discards: [ 0  8 29]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 359.3027 - loglik: -2.4760e+02 - logprior: -1.1170e+02
Epoch 2/2
10/10 - 1s - loss: 268.4713 - loglik: -2.3849e+02 - logprior: -2.9977e+01
Fitted a model with MAP estimate = -254.8301
expansions: []
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 378.0490 - loglik: -2.4873e+02 - logprior: -1.2932e+02
Epoch 2/10
10/10 - 1s - loss: 277.4710 - loglik: -2.4038e+02 - logprior: -3.7088e+01
Epoch 3/10
10/10 - 0s - loss: 253.1817 - loglik: -2.3894e+02 - logprior: -1.4237e+01
Epoch 4/10
10/10 - 1s - loss: 244.4101 - loglik: -2.3841e+02 - logprior: -5.9963e+00
Epoch 5/10
10/10 - 0s - loss: 239.3505 - loglik: -2.3736e+02 - logprior: -1.9853e+00
Epoch 6/10
10/10 - 1s - loss: 236.4470 - loglik: -2.3665e+02 - logprior: 0.2070
Epoch 7/10
10/10 - 0s - loss: 234.5894 - loglik: -2.3617e+02 - logprior: 1.5791
Epoch 8/10
10/10 - 1s - loss: 233.3389 - loglik: -2.3586e+02 - logprior: 2.5202
Epoch 9/10
10/10 - 1s - loss: 232.3381 - loglik: -2.3560e+02 - logprior: 3.2643
Epoch 10/10
10/10 - 1s - loss: 231.4966 - loglik: -2.3539e+02 - logprior: 3.8944
Fitted a model with MAP estimate = -231.0559
Time for alignment: 28.4453
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 438.6913 - loglik: -3.1451e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 317.7151 - loglik: -2.8391e+02 - logprior: -3.3802e+01
Epoch 3/10
10/10 - 1s - loss: 281.6205 - loglik: -2.6558e+02 - logprior: -1.6043e+01
Epoch 4/10
10/10 - 0s - loss: 265.0416 - loglik: -2.5547e+02 - logprior: -9.5700e+00
Epoch 5/10
10/10 - 1s - loss: 255.9734 - loglik: -2.4993e+02 - logprior: -6.0437e+00
Epoch 6/10
10/10 - 0s - loss: 250.8732 - loglik: -2.4677e+02 - logprior: -4.1010e+00
Epoch 7/10
10/10 - 1s - loss: 248.2170 - loglik: -2.4540e+02 - logprior: -2.8147e+00
Epoch 8/10
10/10 - 1s - loss: 246.8697 - loglik: -2.4488e+02 - logprior: -1.9901e+00
Epoch 9/10
10/10 - 1s - loss: 246.0099 - loglik: -2.4455e+02 - logprior: -1.4601e+00
Epoch 10/10
10/10 - 1s - loss: 245.3965 - loglik: -2.4437e+02 - logprior: -1.0215e+00
Fitted a model with MAP estimate = -245.1347
expansions: [(9, 2), (13, 1), (14, 2), (15, 1), (16, 1), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 398.5441 - loglik: -2.5943e+02 - logprior: -1.3912e+02
Epoch 2/2
10/10 - 1s - loss: 301.2188 - loglik: -2.4393e+02 - logprior: -5.7289e+01
Fitted a model with MAP estimate = -284.8862
expansions: [(0, 2)]
discards: [ 0  8 29]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 359.3027 - loglik: -2.4760e+02 - logprior: -1.1170e+02
Epoch 2/2
10/10 - 1s - loss: 268.4713 - loglik: -2.3849e+02 - logprior: -2.9977e+01
Fitted a model with MAP estimate = -254.8301
expansions: []
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 378.0489 - loglik: -2.4873e+02 - logprior: -1.2932e+02
Epoch 2/10
10/10 - 0s - loss: 277.4710 - loglik: -2.4038e+02 - logprior: -3.7088e+01
Epoch 3/10
10/10 - 0s - loss: 253.1816 - loglik: -2.3894e+02 - logprior: -1.4237e+01
Epoch 4/10
10/10 - 0s - loss: 244.4099 - loglik: -2.3841e+02 - logprior: -5.9961e+00
Epoch 5/10
10/10 - 1s - loss: 239.3503 - loglik: -2.3737e+02 - logprior: -1.9849e+00
Epoch 6/10
10/10 - 1s - loss: 236.4465 - loglik: -2.3665e+02 - logprior: 0.2073
Epoch 7/10
10/10 - 0s - loss: 234.5888 - loglik: -2.3617e+02 - logprior: 1.5795
Epoch 8/10
10/10 - 0s - loss: 233.3382 - loglik: -2.3586e+02 - logprior: 2.5208
Epoch 9/10
10/10 - 1s - loss: 232.3376 - loglik: -2.3560e+02 - logprior: 3.2650
Epoch 10/10
10/10 - 1s - loss: 231.4957 - loglik: -2.3539e+02 - logprior: 3.8951
Fitted a model with MAP estimate = -231.0550
Time for alignment: 27.6739
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 438.6913 - loglik: -3.1451e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 1s - loss: 317.7151 - loglik: -2.8391e+02 - logprior: -3.3802e+01
Epoch 3/10
10/10 - 1s - loss: 281.6205 - loglik: -2.6558e+02 - logprior: -1.6043e+01
Epoch 4/10
10/10 - 1s - loss: 265.0416 - loglik: -2.5547e+02 - logprior: -9.5700e+00
Epoch 5/10
10/10 - 1s - loss: 255.9734 - loglik: -2.4993e+02 - logprior: -6.0437e+00
Epoch 6/10
10/10 - 1s - loss: 250.8732 - loglik: -2.4677e+02 - logprior: -4.1010e+00
Epoch 7/10
10/10 - 1s - loss: 248.2170 - loglik: -2.4540e+02 - logprior: -2.8147e+00
Epoch 8/10
10/10 - 1s - loss: 246.8697 - loglik: -2.4488e+02 - logprior: -1.9901e+00
Epoch 9/10
10/10 - 1s - loss: 246.0101 - loglik: -2.4455e+02 - logprior: -1.4601e+00
Epoch 10/10
10/10 - 1s - loss: 245.3965 - loglik: -2.4437e+02 - logprior: -1.0215e+00
Fitted a model with MAP estimate = -245.1346
expansions: [(9, 2), (13, 1), (14, 2), (15, 1), (16, 1), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 398.5441 - loglik: -2.5943e+02 - logprior: -1.3912e+02
Epoch 2/2
10/10 - 1s - loss: 301.2188 - loglik: -2.4393e+02 - logprior: -5.7289e+01
Fitted a model with MAP estimate = -284.8862
expansions: [(0, 2)]
discards: [ 0  8 29]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 359.3027 - loglik: -2.4760e+02 - logprior: -1.1170e+02
Epoch 2/2
10/10 - 0s - loss: 268.4713 - loglik: -2.3849e+02 - logprior: -2.9977e+01
Fitted a model with MAP estimate = -254.8301
expansions: []
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 378.0489 - loglik: -2.4873e+02 - logprior: -1.2932e+02
Epoch 2/10
10/10 - 1s - loss: 277.4710 - loglik: -2.4038e+02 - logprior: -3.7088e+01
Epoch 3/10
10/10 - 1s - loss: 253.1816 - loglik: -2.3894e+02 - logprior: -1.4237e+01
Epoch 4/10
10/10 - 1s - loss: 244.4099 - loglik: -2.3841e+02 - logprior: -5.9960e+00
Epoch 5/10
10/10 - 1s - loss: 239.3503 - loglik: -2.3737e+02 - logprior: -1.9849e+00
Epoch 6/10
10/10 - 1s - loss: 236.4465 - loglik: -2.3665e+02 - logprior: 0.2074
Epoch 7/10
10/10 - 1s - loss: 234.5888 - loglik: -2.3617e+02 - logprior: 1.5796
Epoch 8/10
10/10 - 1s - loss: 233.3383 - loglik: -2.3586e+02 - logprior: 2.5208
Epoch 9/10
10/10 - 1s - loss: 232.3376 - loglik: -2.3560e+02 - logprior: 3.2650
Epoch 10/10
10/10 - 1s - loss: 231.4957 - loglik: -2.3539e+02 - logprior: 3.8951
Fitted a model with MAP estimate = -231.0549
Time for alignment: 27.7116
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 438.6913 - loglik: -3.1451e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 317.7151 - loglik: -2.8391e+02 - logprior: -3.3802e+01
Epoch 3/10
10/10 - 0s - loss: 281.6205 - loglik: -2.6558e+02 - logprior: -1.6043e+01
Epoch 4/10
10/10 - 1s - loss: 265.0416 - loglik: -2.5547e+02 - logprior: -9.5700e+00
Epoch 5/10
10/10 - 1s - loss: 255.9734 - loglik: -2.4993e+02 - logprior: -6.0437e+00
Epoch 6/10
10/10 - 0s - loss: 250.8732 - loglik: -2.4677e+02 - logprior: -4.1010e+00
Epoch 7/10
10/10 - 1s - loss: 248.2170 - loglik: -2.4540e+02 - logprior: -2.8147e+00
Epoch 8/10
10/10 - 1s - loss: 246.8697 - loglik: -2.4488e+02 - logprior: -1.9901e+00
Epoch 9/10
10/10 - 1s - loss: 246.0100 - loglik: -2.4455e+02 - logprior: -1.4601e+00
Epoch 10/10
10/10 - 1s - loss: 245.3965 - loglik: -2.4437e+02 - logprior: -1.0215e+00
Fitted a model with MAP estimate = -245.1348
expansions: [(9, 2), (13, 1), (14, 2), (15, 1), (16, 1), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 398.5441 - loglik: -2.5943e+02 - logprior: -1.3912e+02
Epoch 2/2
10/10 - 1s - loss: 301.2188 - loglik: -2.4393e+02 - logprior: -5.7289e+01
Fitted a model with MAP estimate = -284.8862
expansions: [(0, 2)]
discards: [ 0  8 29]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 359.3027 - loglik: -2.4760e+02 - logprior: -1.1170e+02
Epoch 2/2
10/10 - 0s - loss: 268.4713 - loglik: -2.3849e+02 - logprior: -2.9977e+01
Fitted a model with MAP estimate = -254.8301
expansions: []
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 378.0490 - loglik: -2.4873e+02 - logprior: -1.2932e+02
Epoch 2/10
10/10 - 1s - loss: 277.4710 - loglik: -2.4038e+02 - logprior: -3.7087e+01
Epoch 3/10
10/10 - 0s - loss: 253.1815 - loglik: -2.3894e+02 - logprior: -1.4237e+01
Epoch 4/10
10/10 - 0s - loss: 244.4099 - loglik: -2.3841e+02 - logprior: -5.9961e+00
Epoch 5/10
10/10 - 1s - loss: 239.3503 - loglik: -2.3736e+02 - logprior: -1.9851e+00
Epoch 6/10
10/10 - 0s - loss: 236.4468 - loglik: -2.3665e+02 - logprior: 0.2071
Epoch 7/10
10/10 - 0s - loss: 234.5892 - loglik: -2.3617e+02 - logprior: 1.5792
Epoch 8/10
10/10 - 0s - loss: 233.3386 - loglik: -2.3586e+02 - logprior: 2.5204
Epoch 9/10
10/10 - 1s - loss: 232.3381 - loglik: -2.3560e+02 - logprior: 3.2646
Epoch 10/10
10/10 - 0s - loss: 231.4963 - loglik: -2.3539e+02 - logprior: 3.8946
Fitted a model with MAP estimate = -231.0555
Time for alignment: 26.5945
Fitting a model of length 43 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 438.6913 - loglik: -3.1451e+02 - logprior: -1.2418e+02
Epoch 2/10
10/10 - 0s - loss: 317.7151 - loglik: -2.8391e+02 - logprior: -3.3802e+01
Epoch 3/10
10/10 - 0s - loss: 281.6205 - loglik: -2.6558e+02 - logprior: -1.6043e+01
Epoch 4/10
10/10 - 0s - loss: 265.0416 - loglik: -2.5547e+02 - logprior: -9.5700e+00
Epoch 5/10
10/10 - 0s - loss: 255.9734 - loglik: -2.4993e+02 - logprior: -6.0437e+00
Epoch 6/10
10/10 - 0s - loss: 250.8732 - loglik: -2.4677e+02 - logprior: -4.1010e+00
Epoch 7/10
10/10 - 0s - loss: 248.2170 - loglik: -2.4540e+02 - logprior: -2.8147e+00
Epoch 8/10
10/10 - 1s - loss: 246.8696 - loglik: -2.4488e+02 - logprior: -1.9901e+00
Epoch 9/10
10/10 - 0s - loss: 246.0100 - loglik: -2.4455e+02 - logprior: -1.4601e+00
Epoch 10/10
10/10 - 1s - loss: 245.3965 - loglik: -2.4437e+02 - logprior: -1.0215e+00
Fitted a model with MAP estimate = -245.1349
expansions: [(9, 2), (13, 1), (14, 2), (15, 1), (16, 1), (23, 3), (31, 3), (32, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 398.5441 - loglik: -2.5943e+02 - logprior: -1.3912e+02
Epoch 2/2
10/10 - 1s - loss: 301.2189 - loglik: -2.4393e+02 - logprior: -5.7289e+01
Fitted a model with MAP estimate = -284.8862
expansions: [(0, 2)]
discards: [ 0  8 29]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 359.3027 - loglik: -2.4760e+02 - logprior: -1.1170e+02
Epoch 2/2
10/10 - 0s - loss: 268.4713 - loglik: -2.3849e+02 - logprior: -2.9977e+01
Fitted a model with MAP estimate = -254.8301
expansions: []
discards: [ 0 40]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 363 sequences.
Batch size= 363 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 378.0488 - loglik: -2.4873e+02 - logprior: -1.2932e+02
Epoch 2/10
10/10 - 1s - loss: 277.4712 - loglik: -2.4038e+02 - logprior: -3.7088e+01
Epoch 3/10
10/10 - 1s - loss: 253.1820 - loglik: -2.3894e+02 - logprior: -1.4238e+01
Epoch 4/10
10/10 - 1s - loss: 244.4104 - loglik: -2.3841e+02 - logprior: -5.9968e+00
Epoch 5/10
10/10 - 1s - loss: 239.3510 - loglik: -2.3736e+02 - logprior: -1.9858e+00
Epoch 6/10
10/10 - 1s - loss: 236.4474 - loglik: -2.3665e+02 - logprior: 0.2065
Epoch 7/10
10/10 - 1s - loss: 234.5898 - loglik: -2.3617e+02 - logprior: 1.5786
Epoch 8/10
10/10 - 1s - loss: 233.3394 - loglik: -2.3586e+02 - logprior: 2.5197
Epoch 9/10
10/10 - 1s - loss: 232.3386 - loglik: -2.3560e+02 - logprior: 3.2638
Epoch 10/10
10/10 - 1s - loss: 231.4970 - loglik: -2.3539e+02 - logprior: 3.8938
Fitted a model with MAP estimate = -231.0561
Time for alignment: 26.6620
Computed alignments with likelihoods: ['-231.0559', '-231.0550', '-231.0549', '-231.0555', '-231.0561']
Best model has likelihood: -231.0549  (prior= 4.2095 )
time for generating output: 0.1046
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/scorptoxin.projection.fasta
SP score = 0.8853919239904988
Training of 5 independent models on file biotin_lipoyl.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea093ea700>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea44bc4a90>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea4d054640>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 394.5167 - loglik: -3.9140e+02 - logprior: -3.1151e+00
Epoch 2/10
19/19 - 1s - loss: 355.4586 - loglik: -3.5430e+02 - logprior: -1.1626e+00
Epoch 3/10
19/19 - 1s - loss: 340.2668 - loglik: -3.3902e+02 - logprior: -1.2432e+00
Epoch 4/10
19/19 - 1s - loss: 337.3082 - loglik: -3.3609e+02 - logprior: -1.2157e+00
Epoch 5/10
19/19 - 1s - loss: 336.1127 - loglik: -3.3494e+02 - logprior: -1.1648e+00
Epoch 6/10
19/19 - 1s - loss: 335.1874 - loglik: -3.3404e+02 - logprior: -1.1382e+00
Epoch 7/10
19/19 - 1s - loss: 333.9183 - loglik: -3.3280e+02 - logprior: -1.1126e+00
Epoch 8/10
19/19 - 1s - loss: 333.0882 - loglik: -3.3198e+02 - logprior: -1.0940e+00
Epoch 9/10
19/19 - 1s - loss: 332.0388 - loglik: -3.3094e+02 - logprior: -1.0850e+00
Epoch 10/10
19/19 - 1s - loss: 331.5423 - loglik: -3.3044e+02 - logprior: -1.0834e+00
Fitted a model with MAP estimate = -330.4740
expansions: [(0, 3), (11, 2), (12, 1), (14, 1), (15, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 345.9282 - loglik: -3.4153e+02 - logprior: -4.3932e+00
Epoch 2/2
19/19 - 1s - loss: 329.9145 - loglik: -3.2838e+02 - logprior: -1.5317e+00
Fitted a model with MAP estimate = -326.4597
expansions: [(0, 2)]
discards: [15 22 30 51 62 67]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 333.5443 - loglik: -3.2941e+02 - logprior: -4.1294e+00
Epoch 2/2
19/19 - 1s - loss: 327.4480 - loglik: -3.2622e+02 - logprior: -1.2246e+00
Fitted a model with MAP estimate = -325.1002
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 330.1306 - loglik: -3.2678e+02 - logprior: -3.3465e+00
Epoch 2/10
21/21 - 1s - loss: 325.2421 - loglik: -3.2382e+02 - logprior: -1.4197e+00
Epoch 3/10
21/21 - 1s - loss: 323.9549 - loglik: -3.2290e+02 - logprior: -1.0529e+00
Epoch 4/10
21/21 - 1s - loss: 323.3238 - loglik: -3.2231e+02 - logprior: -1.0136e+00
Epoch 5/10
21/21 - 1s - loss: 322.6034 - loglik: -3.2160e+02 - logprior: -9.9536e-01
Epoch 6/10
21/21 - 1s - loss: 320.6025 - loglik: -3.1960e+02 - logprior: -9.9248e-01
Epoch 7/10
21/21 - 1s - loss: 319.0858 - loglik: -3.1811e+02 - logprior: -9.6794e-01
Epoch 8/10
21/21 - 1s - loss: 317.4563 - loglik: -3.1649e+02 - logprior: -9.5717e-01
Epoch 9/10
21/21 - 1s - loss: 316.2062 - loglik: -3.1525e+02 - logprior: -9.4205e-01
Epoch 10/10
21/21 - 1s - loss: 314.4296 - loglik: -3.1347e+02 - logprior: -9.3618e-01
Fitted a model with MAP estimate = -312.2798
Time for alignment: 49.1050
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 7s - loss: 394.4324 - loglik: -3.9132e+02 - logprior: -3.1172e+00
Epoch 2/10
19/19 - 1s - loss: 355.4083 - loglik: -3.5425e+02 - logprior: -1.1621e+00
Epoch 3/10
19/19 - 1s - loss: 339.5660 - loglik: -3.3832e+02 - logprior: -1.2487e+00
Epoch 4/10
19/19 - 1s - loss: 336.8716 - loglik: -3.3566e+02 - logprior: -1.2136e+00
Epoch 5/10
19/19 - 1s - loss: 335.7053 - loglik: -3.3455e+02 - logprior: -1.1507e+00
Epoch 6/10
19/19 - 1s - loss: 334.9753 - loglik: -3.3385e+02 - logprior: -1.1229e+00
Epoch 7/10
19/19 - 1s - loss: 333.5579 - loglik: -3.3246e+02 - logprior: -1.0942e+00
Epoch 8/10
19/19 - 1s - loss: 332.5687 - loglik: -3.3148e+02 - logprior: -1.0827e+00
Epoch 9/10
19/19 - 1s - loss: 331.9026 - loglik: -3.3081e+02 - logprior: -1.0788e+00
Epoch 10/10
19/19 - 1s - loss: 330.9962 - loglik: -3.2990e+02 - logprior: -1.0830e+00
Fitted a model with MAP estimate = -330.2225
expansions: [(0, 3), (11, 2), (13, 1), (14, 1), (19, 2), (24, 2), (33, 1), (39, 2), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 79 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 345.5496 - loglik: -3.4117e+02 - logprior: -4.3751e+00
Epoch 2/2
19/19 - 1s - loss: 329.3744 - loglik: -3.2787e+02 - logprior: -1.5074e+00
Fitted a model with MAP estimate = -326.2168
expansions: [(0, 2)]
discards: [15 26 33 51 63 67]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 333.5937 - loglik: -3.2947e+02 - logprior: -4.1237e+00
Epoch 2/2
19/19 - 1s - loss: 327.1087 - loglik: -3.2589e+02 - logprior: -1.2209e+00
Fitted a model with MAP estimate = -324.9766
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 329.7356 - loglik: -3.2640e+02 - logprior: -3.3315e+00
Epoch 2/10
21/21 - 1s - loss: 325.3487 - loglik: -3.2395e+02 - logprior: -1.3970e+00
Epoch 3/10
21/21 - 1s - loss: 323.7571 - loglik: -3.2271e+02 - logprior: -1.0486e+00
Epoch 4/10
21/21 - 1s - loss: 323.0093 - loglik: -3.2200e+02 - logprior: -1.0079e+00
Epoch 5/10
21/21 - 1s - loss: 321.8816 - loglik: -3.2088e+02 - logprior: -9.9501e-01
Epoch 6/10
21/21 - 1s - loss: 320.8400 - loglik: -3.1984e+02 - logprior: -9.8894e-01
Epoch 7/10
21/21 - 1s - loss: 319.1850 - loglik: -3.1821e+02 - logprior: -9.6463e-01
Epoch 8/10
21/21 - 1s - loss: 317.5411 - loglik: -3.1657e+02 - logprior: -9.5416e-01
Epoch 9/10
21/21 - 1s - loss: 316.1733 - loglik: -3.1522e+02 - logprior: -9.3204e-01
Epoch 10/10
21/21 - 1s - loss: 314.4341 - loglik: -3.1348e+02 - logprior: -9.3297e-01
Fitted a model with MAP estimate = -312.7904
Time for alignment: 56.3746
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 394.4219 - loglik: -3.9131e+02 - logprior: -3.1161e+00
Epoch 2/10
19/19 - 1s - loss: 354.2831 - loglik: -3.5312e+02 - logprior: -1.1618e+00
Epoch 3/10
19/19 - 1s - loss: 339.5563 - loglik: -3.3832e+02 - logprior: -1.2392e+00
Epoch 4/10
19/19 - 1s - loss: 336.8535 - loglik: -3.3565e+02 - logprior: -1.2067e+00
Epoch 5/10
19/19 - 1s - loss: 335.9158 - loglik: -3.3477e+02 - logprior: -1.1477e+00
Epoch 6/10
19/19 - 1s - loss: 334.3629 - loglik: -3.3324e+02 - logprior: -1.1207e+00
Epoch 7/10
19/19 - 1s - loss: 333.7540 - loglik: -3.3265e+02 - logprior: -1.0954e+00
Epoch 8/10
19/19 - 1s - loss: 332.3172 - loglik: -3.3123e+02 - logprior: -1.0755e+00
Epoch 9/10
19/19 - 1s - loss: 331.1804 - loglik: -3.3010e+02 - logprior: -1.0707e+00
Epoch 10/10
19/19 - 1s - loss: 330.7794 - loglik: -3.2970e+02 - logprior: -1.0644e+00
Fitted a model with MAP estimate = -329.9631
expansions: [(0, 3), (13, 3), (14, 1), (19, 2), (20, 2), (33, 1), (39, 2), (46, 1), (47, 1), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 345.6454 - loglik: -3.4131e+02 - logprior: -4.3378e+00
Epoch 2/2
19/19 - 1s - loss: 330.5389 - loglik: -3.2907e+02 - logprior: -1.4707e+00
Fitted a model with MAP estimate = -327.2680
expansions: [(0, 2)]
discards: [17 26 29 51 66]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 333.8838 - loglik: -3.2978e+02 - logprior: -4.1074e+00
Epoch 2/2
19/19 - 1s - loss: 327.8214 - loglik: -3.2662e+02 - logprior: -1.2024e+00
Fitted a model with MAP estimate = -325.1371
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 329.7713 - loglik: -3.2644e+02 - logprior: -3.3351e+00
Epoch 2/10
21/21 - 1s - loss: 324.7466 - loglik: -3.2334e+02 - logprior: -1.4023e+00
Epoch 3/10
21/21 - 1s - loss: 323.1440 - loglik: -3.2210e+02 - logprior: -1.0458e+00
Epoch 4/10
21/21 - 1s - loss: 323.0855 - loglik: -3.2207e+02 - logprior: -1.0182e+00
Epoch 5/10
21/21 - 1s - loss: 321.6670 - loglik: -3.2065e+02 - logprior: -1.0119e+00
Epoch 6/10
21/21 - 1s - loss: 319.8301 - loglik: -3.1884e+02 - logprior: -9.8596e-01
Epoch 7/10
21/21 - 1s - loss: 318.6588 - loglik: -3.1768e+02 - logprior: -9.6740e-01
Epoch 8/10
21/21 - 1s - loss: 316.8289 - loglik: -3.1586e+02 - logprior: -9.5753e-01
Epoch 9/10
21/21 - 1s - loss: 315.5056 - loglik: -3.1455e+02 - logprior: -9.4154e-01
Epoch 10/10
21/21 - 1s - loss: 313.6808 - loglik: -3.1272e+02 - logprior: -9.3531e-01
Fitted a model with MAP estimate = -312.1771
Time for alignment: 54.0993
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 394.6523 - loglik: -3.9154e+02 - logprior: -3.1134e+00
Epoch 2/10
19/19 - 1s - loss: 355.4708 - loglik: -3.5432e+02 - logprior: -1.1506e+00
Epoch 3/10
19/19 - 1s - loss: 342.2682 - loglik: -3.4107e+02 - logprior: -1.2015e+00
Epoch 4/10
19/19 - 1s - loss: 339.5178 - loglik: -3.3836e+02 - logprior: -1.1559e+00
Epoch 5/10
19/19 - 1s - loss: 338.3819 - loglik: -3.3728e+02 - logprior: -1.0994e+00
Epoch 6/10
19/19 - 1s - loss: 337.2551 - loglik: -3.3618e+02 - logprior: -1.0746e+00
Epoch 7/10
19/19 - 1s - loss: 335.8850 - loglik: -3.3482e+02 - logprior: -1.0575e+00
Epoch 8/10
19/19 - 1s - loss: 334.6639 - loglik: -3.3361e+02 - logprior: -1.0417e+00
Epoch 9/10
19/19 - 1s - loss: 333.9604 - loglik: -3.3291e+02 - logprior: -1.0385e+00
Epoch 10/10
19/19 - 1s - loss: 332.6916 - loglik: -3.3165e+02 - logprior: -1.0260e+00
Fitted a model with MAP estimate = -331.9956
expansions: [(0, 4), (11, 2), (12, 1), (14, 1), (15, 2), (33, 1), (34, 1), (39, 2), (46, 1), (47, 2), (49, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 345.4261 - loglik: -3.4106e+02 - logprior: -4.3665e+00
Epoch 2/2
19/19 - 1s - loss: 329.3085 - loglik: -3.2780e+02 - logprior: -1.5104e+00
Fitted a model with MAP estimate = -325.8602
expansions: []
discards: [16 23 51 66]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 331.1766 - loglik: -3.2817e+02 - logprior: -3.0098e+00
Epoch 2/2
19/19 - 1s - loss: 327.0463 - loglik: -3.2587e+02 - logprior: -1.1811e+00
Fitted a model with MAP estimate = -325.2138
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 6s - loss: 327.8001 - loglik: -3.2539e+02 - logprior: -2.4137e+00
Epoch 2/10
21/21 - 1s - loss: 324.5432 - loglik: -3.2346e+02 - logprior: -1.0846e+00
Epoch 3/10
21/21 - 1s - loss: 324.0723 - loglik: -3.2301e+02 - logprior: -1.0664e+00
Epoch 4/10
21/21 - 1s - loss: 322.7073 - loglik: -3.2171e+02 - logprior: -9.9986e-01
Epoch 5/10
21/21 - 1s - loss: 322.3614 - loglik: -3.2138e+02 - logprior: -9.7527e-01
Epoch 6/10
21/21 - 1s - loss: 320.6601 - loglik: -3.1969e+02 - logprior: -9.6272e-01
Epoch 7/10
21/21 - 1s - loss: 319.5352 - loglik: -3.1858e+02 - logprior: -9.4077e-01
Epoch 8/10
21/21 - 1s - loss: 317.6624 - loglik: -3.1673e+02 - logprior: -9.2330e-01
Epoch 9/10
21/21 - 1s - loss: 316.1346 - loglik: -3.1520e+02 - logprior: -9.1365e-01
Epoch 10/10
21/21 - 1s - loss: 314.5948 - loglik: -3.1367e+02 - logprior: -9.0713e-01
Fitted a model with MAP estimate = -312.7633
Time for alignment: 52.0316
Fitting a model of length 59 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 4s - loss: 394.4500 - loglik: -3.9133e+02 - logprior: -3.1194e+00
Epoch 2/10
19/19 - 1s - loss: 355.7946 - loglik: -3.5460e+02 - logprior: -1.1984e+00
Epoch 3/10
19/19 - 1s - loss: 341.6286 - loglik: -3.4034e+02 - logprior: -1.2835e+00
Epoch 4/10
19/19 - 1s - loss: 338.9816 - loglik: -3.3773e+02 - logprior: -1.2536e+00
Epoch 5/10
19/19 - 1s - loss: 337.4542 - loglik: -3.3625e+02 - logprior: -1.1978e+00
Epoch 6/10
19/19 - 1s - loss: 336.1690 - loglik: -3.3502e+02 - logprior: -1.1461e+00
Epoch 7/10
19/19 - 1s - loss: 333.9208 - loglik: -3.3281e+02 - logprior: -1.1022e+00
Epoch 8/10
19/19 - 1s - loss: 332.9347 - loglik: -3.3184e+02 - logprior: -1.0806e+00
Epoch 9/10
19/19 - 1s - loss: 331.7111 - loglik: -3.3062e+02 - logprior: -1.0795e+00
Epoch 10/10
19/19 - 1s - loss: 331.1786 - loglik: -3.3009e+02 - logprior: -1.0751e+00
Fitted a model with MAP estimate = -330.0244
expansions: [(0, 3), (8, 1), (13, 1), (14, 1), (15, 1), (34, 1), (37, 2), (44, 1), (46, 1), (47, 2), (50, 2), (51, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 76 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 343.8519 - loglik: -3.3947e+02 - logprior: -4.3784e+00
Epoch 2/2
19/19 - 1s - loss: 329.0626 - loglik: -3.2764e+02 - logprior: -1.4213e+00
Fitted a model with MAP estimate = -325.8622
expansions: [(0, 2)]
discards: [45 59 64]
Re-initialized the encoder parameters.
Fitting a model of length 75 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 4s - loss: 332.6510 - loglik: -3.2853e+02 - logprior: -4.1176e+00
Epoch 2/2
19/19 - 1s - loss: 326.7191 - loglik: -3.2551e+02 - logprior: -1.2094e+00
Fitted a model with MAP estimate = -324.1815
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 74 on 11833 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
21/21 - 4s - loss: 329.2110 - loglik: -3.2588e+02 - logprior: -3.3304e+00
Epoch 2/10
21/21 - 1s - loss: 324.5731 - loglik: -3.2319e+02 - logprior: -1.3846e+00
Epoch 3/10
21/21 - 1s - loss: 323.5810 - loglik: -3.2253e+02 - logprior: -1.0528e+00
Epoch 4/10
21/21 - 1s - loss: 322.4106 - loglik: -3.2138e+02 - logprior: -1.0313e+00
Epoch 5/10
21/21 - 1s - loss: 321.8189 - loglik: -3.2081e+02 - logprior: -1.0022e+00
Epoch 6/10
21/21 - 1s - loss: 320.3353 - loglik: -3.1935e+02 - logprior: -9.8214e-01
Epoch 7/10
21/21 - 1s - loss: 318.5767 - loglik: -3.1760e+02 - logprior: -9.6685e-01
Epoch 8/10
21/21 - 1s - loss: 317.0399 - loglik: -3.1607e+02 - logprior: -9.5572e-01
Epoch 9/10
21/21 - 1s - loss: 315.4701 - loglik: -3.1451e+02 - logprior: -9.3876e-01
Epoch 10/10
21/21 - 1s - loss: 313.9973 - loglik: -3.1304e+02 - logprior: -9.3506e-01
Fitted a model with MAP estimate = -312.4264
Time for alignment: 50.4671
Computed alignments with likelihoods: ['-312.2798', '-312.7904', '-312.1771', '-312.7633', '-312.4264']
Best model has likelihood: -312.1771  (prior= -0.9286 )
time for generating output: 0.1326
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/biotin_lipoyl.projection.fasta
SP score = 0.9451029320024953
Training of 5 independent models on file uce.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe939142e80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe927ba3340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9380ddc70>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 784.1071 - loglik: -7.7599e+02 - logprior: -8.1197e+00
Epoch 2/10
13/13 - 2s - loss: 729.1384 - loglik: -7.2726e+02 - logprior: -1.8767e+00
Epoch 3/10
13/13 - 2s - loss: 688.7612 - loglik: -6.8708e+02 - logprior: -1.6766e+00
Epoch 4/10
13/13 - 2s - loss: 677.3154 - loglik: -6.7546e+02 - logprior: -1.8576e+00
Epoch 5/10
13/13 - 2s - loss: 673.1811 - loglik: -6.7127e+02 - logprior: -1.9141e+00
Epoch 6/10
13/13 - 2s - loss: 671.7139 - loglik: -6.6989e+02 - logprior: -1.8177e+00
Epoch 7/10
13/13 - 2s - loss: 668.5684 - loglik: -6.6677e+02 - logprior: -1.7974e+00
Epoch 8/10
13/13 - 2s - loss: 668.2632 - loglik: -6.6644e+02 - logprior: -1.8213e+00
Epoch 9/10
13/13 - 2s - loss: 667.3308 - loglik: -6.6552e+02 - logprior: -1.8087e+00
Epoch 10/10
13/13 - 2s - loss: 665.7608 - loglik: -6.6391e+02 - logprior: -1.8446e+00
Fitted a model with MAP estimate = -665.2439
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (24, 1), (36, 1), (37, 1), (38, 1), (50, 1), (51, 1), (63, 1), (75, 1), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 677.7611 - loglik: -6.6830e+02 - logprior: -9.4614e+00
Epoch 2/2
13/13 - 2s - loss: 659.2757 - loglik: -6.5552e+02 - logprior: -3.7577e+00
Fitted a model with MAP estimate = -656.6688
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 660.9340 - loglik: -6.5357e+02 - logprior: -7.3600e+00
Epoch 2/2
13/13 - 2s - loss: 652.0986 - loglik: -6.5035e+02 - logprior: -1.7494e+00
Fitted a model with MAP estimate = -650.7752
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 7s - loss: 663.1907 - loglik: -6.5395e+02 - logprior: -9.2372e+00
Epoch 2/10
13/13 - 2s - loss: 653.6801 - loglik: -6.5090e+02 - logprior: -2.7771e+00
Epoch 3/10
13/13 - 2s - loss: 651.1927 - loglik: -6.5031e+02 - logprior: -8.8096e-01
Epoch 4/10
13/13 - 2s - loss: 648.9962 - loglik: -6.4866e+02 - logprior: -3.3867e-01
Epoch 5/10
13/13 - 2s - loss: 649.7317 - loglik: -6.4953e+02 - logprior: -2.0043e-01
Fitted a model with MAP estimate = -648.5353
Time for alignment: 69.7776
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 784.7750 - loglik: -7.7666e+02 - logprior: -8.1146e+00
Epoch 2/10
13/13 - 2s - loss: 729.5334 - loglik: -7.2767e+02 - logprior: -1.8654e+00
Epoch 3/10
13/13 - 2s - loss: 689.0865 - loglik: -6.8742e+02 - logprior: -1.6692e+00
Epoch 4/10
13/13 - 2s - loss: 676.5870 - loglik: -6.7473e+02 - logprior: -1.8523e+00
Epoch 5/10
13/13 - 2s - loss: 673.8722 - loglik: -6.7194e+02 - logprior: -1.9335e+00
Epoch 6/10
13/13 - 2s - loss: 672.0715 - loglik: -6.7023e+02 - logprior: -1.8381e+00
Epoch 7/10
13/13 - 2s - loss: 668.5233 - loglik: -6.6670e+02 - logprior: -1.8212e+00
Epoch 8/10
13/13 - 2s - loss: 669.7715 - loglik: -6.6794e+02 - logprior: -1.8307e+00
Fitted a model with MAP estimate = -668.1112
expansions: [(7, 2), (8, 2), (11, 1), (14, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (51, 1), (52, 1), (53, 2), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 678.9395 - loglik: -6.6949e+02 - logprior: -9.4500e+00
Epoch 2/2
13/13 - 2s - loss: 659.0005 - loglik: -6.5517e+02 - logprior: -3.8277e+00
Fitted a model with MAP estimate = -656.5685
expansions: [(0, 3)]
discards: [ 0 68 95]
Re-initialized the encoder parameters.
Fitting a model of length 142 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 661.6722 - loglik: -6.5434e+02 - logprior: -7.3356e+00
Epoch 2/2
13/13 - 2s - loss: 649.4909 - loglik: -6.4776e+02 - logprior: -1.7289e+00
Fitted a model with MAP estimate = -649.8936
expansions: []
discards: [ 0  2 89]
Re-initialized the encoder parameters.
Fitting a model of length 139 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 662.8616 - loglik: -6.5368e+02 - logprior: -9.1860e+00
Epoch 2/10
13/13 - 2s - loss: 653.6006 - loglik: -6.5085e+02 - logprior: -2.7502e+00
Epoch 3/10
13/13 - 2s - loss: 650.1582 - loglik: -6.4928e+02 - logprior: -8.8256e-01
Epoch 4/10
13/13 - 2s - loss: 650.1260 - loglik: -6.4980e+02 - logprior: -3.2759e-01
Epoch 5/10
13/13 - 2s - loss: 649.6592 - loglik: -6.4946e+02 - logprior: -1.9634e-01
Epoch 6/10
13/13 - 2s - loss: 647.7673 - loglik: -6.4763e+02 - logprior: -1.3870e-01
Epoch 7/10
13/13 - 2s - loss: 647.9899 - loglik: -6.4785e+02 - logprior: -1.3369e-01
Fitted a model with MAP estimate = -646.3432
Time for alignment: 68.4583
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 784.6743 - loglik: -7.7656e+02 - logprior: -8.1137e+00
Epoch 2/10
13/13 - 2s - loss: 729.2831 - loglik: -7.2741e+02 - logprior: -1.8686e+00
Epoch 3/10
13/13 - 2s - loss: 689.2120 - loglik: -6.8754e+02 - logprior: -1.6730e+00
Epoch 4/10
13/13 - 2s - loss: 677.0612 - loglik: -6.7517e+02 - logprior: -1.8906e+00
Epoch 5/10
13/13 - 2s - loss: 674.4469 - loglik: -6.7248e+02 - logprior: -1.9634e+00
Epoch 6/10
13/13 - 2s - loss: 671.7076 - loglik: -6.6986e+02 - logprior: -1.8414e+00
Epoch 7/10
13/13 - 2s - loss: 668.6244 - loglik: -6.6679e+02 - logprior: -1.8342e+00
Epoch 8/10
13/13 - 2s - loss: 669.5355 - loglik: -6.6767e+02 - logprior: -1.8561e+00
Fitted a model with MAP estimate = -667.6135
expansions: [(7, 2), (8, 2), (14, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (49, 1), (50, 1), (51, 1), (70, 2), (75, 2), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 678.5287 - loglik: -6.6907e+02 - logprior: -9.4573e+00
Epoch 2/2
13/13 - 2s - loss: 658.6179 - loglik: -6.5483e+02 - logprior: -3.7884e+00
Fitted a model with MAP estimate = -656.5117
expansions: [(0, 3)]
discards: [ 0 86 94]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 660.9235 - loglik: -6.5361e+02 - logprior: -7.3100e+00
Epoch 2/2
13/13 - 2s - loss: 651.0193 - loglik: -6.4933e+02 - logprior: -1.6889e+00
Fitted a model with MAP estimate = -650.2994
expansions: []
discards: [ 0  2 24]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 663.1324 - loglik: -6.5396e+02 - logprior: -9.1772e+00
Epoch 2/10
13/13 - 2s - loss: 654.0980 - loglik: -6.5133e+02 - logprior: -2.7661e+00
Epoch 3/10
13/13 - 2s - loss: 650.6321 - loglik: -6.4977e+02 - logprior: -8.6027e-01
Epoch 4/10
13/13 - 2s - loss: 650.1697 - loglik: -6.4987e+02 - logprior: -2.9895e-01
Epoch 5/10
13/13 - 2s - loss: 648.5784 - loglik: -6.4841e+02 - logprior: -1.6949e-01
Epoch 6/10
13/13 - 2s - loss: 649.4171 - loglik: -6.4930e+02 - logprior: -1.1494e-01
Fitted a model with MAP estimate = -647.5932
Time for alignment: 65.2712
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 782.5911 - loglik: -7.7447e+02 - logprior: -8.1196e+00
Epoch 2/10
13/13 - 2s - loss: 732.7153 - loglik: -7.3083e+02 - logprior: -1.8808e+00
Epoch 3/10
13/13 - 2s - loss: 692.0472 - loglik: -6.9030e+02 - logprior: -1.7510e+00
Epoch 4/10
13/13 - 2s - loss: 677.6299 - loglik: -6.7566e+02 - logprior: -1.9654e+00
Epoch 5/10
13/13 - 2s - loss: 673.2618 - loglik: -6.7117e+02 - logprior: -2.0950e+00
Epoch 6/10
13/13 - 2s - loss: 669.6321 - loglik: -6.6760e+02 - logprior: -2.0301e+00
Epoch 7/10
13/13 - 2s - loss: 668.1807 - loglik: -6.6619e+02 - logprior: -1.9868e+00
Epoch 8/10
13/13 - 2s - loss: 668.2917 - loglik: -6.6629e+02 - logprior: -1.9960e+00
Fitted a model with MAP estimate = -666.6341
expansions: [(7, 2), (8, 2), (14, 1), (15, 1), (16, 2), (21, 1), (22, 1), (23, 1), (37, 1), (38, 1), (39, 1), (49, 1), (50, 1), (53, 2), (70, 2), (75, 1), (76, 1), (77, 2), (86, 1), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 678.1939 - loglik: -6.6864e+02 - logprior: -9.5575e+00
Epoch 2/2
13/13 - 2s - loss: 657.7588 - loglik: -6.5397e+02 - logprior: -3.7900e+00
Fitted a model with MAP estimate = -655.8152
expansions: [(0, 3)]
discards: [ 0 68 88]
Re-initialized the encoder parameters.
Fitting a model of length 141 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 659.8710 - loglik: -6.5253e+02 - logprior: -7.3421e+00
Epoch 2/2
13/13 - 2s - loss: 652.1798 - loglik: -6.5045e+02 - logprior: -1.7314e+00
Fitted a model with MAP estimate = -650.2218
expansions: []
discards: [ 0  2 24]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 662.5367 - loglik: -6.5333e+02 - logprior: -9.2076e+00
Epoch 2/10
13/13 - 2s - loss: 653.8705 - loglik: -6.5118e+02 - logprior: -2.6896e+00
Epoch 3/10
13/13 - 2s - loss: 651.6224 - loglik: -6.5075e+02 - logprior: -8.7144e-01
Epoch 4/10
13/13 - 2s - loss: 649.7369 - loglik: -6.4943e+02 - logprior: -3.0121e-01
Epoch 5/10
13/13 - 2s - loss: 650.2347 - loglik: -6.5008e+02 - logprior: -1.5285e-01
Fitted a model with MAP estimate = -648.6059
Time for alignment: 62.4325
Fitting a model of length 110 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 785.6345 - loglik: -7.7752e+02 - logprior: -8.1178e+00
Epoch 2/10
13/13 - 2s - loss: 729.2972 - loglik: -7.2741e+02 - logprior: -1.8890e+00
Epoch 3/10
13/13 - 2s - loss: 693.3437 - loglik: -6.9161e+02 - logprior: -1.7292e+00
Epoch 4/10
13/13 - 2s - loss: 676.4202 - loglik: -6.7449e+02 - logprior: -1.9284e+00
Epoch 5/10
13/13 - 2s - loss: 673.6057 - loglik: -6.7163e+02 - logprior: -1.9761e+00
Epoch 6/10
13/13 - 2s - loss: 668.8030 - loglik: -6.6695e+02 - logprior: -1.8546e+00
Epoch 7/10
13/13 - 2s - loss: 670.4877 - loglik: -6.6863e+02 - logprior: -1.8513e+00
Fitted a model with MAP estimate = -668.1199
expansions: [(7, 2), (8, 2), (11, 1), (13, 1), (14, 1), (21, 1), (22, 1), (23, 1), (24, 1), (36, 1), (37, 1), (38, 1), (50, 1), (51, 1), (63, 1), (75, 1), (76, 4), (97, 1), (98, 2), (99, 2), (100, 1), (101, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 5s - loss: 675.4045 - loglik: -6.6593e+02 - logprior: -9.4709e+00
Epoch 2/2
13/13 - 2s - loss: 659.0333 - loglik: -6.5528e+02 - logprior: -3.7489e+00
Fitted a model with MAP estimate = -656.3302
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 140 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 660.4233 - loglik: -6.5307e+02 - logprior: -7.3582e+00
Epoch 2/2
13/13 - 2s - loss: 651.6516 - loglik: -6.4991e+02 - logprior: -1.7433e+00
Fitted a model with MAP estimate = -650.3220
expansions: []
discards: [0 2]
Re-initialized the encoder parameters.
Fitting a model of length 138 on 4558 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 5s - loss: 662.1047 - loglik: -6.5288e+02 - logprior: -9.2244e+00
Epoch 2/10
13/13 - 2s - loss: 654.4241 - loglik: -6.5167e+02 - logprior: -2.7502e+00
Epoch 3/10
13/13 - 2s - loss: 651.2844 - loglik: -6.5038e+02 - logprior: -9.0551e-01
Epoch 4/10
13/13 - 2s - loss: 648.7238 - loglik: -6.4835e+02 - logprior: -3.7172e-01
Epoch 5/10
13/13 - 2s - loss: 649.2335 - loglik: -6.4901e+02 - logprior: -2.2404e-01
Fitted a model with MAP estimate = -648.5355
Time for alignment: 60.0939
Computed alignments with likelihoods: ['-648.5353', '-646.3432', '-647.5932', '-648.6059', '-648.5355']
Best model has likelihood: -646.3432  (prior= -0.1388 )
time for generating output: 0.1648
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/uce.projection.fasta
SP score = 0.9393939393939394
Training of 5 independent models on file ghf1.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea77d6f490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe938082970>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8c0303790>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 31s - loss: 2261.2776 - loglik: -2.2579e+03 - logprior: -3.3594e+00
Epoch 2/10
25/25 - 28s - loss: 2011.8542 - loglik: -2.0109e+03 - logprior: -9.9318e-01
Epoch 3/10
25/25 - 28s - loss: 1951.0696 - loglik: -1.9491e+03 - logprior: -1.9513e+00
Epoch 4/10
25/25 - 28s - loss: 1942.4371 - loglik: -1.9401e+03 - logprior: -2.3405e+00
Epoch 5/10
25/25 - 28s - loss: 1938.4784 - loglik: -1.9361e+03 - logprior: -2.3541e+00
Epoch 6/10
25/25 - 28s - loss: 1938.4734 - loglik: -1.9362e+03 - logprior: -2.2432e+00
Epoch 7/10
25/25 - 29s - loss: 1923.4856 - loglik: -1.9215e+03 - logprior: -2.0007e+00
Epoch 8/10
25/25 - 28s - loss: 1936.2371 - loglik: -1.9341e+03 - logprior: -2.1298e+00
Fitted a model with MAP estimate = -1930.2230
expansions: [(0, 2), (46, 1), (88, 2), (124, 1), (130, 1), (142, 1), (143, 1), (144, 1), (146, 1), (159, 1), (160, 2), (161, 1), (164, 1), (165, 1), (169, 5), (170, 3), (171, 1), (172, 1), (180, 1), (182, 1), (183, 1), (184, 4), (185, 1), (186, 1), (193, 1), (197, 1), (198, 2), (202, 1), (204, 1), (205, 1), (208, 1), (212, 1), (214, 1), (220, 1), (221, 1), (222, 2), (223, 1), (224, 1), (226, 1), (227, 1), (228, 1), (233, 1), (235, 1), (236, 1), (247, 1), (253, 3), (254, 2), (255, 1), (257, 1), (258, 1), (266, 1), (280, 1), (281, 1), (282, 1), (296, 1), (298, 1), (299, 2), (300, 1), (303, 1), (313, 2), (318, 2), (319, 2), (324, 1), (326, 2), (350, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 460 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 1924.6132 - loglik: -1.9189e+03 - logprior: -5.7278e+00
Epoch 2/2
25/25 - 41s - loss: 1884.4380 - loglik: -1.8843e+03 - logprior: -1.3180e-01
Fitted a model with MAP estimate = -1882.9602
expansions: [(313, 1), (440, 1)]
discards: [  2  91 173 189 216 314 315 372]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 46s - loss: 1893.6550 - loglik: -1.8908e+03 - logprior: -2.8874e+00
Epoch 2/2
25/25 - 40s - loss: 1881.5300 - loglik: -1.8821e+03 - logprior: 0.5996
Fitted a model with MAP estimate = -1880.5014
expansions: []
discards: [418 434]
Re-initialized the encoder parameters.
Fitting a model of length 452 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 1892.8566 - loglik: -1.8903e+03 - logprior: -2.5098e+00
Epoch 2/10
25/25 - 40s - loss: 1877.7131 - loglik: -1.8794e+03 - logprior: 1.6905
Epoch 3/10
25/25 - 40s - loss: 1883.8113 - loglik: -1.8861e+03 - logprior: 2.2492
Fitted a model with MAP estimate = -1877.8324
Time for alignment: 668.7609
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 2258.7793 - loglik: -2.2554e+03 - logprior: -3.3737e+00
Epoch 2/10
25/25 - 28s - loss: 2007.9468 - loglik: -2.0072e+03 - logprior: -7.0384e-01
Epoch 3/10
25/25 - 28s - loss: 1950.0909 - loglik: -1.9486e+03 - logprior: -1.5118e+00
Epoch 4/10
25/25 - 29s - loss: 1943.1918 - loglik: -1.9417e+03 - logprior: -1.5229e+00
Epoch 5/10
25/25 - 29s - loss: 1938.3549 - loglik: -1.9369e+03 - logprior: -1.4512e+00
Epoch 6/10
25/25 - 28s - loss: 1935.7633 - loglik: -1.9343e+03 - logprior: -1.4780e+00
Epoch 7/10
25/25 - 28s - loss: 1934.6066 - loglik: -1.9330e+03 - logprior: -1.6085e+00
Epoch 8/10
25/25 - 29s - loss: 1932.1458 - loglik: -1.9305e+03 - logprior: -1.6729e+00
Epoch 9/10
25/25 - 29s - loss: 1932.6279 - loglik: -1.9309e+03 - logprior: -1.7205e+00
Fitted a model with MAP estimate = -1929.1512
expansions: [(0, 2), (52, 1), (145, 1), (146, 1), (147, 1), (165, 3), (170, 1), (175, 4), (176, 2), (177, 2), (178, 1), (179, 1), (190, 1), (191, 1), (192, 5), (193, 2), (195, 1), (196, 4), (197, 1), (199, 1), (200, 1), (202, 1), (204, 1), (206, 1), (207, 1), (210, 1), (211, 1), (216, 1), (222, 1), (223, 1), (224, 3), (225, 1), (226, 1), (228, 1), (229, 1), (230, 1), (237, 1), (238, 1), (250, 1), (252, 1), (253, 1), (255, 3), (256, 1), (257, 1), (259, 4), (267, 1), (281, 1), (282, 1), (283, 1), (299, 2), (300, 1), (301, 2), (303, 1), (317, 1), (318, 1), (319, 1), (321, 2), (326, 1), (327, 2), (341, 1), (356, 1), (361, 1), (363, 1), (364, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 461 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 1935.2365 - loglik: -1.9295e+03 - logprior: -5.7060e+00
Epoch 2/2
25/25 - 41s - loss: 1885.5114 - loglik: -1.8851e+03 - logprior: -4.4031e-01
Fitted a model with MAP estimate = -1884.2901
expansions: [(413, 1), (441, 1)]
discards: [  2 186 187 188 217 270 323 403 425]
Re-initialized the encoder parameters.
Fitting a model of length 454 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 1892.3120 - loglik: -1.8893e+03 - logprior: -3.0323e+00
Epoch 2/2
25/25 - 40s - loss: 1881.6619 - loglik: -1.8828e+03 - logprior: 1.1650
Fitted a model with MAP estimate = -1878.6853
expansions: [(185, 4), (312, 1)]
discards: [314 434]
Re-initialized the encoder parameters.
Fitting a model of length 457 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 1888.3470 - loglik: -1.8859e+03 - logprior: -2.4854e+00
Epoch 2/10
25/25 - 40s - loss: 1879.2454 - loglik: -1.8809e+03 - logprior: 1.6739
Epoch 3/10
25/25 - 40s - loss: 1878.6748 - loglik: -1.8809e+03 - logprior: 2.2614
Epoch 4/10
25/25 - 41s - loss: 1875.4425 - loglik: -1.8779e+03 - logprior: 2.4679
Epoch 5/10
25/25 - 40s - loss: 1868.6033 - loglik: -1.8713e+03 - logprior: 2.7254
Epoch 6/10
25/25 - 40s - loss: 1879.4828 - loglik: -1.8824e+03 - logprior: 2.9467
Fitted a model with MAP estimate = -1869.7592
Time for alignment: 820.8015
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 2261.2439 - loglik: -2.2578e+03 - logprior: -3.4143e+00
Epoch 2/10
25/25 - 29s - loss: 2004.5189 - loglik: -2.0037e+03 - logprior: -8.5452e-01
Epoch 3/10
25/25 - 29s - loss: 1957.0194 - loglik: -1.9554e+03 - logprior: -1.5843e+00
Epoch 4/10
25/25 - 29s - loss: 1952.2605 - loglik: -1.9505e+03 - logprior: -1.7483e+00
Epoch 5/10
25/25 - 28s - loss: 1939.8292 - loglik: -1.9382e+03 - logprior: -1.5879e+00
Epoch 6/10
25/25 - 29s - loss: 1935.5817 - loglik: -1.9340e+03 - logprior: -1.5565e+00
Epoch 7/10
25/25 - 29s - loss: 1939.9048 - loglik: -1.9383e+03 - logprior: -1.6489e+00
Fitted a model with MAP estimate = -1935.8601
expansions: [(0, 2), (43, 1), (127, 1), (135, 1), (143, 1), (146, 1), (163, 1), (164, 2), (168, 1), (175, 4), (176, 2), (177, 2), (192, 4), (193, 2), (194, 1), (195, 2), (198, 1), (199, 3), (200, 1), (202, 1), (203, 2), (205, 1), (206, 2), (208, 1), (209, 2), (211, 1), (215, 1), (217, 1), (218, 1), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (234, 1), (236, 2), (237, 1), (248, 1), (254, 2), (255, 4), (257, 2), (266, 1), (280, 1), (282, 1), (283, 1), (299, 1), (300, 1), (301, 2), (313, 1), (314, 1), (316, 1), (318, 2), (340, 2), (353, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 458 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 46s - loss: 1928.0731 - loglik: -1.9225e+03 - logprior: -5.5342e+00
Epoch 2/2
25/25 - 40s - loss: 1897.8169 - loglik: -1.8978e+03 - logprior: -4.6985e-02
Fitted a model with MAP estimate = -1891.4110
expansions: [(425, 2)]
discards: [  2   3 172 187 188 189 213 217 237 249 316 438]
Re-initialized the encoder parameters.
Fitting a model of length 448 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 42s - loss: 1901.7125 - loglik: -1.8987e+03 - logprior: -3.0579e+00
Epoch 2/2
25/25 - 39s - loss: 1881.4698 - loglik: -1.8825e+03 - logprior: 1.0585
Fitted a model with MAP estimate = -1882.2677
expansions: [(214, 1), (242, 1), (415, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 451 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 1887.6155 - loglik: -1.8849e+03 - logprior: -2.6857e+00
Epoch 2/10
25/25 - 40s - loss: 1884.0819 - loglik: -1.8857e+03 - logprior: 1.6392
Epoch 3/10
25/25 - 40s - loss: 1881.3561 - loglik: -1.8834e+03 - logprior: 2.0897
Epoch 4/10
25/25 - 40s - loss: 1878.9795 - loglik: -1.8814e+03 - logprior: 2.3990
Epoch 5/10
25/25 - 40s - loss: 1879.3560 - loglik: -1.8820e+03 - logprior: 2.6600
Fitted a model with MAP estimate = -1874.2474
Time for alignment: 716.6945
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 32s - loss: 2252.4160 - loglik: -2.2490e+03 - logprior: -3.4121e+00
Epoch 2/10
25/25 - 29s - loss: 1997.7789 - loglik: -1.9969e+03 - logprior: -9.2782e-01
Epoch 3/10
25/25 - 28s - loss: 1951.5291 - loglik: -1.9499e+03 - logprior: -1.6305e+00
Epoch 4/10
25/25 - 29s - loss: 1936.3699 - loglik: -1.9348e+03 - logprior: -1.5912e+00
Epoch 5/10
25/25 - 29s - loss: 1930.1233 - loglik: -1.9286e+03 - logprior: -1.5407e+00
Epoch 6/10
25/25 - 29s - loss: 1929.7340 - loglik: -1.9281e+03 - logprior: -1.5989e+00
Epoch 7/10
25/25 - 29s - loss: 1927.6868 - loglik: -1.9260e+03 - logprior: -1.6821e+00
Epoch 8/10
25/25 - 29s - loss: 1928.7878 - loglik: -1.9270e+03 - logprior: -1.7954e+00
Fitted a model with MAP estimate = -1923.9614
expansions: [(0, 2), (126, 1), (135, 1), (145, 1), (165, 1), (166, 2), (171, 1), (176, 4), (177, 2), (178, 2), (179, 1), (192, 1), (193, 1), (194, 4), (195, 1), (196, 1), (198, 1), (199, 1), (200, 2), (201, 1), (203, 1), (204, 1), (206, 1), (207, 1), (208, 1), (209, 1), (210, 1), (213, 1), (214, 1), (219, 1), (225, 1), (226, 1), (227, 2), (228, 1), (229, 1), (230, 2), (231, 1), (232, 1), (237, 1), (239, 1), (240, 1), (251, 1), (253, 1), (254, 1), (257, 3), (258, 2), (260, 5), (268, 1), (282, 1), (283, 1), (284, 1), (301, 1), (302, 1), (303, 2), (305, 1), (306, 1), (315, 1), (318, 1), (319, 1), (322, 2), (327, 1), (340, 2), (341, 1), (356, 1), (358, 1), (360, 1), (363, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 460 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 44s - loss: 1922.0629 - loglik: -1.9165e+03 - logprior: -5.5215e+00
Epoch 2/2
25/25 - 41s - loss: 1886.0077 - loglik: -1.8856e+03 - logprior: -3.7155e-01
Fitted a model with MAP estimate = -1880.1513
expansions: [(314, 4), (423, 1)]
discards: [  2   3 186 187 188 216 318 319 320 321 322 402]
Re-initialized the encoder parameters.
Fitting a model of length 453 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 43s - loss: 1895.2269 - loglik: -1.8924e+03 - logprior: -2.8238e+00
Epoch 2/2
25/25 - 40s - loss: 1877.1742 - loglik: -1.8785e+03 - logprior: 1.3031
Fitted a model with MAP estimate = -1879.7015
expansions: [(184, 4)]
discards: [310 311]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 1888.8728 - loglik: -1.8867e+03 - logprior: -2.1474e+00
Epoch 2/10
25/25 - 40s - loss: 1877.1481 - loglik: -1.8789e+03 - logprior: 1.7636
Epoch 3/10
25/25 - 40s - loss: 1881.8427 - loglik: -1.8842e+03 - logprior: 2.3109
Fitted a model with MAP estimate = -1876.2448
Time for alignment: 670.5761
Fitting a model of length 370 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 34s - loss: 2254.8040 - loglik: -2.2515e+03 - logprior: -3.3539e+00
Epoch 2/10
25/25 - 29s - loss: 2016.5247 - loglik: -2.0158e+03 - logprior: -6.9448e-01
Epoch 3/10
25/25 - 29s - loss: 1952.8065 - loglik: -1.9513e+03 - logprior: -1.5401e+00
Epoch 4/10
25/25 - 29s - loss: 1946.2100 - loglik: -1.9442e+03 - logprior: -1.9966e+00
Epoch 5/10
25/25 - 29s - loss: 1939.5122 - loglik: -1.9381e+03 - logprior: -1.4087e+00
Epoch 6/10
25/25 - 29s - loss: 1936.3085 - loglik: -1.9349e+03 - logprior: -1.4427e+00
Epoch 7/10
25/25 - 29s - loss: 1931.9695 - loglik: -1.9305e+03 - logprior: -1.4866e+00
Epoch 8/10
25/25 - 29s - loss: 1934.2021 - loglik: -1.9327e+03 - logprior: -1.5018e+00
Fitted a model with MAP estimate = -1929.5578
expansions: [(0, 2), (146, 1), (147, 1), (165, 1), (166, 3), (171, 1), (175, 5), (176, 2), (177, 2), (178, 1), (191, 3), (192, 5), (193, 2), (196, 1), (197, 3), (198, 2), (200, 1), (201, 2), (204, 2), (206, 1), (207, 2), (208, 1), (209, 1), (213, 1), (215, 1), (221, 1), (222, 1), (223, 2), (224, 1), (225, 1), (226, 2), (227, 1), (228, 1), (233, 1), (235, 1), (236, 1), (247, 1), (249, 1), (253, 4), (254, 2), (256, 3), (257, 1), (279, 1), (280, 1), (281, 1), (297, 1), (299, 2), (300, 1), (303, 1), (317, 1), (318, 1), (319, 1), (321, 2), (324, 1), (325, 1), (327, 2), (339, 1), (350, 1), (353, 1), (358, 1), (360, 1), (366, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 465 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 45s - loss: 1930.7545 - loglik: -1.9249e+03 - logprior: -5.8995e+00
Epoch 2/2
25/25 - 42s - loss: 1882.7980 - loglik: -1.8818e+03 - logprior: -1.0411e+00
Fitted a model with MAP estimate = -1880.7616
expansions: [(443, 1)]
discards: [  2   3 186 187 188 217 231 249 320 321 324 325 376 404 428 445]
Re-initialized the encoder parameters.
Fitting a model of length 450 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
25/25 - 42s - loss: 1893.7345 - loglik: -1.8902e+03 - logprior: -3.5809e+00
Epoch 2/2
25/25 - 39s - loss: 1881.1757 - loglik: -1.8820e+03 - logprior: 0.7974
Fitted a model with MAP estimate = -1880.8876
expansions: [(184, 4), (312, 1), (314, 1)]
discards: [428]
Re-initialized the encoder parameters.
Fitting a model of length 455 on 4358 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
25/25 - 43s - loss: 1891.8656 - loglik: -1.8896e+03 - logprior: -2.2705e+00
Epoch 2/10
25/25 - 40s - loss: 1878.9949 - loglik: -1.8807e+03 - logprior: 1.6603
Epoch 3/10
25/25 - 40s - loss: 1876.0403 - loglik: -1.8782e+03 - logprior: 2.1444
Epoch 4/10
25/25 - 40s - loss: 1880.5225 - loglik: -1.8830e+03 - logprior: 2.5191
Fitted a model with MAP estimate = -1875.4890
Time for alignment: 711.8454
Computed alignments with likelihoods: ['-1877.8324', '-1869.7592', '-1874.2474', '-1876.2448', '-1875.4890']
Best model has likelihood: -1869.7592  (prior= 3.1560 )
time for generating output: 0.4277
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ghf1.projection.fasta
SP score = 0.8873404064017263
Training of 5 independent models on file ldh.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9e74364f0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8c8fdb490>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe938cc5310>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 666.3840 - loglik: -6.6188e+02 - logprior: -4.5045e+00
Epoch 2/10
16/16 - 3s - loss: 591.1237 - loglik: -5.8962e+02 - logprior: -1.5077e+00
Epoch 3/10
16/16 - 3s - loss: 557.5223 - loglik: -5.5562e+02 - logprior: -1.9051e+00
Epoch 4/10
16/16 - 3s - loss: 547.4316 - loglik: -5.4573e+02 - logprior: -1.7048e+00
Epoch 5/10
16/16 - 3s - loss: 541.6022 - loglik: -5.3997e+02 - logprior: -1.6265e+00
Epoch 6/10
16/16 - 3s - loss: 542.3688 - loglik: -5.4081e+02 - logprior: -1.5545e+00
Fitted a model with MAP estimate = -539.0482
expansions: [(0, 1), (15, 1), (16, 2), (17, 1), (18, 1), (76, 1), (97, 1), (99, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 542.4724 - loglik: -5.3867e+02 - logprior: -3.8053e+00
Epoch 2/2
33/33 - 5s - loss: 535.3935 - loglik: -5.3384e+02 - logprior: -1.5513e+00
Fitted a model with MAP estimate = -533.8636
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 8s - loss: 537.5889 - loglik: -5.3434e+02 - logprior: -3.2526e+00
Epoch 2/10
33/33 - 4s - loss: 534.1678 - loglik: -5.3271e+02 - logprior: -1.4589e+00
Epoch 3/10
33/33 - 5s - loss: 533.9327 - loglik: -5.3259e+02 - logprior: -1.3415e+00
Epoch 4/10
33/33 - 4s - loss: 531.1367 - loglik: -5.2989e+02 - logprior: -1.2418e+00
Epoch 5/10
33/33 - 5s - loss: 529.8839 - loglik: -5.2870e+02 - logprior: -1.1826e+00
Epoch 6/10
33/33 - 5s - loss: 529.2727 - loglik: -5.2817e+02 - logprior: -1.0990e+00
Epoch 7/10
33/33 - 4s - loss: 524.1823 - loglik: -5.2314e+02 - logprior: -1.0350e+00
Epoch 8/10
33/33 - 4s - loss: 522.5406 - loglik: -5.2153e+02 - logprior: -9.9417e-01
Epoch 9/10
33/33 - 5s - loss: 519.9355 - loglik: -5.1896e+02 - logprior: -9.5674e-01
Epoch 10/10
33/33 - 4s - loss: 518.9331 - loglik: -5.1800e+02 - logprior: -9.1861e-01
Fitted a model with MAP estimate = -518.8858
Time for alignment: 100.0151
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 7s - loss: 665.3881 - loglik: -6.6090e+02 - logprior: -4.4886e+00
Epoch 2/10
16/16 - 4s - loss: 593.6109 - loglik: -5.9213e+02 - logprior: -1.4844e+00
Epoch 3/10
16/16 - 3s - loss: 556.2602 - loglik: -5.5447e+02 - logprior: -1.7901e+00
Epoch 4/10
16/16 - 3s - loss: 545.9352 - loglik: -5.4428e+02 - logprior: -1.6504e+00
Epoch 5/10
16/16 - 3s - loss: 543.0256 - loglik: -5.4144e+02 - logprior: -1.5868e+00
Epoch 6/10
16/16 - 3s - loss: 539.5897 - loglik: -5.3795e+02 - logprior: -1.6346e+00
Epoch 7/10
16/16 - 3s - loss: 540.4781 - loglik: -5.3885e+02 - logprior: -1.6245e+00
Fitted a model with MAP estimate = -537.3081
expansions: [(0, 1), (12, 1), (15, 1), (16, 2), (17, 1), (18, 1), (25, 4), (57, 1), (82, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 126 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 9s - loss: 542.1539 - loglik: -5.3828e+02 - logprior: -3.8747e+00
Epoch 2/2
33/33 - 5s - loss: 534.8903 - loglik: -5.3326e+02 - logprior: -1.6318e+00
Fitted a model with MAP estimate = -532.7176
expansions: [(20, 1)]
discards: [24 33 34 35]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 537.4839 - loglik: -5.3422e+02 - logprior: -3.2679e+00
Epoch 2/2
33/33 - 4s - loss: 533.4261 - loglik: -5.3194e+02 - logprior: -1.4855e+00
Fitted a model with MAP estimate = -533.1486
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 536.5876 - loglik: -5.3343e+02 - logprior: -3.1618e+00
Epoch 2/10
33/33 - 5s - loss: 534.0695 - loglik: -5.3270e+02 - logprior: -1.3706e+00
Epoch 3/10
33/33 - 4s - loss: 531.5195 - loglik: -5.3029e+02 - logprior: -1.2268e+00
Epoch 4/10
33/33 - 4s - loss: 528.9017 - loglik: -5.2776e+02 - logprior: -1.1357e+00
Epoch 5/10
33/33 - 4s - loss: 533.2806 - loglik: -5.3223e+02 - logprior: -1.0470e+00
Fitted a model with MAP estimate = -528.1783
Time for alignment: 98.7353
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 664.5006 - loglik: -6.6001e+02 - logprior: -4.4866e+00
Epoch 2/10
16/16 - 3s - loss: 593.9716 - loglik: -5.9252e+02 - logprior: -1.4485e+00
Epoch 3/10
16/16 - 4s - loss: 555.7721 - loglik: -5.5408e+02 - logprior: -1.6881e+00
Epoch 4/10
16/16 - 3s - loss: 546.7387 - loglik: -5.4508e+02 - logprior: -1.6594e+00
Epoch 5/10
16/16 - 3s - loss: 541.2941 - loglik: -5.3978e+02 - logprior: -1.5081e+00
Epoch 6/10
16/16 - 3s - loss: 540.6415 - loglik: -5.3917e+02 - logprior: -1.4682e+00
Epoch 7/10
16/16 - 3s - loss: 537.6235 - loglik: -5.3615e+02 - logprior: -1.4684e+00
Epoch 8/10
16/16 - 3s - loss: 538.2128 - loglik: -5.3675e+02 - logprior: -1.4583e+00
Fitted a model with MAP estimate = -535.2522
expansions: [(0, 1), (15, 1), (16, 2), (20, 1), (89, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 119 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 9s - loss: 548.5470 - loglik: -5.4270e+02 - logprior: -5.8501e+00
Epoch 2/2
16/16 - 4s - loss: 538.7481 - loglik: -5.3658e+02 - logprior: -2.1689e+00
Fitted a model with MAP estimate = -537.2685
expansions: [(21, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 119 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 7s - loss: 543.0720 - loglik: -5.3798e+02 - logprior: -5.0928e+00
Epoch 2/2
16/16 - 3s - loss: 537.1301 - loglik: -5.3548e+02 - logprior: -1.6524e+00
Fitted a model with MAP estimate = -536.0714
expansions: [(0, 1), (18, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 121 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 8s - loss: 540.3667 - loglik: -5.3680e+02 - logprior: -3.5637e+00
Epoch 2/10
33/33 - 5s - loss: 534.2826 - loglik: -5.3285e+02 - logprior: -1.4324e+00
Epoch 3/10
33/33 - 4s - loss: 534.7158 - loglik: -5.3342e+02 - logprior: -1.2973e+00
Fitted a model with MAP estimate = -533.1736
Time for alignment: 87.3833
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 664.8219 - loglik: -6.6033e+02 - logprior: -4.4921e+00
Epoch 2/10
16/16 - 3s - loss: 591.6573 - loglik: -5.9018e+02 - logprior: -1.4785e+00
Epoch 3/10
16/16 - 3s - loss: 559.7670 - loglik: -5.5809e+02 - logprior: -1.6808e+00
Epoch 4/10
16/16 - 3s - loss: 555.2809 - loglik: -5.5377e+02 - logprior: -1.5144e+00
Epoch 5/10
16/16 - 4s - loss: 548.6246 - loglik: -5.4713e+02 - logprior: -1.4947e+00
Epoch 6/10
16/16 - 3s - loss: 539.8187 - loglik: -5.3830e+02 - logprior: -1.5122e+00
Epoch 7/10
16/16 - 3s - loss: 542.2850 - loglik: -5.4074e+02 - logprior: -1.5422e+00
Fitted a model with MAP estimate = -539.5049
expansions: [(0, 1), (13, 1), (15, 1), (16, 2), (17, 1), (18, 1), (25, 4), (57, 1), (81, 1), (97, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 126 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 543.4916 - loglik: -5.4005e+02 - logprior: -3.4425e+00
Epoch 2/2
33/33 - 4s - loss: 535.7115 - loglik: -5.3413e+02 - logprior: -1.5859e+00
Fitted a model with MAP estimate = -534.5617
expansions: [(20, 1)]
discards: [24 33 34 35]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 8s - loss: 539.0654 - loglik: -5.3597e+02 - logprior: -3.0949e+00
Epoch 2/2
33/33 - 5s - loss: 535.1849 - loglik: -5.3376e+02 - logprior: -1.4281e+00
Fitted a model with MAP estimate = -533.8639
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 537.0238 - loglik: -5.3403e+02 - logprior: -2.9972e+00
Epoch 2/10
33/33 - 4s - loss: 534.5350 - loglik: -5.3324e+02 - logprior: -1.2988e+00
Epoch 3/10
33/33 - 5s - loss: 533.6108 - loglik: -5.3242e+02 - logprior: -1.1939e+00
Epoch 4/10
33/33 - 4s - loss: 531.2170 - loglik: -5.3011e+02 - logprior: -1.1032e+00
Epoch 5/10
33/33 - 5s - loss: 530.3231 - loglik: -5.2930e+02 - logprior: -1.0232e+00
Epoch 6/10
33/33 - 4s - loss: 528.9190 - loglik: -5.2796e+02 - logprior: -9.5017e-01
Epoch 7/10
33/33 - 5s - loss: 526.2656 - loglik: -5.2538e+02 - logprior: -8.7954e-01
Epoch 8/10
33/33 - 5s - loss: 522.5800 - loglik: -5.2175e+02 - logprior: -8.2018e-01
Epoch 9/10
33/33 - 4s - loss: 519.8484 - loglik: -5.1906e+02 - logprior: -7.6943e-01
Epoch 10/10
33/33 - 5s - loss: 519.8905 - loglik: -5.1915e+02 - logprior: -7.2312e-01
Fitted a model with MAP estimate = -518.9552
Time for alignment: 122.6497
Fitting a model of length 112 on 7367 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 665.3553 - loglik: -6.6086e+02 - logprior: -4.4932e+00
Epoch 2/10
16/16 - 3s - loss: 591.5944 - loglik: -5.9013e+02 - logprior: -1.4679e+00
Epoch 3/10
16/16 - 3s - loss: 559.8937 - loglik: -5.5822e+02 - logprior: -1.6766e+00
Epoch 4/10
16/16 - 4s - loss: 546.2441 - loglik: -5.4468e+02 - logprior: -1.5585e+00
Epoch 5/10
16/16 - 4s - loss: 546.3575 - loglik: -5.4483e+02 - logprior: -1.5271e+00
Fitted a model with MAP estimate = -541.6506
expansions: [(0, 1), (12, 1), (15, 1), (16, 2), (17, 1), (18, 1), (69, 1), (97, 1), (98, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 10s - loss: 543.2728 - loglik: -5.3954e+02 - logprior: -3.7338e+00
Epoch 2/2
33/33 - 4s - loss: 535.5764 - loglik: -5.3402e+02 - logprior: -1.5515e+00
Fitted a model with MAP estimate = -534.2403
expansions: [(20, 1)]
discards: [24]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
33/33 - 7s - loss: 538.0652 - loglik: -5.3487e+02 - logprior: -3.1981e+00
Epoch 2/2
33/33 - 5s - loss: 535.1155 - loglik: -5.3369e+02 - logprior: -1.4267e+00
Fitted a model with MAP estimate = -533.7434
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 122 on 7367 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
33/33 - 7s - loss: 536.5156 - loglik: -5.3341e+02 - logprior: -3.1035e+00
Epoch 2/10
33/33 - 4s - loss: 535.0954 - loglik: -5.3378e+02 - logprior: -1.3105e+00
Epoch 3/10
33/33 - 4s - loss: 534.0564 - loglik: -5.3287e+02 - logprior: -1.1833e+00
Epoch 4/10
33/33 - 4s - loss: 531.9023 - loglik: -5.3081e+02 - logprior: -1.0866e+00
Epoch 5/10
33/33 - 4s - loss: 529.4821 - loglik: -5.2846e+02 - logprior: -1.0187e+00
Epoch 6/10
33/33 - 5s - loss: 529.3350 - loglik: -5.2840e+02 - logprior: -9.2983e-01
Epoch 7/10
33/33 - 5s - loss: 523.4761 - loglik: -5.2260e+02 - logprior: -8.6835e-01
Epoch 8/10
33/33 - 4s - loss: 520.2449 - loglik: -5.1943e+02 - logprior: -8.0479e-01
Epoch 9/10
33/33 - 4s - loss: 518.6232 - loglik: -5.1785e+02 - logprior: -7.5778e-01
Epoch 10/10
33/33 - 5s - loss: 516.8325 - loglik: -5.1611e+02 - logprior: -7.0595e-01
Fitted a model with MAP estimate = -516.2193
Time for alignment: 114.8061
Computed alignments with likelihoods: ['-518.8858', '-528.1783', '-533.1736', '-518.9552', '-516.2193']
Best model has likelihood: -516.2193  (prior= -0.6818 )
time for generating output: 0.2537
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/ldh.projection.fasta
SP score = 0.4475243179989761
Training of 5 independent models on file Rhodanese.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea77a96cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9bc33f430>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea2b437130>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 620.2616 - loglik: -6.1714e+02 - logprior: -3.1210e+00
Epoch 2/10
19/19 - 2s - loss: 588.5400 - loglik: -5.8740e+02 - logprior: -1.1379e+00
Epoch 3/10
19/19 - 2s - loss: 574.7002 - loglik: -5.7327e+02 - logprior: -1.4300e+00
Epoch 4/10
19/19 - 2s - loss: 571.9842 - loglik: -5.7061e+02 - logprior: -1.3696e+00
Epoch 5/10
19/19 - 2s - loss: 569.4393 - loglik: -5.6811e+02 - logprior: -1.3283e+00
Epoch 6/10
19/19 - 2s - loss: 567.8237 - loglik: -5.6651e+02 - logprior: -1.3125e+00
Epoch 7/10
19/19 - 2s - loss: 567.2560 - loglik: -5.6595e+02 - logprior: -1.2956e+00
Epoch 8/10
19/19 - 2s - loss: 565.5474 - loglik: -5.6426e+02 - logprior: -1.2758e+00
Epoch 9/10
19/19 - 2s - loss: 563.6946 - loglik: -5.6241e+02 - logprior: -1.2698e+00
Epoch 10/10
19/19 - 2s - loss: 559.4820 - loglik: -5.5821e+02 - logprior: -1.2546e+00
Fitted a model with MAP estimate = -535.5295
expansions: [(6, 2), (7, 2), (8, 1), (10, 1), (22, 4), (33, 9), (38, 2), (39, 1), (42, 2), (58, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 111 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 9s - loss: 582.7690 - loglik: -5.7886e+02 - logprior: -3.9101e+00
Epoch 2/2
19/19 - 2s - loss: 565.0049 - loglik: -5.6293e+02 - logprior: -2.0729e+00
Fitted a model with MAP estimate = -532.2578
expansions: [(0, 2)]
discards: [ 0  7 28 29 30 46 47 48 49 50 57 82 88 89]
Re-initialized the encoder parameters.
Fitting a model of length 99 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 565.5946 - loglik: -5.6272e+02 - logprior: -2.8783e+00
Epoch 2/2
19/19 - 2s - loss: 561.7933 - loglik: -5.6068e+02 - logprior: -1.1135e+00
Fitted a model with MAP estimate = -531.1332
expansions: [(42, 5), (43, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 533.3660 - loglik: -5.3080e+02 - logprior: -2.5703e+00
Epoch 2/10
23/23 - 3s - loss: 528.8015 - loglik: -5.2776e+02 - logprior: -1.0390e+00
Epoch 3/10
23/23 - 3s - loss: 528.4219 - loglik: -5.2748e+02 - logprior: -9.4427e-01
Epoch 4/10
23/23 - 3s - loss: 526.7476 - loglik: -5.2583e+02 - logprior: -9.1915e-01
Epoch 5/10
23/23 - 3s - loss: 525.5072 - loglik: -5.2460e+02 - logprior: -9.0330e-01
Epoch 6/10
23/23 - 3s - loss: 524.8713 - loglik: -5.2397e+02 - logprior: -8.9036e-01
Epoch 7/10
23/23 - 3s - loss: 523.7881 - loglik: -5.2291e+02 - logprior: -8.7124e-01
Epoch 8/10
23/23 - 3s - loss: 523.3502 - loglik: -5.2247e+02 - logprior: -8.6510e-01
Epoch 9/10
23/23 - 3s - loss: 519.9045 - loglik: -5.1903e+02 - logprior: -8.5023e-01
Epoch 10/10
23/23 - 3s - loss: 516.6114 - loglik: -5.1572e+02 - logprior: -8.6530e-01
Fitted a model with MAP estimate = -511.4608
Time for alignment: 89.4266
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 620.6242 - loglik: -6.1751e+02 - logprior: -3.1182e+00
Epoch 2/10
19/19 - 2s - loss: 589.4271 - loglik: -5.8830e+02 - logprior: -1.1242e+00
Epoch 3/10
19/19 - 2s - loss: 574.5090 - loglik: -5.7309e+02 - logprior: -1.4180e+00
Epoch 4/10
19/19 - 2s - loss: 571.0205 - loglik: -5.6964e+02 - logprior: -1.3801e+00
Epoch 5/10
19/19 - 2s - loss: 568.9019 - loglik: -5.6756e+02 - logprior: -1.3339e+00
Epoch 6/10
19/19 - 2s - loss: 567.4911 - loglik: -5.6618e+02 - logprior: -1.3093e+00
Epoch 7/10
19/19 - 2s - loss: 566.0870 - loglik: -5.6480e+02 - logprior: -1.2820e+00
Epoch 8/10
19/19 - 2s - loss: 565.5032 - loglik: -5.6422e+02 - logprior: -1.2684e+00
Epoch 9/10
19/19 - 2s - loss: 562.8448 - loglik: -5.6157e+02 - logprior: -1.2602e+00
Epoch 10/10
19/19 - 2s - loss: 558.5864 - loglik: -5.5731e+02 - logprior: -1.2532e+00
Fitted a model with MAP estimate = -535.0821
expansions: [(6, 2), (7, 2), (8, 1), (10, 1), (21, 2), (33, 9), (38, 2), (39, 1), (42, 2), (54, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 109 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 582.3442 - loglik: -5.7844e+02 - logprior: -3.9016e+00
Epoch 2/2
19/19 - 2s - loss: 564.9623 - loglik: -5.6291e+02 - logprior: -2.0524e+00
Fitted a model with MAP estimate = -532.6441
expansions: [(0, 2)]
discards: [ 0  7 27 55 76 86 87]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 564.0877 - loglik: -5.6120e+02 - logprior: -2.8920e+00
Epoch 2/2
19/19 - 2s - loss: 560.5778 - loglik: -5.5944e+02 - logprior: -1.1328e+00
Fitted a model with MAP estimate = -530.3252
expansions: [(45, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 531.9295 - loglik: -5.2938e+02 - logprior: -2.5473e+00
Epoch 2/10
23/23 - 3s - loss: 529.1020 - loglik: -5.2808e+02 - logprior: -1.0174e+00
Epoch 3/10
23/23 - 3s - loss: 526.7267 - loglik: -5.2578e+02 - logprior: -9.4863e-01
Epoch 4/10
23/23 - 3s - loss: 527.0271 - loglik: -5.2612e+02 - logprior: -8.9987e-01
Fitted a model with MAP estimate = -526.0433
Time for alignment: 72.3171
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 620.5469 - loglik: -6.1743e+02 - logprior: -3.1190e+00
Epoch 2/10
19/19 - 2s - loss: 589.4500 - loglik: -5.8833e+02 - logprior: -1.1155e+00
Epoch 3/10
19/19 - 2s - loss: 575.0524 - loglik: -5.7365e+02 - logprior: -1.4043e+00
Epoch 4/10
19/19 - 2s - loss: 571.3543 - loglik: -5.6998e+02 - logprior: -1.3762e+00
Epoch 5/10
19/19 - 2s - loss: 569.4884 - loglik: -5.6814e+02 - logprior: -1.3434e+00
Epoch 6/10
19/19 - 2s - loss: 567.8560 - loglik: -5.6652e+02 - logprior: -1.3350e+00
Epoch 7/10
19/19 - 2s - loss: 567.0848 - loglik: -5.6575e+02 - logprior: -1.3305e+00
Epoch 8/10
19/19 - 2s - loss: 566.4925 - loglik: -5.6517e+02 - logprior: -1.3158e+00
Epoch 9/10
19/19 - 2s - loss: 563.2807 - loglik: -5.6196e+02 - logprior: -1.3093e+00
Epoch 10/10
19/19 - 2s - loss: 559.5421 - loglik: -5.5823e+02 - logprior: -1.2959e+00
Fitted a model with MAP estimate = -536.0362
expansions: [(6, 2), (7, 2), (8, 1), (10, 1), (21, 2), (36, 7), (39, 2), (40, 1), (43, 2), (59, 2), (60, 1), (61, 1), (62, 2), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 580.9976 - loglik: -5.7707e+02 - logprior: -3.9243e+00
Epoch 2/2
19/19 - 2s - loss: 565.6467 - loglik: -5.6356e+02 - logprior: -2.0816e+00
Fitted a model with MAP estimate = -533.2129
expansions: [(0, 2)]
discards: [ 0  7 27 46 47 48 49 50 53 79 85 87]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 565.8045 - loglik: -5.6294e+02 - logprior: -2.8654e+00
Epoch 2/2
19/19 - 2s - loss: 562.7277 - loglik: -5.6163e+02 - logprior: -1.0930e+00
Fitted a model with MAP estimate = -531.6225
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 97 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 532.7941 - loglik: -5.3029e+02 - logprior: -2.5019e+00
Epoch 2/10
23/23 - 3s - loss: 529.5643 - loglik: -5.2860e+02 - logprior: -9.6771e-01
Epoch 3/10
23/23 - 3s - loss: 529.5794 - loglik: -5.2867e+02 - logprior: -9.0347e-01
Fitted a model with MAP estimate = -528.6316
Time for alignment: 68.3165
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 620.2951 - loglik: -6.1717e+02 - logprior: -3.1201e+00
Epoch 2/10
19/19 - 2s - loss: 589.1636 - loglik: -5.8803e+02 - logprior: -1.1381e+00
Epoch 3/10
19/19 - 2s - loss: 575.2479 - loglik: -5.7381e+02 - logprior: -1.4373e+00
Epoch 4/10
19/19 - 2s - loss: 570.8254 - loglik: -5.6944e+02 - logprior: -1.3859e+00
Epoch 5/10
19/19 - 2s - loss: 568.9224 - loglik: -5.6758e+02 - logprior: -1.3435e+00
Epoch 6/10
19/19 - 2s - loss: 567.5824 - loglik: -5.6626e+02 - logprior: -1.3152e+00
Epoch 7/10
19/19 - 2s - loss: 567.0324 - loglik: -5.6572e+02 - logprior: -1.3001e+00
Epoch 8/10
19/19 - 2s - loss: 565.1050 - loglik: -5.6381e+02 - logprior: -1.2856e+00
Epoch 9/10
19/19 - 2s - loss: 563.8040 - loglik: -5.6252e+02 - logprior: -1.2704e+00
Epoch 10/10
19/19 - 2s - loss: 559.4797 - loglik: -5.5819e+02 - logprior: -1.2697e+00
Fitted a model with MAP estimate = -535.6428
expansions: [(6, 2), (7, 2), (8, 1), (10, 1), (21, 2), (38, 2), (39, 2), (40, 1), (41, 1), (42, 2), (58, 2), (60, 1), (62, 2), (63, 2), (65, 1), (67, 1), (69, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 6s - loss: 582.9153 - loglik: -5.7900e+02 - logprior: -3.9167e+00
Epoch 2/2
19/19 - 2s - loss: 567.7630 - loglik: -5.6576e+02 - logprior: -2.0023e+00
Fitted a model with MAP estimate = -534.3028
expansions: [(0, 2), (58, 2)]
discards: [ 0  7 27 47 48 55 74 80 81]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 566.5109 - loglik: -5.6364e+02 - logprior: -2.8745e+00
Epoch 2/2
19/19 - 2s - loss: 562.6320 - loglik: -5.6151e+02 - logprior: -1.1178e+00
Fitted a model with MAP estimate = -531.8674
expansions: [(56, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 98 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 5s - loss: 533.2181 - loglik: -5.3071e+02 - logprior: -2.5077e+00
Epoch 2/10
23/23 - 3s - loss: 530.0142 - loglik: -5.2904e+02 - logprior: -9.6994e-01
Epoch 3/10
23/23 - 3s - loss: 528.6672 - loglik: -5.2776e+02 - logprior: -9.0472e-01
Epoch 4/10
23/23 - 2s - loss: 527.9774 - loglik: -5.2710e+02 - logprior: -8.7304e-01
Epoch 5/10
23/23 - 3s - loss: 528.1364 - loglik: -5.2728e+02 - logprior: -8.5099e-01
Fitted a model with MAP estimate = -526.6305
Time for alignment: 72.4190
Fitting a model of length 78 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
19/19 - 5s - loss: 620.3954 - loglik: -6.1728e+02 - logprior: -3.1194e+00
Epoch 2/10
19/19 - 2s - loss: 588.6530 - loglik: -5.8754e+02 - logprior: -1.1134e+00
Epoch 3/10
19/19 - 2s - loss: 575.2217 - loglik: -5.7382e+02 - logprior: -1.3985e+00
Epoch 4/10
19/19 - 2s - loss: 570.8846 - loglik: -5.6952e+02 - logprior: -1.3647e+00
Epoch 5/10
19/19 - 2s - loss: 568.4528 - loglik: -5.6711e+02 - logprior: -1.3354e+00
Epoch 6/10
19/19 - 2s - loss: 567.2698 - loglik: -5.6594e+02 - logprior: -1.3251e+00
Epoch 7/10
19/19 - 2s - loss: 566.7455 - loglik: -5.6543e+02 - logprior: -1.3067e+00
Epoch 8/10
19/19 - 2s - loss: 565.8176 - loglik: -5.6452e+02 - logprior: -1.2909e+00
Epoch 9/10
19/19 - 2s - loss: 563.5507 - loglik: -5.6225e+02 - logprior: -1.2821e+00
Epoch 10/10
19/19 - 2s - loss: 559.7424 - loglik: -5.5845e+02 - logprior: -1.2759e+00
Fitted a model with MAP estimate = -535.5216
expansions: [(6, 2), (7, 2), (8, 1), (10, 1), (21, 2), (33, 9), (40, 1), (43, 2), (59, 2), (60, 1), (61, 1), (62, 2), (63, 2), (65, 1), (67, 1), (73, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 108 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 580.5219 - loglik: -5.7660e+02 - logprior: -3.9171e+00
Epoch 2/2
19/19 - 2s - loss: 565.0118 - loglik: -5.6295e+02 - logprior: -2.0653e+00
Fitted a model with MAP estimate = -532.5141
expansions: [(0, 2)]
discards: [ 0  7 27 79 85 87]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 5s - loss: 563.8060 - loglik: -5.6089e+02 - logprior: -2.9167e+00
Epoch 2/2
19/19 - 2s - loss: 560.5058 - loglik: -5.5935e+02 - logprior: -1.1578e+00
Fitted a model with MAP estimate = -529.8855
expansions: [(42, 1), (44, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 106 on 14049 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
23/23 - 6s - loss: 532.5342 - loglik: -5.2996e+02 - logprior: -2.5764e+00
Epoch 2/10
23/23 - 3s - loss: 527.8253 - loglik: -5.2679e+02 - logprior: -1.0344e+00
Epoch 3/10
23/23 - 3s - loss: 527.7952 - loglik: -5.2685e+02 - logprior: -9.4429e-01
Epoch 4/10
23/23 - 3s - loss: 526.3729 - loglik: -5.2546e+02 - logprior: -9.1499e-01
Epoch 5/10
23/23 - 3s - loss: 525.5776 - loglik: -5.2468e+02 - logprior: -8.9574e-01
Epoch 6/10
23/23 - 3s - loss: 525.1112 - loglik: -5.2422e+02 - logprior: -8.7915e-01
Epoch 7/10
23/23 - 3s - loss: 523.5226 - loglik: -5.2263e+02 - logprior: -8.8163e-01
Epoch 8/10
23/23 - 3s - loss: 522.4341 - loglik: -5.2155e+02 - logprior: -8.6467e-01
Epoch 9/10
23/23 - 3s - loss: 520.9676 - loglik: -5.2009e+02 - logprior: -8.5438e-01
Epoch 10/10
23/23 - 3s - loss: 515.5602 - loglik: -5.1467e+02 - logprior: -8.6128e-01
Fitted a model with MAP estimate = -511.9510
Time for alignment: 88.1639
Computed alignments with likelihoods: ['-511.4608', '-526.0433', '-528.6316', '-526.6305', '-511.9510']
Best model has likelihood: -511.4608  (prior= -0.9043 )
time for generating output: 0.1562
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/Rhodanese.projection.fasta
SP score = 0.7516268980477223
Training of 5 independent models on file slectin.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe938d8b190>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3b9e72e0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9efc6bb50>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 821.6115 - loglik: -7.7612e+02 - logprior: -4.5495e+01
Epoch 2/10
10/10 - 2s - loss: 757.4114 - loglik: -7.4707e+02 - logprior: -1.0342e+01
Epoch 3/10
10/10 - 2s - loss: 721.0970 - loglik: -7.1698e+02 - logprior: -4.1136e+00
Epoch 4/10
10/10 - 2s - loss: 698.2334 - loglik: -6.9626e+02 - logprior: -1.9765e+00
Epoch 5/10
10/10 - 2s - loss: 688.3927 - loglik: -6.8746e+02 - logprior: -9.3518e-01
Epoch 6/10
10/10 - 2s - loss: 684.2038 - loglik: -6.8384e+02 - logprior: -3.5808e-01
Epoch 7/10
10/10 - 2s - loss: 682.7421 - loglik: -6.8273e+02 - logprior: -9.3658e-03
Epoch 8/10
10/10 - 2s - loss: 679.8112 - loglik: -6.7997e+02 - logprior: 0.1595
Epoch 9/10
10/10 - 2s - loss: 680.1187 - loglik: -6.8035e+02 - logprior: 0.2334
Fitted a model with MAP estimate = -679.4933
expansions: [(11, 3), (12, 1), (17, 1), (18, 1), (26, 1), (48, 2), (58, 2), (59, 1), (62, 2), (65, 1), (73, 1), (74, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 8s - loss: 739.6476 - loglik: -6.8889e+02 - logprior: -5.0754e+01
Epoch 2/2
10/10 - 2s - loss: 695.9607 - loglik: -6.7658e+02 - logprior: -1.9376e+01
Fitted a model with MAP estimate = -688.0613
expansions: [(10, 1), (13, 1), (34, 3)]
discards: [  0  55  67  73 107]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 724.7499 - loglik: -6.7496e+02 - logprior: -4.9787e+01
Epoch 2/2
10/10 - 2s - loss: 687.6840 - loglik: -6.7015e+02 - logprior: -1.7533e+01
Fitted a model with MAP estimate = -681.4035
expansions: [(0, 3)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 707.5784 - loglik: -6.6871e+02 - logprior: -3.8866e+01
Epoch 2/10
10/10 - 2s - loss: 674.4623 - loglik: -6.6697e+02 - logprior: -7.4967e+00
Epoch 3/10
10/10 - 2s - loss: 667.6668 - loglik: -6.6661e+02 - logprior: -1.0528e+00
Epoch 4/10
10/10 - 2s - loss: 663.7096 - loglik: -6.6531e+02 - logprior: 1.6044
Epoch 5/10
10/10 - 2s - loss: 661.9300 - loglik: -6.6500e+02 - logprior: 3.0752
Epoch 6/10
10/10 - 2s - loss: 661.2948 - loglik: -6.6520e+02 - logprior: 3.9024
Epoch 7/10
10/10 - 2s - loss: 658.8378 - loglik: -6.6318e+02 - logprior: 4.3443
Epoch 8/10
10/10 - 2s - loss: 657.9191 - loglik: -6.6251e+02 - logprior: 4.5936
Epoch 9/10
10/10 - 2s - loss: 658.1072 - loglik: -6.6290e+02 - logprior: 4.7956
Fitted a model with MAP estimate = -657.1952
Time for alignment: 62.0719
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 821.4858 - loglik: -7.7599e+02 - logprior: -4.5495e+01
Epoch 2/10
10/10 - 2s - loss: 757.1252 - loglik: -7.4678e+02 - logprior: -1.0345e+01
Epoch 3/10
10/10 - 2s - loss: 720.6806 - loglik: -7.1652e+02 - logprior: -4.1557e+00
Epoch 4/10
10/10 - 2s - loss: 697.3121 - loglik: -6.9526e+02 - logprior: -2.0546e+00
Epoch 5/10
10/10 - 2s - loss: 687.2172 - loglik: -6.8625e+02 - logprior: -9.6413e-01
Epoch 6/10
10/10 - 2s - loss: 683.9794 - loglik: -6.8366e+02 - logprior: -3.1617e-01
Epoch 7/10
10/10 - 2s - loss: 681.8615 - loglik: -6.8192e+02 - logprior: 0.0599
Epoch 8/10
10/10 - 2s - loss: 679.4930 - loglik: -6.7966e+02 - logprior: 0.1675
Epoch 9/10
10/10 - 2s - loss: 679.8726 - loglik: -6.8007e+02 - logprior: 0.1959
Fitted a model with MAP estimate = -678.8321
expansions: [(7, 1), (8, 2), (10, 1), (17, 1), (18, 1), (26, 1), (28, 4), (42, 1), (58, 2), (59, 1), (62, 2), (65, 1), (73, 1), (82, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 737.3927 - loglik: -6.8680e+02 - logprior: -5.0595e+01
Epoch 2/2
10/10 - 2s - loss: 693.9357 - loglik: -6.7485e+02 - logprior: -1.9084e+01
Fitted a model with MAP estimate = -685.6659
expansions: [(7, 1), (9, 1)]
discards: [  0  70  76 110]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 721.8353 - loglik: -6.7233e+02 - logprior: -4.9509e+01
Epoch 2/2
10/10 - 2s - loss: 687.1965 - loglik: -6.7035e+02 - logprior: -1.6845e+01
Fitted a model with MAP estimate = -679.5467
expansions: [(0, 3)]
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 707.3638 - loglik: -6.6858e+02 - logprior: -3.8781e+01
Epoch 2/10
10/10 - 2s - loss: 674.3085 - loglik: -6.6684e+02 - logprior: -7.4710e+00
Epoch 3/10
10/10 - 2s - loss: 668.1956 - loglik: -6.6716e+02 - logprior: -1.0308e+00
Epoch 4/10
10/10 - 2s - loss: 662.8492 - loglik: -6.6449e+02 - logprior: 1.6378
Epoch 5/10
10/10 - 2s - loss: 661.5074 - loglik: -6.6463e+02 - logprior: 3.1274
Epoch 6/10
10/10 - 2s - loss: 660.3338 - loglik: -6.6429e+02 - logprior: 3.9573
Epoch 7/10
10/10 - 2s - loss: 658.0786 - loglik: -6.6248e+02 - logprior: 4.4012
Epoch 8/10
10/10 - 2s - loss: 658.1915 - loglik: -6.6283e+02 - logprior: 4.6425
Fitted a model with MAP estimate = -657.2616
Time for alignment: 58.7995
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 820.8682 - loglik: -7.7537e+02 - logprior: -4.5494e+01
Epoch 2/10
10/10 - 2s - loss: 757.9336 - loglik: -7.4759e+02 - logprior: -1.0348e+01
Epoch 3/10
10/10 - 2s - loss: 722.2382 - loglik: -7.1805e+02 - logprior: -4.1841e+00
Epoch 4/10
10/10 - 2s - loss: 698.3020 - loglik: -6.9626e+02 - logprior: -2.0453e+00
Epoch 5/10
10/10 - 2s - loss: 688.7216 - loglik: -6.8774e+02 - logprior: -9.7651e-01
Epoch 6/10
10/10 - 2s - loss: 685.0233 - loglik: -6.8469e+02 - logprior: -3.3022e-01
Epoch 7/10
10/10 - 2s - loss: 681.5904 - loglik: -6.8156e+02 - logprior: -2.9050e-02
Epoch 8/10
10/10 - 2s - loss: 680.1796 - loglik: -6.8024e+02 - logprior: 0.0658
Epoch 9/10
10/10 - 2s - loss: 679.1175 - loglik: -6.7923e+02 - logprior: 0.1193
Epoch 10/10
10/10 - 2s - loss: 678.9678 - loglik: -6.7913e+02 - logprior: 0.1655
Fitted a model with MAP estimate = -678.2632
expansions: [(7, 2), (10, 1), (17, 1), (18, 1), (29, 3), (42, 2), (49, 2), (58, 2), (59, 1), (62, 2), (64, 1), (73, 1), (82, 1), (84, 1), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 740.6479 - loglik: -6.8992e+02 - logprior: -5.0726e+01
Epoch 2/2
10/10 - 2s - loss: 695.4020 - loglik: -6.7629e+02 - logprior: -1.9114e+01
Fitted a model with MAP estimate = -688.3275
expansions: [(9, 3)]
discards: [ 0 58 70 76]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 723.7543 - loglik: -6.7411e+02 - logprior: -4.9644e+01
Epoch 2/2
10/10 - 2s - loss: 688.2062 - loglik: -6.7097e+02 - logprior: -1.7239e+01
Fitted a model with MAP estimate = -680.8480
expansions: [(0, 3)]
discards: [ 0 52]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 708.7998 - loglik: -6.6996e+02 - logprior: -3.8841e+01
Epoch 2/10
10/10 - 2s - loss: 674.2484 - loglik: -6.6675e+02 - logprior: -7.4978e+00
Epoch 3/10
10/10 - 2s - loss: 667.5737 - loglik: -6.6654e+02 - logprior: -1.0285e+00
Epoch 4/10
10/10 - 2s - loss: 664.2936 - loglik: -6.6594e+02 - logprior: 1.6471
Epoch 5/10
10/10 - 2s - loss: 662.5576 - loglik: -6.6569e+02 - logprior: 3.1309
Epoch 6/10
10/10 - 2s - loss: 660.4645 - loglik: -6.6443e+02 - logprior: 3.9649
Epoch 7/10
10/10 - 2s - loss: 659.0135 - loglik: -6.6342e+02 - logprior: 4.4096
Epoch 8/10
10/10 - 2s - loss: 658.6841 - loglik: -6.6334e+02 - logprior: 4.6579
Epoch 9/10
10/10 - 2s - loss: 657.5415 - loglik: -6.6240e+02 - logprior: 4.8603
Epoch 10/10
10/10 - 2s - loss: 656.9081 - loglik: -6.6197e+02 - logprior: 5.0682
Fitted a model with MAP estimate = -656.9106
Time for alignment: 65.0464
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 821.4656 - loglik: -7.7597e+02 - logprior: -4.5495e+01
Epoch 2/10
10/10 - 2s - loss: 756.6660 - loglik: -7.4632e+02 - logprior: -1.0343e+01
Epoch 3/10
10/10 - 2s - loss: 723.2900 - loglik: -7.1926e+02 - logprior: -4.0348e+00
Epoch 4/10
10/10 - 2s - loss: 701.1718 - loglik: -6.9936e+02 - logprior: -1.8136e+00
Epoch 5/10
10/10 - 2s - loss: 689.1762 - loglik: -6.8844e+02 - logprior: -7.3380e-01
Epoch 6/10
10/10 - 2s - loss: 684.4599 - loglik: -6.8418e+02 - logprior: -2.8265e-01
Epoch 7/10
10/10 - 2s - loss: 681.6960 - loglik: -6.8166e+02 - logprior: -3.5756e-02
Epoch 8/10
10/10 - 2s - loss: 679.4908 - loglik: -6.7960e+02 - logprior: 0.1123
Epoch 9/10
10/10 - 2s - loss: 679.3232 - loglik: -6.7951e+02 - logprior: 0.1874
Epoch 10/10
10/10 - 2s - loss: 677.7753 - loglik: -6.7800e+02 - logprior: 0.2269
Fitted a model with MAP estimate = -677.9618
expansions: [(7, 2), (10, 1), (17, 1), (18, 2), (19, 1), (26, 1), (27, 4), (58, 2), (59, 1), (62, 2), (64, 1), (68, 1), (72, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 132 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 739.2612 - loglik: -6.8860e+02 - logprior: -5.0662e+01
Epoch 2/2
10/10 - 2s - loss: 695.9332 - loglik: -6.7686e+02 - logprior: -1.9071e+01
Fitted a model with MAP estimate = -688.0052
expansions: [(9, 3)]
discards: [  0  21  70  76 110]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 723.0527 - loglik: -6.7354e+02 - logprior: -4.9508e+01
Epoch 2/2
10/10 - 2s - loss: 688.2057 - loglik: -6.7119e+02 - logprior: -1.7020e+01
Fitted a model with MAP estimate = -680.2169
expansions: [(0, 3)]
discards: [ 0 35]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 708.5230 - loglik: -6.6976e+02 - logprior: -3.8766e+01
Epoch 2/10
10/10 - 2s - loss: 673.6186 - loglik: -6.6617e+02 - logprior: -7.4468e+00
Epoch 3/10
10/10 - 2s - loss: 666.4827 - loglik: -6.6549e+02 - logprior: -9.9139e-01
Epoch 4/10
10/10 - 2s - loss: 664.5556 - loglik: -6.6623e+02 - logprior: 1.6796
Epoch 5/10
10/10 - 2s - loss: 662.1599 - loglik: -6.6532e+02 - logprior: 3.1644
Epoch 6/10
10/10 - 2s - loss: 660.0112 - loglik: -6.6400e+02 - logprior: 3.9911
Epoch 7/10
10/10 - 2s - loss: 658.7004 - loglik: -6.6314e+02 - logprior: 4.4360
Epoch 8/10
10/10 - 2s - loss: 657.5873 - loglik: -6.6227e+02 - logprior: 4.6866
Epoch 9/10
10/10 - 2s - loss: 657.5192 - loglik: -6.6242e+02 - logprior: 4.8989
Epoch 10/10
10/10 - 2s - loss: 656.6659 - loglik: -6.6177e+02 - logprior: 5.1048
Fitted a model with MAP estimate = -656.3949
Time for alignment: 64.9001
Fitting a model of length 105 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 821.1823 - loglik: -7.7569e+02 - logprior: -4.5494e+01
Epoch 2/10
10/10 - 2s - loss: 757.5070 - loglik: -7.4716e+02 - logprior: -1.0348e+01
Epoch 3/10
10/10 - 2s - loss: 723.7076 - loglik: -7.1959e+02 - logprior: -4.1193e+00
Epoch 4/10
10/10 - 2s - loss: 702.8376 - loglik: -7.0105e+02 - logprior: -1.7843e+00
Epoch 5/10
10/10 - 2s - loss: 691.6467 - loglik: -6.9099e+02 - logprior: -6.5363e-01
Epoch 6/10
10/10 - 2s - loss: 685.4090 - loglik: -6.8515e+02 - logprior: -2.5787e-01
Epoch 7/10
10/10 - 2s - loss: 682.6205 - loglik: -6.8261e+02 - logprior: -1.3521e-02
Epoch 8/10
10/10 - 2s - loss: 681.3026 - loglik: -6.8155e+02 - logprior: 0.2451
Epoch 9/10
10/10 - 2s - loss: 679.9713 - loglik: -6.8031e+02 - logprior: 0.3373
Epoch 10/10
10/10 - 2s - loss: 680.7302 - loglik: -6.8109e+02 - logprior: 0.3663
Fitted a model with MAP estimate = -679.5551
expansions: [(12, 1), (17, 1), (18, 1), (26, 2), (27, 3), (41, 1), (42, 1), (58, 2), (59, 1), (62, 3), (70, 1), (72, 1), (90, 2), (91, 6)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 130 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 740.1569 - loglik: -6.8944e+02 - logprior: -5.0713e+01
Epoch 2/2
10/10 - 2s - loss: 697.4234 - loglik: -6.7838e+02 - logprior: -1.9047e+01
Fitted a model with MAP estimate = -689.4011
expansions: []
discards: [  0  31  32  33  68  75 108]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 727.0862 - loglik: -6.7742e+02 - logprior: -4.9663e+01
Epoch 2/2
10/10 - 2s - loss: 691.4785 - loglik: -6.7496e+02 - logprior: -1.6521e+01
Fitted a model with MAP estimate = -682.8840
expansions: [(0, 6), (5, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 129 on 932 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 711.1262 - loglik: -6.7193e+02 - logprior: -3.9198e+01
Epoch 2/10
10/10 - 2s - loss: 676.6659 - loglik: -6.6898e+02 - logprior: -7.6811e+00
Epoch 3/10
10/10 - 2s - loss: 669.2473 - loglik: -6.6802e+02 - logprior: -1.2303e+00
Epoch 4/10
10/10 - 2s - loss: 664.9377 - loglik: -6.6635e+02 - logprior: 1.4110
Epoch 5/10
10/10 - 2s - loss: 663.0578 - loglik: -6.6591e+02 - logprior: 2.8578
Epoch 6/10
10/10 - 2s - loss: 660.7020 - loglik: -6.6442e+02 - logprior: 3.7140
Epoch 7/10
10/10 - 2s - loss: 660.2157 - loglik: -6.6440e+02 - logprior: 4.1900
Epoch 8/10
10/10 - 2s - loss: 658.3862 - loglik: -6.6284e+02 - logprior: 4.4556
Epoch 9/10
10/10 - 2s - loss: 657.6907 - loglik: -6.6234e+02 - logprior: 4.6478
Epoch 10/10
10/10 - 2s - loss: 658.0151 - loglik: -6.6286e+02 - logprior: 4.8466
Fitted a model with MAP estimate = -657.2758
Time for alignment: 64.2789
Computed alignments with likelihoods: ['-657.1952', '-657.2616', '-656.9106', '-656.3949', '-657.2758']
Best model has likelihood: -656.3949  (prior= 5.2156 )
time for generating output: 0.1480
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/slectin.projection.fasta
SP score = 0.9211136890951276
Training of 5 independent models on file hip.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8c9346520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe91ee336a0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9abae0100>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 642.1185 - loglik: -3.7520e+02 - logprior: -2.6692e+02
Epoch 2/10
10/10 - 1s - loss: 414.0761 - loglik: -3.4412e+02 - logprior: -6.9961e+01
Epoch 3/10
10/10 - 1s - loss: 353.6435 - loglik: -3.2295e+02 - logprior: -3.0698e+01
Epoch 4/10
10/10 - 1s - loss: 324.0298 - loglik: -3.0765e+02 - logprior: -1.6376e+01
Epoch 5/10
10/10 - 1s - loss: 308.0844 - loglik: -2.9954e+02 - logprior: -8.5479e+00
Epoch 6/10
10/10 - 1s - loss: 299.1320 - loglik: -2.9544e+02 - logprior: -3.6898e+00
Epoch 7/10
10/10 - 1s - loss: 293.9304 - loglik: -2.9300e+02 - logprior: -9.2777e-01
Epoch 8/10
10/10 - 1s - loss: 290.1538 - loglik: -2.9087e+02 - logprior: 0.7211
Epoch 9/10
10/10 - 1s - loss: 287.7440 - loglik: -2.9003e+02 - logprior: 2.2824
Epoch 10/10
10/10 - 1s - loss: 286.2914 - loglik: -2.8967e+02 - logprior: 3.3765
Fitted a model with MAP estimate = -285.6667
expansions: [(0, 3), (10, 1), (17, 3), (24, 1), (25, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 660.0914 - loglik: -3.0495e+02 - logprior: -3.5514e+02
Epoch 2/2
10/10 - 1s - loss: 389.3906 - loglik: -2.8336e+02 - logprior: -1.0603e+02
Fitted a model with MAP estimate = -339.4638
expansions: []
discards: [ 0 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 7s - loss: 586.5392 - loglik: -2.8613e+02 - logprior: -3.0041e+02
Epoch 2/2
10/10 - 1s - loss: 394.2765 - loglik: -2.7980e+02 - logprior: -1.1447e+02
Fitted a model with MAP estimate = -364.0305
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 558.6500 - loglik: -2.8361e+02 - logprior: -2.7504e+02
Epoch 2/10
10/10 - 1s - loss: 351.5124 - loglik: -2.7825e+02 - logprior: -7.3264e+01
Epoch 3/10
10/10 - 1s - loss: 300.2104 - loglik: -2.7787e+02 - logprior: -2.2341e+01
Epoch 4/10
10/10 - 1s - loss: 280.5036 - loglik: -2.7663e+02 - logprior: -3.8764e+00
Epoch 5/10
10/10 - 1s - loss: 268.6835 - loglik: -2.7407e+02 - logprior: 5.3883
Epoch 6/10
10/10 - 1s - loss: 261.5413 - loglik: -2.7225e+02 - logprior: 10.7098
Epoch 7/10
10/10 - 1s - loss: 257.2786 - loglik: -2.7146e+02 - logprior: 14.1792
Epoch 8/10
10/10 - 1s - loss: 253.8134 - loglik: -2.7036e+02 - logprior: 16.5447
Epoch 9/10
10/10 - 1s - loss: 250.5654 - loglik: -2.6883e+02 - logprior: 18.2660
Epoch 10/10
10/10 - 1s - loss: 247.5794 - loglik: -2.6725e+02 - logprior: 19.6670
Fitted a model with MAP estimate = -245.9422
Time for alignment: 31.0378
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 642.1185 - loglik: -3.7520e+02 - logprior: -2.6692e+02
Epoch 2/10
10/10 - 1s - loss: 414.0760 - loglik: -3.4412e+02 - logprior: -6.9961e+01
Epoch 3/10
10/10 - 1s - loss: 353.6435 - loglik: -3.2295e+02 - logprior: -3.0698e+01
Epoch 4/10
10/10 - 1s - loss: 324.0298 - loglik: -3.0765e+02 - logprior: -1.6376e+01
Epoch 5/10
10/10 - 1s - loss: 308.0843 - loglik: -2.9954e+02 - logprior: -8.5479e+00
Epoch 6/10
10/10 - 1s - loss: 299.1320 - loglik: -2.9544e+02 - logprior: -3.6898e+00
Epoch 7/10
10/10 - 1s - loss: 293.9304 - loglik: -2.9300e+02 - logprior: -9.2776e-01
Epoch 8/10
10/10 - 1s - loss: 290.1538 - loglik: -2.9087e+02 - logprior: 0.7211
Epoch 9/10
10/10 - 1s - loss: 287.7438 - loglik: -2.9003e+02 - logprior: 2.2824
Epoch 10/10
10/10 - 1s - loss: 286.2912 - loglik: -2.8967e+02 - logprior: 3.3765
Fitted a model with MAP estimate = -285.6649
expansions: [(0, 3), (10, 1), (17, 3), (24, 1), (25, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 660.0916 - loglik: -3.0495e+02 - logprior: -3.5514e+02
Epoch 2/2
10/10 - 1s - loss: 389.3906 - loglik: -2.8336e+02 - logprior: -1.0603e+02
Fitted a model with MAP estimate = -339.4638
expansions: []
discards: [ 0 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 586.5392 - loglik: -2.8613e+02 - logprior: -3.0041e+02
Epoch 2/2
10/10 - 1s - loss: 394.2765 - loglik: -2.7980e+02 - logprior: -1.1447e+02
Fitted a model with MAP estimate = -364.0305
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 558.6500 - loglik: -2.8361e+02 - logprior: -2.7504e+02
Epoch 2/10
10/10 - 1s - loss: 351.5124 - loglik: -2.7825e+02 - logprior: -7.3264e+01
Epoch 3/10
10/10 - 1s - loss: 300.2105 - loglik: -2.7787e+02 - logprior: -2.2342e+01
Epoch 4/10
10/10 - 1s - loss: 280.5038 - loglik: -2.7663e+02 - logprior: -3.8765e+00
Epoch 5/10
10/10 - 1s - loss: 268.6835 - loglik: -2.7407e+02 - logprior: 5.3880
Epoch 6/10
10/10 - 1s - loss: 261.5416 - loglik: -2.7225e+02 - logprior: 10.7092
Epoch 7/10
10/10 - 1s - loss: 257.2788 - loglik: -2.7146e+02 - logprior: 14.1789
Epoch 8/10
10/10 - 1s - loss: 253.8138 - loglik: -2.7036e+02 - logprior: 16.5443
Epoch 9/10
10/10 - 1s - loss: 250.5667 - loglik: -2.6883e+02 - logprior: 18.2655
Epoch 10/10
10/10 - 1s - loss: 247.5810 - loglik: -2.6725e+02 - logprior: 19.6663
Fitted a model with MAP estimate = -245.9457
Time for alignment: 29.6322
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 642.1185 - loglik: -3.7520e+02 - logprior: -2.6692e+02
Epoch 2/10
10/10 - 1s - loss: 414.0760 - loglik: -3.4412e+02 - logprior: -6.9961e+01
Epoch 3/10
10/10 - 1s - loss: 353.6435 - loglik: -3.2295e+02 - logprior: -3.0698e+01
Epoch 4/10
10/10 - 1s - loss: 324.0298 - loglik: -3.0765e+02 - logprior: -1.6376e+01
Epoch 5/10
10/10 - 1s - loss: 308.0844 - loglik: -2.9954e+02 - logprior: -8.5479e+00
Epoch 6/10
10/10 - 1s - loss: 299.1319 - loglik: -2.9544e+02 - logprior: -3.6898e+00
Epoch 7/10
10/10 - 1s - loss: 293.9303 - loglik: -2.9300e+02 - logprior: -9.2776e-01
Epoch 8/10
10/10 - 1s - loss: 290.1539 - loglik: -2.9087e+02 - logprior: 0.7211
Epoch 9/10
10/10 - 1s - loss: 287.7437 - loglik: -2.9003e+02 - logprior: 2.2824
Epoch 10/10
10/10 - 1s - loss: 286.2910 - loglik: -2.8967e+02 - logprior: 3.3765
Fitted a model with MAP estimate = -285.6646
expansions: [(0, 3), (10, 1), (17, 3), (24, 1), (25, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 660.0917 - loglik: -3.0495e+02 - logprior: -3.5514e+02
Epoch 2/2
10/10 - 1s - loss: 389.3906 - loglik: -2.8336e+02 - logprior: -1.0603e+02
Fitted a model with MAP estimate = -339.4638
expansions: []
discards: [ 0 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 586.5392 - loglik: -2.8613e+02 - logprior: -3.0041e+02
Epoch 2/2
10/10 - 1s - loss: 394.2765 - loglik: -2.7980e+02 - logprior: -1.1447e+02
Fitted a model with MAP estimate = -364.0304
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 558.6500 - loglik: -2.8361e+02 - logprior: -2.7504e+02
Epoch 2/10
10/10 - 1s - loss: 351.5124 - loglik: -2.7825e+02 - logprior: -7.3264e+01
Epoch 3/10
10/10 - 1s - loss: 300.2105 - loglik: -2.7787e+02 - logprior: -2.2341e+01
Epoch 4/10
10/10 - 1s - loss: 280.5038 - loglik: -2.7663e+02 - logprior: -3.8765e+00
Epoch 5/10
10/10 - 1s - loss: 268.6834 - loglik: -2.7407e+02 - logprior: 5.3880
Epoch 6/10
10/10 - 1s - loss: 261.5415 - loglik: -2.7225e+02 - logprior: 10.7094
Epoch 7/10
10/10 - 1s - loss: 257.2791 - loglik: -2.7146e+02 - logprior: 14.1789
Epoch 8/10
10/10 - 1s - loss: 253.8143 - loglik: -2.7036e+02 - logprior: 16.5444
Epoch 9/10
10/10 - 1s - loss: 250.5667 - loglik: -2.6883e+02 - logprior: 18.2656
Epoch 10/10
10/10 - 1s - loss: 247.5814 - loglik: -2.6725e+02 - logprior: 19.6664
Fitted a model with MAP estimate = -245.9448
Time for alignment: 29.8766
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 642.1185 - loglik: -3.7520e+02 - logprior: -2.6692e+02
Epoch 2/10
10/10 - 1s - loss: 414.0761 - loglik: -3.4412e+02 - logprior: -6.9961e+01
Epoch 3/10
10/10 - 1s - loss: 353.6435 - loglik: -3.2295e+02 - logprior: -3.0698e+01
Epoch 4/10
10/10 - 1s - loss: 324.0298 - loglik: -3.0765e+02 - logprior: -1.6376e+01
Epoch 5/10
10/10 - 1s - loss: 308.0843 - loglik: -2.9954e+02 - logprior: -8.5479e+00
Epoch 6/10
10/10 - 1s - loss: 299.1319 - loglik: -2.9544e+02 - logprior: -3.6898e+00
Epoch 7/10
10/10 - 1s - loss: 293.9302 - loglik: -2.9300e+02 - logprior: -9.2776e-01
Epoch 8/10
10/10 - 1s - loss: 290.1541 - loglik: -2.9087e+02 - logprior: 0.7211
Epoch 9/10
10/10 - 1s - loss: 287.7438 - loglik: -2.9003e+02 - logprior: 2.2824
Epoch 10/10
10/10 - 1s - loss: 286.2913 - loglik: -2.8967e+02 - logprior: 3.3765
Fitted a model with MAP estimate = -285.6673
expansions: [(0, 3), (10, 1), (17, 3), (24, 1), (25, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 660.0914 - loglik: -3.0495e+02 - logprior: -3.5514e+02
Epoch 2/2
10/10 - 1s - loss: 389.3906 - loglik: -2.8336e+02 - logprior: -1.0603e+02
Fitted a model with MAP estimate = -339.4638
expansions: []
discards: [ 0 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 6s - loss: 586.5392 - loglik: -2.8613e+02 - logprior: -3.0041e+02
Epoch 2/2
10/10 - 1s - loss: 394.2765 - loglik: -2.7980e+02 - logprior: -1.1447e+02
Fitted a model with MAP estimate = -364.0304
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 558.6500 - loglik: -2.8361e+02 - logprior: -2.7504e+02
Epoch 2/10
10/10 - 1s - loss: 351.5124 - loglik: -2.7825e+02 - logprior: -7.3264e+01
Epoch 3/10
10/10 - 1s - loss: 300.2105 - loglik: -2.7787e+02 - logprior: -2.2342e+01
Epoch 4/10
10/10 - 1s - loss: 280.5037 - loglik: -2.7663e+02 - logprior: -3.8765e+00
Epoch 5/10
10/10 - 1s - loss: 268.6834 - loglik: -2.7407e+02 - logprior: 5.3880
Epoch 6/10
10/10 - 1s - loss: 261.5413 - loglik: -2.7225e+02 - logprior: 10.7095
Epoch 7/10
10/10 - 1s - loss: 257.2788 - loglik: -2.7146e+02 - logprior: 14.1790
Epoch 8/10
10/10 - 1s - loss: 253.8144 - loglik: -2.7036e+02 - logprior: 16.5444
Epoch 9/10
10/10 - 1s - loss: 250.5667 - loglik: -2.6883e+02 - logprior: 18.2657
Epoch 10/10
10/10 - 1s - loss: 247.5804 - loglik: -2.6725e+02 - logprior: 19.6665
Fitted a model with MAP estimate = -245.9440
Time for alignment: 30.1542
Fitting a model of length 53 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 3s - loss: 642.1185 - loglik: -3.7520e+02 - logprior: -2.6692e+02
Epoch 2/10
10/10 - 1s - loss: 414.0760 - loglik: -3.4412e+02 - logprior: -6.9961e+01
Epoch 3/10
10/10 - 1s - loss: 353.6435 - loglik: -3.2295e+02 - logprior: -3.0698e+01
Epoch 4/10
10/10 - 1s - loss: 324.0298 - loglik: -3.0765e+02 - logprior: -1.6376e+01
Epoch 5/10
10/10 - 1s - loss: 308.0843 - loglik: -2.9954e+02 - logprior: -8.5479e+00
Epoch 6/10
10/10 - 1s - loss: 299.1320 - loglik: -2.9544e+02 - logprior: -3.6898e+00
Epoch 7/10
10/10 - 1s - loss: 293.9301 - loglik: -2.9300e+02 - logprior: -9.2776e-01
Epoch 8/10
10/10 - 1s - loss: 290.1536 - loglik: -2.9087e+02 - logprior: 0.7211
Epoch 9/10
10/10 - 1s - loss: 287.7437 - loglik: -2.9003e+02 - logprior: 2.2824
Epoch 10/10
10/10 - 1s - loss: 286.2914 - loglik: -2.8967e+02 - logprior: 3.3765
Fitted a model with MAP estimate = -285.6654
expansions: [(0, 3), (10, 1), (17, 3), (24, 1), (25, 1), (27, 2), (34, 1), (35, 2), (38, 3), (44, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 71 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 660.0913 - loglik: -3.0495e+02 - logprior: -3.5514e+02
Epoch 2/2
10/10 - 1s - loss: 389.3905 - loglik: -2.8336e+02 - logprior: -1.0603e+02
Fitted a model with MAP estimate = -339.4638
expansions: []
discards: [ 0 36 47 52]
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 3s - loss: 586.5392 - loglik: -2.8613e+02 - logprior: -3.0041e+02
Epoch 2/2
10/10 - 1s - loss: 394.2765 - loglik: -2.7980e+02 - logprior: -1.1447e+02
Fitted a model with MAP estimate = -364.0305
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 67 on 167 sequences.
Batch size= 167 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 558.6500 - loglik: -2.8361e+02 - logprior: -2.7504e+02
Epoch 2/10
10/10 - 1s - loss: 351.5124 - loglik: -2.7825e+02 - logprior: -7.3264e+01
Epoch 3/10
10/10 - 1s - loss: 300.2105 - loglik: -2.7787e+02 - logprior: -2.2342e+01
Epoch 4/10
10/10 - 1s - loss: 280.5038 - loglik: -2.7663e+02 - logprior: -3.8765e+00
Epoch 5/10
10/10 - 1s - loss: 268.6835 - loglik: -2.7407e+02 - logprior: 5.3879
Epoch 6/10
10/10 - 1s - loss: 261.5415 - loglik: -2.7225e+02 - logprior: 10.7093
Epoch 7/10
10/10 - 1s - loss: 257.2789 - loglik: -2.7146e+02 - logprior: 14.1789
Epoch 8/10
10/10 - 1s - loss: 253.8138 - loglik: -2.7036e+02 - logprior: 16.5443
Epoch 9/10
10/10 - 1s - loss: 250.5662 - loglik: -2.6883e+02 - logprior: 18.2655
Epoch 10/10
10/10 - 1s - loss: 247.5807 - loglik: -2.6725e+02 - logprior: 19.6663
Fitted a model with MAP estimate = -245.9446
Time for alignment: 29.2409
Computed alignments with likelihoods: ['-245.9422', '-245.9457', '-245.9448', '-245.9440', '-245.9446']
Best model has likelihood: -245.9422  (prior= 20.3487 )
time for generating output: 0.1199
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hip.projection.fasta
SP score = 0.8293051359516617
Training of 5 independent models on file peroxidase.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9f868e310>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9bc529b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9eff82ca0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 1424.6567 - loglik: -1.4207e+03 - logprior: -4.0030e+00
Epoch 2/10
26/26 - 9s - loss: 1321.4913 - loglik: -1.3204e+03 - logprior: -1.1332e+00
Epoch 3/10
26/26 - 9s - loss: 1308.4135 - loglik: -1.3073e+03 - logprior: -1.1043e+00
Epoch 4/10
26/26 - 9s - loss: 1307.1237 - loglik: -1.3060e+03 - logprior: -1.1206e+00
Epoch 5/10
26/26 - 9s - loss: 1305.8323 - loglik: -1.3047e+03 - logprior: -1.1450e+00
Epoch 6/10
26/26 - 9s - loss: 1299.4335 - loglik: -1.2982e+03 - logprior: -1.1971e+00
Epoch 7/10
26/26 - 9s - loss: 1291.5319 - loglik: -1.2902e+03 - logprior: -1.2860e+00
Epoch 8/10
26/26 - 9s - loss: 1268.8228 - loglik: -1.2673e+03 - logprior: -1.4905e+00
Epoch 9/10
26/26 - 9s - loss: 1174.3413 - loglik: -1.1709e+03 - logprior: -3.4100e+00
Epoch 10/10
26/26 - 9s - loss: 1104.3586 - loglik: -1.0993e+03 - logprior: -5.0214e+00
Fitted a model with MAP estimate = -1078.7404
expansions: []
discards: [ 15  16  17  28  29  30  31  32  33  91 107 118 119 120 130 131 132 133
 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169
 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187
 188 189 190 191 192 193 194 195 196 197 198 199]
Re-initialized the encoder parameters.
Fitting a model of length 116 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 8s - loss: 1513.9778 - loglik: -1.5093e+03 - logprior: -4.6536e+00
Epoch 2/2
26/26 - 5s - loss: 1470.5806 - loglik: -1.4704e+03 - logprior: -1.7370e-01
Fitted a model with MAP estimate = -1451.7777
expansions: [(0, 92), (107, 2), (116, 105)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1421.0822 - loglik: -1.4161e+03 - logprior: -4.9869e+00
Epoch 2/2
26/26 - 9s - loss: 1329.9921 - loglik: -1.3281e+03 - logprior: -1.9246e+00
Fitted a model with MAP estimate = -1319.0160
expansions: [(95, 1), (196, 1)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  36  37  38  39  40  41  42  43  44  45  47  54
  62  63  64  65  66  67 176 177 178]
Re-initialized the encoder parameters.
Fitting a model of length 167 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 10s - loss: 1358.7104 - loglik: -1.3523e+03 - logprior: -6.3788e+00
Epoch 2/10
26/26 - 7s - loss: 1346.5447 - loglik: -1.3453e+03 - logprior: -1.2235e+00
Epoch 3/10
26/26 - 7s - loss: 1341.7765 - loglik: -1.3407e+03 - logprior: -1.0828e+00
Epoch 4/10
26/26 - 7s - loss: 1341.9114 - loglik: -1.3409e+03 - logprior: -1.0291e+00
Fitted a model with MAP estimate = -1339.3108
Time for alignment: 185.1371
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 1424.6353 - loglik: -1.4206e+03 - logprior: -4.0324e+00
Epoch 2/10
26/26 - 9s - loss: 1322.7493 - loglik: -1.3217e+03 - logprior: -1.0537e+00
Epoch 3/10
26/26 - 9s - loss: 1311.1935 - loglik: -1.3102e+03 - logprior: -9.6564e-01
Epoch 4/10
26/26 - 9s - loss: 1308.7057 - loglik: -1.3078e+03 - logprior: -9.5309e-01
Epoch 5/10
26/26 - 9s - loss: 1305.1372 - loglik: -1.3042e+03 - logprior: -9.6185e-01
Epoch 6/10
26/26 - 9s - loss: 1304.2003 - loglik: -1.3032e+03 - logprior: -9.5720e-01
Epoch 7/10
26/26 - 9s - loss: 1301.2708 - loglik: -1.3002e+03 - logprior: -1.0157e+00
Epoch 8/10
26/26 - 9s - loss: 1293.7677 - loglik: -1.2927e+03 - logprior: -1.0517e+00
Epoch 9/10
26/26 - 9s - loss: 1267.9800 - loglik: -1.2664e+03 - logprior: -1.5357e+00
Epoch 10/10
26/26 - 9s - loss: 1161.4672 - loglik: -1.1555e+03 - logprior: -5.9826e+00
Fitted a model with MAP estimate = -1115.0426
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  87  88 100 101 113 114 115 116
 117 118 119 120 124 125 126 132 133 134 150 168 171 172 173 174 175 176
 177 178 179 180 181 182 183 196]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 4514 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 1516.4026 - loglik: -1.5075e+03 - logprior: -8.9061e+00
Epoch 2/2
13/13 - 3s - loss: 1495.1127 - loglik: -1.4931e+03 - logprior: -1.9815e+00
Fitted a model with MAP estimate = -1492.5269
expansions: [(0, 117), (84, 133)]
discards: [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28
 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52
 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76
 77 78 79 80 81 82 83]
Re-initialized the encoder parameters.
Fitting a model of length 255 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 15s - loss: 1422.5802 - loglik: -1.4167e+03 - logprior: -5.8937e+00
Epoch 2/2
26/26 - 12s - loss: 1304.8621 - loglik: -1.3040e+03 - logprior: -8.3396e-01
Fitted a model with MAP estimate = -1294.5473
expansions: [(0, 12), (38, 1), (46, 1), (49, 1), (51, 1), (53, 1), (54, 1), (79, 1), (80, 1), (93, 4), (99, 1), (100, 1), (101, 1)]
discards: [216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233
 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253 254]
Re-initialized the encoder parameters.
Fitting a model of length 243 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 15s - loss: 1313.4087 - loglik: -1.3070e+03 - logprior: -6.3844e+00
Epoch 2/10
26/26 - 11s - loss: 1297.8217 - loglik: -1.2971e+03 - logprior: -7.0200e-01
Epoch 3/10
26/26 - 11s - loss: 1297.3789 - loglik: -1.2975e+03 - logprior: 0.1104
Epoch 4/10
26/26 - 11s - loss: 1289.6228 - loglik: -1.2898e+03 - logprior: 0.1387
Epoch 5/10
26/26 - 11s - loss: 1290.3789 - loglik: -1.2906e+03 - logprior: 0.1933
Fitted a model with MAP estimate = -1289.6138
Time for alignment: 221.3737
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 1419.1887 - loglik: -1.4152e+03 - logprior: -4.0086e+00
Epoch 2/10
26/26 - 9s - loss: 1325.3162 - loglik: -1.3240e+03 - logprior: -1.3153e+00
Epoch 3/10
26/26 - 9s - loss: 1308.5695 - loglik: -1.3073e+03 - logprior: -1.2854e+00
Epoch 4/10
26/26 - 9s - loss: 1308.8053 - loglik: -1.3074e+03 - logprior: -1.3627e+00
Fitted a model with MAP estimate = -1304.6473
expansions: [(130, 1), (173, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 201 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1315.3866 - loglik: -1.3088e+03 - logprior: -6.6132e+00
Epoch 2/2
26/26 - 9s - loss: 1310.1980 - loglik: -1.3079e+03 - logprior: -2.2996e+00
Fitted a model with MAP estimate = -1307.3314
expansions: [(0, 5)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 12s - loss: 1314.6896 - loglik: -1.3102e+03 - logprior: -4.4708e+00
Epoch 2/2
26/26 - 9s - loss: 1304.5702 - loglik: -1.3036e+03 - logprior: -9.5223e-01
Fitted a model with MAP estimate = -1305.2260
expansions: []
discards: [2 3 4]
Re-initialized the encoder parameters.
Fitting a model of length 202 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 1310.9214 - loglik: -1.3067e+03 - logprior: -4.2073e+00
Epoch 2/10
26/26 - 9s - loss: 1305.9406 - loglik: -1.3052e+03 - logprior: -7.0784e-01
Epoch 3/10
26/26 - 9s - loss: 1303.8353 - loglik: -1.3034e+03 - logprior: -4.7282e-01
Epoch 4/10
26/26 - 9s - loss: 1306.1356 - loglik: -1.3057e+03 - logprior: -4.5843e-01
Fitted a model with MAP estimate = -1302.6218
Time for alignment: 153.6093
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 12s - loss: 1424.0822 - loglik: -1.4201e+03 - logprior: -4.0156e+00
Epoch 2/10
26/26 - 9s - loss: 1316.0306 - loglik: -1.3148e+03 - logprior: -1.2668e+00
Epoch 3/10
26/26 - 9s - loss: 1310.4088 - loglik: -1.3091e+03 - logprior: -1.2844e+00
Epoch 4/10
26/26 - 9s - loss: 1304.7854 - loglik: -1.3035e+03 - logprior: -1.2386e+00
Epoch 5/10
26/26 - 9s - loss: 1302.2821 - loglik: -1.3010e+03 - logprior: -1.2699e+00
Epoch 6/10
26/26 - 9s - loss: 1298.8986 - loglik: -1.2976e+03 - logprior: -1.3369e+00
Epoch 7/10
26/26 - 9s - loss: 1294.3353 - loglik: -1.2929e+03 - logprior: -1.4091e+00
Epoch 8/10
26/26 - 9s - loss: 1270.7216 - loglik: -1.2691e+03 - logprior: -1.5631e+00
Epoch 9/10
26/26 - 9s - loss: 1189.0269 - loglik: -1.1851e+03 - logprior: -3.8902e+00
Epoch 10/10
26/26 - 9s - loss: 1104.6376 - loglik: -1.0987e+03 - logprior: -5.9580e+00
Fitted a model with MAP estimate = -1086.0656
expansions: []
discards: [ 15  16  17  18  19  20  21  27  28  29  30  31  32  60  61  62  63  64
  65  99 100 101 102 103 104 105 106 107 108 144 145 146 147 148 149 150
 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168
 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186
 187 188 189 190 191 192 193 194 195 196 197 198 199]
Re-initialized the encoder parameters.
Fitting a model of length 115 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 9s - loss: 1508.9247 - loglik: -1.5044e+03 - logprior: -4.5285e+00
Epoch 2/2
26/26 - 6s - loss: 1460.6536 - loglik: -1.4596e+03 - logprior: -1.0675e+00
Fitted a model with MAP estimate = -1436.4165
expansions: [(0, 76), (115, 116)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  35  36  37
  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55
  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73
  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91
  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109
 110 111 112 113 114]
Re-initialized the encoder parameters.
Fitting a model of length 194 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 11s - loss: 1414.6385 - loglik: -1.4100e+03 - logprior: -4.6457e+00
Epoch 2/2
26/26 - 8s - loss: 1324.4778 - loglik: -1.3232e+03 - logprior: -1.2438e+00
Fitted a model with MAP estimate = -1314.3680
expansions: [(0, 10), (109, 1), (167, 2)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26]
Re-initialized the encoder parameters.
Fitting a model of length 180 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 10s - loss: 1335.4706 - loglik: -1.3303e+03 - logprior: -5.1289e+00
Epoch 2/10
26/26 - 8s - loss: 1315.9030 - loglik: -1.3145e+03 - logprior: -1.4345e+00
Epoch 3/10
26/26 - 8s - loss: 1317.1947 - loglik: -1.3159e+03 - logprior: -1.2746e+00
Fitted a model with MAP estimate = -1315.1160
Time for alignment: 178.6181
Fitting a model of length 200 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 1426.4728 - loglik: -1.4224e+03 - logprior: -4.0415e+00
Epoch 2/10
26/26 - 9s - loss: 1324.4674 - loglik: -1.3234e+03 - logprior: -1.0234e+00
Epoch 3/10
26/26 - 9s - loss: 1313.5645 - loglik: -1.3126e+03 - logprior: -9.5099e-01
Epoch 4/10
26/26 - 9s - loss: 1312.1021 - loglik: -1.3112e+03 - logprior: -9.3016e-01
Epoch 5/10
26/26 - 9s - loss: 1305.8134 - loglik: -1.3048e+03 - logprior: -9.7252e-01
Epoch 6/10
26/26 - 9s - loss: 1304.6118 - loglik: -1.3036e+03 - logprior: -9.8986e-01
Epoch 7/10
26/26 - 9s - loss: 1299.5686 - loglik: -1.2985e+03 - logprior: -1.0409e+00
Epoch 8/10
26/26 - 9s - loss: 1291.1682 - loglik: -1.2901e+03 - logprior: -1.1006e+00
Epoch 9/10
26/26 - 9s - loss: 1259.2047 - loglik: -1.2571e+03 - logprior: -2.0893e+00
Epoch 10/10
26/26 - 9s - loss: 1152.7128 - loglik: -1.1473e+03 - logprior: -5.4179e+00
Fitted a model with MAP estimate = -1111.3725
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  48  62  63  64  65  66  67  68
  78  79  80  81 117 118 119 120 121 124 125 126 127 133 134 167 168 169
 170 171 172 173 174 175 176 177 178 179 180 181 182 183 196]
Re-initialized the encoder parameters.
Fitting a model of length 113 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 8s - loss: 1509.6111 - loglik: -1.5044e+03 - logprior: -5.2571e+00
Epoch 2/2
26/26 - 6s - loss: 1469.5059 - loglik: -1.4686e+03 - logprior: -8.7159e-01
Fitted a model with MAP estimate = -1455.8224
expansions: [(0, 124), (113, 104)]
discards: [  3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20
  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38
  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56
  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74
  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92
  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110
 111 112]
Re-initialized the encoder parameters.
Fitting a model of length 231 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
26/26 - 13s - loss: 1415.5198 - loglik: -1.4105e+03 - logprior: -5.0100e+00
Epoch 2/2
26/26 - 10s - loss: 1309.1783 - loglik: -1.3075e+03 - logprior: -1.6369e+00
Fitted a model with MAP estimate = -1296.3457
expansions: [(0, 9), (43, 1), (82, 1), (126, 1), (175, 1)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 60 61 62 63]
Re-initialized the encoder parameters.
Fitting a model of length 225 on 4514 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
26/26 - 13s - loss: 1309.2037 - loglik: -1.3038e+03 - logprior: -5.4352e+00
Epoch 2/10
26/26 - 10s - loss: 1292.1171 - loglik: -1.2909e+03 - logprior: -1.2125e+00
Epoch 3/10
26/26 - 10s - loss: 1291.5242 - loglik: -1.2905e+03 - logprior: -1.0064e+00
Epoch 4/10
26/26 - 10s - loss: 1290.8640 - loglik: -1.2899e+03 - logprior: -9.9003e-01
Epoch 5/10
26/26 - 10s - loss: 1288.3052 - loglik: -1.2874e+03 - logprior: -9.1631e-01
Epoch 6/10
26/26 - 10s - loss: 1286.3225 - loglik: -1.2854e+03 - logprior: -8.6858e-01
Epoch 7/10
26/26 - 10s - loss: 1283.6954 - loglik: -1.2829e+03 - logprior: -8.1738e-01
Epoch 8/10
26/26 - 10s - loss: 1283.6141 - loglik: -1.2828e+03 - logprior: -8.1931e-01
Epoch 9/10
26/26 - 10s - loss: 1275.6588 - loglik: -1.2748e+03 - logprior: -8.2875e-01
Epoch 10/10
26/26 - 10s - loss: 1264.2815 - loglik: -1.2635e+03 - logprior: -8.0066e-01
Fitted a model with MAP estimate = -1244.4494
Time for alignment: 265.4440
Computed alignments with likelihoods: ['-1078.7404', '-1115.0426', '-1302.6218', '-1086.0656', '-1111.3725']
Best model has likelihood: -1078.7404  (prior= -4.6630 )
time for generating output: 0.3023
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/peroxidase.projection.fasta
SP score = 0.09612903225806452
Training of 5 independent models on file TNF.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe953230c10>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe952fefbe0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe952fefe50>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 769.0436 - loglik: -6.9229e+02 - logprior: -7.6752e+01
Epoch 2/10
10/10 - 1s - loss: 683.8163 - loglik: -6.6617e+02 - logprior: -1.7643e+01
Epoch 3/10
10/10 - 1s - loss: 645.2546 - loglik: -6.3883e+02 - logprior: -6.4249e+00
Epoch 4/10
10/10 - 1s - loss: 626.5701 - loglik: -6.2425e+02 - logprior: -2.3207e+00
Epoch 5/10
10/10 - 1s - loss: 616.8635 - loglik: -6.1658e+02 - logprior: -2.7914e-01
Epoch 6/10
10/10 - 1s - loss: 610.4814 - loglik: -6.1123e+02 - logprior: 0.7480
Epoch 7/10
10/10 - 1s - loss: 606.4135 - loglik: -6.0787e+02 - logprior: 1.4528
Epoch 8/10
10/10 - 1s - loss: 604.8719 - loglik: -6.0675e+02 - logprior: 1.8819
Epoch 9/10
10/10 - 1s - loss: 601.5986 - loglik: -6.0369e+02 - logprior: 2.0975
Epoch 10/10
10/10 - 1s - loss: 597.0026 - loglik: -5.9906e+02 - logprior: 2.0602
Fitted a model with MAP estimate = -594.3933
expansions: [(9, 2), (10, 1), (12, 4), (18, 2), (30, 1), (36, 4), (45, 3), (48, 1), (54, 2), (68, 2), (87, 8), (89, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 128 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 699.8687 - loglik: -6.3094e+02 - logprior: -6.8926e+01
Epoch 2/2
10/10 - 2s - loss: 619.4613 - loglik: -6.0389e+02 - logprior: -1.5567e+01
Fitted a model with MAP estimate = -605.2294
expansions: [(85, 3), (106, 2), (108, 2)]
discards: [  0  40  80 117]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 684.5466 - loglik: -5.9974e+02 - logprior: -8.4808e+01
Epoch 2/2
10/10 - 2s - loss: 624.3884 - loglik: -5.9213e+02 - logprior: -3.2263e+01
Fitted a model with MAP estimate = -614.7143
expansions: [(0, 6)]
discards: [  0  44  46  71 110 111]
Re-initialized the encoder parameters.
Fitting a model of length 131 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 661.6510 - loglik: -5.9338e+02 - logprior: -6.8267e+01
Epoch 2/10
10/10 - 2s - loss: 602.6698 - loglik: -5.8833e+02 - logprior: -1.4338e+01
Epoch 3/10
10/10 - 2s - loss: 589.9531 - loglik: -5.8716e+02 - logprior: -2.7913e+00
Epoch 4/10
10/10 - 2s - loss: 583.7482 - loglik: -5.8585e+02 - logprior: 2.1008
Epoch 5/10
10/10 - 2s - loss: 578.9908 - loglik: -5.8383e+02 - logprior: 4.8417
Epoch 6/10
10/10 - 2s - loss: 575.6887 - loglik: -5.8212e+02 - logprior: 6.4300
Epoch 7/10
10/10 - 2s - loss: 572.6877 - loglik: -5.8005e+02 - logprior: 7.3624
Epoch 8/10
10/10 - 2s - loss: 571.3041 - loglik: -5.7929e+02 - logprior: 7.9878
Epoch 9/10
10/10 - 2s - loss: 569.6116 - loglik: -5.7813e+02 - logprior: 8.5198
Epoch 10/10
10/10 - 2s - loss: 567.7203 - loglik: -5.7665e+02 - logprior: 8.9287
Fitted a model with MAP estimate = -566.3456
Time for alignment: 52.5951
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 769.5297 - loglik: -6.9278e+02 - logprior: -7.6750e+01
Epoch 2/10
10/10 - 1s - loss: 682.6298 - loglik: -6.6498e+02 - logprior: -1.7652e+01
Epoch 3/10
10/10 - 1s - loss: 644.7489 - loglik: -6.3827e+02 - logprior: -6.4749e+00
Epoch 4/10
10/10 - 1s - loss: 624.4571 - loglik: -6.2211e+02 - logprior: -2.3446e+00
Epoch 5/10
10/10 - 1s - loss: 615.6631 - loglik: -6.1557e+02 - logprior: -9.7409e-02
Epoch 6/10
10/10 - 1s - loss: 610.0211 - loglik: -6.1106e+02 - logprior: 1.0427
Epoch 7/10
10/10 - 1s - loss: 606.4037 - loglik: -6.0805e+02 - logprior: 1.6455
Epoch 8/10
10/10 - 1s - loss: 603.2521 - loglik: -6.0507e+02 - logprior: 1.8202
Epoch 9/10
10/10 - 1s - loss: 600.7406 - loglik: -6.0253e+02 - logprior: 1.7948
Epoch 10/10
10/10 - 1s - loss: 597.2459 - loglik: -5.9917e+02 - logprior: 1.9296
Fitted a model with MAP estimate = -594.7883
expansions: [(11, 4), (12, 1), (18, 1), (19, 1), (36, 4), (45, 3), (63, 3), (79, 1), (87, 7), (89, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 698.7988 - loglik: -6.2988e+02 - logprior: -6.8922e+01
Epoch 2/2
10/10 - 1s - loss: 620.8905 - loglik: -6.0553e+02 - logprior: -1.5357e+01
Fitted a model with MAP estimate = -606.6158
expansions: [(102, 3), (104, 1)]
discards: [  0  43  80  81 112]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 687.0057 - loglik: -6.0182e+02 - logprior: -8.5185e+01
Epoch 2/2
10/10 - 1s - loss: 626.4814 - loglik: -5.9393e+02 - logprior: -3.2549e+01
Fitted a model with MAP estimate = -617.3628
expansions: [(0, 5)]
discards: [ 0 99]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 661.3058 - loglik: -5.9283e+02 - logprior: -6.8473e+01
Epoch 2/10
10/10 - 1s - loss: 604.2313 - loglik: -5.8995e+02 - logprior: -1.4281e+01
Epoch 3/10
10/10 - 1s - loss: 591.5526 - loglik: -5.8865e+02 - logprior: -2.9064e+00
Epoch 4/10
10/10 - 1s - loss: 586.4943 - loglik: -5.8837e+02 - logprior: 1.8769
Epoch 5/10
10/10 - 1s - loss: 582.8836 - loglik: -5.8744e+02 - logprior: 4.5579
Epoch 6/10
10/10 - 1s - loss: 579.1637 - loglik: -5.8535e+02 - logprior: 6.1864
Epoch 7/10
10/10 - 1s - loss: 576.7212 - loglik: -5.8386e+02 - logprior: 7.1383
Epoch 8/10
10/10 - 1s - loss: 574.8782 - loglik: -5.8262e+02 - logprior: 7.7380
Epoch 9/10
10/10 - 1s - loss: 573.6411 - loglik: -5.8185e+02 - logprior: 8.2053
Epoch 10/10
10/10 - 1s - loss: 572.1993 - loglik: -5.8085e+02 - logprior: 8.6508
Fitted a model with MAP estimate = -571.0919
Time for alignment: 50.9582
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 769.7213 - loglik: -6.9297e+02 - logprior: -7.6748e+01
Epoch 2/10
10/10 - 1s - loss: 682.4683 - loglik: -6.6481e+02 - logprior: -1.7662e+01
Epoch 3/10
10/10 - 1s - loss: 644.7792 - loglik: -6.3821e+02 - logprior: -6.5667e+00
Epoch 4/10
10/10 - 1s - loss: 625.9719 - loglik: -6.2351e+02 - logprior: -2.4599e+00
Epoch 5/10
10/10 - 1s - loss: 617.1500 - loglik: -6.1697e+02 - logprior: -1.8380e-01
Epoch 6/10
10/10 - 1s - loss: 610.6881 - loglik: -6.1170e+02 - logprior: 1.0169
Epoch 7/10
10/10 - 1s - loss: 606.8925 - loglik: -6.0826e+02 - logprior: 1.3725
Epoch 8/10
10/10 - 1s - loss: 604.0999 - loglik: -6.0552e+02 - logprior: 1.4251
Epoch 9/10
10/10 - 1s - loss: 601.5642 - loglik: -6.0327e+02 - logprior: 1.7055
Epoch 10/10
10/10 - 1s - loss: 598.1290 - loglik: -5.9996e+02 - logprior: 1.8294
Fitted a model with MAP estimate = -596.1044
expansions: [(10, 1), (11, 4), (12, 1), (19, 2), (36, 4), (45, 3), (55, 1), (79, 1), (82, 1), (83, 1), (84, 4), (87, 4), (89, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 700.6014 - loglik: -6.3148e+02 - logprior: -6.9125e+01
Epoch 2/2
10/10 - 1s - loss: 621.5107 - loglik: -6.0552e+02 - logprior: -1.5993e+01
Fitted a model with MAP estimate = -607.0420
expansions: [(110, 2)]
discards: [  0  10  48  76 107]
Re-initialized the encoder parameters.
Fitting a model of length 122 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 687.1384 - loglik: -6.0169e+02 - logprior: -8.5453e+01
Epoch 2/2
10/10 - 1s - loss: 626.2349 - loglik: -5.9352e+02 - logprior: -3.2711e+01
Fitted a model with MAP estimate = -617.0330
expansions: [(0, 5), (68, 1), (73, 1)]
discards: [ 0 44]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 7s - loss: 660.8502 - loglik: -5.9219e+02 - logprior: -6.8664e+01
Epoch 2/10
10/10 - 1s - loss: 602.8809 - loglik: -5.8858e+02 - logprior: -1.4302e+01
Epoch 3/10
10/10 - 1s - loss: 590.5123 - loglik: -5.8771e+02 - logprior: -2.7978e+00
Epoch 4/10
10/10 - 2s - loss: 584.7340 - loglik: -5.8670e+02 - logprior: 1.9693
Epoch 5/10
10/10 - 1s - loss: 580.6341 - loglik: -5.8530e+02 - logprior: 4.6687
Epoch 6/10
10/10 - 1s - loss: 577.2863 - loglik: -5.8363e+02 - logprior: 6.3437
Epoch 7/10
10/10 - 2s - loss: 574.2815 - loglik: -5.8162e+02 - logprior: 7.3414
Epoch 8/10
10/10 - 1s - loss: 571.8638 - loglik: -5.7981e+02 - logprior: 7.9493
Epoch 9/10
10/10 - 1s - loss: 570.0923 - loglik: -5.7848e+02 - logprior: 8.3898
Epoch 10/10
10/10 - 2s - loss: 566.3339 - loglik: -5.7510e+02 - logprior: 8.7649
Fitted a model with MAP estimate = -564.4626
Time for alignment: 49.4992
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 769.5703 - loglik: -6.9282e+02 - logprior: -7.6750e+01
Epoch 2/10
10/10 - 1s - loss: 682.9505 - loglik: -6.6531e+02 - logprior: -1.7638e+01
Epoch 3/10
10/10 - 1s - loss: 645.6868 - loglik: -6.3923e+02 - logprior: -6.4582e+00
Epoch 4/10
10/10 - 1s - loss: 625.8084 - loglik: -6.2344e+02 - logprior: -2.3710e+00
Epoch 5/10
10/10 - 1s - loss: 615.1317 - loglik: -6.1479e+02 - logprior: -3.3765e-01
Epoch 6/10
10/10 - 1s - loss: 609.2259 - loglik: -6.0997e+02 - logprior: 0.7448
Epoch 7/10
10/10 - 1s - loss: 605.9694 - loglik: -6.0744e+02 - logprior: 1.4724
Epoch 8/10
10/10 - 1s - loss: 602.6150 - loglik: -6.0439e+02 - logprior: 1.7806
Epoch 9/10
10/10 - 1s - loss: 600.0779 - loglik: -6.0200e+02 - logprior: 1.9254
Epoch 10/10
10/10 - 1s - loss: 595.8090 - loglik: -5.9784e+02 - logprior: 2.0367
Fitted a model with MAP estimate = -593.4629
expansions: [(9, 2), (10, 1), (12, 4), (14, 2), (18, 1), (36, 4), (45, 3), (63, 3), (78, 1), (87, 7), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 717.8369 - loglik: -6.3190e+02 - logprior: -8.5936e+01
Epoch 2/2
10/10 - 1s - loss: 640.4198 - loglik: -6.0773e+02 - logprior: -3.2686e+01
Fitted a model with MAP estimate = -625.5987
expansions: [(106, 2)]
discards: [  0  20  45  47  81  82 114]
Re-initialized the encoder parameters.
Fitting a model of length 120 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 686.1028 - loglik: -6.0135e+02 - logprior: -8.4749e+01
Epoch 2/2
10/10 - 1s - loss: 627.3689 - loglik: -5.9556e+02 - logprior: -3.1809e+01
Fitted a model with MAP estimate = -616.9722
expansions: [(0, 4)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 123 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 660.5267 - loglik: -5.9381e+02 - logprior: -6.6721e+01
Epoch 2/10
10/10 - 1s - loss: 604.7206 - loglik: -5.9092e+02 - logprior: -1.3803e+01
Epoch 3/10
10/10 - 1s - loss: 591.8947 - loglik: -5.8924e+02 - logprior: -2.6565e+00
Epoch 4/10
10/10 - 1s - loss: 586.9014 - loglik: -5.8896e+02 - logprior: 2.0543
Epoch 5/10
10/10 - 1s - loss: 582.3871 - loglik: -5.8715e+02 - logprior: 4.7621
Epoch 6/10
10/10 - 1s - loss: 578.8419 - loglik: -5.8521e+02 - logprior: 6.3639
Epoch 7/10
10/10 - 1s - loss: 576.7249 - loglik: -5.8402e+02 - logprior: 7.2946
Epoch 8/10
10/10 - 1s - loss: 574.4813 - loglik: -5.8239e+02 - logprior: 7.9114
Epoch 9/10
10/10 - 1s - loss: 573.3300 - loglik: -5.8173e+02 - logprior: 8.4024
Epoch 10/10
10/10 - 1s - loss: 571.6994 - loglik: -5.8053e+02 - logprior: 8.8339
Fitted a model with MAP estimate = -570.2275
Time for alignment: 46.8109
Fitting a model of length 97 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 769.4443 - loglik: -6.9269e+02 - logprior: -7.6750e+01
Epoch 2/10
10/10 - 1s - loss: 682.9670 - loglik: -6.6532e+02 - logprior: -1.7647e+01
Epoch 3/10
10/10 - 1s - loss: 644.7743 - loglik: -6.3835e+02 - logprior: -6.4200e+00
Epoch 4/10
10/10 - 1s - loss: 625.4232 - loglik: -6.2321e+02 - logprior: -2.2107e+00
Epoch 5/10
10/10 - 1s - loss: 616.8073 - loglik: -6.1688e+02 - logprior: 0.0741
Epoch 6/10
10/10 - 1s - loss: 610.4193 - loglik: -6.1151e+02 - logprior: 1.0946
Epoch 7/10
10/10 - 1s - loss: 606.0256 - loglik: -6.0755e+02 - logprior: 1.5234
Epoch 8/10
10/10 - 1s - loss: 603.6878 - loglik: -6.0551e+02 - logprior: 1.8216
Epoch 9/10
10/10 - 1s - loss: 601.9622 - loglik: -6.0409e+02 - logprior: 2.1317
Epoch 10/10
10/10 - 1s - loss: 600.1095 - loglik: -6.0238e+02 - logprior: 2.2743
Fitted a model with MAP estimate = -598.4655
expansions: [(10, 1), (11, 4), (12, 1), (19, 2), (36, 4), (43, 1), (44, 3), (53, 1), (63, 3), (83, 2), (86, 1), (87, 6), (89, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 126 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 5s - loss: 711.0886 - loglik: -6.2541e+02 - logprior: -8.5678e+01
Epoch 2/2
10/10 - 1s - loss: 635.1748 - loglik: -6.0273e+02 - logprior: -3.2448e+01
Fitted a model with MAP estimate = -621.4695
expansions: [(0, 4)]
discards: [ 0 45 82]
Re-initialized the encoder parameters.
Fitting a model of length 127 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 663.9943 - loglik: -5.9613e+02 - logprior: -6.7863e+01
Epoch 2/2
10/10 - 1s - loss: 605.0992 - loglik: -5.9057e+02 - logprior: -1.4532e+01
Fitted a model with MAP estimate = -595.6143
expansions: [(83, 2)]
discards: [ 1  2  3 70]
Re-initialized the encoder parameters.
Fitting a model of length 125 on 556 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 657.0236 - loglik: -5.9112e+02 - logprior: -6.5906e+01
Epoch 2/10
10/10 - 1s - loss: 603.1540 - loglik: -5.8960e+02 - logprior: -1.3557e+01
Epoch 3/10
10/10 - 1s - loss: 590.4445 - loglik: -5.8800e+02 - logprior: -2.4434e+00
Epoch 4/10
10/10 - 1s - loss: 584.9681 - loglik: -5.8733e+02 - logprior: 2.3628
Epoch 5/10
10/10 - 1s - loss: 581.1058 - loglik: -5.8612e+02 - logprior: 5.0192
Epoch 6/10
10/10 - 1s - loss: 578.2240 - loglik: -5.8478e+02 - logprior: 6.5572
Epoch 7/10
10/10 - 1s - loss: 575.6562 - loglik: -5.8308e+02 - logprior: 7.4227
Epoch 8/10
10/10 - 1s - loss: 573.2308 - loglik: -5.8114e+02 - logprior: 7.9062
Epoch 9/10
10/10 - 1s - loss: 572.5073 - loglik: -5.8084e+02 - logprior: 8.3372
Epoch 10/10
10/10 - 1s - loss: 570.5754 - loglik: -5.7944e+02 - logprior: 8.8619
Fitted a model with MAP estimate = -569.8036
Time for alignment: 49.6013
Computed alignments with likelihoods: ['-566.3456', '-571.0919', '-564.4626', '-570.2275', '-569.8036']
Best model has likelihood: -564.4626  (prior= 8.9699 )
time for generating output: 0.1720
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/TNF.projection.fasta
SP score = 0.7338129496402878
Training of 5 independent models on file egf.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8e35cc340>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8f3e11670>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9ca89a30>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 190.8724 - loglik: -1.8644e+02 - logprior: -4.4350e+00
Epoch 2/10
17/17 - 1s - loss: 163.8213 - loglik: -1.6226e+02 - logprior: -1.5581e+00
Epoch 3/10
17/17 - 1s - loss: 153.3793 - loglik: -1.5180e+02 - logprior: -1.5830e+00
Epoch 4/10
17/17 - 1s - loss: 151.4459 - loglik: -1.4980e+02 - logprior: -1.6463e+00
Epoch 5/10
17/17 - 1s - loss: 151.1087 - loglik: -1.4958e+02 - logprior: -1.5265e+00
Epoch 6/10
17/17 - 1s - loss: 150.6147 - loglik: -1.4907e+02 - logprior: -1.5412e+00
Epoch 7/10
17/17 - 1s - loss: 150.4810 - loglik: -1.4895e+02 - logprior: -1.5223e+00
Epoch 8/10
17/17 - 1s - loss: 150.1452 - loglik: -1.4863e+02 - logprior: -1.5075e+00
Epoch 9/10
17/17 - 1s - loss: 149.8903 - loglik: -1.4838e+02 - logprior: -1.5006e+00
Epoch 10/10
17/17 - 1s - loss: 149.7964 - loglik: -1.4829e+02 - logprior: -1.4935e+00
Fitted a model with MAP estimate = -149.5999
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 2), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 164.1872 - loglik: -1.5858e+02 - logprior: -5.6064e+00
Epoch 2/2
17/17 - 1s - loss: 149.9016 - loglik: -1.4709e+02 - logprior: -2.8146e+00
Fitted a model with MAP estimate = -148.2413
expansions: [(2, 1)]
discards: [ 0 13 16 19]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 157.3455 - loglik: -1.5244e+02 - logprior: -4.9051e+00
Epoch 2/2
17/17 - 1s - loss: 147.2128 - loglik: -1.4554e+02 - logprior: -1.6710e+00
Fitted a model with MAP estimate = -146.8168
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 155.1115 - loglik: -1.5084e+02 - logprior: -4.2756e+00
Epoch 2/10
17/17 - 1s - loss: 146.8325 - loglik: -1.4526e+02 - logprior: -1.5752e+00
Epoch 3/10
17/17 - 1s - loss: 146.5553 - loglik: -1.4521e+02 - logprior: -1.3414e+00
Epoch 4/10
17/17 - 1s - loss: 146.0988 - loglik: -1.4483e+02 - logprior: -1.2688e+00
Epoch 5/10
17/17 - 1s - loss: 146.0388 - loglik: -1.4479e+02 - logprior: -1.2495e+00
Epoch 6/10
17/17 - 1s - loss: 145.3908 - loglik: -1.4417e+02 - logprior: -1.2207e+00
Epoch 7/10
17/17 - 1s - loss: 145.4269 - loglik: -1.4423e+02 - logprior: -1.1943e+00
Fitted a model with MAP estimate = -145.0046
Time for alignment: 34.6455
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 190.6236 - loglik: -1.8619e+02 - logprior: -4.4345e+00
Epoch 2/10
17/17 - 1s - loss: 163.7995 - loglik: -1.6223e+02 - logprior: -1.5675e+00
Epoch 3/10
17/17 - 1s - loss: 153.6432 - loglik: -1.5202e+02 - logprior: -1.6236e+00
Epoch 4/10
17/17 - 1s - loss: 151.2814 - loglik: -1.4965e+02 - logprior: -1.6318e+00
Epoch 5/10
17/17 - 1s - loss: 150.5163 - loglik: -1.4897e+02 - logprior: -1.5419e+00
Epoch 6/10
17/17 - 1s - loss: 150.3760 - loglik: -1.4882e+02 - logprior: -1.5580e+00
Epoch 7/10
17/17 - 1s - loss: 149.8742 - loglik: -1.4834e+02 - logprior: -1.5341e+00
Epoch 8/10
17/17 - 1s - loss: 149.7044 - loglik: -1.4818e+02 - logprior: -1.5210e+00
Epoch 9/10
17/17 - 1s - loss: 149.4895 - loglik: -1.4797e+02 - logprior: -1.5105e+00
Epoch 10/10
17/17 - 1s - loss: 149.3318 - loglik: -1.4782e+02 - logprior: -1.5028e+00
Fitted a model with MAP estimate = -149.1116
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (14, 1), (15, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 163.2130 - loglik: -1.5763e+02 - logprior: -5.5805e+00
Epoch 2/2
17/17 - 1s - loss: 150.1132 - loglik: -1.4735e+02 - logprior: -2.7666e+00
Fitted a model with MAP estimate = -148.1613
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 157.4065 - loglik: -1.5247e+02 - logprior: -4.9356e+00
Epoch 2/2
17/17 - 1s - loss: 147.3971 - loglik: -1.4572e+02 - logprior: -1.6770e+00
Fitted a model with MAP estimate = -146.8399
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 155.0377 - loglik: -1.5076e+02 - logprior: -4.2781e+00
Epoch 2/10
17/17 - 1s - loss: 147.0907 - loglik: -1.4552e+02 - logprior: -1.5744e+00
Epoch 3/10
17/17 - 1s - loss: 146.3738 - loglik: -1.4503e+02 - logprior: -1.3408e+00
Epoch 4/10
17/17 - 1s - loss: 146.0781 - loglik: -1.4480e+02 - logprior: -1.2818e+00
Epoch 5/10
17/17 - 1s - loss: 145.9976 - loglik: -1.4476e+02 - logprior: -1.2394e+00
Epoch 6/10
17/17 - 1s - loss: 145.4886 - loglik: -1.4427e+02 - logprior: -1.2186e+00
Epoch 7/10
17/17 - 1s - loss: 145.2565 - loglik: -1.4404e+02 - logprior: -1.2073e+00
Epoch 8/10
17/17 - 1s - loss: 144.9977 - loglik: -1.4381e+02 - logprior: -1.1803e+00
Epoch 9/10
17/17 - 1s - loss: 144.6663 - loglik: -1.4347e+02 - logprior: -1.1851e+00
Epoch 10/10
17/17 - 1s - loss: 144.3522 - loglik: -1.4318e+02 - logprior: -1.1621e+00
Fitted a model with MAP estimate = -144.1108
Time for alignment: 35.3962
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 190.8183 - loglik: -1.8638e+02 - logprior: -4.4335e+00
Epoch 2/10
17/17 - 1s - loss: 163.5487 - loglik: -1.6200e+02 - logprior: -1.5510e+00
Epoch 3/10
17/17 - 1s - loss: 153.8966 - loglik: -1.5226e+02 - logprior: -1.6315e+00
Epoch 4/10
17/17 - 1s - loss: 152.0673 - loglik: -1.5044e+02 - logprior: -1.6256e+00
Epoch 5/10
17/17 - 1s - loss: 151.4200 - loglik: -1.4989e+02 - logprior: -1.5332e+00
Epoch 6/10
17/17 - 1s - loss: 150.8637 - loglik: -1.4931e+02 - logprior: -1.5474e+00
Epoch 7/10
17/17 - 1s - loss: 150.8463 - loglik: -1.4932e+02 - logprior: -1.5244e+00
Epoch 8/10
17/17 - 1s - loss: 150.4300 - loglik: -1.4891e+02 - logprior: -1.5156e+00
Epoch 9/10
17/17 - 1s - loss: 150.1172 - loglik: -1.4860e+02 - logprior: -1.5059e+00
Epoch 10/10
17/17 - 1s - loss: 150.1212 - loglik: -1.4861e+02 - logprior: -1.5012e+00
Fitted a model with MAP estimate = -149.7697
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 2), (16, 1), (17, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 34 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 164.5030 - loglik: -1.5899e+02 - logprior: -5.5150e+00
Epoch 2/2
17/17 - 1s - loss: 150.5954 - loglik: -1.4806e+02 - logprior: -2.5317e+00
Fitted a model with MAP estimate = -148.1901
expansions: []
discards: [13 16 19]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 155.7402 - loglik: -1.5134e+02 - logprior: -4.3956e+00
Epoch 2/2
17/17 - 1s - loss: 147.1689 - loglik: -1.4557e+02 - logprior: -1.6005e+00
Fitted a model with MAP estimate = -146.7738
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 4s - loss: 154.9644 - loglik: -1.5069e+02 - logprior: -4.2701e+00
Epoch 2/10
17/17 - 1s - loss: 147.0756 - loglik: -1.4551e+02 - logprior: -1.5696e+00
Epoch 3/10
17/17 - 1s - loss: 146.3704 - loglik: -1.4502e+02 - logprior: -1.3489e+00
Epoch 4/10
17/17 - 1s - loss: 146.1937 - loglik: -1.4491e+02 - logprior: -1.2779e+00
Epoch 5/10
17/17 - 1s - loss: 146.0003 - loglik: -1.4476e+02 - logprior: -1.2378e+00
Epoch 6/10
17/17 - 1s - loss: 145.5329 - loglik: -1.4431e+02 - logprior: -1.2209e+00
Epoch 7/10
17/17 - 1s - loss: 145.2397 - loglik: -1.4403e+02 - logprior: -1.2092e+00
Epoch 8/10
17/17 - 1s - loss: 144.9500 - loglik: -1.4377e+02 - logprior: -1.1791e+00
Epoch 9/10
17/17 - 1s - loss: 144.7904 - loglik: -1.4361e+02 - logprior: -1.1772e+00
Epoch 10/10
17/17 - 1s - loss: 144.2832 - loglik: -1.4311e+02 - logprior: -1.1682e+00
Fitted a model with MAP estimate = -144.0787
Time for alignment: 35.5211
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 190.6413 - loglik: -1.8621e+02 - logprior: -4.4336e+00
Epoch 2/10
17/17 - 1s - loss: 163.3493 - loglik: -1.6179e+02 - logprior: -1.5625e+00
Epoch 3/10
17/17 - 1s - loss: 153.0036 - loglik: -1.5139e+02 - logprior: -1.6125e+00
Epoch 4/10
17/17 - 1s - loss: 151.4791 - loglik: -1.4986e+02 - logprior: -1.6231e+00
Epoch 5/10
17/17 - 1s - loss: 151.0890 - loglik: -1.4957e+02 - logprior: -1.5220e+00
Epoch 6/10
17/17 - 1s - loss: 150.9258 - loglik: -1.4939e+02 - logprior: -1.5351e+00
Epoch 7/10
17/17 - 1s - loss: 150.1779 - loglik: -1.4866e+02 - logprior: -1.5172e+00
Epoch 8/10
17/17 - 1s - loss: 150.2345 - loglik: -1.4872e+02 - logprior: -1.5057e+00
Fitted a model with MAP estimate = -149.9601
expansions: [(2, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 163.5309 - loglik: -1.5795e+02 - logprior: -5.5785e+00
Epoch 2/2
17/17 - 1s - loss: 149.8972 - loglik: -1.4721e+02 - logprior: -2.6834e+00
Fitted a model with MAP estimate = -148.0708
expansions: [(2, 1)]
discards: [ 0 13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 157.6337 - loglik: -1.5267e+02 - logprior: -4.9669e+00
Epoch 2/2
17/17 - 1s - loss: 147.2205 - loglik: -1.4555e+02 - logprior: -1.6716e+00
Fitted a model with MAP estimate = -146.8354
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 154.8519 - loglik: -1.5058e+02 - logprior: -4.2754e+00
Epoch 2/10
17/17 - 1s - loss: 147.2648 - loglik: -1.4569e+02 - logprior: -1.5711e+00
Epoch 3/10
17/17 - 1s - loss: 146.2235 - loglik: -1.4488e+02 - logprior: -1.3469e+00
Epoch 4/10
17/17 - 1s - loss: 146.3742 - loglik: -1.4511e+02 - logprior: -1.2675e+00
Fitted a model with MAP estimate = -145.9739
Time for alignment: 30.0483
Fitting a model of length 24 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 6s - loss: 190.7926 - loglik: -1.8636e+02 - logprior: -4.4340e+00
Epoch 2/10
17/17 - 1s - loss: 164.1226 - loglik: -1.6258e+02 - logprior: -1.5446e+00
Epoch 3/10
17/17 - 1s - loss: 154.3106 - loglik: -1.5270e+02 - logprior: -1.6073e+00
Epoch 4/10
17/17 - 1s - loss: 151.9969 - loglik: -1.5037e+02 - logprior: -1.6308e+00
Epoch 5/10
17/17 - 1s - loss: 151.5117 - loglik: -1.4998e+02 - logprior: -1.5259e+00
Epoch 6/10
17/17 - 1s - loss: 150.8103 - loglik: -1.4927e+02 - logprior: -1.5419e+00
Epoch 7/10
17/17 - 1s - loss: 151.0581 - loglik: -1.4953e+02 - logprior: -1.5228e+00
Fitted a model with MAP estimate = -150.4900
expansions: [(7, 1), (10, 1), (11, 2), (12, 3), (13, 1), (16, 1), (18, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 33 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 163.6074 - loglik: -1.5810e+02 - logprior: -5.5066e+00
Epoch 2/2
17/17 - 1s - loss: 150.0723 - loglik: -1.4763e+02 - logprior: -2.4380e+00
Fitted a model with MAP estimate = -147.8391
expansions: []
discards: [13 16]
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
17/17 - 4s - loss: 155.6080 - loglik: -1.5124e+02 - logprior: -4.3711e+00
Epoch 2/2
17/17 - 1s - loss: 147.3387 - loglik: -1.4574e+02 - logprior: -1.5997e+00
Fitted a model with MAP estimate = -146.7872
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 31 on 7774 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
17/17 - 3s - loss: 154.9594 - loglik: -1.5069e+02 - logprior: -4.2651e+00
Epoch 2/10
17/17 - 1s - loss: 147.1083 - loglik: -1.4553e+02 - logprior: -1.5828e+00
Epoch 3/10
17/17 - 1s - loss: 146.4518 - loglik: -1.4511e+02 - logprior: -1.3420e+00
Epoch 4/10
17/17 - 1s - loss: 146.2340 - loglik: -1.4495e+02 - logprior: -1.2878e+00
Epoch 5/10
17/17 - 1s - loss: 145.6718 - loglik: -1.4444e+02 - logprior: -1.2339e+00
Epoch 6/10
17/17 - 1s - loss: 145.6447 - loglik: -1.4441e+02 - logprior: -1.2321e+00
Epoch 7/10
17/17 - 1s - loss: 145.2950 - loglik: -1.4408e+02 - logprior: -1.2070e+00
Epoch 8/10
17/17 - 1s - loss: 144.9562 - loglik: -1.4376e+02 - logprior: -1.1907e+00
Epoch 9/10
17/17 - 1s - loss: 144.5823 - loglik: -1.4339e+02 - logprior: -1.1822e+00
Epoch 10/10
17/17 - 1s - loss: 144.4564 - loglik: -1.4328e+02 - logprior: -1.1650e+00
Fitted a model with MAP estimate = -144.0261
Time for alignment: 33.1925
Computed alignments with likelihoods: ['-145.0046', '-144.1108', '-144.0787', '-145.9739', '-144.0261']
Best model has likelihood: -144.0261  (prior= -1.1856 )
time for generating output: 0.1106
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/egf.projection.fasta
SP score = 0.8228483211115399
Training of 5 independent models on file HMG_box.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ab796ca0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9383c35b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9cdcb5880>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 382.9578 - loglik: -3.7486e+02 - logprior: -8.1004e+00
Epoch 2/10
13/13 - 1s - loss: 352.5007 - loglik: -3.5022e+02 - logprior: -2.2790e+00
Epoch 3/10
13/13 - 1s - loss: 335.5641 - loglik: -3.3358e+02 - logprior: -1.9852e+00
Epoch 4/10
13/13 - 1s - loss: 327.5104 - loglik: -3.2578e+02 - logprior: -1.7283e+00
Epoch 5/10
13/13 - 1s - loss: 325.0962 - loglik: -3.2344e+02 - logprior: -1.6584e+00
Epoch 6/10
13/13 - 1s - loss: 324.1738 - loglik: -3.2260e+02 - logprior: -1.5707e+00
Epoch 7/10
13/13 - 1s - loss: 321.9092 - loglik: -3.2035e+02 - logprior: -1.5589e+00
Epoch 8/10
13/13 - 1s - loss: 321.1570 - loglik: -3.1960e+02 - logprior: -1.5488e+00
Epoch 9/10
13/13 - 1s - loss: 319.6176 - loglik: -3.1807e+02 - logprior: -1.5422e+00
Epoch 10/10
13/13 - 1s - loss: 318.1247 - loglik: -3.1656e+02 - logprior: -1.5584e+00
Fitted a model with MAP estimate = -317.4787
expansions: [(0, 2), (16, 5), (17, 1), (32, 1), (38, 1), (41, 1), (45, 2), (46, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 7s - loss: 334.2153 - loglik: -3.2395e+02 - logprior: -1.0266e+01
Epoch 2/2
13/13 - 1s - loss: 315.0723 - loglik: -3.1219e+02 - logprior: -2.8797e+00
Fitted a model with MAP estimate = -313.0387
expansions: []
discards: [ 0 56]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 323.7000 - loglik: -3.1412e+02 - logprior: -9.5788e+00
Epoch 2/2
13/13 - 1s - loss: 314.7520 - loglik: -3.1109e+02 - logprior: -3.6584e+00
Fitted a model with MAP estimate = -312.8551
expansions: [(13, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 319.9484 - loglik: -3.1227e+02 - logprior: -7.6810e+00
Epoch 2/10
13/13 - 1s - loss: 312.3956 - loglik: -3.1006e+02 - logprior: -2.3364e+00
Epoch 3/10
13/13 - 1s - loss: 310.1380 - loglik: -3.0857e+02 - logprior: -1.5723e+00
Epoch 4/10
13/13 - 1s - loss: 309.9672 - loglik: -3.0869e+02 - logprior: -1.2729e+00
Epoch 5/10
13/13 - 1s - loss: 309.6193 - loglik: -3.0847e+02 - logprior: -1.1511e+00
Epoch 6/10
13/13 - 1s - loss: 308.9525 - loglik: -3.0785e+02 - logprior: -1.1030e+00
Epoch 7/10
13/13 - 1s - loss: 307.4543 - loglik: -3.0635e+02 - logprior: -1.0988e+00
Epoch 8/10
13/13 - 1s - loss: 306.6624 - loglik: -3.0556e+02 - logprior: -1.0998e+00
Epoch 9/10
13/13 - 1s - loss: 305.2113 - loglik: -3.0410e+02 - logprior: -1.1051e+00
Epoch 10/10
13/13 - 1s - loss: 303.6362 - loglik: -3.0250e+02 - logprior: -1.1287e+00
Fitted a model with MAP estimate = -302.5708
Time for alignment: 38.8950
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 382.7030 - loglik: -3.7460e+02 - logprior: -8.1022e+00
Epoch 2/10
13/13 - 1s - loss: 352.1344 - loglik: -3.4987e+02 - logprior: -2.2631e+00
Epoch 3/10
13/13 - 1s - loss: 334.5493 - loglik: -3.3261e+02 - logprior: -1.9415e+00
Epoch 4/10
13/13 - 1s - loss: 329.2127 - loglik: -3.2720e+02 - logprior: -2.0088e+00
Epoch 5/10
13/13 - 1s - loss: 325.5635 - loglik: -3.2369e+02 - logprior: -1.8673e+00
Epoch 6/10
13/13 - 1s - loss: 324.2920 - loglik: -3.2251e+02 - logprior: -1.7825e+00
Epoch 7/10
13/13 - 1s - loss: 322.5668 - loglik: -3.2077e+02 - logprior: -1.7934e+00
Epoch 8/10
13/13 - 1s - loss: 321.7919 - loglik: -3.2001e+02 - logprior: -1.7779e+00
Epoch 9/10
13/13 - 1s - loss: 320.5247 - loglik: -3.1875e+02 - logprior: -1.7701e+00
Epoch 10/10
13/13 - 1s - loss: 319.1963 - loglik: -3.1740e+02 - logprior: -1.7838e+00
Fitted a model with MAP estimate = -318.4472
expansions: [(12, 1), (17, 5), (18, 1), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 334.4316 - loglik: -3.2490e+02 - logprior: -9.5338e+00
Epoch 2/2
13/13 - 1s - loss: 318.3779 - loglik: -3.1402e+02 - logprior: -4.3601e+00
Fitted a model with MAP estimate = -316.3599
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 319.9871 - loglik: -3.1270e+02 - logprior: -7.2901e+00
Epoch 2/2
13/13 - 1s - loss: 313.3387 - loglik: -3.1116e+02 - logprior: -2.1781e+00
Fitted a model with MAP estimate = -312.1429
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 323.5922 - loglik: -3.1422e+02 - logprior: -9.3737e+00
Epoch 2/10
13/13 - 1s - loss: 315.0322 - loglik: -3.1165e+02 - logprior: -3.3856e+00
Epoch 3/10
13/13 - 1s - loss: 312.6802 - loglik: -3.1100e+02 - logprior: -1.6828e+00
Epoch 4/10
13/13 - 1s - loss: 311.0963 - loglik: -3.0983e+02 - logprior: -1.2643e+00
Epoch 5/10
13/13 - 1s - loss: 311.8906 - loglik: -3.1073e+02 - logprior: -1.1596e+00
Fitted a model with MAP estimate = -310.5988
Time for alignment: 34.7950
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 382.6762 - loglik: -3.7458e+02 - logprior: -8.0996e+00
Epoch 2/10
13/13 - 1s - loss: 352.8483 - loglik: -3.5059e+02 - logprior: -2.2624e+00
Epoch 3/10
13/13 - 1s - loss: 335.0107 - loglik: -3.3309e+02 - logprior: -1.9192e+00
Epoch 4/10
13/13 - 1s - loss: 328.9487 - loglik: -3.2695e+02 - logprior: -2.0002e+00
Epoch 5/10
13/13 - 1s - loss: 325.8652 - loglik: -3.2398e+02 - logprior: -1.8860e+00
Epoch 6/10
13/13 - 1s - loss: 323.7940 - loglik: -3.2201e+02 - logprior: -1.7809e+00
Epoch 7/10
13/13 - 1s - loss: 322.8054 - loglik: -3.2101e+02 - logprior: -1.7868e+00
Epoch 8/10
13/13 - 1s - loss: 322.1535 - loglik: -3.2036e+02 - logprior: -1.7876e+00
Epoch 9/10
13/13 - 1s - loss: 320.1615 - loglik: -3.1838e+02 - logprior: -1.7765e+00
Epoch 10/10
13/13 - 1s - loss: 319.5411 - loglik: -3.1774e+02 - logprior: -1.7904e+00
Fitted a model with MAP estimate = -318.4478
expansions: [(12, 1), (17, 5), (18, 1), (29, 1), (34, 1), (41, 1), (44, 1), (45, 1), (46, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 334.5894 - loglik: -3.2507e+02 - logprior: -9.5212e+00
Epoch 2/2
13/13 - 1s - loss: 317.9013 - loglik: -3.1354e+02 - logprior: -4.3612e+00
Fitted a model with MAP estimate = -316.3527
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 320.2246 - loglik: -3.1294e+02 - logprior: -7.2827e+00
Epoch 2/2
13/13 - 1s - loss: 313.0491 - loglik: -3.1086e+02 - logprior: -2.1894e+00
Fitted a model with MAP estimate = -312.1136
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 323.4987 - loglik: -3.1415e+02 - logprior: -9.3484e+00
Epoch 2/10
13/13 - 1s - loss: 314.7081 - loglik: -3.1143e+02 - logprior: -3.2781e+00
Epoch 3/10
13/13 - 1s - loss: 312.1382 - loglik: -3.1048e+02 - logprior: -1.6534e+00
Epoch 4/10
13/13 - 1s - loss: 311.8493 - loglik: -3.1059e+02 - logprior: -1.2585e+00
Epoch 5/10
13/13 - 1s - loss: 310.5270 - loglik: -3.0937e+02 - logprior: -1.1586e+00
Epoch 6/10
13/13 - 1s - loss: 309.6800 - loglik: -3.0850e+02 - logprior: -1.1728e+00
Epoch 7/10
13/13 - 1s - loss: 308.6656 - loglik: -3.0747e+02 - logprior: -1.1938e+00
Epoch 8/10
13/13 - 1s - loss: 307.6314 - loglik: -3.0645e+02 - logprior: -1.1765e+00
Epoch 9/10
13/13 - 1s - loss: 305.9366 - loglik: -3.0475e+02 - logprior: -1.1785e+00
Epoch 10/10
13/13 - 1s - loss: 304.7214 - loglik: -3.0352e+02 - logprior: -1.1896e+00
Fitted a model with MAP estimate = -303.6560
Time for alignment: 38.1872
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 6s - loss: 382.8055 - loglik: -3.7471e+02 - logprior: -8.0991e+00
Epoch 2/10
13/13 - 1s - loss: 351.9671 - loglik: -3.4971e+02 - logprior: -2.2565e+00
Epoch 3/10
13/13 - 1s - loss: 335.1473 - loglik: -3.3320e+02 - logprior: -1.9447e+00
Epoch 4/10
13/13 - 1s - loss: 328.6462 - loglik: -3.2660e+02 - logprior: -2.0432e+00
Epoch 5/10
13/13 - 1s - loss: 325.5901 - loglik: -3.2366e+02 - logprior: -1.9333e+00
Epoch 6/10
13/13 - 1s - loss: 324.3244 - loglik: -3.2248e+02 - logprior: -1.8464e+00
Epoch 7/10
13/13 - 1s - loss: 322.2410 - loglik: -3.2036e+02 - logprior: -1.8776e+00
Epoch 8/10
13/13 - 1s - loss: 320.9572 - loglik: -3.1906e+02 - logprior: -1.8938e+00
Epoch 9/10
13/13 - 1s - loss: 319.3651 - loglik: -3.1747e+02 - logprior: -1.8863e+00
Epoch 10/10
13/13 - 1s - loss: 318.3651 - loglik: -3.1646e+02 - logprior: -1.8965e+00
Fitted a model with MAP estimate = -317.4186
expansions: [(12, 1), (17, 5), (18, 1), (29, 1), (34, 1), (38, 1), (41, 1), (45, 1), (46, 1), (48, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 334.7190 - loglik: -3.2519e+02 - logprior: -9.5313e+00
Epoch 2/2
13/13 - 1s - loss: 318.9526 - loglik: -3.1459e+02 - logprior: -4.3657e+00
Fitted a model with MAP estimate = -316.3821
expansions: [(0, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 320.1903 - loglik: -3.1291e+02 - logprior: -7.2793e+00
Epoch 2/2
13/13 - 1s - loss: 313.0243 - loglik: -3.1084e+02 - logprior: -2.1848e+00
Fitted a model with MAP estimate = -312.1400
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 68 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 323.6810 - loglik: -3.1432e+02 - logprior: -9.3576e+00
Epoch 2/10
13/13 - 1s - loss: 314.8070 - loglik: -3.1153e+02 - logprior: -3.2778e+00
Epoch 3/10
13/13 - 1s - loss: 312.2648 - loglik: -3.1061e+02 - logprior: -1.6567e+00
Epoch 4/10
13/13 - 1s - loss: 311.4995 - loglik: -3.1023e+02 - logprior: -1.2644e+00
Epoch 5/10
13/13 - 1s - loss: 310.7610 - loglik: -3.0960e+02 - logprior: -1.1570e+00
Epoch 6/10
13/13 - 1s - loss: 309.4530 - loglik: -3.0827e+02 - logprior: -1.1759e+00
Epoch 7/10
13/13 - 1s - loss: 308.9557 - loglik: -3.0775e+02 - logprior: -1.1996e+00
Epoch 8/10
13/13 - 1s - loss: 307.2035 - loglik: -3.0602e+02 - logprior: -1.1790e+00
Epoch 9/10
13/13 - 1s - loss: 306.3053 - loglik: -3.0512e+02 - logprior: -1.1784e+00
Epoch 10/10
13/13 - 1s - loss: 304.3677 - loglik: -3.0317e+02 - logprior: -1.1930e+00
Fitted a model with MAP estimate = -303.6852
Time for alignment: 38.0725
Fitting a model of length 55 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 3s - loss: 383.0424 - loglik: -3.7494e+02 - logprior: -8.1025e+00
Epoch 2/10
13/13 - 1s - loss: 353.3040 - loglik: -3.5102e+02 - logprior: -2.2810e+00
Epoch 3/10
13/13 - 1s - loss: 337.8569 - loglik: -3.3595e+02 - logprior: -1.9079e+00
Epoch 4/10
13/13 - 1s - loss: 330.1093 - loglik: -3.2811e+02 - logprior: -1.9958e+00
Epoch 5/10
13/13 - 1s - loss: 326.5856 - loglik: -3.2463e+02 - logprior: -1.9509e+00
Epoch 6/10
13/13 - 1s - loss: 324.3172 - loglik: -3.2247e+02 - logprior: -1.8469e+00
Epoch 7/10
13/13 - 1s - loss: 322.7850 - loglik: -3.2092e+02 - logprior: -1.8663e+00
Epoch 8/10
13/13 - 1s - loss: 320.8760 - loglik: -3.1896e+02 - logprior: -1.9072e+00
Epoch 9/10
13/13 - 1s - loss: 318.9396 - loglik: -3.1703e+02 - logprior: -1.9039e+00
Epoch 10/10
13/13 - 1s - loss: 317.9925 - loglik: -3.1608e+02 - logprior: -1.9080e+00
Fitted a model with MAP estimate = -316.9570
expansions: [(12, 1), (13, 1), (17, 5), (18, 1), (19, 2), (34, 1), (38, 1), (41, 1), (45, 1), (46, 1), (48, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 6s - loss: 334.6979 - loglik: -3.2517e+02 - logprior: -9.5233e+00
Epoch 2/2
13/13 - 1s - loss: 316.5179 - loglik: -3.1215e+02 - logprior: -4.3675e+00
Fitted a model with MAP estimate = -314.7394
expansions: [(0, 2)]
discards: [ 0 26]
Re-initialized the encoder parameters.
Fitting a model of length 70 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
13/13 - 4s - loss: 319.1055 - loglik: -3.1187e+02 - logprior: -7.2349e+00
Epoch 2/2
13/13 - 1s - loss: 310.9879 - loglik: -3.0885e+02 - logprior: -2.1425e+00
Fitted a model with MAP estimate = -310.4195
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 69 on 4779 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
13/13 - 4s - loss: 321.9904 - loglik: -3.1270e+02 - logprior: -9.2884e+00
Epoch 2/10
13/13 - 1s - loss: 312.9857 - loglik: -3.0974e+02 - logprior: -3.2425e+00
Epoch 3/10
13/13 - 1s - loss: 310.4285 - loglik: -3.0883e+02 - logprior: -1.6006e+00
Epoch 4/10
13/13 - 1s - loss: 310.0921 - loglik: -3.0889e+02 - logprior: -1.2049e+00
Epoch 5/10
13/13 - 1s - loss: 309.6324 - loglik: -3.0853e+02 - logprior: -1.1018e+00
Epoch 6/10
13/13 - 1s - loss: 308.8784 - loglik: -3.0779e+02 - logprior: -1.0900e+00
Epoch 7/10
13/13 - 1s - loss: 307.7524 - loglik: -3.0663e+02 - logprior: -1.1236e+00
Epoch 8/10
13/13 - 1s - loss: 306.3943 - loglik: -3.0527e+02 - logprior: -1.1175e+00
Epoch 9/10
13/13 - 1s - loss: 305.3958 - loglik: -3.0427e+02 - logprior: -1.1159e+00
Epoch 10/10
13/13 - 1s - loss: 303.6844 - loglik: -3.0255e+02 - logprior: -1.1298e+00
Fitted a model with MAP estimate = -302.5399
Time for alignment: 37.8445
Computed alignments with likelihoods: ['-302.5708', '-310.5988', '-303.6560', '-303.6852', '-302.5399']
Best model has likelihood: -302.5399  (prior= -1.1263 )
time for generating output: 0.1279
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HMG_box.projection.fasta
SP score = 0.9730878186968839
Training of 5 independent models on file hpr.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8c8e52070>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea5e6c6b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9d6715940>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 473.2295 - loglik: -4.6069e+02 - logprior: -1.2543e+01
Epoch 2/10
11/11 - 1s - loss: 439.5297 - loglik: -4.3625e+02 - logprior: -3.2801e+00
Epoch 3/10
11/11 - 1s - loss: 413.7924 - loglik: -4.1172e+02 - logprior: -2.0680e+00
Epoch 4/10
11/11 - 1s - loss: 398.8466 - loglik: -3.9727e+02 - logprior: -1.5735e+00
Epoch 5/10
11/11 - 1s - loss: 393.0099 - loglik: -3.9168e+02 - logprior: -1.3282e+00
Epoch 6/10
11/11 - 1s - loss: 391.0460 - loglik: -3.8982e+02 - logprior: -1.2260e+00
Epoch 7/10
11/11 - 1s - loss: 388.3711 - loglik: -3.8735e+02 - logprior: -1.0183e+00
Epoch 8/10
11/11 - 1s - loss: 387.8675 - loglik: -3.8703e+02 - logprior: -8.3726e-01
Epoch 9/10
11/11 - 1s - loss: 386.4964 - loglik: -3.8575e+02 - logprior: -7.4213e-01
Epoch 10/10
11/11 - 1s - loss: 385.8893 - loglik: -3.8520e+02 - logprior: -6.8575e-01
Fitted a model with MAP estimate = -385.4666
expansions: [(0, 6), (21, 1), (23, 2), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 87 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 7s - loss: 405.9139 - loglik: -3.9039e+02 - logprior: -1.5527e+01
Epoch 2/2
11/11 - 1s - loss: 383.6423 - loglik: -3.7925e+02 - logprior: -4.3879e+00
Fitted a model with MAP estimate = -380.1939
expansions: []
discards: [ 0 30 38]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 395.3889 - loglik: -3.8130e+02 - logprior: -1.4086e+01
Epoch 2/2
11/11 - 1s - loss: 383.7260 - loglik: -3.7821e+02 - logprior: -5.5128e+00
Fitted a model with MAP estimate = -381.4068
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 391.0244 - loglik: -3.7876e+02 - logprior: -1.2268e+01
Epoch 2/10
11/11 - 1s - loss: 380.4272 - loglik: -3.7722e+02 - logprior: -3.2046e+00
Epoch 3/10
11/11 - 1s - loss: 377.7790 - loglik: -3.7612e+02 - logprior: -1.6590e+00
Epoch 4/10
11/11 - 1s - loss: 377.2882 - loglik: -3.7632e+02 - logprior: -9.6769e-01
Epoch 5/10
11/11 - 1s - loss: 376.0923 - loglik: -3.7536e+02 - logprior: -7.3156e-01
Epoch 6/10
11/11 - 1s - loss: 375.6282 - loglik: -3.7507e+02 - logprior: -5.5854e-01
Epoch 7/10
11/11 - 1s - loss: 374.4016 - loglik: -3.7393e+02 - logprior: -4.6907e-01
Epoch 8/10
11/11 - 1s - loss: 373.7103 - loglik: -3.7328e+02 - logprior: -4.2267e-01
Epoch 9/10
11/11 - 1s - loss: 372.8865 - loglik: -3.7248e+02 - logprior: -4.0801e-01
Epoch 10/10
11/11 - 1s - loss: 371.6844 - loglik: -3.7126e+02 - logprior: -4.2513e-01
Fitted a model with MAP estimate = -371.4049
Time for alignment: 38.2383
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 473.3563 - loglik: -4.6081e+02 - logprior: -1.2544e+01
Epoch 2/10
11/11 - 1s - loss: 439.1731 - loglik: -4.3589e+02 - logprior: -3.2847e+00
Epoch 3/10
11/11 - 1s - loss: 414.9284 - loglik: -4.1289e+02 - logprior: -2.0403e+00
Epoch 4/10
11/11 - 1s - loss: 400.1673 - loglik: -3.9862e+02 - logprior: -1.5434e+00
Epoch 5/10
11/11 - 1s - loss: 393.9224 - loglik: -3.9260e+02 - logprior: -1.3239e+00
Epoch 6/10
11/11 - 1s - loss: 391.1317 - loglik: -3.8990e+02 - logprior: -1.2274e+00
Epoch 7/10
11/11 - 1s - loss: 389.3519 - loglik: -3.8834e+02 - logprior: -1.0096e+00
Epoch 8/10
11/11 - 1s - loss: 387.7354 - loglik: -3.8689e+02 - logprior: -8.3950e-01
Epoch 9/10
11/11 - 1s - loss: 387.0714 - loglik: -3.8632e+02 - logprior: -7.5172e-01
Epoch 10/10
11/11 - 1s - loss: 386.1332 - loglik: -3.8543e+02 - logprior: -6.9989e-01
Fitted a model with MAP estimate = -385.9523
expansions: [(0, 6), (21, 1), (27, 1), (28, 1), (46, 1), (47, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 85 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 404.8362 - loglik: -3.8934e+02 - logprior: -1.5496e+01
Epoch 2/2
11/11 - 1s - loss: 383.0753 - loglik: -3.7880e+02 - logprior: -4.2762e+00
Fitted a model with MAP estimate = -379.9924
expansions: []
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 394.8837 - loglik: -3.8083e+02 - logprior: -1.4057e+01
Epoch 2/2
11/11 - 1s - loss: 383.2401 - loglik: -3.7774e+02 - logprior: -5.4998e+00
Fitted a model with MAP estimate = -381.0734
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 390.6092 - loglik: -3.7836e+02 - logprior: -1.2253e+01
Epoch 2/10
11/11 - 1s - loss: 380.0306 - loglik: -3.7683e+02 - logprior: -3.1984e+00
Epoch 3/10
11/11 - 1s - loss: 378.3687 - loglik: -3.7670e+02 - logprior: -1.6727e+00
Epoch 4/10
11/11 - 1s - loss: 376.7254 - loglik: -3.7576e+02 - logprior: -9.6549e-01
Epoch 5/10
11/11 - 1s - loss: 376.1924 - loglik: -3.7545e+02 - logprior: -7.4261e-01
Epoch 6/10
11/11 - 1s - loss: 375.8315 - loglik: -3.7527e+02 - logprior: -5.5737e-01
Epoch 7/10
11/11 - 1s - loss: 374.1598 - loglik: -3.7369e+02 - logprior: -4.7001e-01
Epoch 8/10
11/11 - 1s - loss: 374.0288 - loglik: -3.7360e+02 - logprior: -4.2157e-01
Epoch 9/10
11/11 - 1s - loss: 372.8951 - loglik: -3.7248e+02 - logprior: -4.1522e-01
Epoch 10/10
11/11 - 1s - loss: 371.8020 - loglik: -3.7137e+02 - logprior: -4.2448e-01
Fitted a model with MAP estimate = -371.4135
Time for alignment: 37.3119
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 473.2887 - loglik: -4.6074e+02 - logprior: -1.2545e+01
Epoch 2/10
11/11 - 1s - loss: 439.5061 - loglik: -4.3622e+02 - logprior: -3.2818e+00
Epoch 3/10
11/11 - 1s - loss: 412.4706 - loglik: -4.1044e+02 - logprior: -2.0350e+00
Epoch 4/10
11/11 - 1s - loss: 398.5755 - loglik: -3.9707e+02 - logprior: -1.5019e+00
Epoch 5/10
11/11 - 1s - loss: 393.0526 - loglik: -3.9179e+02 - logprior: -1.2647e+00
Epoch 6/10
11/11 - 1s - loss: 390.7596 - loglik: -3.8958e+02 - logprior: -1.1809e+00
Epoch 7/10
11/11 - 1s - loss: 388.7061 - loglik: -3.8770e+02 - logprior: -1.0015e+00
Epoch 8/10
11/11 - 1s - loss: 386.9557 - loglik: -3.8610e+02 - logprior: -8.5072e-01
Epoch 9/10
11/11 - 1s - loss: 386.3197 - loglik: -3.8555e+02 - logprior: -7.6597e-01
Epoch 10/10
11/11 - 1s - loss: 385.6443 - loglik: -3.8494e+02 - logprior: -7.0299e-01
Fitted a model with MAP estimate = -385.2495
expansions: [(0, 6), (21, 1), (27, 1), (28, 1), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 405.2419 - loglik: -3.8974e+02 - logprior: -1.5503e+01
Epoch 2/2
11/11 - 1s - loss: 383.5896 - loglik: -3.7925e+02 - logprior: -4.3400e+00
Fitted a model with MAP estimate = -380.1206
expansions: []
discards: [ 0 41]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 4s - loss: 394.5735 - loglik: -3.8050e+02 - logprior: -1.4071e+01
Epoch 2/2
11/11 - 1s - loss: 383.8344 - loglik: -3.7834e+02 - logprior: -5.4983e+00
Fitted a model with MAP estimate = -381.1027
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 390.8279 - loglik: -3.7857e+02 - logprior: -1.2254e+01
Epoch 2/10
11/11 - 1s - loss: 380.3250 - loglik: -3.7713e+02 - logprior: -3.1962e+00
Epoch 3/10
11/11 - 1s - loss: 377.9607 - loglik: -3.7631e+02 - logprior: -1.6548e+00
Epoch 4/10
11/11 - 1s - loss: 376.7346 - loglik: -3.7577e+02 - logprior: -9.6366e-01
Epoch 5/10
11/11 - 1s - loss: 376.4823 - loglik: -3.7575e+02 - logprior: -7.3495e-01
Epoch 6/10
11/11 - 1s - loss: 375.2719 - loglik: -3.7472e+02 - logprior: -5.4774e-01
Epoch 7/10
11/11 - 1s - loss: 374.5678 - loglik: -3.7410e+02 - logprior: -4.6460e-01
Epoch 8/10
11/11 - 1s - loss: 373.5599 - loglik: -3.7314e+02 - logprior: -4.1849e-01
Epoch 9/10
11/11 - 1s - loss: 372.9106 - loglik: -3.7250e+02 - logprior: -4.0561e-01
Epoch 10/10
11/11 - 1s - loss: 371.5662 - loglik: -3.7114e+02 - logprior: -4.2684e-01
Fitted a model with MAP estimate = -371.3982
Time for alignment: 37.3485
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 473.3263 - loglik: -4.6078e+02 - logprior: -1.2544e+01
Epoch 2/10
11/11 - 1s - loss: 438.7920 - loglik: -4.3551e+02 - logprior: -3.2832e+00
Epoch 3/10
11/11 - 1s - loss: 411.9074 - loglik: -4.0986e+02 - logprior: -2.0440e+00
Epoch 4/10
11/11 - 1s - loss: 397.7889 - loglik: -3.9627e+02 - logprior: -1.5214e+00
Epoch 5/10
11/11 - 1s - loss: 392.9399 - loglik: -3.9166e+02 - logprior: -1.2756e+00
Epoch 6/10
11/11 - 1s - loss: 390.0262 - loglik: -3.8888e+02 - logprior: -1.1414e+00
Epoch 7/10
11/11 - 1s - loss: 388.2049 - loglik: -3.8728e+02 - logprior: -9.2016e-01
Epoch 8/10
11/11 - 1s - loss: 387.8527 - loglik: -3.8707e+02 - logprior: -7.7693e-01
Epoch 9/10
11/11 - 1s - loss: 386.2968 - loglik: -3.8559e+02 - logprior: -7.0318e-01
Epoch 10/10
11/11 - 1s - loss: 385.7879 - loglik: -3.8512e+02 - logprior: -6.6025e-01
Fitted a model with MAP estimate = -385.4643
expansions: [(0, 6), (21, 1), (23, 2), (29, 2), (32, 2), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 88 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 406.5709 - loglik: -3.9103e+02 - logprior: -1.5541e+01
Epoch 2/2
11/11 - 1s - loss: 384.1538 - loglik: -3.7971e+02 - logprior: -4.4436e+00
Fitted a model with MAP estimate = -380.5664
expansions: []
discards: [ 0 30 38 43]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 395.5627 - loglik: -3.8146e+02 - logprior: -1.4099e+01
Epoch 2/2
11/11 - 1s - loss: 383.7171 - loglik: -3.7820e+02 - logprior: -5.5122e+00
Fitted a model with MAP estimate = -381.4172
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 390.8615 - loglik: -3.7859e+02 - logprior: -1.2274e+01
Epoch 2/10
11/11 - 1s - loss: 380.6385 - loglik: -3.7743e+02 - logprior: -3.2038e+00
Epoch 3/10
11/11 - 1s - loss: 377.6010 - loglik: -3.7594e+02 - logprior: -1.6626e+00
Epoch 4/10
11/11 - 1s - loss: 377.1949 - loglik: -3.7624e+02 - logprior: -9.5702e-01
Epoch 5/10
11/11 - 1s - loss: 376.2605 - loglik: -3.7552e+02 - logprior: -7.4081e-01
Epoch 6/10
11/11 - 1s - loss: 375.4790 - loglik: -3.7493e+02 - logprior: -5.5118e-01
Epoch 7/10
11/11 - 1s - loss: 374.6565 - loglik: -3.7418e+02 - logprior: -4.7422e-01
Epoch 8/10
11/11 - 1s - loss: 373.8065 - loglik: -3.7338e+02 - logprior: -4.2235e-01
Epoch 9/10
11/11 - 1s - loss: 372.5688 - loglik: -3.7216e+02 - logprior: -4.0931e-01
Epoch 10/10
11/11 - 1s - loss: 372.2242 - loglik: -3.7179e+02 - logprior: -4.2859e-01
Fitted a model with MAP estimate = -371.4351
Time for alignment: 36.8556
Fitting a model of length 67 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 3s - loss: 473.2355 - loglik: -4.6069e+02 - logprior: -1.2544e+01
Epoch 2/10
11/11 - 1s - loss: 438.7174 - loglik: -4.3543e+02 - logprior: -3.2890e+00
Epoch 3/10
11/11 - 1s - loss: 411.7360 - loglik: -4.0965e+02 - logprior: -2.0835e+00
Epoch 4/10
11/11 - 1s - loss: 398.1294 - loglik: -3.9654e+02 - logprior: -1.5851e+00
Epoch 5/10
11/11 - 1s - loss: 392.3220 - loglik: -3.9099e+02 - logprior: -1.3358e+00
Epoch 6/10
11/11 - 1s - loss: 390.0220 - loglik: -3.8884e+02 - logprior: -1.1813e+00
Epoch 7/10
11/11 - 1s - loss: 388.5308 - loglik: -3.8758e+02 - logprior: -9.4695e-01
Epoch 8/10
11/11 - 1s - loss: 387.3511 - loglik: -3.8656e+02 - logprior: -7.9077e-01
Epoch 9/10
11/11 - 1s - loss: 386.4243 - loglik: -3.8571e+02 - logprior: -7.1349e-01
Epoch 10/10
11/11 - 1s - loss: 385.9868 - loglik: -3.8532e+02 - logprior: -6.6609e-01
Fitted a model with MAP estimate = -385.4587
expansions: [(0, 6), (22, 1), (27, 1), (29, 2), (32, 1), (46, 1), (48, 1), (50, 1), (51, 1), (67, 4)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 86 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 3s - loss: 404.4620 - loglik: -3.8895e+02 - logprior: -1.5509e+01
Epoch 2/2
11/11 - 1s - loss: 383.4908 - loglik: -3.7917e+02 - logprior: -4.3241e+00
Fitted a model with MAP estimate = -379.8859
expansions: []
discards: [ 0 37]
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
11/11 - 6s - loss: 394.9686 - loglik: -3.8090e+02 - logprior: -1.4066e+01
Epoch 2/2
11/11 - 1s - loss: 384.1122 - loglik: -3.7861e+02 - logprior: -5.5070e+00
Fitted a model with MAP estimate = -381.3564
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 84 on 3349 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
11/11 - 4s - loss: 390.4825 - loglik: -3.7823e+02 - logprior: -1.2253e+01
Epoch 2/10
11/11 - 1s - loss: 380.9373 - loglik: -3.7774e+02 - logprior: -3.2012e+00
Epoch 3/10
11/11 - 1s - loss: 378.0555 - loglik: -3.7639e+02 - logprior: -1.6656e+00
Epoch 4/10
11/11 - 1s - loss: 376.9324 - loglik: -3.7597e+02 - logprior: -9.6317e-01
Epoch 5/10
11/11 - 1s - loss: 376.3500 - loglik: -3.7561e+02 - logprior: -7.4302e-01
Epoch 6/10
11/11 - 1s - loss: 375.4572 - loglik: -3.7490e+02 - logprior: -5.5097e-01
Epoch 7/10
11/11 - 1s - loss: 374.8828 - loglik: -3.7441e+02 - logprior: -4.7375e-01
Epoch 8/10
11/11 - 1s - loss: 373.2817 - loglik: -3.7285e+02 - logprior: -4.2771e-01
Epoch 9/10
11/11 - 1s - loss: 372.8986 - loglik: -3.7248e+02 - logprior: -4.1658e-01
Epoch 10/10
11/11 - 1s - loss: 371.6668 - loglik: -3.7123e+02 - logprior: -4.2847e-01
Fitted a model with MAP estimate = -371.4300
Time for alignment: 36.2931
Computed alignments with likelihoods: ['-371.4049', '-371.4135', '-371.3982', '-371.4351', '-371.4300']
Best model has likelihood: -371.3982  (prior= -0.3917 )
time for generating output: 0.1140
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/hpr.projection.fasta
SP score = 0.993006993006993
Training of 5 independent models on file tim.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea9c942520>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bbf1610>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3c3d5f10>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 8s - loss: 1108.7682 - loglik: -1.1034e+03 - logprior: -5.3218e+00
Epoch 2/10
24/24 - 6s - loss: 955.1714 - loglik: -9.5302e+02 - logprior: -2.1535e+00
Epoch 3/10
24/24 - 6s - loss: 927.0446 - loglik: -9.2448e+02 - logprior: -2.5608e+00
Epoch 4/10
24/24 - 6s - loss: 924.5040 - loglik: -9.2210e+02 - logprior: -2.4058e+00
Epoch 5/10
24/24 - 6s - loss: 921.2739 - loglik: -9.1890e+02 - logprior: -2.3717e+00
Epoch 6/10
24/24 - 6s - loss: 920.8311 - loglik: -9.1846e+02 - logprior: -2.3646e+00
Epoch 7/10
24/24 - 6s - loss: 920.1480 - loglik: -9.1775e+02 - logprior: -2.3934e+00
Epoch 8/10
24/24 - 6s - loss: 916.2020 - loglik: -9.1376e+02 - logprior: -2.4427e+00
Epoch 9/10
24/24 - 6s - loss: 920.8588 - loglik: -9.1831e+02 - logprior: -2.5465e+00
Fitted a model with MAP estimate = -917.9179
expansions: [(11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (41, 1), (48, 1), (49, 1), (59, 1), (63, 1), (65, 1), (75, 1), (82, 1), (87, 1), (89, 1), (90, 1), (110, 1), (113, 1), (116, 1), (117, 1), (119, 1), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 4), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 10s - loss: 915.8565 - loglik: -9.0871e+02 - logprior: -7.1496e+00
Epoch 2/2
24/24 - 7s - loss: 897.8067 - loglik: -8.9550e+02 - logprior: -2.3096e+00
Fitted a model with MAP estimate = -895.7596
expansions: [(0, 3), (189, 1), (191, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 10s - loss: 897.4839 - loglik: -8.9308e+02 - logprior: -4.4057e+00
Epoch 2/2
24/24 - 8s - loss: 890.7419 - loglik: -8.9079e+02 - logprior: 0.0456
Fitted a model with MAP estimate = -888.8271
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 14s - loss: 899.9893 - loglik: -8.9360e+02 - logprior: -6.3886e+00
Epoch 2/10
24/24 - 8s - loss: 894.0030 - loglik: -8.9318e+02 - logprior: -8.2047e-01
Epoch 3/10
24/24 - 8s - loss: 890.4139 - loglik: -8.9157e+02 - logprior: 1.1598
Epoch 4/10
24/24 - 8s - loss: 889.1331 - loglik: -8.9032e+02 - logprior: 1.1885
Epoch 5/10
24/24 - 8s - loss: 886.0232 - loglik: -8.8738e+02 - logprior: 1.3608
Epoch 6/10
24/24 - 8s - loss: 884.8042 - loglik: -8.8628e+02 - logprior: 1.4793
Epoch 7/10
24/24 - 8s - loss: 882.5678 - loglik: -8.8417e+02 - logprior: 1.6037
Epoch 8/10
24/24 - 8s - loss: 882.3713 - loglik: -8.8399e+02 - logprior: 1.6271
Epoch 9/10
24/24 - 8s - loss: 880.6409 - loglik: -8.8238e+02 - logprior: 1.7453
Epoch 10/10
24/24 - 8s - loss: 878.1013 - loglik: -8.7997e+02 - logprior: 1.8694
Fitted a model with MAP estimate = -879.1551
Time for alignment: 201.4952
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 1107.3586 - loglik: -1.1020e+03 - logprior: -5.3240e+00
Epoch 2/10
24/24 - 6s - loss: 955.9810 - loglik: -9.5384e+02 - logprior: -2.1363e+00
Epoch 3/10
24/24 - 6s - loss: 928.2195 - loglik: -9.2569e+02 - logprior: -2.5284e+00
Epoch 4/10
24/24 - 6s - loss: 922.4235 - loglik: -9.2002e+02 - logprior: -2.3991e+00
Epoch 5/10
24/24 - 6s - loss: 922.8096 - loglik: -9.2045e+02 - logprior: -2.3589e+00
Fitted a model with MAP estimate = -921.2301
expansions: [(11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (75, 1), (82, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (118, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 2), (174, 2), (186, 1), (187, 1), (189, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 917.2802 - loglik: -9.1018e+02 - logprior: -7.1048e+00
Epoch 2/2
24/24 - 8s - loss: 896.1834 - loglik: -8.9387e+02 - logprior: -2.3168e+00
Fitted a model with MAP estimate = -895.0644
expansions: [(0, 3), (190, 1), (192, 1)]
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 893.9807 - loglik: -8.8951e+02 - logprior: -4.4674e+00
Epoch 2/2
24/24 - 8s - loss: 892.6326 - loglik: -8.9262e+02 - logprior: -8.7017e-03
Fitted a model with MAP estimate = -888.9307
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 901.4632 - loglik: -8.9503e+02 - logprior: -6.4363e+00
Epoch 2/10
24/24 - 8s - loss: 893.2431 - loglik: -8.9240e+02 - logprior: -8.4023e-01
Epoch 3/10
24/24 - 8s - loss: 891.4682 - loglik: -8.9266e+02 - logprior: 1.1950
Epoch 4/10
24/24 - 8s - loss: 889.6712 - loglik: -8.9095e+02 - logprior: 1.2792
Epoch 5/10
24/24 - 8s - loss: 884.0346 - loglik: -8.8548e+02 - logprior: 1.4462
Epoch 6/10
24/24 - 8s - loss: 885.5222 - loglik: -8.8711e+02 - logprior: 1.5911
Fitted a model with MAP estimate = -883.6923
Time for alignment: 148.5245
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 1107.1527 - loglik: -1.1018e+03 - logprior: -5.3033e+00
Epoch 2/10
24/24 - 6s - loss: 955.8758 - loglik: -9.5382e+02 - logprior: -2.0554e+00
Epoch 3/10
24/24 - 6s - loss: 929.2683 - loglik: -9.2686e+02 - logprior: -2.4094e+00
Epoch 4/10
24/24 - 6s - loss: 926.2935 - loglik: -9.2403e+02 - logprior: -2.2593e+00
Epoch 5/10
24/24 - 6s - loss: 922.5982 - loglik: -9.2037e+02 - logprior: -2.2230e+00
Epoch 6/10
24/24 - 6s - loss: 920.7023 - loglik: -9.1848e+02 - logprior: -2.2213e+00
Epoch 7/10
24/24 - 6s - loss: 923.2899 - loglik: -9.2103e+02 - logprior: -2.2542e+00
Fitted a model with MAP estimate = -920.1689
expansions: [(11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (47, 1), (48, 1), (49, 1), (64, 1), (66, 1), (76, 1), (83, 1), (88, 2), (91, 1), (111, 1), (114, 1), (117, 1), (118, 1), (119, 2), (120, 2), (122, 1), (150, 1), (152, 1), (154, 4), (158, 1), (172, 1), (173, 4), (186, 1), (187, 1), (190, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 917.1846 - loglik: -9.1002e+02 - logprior: -7.1683e+00
Epoch 2/2
24/24 - 8s - loss: 900.0126 - loglik: -8.9770e+02 - logprior: -2.3082e+00
Fitted a model with MAP estimate = -895.4209
expansions: [(0, 3), (189, 1), (191, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 13s - loss: 897.5383 - loglik: -8.9312e+02 - logprior: -4.4213e+00
Epoch 2/2
24/24 - 8s - loss: 888.7359 - loglik: -8.8876e+02 - logprior: 0.0279
Fitted a model with MAP estimate = -888.8138
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 900.5359 - loglik: -8.9414e+02 - logprior: -6.3998e+00
Epoch 2/10
24/24 - 8s - loss: 894.3464 - loglik: -8.9352e+02 - logprior: -8.2718e-01
Epoch 3/10
24/24 - 8s - loss: 889.7329 - loglik: -8.9098e+02 - logprior: 1.2449
Epoch 4/10
24/24 - 8s - loss: 887.1537 - loglik: -8.8848e+02 - logprior: 1.3249
Epoch 5/10
24/24 - 8s - loss: 888.6789 - loglik: -8.9018e+02 - logprior: 1.5035
Fitted a model with MAP estimate = -885.2478
Time for alignment: 151.2786
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 1103.7008 - loglik: -1.0984e+03 - logprior: -5.3268e+00
Epoch 2/10
24/24 - 6s - loss: 958.5501 - loglik: -9.5639e+02 - logprior: -2.1615e+00
Epoch 3/10
24/24 - 6s - loss: 931.1330 - loglik: -9.2866e+02 - logprior: -2.4757e+00
Epoch 4/10
24/24 - 6s - loss: 923.7372 - loglik: -9.2144e+02 - logprior: -2.2980e+00
Epoch 5/10
24/24 - 6s - loss: 925.7825 - loglik: -9.2348e+02 - logprior: -2.2979e+00
Fitted a model with MAP estimate = -922.7900
expansions: [(11, 1), (12, 1), (13, 1), (14, 2), (15, 1), (16, 1), (17, 1), (18, 2), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (49, 1), (50, 1), (60, 1), (64, 1), (66, 1), (76, 1), (83, 1), (87, 1), (88, 1), (91, 1), (109, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 4), (158, 1), (172, 1), (173, 2), (174, 2), (175, 1), (186, 1), (187, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 239 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 11s - loss: 915.6418 - loglik: -9.0850e+02 - logprior: -7.1423e+00
Epoch 2/2
24/24 - 8s - loss: 896.5197 - loglik: -8.9422e+02 - logprior: -2.3033e+00
Fitted a model with MAP estimate = -894.8597
expansions: [(0, 3), (190, 1), (192, 1)]
discards: [ 0 24]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 12s - loss: 897.6075 - loglik: -8.9315e+02 - logprior: -4.4530e+00
Epoch 2/2
24/24 - 8s - loss: 889.4587 - loglik: -8.8946e+02 - logprior: -2.4427e-03
Fitted a model with MAP estimate = -888.8289
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 11s - loss: 899.7507 - loglik: -8.9333e+02 - logprior: -6.4181e+00
Epoch 2/10
24/24 - 8s - loss: 895.0596 - loglik: -8.9422e+02 - logprior: -8.3813e-01
Epoch 3/10
24/24 - 8s - loss: 889.8746 - loglik: -8.9107e+02 - logprior: 1.1958
Epoch 4/10
24/24 - 8s - loss: 889.0020 - loglik: -8.9029e+02 - logprior: 1.2860
Epoch 5/10
24/24 - 8s - loss: 887.1765 - loglik: -8.8864e+02 - logprior: 1.4643
Epoch 6/10
24/24 - 8s - loss: 886.1235 - loglik: -8.8773e+02 - logprior: 1.6110
Epoch 7/10
24/24 - 8s - loss: 880.5110 - loglik: -8.8212e+02 - logprior: 1.6138
Epoch 8/10
24/24 - 8s - loss: 880.8784 - loglik: -8.8241e+02 - logprior: 1.5368
Fitted a model with MAP estimate = -880.5954
Time for alignment: 163.6117
Fitting a model of length 192 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 9s - loss: 1104.7847 - loglik: -1.0995e+03 - logprior: -5.3088e+00
Epoch 2/10
24/24 - 6s - loss: 956.5062 - loglik: -9.5436e+02 - logprior: -2.1491e+00
Epoch 3/10
24/24 - 6s - loss: 929.7859 - loglik: -9.2722e+02 - logprior: -2.5695e+00
Epoch 4/10
24/24 - 6s - loss: 925.2672 - loglik: -9.2291e+02 - logprior: -2.3564e+00
Epoch 5/10
24/24 - 6s - loss: 921.3865 - loglik: -9.1907e+02 - logprior: -2.3183e+00
Epoch 6/10
24/24 - 6s - loss: 921.2836 - loglik: -9.1898e+02 - logprior: -2.3026e+00
Epoch 7/10
24/24 - 6s - loss: 918.4697 - loglik: -9.1616e+02 - logprior: -2.3063e+00
Epoch 8/10
24/24 - 6s - loss: 920.5532 - loglik: -9.1820e+02 - logprior: -2.3499e+00
Fitted a model with MAP estimate = -918.5829
expansions: [(11, 1), (12, 3), (15, 1), (17, 2), (18, 1), (35, 1), (36, 1), (37, 1), (38, 1), (40, 1), (41, 1), (48, 1), (49, 1), (62, 1), (63, 1), (65, 1), (76, 1), (82, 1), (87, 1), (90, 1), (93, 1), (110, 1), (113, 1), (116, 1), (119, 2), (120, 2), (122, 1), (150, 1), (153, 1), (154, 3), (155, 1), (156, 1), (158, 1), (172, 1), (173, 2), (174, 2), (186, 1), (187, 1), (190, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 238 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 10s - loss: 917.7536 - loglik: -9.1059e+02 - logprior: -7.1586e+00
Epoch 2/2
24/24 - 8s - loss: 900.0134 - loglik: -8.9760e+02 - logprior: -2.4168e+00
Fitted a model with MAP estimate = -896.2400
expansions: [(0, 3), (12, 1), (13, 1), (188, 1)]
discards: [ 0 22]
Re-initialized the encoder parameters.
Fitting a model of length 242 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
24/24 - 10s - loss: 897.7089 - loglik: -8.9328e+02 - logprior: -4.4305e+00
Epoch 2/2
24/24 - 8s - loss: 890.6306 - loglik: -8.9068e+02 - logprior: 0.0454
Fitted a model with MAP estimate = -888.8466
expansions: [(151, 1)]
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 241 on 3904 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
24/24 - 12s - loss: 900.5239 - loglik: -8.9413e+02 - logprior: -6.3953e+00
Epoch 2/10
24/24 - 8s - loss: 892.0875 - loglik: -8.9121e+02 - logprior: -8.7881e-01
Epoch 3/10
24/24 - 8s - loss: 889.5148 - loglik: -8.9067e+02 - logprior: 1.1538
Epoch 4/10
24/24 - 8s - loss: 889.7269 - loglik: -8.9092e+02 - logprior: 1.1948
Fitted a model with MAP estimate = -887.2829
Time for alignment: 148.8339
Computed alignments with likelihoods: ['-879.1551', '-883.6923', '-885.2478', '-880.5954', '-887.2829']
Best model has likelihood: -879.1551  (prior= 1.9523 )
time for generating output: 0.2172
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tim.projection.fasta
SP score = 0.979373982268862
Training of 5 independent models on file tgfb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe724164400>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8e39068b0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3be33e80>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 588.0201 - loglik: -5.6094e+02 - logprior: -2.7080e+01
Epoch 2/10
10/10 - 1s - loss: 527.1899 - loglik: -5.2027e+02 - logprior: -6.9218e+00
Epoch 3/10
10/10 - 1s - loss: 487.0457 - loglik: -4.8344e+02 - logprior: -3.6095e+00
Epoch 4/10
10/10 - 1s - loss: 469.2624 - loglik: -4.6673e+02 - logprior: -2.5347e+00
Epoch 5/10
10/10 - 1s - loss: 461.6457 - loglik: -4.5966e+02 - logprior: -1.9830e+00
Epoch 6/10
10/10 - 1s - loss: 459.3112 - loglik: -4.5774e+02 - logprior: -1.5707e+00
Epoch 7/10
10/10 - 1s - loss: 457.3409 - loglik: -4.5603e+02 - logprior: -1.3131e+00
Epoch 8/10
10/10 - 1s - loss: 455.6441 - loglik: -4.5439e+02 - logprior: -1.2539e+00
Epoch 9/10
10/10 - 1s - loss: 455.4379 - loglik: -4.5416e+02 - logprior: -1.2782e+00
Epoch 10/10
10/10 - 1s - loss: 453.8801 - loglik: -4.5258e+02 - logprior: -1.2940e+00
Fitted a model with MAP estimate = -453.9197
expansions: [(0, 2), (1, 1), (11, 2), (12, 1), (34, 1), (36, 4), (49, 3), (69, 2), (70, 1), (72, 3), (73, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 490.6713 - loglik: -4.5631e+02 - logprior: -3.4363e+01
Epoch 2/2
10/10 - 1s - loss: 453.3996 - loglik: -4.4323e+02 - logprior: -1.0173e+01
Fitted a model with MAP estimate = -447.4594
expansions: [(0, 2), (64, 2)]
discards: [ 0  1 14 93]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 471.8206 - loglik: -4.4376e+02 - logprior: -2.8056e+01
Epoch 2/2
10/10 - 1s - loss: 444.9308 - loglik: -4.3776e+02 - logprior: -7.1749e+00
Fitted a model with MAP estimate = -440.9762
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 467.5663 - loglik: -4.4195e+02 - logprior: -2.5612e+01
Epoch 2/10
10/10 - 1s - loss: 444.3696 - loglik: -4.3800e+02 - logprior: -6.3665e+00
Epoch 3/10
10/10 - 1s - loss: 440.9702 - loglik: -4.3835e+02 - logprior: -2.6215e+00
Epoch 4/10
10/10 - 1s - loss: 437.1788 - loglik: -4.3580e+02 - logprior: -1.3776e+00
Epoch 5/10
10/10 - 1s - loss: 437.2475 - loglik: -4.3666e+02 - logprior: -5.8758e-01
Fitted a model with MAP estimate = -436.1758
Time for alignment: 31.3271
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 588.9933 - loglik: -5.6191e+02 - logprior: -2.7079e+01
Epoch 2/10
10/10 - 1s - loss: 527.0475 - loglik: -5.2014e+02 - logprior: -6.9099e+00
Epoch 3/10
10/10 - 1s - loss: 487.6116 - loglik: -4.8400e+02 - logprior: -3.6102e+00
Epoch 4/10
10/10 - 1s - loss: 468.6401 - loglik: -4.6600e+02 - logprior: -2.6423e+00
Epoch 5/10
10/10 - 1s - loss: 461.3331 - loglik: -4.5912e+02 - logprior: -2.2174e+00
Epoch 6/10
10/10 - 1s - loss: 458.5724 - loglik: -4.5677e+02 - logprior: -1.8001e+00
Epoch 7/10
10/10 - 1s - loss: 456.5063 - loglik: -4.5497e+02 - logprior: -1.5383e+00
Epoch 8/10
10/10 - 1s - loss: 455.0480 - loglik: -4.5354e+02 - logprior: -1.5048e+00
Epoch 9/10
10/10 - 1s - loss: 453.6685 - loglik: -4.5211e+02 - logprior: -1.5594e+00
Epoch 10/10
10/10 - 1s - loss: 452.8901 - loglik: -4.5130e+02 - logprior: -1.5855e+00
Fitted a model with MAP estimate = -452.9721
expansions: [(0, 2), (1, 1), (11, 2), (12, 1), (34, 1), (36, 4), (50, 3), (51, 1), (69, 2), (70, 1), (71, 2), (72, 1), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 490.4013 - loglik: -4.5588e+02 - logprior: -3.4524e+01
Epoch 2/2
10/10 - 1s - loss: 453.5029 - loglik: -4.4321e+02 - logprior: -1.0288e+01
Fitted a model with MAP estimate = -447.5623
expansions: [(0, 2), (44, 1)]
discards: [ 0  1 14 62 89]
Re-initialized the encoder parameters.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 472.7812 - loglik: -4.4470e+02 - logprior: -2.8082e+01
Epoch 2/2
10/10 - 1s - loss: 445.8116 - loglik: -4.3862e+02 - logprior: -7.1881e+00
Fitted a model with MAP estimate = -442.3179
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 5s - loss: 468.7252 - loglik: -4.4311e+02 - logprior: -2.5612e+01
Epoch 2/10
10/10 - 1s - loss: 445.5673 - loglik: -4.3919e+02 - logprior: -6.3728e+00
Epoch 3/10
10/10 - 1s - loss: 441.7938 - loglik: -4.3915e+02 - logprior: -2.6436e+00
Epoch 4/10
10/10 - 1s - loss: 439.7340 - loglik: -4.3834e+02 - logprior: -1.3928e+00
Epoch 5/10
10/10 - 1s - loss: 439.1854 - loglik: -4.3855e+02 - logprior: -6.3367e-01
Epoch 6/10
10/10 - 1s - loss: 437.3468 - loglik: -4.3725e+02 - logprior: -9.6332e-02
Epoch 7/10
10/10 - 1s - loss: 436.2231 - loglik: -4.3637e+02 - logprior: 0.1479
Epoch 8/10
10/10 - 1s - loss: 434.9895 - loglik: -4.3528e+02 - logprior: 0.2887
Epoch 9/10
10/10 - 1s - loss: 435.2070 - loglik: -4.3560e+02 - logprior: 0.3923
Fitted a model with MAP estimate = -434.3970
Time for alignment: 40.0431
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 589.2254 - loglik: -5.6215e+02 - logprior: -2.7079e+01
Epoch 2/10
10/10 - 1s - loss: 526.9056 - loglik: -5.1999e+02 - logprior: -6.9163e+00
Epoch 3/10
10/10 - 1s - loss: 489.5201 - loglik: -4.8589e+02 - logprior: -3.6252e+00
Epoch 4/10
10/10 - 1s - loss: 472.4601 - loglik: -4.6983e+02 - logprior: -2.6257e+00
Epoch 5/10
10/10 - 1s - loss: 463.1855 - loglik: -4.6096e+02 - logprior: -2.2269e+00
Epoch 6/10
10/10 - 1s - loss: 458.7210 - loglik: -4.5684e+02 - logprior: -1.8825e+00
Epoch 7/10
10/10 - 1s - loss: 457.2297 - loglik: -4.5560e+02 - logprior: -1.6238e+00
Epoch 8/10
10/10 - 1s - loss: 455.1677 - loglik: -4.5360e+02 - logprior: -1.5686e+00
Epoch 9/10
10/10 - 1s - loss: 454.2816 - loglik: -4.5270e+02 - logprior: -1.5838e+00
Epoch 10/10
10/10 - 1s - loss: 453.1985 - loglik: -4.5158e+02 - logprior: -1.6184e+00
Fitted a model with MAP estimate = -453.0364
expansions: [(0, 2), (1, 1), (11, 2), (12, 1), (34, 1), (36, 4), (50, 2), (60, 1), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 103 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 490.4072 - loglik: -4.5600e+02 - logprior: -3.4407e+01
Epoch 2/2
10/10 - 1s - loss: 453.6479 - loglik: -4.4355e+02 - logprior: -1.0097e+01
Fitted a model with MAP estimate = -447.7730
expansions: [(0, 2), (59, 2)]
discards: [ 0  1 14]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 472.4559 - loglik: -4.4434e+02 - logprior: -2.8117e+01
Epoch 2/2
10/10 - 1s - loss: 445.3625 - loglik: -4.3809e+02 - logprior: -7.2769e+00
Fitted a model with MAP estimate = -441.8959
expansions: []
discards: [0 1]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 468.2872 - loglik: -4.4261e+02 - logprior: -2.5679e+01
Epoch 2/10
10/10 - 1s - loss: 445.6682 - loglik: -4.3923e+02 - logprior: -6.4408e+00
Epoch 3/10
10/10 - 1s - loss: 440.7387 - loglik: -4.3802e+02 - logprior: -2.7151e+00
Epoch 4/10
10/10 - 1s - loss: 439.4439 - loglik: -4.3799e+02 - logprior: -1.4512e+00
Epoch 5/10
10/10 - 1s - loss: 438.4388 - loglik: -4.3778e+02 - logprior: -6.6217e-01
Epoch 6/10
10/10 - 1s - loss: 436.7078 - loglik: -4.3656e+02 - logprior: -1.4420e-01
Epoch 7/10
10/10 - 1s - loss: 435.8276 - loglik: -4.3591e+02 - logprior: 0.0824
Epoch 8/10
10/10 - 1s - loss: 434.4375 - loglik: -4.3465e+02 - logprior: 0.2163
Epoch 9/10
10/10 - 1s - loss: 434.8517 - loglik: -4.3517e+02 - logprior: 0.3241
Fitted a model with MAP estimate = -433.8447
Time for alignment: 37.5633
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 589.2521 - loglik: -5.6217e+02 - logprior: -2.7078e+01
Epoch 2/10
10/10 - 1s - loss: 526.3049 - loglik: -5.1939e+02 - logprior: -6.9132e+00
Epoch 3/10
10/10 - 1s - loss: 490.4534 - loglik: -4.8684e+02 - logprior: -3.6148e+00
Epoch 4/10
10/10 - 1s - loss: 471.5636 - loglik: -4.6898e+02 - logprior: -2.5788e+00
Epoch 5/10
10/10 - 1s - loss: 465.3215 - loglik: -4.6329e+02 - logprior: -2.0332e+00
Epoch 6/10
10/10 - 1s - loss: 460.8528 - loglik: -4.5928e+02 - logprior: -1.5702e+00
Epoch 7/10
10/10 - 1s - loss: 457.5277 - loglik: -4.5622e+02 - logprior: -1.3050e+00
Epoch 8/10
10/10 - 1s - loss: 455.6566 - loglik: -4.5438e+02 - logprior: -1.2736e+00
Epoch 9/10
10/10 - 1s - loss: 454.2164 - loglik: -4.5289e+02 - logprior: -1.3228e+00
Epoch 10/10
10/10 - 1s - loss: 453.5589 - loglik: -4.5223e+02 - logprior: -1.3223e+00
Fitted a model with MAP estimate = -453.2141
expansions: [(0, 3), (12, 1), (21, 1), (35, 4), (48, 2), (49, 2), (69, 2), (70, 2), (71, 1), (72, 2), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 489.4876 - loglik: -4.5502e+02 - logprior: -3.4473e+01
Epoch 2/2
10/10 - 1s - loss: 453.2387 - loglik: -4.4300e+02 - logprior: -1.0234e+01
Fitted a model with MAP estimate = -446.7632
expansions: [(38, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 467.4279 - loglik: -4.4206e+02 - logprior: -2.5369e+01
Epoch 2/2
10/10 - 1s - loss: 442.7691 - loglik: -4.3632e+02 - logprior: -6.4532e+00
Fitted a model with MAP estimate = -440.4055
expansions: [(59, 1)]
discards: [86]
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 463.5000 - loglik: -4.3946e+02 - logprior: -2.4041e+01
Epoch 2/10
10/10 - 1s - loss: 441.2095 - loglik: -4.3521e+02 - logprior: -6.0018e+00
Epoch 3/10
10/10 - 1s - loss: 437.9238 - loglik: -4.3548e+02 - logprior: -2.4463e+00
Epoch 4/10
10/10 - 1s - loss: 435.4333 - loglik: -4.3437e+02 - logprior: -1.0641e+00
Epoch 5/10
10/10 - 1s - loss: 434.5370 - loglik: -4.3413e+02 - logprior: -4.0489e-01
Epoch 6/10
10/10 - 1s - loss: 433.2626 - loglik: -4.3318e+02 - logprior: -8.2762e-02
Epoch 7/10
10/10 - 1s - loss: 433.0411 - loglik: -4.3318e+02 - logprior: 0.1418
Epoch 8/10
10/10 - 1s - loss: 430.6855 - loglik: -4.3098e+02 - logprior: 0.2971
Epoch 9/10
10/10 - 1s - loss: 430.8660 - loglik: -4.3128e+02 - logprior: 0.4156
Fitted a model with MAP estimate = -430.4153
Time for alignment: 35.6757
Fitting a model of length 83 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 6s - loss: 588.8492 - loglik: -5.6177e+02 - logprior: -2.7077e+01
Epoch 2/10
10/10 - 1s - loss: 527.5911 - loglik: -5.2067e+02 - logprior: -6.9210e+00
Epoch 3/10
10/10 - 1s - loss: 489.7419 - loglik: -4.8608e+02 - logprior: -3.6583e+00
Epoch 4/10
10/10 - 1s - loss: 471.0187 - loglik: -4.6828e+02 - logprior: -2.7390e+00
Epoch 5/10
10/10 - 1s - loss: 462.3488 - loglik: -4.6008e+02 - logprior: -2.2682e+00
Epoch 6/10
10/10 - 1s - loss: 457.9172 - loglik: -4.5605e+02 - logprior: -1.8710e+00
Epoch 7/10
10/10 - 1s - loss: 455.8336 - loglik: -4.5423e+02 - logprior: -1.6034e+00
Epoch 8/10
10/10 - 1s - loss: 454.7141 - loglik: -4.5309e+02 - logprior: -1.6245e+00
Epoch 9/10
10/10 - 1s - loss: 453.3452 - loglik: -4.5164e+02 - logprior: -1.7074e+00
Epoch 10/10
10/10 - 1s - loss: 452.5765 - loglik: -4.5088e+02 - logprior: -1.6991e+00
Fitted a model with MAP estimate = -452.4203
expansions: [(0, 2), (1, 1), (11, 2), (12, 1), (21, 1), (35, 3), (36, 1), (49, 3), (50, 3), (67, 1), (69, 2), (70, 1), (71, 2), (72, 1), (73, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 108 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 487.3829 - loglik: -4.5312e+02 - logprior: -3.4260e+01
Epoch 2/2
10/10 - 1s - loss: 449.1608 - loglik: -4.3909e+02 - logprior: -1.0071e+01
Fitted a model with MAP estimate = -443.6463
expansions: []
discards: [ 0  1 14 92]
Re-initialized the encoder parameters.
Fitting a model of length 104 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
10/10 - 4s - loss: 467.1403 - loglik: -4.4150e+02 - logprior: -2.5644e+01
Epoch 2/2
10/10 - 1s - loss: 443.8417 - loglik: -4.3741e+02 - logprior: -6.4343e+00
Fitted a model with MAP estimate = -440.3200
expansions: [(1, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 105 on 1606 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
10/10 - 4s - loss: 462.7686 - loglik: -4.3873e+02 - logprior: -2.4042e+01
Epoch 2/10
10/10 - 1s - loss: 441.0703 - loglik: -4.3508e+02 - logprior: -5.9887e+00
Epoch 3/10
10/10 - 1s - loss: 437.7059 - loglik: -4.3528e+02 - logprior: -2.4210e+00
Epoch 4/10
10/10 - 1s - loss: 435.5587 - loglik: -4.3451e+02 - logprior: -1.0444e+00
Epoch 5/10
10/10 - 1s - loss: 435.1068 - loglik: -4.3472e+02 - logprior: -3.8688e-01
Epoch 6/10
10/10 - 1s - loss: 433.4864 - loglik: -4.3345e+02 - logprior: -3.6461e-02
Epoch 7/10
10/10 - 1s - loss: 432.6159 - loglik: -4.3279e+02 - logprior: 0.1737
Epoch 8/10
10/10 - 1s - loss: 431.5393 - loglik: -4.3189e+02 - logprior: 0.3525
Epoch 9/10
10/10 - 1s - loss: 430.3810 - loglik: -4.3086e+02 - logprior: 0.4783
Epoch 10/10
10/10 - 1s - loss: 430.4452 - loglik: -4.3100e+02 - logprior: 0.5583
Fitted a model with MAP estimate = -430.3123
Time for alignment: 38.5306
Computed alignments with likelihoods: ['-436.1758', '-434.3970', '-433.8447', '-430.4153', '-430.3123']
Best model has likelihood: -430.3123  (prior= 0.5948 )
time for generating output: 0.1333
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/tgfb.projection.fasta
SP score = 0.8438818565400844
Training of 5 independent models on file HLH.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8d2506cd0>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8bff87b50>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8c99b43d0>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 300.5109 - loglik: -2.9542e+02 - logprior: -5.0935e+00
Epoch 2/10
16/16 - 1s - loss: 275.5285 - loglik: -2.7396e+02 - logprior: -1.5709e+00
Epoch 3/10
16/16 - 1s - loss: 264.2675 - loglik: -2.6271e+02 - logprior: -1.5527e+00
Epoch 4/10
16/16 - 1s - loss: 259.8139 - loglik: -2.5824e+02 - logprior: -1.5693e+00
Epoch 5/10
16/16 - 1s - loss: 257.5845 - loglik: -2.5597e+02 - logprior: -1.6167e+00
Epoch 6/10
16/16 - 1s - loss: 256.5162 - loglik: -2.5490e+02 - logprior: -1.6162e+00
Epoch 7/10
16/16 - 1s - loss: 255.3410 - loglik: -2.5375e+02 - logprior: -1.5818e+00
Epoch 8/10
16/16 - 1s - loss: 254.8751 - loglik: -2.5330e+02 - logprior: -1.5701e+00
Epoch 9/10
16/16 - 1s - loss: 254.5217 - loglik: -2.5295e+02 - logprior: -1.5632e+00
Epoch 10/10
16/16 - 1s - loss: 253.4650 - loglik: -2.5190e+02 - logprior: -1.5585e+00
Fitted a model with MAP estimate = -253.4432
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (14, 1), (17, 1), (23, 4), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 265.8169 - loglik: -2.5949e+02 - logprior: -6.3249e+00
Epoch 2/2
16/16 - 1s - loss: 254.9230 - loglik: -2.5190e+02 - logprior: -3.0251e+00
Fitted a model with MAP estimate = -253.1833
expansions: [(0, 1)]
discards: [ 0 33 36]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 255.6224 - loglik: -2.5103e+02 - logprior: -4.5924e+00
Epoch 2/2
16/16 - 1s - loss: 251.9586 - loglik: -2.5034e+02 - logprior: -1.6181e+00
Fitted a model with MAP estimate = -251.2339
expansions: [(3, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 255.2209 - loglik: -2.5064e+02 - logprior: -4.5787e+00
Epoch 2/10
16/16 - 1s - loss: 251.5645 - loglik: -2.5002e+02 - logprior: -1.5476e+00
Epoch 3/10
16/16 - 1s - loss: 250.6062 - loglik: -2.4933e+02 - logprior: -1.2767e+00
Epoch 4/10
16/16 - 1s - loss: 250.3569 - loglik: -2.4914e+02 - logprior: -1.2134e+00
Epoch 5/10
16/16 - 1s - loss: 249.9372 - loglik: -2.4876e+02 - logprior: -1.1746e+00
Epoch 6/10
16/16 - 1s - loss: 248.9297 - loglik: -2.4778e+02 - logprior: -1.1507e+00
Epoch 7/10
16/16 - 1s - loss: 248.2211 - loglik: -2.4709e+02 - logprior: -1.1246e+00
Epoch 8/10
16/16 - 1s - loss: 247.2440 - loglik: -2.4612e+02 - logprior: -1.1133e+00
Epoch 9/10
16/16 - 1s - loss: 246.6970 - loglik: -2.4560e+02 - logprior: -1.0926e+00
Epoch 10/10
16/16 - 1s - loss: 245.0775 - loglik: -2.4398e+02 - logprior: -1.0862e+00
Fitted a model with MAP estimate = -245.0949
Time for alignment: 52.4681
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 300.4680 - loglik: -2.9537e+02 - logprior: -5.0954e+00
Epoch 2/10
16/16 - 1s - loss: 276.4167 - loglik: -2.7482e+02 - logprior: -1.5958e+00
Epoch 3/10
16/16 - 1s - loss: 266.3095 - loglik: -2.6474e+02 - logprior: -1.5731e+00
Epoch 4/10
16/16 - 1s - loss: 260.2438 - loglik: -2.5869e+02 - logprior: -1.5487e+00
Epoch 5/10
16/16 - 1s - loss: 258.2314 - loglik: -2.5668e+02 - logprior: -1.5492e+00
Epoch 6/10
16/16 - 1s - loss: 257.1397 - loglik: -2.5561e+02 - logprior: -1.5214e+00
Epoch 7/10
16/16 - 1s - loss: 256.4174 - loglik: -2.5492e+02 - logprior: -1.4935e+00
Epoch 8/10
16/16 - 1s - loss: 256.7743 - loglik: -2.5528e+02 - logprior: -1.4879e+00
Fitted a model with MAP estimate = -255.4039
expansions: [(3, 1), (6, 1), (12, 2), (13, 1), (18, 1), (24, 8), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 57 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 265.8948 - loglik: -2.5956e+02 - logprior: -6.3331e+00
Epoch 2/2
16/16 - 1s - loss: 255.7919 - loglik: -2.5273e+02 - logprior: -3.0617e+00
Fitted a model with MAP estimate = -254.0238
expansions: [(0, 1)]
discards: [ 0 14 32 33 38]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 257.1699 - loglik: -2.5256e+02 - logprior: -4.6124e+00
Epoch 2/2
16/16 - 1s - loss: 252.1073 - loglik: -2.5047e+02 - logprior: -1.6382e+00
Fitted a model with MAP estimate = -251.2576
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 259.0002 - loglik: -2.5273e+02 - logprior: -6.2703e+00
Epoch 2/10
16/16 - 1s - loss: 252.7776 - loglik: -2.5003e+02 - logprior: -2.7471e+00
Epoch 3/10
16/16 - 1s - loss: 251.6434 - loglik: -2.5013e+02 - logprior: -1.5109e+00
Epoch 4/10
16/16 - 1s - loss: 250.4568 - loglik: -2.4922e+02 - logprior: -1.2401e+00
Epoch 5/10
16/16 - 1s - loss: 250.6282 - loglik: -2.4942e+02 - logprior: -1.2022e+00
Fitted a model with MAP estimate = -249.8417
Time for alignment: 42.8471
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 6s - loss: 300.5087 - loglik: -2.9541e+02 - logprior: -5.0992e+00
Epoch 2/10
16/16 - 1s - loss: 272.7470 - loglik: -2.7112e+02 - logprior: -1.6241e+00
Epoch 3/10
16/16 - 1s - loss: 261.7419 - loglik: -2.6009e+02 - logprior: -1.6523e+00
Epoch 4/10
16/16 - 1s - loss: 258.2751 - loglik: -2.5663e+02 - logprior: -1.6429e+00
Epoch 5/10
16/16 - 1s - loss: 257.5229 - loglik: -2.5589e+02 - logprior: -1.6291e+00
Epoch 6/10
16/16 - 1s - loss: 257.0626 - loglik: -2.5545e+02 - logprior: -1.6062e+00
Epoch 7/10
16/16 - 1s - loss: 255.1604 - loglik: -2.5358e+02 - logprior: -1.5779e+00
Epoch 8/10
16/16 - 1s - loss: 255.5127 - loglik: -2.5393e+02 - logprior: -1.5709e+00
Fitted a model with MAP estimate = -254.6113
expansions: [(3, 1), (6, 1), (12, 1), (13, 1), (15, 1), (17, 1), (23, 5), (24, 2), (25, 2)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 264.2408 - loglik: -2.5792e+02 - logprior: -6.3188e+00
Epoch 2/2
16/16 - 1s - loss: 254.5023 - loglik: -2.5150e+02 - logprior: -3.0029e+00
Fitted a model with MAP estimate = -253.0772
expansions: [(0, 1)]
discards: [ 0 31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 4s - loss: 256.3694 - loglik: -2.5177e+02 - logprior: -4.6014e+00
Epoch 2/2
16/16 - 1s - loss: 251.2826 - loglik: -2.4965e+02 - logprior: -1.6299e+00
Fitted a model with MAP estimate = -251.1879
expansions: [(3, 1)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 258.3880 - loglik: -2.5211e+02 - logprior: -6.2820e+00
Epoch 2/10
16/16 - 1s - loss: 253.1428 - loglik: -2.5038e+02 - logprior: -2.7602e+00
Epoch 3/10
16/16 - 1s - loss: 252.2046 - loglik: -2.5068e+02 - logprior: -1.5253e+00
Epoch 4/10
16/16 - 1s - loss: 250.1609 - loglik: -2.4892e+02 - logprior: -1.2350e+00
Epoch 5/10
16/16 - 1s - loss: 250.6433 - loglik: -2.4944e+02 - logprior: -1.2055e+00
Fitted a model with MAP estimate = -249.8441
Time for alignment: 42.8482
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 300.0175 - loglik: -2.9492e+02 - logprior: -5.0928e+00
Epoch 2/10
16/16 - 1s - loss: 273.5429 - loglik: -2.7196e+02 - logprior: -1.5877e+00
Epoch 3/10
16/16 - 1s - loss: 262.5837 - loglik: -2.6097e+02 - logprior: -1.6180e+00
Epoch 4/10
16/16 - 1s - loss: 259.5286 - loglik: -2.5797e+02 - logprior: -1.5620e+00
Epoch 5/10
16/16 - 1s - loss: 257.9955 - loglik: -2.5645e+02 - logprior: -1.5457e+00
Epoch 6/10
16/16 - 1s - loss: 257.3969 - loglik: -2.5586e+02 - logprior: -1.5376e+00
Epoch 7/10
16/16 - 1s - loss: 255.8084 - loglik: -2.5430e+02 - logprior: -1.4991e+00
Epoch 8/10
16/16 - 1s - loss: 255.4100 - loglik: -2.5391e+02 - logprior: -1.4907e+00
Epoch 9/10
16/16 - 1s - loss: 254.8748 - loglik: -2.5339e+02 - logprior: -1.4762e+00
Epoch 10/10
16/16 - 1s - loss: 254.2255 - loglik: -2.5274e+02 - logprior: -1.4713e+00
Fitted a model with MAP estimate = -253.9753
expansions: [(3, 1), (6, 1), (13, 1), (15, 2), (17, 1), (23, 8)]
discards: [0]
Re-initialized the encoder parameters.
Fitting a model of length 55 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 265.0004 - loglik: -2.5868e+02 - logprior: -6.3222e+00
Epoch 2/2
16/16 - 1s - loss: 254.6672 - loglik: -2.5165e+02 - logprior: -3.0163e+00
Fitted a model with MAP estimate = -253.1005
expansions: [(0, 1)]
discards: [ 0 31 32]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 256.1159 - loglik: -2.5152e+02 - logprior: -4.5998e+00
Epoch 2/2
16/16 - 1s - loss: 251.5960 - loglik: -2.4996e+02 - logprior: -1.6321e+00
Fitted a model with MAP estimate = -251.1600
expansions: [(3, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 54 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 254.9212 - loglik: -2.5032e+02 - logprior: -4.6048e+00
Epoch 2/10
16/16 - 1s - loss: 251.2816 - loglik: -2.4970e+02 - logprior: -1.5804e+00
Epoch 3/10
16/16 - 1s - loss: 250.7718 - loglik: -2.4946e+02 - logprior: -1.3151e+00
Epoch 4/10
16/16 - 1s - loss: 250.4706 - loglik: -2.4923e+02 - logprior: -1.2441e+00
Epoch 5/10
16/16 - 1s - loss: 249.4435 - loglik: -2.4823e+02 - logprior: -1.2110e+00
Epoch 6/10
16/16 - 1s - loss: 248.7088 - loglik: -2.4752e+02 - logprior: -1.1857e+00
Epoch 7/10
16/16 - 1s - loss: 248.0011 - loglik: -2.4683e+02 - logprior: -1.1620e+00
Epoch 8/10
16/16 - 1s - loss: 247.2899 - loglik: -2.4614e+02 - logprior: -1.1443e+00
Epoch 9/10
16/16 - 1s - loss: 246.5319 - loglik: -2.4540e+02 - logprior: -1.1276e+00
Epoch 10/10
16/16 - 1s - loss: 245.6983 - loglik: -2.4458e+02 - logprior: -1.1118e+00
Fitted a model with MAP estimate = -245.1674
Time for alignment: 51.6018
Fitting a model of length 42 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 300.0880 - loglik: -2.9499e+02 - logprior: -5.0957e+00
Epoch 2/10
16/16 - 1s - loss: 275.7276 - loglik: -2.7413e+02 - logprior: -1.6005e+00
Epoch 3/10
16/16 - 1s - loss: 266.3730 - loglik: -2.6482e+02 - logprior: -1.5544e+00
Epoch 4/10
16/16 - 1s - loss: 261.7441 - loglik: -2.6025e+02 - logprior: -1.4898e+00
Epoch 5/10
16/16 - 1s - loss: 259.1848 - loglik: -2.5769e+02 - logprior: -1.4950e+00
Epoch 6/10
16/16 - 1s - loss: 257.4976 - loglik: -2.5603e+02 - logprior: -1.4646e+00
Epoch 7/10
16/16 - 1s - loss: 256.4380 - loglik: -2.5500e+02 - logprior: -1.4301e+00
Epoch 8/10
16/16 - 1s - loss: 256.3785 - loglik: -2.5495e+02 - logprior: -1.4188e+00
Epoch 9/10
16/16 - 1s - loss: 254.8491 - loglik: -2.5343e+02 - logprior: -1.4099e+00
Epoch 10/10
16/16 - 1s - loss: 254.4691 - loglik: -2.5305e+02 - logprior: -1.4097e+00
Fitted a model with MAP estimate = -254.3217
expansions: [(3, 1), (5, 2), (17, 2), (23, 5), (24, 2), (25, 2)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 56 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 6s - loss: 262.3060 - loglik: -2.5738e+02 - logprior: -4.9302e+00
Epoch 2/2
16/16 - 1s - loss: 252.8617 - loglik: -2.5113e+02 - logprior: -1.7304e+00
Fitted a model with MAP estimate = -251.5334
expansions: []
discards: [31 32 37]
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
16/16 - 5s - loss: 256.0948 - loglik: -2.5124e+02 - logprior: -4.8557e+00
Epoch 2/2
16/16 - 1s - loss: 251.7960 - loglik: -2.5008e+02 - logprior: -1.7129e+00
Fitted a model with MAP estimate = -250.9876
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 53 on 6781 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
16/16 - 4s - loss: 255.6105 - loglik: -2.5073e+02 - logprior: -4.8811e+00
Epoch 2/10
16/16 - 1s - loss: 251.8628 - loglik: -2.5017e+02 - logprior: -1.6893e+00
Epoch 3/10
16/16 - 1s - loss: 250.5432 - loglik: -2.4920e+02 - logprior: -1.3426e+00
Epoch 4/10
16/16 - 1s - loss: 250.5274 - loglik: -2.4927e+02 - logprior: -1.2519e+00
Epoch 5/10
16/16 - 1s - loss: 249.9429 - loglik: -2.4873e+02 - logprior: -1.2116e+00
Epoch 6/10
16/16 - 1s - loss: 248.6643 - loglik: -2.4748e+02 - logprior: -1.1830e+00
Epoch 7/10
16/16 - 1s - loss: 248.6994 - loglik: -2.4753e+02 - logprior: -1.1601e+00
Fitted a model with MAP estimate = -247.7649
Time for alignment: 47.5807
Computed alignments with likelihoods: ['-245.0949', '-249.8417', '-249.8441', '-245.1674', '-247.7649']
Best model has likelihood: -245.0949  (prior= -1.0745 )
time for generating output: 0.1097
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/HLH.projection.fasta
SP score = 0.9159935379644588
Training of 5 independent models on file blmb.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea6f78ca00>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe8b5d6ce80>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe714428460>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 11s - loss: 1197.9908 - loglik: -1.1963e+03 - logprior: -1.6772e+00
Epoch 2/10
39/39 - 9s - loss: 1149.3552 - loglik: -1.1485e+03 - logprior: -8.9722e-01
Epoch 3/10
39/39 - 9s - loss: 1142.1136 - loglik: -1.1412e+03 - logprior: -8.9051e-01
Epoch 4/10
39/39 - 9s - loss: 1137.6317 - loglik: -1.1368e+03 - logprior: -8.6996e-01
Epoch 5/10
39/39 - 9s - loss: 1131.9629 - loglik: -1.1311e+03 - logprior: -8.3905e-01
Epoch 6/10
39/39 - 9s - loss: 1126.0631 - loglik: -1.1252e+03 - logprior: -8.1537e-01
Epoch 7/10
39/39 - 9s - loss: 1114.1862 - loglik: -1.1134e+03 - logprior: -7.9246e-01
Epoch 8/10
39/39 - 9s - loss: 1069.8607 - loglik: -1.0687e+03 - logprior: -1.1071e+00
Epoch 9/10
39/39 - 9s - loss: 906.7291 - loglik: -9.0253e+02 - logprior: -4.1513e+00
Epoch 10/10
39/39 - 9s - loss: 866.1269 - loglik: -8.6195e+02 - logprior: -4.1175e+00
Fitted a model with MAP estimate = -943.3090
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47 116 117 135 136 137 154]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1256.5260 - loglik: -1.2531e+03 - logprior: -3.4584e+00
Epoch 2/2
19/19 - 5s - loss: 1241.1344 - loglik: -1.2404e+03 - logprior: -7.1038e-01
Fitted a model with MAP estimate = -1127.0645
expansions: [(0, 102), (101, 106)]
discards: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71
 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
 96 97]
Re-initialized the encoder parameters.
Fitting a model of length 211 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 19s - loss: 1184.2946 - loglik: -1.1826e+03 - logprior: -1.6940e+00
Epoch 2/2
39/39 - 13s - loss: 1137.0553 - loglik: -1.1361e+03 - logprior: -9.8555e-01
Fitted a model with MAP estimate = -1032.3114
expansions: [(36, 1), (166, 7)]
discards: [  0  67  76  77  78  79  80  81  82  83  84  85  86  87 103 104 129 130
 131 132 133 134 135 136 137 152 154]
Re-initialized the encoder parameters.
Fitting a model of length 192 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 18s - loss: 1028.5299 - loglik: -1.0271e+03 - logprior: -1.3856e+00
Epoch 2/10
51/51 - 14s - loss: 1020.8777 - loglik: -1.0202e+03 - logprior: -6.3738e-01
Epoch 3/10
51/51 - 14s - loss: 1021.3596 - loglik: -1.0207e+03 - logprior: -6.3463e-01
Fitted a model with MAP estimate = -1017.0708
Time for alignment: 237.0878
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1198.0667 - loglik: -1.1964e+03 - logprior: -1.6750e+00
Epoch 2/10
39/39 - 9s - loss: 1147.6539 - loglik: -1.1467e+03 - logprior: -9.3237e-01
Epoch 3/10
39/39 - 9s - loss: 1139.5736 - loglik: -1.1386e+03 - logprior: -9.2964e-01
Epoch 4/10
39/39 - 9s - loss: 1134.7458 - loglik: -1.1338e+03 - logprior: -8.9485e-01
Epoch 5/10
39/39 - 9s - loss: 1130.9614 - loglik: -1.1301e+03 - logprior: -8.8125e-01
Epoch 6/10
39/39 - 9s - loss: 1126.0532 - loglik: -1.1252e+03 - logprior: -8.6950e-01
Epoch 7/10
39/39 - 9s - loss: 1114.4987 - loglik: -1.1136e+03 - logprior: -8.3077e-01
Epoch 8/10
39/39 - 9s - loss: 1072.3500 - loglik: -1.0713e+03 - logprior: -9.8910e-01
Epoch 9/10
39/39 - 9s - loss: 905.5278 - loglik: -9.0132e+02 - logprior: -4.1622e+00
Epoch 10/10
39/39 - 9s - loss: 864.1163 - loglik: -8.5985e+02 - logprior: -4.1980e+00
Fitted a model with MAP estimate = -943.1446
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  97  98 116 117 154]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 8s - loss: 1256.1362 - loglik: -1.2526e+03 - logprior: -3.4876e+00
Epoch 2/2
19/19 - 5s - loss: 1241.5465 - loglik: -1.2408e+03 - logprior: -7.8075e-01
Fitted a model with MAP estimate = -1127.2636
expansions: [(0, 95), (101, 110)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100]
Re-initialized the encoder parameters.
Fitting a model of length 205 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1184.4719 - loglik: -1.1827e+03 - logprior: -1.7355e+00
Epoch 2/2
39/39 - 12s - loss: 1132.6157 - loglik: -1.1317e+03 - logprior: -9.6457e-01
Fitted a model with MAP estimate = -1030.3271
expansions: [(13, 1), (47, 1), (98, 2), (160, 3), (169, 3)]
discards: [  0  50  51  52  53  54 146 147 148 149 150 151 152 153 162 163 164 165]
Re-initialized the encoder parameters.
Fitting a model of length 197 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 18s - loss: 1026.5093 - loglik: -1.0251e+03 - logprior: -1.3792e+00
Epoch 2/10
51/51 - 15s - loss: 1022.0647 - loglik: -1.0215e+03 - logprior: -6.1043e-01
Epoch 3/10
51/51 - 15s - loss: 1019.9934 - loglik: -1.0194e+03 - logprior: -6.0880e-01
Epoch 4/10
51/51 - 15s - loss: 1015.1791 - loglik: -1.0146e+03 - logprior: -5.7517e-01
Epoch 5/10
51/51 - 15s - loss: 1012.3644 - loglik: -1.0118e+03 - logprior: -5.5945e-01
Epoch 6/10
51/51 - 15s - loss: 1009.2162 - loglik: -1.0086e+03 - logprior: -5.5895e-01
Epoch 7/10
51/51 - 15s - loss: 999.7136 - loglik: -9.9912e+02 - logprior: -5.6121e-01
Epoch 8/10
51/51 - 15s - loss: 969.4872 - loglik: -9.6874e+02 - logprior: -6.9934e-01
Epoch 9/10
51/51 - 15s - loss: 859.5027 - loglik: -8.5773e+02 - logprior: -1.7016e+00
Epoch 10/10
51/51 - 15s - loss: 793.2103 - loglik: -7.9120e+02 - logprior: -1.9144e+00
Fitted a model with MAP estimate = -778.9377
Time for alignment: 342.0954
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1199.7025 - loglik: -1.1980e+03 - logprior: -1.6527e+00
Epoch 2/10
39/39 - 9s - loss: 1152.3801 - loglik: -1.1514e+03 - logprior: -9.3469e-01
Epoch 3/10
39/39 - 9s - loss: 1143.2250 - loglik: -1.1422e+03 - logprior: -9.8935e-01
Epoch 4/10
39/39 - 9s - loss: 1138.5276 - loglik: -1.1375e+03 - logprior: -9.7594e-01
Epoch 5/10
39/39 - 9s - loss: 1134.3207 - loglik: -1.1333e+03 - logprior: -9.8080e-01
Epoch 6/10
39/39 - 9s - loss: 1129.5035 - loglik: -1.1285e+03 - logprior: -9.6337e-01
Epoch 7/10
39/39 - 9s - loss: 1117.8663 - loglik: -1.1169e+03 - logprior: -9.2723e-01
Epoch 8/10
39/39 - 9s - loss: 1074.6761 - loglik: -1.0734e+03 - logprior: -1.2103e+00
Epoch 9/10
39/39 - 9s - loss: 909.0366 - loglik: -9.0482e+02 - logprior: -4.1630e+00
Epoch 10/10
39/39 - 9s - loss: 864.5101 - loglik: -8.6026e+02 - logprior: -4.1887e+00
Fitted a model with MAP estimate = -942.6784
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  92  93  94  95  96  97  98 154]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 10s - loss: 1260.1263 - loglik: -1.2567e+03 - logprior: -3.4261e+00
Epoch 2/2
19/19 - 5s - loss: 1239.8602 - loglik: -1.2387e+03 - logprior: -1.1138e+00
Fitted a model with MAP estimate = -1125.4370
expansions: [(0, 81), (101, 128)]
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  46  47  48  49  50  51  52  53  54  55  56
  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74
  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92
  93  94  95  96  97  98  99 100]
Re-initialized the encoder parameters.
Fitting a model of length 212 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1184.6406 - loglik: -1.1829e+03 - logprior: -1.7591e+00
Epoch 2/2
39/39 - 13s - loss: 1135.4622 - loglik: -1.1345e+03 - logprior: -9.9955e-01
Fitted a model with MAP estimate = -1033.2304
expansions: [(38, 1)]
discards: [  0  43  68  81  82  83  84 121 122 124 139 151 152 153]
Re-initialized the encoder parameters.
Fitting a model of length 199 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 18s - loss: 1031.6099 - loglik: -1.0302e+03 - logprior: -1.4496e+00
Epoch 2/10
51/51 - 15s - loss: 1023.2791 - loglik: -1.0226e+03 - logprior: -7.2415e-01
Epoch 3/10
51/51 - 15s - loss: 1023.5532 - loglik: -1.0228e+03 - logprior: -7.3023e-01
Fitted a model with MAP estimate = -1019.3756
Time for alignment: 240.1428
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1197.6106 - loglik: -1.1960e+03 - logprior: -1.6508e+00
Epoch 2/10
39/39 - 9s - loss: 1149.5763 - loglik: -1.1487e+03 - logprior: -8.7166e-01
Epoch 3/10
39/39 - 9s - loss: 1142.8495 - loglik: -1.1419e+03 - logprior: -9.2116e-01
Epoch 4/10
39/39 - 9s - loss: 1138.1473 - loglik: -1.1372e+03 - logprior: -9.5366e-01
Epoch 5/10
39/39 - 9s - loss: 1132.8956 - loglik: -1.1320e+03 - logprior: -9.1823e-01
Epoch 6/10
39/39 - 9s - loss: 1125.9763 - loglik: -1.1251e+03 - logprior: -8.9080e-01
Epoch 7/10
39/39 - 9s - loss: 1113.9532 - loglik: -1.1131e+03 - logprior: -8.6446e-01
Epoch 8/10
39/39 - 9s - loss: 1068.5551 - loglik: -1.0674e+03 - logprior: -1.0755e+00
Epoch 9/10
39/39 - 9s - loss: 899.2358 - loglik: -8.9493e+02 - logprior: -4.2508e+00
Epoch 10/10
39/39 - 9s - loss: 863.4644 - loglik: -8.5916e+02 - logprior: -4.2426e+00
Fitted a model with MAP estimate = -942.9382
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49 117 118 154]
Re-initialized the encoder parameters.
Fitting a model of length 102 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1252.7091 - loglik: -1.2498e+03 - logprior: -2.8692e+00
Epoch 2/2
19/19 - 5s - loss: 1238.5854 - loglik: -1.2385e+03 - logprior: -4.2960e-02
Fitted a model with MAP estimate = -1124.3707
expansions: [(0, 92), (102, 116)]
discards: [  5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22
  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40
  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58
  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76
  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94
  95  96  97  98  99 100 101]
Re-initialized the encoder parameters.
Fitting a model of length 213 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 16s - loss: 1186.1691 - loglik: -1.1834e+03 - logprior: -2.8032e+00
Epoch 2/2
39/39 - 13s - loss: 1131.2927 - loglik: -1.1302e+03 - logprior: -1.1067e+00
Fitted a model with MAP estimate = -1029.7366
expansions: [(125, 1)]
discards: [  0  32  33  38  47  48  49  59  64  72  78  79  80 156 157 158 159 187]
Re-initialized the encoder parameters.
Fitting a model of length 196 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 18s - loss: 1027.8658 - loglik: -1.0265e+03 - logprior: -1.3795e+00
Epoch 2/10
51/51 - 15s - loss: 1024.1011 - loglik: -1.0235e+03 - logprior: -6.1209e-01
Epoch 3/10
51/51 - 14s - loss: 1019.5201 - loglik: -1.0189e+03 - logprior: -5.7365e-01
Epoch 4/10
51/51 - 15s - loss: 1017.5709 - loglik: -1.0170e+03 - logprior: -5.5047e-01
Epoch 5/10
51/51 - 15s - loss: 1014.7798 - loglik: -1.0142e+03 - logprior: -5.2722e-01
Epoch 6/10
51/51 - 15s - loss: 1007.5427 - loglik: -1.0070e+03 - logprior: -5.0896e-01
Epoch 7/10
51/51 - 15s - loss: 1001.2493 - loglik: -1.0007e+03 - logprior: -5.3032e-01
Epoch 8/10
51/51 - 15s - loss: 968.3585 - loglik: -9.6765e+02 - logprior: -6.5984e-01
Epoch 9/10
51/51 - 14s - loss: 856.8296 - loglik: -8.5515e+02 - logprior: -1.6090e+00
Epoch 10/10
51/51 - 15s - loss: 790.3160 - loglik: -7.8848e+02 - logprior: -1.7384e+00
Fitted a model with MAP estimate = -776.4642
Time for alignment: 341.7571
Fitting a model of length 155 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
39/39 - 12s - loss: 1198.0696 - loglik: -1.1964e+03 - logprior: -1.6671e+00
Epoch 2/10
39/39 - 9s - loss: 1148.3292 - loglik: -1.1474e+03 - logprior: -9.3278e-01
Epoch 3/10
39/39 - 9s - loss: 1141.2267 - loglik: -1.1403e+03 - logprior: -9.2167e-01
Epoch 4/10
39/39 - 9s - loss: 1136.5023 - loglik: -1.1356e+03 - logprior: -9.3610e-01
Epoch 5/10
39/39 - 9s - loss: 1133.1740 - loglik: -1.1322e+03 - logprior: -9.4125e-01
Epoch 6/10
39/39 - 9s - loss: 1127.9639 - loglik: -1.1271e+03 - logprior: -8.7750e-01
Epoch 7/10
39/39 - 9s - loss: 1115.4628 - loglik: -1.1146e+03 - logprior: -8.3639e-01
Epoch 8/10
39/39 - 9s - loss: 1069.5133 - loglik: -1.0684e+03 - logprior: -1.0462e+00
Epoch 9/10
39/39 - 9s - loss: 904.8050 - loglik: -9.0068e+02 - logprior: -4.0694e+00
Epoch 10/10
39/39 - 9s - loss: 865.7105 - loglik: -8.6151e+02 - logprior: -4.1358e+00
Fitted a model with MAP estimate = -942.6207
expansions: []
discards: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50 115 116 154]
Re-initialized the encoder parameters.
Fitting a model of length 101 on 10000 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
19/19 - 7s - loss: 1259.4314 - loglik: -1.2561e+03 - logprior: -3.3732e+00
Epoch 2/2
19/19 - 5s - loss: 1241.2822 - loglik: -1.2407e+03 - logprior: -6.1898e-01
Fitted a model with MAP estimate = -1126.9456
expansions: [(0, 97), (101, 109)]
discards: [  0   1   2   3   4   9  10  11  12  13  14  15  16  17  18  19  20  21
  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39
  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57
  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75
  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93
  94  95  96  97  98  99 100]
Re-initialized the encoder parameters.
Fitting a model of length 210 on 10000 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
39/39 - 18s - loss: 1185.7095 - loglik: -1.1840e+03 - logprior: -1.7483e+00
Epoch 2/2
39/39 - 13s - loss: 1138.7833 - loglik: -1.1378e+03 - logprior: -9.6581e-01
Fitted a model with MAP estimate = -1036.2036
expansions: []
discards: [  0 117 124 125 126 127 128 129 130 131 132 133 136 137 138 139 140]
Re-initialized the encoder parameters.
Fitting a model of length 193 on 17200 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
51/51 - 18s - loss: 1032.0605 - loglik: -1.0306e+03 - logprior: -1.4443e+00
Epoch 2/10
51/51 - 14s - loss: 1023.6431 - loglik: -1.0230e+03 - logprior: -6.3439e-01
Epoch 3/10
51/51 - 14s - loss: 1021.6731 - loglik: -1.0210e+03 - logprior: -6.3621e-01
Epoch 4/10
51/51 - 15s - loss: 1019.1505 - loglik: -1.0185e+03 - logprior: -6.1956e-01
Epoch 5/10
51/51 - 14s - loss: 1015.7908 - loglik: -1.0152e+03 - logprior: -6.0220e-01
Epoch 6/10
51/51 - 14s - loss: 1009.2501 - loglik: -1.0086e+03 - logprior: -6.0085e-01
Epoch 7/10
51/51 - 15s - loss: 1002.8246 - loglik: -1.0022e+03 - logprior: -5.9097e-01
Epoch 8/10
51/51 - 15s - loss: 972.0867 - loglik: -9.7135e+02 - logprior: -6.8321e-01
Epoch 9/10
51/51 - 14s - loss: 855.5312 - loglik: -8.5362e+02 - logprior: -1.8353e+00
Epoch 10/10
51/51 - 14s - loss: 790.0935 - loglik: -7.8805e+02 - logprior: -1.9431e+00
Fitted a model with MAP estimate = -777.7654
Time for alignment: 338.5078
Computed alignments with likelihoods: ['-943.3090', '-778.9377', '-942.6784', '-776.4642', '-777.7654']
Best model has likelihood: -776.4642  (prior= -1.7814 )
time for generating output: 0.2769
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/blmb.projection.fasta
SP score = 0.03650586701434159
Training of 5 independent models on file proteasome.fasta
Configuration:
transition_init : {'begin_to_match': <learnMSA.msa_hmm.Configuration.EntryInitializer object at 0x7fe84049d2e0>, 'match_to_end': <learnMSA.msa_hmm.Configuration.ExitInitializer object at 0x7fe84049dbe0>, 'match_to_match': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474d00>, 'match_to_insert': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474070>, 'insert_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474760>, 'insert_to_insert': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474dc0>, 'match_to_delete': <learnMSA.msa_hmm.Configuration.MatchTransitionInitializer object at 0x7fe840474790>, 'delete_to_match': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e20>, 'delete_to_delete': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474a60>, 'left_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474be0>, 'left_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474490>, 'right_flank_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474e80>, 'right_flank_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474160>, 'unannotated_segment_loop': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474ee0>, 'unannotated_segment_exit': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f10>, 'end_to_unannotated_segment': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fd0>, 'end_to_right_flank': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474fa0>, 'end_to_terminal': <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474f40>} , flank_init : <tensorflow.python.keras.initializers.initializers_v2.Zeros object at 0x7fe840474820> , emission_init : <learnMSA.msa_hmm.Configuration.EmissionInitializer object at 0x7fe8404740d0>
 , insertion_init : <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe840474220> , alpha_flank : 7000 , alpha_single : 1000000000.0
 , alpha_frag : 10000.0 , max_surgery_runs : 4 , length_init_quantile : 0.5
 , surgery_quantile : 0.5 , min_surgery_seqs : 10000.0 , len_mul : 0.8
 , batch_size : <function get_adaptive_batch_size at 0x7fe8404c5430> , learning_rate : 0.1 , epochs : [10, 2, 10]
 , use_prior : True , dirichlet_mix_comp_count : 1 , use_anc_probs : True
 , trainable_rate_matrices : True , encoder_initializer : [<tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea3bec9280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fe9ab079280>, <tensorflow.python.ops.init_ops_v2.Constant object at 0x7fea09175340>] , frozen_insertions : True
 , surgery_del : 0.5 , surgery_ins : 0.5 , emission_func : <function matvec at 0x7fe961c7a790>
 , emission_matrix_generator : <function make_default_emission_matrix at 0x7fe956650820> , emission_prior : <learnMSA.msa_hmm.MsaHmmCell.AminoAcidPrior object at 0x7fe8404744c0> , kernel_dim : alphabet_size
 , num_rate_matrices : 1 , per_matrix_rate : False , matrix_rate_l2 : 1e-07
 , shared_rate_matrix : False , equilibrium_sample : True
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 989.5435 - loglik: -9.8361e+02 - logprior: -5.9326e+00
Epoch 2/10
14/14 - 4s - loss: 939.3239 - loglik: -9.3807e+02 - logprior: -1.2524e+00
Epoch 3/10
14/14 - 3s - loss: 905.2584 - loglik: -9.0404e+02 - logprior: -1.2166e+00
Epoch 4/10
14/14 - 3s - loss: 891.6874 - loglik: -8.9052e+02 - logprior: -1.1655e+00
Epoch 5/10
14/14 - 4s - loss: 885.9626 - loglik: -8.8482e+02 - logprior: -1.1419e+00
Epoch 6/10
14/14 - 3s - loss: 882.3600 - loglik: -8.8129e+02 - logprior: -1.0672e+00
Epoch 7/10
14/14 - 3s - loss: 881.0811 - loglik: -8.8005e+02 - logprior: -1.0255e+00
Epoch 8/10
14/14 - 3s - loss: 874.1009 - loglik: -8.7308e+02 - logprior: -1.0086e+00
Epoch 9/10
14/14 - 3s - loss: 873.5358 - loglik: -8.7252e+02 - logprior: -1.0090e+00
Epoch 10/10
14/14 - 4s - loss: 869.8241 - loglik: -8.6880e+02 - logprior: -1.0166e+00
Fitted a model with MAP estimate = -868.8198
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (61, 1), (69, 1), (73, 1), (83, 1), (103, 6), (117, 1), (124, 1), (128, 1), (131, 1), (133, 3), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 889.9883 - loglik: -8.8580e+02 - logprior: -4.1885e+00
Epoch 2/2
29/29 - 5s - loss: 868.0764 - loglik: -8.6731e+02 - logprior: -7.6944e-01
Fitted a model with MAP estimate = -865.5033
expansions: [(125, 1), (126, 1), (164, 1), (165, 1)]
discards: [41 91]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 868.9713 - loglik: -8.6597e+02 - logprior: -2.9992e+00
Epoch 2/2
29/29 - 5s - loss: 865.5454 - loglik: -8.6499e+02 - logprior: -5.5248e-01
Fitted a model with MAP estimate = -863.6541
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 12s - loss: 868.0271 - loglik: -8.6509e+02 - logprior: -2.9377e+00
Epoch 2/10
29/29 - 5s - loss: 863.5633 - loglik: -8.6315e+02 - logprior: -4.0815e-01
Epoch 3/10
29/29 - 5s - loss: 863.8406 - loglik: -8.6361e+02 - logprior: -2.3063e-01
Fitted a model with MAP estimate = -861.5348
Time for alignment: 112.1081
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 990.4064 - loglik: -9.8447e+02 - logprior: -5.9328e+00
Epoch 2/10
14/14 - 3s - loss: 937.7800 - loglik: -9.3654e+02 - logprior: -1.2402e+00
Epoch 3/10
14/14 - 3s - loss: 903.0689 - loglik: -9.0187e+02 - logprior: -1.1963e+00
Epoch 4/10
14/14 - 3s - loss: 890.6778 - loglik: -8.8961e+02 - logprior: -1.0643e+00
Epoch 5/10
14/14 - 4s - loss: 884.2106 - loglik: -8.8315e+02 - logprior: -1.0594e+00
Epoch 6/10
14/14 - 3s - loss: 881.7882 - loglik: -8.8073e+02 - logprior: -1.0507e+00
Epoch 7/10
14/14 - 3s - loss: 876.7062 - loglik: -8.7567e+02 - logprior: -1.0285e+00
Epoch 8/10
14/14 - 3s - loss: 873.4058 - loglik: -8.7238e+02 - logprior: -1.0158e+00
Epoch 9/10
14/14 - 3s - loss: 871.3657 - loglik: -8.7036e+02 - logprior: -9.9765e-01
Epoch 10/10
14/14 - 3s - loss: 869.2361 - loglik: -8.6822e+02 - logprior: -1.0042e+00
Fitted a model with MAP estimate = -866.6943
expansions: [(0, 3), (20, 1), (21, 1), (23, 1), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (56, 1), (57, 1), (69, 1), (71, 1), (74, 1), (102, 4), (103, 3), (117, 1), (124, 1), (128, 1), (131, 1), (133, 3), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 888.8372 - loglik: -8.8464e+02 - logprior: -4.2019e+00
Epoch 2/2
29/29 - 5s - loss: 868.8619 - loglik: -8.6802e+02 - logprior: -8.3943e-01
Fitted a model with MAP estimate = -866.2705
expansions: [(96, 1), (165, 1)]
discards: [41]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 869.5715 - loglik: -8.6647e+02 - logprior: -3.1039e+00
Epoch 2/2
29/29 - 5s - loss: 865.9134 - loglik: -8.6529e+02 - logprior: -6.2359e-01
Fitted a model with MAP estimate = -864.1846
expansions: [(125, 1)]
discards: [126]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 9s - loss: 868.7153 - loglik: -8.6578e+02 - logprior: -2.9310e+00
Epoch 2/10
29/29 - 5s - loss: 865.6105 - loglik: -8.6519e+02 - logprior: -4.1721e-01
Epoch 3/10
29/29 - 5s - loss: 863.1534 - loglik: -8.6292e+02 - logprior: -2.2769e-01
Epoch 4/10
29/29 - 5s - loss: 861.3772 - loglik: -8.6125e+02 - logprior: -1.1986e-01
Epoch 5/10
29/29 - 5s - loss: 856.9103 - loglik: -8.5689e+02 - logprior: -1.7047e-02
Epoch 6/10
29/29 - 5s - loss: 854.5067 - loglik: -8.5457e+02 - logprior: 0.0709
Epoch 7/10
29/29 - 5s - loss: 847.4505 - loglik: -8.4758e+02 - logprior: 0.1430
Epoch 8/10
29/29 - 5s - loss: 832.3474 - loglik: -8.3247e+02 - logprior: 0.1364
Epoch 9/10
29/29 - 5s - loss: 797.3741 - loglik: -7.9733e+02 - logprior: -2.1980e-02
Epoch 10/10
29/29 - 5s - loss: 742.5040 - loglik: -7.4252e+02 - logprior: 0.0493
Fitted a model with MAP estimate = -721.8728
Time for alignment: 149.9465
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 7s - loss: 990.1569 - loglik: -9.8422e+02 - logprior: -5.9392e+00
Epoch 2/10
14/14 - 3s - loss: 932.0770 - loglik: -9.3081e+02 - logprior: -1.2714e+00
Epoch 3/10
14/14 - 3s - loss: 902.8483 - loglik: -9.0163e+02 - logprior: -1.2136e+00
Epoch 4/10
14/14 - 3s - loss: 890.0845 - loglik: -8.8897e+02 - logprior: -1.1128e+00
Epoch 5/10
14/14 - 3s - loss: 883.4384 - loglik: -8.8234e+02 - logprior: -1.0923e+00
Epoch 6/10
14/14 - 3s - loss: 882.6535 - loglik: -8.8159e+02 - logprior: -1.0595e+00
Epoch 7/10
14/14 - 3s - loss: 877.1918 - loglik: -8.7614e+02 - logprior: -1.0499e+00
Epoch 8/10
14/14 - 3s - loss: 873.6353 - loglik: -8.7257e+02 - logprior: -1.0543e+00
Epoch 9/10
14/14 - 3s - loss: 872.2476 - loglik: -8.7118e+02 - logprior: -1.0533e+00
Epoch 10/10
14/14 - 3s - loss: 866.6473 - loglik: -8.6558e+02 - logprior: -1.0597e+00
Fitted a model with MAP estimate = -865.7323
expansions: [(0, 2), (9, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (33, 1), (49, 1), (50, 1), (52, 1), (53, 1), (54, 1), (57, 1), (65, 1), (70, 3), (89, 1), (102, 3), (103, 3), (118, 1), (124, 1), (128, 1), (131, 1), (133, 2), (134, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 891.4325 - loglik: -8.8718e+02 - logprior: -4.2493e+00
Epoch 2/2
29/29 - 5s - loss: 867.3212 - loglik: -8.6651e+02 - logprior: -8.1175e-01
Fitted a model with MAP estimate = -865.3631
expansions: [(89, 1), (125, 1), (128, 1)]
discards: [126 165]
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 868.2697 - loglik: -8.6528e+02 - logprior: -2.9921e+00
Epoch 2/2
29/29 - 5s - loss: 864.8931 - loglik: -8.6436e+02 - logprior: -5.3069e-01
Fitted a model with MAP estimate = -863.5300
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 182 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 9s - loss: 868.0512 - loglik: -8.6512e+02 - logprior: -2.9290e+00
Epoch 2/10
29/29 - 5s - loss: 863.6511 - loglik: -8.6326e+02 - logprior: -3.8795e-01
Epoch 3/10
29/29 - 5s - loss: 862.8672 - loglik: -8.6265e+02 - logprior: -2.1119e-01
Epoch 4/10
29/29 - 5s - loss: 859.6086 - loglik: -8.5950e+02 - logprior: -1.0320e-01
Epoch 5/10
29/29 - 5s - loss: 856.8887 - loglik: -8.5688e+02 - logprior: -3.0210e-05
Epoch 6/10
29/29 - 5s - loss: 852.8529 - loglik: -8.5294e+02 - logprior: 0.0972
Epoch 7/10
29/29 - 5s - loss: 845.7312 - loglik: -8.4588e+02 - logprior: 0.1590
Epoch 8/10
29/29 - 5s - loss: 832.0074 - loglik: -8.3215e+02 - logprior: 0.1607
Epoch 9/10
29/29 - 5s - loss: 798.1443 - loglik: -7.9814e+02 - logprior: 0.0166
Epoch 10/10
29/29 - 5s - loss: 748.4208 - loglik: -7.4845e+02 - logprior: 0.0565
Fitted a model with MAP estimate = -724.2538
Time for alignment: 149.2153
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 989.6188 - loglik: -9.8368e+02 - logprior: -5.9395e+00
Epoch 2/10
14/14 - 3s - loss: 936.5918 - loglik: -9.3534e+02 - logprior: -1.2471e+00
Epoch 3/10
14/14 - 3s - loss: 903.2509 - loglik: -9.0201e+02 - logprior: -1.2414e+00
Epoch 4/10
14/14 - 3s - loss: 890.0322 - loglik: -8.8889e+02 - logprior: -1.1426e+00
Epoch 5/10
14/14 - 3s - loss: 885.1981 - loglik: -8.8411e+02 - logprior: -1.0874e+00
Epoch 6/10
14/14 - 3s - loss: 881.3198 - loglik: -8.8026e+02 - logprior: -1.0570e+00
Epoch 7/10
14/14 - 3s - loss: 877.8561 - loglik: -8.7680e+02 - logprior: -1.0542e+00
Epoch 8/10
14/14 - 3s - loss: 875.0723 - loglik: -8.7401e+02 - logprior: -1.0521e+00
Epoch 9/10
14/14 - 3s - loss: 871.6557 - loglik: -8.7059e+02 - logprior: -1.0547e+00
Epoch 10/10
14/14 - 3s - loss: 868.5491 - loglik: -8.6746e+02 - logprior: -1.0754e+00
Fitted a model with MAP estimate = -866.2548
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 1), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (54, 1), (57, 1), (64, 2), (65, 1), (75, 1), (82, 1), (89, 1), (102, 3), (103, 3), (117, 1), (120, 2), (128, 1), (131, 1), (133, 1), (136, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 892.0922 - loglik: -8.8790e+02 - logprior: -4.1937e+00
Epoch 2/2
29/29 - 5s - loss: 866.5381 - loglik: -8.6569e+02 - logprior: -8.4709e-01
Fitted a model with MAP estimate = -864.8359
expansions: [(82, 1), (127, 1), (130, 1)]
discards: [ 41 128 150]
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 870.2760 - loglik: -8.6728e+02 - logprior: -2.9960e+00
Epoch 2/2
29/29 - 5s - loss: 863.6467 - loglik: -8.6311e+02 - logprior: -5.3401e-01
Fitted a model with MAP estimate = -863.3844
expansions: []
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 183 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 866.9005 - loglik: -8.6397e+02 - logprior: -2.9274e+00
Epoch 2/10
29/29 - 5s - loss: 865.9017 - loglik: -8.6551e+02 - logprior: -3.9603e-01
Epoch 3/10
29/29 - 5s - loss: 861.8301 - loglik: -8.6161e+02 - logprior: -2.2053e-01
Epoch 4/10
29/29 - 5s - loss: 860.2491 - loglik: -8.6013e+02 - logprior: -1.2065e-01
Epoch 5/10
29/29 - 5s - loss: 855.6441 - loglik: -8.5562e+02 - logprior: -1.5894e-02
Epoch 6/10
29/29 - 5s - loss: 855.0756 - loglik: -8.5514e+02 - logprior: 0.0762
Epoch 7/10
29/29 - 5s - loss: 848.2101 - loglik: -8.4834e+02 - logprior: 0.1403
Epoch 8/10
29/29 - 5s - loss: 835.5162 - loglik: -8.3565e+02 - logprior: 0.1521
Epoch 9/10
29/29 - 5s - loss: 803.3619 - loglik: -8.0338e+02 - logprior: 0.0355
Epoch 10/10
29/29 - 5s - loss: 752.9472 - loglik: -7.5292e+02 - logprior: 0.0025
Fitted a model with MAP estimate = -723.8917
Time for alignment: 149.2957
Fitting a model of length 146 on 5732 sequences.
Batch size= 512 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
14/14 - 6s - loss: 988.9940 - loglik: -9.8306e+02 - logprior: -5.9311e+00
Epoch 2/10
14/14 - 3s - loss: 936.6465 - loglik: -9.3539e+02 - logprior: -1.2536e+00
Epoch 3/10
14/14 - 3s - loss: 902.4381 - loglik: -9.0123e+02 - logprior: -1.2110e+00
Epoch 4/10
14/14 - 3s - loss: 888.9947 - loglik: -8.8785e+02 - logprior: -1.1424e+00
Epoch 5/10
14/14 - 3s - loss: 884.3708 - loglik: -8.8325e+02 - logprior: -1.1177e+00
Epoch 6/10
14/14 - 3s - loss: 880.5157 - loglik: -8.7943e+02 - logprior: -1.0794e+00
Epoch 7/10
14/14 - 3s - loss: 877.2759 - loglik: -8.7624e+02 - logprior: -1.0338e+00
Epoch 8/10
14/14 - 3s - loss: 873.1339 - loglik: -8.7213e+02 - logprior: -9.9513e-01
Epoch 9/10
14/14 - 3s - loss: 871.1234 - loglik: -8.7013e+02 - logprior: -9.8746e-01
Epoch 10/10
14/14 - 3s - loss: 869.4429 - loglik: -8.6844e+02 - logprior: -9.9039e-01
Fitted a model with MAP estimate = -866.0366
expansions: [(0, 2), (10, 1), (22, 1), (23, 2), (24, 2), (25, 1), (26, 1), (32, 2), (49, 1), (50, 1), (52, 1), (53, 1), (58, 1), (66, 2), (90, 2), (102, 4), (103, 3), (117, 1), (124, 1), (128, 1), (131, 1), (133, 1), (137, 1), (139, 1)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 181 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 891.9448 - loglik: -8.8774e+02 - logprior: -4.2069e+00
Epoch 2/2
29/29 - 5s - loss: 868.1676 - loglik: -8.6741e+02 - logprior: -7.5995e-01
Fitted a model with MAP estimate = -866.1091
expansions: [(125, 1)]
discards: [ 30  42  84 109 127]
Re-initialized the encoder parameters.
Fitting a model of length 177 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/2
29/29 - 8s - loss: 870.4843 - loglik: -8.6745e+02 - logprior: -3.0326e+00
Epoch 2/2
29/29 - 5s - loss: 866.4481 - loglik: -8.6588e+02 - logprior: -5.6488e-01
Fitted a model with MAP estimate = -865.4332
expansions: [(85, 3)]
discards: []
Re-initialized the encoder parameters.
Fitting a model of length 180 on 5732 sequences.
Batch size= 256 Learning rate= 0.1
Using 1 GPUs.
Epoch 1/10
29/29 - 10s - loss: 869.6069 - loglik: -8.6664e+02 - logprior: -2.9694e+00
Epoch 2/10
29/29 - 5s - loss: 864.1971 - loglik: -8.6374e+02 - logprior: -4.5955e-01
Epoch 3/10
29/29 - 5s - loss: 862.4479 - loglik: -8.6215e+02 - logprior: -2.9518e-01
Epoch 4/10
29/29 - 5s - loss: 861.8190 - loglik: -8.6167e+02 - logprior: -1.4963e-01
Epoch 5/10
29/29 - 5s - loss: 856.2432 - loglik: -8.5618e+02 - logprior: -5.6015e-02
Epoch 6/10
29/29 - 5s - loss: 852.2665 - loglik: -8.5229e+02 - logprior: 0.0308
Epoch 7/10
29/29 - 5s - loss: 847.3607 - loglik: -8.4744e+02 - logprior: 0.0862
Epoch 8/10
29/29 - 5s - loss: 830.0494 - loglik: -8.3010e+02 - logprior: 0.0615
Epoch 9/10
29/29 - 5s - loss: 800.0760 - loglik: -8.0000e+02 - logprior: -5.2809e-02
Epoch 10/10
29/29 - 5s - loss: 749.9263 - loglik: -7.5000e+02 - logprior: 0.1049
Fitted a model with MAP estimate = -726.1756
Time for alignment: 147.2458
Computed alignments with likelihoods: ['-861.5348', '-721.8728', '-724.2538', '-723.8917', '-726.1756']
Best model has likelihood: -721.8728  (prior= 0.3358 )
time for generating output: 0.2467
Wrote file ../MSA-HMM-Analysis/results/learnMSA_new/alignments/proteasome.projection.fasta
SP score = 0.019087895759515375
